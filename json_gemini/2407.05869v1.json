{"title": "PORCA: Root Cause Analysis with Partially Observed Data", "authors": ["Chang Gong", "Di Yao", "Jin Wang", "Wenbin Li", "Lanting Fang", "Yongtao Xie", "Kaiyu Feng", "Peng Han", "Jingping Bi"], "abstract": "Root Cause Analysis (RCA) aims at identifying the underlying causes of system faults by uncovering and analyzing the causal structure from complex systems. It has been widely used in many application domains. Reliable diagnostic conclusions are of great importance in mitigating system failures and financial losses. However, previous studies implicitly assume a full observation of the system, which neglect the effect of partial observation (i.e., missing nodes and latent malfunction). As a result, they fail in deriving reliable RCA results.\nIn this paper, we unveil the issues of unobserved confounders and heterogeneity in partial observation and come up with a new problem of root cause analysis with partially observed data. To achieve this, we propose PORCA, a novel RCA framework which can explore reliable root causes under both unobserved confounders and unobserved heterogeneity. PORCA leverages magnified score-based causal discovery to efficiently optimize acyclic directed mixed graph under unobserved confounders. In addition, we also develop a heterogeneity-aware scheduling strategy to provide adaptive sample weights. Extensive experimental results on one synthetic and two real-world datasets demonstrate the effectiveness and superiority of the proposed framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Root Cause Analysis (RCA) aims to identify the underlining system failure so as to ensure the availability and reliability. It has been widely used in many fields such as telecommu-nication [1], IT operations [2], [3], and manufacturing [4], [5]. There are complex dependencies and interactions between components in systems of these fields. Due to the large scale and complexity of such systems, they are vulnerable to failures, which could potentially lead to huge economic loss and degradation of user experiences. Many companies (e.g., Google, Amazon, Alibaba) have made huge efforts to apply RCA in system monitoring to address such issues.\nThere is a long stream of research in both data mining [3], [6], [7], [8] and software engineering [9] communities to ensure reliable RCA. The objective is to provide reliable and robust localization for engineers to help address failures in a timely manner and avoid meaningless labours due to noisy false alarms. With the development of causal inference and trustworthy machine learning [10], [11], [12], [13] techniques, recent efforts build a causal dependency graph to represent system architecture and utilize causal discovery methods to boost the RCA pipeline [7], [9], [14], [15], [16]. In such a graph, the nodes represent the performance metrics or services while edges represent the casual effects between them. However, most of existing RCA approaches impose stringent assumption on causal sufficiency and neglect the existence and potential hazards of missing data. The example in Figure 1 shows a simplified illustration of a manufacturing testbed, the actuator can manipulate both two downstream pumps. In many cases, small entities related human activities like this are often neglected or not monitored in system analysis [17], [18]. Besides, the attack or latent malfunction on the pump node in Figure 1 would change the data distribution and cause spurious edges in causal analysis. Similar issues also exist in a wide range of AIOps. For example, in microservice systems, biased models and misleading causal conclusion will result in false alarms and missing calls in root cause analysis [19], [20]. Some previous studies emphasize on the concept of \"observability\" and leverage data engineering techniques, e.g., increasing the number of monitoring metrics or dive into metric, trace, and log data iteratively, to enhance perception domain [2]. However, it is difficult to avoid such issues solely on the data level in real world application.\nWitnessing the limitation of existing solutions, we argue that RCA should be able to automatically recognize and mitigate issues caused by partial observation on the model level. To reach this goal, we need to address technical challenges from two aspects:\nUnobserved Confounders. In the partially observed data, the neglected entities in the system may serve as unobserved confounders and lead to spurious correlation in root cause analysis. Without awareness of unobserved confounders, false edges and misleading causal conclusions may be derived, which will lead to false alarms in root cause analysis. But the practical constraints and statistical limitations make it difficult to mitigate their effects. On the one hand, vanilla structural causal models are misspecified in practice and might fail to account for latent nodes [21]. On the other hand, although some studies explore the additional statistical rules [22] for uncovering relations with confounders, the heavy computa-tional cost restricts their utilization in RCA.\nUnobserved Heterogeneity. Previous studies [23], [24] have proved that the heterogeneity of exogenous factors or noise distributions will lead to spurious edges and causal conclu-sions. Nevertheless, it's nontrivial to account for heterogeneity in RCA, as they are latent to analyzers. Although some previous studies try to alleviate this via manual labeling or rule-based change point detection [7], [9], they still suffer from resource limitation or inflexiblility in dealing with different kinds of malfunctions and sophisticated attacks. What's more, it is further challenging to discover the true causal structures under the circumstance of both unobserved confounders and unobserved heterogeneity [25].\nTo address these challenges, we come up with a new problem of Partially Observed Root Cause Analysis (PORCA) that aims at tackling both issues and propose a comprehensive solution to it. To deal with unobserved confounders, we first replace existing causal models that have strong assumptions with magnified structural causal models to avoid potential model misspecification. We propose a magnified score func-tion which allows for efficient causal discovery in a gradient-descent way. In the aspect of unobserved heterogeneity, we distinguish normal and abnormal observations in a flexible manner via the distortion of causal mechanisms. To be spe-"}, {"title": "II. RELATED WORK", "content": "A. Root Cause Analysis\nRoot cause analysis aims to identify the underlying causes of system problem. It shows great practical values and has been a popular research topic in a wide range of domains [1], [2], [4]. Many causal approaches are introduced in root cause analysis to derive more reliable diagnostic conclusions [3], [9]. Most of them follow a two stage framework, which first conduct causal discovery reflecting the complex depen-dencies within the system and then leverage downstream analysis to find out potential root causes [9], [14], [26]. And some recent researches explore advanced topics, such as learning from human feedback [6], and propagation on network of networks [16]. However, few works noticed the issues originated from partial observations, i.e., unobserved confounders and unobserved heterogeneity. As for the first issue, several endeavors [9], [27] have mentioned the existence of unobserved confounders in RCA, but not accounted for it technically. Recently, Xu et al. [19] emphasize the effect of unobserved confounders. However, they focus on the design of experimental trail and debiasing, which is orthogonal to our work. As for the second issue, several studies [7], [9] try to identify data heterogeneity via rule-based change point detections while neglect the latent malfunctions. Chakraborty et al. [20] alleviate heterogeneity by introducing external nodes, which relies on domain knowledge and cannot extend to general cases. Our study proposes a framework which systematically account for both unobserved confounders and unobserved heterogeneity in root cause analysis.\nB. Causal Discovery\nCausal discovery aims at uncovering causal structures from observational data. By providing insights towards complex"}, {"title": "III. PRELIMINARY", "content": "A. Causality Basics\nFor the sake of clarity, we follow Pearl's framework [35] and first briefly describe basic concepts. Then, the idea of differentiable score-based causal discovery will be reviewed.\nA causal graph represents the causal relations between variables as the directed acyclic graph (DAG) $G = (V,E)$. $V$ denotes the set of nodes, where each node corresponds to a variable, for example, a system metric. And $E = {(v_i, v_j) | v_i, v_j \u2208 V}$ denotes the set of edges, where each edge $(v_i,v_j)$ represents a directional relationship from the causal variable $v_i$ to the outcome variable $v_j$. On this ba-sis, Structural Causal Model (SCM) captures the asymmetry between causal direction on the data generation process. To be specific, SCM with additive noise can be formalized as $x = f(x)+u$, where independent noise term $u$ reflects effect of the exogenous factor. Let metric $x_i$ correspond to node $v_i$, the mapping function $f(\u00b7)$ could be deduced via the DAG $G$. As a scalable framework with promising statistical properties [35], SCM and its variants have been a mainstream methodology in causal machine learning [36], [10].\nCausal discovery aims at uncovering the graph structure corresponding to SCM, which can provide valuable insights for further analysis. Compared with previous studies, differ-entiable score-based causal discovery [21] can be seamlessly integrated with machine learning techniques and offers addi-tional advantages, such as flexibility, scalability, and the ability to model nonlinear relations. The basic idea is (i) to define a score function S that evaluates how well a given causal structure fits the observed data; (ii) to specify the constraints $h(\u00b7)$ such as acyclicity; (iii) to derive causal conclusions by optimizing the score function, which can be conducted in a gradient-descent way.\nB. Problem Definition\nTo involve partially observed data into RCA, we need to explicitly account for unobserved confounders and heterogene-ity. Previous studies use acyclic directed mixed graphs (AD-MGs) [28], [37] with both directed and bidirected edges, where the latter indicates the existence of unobserved confounders. Let $d$ be the number of observed metrics, $D\u2208 R^{d\u00d7d}$ and $B\u2208 R^{d\u00d7d}$ represent directed and bidirected adjacency matrix, respectively. In this paper, we turn the mixed graphs $(D, B)$ to magnified adjacency matrix form $M\u2208 R^{(d+r)\u00d7(d+r)}$, where $r$ is the number of latent nodes $C$. To be specific, the $x_i \u2192 x_j$ in $B$ can be represented as $c_k \u2192 x_i$ and $c_k \u2192 x_j$ in $M$. In addition, the noise term $u$ has heterogeneous distribution with latent malfunction. To formally include above items, we leverage magnified SCM as Definition 1.\nDEFINITION 1 (Magnified Structural Causal Model): The partially observed data can be modulated as:\n$[x, c] = f_M(x, c) + u,$\nwhere $f_M(\u00b7)$ describes how observed and latent nodes being affected and can be transformed as magnified adjacency matrix $M$. And the noise term $u$ follows heterogeneous distri-butions across different observations.\nBased on the above definition, in this paper, the problem of PORCA can be formulated as following: Given observed metrics $X = (x_{1:T}^1, ..., x_{1:T}^i)$, for alarm of front-end metric $y^t$ ($y \u2208 X$), we aim to build a model that (i) uncovers causal structures of the system by leveraging magnified SCM to simultaneously account for unobsreved confounders and heterogeneity; (ii) identifies the top K metrics in X as the potential root causes of y.\nC. Method Overview\nAs shown in Figure 2, the proposed PORCA framework contains three concise components: (i) magnified score-based causal discovery; (ii) heterogeneity-aware scheduling; and (iii) deconfounded root cause localization. Assume $\u0398_{MSB}$ and $\u0398_{HAS}$ denote sets of learned parameters for magnified score-based causal discovery and heterogeneity-aware scheduling, respectively. In the causal discovery component, we optimize $\u0398_{MSB}$ for magnified SCM. In the scheduling component, given $\u0398_{MSB}$, we derive adaptive weights to schedule causal discovery procedures by optimizing $\u0398_{HAS}$. In the localization component,"}, {"title": "IV. METHODOLOGY", "content": "A. Magnified Score-Based Causal Discovery\nGiven partially observed data $X = (x_{1:T}^1, ..., x_{1:T}^i)$, magni-fied score-based causal discovery aims to continuously op-timize parameters in maginified SCMs, which allows for inferring causal structures at the presence of unobserved con-founders in a Bayesian perspective. To this end, it first derives latent representations as posterior distribution for unobserved confounders C, and the magnified graph M. And it takes these representations and historical information as input to learn the system dynamics via neural networks. Then the algebraic constraints [21], [30] on acyclic and ancestral properties is utilized for continuously optimization. This could be realized in three steps, i.e., (i) ADMG representation learning, (ii) structural causal networks, and (iii) magnified score function.\n1) ADMG Representation Learning: The magnified score-based causal discovery process is built on the basis of the ADMG structure M and the unobserved confounders C $(c_{1:T}^1, ..., c_{1:T}^i)$. To parameterize them, we decompose the likelihood of an edge $p(M_{ij})$ via ENCO representation [34], i.e., $\u03b3_{ij}$ and $b_{ij}$ denotes the existence and direction of edges, respectively. Thus the posterior structural distribution to be approximated could be denoted as Equation (1):\n$q_{\u03b3,\u03b8}(M_{ij}) \u223c Bern(sigmoid(\u03b3_{ij}).sigmoid(\u03b3_{ij})).$ (1)\nAs for the representation of unobserved confounders C, we parameterize it as the mixture of Gaussian distributions in Equation (2):\n$q(c) \u223c \\{N(\u03bc_c, \u03c3_c)\\}_{k=1}$ (2)\nwhere the parameters $\u03bc_c$ and $\u03c3_c$ are determined by MLP $f_{gauss} (x)$. The learned representation can be reckoned as C's posterior distribution given partial observations X.\n2) Structural Causal Networks: We utilize structural causal networks to model the complex nonlinear mappings entailed in magnified SCM, i.e., $x_j = f_{M,j} (x, c)$. According to the decoupling assumption [30], the formulation of structural causal network $f_{M,j} (\u00b7)$ is detailed as Equation (3):\n$x_j^t = f_{obs,j} (\u03a3_{i=1}^d M_{ij}g_i(x^{<t}))+f_{conf,j} (\u03a3_{i=d+1}^{d+r} M_{ij}g_i(c^{<t}))$ (3)\nWe follow [38] and let $x^{<t} = (...,x^{t-2},x^{t-1})$ denote the temporal information of variable $i$. And $f_{obs,j} (\u00b7)$ aggregates the observed variables' impact on target variable $j$, $f_{conf,j} (\u00b7)$ approximates the influence of unobserved confounders, and $g_i(\u00b7)$ introduces nonlinearity for the effect of each component $i$. They can be implemented based on MLPs.\nLet \u03a9 denote parameters to be optimized in structural causal networks. To reduce the search space of \u03a9, we introduce shared weights for similar neural networks and trainable hidden representations indicating different variables. To be"}, {"title": "B. Heterogeneity-Aware Scheduling", "content": "Next we introduce how to handle unobserved heterogeneity (e.g., latent attack, device fault), which could hurt the perfor-mance of causal discovery and the following process of root cause analysis. The core idea is to develop a heterogeneity-aware scheduling approach. It is inspired by curriculum learn-ing which mocks the recognition of human-being to learn from easy tasks to hard ones [40]. It has been well explored in enhancing causal learning with heterogeneous data [23], [41]. To be more specific, we reshape the optimization procedure of score-based causal discovery with adaptive sample weights. Given learned parameters $\u0398_{MSB}$ for magnified SCM of an iteration, we first distinguish the heterogeneity of different samples via the goodness of data reconstruction. Then we concurrently derive optimal sample weights mitigating the influences of unobserved heterogeneity."}, {"title": "C. Deconfounded Root Cause Localization", "content": "Given front-end metrics $y^t$ and learned magnified SCM, we aim to explore the potential root causes and return the top K candidates. Our deconfounded root cause localization algorithm achieves this goal by synergizing both propagation property in the topology structure and node-level anomaly scores in three steps: (i) deconfounded random walk, (ii) node-level anomalous rank, and (iii) potential root cause score.\n1) Deconfounded Random Walk: The random walk algo-rithm is proved a good performance in capturing anomaly propagation [7], [9]. We adopt one of its variants to further mit-igate the spurious correlation from unobserved confounders. We first decompose M into (D, B), in which D represents the causal dependency of anomaly propagation and B entails spurious correlations due to unobserved confounders. Then the learned matrix D is transposed as $D^T$. We leverage random walk algorithm with restart from front-end metric on $D^T$ to get probability score of each candidate node. To be specific, the transition distribution H can be formulated as Equation (11):\n$H[i, j] = \\frac{(1 - \u03b4)D^T[i, j]}{\u2211_{k=1}^d D^T[i, k] }$ (11)\nwhere \u03b4\u2208 [0, 1] is the probability of jumping behaviour. The walker stops after $N_{rw}$ steps, and each node is visited \u03bei times.\n2) Node-level Anomalous Rank: The causal Markov factor-ization in magnified SCM, i.e., $P(x^t) = \u03a0_{i=1}^d P(x^t|PA(x))$, allows for analyzing autonomous causal mechanism on the node-level [42]. We take the violation of causal mechanism as anomaly and rank the goodness of reconstruction of each mapping function $f_{M,i} (x^{<t}, c^{<t})$ as the anomaly degree \u03b7 of node i. Thus we have\n$\u03b7 = RANK(log p_\u03a9 (x_i^1), ..., log p_\u03a9 (x_i^i), ..., log p_\u03a9 (x_i^T)).$ (12)\nIt compares log p\u03a9(x) with all other observations from [1, T] to quantify the violation of causal mechanism."}, {"title": "D. Theoretical Analysis of PORCA", "content": "This section first briefly introduce Lemma 1 and its proof, which indicates identifiable ADMG can be recovered via optimizing magnified score function. Following that, Lemma 2 is brought forth to prove the optimal characteristics of the weights employed in the scheduling process. Lastly, the computational complexity of the proposed method is analyzed.\nLEMMA 1 (ADMG's Identifiability with Magnified Score Function): By assuming the effect of observed and unobserved nodes in the additive noise SCMs to be decoupled, i.e., $[x, c] = f_{b,x} (x, \u03a9) + f_{b,c} (c, \u03a9)+u$, the ground-truth ADMG M can be recovered when magnified score function S is optimized.\nThe proof of Lemma 1 consists of two decomposable parts, which we will briefly present. (i) Structural identifiability of ADMG (M). Under the decoupled assumption, the previous results in [30] can be extended that any potential situations of Mij = (Dij, Bij), i.e., (0, 1), (1, 0), (0,0) are distinguishable from the perspective of $p_\u03a9(x, M)$. (ii) Vanilla score func-tion (S) allows for efficiently recovering ADMG. The score function S serves as the evidence lower bound (ELBO) to be maximized, i.e., $S = L_{ELBO} <\u2211_t log p_\u03a9(x^t)$. To be specific, with the increases of observations, the graph restriction term $R_G$ asymptotically equals zero. Let p(x, M\u25e6) denote the data generation distribution with ground-truth ADMG M\u25e6, the asymptotic estimation of the ELBO can be derived in Equation (14):\n$L_{ELBO} = \u222bp(x, M^\u25e6)  \u03a3_{M\u2208M^\u25e6} q_{\u03b3,\u03b8} (M) log p_\u03a9(x|M)dx$. (14)\nAnd the optimal value is achieved when the MLE solution $(\u03a9^*, M^*)$ satisfies the Equation (15):\n$E_{p(x,M^\u25e6)} [log p_{\u03a9^*} (x|M^*)] = E_{p(x,M^\u25e6)} [log p_\u03a9(x, M^\u25e6)]$. (15)\nLEMMA 2 (Optimal Property of Heterogeneity-Aware Weights): In the scheduling phase, suppose that the obser-vation at t1 has a relative smaller data likelihood than that of t2, i.e., $L(x^{t_1}) <L(x^{t_2})$, where t1, t2 \u2208 {1, ...,T}. Then the heterogeneity-aware weights $w^{\u2217,t_1}$ and $w^{\u2217,t_2}$ have $w^{\u2217,t_1} > w^{\u2217,t_2}$. The equality only holds in thresholding conditions, i.e., $w^{\u2217,t_1} = w^{\u2217,t_2} = \u03c4 or w^{\u2217,t_1} = w^{\u2217,t_2} = 1$.\nTo prove Lemma 2, we additionally introduce a small per-mutation term \u03f5 on weights w\u2217, such that \u03f5\u2208 (0, min(w\u2217,t1\u2212\u03c4, \u03c4\u2212w\u2217,t2)). Then we replace $w^{\u2217,t_1} , w^{\u2217,t_2}$ in w\u2217 with permutated version w', i.e., $(w^{\u2217,t_1} + \u03f5), (w^{\u2217,t_2} \u2212 \u03f5)\u2208 w'$. Thus, the contradiction of the reweighted score functions can be formed in Equation (16):\n$S_{w^\u2217} \u2212 S_{w'} = \u03f5\u00b7 [L(x^{t_2}) \u2212 L(x^{t_1})] > 0,$ (16)\nwhich contradicts with w\u2217\u2208 arg minSw (x, c, M). Therefore, we have $w^{\u2217,t_1} > w^{\u2217,t_2}$ as stated in Lemma 2.\nComplexity analysis We now analyze the computational complexity of each round in magnified score-based causal discovery. Computing score function S and its gradient needs O(WT) time, where W is the number of parameters $\u0398_{MSB}$ to be optimized which scales according to a quadratic relation-ship with d and r. For scheduling, the time complexity for getting optimal weights is O(W'T), where W' is the number"}, {"title": "V. EXPERIMENT", "content": "In this section, we evaluate the performance of PORCA and answer the following questions:\n\u2022 Q1: What is the performance of PORCA in RCA compared with previous works without considering partially observed data?\n\u2022 Q2: How could PORCA recover causal structures as inter-mediate results of RCA?\n\u2022 Q3: What are the capabilities of the deconfounding module and heterogeneity-aware scheduling module?\n\u2022 Q4: Is PORCA sensitive to parameter changes?\nMoreover, we provide a case study to illustrate the causal patterns entailed in PORCA can be beneficial to RCA in practice.\nA. Experimental Settings\n1) Datasets: We evaluate the performance of PORCA on both synthetic and real-world datasets.\nTo remedy the problem that real-world datasets have no ground-truth of causal structure or explicitly labeled fault injection, we construct a synthetic dataset named Simulation that consists of 20 nodes with 1000 timesteps. The causal structure is predefined, and the faults are injected by changing the edge weights and noise distributions. The details of the data generation procedure, including graph sampling, metrics generation, and fault injection are available in our code link. With the propagation of causal networks, we can obtain the synthetic dataset. It allows us to explicitly and quantitatively account for both unobserved confounders and unobserved heterogeneity in the experimental evaluation.\nFor the real-world scenarios, we choose two public datasets: CRACs is the monitoring data of a cooling system in a data center [43]. It consists of 38 variables from January 1st, 2023 to May 1st, 2023. The occurrences and roots of abnormal tem-perature need to be identified for stable maintenance. SWaT is monitoring data collected from a real-world water treatment testbed [44]. It consists of 51 metrics from December 22nd, 2015 to January 2nd, 2016. Physical and cyber attacks took place in the last four days.\n2) Experimental Protocol: We evaluate the performance of PORCA on two tasks i.e., root cause analysis and causal discovery. In RCA experiment, the main results are the ranking based results on three datasets. For each dataset, we randomly mask 4 nodes as the unobserved confounders. Moreover, we change the number of masked nodes to test the robustness of PORCA. In the causal discovery experiment, we report the"}, {"title": "VI. CONCLUSION", "content": "In this paper, we define the problem of Root Cause Analysis with Partially Observed data (PORCA), which is essential in ensuring availability and reliability. To tackle the issues of unobserved confounders and unobserved heterogeneity, we propose a brand new framework as the solution. We prove that PORCA is capable of identifying the true causal structures with unobserved confounders, and the learned weights can correspond to the heterogeneity. We conduct an extensive set of experiments on the synthetic dataset and real-world data from two testbeds. The results show that PORCA outperform all the compared baselines with partially observed data and works well in real-world applications."}]}