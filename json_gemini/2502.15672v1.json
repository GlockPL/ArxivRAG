{"title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling", "authors": ["Florent Bartoccioni", "Elias Ramzi", "Victor Besnier", "Shashanka Venkataramanan", "Tuan-Hung Vu", "Yihong Xu", "Loick Chambon", "Spyros Gidaris", "Serkan Odabas", "David Hurych", "Renaud Marlet", "Alexandre Boulch", "Mickael Chen", "\u00c9loi Zablocki", "Andrei Bursuc", "Eduardo Valle", "Matthieu Cord"], "abstract": "We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving. VaViM is a simple auto-regressive video model that predicts frames using spatio-temporal token sequences. We show that it captures the semantics and dynamics of driving scenes. VaVAM, the video-action model, leverages the learned representations of VaViM to generate driving trajectories through imitation learning. Together, the models form a complete perception-to-action pipeline. We evaluate our models in open- and closed-loop driving scenarios, revealing that video-based pre-training holds promise for autonomous driving. Key insights include the semantic richness of the learned representations, the benefits of scaling for video synthesis, and the complex relationship between model size, data, and safety metrics in closed-loop evaluations. We release code and model weights at github.com/valeoai/VideoActionModel.", "sections": [{"title": "Introduction", "content": "Large-scale generative models have shattered the status quo of video generation with photorealistic, temporally coherent, high-fidelity videos synthesized from textual prompts. While generalist models such as Sora, Veo-2 [89] and VideoJAM [17] demonstrate those capabilities at large, specialist models such as GAIA-1 [44] and VISTA [31] showcase impressive performance in predicting future frames of driving videos. Generating plausible future frames suggests that those models capture meaningful representations of the world, but the exact nature of such representations, as well as any practical utility they might have for actual driving, remain open questions. To what extent do those representations encode driving-relevant features, such as scene dynamics, geometry, and semantics? How far do they apply to actual autonomous systems, enhancing downstream tasks, such as motion planning?"}, {"title": "Related work", "content": "Today's sophisticated architectures for video generation have significantly evolved since the days of Generative Adversarial Networks (GANs) [33, 76, 85]. Beyond basic generation capabilities, recent advances have focused on improving visual quality and enabling precise control over generated content. That dual evolution is visible in key developments in video data representation and video generation aligned to conditioning signals.\nThese methods work with real-valued embeddings in a continuous latent space, often operating on a pre-trained Variational Autoencoder (VAE) [52, 39] to compress the image or video signal spatially and reduce computation. Most modern approaches in this category build on either Diffusion or Flow Matching models. Diffusion Models (DM) [40, 75] learns to model the data distribution through a denoising process, effectively capturing the data distribution of images."}, {"title": "VaViM = Auto-regressive Video-Model on tokenized video stream", "content": "At its core, the auto-regressive video model captures the underlying dynamics of driving scenes by modeling the joint distribution of spatio-temporal token sequences. It operates on discrete video tokens, i.e., compact representations of video frames obtained through an image tokenizer (Section 3.1.1). By learning to predict the next token in these sequences (Section 3.1.2), VaViM builds a rich understanding of the temporal patterns in driving environments."}, {"title": "Image Tokenizer", "content": "The architecture starts by transforming continuous image data into a discrete sequence of tokens using vector quantization, a process known as visual tokenization [88], by mapping local image features to their nearest neighbors in a learned codebook. The codebook acts as a compressed visual vocabulary, enabling efficient auto-regressive modeling while preserving the essence of the visual information for downstream tasks.\nMore formally, consider a video clip with T frames, with each frame $X_t \\in [R^{h \\times w \\times c}$ for $t \\in \\{1,..., T\\}$. Here, h \u00d7 w is the spatial resolution, and c is the number of channels. The encoder $f_\\theta : X \\rightarrow e$ processes each frame independently to produce a latent embedding $e \\in R^{h' \\times w' \\times d}$. For a given frame embedding e, at each spatial location (i, j) \u2208 h' \u00d7 w', we quantize $e_{(i,j)}$ to a discrete token $q_{(i,j)}$ by performing a nearest-neighbor lookup in the codebook $\\{e_k\\}$:\n$q_{(i,j)} := arg\\underset{k}{min} ||e_{(i,j)} \u2013 e_k||_2$.\nIn this equation, ek represents the embedding vectors in a shared codebook of size $[R^{K\\times d}$, where K is the number of discrete entries or codebook vectors, and d is the dimensionality of each vector. The discrete token map q is then used to retrieve the corresponding embeddings, resulting in the embedding map eq.\nTo map the tokenized representation back into the image domain, we use a decoder ge, which takes the embedding map eq and generates the reconstructed output $\\hat{x} := g_\\theta(e_q)$. That process resembles a standard autoencoder but includes a unique non-linearity that maps latent representations to one of K embedding vectors.\nDuring training, the codebook is initialized randomly and jointly optimized with the encoder and decoder. That adapts the codebook to the data distribution, capturing the most relevant visual features for the task.\nThe image tokenizer is trained using a vector quantization objective, combining reconstruction, commitment, and adversarial losses to ensure high-fidelity and perceptually realistic image recon- structions. A straight-through estimator computes the gradients, thus handling the non-differentiable nearest-neighbor lookup. For detailed formulations, see [81]."}, {"title": "Auto-regressive next token predictor", "content": "The second stage of our approach generates videos in the latent space of the pre-trained Vector Quantized Variational AutoEncoder (VQ-VAE) tokenizer. We use an auto-regressive framework inspired by Large Language Model (LLM) pre-training, employing a transformer decoder to predict tokens sequentially. That allows the model to generate video content patch by patch, capturing spatial and temporal dependencies.\nObjective: The model learns the conditional probability of each token given its preceding tokens. For a sequence of n tokens $Q = [q^0, q^1, . . ., q^{n-1}]$, the joint distribution is factorized as the product of conditional probabilities:\n$P(Q; \\theta) = \\prod_{i=1}^{n} P(q^i|q^0, q^1,...,q^{i-1};\\theta)$\nwhere \u03b8 are the model parameters.\nWe train the model to minimize the negative log-likelihood of the observed token sequences:\n$L_\\theta =  -\\sum_{i=1}^{n} log P(q^i|q^0, q^1 ..., q^{i-1}; \\theta)$\nWe use a softmax function on the model's logits to produce a probability distribution over the vocabulary. We train the model using teacher forcing with cross-entropy loss, aligning the predicted and true token distributions."}, {"title": "VaViM: model architecture", "content": "Building upon the previously described tokenizer discretization, we employ a GPT-2 [70] architecture to model the temporal dynamics of video tokens auto-regressively (following Equation 2). While the tokenizer's vocabulary focuses on the perceptual compression of individual frames, our model learns a new set of embeddings optimized for capturing spatio-temporal relationships in the token sequence. Those embeddings map the tokenizer's discrete codes q into a continuous latent space z where spatio-temporal relationships can be modeled auto-regressively.\nAt each layer $l \\in L$, where L is the total number of layers, the computation is as follows:\n$z \\leftarrow z + CausalAttn(LN(z))$\n$z \\leftarrow z + FFN(LN(z)).$\nwhere FFN(\u00b7) denotes a fully connected layer, CausalAttn(\u00b7) is a causal attention layer with mask- ing [92], and LN(\u00b7) denotes layer normalization [54]. We use GELU [37] as the activation function and employ weight tying between the input embedding layer and the output projection layer to reduce the number of parameters. Additionally, at inference time, a KV cache [66] is maintained for efficient auto-regressive sampling.\nFollowing GAIA-1 [44], we use two types of learned positional embeddings to capture both spatial and temporal dependencies. The spatial positional embedding is shared across all frames, allowing the model to capture spatial dependencies within each image independently. In contrast, the temporal positional embedding is unique for each frame, enabling the model to capture dependencies across frames in a video. By combining these two positional embeddings, the model effectively learns both intra-frame and inter-frame relationships."}, {"title": "VaVAM = VaViM + action expert", "content": "Whether video generation pre-training effectively captures the features essential for safe and reliable driving is a key question. To bridge the gap between pre-trained video representations and driving decisions, we introduce an action expert module, forming VaVAM by complementing VaViM with decision-making. The action expert, inspired by \u03c00 [8], uses flow matching to generate actions by progressively denoising a noisy ego-trajectory, illustrated on the bottom left of Figure 2a, into a coherent driving trajectory. The denoising is conditioned on high-level driving commands (e.g., 'turn left', 'go straight') and video features from VaViM encoding the scene dynamics. While \u03c0\u03bf conditions on single frames for robotic manipulation, we extend it to driving by exploiting the temporal contexts of multiple frames that are crucial for understanding dynamic scenarios.\nWe adopt flow matching instead of alternatives such as action quantization [53] because it directly learns the vector fields that define the transport to the target probability distribution, enabling accurate action generation while effectively modeling complex multimodal distributions. That is particularly important for our trajectory dataset, which is challenging to capture with quantization-based methods due to its long-tail distribution dominated by straight trajectories, with maneuvers such as U-turns appearing rarely (Figure 4).\nMore formally, we assume a dataset of driving recordings and their associated high-level commands $D = \\{(O_t, A_t, C_t)\\}$, with $O_t = [o_t,..., o_{t-N}]$ representing a sequence of images observed up to the N past frames; $c_t \\in \\{left, right, straight\\}$ being the high-level commands, which act as a guide for the vehicle direction, e.g., \u2018turn left\u2019, on Figure 2a; and $A_t = [a_{t+1},..., a_{t+H}]$ being the 'action', defined as a sequence of [x, y] ego-positions in the BEV reference-frame specifying the dynamic profile of the driving path to undertake. We illustrate the 'action' trajectory at the top left of Figure 2a. The 'action' is extracted from the pre-recorded ego-motion over the next H future timesteps after the current timestep t.\nThrough the combination of VaViM and the action expert module, VaVAM effectivelly learns the conditional vector fields $v_\\theta (A^\\tau ; O_t, c_t)$ that transport actions sampled from a noised distribution $A^\\tau = [a^\\tau_{t+1}, a^\\tau_{t+2},\u00b7\u00b7\u00b7, a^\\tau_{t+H}]$ to actions At from the observed distribution Ot and high-level commands ct. That denoising process is formalized in Section 4.4."}, {"title": "Action Encoder:", "content": "It projects the actions into a latent space using an MLP and incorporates positional embeddings of the flow matching step 7, a learned temporal embedding (\u201caction at time t\u201d) and a learned embedding for each high-level command (left, right, straight)."}, {"title": "Joint Attention Transformer:", "content": "This module enables interaction between action representations and visual features, conditioning the denoising process on observed scene dynamics coming from VaViM. We use a specialized attention masking scheme illustrated in Figure 2bAction tokens attend to all past context frames and all other action tokens within the same frame.Visual tokens maintain causal masking to preserve their sequential nature, preventing them from being conditioned by future observations."}, {"title": "Action Decoder:", "content": "It maps the latent action features back to the action space with a linear layer, predicting the denoising vector field $v_\\theta (A; O_t, c_t)$.\nThe architecture provides two key advantages. First, VaViM and the action expert interact exclusively through joint attention. This design choice allows the action expert to use a smaller MLP dimen- sionality than VaViM while maintaining matching dimensions in attention layers. Such dimensional reduction is crucial for efficiency, as the action expert performs multiple forward passes during iterative denoising and sampling. Second, the layer-wise joint attention addresses the challenge of feature extraction from VaViM's layers. Different layers capture varying levels of abstraction\u2014from raw vocabulary embeddings to task-specific features. Rather than selecting and committing to a single layer, the joint attention mechanism learns to extract relevant features across VaViM's entire depth.\nDuring inference, we sample trajectories by integrating the denoising vector field over 10 steps using the forward Euler method, starting from random noise $A^\\tau \\sim N$. That integration process progressively refines the noisy actions into a coherent driving trajectory that satisfies both the high-level command and environmental constraints captured by the temporal \u00e0"}, {"title": "Data and Training", "content": "Our desiderata for the data were to find a large dataset of non-annotated data for the pre-training and a sufficient amount of annotated data (with trajectories synchronized with perception) for fine- tuning. To that end, we train VaViM and VaVAM on a collection of three datasets: OpenDV [96], a massive non-annotated web dataset, and nuPlan [13] and nuScenes [12], dedicated automotive datasets captured with multiple sensors."}, {"title": "VaViM pre-training", "content": "Training large autoregressive models requires careful consideration of both parameterization and scaling strategy. In this section, we present our approach to efficiently scale VaViM beyond 1 billion parameters.\nCompute-Efficient Scaling with \u00b5P: We adopt the Maximal Update Parametrization (\u00b5P) [95] to enable efficient scaling, inspired by recent advances in large-scale model training [45, 27, 97, 55]."}, {"title": "Fine-tuning VaViM on target dataset", "content": "Building upon our pre-trained model, we implement a carefully designed fine-tuning strategy that leverages multiple datasets to enhance the model's performance. Our approach begins with a checkpoint selected from the \u2018stable' phase of pre-training.\nFor the fine-tuning phase, we construct a diverse training mix combining three complementary datasets: (1) OpenDV, a large-scale, diverse dataset that provides broad coverage of general driving scenarios across the world; (2) nuPlan, a more specialized dataset that aligns with our subsequent imitation learning phase; (3) nuScenes, that serves the dual purpose of supporting imitation learning and targetting the NeuroNCAP evaluation, i.e., the target task of driving.\nFrom OpenDV, we initially extracted around 59M overlapping video clips and allocated 90% of them for pre-training (warmup and stable learning rate phases) and reserved the remaining 10% for fine-tuning (learning rate decay phase). However, rather than utilizing the entire fine-tuning portion, we strategically sample from multiple sources to create a balanced training mix:"}, {"title": "Training VaVAM with imitation learning", "content": "A carefully structured imitation learning allows transforming our pre-trained video model (VaViM) into an actionable video-action model (VaVAM). This section outlines how we enable end-to-end driving capabilities while preserving the rich visual representations learned during pre-training.\nAs discussed in Section 3.2, our approach employs flow matching, building upon the framework introduced in \u03c00 [8]. More formally, given a dataset of expert demonstrations with associated high-level commands $D = \\{(O_t, A_t, C_t)\\}$, with $O_t = [O_t, ..., O_{t\u2212N}]$ representing the sequence of images observed up to N past frames, the high-level command $c_t \\in \\{left, right, straight\\}$ and the expert trajectory $A_t = [a_{t+1},..., a_{t+H}]$ of future positions over horizon H, we learn to denoise trajectories through a conditional probability flow.\nThe key insight of flow matching lies in its elegant formulation of the forward process and induced vector field. We learn a conditional denoising vector field $v_\\theta$, which defines how to progressively transform noisy trajectories back into expert-like behavior. The training process follows a forward noising schedule defined by:\n$A^\\tau = \\tau A_t + (1 \u2013 \\tau)\\epsilon,  \\epsilon\\sim \\mathcal{N}(0, I)$\nThat process represents a linear interpolation between the expert action $A_t$ and Gaussian noise $\\epsilon$. The variable $\u03c4 \u2208 [0, 1]$ represents the noise level. This process smoothly interpolates between expert actions (\u03c4 = 0) and pure noise (\u03c4 = 1) and traces out paths in the action manifold. For training, the action expert uses the following objective to predict the denoising vector field $v_\\theta$:\n$\\mathcal{L}_T (\\theta) =  \\mathbb{E}_{p(A_t, O_t,c_t),q(A_\\tau|A_t)}||v_\\theta(A_\\tau, O_t, c_t) \u2013 u(A|A_t)||^2$\nwhere $q(A_\\tau|A_t)$ is the forward process defined above and $u(A|A_t)$ is the optimal transport vector field. The optimal transport vector field $u(A|A_t)$ represents the ideal direction in which noisy actions should move to become expert actions. Our learned vector field $v_\\theta$ approximates this optimal transport. The vector field acts as the generator of a continuous transformation on the manifold of plausible driving actions. It generates a flow that transforms a simple distribution (Gaussian noise) into our target distribution of expert actions. During inference, we generate action sequences by integrating the learned vector field:\n$A^{\\tau+\\delta} = A^\\tau + \u03b4 \u00b7 v_\\theta (A^\\tau, O_t, c_t)$\nusing 10 steps of the forward Euler method, starting from random noise $A^\\tau \\sim \\mathcal{N}(0, I)$.\nThat framework enables our model to capture complex multimodal action distributions directly from the expert demonstration. The effectiveness of this approach is extensively demonstrated in Section 5.2, where we show strong performance in both open-loop prediction and closed-loop driving scenarios."}, {"title": "Implementation and Training details", "content": "We use a pre-trained image tokenizer, LlamaGen [81], which is based on the VQGAN architecture. Specifically, we use the stride=16 tokenizer, which has 72M parameters. It has a vocabulary size of 16,384 with codewords of 8 dimensions. We use images of size 512\u00d7288, resulting in a token map of 32\u00d718, or 576 tokens.\nVaViM is based off a GPT-2 transformer architecture [70]. We train it with a context length of 8 frames, resulting in a maximum context of 4,608 tokens. It has 24 layers, a vocabulary size of 16,384, with a width scaling from 768 (VaViM-S) to 1024 (VaViM-B), up to 2048 (VaViM-L). This results in a codebook of size 12.6M, 16.8M, and 33.65M respectively. We keep the dimensionality of the heads fixed at 128, making the number of attention heads scale with the model size. We set a standard multiplication factor of 4 for the FFN hidden dimensionality. We optimize it with AdamW [60], a base learning rate of 0.0041, a weight decay of 1e-7, and \u03b2 = (0.9, 0.95) while clipping the gradient with a norm of 1.0. Finally, we initialize with a standard deviation of 0.0289.\nAs described in Section 4.2, the \u00b5P parameterization scales the learning rate per layer according to the width layer (see our code or [95] from exact specifications). We train all our models with a batch size of 384 and vary the number of GPUs depending on the model size to maximize GPU utilization."}, {"title": "Evaluation", "content": "To evaluate the quality of the generation of our VaViM, we use the Frechet Inception Distance (FID) [38]. To account for non-object-centric settings typical of driving datasets, we use the features from a DINOv2 model, which have been shown to be richer.\nSpecifically, given a context of 4 frames, we generate 4 frames with VaViM. We use the features of the 4 context frames as reference images to compute the FID. We compute an FID score for each future frame individually (FID@t), i.e., FID@2 means the FID of the second generated frame. Since VaViM does not directly generate images but tokens, we compute the FID for the LLamaGen-VQGAN tokenizer to act as an upper bound, as it serves as an \u201coracle\", i.e., it encodes a real frame of the \"future\". We choose the same reference frames for the FID. And compute the FID individually for the 4 ground-truth future frames. That setting changes slightly compared to the reconstruction FID, as the features of the original images considered for reconstruction are not part of the reference images used to compute the FID. Note that, although we pre-train on a video clip, we do not use Frechet Video Distance [86] (FVD) for evaluation because it relies on an I3D [15] that requires at least 10 frames as input."}, {"title": "Semantic segmentation", "content": "In Table 3, we evaluate VaViM for semantic segmentation using the Humming-bird approach [5]. Specifically, we use the features of layer 12 of our VaViM's transformer to encode a frame. We sample 10 patch-features per image, using the sampling approach of the open-source Humming-bird implementation [67]. We report the mean Intersection over Union (mIoU), which measures the overlap between predicted and ground truth segmentations across all classes, providing a comprehensive assessment of per-class and overall segmentation quality. Higher mIoU scores indicate better alignment with ground-truth annotations.\nAs observed in recent studies [71], auto-regressive models struggle compared to discriminative models like Dino and Dinov2 [14, 65]. We can see Dino models outperform VaViM on all setups. However, this study shows that, even with the auto-regressive pre-training that does not explicitly enforce semantic understanding, VaViM still manages to segment the different datasets in a zero-shot manner: indeed, neither Cityscapes nor KITTI was part of the training data mix. Table 3 also shows that during fine-tuning on our target datasets, VaViM does not lose the representation capability learned on the diverse pre-training data.\nMoreover, we conduct a qualitative study on the features obtained with VaViM on Figure 7. As proposed by [14], we visualize as RGB primaries the 3 main components of a PCA of the features. The semantic consistency of VaViM's is visible as similar colors being assigned to objects of the same class (e.g., pedestrians, cars, or road), which suggests the features hold semantic meaning, even if they are not invariant enough to perform at semantic segmentation."}, {"title": "VaVAM driving evaluation", "content": "In Table 4, we evaluate VaVAM in an open-loop setup. We compute the minADEk (\u2193), i.e., the minimum over k sampled trajectories of the Average Distance Error, taken as the average of point- wise $L^2$ distances between the closest (among k = 5) sampled trajectory and the ground-truth expert trajectory. The metric is calculated for both nuPlan [13] and nuScenes [12]."}, {"title": "Closed-loop evaluation", "content": "While the previous open-loop evaluation demonstrates strong trajectory prediction accuracy, it fails to capture the cascading effects of the model's decisions. Closed-loop evaluation addresses that limitation by allowing decisions to influence future observations, thus providing a more realistic assessment of safety-critical behavior.\nTo evaluate our model's performance in closed-loop, we employ NeuroNCAP [59], a simulator specifically designed for testing autonomous driving systems in safety-critical scenarios. To the best of our knowledge, it is currently the only existing data-based closed-loop simulator. Other solutions are either synthetic [28] (leading to domain gap) or based on view reprojection [2, 3] (leading to limited novel views).\nNeuroNCAP employs a NeRf-based simulator that executes the driving model decision and generates the corresponding novel view. That enables photorealistic closed-loop evaluation of driving models. In particular, a key feature of NeuroNCAP is its ability to insert pre-defined adversarial agents into the scene, such as a vehicle following hazardous trajectories.\nUsing that capability, the framework creates challenging test conditions inspired by the European New Car Assessment Programme (Euro NCAP), featuring three primary scenario types: stationary obstacles in the ego-lane, frontal collisions with oncoming vehicles, and side collision scenarios from cross-traffic. In our experiments, we leverage this framework to systematically evaluate our VaVAM model's ability to handle those challenging scenarios while maintaining its intended trajectory."}, {"title": "Collision metrics:", "content": "NeuroNCAP's evaluation protocol relies on two metrics: (1) the collision rate as a percentage of scenarios without collision and (2) the NeuroNCAP score (NNS) that assigns scores based on collision avoidance success and impact velocity reduction, offering a quantitative measure of the model's safety performance. More formally:\n$NNS \\triangleq \\begin{cases} 15.0 & \\text{if no collision}\\\\ 4.0 \\cdot \\text{max}(0, 1 - V_i/v_r) & \\text{otherwise} \\end{cases}$\nBaselines: We first compare VaVAM with existing NeuroNCAP baselines. The Base-U and Base- V baselines are na\u00efve methods that use the perception outputs from UniAD [47] and VAD [48], respectively. They operate on a simple rule-based approach: maintaining constant velocity unless"}, {"title": "Mean Deviation And Progress Metrics:", "content": "To address those limitations and provide a more comprehensive evaluation, we propose to use two complementary metrics. First, we measure the mean deviation from the guiding trajectory, which quantifies how well the model adheres to intended driving paths. It differs from the ADE metrics by measuring the mean instantaneous distance to the closest point of the reference trajectory instead of ADE's pairwise distance. Second, we introduce a goal-progress metric that measures the relative reduction in distance to the destination, formally defined as:\n$progress\\_toward\\_goal = max(0.0, \\frac{d_{initial} \u2013 d_{final}}{d_{initial}})$\nwhere $d_{initial}$ and $d_{final}$ represent the initial and final distances to the goal, respectively. That for- mulation ensures the metric remains bounded between 0 and 1, where 1 indicates complete goal"}, {"title": "Conclusion", "content": "VaViM and VaVAM are a significant step forward in applying large-scale unlabeled pre-training to autonomous driving, offering several exciting discoveries. First, the successful transfer of the pre- trained representations to driving tasks demonstrates the versatility of our approach. Complex driving behaviors are learned directly from raw video without requiring expensive semantic annotations. Particularly encouraging is VaVAM's reduction of existing methods' collision rates by 27% while maintaining comparable progress metrics. Second, the performance on out-of-distribution datasets like KITTI and Cityscapes demonstrates our approach's robustness and generalization capabilities. Third, our scaling experiments reveal a clear path forward. The empirical scaling laws we established suggest substantial headroom for improvement, notably through increased training data.\nBy releasing our complete codebase, training recipes, scaling laws, and model weights, we aim to accelerate progress in video-based autonomous driving. We envision several promising directions for future work:\nDecoupling high-level command path from actual expert trajectory so that, in the imitation training set, the model observes the expert deviating from the high-level command path.\nExtending our approach to leverage multi-camera setups for enhanced scene understanding.\nExploring more sophisticated action generation frameworks that maintain the benefits of our current approach while improving safety-critical behavior.\nInvestigating larger-scale pre-training on even more diverse driving datasets.\nUsing a better tokenizer than the current LLaMaGen-VQGAN, adapted to driving scenarios and better able to capture fine visual details (text on signs, road markings, traffic lights, etc.).\nThe key limitation of our work lies in the gap between VaViM's ability to model future states and VaVAM's current reliance on imitation learning. While VaViM can generate plausible future video streams, we have not yet leveraged that predictive power for planning and control. A critical missing piece is a reward model that distinguishes between favorable and critical latent states, enabling more sophisticated planning strategies beyond pure imitation. That presents an exciting opportunity to transform our reactive system into a proper \u2018world-model\u2019-based planning pipeline.\nAdditionally, while comprehensive for driving performance, our evaluation framework does not fully evaluate the depth of physical understanding learned by our video model. Future work should develop more nuanced evaluation metrics, similar to Physics-IQ [63]'s spatial and temporal mIoU approach to assess different aspects of physical understanding. Such metrics would provide deeper insights into what our models learn about scene dynamics, object interactions, and physical constraints."}]}