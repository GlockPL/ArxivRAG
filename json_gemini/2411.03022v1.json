{"title": "Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras", "authors": ["Roberto Ria\u00f1o", "Gorka Abad", "Stjepan Picek", "Aitor Urbieta"], "abstract": "While security vulnerabilities in traditional Deep Neural Networks (DNNs) have been extensively studied, the susceptibility of Spiking Neural Networks (SNNs) to adversarial attacks remains mostly underexplored. Until now, the mechanisms to inject backdoors into SNN models have been limited to digital scenarios; thus, we present the first evaluation of backdoor attacks in real-world environments.\nWe begin by assessing the applicability of existing digital backdoor attacks and identifying their limitations for deployment in physical environments. To address each of the found limitations, we present three novel backdoor attack methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess the effectiveness of traditional backdoor procedures and defenses adapted for SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that while these procedures and defenses can mitigate some attacks, they often fail against stronger methods like Flashy Backdoor or sacrifice too much clean accuracy, rendering the models unusable.\nOverall, all our methods can achieve up to a 100% Attack Success Rate while maintaining high clean accuracy in every tested dataset. Additionally, we evaluate the stealthiness of the triggers with commonly used metrics, finding them highly stealthy. Thus, we propose new alternatives more suited for identifying poisoned samples in these scenarios. Our results show that further research is needed to ensure the security of SNN-based systems against backdoor attacks and their safe application in real-world scenarios. The code, experiments, and results are available in our repository.", "sections": [{"title": "1. Introduction", "content": "Deep Neural Networks (DNNs) have revolutionized the field of Artificial Intelligence (AI), offering unprecedented advancements in various tasks, such as image recognition [14], natural language processing [18], or speech recognition [34]. Their capacity to process and learn from vast amounts of data makes them ideal for various applications, ranging from medical analysis [29] to autonomous driving systems [66] and personal assistants [37].\nHowever, despite their success and wide application, DNN training is still a problem, as it demands significant computational resources and time, especially for large models like, e.g., GPT-3 [67]. Training such models requires extensive hardware and consumes a considerable amount of energy, posing environmental and economic challenges. For example, assuming that the GPT-3 model was trained using 1024 NVIDIA A100 GPUs in a data center, the time required to train this model would be 34 days [63], consuming about 235 MWh. Moreover, according to the method proposed by Lannelongue et al. [46], it would have a carbon footprint of 62.47 T CO\u2082e (CO2 equivalent), which would take 5680 years\u00b2 for a tree to process. This immense carbon footprint has led researchers to explore alternative, more sustainable models.\nThe motivation for using Spiking Neural Networks (SNNs) extends beyond their computational capabilities, emerging as a promising solution to this challenge [23], [24], [33], [80]. Unlike conventional DNNs that operate on continuous activations, SNNs simulate the spiking behavior observed in biological neurons [44]; this unique characteristic allows SNNs to capture the temporal dynamics of information processing, mirroring the precise timing and synchronization observed in the human brain [31]. SNNs exhibit a form of event-driven computation, meaning they only activate when necessary, leading to potential energy efficiency improvements compared to traditional neural network architectures. This advantage was demonstrated by Kundu et al. [45], who showed that SNNs achieved up to a 12.2\u00d7 improvement in computational energy efficiency compared to DNNs with a similar parameter count. Additionally, SNNs can be more robust to noise and perturbations, allowing them to maintain accuracy in real-world situations under varying environmental conditions [47].\nThe inherent ability of SNNs to manage temporal dependencies within data makes them particularly useful in tasks where the sequence and timing of events matter, such as in sensory processing [80]. This capability becomes evident when employing data acquired through Dynamic Vision Sensor (DVS) cameras [75]. Unlike traditional cameras capturing absolute brightness at a constant rate,"}, {"title": "2. Background", "content": "2.1. Spiking Neural Networks\nSNNs, similar to traditional DNNs, are composed of layers of interconnected spiking neurons, including input, hidden, and output layers. However, unlike conventional DNNs that operate on continuous activations, SNNs simulate the spiking behavior observed in biological neurons [31] by using discrete events, known as spikes, to represent the output of a neuron. These spikes are generated when the neuron's membrane potential reaches a certain threshold (see Figure 1). This unique characteristic allows SNNs to capture the temporal dynamics of"}, {"title": "2.2. Backdoor Attacks", "content": "Backdoor attacks focus on injecting hidden malicious behaviors into the models, behaving legitimately for benign inputs but abnormally in the presence of the predefined trigger [35]. These attacks can be mostly classified into two main groups [51]: poisoning-based and non-poisoning-based attacks. The former involves manipulating the training data to inject the desired behavior into the model. At the same time, Poisoning-based attacks can also be divided into two groups: clean-label attacks [82], which maintain the appearance of the poisoned input consistent with their label, and dirty-label attacks [35], which change the poisoned examples' labels for them to fit the desired behavior. Non-poisoning-based attacks focus on embedding the backdoor into the model without altering the training data directly. These include weight-oriented backdoors [21], structure-modified backdoors, also known as model injection [79], and code-poisoning backdoors [7].\nThere have been many studies on different types of triggers depending on the domain in which the backdoor is implemented. In the case of the image domain, a pixel pattern in a particular part of the image can serve as a trigger. Still, many other types have also been discussed in this domain; for example, Haoliang et al. [49] developed a novel color stripe pattern trigger, which could be projected onto real faces through a flickering LED to deceive face recognition models. In poisoning-based attacks, which we investigate, the models are trained on a mixture of clean and backdoor samples; that way, they learn only to misclassify the examples containing the trigger.\nFurther focusing on dirty label attacks, during the training process, a model $F_\\theta(x)$ learns from a dataset comprising both clean examples $D_{cl}$ of size $n$ and poisoned examples $D_{ps}$ of size $m$. It is important to note that the size of $D_{ps}$ is typically much smaller than $D_{cl}$, with the poisoning rate indicating this proportion, expressed as $ \\epsilon = \\frac{m}{n} $. A clean sample is represented by $ \\{(x, y)\\} \\in D_{cl} $, where x is the input and y is the corresponding label. A poisoned sample is represented by $ \\{(x, \\hat{y})\\} \\in D_{ps} $, where a mask m is applied to the original input x to introduce a trigger p at a specific location, resulting in the modified input x. In a dirty-label attack, the original label y is replaced with the desired malicious label $ \\hat{y} $. Equation (3) shows the behavior of the backdoored model when encountering each type of sample: R represents the benign behavior and B the backdoor behavior.\n$$ F(x) = \\begin{cases} R \\text{ if } x \\in D_{cl} \\\\ B \\text{ if } x \\in D_{ps} \\end{cases} $$\nThe main goal of the training process is to find the optimal parameters \u03b8 for a clean dataset that consists of n samples. In this case, a second term is added to the equation to introduce the backdoor behavior to include the poisoned samples and labels [2].\n$$ \\theta' = \\arg \\min_\\theta \\sum_{(x, y) \\in D_{cl}} L(F_\\theta(x), y) + \\sum_{(x, \\hat{y}) \\in D_{ps}} L(F_\\theta(x), \\hat{y}) $$\nFollowing this methodology, we aim to maintain the correct behavior of the model when faced with clean inputs, represented by the first term of the equation, while ensuring the introduction of the backdoor behavior when faced with poisoned triggers, represented by the second term."}, {"title": "3. Related Work", "content": "Recent investigations undertook a comparative analysis of the inherent security vulnerabilities in SNNs, revealing their susceptibility to adversarial examples with both traditional data [77] and neuromorphic data from already existing datasets [53], data directly taken from DVS cameras [62], or even using neuromorphic datasets with a model running on a neuromorphic chip in white-box scenarios [13]. Researchers also tested the effect of adversarial examples on black-box scenarios with traditional data [61], [68], demonstrating that SNNs are susceptible to adversarial attacks with both traditional and neuromorphic data in white or black-box scenarios. Additionally, subsequent research [83] has shown that SNNs are also prone to attacks hardware-level attacks, where intentional bit-flips can result in misclassification.\nDespite these vulnerabilities, SNNs offer inherent advantages in terms of robustness due to their biological inspiration. Chen et al. [16] proposed an SNN-based image purification model to defend against adversarial attacks. This method uses a noise extraction network and a non-blind denoising network to reconstruct clean images from noisy ones, enhancing the robustness of SNNs against adversarial examples. This approach leverages the visual masking effect and filtering theory to create a robust pre-processing module that can be integrated with various classifiers. The Homeostatic Spiking Neural Networks model, developed by Geng and Li [30], introduces a threshold-adapting leaky integrate-and-fire neuron model. This model incorporates a self-stabilizing dynamic thresholding mechanism, clipping adversarial noise propagation and enhancing the robustness of SNNs in an unsupervised manner. The HoSNNs demonstrate significant improvements in accuracy against the Fast Gradient Sign Method and the Projected Gradient Descent attacks compared to traditional SNN models.\nOther studies also considered the internal membrane perturbation in response to small disturbances. Din et al. [19] introduced the concept of membrane potential perturbation dynamics, which analyzes the changes in a neuron's internal state when these disturbances are present. They used this concept to develop a dynamic LIF neuron with additional adjustable parameters, enabling better adaptation to input changes. Unlike previous methods, they focused on reducing the internal changes rather than just the spike outputs, demonstrating significant improvements in the robustness of SNNs against adversarial attacks.\nTo our knowledge, only a few works explore backdoor attacks in SNNs [1]\u2013[3], [26], [41], but none explore the possibility of real-world scenarios, focusing only on virtual environments. The first paper on backdoor attacks for SNNs [1] considered two types of triggers, a static and a moving square, present in all the frames of the poisoned data during training. However, this study only examined limited poisoning rates and trigger sizes without implementing any backdoor defense mechanisms.\nThe second one [2] builds upon the initial study by exploring the feasibility of backdoor attacks in SNNs, with a particular focus on the stealthiness of the trigger and its robustness against defenses. The authors improved the performance of the static attacks proposed in [1]"}, {"title": "4. Physical Dataset Expansion", "content": "In this section, we follow the process used to capture and adapt new dataset samples to the original DVS128-Gestures dataset [6], along with the challenges of replicating digital triggers in physical environments. We chose this dataset as it is, to our knowledge, one of the few widely available options that provide data directly captured by DVS cameras for classification tasks rather than being converted from frame-based datasets. We also considered the ASL-DVS [9] dataset of American Sign Language gestures but discarded it due to the short time duration of the samples, i.e., 0.1 seconds, making it unsuitable for trigger insertion. The DVS128-Gestures dataset also allowed us to directly compare the performance of our attacks in digital settings with previously studied backdoor methods while being able to analyze the differences between the efficacy of the digital triggers and their physical reproductions.\n4.1. Dynamic Vision Sensors\nDVS cameras operate differently from traditional frame-based cameras, capturing asynchronous changes in light intensity (events) at a pixel level rather than producing full video frames [27]. Each pixel in a DVS camera independently reports changes in brightness, generating spikes of information when such changes occur.\nEach event recorded by a DVS includes specific data: the X, Y coordinates of the pixel location, the polarity p indicating whether the change is an increase (ON event) or decrease (OFF event) in light intensity, and a timestamp T that provides the precise time of the event, usually measured in microseconds. This efficient encoding scheme helps reduce energy consumption and data bandwidth [45].\nThe pixel operation in a DVS involves three main components [55]: a photoreceptor, a differencing circuit, and a comparator. The photoreceptor measures light intensity and converts it into a logarithmic signal. The differencing circuit compares the current intensity to a stored reference value. When the difference exceeds a predefined threshold, the comparator generates an event (see Figure 2).\n4.2. Data Collection Process\nThe data collection process involved recording gestures performed by subjects under controlled conditions to match the settings of the original dataset using the Metavision EVK3-HD camera. The participants performed the same set of gestures as in the DVS128-Gesture dataset.\nWe carefully adjust the camera parameters to align the captured data with the characteristics of the original dataset. Two of the most important parameters to obtain low noise, clear samples are Event Generation Thresholds, which controls the thresholds that determine the minimum change in light intensity required for a pixel to generate an event, and Pixel Rest Time, also known as the refractory period, which defines the minimum time between successive events from the same pixel. The tuning of these parameters is important to match the number of events present in the original samples while reducing background noise. For the specific parameter values, refer to Appendix C.\nAfter capturing the new samples, we preprocess them to align with the data on the original dataset. As the Metavision EVK3-HD has a much higher resolution of 1280\u00d7720, we crop our samples to a size of 512\u00d7512"}, {"title": "4.3. Trigger Reproduction and Challenges", "content": "After assessing the static backdoor proposed by Abad et al. [1]. We observed two main challenges for applying this approach in physical environments: 1) the trigger is inserted in all frames of each poisoned sample, and 2) the trigger polarity and size remain fixed through all the sample frames in each experiment. These limitations make the reproduction of triggers challenging due to the adaptive behavior of the DVS pixels, which adjust their firing thresholds based on recent light intensity changes. Unlike digital settings where triggers can be precisely inserted into specific positions and timing, physical reproduction requires manipulating light sources while accounting for the DVS's sensitivity and adaptation mechanisms. A static trigger, such as a fixed visual pattern, would only produce events initially and then become undetectable as the pixels adapt. To be detected again while maintaining the same polarity would require continuously increasing or decreasing its luminosity. This approach is impractical in real-world scenarios, as such constant change would affect the surrounding area, introducing unwanted noise."}, {"title": "5. Attacks on SNNS", "content": "5.1. Threat Model\nWe consider a scenario where a user wishes to train an SNN, F, but either lacks the resources to train the model independently or does not have the means to obtain its own dataset D.\nThe attacker's goal is to ensure that the backdoored model, Fe, behaves normally on clean data but follows the attacker's desired outcome when a specific trigger is present. To achieve this, we assume they have similar capabilities to prior research on backdoor injection [52], [54], [82], [87]. They can inject a small number of poisoned samples into the training data, Dtrain, either by tampering with the public dataset the user obtains or by manipulating the data during outsourced training through an MLaaS provider (e.g., Google Cloud, AWS). Limiting the amount of poisoned data is essential to avoid detection during any dataset analysis, as introducing a large number of poisoned samples increases the risk of being identified. Additionally, the attacker has no further control over the model training, knowledge of the internal weights, or the architecture of the trained model.\nIn either scenario, the user holds back a validation set, $D_{valid}$, to check the model's performance, which the attacker cannot access.\nMoreover, no further assumptions need to be made on either scenario to perform our attacks in the physical setting. We only assume that the attacker can mimic the digitally crafted triggers and access the camera used by the user to capture the physical samples to perform the attack."}, {"title": "5.2. Methodology", "content": "To ensure the reliability of our results, we conducted all the digital environment experiments on three datasets, NMnist [65], Cifar10-DVS [50], and DVS128-Gesture [6], and repeated each experiment several times to minimize variability. We chose the NMnist, Cifar10-DVS, and DVS128-Gesture datasets for our experiments due to their widespread use in neuromorphic research [2], [26], [41], [53] (for a more detailed description of each dataset or the models used, refer to Appendix B); each one of them was preprocessed to fit the input requirements of our SNN models. These datasets provide a variety of challenges and are suitable for testing the robustness and effectiveness of backdoor attacks on SNNs.\nWith the objective of overcoming the challenges discussed in Section 4, we propose two novel attacks: Framed Backdoor and Strobing backdoor, which can be combined into the Flashy backdoor to be applicable in real-world scenarios with DVS cameras.\n5.2.1. Framed Backdoor. We use this attack to evaluate the effect that the trigger size, number of frames, and the temporal location of the trigger have on the performance of the original [1] approach. Our experiments focus on a single position of the input space, and we vary the trigger's size, length, and temporal position for each possible polarity. We conduct our experiments with a $ \\epsilon $ value of 0.1 and the trigger's location set to the top-left (see Figure 4). We experiment with four different trigger sizes, ranging from 10% to 100% of the original frame, and varying trigger lengths, ranging from a single frame to eight consecutive frames (of a total of sixteen). Lastly, we test the influence of temporal position by injecting the backdoor at the input space's start, middle, and end.\nThe function $A(x, p, l, s, i, d)$ is a formal representation of the process of generating a set of poisoned inputs $D_{ps} : \\in D_{ps}$, where x denotes the clean sample that is subject to poisoning. The function takes in five parameters: p, representing the polarity of the trigger to be introduced, with four different options (ranging from"}, {"title": "5.2.2. Strobing Backdoor", "content": "This attack implements the idea of an intermittent trigger to exploit the temporal dynamics of SNNs and evade the adaptative threshold of DVS cameras. The DVS adapts each pixel's firing threshold to the luminosity of the last event they generated; consequently, if a static trigger is introduced, it would only fire once before adapting and would not be captured again until a change is detected. Thus, we approach the Strobing Backdoor using the same methodology as the Framed Backdoor, but this time, we inserted a certain amount of clean frames between each pair of poisoned frames. This change allows the DVS to adapt to the environment and reset its luminosity threshold (see Figure 5), enabling the attacker to maintain the same trigger luminosity, shape, and position while making sure that the trigger appears in the intended frames (see Figure 6). We also evaluate the effect that the size of the clean gap between the trigger frames has on the training process and the model's performance.\n$A(x, p, l, s, i, d, g)$ takes the parameters described to create a poisoned set of inputs $D_{ps}$. To select the poisoned frames of each sample, we use the following formulation:\n$$ \\forall t \\in [0, T), x_t = \\begin{cases} x_t, & \\text{if } t \\in [i, i+d) \\land (t-i) = 0 \\mod (g + 1) \\\\ X_t, & \\text{otherwise} \\end{cases} $$\nwhere $x_t$ represent sthe frames where the trigger is inserted and $X_t$ the clean frames. Note that if g = 0, this would be equivalent to the Framed Backdoor."}, {"title": "5.2.3. Flashy Backdoor", "content": "Finally, we combine the ideas of both Framed and Strobing Backdoors to create the Flashy backdoor and overcome the limitations found for real-world environment attack deployment. The main idea is to simulate the light of a strobing flashlight being pointed at the camera as our trigger while ensuring that the DVS camera would have enough time to reset its luminosity threshold, following the same principle explained in Figure 5 but expanded to all the pixels of the DVS camera instead of a specific section (see Figure 7). By doing this, we can maintain the same luminosity and trigger size for all our triggers."}, {"title": "5.2.4. Physical Trigger Reproduction", "content": "Once we tested this premise in virtual environments, we moved on to a physical environment.\nWe train a backdoored model with virtually introduced triggers on the DVS128-Gesture. We then capture new samples replicating the digital trigger by intermittently shining two common light sources, i.e., a household flashlight (75-110 lumens) and a regular red laser pointer directed at the camera. It is important to note that the attacker does not introduce any new samples on the dataset or modify the already trained networks in any way, i.e., no fine-tuning with the new poisoned samples was performed. This way, we evaluate the ability of the attacker to recreate the virtual triggers in physical environments."}, {"title": "6. Evaluation", "content": "6.1. Experimental Results\nWe evaluate the attacks with commonly used metrics:\n1) ASR measures the backdoor performance of the model based on a holdout fully backdoored dataset. To ensure accurate measurement, for every poisoned sample x, we verify that the original label y differs from the target backdoor label \u0177. If there is any sample x \u2208 Dps where y = \u0177, we replace that sample with another one to guarantee the correct assessment of ASR.\n2) Clean accuracy measures the model's performance on a holdout clean dataset.\nAll attacks were tested on each dataset three times, and the standard deviation (STD) was measured to account for variability.\n6.2. Framed Backdoor\nTo assess the viability of the proposed attack, we first experiment with the BadNets [35] approach, but limiting the position of the trigger to the upper-left corner and maintaining a constant poisoning rate e of 0.1. Then, we introduced the trigger in different time sections of the samples (start, middle, or end), with trigger duration ranging from 1 to 8 frames and four different trigger sizes:"}, {"title": "6.3. Strobing Backdoor", "content": "The next step is to analyze the effect a non-continuous trigger has on the model performance. We maintain the same limitations as in the previous test, but this time, we introduce a clean frame between each pair of poisoned frames. We proceed again to introduce the trigger in each time section, with trigger duration ranging from 3 to 13 frames.\nOur testing shows that the attack's performance is mostly equal or occasionally better when the trigger is positioned at the start or middle (see Table 3), without observing any degradation in the clean accuracy (see Table 3). However, when positioned at the end, its performance increases compared to the Framed Backdoor (see Table 4). This approach also enhances the attack's stealthiness, as it requires the trigger to be present in fewer frames than its continuous counterpart, and it yields improved performance in cases where the trigger is not present at the start (see Table 3).\nAfter doing a hyperparameter analysis, we conclude that modifying the trigger gap does not have any substantial effect on the clean accuracy. However, it positively affects ASR when the trigger is positioned at the end. Still, careful consideration of the gap is needed, as it can also make the training less stable for the backdoor behavior,"}, {"title": "6.4. Flashy Backdoor", "content": "We merge both concepts presented in the previous attacks, the framing and the strobing, but this time, we fix the trigger size to 100% of the original frames and ensure there is at least a clean frame between two poisoned frames. This approach ensures that the DVS camera has time to reset its threshold (see Figure 5) while making the triggers reproducible in real-world scenarios.\nAs shown in Table 1 and Table 3, our results suggest that combining the full-frame size trigger with the benefits of strobing results in better ASR in every scenario while maintaining the same or even achieving superior clean accuracy in all cases. These results suggest that it is better to concentrate the trigger in fewer frames rather than spreading it through the sample. In Table 1, poisoning 30% of the first 4 frames of the samples obtains substantially better results than poisoning 20% of 6 frames, even if the poisoning relative to the total amount of pixels in the samples is the same. Additionally, Table 2 and Table 4 show that separating the frames in which the trigger is inserted can make the insertion of the backdoor more stable and perform better when inserted in later temporal locations, making it easier to deploy in physical scenarios. Thus, we conclude that apart from being a necessary step for the transference to real-world environments, introducing a full-frame strobing flash is also beneficial in digital scenarios for both the attack's performance and stability."}, {"title": "6.5. Physical Environment Evaluation", "content": "Lastly, we transfer the Flashy Backdoor attack to a physical environment. As our DVS camera had different specifications compared to the one used to capture the DVS128-Gesture dataset, we first started by ensuring our model was able to identify our newly captured samples correctly. We capture both clean and poisoned samples with our DVS camera; the captured samples consist of 4 different subjects, with a total of 89 clean samples and 78 poisoned samples. Using these samples, we created"}, {"title": "6.6. Evaluation Against Backdoor Defenses", "content": "Given the lack of specific backdoor defenses for SNNs, we experiment with two commonly used methods in DL that have been studied as a means to enhance the security of already trained models, i.e., pruning and fine-tuning, and apply a defense from the DNN domain to work with neuromorphic data in SNNs: fine-pruning [56].\n6.6.1. Pruning. Pruning is commonly used to reduce the computational expense of DNNs [36], [48], [88], showing that a significant amount of neurons can be pruned before starting to compromise the classification accuracy. However, it was also explored as a method to enhance the security of corrupt DNNs by Liu et al. [56]. The premise behind this defense is that the attacked model learns to misbehave on backdoored inputs while behaving correctly on clean inputs because there are \"backdoor neurons\" that remain dormant in the presence of clean inputs but activate under the presence of the specified trigger, which was empirically demonstrated by Gu et al. [35].\nWe apply this procedure to the last convolutional layer of the model with a pruning rate of t = 0.8, meaning that 80% of the neurons in that layer are pruned. We also focus on the effect that the length of the trigger and the clean gap has in each proposed attack on the DVS128-Gesture, NMnist, and Cifar10-DVS datasets (see Table 7). As can be seen, even though the initial clean accuracy and ASR are high before pruning, we observe a substantial decrease in the clean accuracy across all datasets (except on NMnist, where the drop is less noticeable) after pruning.\nUpon examining these results, a general tendency can be observed, as pruned models trained with bigger triggers tend to have a more substantial drop in clean accuracy and less impact on ASR. This outcome was expected; a smaller trigger can be more discrete, but it also has less impact on the model, mainly injecting the backdoor on less influential neurons. The pruning process first focuses on removing these less influential neurons, making it easier to erase the backdoor behavior when the triggers are small. However, with the introduction of larger triggers, their influence on the model is also amplified, making them more challenging to target through this procedure.\n6.6.2. Fine-Tuning. In general, fine-tuning refers to taking a pre-trained neural network model and further training it on a new dataset or task. The pre-trained model is typically initialized with weights learned from training on a large, general dataset. During fine-tuning, the parameters of the pre-trained model are adjusted or fine-tuned to adapt to the specifics of the new dataset or task. This adjustment allows the model to learn task-specific features and patterns while leveraging the knowledge captured by the pre-trained model.\nIn our case, we take the given backdoored model and retrain it for a fixed amount of epochs on a clean dataset, allowing the model to try to \"forget\" the backdoor behavior. This is based on the concept of \"catastrophic forgetting\", which refers to the phenomenon where a neural network model trained on a specific task or dataset forgets previously learned information when trained on a new task or dataset [25]. We fine-tune the model for 10% of the network's original training epochs with the clean DVS128-Gesture, NMnist, and Cifar10-DVS datasets, focusing again on each attack type's trigger length and size (see Table 7). This decision was taken based on both prior studies that showed a significant ASR reduction with a small amount of fine-tuning epochs [56], [76] and the threat model, as the user who requested the service would not have the resources needed for extensive training.\nOur results suggest that fine-tuning the models for a small number of epochs can reduce the backdoor effect for certain datasets and attack configurations, particularly those involving smaller triggers and shorter trigger lengths on the DVS128-Gesture dataset. However, longer triggers and larger trigger sizes tend to be more resilient, showing less reduction in ASR after fine-tuning. It is also noteworthy that including a clean frame between triggers resulted in higher ASRs across all tested datasets than their continuous counterparts.\n6.6.3. Fine-Pruning. This defense [56] is a combination of both pruning and fine-tuning. The defender first prunes the network and then fine-tunes the result. The main intuition behind this defense is that the initial pruning removes the backdoor neurons, and the following fine-tuning restores the drop in classification accuracy on the clean inputs introduced by the pruning.\nEven if this defense can be effective when the trigger size is small, as demonstrated by Abad et al. [2], when applied against larger triggers, the results are not straightforward. We can observe a higher success when compared to the previous two methods, achieving a substantial reduction in ASR and being able to recover almost all of the clean accuracy in most cases. But when faced with bigger triggers in NMist or Cifar10-DVS datasets, it is not able to completely eliminate the backdoor behavior (see Table 7).\nWe hypothesize that the reason this defense performs better than fine-tuning alone, even though pruning by itself does not seem to be effective, is that pruning the less important neurons allows the fine-tuning process to focus on the more critical neurons where the backdoor is embedded. By eliminating the neurons that contribute least to the model's overall performance, pruning reduces the model's capacity to retain the backdoor behavior. This targeted approach enables the fine-tuning phase to overwrite the backdoor-influenced parameters, thus enhancing the defense's effectiveness.\nIt is also to be noted that fine-pruning focuses on identifying and nullifying the model parameters that introduce the malicious behavior. However, as demonstrated in previous works [2], when faced with an adaptive attacker, it becomes easily evaded. As discussed by Abad et al. [2], an attacker who knows that the last convolutional layer is targeted for pruning could limit the backdoor injection to the rest of the network or reduce the poisoning rate, achieving high ASR and nullify the pruning procedure."}, {"title": "6.7. Evaluation of the Stealthiness", "content": "Assessing the stealthiness of triggers in backdoor attacks is critical to ensure their effectiveness while remaining undetected. In practical applications, the datasets employed are often extensive [28", "5": "but also prone to human error and subjectivity [81", "43": ".", "59": ".", "86": "is a widely used metric that calculates the average square values of the differences between the corresponding pixel values of the original and the modified images. A lower MSE value indicates higher similarity between the images, suggesting that the trigger is less perceptible. However, this metric is discussed not to be a good representation of how a human user would discern the change in the image [86", "85": "is another widely used metric that assesses the visual impact of changes in structural information between two images [59", "12": "."}, {"86": "which in this case can be very beneficial since the pixels have no set range; it depends on the number of activations that occurred on the selected time frame. This metric expresses the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. PSNR is expressed in decibels (dB) and is inversely related to the MSE.\nPSNR is particularly useful in scenarios where the human perception of image quality is crucial. It is extensively used in image compression, transmission, and processing to evaluate how much an image has been degraded by these processes [12"}]}