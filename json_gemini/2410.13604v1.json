{"title": "Large Language Models as Narrative-Driven Recommenders", "authors": ["Lukas Eberhard", "Thorsten Ruprechter", "Denis Helic"], "abstract": "Narrative-driven recommenders aim to provide personalized suggestions for user requests expressed in free-form text such as \u201cI want to watch a thriller with a mind-bending story, like Shutter Island.\" Although large language models (LLMs) have been shown to excel in processing general natural language queries, their effectiveness for handling such recommendation requests remains relatively unexplored. To close this gap, we compare the performance of 38 open- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in a movie recommendation setting. For this, we utilize a gold-standard, crowdworker-annotated dataset of posts from reddit's movie suggestion community and employ various prompting strategies, including zero-shot, identity, and few-shot prompting. Our findings demonstrate the ability of LLMs to generate contextually relevant movie recommendations, significantly outperforming other state-of-the-art approaches, such as doc2vec. While we find that closed-source and large-parameterized models generally perform best, medium-sized open-source models remain competitive, being only slightly outperformed by their more computationally expensive counterparts. Furthermore, we observe no significant differences across prompting strategies for most models, underscoring the effectiveness of simple approaches such as zero-shot prompting for narrative-driven recommendations. Overall, this work offers valuable insights for recommender system researchers as well as practitioners aiming to integrate LLMs into real-world recommendation tools.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, tools powered by large language models (LLMs), such as OpenAI's chatbot ChatGPT, gained attention due to their high performance in various natural language processing (NLP) tasks. For example, LLMs showed increased performance on tasks such as translation, question-answering, cloze tests [9], linguistic analyses of generated content [21], or re-ranking tasks [52]. In some initial studies related to recommender systems, LLMs also demonstrated great potential for specific recommendation tasks, such as explanation [34], ranking [11], as well as conversational [23], content-based [33], or next-item recommendations [55]. However, suitability of LLMs for a particularly promising narrative-driven recommendation scenario [8] remains still relatively unexplored.\nIn narrative-driven recommendation scenarios users pose free-form requests such as \u201cI just want to see a movie where the good guy kicks some ass!\u201d These queries are commonly submitted to online forums such as reddit, where communities respond with tailored suggestions (Fig. 1, right). While asynchronous community responses to such posts generally fulfill users' requests, recent studies applying traditional machine learning methods to such recommendation scenarios indicate potential for improvement, as the problem of narrative-driven recommendations proves difficult for existing recommender approaches [15\u201317]. Given these challenges, in this paper, we set out to evaluate the potential of LLMs in such a recommendation scenario, leveraging their capabilities to better understand and process user-generated narratives.\nTo this end, we probe LLMs with movie recommendation requests originally posed by real users to the r/MovieSuggestions\u00b9 community on reddit (Fig. 1, center). Utilizing these user requests along with the accompanying comments that contain community recommendations [17], we investigate how well responses by open- and closed-source LLMs match the recommendations from the reddit community. In total, we evaluate 38 state-of-the-art LLMs, categorized by size, ranging from tiny (<4 billion parameters) to large (\u226550 billion parameters), employing various prompting strategies. Specifically, we assess the performance of these models using zero-shot [30], identity [29], and few-shot [18, 19] prompting (Fig. 1, left). Finally, we compare the performance of the evaluated LLMs with other recommendation approaches, such as doc2vec [31].\nOur results demonstrate that LLMs can effectively respond to natural language prompts, translating user requests into relevant recommendations. We present four key findings from our comprehensive evaluation of movie recommendation tasks. First, we observe that using basic zero-shot prompting, LLMs are able to generate movie recommendations that rival or surpass those of traditional and state-of-the-art recommender approaches. In particular, GPT-4o as the overall best-performing LLM exhibits a recommendation performance 70% higher than the baseline. Second, expanding the prompting techniques through identity or few-shot prompting does not significantly improve recommendation performance, underscoring the strength of LLMs when using simple zero-shot prompting for this task. Third, we find that medium-sized open-source models (e.g., Gemma 2 27B) perform competitively with similarly sized closed-source models (e.g., GPT-3.5 Turbo) and even larger open-source models (e.g., Mistral Large 2 123B). Finally, our findings hold even when accounting for potential data leakage, specifically the possibility that data used for our evaluation may have also been incorporated during the pre-training phase of certain LLMs. Altogether, this work offers practical insights for both recommender system researchers and practitioners looking to integrate LLMs into real-world recommendation applications. Finally, we make our code and employed datasets publicly available.\u00b2"}, {"title": "2 LLMS AS RECOMMENDER SYSTEMS", "content": "2.1 Setup\nDataset. We use a crowdworker-curated dataset\u00b3 for our experiments [17]. This dataset consists of annotated submissions and comments from reddit [7], more specifically from the subreddit r/MovieSuggestions. In particular, this dataset includes reddit submission IDs, titles, and original texts, as well as movie titles, actor names, keywords, and genres that were annotated by crowdworkers. Each recommendation request in the dataset contains one or more positively mentioned movies (i.e., examples of movies that the user liked before) as well as additional information, such as negatively mentioned movies (i.e., movies that the user did not like before), positively or negatively mentioned keywords (describing further aspects of the movies), and genres. While the full dataset contains 1 480 submissions (from August 2011 to July 2017) making up test and training data, for fair comparison with prior work [15] we solely utilize the test set of 296 submissions (from November 2016 to July 2017), with 778 unique mentioned movies and 4329 comments including over 11000 individual recommendations by reddit users and 3 593 different movies as suggestions.\nLLMs. In our experiments, we use 35 open-source and three commercial OpenAI LLMs, spanning 13 distinct model families (Table 1).\n2.2 Prompting Experiments\nWe evaluate three popular prompting strategies including zero-shot, identity, and few-shot prompting in separate experiments. In particular, for each reddit submission from the test dataset we generate a single LLM request that is composed of a system prompt and a user prompt (Fig. 1, center). While the user prompt maintains a consistent format across all experiments\u2014comprising the title and body of a submission within specified tags\u2014the system prompt varies depending on the experiment. The system prompt includes a Task section and, depending on the experiment, optional Persona and Examples sections (Fig. 1, left).\nZero-Shot Prompting. In this experiment, we assess out-of-the-box performance of LLMs as narrative-driven recommenders in the movie domain. Hence, our zero-shot prompt consists of a Task section including instructions for the model to recommend movies based on a user's specific request provided in the form of tags, and constraints that define the format specifications and limitations for the expected output. To facilitate post-processing, we request the model to return a JSON object containing exactly ten movie recommendations to calculate our evaluation metrics @10 (e.g., F1@10)-for fair comparisons with previous results [15]. To simplify the distinct mapping of movies during evaluation, we request from LLMs to format each recommendation as a single string including the movie's title and release year (e.g., \"Titanic (1997)\"). Additionally, to compare recommendations with the gold-standard recommendations from the reddit community, we request that recommended movies are released before the date of the original reddit submission. Finally, the zero-shot prompt serves as the foundation that we extend in the remaining two experiments.\nIdentity Prompting. The Persona section of the prompt templates defines the LLM identity such as a reddit user, a movie critic, or a movie recommender. Hence, in the identity prompting experiments, this section is prefixed to the system prompt used in the zero-shot experiment to direct LLM responses towards a particular persona.\nFew-Shot Prompting. In few-shot prompting, we provide multiple random input-output examples that showcase the requested response structure. Thus, we extend the zero-shot system prompt with the Examples section containing either one, five, or ten examples to steer the LLM's generating process. These examples contain exemplary user prompts as well as JSON responses."}, {"title": "2.3 Evaluation of LLM Responses", "content": "Following our prompting experiments, we parse the corresponding LLM responses, match the contained titles to actual movies, and compute established evaluation metrics to assess LLM performance.\nIdentifying Recommendations. After prompting the LLMs, we proceed by extracting recommendations from the JSON responses. In particular, we apply a regex to parse correct recommendations in the format \u201c<movie_title> (<release_year>)\u201d (Fig. 1, center) and remove any duplicate strings as well as ignoring case. Finally, for responses containing more than ten movies, we randomly sample ten to compute our evaluation metrics.\nTitle Matching. We compare LLM recommendations to the recommendations from the reddit community by exact matching of title and release year. As the community recommendations in the dataset are identified by their IDs on IMDb, we first filter out recommendations that were already mentioned in the submission text. Then, we retrieve the titles of the remaining community recommendations from IMDb in all available languages (e.g., original title: \u201cUn proph\u00e8te (2009)\u201d, international title: \u201cA Prophet (2009)\u201d). Additionally, we apply soft matching using Ratcliff/Obershelp pattern-matching [47] with a threshold of 0.9. As we observe no significant difference in the results between exact and soft matching, we only report the exact matching results.\nEvaluation Metrics. We compute four standard recommendation metrics\u2014precision, recall, F1 score, and Normalized Discounted Cumulative Gain (NDCG) [45, 60], with fixed cut-off value (i.e., Precision@10, Recall@10, F1@10, NDCG@106)-for each request and report their overall means over the whole test dataset (i.e., macro average). Due to space limitation, we report results aggregated by model size category and prompting strategy for all metrics and only F1 results for individual models in the main text. We present more detailed results for all evaluation metrics in the Appendix."}, {"title": "3 RESULTS", "content": "We present our experimental findings across 38 evaluated LLMs. First, we assess the structural correctness and validity of the LLM responses in relation to the constraints posed in our prompts. We then evaluate the performance of LLM recommendation, especially considering different-sized LLMs, the effectiveness of different prompting strategies, and open- versus closed-source models. Finally, we compare LLMs to other recommendation approaches and present a robustness experiment regarding potential data leakage. To statistically substantiate our results, we bootstrap the dataset in all experiments and report bootstrapped 95% confidence intervals.\n3.1 Format Adherence\nWe assess the compliance of the LLM-generated outputs with the formatting and structural instructions by checking correctness of the JSON format, the total number of returned recommendations, the frequency of unique movie entries within a recommendation list, and the movie release year.\nJSON Format. Our analysis of LLM responses reveals that the fraction of valid JSON responses exceeds 97% for all aggregations of model size category and prompting strategy (Fig. 2a). This demonstrates high proficiency of LLMs in generating correctly formatted responses. Large LLMs exhibit exceptional performance, achieving valid JSON output in over 99.9% of cases across all prompting strategies. In more details (Appendix, Fig. 8), the GPT models as well as Gemma 2 9B, Llama 3 70B, Llama 3.1 8B and 70B, and Qwen 2.5 14B consistently produce 100% valid JSON responses. In contrast, some of the smaller models are more prone to error, such as Llama 3.2 1B with 93.39%, Mistral 7B with 91.75%, Qwen 2 0.5B with 92.76%, Yi 6B with 94.45%, and Phi-3 14B with around 94% of correct JSON responses. Moreover, for Phi-3 14B we observe a significant decline in performance for few-shot prompting, likely due to the model's difficulty in processing longer requests.\nNumber of Recommendations. Compliance with the constraint of responding with exactly ten recommendations varies considerably across model size categories and prompting strategies (Fig. 2b). Large LLMs reach nearly perfect adherence to the prompt, with the fraction of responses containing precisely ten recommendations exceeding 98% on average over all models and prompting strategies. Medium-sized models attain an average score of 96% over all prompting strategies with slightly greater variability than their larger counterparts. In contrast, smaller, especially tiny-sized models demonstrate more inconsistent behavior. On average, less than 40% of valid JSON responses from tiny models contain exactly ten recommendations. Furthermore, different prompting approaches (e.g., zero-shot versus few-shot) show only marginal and no significant differences in the average number of recommendations, except for small-sized LLMs with a significant higher average when using few-shot prompting. This highlights the robustness of modern LLMs in meeting specific output requirements with simple prompting configurations, particularly for well-trained and larger architectures. In detail, we observe that each large LLM consistently provide exactly ten recommendations in over 91% of the requests, regardless of the prompting strategy (Appendix, Fig. 9). Qwen2 72B achieves the highest adherence to the number of recommendations constraint, with more than 99.9% of its responses meeting this criterion. In contrast, the smaller models in the tiny category, such as Qwen2 and Qwen2.5 0.5B, frequently fail to include any movies in the zero-shot and identity prompting experiments, with over 92% of their responses lacking movie recommendations. However, the performance of these smaller models improve significantly when provided with few-shot prompts, reducing the proportion of responses without any movie recommendations.\nUnique Movies. On average across all model size categories and prompting strategies, LLMs return over 99% unique movies in their valid outputs (Fig. 2c). However, tiny and small models exhibit a marginal but statistically significant increase in duplicate movie recommendations. Gemma 2B and 7B, alongside Qwen 2.5 7B, show the highest rates of duplication, averaging 2-4% duplicate recommendations (Appendix, Fig. 10)."}, {"title": "3.2 Recommendation Performance", "content": "Our evaluation of the recommendation performance of various LLMs reveals several key insights regarding different model sizes and prompting strategies. In particular, we highlight the general suitability of medium and large LLMs as narrative-driven recommenders, the effectiveness of zero-shot prompting, competitiveness of medium- with larger-parameterized LLMs, as well as high performance of open-source models. Figure 3 depicts aggregated performance results over model size categories and Figure 4 shows F1 scores (i.e., F1@10) of all models categorized by model families across various prompting strategies. We list further detailed results for all models in the Appendix (Figs. 12, 13, 14, and 15).\nMedium and Large LLMs Outperform Baselines. To evaluate the general suitability of LLMs as narrative-driven recommenders, we compare our results with the state-of-the-art recommendation baselines evaluated on the same dataset [15]. The baseline methods utilized a range of established recommendation algorithms, such as doc2vec, collaborative filtering, matrix factorization, or a TF-IDF-based approach, with the doc2vec approach being the best-performing method with an F1 score of 0.1258 [0.1125, 0.1388] (horizontal lines in Figs. 3 and 4).\nOur experimental results show that the medium and large model categories significantly outperform the traditional methods across all evaluated configurations regardless of the prompting strategy. Specifically, all of the large, the majority of the medium, as well as some of the small LLMs achieve higher scores across all metrics, indicating a substantial improvement when generating movie recommendations from narrative user queries. For example, apart from most of the medium and large models even small-sized LLMs, such as Gemma 2 7B or GPT-4o mini, outperform the best state-of-the-art recommender method on this dataset, doc2vec, across all applied prompting strategies (Fig. 4 and Appendix, Figs. 12, 13, 14, and 15). In particular, the best-performing LLM, large-sized GPT-4o, with identity prompting and the persona movie critic achieves an F1 score of 0.2157 [0.2009, 0.2302], which is more than 70% higher than the performance of doc2vec.\nGiven the minor differences in experimental setups between our work and the studies from which we derive our baselines [15, 17], we conduct an additional sensitivity experiment. Notably, the recommender approaches serving as our baselines generated exactly ten unique movie recommendations from a predefined IMDb movie pool of around 12 000 movies. In this sensitivity experiment, we map our LLM recommendations to that same movie pool and filter out all others. If the LLM produces fewer than ten recommendations, we repeat the request until we collect exactly ten movies. For this experiment, we only use GPT-3.5 Turbo-a medium-sized, closed-source LLM that performs strongly in our benchmarks and shows comparable results to similar-sized open-source models (e.g., Gemma 2 27B) as well as other closed-source models of varying sizes (e.g., GPT-4o). Using identity prompting with the reddit user persona, we obtain an F1 score of 0.2348 [0.2189, 0.2508], indicating a further improvement in LLM performance over the baselines.\nEffectiveness of Zero-Shot Prompting. Our results demonstrate that zero-shot prompting achieves a high level of performance across all model size categories (Fig. 3), suggesting that additional prompting complexity with identity or few-shot prompting yields only minor gains in recommendation accuracy. This pattern is consistent across all model sizes, indicating that zero-shot prompting is a robust strategy for a variety of models, from tiny to large. In more details, we find that the performance gains from zero-shot to identity and few-shot prompting are more substantial for some of the smaller models, suggesting that these strategies are particularly useful for improving the capabilities of less parameterized models (Fig. 4 and Appendix, Figs. 12, 13, 14, and 15). For example, the F1 score using the small-sized Mistral 7B model with zero-shot prompting of 0.1053 [0.0929, 0.1174] significantly increases when applying few-shot prompting up to 0.1342 [0.1219, 0.1462].\nMedium-Sized Models Compete With Larger LLMs. Despite the general expectation that larger models would significantly outperform smaller ones, our results demonstrate that medium-sized models with parameter counts between 10 and 50 billion can achieve almost equivalent accuracy as their larger counterparts. The aggregated performance scores by model size category and prompting strategy highlight this finding (Fig. 3). Moreover, Figure 4 also illustrates this performance across various models and model families. Model families such as Gemma 2 are competitive with more computationally expensive LLMs such as the larger GPT or Mistral models, achieving similar F1 scores. More detailed results for all models in Appendix (Figs. 12, 13, 14, and 15) further highlight our findings. For example, the best-performing medium-sized GPT-3.5 Turbo model with an F1 score of 0.1932 [0.1876, 0.1986] on average over all prompting strategies, almost performs on par with the large-sized GPT-4o, which exhibits the highest score of all models with an F1 score of 0.2137 [0.2084, 0.2191].\nClosed-Source Over Open-Source Models. We find that the closed-source GPT models show prominent performances compared to most of the similar sized open-source models within their respective model size categories (Fig. 4). The results further exhibit that medium-sized open-source models often outperform smaller closed-source models, suggesting that even without proprietary data, open-source models can be competitive with appropriate parameter sizes. In more details, the large closed-source model GPT-4o achieves the highest F1 scores of over 0.21 throughout all prompting strategies (Appendix, Fig. 12). The best-performing open-source model, Mistral Large 2 123B, reaches an F1 score of around 0.20. Medium-sized open-source models such as Gemma 2 27B perform on a similar accuracy level as the closed-source medium-sized GPT-3.5 Turbo model. In the category of small models we find that the closed-source GPT-4o mini slightly but not significantly outperforms the best open-source counterpart, Gemma 2 9B."}, {"title": "3.3 Addressing Potential Data Leakage", "content": "Given the setup of our experiment, our results may be susceptible to data leakage due to the possible overlap of submissions in our reddit dataset with the LLMs' training data [49]. For this, we follow the methodology of prior work [50] to compare model performance before and after the knowledge cutoff to detect any significant pre- and post-cutoff differences.\nSetup. As previously, we conduct this robustness analysis with GPT-3.5 Turbo. Given that GPT-3.5 Turbo's knowledge cutoff (i.e., September 2021) is outside the timeframe of our original dataset, we first use GPT-4o as a higher-parameterized expert model to extract recommendations from reddit posts made after this cutoff. Using this dataset, we then repeat our zero-shot experiment with GPT-3.5 Turbo and compare the results to the main zero-shot results.\nRobustness Analysis Dataset. We consider all submissions from r/MovieSuggestions submitted from October 2021 to July 2024 that contain the word \u201crequest\u201d in the submission title to ensure the post contains actual movie inquiries (92 040 submissions). We then keep all submissions with at least five comments with more upvotes than downvotes, focusing on high-quality and engaging user discussions (441 submissions remaining). In contrast to the crowdworker-labeled dataset for our previous experiments, we now employ GPT-4o to extract movie mentions from the requesting user's post as well as from commenters' replies (Appendix, Fig. 7 for the prompt). Finally, similar as in our original dataset, we exclude submissions with no movies mentioned in the request and less than ten recommendations made by commenters. This final robustness analysis dataset consists of 236 submissions.\nPerformance After Knowledge Cutoff. We rerun our zero-shot prompting experiment using GPT-3.5 Turbo on the robustness analysis dataset, achieving an F1@10 of 0.1667 [0.1513, 0.1814],\u2077 which does not indicate a significant decline compared to our original results (F1@10: 0.1962 [0.1809, 0.2112]). However, we observe a minor decline in performance, potentially due to movies released after the knowledge cutoff being relevant recommendations for certain requests in the robustness analysis dataset. This supports the validity of our findings, even considering that parts of our evaluation dataset could have been included in the LLMs' training data."}, {"title": "4 DISCUSSION", "content": "Our findings reveal that LLMs are highly suitable as narrative-driven recommenders. LLMs demonstrate remarkable utility, especially compared to traditional recommender baselines, in generating contextually relevant recommendations from free-form narrative inputs, which can be particularly valuable in enhancing user experience in personalized movie suggestions. In particular, we compare the performance of LLMs as narrative-driven movie recommenders to state-of-the-art recommender approaches [15] and find substantial leaps in recommendation quality. We attribute this to the superior ability of LLMs to interpret complex natural language expressions and nuances, which traditional methods, such as matrix factorization or doc2vec, often fail to capture effectively. Unlike these traditional systems, which rely on extensive feature engineering and domain-specific fine-tuning, LLMs can extract meaningful contextual information directly from narrative requests due to their extensive pre-training.\nMoreover, similarly to recent work that demonstrated the effectiveness of zero-shot prompting in various NLP tasks [48, 55], we find that more extensive prompting strategies such as identity or few-shot prompting do not outperform zero-shot prompting. These results suggest that even simple prompting strategies can effectively capture essential user preferences in narrative-driven recommendation tasks, reducing the need for extensive prompt engineering, model training, or fine-tuning. This result aligns with a recent study that evaluated the performance of various prompts for GPT-3.5 Turbo on multiple recommendation scenarios based on Amazon customer reviews, such as rating prediction [32].\nNevertheless, while we find that simple zero-shot prompting performs as well as other strategies such as few-shot prompting, more sophisticated techniques could potentially further enhance recommendation quality. For example, Wang and Lim [55] explored the application of LLMs utilizing GPT-3 for zero-shot next-item recommendation in the context of movie suggestions. The authors developed a three-step prompting strategy to execute subtasks that capture user preferences, select representative previously watched movies, and recommend a ranked list of ten movies. Their experimental evaluation on the MovieLens 100K dataset indicates that this method can outperform traditional recommendation models that require extensive training."}, {"title": "5 FURTHER RELATED WORK", "content": "The emergence of LLMs, such as BERT [14] or GPT [46], has led to new developments in NLP. Due to the significant increase in model sizes and the massive amount of training data, LLMs have shown ability of understanding and executing a wide variety of tasks, such as reasoning [10], software engineering [24] or language translation [9]. OpenAI's ChatGPT as well as several competitive open-source LLMs, including Meta's Llama, Mistral, and Microsoft's Phi, have demonstrated exceptional performance, surpassing prior state-of-the-art NLP models across numerous tasks [22, 25, 57, 62].\nLLMs are typically instructed through prompts\u2014instructions to translate texts, answer questions, or write essays. Recent advancements in LLMs have sparked growing interest in enhancing their task performance through various prompting strategies [9], such as zero-shot, identity, or few-shot prompting. Further promising prompting strategies, such as chain-of-thought [56] or tree-of-thought [59], enhance the reasoning abilities of generative models, particularly in tasks such as question answering, by guiding them through human-like logical reasoning processes.\nAdditionally, recent work explored the suitability of LLMs for recommenders, revealing that models, such as Llama and GPT, improve user satisfaction with richer explanations [34], excel in data-sparse [43] and cold-start [51] scenarios, and can deliver highly relevant recommendations through conversational interactions [38].\nThis Work. In this paper, we are-to the best of our knowledge-the first to address the suitability of LLMs as narrative-driven recommenders. In particular, we utilize user-submitted text from reddit, consisting of narrative requests for movie recommendations, to evaluate the quality of LLM-generated recommendations."}, {"title": "6 CONCLUSION", "content": "In this paper, we evaluate the suitability of LLMs in a narrative-driven movie recommendation setting, comparing their performance to traditional state-of-the-art recommender systems. Our comprehensive analysis of 38 LLMs, both open- and closed-source, reveals that LLMs can effectively generate personalized movie recommendations from user-provided narratives, significantly outperforming traditional approaches, such as doc2vec. We find that while larger closed-source models generally demonstrate superior performance, medium-sized open-source models remain competitive, offering a viable trade-off between computational cost and recommendation quality. Notably, we observe minimal differences in effectiveness between zero-shot, identity, and few-shot prompting, indicating that simple approaches are sufficient for generating high-quality recommendations. These findings highlight the potential of LLMs to transform narrative-driven recommender systems, offering scalable solutions for integrating natural language capabilities into real-world applications. Our results also underscore the versatility of LLMs in real-world recommender applications. Minimizing prompt complexity is crucial, as it reduces operational overhead and latency, simplifying the integration of LLMs into existing recommendation frameworks. This finding is particularly valuable for researchers and practitioners aiming to incorporate LLMs in practical settings without the additional computational costs of model fine-tuning or complex prompt engineering."}]}