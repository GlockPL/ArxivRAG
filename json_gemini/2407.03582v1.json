{"title": "Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content", "authors": ["Andrew Bouras"], "abstract": "The generation of diverse, high-quality outputs from language models (LLMs) is essential for various applications, including education and content creation. Achieving true randomness and avoiding repetition remains a significant challenge. This study employs the Linear Congruential Generator (LCG) method for systematic fact selection, combined with AI-powered content generation. Using LCG, we ensured unique combinations of gastrointestinal physiology and pathology facts across multiple rounds, integrating these facts into prompts for GPT-40 to create clinically relevant, vignette-style outputs. Over 14 rounds, 98 unique outputs were generated, demonstrating LCG's effectiveness in producing diverse and high-quality content. This method addresses key issues of randomness and repetition, enhancing the quality and efficiency of LLM-generated content for various applications.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Background", "content": ""}, {"title": "Randomness in Large Language Models", "content": "Randomness is essential in LLMs for generating diverse content. LLMs rely on stochastic processes to produce varied and human-like text, ensuring outputs are not deterministic. This randomness allows LLMs to generate different responses to the same prompt, enhancing their utility in content creation, conversational agents, and educational tools [1]. Randomness also helps maintain diversity and quality in text generation [2] and facilitates coherent and contextually appropriate responses for applications like paraphrase generation and text style transfer [3]. Techniques like the LCG systematically incorporate randomness, ensuring variability and control in outputs [4]."}, {"title": "Multiple-Choice Questions", "content": "Multiple-Choice Questions (MCQs) are crucial in medical education, serving as key tools for both formative and summative assessments. They efficiently evaluate a wide range of knowledge and clinical reasoning skills across medical disciplines [8]. MCQs offer objective scoring, comprehensive content coverage, and the ability to assess large cohorts simultaneously [9]. Well-constructed MCQs can distinguish between varying levels of student performance, providing insights into learners' comprehension of complex medical concepts [10].\nCreating high-quality MCQs is challenging. Developing questions that are clinically relevant and cognitively demanding requires significant time, expertise, and resources [11]. Educators must craft questions that assess higher-order thinking skills, including application, analysis, and evaluation of medical knowledge [12]. Ensuring diversity in question content and preventing repetition across assessments is particularly challenging given the extensive and evolving nature of medical curricula [13]. The rapid pace of medical advancements necessitates continual updates to assessment materials [14]."}, {"title": "Objective", "content": "This study investigates integrating controlled randomness into LLMs to generate clinically relevant content, focusing on high-quality MCQs in medical education. By employing the LCG method, we aim to combine algorithmic precision with the adaptive capabilities of AI-powered language models.\nOur research addresses the challenge of introducing structured variability into AI-generated content while maintaining clinical accuracy and relevance. Using the LCG method for systematic fact selection and integrating it with advanced language models, we aim to create a framework that produces diverse, cognitively challenging, and clinically pertinent educational materials.\nThe broader implications extend beyond question generation, exploring how controlled randomness can enhance the versatility and applicability of LLMs in specialized domains like medical education. This approach aims to improve the quality and diversity of assessment materials and provide new methodologies for guiding AI systems in generating content that adheres to specific criteria while maintaining creativity and relevance.\nUltimately, this research seeks to advance AI applications in education by offering a solution that balances algorithmic control with AI adaptability. This approach could transform the development and utilization of educational materials in medical education and beyond."}, {"title": "Methodology", "content": ""}, {"title": "Linear Congruential Generator", "content": "LCG is a fundamental and widely-used pseudo-random number generator (PRNG) algorithm that operates on the principle of linear recurrence. The LCG generates a sequence of numbers using the recurrence relation:\n$X_{n+1} = (aX_n + c) \\mod m$\nWhere:\n$X_n$ is the current number in the sequence.\na is the multiplier.\nc is the increment.\nm is the modulus.\nThe choice of parameters $a, c, m$, and the seed value $X_o$ is critical for the performance of the LCG. These parameters determine the period, randomness, and statistical properties of the generated sequence. In our study, the following parameters were used:\nSeed ($X_o$): 12345\nMultiplier (a): 1103515245\nIncrement (c): 12345\nModulus (m): $2^{31}$\nThese parameters were selected based on established practices to balance computational simplicity with the quality of randomness [7]. The seed value influences the initial state but does not affect the statistical properties beyond the initial value. The multiplier 1103515245 ensures a long period and good randomness properties, while the increment 12345 avoids trivial sequences. The modulus $2^{31}$ provides a wide range for the output values.\nThe LCG algorithm with these parameters ensures desirable statistical properties, such as uniform distribution and long period, which are essential for avoiding repetitions and maintaining randomness in simulations and randomized content generation [4].\nIn this study, the LCG generated sequences of indices to select facts from a predefined pool of 100 clinically relevant facts about gastrointestinal physiology and pathology. Mapping the generated pseudo-random numbers to these indices ensured each round of MCQ generation was based on a unique and diverse set"}, {"title": "Fact Selection Process", "content": "We created a comprehensive fact mapping containing 100 clinically relevant facts about gastrointestinal physiology and pathology. Each fact was assigned a unique index from 1 to 100. The LCG algorithm was used to generate a sequence of pseudo-random numbers, which were then mapped to these fact indices. The fact selection process adhered to two primary criteria: uniqueness and clinical relevance. A set of used facts was maintained throughout the experiment to ensure that each fact had not been previously used, guaranteeing uniqueness across rounds. This approach is crucial for maintaining the diversity and comprehensiveness of the content generated [7].\nEnsuring clinical relevance was paramount; all facts in the mapping were curated to ensure their relevance to gastrointestinal medicine, covering a wide range of topics from basic physiology to complex pathological conditions. This curated approach helps in generating content that is not only diverse but also highly pertinent to the medical field [5]. The process of fact selection using LCG was methodically structured to avoid biases and ensure a balanced representation of different clinical scenarios, thereby enriching the educational and practical value of the generated content [6].\nBy systematically managing and directing the randomness through the LCG algorithm, we were able to achieve a high degree of control over the selection process, thus maintaining the integrity and quality of the educational content produced [17]. This innovative approach underscores the potential of combining algorithmic precision with large language models to enhance the generation of clinically relevant content."}, {"title": "MCQ Generation", "content": "For generating MCQs, we utilized a carefully crafted base prompt. This prompt instructed the AI model to create clinically relevant, vignette-style questions that required the application of knowledge rather than simple recall. The prompt specified the creation of seven high-quality MCQs per round, including patient demographics, presenting symptoms, and relevant medical history in each vignette. Additionally, it required the provision of five plausible answer choices per question listed alphabetically, and the inclusion of a correct answer and brief explanation for each question.\nThe selected facts from the LCG process were integrated into this base prompt, providing the specific content around which the MCQs were to be generated. This integration ensured that each set of MCQs was based on a unique combination of facts, promoting diversity in the generated questions. For the"}, {"title": "Experiment Setup", "content": ""}, {"title": "Rounds of MCQ Generation", "content": "This study conducted 14 rounds of MCQ generation using the LCG method. Each round aimed to produce a set of seven high-quality, clinically relevant MCQs based on unique facts from a predefined pool of 100 gastrointestinal physiology and pathology facts.\nTo ensure the uniqueness of facts across rounds, we implemented the LCG algorithm with specific parameters: seed ($X_0$) of 12345, multiplier (a) of 1103515245, increment (c) of 12345, and modulus (m) of $2^{31}$. This algorithm generated a sequence of pseudo-random numbers, which were then mapped to fact indices. A set of used facts was maintained throughout the experiment to ensure each fact was not reused in subsequent rounds, thus guaranteeing uniqueness across all rounds [?].\nThe rigorous application of the LCG parameters ensured that the generated sequences exhibited good statistical properties and long periods, which are essential for maintaining the randomness and unpredictability of the selection process [7]. This systematic approach was crucial for preventing repetition and ensuring the diversity and comprehensiveness of the generated MCQs. The consistency and reliability of the LCG method in generating unique fact sets were pivotal in achieving the study's objectives, thus showcasing the robustness of this algorithm in educational content generation."}, {"title": "Evaluation Metrics", "content": "The quality of the generated MCQs was evaluated based on several criteria. Clinical relevance was a primary consideration, with each question assessed for its applicability to real-world medical scenarios. This ensured that the questions were not only theoretically sound but also practically useful. Cognitive level was another critical factor; questions were evaluated using Bloom's Taxonomy"}, {"title": "Results", "content": ""}, {"title": "Overview of the MCQs Generated in Each Round", "content": "This study employed the LCG method to produce clinically relevant MCQs across multiple rounds. Each round aimed to create high-quality, diverse questions by selecting unique facts not previously used. The LCG method demonstrated effectiveness in generating varied and comprehensive MCQs, as evidenced by the examples below.\nIn Round 1, the LCG method utilized facts [7, 76, 25, 74, 79, 60, 93] to create questions. For instance, one MCQ focused on a 65-year-old male presenting with dysphagia, weight loss, and regurgitation. The question tested the ability to diagnose esophageal carcinoma based on clinical presentation and endoscopic findings, demonstrating the application of fact 76 (colon carcinoma characteristics) to a related gastrointestinal malignancy.\nIn Round 2, using facts [94, 11, 68, 45, 98, 83, 72], the generated questions included one about a 45-year-old male with recurrent epigastric pain improving with eating. This question led to a diagnosis of duodenal ulcer, showcasing the integration of fact 68 (large bowel obstruction etiology) in crafting a question"}, {"title": "Analysis of Fact Overlap and Significance of Findings", "content": "In this study, the LCG was employed to ensure the generation of unique and diverse MCQs by selecting distinct facts across multiple rounds. To evaluate the effectiveness of this method, an analysis was conducted to identify any overlap of facts between different rounds of MCQ generation. The LCG algorithm was initialized with specific parameters (seed: 12345, multiplier: 1103515245, increment: 12345, modulus: $2^{31}$) to generate a sequence of pseudo-random numbers. These numbers were then mapped to fact indices, ensuring that the same fact was not used more than once across the rounds.\nThe generated MCQs and the corresponding used facts for each round were recorded. For example, Round 1 used facts [7, 76, 25, 74, 79, 60, 93], while Round 2 used facts [94, 11, 68, 45, 98, 83, 72]. Subsequent rounds followed a similar pattern of unique fact selection. An in-depth comparison of the used facts between rounds was performed to detect any overlap. The analysis revealed no overlap of facts across the rounds, demonstrating the effectiveness of the LCG method in maintaining the uniqueness of the selected facts."}, {"title": "Discussion", "content": ""}, {"title": "Interpretation of Results", "content": "The results of our study demonstrate the effectiveness of the LCG method in producing high-quality, diverse MCQs for medical education. The LCG algorithm consistently generated unique combinations of facts across multiple rounds, resulting in a wide range of clinically relevant questions. This approach proved superior to traditional methods of MCQ creation, which often rely on manual selection of topics and can inadvertently lead to repetition or bias in question content. The systematic nature of the LCG method ensured comprehensive coverage of the gastrointestinal physiology and pathology domain, providing a balanced representation of topics in the generated questions [28].\nThe quality of the MCQs produced using this method was consistently high, with questions demonstrating clinical relevance, appropriate difficulty levels, and the ability to test higher-order thinking skills. The integration of selected facts into vignette-style questions allowed for the creation of complex, realistic clinical scenarios that effectively challenge learners' application of knowledge [34]. This approach aligns well with current trends in medical education that emphasize the importance of context-rich, application-focused assessment [19].\nFurthermore, the use of LCG in MCQ generation addresses the challenge of"}, {"title": "Limitations", "content": "Despite these promising results, it is important to acknowledge the limitations of our study. The scope was confined to gastrointestinal physiology and pathology, and while this allowed for a focused analysis, it may limit the generalizability of our findings to other medical specialties. Additionally, our reliance on the GPT-40 model for question generation, while producing high-quality results, ties the effectiveness of our method to the capabilities of this specific AI model. Future studies should explore the applicability of this method across different medical specialties and with various AI models to establish its broader utility in medical education [29]. Furthermore, exploring the integration of other advanced AI models could provide insights into the versatility and scalability of this approach [30]. Future research should also consider a broader set of evaluation metrics, including learner feedback and longitudinal studies to assess the long-term impact on educational outcomes [31]."}, {"title": "Future Work", "content": "There are several avenues for future work that could build upon and extend the findings of this study. Exploring other randomization methods, such as the Mersenne Twister algorithm or cryptographic random number generators, could provide valuable comparisons and potentially uncover even more effective approaches to MCQ generation [32]. Additionally, incorporating machine learning techniques to analyze and refine the generated questions based on learner performance data could further enhance the quality and educational value of the MCQs [33].\nFuture research could also focus on improving the MCQ generation process itself. This might include developing more sophisticated prompts that incorporate additional context or learning objectives, or exploring ways to automatically validate the clinical accuracy of generated questions [34]. Furthermore, investigating the potential for this method to generate other types of assessment items, such as extended matching questions or key feature problems, could broaden its applicability in medical education [35].\nIn conclusion, our study demonstrates the potential of combining algorithmic fact selection with AI-powered question generation to produce high-quality, diverse MCQs for medical education. While further research is needed to fully explore its capabilities and limitations, this approach represents a promising direction for enhancing the efficiency and effectiveness of assessment creation in"}, {"title": "Conclusion", "content": "This study has demonstrated the efficacy of the LCG method in producing high-quality, diverse MCQs for medical education, specifically in the field of gastrointestinal physiology and pathology. Our findings reveal that the LCG algorithm, when coupled with AI-powered question generation, consistently produces unique, clinically relevant, and cognitively challenging MCQs across multiple rounds of generation.\nThe significance of using LCG for MCQ generation in medical education cannot be overstated. This method addresses several key challenges in assessment creation, including the need for diversity in question content, the avoidance of unintentional repetition, and the efficient coverage of a broad range of topics. By systematically selecting unique facts for each round of question generation, the LCG method ensures a comprehensive representation of the subject matter, thereby enhancing the educational value of the assessment materials.\nThe potential impact of this methodology extends beyond the immediate benefits of improved MCQ generation. This approach represents a step towards more standardized, efficient, and scalable methods of creating high-quality assessment materials in medical education. It has the potential to significantly reduce the time and resources required for question creation while maintaining or even improving the quality of assessments. Furthermore, this method can be adapted to various medical specialties and potentially to other fields of education, offering a versatile tool for educators and assessment developers.\nAs medical knowledge continues to expand rapidly, the need for efficient, accurate, and diverse assessment methods becomes increasingly crucial. The LCG-based MCQ generation methodology presented in this study offers a promising solution to meet this need. By leveraging algorithmic fact selection and AI-powered question creation, we can ensure that future medical professionals are assessed using questions that are not only diverse and comprehensive but also align closely with real-world clinical scenarios.\nIn conclusion, while further research and refinement are necessary, the findings of this study suggest that the integration of LCG-based fact selection with AI-powered question generation has the potential to revolutionize the creation of assessment materials in medical education. This approach paves the way for more efficient, effective, and equitable assessment practices, ultimately contributing to the improvement of medical education and, by extension, healthcare delivery."}]}