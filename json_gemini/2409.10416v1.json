{"title": "Geometric Clustering for Hardware-Efficient Implementation of Chromatic Dispersion Compensation", "authors": ["Geraldo Gomes", "Pedro Freire", "Jaroslaw E. Prilepsky", "Sergei K. Turitsyn"], "abstract": "Power efficiency remains a significant challenge in modern optical fiber communication systems, driving efforts to reduce the computational complexity of digital signal processing, particularly in chromatic dispersion compensation (CDC) algorithms. While various strategies for complexity reduction have been proposed, many lack the necessary hardware implementation to validate their benefits. This paper provides a theoretical analysis of the tap overlapping effect in CDC filters for coherent receivers, introduces a novel Time-Domain Clustered Equalizer (TDCE) technique based on this concept, and presents a Field-Programmable Gate Array (FPGA) implementation for validation. We developed an innovative parallelization method for TDCE, implementing it in hardware for fiber lengths up to 640 km. A fair comparison with the state-of-the-art frequency domain equalizer (FDE) under identical conditions is also conducted. Our findings highlight that implementation strategies, including parallelization and memory management, are as crucial as computational complexity in determining hardware complexity and energy efficiency. The proposed TDCE hardware implementation achieves up to 70.7% energy savings and 71.4% multiplier usage savings compared to FDE, despite its higher computational complexity.", "sections": [{"title": "I. INTRODUCTION", "content": "ENERGY consumption has long been a critical focus in optical transmission systems [1]\u2013[4]. In pluggable optical coherent transceivers, digital signal processing (DSP) typically accounts for about 50% of total power usage [5], [6], with the CDC being one of the primary power-consuming components [7]. Recent proposals, such as chirp filtering [8] and uniform quantization of filter taps [9], [10], have shown significant computational complexity reductions but lack validation through hardware implementation to confirm benefits in energy efficiency and chip area. Some designs, like the finite field approach [11], have been implemented in hardware. However, these efforts were focused on short-reach fiber systems (240 km) and relied on scaling factors, assuming a linear growth in the impact of different parameters like throughput and technology process size, for comparison to the state-of-the-art.\nThis paper provides a theoretical analysis of the tap redundancy in CDC filters for coherent receivers. It presents different approaches to decrease the complexity of time domain equalization based on this phenomenon with the associated trade-offs. We introduce a novel approach called the Time-Domain Clustered Equalizer (TDCE), which leverages the tap redundancy phenomenon to reduce the complexity of the time-domain equalizer. Moreover, the proposed technique is suitable for optimization using machine learning, allowing us to gain further reduction of the complexity. Our equalizer is implemented in a Field-Programmable Gate Array (FPGA) for different fiber lengths up to 640 km. These implementations are compared, under the same conditions, against the frequency domain equalizer (FDE) [12] implementation using Fast Fourier Transform (FFT) algorithm [13], as the state-of-the-art because the latter has been widely studied [14], [15] and implemented in Application-Specific Integrated Circuit (ASIC) for CDC [16], [17]. We evaluate chip area usage, hardware resources, and energy efficiency using nJ/recovered-bit for both solutions.\nImportantly, we show that the implementation strategy plays a vital role (comparable with the computational complexity) in the analysis of the efficiency of the physical resources utilization. In particular, we emphasize that two factors are critically important: (i) the innovative parallelization of TDCE and (ii) the memory implementation. It is demonstrated that a lower multiplier count is possible for a higher complexity algorithm due to the hardware implementation strategy utilized. Our analysis illustrates that the computational complexity metric alone is insufficient to determine the hardware characteristics of the equalizer. It is necessary to examine other crucial points, such as memory implementation and the degree of parallelization the algorithm allows. Furthermore, in agreement with Ref. [18], we show that theoretical multiplication complexity is not a good predictor for the energy efficiency of CDC filters.\nIn summary, our work provides a hardware implementation of the TDCE design, demonstrating its effectiveness in reducing power consumption and multiplier usage across various fiber lengths compared to FDE. Despite higher theoretical computational complexity, our results highlight the significant impact of hardware implementation strategies on energy consumption and resource efficiency."}, {"title": "II. THEORETICAL ANALYSIS", "content": "There have been several works dealing with the CDC that employed clustering of time-domain filter taps in IM/DD pre-compensation scenarios, arguing the tap redundancy [19]\u2013[21], though without a theoretic explanation. Our paper deals with a different problem \u2013 the CDC for coherent detection systems, which requires a complex filter that has different types of redundancy. This section will explain the redundancy effect.\nTo begin the explanation, we reiterate the well-known observation that each filter tap represents a point on a circle in the complex plane [22]. It is because each filter tap is given by [23]:\n$g [m] = V \\frac{j c T^2}{D \\lambda^2 z}  exp \\left[-\\frac{\\pi c T^2}{D \\lambda^2 z} -j \\frac{\\pi m^2}{2} \\right]$,\n in which, $z$ is the fiber length, $m$ is the filter tap index, $j$ is the imaginary unit, $D$ is the dispersion coefficient, $c$ is the speed of light, $\\lambda$ is the wavelength of the carrier, and $T$ is the sampling period. The boundaries for $m$ are calculated as follows [23]:\n$-\\frac{N}{2} \\leq m \\leq \\frac{N}{2}, N = 2 \\sqrt{\\frac{D \\lambda^2 z}{2 c T^2}} + 1$,\n in which, $N$ is the maximum number of filter taps, taking into consideration the Nyquist frequency to avoid aliasing, and $[x]$ means the nearest integer less than $x$. As $m$ can assume negative values according to Eq. (2), we prefer to use in this paper the index $k$ given by:\n$k=m+\\frac{N}{2}, k = 0, ..., N \u2013 1$,\nIn Fig. 1, the filter taps calculated by Eq. (1) are plotted in the complex plane (left) and it is possible to observe the overlapping of filter taps, while in (right) a heat-map with brighter spots in areas of high accumulation of taps illustrate the formation of clusters.\nThis cluster formation can be explained by the fact that the filter tap definition given by Eq. (1) represents a rotating vector with constant amplitude and varying phase as tap index $k$ varies. Therefore, a multiplication by a filter tap can be seen as simply a phase shift of the input sample with a constant scaling factor at the end. As the absolute phase increases for different values of $k$, many phases will be repeated (because the even symmetry due to the $m^2$ term) or have values very close to each other on the complex plane circle because the values greater than $2 \\pi$ just represent multiple rounds on the complex circle.\nIn this paper, we show how the analysis of the clusters of phase shifts makes it possible to reduce the complexity of the time domain equalizer through the utilization of factorization of the finite impulse response (FIR) filtering process, executing first the sums of the samples associated with filter taps that are grouped and then multiplying the sum result by the filter tap that represents the group, which we call the clustered filter tap.\nTo investigate further this overlapping, in Fig. 2 we plot circular histograms to statistically analyze the angle distribution for equalizing filters in two different scenarios. The filter size utilized $M$ was 60% of the maximum number of taps $N$ in Eq. (2), which gives a considerable reduction in the filter size with minimal penalties [23].\nFig. 2 reveals two distinct patterns: for short-range dis-tances, we see a cluster formation (also illustrated in Fig 1), and for long-haul distances, the angle distribution is more spread but with some more occupied clusters (e.g., near 45\u00b0 and 270\u00b0).\nTo assess how concentrated in clusters or evenly spread the filter taps are for different fiber lengths, let us introduce a simple metric $\\rho$,\n$\\rho = \\frac{N_M - N_L}{\\mu}$,\nwhich is the difference between the number of taps in the most ($N_M$) and lowest ($N_L$) populated bins divided by the mean quantity of taps per bin $\\mu$, which is the total number of taps $M$ divided by the total number of bins. In an ideal uniform distribution, all the bins should be populated equally, therefore $N_M = N_L$ and $\\rho = 0$. However, if the difference $N_M - N_L$ is bigger in comparison to $\\mu$, it means that at least one bin or more contains a higher concentration of taps than the others.\nLooking at Fig. 3, one can conclude that for shorter dis-tances, the distribution has one or more points of concentration"}, {"title": "B. Complexity reduction by cluster analysis", "content": "Looking at the time domain convolution equation,\n$y[n] = \\sum_{k=0}^{M-1} x[k]g[n \u2013 k]$,\nwhere M is the filter size, we observe that the filter taps g[n \u2013 k] are directly multiplied by the input samples x[k] to produce the output samples y[n]. Thus, if some filter taps are grouped in clusters, each cluster can be represented by a single tap. This approximation can help decrease the complexity by using the distributive property of multiplication: we can first sum all the input samples associated with the grouped filter taps and then multiply this summed result by the tap representing the cluster. A similar use of this distributive property was employed to utilize the even symmetry of the transfer function [9], [24] and uniform quantization [9], [10], although none of those works harnessed the geometrical interpretation exploited here.\nIn this subsection, we explain and calculate the com-putational complexity, in terms of real multiplications per recovered symbol, of the resulting clustered filter obtained by considering the accumulation of angles in the complex plane, which we call complex-value (CV) clustering.\nThe FIR filtering by using Eq. (5) can simplified by CV clustering and expressed as a simplified convolution:\n$y[n] = \\sum_{k=0}^{N_c-1}x_s[k]. g_c[k]$.\nin which n is the output sample index, $g_c$ represents the clustered filter taps to be multiplied by $x_s$, which represents the summation of input samples that for each n are associated with the same cluster of filter taps, and $N_c$ is the total number of complex value clusters.\nThe index of the input samples that must be summed before the multiplication by a specific clustered filter tap can be defined by a set $Q_j$ in which $j$ is the cluster's index varying from 0 to ($N_c \u2013 1$). Therefore, the summation of associated input samples can be represented by:\n$x_s[j] = \\sum_{k \\in Q_j} x[k]$.\nThese sets $Q_j$ play the same role as the routing block described in Ref. [9].\nDefining the complexity (C) as the number of real multi-plications per recovered sample and noting that each complex multiplication is made up of 4 real multiplications, this strategy leads to the expression for the complex clustering complexity:\n$C_{cv} = 4 \\cdot N_c$.\nAs we aim to demonstrate that, in hardware design, memory usage plays a significant role in both area usage and power consumption, we calculate here the minimum quantity of different memory positions needed to obtain the previously summed values $x_s$ (that we will also refer as pre-summed values). To calculate this, we observe that for each multiplica-tion by $g_c$ in Eq. (6), the real and imaginary values of $x_s[k]$ must be calculated previously and saved in memory, which means that $2 \\cdot N_c$ memory positions are necessary to store $x_s$."}, {"title": "III. METHODOLOGY", "content": "In this work, we compare two variations of our approach with the state-of-the-art FDE. To find the clusters of filter taps in the complex plane, both TDCE variations use the unsupervised KNN algorithm [25], which has been successful in clustering and reducing equalization complexity in other scenarios [19]\u2013[21]. One variation, TDCE GD, employs su-pervised learning with the Gradient Descent algorithm (GD) to further optimize the clusters to decrease the approximation errors caused by the clustering. The second variation, TDCE KNN, does not use supervised learning. The methodology for both approaches will be explained individually, considering that TDCE KNN methodology also applies to TDCE GD, with the latter including additional fine-tuning."}, {"title": "A. TDCE KNN. Methodology", "content": "The clustering process is depicted on the left side of Fig. 5, showing the software implementation part. This involves using prior knowledge of variables T, D, \u03bb, and z to calculate the truncated filter response g[k] via Eq. (1). Subsequently, the KNN algorithm identifies the mapping sets Qj and centroids for the clustered filter taps gc[k]. The right side of Fig. 5 illustrates the hardware implementation of clustered filtering. This involves routing the input samples to the correct group based on the sample mappings Qj, performing the summation of the samples in the same group to obtain xs[k], and at the end, multiplying the pre-summed values xs[k] with clustered filter taps gc[k], and summing the outputs to get the recovered the symbol.\nOur objective is to reach a BER < 3.8 \u00d7 10-3 in the final TDCE and FDE filters, corresponding to the error-free pre-FEC threshold with 7% overhead [26]. However, overall performance is compromised due to the inherent approxima-tions in the clustering process. Therefore, we conducted an evaluation of the truncated filter response, Eq. (1), across various filter sizes, ranging from N given by Eq. (2), to the minimum size that guarantees a BER below 1 \u00d7 10-3, as depicted in Fig. 4(a). Although this performance is better than our goal, the clustering process will decrease it. Following this, we evaluated the impact of different cluster quantities for each TDCE variation to determine the minimum number of clusters, ensuring a BER < 3.8 \u00d7 10\u22123 as shown in Fig. 4(b) for TDCE KNN. A similar approach, explained in detail in the sub-section III-C, is used for a fair comparison with the minimum complexity FDE.\nTo assess the filter performances, we utilized data obtained from numerical modeling of a single-channel, 32 GBaud, 16-QAM dual-polarization transmission at the optimal launch power across standard single-mode fiber (SSMF). The propa-gation dynamic was simulated employing the Manakov equa-tion and the split-step Fourier method [27]. The SSMF param-eters used were: dispersion D = 16.8 ps/(nm\u00b7km), nonlinearity coefficient \u03b3 = 1.2 (W\u00b7km)-1, and attenuation \u03b1 = 0.21 dB/km. Additionally, erbium-doped fiber amplifiers (with a noise figure of 4.5 dB) were included after each fiber span of 80 km. The number of spans ranged from 1 to 8, and all filters operated at 2 samples per symbol."}, {"title": "B. TDCE GD (fine-tuning). Methodology", "content": "The taps clusterization process presented utilizes KNN, which is an unsupervised learning method that introduces an approximation error, creating a trade-off between complexity and performance (Fig. 4b). This section presents the applica-tion of supervised learning through the GD algorithm to de-crease this approximation error and achieve lower complexity.\nTo optimize the process, we utilized the clustering scheme shown in Fig. 5(left), identical to the one used for TDCE KNN, to produce clustered filter taps (centroids). We subsequently developed a convolutional neural network (CNN) in software, initializing it with these centroids. The CNN was trained using pre-summed values xs[n], calculated by Eq. (7) from samples that simulated propagation through SSMF, as features (input data). The labels (desired predictions) were derived from the samples prior to optical fiber propagation simulation.\nThe total experimental dataset used consisted of 218 symbols for the training dataset and 218 independently generated symbols for the testing phase, each with different random seeds. Each training epoch utilized 216 randomly selected samples from the training dataset, ADAM optimizer with a learning rate of 10-5, 400 training epochs. A training \u201cstop condition\" of 50 epochs with no BER improvement was also implemented. The testing phase utilized the first 216 samples from the testing dataset, as this quantity was enough for the model to continuously learn until the early stop condition was reached.\nAn interesting finding of this strategy is that the centroids optimized by supervised learning became outside of the circle as an effect of correcting the approximation error introduced by the clustering process, as shown in Fig. 4(f). As a result, this fine-tuning reduced the number of clusters needed to reach the pre-FEC threshold considered in this paper, leading to a lower complexity filter with complexity savings of 33.3%, 20% and 20% for 1, 2, and 4 spans, respectively, in comparison to TDCE KNN and, 30.3%, 14.3%, 22.4% in comparison to FDE, as illustrated in Fig. 4(g)."}, {"title": "C. FDE. Methodology", "content": "We aim to benchmark our solution against the simplest FDE available to ensure a fair comparison. We assessed the performance versus filter size as shown in Fig. 4(a) for the time-domain FIR filtering, identifying the minimum filter size necessary to satisfy the condition BER < 3.8 \u00d7 10-3. Then, we determined the FFT size (NFFT) that minimizes the complexity of the FDE, CFFT:\n$C_{FFT} = N_{FFT} \\frac{8 \\beta log_2 (N_{FFT})+4}{N_{FFT}-M+1}$,\nwhere $N_{FFT}$ represents the FFT size (block size), M denotes the filter size, and $\\beta$ is a constant equal to 1/2 when using the Radix-2 architecture with FFT sizes that are powers of 2, or $\\beta$ = 3/8 when using the Radix-4 architecture with FFT sizes that are powers of 4. We utilized the overlap-save method [28] to segment the input signal into blocks before applying the FDE. Therefore, the denominator of Eq. (8) represents the total number of samples not corrupted by the overlap-save method. This optimization process of finding the optimization process is illustrated in the plot of Fig. 4(c).\nBy using the methodology described in this section and in the previous ones, the filter parameters in our approach were defined as described in Table I. In that Table N is the maximum filter size as defined by Eq. (2), $M_{TDCE}$ is the filter size to achieve BER < 1 \u00d7 10-3 before clustering for both TDCE variations, $M_{FDE}$ is the filter size for FDE, $N_{c(KNN)}$ and $N_{c(GD)}$ are the number of complex clusters for the TDCE KNN and TDCE GD respectively and NFFT is the FFT size used for FDE.\nAlthough the pre-FEC threshold used to define the filter size for FDE was higher than the one for TDCE, it required slightly more taps ($M_{FDE}$) than the TDCE ($M_{TDCE}$) in some cases, as shown in Table I. As the difference in the number of filter taps was 6.3% in the worst case (8 spans), we considered it negligible.\nHaving established the optimal number of clusters and FFT sizes, we proceed to calculate the complexity; the respective dependencies are given in Fig. 4(g) for the FDE and two TDCE variants. Notably, employing a time-domain approach allows us to attain a complexity level comparable to that of the FDE as measured in real multiplications per recovered symbol."}, {"title": "IV. FPGA IMPLEMENTATION", "content": "As the index of the filter taps overlapped at a specific angle is not consecutive, it is crucial to route the input samples cor-rectly to the appropriate pre-summed array memory xs[k] for the accumulation expressed in Eq. (7). This routing presents a significant challenge similar to the routing challenge men-tioned in Ref. [9] as one of the further optimizations required in that approach. Further on, we explain the implementation of this routing block in detail.\nThe sample routing of our implementation is inspired by the ideas presented in Ref. [29]. Unlike the multiple Qj sets described in Eq. (7), we use a single set, Q, as shown in"}, {"title": "A. Simplified Convolution", "content": "Fig. 6. This set Q contains pointers for each filter tap position, indicating where each input sample should be accumulated in xs. This method enables sequential input sample reading and simultaneous routing using Q.\nFig. 6(left) illustrates a hypothetical output calculation for y[3], treating convolution as the dot-product of a reversed sliding filter with M taps over input samples. As detailed earlier and shown in Fig. 6, the KNN-based clustering process generates centroids for clustered filter taps (Nc clusters) and sample mapping (Q). This mapping, implemented as sequential memory, directs each input sample to a pre-summed set xs. This routing allows a simplified dot-product between xs and gc, as depicted in Fig. 6(right), leveraging the distributive property of multiplication."}, {"title": "B. Parallelization", "content": "The parallelization of TDCE is a novel aspect of our research. To explain this process, we expand on the simplified convolution shown in Fig. 6. As the simplified dot-product is simple and has a straightforward implementation, we focus on the summation process that involves issues like routing and parallel memory access. The pseudo-code in Algorithm 1 illustrates the steps to obtain xs, similar to the method in Ref. [29].\nA possible strategy to boost computation speed is to fully unroll the loop in Algorithm 1 and execute all iterations in parallel. This requires reading M positions of Q and accessing M input sample positions simultaneously for accurate accu-mulation into xs[k]. While this increases computational speed, it significantly increases hardware resource demands due to parallel random memory access and numerous sum operations. To efficiently use FPGA resources, we compute L sequential output samples simultaneously rather than parallelizing each output sample. This optimization leverages the interpretation of time-domain convolution as a series of dot-products, de-tailed in Fig. 8.\nFig. 8 illustrates convolution as a series of dot-products between sliding reversed filter taps and input samples, high-lighting that the same filter tap multiplies consecutive input samples for successive outputs. Each filter tap in the original filter belongs to a cluster in the clustered filter, and each input sample must be routed to its corresponding clustered filter tap, as shown in Fig. 6. Since L consecutive input samples are multiplied by the same filter tap across L consecutive outputs, all L input samples must be mapped to the same cluster. To utilize this property, we developed Algorithm 2.\nAlgorithm 2 leverages the property from Fig. 8, where the cluster index is read from Q once and used for all the innermost loop iterations. This property enables sequential reading of the input sample array x. Since each output sample requires its own xs set, xs is now a bidimensional array"}, {"title": "C. TDCE Hardware Implementation Architecture", "content": "The hardware implementation of the TDCE is summarized in Fig. 7. To achieve high throughput, the architecture lever-ages the fact that the structure producing the pre-summed array xs[k] is idle during the simplified dot-product stage, as shown in Fig. 6(right). This allows this structure to start processing the next samples while the simplified convolution of the previous ones is still ongoing. The Dataflow Controller block manages this process by controlling the Control Unit blocks for summation and simplified convolution.\nIn the summation structure, the sample mapping (Q) is stored in a register and read sequentially for each filter tap (M). Control Unit 1, enabled by the Dataflow Controller, enables the parallel reading of L samples and one mapping simultaneously, M times. This approach minimizes random access, as explained in Section IV-B. By pipelining this structure, we can organize the loop tasks reading input, reading cluster index, reading memory banks, accumulation, and updating memory banks to start a new iteration loop every clock cycle.\nControlled by the Dataflow Controller, the Memory Transfer block performs a bulk transfer of data from the summation memory banks (left) to secondary memory banks (right). This frees up the summation memory banks to process the next L samples. Consequently, the structure can process new input samples before the simplified dot-product is finished.\nAfter the Summation process is completed, the Simplified Dot-Product is executed. The number of multiplications in the Simplified Dot-Product is significantly less than the number of additions in the Summation, allowing multiplications to be performed at a lower rate, thus saving multipliers. For L input samples, we define the number of parallel samples Lp to be multiplied such that L is a multiple of Lp. This strategy ensures that the Simplified Dot-Product takes a similar amount of time as the summation process, enabling both operations to overlap. Control Unit 2 enables the reading of blocks of size Lp to be multiplied by the same clustered filter tap. The resulting Lp samples are accumulated in an output memory, which, at the end of the process, contains the L output equalized samples. This multiply-and-accumulate (MAC) process is done for each cluster, resulting in Nc iterations. Each iteration consists of L/Lp internal iterations to perform MAC operations for all L samples. Due to the overlapped execution, the system's throughput (TH) is given by:\n$TH = \\frac{L}{\\alpha M}$,\nwhere \u03b1 is an empirical constant defined by the dataflow overlapping, and M is the filter size."}, {"title": "D. FDE Hardware Implementation Architecture", "content": "The FDE is used for comparison in this work because it is the common state-of-the-art reference in previous studies [8], [9], [11]. It offers the lowest complexity (calculated by Eq. (8)) compared to time-domain FIR filters [24], a higher degree of parallelization and stability compared to IIR filters [24], and is preferred for ASIC implementations [16], [17]. The FDE is based on the FFT algorithm, which includes a forward FFT to transform the input signal block, multiplication in the frequency domain (FD) between the FD representations of the input signal and transfer function, followed by an IFFT operation, and discarding of corrupted samples by the circular convolution (Fig. 9 right) [30]. To implement FDE, a method for block division and subsequent filtering of the signal is required, with Overlap-Add (OA) and Overlap-Save (OS) being well-known options [28]. In this work, we use OS for our FDE implementation. However, hardware resources for block division were not considered, as we aim to compare only the mathematical operations of the filters.\nTo compare our solution with the state-of-the-art FDE, we used the FFT HLS Library [31] developed and optimized by AMD\u00ae for the FFT/IFFT blocks. This ensures an unbiased and fair comparison since it was developed independently of our research. This FFT module offers different architecture op-tions, allowing users to choose the suitable trade-off between hardware resources and throughput (Fig. 9 left). In this work, we used the Radix 4 Burst/IO architecture for medium output throughput, as the high throughput realization of TDCE will be addressed in future studies.\nThe Radix 4 Burst/IO architecture is not pipelined and has a latency before outputting data for each transformation cycle. To mitigate this and achieve higher throughput for FDE, we applied a dataflow HLS directive to implement the structure shown in Fig. 9(right). This allows the FFT block's data output to overlap with the FD multiplication and the IFFT's subsequent latency. Using dataflow, the throughput for FDE can be estimated by:\n$TH = \\frac{N_{FFT} - M + 1}{\\delta A_{FFT}}$,\nin which $A_{FFT}$ is the latency for the FFT size $N_{FFT}$ used and \u03b4 is an empirical constant determined by the dataflow structure. As the IP block from AMD\u00ae does not allow a controllable parallelization, for simplicity, the throughput of TDCE was adjusted accordingly to match the same FDE throughput by adjusting the L parameter in Eq. (9)."}, {"title": "E. Implementation Parameters and Methodology", "content": "In all subsequent analyses, we implemented both filters on a VCK190 FPGA by simulations in the AMD\u00ae Vitis IDE at a clock frequency of 250 MHz. Both filters applied identical quantization to the taps and signals to ensure a fair comparison. A total of 16 bits, with 5 for integer bits, were used for the TDCE, and a total of 16 bits, with 1 integer bit, were used for the FDE to meet the threshold. All the filters were implemented aiming a BER< 3.8 \u00d7 10-3, and the achieved BER values for the parameters used are described in Table II.\nAdditionally, all the TDCE implementations were designed to achieve the same throughput as FDE with a maximum error difference of less than 7%, balancing the trade-off between digital filter throughput and hardware resources. The parameters of the parallelization of TDCE approaches and the achieved throughput are summarized in Table III.\nThe equalizers were implemented using C++ in Vitis HLS 2023.2, which translates the code to HDL and generates RTL. The RTL was then implemented in Vivado 2023.2 using the default routing strategy. Energy consumption was assessed with the Power Estimator tool in Vivado. Power consumption in digital circuits comprises static and dynamic power. We focused on the dynamic power of the filters, excluding static power, which is associated with all inner circuits regardless of their usage in FPGA implementations. Additionally, dynamic power is the dominant factor in power consumption for ASIC implementations [17]."}, {"title": "F. Memory Impact", "content": "Memory requirements and their hardware implementation are often not discussed in the evaluation of proposed algo-rithms for CDC [8]\u2013[10]. Here, we demonstrate that this is an important issue to be considered that can impact the hardware implementation results noticeably.\nTo analyze the impact of memory implementation, TDCE KNN and FDE were implemented on FPGA, following the steps in Section III for a fiber length of 4 spans (320 km). Results are shown in Fig. 11. The implementation used two types of memory for the bank memories of xs[k] (Fig. 7) in TDCE and inside the FFT/IFFT blocks of FDE (Fig. 9 right). Table I shows that for TDCE, L = 20 was used to achieve 81.3Mb/s, while for FDE, NFFT = 1024 was used to achieve 79.5Mb/s.\nFig. 11(a,b) illustrates the impact of using BRAMs for TDCE memory implementation. With L = 20 parallel samples processed, the total number of BRAMs is 40, as L memory banks are used before and after the Memory Transfer block (Fig. 7). This configuration reduces the quantities of LUTs and FFs compared to using LUTRAM, while the number of DSPs remains unaffected. However, dynamic power is higher when using BRAMs."}, {"title": "G. Results for Different Distances", "content": "From Fig. 10(a), we observe that the chip areas required by the GD and KNN approaches are quite similar due to their use of similar amounts of resources. Additionally, for low dispersion scenarios like 1 fiber span, the area required for the TDCE designs is smaller than that required for the FDE. However, for higher dispersion scenarios, like 8 spans, the area required is similar for all designs.\nFig. 10(b) shows that, due to the memory implementation choices, both TDCE filters do not use BRAMS, in contrast to the FDE, which uses 8. However, the LUT quantities used by TDCE approaches are higher than those for the FDE, as explained in subsection IV-F. In addition, despite having the highest complexity in this work according to Fig. 4(f), TDCE KNN used about one-third of the multipliers required by FDE and the same amount of multipliers as TDCE GD. This is because the multiplications for TDCE are done after the summation with overlapping between tasks (Fig. 7).\nTo illustrate how our parallelization can save multipliers, consider the TDCE KNN for 4 spans with M = 97, L = 20, and Nc = 10 (Table I). The summation process takes about M clock cycles, independent of L (subsection IV-B), plus around Nc clock cycles to transfer data to secondary memory banks, totaling approximately M + Nc = 107 clock cycles, which can be rounded to 110 due to extra dataflow cycles. Each output sample requires Nc = 10 multiplications, totaling 200 multiplications that can be completed in the 110 clock cycles due to task overlapping. Thus, the multiplications can be performed with a smaller degree of parallelism (Lp < L), allowing for two complex multiplications per clock cycle (Lp = 2). Since each complex multiplication has two real multiplications, this implementation uses only 4 multipliers compared to the 14 used by FDE.\nDespite having lower complexity, the GD implementation required the same number of multipliers as the KNN due to needing Nc = 8 clusters. This requires fewer parallel samples, L = 18, totaling L\u00b7Nc = 144 complex multiplications. These cannot be performed sequentially in approximately M+Nc = 105 clock cycles (TDCE GD), necessitating the same degree of parallelism as the KNN, with 2 complex multiplications per clock.\nRegarding energy efficiency, Fig. 10(c) shows that TDCE approaches have similar power consumption. This similarity is due to the similar quantities of required hardware re-sources and the same algorithm implementation. Both TDCE approaches consume less power than FDE, with significant en-ergy savings in DSP and BRAM usage. This can be attributed to the fewer multipliers required by TDCE designs and the use of LUTs and FFs instead of BRAMs. Additionally, energy savings in Signals are notable, thanks to the simpler TDCE algorithm, which avoids the interconnected structures of the FFT algorithm that uses deeply interconnected \"butterflies\" for operations [13]. These power-distributed savings result in superior overall energy efficiency for TDCE approaches compared to FDE, as shown in Fig. 10(d), with an efficiency gain of up to 70.4%, even for the higher complexity TDCE KNN. This demonstrates that hardware implementation is as crucial as mathematical complexity in determining energy efficiency. Both TDCE designs exhibit very similar energy efficiency despite the GD approach being less complex. This is because the reduced number of required multiplications did not significantly reduce the number of multipliers due to the dataflow structure."}, {"title": "V. CONCLUSION", "content": "We presented a geometrical interpretation of the CDC trans-fer function and explained the effect of filter taps overlapping. This overlapping generates clusters of filter taps, which we harnessed to decrease the theoretical complexity of time-domain FIR filtering. Our approach was further optimized using machine learning to reduce computational complexity. We implemented the proposed solution and state-of-the-art FDE on FPGA for different fiber lengths (from 80 km to 640 km SSMF) and examined energy efficiency using the nJ/bit metric. We quantified the impact of memory implemen-tation on resource quantity, energy efficiency, chip area usage, and routability in FPGA realization. The developed TDCE technique demonstrated up to 71.4% savings in the number of multipliers, even for higher computational complexity cases in comparison to FDE. Furthermore, we showed that TDCE can be flexibly parallelized by leveraging the property that in linear convolution, the same filter tap multiplies consecutive input samples across consecutive output sample calculations.\nIn conclusion, for the considered fiber lengths, the proposed TDCE achieved up to 70.7% energy savings compared to"}]}