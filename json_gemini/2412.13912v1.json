{"title": "Energy-Efficient SLAM via Joint Design of Sensing, Communication, and Exploration Speed", "authors": ["Zidong Han", "Ruibo Jin", "Xiaoyang Li", "Bingpeng Zhou", "Qinyu Zhang", "Yi Gong"], "abstract": "To support future spatial machine intelligence applications, lifelong simultaneous localization and mapping (SLAM) has drawn significant attentions. SLAM is usually realized based on various types of mobile robots performing simultaneous and continuous sensing and communication. This paper focuses on analyzing the energy efficiency of robot operation for lifelong SLAM by jointly considering sensing, communication and mechanical factors. The system model is built based on a robot equipped with a 2D light detection and ranging (LiDAR) and an odometry. The cloud point raw data as well as the odometry data are wirelessly transmitted to data center where real-time map reconstruction is realized based on an unsupervised deep learning based method. The sensing duration, transmit power, transmit duration and exploration speed are jointly optimized to minimize the energy consumption. Simulations and experiments demonstrate the performance of our proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapidly growing need of spatial machine intelligence, generalized integrated sensing and communication (ISAC) technologies have attracted tremendous academic and industrial attentions [1]. As a fundamental technology, simultaneous localization and mapping (SLAM) can support a series of spatial intelligence applications, such as auto-driving and unmanned factory, which highly depend on various types of mobile robotic agents [2]. Most of the robots are expected to perceive the environment, estimate their system states, interact with edge server and/or other agents via wireless communications, and make decisions autonomously.\nHowever, real-world environments are usually non-static due to the dynamic changes caused by both ephemeral and persistent objects, which leads to inaccuracy of localization and mapping [3]. To address this problem, lifelong SLAM has been proposed to continuously build and maintain the map [4]-\n[6]. Meanwhile, the sensed data and state information should be delivered on time over time-varying communication links [7]. The dynamic properties of both physical environment and communication condition affect the robustness as well as the performance of decision-making and control.\nFor lifelong SLAM, the long-term energy efficiency and endurance of these robots should be concerned, since they are usually powered by batteries. For the case that the computation in SLAM is offloaded to edge server, the robot generally consists of perception sensors, odometry, electric motors, micro control unit (MCU) module, and wireless communication modules. Traditionally, the energy consumptions for robot sensing, movement and wireless data transmission are analyzed independently in its own field, but in fact, they are coupled. The integrated design of sensing, communication, decision and control has drawn great attention in communications society very recently [8]. For example, if the sensed data is wirelessly transmitted to edge server for real-time mapping after each sensing period, then for a specific task area, changing movement speed will change the places that communications happen and thus influence the energy consumption for both movement and communication. Meanwhile, the corresponding change of total data amount will also affect the mapping performance. Moreover, regarding to the scalability problem in different scenarios, e.g., different map structures or sizes, the proportions of sensing, movement and communication energy consumption are quite different. How to analyze and optimize the energy efficiency of the robot system operation for completing SLAM tasks is an open issue.\nIn this paper, we investigate an energy consumption minimization problem for real-time mapping under the timeliness requirement. The system model is established based on an actual mobile robot system equipped with a two-dimensional light detection and ranging (2D-LiDAR), an odometry and a wireless communication module. The SLAM task is divided into several operation periods according to the exploration speed and 360-degree sensing duration. The data generated by LiDAR and odometry is wirelessly transmitted to the edge server for map reconstruction which is realized by deep neural networks (DNN) based method. The sensing duration, transmit power, transmission duration and exploration speed are jointly designed to minimize the energy consumption. We build a square area and acquire the raw data from LiDAR and odometry to establish a dataset for map learning. We classify it into edge and corner subset in our experiment, and we find it have good scalability. Our results demonstrate a promising new perspective to design energy-efficient SLAM by comprehensively considering sensing, communication and"}, {"title": "II. SYSTEM MODEL", "content": "The SLAM system considered in this paper is composed of a mobile robot and an edge server (data center), as shown in Fig. 1. The robot is equipped with a 2D-LiDAR module to perform ranging, an odometry module to measure the state of the robot, a wireless module to transmit the data generated from both LiDAR and odometer to the data center, and an MCU to link and control the above modules. After receiving the data, the real-time mapping, i.e., map reconstruction process, is performed at the data center.\nThe target area under mapping is a square area with a side length of $L$ m, and an access point linked to the data center is located at one of the four corners. The robot moves along the edge of the area at a distance of $e$ m in counterclockwise direction [11].\nThe entire SLAM mapping task is divided into several operation periods. For each period, the LiDAR performs a 360-degree sensing process with 1-degree angular resolution. The robot moves and senses simultaneously, and stores the data generated from LiDAR and odometry into cache memory. In next period, the data is transmitted wirelessly from the robot to the data center for map reconstruction. The timing diagram of system operation at robot side is shown in Fig. 2. It is assumed that the time length for communication is no longer than that for sensing, to ensure the timeliness of data delivery. Assume the velocity of the robot $v$ remains constant in this task, the number of periods $N_m$ is\n$N_m = \\frac{4(L - 2e)}{vt_{sens}},$ (1)\nwhere $t_{sens}$ denotes the 360-degree LiDAR sensing duration, which is equal to the length of each period. It is worth noting that the robot experiences $N_m$ sensing periods, and moves to the end point in the $(N_m + 1)$-th period while transmitting the data sensed in the $N_m$-th period."}, {"title": "B. Model of sensing process", "content": "The target task area can be transformed into occupancy map $m_o: R^2 \\rightarrow {0,1}$, where\n$m_o(x, y) = \\begin{cases}\n1, & \\text{there is an object at the position } (x, y), \\\\\n0, & \\text{there is empty at the position } (x, y).\n\\end{cases}$ (2)\nIt maps a global coordinate to the corresponding occupancy [13]. For the k-th period, the robot's state is represented as $V_k \\in \\mathbb{R}^{12\\times 1}$, in detail, $V_k = (p_k^T, u_k^T, w_k^T, a_k^T)^T$.\n$p_k = (x_k, y_k, z_k)^T$ represents the coordinates $x_k, y_k, z_k$ of the robot position. $u_k = (u_{k,x}, u_{k,y}, u_{k,z})^T$ represents the velocities of the robot in the x, y, z directions. $w_k = (w_{k,x}, w_{k,y}, w_{k,z})^T$ represents the angular velocity of the robot in the x, y, z directions. $a_k = (a_{k,x}, a_{k,y}, a_{k,z})^T$ represents the accelerations of the robot in the x, y, z directions.\nThe data sensed by LiDAR in the k-th period is $Z_k \\in \\mathbb{R}^{360\\times 1}$, its elements $Z_{k\\theta}$ are the measured distance between the robot and an object at the angle $\\theta$. So the sensing process is expressed as [9]\n$Z_k = f(V_k, m_o) + \\delta_k,$ (3)\nwhere $f(V_k, m_o)$ is the sensing function, and $\\delta_k \\in \\mathbb{R}^{360\\times 1}$ represents the sensing error. Assume the sening error is with Gaussian distribution, i.e., $\\delta_k \\sim N(0, S_k)$, where $S_k = diag(S_{k,1}, S_{k,2},...,S_{k,360}) \\in \\mathbb{R}^{360\\times 360}$ and $s_{\\theta}$ is the variance at angle $\\theta$.\nThe sensed data of the odometry is $U_k = (a_{k,x}, a_{k,y}, a_{k,z}, w_{k,x}, w_{k,y}, w_{k,z})^T \\in \\mathbb{R}^{6\\times 1}$ where $a_{k,x}, u_{k,y}, a_{k,z}$ are the measured accelerations of robot in the x, y, z directions, and $w_{k,x}, w_{k,y}, w_{k,z}$ are the measured angular velocity of robot in the x, y, z directions. It can be modeled as\n$U_k = g(V_{k-1}) + \\varepsilon_k,$ (4)\nwhere $g(V_{k-1})$ is the sensing function which measures the acceleration and angular velocity of the robot based on the system state $V_{k-1}$, $\\varepsilon_k \\in \\mathbb{R}^{6\\times 1}$ denotes the sensing error under Gaussian distribution, i.e., $\\varepsilon_k \\sim N(0, \\Sigma_k)$."}, {"title": "C. Data processing at data center", "content": "Map Reconstruction: We adopt DeepMapping [13] for the map reconstruction process, which is based on an unsupervised deep learning method. Fig. 3 illustrates the structure of DeepMapping. The estimated point cloud map is updated in real time, denoted by $M = {M_1,\\dots,M_{N_m+1}}$. The L-Net extracts the features from each local LiDAR data $Z_k$ based on the PointNet [10] and then estimates the pose. KF is Kalman filter to correct the pose estimated by L-net. TF is a transformation module that transforms the local coordinates of $Z_k$ to global coordinates, defined as $Z_{global,k}$. The sampling module is used to samples unoccupied points in the LiDAR scanned areas. Then all the sample points are added to obtain estimated point cloud map $M_{N_m+1}$. M-Net is a binary classification network that uses the estimated point cloud map to predict the occupancy probability for each point being occupied. Those occupancy probabilities are used for computing the loss of map reconstruction.\nFor M-Net, it is a continuous occupancy map $m_{\\phi}: \\mathbb{R}^2 \\rightarrow [0,1]$ that maps a global coordinate to the corresponding occupancy probability, where $\\phi$ are learnable parameters. Let $s(Z_{global,k})$ represents the sampling point of the unoccupied area, the loss function is defined as\n$\\mathcal{L}_{cls}(\\phi) = \\frac{1}{N_m + 1} \\sum_{k=1}^{N_m} (B[m_{\\phi}(Z_{global,k}), 1] + B[m_{\\phi}(s(Z_{global,k})), 0]),$ (5)\nwhere $B[m_{\\phi}(Z_{global,k}), 1]$ denotes the Binary Cross-Entropy (BCE) for all points in point cloud $Z_{global,k}$, $B[m_{\\phi}(s(Z_{global,k})),0]$ is the BCE for correspondingly unoccupied locations.\nFor L-Net, the loss is defined as\n$\\mathcal{L}_{ch}(\\psi) = \\sum_{i=1}^{N_m} \\sum_{j \\in [i-1, i+1]} D(Z_{global,i}, Z_{global,j}),$ (6)\nwhere $D(Z_{global,i}, Z_{global,j})$ represents the Chamfer distance between point cloud $Z_{global,i}$ and its temporal neighbors $Z_{global,j}$.\nTherefore, the loss function of entire map reconstruction process is defined as\n$\\mathcal{L}(\\phi, \\psi) = \\mathcal{L}_{ch} + \\gamma \\mathcal{L}_{cls},$ (7)\nwhere $\\gamma$ is a hyperparameter."}, {"title": "D. Model of wireless communication process", "content": "Let $b_k = (Z_k, U_k) \\in \\mathbb{R}^{363 \\times 1}$ denote the required information which is needed for mapping. For each element in $Z_k$, we assume to use $a_1$ bits to express it regarding to data structure. Similarly, $a_2$ bits are used for each element in $U_k$. Thus, let $b \\in \\mathbb{R}^{(360a_1 + 6a_2) \\times 1}$ denote the complete data sequence in a single period, which is assumed to be coded into several blocks. For simplicity, define $x_{k-1}$ as the coded data sequence for the (k-1)-th period, thus the transmission process in the k-th period can be modeled as\n$y_k = h_k x_k + \\omega_k,$ (8)\nwhere $\\omega_k \\sim \\mathcal{N}(0, \\sigma^2)$ denotes the Gaussian noise.\nAssume the communication process is performed in a multi-path signal propagation environment, there exists a line-of-sight (LoS) path $h_{LoS,k} = \\alpha_{0,k}$ and $m$ non-LoS (NLOS) paths $h_{NLOS,k} = \\sum_{n=1}^{m} A_{n,k}e^{i\\theta_{n,k}}$. $\\alpha_{0,k}$ and $a_{n,k}$ are the amplitude coefficients and $\\Theta_{n,k}$ is the phase shift. Thus, the channel is expressed as\n$h_k = h_{LoS,k} + h_{NLoS,k}.$ (9)\nThe magnitude of the channel, denoted by $|h_k|$, follows Rice distribution [14].\nAssume the transmit power remains constant within each period and the free-space path loss model is adopted, then the received power at the data center in the k-th period is given by [15]:\n$P_{rx,k}(t) = \\frac{P_{tx,k} G_t G_r \\lambda^2}{(4\\pi d(t))^2},$ (10)\nwhere $\\lambda$ is the signal wavelength, $G_t$ is the transmit antenna gain of the robot, $G_r$ is the receive antenna gain of the data center, and $P_{tx,k}$ is the signal transmit power of the robot. Then the real-time transmission rate $R_k(t)$ can be expressed as\n$R_k(t) = B \\log_2(1 + \\frac{P_{rx,k}(t)|h_k|^2}{\\sigma^2}),$ (11)\nwhere B denotes the communication bandwidth. The distance between the robot and the data center $d(t)$ can be derived as\n$d(t) = \\begin{cases}\n\\sqrt{(vt + e)^2 + e^2}, & 0 < t < \\frac{L - 2e}{v}, \\\\\n\\sqrt{(vt - L + 3e)^2 + (L - e)^2}, & \\frac{L - 2e}{v} < t < \\frac{2(L - 2e)}{v}, \\\\\n\\sqrt{(3L - 5e - vt)^2 + (L - e)^2}, & \\frac{2(L - 2e)}{v} < t < \\frac{3(L - 2e)}{v}, \\\\\n\\sqrt{(4L - 7e - vt)^2 + e^2}, & \\frac{3(L - 2e)}{v} < t < \\frac{4(L - 2e)}{v}\n\\end{cases}$ (12)\nTherefore, the data amount that can be transmitted within the k-th period is given by\n$I_k = \\int_{(k-1)t_{sens}}^{(k-1)t_{sens} + t_{comm}} B \\log_2(1 + \\frac{P_{rx,k}(t)|h_k|^2}{\\sigma^2}) dt,$ (13)\nwhere $k = 2,3,..., N_m + 1$ let $t_{comm} = \\rho t_{sens}$ denote the communication time length with $\\rho \\in (0,1]$. For simplicity, for the $N_m+1$-th period, we assume the communication time length is equal to that in other periods, which does not affect the results since $N_m$ is usually large and the robot is relatively close to the access point."}, {"title": "E. Robot system energy consumption", "content": "The average mechanical power for the robot to move on a flat floor can be modeled as [16]\n$P_e = \\frac{1}{2} \\kappa_1 v^3 + \\kappa_2 v,$ (14)\nwhere $\\kappa_1$ denotes the air resistance coefficient (determined by air density, fluid drag coefficient and frontal area), $\\kappa_2$ denotes the friction coefficient (determined by downforce and rolling friction factor). For other more complex moving resistance model, one can refer to [17] and [18]. The energy consumption of the LiDAR is assumed to be a constant $E_L$ for each period. Therefore, for our entire SLAM mapping task, the total energy consumption of the robot is defined as the sum-energy of the communication energy consumption $E_{comm}$, LiDAR energy consumption $E_{LiDAR}$ and mechanical energy consumption $E_{mech}$ :\n$E_{total} = E_{comm} + E_{LiDAR} + E_{mech},$ (15)\nwhere\n$E_{comm} = \\sum_{k=2}^{N_m+1} P_{tx,k}t_{comm},$ (16a)\n$E_{LiDAR} = N_m E_L,$ (16b)\n$E_{mech} = P_e \\frac{4(L-2e)}{v}.$ (16c)"}, {"title": "III. PROBLEM FORMULATION AND SOLUTION", "content": "Our objective is to minimize the total energy consumption of the robot for completing the SLAM mapping task while meeting the timeliness requirement, with respect to transmit power, communication time length, movement velocity and LiDAR sensing cycle time (also period length here). To this end, the corresponding optimization problem is formulated as\n$\\begin{aligned}\n&\\underset{\\{P_{tx,k}, \\rho,t_{sens}, v\\}}{\\text{min}} \\quad E_{total}(P_{tx,k}, \\rho, t_{sens}, v) \\\\\n&\\text{s.t.} \\quad I_k \\geq 360a_1 + 6a_2, \\\\ &\\qquad N_m t_{sens} \\leq T_{max}, \\\\ &\\qquad P_{tx,k} > 0, \\\\ &\\qquad 0 < \\rho < 1, \\\\ &\\qquad N_m \\geq N_D,\n\\end{aligned}$ (16)\nwhere $T_{max}$ is defined as the maximum allowed time length to complete the task, $N_D$ is defined as the minimum number of LiDAR sensing cycles for SLAM, $k = 1,2,..., N_m$. (26a) is the requirement for data transmission. (26b) is the task time requirement. (26c) ensures the effectiveness of the power parameter. (26d) is the requirement for communication time length. (26e) is the requirement on number of LiDAR sensing cycles."}, {"title": "B. Solving approach", "content": "In spite of the coupling effect among the system parameters, the relationship between the total energy consumption and the communication parameters is analyzed as shown in the following lemma.\nLemma 1. For problem (P1), given $v$ and $t_{sens}$, $E_{total}$ is minimized when $\\rho^* = 1$.\nProof. See Appendix A.\nAccording to Lemma 1, the optimal $P_{tx,k}^*$ can be achieved based on the following relationship:\n$P_{tx,k}^* = \\frac{\\frac{N_s (360a_1 + 6a_2)}{B t_{sens}} - 1}{\\prod_{i=1}^{N_s} (1 + \\frac{P_{tx,k} \\mu_k}{d^2[(k-1 + \\frac{i}{N_s})t_{sens}]})},$ (17)\nfor $k = 2, 3, ..., N_m + 1$. Then, problem (P1) can be transformed into the following problem:\n$\\begin{aligned}\n&\\underset{\\{t_{sens}, v\\}}{\\text{min}} \\quad E_{total} (P_{tx, N_m, k}, \\rho^*, t_{sens}, v) \\\\\n&\\text{s.t.} \\quad N_m t_{sens} \\leq T_{max}, \\\\ &\\qquad N_m \\geq N_D,\n\\end{aligned}$ (18)\nwhere $P_{tx, N_m, k}$ represents the transmission power in the k-th period and variable $N_m$ is determined by both $t_{sens}$ and $v$. Thus, the total energy can be expressed as\n$E_{total} (P_{tx, N_m, k}, \\rho^*, t_{sens}, v) = (\\frac{1}{2} \\kappa_1 v^2 + \\kappa_2) (\\frac{4(L-2e)}{v}) + N_m E_L + \\sum_{k=2}^{N_m+1} (P_{tx, N_m, k}t_{sens}).$ (19)\nGiven a constant $v$ and assume $N_D >> 1$, then $N_m \\approx \\frac{4(L-2e)}{vt_{sens}}$. According to (28b), we can have\n$t_{sens} < \\frac{4(L-2e)}{vN_D} \\\\\\frac{4(L-2e)}{v} \\leq T_{max}.$ (20)\nIt can be observed from (29) that the energy consumption for robot movement and LiDAR sensing decreases as $t_{sens}$ increases regardless of the physical coefficients. Thus, it is necessary to validate how the total energy changes with $t_{sens}$. As shown in Fig. 4, the total energy as well as the communication energy monotonically decrease with $t_{sens}$, where $t_{sens}$ is taken in a practical range for real-world LiDAR hardware. Therefore, the optimal period length is achieved as\n$t_{sens} = \\frac{4(L - 2e)}{vN_D}.$ (21)\nConsequently, $E_{total}$ with respect only to speed is expressed as\n$E_{total}(v) = (\\frac{1}{2} \\kappa_1 v^2 + \\kappa_2) (\\frac{4(L-2e)}{v}) + N_D E_L + \\frac{4(L-2e)}{vN_D} \\sum_{k=2}^{N_D+1} P_{tx, N_D, k}.$ (22)"}, {"title": "IV. EXPERIMENT AND NUMERICAL RESULTS", "content": "This section provides the numerical results in which the map reconstruction is trained based on SLAM experiments. Dataset. The map reconstruction process is realized based on a dataset built by ourself. A microROS robot, which is equipped with a 2D LiDAR of 360-degree field of view (FOV) and an odometry, is used to obtain the cloud point raw data. The data is transmitted back via a WiFi link. As shown in Fig. X, we acquire the LiDAR raw data by creating a 2.25x2.25 m\u00b2 square area by fence and generally classify it into two subset, i.e., edge and corner. Based on the preset trajectory of robot movement, we totally accumulate cloud point data of 20 cycles in that area, i.e., about 180 m edge and 80 corners. We randomly select part of this dataset and create up to 20x20 m\u00b2 square area for map learning and energy consumption evaluation. The deep learning network parameters are optimized and trained on an Nvidia GTX3080.\nSystem parameters. For communication, the bandwidth is set as $B = 10$ MHz. The wavelength $\\lambda$ is 0.125 m for 2.4 GHz. The average noise power spectral density over this band is -110 dBm/Hz. The channel under multipath is modeled as Rice distribution. For 2D LiDAR sensing, the number of bits to represent cloud point data and odometry data is $a_1 = a_2 = 64$. The time length of a single sensing cycle is considered within 0.06 s to 0.2 s, i.e., about 5 to 16 Hz scan frequency. For each sensing cycle, the energy $E_L$ is set to be 0.025 J. For robot movement, $\\kappa_1$ and $\\kappa_2$ are set to be 0.003 and 0.4, respectively. The area size $L$ is considered from 2 to 20 m and $e$ is 0.45 m. The allowed time for SLAM task is set as $T_{max} = 40$ s. The requirement on the number of periods is $N_D = 400."}, {"title": "V. CONCLUSION", "content": "This paper focuses on the energy consumption problem to support energy-efficient lifelong SLAM and the corresponding potential spatial machine intelligence applications. The system model is built based on a robot performing SLAM via 2D LiDAR sensing and deep learning based map reconstruction method. It is found that the energy proportions of sensing, communication and movement are quite different with increasing area sizes. Our results demonstrate a promising new perspective to analyze the energy efficiency for lifelong SLAM, leading to many interesting research directions such as multi-agent resource allocation, more efficient design of channel estimation, re-localization and loop closure."}], "appendices": [{"title": "A. Proof of Lemma 1", "content": "Proof. Give a constant $v$ and $t_{sens}$, $E_{total}$ is simplified as a function with respect to $p$ and $P_{tx,k}$ that are only relevant with $E_{comm}$."}, {"title": null, "content": "Due to the difficulty to achieve the closed-form expression According to formula (14) and (26a),we have\nof the continuous-time integration in (14), for the interval\n$[(k-1)t_{sens},(k-1)t_{sens}]$, it can be discretized into $N_s$\n$I_k  \\approx \\sum^{N_s}_{i=1} B \\log_2(1+ \\frac{P_{tx,k}\\mu_k}{d^2 [(k-1+\\frac{i}{N_s})t_{sens}]}) \\frac{\\rho t_{sens}}{N_s} dt$ (51)\nequal-length sub-intervals and the $i$th sub-interval is $[(k$\n$1+\\frac{i-1}{N_s})t_{sens},(k-1+\\frac{i}{N_s})t_{sens}]$ where $i=1,2,\\dots,N_s$. So, according to the implicit function derivative rule [19], we have\nFor the $i$th sub-interval, we defined $\\rho_i$ as the ratio between\ncommunication time length to the sub-interval length, where  $\\frac{\\partial P_{tx,k}}{\\partial \\rho} = -  \\frac{\\frac{\\partial I_k}{\\partial \\rho} \\frac{1}{B \\log 2}}{ [\\frac{\\partial}{\\partial P_{tx,k}} (\\frac{ \\rho t_{sens}}{N_s} dt)][(1+\\frac{P_{tx,k}\\mu_k}{d^2 [(k-1+\\frac{i}{N_s})t_{sens}]}])^{-1}]}$ (52)\n$\\rho_i \\in (0,1]$. Thus, we have \\\\\n$\nThen \\\\ $I_k \\approx \\sum^{N_s}_{i=1} B \\log_2(1+ \\frac{P_{tx,k}\\mu_k}{d^2 [(k-1+\\frac{i}{N_s})t_{sens}]}) \\frac{\\rho t_{sens}}{N_s}$ (53)\nTherefore, regarding to the derivative of Ecomm with respect to p, \\\\\n$\nWhere $N_s$ should be chosen as a relatively large positive Therefore regarding to the derivative of Ecomm with respect to p,\nInteger and let. \\\\\n$\nThen $ \\frac{\\partial E_{comm}}{\\partial \\rho} = \\frac{\\partial E_{comm}}{\\partial P_{tx,k}} \\frac{\\partial P_{tx,k}}{\\partial \\rho} +  \\frac{\\partial E_{comm}}{\\partial I_{k,i}} \\frac{\\partial I_{k,i}}{\\partial \\rho}  = 0$ (54)\n$\n$\nThen $ \\frac{\\partial E_{comm}}{\\partial \\rho}  0$\nSince Ik and p are independent, we have. \\\\\n$\nSo the optimal is: \\\\\n$\nSo the optimal is: \\\\\nSince the optimal power is 1. \\\\\nBecause all the powers are 1, then Etotal is 1."}]}