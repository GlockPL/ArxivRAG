{"title": "KARUSH-KUHN-TUCKER CONDITION-TRAINED NEURAL NETWORKS (KKT NETS)*", "authors": ["Shreya Arvind", "Rishabh Pomaje", "Rajshekhar V. Bhat"], "abstract": "This paper presents a novel approach to solving convex optimization problems by leveraging the fact that, under certain regularity conditions, any set of primal or dual variables satisfying the Karush-Kuhn-Tucker (KKT) conditions is necessary and sufficient for optimality. Similar to Theory-Trained Neural Networks (TTNNs), the parameters of the convex optimization problem are input to the neural network, and the expected outputs are the optimal primal and dual variables. A choice for the loss function in this case is a loss, which we refer to as the KKT Loss, that measures how well the network's outputs satisfy the KKT conditions. We demonstrate the effectiveness of this approach using a linear program as an example. For this problem, we observe that minimizing the KKT Loss alone outperforms training the network with a weighted sum of the KKT Loss and a Data Loss (the mean-squared error between the ground truth optimal solutions and the network's output). Moreover, minimizing only the Data Loss yields inferior results compared to those obtained by minimizing the KKT Loss. While the approach is promising, the obtained primal and dual solutions are not sufficiently close to the ground truth optimal solutions. In the future, we aim to develop improved models to obtain solutions closer to the ground truth and extend the approach to other problem classes.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, there has been growing interest in utilizing the deep learning framework to solve optimization problems. In this work, we present a neural network-based approach that leverages the Karush-Kuhn-Tucker (KKT) conditions to approximately solve convex optimization problems.\nThe general form of a convex optimization problem, Boyd & Vandenberghe (2004), is expressed as:\n\nminx\u2208Rnf0(x), (1a)\nsubject tofi(x)\u22640,i=1,...,m, (1b)\ngi(x)=0,i=1,...,p, (1c)\n\nwhere x = [x1,x2,...,xn] \u2208 Rn, fi : Rn \u2192 R are convex functions and gi: Rn \u2192 Rare affine. The domain of the above problem is defined as: D = \u22c2\u221ei=0domfi\u22c2\u221ei=1domgi. A dual formulation of the above optimization problem is derived using the Lagrangian, where the"}, {"title": "2 NEURAL NETWORK APPROACH TO SOLVING CONVEX OPTIMIZATION PROBLEMS", "content": "As mentioned, for a convex optimization problem, if we find x*, \u03bb* and \u03bd* that satisfy equation 4b - equation 4e, they must be the optimal primal and dual solutions. In our approach, we take the parameters of a problem as input, and the expected output is x*, \u03bb* and \u03bd*. Our loss function includes what we refer to as KKT Loss (LKKT), which is a weighted sum of the primal feasibility loss, dual feasibility loss, complementary slackness loss, and stationarity loss, where\n\nPrimal Feasibility Loss, LPF = 1m\u2211mi=1max(0,fi(x))2, (5)\nDual Feasibility Loss, LDF = 1m\u2211mi=1max(0,\u2212\u03bbi)2, (6)\nComplementary Slackness Loss, LCS = 1m\u2211mi=1(\u03bbi\u22c5fi(x))2, (7)\nStationarity Loss, LS = 1n||\u2207fo(x)+\u2211mi=1\u03bbi\u2207fi(x)+\u2211pj=1\u03bdj\u2207gj(x)||22, (8)\n\nwhere x\u0302, \u03bb\u0302, and \u03bd\u0302 represent the neural network outputs corresponding to x*, \u03bb* and \u03bd*, respectively, and ||.||2 represents the 2-norm of a vector. Concretely, the KKT Loss is defined as follows:\n\nLKKT(\u03b11,\u03b12,\u03b13,\u03b14)=\u03b11LPF+\u03b12LDF+\u03b13LCS+\u03b14LS, (9)"}, {"title": "3 DATASET GENERATION, MODEL TRAINING, AND RESULTS", "content": "In this section, we present the dataset generation and preparation, along with the training of a neural network model and the results obtained. Note that to minimize only the KKT Loss, we need only the model parameters and do not require the ground truth optimal solutions. However, to minimize the Data Loss and evaluate performance, we need the ground truth optimal solution for the given model parameters."}, {"title": "3.1 DATA SET GENERATION", "content": "In this work, we consider a class of optimization problems that can be expressed with explicit, closed-form expressions, referred to as parameterized problems.\nFor training a neural network (in our case, to minimize the Data Loss), we require labeled data, specifically the problem parameters and the corresponding optimal primal and dual solutions. We can artificially generate this data, which consists of the parameters of a problem instance and the corresponding solutions. To achieve this, we can use random number generators to populate the parameters of optimization problems. Furthermore, to find the optimal primal and dual solutions, one can use any of the numerous available solvers. Specifically, for our data, we used the CVXPY, Agrawal et al. (2018); Diamond & Boyd (2016). Below, we explain how the data can be generated using the standard form of a quadratic programming (QP) problem, which is as follows:\n\nminx12xTPx+qTx+r, (12a)\nsubject toGx\u2264h, (12b)\nAx=b, (12c)\n\nwhere P \u2208 Rn\u00d7n, q\u2208Rn, r\u2208R, G\u2208 Rm\u00d7n, h\u2208 Rm, A \u2208 Rp\u00d7n, b \u2208 Rp are the parameters of the problem that we take as inputs to the neural network.\nWe use a random number generator to populate the entries of the matrices and vectors (of parameters) in the above expression. We then normalize the entries in the matrices and vectors to the interval [-1,1], as described below. Let \u0398 = max{Pmax, qmax, r, Gmax, hmax, Amax, bmax}, where each element of the set above is the maximum absolute value of the entries of the corresponding matrix or vector. For example, Gmax = max{|gij|}, where gij is the element in the ith row and jth column. We then perform the complete normalization as follows: P = P/\u0398, q\u0303 = q/\u0398, r\u0303 = r/\u0398,"}, {"title": "3.2 TRAINING THE NETWORK", "content": "While the approach has the potential to work for any parameterized convex optimization problem, we consider the following simple linear optimization problem and generate the dataset:\n\nminx\u2208R2cx, (14a)\nsubject toAx\u2264b, (14b)\n\nwhere A \u2208 R2\u00d72, c\u2208 R2, and b \u2208 R\u00b2 are the parameters of the problem, the flattened versions of which are taken as inputs to the neural network. As mentioned, we used CVXPY to generate instances of these problems, where the elements in A, c, and b were randomly generated. Each problem was normalized and solved to obtain the primal and dual solutions. The resulting dataset, consisting of the coefficient matrices along with the primal and dual solutions, was used to train the network. Only those problems that were feasible and resulted in optimal, accurate solutions were considered for training purposes.\nThe network was trained using three different loss configurations: only Data Loss, with \u03b11 = \u03b12 = \u03b13 = \u03b14 = 0 and \u03b2 = 1; only KKT Loss with \u03b11 = 0.1, \u03b12 = 0.1, \u03b13 = 0.2, \u03b14 = 0.6, and \u03b2 = 0; and a combination of KKT and Data Loss, with \u03b11 = 0.1, \u03b12 = 0.1, \u03b13 = 0.2, \u03b14 = 0.6, and \u03b2 = 1."}, {"title": "3.3 RESULTS", "content": "Fig. 1 shows the decrease in training loss when different loss functions are considered for the KKT Net. We observe that, regardless of which loss is used, the model demonstrates its ability to learn, as indicated by the reduction in losses throughout the training process.\nWe next present results for the inference done on the trained models on an independently generated dataset, which is normalized before using it for inference similar to what is done during training.\nThe root-mean-square error (RMSE) between the primal and dual solutions output by the trained networks\u2014each trained to minimize different combinations of KKT and Data Loss\u2014and the ground truth optimal solutions obtained using CVXPY is presented in Table 1. We observe that the performance of the KKT Net, when trained to minimize different combinations of KKT and Data Loss"}, {"title": "4 CONCLUSION", "content": "In this paper, we presented an approach to solving convex optimization problems using a neural network, where the input consisted of the problem parameters and the expected output was the optimal primal and dual variables. We formulated the problem to minimize the KKT Loss, which measures how closely the solution output by the neural network satisfies the KKT conditions. Additionally, we considered a combined loss, defined as a weighted sum of the KKT Loss and what we referred to as Data Loss, which is the MSE between the ground truth optimal primal and dual variables and those predicted by the neural network. We used a simple linear program to evaluate the performance of this approach and found that the neural network was able to learn to output the optimal primal and dual solutions. Training the network to minimize only the KKT Loss resulted in better performance, with a larger fraction of smaller errors compared to training it to minimize the combined loss or only the Data Loss. While the approach is promising, the obtained primal and dual solutions are not sufficiently close to the ground truth optimal solutions. In future work, we aim to develop improved models to obtain solutions that are closer to the ground truth and to extend the approach to other classes of convex and non-convex optimization problems."}]}