{"title": "Secret Use of Large Language Models", "authors": ["ZHIPING ZHANG", "CHENXINRAN SHEN", "BINGSHENG YAO", "DAKUO WANG", "TIANSHI LI"], "abstract": "The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users' secret use of LLM, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancements in Large Language Models (LLMs), coupled with the wide availability of LLM- based conversational agents, have democratized the power of AI. Given the open-ended nature of LLMs, the purposes, contexts, and consequences of using them become more directly determined by the end-users. As a result, users are playing an increasingly important role in conforming to responsible Al principles such as fairness, transparency and privacy [23, 26], and ensuring AI is not used in a way that causes harms to humanity.\nTransparency, as one of the responsible AI principles, has been significantly impacted by LLMs [57]. Prior literature on AI transparency has focused on transparency via explainability techniques, regulatory or legal frameworks, or data-usage disclosures, attributing the responsibility"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1 Secretive Behavior", "content": "We consider the secret use of LLMs one special case of secretive behavior, and review the psychology literature on secretive behavior in other contexts. A broader view of secrecy regards it as an intention to keep information unknown to one or more one individual, with two main psychological paths [80, 81]. One is \u201cmind-wandering to secret,\u201d an internal and subconscious process where secret occupies an individual's mind [80, 81]. The other is \u201cconcealment of secret,\u201d an external process where one makes conscious decisions to not disclose the information to others [80, 81]. Our study falls under the path of \u201cconcealment of secrets\u201d, focusing on the secretive behavior or intentions that associate with their external actions.\nSeveral studies on secretive behavior lies in secret consumption in different areas, such as secrete consumption on counterfeit [8], secret consumption of meat [46] and secret snack consumption behavior [30]. A large portion of studies about specific secretive behavior lies in area of \u201cknowledge hiding behavior\u201d [3, 18, 49, 77]. Connelly et al. [18] and Serenko and Bontis [77] introduced early research that formally studied the factors influencing people's knowledge hiding behavior in organizations. They used a mixed-methods approach, asking participants to share their personal related stories and then measuring participant's perceptions related to the behavior [18, 77]. These studies inspired our study design, which also employed a mixed-methods approach, including an open-ended case-listing survey followed by a controlled online experiment.\nResearch on users' secret use of AI or LLMs is limited, showing a gap that our study aims to address. One related study about AI Ghostwriter Effect [24] found that users refrain from declaring Al authorship even though they do not consider themselves the owners and authors of AI-generated text. Their study design focused on an AI-assisted postcard writing task, while our research exam- ined tasks that could broadly engender passive non-disclosure and active concealment behaviors. Through the unique lens of secret use behaviors, our work identifies new social factors that affect these behaviors, such as users' own moral judgments and their perceived social expectations."}, {"title": "2.2 Al Transparency", "content": "With the deeper integration of AI into people's daily lives and decision-making, the lack of trans- parency in Al systems can lead to doubts about the reliability of decisions made by AI [15, 51, 84, 88]. Current research in the field of HCI aimed at enhancing AI transparency can be divided into two primary categories: the ways in which users perceive these AI systems, and the development of tools by researchers and engineers to increase the explainability of AI systems.\nResearchers conducted lots of studies to understand how users perceive AI systems [47, 72, 78, 79]. Shin [79] found that users tend to view algorithms as more trustworthy and useful when they perceive them to be fair, accountable, transparent, and explainable. Similarly, Yu and Li [91], through interviews with 235 employees about their views on AI at work, found that transparency in AI decision-making improved both its perceived effectiveness and addressed the associated discomfort.\nGiven the insights about users' perceptions and needs, researchers have developed tools to enhance users' ability to assess the transparency of the AI systems they use [25, 38, 42, 58, 93]. For example, Transparency-Check [74] is a tool to aid users in evaluating the transparency of AI-based systems, particularly those with personalization. This tool includes questions around data collection, algorithmic models, personalized recommendations, and user control.\nPrior research of AI transparency has primarily viewed AI researchers and practitioners as the information provider, with users being the recipients of the information [19]. However, the user-driven, open-ended nature of the use scenarios of large language models (LLMs) and the extensive use of LLM-based conversational agents in critical domains have raised concerns about the transparency of how LLMs are used by users, anticipating users to play an important role as information providers as well. By examining the contextual factors and psychological process that affect users' secret use behavior, our findings shed light on users' ability to fulfill their responsibility towards AI transparency. Our research broadens the scope of research in the field of AI transparency."}, {"title": "2.3 Existing Community-Level Requirements of Transparency for LLM Use", "content": "With the rapid growth of LLMs, some agencies have announced regulations or recommendations regarding the use of LLMs in their respective communities. Despite their different strategies of handling the impact of LLMs, a common theme among these regulations and recommendations is transparency. We summarize major examples below.\nUniversities. Universities have implemented varied regulations regarding students' obligations to disclose their use of LLMs in coursework. For example, University of Minnesota requires students to reference all the content in their assignment generated by the AI tools, or they will be regarded as scholastic dishonesty [86]. University of British Columbia allows students to use LLMs with instructors' approvals and provides a guide on how to cite contents generated by LLMs [85].\nJournalism, Social Media and Blog. Journalism and social media platforms also put regulations to standardize the use of LLMs. The Associated Press encourages its journalists to verify that material received from other sources is also free from AI-generated content, which relies on a reliable source of whether AI is involved in the generation of the content [2]. Medium, an online publishing platform, announced that the use of LLMs is permitted for writing stories [62]. However, to promote transparency, stories that incorporate AI assistance must be clearly labeled as such. YouTube encourages its content creators to maintain transparency regarding the use of AI [90].\nAcademia. Academia conferences and journals often request authors to fully reveal their use of LLMs. For example, the Association for Computing Machinery (ACM) and Elsevier have updated their policy regarding authorship and the use of LLMs [5, 27]. The creation of content with LLM tools and technologies is permitted, but it must be fully disclosed in the work."}, {"title": "3 STUDY 1: ONLINE SURVEY", "content": "To understand when and why people hide their use of LLMs, we first conducted an online survey with 180 LLM users, asking them to reflect on their past experiences of secret use of LLMs, list the secret use scenarios, and explain the reasons. 112 participants (62%) reported they had LLM secret use experience while others not. Finally, we collected 125 unique and valid secret use cases with rationales (noting that some submitted multiple cases). We qualitatively analyzed the data and derived a taxonomy summarizing eight common reasons that lead to the secret use."}, {"title": "3.1 Survey Design", "content": "The main task in the questionnaire was to share the secret use experience of LLMs. At the beginning of the survey, there were screening questions to determine whether participants had relevant experiences with LLMs and any secret usage. Given that users may not be familiar with jargon like \u201cLarge Language Models (LLMs)\u201d or \u201cLLM-based conversational agents\u201d [94], we first explained the qualitifications in the screening process by showing example LLM applications such as ChatGPT, OpenAI API Playground, Bard, Pi.ai, and Claude.ai. We then refer to them throughout the survey using a term more friendly to lay users, \u201cAI chatbot\u201d. Responses from those without LLM experiences were excluded in our results.\nFor respondents who reported no secret use experiences or intention on the secret use, we asked follow-up questions about their reasons. Respondents with secret usage experiences were prompted to share up to three stories, detailing their scenarios, reasons for secrecy, and coping strategies. To ensure that the users provide precise and comprehensive information to describe the scenario, we used fill-in-the-blank questions. These questions guided them to reflect and report on each case in various aspects, including the name of the LLM service, the specific task, the entities they want to keep the AI use secret from, and their feelings. This was followed by three questions on reasons for secrecy, feelings about secret usage, and any measures taken to maintain secrecy.\nThe survey questionnaire was built on Jotform\u00b2 and distributed through Prolific\u00b3. Our pilot study indicated it took about 4 minutes to complete the survey if the participant sharing only one secret use case. Therefore, basic compensation was set at $0.4, with an additional $0.2 for each extra case shared. To ensure a diverse sample, we released the survey at different times across weekdays and weekends, and controlled for a balanced gender distribution between female and male participants."}, {"title": "3.2 Qualitative Analysis Methods", "content": "We conducted open coding to analyze the scenarios of the secret use of AI reported by the partici- pants, as well as the self-reported feelings and rationales."}, {"title": "3.2.1 Reason of Secret Use Analysis", "content": "Our main goal was to establish a typology of reasons why participants wanted to hide their usage of LLMs. To this end, two researchers iteratively coded the self-reported feelings and rationales for each case reported by the participants. The two researchers first collectively reviewed a subset of the valid cases to establish an initial coding scheme. Next, they carried out several rounds of independent relabeling, each time with a small subset of the dataset, in order to refine the typology and align the two coders' interpretations of the categories. At the end of each round, they calculated the inter-rater reliability (IRR) and discussed resolving discrepancies and adjusting the coding scheme when needed. This process was repeated iteratively until a high IRR (> 0.8) was reached. The coding scheme was finalized after five rounds of coding and revisions. To validate the coding scheme, we collected a new dataset using the same protocol. The same two coders independently coded the new dataset. The inter-rater reliability (IRR) score on the validation set was 0.834, as measured by Krippendorff's alpha. This measurement is a standard for inter-rater reliability measure for codes that are not mutually exclusive [34]. We selected this measure because each case may be associated with multiple reasons.\nOur final coding scheme comprised nine categories, eight for specific reasons and one category \u201cOther or unclear reason\u201d for labeling short or ambiguous responses that did not provide enough information for analysis."}, {"title": "3.2.2 Scenario of Secret Use Analysis", "content": "We conducted a thematic analysis to analyze the scenarios reported by the participants. Two researchers independently reviewed all the valid cases, and each identified some themes around the scenarios. They then discussed together to combine the two sets of themes and synthesize them into a list of themes characterizing the tasks that engendered the secret use behavior. Six themes about the scenarios were identified in the final coding scheme. We did not use IRR because the primary goal is to yield themes of scenarios rather than seeking agreement [61]."}, {"title": "3.3 Methodological Limitations", "content": "Our method has certain limitations that should be taken into account when evaluating the results. Firstly, the information in the participants' responses is often limited, which may not fully capture their reasoning. To facilitate data collection, we asked participants to describe their feelings by filling in the blanks in a questionnaire, as shown in Figure 1. Participants tended to use only one"}, {"title": "3.4 Survey Results", "content": ""}, {"title": "3.4.1 An Overview of the Contexts of Secret Use of LLMs", "content": "Based on participants' reports, we identified the following types of scenarios that have led to a tendency to conceal the use of LLMs, including creative writing (11%), academic writing (3%), school works (10%), work tasks (15%), social connections (14%), sensitive topics (36%), and other miscellaneous cases\u2074 (including entertainment, brainstorming, etc). We introduce each scenario in depth below.\nCreative writing. This category referred to scenarios where participants used LLMs for creative writing and prefer to keep the LLM usage secret from other people. For example, SP15 (P15 in survey study) and SP25 used ChatGPT for writing fiction and fanfiction, and chose to keep their use of AI tools a secret. Similar secretive behavior was obeserved in SP36, who used NovelAI for writing stories, and SP56, who used ChatGPT for YouTube video scripts creation. This category was specifically created to explore how people feel when they combine AI-powered tools with their own creativity.\nAcademic writing. Several participants expressed a desire to conceal their use of LLMs in academic writing, such as writing papers or articles and generating research ideas. For example, SP8 used ChatGPT for paragraph writing in academic works and chose to keep the usage secret from everyone, believing that the ideas generated were not originally his. We designated this category to gain insights into how people employ LLMs in conjunction with their expertise in academic fields.\nSchool works. Participants also indicated a desire to conceal their use of LLMs in school-related tasks, such as completing an essay or homework assignments. Comparing with \u201cacademic writing\u201d, the \"school works\u201d category places a greater emphasis on the educational environment.\nWork tasks. Many participants mentioned using LLMs for various work-related tasks, preferring to keep this usage confidential. Examples including coding, data analysis, and other general job responsibilities. The types of tasks are closely related to participants' work domains and job positions. For example, SP16 used ChatGPT for policy writing but was concerned about being discovered by his employer. SP13 used ChatGPT to develop marketing plans and preferred to keep this use a secret within his industry.\nSocial connection. A large portions of reported secret use experiences involved using LLMs to maintain social connection including personal and professional relationships. For example, SP17 and SP40 used ChatGPT to generate love messages to their partners and kept the use of LLM tool secret. SP94 chose to hide their use of Bard in writing work emails from colleagues.\nSensitive topics. Tasks under this category are mostly related to sensitive interaction content, such as sex or erotica, health, medical topics, personal advice or therapy. For example, SP54 used Character.ai for sex roleplay and was afraid of being found out. SP84 turned to Bing chat for medical advice and thought that his medical details were not free to other. This category often associates"}, {"title": "3.4.2 A Typology of Causes to Secret Use of LLMs", "content": "We then introduce the results about the reasons behind the secret use of LLMs, synthesized from the self-reported reasons.\nWe first want to introduce a special category, \u201cContent sensitivity concerns\u201d, which emphasizes concerns related to the task itself or the content shared via the interaction with LLMs being discovered, rather than the fact of using LLMs. For instance, SP107 consulted ChatGPT for health advice and wanted to keep the interaction private: \u201cMy health and health record are private things. When I go online in search of information on my condition or a potential condition it's important to keep that confidential.\u201d Since this category diverges from our research focus on the concealment of LLM usage, we have excluded it from our detailed analysis.\nAfter excluding the categories of \u201cOther and unclear reasons\u201d (11 cases) and \u201cContent sensitivity concerns\" (37 cases), we identified seven categories encompassing 77 valid instances directly related to concealing LLM usage. These categories are grouped into two themes: Internal Judgment and Perceived External Judgment.\nInternal judgment encompasses reasons related to the participants' self-assessment of using AI. This includes \u201cQuestioning their own competence\u201d, \u201cNot a wise choice to use LLMs\u201d, and \u201cMoral doubts on using LLMs\u201d.\nQuestioning their own competence (8/77, 10%). When using LLMs for completing tasks, some individuals experienced feelings of incompetence, inadequacy, or unprofessionalism. For instance, SP3 felt incompetent when using ChatGPT for coding assistance, stating, \u201cIt's not the best feeling to know you need help from an AI bot.\u201d Similarly, SP7, who used ChatGPT for creating an essay outline, felt ashamed and preferred to keep her use of LLMs a secret. She expressed, \u201cI think it would make me feel inadequate because I used an outline.\u201d\nNot a wise choice to use LLMs (9/77, 12%). Some individuals frequently questioned whether using LLMs was the effective or optimal choice for completing tasks. In these cases, participants often implied a doubt on the effectiveness of the LLM-based CAs. For example, SP37 used ChatGPT for family advice while she felt ashamed and preferred keep this usage secret as she said \u201cI know that the AI doesn't understand family matters and how to respond in a way that a human would.\u201d SP31 kept her usage of ChatgGPT in health consulting a secret and she said \u201cIt's stuff I should of went to a doctor for and not been asking a chatbot.\u201d\nMoral doubts on using LLMs (26/77, 34%). While using LLMs, some individuals believed that employing these tools for certain tasks might be immoral. For example, SP66, who utilized ChatGPT to help draft a recommendation letter for a National Interest Waiver, felt like a fraud towards the people she worked with. Similarly, SP7 felt he was cheating by writing with the assistance of ChatGPT, stating, \u201cIt felt like I was cheating. I used it (ChatGPT) to write when I should be able to do that on my own.\"\nA different category of reasons, perceived external judgment, covers reasons where one conceals their use of LLMs due to apprehension about external criticism. This might have stem from the general societal ambivalence towards emerging, potent technologies like LLMs. This category included four reasons, \u201cFear of capabilities being critiqued\u201d, \u201cFear of personality/attitudes being judged\u201d, \u201cSincerity concerns in personal relationships\u201d and \u201cUsing AI for certain tasks is considered unethical/inappropriate by others\u201d.\nFear of capabilities being critiqued (8/77, 10%). Some individuals chose to conceal their use of AI due to fears that their abilities would be judged by others. For instance, SP85, who used ChatGPT\""}, {"title": "4 STUDY 2 EXPERIMENT: HYPOTHESES", "content": "Our first survey yielded qualitative insights around the scenarios in which users hide their use of LLMs and the underlying reasons for such behavior. Based on the survey results, we conduct a controlled experiment to quantitatively investigate the effect of task types and individual differences on the propensity for covert LLM usage, and the mediation effects of users' internal and perceived external judgments. We formally introduce the hypotheses below."}, {"title": "4.1 Intentions of Secret Use of LLMs", "content": "Previous research has identified two primary forms of secrecy behaviors: concealment by an act of omission (withholding information) and concealment by an act of commission (actively deceiving or providing false information) [12, 45, 50, 69]. In our context, the secret use of LLMs may also be exhibited in these two forms. Therefore, to provide a more comprehensive understanding about factors that affect the intentions of secret use of LLMs, we measure two variables representing different levels of concealment behavior in our experiment: passive non-disclosure and active concealment. In our study, we defined passive non-disclosure as not actively disclosing the use of LLMs while being honest about it when asked. And active concealment refers to the behavior of actively hiding the use of LLMs, even when directly inquired about it."}, {"title": "4.2 Effects of Task Types on the Intentions of Secret Use of LLMs", "content": "Our qualitative results about the scenarios of secret use of LLMs (see 3.4.1) suggest that certain types of tasks are associated with individual users' secret use of LLMs. This observation leads to the"}, {"title": "H1", "content": "Different task types cause different levels of intentions to hide the use of LLMs."}, {"title": "4.3 Effects of Individual Differences on the Intentions of Secret Use of LLMS", "content": "Prior studies have indicated the potential influence of individual differences on the intention of secrecy [53, 80]. In our experiment, we build upon the findings from the first survey and prior literature to establish hypotheses about how individual differences influence individual users' intentions of secret use of LLMs.\nSelf-esteem. Survey results about causes of secret use of LLMs introduced two main categories of self-reported reasons: internal judgment and perceived external judgment (see 3.4.2). The way individuals construct these judgments and their response to them, which in turn influences their behavior, can be potentially affected by their level of self-esteem [52]. For example, individuals with high self-esteem may feel more confident and less concerned about external judgments, potentially making them more open to disclosing their use of LLMs. Therefore, we hypothesize that:"}, {"title": "H2.1", "content": "Users' level of self-esteem affects their intentions to hide the use of LLMs."}, {"title": "Self-efficacy", "content": "Judgments about users' capabilities are featured in the self-reported reasons for con- cealing the use of LLMs (see 3.4.2), encompassing both internal (questioning users' own competence because of the need of AI) and external perspectives (fear of being critiqued by others for using AI assistance). These types of judgments are intrinsically linked to an individual's self-efficacy [6]. For instance, individuals with higher self-efficacy might feel confident in their abilities, reducing the need for concealment of AI usage. Therefore, we hypothesize that:"}, {"title": "H2.2", "content": "Users' level of self-efficacy affects their intentions to hide the use of LLMs."}, {"title": "Prosocialness", "content": "Prosocial users, who are more inclined towards societal welfare [13], may view the disclosure of their LLM usage as a contribution to the \u201cgreater good\u201d in promoting responsible Al usage. However, they might also balance this against potential ethical judgments from others. This interplay suggests that one's level of prosocialness could be a potential factor influencing their decision-making process regarding the disclosure of AI usage. To explore this, we hypothesize that:"}, {"title": "H2.3", "content": "Users' level of prosocialness affects their intentions to hide the use of LLMs."}, {"title": "General privacy concern", "content": "General privacy concerns often revolve around the control and unau- thorized use of personal data [59]. While the concerns about being discovered using LLMs do not directly pertain to data privacy, it relates to the revelation of the fact that AI is being used. Recognizing this nuanced aspect of privacy, we are interested in exploring how general privacy concerns impact the intention to hide the use of LLMs. Hence, we hypothesize that:"}, {"title": "H2.4", "content": "Users' level of general privacy concern affects their intentions to hide the use of LLMs."}, {"title": "Demographics", "content": "A large body of research has identified the influence of demographic factors on people's intentions to perform secrecy behaviors [80], ethical technology use behaviors [53],"}, {"title": "H2.5", "content": "Users' demographic factors (gender, age, LLM use frequency, and education) affect their intentions to hide the use of LLMs."}, {"title": "4.4 Mediation Effects of Users' Internal Judgement and Perceived External Judgement about the LLM Usage", "content": "The reasons users self-reported for concealing their use of LLMs fundamentally reflect their percep- tions regarding the secret usage behavior (internal judgement and perceived external judgement), as discussed in Section 3.4.2. According to the Self-Discrepancy Theory, when there are discrepancies between a person's actual self (e.g., using LLMs for a task) and their ideal or ought self (e.g., their negative internal judgment or perceived external judgement), negative emotions are elicited. This can significantly influence people's behaviors [60, 67]. Following the investigation of the impact of each independent variable, we want to further understand the causal mechanisms behind the relationship between the independent variables and the outcome variable - the secrecy behavior. Specifically, we focus on the mediation effect of the internal judgement and perceived external judgement, to examine how task types and individual differences influence individuals' intentions to hide the use of LLMs through these two categories of internal beliefs. More formally, we propose the following hypothesis:"}, {"title": "H3.1", "content": "Internal judgement mediates the relationship between the independent variables (i.e., task types and individual differences) and the intentions to hide the use of LLMs."}, {"title": "H3.2", "content": "Perceived external judgement mediates the relationship between the independent variables (i.e., task types and individual differences) and the intentions to hide the use of LLMs."}, {"title": "5 STUDY 2 EXPERIMENT: METHODOLOGY", "content": "To test the hypotheses about factors that influence individual users' intention of secret use of LLMs, we conducted a randomized between-subjects online experiment (N = 300), using Prolific for recruitment. The sample size was determined before the formal study based on the power analysis results ($\\alpha$ = 0.05, power > 0.8, the effect size $f^2$ = 0.03). The effect size $f^2$ was estimated using pilot study data (N = 76), indicating a small to medium impact according to Cohen's criteria [17]. The questionnaire was hosted on Qualtrics. The experiment was conducted in December 2023 and January 2024. Our study has been reviewed and approved by our institution's IRB."}, {"title": "5.1 Experiment Design", "content": "Our experiment followed a between-subjects design, in which participants were randomly assigned into six conditions, including a baseline condition \u201cGeneral information search\u201d, as well as five experimental conditions based on the five task themes identified from our first study (Section 3.4.1). The selection criteria for the baseline condition include that 1) it is a common LLM use case [94]; 2) it was not mentioned as a secret use case by Study 1 participants. Each condition presents different hypothetical scenarios where users have used LLMs for different tasks. The task description shown to the participants can be found in Appendix B. These manipulations allow us to test the effects of task types on individual users' intentions to conceal their use of LLMs (H1). Then we gathered data on participants' self-reported perceptions (internal and external judgements) of LLMs based on the"}, {"title": "5.2 Experiment Procedure", "content": "Our experiment consisted of three steps.\nStep 1. Task description and intention questions: Participants first received a hypothetical scenario describing the use of LLMs for tasks, randomly selected from the six conditions (one baseline, five experimental). They were then asked to report their intentions to conceal this usage within the task context. We clarified that the concealment intentions referred specifically to the fact of using LLMs, not the data transmitted during the interactions. After reporting the intentions, they were asked to elaborate on their rationales, which helps us understand their selections.\nStep 2. Questions about the LLM usage: Participants were re-presented with the same hypothet- ical scenario and responded to two sets of questions about their internal and external judgements of LLM usage in that context. To minimize order bias, question order was randomized at both the group level and within each group. We inserted an attention check question after all other questions (\"Please select [Strongly agree] to show you are paying attention to this survey.\u201d). If they selected other answers, their responses would be excluded from our final dataset. In the second page, participants were asked whether they have similar experiences. For those who had similar experiences, an open-ended question would be provided at the next page that allowed participants to freely share their similar experience, choice on the concealment, and the reasons. To prevent users' responses from being influenced by later questions, turning back to the previous pages was not allowed at any stage of the whole questionnaire.\nStep 3. Questions about individual differences: In this final step, participants completed vali- dated scales measuring self-esteem [71], self-efficacy [75], prosocialness [13], and general privacy concerns [59], presented in a randomized order across different pages. This was followed by demographic questions (age, gender, and education background)."}, {"title": "5.3 Operationalization", "content": ""}, {"title": "5.3.1 Dependent Variables", "content": "We asked participants to report their intentions on two forms of concealment on the use of LLMs on a 7-point likert scale (1 = strongly disagree, 7 = strongly agree) in Step 1 (Section 5.2). After introducing the hypothetical scenario, we asked participants to rate to what extent they agreed or disagreed with the following statements: Passive non- disclosure: \u201cI would not mention that I used an AI chatbot for this task/purpose on my own\u201d. Active concealment: \u201cEven when others ask me, I prefer to hide the use of an AI chatbot for this task/purpose\"."}, {"title": "5.3.2 Independent Variables", "content": "We introduce the operationalization of the two categories of indepen- dent variables, task types, and individual differences.\nTask type: The task type is a categorical variable with six levels, as six task types were selected to create hypothetical use scenarios of LLMs. We chose the \u201cGeneral information search\u201d task type, which is a common use case of LLMs [94] and not mentioned by our survey participants, as the reference level. The other five levels of the variable are \u201ccreative writing\u201d, \u201cacademic writing\u201d, \u201cschool work\u201d, \u201cwork tasks\u201d and \u201csocial connection\u201d, corresponding to the types of tasks identified from our qualitative analysis of the survey results (Section 3.4.1).\nSelf-esteem: Participants' self-esteem was measured using the 10-item Rosenberg Self-Esteem Scale with ten questions that are on a 4-point likert scale [71]. Higher scores in questions 1, 3, 4, 7, 10 contribute higher self-esteem while other questions contribute inversely [71]. To score"}, {"title": "5.3.3 Mediator Variables", "content": "The mediator variables in our experiment are users' internal judgements and perceived external judgements on the use of LLMs. We operationalized them as two latent variables, each explained by three statements about the internal judgements or three statements about the perceived external judgements, respectively. The statements are derived from the reasons for concealing the use of LLMs identified in our first study (Section 3.4.2). Using latent variables allows for evaluation of item contributions, accounts for measurement error, and provides a more accurate depiction of the relationships between constructs, particularly when the relationships between observed variables and underlying constructs are unknown or complex [73, 92]. We asked participants to rate their agreement or disagreement with the six statements using a 7-point Likert scale (1=strongly disagree, 7=strongly agree) in Step 2 (Section 5.2).\nFactor Analysis provides support for our construct validity, showing that two groups of variables have high loadings on their respective factors [11]. The Cronbach's alpha also indicates high reliability of statements within each group in our sample [95]. The two groups of statements are listed as follows.\nInternal judgements on the use of LLMs (Cronbach's alpha = 0.79): \u201cI feel less competent when I need an AI chatbot to assist me with this task\u201d (Factor loading = 0.77); \u201cI have doubts about"}, {"title": "5.4 Participants", "content": "We recruited participants based in the U.S. through Prolific and compensated $1.4 each. The recruitment process involved several steps to obtain a sample of N = 300 LLM users, evenly distributed across six conditions (50 participants per group). First, we included a question about LLM usage frequency (Section 5.3.2) to ensure that only LLM users proceeded to the main study. And then we manually reviewed the responses from the main study to filter out those failing the attention check or providing contradictory responses (e.g., disagreeing with intentions to conceal usage but explaining they would have concealed it in the hypothetical context). We also verified that the quality of all the open-ended responses in the final sample is high. As exclusions occurred, we adjusted the randomizer settings to maintain an even distribution across the six conditions. The recruitment process continued to roll until 300 valid responses were reached (50 in each condition). Meanwhile, we employed the balanced sample distribution mode in Prolific for a balanced gender distribution. Ultimately, we received 318 responses, of which 18 were excluded, resulting in a final valid sample of 300 participants. The summary of the demographics of our experiment sample (N = 300) is shown in the Appendix A."}, {"title": "5.5 Methodological Limitations", "content": "Our use of hypothetical scenarios as the background context may introduce some limitations. Some participants might not have real-life experience with the specific tasks scenario. For example, a junior college student might not have worked in professional work context. And some participants might not use LLMs in the same scenario in real life. To mitigate those problems raised from hypothetical scenario, we selected tasks that are generally common or easy to imagine. Additionally, the participant's perceptions about the hypothetical scenarios may be sensitive to the wording of the description. To mitigate the confounding impact on the judgement of different conditions, we have ensured that the descriptions are at similar length and levels of detail. We also want to note that the hypothetical scenarios may not represent the way the task in the corresponding group is performed in real life, particularly for users who might choose not to use LLMs in the given scenarios. We tried to mitigate the potential bias by comparing results across different groups all using the hypothetical scenarios. However, the impact on the secret use rate of each scenario remains unknown. Future studies could consider including users' willingness to use LLMs as a potential factor in their decision-making regarding secret use, and examine how users commonly perform these tasks in real life. Lastly, cognitive dissonance regarding LLM usage in the scenarios could also influence responses. For example, if a participant"}]}