{"title": "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory (Non-)Compliance", "authors": ["Georgi Ganev"], "abstract": "Synthetic tabular data, or data generated by machine learning generative models, is gaining popularity beyond academia and moving into real-world deployments. Examples include releasing public census data by US (Abowd et al., 2022), UK (UK ONS, 2023), and Israel (Hod & Canetti, 2024), as well as sharing sensitive financial and health data through private synthetic data vendors (UK ICO, 2023; Microsoft, 2024). While these releases satisfy a formal definition of privacy, i.e., Differential Privacy (DP) (Dwork et al., 2006), this is still not the norm in numerous scientific papers (Park et al., 2018; Lu et al., 2019; Zhao et al., 2021; Borisov et al., 2023; Yoon et al., 2023; Kotelnikov et al., 2023; Zhang et al., 2024) and leading synthetic data vendors (Mostly AI, 2020; Syntegra, 2021; Panfilo & Aindo, 2022; Syntho, 2023). Instead, the papers/companies rely entirely on empirical ad-hoc privacy metrics based on the similarity between synthetic and real personal datasets.\nMain Question. This prompts asking: \"Is using similarity-based privacy metrics enough to consider synthetic data regulatory compliant?\" Due to their fundamental issues and unreliable, inconsistent nature, we argue that it is not.", "sections": [{"title": "1. Motivation", "content": "Synthetic tabular data, or data generated by machine learning generative models, is gaining popularity beyond academia and moving into real-world deployments. Examples include releasing public census data by US (Abowd et al., 2022), UK (UK ONS, 2023), and Israel (Hod & Canetti, 2024), as well as sharing sensitive financial and health data through private synthetic data vendors (UK ICO, 2023; Microsoft, 2024). While these releases satisfy a formal definition of privacy, i.e., Differential Privacy (DP) (Dwork et al., 2006), this is still not the norm in numerous scientific papers (Park et al., 2018; Lu et al., 2019; Zhao et al., 2021; Borisov et al., 2023; Yoon et al., 2023; Kotelnikov et al., 2023; Zhang et al., 2024) and leading synthetic data vendors (Mostly AI, 2020; Syntegra, 2021; Panfilo & Aindo, 2022; Syntho, 2023). Instead, the papers/companies rely entirely on empirical ad-hoc privacy metrics based on the similarity between synthetic and real personal datasets.\nMain Question. This prompts asking: \"Is using similarity-based privacy metrics enough to consider synthetic data regulatory compliant?\" Due to their fundamental issues and unreliable, inconsistent nature, we argue that it is not."}, {"title": "2. Definitions", "content": "Synthetic Data. We denote a real personal dataset as $D$. A generative model, $G$, is trained on $D_{train}$ (a subset of $D$; the remaining data, $D_{test}$, is set aside for test purposes) to capture a probability representation, and could later be sampled to generate new (synthetic) data $D_{synth}$ of arbitrary size (see bottom of Fig. 1). Popular generative models include Graphical Models (Zhang et al., 2017; McKenna et al., 2021), GANs (Xie et al., 2018; Jordon et al., 2019; Xu et al., 2019), Diffusion Models (Kotelnikov et al., 2023; Zhang et al., 2024), and Transformers (Borisov et al., 2023).\nSimilarity-based Privacy Metrics (SBPMs). The intuition behind SBPMs is that $D_{synth}$ should be representable and close to $D_{train}$, but not closer than to $D_{test}$ (Platzer & Reutterer, 2021; Mobey Forum, 2022). More precisely, the closest pairwise distances $d_{synth} = d(D_{train}, D_{synth})$ and $d_{test} = d(D_{train}, D_{test})$ are computed and their distributions compared through a statistical test (see Fig. 1). The passing criterion is a comparison between a simple statistic run on each distribution, e.g., average/5th percentile (p5). In this paper, we focus on the three most widely used SBPMs by scientific papers and synthetic data vendors. Finally, $D_{synth}$ is deemed private if all three privacy tests pass (Mostly AI, 2020; Panfilo & Aindo, 2022).\nIdentical Match Share (IMS) calculates the proportion of exact copies (statistic: average; test: $d_{synth} \\leq d_{test}$).\nDistance to Closest Records (DCR) calculates the distances to their nearest neighbor in $D_{train}$ (statistic: p5; test: $d_{synth} \\geq d_{test}$). DCR is meant to protect against scenarios where $D_{train}$ is slightly perturbed and passed as $D_{synth}$.\nNearest Neighbor Distance Ratio (NNDR) follows DCR but divides the distances by the distance to their second neighbor (statistic: p5; test: $d_{synth} \\geq d_{test}$). The relative computations are supposed to further protect the outliers.\nGDPR. EP and Council (2016a) define personal data as \"any information relating to an identified or identifiable living individual.\" Also, EP and Council (2016b) state that effectively anonymized information is not considered personal data and is exempt from data protection regulations. Creating synthetic data from real personal data naturally involves processing it, so whether the result is personal or anonymous depends on the identifiability risk assessment."}, {"title": "3. Fundamental Issues of SBPMs", "content": "We identify several fundamental issues with using SBPMs to reason about privacy through empirical pass/fail tests.\nNo Theoretical Guarantees. SBPMs lack a defined threat model or strategic adversary, which ignores essential security (Anderson, 2020) and regulatory principles like the motivated intruder test. Instead, they rely on arbitrarily chosen average-case statistics and held-out datasets, falling into the \"Generalization Implies Privacy\" fallacy (Del Grosso et al., 2023), where generalization is average-case issue but privacy is a worst-case. Thus, even if a model passes all tests and generalizes, it can still memorize data (Song et al., 2017). Consequently, SBPMs offer no theoretical guarantees and are vulnerable to adversarial attacks. Moreover, it is unclear whether or how the SBPMs correspond to the two technical risks \u2013 singling out and linkability."}, {"title": "4. SBPMs Counter-Examples", "content": "We present three counter-examples showing the unreliability and inconsistency of SBPMs. For all of them, we use a toy dataset, 2d Gauss, which consists of 2,000 points (split evenly between $D_{train}$ and $D_{test}$) drawn from a standard normal 2d distribution with no correlation (see Fig. 2(a)). Approximately 10% of the records, those outside the blue circle, are considered outliers."}, {"title": "5. Possible Countermeasures", "content": "We discuss and disprove the efficacy of three intuitive solution intended to overcome the limitations of SBPMs.\nDP Generative Models. The established framework to limit the ability of an attacker to exploit privacy leakage from trained models is to train them while satisfying DP. Inspired by product deployments by synthetic data providers, we assume access to a single DP trained generative model and unperturbed metrics (per generation run).\nIn this scenario, although the likelihood of the model memorizing and reproducing real data records would be reduced, leakage can still occur when multiple $D_{synths}$ are released along with the metrics. The leakage comes from the privacy metrics themselves; since they require access to $D_{train}$ and are deterministic (ruling out plausible deniability), they compromise the end-to-end DP pipeline. Strategic and motivated adversaries could exploit this vulnerability to reconstruct real data records (Ganev & De Cristofaro, 2023). Adding additional privacy mechanisms on top of the metrics is unlikely to mitigate the problem, as the overall privacy integrity of the entire system needs to be carefully considered.\nDP-fying the Metrics. Another possible solution could be to apply DP to the metrics. However, this would not be robust. Implementing DP to the metrics would require additional privacy budget for each generation run, which contradicts one of the main claimed advantages of adopting synthetic data: the ability to generate unlimited data.\nDisabling Metrics Access. Finally, not disclosing the privacy metrics while still conducting statistical pass/fail tests would create significant issues. First, users and customers would have to blindly trust the provider that the synthetic data meet a certain threshold. Second, it would undermine a key selling point of providers: providing a tangible measure of compliance, essential for the product's transparency and explainability. Additionally, with statistical pass/fail tests, sensitive information could still be vulnerable to present/future privacy attacks."}, {"title": "6. Conclusion", "content": "In this paper, we argue that SBPMs cannot ensure regulatory compliance of synthetic data. SBPMs do not protect against singling out and linkability and, among other fundamental issues, completely ignore the motivated intruder test.\nIn (Ganev & De Cristofaro, 2023), we discuss further fundamental issues of SBPMs, provide more SBPMs counter-examples, propose a novel reconstruction attack, which is capable of recovering the majority of $D_{train}$ outliers, and argue that training DP generative models without access to SBPMs addresses their issues (note that DP comes with its own disadvantages, which we also discuss).\nEmpirical Evaluations. Privacy attacks and empirical evaluations should not be overlooked as they play a crucial role in identifying flaws, errors, and bugs in algorithms and implementations. They contribute significantly to model auditing (Jagielski et al., 2020; Nasr et al., 2023; Annamalai et al., 2024; Ganev et al., 2024) and improve the interpretability of theoretical privacy protections (Houssiau et al., 2022a;b)."}]}