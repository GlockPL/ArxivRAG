{"title": "Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs", "authors": ["Ronit Singhal", "Pransh Patwa", "Parth Patwa", "Aman Chadha", "Amitava Das"], "abstract": "Given the widespread dissemination of misinformation on social media, implementing fact-checking mechanisms for online claims is essential. Manually verifying every claim is highly challenging, underscoring the need for an automated fact-checking system. This paper presents our system designed to address this issue. We utilize the Averitec dataset (Schlichtkrull et al., 2023) to assess the veracity of claims. In addition to veracity prediction, our system provides supporting evidence, which is extracted from the dataset. We develop a Retrieve and Generate (RAG) pipeline to extract relevant evidence sentences from a knowledge base, which are then inputted along with the claim into a large language model (LLM) for classification. We also evaluate the few-shot In-Context Learning (ICL) capabilities of multiple LLMs. Our system achieves an 'Averitec' score of 0.33, which is a 22% absolute improvement over the baseline. All code will be made available on All code will be made available on https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms.", "sections": [{"title": "1 Introduction", "content": "The proliferation of fake news and misinformation on social media platforms has emerged as a significant contemporary issue (Panke, 2020). False online claims have, in some cases, incited riots (Lindsay and Grewar, 2024) and even resulted in loss of life (Kachari, 2018). This problem is particularly amplified during critical events such as elections (Bovet and Makse, 2019) and pandemics (Karimi and Gambrell, 2020; Bae et al., 2022; Morales et al., 2021). Given the vast volume of online content, manually fact-checking every claim is impractical. Therefore, the development of an automated fact verification system is imperative. Moreover, simply assigning a veracity label is inadequate; the prediction must be supported by evidence to ensure the system's transparency and to bolster public trust. Although recent solutions have been proposed (Patwa et al., 2021a; Capuano et al., 2023), the problem remains far from resolved and requires further research efforts.\nIn this paper, we present our system for automated fact verification. Our system classifies a given textual claim into one of four categories: Supported, Refuted, Conflicting Evidence/Cherrypicking, or Not Enough Evidence. Additionally, it provides supporting evidence for the classification. Our approach leverages recent advancements in Large Language Models (LLMs), specifically Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL), to produce evidence-backed veracity predictions. Given a claim and a collection of documents, our system first employs a RAG pipeline to retrieve the three most relevant documents and extract evidence from them. Subsequently, we utilize ICL to determine the veracity of the claim based on the extracted evidence. We evaluated our system on the Averitec dataset (Schlichtkrull et al., 2023), where it outperforms the official baseline by a large margin. Our key contributions are as follows:\n\u2022 We develop a system for automated fact verification that integrates RAG with ICL to provide evidence-based classifications.\n\u2022 Our proposed system requires only a minimal number of training samples, thereby eliminating the need for a large manually annotated dataset.\n\u2022 We conduct experiments with various recent LLMs and provide a comprehensive analysis of the results."}, {"title": "2 Related Work", "content": "Recently, there has been increased research interest in fake news detection and fact checking. Glazkova et al. (2021) proposed and ensemble of BERT (Devlin et al., 2019) for Covid fake news (Patwa et al., 2021b) detection. Harrag and Djahli (2022) employ deep learning techniques for fact checking in Arabic (Baly et al., 2018). (Song et al., 2021) tackle the problems of fake news detection using graph neural networks. The factify tasks (Mishra et al., 2022; Suryavardan et al., 2023b) aim to detect multi-modal fake news. However, these systems only provide the veracity prediction without any evidence.\nOn the fever dataset (Thorne et al., 2018), Krishna et al. (2022) design a seq2seq model to generate natural logic-based inferences as proofs, resulting SoTA performance on the dataset. Schuster et al. (2021) release the VitaminC dataset and propose contrastive learning for fact verification. Hu et al. (2022) propose a DRQA retriever (Chen et al., 2017) based method for fact checking over unstructured information (Aly et al., 2021). These systems provide evidence or explanation to back their predictions but they test the veracity of synthetic claims whereas we test real claims.\nSome researchers have also used LLMs to tackle the problem. Kim et al. (2024) leverage multiple LLMs as agents to enhance the faithfulness of explanations of evidence for fact-checking. Zhang and Gao (2023) design a hierarchical prompting method which directs LLMs to separate a claim into several smaller claims and then verify each of them progressively.\nThere have also been attempts to solve the problem using RAG. Khaliq et al. (2024) utilize multimodal LLMs with a reasoning method called chain of RAG to provide evidence based on text and image. Deng et al. (2024) propose a method to decrease misinformation in RAG pipelines by re-ranking the documents during retrieval based on a credibility score assigned to them. Similar to these systems, we also use RAG and LLMs in our solution.\nFor more detailed surveys, please refer to Thorne and Vlachos (2018); Kotonya and Toni (2020); Guo et al. (2022)."}, {"title": "3 Data", "content": "We utilize the Averitec dataset (Schlichtkrull et al., 2023) for fact-checking purposes. This dataset comprises claims accompanied by a knowledge store (a collection of articles). Each claim is annotated with question-answer pairs that represent the evidence, a veracity label, and a justification for the label. The veracity label can be one of the following: Support (S), Refute (R), Conflicting Evidence/Cherrypicking (C), or Not Enough Evidence (N). A claim is labeled as C when it contains both supporting and refuting evidence. The data distribution, as shown in Table 1, indicates a class imbalance favoring the R class, while the C and N classes have relatively few examples. The final testing was conducted on 2,215 instances. For further details on the dataset, please refer to (Schlichtkrull et al., 2023).\nOn average, each claim contains 17 words. Figure 1 presents a word cloud of the claims, revealing that most claims pertain to topics related to politics and COVID-19."}, {"title": "4 Methodology", "content": "Given a claim and a knowledge store, our system is comprised of three key components: relevant document retrieval, evidence extraction from the documents, and veracity prediction based on the extracted evidence. The first two components form our Retrieval-Augmented Generation (RAG) pipeline."}, {"title": "4.1 Document Retrieval Using Dense Embeddings", "content": "In the document retrieval phase, it is essential to match claims with relevant documents from a knowledge store (in our case, the knowledge store consists of documents provided in the dataset, though it could be replaced with documents retrieved via a search engine). To facilitate this, all documents are first transformed into dense vector embeddings using an embedding model. Since our knowledge store is static, this transformation is a one-time process. The claim in question is then converted into embeddings using the same model.\nOnce the claim is embedded, we utilize Facebook's FAISS (Facebook AI Similarity Search) (Douze et al., 2024) to conduct a nearest-neighbor search within the knowledge store. FAISS is an efficient library for similarity search and clustering of dense vectors. We configured FAISS to retrieve the top three documents most relevant to the claim."}, {"title": "4.2 Evidence Extraction Using LLMs", "content": "After identifying the top three relevant documents, the next step involves extracting evidence supported by these documents. This process consists of two steps:\nQuestion Generation: The claim is transformed into a question that challenges its validity using an LLM. We employ In-Context Learning, which enables the model to generate responses based on a few provided examples, aiding in the creation of nuanced and contextually appropriate questions. The prompt is designed to ensure that the generated question challenges the claim's veracity rather than simply seeking a factual answer. An example prompt is provided in Figure 3.\nAnswer Generation: After generating the question, we provide a single document to a LLM and pose the question. The LLM is prompted to deliver concise and definitive answers derived directly from the content of the document. An example of this prompt is illustrated in Figure Y. This process is repeated for each of the three documents, resulting in three distinct answers for each claim. These answers collectively constitute our evidence. It is important to note that, in our experiments, the LLM used for answer generation does not necessarily need to be the same as the one used for question generation. The prompt utilized in this step is similar to the one depicted in Figure 2."}, {"title": "4.3 Few-Shot ICL for Final Classification", "content": "For the final veracity prediction, we use an LLM to classify a claim based on the three pieces of evidence extracted earlier. The LLM is prompted to choose one out of the four possible classes. The prompt is designed to guide the model through the classification process, ensuring that it correctly interpreted the relationship between the claim and the evidence. An example prompt is given in figure 4.\nOur methodology aligns with recent advancements in retrieval-augmented generation (RAG) models which alleviate hallucination and ICL methods, which have been shown to improve the accuracy of LLMs. The integration of these state-of-the-art methods is an attempt to ensure that the extracted evidence is both relevant, contextually appropriate and for validating the claims accurately."}, {"title": "5 Experiments", "content": "To convert documents into dense embeddings, we utilize the dunzhang/stella_en_1.5B_v5 model\u00b9. This model was chosen because, at the time of our experiments, it was ranked first on the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2022), and it currently holds the second position.\nFor all LLMs used in our experiments, we employ their 4-bit quantized versions via Ollama\u00b2. This quantization enables us to load larger LLMs onto our GPUs.\nFor question generation, we use the Phi-3-medium model (Abdin et al., 2024). The temperature is set to 0, and greedy decoding is used to ensure that the answers are as factual as possible and to minimize hallucinations.\nFor answer generation and final classification, we experiment with multiple LLMs of varying sizes, including InternLM2.5 (Cai et al., 2024), Llama-3.1 (Dubey et al., 2024), Phi-3-medium (Abdin et al., 2024), Qwen2 (Yang et al., 2024), and Mixtral (Jiang et al., 2024). These models were selected based on their performance on the Open LLM Leaderboard (Fourrier et al., 2024) and their availability through Ollama.\nWe utilize an A40 GPU for Mixtral, while all other models are run on an A100 GPU. Our best-performing model, Mixtral, requires an average of 2 minutes for evidence extraction and final prediction. All code will be made available on https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms."}, {"title": "5.1 Evaluation Metrics", "content": "The evaluation metrics used ensure that credit for a correct veracity prediction is given only when the correct evidence has been identified.\nTo evaluate how well the generated questions and answers align with the reference data, the pairwise scoring function METEOR (Banerjee and Lavie, 2005) is used. The Hungarian Algorithm (Kuhn, 1955) is then applied to find the optimal matching between the generated sequences and the reference sequences. This evidence scoring method is referred to as Hungarian METEOR. The system is evaluated on the test set using the following metrics:\n\u2022 Q only: Hungarian METEOR score for the generated questions.\n\u2022 Q + A: Hungarian METEOR score for the concatenation of the generated questions and answers.\n\u2022 Averitec Score: Correct veracity predictions where the Q+A score is greater than or equal to 0.25. Any claim with a lower evidence score receives a score of 0."}, {"title": "6 Results and Analysis", "content": "Table 2 provides a summary of the performance of various models on the development set. The Mixtral 8*22B model (Jiang et al., 2024) achieves the highest Averitec score, while the Llama 3.1 model (Dubey et al., 2024) attains the highest accuracy. These findings indicate that model performance generally improves with increasing model size. Moreover, the relative rankings of these models on the development set differ from their positions on the Open LLM leaderboard (Fourrier et al., 2024), suggesting that superior performance on the Open LLM leaderboard does not necessarily correlate with better performance in the fact verification task."}, {"title": "6.1 Class-wise Performance", "content": "Table 4 presents the class-wise performance of our top three models on the development set. Across all models, the Refuted class emerges as the easiest to predict, while the \"Not Enough Evidence\" and \"Conflicting Evidence/Cherrypicking\" classes present greater challenges. Notably, no single model excels across all classes. Although Mixtral achieves the highest macro F1 score, it is not the top-performing model for any individual class. Qwen2 surpasses the other models in performance across all classes except Refuted. This suggests that exploring ensemble techniques could be a valuable direction for future research.\nFigure 5 illustrates the confusion matrix of Mixtral 8*22B on the development set. It reveals that both the N and C classes are equally likely to be misclassified as the R and S classes. Additionally, there is significant confusion between the S and R classes, highlighting the inherent difficulty of fact verification."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we have introduced our system for evidence-supported automated fact verification. Our system - based on RAG and ICL \u2013 requires only a minimal number of training examples to extract relevant evidence and make veracity predictions. We observed that all LLMs demonstrate sub-optimal performance on the \"Conflicting Evidence/Cherrypicking\" and \"Not Enough Evidence\" categories, which emphasizes the inherent challenges of these categories. Additionally, no single LLM consistently outperforms others across all categories. Our system achieved an Averitec score of 0.33, highlighting the complexity of the problem and indicating a substantial potential for future improvement\nFuture research could involve fine-tuning the LLM using parameter-efficient fine-tuning (PEFT) techniques (Liu et al., 2022; Patwa et al., 2024) and improving performance through the use of ensemble techniques (Mohammed and Kora, 2022). Extending the system to include multi-modal fact verification (Patwa et al., 2022; Suryavardan et al., 2023a) also represents an interesting direction for further investigation."}, {"title": "8 Limitation", "content": "As we are using few-shot ICL, our system cannot make use of large annotated datasets if available, because of the limitation of the prompt size. Furthermore, we assume the availability of high quality LLMs, which might not be the case for some low-resource languages."}, {"title": "9 Ethical Statement", "content": "LLMs are prone to hallucination. In our case, the extracted evidence could be incorrect due to hallucination. Furthermore, the prompts can be tweaked to intentionally generate wrong evidence or predictions. We caution the reader to be aware of such issues and to not misuse the system."}]}