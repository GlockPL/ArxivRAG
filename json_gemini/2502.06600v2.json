{"title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?", "authors": ["Gon\u00e7alo Gomes", "Chrysoula Zerva", "Bruno Martins"], "abstract": "The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.", "sections": [{"title": "1 Introduction", "content": "Computer-generated image captions are nowadays commonly used as descriptive annotations. The Image Captioning (IC) task has been extensively studied, including in multilingual settings, with many recent approaches combining established vision encoders with large language model decoders (Ramos et al., 2023c,a,b; Yang et al., 2023; Geigle et al., 2023; Ramos et al., 2024). The automatic evaluation of captions, accounting for linguistic fluency and alignment with visual contents, has also witnessed a significant effort. Approaches like CLIPScore (Hessel et al., 2021) have been proposed to evaluate captions through cosine similarity between image and text embeddings, leveraging large-scale pre-trained vision-and-language models and achieving high correlations with human judgments. Still, despite the many recent advancements, most approaches are English-centric, while multilingual image captioning evaluation has remained relatively unexplored, and lacking in resources.\nThis work explores the use of CLIPScore in multilingual captioning evaluation. Given the lack of available benchmarks for the evaluation of multilingual captioning metrics, we propose two different evaluation strategies: (1) using a combination of Machine Translation (MT) and Quality Estimation (QE) models to obtain high-quality multilingual data from English-centric benchmarks, and (2) re-purposing multilingual benchmarks originally designed for evaluation of the semantic inference and reasoning capabilities of vision-language models.\nThrough extensive experiments, we show that multilingual CLIP models achieve comparable or even better performance on English benchmarks, while allowing for multilingual assessments. We also propose a multilingual finetuning strategy for CLIPScore, that allows to account for linguistic and cultural diversity while learning from human judgements, resulting in further performance improvements. Performance generally increases according to model size, and larger models, trained on more data, attained similar or even better performance to methods that extended the original CLIPScore (Sarto et al., 2023; Kim et al., 2022; Hu et al., 2023; Kim et al., 2023; Narins et al., 2024; Wada et al., 2024). Tests with machine-translated data show that multilingual CLIPScore can also maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to high-quality assessments across languages.\nOur study highlights the importance of multilingual and multicultural research in the evaluation of image captions, aiming to inspire the development of multilingual frameworks in this domain. We show that multilingual models, trained on equally sized datasets, perform just as well as English-only models on English assessments, while also excelling across various evaluation tasks that involve multilingual and multicultural data, making them far more versatile and valuable assets than the corresponding English-centric models.\nIn sum, our main contributions include (i) a fine-tuning strategy that accounts for linguistic and cultural diversity, as well as alignment with human judgements (Section 3); (ii) an MT-based extension of English-centric benchmarks to multiple languages, incorporating human evaluations and diverse linguistic phenomena, while preserving the original benchmarks' quality (Sections 4.3.2 and 4.3.3); and (iii) an adaptation of existing multilingual and multicultural datasets for captioning metric evaluation (Section 4.3.3). The code and adapted datasets supporting our evaluation experiments are available in a public GitHub repository\u00b9."}, {"title": "2 Related Work", "content": "Conventional image captioning evaluation has relied on reference-based assessments, where machine-generated captions are compared against human-generated ones (i.e., the references). Frequently used metrics such as BLEU or CIDEr (Vedantam et al., 2015) rely on lexical matches, and hence may fail to capture fine nuances and semantic overlaps in rich captions. A recent shift in the evaluation paradigm involves the use of learned vision-and-language models to enable evaluation through reference-free metrics.\nThe CLIPScore metric (Hessel et al., 2021) was one of the first proposals for evaluating image captions that departed from traditional metrics. Grounded in a vision-and-language encoder, specifically the original Contrastive Language-Image Pre-training (CLIP) model (Radford et al., 2021), this strategy employs a modified cosine similarity between representations for the input image and the caption under evaluation. CLIPScore exhibits a high correlation with human judgments across various datasets, and despite being a reference-free metric, it even surpasses established reference-based metrics like BLEU and CIDEr. Today, CLIPScore-based metrics are widely used for evaluating image captions, and have inspired the development of numerous new evaluation metrics that build on CLIP (Sarto et al., 2023; Hu et al., 2023; Kim et al., 2022), including reference-based variants.\nDespite the advancements, recent studies have noted that CLIPScore can lack rating granularity, emphasising the need for better benchmark datasets to evaluate image captioning metrics (Ahmadi and Agrawal, 2024). Datasets like VICR (Narins et al., 2024) or Polaris (Wada et al., 2024) have employed more rigorous methods for collecting human ratings, but remain limited to English. Alongside these datasets, researchers have shown that models specifically trained for image captioning evaluation can slightly outperform CLIPScore. We corroborate these findings by proposing a finetuned version of CLIPScore that surpasses previous variants.\nRecent studies emphasize the importance of evaluating visio-linguistic grounding capabilities with more complex linguistic constructs (Parcalabescu et al., 2022). Additionally, the absence of multilingual and multicultural benchmarks has been a key topic of discussion. For instance, Kim et al. (2023) proposed a multilingual image captioning evaluation method. They finetuned the CLIP text encoder using a language-agnostic approach to distinguish between original and perturbed text. Moreover, the authors introduced a new dataset aimed at evaluating multilingual captioning metrics, although it has not yet been made publicly available.\nThe lack of linguistic and cultural diversity in vision-language datasets has been further criticized by Liu et al. (2021), who argued that relying on English-based lexical databases and image queries introduces a North American and Western European bias, since existing vision-linguistic datasets often fail to capture concepts relevant to non-English languages and cultures outside of Europe and North America. To mitigate this limitation, the authors built MaRVL, which proposes a new protocol for constructing an ImageNet-style class hierarchy that better reflects diverse languages and cultures, together with textual descriptions reflecting these classes. They benchmarked several multilingual multimodal models, but these often performed just above chance, highlighting the challenges of handling out-of-distribution concepts and images. The results call for a reassessment of model robustness and accuracy, and for the development of more inclusive systems.\nSimilarly, Bugliarello et al. (2022) highlighted the importance of multilingual benchmarks, noting that vision-and-language research has primarily focused on English tasks. To address this, the authors proposed a multilingual dataset for evaluating vision-language inference, where models predict entailment relationships between a textual hypothesis and an image premise, expanding the scope of evaluation to include diverse languages."}, {"title": "3 Multilingual CLIPScore", "content": "The CLIPScore metric uses an adjusted cosine similarity to compare representations for the input image and the caption being assessed, as originally described by Hessel et al. (2021). In our work, we adopted the original formulation. A more detailed explanation of the CLIPScore and RefCLIPScore metrics can be found in Appendix A.\nTo boost performance across languages, we propose a strategy to finetune multilingual CLIP models in a setting that considers both linguistic and cultural diversity, while accounting for human preference alignment. Two distinct datasets were used for finetuning. The first, CrossModal-3600, focuses on multilingual captions and multicultural imagery (Thapliyal et al., 2022), whereas the second, VICR, comprises English image-caption pairs that are evaluated by humans (Narins et al., 2024), and which we machine translated to different languages following a strict quality-aware translation scheme to help maintain high quality. We also propose the combination of different training objectives tailored to the specific characteristics of each of the two datasets.\nIn more detail, to enhance the model's ability to process multilingual and multicultural instances, we finetuned it on both datasets using the original CLIP contrastive loss, which can be formally described as follows:\n$L_c = - \\frac{1}{2N}\\sum_{i=1}^N \\left[ \\log \\frac{e^{s_{i,i}/\\tau}}{\\sum_{j=1}^N e^{s_{i,j}/\\tau}} + \\log \\frac{e^{s_{i,i}/\\tau}}{\\sum_{l=1}^N e^{s_{l,i}/\\tau}} \\right]$    (1)\nIn the equation, $N$ is the number of image-text pairs in a batch, $s_{i,j}$ is the similarity score between the i-th image and the j-th text description, and $\\tau$ is a temperature parameter that scales the similarity scores and helps in controlling the concentration level for the distribution of scores.\nFor the second dataset, to improve the alignment of CLIPScore with human ratings, we also considered a Pearson correlation loss:\n$L_p = 1- \\frac{\\sum_{i=1}^n (x_i-\\overline{x}) (y_i-\\overline{y})}{\\|(x-\\overline{x})\\|\\cdot\\|(y-\\overline{y})\\|},$   (2)\nwhere $x$ is the vector of CLIPScore values, $y$ is a vector with the human rating scores, and $\\overline{x}$ and $\\overline{y}$ are the respective average values.\nConsidering that both loss functions can benefit from larger batch sizes, we sample instances for training by alternating between each task, without mixing instances with respect to different losses in the same batch. We accumulate gradients for two steps before updating the network, effectively combining both loss effects while leveraging the benefits of larger batches, i.e., $L \\sim L_C + L_p$."}, {"title": "4 Experimental Evaluation", "content": "This section presents the datasets, the experimental setup, and the results for different CLIP models, considering English, multilingual, and multicultural scenarios for image captioning evaluation."}, {"title": "4.1 Datasets", "content": "To ensure a fair and comprehensive evaluation of our multilingual models, we extended several well-established English-centric datasets to a multilingual scenario. These datasets, originally developed for assessing human judgments in image-captioning tasks, contain one or more human quality assessments for each image-caption pair.\n\u2022 Expert (Flickr8K-Expert), consisting of 5, 664 pairs (Hodosh et al., 2013).\n\u2022 Crowdflower (Flickr8K-CF), with a total of 47, 830 pairs (Hodosh et al., 2013).\n\u2022 Composite, containing with a total of 13, 146 pairs (Aditya et al., 2015).\n\u2022 VICR, with 10,175 training, 2, 310 validation, and 3, 161 test pairs (Narins et al., 2024).\nIn addition to using the aforementioned datasets featuring human ratings, we also evaluated the robustness of our models to various linguistic phenomena using the VALSE dataset (Parcalabescu et al., 2022), which is used to perform evaluation through a binary classification task, and which comprises 6, 704 correct image-caption pairs alongside their foil caption versions.\nWhile the comparison between multilingual and monolingual models on English data offers some insights, it provides a limited perspective on multilingual performance. Unfortunately, high-quality multilingual resources featuring curated human assessments of caption quality are scarce or non-existent, restricting the scope of multilingual evaluation. To overcome this, we developed a translation scheme that leverages large machine translation models (Fan et al., 2021; Alves et al., 2024; Liu et al., 2020), paired with language and translation quality estimation models (Rei et al., 2022, 2023), to translate English captions with pre-existing human assessments into multiple languages. This approach targets high translation quality, filtering out low-quality translations and thus ensuring the validity of human judgments across target languages. Further details about our translation scheme can be found in Appendix D.\nOur language selection is in line with recent machine translation studies (Alves et al., 2024), covering high-resource languages (i.e., English, French, German, Spanish, and Chinese) and also mid- (i.e., Portuguese, Italian, and Russian) to low-resource languages (i.e., Dutch and Korean). We translated both the VICR and VALSE datasets into the nine aforementioned languages using our MT scheme.\nIn addition, to further expand our multilingual evaluation, we used natively multilingual and multicultural datasets, i.e., XVNLI (Bugliarello et al., 2022) and MaRVL (Liu et al., 2021), re-purposing them into classification tasks for the evaluation of image captioning metrics. We also expanded both native multilingual datasets by translating them into English, allowing us to compare a multilingual model with its English-only counterpart."}, {"title": "4.2 Evaluation Metrics", "content": "We evaluate the different models using correlation with human judgements, and also through classification tasks. Regarding the correlation experiments, we measure performance using three different correlation coefficients, namely Spearman p and Kendall $\\tau$ with variations b and c. The correlation metrics are formally defined in Appendix B.\nFor the classification experiments, we measure accuracy under the assumption that a caption entailed by an image should reflect a higher CLIP-Score than a contradiction/foil caption."}, {"title": "4.3 Experiments and Results", "content": "This section presents experimental results for the different models and evaluation datasets, establishing a comparison with previously reported results and contributing to the multi-linguistic exploration of existing models and datasets. We also performed a qualitative study focusing on image-caption pairs that feature concepts that could be associated with cultural bias, which is reported in Appendix G."}, {"title": "4.3.1 Model Selection", "content": "To select the specific CLIP model to be used in our multilingual experiments, following the CLIPScore methodology described by Hessel et al. (2021), we first analysed correlation results between CLIP-Score values and human ratings across four English datasets and considering publicly available CLIP models. Results are reported in Table 1, indicating that CLIPScore estimates improve significantly with larger CLIP models trained on more data, even if the training data is multilingual and the model is tested on English-centric data.\nApple's public ViT-H/14 model, with a 378\u00b2 pixel resolution and trained exclusively on English data, achieves the highest correlation results among the different English models. However, a multilingual model of the same size, trained on a mixture of English and multilingual data from LAION and with significantly less English-specific focus (i.e., laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k), emerges as the second-best alternative.\nBoth models are comparable in size and were trained on 5 billion instances. While Apple's model relied solely on English data, the multilingual LAION model incorporated a diverse mixture of languages. Despite its reduced exposure to English-specific data, the multilingual model delivers competitive performance on English benchmarks, even surpassing Apple's model in several correlation metrics. This indicates that the multilingual training data enabled effective generalization, enhancing the model's capabilities even for tasks involving only English data.\nWe selected the laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k variant for further experiments, as it demonstrated the best performance. From this point forward, we will refer to this multilingual model as MLAION (ML), and to our finetuned version as MLAION-F (MLF).\nAdditionally, experiments detailed in Appendix E demonstrate that simply scaling up model size and training with more diverse data can yield performance on par with more complex captioning evaluation methods. Our study shows that using CLIPScore with better CLIP models can compete and even outperform specialized IC evaluation approaches. Our finetuned multilingual CLIP model outperformed the original multilingual LAION ViT-H and Apple's best model across all CLIPScore variants on well-established human judgment datasets. With references, it managed to surpass specialized architectures like VICR (Narins et al., 2024), Polos (Wada et al., 2024), InfoMetIC (Hu et al., 2023), or RefPACScore (Sarto et al., 2023),"}, {"title": "4.3.2 Correlation on Multilingual Data", "content": "Table 2 displays the correlation between multilingual CLIPScore values and human ratings across the different languages. Our finetuned version achieves significantly better correlations with human judgments, both in reference-based and reference-free settings, across all evaluated languages and correlation metrics. The finetuned CLIPScore model strongly correlates with human preferences in high-resource languages (i.e., English, French, German, Spanish, and Chinese), and it also exhibits equally strong performance in medium- and low-resource languages. Importantly, our finetuned CLIPScore model outperformed the pre-finetuned version even when the latter was using references, achieving a higher average correlation across all metrics without using any references. This observation is particularly useful as it encourages the application of the model to new instances, without requiring additional human input.\nAppendix H provides additional results regarding different model sizes and loss variants for model finetuning. The findings in the appendix support the idea that smaller CLIP models can obtain higher gains in correlation with human judgements when using our finetuning strategy, compared to the original models.\nDelving deeper into the impact of MT quality, we note that in an ideal scenario, i.e. assuming perfect machine translation results and balanced CLIP performance across languages, the correlations between CLIPScore values across the different languages would equal one, signifying a perfect alignment. To explore deviations from this ideal behaviour, we use heatmaps to visually represent the interrelationships between CLIPScore values across the different languages.\nFigure 1 presents Pearson correlation scores between languages, for the best multilingual CLIP-Score model (presented in squared cells), and for our finetuned version (presented in circular cells). The image contains three heatmaps:\n\u2022 The first, on the left, shows CLIPScore correlations for the full VICR test dataset, comparing the finetuned model (upper triangle with circular cells) and the pre-finetuned model (lower triangle with squared cells).\n\u2022 The second and third heatmaps display correlations based on translation quality. They focus on translations with COMETKiwi scores in the bottom 25% (lower triangle) and top 25% percentile (upper triangle). The second heatmap with squared cells represents the pre-finetuned model, and the third heatmap with circular cells shows our finetuned version.\nIn the left heatmap of Figure 1, which includes the entire VICR test set, we observe consistently high correlation values across all languages. The results indicate that CLIPScore correlations are influenced by the quality of the translated captions. The second and third heatmaps show slightly higher correlations for high-quality translations (upper triangles) compared to the poorest translations (lower triangles). However, even the poorest translations still demonstrate relatively high correlations, suggesting that the overall quality of the translations remains strong across different languages.\nIt is worth noting that, as expected, the most impacted languages are those that use a different script, particularly in the non-finetuned case. We also see a significant improvement in correlations between languages when using our finetuned CLIP-Score model. Although this improvement is expected, given that the finetuned model was trained on in-distribution data, it also reflects the high quality of our multilingual training data, further validating our translation strategy."}, {"title": "4.3.3 Multilingual Classification", "content": "This section explores the robustness of the multilingual CLIPScore assessments through different types of classification tasks. Inspired by previous work (Hessel et al., 2021; Sarto et al., 2023) which assessed accuracy in English-only benchmarks, our goal is to delve deeper into the nuanced realm of multilingual and multicultural understanding.\nRobustness to Linguistic Phenomena: Some of the experiments used our machine translated versions of the VALSE dataset, designed to evaluate the robustness to phenomena such as inconsistencies in numeric quantities or spatial relations (Parcalabescu et al., 2022). VALSE comprises seven tests that encompass a range of linguistic structures. In each test, a model is presented with a visual input and is tasked with distinguishing true captions from altered versions (i.e., foils), modified to exhibit a specific feature. The evaluation is based on the assumption that the CLIPScore values for true captions should be higher than those for the corresponding foils. Additional information about the dataset is provided in Appendix C.\nTable 3 displays the average performance across different language variants. The results show that our finetuned model delivers the highest average performance across nearly all languages. Compared to the models reported in the original VALSE dataset paper, our MultiLingual Finetuned (MLF) model was only surpassed by the multi-task ViL-BERT 12-in-1 model (Lu et al., 2020).\nTable 9 in Appendix H contains a more detailed breakdown of the performance across the different tests within VALSE. We observe a significant improvement of 6% to 10%, on average, in the existence quantifier, plurality, and counting adversarial tests with the finetuned model. For the remaining tests, we saw a more modest performance increase, ranging from 1% to 5% compared to the non-finetuned model, with the exception of the action replacement and co-reference tests where performance did not improve. The lower performance for the co-reference tests likely stems from the nature of the task and limitations of our evaluation method. CLIPScore, which was designed to evaluate declarative captions, struggles to effectively score a caption that combines both a question and a yes/no answer. This mismatch may explain the lower results for coreference handling, and can be a key factor for the higher performance of ViL-BERT 12-in-1 in the overall average score shown in Table 3, when compared to CLIPScore strategies.\nIn the VALSE experiments, we also observed that different loss functions contributed distinct benefits to various test scenarios. For example, the Pearson loss proved particularly effective in the existence quantifier and action actant swap tests, compared to the contrastive loss. In contrast, the contrastive loss delivered superior performance in the foil-it and the counting adversarial tests. This demonstrates the advantages of our proposed training strategy, which combines both losses to maximize performance.\nClassification of Multicultural Instances: We also used natively multilingual datasets (i.e., XVNLI and MaRVL) to assess the multilingual and multicultural capabilities of CLIPScore models.\nThese datasets originally aimed at assessing semantic inference between the contents of images and texts, featuring positive and negative instances whose challenges for interpretation are more likely to lie on fine-grained semantic understanding and compositional reasoning instead of object detection, and with captions featuring slight variations in how the contents of an image are expressed.\nEach instance in the XVNLI dataset contains an image-caption pair and a categorical label associated with the relationship between the pair. This label can be either (a) contradiction, (b) neutral, or (c) entailment. Based on these labels, we defined three multilingual classification tasks under this scenario, leveraging concordant/discordant instances as illustrated in Figure 2:\nTask 1: This setting only considers contradiction and entailment instances, under the assumption that the order of the CLIPScore values should match the order of the labels.\nTask 2: We consider a larger set of duplets and the ordering between the three possible labels."}, {"title": "5 Conclusions", "content": "This study highlights the importance of expanding image captioning evaluation to include multilingual and multicultural research, encouraging more inclusive frameworks in this field. Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established English-centric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models.\nWe also propose a finetuning strategy to better leverage and learn from both multi-cultural data and human preferences, and test our models on a set of different datasets and tasks. Our findings show that multilingual models trained with the same amount but with less English-specific data perform equally well on English tasks, while excelling in multilingual and multicultural ones. This reveals the potential of multilingual models to generalize across languages, making them more versatile assets. Additionally, our finetuning approach significantly boosted the model's ability to handle complex linguistic challenges, such as quantifiers, plurality, and numeric inconsistencies, highlighting its adaptability to more intricate language patterns.\nFurther to machine translation of English data, we also propose a strategy to adapt multilingual datasets from other tasks to support captioning evaluation. The integration of natively multilingual and multicultural datasets into both training and evaluation processes mitigates cultural information loss, reinforcing the reliability of our proposed pipeline for training and evaluating multilingual CLIP models, and making them effective tools for real-world multilingual and multicultural evaluation.\nOverall, our work contributes to multilingual captioning evaluation, both in terms of modelling and benchmarking. We hope it will inspire and support further work in this under-researched field."}, {"title": "Limitations and Ethical Considerations", "content": "Although our work does not raise new ethical issues within the domain of vision-language models (e.g., we conducted our experiments on public datasets carefully designed for academic research and extensively used in previous studies), there are still some concerns which we describe below.\nModels like CLIP are, for instance, notorious for their internal biases, e.g. inherited from the training data itself. We, therefore, recommend caution in the use of the approach proposed in this paper and anticipate further research into the specific issue of model biases before relying on our work beyond research environments. Another important limitation in the work reported in this paper concerns the use of machine translated data in some of the evaluation experiments, which, despite our best efforts to avoid translation errors, can still lead to different types of biases and to the reliance on artificially impoverished language. The development of manually curated benchmarks, specifically designed for the assessment of multilingual metrics for image captioning evaluation, is left as an important challenge for future work.\nWe also note that we used GitHub Copilot\u00b2 during the development of our research work, and we used ChatGPT\u00b3 for minor verifications during the preparation of this manuscript."}, {"title": "G A Qualitative Study with Captions Featuring Culturally Related Concepts", "content": "We performed a small qualitative study on image-caption pairs that feature concepts where some languages should exhibit a particular bias (e.g., codfish in the case of Portugal, paella for Spain, beer for Germany, croissant for France, ushanka for Russia, and cheongsam for China). We attempted to see if the multilingual CLIPScore could distinguish between two plausible captions, where one mentions a specific concept that should better match the image. Figure 3 shows that the multilingual CLIPScore is indeed capable of distinguishing nuanced multicultural concepts, favouring culturally specific captions over generic ones."}, {"title": "H Additional Classification Results", "content": "Table 9 presents classification results on the different tasks from the VALSE dataset, separately for each of the considered languages and comparing different finetuning strategies (i.e., without model finetuning, considering only the contrastive loss, only the Pearson correlation loss, or the combined loss function). Results are, in general, better when considering the combined loss function.\nIn turn, Table 10 presents correlation results on the VICR dataset separately for each language and comparing the same two CLIP models under the different finetuning strategies. Results again show that the smaller CLIP model approaches the performance of the larger model, with better results consistently obtained when considering the combined loss function."}]}