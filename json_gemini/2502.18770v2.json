{"title": "Reward Shaping to Mitigate Reward Hacking in RLHF", "authors": ["Jiayi Fu", "Xuandong Zhao", "Chengyuan Yao", "Heng Wang", "Qi Han", "Yanghua Xiao"], "abstract": "Reinforcement Learning from Human Feed-back (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback is essential for the capabilities of powerful large lan-"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Reward Hacking in Traditional RL", "content": "Reward hacking arises when an RL agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended task (Weng, 2024). This aligns with Goodhart's Law: When a measure becomes a target, it ceases to be a good measure. For example: A bicycle agent rewarded for not falling and moving toward a goal (but not penalized for moving away) learns to circle the goal indefinitely (Randl\u00f8v and Alstr\u00f8m, 1998). A walking agent in the DMControl suite, rewarded for matching a target speed, learns to walk unnaturally using only one leg (Lee et al., 2021). An RL agent allowed to modify its body grows excessively long legs to fall forward and reach the goal (Ha, 2018). In the Elevator Action ALE game, the agent repeatedly kills the first enemy on the first floor to accumulate small rewards (Toromanoff et al., 2019).\nAmodei et al. (2016) propose several potential mitigation strategies to address reward hacking, including (1) Adversarial Reward Functions: Treating the reward function as an adaptive agent capable of responding to new strategies where the model achieves high rewards but receives low human ratings. (2) Model Lookahead: Assigning"}, {"title": "2.2 Reward Hacking in RLHF of LLMs", "content": "Reward hacking in RLHF for large language models has been extensively studied. Gao et al. (2023) systematically investigate the scaling laws of reward hacking in small models, while Wen et al. (2024) demonstrate that language models can learn to mislead humans through RLHF. Beyond exploiting the training process, reward hacking can also target evaluators. Although using LLMs as judges is a natural choice given their increasing capabilities, this approach is imperfect and can introduce biases. For instance, LLMs may favor their own responses when evaluating outputs from different model families (Liu et al., 2024b) or exhibit positional bias when assessing responses in sequence (Wang et al., 2023).\nTo mitigate reward hacking, several methods have been proposed. Reward ensemble techniques have shown promise in addressing this issue (Eisenstein et al., 2023; Ram'e et al., 2024; Ahmed et al., 2024; Coste et al., 2023; Zhang et al., 2024), and shaping methods have also proven straightforward and effective (Yang et al., 2024; Jinnai et al., 2024). Miao et al. (2024) introduce an information bottleneck to filter irrelevant noise, while Moskovitz et al. (2023) employ constrained RLHF to prevent reward over-optimization. Chen et al. (2024) propose the ODIN method, which uses a linear layer to separately output quality and length rewards, reducing their correlation through an orthogonal loss function. Similarly, Sun et al. (2023) train instructable reward models to give a more comprehensive reward signal from multiple objectives. Dai et al. (2023) constrain reward magnitudes using regularization terms. Liu et al. (2024a) curate diverse pairwise training data. Additionally, post-"}, {"title": "3 Method", "content": null}, {"title": "3.1 Design Principles", "content": "As detailed in Section 1, we restate our three design principles here: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. To elucidate the rationale behind these principles, we examine the Proximal Policy Optimization (PPO) policy and critic loss functions (notation detailed in Table 3):\n$\\mathcal{L}_{policy}(\\theta) = \\mathbb{E}_t  \\min \\left[ \\frac{\\pi_{\\theta}(y_t|x, y_{\\<t})}{\\pi_{\\theta_{old}}(y_t|x, y_{\\<t})}  \\hat{A}_t, clip \\left( \\frac{\\pi_{\\theta}(y_t|x, y_{\\<t})}{\\pi_{\\theta_{old}}(y_t|x, y_{\\<t})}, 1 - \\epsilon, 1 + \\epsilon \\right)  \\hat{A}_t \\right]$\n$\\mathcal{L}_{critic}(\\alpha) = \\mathbb{E}_t [||V_{\\alpha}(x,y_{\\<t}) - G_t||^2].$\nFor the policy loss, $\\hat{A}_t = \\sum_{l=t}^{T}(\\gamma \\lambda)^{l-t} \\delta_t$ represents the generalized advantage estimation (GAE) at token t, where $\\delta_t = r_t + \\gamma V_{\\alpha_{old}}(S_{t+1}) - V_{\\alpha_{old}}(S_t)$ is the temporal difference (TD) error. $\\pi_{\\theta}$ denotes the current policy model, and $\\pi_{\\theta_{old}}$ refers to the policy model from the previous iteration. $V_{\\alpha_{old}}$ is the critic's value function from the previous iteration. For the critic loss, $G_t = \\sum_{l=t}^T \\gamma^{l-t} r_t$ represents the return, defined as the discounted sum of per-token rewards.\nThe per-token reward at position t, denoted as $r_t$, is defined as:\n$r_t = \\begin{cases}\nr_{RL} - \\eta \\log \\frac{\\pi_{\\theta}(y_t|x,y_{\\<t})}{\\pi_{ref}(y_t|x,y_{\\<t})}  & \\text{if } t = T \\\\\n-\\eta \\log \\frac{\\pi_{\\theta}(y_t|x,y_{\\<t})}{\\pi_{ref}(y_t|x,y_{\\<t})} & \\text{if } t < T\n\\end{cases}$"}, {"title": "3.2 Preference as Reward", "content": "After careful consideration and empirical evaluation, we recommend using the sigmoid function applied to centered rewards as the preferred reward shaping method. The sigmoid function is bounded, has the steepest slope at the initial point (zero), and converges gradually to its upper bound of one. This property makes it particularly suitable for stabilizing the RL training process. Furthermore, our analysis reveals that this shaping approach is intrinsically linked to the hidden preferences encoded within the reward model. The reward model is designed to simulate human preferences, and the RL training process aims to maximize the reward using an RL algorithm. Given a reward model $r_{\\phi}$, the hidden preference between two responses y and y' to a prompt x can be expressed as:\n$P_{\\phi}(y > y'|x) = \\sigma(r_{\\phi}(x,y) - r_{\\phi}(x, y'))$\nThis formulation shows that applying the sigmoid function to centered rewards corresponds precisely to the preference score of the policy response over the reference response. Consequently, we term this method Preference As Reward (PAR), which is defined as follows. To enhance stability, we use multiple M reference rewards:\n$r_{RL} = \\frac{1}{M} \\sum_{m=1}^M \\sigma(r - r_{ref}^m) =  \\frac{1}{M} \\sum_{m=1}^M P(y > y_{ref}^m|x)$\nOur proposed PAR method serves exclusively as a reward shaping technique, which is fundamentally orthogonal to other strategies for mitigating reward hacking, such as robust reward model training (Dai et al., 2023) or the construction of diverse datasets (Liu et al., 2024a).\nThe pseudo-code for the reward shaping procedure under PAR is detailed in Algorithm 4, while the complete implementation of the Proximal Policy Optimization (PPO) algorithm is provided in Algorithm 1. Additionally, the pipeline for reward shaping is illustrated in Figure 1."}, {"title": "4 Experiment", "content": "Our analysis is structured to first validate the three key design principles, followed by a comparison of PAR with other reward mitigation methods, and finally, an evaluation of the data efficiency and robustness of PAR."}, {"title": "4.1 Experimental Setting", "content": "Datasets and Models We utilize two dialogue datasets:HH-RLHF (Bai et al., 2022) and Ultrafeedback-Binarized (Cui et al., 2023), alongside two base models, Gemma-2B (Google, 2024) and Llama3-8B (Meta, 2024), for our experiments. We present the results of Gemma2-2B on the Ultrafeedback-Binarized in this section. For additional results and comprehensive training details, please refer to Appendix B."}, {"title": "Mitigation Baselines", "content": "We evaluate seven baseline methods to mitigate reward hacking, which are described as follows:\n\u2022 WARM (Ram'e et al., 2024): This approach combines the weights of multiple reward models and employs the aggregated model to provide rewards for reinforcement learning training.\n\u2022 ODIN (Chen et al., 2024): This method introduces an additional head (length head) during reward training to capture the response length. Only the quality head is utilized for reinforcement learning training.\n\u2022 Reg (Dai et al., 2023): A regularization term is integrated into the reward training loss, defined as: lreward = E(x,yw,y\u0131)\u223cD[\u2212logo(r(x,yw)r(x,y\u0131)) + \u03b2||r(x,yw)||2 + \u03b2||r'(x,y\u0131)||2].\n\u2022 Meanstd: The reward is normalized using the running mean and running standard deviation: rRL = r\u2212\u03bc/s, where \u00b5 and s represent the running mean and standard deviation, respectively.\n\u2022 Clip: The reward is clipped based on the running mean and standard deviation: rRL = clip(r, \u03bc \u2212 s, \u03bc + s).\n\u2022 Minmax: The reward is normalized using the running minimum and maximum rewards: rRL = (r\u2212rmin)/(rmax\u2212rmin), where rmax and rmin denote the running maximum and minimum rewards, respectively.\n\u2022 LSC (Wang et al., 2024): The reward is normalized using the log-sigmoid-centered shaping method, defined as: rRL = log \u03c3(r\u2212r\u0303), where r\u0303 represents the 85th percentile of the normal distribution, calculated from the mean and variance of the reference rewards."}, {"title": "Evaluation Metrics", "content": "Two primary metrics are employed to monitor training progress, both computed on the test set: Proxy Reward (shown as a solid line) and Winrate (shown as a dashed line). The winrate measures the policy model's winning rate against the SFT model, as evaluated by DeepSeek-V3 (DeepSeek-AI, 2024). For the benchmarks AlpacaEval2.0 (Li et al., 2023) and MT-Bench (Zheng et al., 2023a), six metrics are utilized, with all metrics except the length metric being assessed by DeepSeek-V3."}, {"title": "Training Details", "content": "We briefly outline the training details here; for a comprehensive discussion, please refer to Appendix B. The dataset is preprocessed"}, {"title": "4.2 Principle One", "content": "To validate the first principle that RL reward is ideally bounded, we conducted experiments by employing a larger KL penalty coefficient and constraining the maximum reward during reinforcement learning training (see Figure 3). The results demonstrate that limiting excessive rewards significantly mitigates reward hacking. For instance, increasing the KL penalty coefficient from 0.01 to 0.1 leads to a rise in the winrate curve and a corresponding decline in the reward curve. A similar effect is observed when reducing the reward ceiling (i.e., the maximum reward threshold). Furthermore, Figure 3 reveals that while PAR and kl0.1 exhibit comparable proxy rewards, PAR consistently out-"}, {"title": "4.3 Principle Two and Three", "content": "To validate the second and third principles\u2014which state that RL reward is best formulated as a function of centered reward and exhibit rapid initial growth followed by gradual convergence\u2014we conducted experiments using several sigmoid-like functions, including their centered and uncentered variants, such as tanh, fitted polynomial, sigmoidk2, and sigmoidk3. The results are presented in Figure 5.\nWe observe that functions applied to centered rewards achieve higher winrates compared to their uncentered counterparts, providing strong support for the third principle. Furthermore, all sigmoid-like functions operating on centered rewards exhibit similar performance, and their shapes (see Figure 6) demonstrate rapid growth near the initial point (zero) and slow convergence toward their up-"}, {"title": "4.4 PAR Effectively Mitigates Reward Hacking", "content": null}, {"title": "Reward and Winrate Curve", "content": "As illustrated in Figure 7, the Vanilla PPO suffers from the reward hacking problem severely. To address this issue, we conduct a comprehensive study of several mitigation methods. While some approaches, such as ODIN, Reg, Meanstd, Clip, and LSC, fail to mitigate the problem, others, including WARM, Minmax, and PAR, demonstrate varying degrees of effectiveness over a single training epoch. Notably, the PAR method achieves the highest winrate by the end of the training process.\nAnother intriguing observation is that Vanilla, Meanstd, Clip, and LSC exhibit hacking behavior when the proxy reward reaches a specific threshold, such as 6.0, as shown in Figure 7. In contrast, Minmax and PAR show no signs of hacking, and their proxy rewards do not exceed this threshold."}, {"title": "Benchmark Performance", "content": "We further investigate the generalization ability of the policy model on out-of-distribution (OOD) data. For each mitigation method, we select the checkpoint after one epoch of training and evaluate these checkpoints on two benchmarks: AlpacaEval2.0 and MT-bench. The results, presented in Table 1, align with the training curve depicted in Figure 7. The Vanilla PPO method exhibits complete deterioration, while the top-performing methods are PAR, Minmax, and"}, {"title": "4.5 PAR is Data Efficient", "content": "The default number of reference rewards for each prompt in our PAR method is set to 10. However, we hypothesize that this number may be higher than necessary for PAR to function effectively. To explore this, we conduct an experiment to determine the minimum number of reference rewards required for PAR to perform efficiently. As shown in Figure 8, the results reveal that PARrefl to PARref10 exhibit similar trends in both proxy reward and winrate during training. This suggests that a single reference reward is sufficient for PAR to operate effectively. In contrast, the sigmoid method, which can be viewed as a variant of PAR without any reference rewards, performs significantly worse than PARref1. This indicates that completely eliminating reference rewards is not feasible for maintaining performance."}, {"title": "4.6 PAR is Robust", "content": "To assess the robustness of the mitigation methods discussed earlier, we select the top three performing methods on benchmarks: PAR, Minmax, and WARM. For a more comprehensive evaluation, we extend the training process to two epochs instead of one. The rationale is that if a mitigation method can effectively address the reward hacking problem even under prolonged training, it can be considered robust. The training curves for proxy reward and winrate are presented in Figure 3.\nAmong the three methods, it is evident that Minmax and WARM lack robustness when the training process is extended to two epochs. In contrast, PAR demonstrates consistent robustness throughout the extended training period. Notably, PAR consistently achieves the highest winrate among all methods, further highlighting its effectiveness and reliability in mitigating reward hacking over extended training durations."}, {"title": "5 Discussion", "content": "Reward shaping is not applicable to DPO (Rafailov et al., 2023), as it does not require a reward model during training. We also explore online DPO, which employs the policy model to generate two responses, and the reward model selects the response with the higher reward as the chosen response and the lower reward as the rejected response. However, since most reward shaping techniques are monotonic, they do not alter the binary preference and therefore, they do not influence the training procedure of online DPO.\nFor GRPO (Shao et al., 2024), we argue that its advantage calculation inherently normalizes the"}, {"title": "6 Conclusion", "content": "We identify that for a given reward model, there exists a specific threshold beyond which the proxy reward becomes both meaningless and inaccurate. Based on this observation, we establish three fundamental principles for designing reward shaping methods.\nIn alignment with these principles, we propose an effective shaping method, Preference As Reward (PAR). Through extensive experimentation with various mitigation approaches, our results demonstrate that PAR not only outperforms other baseline methods by the end of one training epoch but also maintains a high winrate after two epochs of training. Notably, PAR is also data-efficient, requiring only a single reference reward to achieve strong performance."}, {"title": "Limitations", "content": "Although our PAR method effectively mitigates reward hacking, it does not improve peak performance, as measured by the winrate of the best checkpoint. Furthermore, its design principles lack precision. While PAR sets the upper bound of the RL reward to 1.0, alternative bounds and their selection criteria remain unexplored. Additionally, the dynamics of reward adjustment\u2014such as the initial rate of increase and the pace of convergence-are not fully elucidated."}, {"title": "Ethical Considerations", "content": "Our research addresses the ethical challenges of reward hacking in RLHF by proposing a method to mitigate this problem. By ensuring robust alignment with human values, enhancing transparency in reward design, and proactively addressing biases and safety risks, our approach aims to develop RLHF systems that are fair, reliable, and aligned with societal well-being."}, {"title": "A Notations", "content": "The definitions of the notations used in this paper are summarized in Table 3."}, {"title": "B Training Details", "content": "Dataset Our experiments are conducted on two datasets: Ultrafeedback-Binarized (Cui et al., 2023) and the helpful-base subset of HH-rlhf (Bai et al., 2022). Both datasets undergo preprocessing to eliminate noise and constrain their overall length. For the Ultrafeedback-Binarized dataset, we select examples where the prompt length, chosen response length, and rejected response length are each less than 512 tokens. Additionally, we ensure that the chosen response score exceeds the rejected response score and that the substring 'confidence' does not appear in either the chosen or rejected responses. For the HH-rlhf dataset, we apply the same length constraints (prompt, chosen, and rejected responses each under 512 tokens). Furthermore, we ensure that each prompt appears only once across both datasets and limit the test set to 256 examples. The training set of Ultrafeedback-Binarized contains around 33,000 examples and HH-RLHF helpful base contains 43,000 examples. All training are carried on 8*A800(80G) GPUs.\nBase Models For the base models, we utilize Gemma-2B (Google, 2024) and Llama3-8B (Meta, 2024). In all training procedures, we implement a linear learning rate scheduler, which gradually"}, {"title": "SFT Model", "content": "The Supervised Fine-Tuned (SFT) model is initialized from the base model and trained on the chosen responses for two epochs with a learning rate of 5e-6. Gradient norm clipping is applied when the norm exceeds 10."}, {"title": "Reward Model", "content": "The reward model is initialized from the base model, with the logit head replaced by a linear head above the last embedding layer to output a scalar value. It is trained for one epoch with a learning rate of 5e-6, achieving an accuracy of approximately 70% on the test set. Gradient norm clipping is applied when the norm exceeds 5.\nFor ODIN training, we use two linear heads to output length reward and quality reward separately, following the training loss described in Chen et al. (2024). Only the quality head is used during RL training.\nFor WARM training, we train five reward models on the same dataset with varying learning rates (3e-6, 4e-6, 5e-6, 6e-6, 7e-6) and different random seeds.\nFor Reg training, we adopt the loss function from (Dai et al., 2023), with a regularization term coefficient of 0.005."}, {"title": "Policy Model", "content": "The policy model is initialized from the SFT model and trained on the same prompts for one epoch using the PPO algorithm with a learning rate of 3e-7. Gradient norm clipping"}, {"title": "Critic Model", "content": "The critic model is initialized from the reward model and trained alongside the policy model for one epoch with a learning rate of 5e-6. Gradient norm clipping is applied when the norm exceeds 5."}, {"title": "Hyper-Parameters", "content": "Responses are sampled from the policy model using a temperature of 0.9, with top-k set to 50, top-p set to 0.9, and a length penalty of 2. The coefficient for the KL penalty is 0.005, and the default number of reference rewards is 10. For PPO training, the buffer size is set to 4, with $\\epsilon = 0.2, \\lambda = 0.95, \\gamma = 1.0$, For GRPO training, the $\\epsilon = 0.2$, the buffer size is 4, and the group size is 5."}, {"title": "C Evaluation", "content": null}, {"title": "C.1 Winrate on Test Set", "content": "To leverage the strong grading capability of DeepSeek-V3 for comparing the SFT model and the policy model on the test set, we design a detailed evaluation prompt. The system prompt and user input format are provided in Listing 1 and 2.\nTo address position bias (Wang et al., 2023), we evaluate each pair of responses twice, alternating their order, and aggregate the scores. Specifically, for two responses A and B, we first evaluate them in the order A-B and then in the order B-A. In each evaluation, the winner receives a score of 1, the loser receives 0, and in the case of a tie, both responses receive 0.5. The final scores of A and B are compared, and the response with the higher score is declared the winner. If the scores are tied, both responses receive 0.5 win counts. The win counts are used to calculate the winrate."}, {"title": "C.2 Benchmark", "content": "We also evaluate the model on two benchmarks, using DeepSeek-V3 to simulate human evaluation. The metrics and their meanings are as follows:"}, {"title": "AlpacaEval 2.0", "content": "\u2022 LC Winrate: The length-controlled win rate measures the model's performance while controlling for the length of generated responses. It compares the model's outputs to a baseline (e.g., the SFT model) and adjusts for the influence of response length on human preferences.\n\u2022 Winrate: The standard win rate measures the proportion of times the model's outputs are pre-"}, {"title": "MT-bench", "content": "\u2022 Length: The average length of the model's generated responses, measured in tokens or characters, providing insight into the model's verbosity.\n\u2022 T1: Turn 1 Score evaluates the model's performance on the first turn of a multi-turn dialogue, assessing relevance, coherence, and informativeness. Scores are normalized as 0-10.\n\u2022 T2: Turn 2 Score evaluates the model's performance on the second turn, measuring its ability to maintain context and provide consistent, high-quality responses. Scores are also normalized as 0-10.\n\u2022 Overall: The overall score is the average of the T1 and T2 scores, providing a comprehensive evaluation of the model's performance across both turns."}, {"title": "D More Results", "content": null}, {"title": "D.1 Llama3-8B and Ultrafeedback Binarized", "content": "Figure 10a presents the PPO training curves for different mitigation methods on Llama3-8B with the Ultrafeedback Binarized dataset. PAR demonstrates robustness against reward hacking and maintains a high win rate throughout one epoch of training."}, {"title": "D.2 Gemma2-2B and HH-RLHF", "content": "The PPO training curves for various mitigation methods on Gemma2-2B with the HH-RLHF dataset are shown in Figure 10b. PAR exhibits resilience to reward hacking and sustains a high win rate during one epoch of training."}, {"title": "D.3 Llama3-8B and HH-RLHF", "content": "Figure 10c illustrates the PPO training curves for different mitigation methods applied to Llama3-8B on the HH-RLHF dataset. While PAR shows signs of reward hacking toward the end of training, it maintains a consistently high win rate (above 60%) for an extended period, from 10,000 to 30,000 steps. We hypothesize that the observed reward hacking in the later stages is due to the convergence rate of the sigmoid function approaching its upper bound."}, {"title": "E Case Study", "content": "We identify several patterns of reward hacking observed in Vanilla PPO training, using the checkpoint trained after one epoch for detailed examination. We show the examples in Figure 11."}, {"title": "FPPO Training", "content": "PPO (Proximal Policy Optimization) is an online reinforcement learning algorithm that generates a response given a prompt, computes a reward for the response using a reward model, and updates the policy and critic models to maximize the reward.\nWe employ several PPO techniques to ensure stable training, including advantage normalization (Zheng et al., 2023b), value loss clipping (Patterson et al., 2023), a replay buffer (Eysenbach et al., 2019), per-token KL penalty, and length penalty. The pseudo-code for the PPO algorithm is provided in Algorithm 1."}, {"title": "G Reward Shaping Is Not Applicable to DPO and GRPO", "content": "In this section, we explain why monotonous reward shaping techniques, such as PAR, are not applicable to the Direct Preference Optimization (DPO). And why linear shaping techniques are not applicable to the Group Relative Policy Optimization (GRPO) algorithms."}, {"title": "G.1 DPO and Reward Shaping", "content": "Vanilla DPO is an offline alignment algorithm that trains the policy model directly on paired responses using a contrastive loss. Since the vanilla DPO algorithm does not rely on an explicit reward model, reward shaping techniques are inherently inapplicable. We also explore an online variant of DPO, which generates two responses for a given prompt and employs a reward model to determine the chosen and rejected responses. The policy model is then trained on these responses (see Algorithm 6). However, any monotonous transformation of the proxy reward will not alter the chosen and rejected responses. For instance, if r\u2081 > r2, then f(r1) > f(r2) for any monotonous function f(\u00b7), including PAR. Consequently, PAR is also not applicable to online DPO."}, {"title": "G.2 GRPO and Reward Shaping", "content": "For GRPO, the advantage value is computed as a normalization of proxy rewards. Consider a prompt x and N responses Y1,..., YN sampled from the policy model. A reward model r\u03d5 assigns scores r1,...,rN to each response. The advantage Ai,t for response yi at token position t is given by:\n$A_{i,t} = \\frac{r_i - \\mu}{s}$\nwhere $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} r_i$, and $s = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (r_i - \\mu)^2}$ are the mean and standard deviation of the rewards, respectively.\nAssume a linear transformation is applied to the proxy reward, such that r' = \u03b1 \u00b7 r + b (\u03b1 > 0). We prove that the new advantage A'i,t is identical to the original Ai,t. First, the new mean $\\hat{\\mu} = \\alpha \\cdot \\mu + b$, and the new standard deviation $\\hat{s} = \\alpha s$. The new advantage is computed as:\n$A'_{i,t} = \\frac{\\alpha r_i + b - (\\alpha \\mu + b)}{\\alpha s} = \\frac{\\alpha (r_i - \\mu)}{\\alpha s} = \\frac{r_i - \\mu}{s} = A_{i,t}$\nThus, linear transformations do not influence the advantage calculation in GRPO. Furthermore, since the sigmoid function is a non-linear function, PAR is applicable to GRPO training. We validate this through experiments, as shown in Figure 9. No reward hacking problem is observed in the GRPO training process, as the advantage calculation inherently performs reward normalization."}]}