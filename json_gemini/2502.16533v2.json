{"title": "A Survey of Graph Transformers: Architectures, Theories and Applications", "authors": ["Chaohao Yuan", "Kangfei Zhao", "Ercan Engin Kuruoglu", "Liang Wang", "Tingyang Xu", "Wenbing Huang", "Deli Zhao", "Hong Cheng", "Yu Rong"], "abstract": "Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph data, a non-Euclidean data structure, is commonly found in various real-world applications, including molecular data, protein interactions, and social networks. Recently, Graph Neural Networks (GNNs) [1], [2] demonstrate impressive capabilities in modeling such data. A representative paradigm to building GNNs is the message-passing, which iteratively aggregates the neighbours' information and updates the node embedding. Nonetheless, the message-passing paradigm encounters several intrinsic limitations, such as over-smoothing [3],[4] and over-squashing [5], making it challenging for GNNs to effectively capture the long-range dependencies within the graphs.\nIn another line of research, the Transformer model [6] has demonstrated remarkable performance across a range of modalities, including Natural Language Processing (NLP) [6], Computer Vision (CV) [7], and time-series analysis [8]. A notable advantage of the Transformer model is its ability to effectively capture long-range dependencies, making it an ideal solution for addressing the limitations inherent in GNNs.\nGraph Transformers (GTs) adapt Transformer architecture to handle both node embeddings and graph structures, demonstrating superior performance compared to message-passing GNNs on a variety of tasks, including the prediction of molecular properties [9], [10], [11], dynamics simulation [12], [13], [14], and graph generation [15].\nIn this survey, we conduct a systematic and comprehensive review of the recent advancements in GTs, examining the developments from the perspectives of architectures, theories, and applications. First, from the perspective of architecture, we categorize GTs into four categories based on their ways of integrating graph structures into Transformers. (1) Multi-level Graph Tokenization. These models utilize tokenization mechanisms to represent edges, subgraphs, and hops as structure-aware tokens, enabling the attention mechanism to effectively capture and learn intrinsic topological relationships. (2) Structural Positional Encoding. These models enhance positional encoding (PE), traditionally used to denote token sequence relationships, to elucidate the structural interrelations among tokens. (3) Structure-aware Attention Mechanisms. Since the attention matrix inherently captures the learned relationships between tokens, these models modify the attention matrix using graph-derived structural information to incorporate node interrelations. (4) Model Ensemble between GNNs and Transformers. In addition to the direct modification of the architectures of Transformer, another effective strategy employs message-passing GNNs to encode structural information, followed by the integration of GNNs with Transformers. We follow the taxonomy of the previous study [16] to organize this category. Additionally, we investigate two critical aspects of Graph Transformer architecture: scalability challenges in large-scale graph processing and specialized architectural designs for geometric graphs.\nAfter reviewing the architectures of GTs, it is important to determine which architecture is more powerful in general or in a specific task, since different architectures include different inductive biases or model the graph in different granularity. To this end, we investigate the expressive ability of GTs. Specifically, we examine the studies that compare the expressivity of GTs using the Weisfeiler-Lehman Test. These theoretical insights will enhance our understanding of how each component contributes to GTs' ability to learn graph data. Additionally, we discuss the relationships between GTS and other current graph learning algorithms, which can further elucidate the strengths and weaknesses of GTs.\nFurthermore, to review the GTs tailored for specific tasks, we thoroughly investigate their applications across eight types of graph data: molecules, proteins, text, social networks, traffic systems, vision, brain and material graphs. For each graph type, we review the different learning tasks, along with the respective datasets and methodologies. Additionally, applying GTs to specific tasks might necessitate an additional pre-training strategy or optimization objective, such as diffusion. These aspects are also incorporated into our review. In the last part of this survey, we will explore the potential future directions of GT.\nThis survey aims to provide a thorough analysis of GTS from multiple perspectives, including their model architectures, expressivity, and applications. While recent reviews have mainly focused on architectural aspects [16], [17], our work distinguishes itself within three aspects: (1) comprehensive architectural insights, (2) systematic analysis of theoretical expressivity of GTs, (3) extensive investigation of cross-domain applications. We also explore prospective research directions with recent advance in GTs.\nRoadmap. The rest of this paper is organized as follows. Section II introduces the preliminaries of the survey. Section III reviews the four categories of GTs from the perspective of architecture. In Section IV, we summarize the expressive capability of GTs and connect them to graph learning methodologies. Section V reviews the applications of GTs. We point out the future research directions in Section VI and conclude the paper in Section VII."}, {"title": "II. PRELIMINARIES", "content": "In this section, we provide a concise overview of the essential notations pertaining to GTs, as well as a summary of the fundamental architecture of the vanilla Transformer and message passing neural networks.\nWe denote a graph as G = (V,E), where V represents the set of nodes and E represents the set of edges. The node set V comprises n nodes, with the feature matrix H \u2208 Rn\u00d7d, where n is the number of nodes and d is the dimension of the node features. The edge set E corresponds to an adjacency matrix AG \u2208 Rn\u00d7n, where $A^G_{ij}$ = 1 if there exists an edge (u,v) in E, and $A^G_{ij}$ = 0 otherwise.\nThe aforementioned notations are adequate for representing a basic graph with an adjacency matrix and node features. In more challenging scenarios, graphs are associated with more features to support various applications 1): Geometric Graph: In addition to node property features, the nodes will have coordinate features X \u2208 R\u00b3, which will be independently handled to achieve invariance or equivariance properties. 2): Graph with edge features: Rather than merely converting the edge information into an adjacency matrix, the edges offer supplementary information that denote more specific relationships between the nodes. 3): Dynamic graph: The graph additionally contains the time domain T compared with static graphs, represented as G = (V,E,T). This indicates that the nodes and edges can change over time. The graph is denoted as G = {(vi, vj, T)n, n = 1,2,...,|E|}, where each tuple (vi, vj, T) represents an edge between node vi and node vj at a specific time \u03c4\u2208 Tand E specifies the edge set for that particular timestep.\nDynamic graphs are generally classified into two categories: discrete-time dynamic graphs (DTDGs) and continuous-time dynamic graphs (CTDGs). In DTDGs, the time domain T\u2208T is divided into k discrete time intervals, denoted as T = [t1: tk]. This allows the dynamic graph G to be represented as a sequence of individual graph snapshots G = [Gt1, Gt2, ..., Gtr]. In contrast, CTDGs model scenarios where the graph evolves continuously over time, providing the exact graph structure at any given timestamp t. In CTDG, changes to the graph structure are triggered only by certain events T.\nMessage Passing Graph Neural Networks: Graph neural networks [1] are a foundational framework for learning rep- resentations in graphs via the message passing mechanism. Given node embeddings H and the adjacency matrix AG, a message passing neural network (MPNN) updates node embeddings via the propagating information from neighboring"}, {"title": "III. ARCHITECTURES", "content": "Vanilla Transformer is essentially a special GNN, where self-attention mechanisms across all nodes are operated in a fully-connected graph. Since Transformers ignore the original graph structure, the core objective of GT is to incorporate edge information into Transformer architecture. Based on the approaches of incorporating this structural prior, in this section, we systematically categorize existing GTs into four classes: 1) Multi-level Graph Tokenization (Section III-A). 2) Structural Positional Encoding (Section III-B). 3) Structure-aware Attention Mechanisms (Section III-C). 4) Model Ensemble between GNNs and Transformers (Section III-D). Following the categorization,\nA. Multi-level Graph Tokenization\nIn the realm of GTs, tokenization plays a crucial role in transforming graph data into a sequential format for processing by Transformer architectures. Distinguished from conventional text-based Transformers where tokenization is trivial, graph data presents unique challenges due to its structural complexity. This section explores four distinct levels of tokenization in GTs from fine-grained to coarse-grained level, i.e., node-level, edge-level, hop-level, and subgraph-level tokenization.\n1) Node-level Tokenization: Node-level tokenization is the most granular approach for tokenization [10], [41], which treats each node in the graph as an individual token. Furthermore, the node involved in attention can be selected through methods such as contrastive learning [42]. This approach is particularly effective when models focus on node-specific features or when the graph's topology is less critical than the attributes of individual nodes. By capturing detailed information about each node, node-level tokenization is well-suited for tasks such as node classification.\n2) Edge-level Tokenization: Edge-level tokenization extends the concept of tokenization to the connections between nodes [24], [40]. Here, each edge in the graph is treated as a token, making this approach ideal for tasks where the relationships or interactions between nodes are of primary interest. Edge-level tokenization can capture the dynamics of these interactions, which is essential for tasks like link prediction or understanding the flow of information across the graph. By focusing on edges, edge-level tokenization can highlight the importance of connectivity patterns in the graph.\n3) Subgraph-level Tokenization: Subgraph-level tokenization treats the entire k-hop subgraph centered on a node as the token representation of this node [25], which is highly effective for capturing the contextual structure and patterns surrounding the nodes. This approach is suitable for tasks like graph classification or regression, as it can effectively encode substructures, clusters, or other higher-order structures within the graph. Notably, the nodes in subgraphs corresponding to different tokens are allowed to overlap, enabling a more flexible and comprehensive representation of the graph.\n4) Hop-level Tokenization: Hop-level tokenization extends subgraph-level tokenization by considering the number of hops and improving scalability [32], [43], [44]. Unlike subgraph-level tokenization, which is restricted to a single specific hop,"}, {"title": "B. Structural Positional Encoding", "content": "In standard Transformers, the positional encoding (PE) module indicates the position of tokens in a sequence. To extend this module to GTs, it is natural to develop methods for representing the positional embeddings of nodes in a graph. Since edge-level, hop-level, and subgraph-level tokenization approaches already incorporate structural information, in this section, we assume that the PE methods are applied at the node-level.\nExisting PE methods can be categorized into absolute PE and relative PE. Similar to standard Transformers, absolute PE assigns a unique positional embedding to each node. This embedding, either learned or parameter-free, is subsequently aggregated or concatenated with the original embedding of the node. In contrast, relative PE focuses on capturing pairwise relationships between nodes and directly applies to the attention matrix. Therefore, we defer the discussion on relative PE to the next section.\nThe main objective of absolute PE can be formulated as utilizing a function f to extract the underlying structural information from the graph, typically from the adjacency matrix A. Equation (10)-(11) show the usage of absolute PE:\nP = f(A), (10)\nH = g(H, P). (11)\nHere, the function f extracts the absolute PE P\u2208 Rnxdp, where n denotes the number of nodes, and dp represents the dimension of the positional embedding for each node. The function g integrates absolute PE with the original node features H, either by concatenation or by employing an MLP to align the dimensions of P and H before summing them.\nLaplacian PE leverages the eigenvectors and eigenvalues obtained from the decomposition of the Laplacian matrix as PE. The decomposition of the Laplacian can be expressed as Equation (12):\n$U^T AU = I - D^{-1/2}AD^{-1/2}$, (12)\nwhere D denotes the degree matrix, I is the identity matrix, A and U represents eigenvalues arranged in a diagonal matrix and eigenvectors. Since the sign of pre-computed eigenvectors is arbitrary, the Laplacian PE approaches randomly adjusts the sign of eigenvectors during the training stage. The first Laplacian PE [22] proposes to utilize the k smallest non-trivial eigenvectors of a node as its PE. Another work SAN [5] introduces a learned Laplacian PE. For a given node vj, SAN uses {i, Uij} as input features for neural networks to learn the PE of node vj, where m is a hyperparameter determining the number of eigenvectors considered.\nDespite the effectiveness of Laplacian PE, it faces two underlying challenges: 1) Non-unique Eigendecompositions. There are different eigendecompositions of the same Laplacian. If a vector v is an eigenvector, then -v is also an eigenvector. There are non-unique solutions for eigenvectors with the multiplicities of eigenvalues. 2) Sensitivity to Perturbations. Minor perturbations in the graph structure can significantly affect the result of eigenvectors, leading to considerable instability in Laplacian PE.\nTo address the first challenge in Laplacian PE, SignNet [45] introduces a sign-invariant network, f, which operates on eigenvectors as Equation (13):\nf(U1,..., Uk) = p (||=1[$(U\u3047) + (\u2212U\u00bf)]), (13)\nwhere pand & are neural networks. This formulation ensures that the neural network remains invariant embeddings to the sign of the eigenvectors. Moreover, to tackle the occurrence of multiple eigenvector choices when there are repeated eigenvalues in the Laplacian matrix, BasisNet [45] proposes a method to extract consistent PE from these matrices.\nTo address the challenge of stability in Laplacian PE, Stable and Expressive PE (SPE) [46] is introduced, which is formulated as Equation (14):\nP(U, \u039b) = \u03c1(U($1(A))UT, U($2(A))UT, ..., U($m(A))UT), (14)\nwhere the input consists of the k smallest eigenvalues \u039b and their corresponding eigenvectors V. Rather than implementing a strict division of eigensubspaces, SPE utilizes a weighted aggregation of eigenvectors that is contingent upon the eigenvalues to ensure stability.\nSingular Value Decomposition (SVD) PE [30] provides a broader scope of applications compared to Laplacian PE, as it can handle directed and weighted graphs flexibly. The SVD PE is computed by Equation (15)-(16):\n$AU^SV^T = (U\\sqrt{\\sum}) \\cdot (V\\sqrt{\\sum}) = \\hat{U}\\hat{V}^T$, (15)\nP = \u00db || V, (16)\nwhere U, V \u2208 Rnxr contain the r left and right singular vectors as their respective columns, each associated with the highest r singular values in the diagonal matrix \u03a3\u2208 Rr. Similar to Laplacian PE, SVD PE involves the random sign flipping of eigenvectors during the training phase. Consequently, building on the concept of SignNet, developing a sign-invariant SVD PE could be a potential direction for future research.\nRandom Walk PE (RWPE) [47] represents a PE derived from the diffusion process of a random walk. The RWPE for a node vi can be mathematically expressed in Equation (17) through a k-step random walk:\nPi = [MMM]\u2208 R, (17)\nwhere M = AD-\u00b9 represents the random walk operator. Distinguished from Laplacian PE, RWPE does not suffer the sign ambiguity. Under the condition that each node possesses a unique k-hop topological neighborhood for a sufficiently"}, {"title": "C. Structure-aware Attention Mechanisms", "content": "In Transformer blocks, the attention matrix governs the interactions between nodes, while tokenization and absolute PE augment node embeddings. These augmented embeddings enable Transformers to incorporate structural prior into attention mechanisms. In this vein, direct modification of the attention matrix is a more forthright approach for capturing structural inductive bias. In this section, for clarity of the paper, we present all structure-aware attention mechanisms in their single-head formulation.\nAdjusting the attention matrix begins with capturing pairwise node interactions in the graph. To this end, GT leverages the graph structure to first generate a structure matrix that encodes node connectivity patterns. This structure matrix can be integrated into the attention matrix by three ways: by attention bias, by attention mask and by edge-level tokenization. In the rest of this section, we elaborate on the three ways respectively.\n1) Structure Matrix as Attention Bias: By attention bias, the structural information is incorporated into the attention mechanism by adding a bias matrix b \u2208 Rn\u00d7n into the inner product of the query and key matrices. Equation (21) shows a general form for computing the attention matrix:\nA = Softmax ((HWQ)(HWK)\u00b2 + b);, (21)\nwhere the attention bias b is specified by different approaches, which is essentially relative PE. Relative PE is computed from the graph structure, aiming to understand pair-wise interactions between nodes. We define a relation matrix P\u2208 RRR as the attention bias b. Pij is determined by the function (Hi, Hj, eij), which encodes the relationships between any pair of nodes, utilizing their embeddings Hi, Hj, and optionally incorporating edge embedding eij.\nGraphormer [10] introduces the shortest path distance (SPD) of a shortest path SPij = [e1, 2, ..., eN] connecting vi to vj into the attention mechanism. Graphormer incorporates two types of attention bias. The first spatial bias (vi, vj) encodes the length of SPij and the second, edge encoding Cij, is to aggregate the edge embeddings in SPij. Consequently, the attention mechanism that incorporates structural information can be expressed by Equation (22):\n$A_{ij} = Softmax \\left(\\frac{(H_iW_Q)(H_jW_K)^T}{\\sqrt{d}} + b_{\\phi(v_i, v_j)} + C_{ij} \\right)$, (22)\n$C_{ij} = \\sum_{n=1}^{N} w_{e_n}^T x_{e_n}$, (23)\nwhere b$(vi,vj) is a learnable scalar indexed by $(vi,vj) and remains consistent across all layers. xen and we denote the feature of the n-th edge en in SPij and its corresponding weight, respectively.\nNevertheless, Graphormer-GD [36] identifies that SPD is incapable to adequately distinguish certain perturbations in the graph structure. To address this limitation, Graphormer- GD introduces a more robust relative PE based on resistance distance (RD). The attention matrix with this relative PE is represented in Equation (24):\nA = $\u03a6_1(D) \\odot Softmax \\left(\\frac{HW_Q(HW_K)^T}{\\sqrt{d}} + \u03a6_2(D) \\right)$, (24)\nwhere D \u2208 Rn\u00d7n represents the distance matrix with Dij = {|SPij|, SPij}. Theoretical analysis demonstrates that RD-WL exhibits superior discriminative power compared to SPD-WL, for differentiating non-isomorphic distance-regular graphs.\nGRIT [33] introduces an approach to learning relative PE by the initialization of random walk probabilities in Equation (25):\n$P_{i,j} = [I, M, M^2, ..., M^{K-1}]_{i,j} \u2208 R^K$, (25)\nwhere M = AD-1 denotes the transition probability matrix of random walk. The initialization of P, combined with MLP processing, is proven to approximate SPD. Additionally, graph-diffusion Weisfeiler-Lehman (GD-WL) with P is strictly better than GD-WL based on SPD.\n2) Structure Matrix as Attention Mask: An alternative approach to incorporating structure-aware attention is to perform an element-wise multiplication between attention matrix and a masking matrix, instead of treating the structure matrix as an attention bias. This approach can be formally expressed as Equation (26):\nA = Softmax ((HWQ)(HWK)\u0422\u00a9 M), (26)"}, {"title": "D. Model Ensemble between GNNs and Transformers", "content": "The most straightforward approach for designing GTs involves strategic combinations of GNNs and Transformers, leveraging both local structure patterns and global contextual relationships. As illustrated in Figure 1, these ensemble architectures can be systematically divided into four categories based on the relative positioning of the GNN and Transformer blocks: 1) Sequential GNN-to-Transformer: feed the output from a GNN input a Transformer. 2) Sequential Transformer- to-GNN: feed the output from a Transformer into a GNN. 3) Interleave GNN and Transformer blocks. 4) Parallel GNN and Transformer: feed the graph into GNN and Transformer concurrently, and fuse the output representations into one representation.\nIn the first category, GTs initially process the graph by inputting it into a GNN, which can be regarded as tokenizing the graph at the subgraph level. The GNN aggregates information from local neighborhoods to refine node embeddings. Then, the augmented node embeddings are fed into a Transformer, enabling the model to learn from subgraph tokens, as discussed in Section III-A.\nThe second category of architectures are commonly employed when Transformer blocks have been pretrained. For instance, in the domain of protein data, Transformers [50] have demonstrated effective capabilities in capturing amino acid residue representations. Protein GT frameworks typically exploit pretrained Transformers to generate an initial node represenation, followed by the refinement of GNN regarding the spatial graph structure. A more comprehensive discussion will be provided in Section V-B.\nInterleaving GNN and Transformer blocks, as the third category of model ensemble, is a simple yet effective architecture. For example, Mesh Graphormer [51] interleaves GNN and Transformer blocks to reconstruct human poses, and GROVER [9] adopts a hybrid strategy of combining GNN and Transformer to learn molecular representations.\nBy paralleling GNN and Transformer, GTs can adaptively learn the importance of both local and global information. GraphGPS [27] utilizes the parallel architecture which combines the outputs from a MPNN and a Transformer. In addition, GraphGPS leverages MPNN to update the edge embeddings, which can be utilized to further update the PE. SGFormer [52] theoretically proves that a single-layer attention is sufficiently expressive to capture the global interactions among nodes. Accordingly, SGFormer proposes a simplified GT architecture that incorporates a single-layer self-loop linear attention mechanism alongside GCN blocks. By combining the final representations from the Transformer and GNN, SGFormer exhibits considerable scalability and competitive performance in node property prediction tasks.\nCoBFormer [38] aims to address the issue of over-globalization in GTs. To this end, CobFormer parallelizes"}, {"title": "E. Towards Scalability in Graph Transformer", "content": "Recall that the self-attention mechanism in Transformers introduces a quadratic computational complexity regarding the number of nodes. Since real-world graphs may contain millions or even billions of nodes, Transformers often struggle to scale to large graph efficiently. Thus, designing efficient attention mechanisms for large-scale graphs remains a significant challenge for the scalability of GTs.\nTo reduce the complexity of attention mechanism to linear, one most straightforward approach is to integrating GNNs with linear Transformers. For example, GraphGPS [27] adopts established Transformers that utilize linear attention mechanisms, e.g., combining Performer [53] and BigBird [54] with other GNN modules. Nonetheless, experiments on GraphGPS reveal that although linear attention mechanisms improve scalability, they tend to degrade performance. SGFormer [35], an alternative ensemble-based GT, introduces a linear attention mechanism with self-loop propagation. Theoretical analysis demonstrates that a single-layer attention is sufficient to capture global interactions, enabling SGFormer to achieve scalability and competitive accuracy in node classification tasks.\nCobFormer [38] presents a bi-level global attention module aimed at mitigating the over-globalization issue while simultaneously reducing model complexity. Initially, CobFormer partitions the entire graph into clusters. Subsequently, a bi-level attention mechanism operates at both the intra-cluster and inter-cluster levels, which reduces memory consumption significantly. Similarly, Polynormer [37] introduces a linear framework by polynomial network, where each output element is represented as a polynomial function of the input features. To enable permutation equivariance and combine local and global information, it calculates local attention on neighboring nodes and global attention on the entire graph as the coefficients in the polynomial network.\nA notable limitation of the structural attention mechanism, as discussed in Section III-C, is its difficult applicability to linear attention. This stems from the fact that linear attention mechanisms do not explicitly construct an attention matrix, making it challenging to incorporate structural information through attention bias or attention mask. To this end, Node- Former [29] introduces an edge-level regularization loss as Equation (36) that encourages attention values between connected nodes in a graph are close to 1.0.\n$L_e(A, A_G) = \\frac{1}{N_L} \\sum_{l=1}^{L} \\sum_{(u,v) \\in E} (1 - A_{u,v}^{(l)})^2 \\frac{1}{d_u}$, (36)\nwhere L denotes the total number of layers in NodeFormer, and du represents the degree of node u. Since this loss function only requires computations over edges, NodeFormer efficiently"}, {"title": "F. Geometric Graph Transformers", "content": "Given the wide range of real-world scientific applications involving GTs, the study of geometric GTs is crucial for modeling 3D graph data, such as molecular systems and protein structures. The core design principle of these frameworks lies in ensuring the 3D invariance and/or equivariance of the model. This section briefly reviews start-of-the-art equivariant GTs that have been successfully applied to 3D graphs modeling.\nThe most straightforward approach to learn the structural relationships is incorporating the 3D relative distance as an additional edge embedding, which remains unchanged under Euclidearn transformations. For example, Graphormer [10] introduces spatial encoding, where an MLP is used to encode the relative distance between atoms, effectively capturing structural relationships. This paradigm has demonstrated its efficacy in various frameworks for learning molecular representations [56]. Additionally, other invariant features, such as the angle between edges [57], can be included to represent orientation information. These invariant features are usually encoded using kernel functions, such as the Radial Basis Function [58], to enhance the model's expressivity.\nTorchMD-Net [11] represents another equivariant model that incorporates the interatomic distance rij into its framework. The process begins by projecting rij into two distinct multidimensional filters, denoted as DK and DV, using the following expressions:\n$D_K = \\phi_1(r_{ij}), D_V = \\phi_2(r_{ij})$, (37)\nwhere 01 and 02 are two MLPs. Subsequently, TorchMD-Net replaces the traditional Softmax function with the SiLU function to compute the attention matrix as shown in Equation (38):\nA = SiLU((HW\u2084)(HWK)TDK) \u00b7 $(dij), (38)\nwhere & denotes a cutoff function that assigns the value of 0 whenever dij exceeds a predefined threshold. The final representation is then computed by:\nZ = 03 (AVDV), (39)\nwhere 03 represents another learnable linear transformation. For tasks such as conformation generation, where the model needs to generate atomic coordinates, Uni-Mol [56] proposes a simple SE(3)-equivariant head, represented as:\n$x_i = x_i + \\frac{1}{n} \\sum_{j=1}^{n} (x_i - X_j)C_{ij}$, (40)\nwhere cij represents the learned relationship embedding between node vi and vj. To improve efficiency, Uni-Mol updates the coordinates only in the final layer of the model.\nGVP-Transformer [59] represents an encoder-decoder framework based on the Transformer architecture, designed for the task of protein inverse folding. The model is structured to intake protein structures and subsequently generate corresponding protein sequences. As an encoder, GVP-Transformer utilizes the GVP-GNN [60], capable of extracting features that are translation-invariant and rotation-equivariant, to effectively model protein structures. This is followed by the application of a Transformer decoder to produce valid protein sequences. Examples of high-order steerable GTs include SE(3)-Transformer [12], Equiformer [13], and EquiformerV2 [14]. These models employ equivariant attention mechanisms utilizing higher-degree representations of steerable features [61], which fall beyond the focus of this survey."}, {"title": "IV. THEORIES", "content": "Beyond the practical effectiveness of GTs it is essential to understand the theoretical foundations underlying GTs. This section begins by reviewing the different expressive capabilities among existing GTs (Section IV-A). Subsequently, we investigate the interconnections between GTs and other graph learning methodologies (Section IV-B).\nFollowing the order in Section III, we here respectively discuss the expressivity in structural tokenization, and comparing absolute PE with relative PE.\n1) Structural Tokenization: In node-level tokenization, each node is treated as an independent token, allowing the model to capture local neighborhood information. However, it may struggle to capture global graph structural patterns that span multiple nodes, potentially being insufficient for expressing graph properties that require global information. Therefore, it is necessary to enhance structural bias with additional positional embeddings [36]. Edge-level tokenization can capture the connectivity between nodes, facilitating models to"}, {"title": "V. APPLICATIONS", "content": "GTs have emerged as a versatile and powerful architecture across a wide range of domains, showcasing remarkable capabilities in capturing intricate relationships and learning sophisticated graph representations. This section presents a systematic review of GTs applications across diverse graphs. We categorize these applications by the type of graph, including molecule, protein, social network, textual network, visual network, and other specialized networks. For each category, we provide an in-depth summary on the learning tasks, datasets and methods respectively,\nA. Tasks on Molecule\nMolecules can be represented as graphs where atoms are nodes, and chemical bonds are edges, using multiple dimensional encoding methods. One-dimensional (1D) encoding, like the string representations of SMILES [164] and SELFIES [165], capture topological details of molecules. The tokenizer for these 1D strings serves as a multi-level graph tokenizer, incorporating both node-level and edge-level tokens. The 2D molecular graph naturally represents molecules by depicting atoms as nodes and bonds as edges. This representation enables direct interpretation of molecular connectivity and structural relationships. Additionally, a 3D graph can be constructed to indicate spatial relationships by connecting each atom with its k nearest neighbors. Each dimensional approach provides unique insights into molecular structures, allowing researchers to analyze biochemical systems from complementary perspectives.\nIn this section, we classify molecular downstream tasks into three categories: molecular property prediction, molecular dynamics simulation, and molecular generation. We also discuss molecular pre-training strategies, where GT is a prevailing backbone model that absorbs prior knowledge from immense unlabeled graph data.\n1) Molecular Property Prediction: While vanilla Transformer has significant potential for modeling molecules as SMILES [164], it fails to encode the structural information of molecules. GTs address this limitation by simultaneoutly capturing local and global features in molecules, generating effective molecular embeddings to support diverse downstream tasks, e.g., molecular property prediction.\nTask Definition: Given a 2D molecule graph G = (V,E), with atoms as nodes V and chemical bonds as edges E, a model e is established to classify or predict the target property y of the molecule.\ny = \u03a6\u03b8(G) (41)\nAdditionally, a geometric graph G = (V,E,X) contains an optional input, the 3D coordinates \u2611 \u2208 Rn\u00d73 of n nodes, providing the 3D conformation of the n atoms. The geometric graph G can also be utilized to predict molecular properties, which generally improves the performance compared with the counterpart 2D graph G.\ny = \u03a60(G) (42)\nDataset: The MoleculeNet [166] benchmark includes two types of tasks, classification and regression, for evaluating molecular properties. Classification tasks include BBBP [167], SIDER [168], ClinTox [169], BACE [170], Tox21 [171] and ToxCast [172]. These datasets describe molecular properties from various perspectives, such as permeability property (BBBP), the side effect of drugs (SIDER), toxicity as compounds (ClinTox, Tox21, ToxCast) and inhibitor of human (BACE). Regression tasks contain QM7 [173], QM8 [174], QM9 [175], ESOL [176], Lipophilicity [177] and FreeSolv [178], evaluating the physical chemistry and quantum mechanics of molecules. Specifically, QM7, QM8 and QM9 contain computer-generated molecular properties, such as HOMO/LUMO, atomization energy, electronic spectra and excited state energy. ESOL, Lipophilicity, Free- Solv datasets record the solubility, lipophilicity and free energy of the molecules, respectively. OGB [179] reformulates the datasets from MoleculeNet to build ogbg-molhiv, ogbg- molpcba datasets with additional features. PCQM-Contact, from LRGB [68], requires the model to predict whether the pairs of distant nodes will contact in 3D space. Consequently, this task emphasizes the ability of long-range modeling, which is appropriate for GTs."}, {"title": "VI. FUTURE DIRECTIONS", "content": "The rapid advancement of GTs opens several promising avenues for future exploration. We highlight these research opportunities as follows.\nGraph Foundational Model by Graph Transformer. Transformer has acted as a cornerstone in building foundational models across various domains, such as natural language processing and computer vision. For instance, large language models have demonstrated remarkable performance across various tasks involving language understanding and generation. However, the critical importance of graph-structured data representation in scientific modeling and social network analysis has led to significant interest in Graph Foundational Models [263]. Pioneering works such as GROVER [9] and DPA-2 [88], which construct the atomic model grounded in GTs, pretrain on molecules and crystals using a multi-task methodology, establish a new paradigm for scientific machine learning. These developments highlight the immense potential of GTs as fundamental building blocks for constructing next-generation Graph Foundational Models across diverse application domains.\nScaling Graph Transformer. Despite the remarkable success achieved in scaling Transformer, a question remains whether scaling up the GTs would similarly enhance performance. Uni-Mol2 [92] scales the GT to billions of parameters, showcasing improvements on molecular downstream tasks. This scalability in Uni-Mol2 is feasible due to abundant molecular graph data. However, scaling GTs in domains with limited graph-structured data remains a significant obstacle. Innovative approaches like LM-design [115] utilizes GNNs as structural adapters for pretrained protein language models, integrating both limited structural information and abundant sequence data from existing protein datasets. Despite these advancements, the field still lacks a comprehensive framework that effectively addresses the fundamental challenges of scaling GTs in data-constrained environments.\nCross-modal Graph Transformer. Integrating a GNN with a pretrained Transformer from the language domain allows the model to generate captions for graphs. MolCA [264], an example of cross-modal GT, employs a molecular GNN to encode molecular representations and feeds these embeddings into a Transformer decoder pretained by languages to generate"}, {"title": "VII. CONCLUSION", "content": "In this survey, we conduct a comprehensive review of the recent advancements in Graph Transformer. Our review begins with methods for integrating structural information into the Transformer framework, covering aspects such as multi-level graph tokenization, structural positional encoding, structure-aware attention mechanisms, and model ensemble between GNNs and Transformers. We also introduce two prevalent challenges associated with Graph Transformer, scalability and equivariance. Additionally, we explore theoretical developments to uncover connections"}]}