{"title": "Who's Driving?\nGame Theoretic Path Risk of AGI\nDevelopment", "authors": ["Robin Young"], "abstract": "Who controls the development of Artificial General Intelligence\n(AGI) might matter less than how we handle the fight for control it-\nself. We formalize this \"steering wheel problem\" as humanity's great-\nest near-term existential risk may stem not from misaligned AGI, but\nfrom the dynamics of competing to develop it. Just as a car crash\ncan occur from passengers fighting over the wheel before reaching any\ndestination, catastrophic outcomes could arise from development com-\npetition long before AGI exists. While technical alignment research\nfocuses on ensuring safe arrival, we show how coordination failures\nduring development could drive us off the cliff first.\nWe present a game theoretic framework modeling AGI develop-\nment dynamics and prove conditions for sustainable cooperative equi-\nlibria. Drawing from nuclear control while accounting for AGI's unique\ncharacteristics, we propose concrete mechanisms including pre-registration,\nshared technical infrastructure, and automated deterrence to stabi-\nlize cooperation. Our key insight is that AGI creates network effects\nin safety: shared investments become more valuable as participation\ngrows, enabling mechanism designs where cooperation dominates de-\nfection. This work bridges formal methodology and policy frameworks,\nproviding foundations for practical governance of AGI competition\nrisks.", "sections": [{"title": "Introduction", "content": "The development of Artificial General Intelligence represents one of human-\nity's biggest opportunities and most consequential challenges. While techni-\ncal alignment - ensuring AGI systems pursue human-compatible objectives\nhas dominated safety discussions, we argue that a more immediate existential\nrisk could preempt alignment concerns entirely: catastrophic competition be-\ntween developers on the pathway to AGI. Just as nuclear weapons research\ntriggered arms races and near-catastrophic conflicts long before any bombs\nwere deployed, AGI competition could precipitate disaster before any system\nis actually built. Nation-states and corporations racing to achieve AGI first\nmay not only sacrifice safety precautions or deploy untested systems, they\ncould engage in preemptive strikes or trigger military conflicts based merely\non the perception of rival progress. We formalize this as the steering wheel\nproblem: humanity risks disaster not from the destination (AGI itself), but\nfrom the struggle to control the journey.\nCurrent AGI risk paradigms focus overwhelmingly on technical alignment\nchallenges. While critical, this neglects the structural dynamics that could\nrender alignment irrelevant. Historical precedents from nuclear arms races to\nantibiotic resistance demonstrate how rational actors in competitive systems\noften converge on collectively suboptimal outcomes [Schelling, 1960]. Early\nsigns of analogous dynamics appear in modern AI development: proprietary\nmodel secrecy, compute arms races, and geopolitical preemptive sanctions\nover semiconductor access. Without intervention, AGI development may be-\ncome a prisoner's dilemma where defection (reckless acceleration) dominates\ncooperation (measured, safe progress).\nWhile prior work has rigorously analyzed technical alignment challenges\nin AGI [Bostrom, 2014, Yudkowsky, 2006], three gaps persist in addressing\ncompetitive dynamics. First, Armstrong et al. [2016] formalize AGI races but\nstop short of proposing concrete mechanisms to stabilize cooperation. Sec-\nond, Dafoe [2018] survey governance paradigms but lack equilibrium analyses\nshowing when cooperation could emerge. Third, Critch and Krueger [2020]\nadvocate for ethical norms without grounding them in incentive-compatible\nstructures. Recent work on compute governance [Sastry et al., 2024] and\nmodel staging [Shevlane et al., 2023] provides empirical grounding for our\nmechanisms.\nOur framework bridges these gaps by (1) proving AGI-specific conditions\nfor cooperative equilibria (Theorem 1), (2) coupling this theory with mech-\nanisms like cryptographic pre-registration (\u00a73), and (3) demonstrating how\nnetwork effects in safety investments invert traditional arms race dynamics.\nUnlike nuclear or biological analogs, our approach exploits AGI's dependence"}, {"title": "Model", "content": "The core insight is that AGI's transformative potential creates network\neffects in safety: shared safety investments (verification systems, aligned ar-\nchitectures) become more valuable as more actors participate. Unlike nuclear\nweapons, where cooperation merely limits harm, AGI collaboration can cre-\nate positive-sum gains through accelerated but safer progress. This enables\nmechanism designs where mutual cooperation dominates defection, provided\nkey conditions hold: credible verification, graduated sanctions, and entry\ncontrols.\nOur framework draws lessons from successful governance of dual-use tech-\nnologies:\n\u2022 The International Atomic Energy Agency's (IAEA) verification proto-\ncols\n\u2022 The Biological Weapons Convention's confidence-building measures\n\u2022 CERN's model of shared infrastructure with veto rights\nbut adapts them to AGI's unique technical and strategic landscape.\nThe paper proceeds as follows: Section 2 formalizes AGI competition\ndynamics and establishes theoretical conditions for cooperative equilibria.\nSection 3 translates these into concrete governance protocols. We conclude\nwith research directions for empirically validating and scaling this framework.", "Players and Actions": null}, {"title": "Players and Actions", "content": "Let N = {1,...,N} denote the set of AGI developers (nation-states and\nprivate labs). Each player i \u2208 N is characterized by:\n\u2022 Compute resources: c\u00bf ~ LogNormal(\u03bc\u03b5, \u03c3\u03b5)\n\u2022 Technical expertise: ei \u2208 [0, 1], where ei 1 represents cutting-edge\ncapability\n\u2022 Risk tolerance: pi \u2208 [0, 1], modulating defection incentives\n\u2022 \u0444: Network multiplier coefficient (small positive value, e.g. $ = 0.1)\nAt each discrete timestep t, players simultaneously choose:\n\u2022 Development strategy: a\u017c(t) \u2208 {Cooperate, Defect}"}, {"title": "State Dynamics", "content": "The system evolves via:\nTechnical capability: Ti(t + 1) = Ti(t) + ari(t)ciei (1+ysi(t)) \nShared knowledge: K(t + 1) = K(t) + \u03b2\u2211si(t)Ti(t) \niEN\nVerification status: Vi(t + 1) = Iaudit(i) \u00b7 Bernoulli(paudit)\nSecurity commitments: S(t + 1) = \u2211 Vi(t+1)Ti(t+1)\niEN\nwhere laudit (i) = 1 if player i is randomly selected for a verification audit\nin timestep t. The term Bernoulli(paudit) indicates that if player i is audited,\nverification succeeds with probability Paudit (and fails with probability 1\nPaudit)."}, {"title": "Payoff Structure", "content": "Player i's utility at time t:\nU\u2081(t) = XT\u2081(t) + \u00b5K(t) + psi(t)K(t) + \u03c3S(t) - \u03be \u03a3\u03a4; (t) - (nri(t)\u00b2 + 0(1 \u2013 Vi(t)))\nj\u2260i\nTotal discounted utility: \u0395[\u03a3\u03c4\u03bf dtU;(t)], where \u03b4\u2208 (0,1) is the discount\nfactor."}, {"title": "Security Commitments", "content": "The shared security infrastructure S(t) represents collective enforcement ca-\npacity (verification networks, sanctions implementation) that grows through\ncontributions from verified players:\nS(t + 1) = \u2211 Vi(t)T;(t)\niEN"}, {"title": "Key Assumptions", "content": "\u2022 Vi(t) \u2208 {0, 1} indicates verification status\n\u2022 Ti(t) represents player i's technical capability\nThe security term oS(t) in (5) captures the deterrence benefits from:\n\u2022 Shared threat intelligence between verified players\n\u2022 Collective sanctions against defectors\n\u2022 Reduced preemptive strike risks through transparency"}, {"title": "Key Assumptions", "content": "Assumption 1 (Bounded Rationality). Players use trigger strategies condi-\ntioned on public history ht = {K(0), . . ., K(t), S(0), . . ., S(t)}.\nAssumption 2 (Imperfect Monitoring). Verification audits detect defection\n(ai = Defect) with probability Pdetection = 0.8, reflecting real-world inspection\nlimits. This means that if a player is audited and is indeed defecting, the\naudit will correctly identify defection with probability Pdetection\u00b7\nAssumption 3 (Dynamic Entry). New players join via Poisson process with\nrate x = 0.05, initializing with Tnew ~ Uniform(0,T)."}, {"title": "Equilibrium Analysis", "content": "Theorem 1 (Cooperative Equilibrium Existence). Under conditions:\n1. Network effects dominate: \u03b2 > \u03b3 + \u03be/\u03bc\n2. Verification is affordable: 0 < 0max = \u03bc\u00b2/8\n3. Punishments are credible: \u00a7 > Emin = \u03bb\u03b1\nthere exists a subgame perfect equilibrium where \u2200i, t, a\u00bf(t) = Cooperate, si(t) =\n1.\nProof Sketch. Applying Fudenberg and Maskin [1986] folk theorem for in-\nfinitely repeated games with perfect public monitoring, cooperation is sus-\ntainable if:\n\u03b4\u2265 \u03c0Defect - \u03c0Cooperate/\u03c0Defect - \u03c0Punishment \nWhere:"}, {"title": "Equilibrium Analysis", "content": "\u2022 Cooperate = \u03bc\u03b2 (knowledge growth benefits)\n\u2022 Defect = \u03bb\u03b1 (private capability gains)\n\u2022 Punishment = \u03be (security losses)\nCondition 3 ensures punishment credibility:\n\u03b4(\u03c0Cooperate - \u03c0Punishment) > \u03c0Defect - \u03c0Cooperate"}, {"title": "Discussion", "content": "Theorem 2 (Defection Prevention). For mechanisms M = (pre-registration, staged deployment),\nthe defection probability satisfies:\nP(Defect) < (Paudit-freq, \u00a7, T) = 1/1+ Paudit-freqT\nwhere Paudit-freq represents the frequency of verification audits, influenced by\nmechanisms like pre-registration, and \u315c represents the planned duration until\nthe next major agreed-upon capability milestone in AGI development.\nIntuition. Pre-registration mechanisms are designed to increase the frequency\nof verification audits (Paudit-freq \u2191, meaning audits are conducted more often\nacross the consortium). Staged deployment raises synchronization (7 \u2191), and\nsanctions boost penalty severity (\u0123\u2191). Their combined effect, represented\nby the product Paudit-freq\u0123\u012b, suppresses defection incentives, leading to the\ndefection probability decreasing as e(paudit-freq, \u03be, \u03c4) becomes smaller.\nIntuitive Explanation of Supermodularity While a formal mathemat-\nical proof of supermodularity in this dynamic game is complex, the intuition\nbehind why we expect payoffs to exhibit supermodularity in information\nsharing is quite straightforward and stems from the network effects inherent\nin knowledge creation.\nSupermodularity, in this context, means that the incentive for player i\nto increase their information sharing (s\u2081 = 1) is strengthened when other\nplayers, such as player j, also increase their information sharing (s; = 1).\nThis positive feedback loop arises because of the shared knowledge pool K(t).\nWhen player i shares information, it contributes to the growth of K(t+1).\nCrucially, when player j also shares information, they further contribute\nto K(t + 1), making the shared knowledge pool even larger. Because all\nplayers benefit from a larger K(t) through the economic payoff term \u00b5K(t)+"}, {"title": "Discussion", "content": "psi(t)K(t), the combined effect of both i and j sharing is greater than the\nsum of their individual effects if they were to share in isolation.\nIn essence, by contributing to a larger, richer shared knowledge base, each\nplayer enhances the returns for all participants, including themselves. This\npositive externality and reinforcing loop is the core of the \"network effects\nin safety\" we emphasize. Cooperation in information sharing becomes more\nattractive as more actors participate, creating a virtuous cycle that can help\nsustain a cooperative equilibrium."}, {"title": "Discussion", "content": "The existential risk posed by AGI development lies not merely in the tech-\nnical challenge of alignment but in the structural incentives that govern how\nactors compete to build it. Imagine a high-stakes game where players, nation-\nstates and private labs, must balance short-term tactical advantages against\ncollective survival. Each lab wields unique resources: compute clusters rival-\ning small nations, proprietary datasets, or algorithmic/architectural secret\nrecipes guarded like state secrets. At every turn, they face a dilemma: share\nprogress to lift the collective ceiling of knowledge or hoard advances to seize\na lead.\nThis tension is formalized through three interlocking systems. First, tech-\nnical capability evolves dynamically where investing resources accelerates\nprogress, but secrecy breeds inefficiency, as isolated efforts miss the com-\npounding gains of open collaboration. A lab that defects (prioritizing speed\nover transparency) might spike its capability temporarily, but at the cost of\neroding the shared knowledge pool that sustains long-term progress. Second,\nverification mechanisms act as the immune system of cooperation: random-\nized audits, inspired by nuclear inspection regimes, detect defection with an\n80% reliability, injecting accountability into an otherwise opaque race. Third,\nsecurity commitments quantify trust, rewarding verified collaborators with\npreferential access to collective resources while isolating defectors.\nThe payoff structure mirrors real-world tradeoffs. Economic gains flow\nfrom both private breakthroughs and shared knowledge, but shadowing this\nis the existential dread of rivals pulling ahead, a fear quantified as a security\npenalty proportional to others' progress. Costs escalate nonlinearly, modeling\nthe reality that scaling compute yields diminishing returns (10,000 GPUs\naren't 10x better than 1,000). Players thus walk a tightrope: maximize\nprivate gains without triggering a collapse of cooperation that would leave\nall worse off.\nCooperation stabilizes only under precise conditions. Network effects\nwhere each participant's contributions amplify the value of others' must out-"}, {"title": "Mechanism Design", "content": "The existential risks inherent in AGI development demand more than tech-\nnical safeguards; they require concrete, verifiable mechanisms that transform\ncompetitive instincts into sustainable cooperation. Drawing lessons from hu-\nmanity's most successful governance experiments while accounting for AGI's\nunique challenges, we propose a framework centered on three interlocking\nmechanisms that align individual incentives with collective survival."}, {"title": "Imaginary Consortium", "content": "To illustrate how our theoretical framework might translate into practice,\nwe present one possible instantiation of an AGI Development Consortium.\nWhile this structure is necessarily speculative and alternative implementa-\ntions might prove superior, exploring a concrete example helps illuminate\nboth the potential and challenges of our proposed mechanisms.\nThe consortium could establish objective technical thresholds for partic-\nipation, creating natural membership tiers based on demonstrable capabil-\nities. Inner circle membership might require both significant compute re-\nsources (e.g., 20% of current leading labs' capacity) and demonstrated tech-\nnical achievement (such as reaching 80% of SOTA performance on standard\nbenchmarks). This two-factor requirement would help ensure members can\nmeaningfully contribute while preventing resource-rich but technically lim-\nited actors from gaining undue influence."}, {"title": "Imaginary Consortium", "content": "Recent developments illustrate why such graduated access could be valu-\nable. The emergence of labs like Deepseek demonstrates how new players can\nrapidly achieve significant capabilities, suggesting the importance of clear\npaths to legitimate participation. Under this structure, core membership\nwould grant real-time access to the shared weight pool and full voting rights\non consortium decisions, while creating corresponding obligations for regular\ncontribution and transparency.\nThe consortium might establish three membership tiers:\nCore members meeting both compute and technical thresholds would gain\nfull access to shared weights and checkpoints, voting rights on major deci-\nsions, and seats on technical oversight committees. This inner circle would\nlikely include major AI labs and national programs with demonstrated ca-\npabilities. In exchange, they would commit to complete weight sharing, pre-\nregistration compliance, and compute contribution to shared infrastructure.\nAssociate members meeting one but not both thresholds would receive\ndelayed access to shared resources and limited voting rights, with a clear\npath to core membership through capability demonstration. This tier could\ninclude emerging labs showing technical promise but still building compute\ncapacity, or well-resourced organizations developing technical expertise.\nObserver status would enable monitoring and partial participation while\nworking toward thresholds, creating transparency without compromising the\nconsortium's technical standards. This could include academic institutions,\nsafety research organizations, and emerging players.\nThis tiered structure, while just one possible implementation, illustrates\nhow theoretical mechanisms might be instantiated. The specific thresholds,\nvoting structures, and access rights would need careful calibration based on\nempirical evidence and stakeholder input. Alternative arrangements empha-\nsizing different criteria or organizational principles might ultimately prove\nmore effective.\nCritical questions remain about optimal threshold levels, voting mecha-\nnisms, and enforcement protocols. How should technical capability be mea-\nsured? What compute commitments create meaningful skin in the game\nwithout being prohibitively exclusive? How should voting rights be weighted\nbetween different member categories? These implementation challenges high-\nlight the need for further research and experimentation.\nThe key insight is that any practical instantiation must balance inclusiv-\nity with meaningful standards, create clear incentives for participation and\ncompliance, and establish objective criteria for membership privileges. While\nthis particular structure may not be optimal, it demonstrates how theoreti-\ncal principles of verification, graduated access, and automated enforcement\ncould be translated into concrete institutional frameworks."}, {"title": "Mechanisms", "content": "Pre-registration acts as the cornerstone, establishing a foundation of verifi-\nable development transparency. Before initiating any major training run, de-\nvelopers must publicly commit to detailed technical specifications\u2014architecture\nblueprints, parameter ranges, and compute allocation plans through cryp-\ntographic commitments that prevent retrospective manipulation. This cre-\nates an immutable development covenant that enables verification without\nexposing proprietary innovations. The mechanism leverages zero-knowledge\nproofs to verify compliance without accessing trade secrets: a lab could prove\nits model stayed within pre-registered parameter bounds without revealing\nexact architectural details, akin to how nuclear inspectors verify enrichment\nlevels without accessing classified designs.\nWeight and checkpoint sharing forms the core of the cooperative\nframework, creating powerful network effects that make collaboration in-\ncreasingly attractive as more actors participate. This mechanism requires\nconsortium members to regularly share model weights and training check-\npoints in a \"closed-open source\u201d fashion, establishing a growing pool of tech-\nnical capability that benefits all participants. The system operates on grad-\nuated access: inner circle members who meet technical and transparency\nrequirements gain real-time access to the cutting edge of AGI development,\nwhile contributing their own advances to the shared pool. This creates a\nnatural incentive structure where the benefits of cooperation such as access\nto the combined insights and capabilities of all participants increasingly out-\nweigh any potential advantages of solitary development.\nAutomated deterrence completes the framework by making defection\nboth immediately detectable and swiftly punishable. The system monitors\ntechnical metrics including compute usage patterns, checkpoint consistency,\nand training trajectories to verify compliance with pre-registered plans. Vi-\nolations trigger automatic consequences: access to the shared weight pool\nis immediately revoked, compute resources are restricted, and consortium\nmembership is suspended. These sanctions activate within 72 hours of veri-\nfied non-compliance, eliminating the political wrangling that has historically\nweakened international governance frameworks. The system includes gradu-\nated responses, from warnings for minor deviations to full exclusion for major\nbreaches, with clear paths to redemption through verified compliance."}, {"title": "Mechanisms Discussion", "content": "This mechanism design transforms AGI development from a competitive race\ninto a cooperative endeavor where shared progress dominates unilateral at-"}, {"title": "Mechanisms Discussion", "content": "tempts. By making weight and checkpoint sharing the central cooperation\nmechanism, we create concrete, verifiable protocols that align individual in-\ncentives with collective safety. The system acknowledges but mitigates in-\nevitable challenges in the form of technical verification complexity, propri-\netary concerns, and coordination overhead through careful mechanism design\nthat makes cooperation the dominant strategy for all participants who seek\nto remain at the cutting edge of AGI development.\nThe framework provides several theoretical guarantees. First, coopera-\ntion becomes verifiable through concrete technical metrics rather than sub-\njective assessments. Second, network effects in the shared weight pool grow\nstronger over time, increasing the cost of defection. Third, automated en-\nforcement ensures swift and predictable consequences for non-compliance,\ncreating credible deterrence. Together, these mechanisms aim to transform\nthe prisoner's dilemma of AGI development into a coordination game where\nmutual cooperation emerges as the rational equilibrium strategy.\nThis is not utopian institutionalism but hard-nosed mechanism design, a\nrecognition that cooperation emerges not from goodwill but from carefully\nengineered incentive structures. Just as TCP/IP's packet acknowledgment\nsystem transformed the anarchic early internet into a reliable communica-\ntion network, these protocols aim to channel AGI's disruptive potential into\nsurvivable innovation pathways."}, {"title": "Limitations and Future Work", "content": "The framework proposed here is not a final roadmap but a first sketch of how\nhumanity might collectively steer toward AGI without crashing the car. Like\nall early navigation systems, it has blind spots and unresolved challenges\nthat demand scrutiny."}, {"title": "Limitations", "content": "Assumptions of Rationality pose the most pressing constraint. Our model\npresumes actors will consistently prioritize long-term survival over short-term\ngains, a stance contradicted by historical arms races and modern AI's move\nfast and break things ethos. In the car metaphor, this assumes passengers\nwill never prefer a reckless shortcut over the safe route, even if it risks a\ncrash.\nVerification Gaps further complicate the picture. Cryptographic pre-\nregistration and probabilistic audits, while innovative, cannot eliminate all\nforms of obfuscation. A lab could train a compact AGI seed model below"}, {"title": "Limitations", "content": "detection thresholds, then rapidly scale it post-pause akin to hiding a nitro\nbooster in the trunk until the last straightaway.\nGeopolitical Fragmentation threatens to split the car into rival vehi-\ncles if political will is not capable of aligning cooperative intents. Our con-\nsortium model assumes a single governance structure, but reality may yield\ncompeting blocs (e.g., U.S.-aligned vs. China-aligned AGI ecosystems). This\nechoes Cold War space races, where duplicated efforts wasted resources but\navoided direct conflict, a suboptimal but plausible equilibrium.\nDynamic Entry of new actors (startups, rogue states) remains under-\ndeveloped. Current mechanisms focus on known players, much like air traffic\ncontrol struggles with rogue drones. Without provisions for rapid onboarding\nand enforcement, a single defector could hijack the steering wheel."}, {"title": "Future Directions", "content": "Three pathways could address these gaps while advancing the metaphor:\nBehavioral Steering Wheels would integrate prospect theory into\ngame dynamics, modeling how real actors, not hyper-rational agents, respond\nto existential risks. Experiments could test whether framing milestones as\nlosses to avoid (crash scenarios) versus gains to achieve (safe AGI) alters\ncooperation rates.\nAnomaly Detection Systems would operationalize the car's dashboard\nwarnings through ML monitoring of compute markets, energy grids, and\nhardware supply chains. Federated learning on encrypted infrastructure logs\ncould spot hidden training runs, much like anti-lock brakes preempt skids.\nCrash Test Simulations using agent-based models would stress-test\nthe framework under adversarial conditions:\n\u2022 Sudden capability jumps (analogous to black ice)\n\u2022 Coalitional defection (e.g., multiple labs bypassing milestones)\n\u2022 External shocks (cyberattacks, pandemics)\nThe road to safe AGI remains uncharted, but our choice is clear: drive\ntogether with imperfect maps or crash separately in the dark."}, {"title": "Conclusion", "content": "Humanity's journey toward AGI is not a solo race but a shared voyage in a\nsingle vehicle, one where passengers fighting over the steering wheel risk ca-\nreening into the abyss long before reaching their destination. The existential"}, {"title": "Conclusion", "content": "stakes of this metaphor cannot be overstated: our species' survival at the\npresent moment depends less on who controls the wheel than on whether we\ncollectively avoid crashing the car. This paper has argued that the greatest\nnear-term danger lies not in AGI's hypothetical misalignment, but in the\nall-too-human impulse to yank the wheel through reckless competition.\nThe mechanisms proposed here: pre-registration, staged deployment, and\nautomated deterrence, are institutional seatbelts for this precarious ride.\nBy synchronizing development timelines, they ensure no actor can suddenly\nswerve the car into uncharted risks. By enforcing transparency, they replace\nthe fog of suspicion with a shared dashboard of progress. By automating\nsanctions, they make reckless acceleration as irrational as slamming the gas\npedal toward a cliff. These are not constraints on innovation but guardrails\nfor a survivable journey.\nCritics may argue that such cooperation is politically naive, pointing to\nthe collapse of climate treaties or the chaos of AI governance today. But\nhistory shows that even bitter rivals can coordinate when stakes are existen-\ntial: U.S. Soviet arms control during the Cuban Missile Crisis, Allied-Axis\nscientific collaboration post-WWII, and the global eradication of smallpox\nall demonstrate humanity's capacity to steer collectively under threat. What\ndistinguishes AGI is the asymmetry of urgency: once the car veers into a\ncrash trajectory, no individual can correct course.\nThis framework rejects both techno-utopianism and fatalism. It acknowl-\nedges that AGI's steering wheel cannot be relinquished. Someone must drive,\nbut the wheel be designed for shared control through aligned incentives and\ngoals. Labs and states need not dissolve their ambitions, only channel them\nthrough protocols that align self-interest with collective survival. The alter-\nnative is a scramble for unilateral advantage that has the passengers wrestling\nover the wheel at highway speeds, ensuring catastrophe regardless of who\n\"wins.\"\nThe path forward is clear but narrow. Near-term actions for establish-\ning compute registries, piloting milestone pauses, and formalizing deterrence\nprotocols must precede AGI's emergence, just as seatbelts must be buckled\nbefore the car moves. Delay risks locking in competition dynamics that no\nmechanism can later unwind.\nIn the end, this is not a call for altruism but for rational self-preservation.\nWhen all passengers share the same fragile vehicle, cooperation ceases to be\nidealism and it becomes the only sane strategy. The steering wheel is in our\nhands; the choice is whether to fight over it or steer together."}]}