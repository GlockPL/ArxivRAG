{"title": "Cost-Saving LLM Cascades with Early Abstention", "authors": ["Michael J. Zellinger", "Rex Liu", "Matt Thomson"], "abstract": "LLM cascades are based on the idea that processing all queries with the largest and most expensive LLMs is inefficient. Instead, cascades deploy small LLMs to answer the majority of queries, limiting the use of large and expensive LLMs to only the most difficult queries. This approach can significantly reduce costs without impacting performance. However, risk-sensitive domains such as finance or medicine place an additional premium on avoiding model errors. Recognizing that even the most expensive models may make mistakes, applications in these domains benefit from allowing LLM systems to completely abstain from answering a query when the chance of making a mistake is significant. However, giving a cascade the ability to abstain poses an immediate design question for LLM cascades: should abstention only be allowed at the final model or also at earlier models? Since the error patterns of small and large models are correlated, the latter strategy may further reduce inference costs by letting inexpensive models anticipate abstention decisions by expensive models, thereby obviating the need to run the expensive models. We investigate the benefits of \"early abstention\" in LLM cascades and find that it reduces the overall test loss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA, TruthfulQA, and XSum). These gains result from a more effective use of abstention, which trades a 4.1% average increase in the overall abstention rate for a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings demonstrate that it is possible to leverage correlations between the error patterns of different language models to drive performance improvements for LLM systems with abstention.", "sections": [{"title": "Introduction", "content": "Following the proliferation of publically available LLMs, many authors have proposed combining different models into LLM systems to enhance reliability and reduce costs. Specifically, LLM cascades have emerged as a powerful design pattern for navigating the trade-off between performance and cost when deploying LLMs (Chen et al. (2023), Aggarwal et al. (2024), Yue et al. (2024), Jitkrittum et al. (2024)).\nThe basic idea behind cascades is that it would be inefficient to process all queries using the largest and most expensive LLM since, on many tasks, relatively small LLMs can correctly answer most queries. Operationally, a cascade first sends each query to a small LLM. If this small model has a high chance of correctly answering the query - as measured by a confidence indicator - the cascade returns its output to the user. Otherwise, the cascade defers the query to a larger and more expensive model. By using small language models to answer easy queries, LLM cascades yield impressive cost reductions without meaningfully compromising performance. However, lowering cost is only one of the goals of an LLM deployment. In risk- sensitive areas such as finance, law, or medicine, the overriding concern is often that mistakes must be avoided at any cost. In such domains, it is natural to give LLMs the ability to abstain from answering a query when the risk of making a mistake is significant.\nHowever, the correlation between the error patterns of large and small LLMs (Zellinger and Thomson (2025)) poses an immediate design question when considering cascades with abstention. Is it sufficient to allow the final model to abstain, or should abstention be possible even for a small model at the beginning of the cascade? Since the error patterns of small and large models are correlated, there is a potential for smaller models to anticipate abstention decisions by larger models. In such cases, the cascade may abstain much more cheaply by avoiding inference of the larger and more expensive models."}, {"title": "Background and Related Work", "content": "This paper combines two distinct strands of research within the large language model (LLM) community: selective prediction, which aims at reducing the error rate by allowing models to abstain on the most difficult queries, and LLM cascades, which focus on combining small and large language models to reduce cost.\nSelective prediction. Selective prediction focuses on giving machine learning models the ability to abstain on difficult queries (El-Yaniv and Wiener (2010)). This perspective is especially useful in risk-sensitive domains, where the cost of making a mistake far outweighs the inconvenience of handling abstained queries. The field goes back to the works of Chow (Chow (1957), Chow (1970)), who analyzed abstention in the context of optical character recognition for scanning business documents. Geifman and El-Yaniv (2017) applied selective prediction in deep learning, obtaining marked reductions in error rates on image classification.\nXin et al. (2021) and Yoshikawa and Okazaki (2023) first applied selective prediction in natural language processing by thresholding various confidence scores derived from a language model's conditional probability distribution. In the modern era of large language models (LLM) following the release of ChatGPT, research on selective prediction has largely taken place in the emerging field of uncertainty quantification for LLMs (Manakul et al. (2023), Farquhar et al. (2024), Lin et al. (2024)). In this field, authors proposing a novel un- certainty score typically evaluate performance by reporting the area under the curve of conditional accuracy vs the proportion of rejected queries (AUARC).\nLLM cascades. LLM cascades leverage confidence scores in a different way from selective prediction. Rather than abstain on difficult queries, a cascade adaptively selects the most cost-effective model to answer each query. An incoming query first goes to a small LLM. If the small model's confidence score is below a chosen threshold, the cascade forwards the query to a larger and more powerful LLM. This approach has yielded impressive cost savings without impacting performance (Chen et al. (2023), Aggarwal et al. (2024), Yue et al. (2024)). Cascades often make use of an LLM's conditional probability distribution for computing confidence scores (Jitkrittum et al. (2024)). However, the question of efficiently selecting confidence thresh- olds has not received much attention. Zellinger and Thomson (2025) have recently proposed a continuous optimization-based algorithm for optimizing the confidence thresholds of the LLMs in a cascade, which is based on probabilistically modeling the joint distribution of LLM confidence scores. We build on this work by extending their optimizer to give cascades the ability to abstain."}, {"title": "Cost-Sensitive LLM Cascades with Abstention", "content": "Cascades with abstention. Let $C = M_1 \\rightarrow \\ldots \\rightarrow M_k$ be a large language model (LLM) cascade. On query $x$, each model $M_i$ uses a confidence score $\\Phi_i = \\Phi_i(x) \\in [0,1]$ to decide whether to answer the query or defer to $M_{i+1}$. To compute a confidence score, we follow the approach of Zellinger and Thomson (2025), which uses logistic regression to calibrate a raw confidence signal $p_{\\text{raw}}(x)$ based on single-token probabilities. On multiple-choice tasks, $p_{\\text{raw}}$ is the maximum probability across answer choices (Hendrycks and Gimpel (2018)), whereas for natural language generation, $p_{\\text{raw}}$ is the probability that the model answers \u201cY\u201d or \u201cN\u201d when asked to verify correctness of its previously generated answer (Kadavath et al. (2022), Zellinger and Thomson (2025)).\nIn a traditional LLM cascade, a model $M_i$ returns the answer to a query if its confidence $\\varphi_i$ exceeds a deferral threshold $\\phi_i$. To give an LLM cascade the ability to abstain on queries, we pair each deferral threshold $\\phi_i$ with an additional abstention threshold $\\xi_i < \\phi_i$. With this formalism, on query $x$ the output $C(x)$ of a cascade $C = M_1 \\rightarrow \\ldots \\rightarrow M_k$ with deferral thresholds $(\\phi_1,..., \\phi_{k-1})$ and abstention thresholds $(\\xi_1, ..., \\xi_k)$ is given recursively by\n$C(x) = \\begin{cases} M_1(x) & \\text{if } \\Phi_1(x) > \\phi_1 \\text{ or } |C| = 1 \\\\ C_{2:k}(x) & \\text{if } \\xi_1 < \\Phi_1(x) < \\phi_1, \\\\ \\O & \\text{if } \\Phi_1(x) < \\xi_1, \\end{cases}$\nwhere $C_{2:k}$ is the subcascade $M_2 \\rightarrow \\ldots \\rightarrow M_k$, and returning $\\O$ means that the cascade abstains on the query. Note that the final model $M_k$ of the cascade has an abstention threshold but no deferral threshold, since there is no downstream model to which it could defer the query.\nThe rationale for adding abstention thresholds $\\xi_i < \\phi_i$ is that the calibrated confidences of different LLMs are often correlated (Zellinger and Thomson (2025)). Hence, if the confidence $\\Phi_i$ of an earlier model in the cascade is very low, the final model $M_k$ may be more likely to abstain. Anticipating such abstentions by letting earlier models $M_j (j < k)$ directly abstain reduces inference costs by avoiding inference of the subsequent models $M_{j+1}, M_{j+2}, ..., M_k$, which are progressively larger and more expensive to run.\nMulti-Objective Performance Evaluation. For traditional LLM cascades, performance is determined by the tuple $(e, c)$ of error rate $e$ and expected cost $c$. When considering abstention, this tuple expands to the triplet $(e, c, a)$ consisting of error rate, expected cost, and abstention rate. For traditional LLM cascades, it is straightforward to evaluate performance by comparing curves of error vs cost. When incorporating abstention, however, the optimal trade-offs form a two-dimensional surface within the three-dimensional space of performance triplets $(e, c, a)$. Comparing such optimal surfaces poses practical challenges since it involves nonlinear geometry in 3D.\nTo provide a user-friendly framework for evaluating and comparing the multi-objective performance of different LLM cascades, we model a user's preferences for reducing cost and avoiding abstentions by introducing the cost sensitivity $\\Lambda_{\\text{cost}} > 0$ and abstention sensitivity $\\Lambda_{\\text{abs}} \\geq 0$. We define a cost-sensitive LLM cascade with abstention as one whose deferral and abstention thresholds $\\theta = [\\phi, \\xi] = [\\phi_1, ..., \\phi_{k-1}, \\xi_1, ..., \\xi_k] \\in \\mathbb{R}^{2k-1}$ solve the minimization problem\n$\\theta^* = \\arg \\min_\\theta \\mathbb{P}_{\\theta}(\\text{Error}) + \\Lambda_{\\text{cost}} \\mathbb{E}_{\\theta} [\\text{Cost}] + \\Lambda_{\\text{abs}} \\mathbb{P}_{\\theta}(\\text{Abstention}).$\nThe objective (2) aligns with prior work on selective prediction (Cortes et al. (2016)); it corresponds to a loss function that penalizes an incorrect answer by 1, an abstention by $\\Lambda_{\\text{abs}}$, and each incremental unit of cost (typically dollars per million queries) by $\\Lambda_{\\text{cost}}$.\nTo solve the minimization problem (2), we extend the continuous optimization-based approach for tuning deferral thresholds described by Zellinger and Thomson (2025). This approach involves fitting a probabilistic Markov-copula model to the joint distribution of LLM confidence scores. To enforce the constraints $\\xi_i < \\phi_i$ that the abstention thresholds are smaller than the corresponding deferral thresholds, we use the sequential least squares optimizer as implemented in SciPy (Virtanen et al. (2020)). We make our implementation available as part of the niagara Python package for constructing LLM cascades from LLMs available via public APIs."}, {"title": "Results", "content": "In this section, we describe the results of comparing early abstention to final-model abstention. Our main findings are that\n\u2022 early abstention lowers overall test loss by 2.2% on average,\n\u2022 the benefits of early abstention are concentrated in the upper right quadrant of the user preference space, where cost is a concern and abstention is not heavily penalized,\n\u2022 early abstention makes more effective use of the ability to abstain, trading a 4.1% increase in the overall abstention rate for an average cost reduction of 13.0% and an average error reduction of 5.0%.\nLanguage model names. In the tables that follow, we abbreviate the names for language models as follows: L1B through L405B refer to the Llama3 family of models with 1B through 405B parameters; Q32Bc refers to the Qwen2.5 32B Coder model, and Q72B refers to the Qwen2.5 72B model; 40-Mini refers to OpenAI's GPT-40 Mini and 40 refers to GPT-40. We use the instruction-tuned (\u201cinstruct\u201d) versions of all models.\nEarly abstention reduces overall test loss. Table 1 shows the percentage change in overall test loss resulting from allowing the small model in a two-model cascade to abstain directly. On six benchmarks, allowing early abstention lowers the overall test loss (2) by 2.2% on average. On five out of six benchmarks (GSM8K, MedMCQA, MMLU, XSum, and TruthfulQA), early abstention reduces the test loss by 3.1% on average, ranging from 1.2% on MMLU to 7.3% on XSum. Only on TriviaQA, early abstention performed worse, raising the test loss by 2.0%.\nThe benefits are concentrated in the upper right quadrant of user preference space. Figure 1 shows heatmaps of the percentage changes in overall test loss across the user preference space ($\\Lambda_{\\text{cost}}$, $\\Lambda_{\\text{abs}}$), using the cascade Llama3.2 1B \u2192 Llama3.1 405B as an example. The figure illustrates that the benefits of early abstention are highly concentrated in the upper right quadrant of the user preference space, where cost is a concern and the penalty on abstention is not severe. This trend holds generally across cascades."}, {"title": "Conclusion", "content": "In this work, we investigated the question of early abstention in cascade design: when designing an LLM cascade capable of abstaining from difficult queries, does it make sense to allow small models earlier in the cascade to abstain, compared to only abstaining at the final model? Our results indicate that early abstention improves overall performance, as measured by a 2.2% average reduction in overall test loss across six language modeling benchmarks. We find that the benefits are concentrated in the upper right quadrant of the user preference space, where cost is a concern and abstention is not heavily penalized. Overall, early abstention allows a cascade to make more effective use of the ability to abstain, trading a moderate increase in the abstention rate (+4.1% on average) for significant cost savings (-13.0% on average) and some error reduction (-5.0% on average).\nThese positive results are based on the correlations between the error patterns of small and large LLMs. On one level, our findings show that these correlations can be leveraged to improve the performance of an LLM system with abstention. Viewed more broadly, an LLM cascade with early abstention is an example of an integrated LLM system that incorporates interactions between models at all levels to improve performance. As the LLM research community increasingly moves towards ensembles of LLMs collaborating with each other to solve tasks more efficiently and reliably, we expect that leveraging all available interactions between models will become critical for optimizing LLM systems."}, {"title": "Appendix A: Smoothing Procedure", "content": "We apply light smoothing on the grid of optimal thresholds to avoid potential artifacts from numerical optimization. Our procedure consists of two steps:\n\u2022 Step 1: Identify outliers\n\u2022 Step 2: Replace outliers by the mean values of their neighbors\nIn Step 1, we call a threshold vector \u0472 an outlier if $9 := \\sum_{n=1}^4 \\theta_n$ satisfies\n$\\frac{1}{|N|} (\\sum_{n \\in N} \\theta_n)^2 > \\text{Var}(\\theta_n, n \\in N)$,\nwhere n \u2208 N indexes the set of neighboring optimal thresholds on the 2D grid (an interior point has four neighbors).\nReported results in the paper user r = 10. This results in smoothing 3.5% of data points for early abstention and 2.7% of data points for final-model abstention."}, {"title": "Appendix B: Detailed Changes in Error, Cost, and Abstention", "content": "Below we provide detailed results on the changes in the test error rate (Table 3), expected cost (Table 4), and abstention rate (Table 5) resulting from using early abstention, compared to abstaining only at the final model."}]}