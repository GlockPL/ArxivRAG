{"title": "Beyond Labels: Aligning Large Language Models with Human-like Reasoning", "authors": ["Muhammad Rafsan Kabir", "Rafeed Mohammad Sultan", "Ihsanul Haque Asif", "Jawad Ibn Ahad", "Fuad Rahman", "Mohammad Ruhul Amin", "Nabeel Mohammed", "Shafin Rahman"], "abstract": "Aligning large language models (LLMs) with a human rea- soning approach ensures that LLMs produce morally correct and human- like decisions. Ethical concerns are raised because current models are prone to generating false positives and providing malicious responses. To contribute to this issue, we have curated an ethics dataset named Dataset for Aligning Reasons (DFAR), designed to aid in aligning language mod- els to generate human-like reasons. The dataset comprises statements with ethical-unethical labels and their corresponding reasons. In this study, we employed a unique and novel fine-tuning approach that uti- lizes ethics labels and their corresponding reasons (L+R), in contrast to the existing fine-tuning approach that only uses labels (L). The origi- nal pre-trained versions, the existing fine-tuned versions, and our pro- posed fine-tuned versions of LLMs were then evaluated on an ethical- unethical classification task and a reason-generation task. Our proposed fine-tuning strategy notably outperforms the others in both tasks, achiev- ing significantly higher accuracy scores in the classification task and lower misalignment rates in the reason-generation task. The increase in clas- sification accuracies and decrease in misalignment rates indicate that the L+R fine-tuned models align more with human ethics. Hence, this study illustrates that injecting reasons has substantially improved the alignment of LLMs, resulting in more human-like responses. We have made the DFAR dataset and corresponding codes publicly available at https://github.com/apurba-nsu-rnd-lab/DFAR.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been exponential growth in advancements in artifi- cial intelligence (AI) [15], significantly contributing to the resolution of complex problems [10]. A major breakthrough in the domain of artificial intelligence (AI) is the emergence of large language models (LLMs), which have excelled in nat- ural language processing (NLP) tasks [16] such as sentiment analysis, text sum- marization, and text-to-speech, among others. Despite the rapid development, limitations remain regarding the ethical implications of large language models (LLMs) [30]. Large language models (LLMs) are vulnerable in critical domains dealing with sensitive areas such as human ethics, leading to growing appre- hension regarding their alignment with human values. For instance, generative language models might offer violent or harmful information to users, such as instructions for developing malware. Furthermore, LLMs can also provide false and misleading information, such as the claim that Bill Gates is the president of the United States. They may also give incorrect medical information, posing potential harm to patients. Due to these vulnerability issues, this work shows an effective approach to reduce the misalignment of LLMs with human ethics.\nNumerous approaches have already been implemented to address the ethical limitations of LLMs. Hendrycks et al. [11] create a large dataset named ETHICS that encompasses scenarios related to justice, virtue, deontology, utilitarianism, and commonsense. They have fine-tuned various language models on the dataset to classify whether a scenario is ethical or unethical. This work has contributed to the task of aligning LLMs with humans. However, the paper [11] only focuses"}, {"title": "2 Related Works", "content": "Dataset curation for AI alignment. To address the ethical concerns of ar- tificial intelligence (AI), Wang et al. [28] emphasize the significance of data collection in tackling the AI Alignment Problem [32]. To bridge the gap between human and AI perspectives, they conceptualize an instruction $I_k = (x_k, y_k)$, where $x_k$ denotes input and $y_k$ denotes the corresponding response. Humans can annotate the response to ensure that LLMs learn from human responses. For this, Hendrycks et al. [11] introduce the \"ETHICS\" dataset, comprising data pertinent to justice, virtues, common sense, and related aspects. Although sev- eral datasets related to toxicity [5], hate speech [19], and morality [12] have been curated to improve LLM alignment with human values, they typically con- sist only of labels and lack the underlying reasons for those labels. To mitigate this gap, our work begins with constructing an ethics dataset containing human reasoning for ethical-unethical scenarios.\nSupervised fine-tuning. Supervised fine-tuning is a crucial technique for align- ing large language models (LLMs) with human-like reasoning and ethical decision- making. Hendrycks et al. [11] underscore the importance of using supervised"}, {"title": "3 Methodology", "content": "Numerous endeavors have been undertaken to ensure alignment between humans and AI. However, alignment problems persist, particularly concerning human- like reasoning, a concern often overlooked in existing research efforts. In addition to the existing approaches, this work presents a novel approach that contributes to aligning large language models (LLMs) with humans, especially concerning reason generation. Herein, we formally describe our approach for aligning LLM- generated reasoning with humans.\nProblem Formulation. Suppose dataset, D, contains a set of statements $x_i$, binary labels $y_i$, and human-annotated reasoning $r_i$, $D \\rightarrow {x_i, Y_i, r_i}=1$, where $X_i \\in R^P$, $Y_i \\in {0,1}$, $r_i \\in R^Q$, and n represents the number of samples (in our case, 5000). The existing works utilized a dataset $D \\rightarrow {x_i, Y_i}=1$, where reasons $r_i$ were missing. Hence, in existing works, large language models (LLMs) L are"}, {"title": "3.1 DFAR: Dataset for Aligning Reasons", "content": "In numerous instances, generative language models have demonstrated a consid- erable ability to accurately classify ethical and unethical situations [1]. However, they still struggle to generate human-like reasons effectively. In response to this challenge, our initial step involves the construction of a Dataset for Aligning Reasons (DFAR).\nThe DFAR dataset comprises statements sourced from a publicly available ETHICS dataset [11]. ETHICS, a comprehensive alignment dataset, encompasses Commonsense, Virtue, Deontology, Justice, and Utilitarianism data. Our dataset focused on Commonsense and Justice, selecting a subset of 5000 statements from these domains. Each statement is labeled 0 or 1, where 0 denotes \"ethical\" and 1 denotes \"unethical\". The DFAR dataset includes human-annotated reasons for each ethical-unethical scenario, providing precise and detailed explanations with text lengths ranging from 151 to 1171 characters and an average length of 467.45. These annotations are done by 12 annotators, representing both male and female perspectives. The annotators are selected via a sample sheet where ten statements are assigned to assess their eligibility for the dataset annotation task. Among the 5000 data points, 2886 are labeled as \"ethical\", while the remaining are labeled as \"unethical\". Notably, creating the DFAR dataset does not involve the utilization of any AI generative tool such as ChatGPT, ensuring that large language models (LLMs) learn exclusively from human-annotated rationales."}, {"title": "3.2 Supervised Fine-Tuning of LLMs", "content": "To advance the alignment of large language models (LLMs) with human values, fine-tuning LLMs on an ethics-related dataset is essential. We utilize the Dataset for Aligning Reasons (DFAR) for this fine-tuning task. In this study, we conduct two types of fine-tuning: (a) Fine-tuning using labels only and (b) Fine-tuning using both labels and reasons simultaneously. Fig. 2 illustrates the methodology for these two fine-tuning approaches. The first fine-tuning approach is a con- ventional method employed in existing alignment works. The second approach, fine-tuning using both labels and reasons, represents a unique and novel strat- egy absent in prior research. In our study, we fine-tune two popular generative language models, Llama-2 (7 billion) [26] and Mistral (7 billion) [14]. Detailed descriptions of these models are provided below.\nModels. We employ two prominent large language models (LLMs) for our exper- iments: Llama-2 (7B) [26] and Mistral (7B) [14]. Llama-2 (7B), a transformer- based model released by Meta, has 32 attention heads, a vocabulary size of 32,000, and a context length of 4,096, and uses the Swish-Gated Linear Unit (SwiGLU) activation function [24]. Mistral (7B), with a similar parameter count and attention heads, has a larger context length of 8,192 and uses the Sigmoid Linear Unit (SiLU) activation function [8]. Mistral also incorporates grouped- query attention (GQA) and sliding window attention (SWA) to efficiently handle varying sequence lengths. According to Jiang et al. [14], Mistral (7B) outperforms both Llama-2 (7B) and Llama-2 (13B) across all benchmarks, making it a robust choice for our study.\nFine-tuning using Labels. The fine-tuning approach using ethical and un- ethical labels is a common method employed for alignment purposes in existing studies [11]. In our work, we implement this fine-tuning as part of an ablation study. Llama-2 (7B) and Mistral (7B) undergo this fine-tuning approach. The fine-tuning process involves feeding input statements $x_i$ and suitable prompts into the Large Language Model L, generating an output $\\hat{y}_i$ based on the input $x_i$. Subsequently, Cross Entropy Loss (L) is computed between the generated output $\\hat{y}$ and the original label y from the dataset D. In this case, the orig- inal label $y_i$ consists of binary classes: ethical (0) or unethical (1). Therefore, this fine-tuning method is solely supervised by the binary labels. The model's (L) parameters are then updated iteratively to minimize the loss, resulting in a fine-tuned model (see Fig. 2(a)). This fine-tuning approach aims to enable the large language models (LLMs) to learn from binary ethical and unethical labels and accurately classify ethical and unethical scenarios.\nFine-tuning using both Labels & Reasons. Fine-tuning a Large Language Model (LLM) using ethical-unethical labels and their corresponding reasons is a unique and effective approach that aligns language models more closely with human values. This fine-tuning method represents a novel strategy not previously explored in existing works on the alignment problem. We apply this approach to fine-tune both Llama-2 (7B) and Mistral (7B). Initially, input statements $X_i$ and appropriate prompts are fed into the Large Language Model L, which generates an output $\\hat{y}_i$ based on the provided input. Subsequently, Cross Entropy Loss (L) is computed between the LLM-generated output ($\\hat{y}_i$, $r_i$) and the output ($y_i$, $r_i$) from the dataset D. In this fine-tuning method, the generated output $\\hat{y}_i$ is simultaneously guided by the ethical-unethical binary labels and their associated reasons. The model's parameters were then iteratively updated to minimize the loss score, resulting in a fine-tuned model, as depicted in Fig. 2(b). This fine- tuning approach not only enhances the performance of LLMs in ethical-unethical"}, {"title": "4 Experiment", "content": "4.1 Setup\nDataset. We create the Dataset for Aligning Reasons (DFAR) to facilitate the experiment. DFAR consists of 5000 meticulously curated data points, with a thoughtful train-test split ratio of 90% to 10%. This allocation results in 4500 data points dedicated to the training set, which is essential for model refinement, and the remaining 500 points are designated for the test set. To comprehensively assess the models' capabilities, evaluation is conducted on both the test split of DFAR, comprising 500 data points, and the widely recognized ETHOS (multi- label hate speech detection dataSet) benchmarking dataset, which consists of 998 data points. This meticulous approach thoroughly evaluates model per- formance across distinct datasets, comprehensively analyzing their alignment capabilities.\nImplementation details We have conducted two different types of fine-tuning: (a) Fine-tuning using Labels only and (b) Fine-tuning using both Labels and Reasons, both on the Dataset for Aligning Reasons (DFAR). We employ two popular large language models (LLMs): Llama-2 (7B) and Mistral (7B), for our experiments. Due to the large size of these models, approximately 7 bil- lion parameters each, loading them posed a challenge. Therefore, we utilized the Quantized Low-Rank Adapters (QLORA) setup [6] for efficient model loading, enabling deployment within size constraints. Input tokenization was facilitated"}, {"title": "4.2 Results and Analysis", "content": "We provide comprehensive experimentations of our proposal, focusing on large language models (LLMs) across two distinct tasks: classification and reason gen- eration. We utilize data from two separate datasets: DFAR and the ETHOS. The evaluation results for the classification task and the reason-generation task are presented regarding classification accuracy and misalignment rate (MAR), respectively. The MAR is a novel metric proposed to quantify the percentage of LLM responses that are not aligned with human values. Table 4 showcases the accuracy scores and misalignment rates achieved by variants of Llama-2 (7B) and Mistral (7B). The first variant represents the original pre-trained LLM without fine-tuning, the second variant is fine-tuned solely using binary ethics labels (L), and the third variant is fine-tuned using both labels and corresponding reasons (L+R), which demonstrates a practical approach.\nOur observations are as follows: (1) The non-generative models were eval- uated solely on the classification task. The misalignment rates for these mod- els are unavailable because they cannot generate reasons/texts. (2) Although the testing set is the same, the training process of generative models with non- generative models is different. The generative models were exclusively fine-tuned on the DFAR dataset, whereas ETHOS was utilized as a cross-dataset evalua- tion. In contrast, the non-generative models underwent evaluation solely within the dataset. Furthermore, the generative models were not fine-tuned on ETHOS because this dataset lacks reasoning texts that are essential for fine-tuning. (3) In the evaluations on DFAR, the L+R fine-tuned version of Llama-2 (7B) demon- strates superior performance compared to all generative and non-generative models in the classification task, achieving an accuracy of 89.4%. Even on the ETHOS benchmark dataset, Llama-2 (L+R) achieves accuracy levels similar to the best-performing DistilBERT model [23]. Interestingly, Llama-2 (L+R)"}, {"title": "4.3 Ablation Study", "content": "Impact of sampling temperature. Sampling temperature significantly im- pacts the responses generated by large language models (LLMs). In Fig. 4(a), we report the classification accuracies achieved by the L+R fine-tuned versions of Llama-2 (7B) and Mistral (7B) at different sampling temperatures. We ex- periment with five different temperature values: 0.1, 0.4, 0.7, 1.5, and 1.9. For Llama-2 (7B) and Mistral (7B), a sampling temperature of 0.1 outperforms the rest in accuracy. Therefore, we use a sampling temperature of 0.1 for all the experiments. We can notice from Fig. 4(a) that the classification accuracy gen- erally decreases with an increase in sampling temperature values, which aligns with [22].\nImpact of prompts. Prompts also significantly impact the outputs produced by large language models (LLMs). Our study uses five prompt statements to evaluate the performance of the L+R fine-tuned versions of Llama-2 (7B) and Mistral (7B). Fig. 4(b) presents the impact of different prompts on classification accuracy. From Fig. 4(b), it is evident that the fifth prompt performs better for both Llama-2 (7B) and Mistral (7B). Hence, prompt 5 is utilized for all experiments. (See the supplementary material for details)"}, {"title": "4.4 Discussion", "content": "LLMs with human ethics and reasoning. To align large language mod- els (LLMs) with human ethics and reasoning, we develop a novel dataset that includes well-structured human-annotated reasons using statements from the ETHICS dataset [11]. We fine-tune the LLMs to target labels and human- annotated reasons. After fine-tuning, the LLMs have achieved notably high clas- sification accuracies in predicting ethical and unethical scenarios. Moreover, the misalignment rate of the LLMs also decreases significantly, indicating a greater alignment with human reasoning. Our approach demonstrates improved perfor- mance compared to existing approaches in both within-dataset and cross-dataset evaluations. The inclusion of detailed, well-structured, human-annotated reasons for all the ethical-unethical labels in DFAR, without the involvement of any gen- erative AI tools, makes it a suitable dataset for human-AI alignment.\nLimitations. Table 4 shows the L+R fine-tuned models achieved high accu- racies and low misalignment rates. However, slight misalignments still persist, especially in statements lacking specific context. The fine-tuned models assume context themselves if no specific contexts are provided. Examining these mi- nor misalignment issues may require further investigation in the future. With this, large language models (LLMs) can be brought closer to human morality and reasoning, representing a significant advancement in the domain of artificial intelligence (AI) [21], specifically natural language processing (NLP) [13]."}, {"title": "5 Conclusion", "content": "This study introduces an effective fine-tuning approach, leveraging annotated la- bels with corresponding reasons (L+R), which surpasses existing methods solely relying on labeled data (L) for model fine-tuning. Through fine-tuning two pop- ular large language models (LLMs), Llama-2 7B and Mistral 7B, our approach demonstrates superior performance over L-only variant models and the orig- inal pre-trained models, presenting a promising avenue for addressing the AI alignment problem. Both L+R models exhibit significant classification accuracy improvements on our proposed dataset, \u201cDataset For Aligning Reason\" (DFAR), and a cross-hate-speech dataset, ETHOS. The insights gained from integrating reasoning alongside labeled data during fine-tuning prompted an analysis of the model's ability to generate human-like responses. Introducing a novel metric, the misalignment rate (MAR), we quantified the extent to which models de- viate from human reasoning. Lower MAR values signify better alignment with human reasoning. Mistral 7B (L+R) and Llama-2 7B (L+R) models showcase substantial reductions in misalignment rates across datasets compared to the other model variants.\nFuture Work: While our L+R fine-tuned models have achieved commendably low misalignment rates and impressive classification accuracy, addressing re- maining discrepancies necessitates further investigation. The observed minor de- ficiencies in model performance indicate the need for additional data collection. In particular, attributes such as multiple pronouns and socially sensitive terms"}, {"title": "Ethics Statement", "content": "We take ethical considerations very seriously in this study, which involves gen- erating ethical reasoning using LLMs and their evaluations by humans. We re- cruited five human evaluators from diverse demographics on a voluntary basis. Importantly, no sensitive information was collected from the evaluators; only the necessary details to assess their suitability for the task were collected, with any potentially identifying data deleted post-evaluation. Additionally, we ensured that the work would not cause any harm to the evaluators, either physically or mentally.\nThe data from the publicly available ETHOS dataset [19] may contain some abusive language, which could potentially make some evaluators uncomfortable. We implemented strict safety protocols to ensure the LLMs did not produce harmful or abusive content. Moreover, we reject any attempts to insult or demean any race, acknowledging that gender and race are social constructs that warrant respect. Therefore, we believe that our work will not cause any ethical issues."}, {"title": "Misalignment Rate", "content": "In our work, we evaluated the LLMs using two different evaluation tasks: classi- fication task and reason-generation task. For the classification task, we employed classification accuracy, and for the reason-generation task, we used misalignment rate as the evaluation metric.\nClassification Accuracy. In the classification task, the LLMs have to classify statements as either ethical or unethical. The performance of the LLM for this task was calculated using classification accuracy, which computes the percentage of correct predictions among the total number of predictions. The mathematical equation for accuracy is shown below.\n$Accuracy = \\frac{TP+TN}{TP+TN+FN + FP}$"}]}