{"title": "SentiFormer: Metadata Enhanced Transformer\nfor Image Sentiment Analysis", "authors": ["Bin Feng", "Shulan Ruan", "Mingzheng Yang", "Dongxuan Han", "Huijie Liu", "Kai Zhang", "Qi Liu"], "abstract": "As more and more internet users post images online\nto express their daily emotions, image sentiment analysis has\nattracted increasing attention. Recently, researchers generally\ntend to design different neural networks to extract visual fea-\ntures from images for sentiment analysis. Despite the significant\nprogress, metadata, the data (e.g., text descriptions and keyword\ntags) for describing the image, has not been sufficiently explored\nin this task. In this paper, we propose a novel Metadata En-\nhanced Transformer for sentiment analysis (SentiFormer) to fuse\nmultiple metadata and the corresponding image into a unified\nframework. Specifically, we first obtain multiple metadata of the\nimage and unify the representations of diverse data. To adaptively\nlearn the appropriate weights for each metadata, we then design\nan adaptive relevance learning module to highlight more effective\ninformation while suppressing weaker ones. Moreover, we further\ndevelop a cross-modal fusion module to fuse the adaptively\nlearned representations and make the final prediction. Extensive\nexperiments on three publicly available datasets demonstrate the\nsuperiority and rationality of our proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Image sentiment analysis aims to automatically predict the\nsentiment polarity expressed in images. It has a wide range of\napplications in many fields, such as opinion mining [1] and\nrecommendation system [2]. With the increasing popularity of\nsocial media platforms such as Flickr, Instagram and Twitter,\nmore and more people tend to post images online to express\ntheir feelings and opinions of their daily lives. Therefore, this\ntask has become a hot topic, and many significant efforts have\nbeen made in this field to help analyze image sentiment.\nWith the successful accomplishments of deep learning and\ncomputer vision, most recent studies focused on designing\nvarious networks to better extract image features for train-\ning sentiment polarity classifiers. Early researchers [3]\u2013[5]\nmanually designed hand-crafted image features, such as color,\nshape, and texture, to explore the sentiment of images. More\nrecently, with the rapid popularity of CNNs, some studies [6],\n[7] focused on designing various CNN-based networks to\nextract deep features from images for improved sentiment\nanalysis. Rao et al. [8] and Koromilas et al. [9] used deep\nCNN networks for sentiment classification, and demonstrated\nthe superior performance of deep features against hand-\ncrafted features. Rani et al. [10] combined CNN and LSTM\nto better integrate multi-level visual attributes for sentiment\nclassification. With the popularity of attention mechanisms\nand transformer architectures, various neural network modules\n[11]\u2013[16] have been exploited by adaptively learning the\nattended image features. Zhang et al. [11] proposed stacked\nmulti-head self-attention modules to explore the relationship\nbetween semantic regions and sentiment labels. Truong et\nal. [16] designed a concept-oriented transformer to capture the\ncorrelation between image features and specific concepts. With\nthe exploration of a unified architecture for large models, more\nrecent works (e.g., CLIP [17], BLIP [18], ImageBind [19])\nhave been successively proposed. Radford et al. [17] utilized\ncontrastive learning to embed images and text into a shared\nsemantic space, enabling the learning of a universal vision-\nlanguage representation. Liu et al. [20] utilized instruction-\nfollowing data to fine-tune an end-to-end large model for\ngeneral-purpose visual and language understanding. These\nmethods demonstrate their capacity to maintain comprehensive\nunderstanding across diverse data, resulting in remarkable\nperformance in various tasks.\nDespite the significant progress, metadata, the data (e.g., text\ndescriptions and keyword tags) for describing the image, has\nnot been fully explored in this task, which is highly helpful\nfor image sentiment understanding. For example, considering\na landscape photo with metadata of the scene tag, such as\na beach or city, helps in understanding the image's context\nand sentiment. Knowing it was taken at a beach can suggest\nfeelings of relaxation or joy, enhancing sentiment analysis\naccuracy. This brings a new perspective to image sentiment\nanalysis, which is the main focus of this paper: how to\nincorporate metadata into a unified framework and make\ncomprehensive utilization of it for better knowledge reasoning\nand integration.\nTo this end, we propose a novel metadata enhanced trans-\nformer for image sentiment analysis (SentiFormer) to inte-\ngrate multiple metadata and images into a unified framework.\nSpecifically, we first obtain multiple metadata corresponding to\nthe image and use prompt learning for input alignment. Then,\nwe employ CLIP to generate unified initial representations for\nboth images and text. Next, we design an adaptive relevance\nlearning module to highlight more effective information while"}, {"title": "II. MODEL STRUCTURE", "content": "As shown in Fig. 1, our proposed method consists of\nthree modules: the feature representation module, the adaptive\nrelevance learning module, and the cross-modal fusion and\nprediction module.\nA. Feature Representation\nMetadata Representation: In social media, such as Flickr\nand Instagram, when users post an image to express their\nemotions, they usually attach metadata such as descriptions,\ntags, and so on. Due to privacy reasons and copyright protec-\ntion, some metadata are unprovided in many image sentiment\nanalysis datasets. In order to get a full understanding of the\nimpact of metadata on this task, for partial missing metadata,\nwe adopt some methods to generate them with confidence.\nGiven an image $I$, for the text description of the image,\nwe utilize BLIP [18], pre-trained on the COCO dataset, to\ngenerate textual caption $C$. Keyword tags typically include\nobject tags and scene tags, which are highly helpful for image\nsentiment analysis. In order to obtain objects contained in the\nimage, we apply Faster R-CNN [21] to obtain top-k object tags\n$T_{obj} = \\{obj_i\\}_{i=1}^k$ ranked by confidence score. To obtain scene\ntag of the image, we employ Hiera [22] to obtain the most\nlikely one as $T_{sce}$ among 365 categories in Place365 [23].\nFor input alignment and better representation of multiple\nmetadata, we use prompt learning to represent keyword tags\nsince they are word phrases rather than sentences. Specifically,\nwe design a heuristic prompt $P$ = 'the scene or background\nof the image is $T_{sce}$, and the image contains the following\nobjects: $obj_1, obj_2, ..., obj_k$'.\nImage Representation: CLIP [17] has powerful capabilities\nof image and text representation. Therefore, in this paper, we\nemploy CLIP, which uses a ViT-B/32 transformer architecture\nas an image encoder and a masked self-attention transformer\nas a text encoder, to unify and align the representations of\nimages and metadata. After unified encoding, each output of\n$I, C$, and $P$ above is a 512-dimensional vector, which can\nbe represented as follows:\n$$E_v, E_c, E_p = CLIP(I, C, P),$$\nwhere $E_i \\in \\mathbb{R}^{d_e}, i \\in \\{v, c, p\\}$, and $d_e = 512$.\nB. Adaptive Relevance Learning\nUnified Embedding: First, we pass $E_v, E_c, E_p$ through\na fully connected (FC) layer and project them into a $d_h$-\ndimensional space. Then, we use three parallel transformer\nlayers to unify the features of each part, respectively. In\norder to better handle the L-classification task, we expand\neach feature to $L \\times d_h$ dimensions in the transformer layer.\nFollowing Vision Transformer [24], the structure of each\ntransformer layer includes layer normalization, multi-head\nself-attention, and a fully connected layer. In practice, we use\n8-head attention and set the dimension of each head to 64.\nThe process is formulated as follows:\n$$H^i = FC(E_i),$$\n$$H^i = Transformer((W^iH^i_0 + b^i)),$$\nwhere $H_0^i \\in \\mathbb{R}^{1 \\times d_h}$ and $H^i \\in \\mathbb{R}^{L \\times d_h}, i \\in \\{v,c,p\\}$, and\n$W^i \\in \\mathbb{R}^{L \\times 1}, b^i \\in \\mathbb{R}^{L \\times d_h}$ are trainable parameters.\nAdaptive Learning: In image sentiment analysis, some\nmetadata are more relevant to the image content, while others\nare irrelevant information or noise. In order to adaptively learn\nthe appropriate weight for each metadata, we use the multi-\nhead attention (MHA) mechanism to highlight more effective"}, {"title": "", "content": "information while suppressing the weaker. Meanwhile, in\norder to capture the low-level to high-level features of the\nimage, we use multiple cascaded transformer layers to learn a\nsequence of visual features.\n$$H_{j+1}^v = Transformer(H_j^v),$$\nwhere $j = 1, ..., N$, and $N$ is the number of transformer layers.\nIn the interaction between image and metadata, we take this\nsequence of image features as query, and metadata embedding\nas key and value. Furthermore, in order to learn the joint rep-\nresentation of multiple metadata, we first randomly initialize\na new token $H^1_m \\in \\mathbb{R}^{L \\times d_h}$. Then, inspired by ResNet [25],\nwe introduce the residual block to gradually accumulate and\nupdate the learned metadata representation $H^N_m$. The process\nis formulated as follows:\n$$MHA(Q, K, V) = Softmax(\\frac{QW_Q(K W_K)^T}{\\sqrt{d_k}})VW_V,$$\n$$H_c^{2j+1} = MHA (H_m^j, H_c^j, H_c^j),$$\n$$H_p^{2j+1} = MHA (H_m^j, H_p^j, H_p^j),$$\n$$H_m^{j+1} = H_m^j + (H_c^{2j+1} + H_p^{2j+1})W_o,$$\nwhere $j = 1, ..., N$, and $d_k$ means the dimension of each head\nof the multi-head attention. $W_Q, W_K, W_V$, and $W_o$ are\ntrainable parameters.\nC. Cross-modal Fusion and Prediction\nCross-modal Fusion: When predicting different sentiment\nlabels, not all visual information and metadata make the same\ncontribution to the final prediction. Therefore, we adopt a\ncross-modal transformer to tackle this problem.\nWe take $H_m^{N+1}$ and $H_v^{N+1}$ as input, and add an extra_token\n$H_e$ and position embedding. $H_e$ is a $d_h$-dimensional token,\nwhich is used to capture global information at the head\nof a sequence, and position embedding is used to preserve\nsequential information of the input. As shown in Fig. 1, the\ncross-modal transformer is a deep stacking of several cross-\nmodal attention blocks, with a depth of $M$. In the cross-modal\ntransformer, we first take $X_v$ as the query and $X_s$ as the\nkey and value. The cross-modal transformer can effectively\ncapture and fuse information from diverse data to enrich\nfeature representation, which can be formulated as follows.\nFirst, the extra token is concatenated to the input sequence,\nfollowed by adding positional embeddings:\n$$X_v = H_e \\oplus H_v^{N+1} + P_s,$$\n$$X_s = H_e \\oplus H_m^{N+1} + P_t,$$\nwhere $\\oplus$ denotes the concatenation operation along the se-\nquence dimension, and $H_e \\in \\mathbb{R}^{1 \\times d_h}$ represents the added\ntoken. The positional embeddings are defined as $P_s \\in\n\\mathbb{R}^{(L+1) \\times d_h}$ and $P_t \\in \\mathbb{R}^{(L+1) \\times d_h}$.\nNext, in each transformer block, the multi-head attention\ncomputes the attention from the target to the source as follows:\n$$[Q_t, K_s, V_s] = [X_vW_Q', X_sW_K', X_sW_V'],$$\n$$X_{s2t} = Softmax(\\frac{Q_tK_s^T}{\\sqrt{d_s}})V_s,$$\nwhere $W_Q, W_{K'}, W_V$, are trainable parameters, and $d_s$ is\nthe dimension of each attention head.\nThen, we obtain the final cross-modal fusion output $X_{s2t}$\nby repeating cross-modal transformer blocks $M$ times.\nSentiment Prediction: For the image sentiment classifi-\ncation task, we select the first token of the sequence from\nthe cross-modal fusion output. The token at the head of the\nsequence contains global information of the entire sequence.\nThen we construct a classification head to make the final\nprediction, which consists of a linear layer and a softmax layer.\n$$p = Softmax(X_{s2t}[:, 0]W_f + b_f),$$\nwhere $W_f \\in \\mathbb{R}^{d_h \\times L}$ and $b_f \\in \\mathbb{R}^{1 \\times L}$ are trainable parameters.\nFor model learning, we employ the cross-entropy as the loss\nfunction, which is calculated as follows:\n$$L = -\\frac{1}{n} \\sum_{i=1}^n y_i log P(p_i | I),$$\nwhere $y_i$ is the true answer label of the $i^{th}$ instance of the\ndataset, and $n$ represents the number of training instances."}, {"title": "III. EXPERIMENT", "content": "A. Data Description\nWe evaluate our method on FI [26], Twitter_LDL [27] and\nArtphoto [28]. FI is a public dataset built from Flickr and\nInstagram, with 23,308 images and eight emotion categories,\nnamely amusement, anger, awe, contentment, disgust, excite-\nment, fear, and sadness. Twitter_LDL contains 10,045 images\nfrom Twitter, with the same eight categories as the FI dataset.\nFor these two datasets, they are randomly split into 80%\ntraining and 20% testing set. Artphoto contains 806 artistic\nphotos from the DeviantArt website, which we use to further\nevaluate the zero-shot capability of our model.\nB. Experiment Setting\nModel Setting: For feature representation, we set $k = 10$ to\nselect object tags, and adopt clip-vit-base-patch32 as the pre-\ntrained model for unified feature representation. Moreover, we\nempirically set $(d_e,d_h,d_k,d_s) = (512, 128, 16,64)$, and set\nthe classification class $L$ to 8.\nTraining Setting: To initialize the model, we set all weights\nsuch as $W$ following the truncated normal distribution, and"}, {"title": "C. Experiment Result", "content": "We compare our model against several baselines, and the\noverall results are summarized in Table I. We observe that our\nmodel achieves the best performance in both accuracy and\nF1 metrics, significantly outperforming the previous models.\nThis superior performance is mainly attributed to our effective\nutilization of metadata to enhance image sentiment analysis,\nas well as the exceptional capability of the unified sentiment\ntransformer framework we developed. These results strongly\ndemonstrate that our proposed method can bring encouraging\nperformance for image sentiment analysis."}, {"title": "D. Ablation Performance", "content": "In this subsection, we conduct an ablation study to examine\nwhich component is really important for performance improve-\nment. The results are reported in Table II.\nFor information utilization, we observe a significant decline\nin model performance when visual features are removed.\nAdditionally, the performance of SentiFormer decreases when\ndifferent metadata are removed separately, which means that\ntext description, object tag, and scene tag are all critical for\nimage sentiment analysis. Recalling the model architecture,\nwe separately remove transformer layers of the unified repre-\nsentation module, the adaptive learning module, and the cross-\nmodal fusion module, replacing them with MLPs of the same\nparameter scale. In this way, we can observe varying degrees"}, {"title": "E. Visualization", "content": "In Fig. 2, we use t-SNE [39] to reduce the dimension of\ndata features for visualization. The left figure shows metadata\nfeatures before being processed by our model (i.e., embedded\nby CLIP), while the right shows the distribution of features\nafter being processed by our model. We can observe that after\nthe model processing, data with the same label are closer to\neach other, while others are farther away. Therefore, it shows\nthat the model can effectively utilize the information contained\nin the metadata and use it to guide the classification process."}, {"title": "F. Sensitivity Analysis", "content": "In this subsection, we conduct a sensitivity analysis to figure\nout the effect of different depth settings of adaptive learning\nlayers and fusion layers. Taking Fig. 3 (a) as an example, the\nmodel performance improves with increasing depth, reaching\nthe best performance at a depth of 4. When the depth continues\nto increase, the accuracy decreases to varying degrees. Similar\nresults can be observed in Fig. 3 (b). Therefore, we set their\ndepths to 4 and 6 respectively to achieve the best results."}, {"title": "G. Zero-shot Capability", "content": "To validate the model's generalization ability and robustness\nto other distributed datasets, we directly test the model trained\non the FI dataset, without training on Artphoto. From Table III,\nwe can observe that compared with other models trained\non Artphoto, we achieve competitive zero-shot performance,\nwhich shows that the model has good generalization ability in\nout-of-distribution tasks."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduced metadata into image sentiment\nanalysis and proposed a unified metadata-enhanced sentiment\ntransformer. Specifically, we first obtained multiple metadata\nand unified their representations with images. Then, we further\ndeveloped an adaptive relevance learning module and a cross-\nmodal fusion module for better sentiment prediction. Extensive\nexperiments on three datasets demonstrated the superiority and\nrationality of our proposed method."}]}