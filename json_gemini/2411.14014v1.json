{"title": "Trajectory Representation Learning on Road Networks and Grids with Spatio-Temporal Dynamics", "authors": ["Stefan Schestakov", "Simon Gottschalk"], "abstract": "Trajectory representation learning is a fundamental task for applications in fields including smart city, and urban planning, as it facilitates the utilization of trajectory data (e.g., vehicle movements) for various downstream applications, such as trajectory similarity computation or travel time estimation. This is achieved by learning low-dimensional representations from high-dimensional and raw trajectory data. However, existing methods for trajectory representation learning either rely on grid-based or road-based representations, which are inherently different and thus, could lose information contained in the other modality. Moreover, these methods overlook the dynamic nature of urban traffic, relying on static road network features rather than time-varying traffic patterns. In this paper, we propose TIGR, a novel model designed to integrate grid and road network modalities while incorporating spatio-temporal dynamics to learn rich, general-purpose representations of trajectories. We evaluate TIGR on two real-world datasets and demonstrate the effectiveness of combining both modalities by substantially outperforming state-of-the-art methods, i.e., up to 43.22% for trajectory similarity, up to 16.65% for travel time estimation, and up to 10.16% for destination prediction.", "sections": [{"title": "1 INTRODUCTION", "content": "Trajectories, i.e., sequences of locations representing agent movements over time, are increasingly prevalent due to widespread location-aware devices. This abundance of trajectory data has surged research in trajectory data analysis, including trajectory similarity computation [6], destination prediction [18], and travel time estimation [14, 40].\nA fundamental task of trajectory analysis is Trajectory Representation Learning (TRL), which aims to learn low-dimensional and meaningful representations of trajectories from high-dimensional and raw trajectory data. TRL facilitates the utilization of trajectory data for various downstream applications without the need for manual feature engineering and unique models for specific tasks.\nRaw trajectory data often suffers from noise, sparsity, and spatio-temporal irregularities. TRL methods address these issues using either grid or road network discretization, representing trajectories"}, {"title": "2 RELATED WORK", "content": "Trajectory Representation Learning (TRL) encodes raw spatio-temporal data into a compact feature vector, capturing essential trajectory characteristics. TRL methods can be categorized into grid-based and road-based methods.\nGrid-based TRL A grid divides the spatial domain into a regular grid, which preserves structural and spatial concepts and thus is used in many spatio-temporal tasks, such as traffic prediction [25], spatial representation learning [20] and remote sensing [12, 31]. Grid-based TRL methods augment trajectories, e.g., by downsampling or distortion, and reconstruct the original trajectory (t2vec [15]) or contrast augmented views with contrastive learning"}, {"title": "3 PROBLEM DEFINITION", "content": "Definition 3.1. (Trajectory). A trajectory represents the movement of an agent recorded by a GPS-enabled device. Formally, a trajectory can be denoted as \\(T = [(x_i, y_i, t_i)]\\), where \\((x_i, y_i)\\) is the location represented as a spatial coordinates pair and \\(t_i\\) is the timestamp of the location at time step i. \\(|T|\\) is the length of the trajectory, and varies for different trajectories.\nGrid-based methods model trajectories by first defining a grid over the spatial domain (e.g., representing a city) and then mapping each point onto its corresponding grid cell.\nDefinition 3.2. (Grid) A grid is a regular partition of the spatial domain into \\(M \\times N\\) cells. Each cell has a unique pair of indices \\((m, n)\\), where \\(m\\in \\{1, ..., M\\}\\) and \\(n \\in \\{1, ..., N\\}\\). The set of all grid cells is denoted by \\(C\\), where \\(C = \\{C_1, C_2, ..., C_{|C|}\\}\\) and each \\(c_i\\) is associated with a unique pair of indices \\((m, n)\\).\nDefinition 3.3. (Grid-based Trajectory). A grid-based trajectory \\(T^9 = [(c_i, t_i)]_{i=1}^{|T^9|}\\) represents a trajectory as a sequence of grid cells, where each \\(c_i\\) denotes a grid cell in the grid \\(C\\).\nRoad-based methods model trajectories based on a road network by mapping each point onto the road network using a map matching algorithm [32].\nDefinition 3.4. (Road Network) We define a road network as a directed graph \\(G = (V, A, F)\\). \\(V\\) is a set of nodes, where each node \\(v_i \\in V\\) represents a road segment. \\(A\\) is the adjacency matrix, where \\(A_{ij} = 1\\) implies that a road segment \\(v_j\\) is directly accessible from road segment \\(v_i\\), and \\(A_{ij} = 0\\) otherwise. A road network has a feature set \\(F \\in \\mathbb{R}^{|V| \\times f}\\) representing road segment features with dimension \\(f\\).\nDefinition 3.5. (Road-based Trajectory). A road-based trajectory \\(T^r = [(v_i, t_i)]_{i=1}^{|T^r|}\\) represents a trajectory constrained by a road network \\(G\\), where each \\(v_i \\in V\\) denotes a road segment in the road network \\(G = (V, A, F)\\)."}, {"title": "4 TIGR APPROACH", "content": "In this section, we present TIGR's training procedure, which aims to produce similar representations between two masked views of the same data instance and between the same data instances across different modalities.\nAs depicted in Figure 2, TIGR integrates two primary modalities, grid and road, along with a dedicated spatio-temporal component derived from the road modality. Thus, our architecture consists of three parallel branches processing grid, road network, and spatio-temporal dynamics respectively. Each branch embeds a trajectory \\(T\\) into a sequence of token embeddings, thus obtaining \\(T^9, T^r\\) and \\(T^{st}\\). We leverage distinct embedding layers for \\(T^9\\) and \\(T^r\\) to capture the structural properties of both modalities. The spatio-temporal component, on the other hand, aims to extract traffic patterns and temporal dynamics and is detailed in the next section. Subsequently, we apply masking to each branch to obtain distinct views and embed each view of each branch using a modality-agnostic encoder. The objective is then to align the representations within each branch (intra-modal loss) and across branches (inter-modal loss)."}, {"title": "4.1 Spatio-Temporal Extraction", "content": "Modeling spatio-temporal dynamics is crucial for effective TRL. However, current methods neglect temporal information completely [7, 36], or include temporal information, but solely model static road features [13, 16]. Thus, these approaches fail to capture the dynamic nature of traffic patterns changing over time. To address this limitation, we propose a novel spatio-temporal extraction method that effectively models both - dynamic traffic patterns and temporal regularities.\nOur approach consists of three main components: dynamic traffic embedding, temporal embedding, and fusion via local multi-head attention. First, we develop a dynamic traffic embedding that captures changing traffic conditions across different times of day and days of the week, using graph convolutions over transition probabilities to model traffic flow patterns. Second, we introduce a temporal embedding that allows our model to capture recurring temporal patterns (e.g., daily rush hours, weekly trends) and temporal dependencies. Finally, we fuse those embeddings by proposing the local multi-head attention (LMA), based on the observation that traffic within a small spatial and temporal window tends to be very similar, while traffic on distant roads or at different times can differ significantly. Our idea is to leverage the multiple heads used in multi-head attention to perform local attention over short sequences. This approach of spatio-temporal extraction enables TIGR to learn rich representations that account for the complex, time-varying nature of urban traffic."}, {"title": "4.1.1 Dynamic Traffic Embedding", "content": "To capture dynamic traffic behavior and traffic flow, we leverage graph convolutions over transition probabilities. Let \\(X \\in \\mathbb{R}^{|t_a| \\times |t_h|}\\) be the aggregated traffic matrix, containing mean traffic speeds for each weekday \\(t_a\\) and each hour \\(t_h\\). Further, \\(P \\in \\mathbb{R}^{|V| \\times |V|}\\) is the transition probability matrix between connected road segments based on the frequency of trajectories passing through them. For two connected road segments \\(v_i\\) and \\(v_j\\), the transition probability is given by \\(P[i,j]\\) and is calculated from historical trajectories,\n\\[P[i,j] = \\frac{\\#transitions(v_i \\rightarrow v_j) + 1}{\\#total\\_visits(v_i) + |N(v_i)|},\\]\nwhere \\(|N(v_i)|\\) is the number of \\(v_i\\'s\\) neighbors. To leverage \\(P\\) for graph convolutions, we further add self-loops and normalize, i.e., \\(P\\' = D^{-1}(P + I)\\), where \\(D\\) is the degree matrix and \\(I\\) the identity matrix. Formally, the transition probability weighted graph convolution is then defined by:\n\\[h_i = \\sum_{j \\in N(v_i)} P[i,j] W_j x_{(t_a,t_h)},\\)\nwhere \\(N\\) is the neighbor function returning all neighbors of a road, \\(w\\)'s are learnable parameters, \\(x_{(t_a,t_h)} \\in X\\) is the traffic state at weekday \\(t_a\\) and hour \\(t_h\\), and \\(h_i\\) is the dynamic traffic embedding of road segment \\(v_i\\). Given the sequence of roads in \\(T^r\\), we obtain the sequence of dynamic traffic embeddings \\(T^s = (h_1, h_2, h_3, ..., h_{|T^r|})\\)."}, {"title": "4.1.2 Temporal Embedding", "content": "Inspired by the sinusoidal Positional Embeddings [27], we learn time embeddings by utilizing the cosine function to capture periodic behavior. Let \\(t_i\\) be the i-th time point in a trajectory, then we obtain its time embedding \\(t_i\\):\n\\[t_i [k] = \\begin{cases} \\frac{w_k t_i + \\Phi_k}{cos(w_k t_i + \\Phi_k)} & \\text{if } k = 0, \\\\  & \\text{else,} \\end{cases}\\]\nwhere \\(t_i [k]\\) is the k-th element of the embedding vector \\(t_i \\in \\mathbb{R}^q\\) with dimension \\(q\\), and \\(w_k\\) and \\(\\Phi_k\\) are learnable parameters. This representation of time allows periodic behaviors to be captured through the cosine activation function and non-periodic patterns through the linear term. Then, we define the sequence of temporal embeddings of a trajectory \\(T\\) as \\(T^t = (t_1, t_2, t_3, ..., t_{|T^r|})\\)."}, {"title": "4.1.3 Fusion via Local Multi-Head Attention", "content": "First, we propose the local multi-head attention, which aims to leverage each individual head to perform a local attention over a specific subsequence. Let \\(T\\in \\mathbb{R}^{|T|\\times d}\\) be the input with sequence length \\(|T|\\) and dimension \\(d\\). For each head \\(h \\in \\{1, ..., H\\}\\), we split \\(T\\) into \\(H\\) subsequences along the sequence dimension, i.e., \\(T_h \\in \\mathbb{R}^{|T|/H \\times d}\\), and project each \\(T_h\\) into query, key and value matrices:\n\\[Q_h = T_h W^Q, K_h = T_h W^K, V_h = T_h W^V,\\]\nwhere \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}\\) are the weight matrices. Next, we apply the standard attention mechanism for each head:\n\\[Att_h (Q_h, K_h, V_h) = \\text{softmax}(\\frac{Q_h K_h^T}{\\sqrt{d}})V_h.\\]\nFinally, we define the local multi-head attention (LMA) as the concatenation of each attention head along the sequence dimension and a projection with the weight matrix \\(W^O\\):\n\\[LMA(Q, K, V) = [Att_1 || Att_2 || ... || Att_{|H|} ]W^O.\\]\nTo fuse the dynamic traffic and temporal embeddings, we leverage cross-attention utilizing the previously proposed LMA and a concatenation operation:\n\\[T^{st} = LMA(T^s, T^t, T^t) || LMA(T^t, T^s, T^s).\\]\nThis allows both embeddings to attend to the other and to learn more intricate correlations between dynamic traffic and time periodicity through local attention."}, {"title": "4.2 Masking", "content": "After embedding each branch, we obtain a sequence of token embeddings \\(T^b\\) for each of the branches \\(b \\in \\{g, r, st\\}\\). Next, we apply masking by randomly dropping some of the tokens and obtain two distinct views for each branch: \\(T^b\\) (View 1) and \\(T^b\\) (View 2). The role of masking is twofold. First, we obtain two different views of the same data instance to guide the contrastive learning within each branch. Second, similar to how masked autoencoders (MAE) [11] learn representations, the encoder has to develop an understanding of spatial relationships and learn intrinsic information about the trajectory, i.e., the underlying route. We investigate three masking strategies:\n\\begin{itemize}\n    \\item Random masking (RM): We randomly select a subset of tokens in a trajectory \\(T^b\\), based on the share \\(p_{RM} \\in (0, 1)\\), and mask (i.e., drop) them.\n    \\item Consecutive masking (CM): We mask a consecutive amount of points within the trajectory, where the amount of points is defined by the share \\(p_{CM} \\in (0, 1)\\).\n    \\item Truncation (TC): Similarly, we mask consecutive points from the origin or destination of a trajectory \\(T^b\\), leading to a truncation of the trajectory. We apply \\(p_{TC} \\in (0, 1)\\) to define the share.\n\\end{itemize}"}, {"title": "4.3 Encoder", "content": "The encoder transforms each masked trajectory into a low-dimensional embedding. Our approach employs two types of encoders: a target encoder \\(F_b(\\cdot)\\) that encodes \\(T^b\\) into \\(z_b \\in \\mathbb{R}^d\\), and an anchor encoder \\(F'_b(\\cdot)\\) that encodes \\(\\tilde{T}^b\\) into \\(\\tilde{z}_b \\in \\mathbb{R}^d\\). Both encoders have different parameters, where the target encoder's parameters \\(\\theta\\) are updated using an exponential moving average (EMA) of the anchor encoder's parameters \\(\\theta\\): \\(\\tilde{\\theta} \\leftarrow \\mu \\tilde{\\theta} + (1 - \\mu)\\theta\\), where \\(\\mu \\in [0, 1]\\) is a target decay rate. This EMA update ensures a more stable learning target and prevents representation collapse [1].\nFor the encoder architecture, we base our design on the Transformer [27] with specific enhancements. We incorporate Rotary Positional Embeddings (RoPE) [26], which capture relative positional information during the attention process, improving upon standard positional embeddings that only represent absolute positions. Additionally, we utilize Root Mean Square Layer Normalization (RMSNorm) [38] and apply it before the multi-head attention and feed-forward layers, rather than after. This modification further stabilizes the training process. These architectural choices enable our encoders to effectively capture the complex spatial and temporal relationships within trajectory data."}, {"title": "4.4 Inter- and Intra-Modal Losses", "content": "Next", "23": "n\\[\\mathcal{L}(x, y) = -\\log \\frac{\\exp(x \\cdot y / \\tau)}{\\sum_{j=0}^{|\\mathcal{Q}_{neg}|} \\exp(x \\cdot y_j / \\tau) + \\exp(x \\cdot y / \\tau)},\\"}]}