{"title": "Conformalized Answer Set Prediction for Knowledge Graph Embedding", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Jiarong Pan", "Bo Xiong", "Yunjie He", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to distinguish plausible from implausible answers, posing challenges for the application of KGE methods in high-stakes domains like medicine. We address this issue by applying the theory of conformal prediction that allows generating answer sets, which contain the correct answer with probabilistic guarantees. We explain how conformal prediction can be used to generate such answer sets for link prediction tasks. Our empirical evaluation on four benchmark datasets using six representative KGE methods validates that the generated answer sets satisfy the probabilistic guarantees given by the theory of conformal prediction. We also demonstrate that the generated answer sets often have a sensible size and that the size adapts well with respect to the difficulty of the query.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graph Embeddings (KGE) map entities and predicates into numerical vectors, providing non-classical reasoning capabilities by exploiting similarities and analogies between entities and relations. KGE models are typically evaluated through link prediction. To answer queries in the form of (head entity, predicate,?) or (?, predicate, tail entity), all possible entities are ranked according to their plausibility scores returned by the KGE models. Higher-ranked positions of the correct answers indicate better model performance.\nHowever, ranking all possible entities has limited practical value, since the rankings do not distinguish plausible answers from implausible ones with high quality. It is crucial to provide tight answer sets that provably cover the true answer, particularly in high-stakes domains like medicine, where reliable predictions and risk assessment are critical.\nIdentifying a set of plausible answer entities from the entire entity set is challenging. KGE models are trained to assign higher plausibility scores to true triples than to false ones. However, due to the lack of ground truth negative triples in KGs, negative examples used for training are typically generated by corrupting existing triples. Some of the generated negative samples may actually be valid but unobserved triples, which can mislead the KGE model and result in incorrect plausibility scores. Furthermore, the training process uses gradient descent-based optimization techniques, which do not guarantee convergence to the global optimum. As a result, the plausibility scores returned by KGE models not only fail to ensure correct triple ranking, but they also lack a clear probabilistic interpretation, meaning they do not correspond to the actual likelihood of a triple being true (i.e., they are uncalibrated). Therefore, constructing answer sets based on the uncalibrated plausibility scores can lead to somewhat arbitrary result sets.\nPrior works suggest to apply off-the-shelf calibration techniques, such as Platt scaling and Isotonic regression, to map uncalibrated plausibility scores to the expected correctness of predictions. However, perfect calibration is impossible in practice, both Platt scaling and Isotonic regression are empirical calibration techniques that lack formal probabilistic guarantees and are highly sensitive to the calibration set.\nIn contrast, our work diverges from these approaches by applying the theory of conformal prediction to construct answer sets with formal statistical guarantees. Intuitively, conformal prediction tests whether each possible answer to a query conforms to the existing query-answer pairs (triples) in the training set. The answer set is then constructed by including all answers that do not reject the true hypothesis. To the best of our knowledge, this is the first method that does not merely convert plausibility scores into probabilities but instead ensures statistical validity in the uncertainty quantification of the predictions within the context of KGE.\nConformal prediction is a general framework rather than a specific algorithm. In our paper, we carefully design conformal predictors tailored to the link prediction task such that the answer sets are (1) probabilistically guaranteed to include the true answer entity at a specified confidence level, (2) tight, and (3) adaptive, providing smaller sets for easier queries than harder ones. We perform extensive experiments on commonly used benchmark datasets and a variety of KGE methods. Our empirical results show that: (1) conformal predictors satisfy the statistical guarantees in Proposition 1 and produce tighter answer sets compared to other baselines (Experiments 1); (2) conformal predictors generate answer sets that adapt to query difficulty, yielding smaller sets for easier queries than for harder ones (Experiment 2); (3) high-quality answer sets can be obtained with a relatively small calibration set (Experiment 3); and (4) conformal predictors are effective across different user-specified error rates (Experiment 4)."}, {"title": "2 Related Work", "content": "Uncertainty quantification in KGE methods remains a relatively unexplored area. Existing approaches incorporate uncertainty into KGE by modeling entities and relations using probability distributions. However, these methods primarily focus on enhancing the performance of KGE models through more expressive representations, without systematically analyzing or rigorously evaluating the quality of uncertainty in embeddings or predictions.\nFurthermore, research by Tabacof and Costabello and Safavi et al. applies off-the-shelf calibration techniques, such as Platt scaling and Isotonic regression, to KGE methods. These techniques aim to convert uncalibrated plausibility scores into probabilities by minimizing the negative log-likelihood on a validation set. However, these approaches are quite sensitive to the validation set and do not provide formal guarantees about the generated probabilities.\nThis paper applies conformal prediction, which has its roots in online learning literature, is a method that produces predictive sets ensuring coverage guarantees. This approach has been successfully applied across various domains, including image classification, natural language processing and node classification/regression on graphs. However, to the best of our knowledge, it has not yet been applied to KGE."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Knowledge Graph Embedding", "content": "We consider a knowledge graph (KG) G \u2286 E \u00d7 R \u00d7 E defined over a set E of entities and a set R of relation names. The elements in G are called triples and denoted as < h, r, t >. A KGE model M : E \u00d7 R \u00d7 E \u2192 \u211d associates each triple with a score that measures the plausibility that the triple holds. The parameters \u0398 are learned to let M\u0398 assign higher plausibility scores to positive triples (real facts) while assigning lower plausibility scores to negative triples (false facts).\nNote that the interpretation of plausibility scores varies across different types of KGE methods. In distance-based models like TransE and RotatE, the plausibility score is determined by the negative distance in the embedding space. In semantic matching models such as RESCAL and DistMult, plausibility scores are derived from similarity measures (often computed through the dot product of entity embeddings)."}, {"title": "3.2 Conformal Prediction", "content": "Conformal prediction (a.k.a conformal inference) is a general framework for producing answer sets that cover the ground truth with probabilistic guarantees. In this section, we recall some basics from"}, {"title": "4 KGE-based Answer Set Prediction", "content": "In this section, we formally define the KGE-based answer set prediction task and outline three key desiderata guiding the development of effective set predictors. We then introduce and discuss several basic set predictors."}, {"title": "4.1 Problem Definition and Desiderata", "content": "We reformulate the link prediction task as an answer set prediction task. Instead of object-label pairs (xi, yi) in section 3, each data point is a triple tr(qi, ei). Here, qi denotes a query in form of either (h, r, ?) or (?, r, t), and tr(q, e) corresponds to the respective triple (h, r, e) or (e, r,t).\nGiven a set of (training) triples \ud835\udcaf1:n = {tr(q1, e1), ..., tr(qn, en)}, a test query qn+1 and a user-specific error rate \u03b5, we aim to predict a set of entities \u0108(qn+1) \u2286 E that covers the true answer en+1 with probability at least 1 \u2212 \u03b5.\n\u2119(en+1 \u2208 \u0108(qn+1)) \u2265 1 \u2212 \u03b5        (4)\nWe refer to Equation 4 as the coverage desideratum. However, this criterion alone is insufficient, as it can be trivially met by a predictor that always outputs sets containing all possible answers. To develop sensible set predictors, we also consider the size desideratum and the adaptiveness desideratum. The size desideratum emphasizes the need for smaller sets, as smaller sets are generally more informative. The adaptiveness desideratum requires that the set sizes reflect query difficulty: smaller sets should correspond to easier queries, while larger sets should be used for harder queries."}, {"title": "4.2 Basic Set Predictors", "content": "Naive Predictor. Given a query, assume the KGE model provides the probability of each possible answer entity being true. A straightforward approach towards our goal is to construct the set by including entities from highest to lowest probability until their sum exceeds the threshold 1 \u2212 \u03b5. We refer to this approach as the naive predictor and provide its pseudocode in Algorithm 2 (Appendix A). However, the plausibility scores provided by KGE models are not calibrated. We convert these plausibility scores into a \"probability distribution\" using a softmax function.\nPlatt Predictor. Following the recommendations of , we improve the naive predictor by using a multiclass Platt scaling to calibrate the plausibility scores and then construct sets based on these calibrated probabilities. We refer to this method as the Platt predictor and provide more details of this predictor in Appendix A.2.\nTopK Predictor. Another straightforward approach is to construct the set with the Top-K entities from the ranking, referred to as the topk predictor. We select K to ensure the Top-K entities cover the correct answers for 1 \u2212 \u03b5 of the validation queries."}, {"title": "5 Conformal Prediction for KGE-based Answer Set Prediction", "content": "To improve the basic set predictors, we apply conformal prediction, a general framework that requires adaptation to be effective in the context of KGE. The two essential components in this design are the nonconformity measure and the method for constructing answer sets. In this section, we propose several KGE-specific nonconformity measures and outline an efficient approach to constructing answer sets."}, {"title": "5.1 Nonconformity Measures", "content": "The probabilistic guarantees in Theorem 1 hold under i.i.d assumption, regardless of the data distribution or the definition of the nonconformity measure. However, the size of the resulting answer sets depends on how effectively the nonconformity measure captures the underlying structure of the data. Next, we introduce several nonconformity measures for KGE models and explain the rationale behind each one.\nFormally, given a set of training triples \ud835\udcaf1:n and a test triple tn+1 := tr(qn+1, en+1), the nonconformity measure S(\ud835\udcaf1:n, tn+1) estimates how unusual the triple tn+1 is as a part of \ud835\udcaf1:n.\nNegScore. The underlying idea of KGE methods is to assign higher plausibility scores to positive triples and lower scores to negative triples. Therefore, a natural choice for the nonconformity score is the negative value of the plausibility score. The intuition here is that a lower plausibility score indicates a higher nonconformity, suggesting that the triple is less consistent with the existing triples represented in the training set. Formally, let \ud835\udcdc\ud835\udcaf1:n denote a KGE model trained on \ud835\udcaf1:n, then the corresponding nonconformity measure is defined as\nS(\ud835\udcaf1:n, tn+1) = \u2212\ud835\udcdc\ud835\udcaf1:n (tn+1)        (5)\nMinmax. While the NegScore predictor directly uses the raw plausibility scores, the scale of these scores can vary significantly across different queries, potentially affecting the consistency and reliability of the nonconformity measure. To address this, we normalize the scores for each query using min-max scaling. This ensures that the nonconformity score reflects not only the raw plausibility but also the relative position of the triple within the score distribution for all possible triples in a given query. We then define the nonconformity measure as\nS(\ud835\udcaf1:n, tn+1) = \u2212\ud835\udcdc\ud835\udcaf1:n (tn+1),        (6)\nwhere\n\ud835\udcdc(tr(q, e)) =\n\ud835\udcdc(tr(q, e)) \u2212 mine'\u2208E \ud835\udcdc(tr(q, e'))\nmaxe'\u2208E \ud835\udcdc(tr(q, e')) \u2212 mine'\u2208E \ud835\udcdc(tr(q, e'))\u22c5        (7)\n(8)\nSoftmax. Another approach to normalizing plausibility scores is by using the softmax function, which converts the plausibility scores into a (uncalibrated) \"probability distribution\" over all possible answers for a given query. Unlike min-max scaling, Softmax scaling is more sensitive to the relative differences between scores, naturally highlighting the most likely triples while acknowledging others. This can result in more nuanced nonconformity measures, especially when the score distribution has varying degrees of separation between true and false triples. The nonconformity score is then defined as the of softmax outputs and the \"ground truth\" probability, which is assumed to be 1 for the true answer.\nS(\ud835\udcaf1:n, tn+1) = 1 \u2212 \ud835\udcdc\ud835\udcaf1:n (tn+1),        (9)"}, {"title": "5.2 Answer Set Construction", "content": "If we construct answer sets as describe in Section 3, we need to retrain the KGE model with \ud835\udcaf1:n \u222a {tr(qn+1, e)} and recalculate the nonconformity scores of training triples for testing each triple tr(qn+1, e) (for all e \u2208 E). It is computationally not feasible for KGE methods, given the huge number of possible entities e \u2208 E and the time-consuming training and hyper-parameter tuning process.\nWe adopt so called split/inductive conformal prediction to address this issue (see Algorithm 1 for details). The training set of size n is first divided into a proper training set \ud835\udcaf1:m of size m < n and a calibration set \ud835\udcafm+1:n of size n \u2212 m. Rather than using the entire training set to train the KGE model and evaluate nonconformity scores, we train the KGE model once on the proper training set \ud835\udcaf1:m and use it to calculate the nonconformity scores on the calibration set \ud835\udcafm+1:n. Intuitively, if the calibration set is chosen randomly and is sufficiently large, its empirical coverage should closely match the true coverage probability for a new query. This strategy significantly increases the efficiency of the conformal predictors while preserving the probabilistic guarantees in Theorem 1.\nFormally, in split conformal prediction, if we construct answer sets by including all entity e \u2208 E such that\n|{i = m + 1, ..., n + 1 : \u03b1i \u2265 \u03b1n+1}| +1>\nn\u2212m+1\u03b5,\nwhere\n\u03b1i := S(\ud835\udcaf1:m, tr(qi, ei)), i = m + 1, ..., n\n\u03b1n+1 := S(\ud835\udcaf1:m, tr(qn+1, e)).\nWe have the following proposition:\nProposition 1 . Given a set of triples \ud835\udcaf1:n+1 that are i.i.d, an error rate \u03b5 \u2208 (0, 1) and any nonconformity measure S. If n is large, the answer set of a test query \u0108(qn+1) constructed following Equation 11 satisfies\n\u2119(en+1 \u2208 \u0108(qn+1)) \u2265 1 \u2212 \u03b5        (11)\n(12)\nfurthermore, if there are no ties between nonconformity scores in the calibration set \ud835\udcafm+1:n, we have\n\u2119(en+1 \u2208 \u0108(qn+1)) \u2264 1 \u2212 \u03b5 + 1/(n \u2212 m + 1)         (13)\nThe proof of Proposition 1 can be found in. Note that the additional assumption for Equation 13 is a quite weak assumption, by using a random tie-breaking rule, this assumption could be avoided entirely."}, {"title": "6 Experiments", "content": "In this section, we present five experiments that evaluate the quality of the answer sets from (baseline) predictors in Section 4 (naive, Platt, topk) and conformal predictors (NegScore, Softmax, Minmax) in Section 5 based on coverage, size and adaptiveness desiderata.\nIn our experiments, we use four commonly used benchmark link prediction datasets: WN18\n6.1 Experiment 1: Coverage and Set Size on WN18 and FB15k\nA good set predictor should cover the true answer with a probability of at least 1 \u2212 \u03b5 (coverage desideratum) and predict smaller sets (size desideratum). In this experiment, we evaluate desiderata by measuring the coverage (i.e., \u2119(en+1 \u2208 \u0108(qn+1))) and the average size of answer sets for each method.\nEach procedure is repeated 15 times, and we report the mean and standard deviation (in brackets) across trials in Table 1. As usual in the evaluation of link prediction, for each query, we consider only answer candidates that did not already occur in the training and validation data.\nAs demonstrated in Proposition 1, conformal predictors consistently meet the coverage desideratum, with coverage tightly concentrated around 1 \u2212 \u03b5. Compared to baseline predictors that also satisfy the coverage desideratum, conformal predictors outperform them in terms of producing smaller answer sets, thus better satisfying the size desideratum.\nThe naive predictor, on the other hand, frequently fails to meet the coverage desideratum, often providing lower coverage than necessary, indicating that the plausibility scores are generally overconfident. While applying a calibration technique to the naive predictor (Platt predictor) improves coverage, it still does not meet the coverage guarantee, and the resulting significant increase in set size makes it impractical for use.\nThe topk predictor meets the coverage desideratum but generally produces larger and fixed-sized answer sets compared to the conformal predictors. In Appendix A.3, we also discuss simpler fixed-sized predictors and compare them to the topk predictor. It is worth noting that the topk predictor can be viewed as a specific case of the conformal predictor, where the nonconformity score is defined by the rank position.\nAdditionally, we observed that there is no universally optimal nonconformity score for conformal predictors; the choice is model- and dataset-dependent. For instance, NegScore seems to better capture the nonconformity of triples in distance-based models (TransE, RotatE), while Softmax and Minmax scores are more suitable for semantic matching models (RESCAL, DistMult, and ComplEx).\nThe calibration technique in should theoretically enhance the design of the nonconformity measure and thereby improve the conformal predictor. However, in our setting, it fails to do so. The results are presented in Table 7, with a discussion of potential reasons provided in the Appendix C.\nDue to space constraints, additional results, including those without filtering existing answers and results from more datasets, are provided in Appendix D. The conclusions are consistent across all scenarios.\n6.2 Experiment 2: Adaptiveness of Answer Sets\nThis experiment aims to determine whether the size of answer sets adapts well to the difficulty of the query. Unfortunately, there is no well justified way to evaluate query difficulty at the moment. We therefore follow the experimental protocol used in for computer vision tasks. The authors evaluate difficulty by looking at the rank of the true label in the ranking obtained from the classifier by ordering labels according to their softmax-probabilities. The higher the rank, the more difficult the query. Analogously, we use the rank of the true answer given by the KGE model to evaluate query difficulty.\nWe categorize queries by difficulty levels based on the rank of the true answer (e.g., 1-100, 101-200, etc.). For each difficulty level, we calculate the average size of answer sets. Figure 1 illustrates the size of answer sets stratified by query difficulty. The x-axis represents rank intervals from 1 to 3000, segmented into 100-rank bins (reflecting different difficulty levels), while the y-axis shows the average size of answer sets within each interval.\nWe observe that the size of answer sets generated by conformal predictors closely aligns with the difficulty levels of the queries, thereby fulfilling the adaptiveness desideratum. This is a valuable property because, in practice, the true answer to a query is unknown. By examining the size of the answer set, we can estimate the predictive uncertainty for the query.\n6.3 Experiment 3: Impact of Calibration Set Size on Answer Set Quality\nIn this experiment, we investigate the impact of size of the calibration set, \ud835\udcafm+1:n, on the quality of answer sets in terms of coverage and size desiderata. We randomly sampled calibration sets of 10, 100, 200, and 500 triples from the validation set for use in conformal prediction. We then evaluated the coverage and average size of the resulting answer sets. This process was repeated 20 times to compute the mean and standard deviation of the results. For comparison, we also evaluated the answer sets generated using the entire validation set as the calibration set.\nAs shown in Proposition 1, the coverage of conformal predictors with an i.i.d calibration set should fall between 1 \u2212 \u03b5 and 1 \u2212 \u03b5 + 1/(n \u2212 m + 1), where n \u2212 m is the size of the calibration set. This is confirmed by the results in Table 2. The size of answer sets generated by split conformal predictors depends on the alignment between the distribution of nonconformity scores in the calibration set and those in the original training set (which includes both the proper training set and the calibration set). A larger calibration set typically better represents the original training set, leading to tighter answer sets, as confirmed by the results in Table 2. Notably, even with a relatively small calibration set, the quality of the answer sets closely approximates that obtained using the entire validation set.\n6.4 Experiment 4: Impact of Error Rate on Answer Set Quality\nIn this experiment, we examine the effect of the user-specified error rate (\u03b5) on the quality of answer sets. Figure 2 illustrates how \u03b5 influences the size of answer sets (upper diagram) and the coverage of answer sets (lower diagram) across various predictors. The red line in lower diagram correspond to the desired coverage 1 \u2212 \u03b5.\nAs expected, the size of answer sets decreases as \u03b5 increases, aligning with the requirements discussed in Section 3. The topk and conformal predictors consistently generate smaller answer sets compared to the naive and Platt predictors. Notably, conformal predictors produce the most compact answer sets when the error rate is set to a very low value. In terms of coverage, conformal predictors consistently meet the probabilistic guarantee in Proposition 1 across the range of \u03b5 values."}, {"title": "7 Discussion", "content": "Our method predicts answer sets for queries with a guaranteed coverage of the true answer at a pre-specified probability, such as 90%, while maintaining a small average size. Unlike ranking-based outputs, our approach is particularly well-suited for decision-making in high-stakes domains, including medical diagnosis, drug discovery, and fraud detection. For instance, a doctor could use our method to automatically eliminate a large number of irrelevant diseases, thereby referring the patient to the most appropriate specialists. Additionally, our method is easy to implement and is compatible not only with any KGE models but also with embedding methods capable of answering more complex queries, such as Query2Box and CQD .\nThe adaptability of our answer sets to the uncertainty of queries also enables our method to quantify the predictive uncertainty of KGE models. This feature broadens the applicability of our approach by systematically identifying hard or uncertain queries during testing. Detecting such queries can help identify potential failure cases or outliers, alerting users when the model's predictions may be unreliable."}, {"title": "8 Limitations", "content": "A limitation of our method is the requirement to divide the training set into two parts: one for training the model and another for calculating the nonconformity scores, due to the adoption of split conformal prediction. This division reduces the number of triples available for model training. However, this issue is mitigated by the fact that the validation set, typically reserved for hyperparameter tuning, can also serve as the calibration set. Moreover, as demonstrated in Experiment 4, even a small subset of the validation set is sufficient to produce nearly optimal answer sets.\nAnother limitation is that the probabilistic guarantee provided by Theorem 1 and Proposition 1 relies on the i.i.d. assumption, which may not hold under distribution shifts. We are currently extending our conformal predictors to covariant shift case, where only the input distribution \u2119(\ud835\udc4b) changes while the conditional distribution \u2119(\ud835\udc4c|\ud835\udc4b) remains the same. We begin with the simpler scenario where the likelihood ratio between the training and test distributions is known. Following , we weight each nonconformity score proportionally to the likelihood ratio to ensure the probabilistic guarantee in Proposition 1 holds beyond the i.i.d. assumption."}, {"title": "A Baseline Predictors", "content": ""}, {"title": "A.1 Naive Predictor", "content": "We provide detailed pseudocode for naive predictor in this section. See Algorithm 2 for details."}, {"title": "A.2 Platt Predictor", "content": "The Platt predictor enhances the naive predictor using a calibration technique. The only difference in its procedure, as outlined in line 5 of Algorithm 2, is the modification of softmax outputs through temperature scaling a multiclass extension of Platt scaling .\nTemperature scaling employs a single scalar parameter T > 0 across all possible answer entities for a given query. Let M\u0398(tr(qn+1), ei) represent the plausibility score of entity ei \u2208 \u2130 for query qn+1. The calibrated score \u015di is then calculated as\n\u015di = \u03c3(M\u0398(tr(qn+1), ei)/T),        (14)\nwhere \u03c3(\u00b7) is the softmax function.\nThe parameter T, known as the temperature, \"softens\" the softmax output by increasing its entropy when T > 1. As T \u2192 \u221e, the probability \u015di approaches 1/|\u2130|, indicating maximum uncertainty. When T = 1, the original softmax output is recovered. Conversely, as T \u2192 0, the probability collapses to a point mass (\u015d1 = 1). The optimal"}, {"title": "A.3 Fixed-sized Predictor", "content": "The ranking-based metric Hits@K evaluate how often KGE models place the correct answers within the top-K entities, implicitly suggesting that the top-K entities should be chosen as answer sets. Based on Hits@K, we evaluate the quality of fixed-sized set predictor, which produce top-K entities (with a manually chosen K) as the answer set.\nWe select K values commonly used in Hits@K metrics (1, 3, 10, 100). The results in Table 3 and 4 demonstrate that coverage is highly sensitive to the choice of K. Concretely, the fixed-sized set predictor either fails to meet the coverage desideratum or generate unnecessarily large answer sets.\nConsequently, in the main body of the paper, we adopt the topk predictor, where we use \ud835\udc3e that cover the true answer in 1 \u2212 \u03b5 of queries in the validation set. The topk predictor effectively balances the trade-off between coverage and average size. However, unlike conformal predictors, the topk predictor cannot adapt answer set sizes to the difficulty of individual queries, as it uses the same size for all queries."}, {"title": "B Detailed Experimental Setting", "content": ""}, {"title": "B.1 Information About KGE Models and Benchmark Datasets", "content": "We provide the statistics of the benchmark datasets in Table 5 and the scoring functions of KGE methods in Table 6."}, {"title": "B.2 Personal Identification Issue in FB15k and FB15k237", "content": "While FB15k and FB15k237 contain information about individuals, it typically focuses on well-known public figures such as celebrities, politicians, and historical figures. Since this information is already widely available online and in various public sources, its inclusion in Freebase doesn't significantly compromise individual privacy compared to datasets containing sensitive personal information."}, {"title": "B.3 Details of Pre-training KGE Models", "content": "For training KGE models, we use the implementation of LibKGE and basically follow the hyperparameter search strategy in . All experiments were conducted on a Linux machine with a 40GB NVIDIA A100 SXM4 GPU.\nWe first conduct quasi-random hyperparameter search via a Sobol sequence, which aims to distribute hyperparameter settings evenly to avoid \"clumping\" effects . More specifically, for each dataset and model, we generated 30 different configurations per valid combination of training type and loss function. we added a short Bayesian optimization phase (best configuration so far + 30 new trials) to tune the hyperparameters further. All above steps are conducted using Ax framework (https://ax.dev/)\nWe use a large hyperparameter space including loss functions (pairwise margin ranking with hinge loss, binary cross entropy, cross entropy), regularization techniques (none/L1/L2/L3, dropout), optimizers (Adam, Adagrad), and initialization methods used in the KGE community as hyperparameters. We consider 128, 256, 512 as possible embedding sizes. More details see in , Table 5).\nThe hyperparameters of the baseline models are located within the software folder we submitted. Concretely, all configuration files (*.yaml) that we use for training baseline models/competing models/models for aggregation can be found in folder \"configs\"."}, {"title": "C Calibrated Conformal Predictor", "content": "Conformal prediction is a theoretical framework that quantifies predictive uncertainty by ensuring answer sets meet probabilistic guarantees, followed by identifying a nonconformity measure that minimizes the size of these sets. Optimal answer sets are achieved when nonconformity scores accurately reflect the confidence of the predictions.\nWhile calibrating plausibility scores from KGE models should theoretically improve the naive predictor and the nonconformity measure for conformal predictor, our results suggest otherwise. As shown in Table 1, 8, 9 and 10, Platt predictors yield excessively large answer sets. Further experiments comparing softmax conformal prediction before and after temperature scaling (Table 7) reveal that temperature scaling generally increases answer set sizes. Although smaller sets are observed for TransE, they are still not competitive with the best predictors for TransE in Table 1. These observations contradict our expectations. We next explore the reasons for these outcomes in KGE models.\nFirst, as detailed in Appendix A.2, temperature scaling adjusts plausibility scores by dividing by a temperature parameter T, optimized by minimizing negative log-likelihood on validation set. This calibration assumes two key points: (1) uniform miscalibration, where plausibility scores are consistently miscalibrated across the model (e.g., the KGE model is uniformly overconfident or underconfident for all queries); and (2) monotonic calibration, where the relative ordering of plausibility scores aligns with calibrated probabilities. These assumptions are overly stringent for KGE models, which tend to be overconfident with queries that have many correct answers and underconfident with those having fewer correct answers. Additionally, the relative ordering of plausibility scores is highly sensitive to minor hyperparameter changes.\nMoreover, applying temperature scaling or other calibration techniques requires formulating link prediction as a classification task. However, the validation set exhibits a long-tail distribution in the number of triples associated with certain entities, i.e. many entities have few or even no associated triples. It leads to insufficient data for effective calibration for entities associated with fewer triples."}, {"title": "D Further Results for Coverage & Set Size Evaluation", "content": ""}, {"title": "D.1 Coverage and Set Size on WN18RR and FB15k237", "content": "We repeated the experiment on WN18RR and FB15k237, datasets known to be more challenging than WN18 and FB15k due to the removal of inverse relations .\nThe results for the filtered answer sets are presented in Table 8, while the unfiltered results are available in Table 10 in Appendix D. The conclusions from Experiment 1 remain consistent; however, we observe a significant increase in set sizes for all set predictors, particularly for WN18RR. This increase is desirable, as it aligns with the adaptiveness desideratum, where the set predictor is expected to output smaller sets for simple queries and larger sets for harder ones."}, {"title": "E Further Results for Adaptiveness Evaluation", "content": "In Figure 3 - 14, we show the size of answer sets stratified by the difficulty level of queries for different conformal predictors across six representative KGE models and four benchmark datasets."}, {"title": "F AI Assistants In Writing", "content": "We use ChatGPT to enhance our writing skills, abstaining from its use in research and coding endeavors."}]}