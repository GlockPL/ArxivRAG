{"title": "Conformalized Answer Set Prediction for Knowledge Graph Embedding", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Jiarong Pan", "Bo Xiong", "Yunjie He", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to distinguish plausible from implausible answers, posing challenges for the application of KGE methods in high-stakes domains like medicine. We address this issue by applying the theory of conformal prediction that allows generating answer sets, which contain the correct answer with probabilistic guarantees. We explain how conformal prediction can be used to generate such answer sets for link prediction tasks. Our empirical evaluation on four benchmark datasets using six representative KGE methods validates that the generated answer sets satisfy the probabilistic guarantees given by the theory of conformal prediction. We also demonstrate that the generated answer sets often have a sensible size and that the size adapts well with respect to the difficulty of the query.", "sections": [{"title": "Introduction", "content": "Knowledge Graph Embeddings (KGE) map entities and predicates into numerical vectors, providing non-classical reasoning capabilities by exploiting similarities and analogies between entities and relations (Wang et al., 2017; Biswas et al., 2023). KGE models are typically evaluated through link prediction (Bordes et al., 2013; Sun et al., 2019; Nickel et al., 2011). To answer queries in the form of (head entity, predicate,?) or (?, predicate, tail entity), all possible entities are ranked according to their plausibility scores returned by the KGE models. Higher-ranked positions of the correct answers indicate better model performance.\nHowever, ranking all possible entities has limited practical value, since the rankings do not distinguish plausible answers from implausible ones with high quality. It is crucial to provide tight answer sets that provably cover the true answer, particularly in high-stakes domains like medicine, where reliable predictions and risk assessment are critical.\nIdentifying a set of plausible answer entities from the entire entity set is challenging. KGE models are trained to assign higher plausibility scores to true triples than to false ones. However, due to the lack of ground truth negative triples in KGs, negative examples used for training are typically generated by corrupting existing triples (Bordes et al., 2013; Sun et al., 2019). Some of the generated negative samples may actually be valid but unobserved triples, which can mislead the KGE model and result in incorrect plausibility scores. Furthermore, the training process uses gradient descent-based optimization techniques (Rumelhart et al., 1986), which do not guarantee convergence to the global optimum. As a result, the plausibility scores returned by KGE models not only fail to ensure correct triple ranking, but they also lack a clear probabilistic interpretation, meaning they do not correspond to the actual likelihood of a triple being true (i.e., they are uncalibrated). Therefore, constructing answer sets based on the uncalibrated plausibility scores can lead to somewhat arbitrary result sets.\nPrior works suggest to apply off-the-shelf calibration techniques, such as Platt scaling (Platt et al., 1999) and Isotonic regression (Kruskal, 1964), to map uncalibrated plausibility scores to the expected correctness of predictions (Tabacof and Costabello, 2020; Safavi et al., 2020). However, perfect calibration is impossible in practice (Gupta et al., 2020), both Platt scaling and Isotonic regression are em-"}, {"title": "Related Work", "content": "Uncertainty quantification in KGE methods remains a relatively unexplored area. Existing approaches incorporate uncertainty into KGE by modeling entities and relations using probability distributions (He et al., 2015; Xiao et al., 2016). However, these methods primarily focus on enhancing the performance of KGE models through more expressive representations, without systematically analyzing or rigorously evaluating the quality of uncertainty in embeddings or predictions.\nFurthermore, research by Tabacof and Costa-bello (2020) and Safavi et al. (2020) applies off-the-shelf calibration techniques, such as Platt scaling and Isotonic regression, to KGE methods. These techniques aim to convert uncalibrated plausibility scores into probabilities by minimizing the negative log-likelihood on a validation set. However, these approaches are quite sensitive to the validation set and do not provide formal guarantees about the generated probabilities.\nThis paper applies conformal prediction, which has its roots in online learning literature, is a method that produces predictive sets ensuring coverage guarantees (Vovk et al., 2005). This approach has been successfully applied across various domains, including image classification (Angelopoulos et al., 2021), natural language processing (Maltoudoglou et al., 2020) and node classification/regression on graphs (Huang et al., 2024; Zargarbashi et al., 2023; Zargarbashi and Bojchevski, 2023). However, to the best of our knowledge, it has not yet been applied to KGE."}, {"title": "Preliminaries", "content": ""}, {"title": "Knowledge Graph Embedding", "content": "We consider a knowledge graph (KG) $G \\subset E \\times R \\times E$ defined over a set $E$ of entities and a set $R$ of relation names. The elements in $G$ are called triples and denoted as $< h, r, t >$. A KGE model $M:E\\times R\\times E \\rightarrow \\mathbb{R}$ associates each triple with a score that measures the plausibility that the triple holds. The parameters $\\Theta$ are learned to let $M_{\\Theta}$ assign higher plausibility scores to positive triples (real facts) while assigning lower plausibility scores to negative triples (false facts).\nNote that the interpretation of plausibility scores varies across different types of KGE methods. In distance-based models like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019), the plausibility score is determined by the negative distance in the embedding space. In semantic matching models such as RESCAL (Nickel et al., 2011) and DistMult (Yang et al., 2015), plausibility scores are derived from similarity measures (often computed through the dot product of entity embeddings)."}, {"title": "Conformal Prediction", "content": "Conformal prediction (a.k.a conformal inference) is a general framework for producing answer sets that cover the ground truth with probabilistic guarantees (Vovk et al., 2005). In this section, we recall some basics from (Vovk et al., 2005; Shafer and Vovk, 2008).\nLet $(x_i, y_i)$ denote a data point with an object $x_i$ and its label $y_i$. The objects are elements of an object space $X$, and the labels are elements of a label space $Y$. For a more compact notation, we write $z_i$ for $(x_i, y_i)$, and call $Z := X \\times Y$ the example space. Furthermore, we let $Z_{1:n} = \\{z_1,...z_n\\} \\subseteq Z$ be the set of $n$ examples and denote $Z^*$ as the set of all possible example sets. A conformal predictor $\\Gamma : Z^* \\times X \\rightarrow 2^Y$ aims to predict a subset of $Y$ large enough to cover the ground truth with high probability. Given a training set $Z_{1:n}$ and any new object $x_{n+1} \\in X$, the conformal predictor $\\Gamma$ should, for every probability of error $\\epsilon \\in (0, 1)$, produce a answer set $\\Gamma^{\\epsilon}(Z_{1:n}, x_{n+1})$ for the input object $x_{n+1}$ that contains the ground truth label $y_{n+1}$ with probability at least $1 - \\epsilon$. Moreover, the answer sets are required to shrink as $\\epsilon$ increases: $\\Gamma^{\\epsilon_1} \\subset \\Gamma^{\\epsilon_2}$ whenever $\\epsilon_1 \\geq \\epsilon_2$.\nTo specify such a conformal predictor, we first need to define a nonconformity measure $S : Z^* \\times Z \\rightarrow \\mathbb{R}$. $S(Z_{1:n}, z_{n+1})$ measures how unusual the example $z_{n+1}$ is as an element of $Z_{1:n}$. Given any such a nonconformity measure $S$, if we construct the answer set $\\Gamma^{\\epsilon}(Z_{1:n}, x_{n+1})$ by including all $y \\in Y$ such that\n$\\frac{\\{i = 1, ..., n + 1 : \\alpha_i > \\alpha_{n+1}\\}|}{n+1} > \\epsilon$, (1)\nwhere\n$\\alpha_i := S(Z_{1:n} \\cup \\{(x_{n+1}, y)\\}, (x_i, y_i)), i = 1,...,n$\n$\\alpha_{n+1} := S(Z_{1:n} \\cup \\{(x_{n+1}, y)\\}, (x_{n+1},y))$,\nthen we have following probabilistic guarantees:\nSuppose n is large, and a set of examples $Z_{1:n+1}$ are independent and identically distributed (i.i.d.). Given $\\epsilon \\in (0,1)$, the answer set of the object $x_{n+1}$ constructed by a conformal predictor $\\Gamma^{\\epsilon} (Z_{1:n}, x_{n+1})$ cover the ground truth $y_{n+1}$ at least with probability $1 - \\epsilon$\n$\\mathbb{P}(y_{n+1} \\in \\Gamma^{\\epsilon}(Z_{1:n}, x_{n+1})) \\geq 1 - \\epsilon$ (2)\nfurthermore, if there are no ties between $\\alpha_i$, then it is also holds that\n$\\mathbb{P}(y_{n+1} \\in \\Gamma^{\\epsilon}(Z_{1:n}, x_{n+1})) \\leq 1-\\epsilon + \\frac{1}{n+1}$ (3)\nThe proof of Equation 2 is provided in (Vovk et al., 2005, section 2.1.3). Intuitively, the construction of $\\Gamma^{\\epsilon} (Z_{1:n}, x_{n+1})$ can be understood as an application of the widely accepted Neyman-Pearson theory (Lehmann et al., 1986) for hypothesis testing and confidence intervals (Shafer and Vovk, 2008). Here, we test for all $y \\in Y$ that the hypothesis $H$ (the example $(x_{n+1}, y)$ conforms to $Z_{1:n}$) by evaluating the nonconformity score of $(x_{n+1}, y)$. We construct the answer set by including all $y$, for which $(x_{n+1}, y)$ is not rejected by the test.\nAdditionally, the proof of Equation 3 is detailed in (Lei et al., 2018, Appendix A.1). Notably, the theorem remains valid under the weaker assumption of exchangeability (Vovk et al., 2005, section 2.1.1)."}, {"title": "KGE-based Answer Set Prediction", "content": "In this section, we formally define the KGE-based answer set prediction task and outline three key desiderata guiding the development of effective set predictors. We then introduce and discuss several basic set predictors."}, {"title": "Problem Definition and Desiderata", "content": "We reformulate the link prediction task as an answer set prediction task. Instead of object-label pairs $(x_i, y_i)$ in section 3, each data point is a triple $tr(q_i, e_i)$. Here, $q_i$ denotes a query in form of either $(h, r, ?)$ or $(?, r, t)$, and $tr(q, e)$ corresponds to the respective triple $(h, r, e)$ or $(e, r, t)$.\nGiven a set of (training) triples $T_{1:n} = \\{tr(q_1,e_1),..., tr(q_n, e_n)\\}$, a test query $q_{n+1}$ and a user-specific error rate $\\epsilon$, we aim to predict a set of entities $\\hat{C}(q_{n+1}) \\subseteq E$ that covers the true answer $e_{n+1}$ with probability at least $1 - \\epsilon$.\n$\\mathbb{P}(e_{n+1} \\in \\hat{C}(q_{n+1})) \\geq 1 - \\epsilon$ (4)\nWe refer to Equation 4 as the coverage desideratum. However, this criterion alone is insufficient, as it can be trivially met by a predictor that always outputs sets containing all possible answers. To develop sensible set predictors, we also consider the size desideratum and the adaptiveness desideratum. The size desideratum emphasizes the need for smaller sets, as smaller sets are generally more informative. The adaptiveness desideratum requires that the set sizes reflect query difficulty: smaller sets should correspond to easier queries, while larger sets should be used for harder queries."}, {"title": "Basic Set Predictors", "content": "Naive Predictor. Given a query, assume the KGE model provides the probability of each possible answer entity being true. A straightforward approach towards our goal is to construct the set by including entities from highest to lowest probability until their sum exceeds the threshold $1 - \\epsilon$. We refer to this approach as the naive predictor and provide its pseudocode in Algorithm 2 (Appendix A). However, the plausibility scores provided by KGE models are not calibrated. We convert these plausibility scores into a \"probability distribution\" using a softmax function.\nPlatt Predictor. Following the recommendations of (Tabacof and Costabello, 2020; Safavi et al., 2020), we improve the naive predictor by using a multiclass Platt scaling (Guo et al., 2017) to calibrate the plausibility scores and then construct sets based on these calibrated probabilities. We refer to this method as the Platt predictor and provide more details of this predictor in Appendix A.2.\nTopK Predictor. Another straightforward approach is to construct the set with the Top-K entities from the ranking, referred to as the topk predictor. We select K to ensure the Top-K entities cover the correct answers for $1 - \\epsilon$ of the validation queries."}, {"title": "Conformal Prediction for KGE-based Answer Set Prediction", "content": "To improve the basic set predictors, we apply conformal prediction, a general framework that requires adaptation to be effective in the context of KGE. The two essential components in this design are the nonconformity measure and the method for constructing answer sets. In this section, we propose several KGE-specific nonconformity measures and outline an efficient approach to constructing answer sets."}, {"title": "Nonconformity Measures", "content": "The probabilistic guarantees in Theorem 1 hold under i.i.d assumption, regardless of the data distribution or the definition of the nonconformity measure. However, the size of the resulting answer sets depends on how effectively the nonconformity measure captures the underlying structure of the data. Next, we introduce several nonconformity measures for KGE models and explain the rationale behind each one.\nFormally, given a set of training triples $T_{1:n}$ and a test triple $t_{n+1} := tr(q_{n+1}, e_{n+1})$, the nonconformity measure $S(T_{1:n}, t_{n+1})$ estimates how unusual the triple $t_{n+1}$ is as a part of $T_{1:n}$.\nNegScore. The underlying idea of KGE methods is to assign higher plausibility scores to positive triples and lower scores to negative triples. Therefore, a natural choice for the nonconformity score is the negative value of the plausibility score. The intuition here is that a lower plausibility score indicates a higher nonconformity, suggesting that the triple is less consistent with the existing triples represented in the training set. Formally, let $M_{T_{1:n}}$ denote a KGE model trained on $T_{1:n}$, then the corresponding nonconformity measure is defined as\n$S(T_{1:n}, t_{n+1}) = -M_{T_{1:n}}(t_{n+1})$ (5)\nMinmax. While the NegScore predictor directly uses the raw plausibility scores, the scale of these scores can vary significantly across different queries, potentially affecting the consistency and reliability of the nonconformity measure. To address this, we normalize the scores for each query using min-max scaling. This ensures that the nonconformity score reflects not only the raw plausibility but also the relative position of the triple within the score distribution for all possible triples in a given query. We then define the nonconformity measure as\n$S(T_{1:n}, t_{n+1}) = - \\overline{M_{T_{1:n}}}(t_{n+1})$, (6)\nwhere\n$\\overline{M}(tr(q,e)) = \\frac{M(tr(q, e)) - min_{e' \\in E} M(tr(q, e'))}{max_{e' \\in E} M(tr(q, e')) - min_{e' \\in E} M(tr(q, e'))}$. (7) (8)\nSoftmax. Another approach to normalizing plausibility scores is by using the softmax function, which converts the plausibility scores into a (uncalibrated) \"probability distribution\" over all possible answers for a given query. Unlike min-max scaling, Softmax scaling is more sensitive to the relative differences between scores, naturally highlighting the most likely triples while acknowledging others. This can result in more nuanced nonconformity measures, especially when the score distribution has varying degrees of separation between true and false triples. The nonconformity score is then defined as the of softmax outputs and the \"ground truth\" probability, which is assumed to be 1 for the true answer.\n$S(T_{1:n}, t_{n+1}) = 1 - M_{T_{1:n}}(t_{n+1})$, (9)"}, {"title": "Answer Set Construction", "content": "If we construct answer sets as describe in Section 3, we need to retrain the KGE model with $T_{1:n} \\cup \\{tr(q_{n+1},e)\\}$ and recalculate the nonconformity scores of training triples for testing each triple $tr(q_{n+1}, e)$ (for all $e \\in E$). It is computationally not feasible for KGE methods, given the huge number of possible entities $e \\in E$ and the time-consuming training and hyper-parameter tuning process.\nWe adopt so called split/inductive conformal prediction (Vovk et al., 2005; Lei et al., 2015) to address this issue (see Algorithm 1 for details). The training set of size n is first divided into a proper training set $T_{1:m}$ of size $m < n$ and a calibration set $T_{m+1:n}$ of size $n - m$. Rather than using the entire training set to train the KGE model and evaluate nonconformity scores, we train the KGE model once on the proper training set $T_{1:m}$ and use it to calculate the nonconformity scores on the calibration set $T_{m+1:n}$. Intuitively, if the calibration set is chosen randomly and is sufficiently large, its empirical coverage should closely match the true coverage probability for a new query. This strategy significantly increases the efficiency of the conformal predictors while preserving the probabilistic guarantees in Theorem 1 (Lei et al., 2018).\nFormally, in split conformal prediction, if we construct answer sets by including all entity $e \\in E$ such that\n$\\frac{\\{i = m + 1, ..., n + 1 : \\alpha_i \\geq \\alpha_{n+1}\\} + 1}{n - m + 1} > \\epsilon$, (11)\nwhere\n$\\alpha_i := S(T_{1:m}, tr(q_i, e_i)), i = m + 1,..., n$\n$\\alpha_{n+1} := S(T_{1:m}, tr(q_{n+1}, e))$.\nWe have the following proposition:\nGiven a set of triples $T_{1:n+1}$ that are i.i.d, an error rate $\\epsilon \\in (0,1)$ and any nonconformity measure S. If n is large, the answer set of a test query $\\hat{C}(q_{n+1})$ constructed following Equation 11 satisfies\n$\\mathbb{P}(e_{n+1} \\in \\hat{C}(q_{n+1})) \\geq 1 - \\epsilon$ (12)\nfurthermore, if there are no ties between nonconformity scores in the calibration set $T_{m+1:n}$, we have\n$\\mathbb{P}(e_{n+1} \\in \\hat{C}(q_{n+1})) \\leq 1 - \\epsilon + \\frac{1}{n-m+1}$ (13)\nThe proof of Proposition 1 can be found in (Lei et al., 2018, Appendix A.1) 1. Note that the additional assumption for Equation 13 is a quite weak assumption, by using a random tie-breaking rule, this assumption could be avoided entirely."}, {"title": "Experiments", "content": "In this section, we present five experiments that evaluate the quality of the answer sets from (baseline) predictors in Section 4 (naive, Platt, topk) and conformal predictors (NegScore, Softmax, Minmax) in Section 5 based on coverage, size and adaptiveness desiderata.\nIn our experiments, we use four commonly used benchmark link prediction datasets: WN18"}, {"title": "Experiment 1: Coverage and Set Size on WN18 and FB15k", "content": "A good set predictor should cover the true answer with a probability of at least $1 - \\epsilon$ (coverage desideratum) and predict smaller sets (size desideratum). In this experiment, we evaluate desiderata by measuring the coverage (i.e., $\\mathbb{P}(e_{n+1} \\in \\hat{C}(q_{n+1}))$) and the average size of answer sets for each method.\nEach procedure is repeated 15 times, and we report the mean and standard deviation (in brackets) across trials in Table 1. As usual in the evaluation of link prediction, for each query, we consider only answer candidates that did not already occur in the training and validation data.\nAs demonstrated in Proposition 1, conformal predictors consistently meet the coverage desideratum, with coverage tightly concentrated around $1 - \\epsilon$. Compared to baseline predictors that also satisfy the coverage desideratum, conformal predictors outperform them in terms of producing smaller answer sets, thus better satisfying the size desideratum.\nThe naive predictor, on the other hand, frequently fails to meet the coverage desideratum, often providing lower coverage than necessary, indicating that the plausibility scores are generally overconfident. While applying a calibration technique to the naive predictor (Platt predictor) improves coverage, it still does not meet the coverage guarantee, and the resulting significant increase in set size makes it impractical for use.\nThe topk predictor meets the coverage desideratum but generally produces larger and fixed-sized answer sets compared to the conformal predictors. In Appendix A.3, we also discuss simpler fixed-sized predictors and compare them to the topk predictor. It is worth noting that the topk predictor can be viewed as a specific case of the conformal predictor, where the nonconformity score is defined by the rank position.\nAdditionally, we observed that there is no universally optimal nonconformity score for conformal predictors; the choice is model- and dataset-dependent. For instance, NegScore seems to better capture the nonconformity of triples in distance-based models (TransE, RotatE), while Softmax and Minmax scores are more suitable for semantic matching models (RESCAL, DistMult, and ComplEx).\nThe calibration technique in (Tabacof and Costabello, 2020; Safavi et al., 2020) should theoretically enhance the design of the nonconformity measure and thereby improve the conformal predictor. However, in our setting, it fails to do so. The results are presented in Table 7, with a discussion of potential reasons provided in the Appendix C.\nDue to space constraints, additional results, including those without filtering existing answers and results from more datasets, are provided in Appendix D. The conclusions are consistent across all scenarios."}, {"title": "Experiment 2: Adaptiveness of Answer Sets", "content": "This experiment aims to determine whether the size of answer sets adapts well to the difficulty of the query. Unfortunately, there is no well justified way to evaluate query difficulty at the moment. We therefore follow the experimental protocol used in (Angelopoulos et al., 2021) for computer vision tasks. The authors evaluate difficulty by looking at the rank of the true label in the ranking obtained from the classifier by ordering labels according to their softmax-probabilities. The higher the rank, the more difficult the query. Analogously, we use the rank of the true answer given by the KGE model to evaluate query difficulty.\nWe categorize queries by difficulty levels based on the rank of the true answer (e.g., 1-100, 101-200, etc.). For each difficulty level, we calculate the average size of answer sets. illustrates the size of answer sets stratified by query difficulty. The x-axis represents rank intervals from 1 to 3000, segmented into 100-rank bins (reflecting different difficulty levels), while the y-axis shows the average size of answer sets within each interval.\nWe observe that the size of answer sets generated by conformal predictors closely aligns with the difficulty levels of the queries, thereby fulfilling the adaptiveness desideratum. This is a valuable property because, in practice, the true answer to a query is unknown. By examining the size of the answer set, we can estimate the predictive uncertainty for the query."}, {"title": "Experiment 3: Impact of Calibration Set Size on Answer Set Quality", "content": "In this experiment, we investigate the impact of size of the calibration set, $T_{m+1:n}$, on the quality of answer sets in terms of coverage and size desiderata. We randomly sampled calibration sets of 10, 100, 200, and 500 triples from the validation set for use in conformal prediction. We then evaluated the coverage and average size of the resulting answer sets. This process was repeated 20 times to compute the mean and standard deviation of the results. For comparison, we also evaluated the answer sets generated using the entire validation set as the calibration set.\nAs shown in Proposition 1, the coverage of conformal predictors with an i.i.d calibration set should fall between $1 - \\epsilon$ and $1-\\epsilon + \\frac{1}{n-m}$, where $n-m$ is the size of the calibration set. This is confirmed by the results in Table 2. The size of answer sets generated by split conformal predictors depends on the alignment between the distribution of nonconformity scores in the calibration set and those in the original training set (which includes both the proper training set and the calibration set). A larger calibration set typically better represents the original training set, leading to tighter answer sets, as confirmed by the results in Table 2. Notably, even with a relatively small calibration set, the quality of the answer sets closely approximates that obtained using the entire validation set."}, {"title": "Experiment 4: Impact of Error Rate on Answer Set Quality", "content": "In this experiment, we examine the effect of the user-specified error rate ($\\epsilon$) on the quality of answer sets. shows how $\\epsilon$ influences the size of answer sets (upper diagram) and the coverage of answer sets (lower diagram) across various predictors. The red line in lower diagram correspond to the desired coverage $1 - \\epsilon$.\nAs expected, the size of answer sets decreases as $\\epsilon$ increases, aligning with the requirements discussed in Section 3. The topk and conformal predictors consistently generate smaller answer sets compared to the naive and Platt predictors. Notably, conformal predictors produce the most compact answer sets when the error rate is set to a very low value. In terms of coverage, conformal predictors consistently meet the probabilistic guarantee in Proposition 1 across the range of $\\epsilon$ values."}, {"title": "Discussion", "content": "Our method predicts answer sets for queries with a guaranteed coverage of the true answer at a pre-specified probability, such as 90%, while maintaining a small average size. Unlike ranking-based outputs, our approach is particularly well-suited for decision-making in high-stakes domains, including medical diagnosis, drug discovery, and fraud detection. For instance, a doctor could use our method to automatically eliminate a large number of irrelevant diseases, thereby referring the patient to the most appropriate specialists. Additionally, our method is easy to implement and is compatible not only with any KGE models but also with embedding methods capable of answering more complex queries, such as Query2Box (Ren et al., 2020) and CQD (Arakelyan et al.).\nThe adaptability of our answer sets to the uncertainty of queries also enables our method to quantify the predictive uncertainty of KGE models. This feature broadens the applicability of our approach by systematically identifying hard or uncertain queries during testing. Detecting such queries can help identify potential failure cases or outliers, alerting users when the model's predictions may be unreliable."}, {"title": "Limitations", "content": "A limitation of our method is the requirement to divide the training set into two parts: one for training the model and another for calculating the nonconformity scores, due to the adoption of split conformal prediction. This division reduces the number of triples available for model training. However, this issue is mitigated by the fact that the validation set, typically reserved for hyperparameter tuning, can also serve as the calibration set. Moreover, as demonstrated in Experiment 4, even a small subset of the validation set is sufficient to produce nearly optimal answer sets.\nAnother limitation is that the probabilistic guarantee provided by Theorem 1 and Proposition 1 relies on the i.i.d. assumption, which may not hold under distribution shifts. We are currently extending our conformal predictors to covariant shift case, where only the input distribution $P(X)$ changes while the conditional distribution $P(Y|X)$ remains the same. We begin with the simpler scenario where the likelihood ratio between the training and test distributions is known. Following (Tibshirani et al., 2019), we weight each nonconformity score proportionally to the likelihood ratio to ensure the probabilistic guarantee in Proposition 1 holds beyond the i.i.d. assumption."}]}