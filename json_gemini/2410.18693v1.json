{"title": "UNLEASHING REASONING CAPABILITY OF LLMS VIA SCALABLE QUESTION SYNTHESIS FROM SCRATCH", "authors": ["Yuyang Ding", "Xinyu Shi", "Xiaobo Liang", "Juntao Li", "Qiaoming Zhu", "Min Zhang"], "abstract": "The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance. Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs. To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints. With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets. It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet.", "sections": [{"title": "INTRODUCTION", "content": "How to improve the reasoning capabilities of Large Language Models (LLMs) has attracted significant attention. The success of recent advanced models, such as OpenAI o1 and Claude-3.5, heavily depends on access to extensive, diverse, and high-quality reasoning datasets. However, the proprietary nature of the data presents a significant barrier to the open-source community. Recent works have highlighted data synthesis as a promising approach (Ntoutsi et al., 2020) to address data scarcity for instruction tuning (Inan et al., 2023). As recent works have disclosed that crafting the right questions is crucial for eliciting the reasoning capabilities of LLMs (Yu et al., 2023a; Shah et al., 2024), the core of reasoning data synthesis lies in creating large-scale and novel questions.\nPrevious efforts in reasoning data synthesis have demonstrated the effectiveness of leveraging powerful language models to generate instructions. We categorize these approaches into two types: question-driven approaches and knowledge-driven approaches. Question-driven methods include question rephrasing (Yu et al., 2023a), evol-instruct (Xu et al., 2023; Luo et al., 2023; Zeng et al., 2024), question back-translation (Lu et al., 2024), or providing few-shot examples (Mitra et al., 2024). These methods are limited in data diversity, as the generated problems closely resemble the seed questions, with only minor modifications such as added conditions or numerical changes. This lack of diversity hampers their scalability potential. To improve question diversity, recent knowledge-driven works (Huang et al., 2024b) scale question synthesis by constructing knowledge bases (Li et al., 2024b) or concept graphs (Tang et al., 2024) and sampling key points (Huang et al., 2024a) from them to generate new questions. Nevertheless, the above two types of approaches commonly rely on strong models, like GPT-4, to synthesize new questions, but the high API costs make it impractical to generate large-scale data. As a result, despite these advancements, the open-source community still faces a shortage of high-quality data at scale and cost-effective synthesis methods.\nTo meet this requirement, we explore a scalable, low-cost method for data synthesis. We observe that using problem-solving models to directly synthesize reasoning questions, as explored in Yu et al. (2023b) and Xu et al. (2024), falls short in synthesizing reasoning data, as shown in Figure 1 (see Llama3-8B-Magpie results). Accordingly, we propose a novel, scalable, and cost-effective data synthesis method, ScaleQuest, which first introduces a two-stage question-tuning process consisting of Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capability of problem-solving models. Once fine-tuned, these models can then generate diverse questions by sampling from a broad search space without the need for additional seed questions or knowledge constraints. The generated questions can be further refined through a filtering process, focusing on language clarity, solvability, and appropriate difficulty. Moreover, we introduce an extra reward-based filtering strategy to select high-quality responses.\nWe generated data based on two lightweight, open-source models: DeepSeekMath-7B-RL (Shao et al., 2024) and Qwen2-Math-7B-Instruct (Yang et al., 2024a), producing a final dataset of 1 million question-answer pairs. As shown in Figure 1, our synthetic dataset boosts performance by 29.2% to 46.4% across four major open-source models: Mistral-7B (Jiang et al., 2023), Llama3-8B (Dubey et al., 2024), DeepSeekMath-7B (Shao et al., 2024), and Qwen2-Math-7B (Yang et al., 2024a). Compared with other publicly available datasets such as MetaMath (Yu et al., 2023a), DART-Math (Tong et al., 2024), and NuminaMath (Li et al., 2024c), our approach demonstrates great scalability in both in-domain and out-of-domain evaluation. In terms of in-domain evaluation, our method outperforms existing high-quality open-source datasets, achieving better results with the same amount of data. For out-of-domain evaluation, compared with other datasets, the performance of our synthetic dataset continues to show promising trends as the volume of training data increases, indicating significant potential for further improvements through ongoing data scaling."}, {"title": "SCALEQUEST: SCALING QUESTION SYNTHESIS FROM SCRATCH", "content": "In this section, we first explain the motivation and process of our question generation method (section 2.1). Then, we introduce how to train a question generator via Question Fine-Tuning (section 2.2) and Question Preference Optimization (section 2.3). Next, we use the question generator to generate math questions, followed by a filtering process (section 2.4). Finally, we describe the response generation process (section 2.5). The overview of our method is illustrated in Figure 2."}, {"title": "2.1 QUESTION GENERATION FROM SCRATCH", "content": "The question generation process involves providing only a few prefix tokens from an instruction template (e.g., \u201c<|begin_of_sentence|>User:", "<|begin_of_sentence|>User: {Question}. Assistant: {Response}\"), could potentially be leveraged to generate questions directly (Xu et al., 2024). This is because, during instruction tuning, the model is trained using a causal mask, where each token only attends to preceding tokens. This ensures that the hidden states evolve based on past context without future token influence. However, during instruction tuning, the actual loss is calculated based on the response, i.e.,\n$\\mathcal{L} = -\\log P(y_i|X, Y_{<i}),$\nwhere $X = \\{x_1,x_2,...,x_m\\}$ denotes question and $Y = \\{Y_1, Y_2, ..., Y_n \\}$ denotes response. Since $P(x_i|x_{i<i})$ is inherently modeled, we need to activate the model's capability for question generation.\"\n    },\n    {\n      \"title\": \"2.2 QUESTION FINE-TUNING (QFT)\",\n      \"content\": \"To activate the model's question generation capability, we first perform Question Fine-Tuning (QFT), where we train the problem-solving model using a small set of problems. To ensure that the generator stops after producing the questions and does not continue generating a response, we added an end-of-sentence token at the end of each question. We used approximately 15K problems (without solutions) by mixing the training set of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets as training samples. We train DeepSeekMath-7B-RL Shao et al. (2024) and Qwen2-Math-7B-Instruct Yang et al. (2024a) with these samples.\nThe purpose of utilizing these problems is to activate the model's question-generation capability rather than to make the model memorize them. To validate this hypothesis, we trained the model separately using the GSM8K and MATH datasets and compared whether the distribution of the generated questions matched that of the training data. To evaluate the\"\n    },\n    {\n      \"title\": \"2.3 QUESTION PREFERENCE OPTIMIZATION (QPO)\",\n      \"content\": \"The model is able to generate meaningful and diverse questions after QFT, but the quality is still not high enough, as shown in Figure 2. This is reflected in two aspects: (1) solvability: the math problem should have appropriate constraints and correct answers, and (2) difficulty: the model needs to learn from more challenging problems, yet some of the generated questions are still too simple. To address these two aspects, we applied Question Preference Optimization (QPO).\nWe first used the model after QFT to generate 10K questions. Then, we optimized these samples using an external LLM, focusing primarily on solvability and difficulty. We found that simultaneously optimizing both posed a challenge for the LLMs. Therefore, for each sample, we randomly selected one of the two optimization directions, prioritizing either solvability or difficulty. The optimization prompts can be found in Figure 9 and 10. The optimized questions, denoted as $y_w$, are treated as preferred data, while the original questions before optimization, denoted as $y_r$, are considered dispreferred data. We modified the loss for Direct Preference Optimization (DPO) (Rafailov et al., 2024) formulation to fit our approach:\n$\\mathcal{L}_{QPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(y_w, y_l) \\sim D}[\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w)}{\\pi_{ref}(y_w)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l)}{\\pi_{ref}(y_l)})],$\nThe question optimization process placed significant demands on the model's ability to follow complex instructions. We experimented with two question optimization models: Qwen2-Math-7B-Instruct and GPT-4o-mini. To evaluate improvements in solvability and difficulty, we used GPT-4o, with the prompts for this evaluation provided in Figure 11 and 12. The results are shown in Figure 4. In terms of solvability, Qwen2-Math-7B-Instruct proved inadequate for this task, as the optimized questions resulted in decreased solvability. A possible reason for this is the model's insufficient ability to follow instructions accurately, resulting in many answers that fail to meet the specified optimization constraints. Consequently, we selected GPT-4o-mini as the question optimization model.\"\n    },\n    {\n      \"title\": \"2.4 QUESTION FILTERING\",\n      \"content\": \"After the QFT and QPO phases, we obtained two question generators: DeepSeekMath-QGen and Qwen2-Math-QGen. There are still some minor issues in the generated questions, primarily related to language, solvability, and difficulty. To address these challenges, we applied the following filtering steps:\nLanguage Filtering The question generator models still produce a substantial number of math questions in other languages, accounting for approximately 20%. Since our focus is on English\"\n    },\n    {\n      \"title\": \"Solvability Filering\",\n      \"content\": \"Although QPO effectively enhances the solvability of generated questions, some questions remain nonsensical. This is primarily due to (1) poorly constrained questions, where missing conditions, redundant conditions, or logical inconsistencies occur, and (2) questions that do not yield meaningful outcomes (e.g., answers involving the number of people should result in a non-negative integer). To filter out such samples, we used Qwen2-Math-7B-Instruct to evaluate whether the question is meaningful and whether the conditions are sufficient. The prompts used for the solvability check are provided in Figure 11.\"\n    },\n    {\n      \"title\": \"Difficulty Sampling\",\n      \"content\": \"We measure the difficulty of a question using the fail rate (Tong et al., 2024) - the proportion of incorrect responses when sampling n responses for a given question. This metric aligns with the intuition that harder questions tend to result in fewer correct responses. Following Tong et al. (2024), we used DeepseekMath-7B-RL as the sampling model to evaluate the difficulty of each question in the training sets of GSM8K and MATH, obtaining the fail rate for each question as its difficulty score. We then used this data to train a difficulty scorer. Specifically, we built upon DeepseekMath-7B-Base and added a classification head on top of the model's hidden state. The difficulty score d is computed and optimized as:\n$d = W h_l + b, \\quad \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (y_i - d_i)^2,$\nwhere $W$ and $b$ are the weights and biases of the classification head, $h_l$ represents the last hidden state of the sequence, and $d_i$ is the predicted difficulty score for the i-th question. The loss function $\\mathcal{L}$ is the mean squared error (MSE), where $y_i$ represents the true difficulty score for the i-th question. We then used the scorer to predict the difficulty of each synthetic question and sample based on the question's difficulty. Specifically, we filtered out a portion of the questions generated by DeepSeekMath-QGen that were overly simple. In contrast, the difficulty distribution of Qwen2-Math-QGen was more balanced, so no sampling was necessary.\"\n    },\n    {\n      \"title\": \"2.5 RESPONSE GENERATION WITH REWARD FILTERING\",\n      \"content\": \"Prior efforts to guarantee the quality of solutions include two aspects: (1) rejection sampling (Yuan et al., 2023): Large language models (LLMs) are tasked with generating multiple responses, specifically reasoning paths, for each instruction. Only reasoning paths that lead to the correct answer are preserved as solutions (Tong et al., 2024). (2) If the correct answer is unavailable, a majority voting method is used (Huang et al., 2024a), selecting the answer that appears most frequently across multiple reasoning paths and retaining these as the solutions. We use the reward model score as a metric for evaluating the quality of responses, considering its broader applicability, as there is often no single correct answer in other reasoning tasks like code generation and tool planning. Specifically, for each question, we generate 5 solutions and select the solution with the highest reward model scores as the preferred solution. In our experiments, we use InternLM2-7B-Reward (Cai et al., 2024) as our reward model.\"\n    },\n    {\n      \"title\": \"EXPERIMENT\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"3.1 EXPERIMENTAL SETUP\",\n      \"content\": \"Training Problem Designers Our question synthesis process relies on two problem designer models: Deepseek-QGen and Qwen2-Math-QGen, which were trained using QFT (section 2.2) and QPO (section 2.3), based on DeepSeekMath-7B-RL (Shao et al., 2024) and Qwen2-Math-7B-Instruct (Yang et al., 2024a), respectively. During the QFT stage, both models are trained on a mixed training subset of GSM8K and MATH problems, containing a total of 15K problems. We trained for only 1 epoch, considering that training for more epochs might cause the models to overfit the training problems and negatively impact the diversity of generated questions. We also used sequence packing (Krell et al., 2021) to accelerate training. In the QPO stage, we use 10K preference data for training, with a learning rate of 5e-7 and a batch size of 128.\"\n    },\n    {\n      \"title\": \"Question Generation\",\n      \"content\": \"The two question generation models were then utilized to generate a total of 2 million questions, with 1 million from each model. During this process, we set the maximum generation length to 512, a temperature of 1.0, and a top-p value of 0.99. To ensure quality, we applied a question filtering pipeline (section 2.4) that involved language filtering, solvability filtering, and difficulty sampling. This process refined the dataset, leaving approximately 1M questions to form the final question pool, 400K from Deepseek-QGen and 600K from Qwen2-Math-QGen.\"\n    },\n    {\n      \"title\": \"Response Generation\",\n      \"content\": \"Based on the problems, we synthesized responses (section 2.5) using Qwen2-Math-7B-Instruct (Yang et al., 2024a). In the process, we set the maximum generation length to 2048, with a temperature of 0.7 and top-p of 0.95. We use chain-of-thought prompt (Wei et al., 2022) to synthesize solutions. We use vLLM (Kwon et al., 2023) to accelerate the generation and Ray (Moritz et al., 2018) to deploy distributed inference. For each problem, we sampled 5 solutions and selected the one with the highest reward score as the final response. The final dataset consists of 1 million problem-solution pairs.\"\n    },\n    {\n      \"title\": \"Instruction Tuning\",\n      \"content\": \"We conducted instruction tuning on the synthetic problems and solutions using two general base models, Mistral-7B (Jiang et al., 2023) and Llama3-8B (Dubey et al., 2024), as well as two math-specialized base models, DeepSeekMath-7B (Shao et al., 2024) and Qwen2-Math-7B (Yang et al., 2024a). All models were fine-tuned for 3 epochs in our experiments unless specified otherwise. We used a linear learning rate schedule with a 3% warm-up ratio, reaching a peak of 5e-5 for Llama3 and DeepSeekMath and le-5 for the other models, followed by cosine decay to zero.\"\n    },\n    {\n      \"title\": \"Evaluation and Metrics\",\n      \"content\": \"We assessed the fine-tuned models' performance across four datasets of increasing difficulty. Along with the widely used GSM8K (elementary level) and MATH (competition level), we included two more challenging benchmarks: College Math (Yuan et al., 2023) (college level) and Olympiad Bench (He et al., 2024) (Olympiad level). For evaluation, we employed the script from Tong et al. (2024) to extract final answers and determine correctness by comparing answer equivalency. The generated outputs were all in the form of natural language Chain-of-Thought (CoT) reasoning (Wei et al., 2022) through greedy decoding, with no tool integration, and we report zero-shot pass@1 accuracy.\"\n    },\n    {\n      \"title\": \"Compared Baselines\",\n      \"content\": \"The main point of comparison is data synthesis methods, including: (1) WizardMath (Luo et al., 2023) proposes a reinforced Evol Instruct method; (2) MetaMath (Yu et al., 2023a) introduces three types of question bootstrapping; (3) MMIQC (Liu & Yao, 2024) proposes an iterative question composing method; (4) Orca-Math (Mitra et al., 2024) augments existing datasets using an Agent-Instruct method; (5) KPMath (Huang et al., 2024a) utilizes inherent topics and key points to synthesize problems; and (6) MathScale (Tang et al., 2024) builds a concept graph to generate new questions. In addition to this, we also involved other large math corpus like (7) DART-Math (Tong et al., 2024) enhances the response generation process through difficulty-guided rejection sampling; (8) Numina-Math (Li et al., 2024c) collects a large corpus by combining existing synthetic data with real-world datasets. We found that different scripts yielded varying evaluation results. To ensure consistency, we evaluated all released models using the same evaluation scripts. For methods without available results or released models, we retrained the models using their publicly available data.\"\n    },\n    {\n      \"title\": \"3.2 MAIN RESULTS\",\n      \"content\": \"ScaleQuest significantly outperforms others Table 1 presents the results. ScaleQuest significantly outperforms previous synthetic methods, with average performance improvements ranging from 5.6% to 11.5% over the prior state-of-the-art (SoTA) on both general base models and math-specialized foundation models. Qwen2-Math-7B-ScaleQuest achieved a zero-shot pass@1 accuracy of 73.4 on the MATH benchmark, matching the performance of GPT-4-Turbo. For out-of-domain tasks, Qwen2-Math-7B-ScaleQuest outperformed its teacher model, Qwen2-Math-7B-Instruct, with scores of 89.7 on the GSM8K benchmark, 73.4 on the MATH benchmark, and 38.5 on the Olympiad benchmark. It's important to highlight that Qwen2-Math-7B-Instruct has undergone Group Relative Policy Optimization (GRPO) (Shao et al., 2024), utilizing the powerful reward model Qwen2-Math-RM-72B (Yang et al., 2024a), while our model is only an instruction tuning version. To ensure a fair comparison with other baselines, we have only applied supervised fine-tuning (SFT) in this work, leaving the preference tuning process for future work.\nScaleQuest scales well with increasing data We also explored the scalability of our dataset. We used our constructed dataset along with publicly available datasets, including MetaMath (Yu et al., 2023a), DART-Math (Tong et al., 2024), and Numina-Math (Li et al., 2024c). We trained the model using Llama3-8B and observed how its performance scaled with increasing data size. The results are presented in Figure 1. For the in-domain evaluation (MATH), our method demonstrates high data efficiency, achieving superior results with the same amount of data. In out-of-domain evaluations (Olympiad Bench), it also shows strong scalability, continuing to improve even as other datasets reach their limits. A limited question set leads to constrained improvements in model performance, as demonstrated by the results of DART-Math, which relies on a small number of questions and generates numerous correct answers through rejection sampling. Limited questions face a scalability ceiling, as the lack of diversity in the question set restricts further performance growth. Our results further demonstrate that diverse questions support sustained performance growth, emphasizing the need for broader and more varied question generation.\"\n    },\n    {\n      \"title\": \"3.3 ABLATION STUDY\",\n      \"content\": \"Ablation on each sub-method To validate the effectiveness of each of our sub-methods, including QFT, QPO, and reward filtering, we conducted an ablation study. We evaluated the quality of the questions generated by the models across three dimensions: solvability, difficulty, and performance in instruction tuning. To assess the model's solvability and difficulty, we used GPT-4o-mini as the\"\n    },\n    {\n      \"title\": \"3.4 COST ANALYSIS\",\n      \"content\": \"The data synthesis process was conducted on a server with 8 A100-40G-PCIe GPUs. We summarize our overall costs in Table 4. Generating 1 million data samples required only 522.9 GPU hours (approximately 2.7 days on an 8-GPU server), with an estimated cost of $680.8 for cloud server rental. This is only about 10% of the cost of generating the same data using GPT-4o. This demonstrates that our data generation method is significantly more cost-effective.\"\n    },\n    {\n      \"title\": \"5 CONCLUSION\",\n      \"content\": \"In this work, we propose ScaleQuest, a novel data synthesis framework that unlocks the ability of open-source smaller models to independently generate large-scale, high-quality reasoning data from scratch, at a low cost. By training the problem-solving models on a small subset of questions, we effectively activate their question-generation capabilities. We also introduce a response enhancement method. With these techniques, we successfully developed a fully synthetic math reasoning dataset consisting of 1 million question-answer pairs. Using this dataset, we fine-tuned the model and achieved remarkable improvements, with gains ranging from 29.2% to 46.4% compared to the base model. The fine-tuned 7B model, Qwen2-Math-7B-ScaleQuest, outperforms all competitors in the 7B-70B range and even surpasses proprietary models like GPT-4-Turbo and Claude-3.5-Sonnet.\nDue to time and cost constraints, there are several areas where our approach can be further optimized. For instance, leveraging more powerful, larger problem-solving models like Qwen2.5-Math-72B-Instruct (Yang et al., 2024b) for question and response generation, using advanced models such as GPT-4o for constructing preference data for Question Preference Optimization, and further scaling up the generation of synthetic data. Each stage of our process has significant room for improvement. In this paper, we have demonstrated the potential of this framework, laying the groundwork for future enhancements.\nFurthermore, despite the progress made in this work, there are still several limitations that need to be addressed. In our future research, we will concentrate on the following areas:\n\u2022 Large-scale and diverse high-quality data: This work chooses mathematical reasoning as a case study to demonstrate the effectiveness of our method. In the future, we will focus on broader and more complex tasks such as science and competitive programming. Additionally, future research will aim to continuously scale data synthesis to explore the scaling laws for synthetic data and seek a more efficient approach to scaling data generation.\n\u2022 Self-improvement capability: Our experiments demonstrate the model's self-improvement capability, meaning that it can generate data of higher quality than its original training set. This is evident as Qwen2-Math-7B-ScaleQuest slightly outperforms Qwen2-Math-7B-Instruct. To fur-\"\n    },\n    {\n      \"title\": \"A ADDITIONAL DATA STATISTICS\",\n      \"content\": \"Filtering process The entire data generation process is illustrated in Figure 6. After using the two question generators to produce 2 million questions from scratch, we performed a filtering process, including language filtering, solvability checks, and difficulty sampling. These steps filtered out 20.1%, 19.4%, and 9.2% of the samples, respectively, resulting in a final question set of 1 million questions. In the subsequent response generation process, we filtered out responses without answers by checking for key phrases such as \u201cThe answer is": "r \u201c\\boxed{}\u201d. This step eliminated a negligible portion of the samples, as most of the filtered questions were solvable and did not pose any confusion for the response generation model.\nDataset Coverage We analyze the dataset coverage through two aspects: (1) Problem Topic Coverage, such as algebra and geometry. Following Huang et al. (2024a), we use GPT-4o to categorize the topics of the given questions, with prompt illustrated in Figure 13. Figure 7 presents the results. We found that the topics covered the major areas of mathematics, such as arithmetic, algebra, geometry, and others. (2) Embedding space analysis. Following Zhao et al. (2024) and Xu et al. (2024), we first compute the input embeddings of the questions and then project them into a two-dimensional space using t-SNE (Van der Maaten & Hinton, 2008). We included only real-world datasets, such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and NuminaMath (Li et al., 2024c) (which contains a small portion of synthetic questions). As shown in Figure 8, our synthetic data closely resembles the real-world questions."}, {"title": "Safety Analysis", "content": "We used Llama3-8B-Guard (Inan et al., 2023) as a discriminator model to detect any unsafe elements in the data. After sampling 10K instances from the 1 million samples, we found that only 0.1% were flagged as unsafe."}, {"title": "Generated Examples", "content": "We sampled several generated examples from our datasets, as shown in Figure 16, 17 and 18. The generated math problems are of high quality, driving effective learning."}, {"title": "B PROMPTS", "content": "Prompts for Problem Solvability Optimization\nPlease act as a professional math teacher.\nYour goal is to create high quality math word problems to help students learn math.\nYou will be given a math question. Please optimize the Given Question and follow the instructions.\nTo achieve the goal, please follow the steps:\n# Please check that the given question is a math question and write detailed solution to the Given Question.\n# Based on the problem-solving process, double check the question is solvable.\n# If you feel that the given question is not a meaningful math question, rewrite one that makes sense to you. Otherwise, modify the Given question according to your checking comment to ensure it is solvable and of high quality.\n# If the question can be solved with just a few simple thinking processes, you can rewrite it to explicitly request multiple-step reasoning.\nYou have five principles to do this:\n# Ensure the optimized question only asks for one thing, be reasonable and solvable, be based on the Given Question (if possible), and can be answered with only a number (float or integer). For example, DO NOT ask, 'what is the amount of A, B and C?'.\n# Ensure the optimized question is in line with common sense of life. For example, the amount someone has or pays must be a positive number, and the number of people must be an integer.\n# Ensure your student can answer the optimized question without the given question. If you want to use some numbers, conditions or background in the given question, please restate them to ensure no information is omitted in your optimized question.\n# Please DO NOT include solution in your question.\nGiven Question: problem\nYour output should be in the following format:\nCREATED QUESTION: [your created question]\nVERIFICATION AND MODIFICATION: [solve the question step-by-step and modify it to follow all principles]\nFINAL QUESTION: [your final created question]"}, {"title": "Prompts for Problem Difficulty Optimization", "content": "You are an Math Problem Rewriter that rewrites the given #Problem# into a more complex version. Please follow the steps below to rewrite the given \"#Problem#\" into a more complex version.\nStep 1: Please read the \"#Problem#\" carefully and list all the possible methods to make this problem more complex (to make it a bit harder for well-known AI assistants such as ChatGPT and GPT4 to handle). Note that the problem itself might be erroneous, and you need to first correct the errors within it.\nStep 2: Please create a comprehensive plan based on the #Methods List# generated in Step 1 to make the #Problem# more complex. The plan should include several methods from the #Methods List#.\nStep 3: Please execute the plan step by step and provide the #Rewritten Problem#. #Rewritten Problem# can only add 10 to 20 words into the \"#Problem#\".\nStep 4: Please carefully review the #Rewritten Problem# and identify any unreasonable parts. Ensure that the #Rewritten Problem# is only a more complex version of the #Problem#. Just provide the #Finally Rewritten Problem# without any explanation and step-by-step reasoning guidance.\nPlease reply strictly in the following format:\nStep 1 #Methods List#:\nStep 2 #Plan#:\nStep 3 #Rewritten Problem#:\nStep 4 #Finally Rewritten Problem#:\n#Problem#: Problem"}, {"title": "Prompts for Problem Solvability Check", "content": "Please act as a professional math teacher.\nYour goal is to determine if the given problem is a valuable math problem. You need to consider two aspects:\n1. The given problem is a math problem.\n2. The given math problem can be solved based on the conditions provided in the problem (You can first try to solve it and then judge its solvability).\nPlease reason step by step and conclude with either 'Yes' or 'No'.\nGiven Problem: Problem"}, {"title": "Prompts for Difficulty Classification", "content": "# Instruction\nYou first need to identify the given user intent and then label the difficulty level of the user query based on the content of the user query.\n## User Query\nInput\n## Output Format\nGiven the user query, in your output, you first need to identify the user intent and the knowledge needed to solve the task in the user query.\nThen, rate the difficulty level of the user query as very easy, easy, medium, hard, or very hard.\nNow, please output the user intent and difficulty level below in a json format by filling in the placeholders in []:\n{{\n\"intent\": \"The user wants to [....]\",\n\"knowledge\": \"To solve this problem, the models need to know [....]\",\n\"difficulty\": \"[very easy/easy/medium/hard/very hard]\"\n}}"}, {"title": "Prompts for Topic Classification", "content": "As a mathematics education specialist, please analyze the topics of the provided question and its answer. Specific requirements are as follows:\n1. You should identify and categorize the main mathematical topics involved in the problem. If knowledge from non-mathematical fields is used, it is classified into Others - xxx, such as Others - Problem Context.\n2. You should put your final answer between <TOPIC> and </TOPIC>.\nQuestion: Compute cos 330\u00b0.\nAnswer: We know that 330\u00b0 = 360\u00b0 - 30\u00b0.\nSince cos(360\u00b0 \u2013 0) = cos 0 for all angles 0, we have cos 330\u00b0 = cos 30\u00b0.\nSince cos 30\u00b0 = \u221a3, we can conclude that cos 330\u00b0 =\u221a3 .\n2\n2\nAnalysis: <TOPIC>Trigonometry - Cosine Function</TOPIC>\nQuestion: Question\nAnswer: Answer\nAnalysis:"}, {"title": "Examples for Solvability Optimization", "content": "Problems 1 (Before Optimization):\nThere are 10 survivors in an emergency room. Each survivor is either a child", "Optimization)": "nThere are 1"}]}