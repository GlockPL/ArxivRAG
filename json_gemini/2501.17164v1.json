{"title": "Split Knowledge Distillation for Large Models in IoT: Architecture, Challenges, and Solutions", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Qiaohua Lin", "Yaping Sun", "Hui Wang"], "abstract": "Large models (LMs) have immense potential in Internet of Things (IoT) systems, enabling applications such as intelligent voice assistants, predictive maintenance, and healthcare monitoring. However, training LMs on edge servers raises data privacy concerns, while deploying them directly on IoT devices is constrained by limited computational and memory resources. We analyze the key challenges of training LMs in IoT systems, including energy constraints, latency requirements, and device heterogeneity, and propose potential solutions such as dynamic resource management, adaptive model partitioning, and clustered collaborative training. Furthermore, we propose a split knowledge distillation framework to efficiently distill LMs into smaller, deployable versions for IoT devices while ensuring raw data remains local. This framework integrates knowledge distillation and split learning to minimize energy consumption and meet low model training delay requirements. A case study is presented to evaluate the feasibility and performance of the proposed framework.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of large models (LMs), particularly large language models, has reshaped the landscape of artificial intelligence (AI), driving innovation across numerous domains. These revolutionary models, such as GPT, LLaMA, and their successors, have demonstrated exceptional capabilities in natural language understanding, human-like text generation, and decision-making [1]. Beyond their current success, LMs are increasingly recognized for their potential to empower the Internet of Things (IoT) system, where they can transform IoT devices into intelligent agents capable of understanding context, predicting user needs, and optimizing system performance. By leveraging their advanced inference and communication capabilities, LMs can facilitate seamless interactions and efficient operations within complex and interconnected IoT networks.\nDespite their transformative capabilities, LMs are often too resource-intensive for deployment on IoT devices, which are constrained by limited computational power, memory, and energy resources [2]. For example, practical industry LMs like GPT-3 175B require over 350 GB to store their parameters, which needs at least five NVIDIA H100 GPUs to accommodate this model [3]. Therefore, the mainstream LMs are highly difficult to be deployed on IoT devices such as the Jetson platform, which have much more limited memory and computational resources. To address this, knowledge distillation provides an effective solution by training a smaller student model using the outputs of a larger teacher model. This process transfers the teacher's knowledge while significantly reducing the model size and complexity. For example, student models such as MT-CoT, UniNER 7B, and ILD retain much of the teacher LM's performance while being lightweight enough to be deployed on IoT devices [4]. By doing this, advanced AI functionalities can be achieved in resource-constrained environments, bridging the gap between LM capabilities and IoT limitations.\nHowever, many private institutions, companies, or mobile carriers cannot use the dataset collected by users for distilling LMs due to data privacy and security concerns. To address this challenge, split learning has emerged as a promising solution that ensures data privacy while reducing the computational burden on IoT devices. In split learning, the initial layers of the model are executed locally on IoT devices, and only the intermediate activations stripped of raw data information are transmitted to the edge server, which processes the remaining layers of the model [5]. Although its advantages, leveraging split learning to distill LMs into small models for IoT applications still faces significant challenges. First, the energy consumption associated with transmitting intermediate activations and running computationally intensive layers on the edge server is substantial, particularly for resource-constrained IoT environments. Second, the efficiency of model training can be hindered by communication latency and the frequent synchronization required between devices and servers, which may slow down the overall training process and impact real-time applications. Addressing these issues is crucial to unlock the full potential of split learning for LMs in IoT systems.\nIn this article, we analyze the key challenges of deploying LMs in IoT systems, including energy constraints, latency requirements, and device heterogeneity, and propose potential solutions such as dynamic resource management, adaptive model partitioning, and clustered collaborative training. To address these challenges, we propose a split knowledge distillation framework that integrates the knowledge distillation and split learning to efficiently distill LMs into smaller, deployable models for IoT devices, while ensuring that raw data remains local to preserve privacy. This framework is designed to minimize energy consumption and meet low model training"}, {"title": "II. BACKGROUND", "content": "As a significant advancement in LMs, large language models are built on the transformer architecture, which uses a self-attention mechanism to efficiently capture long-range dependencies and understand world context [6]. This design allows LMs to process large datasets with parallel processing, setting them apart from earlier sequential models like recurrent neural networks (RNNs). This evolution has enabled breakthroughs in natural language processing tasks which require deep contextual understanding and scalability. Over time, driven by advances in hardware, software, and optimization techniques, LMs have evolved from models with millions of parameters to those with billions or even trillions of parameters, such as GPT-4 and LLaMA-3.\nLarge amounts of data are generated from heterogeneous IoT devices, which is beneficial to users, businesses, and industries. However, much data could be unstructured and complex, presenting challenges for effective data processing. In this context, LMs have the potential to make IoT data more meaningful, mainly through their capabilities in natural language processing and understanding. When integrated into IoT systems, LMs can power applications in smart cities, robotic control, and healthcare, enabling intelligent decision-making, automation, and enhanced user interaction across diverse domains, as shown in Fig. 1. For example, in a Tactile IoT system used for remote robotic surgery, LMs can analyze sensor data, integrating medical expertise to assist in tasks like detecting tissue resistance during surgery, improving accuracy and safety [7]. Conversely, LMs benefit from IoT by leveraging real-time, multi-modal, and rich data to enhance their understanding of advanced meaning and context-based decisions. The heterogeneous IoT devices provide continuously updated data, enabling LMs to adapt to changing environments and deliver more accurate and up-to-date responses.\nFor resource-constrained environments like IoT, deploying LMs presents significant challenges due to their size and computational demands. To address these issues, various optimization techniques are employed to reduce the model size and computational costs. Methods like quantization, which reduces the bit width of data, and pruning, which removes unnecessary weights, are commonly used to make LMs more efficient [8]. Additionally, parameter-efficient fine-tuning methods, such as low-rank adaptation, focus on fine-tuning only critical parameters, enabling more efficient training with significantly reduced computational requirements [9]. Furthermore, knowledge distillation transfers the knowledge from an LM to a smaller one, which reduces the model's size, thereby making LMs more suitable for IoT devices."}, {"title": "B. Knowledge Distillation", "content": "To address the challenges of deploying LMs in constrained IoT environments, knowledge distillation is widely adopted. It plays a critical role in compressing advanced LMs like GPT-3 or LLaMA into compact versions without significant loss in capabilities. For example, as shown in Table I, the GPT-3 model (700 GB) can be distilled into a student model like MT-COT (12 GB) with a 58\u00d7 compression rate while retaining 98% of the teacher LM's performance. The distillation process begins by guiding the teacher LM to focus on a specific target domain or skill, achieved by employing structured prompts, precise instructions, or domain-specific datasets. This ensures that the teacher LM produces outputs demonstrating expertise in general cognitive skills (e.g., context following) or specialized fields (e.g., law and science).\nDuring the training phase, the student model learns to approximate the behavior of the teacher LM by minimizing a loss function that combines two components: the Kullback-Leibler (KL) divergence between the teacher LM's soft labels and the student model's predictions, and the cross-entropy loss between the true labels and the student's predictions. The soft labels provide richer information by encoding the teacher LM's probability distribution over all possible outputs, helping the student generalize effectively. By optimizing this combined loss, the student model can mimic the teacher LM's decision-making while operating with a smaller architecture. This enables the creation of lightweight models that maintain high accuracy and efficiency, suitable for IoT devices with limited computational resources and energy budgets."}, {"title": "C. Split Learning", "content": "Split learning has garnered significant attention as an effective solution for training AI models while preserving data privacy in resource-constrained environments, particularly in edge networks and IoT systems. Existing works on split learning have demonstrated that the energy consumption in the model training can be effectively reduced through strategies such as cut layer selection, resource management, and architectural design. Kang et al. [10] analyzed the per-layer execution time and energy consumption in different AI models, and then the optimal cut layer can be determined for the best latency or energy consumption. Ayad et al. [11] proposed a modified split learning approach to achieve the electrocardiography classification while reducing the communication overhead and computation workload.\nSplit learning has shown immense potential in various application domains, including autonomous driving, robotic control, and healthcare. In autonomous driving, split learning enables the efficient training and fine-tuning of AI models for tasks such as sensor fusion, object detection, and decision-making, all while minimizing energy consumption and ensuring real-time responsiveness [12]. The framework facilitates adaptive model training for motion planning and environment interaction, addressing the energy constraints of edge devices commonly deployed in robotics. In healthcare, split learning has been applied to privacy-sensitive tasks such as medical image analysis and remote patient monitoring, where it reduces the computational burden on wearable or IoT devices while maintaining stringent"}, {"title": "III. CHALLENGES AND POTENTIAL SOLUTIONS", "content": ""}, {"title": "A. Challenges", "content": "1) Limited Energy Consumption: IoT devices often operate under strict energy constraints, which poses a significant challenge when deploying split knowledge distillation frameworks. Training and distilling LMs require frequent communication between IoT devices and edge servers, as well as substantial local and server-side computations. This energy-intensive process can quickly drain device batteries and increase operational costs, particularly in large-scale IoT networks. Moreover, high energy consumption limits the feasibility of real-time AI applications, such as robotic controlling or healthcare monitoring, where devices must operate continuously over extended periods without recharging.\nThe challenge is further compounded by the diverse energy capacities of IoT devices, leading to imbalanced workloads across the system. Devices with limited power may experience frequent interruptions or degraded performance, disrupting collaborative model training or inference. Addressing energy consumption is thus a critical requirement for achieving sustainable and reliable IoT deployments.\n2) Low Model Training Delay Requirement: The second challenge in IoT applications is meeting the low model training delay requirement, as real-time processing is often crucial. Tasks such as smart city management, industrial automation, and healthcare monitoring demand quick decision-making, where delays in model training could significantly impact the system's responsiveness and performance. Model training, particularly in the context of LMs, can introduce significant delays due to the complexity of AI models.\nThe training delay issue is exacerbated in split learning systems, where the training process is distributed between the edge server and IoT devices. This distribution introduces additional communication overhead and synchronization delays, further increasing the time required to complete each training round. Meeting low latency requirements while ensuring model accuracy is challenging, as optimizing the system for both speed and performance demands careful coordination of resources, network communication, and computation.\n3) Heterogeneous IoT Devices: The third significant challenge in IoT systems is heterogeneous IoT devices that operate within these networks. These devices vary greatly in computational power, memory, and energy capabilities, which can cause imbalances during model training or inference. For example, low-power sensors or devices with limited processing capacity may need help to participate in complex model training tasks, resulting in inefficiencies or delays. This heterogeneity makes it difficult to ensure that all devices can effectively collaborate in the model training process, particularly when deploying LMs that require substantial computational resources."}, {"title": "B. Potential Solutions", "content": "1) Dynamic Resource Management: Dynamic resource management offers a robust solution to minimize energy consumption while maintaining the IoT system's performance. This approach ensures efficient resource utilization across IoT devices and edge servers by dynamically adapting computational and communication workloads based on energy availability and task requirements.\nOne key strategy is dynamic GPU frequency scaling on edge servers. The system can balance computational performance and energy usage by adjusting GPU frequencies in real time based on workload demands. For example, lower frequencies can be used during idle periods, significantly reducing energy consumption without impacting latency-sensitive operations.\nAnother critical technique is energy-aware task scheduling, which allocates tasks to IoT devices based on their current energy levels and capabilities. High-energy tasks can be offloaded to devices with greater energy reserves or processed on the edge server, while low-energy devices handle simpler operations. Furthermore, compression of intermediate activations reduces the size of data transmitted between devices and servers, lowering communication energy costs. Together, these strategies optimize energy efficiency, making split knowledge distillation viable for resource-constrained IoT environments.\n2) Adaptive Model Partition: To address the challenge of low training delay, a solution is to optimize the division of the model between IoT devices and the edge server. By dynamically adjusting the partition point (cut layer) of the model based on real-time network conditions, computational resources, and task requirements, this approach ensures minimal delay in both training and inference.\nIn this method, the device-side portion of the model handles the less computationally intensive layers, while the server-side portion processes the more complex parts of the model. The adaptive selection of the cut layer ensures that IoT devices with limited computational resources are not burdened with excessive processing, while the edge server can handle more complex computations, reducing latency. Moreover, this partitioning approach can adjust in response to changes in network bandwidth, device workload, and energy availability, enabling efficient use of resources and minimizing delays in training.\nBy optimizing the distribution of training tasks across the system, adaptive model partitioning enables faster model updates and real-time inference, meeting the stringent latency requirements of IoT applications without compromising the accuracy or efficiency of the distillation process.\n3) Clustered Collaborative Training: To address the challenge of heterogeneous IoT devices, clustered collaborative training offers a practical solution by grouping IoT devices with similar resource capabilities into clusters. Within each cluster, devices can focus on model training tasks that match their computational power, allowing for more efficient use of resources. For example, high-performance devices in a cluster can handle more complex model layers, while lower-resource devices can focus on less demanding tasks, such as data preprocessing or training earlier model layers.\nThis approach ensures that all devices can effectively participate in training and reduces the communication overhead between devices and the edge server by limiting data exchange to relevant clusters. By optimizing task distribution and communication, clustered collaborative training enables efficient model training across heterogeneous IoT devices, ensuring that resource-constrained devices are not overburdened and that high-performance devices are fully utilized. This balanced approach helps maintain scalability and performance in IoT networks, allowing them to handle LMs effectively despite their diverse device ecosystem."}, {"title": "IV. PROPOSED SPLIT KNOWLEDGE DISTILLATION FRAMEWORK", "content": ""}, {"title": "A. System Model", "content": "Deploying LMs on IoT devices is challenging due to their limited computational and memory resources. In addition, to protect data privacy, raw data generated on IoT devices cannot be directly transmitted to edge servers for model training. Hence, we propose a split knowledge distillation framework to distill LMs into a smaller model, deployable versions on IoT devices while ensuring that raw data remains stored locally on the devices. In this framework, an edge server collaborates with IoT devices to jointly perform model training and distillation, and then these devices implement the distilled model for real-time inference. The edge server and IoT devices are illustrated as follows.\n\u2022 Edge server: The edge server is deployed on an access point, maintaining the complete teacher LM and student model to ensure comprehensive access to global model parameters. The access point is responsible for gathering network information, such as device computing capabilities and channel conditions, which supports the server in making decisions about the cut layer selection and energy management.\n\u2022 IoT devices: IoT devices operate within the signal coverage area of the access point and serve as lightweight endpoints in the framework. These devices have significantly lower computing power and storage capacity. Hence, each device hosts only a partial segment of the teacher LM while deploying the complete student model.\nThe teacher LM is partitioned into the embedding module and the remaining layers. The remaining layers are most computationally expensive and consist of subsequent transformer blocks and a task module. The edge server stores the entire teacher LM, while each IoT device only stores the embedding module. In the training process, each IoT device executes the embedding module of the teacher LM, while the edge server"}, {"title": "B. System Workflow", "content": "The edge server executes the model training with each IoT device in a line manner, as shown in Fig. 2. Several training rounds are performed between the server and these devices until a satisfactory performance of the student model is achieved. In a training round, the server may conduct several local epochs of model training with a device, where the number of local epochs depends on these factors such as the device's dataset size, computational capacity, and wireless channel conditions. After completing the local epochs, the device uploads the updated model parameters to the server. The details of a training round are as follows.\nThe edge server dynamically selects an IoT device for collaborative training based on current wireless conditions and device resources. It determines the optimal cut layer between the device-side and server-side portions of the student model to balance energy, computation, and communication efficiency. During training, the server transmits the index of the cut layer to the selected IoT device. The device processes its local data through the student model's initial layers and the teacher LM's embedding module, generating intermediate outputs, referred to as smashed data. These outputs are then transmitted to the edge server for further processing, ensuring raw data remains local to the device to preserve privacy.\nThe edge server completes the forward propagation using the remaining layers of the student model and the teacher LM. It computes a combined loss function consisting of KL divergence, which aligns the student model's predictions with the teacher LM's soft outputs, and cross-entropy loss, which ensures alignment with ground-truth labels. This joint loss function guides the training process to achieve both accuracy and compactness.\nOnce the loss gradients are computed, the edge server transmits them back to the IoT device, which performs local backpropagation to update the parameters of the device-side layers. This iterative process allows the device to refine its local model without requiring direct access to the full teacher LM or server-side data, significantly reducing communication overhead. The server and devices repeat these training rounds collaboratively across all participating devices until the student model performs satisfactorily. This process ensures the distilled model is lightweight, accurate, and optimized for deployment in resource-constrained IoT environments."}, {"title": "V. CASE STUDY", "content": ""}, {"title": "A. Considered Scenario", "content": "To evaluate the performance of the proposed split knowledge distillation framework, we consider a realistic scenario where an edge server collaborates with 10 IoT-enabled vehicles to perform knowledge distillation. The task involves distilling a large LLAMA 3.2 8B model into a small LLaMA 3.2 1B model. The teacher LM comprises 64 transformer blocks, while the student model contains 12 transformer blocks. This distilled model can support autonomous driving systems, offering functionalities such as text generation, knowledge graph construction, and real-time decision-making. The experimental setup assumes a single base station with an edge server, providing seamless connectivity to the 10 vehicles. These vehicles are equipped with heterogeneous IoT devices, showcasing varying computational capabilities. The computational resources of the edge server and the IoT devices are detailed in Table II. The edge server is a high-performance computer equipped with an Nvidia RTX 4090 GPU, capable of handling the intensive computational requirements of the large LLaMA model. On the other hand, the 10 vehicles are equipped with a diverse range of Jetson devices, reflecting the resource heterogeneity commonly found in real-world IoT environments.\nAs the vehicles operate within the base station's coverage area, they initiate model training tasks at the beginning of each trial. During the experiment, the vehicles follow a predefined trajectory at a constant speed of 30 km/h, simulating a typical urban driving scenario. The edge server and vehicles communicate over a 5G mmWave channel. The link bitrate is dynamically determined using the 5G New Radio (NR) CQI-to-MCS mapping table [14], providing a realistic approximation of mobile cellular network conditions. To further emulate real-world dynamics, the framework is tested under three distinct channel quality conditions: Good, Normal, and Poor, which correspond to noise spectral densities of -166 dBm/Hz, -163 dBm/Hz, and -160 dBm/Hz, respectively [15]. These conditions emulate the typical variations in signal quality encountered in real-world vehicular networks, allowing for a comprehensive assessment of the framework's robustness and adaptability to channel fluctuations."}, {"title": "B. Simulation Results", "content": "We first evaluate the model training performance of the proposed approach in terms of training delay and energy consumption. We compared the proposed approach with two benchmark methods: (i) Server-only, where devices train the"}, {"title": "VI. CONCLUSION", "content": "We have explored the challenges and potential solutions for training LMs in IoT systems, emphasizing energy efficiency, low training latency, and heterogeneous device capabilities. To address these challenges, we have proposed a split knowledge distillation framework that integrates knowledge distillation and split learning. This framework efficiently distills LMs into smaller, deployable versions suitable for IoT devices while ensuring that raw data remains local to preserve privacy. Through a case study, we have validated the framework's feasibility and effectiveness, demonstrating its potential for enabling advanced AI applications in resource-constrained IoT environments."}]}