{"title": "Music2Latent2: Audio Compression with Summary Embeddings and Autoregressive Decoding", "authors": ["Marco Pasini", "Stefan Lattner", "Gy\u00f6rgy Fazekas"], "abstract": "Efficiently compressing high-dimensional audio signals into a compact and informative latent space is crucial for various tasks, including generative modeling and music information retrieval (MIR). Existing audio autoencoders, however, often struggle to achieve high compression ratios while preserving audio fidelity and facilitating efficient downstream applications. We introduce Music2Latent2, a novel audio autoencoder that addresses these limitations by leveraging consistency models and a novel approach to representation learning based on unordered latent embeddings, which we call summary embeddings. Unlike conventional methods that encode local audio features into ordered sequences, Music2Latent2 compresses audio signals into sets of summary embeddings, where each embedding can capture distinct global features of the input sample. This enables to achieve higher reconstruction quality at the same compression ratio. To handle arbitrary audio lengths, Music2Latent2 employs an autoregressive consistency model trained on two consecutive audio chunks with causal masking, ensuring coherent reconstruction across segment boundaries. Additionally, we propose a novel two-step decoding procedure that leverages the denoising capabilities of consistency models to further refine the generated audio at no additional cost. Our experiments demonstrate that Music2Latent2 outperforms existing continuous audio autoencoders regarding audio quality and performance on downstream tasks. Music2Latent2 paves the way for new possibilities in audio compression.", "sections": [{"title": "I. INTRODUCTION", "content": "Representing high-dimensional audio data in a compact and informative latent space is valuable for various tasks, spanning generative modeling, music information retrieval (MIR), and audio compression. While recent audio autoencoders have made significant strides in learning such representations, they still struggle to achieve high compression ratios while preserving audio fidelity and enabling downstream applications. Existing approaches typically encode audio into ordered sequences of discrete tokens or continuous embeddings, where each element describes a short audio segment. However, these methods inherently limit compression efficiency, as global audio features, such as timbre or tempo in the context of music samples, are redundantly encoded across multiple tokens or embeddings.\nThis work introduces Music2Latent2, a novel autoregressive audio autoencoder that overcomes these limitations by using unordered embeddings, which we call summary embeddings: each summary embedding can capture distinct global features of a large chunk of the audio signal. This is achieved by using learned embeddings and transformer blocks, and it allows for a more efficient allocation of information within the latent space, leading to higher reconstruction quality without compromising the compression ratio. A consistency model that decodes the audio from latent embeddings is trained using causal masking in the self-attention layers, enabling it to attend to past audio segments during decoding, thus ensuring coherent reconstruction and avoiding boundary artifacts. Furthermore, Music2Latent2 uses a novel two-step decoding procedure that exploits autoregressive decoding to achieve higher reconstruction quality without increasing computational cost. Our experiments show that Music2Latent2 significantly outperforms continuous audio autoencoder baselines on audio quality of reconstructions at the same and at double the compression ratio, while achieving competitive results on MIR downstream tasks."}, {"title": "II. RELATED WORK", "content": "1) Audio Autoencoders: The autoencoder used in Musika [1] and the autoencoder proposed in [2] reconstruct the magnitude and phase components of a spectrogram, enabling fast inference but requiring a two-stage training process with an adversarial objective. Stable Audio and Stable Audio 2 [3]-[5] make use of audio autoencoders to produce latents for training generative models, but these autoencoders still rely on adversarial training and a careful balance between multiple loss terms. Mo\u00fbsai [6] introduces a diffusion autoencoder for learning an invertible audio representation, but while only using a single loss for training, inference requires multiple sampling steps. Music2Latent [7] is a consistency autoencoder that is both trained with a single loss term and decodes samples in a single step, and it outputs an ordered sequence of latents. SoundStream [8], EnCodec [9], and Descript Audio Codec (DAC) [10] encode samples to discrete codes using Residual Vector Quantization (RVQ). These models can achieve high fidelity reconstructions and are well-suited for training autoregressive models [11]\u2013[13]. They also operate at lower time compression ratios compared to the continuous counterparts, and are thus not directly comparable to our work. The idea of using unordered embeddings to maximise the compression ratio has been successfully used in the vision domain for discrete autoencoders [14].\n2) Consistency Models: Consistency models [15], [16] have shown impressive results in image generation tasks [17], achieving high-fidelity generation with single-step sampling. The application of consistency models to audio generation remains relatively unexplored. CoMoSpeech [18] explores consistency distillation for speech synthesis, but relies on a pre-trained diffusion model. Music2Latent [7] is the first autoencoder to successfully apply consistency models for audio compression and representation learning."}, {"title": "III. BACKGROUND", "content": "Consistency models learn a mapping from any point on a diffusion trajectory to its origin, effectively reversing the diffusion process. The ordinary differential equation (ODE) of the probability flow is introduced by [19] as: $\\frac{dx}{d\\sigma} = -\\sigma \\nabla_x \\log p_{\\sigma}(x)$ with $\\sigma\\in [\\sigma_{min}, \\sigma_{max}]$ and where $p_{\\sigma}(x)$ is the perturbed data distribution after adding Gaussian noise with standard deviation $\\sigma$ to the original data distribution $p_{data}(x)$. $\\nabla_x \\log p_{\\sigma}(x)$ is the score function [20]\u2013[22].\nThe ODE establishes a bijective mapping between a noisy sample $x_{\\sigma} \\sim p_{\\sigma}(x)$ and $x_{\\sigma_{min}} \\sim p_{\\sigma_{min}}(x) \\approx x \\sim p_{data}(x)$. A consistency model $f_{\\theta}(x_{\\sigma},\\sigma)$ is trained to approximate the consistency function $f(x_{\\sigma}, \\sigma) \\rightarrow x_{\\sigma_{min}}$, and is parameterised as: $f_{\\theta}(x_{\\sigma}, \\sigma) = C_{skip}(\\sigma)x_{\\sigma} + C_{out}(\\sigma) F_{\\theta}(x_{\\sigma}, \\sigma)$, where $F_{\\theta}(x_{\\sigma}, \\sigma)$ is a neural network, and $C_{skip}(\\sigma)$ and $C_{out}(\\sigma)$ are differentiable functions that satisfy the boundary condition $f_{\\theta}(x_{\\sigma_{min}},\\sigma_{min}) = x_{\\sigma_{min}}$. Consistency training allows to train consistency models without a teacher diffusion model. It involves discretising the probability flow ODE using a sequence of noise levels $\\sigma_{min} = \\sigma_1 < \\sigma_2 < ... < \\sigma_N = \\sigma_{max}$ and minimising the loss $L_{CT} = E [\\lambda(\\sigma_i,\\sigma_{i+1})d (f_{\\theta}(x_{\\sigma_{i+1}}, \\sigma_{i+1}), sg(f_{\\theta^-}(x_{\\sigma_i}, \\sigma_i))) ]$, where $d(x, y)$ is a distance metric, $\\lambda(\\sigma_i, \\sigma_{i+1})$ is a loss scaling factor, $f_{\\theta^-}$ is a stop-gradient version of $f_{\\theta}$. The consistency model $f_{\\theta}(x, \\sigma)$ generates a sample $x$ in one step from $z \\sim N(0, I)$ by computing $x = f_{\\theta}(\\sigma_{max} z, \\sigma_{max})$."}, {"title": "IV. MUSIC2LATENT2", "content": "1) Audio Representation: Similarly to [7], Music2Latent2 uses complex-valued STFT spectrograms as the input representation for audio signals [23], [24]. The 2D nature of spectrograms allows for the direct application of UNet [25] and DiT [26] architectures that have been successfully used in diffusion-based image generation. We also use the spectrogram amplitude transformation from [7], [27] to address the challenge of varying amplitude value distributions across frequencies. We treat the complex STFT spectrogram as a 2-channel representation, with each channel corresponding to the real and imaginary components, respectively.\n2) Architecture: The architecture, as shown in Fig. 1, is similar to Music2Latent and includes an encoder, a decoder, and a consistency model. Music2Latent has a convolutional architecture that allows the decoding of audio samples with different lengths than those used during training. In contrast, Music2Latent2 includes transformer blocks in the three modules, making the decoding of arbitrary-length audios challenging. This is due to the quadratic scaling of memory requirements of self-attention with increasing audio length and the difficulty of transformers generalising to sequence lengths different from those used during training. We thus propose to perform decoding via chunked autoregression, which allows us to use the same sequence length at both training and inference. All architecture components operate on independent chunks, except for the transformer blocks of the consistency model, which operate on two consecutive chunks with causal masking.\nThe encoder takes a spectrogram chunk as input and uses a convolutional patchifier to downsample it into lower time and frequency resolution patches. We then apply the technique proposed in TiTok [14], appending a set of K learnable latent embeddings to the flattened sequence of audio patches. This augmented sequence is then fed into a stack of transformer blocks, allowing the model to learn global relationships between audio features and the learnable latents., We then discard the audio embeddings, retaining only the K resulting summary embeddings, which can now contain global information about the input chunk. A tanh function is used to constrain the K $d_{lat}$-dimensional embeddings in the (-1, 1) range [1], [6], [7].\nThe decoder mirrors the architecture of the encoder, and it takes as input a set of K summary embeddings. In place of the audio embeddings, learnable \"mask\" embeddings are concatenated. This combined sequence is then processed by a stack of transformer blocks. The resulting audio embeddings are kept and fed into a convolutional de-patchifier, which gradually upsamples them. The only goal of the decoder is to feed intermediate features at different resolutions to the patchifier of the consistency model via cross-connections.\nThe consistency model uses a patchifier, transformer blocks and a de-patchifier, in order to produce an output with the same shape as the noisy spectrogram given as input. There are additive skip-connections between each resolution level of the patchifier and de-patchifier. There are also additive cross-connections from the decoder to each level of the patchifier to \"leak\" to the model information from the summary embeddings. As noted in [7], we find this design choice crucial to decode in a single step. Since at inference the input to the consistency model is an uninformative fully noisy spectrogram, the model greatly benefits from access to semantic features about which sample to reconstruct at early layers of the architecture. The transformer blocks accept audio embeddings from two consecutive chunks as input and perform chunked causal self-attention.\n3) Training Process: We train Music2Latent2 on two consecutive spectrogram chunks x of length spec_length. Each chunk is processed independently except for the transformer in the consistency model, where we concatenate the flattened sequence of both samples into a single sequence and use causal masking in the self-attention layers. This effectively teaches the model to condition the generation of the current audio segment on the preceding segment, resulting in a coherent reconstruction without boundary artifacts. We thus have:\n$x_{left, right} = CM_{\\theta_s} \\left( Dec(Enc(x_{left})), x_{left} + \\epsilon_{left} \\sigma_{left},\\ Dec(Enc(x_{right})), x_{right} + \\epsilon_{right} \\sigma_{right}\\right)$ where Enc, Dec, and CM are the Encoder, Decoder and Consistency Model, $\\sigma \\in [\\sigma_{min}, \\sigma_{max}]$ are noise levels and $\\epsilon \\sim N(0, I)$. During training, we sample independent noise levels $\\sigma_{left}$ and $\\sigma_{right}$. This allows us to dynamically change the noise level of each chunk at inference. We adopt the same EDM framework used by [16] regarding the Pseudo-Huber loss d(x, y) [28] and the loss weighting :\n$L = E \\left[ \\lambda(\\sigma_{left}+A_{\\sigma}, \\sigma_{right}+A_{\\sigma} \\right)d \\left(CM_{\\theta_{\\sigma_{left}+A_{\\sigma}}}, CM_{\\theta_{\\sigma_{right}+A_{\\sigma}}}\\right)\\right]$"}, {"title": "4) Inference:", "content": "To encode an audio signal of arbitrary length, we first compute its spectrogram with temporal dimension N spec_len (zero-padding if necessary). The spectrogram is then split into T chunks. Each chunk is processed independently by the encoder, producing T sets of K summary embeddings. Crucially, as the encoder operates on fixed-length chunks, the encoding process can be fully parallelised.\nTo decode we use an autoregressive approach, as shown in Fig. 2. First, the decoder produces cross-connections for each timestep t from the corresponding K summary embeddings. The first chunk $x_{2t=0}$ is decoded independently in a single step by the consistency model, conditioned on the cross-connections. For $x_{2t>0}$, the previously decoded $x_{2t-1}$ is corrupted with Gaussian noise at a controlled noise level $\\sigma_{cond}$. Thereby, when decoding $x_{2t}, x_{2t-1}$ is decoded again, in the same model evaluation. With consistency models, generation quality often improves when sampling with more than a single step [15], [16], and we exploit this at no additional computational cost. By re-introducing noise in $x_{2t-1}$, we can also avoid the error accumulation characteristic of autoregressive models, especially if trained on continuous data [29]. The added noise introduces a degree of uncertainty into the past audio segment which results in the model not copying over the previously committed errors [30].\n5) Implementation Details: The patchifiers and de-patchifiers are implemented using the same convolutional blocks as in Music2Latent [21]. We use sinusoidal embeddings with 256 channels [31] to represent the noise levels, taking $\\log(\\sigma)$ as input. We condition all consistency model layers on the noise level using AdaLN [26]. All skip and cross-connections across the model are additive. For all patchifiers we use 5 resolution levels, adopting [3, 3, 3, 4, 5, 1] layers per level and [64, 128, 256, 256, 256, 256] channels per level. The architecture of the de-patchifiers is mirrored. For each of the three modules we use 16 pre-LN transformer blocks with $d_{dim} = 256, n_{heads} = 4, mlp_{mult} = 4$. The resulting model has ~ 100 million parameters. The remaining hyperparameters regarding the EDM framework, Pseudo-Huber loss function, consistency step schedule and STFT spectrogram calculation/rescaling are the ones used in Music2Latent [7]. We train the model on waveforms of 67,072 samples, whose STFT spectrograms are then split in half along the time axis so each chunk has $s p e c_{-} l e n g t h=64$. We choose $d_{lat} = 64$ and K = 8, and the model thus produces summary embeddings of 44.1 kHz audio at a sampling rate of ~ 11 Hz, with a time and total compression ratio of 4096x and 64x, respectively. We train with batch_size = 16 for 1M iterations using RAdam [32] ($lro = 1e-4, \\beta_1 = 0.9, \\beta_2 = 0.999$). A cosine learning rate decay with lrfinal = 1e-6 and an Exponential Moving Average (EMA) of the parameters with momentum = 0.9999 are used. Training takes ~ 10 days on a single A100 GPU."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "1) Experimental Setting: We train Music2Latent2 on the same open datasets as Music2Latent: music from the MTG Jamendo dataset [33], and speech from the DNS Challenge 4 dataset [34], keeping the original sampling rates of 44.1 kHz and 48 kHz and sampling from each with equal weights. We start from and expand on the evaluation framework originally proposed in [7] and we thus use MusicCaps [13] as the evaluation dataset. We choose the following continuous autoencoder baselines: the autoencoder from Musika [1], the autoencoder used in [2] to train an accompaniment generation model (we"}, {"title": "VI. CONCLUSION", "content": "This work introduced Music2Latent2, a novel autoregressive audio autoencoder leveraging summary embeddings and consistency models for high-fidelity audio compression. By encoding audio into sets of summary embeddings, Music2Latent2 achieves higher reconstruction quality at the same compression ratio compared to conventional ordered embedding approaches. The autoregressive design enables processing of arbitrary-length audio signals while maintaining coherence and avoiding boundary artifacts. A two-step decoding process further improves the quality of reconstructions at no additional cost. Our experiments demonstrate Music2Latent2's superior performance over existing continuous audio autoencoders on both reconstruction audio quality metrics and downstream MIR tasks. Music2Latent2 opens novel possibilities for neural audio compression and efficient generative modeling."}]}