{"title": "PROCESS SUPERVISION-GUIDED POLICY OPTIMIZATION FOR CODE GENERATION", "authors": ["Ning Dai", "Zheng Wu", "Renjie Zheng", "Ziyun Wei", "Wenlei Shi", "Xing Jin", "Guanlin Liu", "Chen Dun", "Liang Huang", "Lin Yan"], "abstract": "Reinforcement learning (RL) with unit test feedback has enhanced large language models'\n(LLMs) code generation, but relies on sparse rewards provided only after complete code\nevaluation, limiting learning efficiency and incremental improvements. When generated\ncode fails all unit tests, no learning signal is received, hindering progress on complex tasks.\nTo address this, we propose a Process Reward Model (PRM) that delivers dense, line-level\nfeedback on code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs and inte-\ngrating them into the RL framework, finding that using PRMs both as dense rewards and\nfor value function initialization significantly boosts performance. Our approach increases\nour in-house LLM's pass rate from 28.2% to 29.8% on LiveCodeBench and from 31.8%\nto 35.8% on our internal benchmark. Our experimental results highlight the effectiveness\nof PRMs in enhancing RL-driven code generation, especially for long-horizon scenarios.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has revolutionized code generation, enabling mod-\nels to achieve near-human performance on programming tasks (Chen et al., 2021; Li et al., 2022; OpenAI,\n2023). These models have demonstrated remarkable abilities to generate syntactically correct and func-\ntionally viable code snippets, significantly aiding software development processes. Building upon these\nsuccesses, recent research has explored the use of reinforcement learning (RL) from unit test feedback to\nfurther enhance the code generation capabilities of LLMs (Le et al., 2022; Shojaee et al., 2023; Liu et al.,\n2023; Dou et al., 2024). By incorporating unit tests as a reward mechanism, these methods aim to guide\nLLMs toward generating code that not only compiles but also passes specified test cases, thereby improving\noverall code reliability and quality.\nHowever, a significant challenge arises from the nature of the reward signals derived from unit tests. These\nsignals are inherently sparse, as they are only received at the end of an episode after the entire code snippet\nhas been generated and evaluated. This delay in feedback impedes learning efficiency and limits the model's\nability to make incremental improvements during code generation. When an LLM fails to generate code\nthat passes any unit tests, it receives no meaningful learning signal, making it difficult to learn to solve\nmore complex coding problems. In contrast, human programmers typically do not rewrite code from scratch\nwhen their programs fail unit tests. Instead, they analyze the code to pinpoint and fix errors, leveraging\ntheir understanding of programming logic and structure to iteratively improve upon the current version.\nThis process of step-by-step refinement, which involves receiving and acting upon fine-grained feedback, is\nmissing in the current RL training loop for code generation from unit test feedback."}, {"title": "PROBLEM FORMALIZATION", "content": "In code generation tasks, we define a code generation problem as a sequence of tokens $x = (x_1, x_2,..., x_m)$,\nwhere each $x_i$ denotes the i-th element or token of the input prompt, which may include problem de-\nscriptions. The primary objective for the model in this context is to process the given input x and gen-\nerate a coherent and syntactically correct sequence of code tokens. This sequence is denoted as $y =\n(y^{(1)}, y^{(2)}, ..., y^{(T)})$, where T represents the total number of code generation steps. Each individual code\ngeneration step, $y^{(t)}, t = 1,2,..., T$, is composed of a series of tokens $y^{(t)}_1, y^{(t)}_2 ... y^{(t)}_{n_t}$, where $y^{(t)}_i$\ncorresponds to the i-th token within the t-th step, and $n_t$ denotes the number of tokens in this step.\nTypically, a pre-trained language model (LM), denoted as $p_\\theta$, is employed to model the conditional probabil-\nity distribution of the code generation steps y, given the code generation problem x, which is mathematically\nrepresented as $p_\\theta(y | x)$, parameterized by $\\theta$. The model is optimized through training on a dataset $D_{xy}$\ncontaining pairs of prompts and their corresponding code solutions. This training process, often referred to\nas Supervised Fine-Tuning (SFT), involves maximizing the log-likelihood of the dataset."}, {"title": "BASELINE METHOD: REINFORCEMENT LEARNING FROM UNIT TEST FEEDBACK", "content": "Code generation tasks can be formulated within a Reinforcement Learning (RL) framework, where code\ngeneration is treated as a sequence of decision-making steps. Once the model has undergone SFT, the RL\nphase is employed to refine the model's ability to generate functionally correct code using feedback from\nunit tests (Liu et al., 2023). Unit test feedback is derived by executing the generated program on predefined"}, {"title": "PROCESS SUPERVISION-GUIDED POLICY OPTIMIZATION", "content": "While the Reinforcement Learning from Unit Test Feedback (RLTF) offers a framework for improving\ncode generation models, it suffers from significant limitations due to the sparsity of its reward signal. The\nbinary nature of unit test feedback\u2014indicating only whether the entire program passes or fails-provides\nno guidance on which specific parts of the code contributed to the outcome. This lack of intermediate\nfeedback makes it challenging for the model to identify and correct errors during training, leading to slow\nconvergence and suboptimal performance. In contrast, human programmers iteratively develop and refine\ntheir code. When a program fails to pass unit tests, they do not typically rewrite it from scratch. Instead,\nthey analyze the code to pinpoint and fix errors, leveraging their understanding of programming logic and\nstructure. This process of step-by-step refinement is crucial for efficient problem-solving.\nMotivated by this observation, we propose Process Supervision-Guided Policy Optimization, a method\nthat integrates fine-grained feedback into the RL framework. Figure 1 illustrates the overview of our ap-\nproach. By providing intermediate rewards that assess the correctness of partial code sequences, our ap-\nproach guides the model more effectively toward generating functionally correct programs. This is achieved\nthrough a Process Reward Model (PRM) (Lightman et al., 2023) that evaluates each code generation step,\noffering dense reward signals that addresses the limitations of sparse end-of-trajectory rewards."}, {"title": "PROCESS SUPERVISION VIA PROCESS REWARD MODELS", "content": "Our method introduces a PRM to assess the correctness of each line of the code during the generation\nprocess. The PRM serves as an oracle that provides intermediate rewards based on the potential of the\ncurrent code prefix to be extended into a correct program. By offering intermediate feedback, the PRM\nhelps the model identify and reinforce beneficial code generation patterns while discouraging those that"}, {"title": "DATA COLLECTION", "content": "To effectively train the PRM, we require a dataset that\nprovides fine-grained annotations indicating the correct-\nness of partial code sequences. However, manually an-\nnotating the correctness of each line of code generated\nby the model is costly and not scalable. Instead, we em-\nploy an automated approach inspired by techniques used\nin recent works (Wang et al., 2024a;b; Luo et al., 2024).\nOur method leverages the model's own capabilities to\ngenerate completions for partial code prefixes and uses\nautomated testing to assess their correctness. The key\nidea is to determine whether a partial code prefix can be\nextended by any means into a complete program that\npasses all unit tests. If so, we consider the prefix as po-\ntentially correct; otherwise, it is labeled as incorrect.\nGiven a prompt x, we generate a complete code response\n$y = (y^{(1)},y^{(2)}, ..., y^{(T)})$ using the current policy $p_\\theta$.\nOur goal is to determine the correctness of each partial\ncode prefix $y_{\\leq t}$ for t = 1, 2, ..., T. To achieve this, we\nemploy a best-of-K sampling strategy to approximate an\noracle capable of completing partial code prefixes. For\neach partial code prefix $y_{\\leq t}$, we generate K potential\ncompletions {$c_k$}$_{k=1}$ using the current policy. We then\nform full programs $P_k = (y_{\\leq t},c_k)$ and execute them\nagainst the unit tests U. If any of these programs pass\nall unit tests, we label the partial code prefix as correct;\notherwise, it is labeled as incorrect. To efficiently iden-\ntify the transition point where errors occur, we employ a\nbinary search over the code generation steps (Luo et al.,\n2024), which is formalized in Algorithm 1. For example,\nconsider a code response divided into five steps (T = 5),\nas shown in Figure 2. The partial prefix up to $y^{(3)}$ can\nbe completed to pass all unit tests, so it is labeled as cor-\nrect. The prefix up to $y^{(4)}$ cannot, meaning steps beyond\n$y^{(3)}$ are labeled as incorrect. For each partial code prefix\n$y_{\\leq m}$, the label $l_m$ is assigned based on the outcome of\nthe completion attempts:\n$l_m = \\begin{cases}\n+1, & \\text{if any } P_k \\text{ passes all unit tests} \\\\\n-1, & \\text{otherwise}\n\\end{cases}$         (1)\nwhich indicate whether the prefix is potentially correct\n(can be completed to a correct program) or incorrect (contains unrecoverable errors)."}, {"title": "TRAINING", "content": "Using the collected data {(x,$y_{\\leq m}$,$l_m$)}, we train the PRM R to predict the correctness of partial code\nprefixes. The PRM learns to assign higher rewards to prefixes labeled as correct and lower rewards to those"}, {"title": "INTEGRATING PRM INTO RL TRAINING", "content": "Given a learned PRM, we aim to identify best practices for enhancing code generation during RL training.\nWhile prior work Lightman et al. (2023); Wang et al. (2024a); Jiao et al. (2024); Wang et al. (2024b); Luo\net al. (2024) uses PRMs to verify intermediate steps in mathematical tasks, this approach is less relevant for\ncode generation. In mathematical domains, LLMs may generate correct answers with faulty reasoning Light-\nman et al. (2023), making intermediate verification essential. However, in code generation, problems are\ntypically accompanied by multiple unit tests, making it improbable for incorrect code to pass all tests. As\na result, the emphasis on intermediate verification is less applicable. Instead, we propose leveraging PRMS\nas auxiliary sources of dense signals to facilitate better exploration during RL training. While preliminary\nattempts have been made to incorporate PRMs into RL trainingWang et al. (2024a), these efforts are limited\nand warrant a more thorough investigation. We explore the following methods to integrate PRMs effectively:\nPRM as Dense Rewards. Similar to Wang et al. (2024a), we use PRMs to provide step-level reward\nsignals that guide more efficient policy exploration during RL training. By rating the correctness of each\nline in the code response, the PRM supplies \u201cdense\u201d rewards that encourage the policy to explore more\npromising code paths, leading to improved performance.\nPRM as Value Initialization. The PRM's method of annotating code, by fixing a prefix $y_{\\leq t}$ and rolling\nout the policy to sample correct responses, can be viewed as a \u201chard\u201d value estimation of $y_{\\leq t}$ . We hy-\npothesize that the PRM's capability to provide line-level feedback could serve as a useful inductive bias for\ninitializing the value function in RL algorithms, which can ease the credit assignment problem by offering a\nmore informed starting point.\nPRM as Both Dense Rewards and Value Initialization. To fully capitalize on the advantages of PRMs,\nwe combine both approaches. By using PRMs for dense rewards and as an initialization for the value model,\nwe aim to enhance RL training through improved exploration and more effective credit assignment."}, {"title": "EXPERIMENTAL RESULTS", "content": "We utilize in-house datasets to train our model for code generation. Specifically,\nthe training set, $D_{train}$, is a comprehensive Reinforcement Learning with Human Feedback (RLHF) dataset\nthat includes, as a subset, approximately 30, 000 diverse coding problems. Each of these problems is paired\nwith unit tests designed to validate the functional correctness of the generated code. For evaluation, we\nemploy two benchmarks: the publicly available LiveCodeBench( Jain et al. (2024)) and our proprietary\ncode generation benchmark (InHouseBench). LiveCodeBench is a rigorous benchmark consisting of 612\ncoding tasks, collected between May 1, 2023, and September 1, 2024, designed to assess the code generation\ncapabilities of LLMs. InHouseBench comprises 245 challenging coding problems in Chinese, spanning both\nPython and C++. All test problems are novel and unseen in public datasets, ensuring there is no risk of data\ncontamination. The benchmark contains two problem categories: (1) Contest, which includes 169 coding\ncompetition-style problems, and (2) NL2Alg, which comprises 76 problems focused on translating natural"}, {"title": "KEY CONSIDERATIONS FOR INTEGRATING PRM INTO RL TRAINING", "content": "While integrating PRM into RL training might seem straightforward, we found that achieving effective\nresults requires careful attention to several critical factors. In this section, we highlight key implementation\ndetails essential for the successful application of PRM in RL training."}, {"title": "PRM TRAINING: MORE DATA OR BETTER DATA?", "content": "Recent research on LLMs highlights that data quality often outweighs quantity (Gunasekar et al., 2023; Li\net al., 2023b). We found the same holds true for PRM training data selection. Although automated data\nannotation allows for the generation of large volumes of PRM training data through model sampling, our\nexperiments showed that increasing data volume can sometimes degrade PRM performance when integrated\ninto RL. In contrast, a smaller, carefully curated subset of the full dataset led to better supervision and\nimproved outcomes. For example, when all sampled responses to a given prompt either consistently pass (or\nfail) unit tests, PRM gains little useful information. In such cases, the model can only learn to predict the\ncorrect (or incorrect) label when it encounters the same prompt again, limiting its ability to generalize. We\nexplored various strategies for selecting and filtering data, as detailed in Section 4.3."}, {"title": "RL TRAINING: ALLEVIATING PRM HACKING", "content": "Reward model hacking (Skalse et al., 2022) is a well-known issue in RLHF training, where the policy learns\nto exploit flaws in the reward model to achieve high rewards without genuinely improving the quality of\nresponse. Similarly, we observed that PRM is also susceptible to such exploitation. Here we discuss two key\npractical strategies to mitigate the risk of PRM hacking and ensure the reward signals remain aligned with\nthe intended task objectives.\nPRM reward length normalization. As described in Section 4.1, when used to provide dense rewards,\nPRM assigns line-level reward signals at the end-of-line tokens in the LLM-generated response. However,\nif we directly use the predictions of the learned PRM, denoted as $R$, as the reward signal $R_{PRM}$ in 3, we\nobserved that this can be exploited. Specifically, the policy may generate numerous lines for which PRM\npredicts positive rewards, thus inflating the overall reward. This occurs because writing more lines allows\nthe model to accumulate excessive intermediate rewards, effectively hacking the optimization objective. To\nmitigate this issue, we apply length normalization to the PRM predictions. Given a prompt x and a response\ny with T lines, $y = (y^{(1)}, y^{(2)}, ..., y^{(T)})$, we define the PRM dense reward signal at the m-th line as:\n$\\frac{1}{T} R_{PRM}(y^{(m)}) = \\frac{1}{T}R(x, y_{\\leq m})$.\nThis normalization ensures that the policy does not gain higher cumulative rewards by generating trivial or\nunnecessarily long responses, as the accumulated reward is bounded within the range of [-1,1] regardless\nof the response length.\nAssigning additional neutral label into PRM training. While length normalization helps reduce PRM\nexploitation, it is insufficient to fully prevent PRM hacking. Even with normalization, we empirically ob-\nserved that the model can still exploit PRM by generating excessive comment lines within the code. The\nunderlying issue is that writing a correct comment is often far easier than producing correct code. As a result,\nthe model can artificially inflate the PRM reward by including unnecessary comment lines. To address this\nissue, we introduce an additional neutral label in the PRM annotation, as defined in Equation 1:\n$l_m = \\begin{cases}\n+1, & \\text{if any } P_k \\text{ passes all unit tests} \\\\\n0, & \\text{if the line is a comment} \\\\\n-1, & \\text{otherwise}\n\\end{cases}$\nBy assigning a neutral label (0) to comment lines, we remove the reward bias that encourages the model to\ngenerate unnecessary comments. This adjustment ensures that only meaningful contributions to the code,\nrather than extraneous comments, are rewarded by the PRM."}, {"title": "MAIN RESULTS AND ANALYSIS", "content": null}, {"title": "CONCLUSIONS AND LIMITATIONS", "content": "In this work, we addressed the challenge of sparse reward signals in reinforcement learning (RL) for code\ngeneration by introducing a Process Reward Model (PRM) that provides dense, line-level feedback. This ap-\nproach mirrors human-like code refinement and improves learning efficiency. Our experiments demonstrate\nthat integrating PRMs significantly enhances the pass rates of code generation models on both the Live-\nCodeBench dataset and proprietary benchmarks. This method has the potential to improve long-horizon\ncode generation scenarios, advancing the state-of-the-art in LLM-based code generation.\nDespite the promising results, our approach has several limitations that warrant further investigation. First,\nthe effectiveness of the PRM relies heavily on the quality of the collected data. While we automated data\ncollection using binary search and unit tests, this method may not capture all nuances of code correctness\nand may introduce noise, especially in more complex or ambiguous programming tasks. Second, the com-\nputational cost of collecting PRM training data is still substantial, even though we employed binary search\nto mitigate it. Third, our automated PRM data collection method requires external verification (unit tests in\nour case), which is not applicable to many other domains such as creative writing or open-ended generation\nproblems. This could limit the applicability of our approach in those areas."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets and Evaluation."}]}