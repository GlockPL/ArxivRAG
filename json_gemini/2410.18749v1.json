{"title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?", "authors": ["Md. Khairul Islam", "Andrew Wang", "Tianhao Wang", "Yangfeng Ji", "Judy Fox", "Jieyu Zhao"], "abstract": "Differential privacy (DP) is applied when fine-tuning pretrained large language models (LLMs) to limit leakage of training examples. While most DP research has focused on improving a model's privacy-utility tradeoff, some find that DP can be unfair to or biased against underrepresented groups. In this work, we show the impact of DP on bias in LLMs through empirical analysis. Differentially private training can increase the model bias against protected groups w.r.t AUC-based bias metrics. DP makes it more difficult for the model to differentiate between the positive and negative examples from the protected groups and other groups in the rest of the population. Our results also show that the impact of DP on bias is not only affected by the privacy protection level but also the underlying distribution of the dataset.", "sections": [{"title": "Introduction", "content": "The advent of transformer-based language models such as BERT (Devlin et al. 2019) has led to significant advancements in different NLP tasks. Much of the success of large language models ultimately derives from the vast amounts of data used to train these models. However, the use of a large training dataset raises concerns about data privacy, where the model can be used to detect the presence of sensitive information in the training data. To defend against these attacks, Differentially Private (DP) training techniques (Dwork et al. 2006; Abadi et al. 2016) have been used during the model training or fine-tuning process (Yu et al. 2021). These techniques ensure that a model does not leak sensitive training data. Otherwise, an attacker can extract the dataset (Carlini et al. 2021) using inference attacks.\nHowever, recent works in data privacy indicate that DP training may cause machine learning models to become more biased (Cummings et al. 2019; Bagdasaryan and Shmatikov 2019; Sanyal, Hu, and Yang 2022). However, most of these works focus on the computer vision domain or tabular datasets. With the wide usage of NLP models and the urgency to realize trustworthy NLP, we need to understand whether we can obtain an NLP model equipped with privacy and fairness, especially for the pre-trained language models. An NLP model is considered biased when the model is unable to perform on protected social groups equally as well as on others. For example, prior research has demonstrated a coreference resolution model can behave very differently for different demographic groups (Zhao et al. 2018; Rudinger et al. 2018). DP may introduce bias because it steers a model away from relying on a select few data points, causing that model to attend poorly to social groups that are underrepresented in the training data.\nIn this work, we explore the impact of differential privacy on model bias in the pre-trained BERT language model. The degree of which can be tuned by adjusting the privacy budget parameter. We train the model with different privacy budgets and measure the bias across six identity subgroups using multiple metrics. We consider bias in the context of the toxic language detection task, which has been shown to produce biased models (Davidson, Bhattacharya, and Weber 2019). We choose two popular datasets, Jigsaw Unintended Bias (Borkan et al. 2019) and the Measuring Hate Speech from UCBerkeley (Kennedy et al. 2020). We use both prediction and probability-based bias metrics to analyze the effect of DP on the bias from different perspectives. We then investigate them in each identity group for any discriminatory behavior against them.\nContributions: In this work, we present a detailed analysis of the impact of DP training on bias in fine-tuned language models. We present our results on two popular hate speech datasets by training our models at different privacy levels and analyzing how it affects the model bias. We show that DP training makes the model more biased in terms of AUC-based metrics. DP also has negative effects on the model's utility when adopted to pertained LLM. Our findings will give new insights into the privacy and bias trade-off, which can help NLP researchers incorporate DP into their works."}, {"title": "Related Work", "content": "Prior research has shown from a theoretical perspective that DP has a detrimental effect on model fairness (Cummings et al. 2019; Tran, Dinh, and Fioretto 2021). Cummings et al. (2019) assume the conditions of \"pure DP\" (Dwork et al. 2006), and demonstrate that such a model cannot achieve perfect equal opportunity between social groups. Tran, Dinh, and Fioretto (2021) finds that model fairness has a disproportionately negative impact on accuracy for certain social groups.\nIn computer vision, recent works have empirically investigated the effects of DP on model fairness in models with more realistic privacy settings. Empirical analyses have found that DP can worsen accuracy for certain subgroups in image recognition tasks (Bagdasaryan and Shmatikov 2019; Sanyal, Hu, and Yang 2022) and synthetic data generation tasks (Ganev, Oprisanu, and De Cristofaro 2021). Uniyal et al. (2021) showed both DP-SGD and PATE have a disparate impact on the under-represented groups, but PATE has significantly less disproportionate impact on utility compared to DP-SGD.\nBagdasaryan and Shmatikov (2019) also found that DP can worsen model bias in sentiment analysis. However, they only considered a single bias metric (accuracy degradation between privileged and unprivileged groups) on a single dataset using a glove-based model. In our paper, we analyze pre-trained BERT models on multiple datasets using multiple bias metrics.\nBalancing between fairness and privacy can provide a significant impact in using private models in practice. Private-FairNR (Cummings et al. 2019) algorithm approximately satisfies fairness for a private learner sampling hypothesis. Lyu, He, and Li (2020) formally guaranteed the privacy of extracted text representation, while also helping model fairness. They aimed to protect the test phase privacy of end users while adopting local DP (LDP) with the Laplace mechanism."}, {"title": "Model Bias in NLP", "content": "Evaluating biases in NLP models requires a metric over some demographic groups. In this section, we describe the terminology for those groups and the metrics for bias evaluation."}, {"title": "Terminology", "content": "Protected attributes refer to sensitive attributes such as gender and race that should not be used to discriminate against individuals (Hardt, Price, and Srebro 2016). Bias occurs when a model experiences a degradation in performance when inferring examples of certain social groups implied by a protected attribute such as gender or race. In our calculations of bias, we refer to a subgroup as the social group whose bias we are measuring and background as the rest of the evaluation set (Borkan et al. 2019). Prediction-based bias metrics calculate the bias against the protected attributes using the model's predicted label (e.g. positive/negative), whereas Probability-based bias metrics use the prediction probability to calculate bias. These definitions of bias metrics are done following Czarnowska, Vyas, and Shah (2021)."}, {"title": "Protected Attributes", "content": "Bias in NLP has been well studied within the protected attributes of gender (Zhao et al. 2019; Ravfogel et al. 2020) and race (Dixon et al. 2018; Davidson, Bhattacharya, and Weber 2019). Following this line of research, we choose to examine bias for sensitive attributes gender and race. In gender attribute the identity subgroups are male/men, female/women, and transgender. For race attribute the identity subgroups are white, black, and asian."}, {"title": "Bias Evaluation Metrics", "content": "A \"degradation in performance\" indicative of model bias can be measured in different ways. We consider metrics such as equality of odds metrics because of their prolific use in other NLP model fairness literature (Hardt, Price, and Srebro 2016; Borkan et al. 2019; Pruksachatkun et al. 2021; Reddy et al. 2021) and Bias-AUC because of its use as the benchmark in the Jigsaw Unintended Bias competition. We summarize all the different bias evaluation metrics we consider in Table 1. The implementations follow Hardt, Price, and Srebro (2016); Borkan et al. (2019) and Reddy et al. (2021). More details about these metrics are in Appendix A."}, {"title": "Differential Privacy", "content": "Differential privacy (Dwork et al. 2006) (DP) aims to preserve privacy by means of a quantifiable protection guarantee and acceptable utility in the context of statistical information disclosure. It is the de facto definition for privacy.\nMany differing definitions of Differential Privacy exist. In the context of our work, we use the notion of ($\\epsilon$, $\\delta$)-privacy. Following Dwork et al. (2006), if we have some arbitrary operation A with output space S and two datasets D, D' that differ in only a single record, then we can formulate ($\\epsilon$, $\\delta$)-privacy as\n$Pr(A(D) \\in S) < e^{\\epsilon} Pr(A(D') \\in S) + \\delta$.\nBy limiting any effect due to the inclusion of one individual's data (by the parameter $\\epsilon$), the DP notion approximates the effect of \"opting-out\u201d: whether an individual's data is included or not does not influence the result much, thus the fact that the individual participated in the data release is protected.\nTo satisfy DP, noise is added to the aggregated-level results such that an individual's information disclosure is bounded. Our implementation in this paper uses the Gaussian mechanism (Dwork and Roth 2014) to guarantee ($\\epsilon$, $\\delta$)-DP.\nDP in machine learning: When training models with DP, perturbations are added to the gradients (i.e., clipping the gradients and then adding Gaussian noise) (Abadi et al. 2016). More specifically, during the t-th iteration the optimizer will compute noisy gradients as:\n$\\bar{g} = \\frac{1}{ | B |} ( \\sum_{x_i \\in B } \\hat{g_i} + N(0, \\sigma^2 C^2 I) )$,\nwhere B is a subsampled batch used to compute the gradients, $w_{t-1}$ is current model before t-th iteration, $\\sigma$ is noise multiplier, $\\hat{g_i} =  \\nabla_w \\ell(x_i; w_{t-1}) / \\text{min} \\{ 1, \\frac{||  \\nabla_w \\ell(x_i; w_{t-1}) ||_2}{C} \\}$ (i.e., each gradient is clipped by C, so that $ \\sum  \\hat{g_i} $ has bounded l2-sensitivity and we can use the Gaussian mechanism to ensure DP), and $\\bar{g}$ is the (noisy) gradient used to update the model.\nTraining a model requires multiple training epochs. Our formulation of DP is amenable to this practice. If we have"}, {"title": "Datasets", "content": "We choose two popular toxicity detection datasets for our study, Jigsaw Unintended Bias (Borkan et al. 2019) and UCBerkeley Hate Speech (Kennedy et al. 2020). Both datasets (1) have target labels so that we can use supervised learning, (2) are for text classification using NLP techniques, and (3) have annotated social groups for all examples."}, {"title": "Jigsaw Unintended Bias", "content": "The Jigsaw Unintended Bias dataset was developed to learn and minimize any unintended bias against different identities that a machine learning model might learn when trying to predict toxicity 1. Here toxicity is defined as anything rude or disrespectful that can make someone leave a discussion. The dataset has annotations for demographic groups by disability, gender, race or ethnicity, religion, and sexual orientation. The complete dataset has about 2 million examples. We report the label distribution for each identity in Table 2.\nTrain/Validation/Test split: We undersampled the training dataset using a 2:1 ratio between the non-toxic and toxic labels. Due to computing resource limitations, we then halved the training set, making sure to preserve the 2:1 label"}, {"title": "UCBerkeley Hate Speech", "content": "This dataset 2 is a collection of online comments from three major social media platforms (YouTube, Twitter, and Reddit) which were later labeled by human annotators through crowd-sourcing (Kennedy et al. 2020). It provides a unique way to measure hate speech at eight theorized qualitative from genocidal hate speech to counter speech. The dataset comes with annotations for the targeted group in the comment text.\nPre-processing: The original dataset has 135,556 comments and the annotations for 'hatespeech' contain 3 classes: 0 for neutral or counter speech, 1 when the annotator is unclear, and 2 for hate speech. For the simplicity of the experiment, we dropped comments with label 1, converting the task to a binary classification where hate speech is a positive class and non-hate speech is negative. The dataset also had multiple annotations per comment. We aggregated the annotations for each comment. If any comment had the same label at least from 50% of the annotators, then it was chosen as true, otherwise false. After aggregation, we had 38,564 comments left. Additionally, the dataset contains transgender identity labels split into multiple groups (transgender_men, transgender_women, transgender_unspecified). We combined them in a single transgender column for bias calculation.\nTrain/Validation/Test split: We randomly split the aggregated data into train, validation, and test sets using a 70:15:15 ratio."}, {"title": "Experimental Setup", "content": "Model: We use the pre-trained BERT-base-uncased model from HuggingFace 3 to perform all our experiments in this section. For training the model on downstream tasks we choose only to train the last three layers (final encoder layer, pooler, classifier). The rest of the layers were frozen, yielding 7.6 M trainable parameters out of a total of 109M. We choose to train only these layers because: 1) DP is more effective when applied to fewer layers, and 2) we can utilize BERT's rich pre-trained embeddings.\nInput texts were tokenized using the BERT-base-uncased tokenizer from HuggingFace. The comment texts were generally not very lengthy, so we kept the maximum sequence length to 128 across both datasets. The batch size was set to 64.\nOptimization: We use the Adam optimizer with cross-entropy loss and learning rate 10-3. We train each model for a maximum of 10 epochs. At each epoch, the trained model is evaluated on the validation set and saved if the F1 score improves. Early stopping patience was 3. We also used a learning rate scheduler (ReduceLROnPlateau) to reduce the learning rate by a factor of 0.1 if the validation F1 score does not improve for more than one epoch.\nPrivacy: We use the Pytorch Opacus library (Yousefpour et al. 2021). It provides a privacy engine to train models with DP-SGD (Abadi et al. 2016). DP-SGD was chosen since it is the most widely used one in the related works (Bagdasaryan and Shmatikov 2019; Tran, Dinh, and Fioretto 2021; Anil et al. 2021), supports iterative training process (McMahan et al. 2018) and available as a framework. We use the make_private_with_epsilon method offered by the library, which takes as input the model to be trained, optimizer, training data, number of epochs, target $\\epsilon$, target $\\delta$ and maximum gradient norm. The target epsilon is the privacy budget we want to achieve. For a reasonable privacy guarantee, $\\epsilon$ should be set below 10 (Abadi et al. 2016) and this setting has been followed in other applications of DP on NLP (Bagdasaryan and Shmatikov 2019; Anil et al. 2021; Lyu, He, and Li 2020). For our task, we experimented with"}, {"title": "Results", "content": "Here we present the impact of adding DP on the overall model utility for both datasets. Table 3 shows that for both datasets the model utility decreases with stricter privacy (smaller $\\epsilon$). However, for the UCBerkeley dataset, the false positive rate increases, and the recall drops significantly. This shows the model predicts fewer positive cases with added privacy. The recall drop is also significant for Jigsaw. This decrease in overall performance also impacts the performance of the identity subgroups."}, {"title": "Prediction Based Metrics", "content": "Equality of Odds, parity, and protected accuracy are prediction-based bias metrics. They calculate the bias score based on the model's prediction. We present the results in Table 4 for both datasets. For each identity, we report the best and the worst results, as well as the privacy budget that achieves that result. The closer these scores are to 1, the less the bias is.\nTable 4 shows several trends depending on the dataset and metric. The equality of odds always improves with a strict privacy budget (small $\\epsilon$). However, this is due to a significant drop in recall (Figure 5) for most groups. They are reduced to a smaller score range. Thus the TPR difference becomes smaller, hence improving the EqOpp1 (Figure 3).\nThe trend in demographic parity is the opposite in both datasets. With a stricter privacy budget, parity decreased in the Jigsaw (2-3%) but increased in the UCBerkeley dataset"}, {"title": "Probability Based Metrics", "content": "This section presents the bias calculated using the metrics presented by Borkan et al. (2019). These metrics are dependent on the model's prediction probability, hence better representing the bias in the model's confidence. They are also threshold agnostic, unlike prediction-based metrics.\nFigure 1 shows that for stricter privacy (smaller $\\epsilon$), both BNSP and BPSN drop significantly for most identities. A drop in BNSP means the scores for positive examples in these subgroups are lower than the scores for other negative examples in the background data. These examples would likely appear as false negatives within the subgroup at many thresholds (Borkan et al. 2019).\nSimilarly, a drop in BPSN means scores for negative examples in these subgroups are higher than scores for other positive examples in the background. These examples would likely appear as false positives within these subgroups at many thresholds (Borkan et al. 2019). A decrease in the subgroup AUC score shows that the model can not understand and separate the positive and negative examples within the subgroup. These drops between non-DP training and training with DP at $\\epsilon \\leq 0.5$ are highlighted in Table 5, showing an increase in bias at stricter privacy budgets, compared to non-DP training.\nFigure 1 shows some interesting cases. In the Jigsaw dataset, white and black identities have much lower AUC and BPSN scores compared to others. Similarly in the UCBerkeley dataset, men and women have much lower AUC and BPSN scores than other identities. This shows that the DP models more often tend to label non-toxic comments mentioning these identities as toxic, compared to the non-DP models."}, {"title": "Discussion", "content": "Equality of odds is a function of relative true positive and false positive rates between a subgroup and the background population. As such we investigate why the addition of noise does not decrease relative TPR (recall) and FPR. DP adds noise in the training phase which adversely affects overall model performance (Table 3). The model experiences a degradation in the recall for all social groups as the privacy setting increases. As illustrated in Figure 5 in Appendix D, we find that with private training, the recall values grow more similar. The direct effect of this phenomenon is to minimize the difference in TPR between a subgroup and a background population, contributing to an overall improvement in equality of odds. However, we find that such"}, {"title": "Conclusion", "content": "In this work, we explore how differential privacy affects the bias in NLP models. We found DP increases model bias and the impact of that increase varies across different identities. We perform our empirical analysis on two hate/toxic language detection datasets. We evaluated the gender and racial bias of the model using different bias metrics for models trained at different privacy budgets ($\\epsilon$). We found that (Table 5) stronger privacy budgets cause the model to have more difficulty distinguishing between the positive/negative examples in the identity subgroup from negative/positive examples in other subgroups at different prediction thresholds (Borkan et al. 2019). We also observe an increase in equality of odds at a much stricter privacy level, mainly because the recall drops significantly for each group, reducing the difference between them. However, the protected accuracy also drops in most cases. Our overall observations confirm that DP increases bias in the NLP models for hate speech detection, and NLP researchers need to be aware of this bias when adding privacy to NLP models."}, {"title": "Limitations", "content": "We make our observations based on toxicity and hate speech detection tasks. However, bias in NLP has also been investigated in other tasks like coreference resolution (Zhao et al. 2019), sentiment analysis (Kiritchenko and Mohammad 2018), and question answering. Whether trends found in our results persist in those tasks too, is something to be explored for future works. We consider six different identity subgroups across two protected attributes (race, and gender). There exist more sensitive attributes in the dataset like religion and sexual orientation which are not explored here but can be explored in future works. We saw similar trends in bias across both selected attributes and the identity subgroups. Even with new attributes or subgroups, the trends should persist similarly."}]}