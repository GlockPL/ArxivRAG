{"title": "3D Reconstruction of Shoes for Augmented Reality", "authors": ["Pratik Shrestha", "Sujan Kapali", "Swikar Gautam", "Vishal Pokharel", "Santosh Giri"], "abstract": "This paper introduces a mobile-based solution that enhances online shoe shopping through 3D modeling and Augmented Reality (AR), leveraging the efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D images, the framework generates realistic 3D shoe models from 2D images, achieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables immersive AR interactions via smartphones. A custom shoe segmentation dataset of 3120 images was created, with the best-performing segmentation model achieving an Intersection over Union (IoU) score of 0.95. This paper demonstrates the potential of 3D modeling and AR to revolutionize online shopping by offering realistic virtual interactions, with applicability across broader fashion categories.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancements in 3D modeling and computer graphics have revolutionized various industries, enabling the creation of immersive and realistic Augmented Reality (AR) solutions. These technologies are increasingly utilized in the fashion industry to enhance customer experiences by enabling interaction and visualization of products before purchase. Despite these advancements, traditional online shopping platforms remain limited by their reliance on static 2D images, which fail to replicate the exploratory and sensory experience of physical retail environments.\nHistorically, generating 3D models from 2D images has been a labor-intensive and time-consuming process, requiring significant human intervention. However, recent breakthroughs have accelerated the modeling process while achieving higher levels of precision and realism. Among the approaches to 3D modeling, Photogrammetry, Neural Radiance Fields (NeRF), and Gaussian Splatting have emerged as prominent techniques, each with distinct advantages and limitations.\nPhotogrammetry, although widely used, encounters challenges such as handling reflective surfaces, dealing with occlusions, and requiring high-quality input images, which limit its practicality. NeRF, a deep learning-based method, generates highly detailed models but is computationally intensive, making real-time applications infeasible. In contrast, Gaussian Splatting offers a significant advantage by enabling faster training, real-time rendering, and easy modification of the generated models. This paper seeks to find techniques to leverage 3D Gaussian Splatting to streamline the creation of realistic 3D models from 2D images, focusing on its application in enhancing online shopping experiences."}, {"title": "II. LITERATURE REVIEW", "content": "Recent advancements in deep learning have revolutionized the synthesis and rendering of 3D models from 2D images, presenting efficient alternatives to conventional Photogrammetry techniques [1] [2] [3]. Among these advancements, two prominent methods, Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting [3], have garnered considerable attention for their respective strengths and applications.\nNeRF, pioneered by Shrinivasan, Mildenhall, and Tancik in 2020, operates by harnessing radiance fields for view synthesis. By employing Multi-Layer Perceptrons (MLPs), this method learns to predict both the volume density and color of a point based on a 5D input-comprising the spatial location of the camera and its viewing direction. Subsequently, NeRF synthesizes views by querying points along marching rays for color and volume density and applies classical volumetric rendering techniques for rendering. This technique excels in producing true-to-life renderings by learning the volumetric representation of a scene from a collection of images. However, its training process demands significant computational resources, making it impractical for real-time applications.\nIn contrast, 3D Gaussian Splatting, introduced by Kerbel, Kopanas, Leimkuhler, and Drettakis, leverages Gaussians to render views. Firstly, a point cloud is generated from images using the Structure from Motion(SfM). [4] This approach involves the conversion of individual points within a point cloud into Gaussians, and fine-tuning the parameters of these Gaussians through stochastic gradient descent. Notably, this technique offers a substantial leap in speed compared to the original NeRF, enabling real-time rendering capabilities. As a result, it emerges as a promising solution for scenarios requiring instantaneous interactions, making it particularly suitable for applications such as virtual try-on experiences.\nThe image given to model for 3D reconstruction should have its background removed. There are wide variety of segmentation models [5] [6] [7] [8] [9] available for this task. However, YOLOv8 [10], leveraging state-of-the-art progressions in deep learning and computer vision, is commonly employed because of its light weight and development tools. YOLOv8 is proficient in various vision AI tasks, encompassing detection, segmentation, pose estimation, tracking, and classification."}, {"title": "III. METHODOLOGY", "content": "In the 3D Gaussian Splatting Model, we achieve accurate modeling by over-fitting a single object. Consequently, a vast data-set isn't necessary. But, we do need data for segmentation of shoes from the given images. For this purpose, we collected 101 videos, each lasting 30 seconds, from students at Pulchowk Campus. These videos were taken with different cameras, such as Poco X3, Samsung Galaxy S9, Redmi 12, and others.\nFrom each video, we extracted 30 images by evenly dividing the video frames. These images were then used for shoe segmentation to generate 3D models. The segmentation process involved isolating the shoe from the background in each image. These segmented images formed the foundational data for our subsequent 3D modeling."}, {"title": "B. Background Masking", "content": "We used Meta's Segment Anything Model (SAM) [8] to generate segmentation maps from images, which provided accurate results. However, the model required over 6GB of GPU memory, making it costly to host on a server due to its high computational demands. To address this, we used SAM to create a dataset and then trained a smaller, more efficient model that significantly reduces computational requirements. The steps we followed are outlined below:\nDataset Collection: We collected 101 videos of distinct shoes each of length 30 seconds. We collected 30 images from each video by sampling uniformly across the duration of the video. This gave us a total of 3030 images.\nAnnotation: We use an automated pipeline to annotate the images. First, we used a dataset [14] from Roboflow to train an YOLO model to detect shoes in images. Then, we used SAM to annotate the images, sending in the bounding box obtained from the YOLO model as prompt.\nCorrection of improper annotations: The automated annotation pipeline did not provide correct annotation on all images which arose from inaccuracies in YOLO model while detecting shoes and inaccuracies in segmentation map from SAM. However, such instances were very few in number and hence, did not require much time to correct manually. We manually skimmed through all the annotations correcting the ones with errors to ensure the correctness of whole dataset. Furthermore, there were also some frames which did not have shoes. We removed such images.\nTraining: For training, we split the data into train, validation and test set with 80%, 10% and 10% of the total data respectively. We wrote the split script to ensure that images from a single video does not end up in multiple splits which might bump the accuracy simply by overfitting the training samples. We trained the dataset on YOLO v8 and Unet model pre-trained on the COCO128 dataset. The model provided results which is decent enough for the task of 3D reconstruction. The final model size is 6.5 MB which is orders of magnitude smaller than the SAM model which is about 1.5GB. This significantly reduces the required computational resources and server costs for hosting the model."}, {"title": "C. Colmap", "content": "We used COLMAP, a popular tool for 3D reconstruction, to process our data. It helped us create accurate 3D models from input images by estimating camera poses and generating sparse and dense point clouds. The images were first aligned using COLMAP's feature-matching and structure-from-motion pipeline. After alignment, dense reconstruction was performed to generate detailed 3D points. The output was then used as input for the next stages of our project."}, {"title": "D. Gaussian Splatting Model", "content": "We used the implementation provided in the Gaussian Splatting repository with some adjustments. The pre-processing step included cropping each image to its maximum size along both dimensions. The model was then trained for 7000 iterations, which took approximately 10 minutes."}, {"title": "E. Mesh Extraction", "content": "We used the implementation from the Sugar repository with some adjustments. The model was trained for 9000 refinement iterations, which was the most time-consuming step. Extracting the mesh took approximately an hour. The final images from the undistortion phase had a black background, resulting in many black faces in the final mesh. To address this, we wrote a script to remove all black vertices and extract the largest connected component, ensuring a cleaner mesh."}, {"title": "F. Augmented Reality", "content": "To predict the pose of the user's feet in augmented reality (AR), we employed Lens Studio. The process of fitting a shoe onto the user's leg involves the following steps:\nWe began by creating a 3D model using Gaussian Splatting, then combined the obj, mtl, and png files using Blender.\nNext, we imported the GLB files of both the left and right foot shoes into Lens Studio.\nUsing Lens Studio's leg detection template, we aligned the 3D model feet with the user's actual feet.\nTo address occlusion, we placed transparent cylinders through the holes of the shoes.\nFinally, we published the lens in Lens Studio and integrated it into our Flutter app."}, {"title": "G. Mobile Application", "content": "We integrated individual pieces in a mobile application which allows user to enjoy AR experience in their mobile phones. We utilized flutter along with SnapAR package to develop the app."}, {"title": "H. Overall System Design", "content": "The pipeline comprises several integral components working in tandem to create a comprehensive and immersive AR experience. First, the Background Masking element eliminates image backgrounds, facilitating the generation of masked images fed into the model for 3D shoe modeling. Colmap generates a 3D point cloud or mesh representing reconstructed scenes. The Gaussian splatting model refines this representation by using multiple 3D Gaussian distributions to create a smooth point cloud depiction. Sugar converts the obtain point cloud to mesh format. Foot pose estimation model involves keypoints prediction, pose estimation, and segmentation for occlusion identification, ensuring a realistic representation of the shoe and leg interaction. We utilize SnapChat's Camerakit to implement this functionality. The project culminates in a mobile application merging these components, enabling users to engage in AR."}, {"title": "I. Evaluation and Testing", "content": "The quality of the final augmented output largely depends on the quality of the 3d model generated, we have to ensure that the initial output is as realistic as possible. We used Peak Signal-to-Noise Ratio to evaluate the performance of our method.\nThe PSNR is expressed in decibels (dB) and is calculated using the following formula:\nPSNR = 10 \\cdot log_{10} \\left( \\frac{MAX^2}{MSE} \\right) \\text{,}\nwhere:\nMAX = maximum possible pixel value of the image or video (e.g., 255 for an 8-bit grayscale image)\nMSE = Mean Squared Error between the original and the reconstructed image\nThe Mean Squared Error (MSE) is calculated as follows:\nMSE = \\frac{1}{N \\times M} \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\left( \\text{Original}(i, j) - \\text{Reconstructed}(i, j) \\right)^2 \\text{,}\nwhere:\nN = number of rows in the image or video\nM = number of columns in the image or video\nFor the segmentation task, we evaluate our models over IOU.\nIOU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}} = \\frac{TP}{TP + FP + FN} \\text{,}\nTrue Positive (TP): Instances where the model correctly predicts the presence of a positive class\nFalse Positive (FP) : Instances where the model incorrectly predicts the presence of a positive class\nFalse Negative (FN): Instances where the model fails to predict the presence of a positive class\nThe final qualitative evaluation of our application was done by testing the system on various input conditions. A robust system can handle various conditions of different foot poses and can realize a realistic AR effect in practical scenes."}, {"title": "IV. RESULTS AND DISCUSSIONS", "content": "Following are the results we obtained grouped by system components:"}, {"title": "A. Background Masking", "content": "The data collection phase resulted in 3,000 images. Pre-processing was done as explained in Implementation section. Following are the results we obtained after training different models.\nWe trained the dataset on YOLO v8 and Unet model pre-trained on the COCO128 dataset. The model provided results which is decent enough for the task of 3D reconstruction. The final model size is 6.5 MB which is orders of magnitude smaller than the SAM model which is about 1.5GB. This significantly reduces the required computational resources and server costs for hosting the model."}, {"title": "B. Gaussian Splatting Model", "content": "We drew random samples from our dataset and evaluated the Gaussian Splatting model. We obtained an average PSNR of 34."}, {"title": "C. Mesh Extraction", "content": "The result of Gaussian splatting was used to extract mesh using Sugar. Following is the trajectory of loss we obtained for a shoe: Initially the loss decreased, but after some iterations it abruptly increased. For optimal results, best model should be saved."}, {"title": "V. CONCLUSION", "content": "This work presents a robust system for 3D shoe modeling and AR integration, demonstrating the potential of Gaussian Splatting for efficient and accurate 3D reconstruction. By addressing limitations in data processing and model complexity, the proposed framework achieves realistic rendering suitable for mobile applications. Future efforts will focus on enhancing real-time rendering capabilities, improving occlusion handling, and extending the pipeline to other product categories for broader application in the fashion industry."}]}