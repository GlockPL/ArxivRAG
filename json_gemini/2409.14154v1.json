{"title": "MSSDA: Multi-Sub-Source Adaptation for Diabetic Foot Neuropathy Recognition", "authors": ["Yan Zhong", "Zhixin Yan", "Yi Xie", "Shibin Wu", "Huaidong Zhang", "Lin Shu", "Peiru Zhou"], "abstract": "Diabetic foot neuropathy (DFN) is a critical factor leading to diabetic foot ulcers, which is one of the most common and severe complications of diabetes mellitus (DM) and is associated with high risks of amputation and mortality. Despite its significance, existing datasets do not directly derive from plantar data and lack continuous, long-term foot-specific information. To advance DFN research, we have collected a novel dataset comprising continuous plantar pressure data to recognize diabetic foot neuropathy. This dataset includes data from 94 DM patients with DFN and 41 DM patients without DFN. Moreover, traditional methods divide datasets by individuals, potentially leading to significant domain discrepancies in some feature spaces due to the absence of mid-domain data. In this paper, we propose an effective domain adaptation method to address this proplem. We split the dataset based on convolutional feature statistics and select appropriate sub-source domains to enhance efficiency and avoid negative transfer. We then align the distributions of each source and target domain pair in specific feature spaces to minimize the domain gap. Comprehensive results validate the effectiveness of our method on both the newly proposed dataset for DFN recognition and an existing dataset.", "sections": [{"title": "Introduction", "content": "Diabetes mellitus (DM) is a serious non-communicable disease and a global public health issue. According to the International Diabetes Federation (Cho et al. 2018), approximately 537 million adults worldwide had diabetes in 2021, and this number is projected to reach 693 million by 2045. Over 30% of diabetic patients will develop diabetic peripheral neuropathy (DPN), with the incidence increasing with age (van Schie 2008; Carls et al. 2011). DPN affects the autonomic, sensory, and motor nervous systems, compromising skin integrity and sensation in the feet, thereby increasing susceptibility to injury and diabetic foot ulcers (DFU) (Fernando et al. 2013).\nEarly screening and proactive management can prevent 45% to 85% of foot ulcers (Association et al. 1999). Researches (Coppini et al. 2001; Kastenbauer et al. 2001) indicate that uneven plantar pressure distribution, particularly high pressure in certain areas, significantly contributes to foot ulcers, with a correlation of 70% to 90%. These studies have spurred research into wearable footwear to monitor plantar pressure data (Wang et al. 2020; Sazonov, Hegde, and Tang 2013) and analyze real-time changes to screen for peripheral neuropathy early, which is crucial for preventing foot ulcers. Studies on plantar pressure abnormality detection have achieved high accuracy in intra-subject setting using artificial neural networks (Wafai et al. 2014), SVM (Botros et al. 2016) and backpropagation neural network (Liu 2017).\nDFN is a is a specific form of DPN, primarily affecting the nervous system of the feet in diabetic patients. In the field of diabetic foot neuropathy recognition, however, there is no dataset specifically focused on foot information. Some datasets are constructed from electronic health records, including age and pain-related medication prescriptions (DuBrava et al. 2017) while others include more complex indicators such as patient medical history, physical examination and biochemical test results (Lian et al. 2023). All these datasets are not directly related to foot information, lacking continuous, long-term foot-specific information. These issues motivat us to create a dataset for DFN recognition collected using wearable shoes, which contains more foot-specific information. We name it DFN-DS, which includes 5-minute continuous plantar pressure data from 94 DM patients with DFN and 41 DM patients without DFN.\nAlthough many methods perform well under intra-subject settings, they often perform poorly under cross-subject settings due to the significant data distribution gap between individuals. Transfer learning methods are introduced to reduce this gap in biomedical signal processing. In ECG tasks, MS-MDA (Chen et al. 2021) achieve good performance in"}, {"title": "Related Work", "content": "SDA aims to reduce the domain gap in the feature space when data may follow different distributions, which often leads to poor performance of traditional methods. Based on the generalization bound (Ben-David et al. 2006, 2010) measured by Maximum Mean Discrepancy (MMD), DAN (Long et al. 2015a) is proposed to mitigate the shift in feature space. Deep CORAL (Sun and Saenko 2016) calculate the distribution gap between source domain and target domain using second-order statistics instead of MMD. Subsequently, JAN (Long et al. 2017) is developed to address the joint distribution gap. MCD (Saito et al. 2018) is then proposed to approximate the disparity difference in the bound by the disagreement between two classifiers' outputs. Domain-adversarial methods are developed with the emergence of Generative Adversarial Network (GAN). DANN (Ganin et al. 2016) is the first to propose adversarial domain adaptation based on the generalization bound and the adversarial idea. CDAN (Long et al. 2018) advances the theoretical underpinnings with Disparity Discrepancy, pushing the boundaries of domain adaptation methods. Inspired by these methods, MhNet and SFDA are proposed and arrive good performance in falling risk assessment tasks (Wu et al."}, {"title": "Multi-source Domain Adaptation (MDA)", "content": "Unlike single-source domain adaptation, multi-source domain adaptation involves multiple source domains, introducing more complex inter-domain gaps. DSAN (Zhu et al. 2020) divides the dataset into several domains, considering data with the same category labels to share the same domain label. M3SDA (Peng et al. 2019) aligns multiple source domains with the target domain while also ensuring alignment among all the source domains, which aims to unify data from different domains within a common feature space. In contrast, MFSAN (Zhu, Zhuang, and Wang 2019) aligns the distributions of each pair of source and target domains in multiple feature spaces, processing N alignments simultaneously for N source domains and a target domain.\nMany works in the field of biomedical signal process are related to MDA methods, such as gait analysis, ECG tasks, EMG tasks and EEG tasks. Usually in their methods, datasets are divided by individual, with each individual's data constituting a separate domain. Some approaches align multi-source domains and a target domain in a feature spaces using the idea of M3SDA (Wu et al. 2023; Mu et al. 2020). Some others process alignment with the idea of MFSAN, meaning that they try to align multi-source domains and a target domain in multi feature spaces (Chen et al. 2021; Deng, Tu, and Xu 2021; She et al. 2023; Guo, Gu, and Yang 2021).\nHowever, these methods are typically designed for datasets with fewer than 50 subjects, most having less than 20. As the number of subjects increases to 100 or even 500, their approaches require substantial computational resources and time due to cross-domain computations and a parallel architecture based on the number of subjects. Moreover, using data from all individuals without selection can lead to negative transfer, while the distribution gap can be hard to reduce because of the absence of mid-domain data to bridge source domain and target domain under a patient a domain setting. To address these problems, we propose to split datasets based on convolutional feature statistics, rather than relying on subjects as in conventional methods. Additionally, our method incorporates a process for selecting appropriate sub-source domains to avoid negative transfer, ensuring that not all source domains are considered for alignment with the target domain. We then leverage the idea of aligning the distributions of each pair of source and target domains across multiple feature spaces to enhance performance."}, {"title": "Proposed Method", "content": "Considering that we have a source domain constructed by N individuals' data Ds = {D1, D2, ..., DN }, an target domain constructed by one testing individual's data Dr and total dataset DTotal = DsUDr. Note that Ds \u2229 DT = 0. We get labeled source samples {(x,y)} from Ds, where |Ds| refer to the total number of samples in Ds. Similarly we have unlabelled target samples {x} DT from target domain Dr and all unlabeled samples {m} DTotal from DTotal."}, {"title": "Stage 1. Contrastive Learning", "content": "Here we show the details in stage 1. Contrastive learning is widely used as a pre-training method to enhance the feature extractor's ability to learn effective representations in the field of biomedical signal process (Wang et al. 2024; Lai et al. 2023). The main idea of it is to mine data consistency by bringing similar data (or positive pair) closer together and pushing dissimilar data (or negative pair) further apart. Unlike previous methods, we do not employ contrastive learning as a pre-training technique. Instead, we harness its influence within the feature space to widen the distance between all samples whatever from DT or Ds, thereby enhancing the clustering effect in the second stage, which will be shown in detail below. Specifically, we set (xi, xi) as a positive pair, (xi, xj) and (xi, xj) as negative pairs, where Zi is the augmented view of xi, i \u2260 j and xi, xj \u2208 DTotal. We then make all samples distinguishable from each other in the feature space using contrastive loss, defined as\n$L_c = E[-log{\\frac{exp(h_i \\cdot h_{i'})}{\\sum_{j=1}^{J}exp(h_i \\cdot h_{j'}) + 1[i\\neq j]exp(h_i \\cdot h_j)}}]$\nwhere hi = Fo(xi), Xi \u2208 DTotal, J = |DTotal, Fo stands for the feature extractor used in this stage, means dot product and 1[i\u2260j] stands for an indicator function that equals 1 when i \u2260 j and 0 other wise. Note that the feature extractor Fo well-trained in this stage will be used in following stages."}, {"title": "Stage 2. Clustering and Get Pseudo Domain Labels", "content": "Inspired by Matsuura and Harada's work (Matsuura and Harada 2020), we assume that the latent domains of data are reflected in their styles, specifically in the convolutional feature statistics (mean and standard deviations). Therefore, we firstly obtain convolutional feature statistics from the well-trained feature extractor Fo. Specifically, we calculate the mean and standard deviations of the source samples by channel c,\n$\\mu_c(F_o(x_i)) = \\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (F_o(i))_{chw},$\nwhere xi \u2208 DTotal, c, h, w respectively refer to channel, height and width of the representation of xi transformed by well-trained Fo in the feature space.\nWe represent them in a simple concise form in Figure 2, in which h\u2081 = Fo(xi), \u03bc(hi) = {\u03bcc(Fo(xi)}C=1, \u03c3(hi) = {\u03c3\u03b5(Fo(xi)}C=1. Then we utilize GMM as the clustering method on these convolutional feature statistics. Finally, the influence of different cluster numbers will be evaluated by the Bayesian Information Criterion (BIC) (Schwarz 1978), motivated by SSDA (Lu et al. 2021),\n$BIC = -2 ln L + klnm,$\nwhere L represents the maximized value of the likelihood function for the estimated model, k represents the number of free parameters to be estimated, and m is the sample size. We seek proper cluster number K which minimizes BIC.\nUsing BIC, we determine a certain number of clusters, which differs from Matsuura and Harada's approach. Additionally, they utilize a stack of convolutional feature statistics obtained from lower layers of the feature extractor, whereas we choose those from the last layer because it is task-specific that separate the data as effectively as possible, aligning with our objectives.\nIn the end of stage 2, after assigning K types of numerical pseudo-domain labels to source samples, we get K sub-source domains and K cluster centers, denoted as Dsubk,Centersubk, k \u2208 {1, ..., K}, respectively."}, {"title": "Stage 3. Select Sub-Source Domains to Align the Target Domain", "content": "Compared with the previous works in the field of biomedical signal process, our method do not use all the source domains, instead, we select some of them to avoid negative transfer which may be happened as the individual number of the dataset increases. We select M sub-source domains by the distance between the sub-source domain center Centersubk and the target samples, which is calculated as followed:\ndisk = max ||T(\u03bc(Fo(x)), \u03c3(Fo(x))), Centersubk || 2,\nwhere T stands for PCA with dim = 2 and || . ||2 refers to L2 norm. Then we select M (M < K) sub-source domains by the distance calculated in Eq.(5) :\nDssub\u2081 =\nargmin disk,\nk\u20ac{1,...,K}\nDssubm =\nargmin\ndisk,\nk\u2208{1,...,K}\\{Dssub\u2081,\u2026\u2026\u2026,Dssub M-1}\nwhere M is determined by user. We name them selected sub-source domain 1,..., selected sub-source domain M, respectively, as shown in Figure 2. Then we try to reduce the domain gap between these M sub-source domains and target domain in order to improve cross-subjects performance. We utilize the idea that aligns the distributions of each pair of source and target domains in multiple feature spaces (Zhu, Zhuang, and Wang 2019). In a traditional domain-adversarial framework, such as DANN (Ganin et al. 2016), there are three parts, a feature extractor F to get domain-invariant features, a domain discriminator D to distinguish which domain the output of the feature extractor is from and a classifier C to predict the category label of the output of the feature extractor. By loss function, F is encouraged to extract the features that are challenging for D to distinguish, while the D is trained to correctly predict the domain label of the output of the feature extractor. This creates an adversarial relationship between F and D, allowing F to extract domain-invariant features across different domains. The framework we used in stage 3 can be seen as a parallel architecture of M DANNs. The target domain is aligned with each source domain in a specified sub-networks, as showed in Figure 2. Loss function is calculated as bellow:\n$L_{cls} = \\sum_{k=1}^{M}\\sum_{(x,y)\\sim D_{ssub_k}} L_{CE}(C_k(F_k(x)), y),$\n$L_{adv} = \\sum_{k=1}^{M}\\sum_{x\\sim D_{ssub_k},\\newline x\\sim D_{T}} L_{CE}(D_k(F_k(x)),1)+\nL_{CE}(D_k(F_k(x)),0),$\n$L_{total} = L_{cls} - \\alpha L_{adv},$\nwhere LCE denotes the use of cross-entropy, Lcls represents the category classification loss, Lady refers to the adversarial loss, \u03b1 is a trade-off parameter and Ltotal is the total loss. Finally, to predict the labels of target samples, we compute the average of all classifier outputs."}, {"title": "Experiments", "content": "We evaluate our proposed method (MSSDA) with some transfer learning methods that are widely used in the field of biomedical signal process on two datasets: one is our proposed dataset DFN-DS and the other one is Fall Risk Assessment dataset (Hu et al. 2022), both involving plantar pressure data. By using these two dataset, we verify whether our method is suitable for biomedical signal datasets. Our code and dataset DFN-DS will be available."}, {"title": "Data Preparation", "content": "It includes plantar pressure data from 135 subjects, with 94 diabetic patients having DFN (labeled as 1) and 41 without DFN (labeled as 0). We recorded data as each patient walked freely in a straight line for 5 minutes. After data cleaning we obtained 6,983 samples. Data has the shape x \u2208 R147\u00d716, where 147 is the time length and 16 is the channel number. In"}, {"title": "Baselines and Implementation Details", "content": "We compare MSSDA with various kinds of transfer learning methods that are wildly used in biomedical signal process, including Deep CORAL (D-CORAL) (Sun and Saenko 2016), Deep Adaptation Network (DAN) (Long et al. 2015a), Joint Adaptation Network (JAN) (Long et al. 2017), Maximum Classifier Discrepancy (MCD) (Saito et al. 2018), Domain-Adversarial Neural Network (DANN) (Ganin et al. 2016), Conditional Adversarial Domain Adaptation (CDAN) (Long et al. 2018) and Multi-scale spatio-temporal hierarchical network (MhNet) (Wu et al. 2022). Note that ERM refers to Empirical Risk Minimization, which means to train the model without transfer learning loss.\nAll methods are implemented using the PyTorch framework and reproduced on a GeForce GTX 4060. In Stage 1, we utilize a network with 4 layers of 1D CNN for contrastive learning on DFN-DS, and a network with 3 layers of 1D CNN for the FRA. In Stage 3, the feature extractor for DFN-DS remains consistent across all methods, comprising 7 layers of 1D CNN, while the FRA uses 3 layers of 1D CNN. Additionally, the domain discriminator and classifier frameworks in the domain-invariant methods are identical, featuring 3-layer fully connected networks for DFN-DS and 2-layer fully connected networks for FRA. All experiments are conducted after balancing the dataset using data reuse techniques.\nWhile in stage 1, we train the feature extractor Fo by contrastive learning for 5000 epochs using AdamW as the optimizer with initial learning rate of 5e-3, a batch size of 64 for DFN-DS. As for FRA, we set the initial learning rate as 1e-3 with a batch size of 32, other paremeters remain the same. In stage 3, same as other methods, we use Adam as our optimizer with a weight decay set to 1e-4. We select the learning rate lr from {1e-2, 8e-3, 5e-3} for best performance. Additionally, we select the weights of the domain adaptation loss from {0.2, 0.5, 1, 2} to get the best performance of the models, which is 1 for our method in FRA and 2 on DFN-DS. For both DFN-DS and FRA, M used in our method is set to 2."}, {"title": "Comparison Results", "content": "As shown in Table 1, comparing all methods on our proposed dataset DFN-DS, our approach excels across almost all metrics. In terms of recall, while our method ranks second, it still demonstrates substantial improvement compared to alternative approaches. Most importantly, our method achieves an accuracy that is at least 5.9% higher than other methods, which is quite remarkable. Note that the high precision of our method indicates that few patients without DFN are mistakenly predicted as having DFN. Specifically, the cluster number evaluated by BIC is 6 on DFN-DS.\nAs shown in Table 2, under challenging and real-time conditions, our approach excels across almost all metrics. We achieve the highest recall at the cost of a 0.001% lower accuracy compared to MhNet. Note that we label gaits with high fall risk as positive cases. Thus, recall is a critical metric in our context, as it emphasizes our ability to identify individuals at high risk of falling, which is significantly more important in real-world scenarios. Specifically, the number of clusters evaluated by BIC is 11 on FRA."}, {"title": "Further Analysis", "content": "In this section, we describe ablation studies to investigate the effect of different components of our method with M = 2 on DFN-DS."}, {"title": "Other Strategies to Select Sub-Source Domains", "content": "In Table 6, dis. stands for the distance calculated in Eq. (5) while sum refers to the total sum of Euclidean distances between all target samples and the cluster centers. All in usage refers that we use all sub-source domains to train the model, which corresponds to the method with MA only in Table 3. Considering comprehensively, our strategy is the best according to the results."}, {"title": "Sub-source Domain Performance", "content": "In Table 7, we firstly compare the performance of using different single sub-source domain as the source domain on"}, {"title": "Performance in Other Thresholds", "content": "In Figure 3, we compare the accuracy of our method with other domain-adversarial methods at different thresholds. These transfer learning methods improve accuracy on the target domain, but our method's accuracy drops more significantly as the threshold increases, likely due to excessive pursuit of transferability (Chen et al. 2019; Cui et al. 2022, 2020). This issue poses a challenge to the credibility of our approach and will be a focus of our future work."}, {"title": "Conlusion", "content": "We propose a dataset for Diabetic Foot Neuropathy (DFN) recognition, which includes continuous plantar pressure data from 94 DM patients with DFN and 41 DM patients without DFN. Previous works in biomedical signal processing either divide the dataset by patient or do not separate the dataset at all. Additionally, few of these studies carefully select data to avoid negative transfer. Our framework addresses these shortcomings. It divides the dataset based on convolutional feature statistics and employs a straightforward yet effective strategy to select appropriate sub-source domains for multi-source domain adaptation, simultaneously aligning the domain-specific distributions of each source-target domain pair. Extensive experiments on two plantar pressure datasets demonstrate the effectiveness of the proposed framework."}]}