{"title": "WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation", "authors": ["Chenghao Qian", "Yuhu Guo", "Yuhong Mo", "Wenjing Li"], "abstract": "In this work, we propose a novel approach, namely WeatherDG, that can generate realistic, weather-diverse, and driving-screen images based on the cooperation of two foundation models, i.e, Stable Diffusion (SD) and Large Language Model (LLM). Specifically, we first fine-tune the SD with source data, aligning the content and layout of generated samples with real-world driving scenarios. Then, we propose a procedural prompt generation method based on LLM, which can enrich scenario descriptions and help SD automatically generate more diverse, detailed images. In addition, we introduce a balanced generation strategy, which encourages the SD to generate high-quality objects of tailed classes under various weather conditions, such as riders and motorcycles. This segmentation-model-agnostic method can improve the generalization ability of existing models by additionally adapting them with the generated synthetic data. Experiments on three challenging datasets show that our method can significantly improve the segmentation performance of different state-of-the-art models on target domains. Notably, in the setting of \"Cityscapes to ACDC\", our method improves the baseline HRDA by 13.9% in mIoU. See the project page for more results:weatherDG.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation is a fundamental task in autonomous driving. Despite significant achievements in this field, existing models still face serious challenges when deploying in unseen domains due to the well-known domain shift problem [1]. In addition, this issue will be more serious when the unseen domains are with adverse weather conditions [2], [3], such as foggy, rainy, snowy, and nighttime scenarios [4].\nOne naive way to solve the above problem is collecting more diverse training data. However, labelling segmentation is a time-consuming process, as we need to annotate every pixel in an image. Hence, domain generalization becomes popular in solving the domain shift problem [5], in which the goal is to train a model that can generalize to unseen domains using only the given source data. Existing domain generalization methods can be generally divided into two categories: normalization [6], [7] and data augmentation [8], [9]. In this paper, we focus on the latter category, which is more flexible to different model structures and can be easily integrated with the former techniques.\nData augmentation for domain generalization aims to generate new, diverse and realistic synthetic images for aug-"}, {"title": "II. RELATED WORK", "content": "Domain Generalization for Semantic Segmentation (DGSS). DGSS aims to train deep neural networks that perform well on semantic segmentation tasks across mul- tiple unseen domains. Existing DGSS methods [5], [17], [18] attempt to address the domain gap problem through two main approaches: normalization and data augmentation. Normalization-based methods [6], [7] train by normalizing the mean and standard deviation of source features or whitening the covariance of these features. Data augmentation-based method [5] transform source images into randomly stylized versions, guiding the model to capture domain-invariant shape features as texture cues are replaced with random styles [19].\nFor instance, SHADE [20] creates new styles derived from the foundational styles of the source domain, while MoDify [5] utilizes difficulty-aware photometric augmentations.\nWith the advent of generative diffusion models, several studies [8], [9] have proposed content augmentation to enhance generalization. However, these approaches often either lack realism or fail to adequately consider variations in weather and lighting conditions.\nUnsupervised Domain Adaptation (UDA). UDA aims to boosts model performance on domain-specific data without needing labeled examples. Existing UDA techniques can be categorized into three main approaches: discrepancy mini- mization, adversarial training, and self-training. Discrepancy minimization reduces the differences between domains by using statistical distance functions [21]. Adversarial training involves a domain discriminator within a GAN framework to encourage domain-invariant input [22], feature [23] or output [24]. Self-training generates pseudo-labels for the target domain based on predictions made using confidence thresholds [25] or pseudo-label prototypes [26]. Recently, DATUM [27] introduces a one-shot domain adaptation method that generates a dataset using a single image from the target domain and pairs it with unsupervised domain adaptation training methods to bridge the sim-to-real gap. Futher, PODA [28] leveraged the capabilities of the CLIP model to enable zero-shot domain adaptation using prompts."}, {"title": "III. PROPOSED METHOD", "content": "A. Overview\nWeatherDG aims to generate images tailored to weather- specific autonomous driving scenes, enhancing semantic seg- mentation in challenging conditions. We begin by fine-tuning a diffusion model to adapt scene priors from the source domain, ensuring the generated images are within a driving scene (Section III-B). Next, we employ a procedural prompt generation method to create detailed prompts that enable the diffusion model to produce realistic and diverse weather and lighting effects (Section III-C). Additionally, we incorporate a probability-oriented sampling strategy for prompt generation. Following this, we use UDA training methods to leverage"}, {"title": "B. SD Fine-tuning", "content": "Recently, diffusion models have advanced the field of gen- erative domain adaptation, showcasing exceptional capabilities in producing photo-realistic images conditioned on text [30]. However, directly applying diffusion models in autonomous driving setting presents challenges due to shifts in scene priors like style and layout. For example, the model often generates artistic images or adopt a bird's eye view perspective, which is different from the images in real-world autonomous driving datasets, as shown in Figure 4b. When these images are incorporated during training, they can disrupt the knowledge the model has acquired from the labeled source domain, thereby harming the semantic segmentation performance. To address these issues, we finetune a diffusion model [16] to generate diverse images that retain content and layouts relevant to source domain. The input consists of a clean image paired with a corresponding prompt text, while the output is an image tailored to the autonomous driving domain.\nWe follow similar text-to-image diffusion model training procedures described in the relevant work [27], [34]. Specifi- cally, we use a unique identifier in the prompt to link priors to one single image choosen from the source domain dataset. For instance, the prompt \"A photo of V* driving scene\" associates with image patches cropped from the selected image, where V* identifies the scene, and \"driving scene\" describes it broadly. This prevents language drift and improves model performance [34]. The training procedure is presented in Fig- ure 4a. After training, the model captures scene priors through the unique identifier V and can generate new instances in similar contexts within autonomous driving datasets, as shown in Figure 4b."}, {"title": "C. Procedural Prompt Generation", "content": "To enable the diffusion model to generate weather and lighting effects, it is essential to integrate specific weather"}, {"title": "a) Instance sampler", "content": "During the inference phase, we can generate a specific instance by simply adding the instance name <CLS >to the prompt, which serves as the task for the instance sampler. However, we notice that the seman- tic segmentation evaluation metrics for \"thing\" classes are significantly lower compared to \"stuff\" classes in most au- tonomous driving datasets. This disparity can be attributed to the class imbalance problem where \"stuff\" classes have more occurances in the images than \u201cthing\u201d classes. In particular, this issue is exacerbated under adverse weather conditions as dynamic objects are less frequently present on the road in snowy or nightime scenes.\nTherefore, we aim to enhance the instance sampler agent with the capability to employ a probability-oriented sampling strategy, giving underrepresented classes under adverse weath- ers higher sampling probabilities. This increases the likelihood of these rare classes presented in the generated images. In detail, we first compute the semantic label distribution of i-th thing classes $E_i$ in a typical adverse weather dataset [35] by:\n\n$E_i = \\frac{D_i}{D_{thing}}$\n\nwhere $D_i$ represents number of semantics labels of each thing class, $D_{thing}$ stands for number of all thing classes. Then the sampling probability $P_i$ can be formulated as:\n\n$P_i = \\frac{\\frac{1}{E_i}}{\\sum_{i=1}^{X} \\frac{1}{E_i}}$\n\nThen, we allow the instance sampler agent to query the sampling probability to generate prompts. In this way, we can alleviate the shortage of instances that rarely appear in adverse weather conditions."}, {"title": "b) Scene composer", "content": "Intuitively, to enable the generation of images covering a wide range of weather effects and different times of the day, we can design the prompt template as 'A photo of <CLS >, <WEATHER >, <TIME >' to describe the image to be generated. Here, <CLS > refers to the classes in the typical autonomous driving dataset, <WEATHER > specifies the weather conditions, and <TIME > indicates the time of the day. To enhance the balance and diversity of the generated dataset, we include three common weather conditions: snowy, rainy, and foggy, along with two distinct times of day: daytime and nighttime. Each condition and time period is equally represented in the dataset to ensure"}, {"title": "c) Scene descripitor", "content": "To ensure the generated images reflect a variety of environmental effects and capture intri- cate details that resemble real-world scenes, we found that providing detailed descriptions of weather conditions and lighting is useful. For example, for the prompt \"A photo of a motorcycle in rainy, daytime\", we enhance the \"rainy\" aspect by including details such as \"under a grey and overcast sky with raindrops on the pavement\". Similarly, for \u201cdaytime\", we add specifics like \"streetlights casting a warm glow in the late morning rain\". Using the crafted prompt, the model incorporates elements like buildings, streetlights, a reflective road, and a misty sky into the scene, significantly enhancing the complexity and realism of the generated images. This enables the model trained with these images to generalize better to real-world scenarios.\nHowever, manually crafting these detailed descriptions can be labor-intensive. By using LLM, we can automatically gen- erate these nuanced elements, making the creation of highly detailed scenes more efficient. Consequently, we incorporate another LLM agent that receives prompts produced by Esc and uses the provided information to generate various detailed descriptions of scenes. In this way, we can enable a diversity of weather and lighting effects generation."}, {"title": "d) Procedural generation", "content": "The procedural prompt gen- eration pipeline is as follows: first, the instance sampler Eis queries the sampling probability to select the class to be"}, {"title": "D. Sample Generation & Model training", "content": "With the prompt T, we send it to the fine-tuned diffusion model to generate image samples. Although the model can produce realistic images featuring challenging weather and lighting conditions, it is still difficult to incorporate pixel- level information because the images lack semantic labels. To overcome this, we employ UDA methods such as DAFormer [1] and HRDA [36], which facilitate the adaptation of a seg- mentation model to an unlabeled target dataset. Specifically, we train these methods using Cityscapes [37] dataset as the source domain, and our generated dataset as the pseudo-target domain. The model is then evaluated on the target three real- world benchmarks [35], [38], [39]. The proposed framework can be adapted to UDA method easily, transforming it into a domain generalization method."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset\nWe conduct the experiments with domain generalization settings, using Cityscapes [37] as source domain dataset, ACDC [35], BDD100k [38] and DarkZurich [39] as test target domain dataset. The Cityscapes dataset comprises 2,975 images captured under standard weather conditions and dur- ing daytime, each accompanied by corresponding semantic segmentation labels for training. The ACDC dataset features images taken under various typical weather and lighting con- ditions, including snowy, rainy, foggy, and nighttime settings. BDD100k contains various weathers and geographic locations. DarkZurich consists of images captured at night, providing a challenging environment for nighttime visual perception tasks. For fine-tuning the diffusion model, we use a single image sampled from the Cityscapes dataset. For unsupervised domain adaptation (UDA) training, we utilize the Cityscapes training set along with images generated by our diffusion model. For evaluation, we use 406 images from the ACDC validation set, 1,000 images from the BDD100k validation set, and 50 images from the DarkZurich validation set."}, {"title": "B. Implementation details", "content": "We utilize the Stable Diffusion [16] as pretrained model and fine-tune it using DreamBooth [34] method. During fine- tuning stage, we randomly crop a patch of 512\u00d7512 size from the images selected from Cityscapes dataset. Then we pair it with customized prompt \u201ca photo of V driving scene\". We use mean square error loss to quantify the differences between the original image patch and the generated image during model"}, {"title": "V. EVALUATION AND DISCUSSION", "content": "A. Comparison with State-of-the-art\nWe evaluate WeatherDG against state-of-the-art domain generalization models using ResNet-50 [40] and MiT-B5 [41] as encoders. As shown in Table I, for models using ResNet-50 as encoder, our model consistently outperforms state-of-the- art methods with the highest average mIoU score. With MiT- B5 as backbone, our model exceeds the second-best model MIC [10] on the ACDC and DarkZurich datasets by over 10% and on the BDD100K dataset by 4.3% in mIoU performance, achieving the best semantic segmentation performance. In addition, we visualize our model's semantic segmentation results under challenging conditions and compare them with MIC. As shown in Figure 1, our model correctly segments sidewalks and sky in foggy and nighttime scenes. In the rainy scene, reflections on the road that are falsely recognized as vehicles by MIC are alleviated by our model. In the snowy scenario, our model successfully detects pedestrians on the road that MIC fails to recognize. These findings indicate our model's superior generalization capabilities compared to state-of-the-art methods in real-world challenging weather and lighting conditions."}, {"title": "B. Influence of UDA methods for training", "content": "C. Influence of SD Fine-tuning.\nTo demonstrate the influence of scene prior adaptation, we compare images generated by the plain stable diffusion model and our fine-tuned model. We use the same prompt, templated as \"A photo of [CLS]\" for each model to generate commonly seen objects in the autonomous driving dataset. The results in Figure 6 show that the plain stable diffusion tends to generate images with an artistic style or cinematic photography effects, as seen with \"truck,\u201d \u201cbicycle,\u201d \u201cmotorcycle,\" and \"bus\". For \"car\u201d and \u201ctrain\u201d, the images present different camera perspectives, such as bird's-eye view. Additionally, for \"traffic light\", \"traffic sign\", and \"person\", the model exhibits"}, {"title": "D. Influence of Procedural Prompt Generation", "content": "To illustrate the effectiveness of our procedural prompt generation, we compare images generated by text prompts created by different LLM agents in Figure 7. The results show that the instance sampler can generate the desired objects with a basic prompt such as \"A photo of [CLS]\u201d. By specifying the general category of weather and time of day, the scene composer only adds basic weather effects to the image, though these effects are subtle. Additionally, the generated samples primarily focus on the subject, often lacking scene details. When the scene descriptor crafts more detailed scene descriptions in the prompt, the model produces images with various realistic weather and lighting effects. As shown in third row, for snowy weather, the model generates a complex scene with heavy accumulated snow on the road and even snowflakes in the air. For rainy weather, the prompt generates reflections, raindrops, and a misty effect, indicating heavy rain. For nighttime, we can see that Esc fails to add sufficient nighttime lighting, but with prompts generated by ESD, the scene in the image exhibit a darker tone and more intricate details, creating a more realistic nighttime environment. In addition, we evaluate the mIoU performance of the DAFormer [1] trained with datasets generated using different LLM agents. In Table V, the results demonstrate that progressively refining"}, {"title": "E. Influence of Numbers of Generated Images", "content": "To investigate the impact of the generated dataset size on the performance of the segmentation model, we evaluate the mIoU performance of the DAFormer model trained with the generated dataset on the ACDC, BDD100K, and DarkZurich datasets. As shown in Figure 8, a substantial performance gain are observed as the number of images increases from"}, {"title": "VI. CONCLUSION", "content": "In this paper, we present WeatherDG, a novel approach for domain generalization in semantic segmentation under adverse weather conditions. By combining Stable Diffusion (SD) with a Large Language Model (LLM), our method enables automated generation of realistic images resembling real-world driving scenarios. Fine-tuning SD, along with pro- cedural prompt generation and a balanced strategy, creates di- verse weather effects and enhances tailed classes in generated images. These images, combined with source data, improve model generalization. Experiments across challenging datasets show that WeatherDG significantly boosts semantic segmenta- tion performance, setting a new benchmark for robustness in autonomous driving."}]}