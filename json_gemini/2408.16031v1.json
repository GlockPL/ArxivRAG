{"title": "EMP: Enhance Memory in Data Pruning", "authors": ["Jinying Xiao", "Ping Li", "Jie Nie", "Zhe Tang"], "abstract": "Recently, large language and vision models have shown strong performance, but due to high pre-training and fine-tuning costs, research has shifted towards faster training via dataset pruning. Previous methods used sample loss as an evaluation criterion, aiming to select the most \"difficult\" samples for training. However, when the pruning rate increases, the number of times each sample is trained becomes more evenly distributed, which causes many critical or general samples to not be effectively fitted. We refer to this as Low-Frequency Learning (LFL). In other words, LFL prevents the model from remembering most samples. In our work, we decompose the scoring function of LFL, provide a theoretical explanation for the inefficiency of LFL, and propose adding a memory term to the scoring function to enhance the model's memory capability, along with an approximation of this memory term. Similarly, we explore memory in Self-Supervised Learning (SSL), marking the first discussion on SSL memory. Using contrastive learning, we derive the memory term both theoretically and experimentally. Finally, we propose Enhance Memory Pruning (EMP), which addresses the issue of insufficient memory under high pruning rates by enhancing the model's memory of data, thereby improving its performance. We evaluated the performance of EMP in tasks such as image classification, natural language understanding, and model pre-training. The results show that EMP can improve model performance under extreme pruning rates. For example, in the CIFAR100-ResNet50 pre-training task, with 70% pruning, EMP outperforms current methods by 2.2%.", "sections": [{"title": "1. INTRODUCTION", "content": "Recently, deep learning technology has achieved significant breakthroughs in fields such as visual recognition and natural language processing. Although these models exhibit exceptional performance, they often require training and fine-tuning on large datasets, especially in the pre-training of large language models (LLMs). This process not only demands substantial computational resources but also consumes a considerable amount of time. Therefore, reducing the burden of training models on large-scale datasets has become increasingly important for promoting the application of deep learning technology across a broader range of fields.\nData pruning accelerates model pre-training by retaining a core subset of typical or general samples, thereby reducing the number of training iterations. Recently, dynamic pruning methods that adjust the retained dataset in real-time have become popular. These methods dynamically score data at each checkpoint, allowing pruned data to be retrained. Since these methods do not require additional training of a proxy network, they have started to gain popularity. Some of these methods use sample loss to construct , selecting the most \"difficult\" samples to train at each checkpoint. indicates that such methods can acquire more information and knowledge. However, we found that these methods are only effective at specific pruning rates. In other words, when the pruning rate is high, these methods exhibit poor performance. Moreover, on datasets that are difficult to fit, such as ImageNet-1K, the performance of low-frequency learning is comparable to, or even lower than, dynamic random pruning methods (see Table 2)."}, {"title": "3. METHODOLOGY", "content": "3.1. Problem Description\nGiven a large-scale dataset \\(D = \\{X,Y\\}\\) containing n training samples, where the input \\(X = \\{x^{(1)},...,x^{(n)}\\}\\), labels \\(Y = \\{y^{(1)}, ..., y^{(n)}\\}\\), and \\(f(Y|X,\\theta)\\) represents the network output parameterized by \\(\\theta\\). The goal of the data is to identify a subset \\(\\hat{D} = \\{\\hat{X},\\hat{Y}\\}\\), where \\(\\hat{D} \\subseteq D\\), thereby accelerating the model training process. It is important to note that the pruning rate s is expressed as \\(s = \\frac{||D - \\hat{D}||}{||D||}\\). In this paper, the data is generated by the distribution \\(p(x, y)\\), and we need to use several information quantities such as entropy: \\(H(X) = -E[log p(x)]\\), mutual information: \\(I(X; Y) = H(X) + H(Y) \u2013 H(X,Y)\\), and Kullback-Leibler divergence: \\(KL(p(x)||q(x)) = E_{x~p(x)}[log(p(x)/q(x))]\\).\n3.2. LFL Leads to Poor Memory\nPrevious works used loss values to score samples, and at each checkpoint, they selected the portion with the highest loss values according to the pruning rate. We believe this is a form of low frequency learning. Specifically, at the first checkpoint, they selected \\(D_1\\), and due to the model's gradient descent on \\(L(D_1,\\theta)\\) during the training process, these data are very likely not to be retained at the next checkpoint. On the contrary, the algorithm tends to retain the data in \\(\\hat{D}\\), at which point the chance of data being retained is averaged, as shown in Figure (1). However, repeated learning can strengthen the model's memory, for example, in language models, the model is more inclined to capture familiar phrases, recognized knowledge. Intuitively, LFL does not provide the model with the opportunity for repeated learning, leading to insufficient memory of the data by the model, especially under high pruning rates.\nIn our experiments, we tested the aforementioned viewpoint, as shown in Figure (3), where we established LFL under an extreme condition where each sample is trained the same number of times, which we refer to as Extreme Low-Frequency Learning (ELFL). It is not difficult to see from the graph that under high pruning rates, the model's training accuracy is not high. This situation is not unexpected, as it can be observed that in the early stages of training, due to the inconsistent gradient information from atypical and noisy examples, they may cancel each other out , at which point the model tends to learn general patterns, and the rise in training accuracy is not significant. In the later stages of training, the InfoBatch method, which uses loss as a criterion, and ELFL do not show a significant increase in training accuracy, which is particularly prominent in the challenging CIFAR100 dataset. Combining this with the long-tail theory, we believe that in the later stages, important or typical samples appear infrequently, and these samples require more repeated memorization, while the model only remembers those long-tail samples that are easier to remember. Therefore, based on the experiments, pruning methods that use loss as a criterion struggle to memorize the data.\nWe theoretically explain why selecting samples with the highest loss at each checkpoint can lead to insufficient model memory, which is a phenomenon observed in practice."}, {"title": "3.3. Enhancing Memory in SL", "content": "In Section 3.2, the inability of LFL to enable model memorization of data was analyzed. In the context of supervised learning, looking at Equation (1), we need to increase \\(I(\\theta; Y|X)\\) to enable the model to remember the data. To achieve this goal, it is first necessary to estimate \\(I(\\theta; Y|X)\\), which involves the independence of \\(\\theta\\) and Y. Previous work has used gradients and differential methods to estimate mutual information. In our work, we decompose \\(I(\\theta; Y|X)\\) as follows:\n\\[ I (\\theta; Y|X) = H (\\theta) + H (Y|X) \u2013 H (\\theta,Y|X) \\]\nGenerally, \\(H(\\theta,Y|X) \\leq H(\\theta)\\), and our proof can be found in the Appendix A. Thus, we can establish a lower bound for \\(I(\\theta; Y|X)\\):\n\\[ I (\\theta; Y|X) \\geq H (Y|X) \\]\nWhere the conditional entropy \\(H(Y|X)\\) is a measure of how uncertain Y is given X. For a sample \\((x^{(i)}, y^{(i)})\\), we represent the conditional entropy of a sample by \\(H(f(x^{(i)},\\theta))\\), where \\(f(x^{(i)}, \\theta)\\) denotes the output of the model with parameters \\(\\theta\\) for the input \\(x^{(i)}\\).\nTherefore, we establish the scoring function for the supervised learning scenario. For the data \\((x^{(i)}, y^{(i)})\\), we use the following formula to score it:\n\\[ score ((x^{(i)}, y^{(i)})) = L ((x^{(i)}, y^{(i)}), \\theta_k) + \\beta H (f (x^{(i)},\\theta_k)) \\]\nIn which, \\(\\theta_k\\) represents the current model parameters, \\(L(\\bullet)\\) generally refers to the cross-entropy function, and \\(\\beta\\) is an adjustable hyperparameter that balances the model's learning of general patterns and memorization of data. The core idea of this scoring function is to use the sample loss to enable the model to obtain as much information as possible, allowing the model to learn not only general patterns but also, through the second term, to prefer samples that are easy to remember, achieving higher model performance.\nLemma 3. For a subset \\(\\hat{D} \\subseteq D\\), if \\(D_k\\) satisfies:\n\\(\\forall (x^{(i)}, y^{(i)}) \\in \\hat{D}, \\forall (x^{(j)}, y^{(j)}) \\in D \u2013 \\hat{D}, score ((x^{(i)}, y^{(i)})) > score ((x^{(j)}, y^{(j)}))\\)\nThen, compared to LFL, the subset \\(\\hat{D}\\) will result in a model with a smaller upper bound on generalization error.\nProof. We assume that the subset generated by LFL is \\(\\hat{D}_{LFL}\\). Previous work has expressed the upper bound on generalization error, which in our context can be represented as:\n\\[ R (\\theta_{\\hat{D}}) \\leq \\hat{R} (\\hat{D}, \\theta_{\\hat{D}}) + \\epsilon \\]\nWhere \\(R(\\theta_{\\hat{D}})\\) is the expected loss, \\(\\hat{R} (\\hat{D}, \\theta_{\\hat{D}})\\) is the empirical risk on \\(\\hat{D}\\), which is the fitting loss, \\(\\epsilon\\) is a coefficient related to the model size and the size of the retained dataset, which remains fixed after the pruning rate and model architecture are set, and \\(\\theta_{\\hat{D}}\\) represents the optimal parameters obtained after optimization with \\(\\hat{D}\\). Our method enhances the model's memory, and likewise, improves the model's fitting capability. Therefore, based on the above experimental and theoretical analysis, we can conclude that:\n\\[ \\hat{R} (\\hat{D}, \\theta_{\\hat{D}}) < \\hat{R} (\\hat{D}_{LFL}, \\theta_{\\hat{D}_{LFL}}) \\]\nThus, Lemma 3 is proven. Compared to LFL that uses sample loss, our scoring function results in a model with a smaller upper bound on generalization error."}, {"title": "3.4. Enhancing Memory in SSL", "content": "In SSL, we mainly discuss Contrastive Learning (CL). CL learns representations of data by focusing on positive samples that are close to each other (such as augmentations from the same image) and excluding negative samples (such as augmentations from different images). By promoting the closeness of positive examples and maximizing the separation between negative examples in the latent space, it learns the representation of the data.\nIn CL, a relatively simple and practical framework is SimCLR , which is an easy-to-implement visual representation contrastive learning framework. We mainly discuss the training process of the basic encoder \\(f(\\bullet) : R^{L \\times 1} \\rightarrow R^{H \\times 1}\\), where \\(f(\\bullet)\\) typically uses a ResNet architecture. For a sample \\(x \\in R^{L \\times 1}\\), two independent data augmentation operators can be sampled to obtain \\(x_i, x_j \\in R^{L \\times 1}\\). The basic encoder maps the two data to intermediate representations \\(f(x_i), f(x_j)\\). During the training phase, a projection head \\(g(\\bullet)\\) is often used to obtain the outputs \\(g(f(x_i)), g(f(x_j))\\). And training is conducted using the NT-Xent loss to decrease.\nTo our knowledge, this is the first time discussing data memory in CL. As discussed in Section 3.2, under high pruning rates, it is necessary to strengthen the model's memory of data to achieve more efficient performance. Since \\(I(\\theta; Y|X)\\) is extracted from the cross-entropy function, and CL training is independent of data labels Y, therefore, directly transferring the memory enhancement methods from SL to CL is not effective.\nPrevious work has explored model memory, where in SL, they compared the model's probability of a sample before and after the sample's participation in training. Since in CL, the model primarily learns the intrinsic features of the data without involving data labels in training, we extend this concept to a more general form:\n\\[ mem (x) = \\frac{loss (x_i, x_j)_{A (D\u2212x)}}{loss (x_i, x_j)_{A (D)}} \\]\nIn which, \\(A(D)\\) represents the optimization algorithm utilizing the entire dataset, and the construction of the \\(loss()\\) function is worth discussing.\nIt is worth noting that in CL, if the model fits the sample x well, it implies that the model will judge the two data generated from x, \\(x_i\\) and \\(x_j\\), as positive samples, and the NT-Xent loss function constructed by \\(x_i\\) and \\(x_j\\) will also decrease. In other words, when the model's memory of the sample x is enhanced, it means that for \\(x_i\\) and \\(x_j\\), the model's output tends to be the same, which is determined by the SimCLR framework. points out that whether it is a linear or nonlinear projection head, \\(f(x_i)\\) and \\(f(x_j)\\) form and maintain more information, and the hidden layers before projection are often richer representations. Therefore, we can explicitly consider that when the sample x is remembered, the basic encoder's output for \\(x_i\\) and \\(x_j\\) will be as similar as possible, so the \\(loss()\\) can be represented as:\n\\[ loss (x_i, x_j) = || f(x_i) \u2212 f (x_j)||_2 \\]\nThe notation \\(|| ||_2\\) represents the L2 norm. The construction of the loss function \\(loss()\\) based on \\(f(x_i)\\) and \\(g(f(x))\\) will be discussed in Section 4.3.\nHowever, for the first term of Equation (11), we found that in datasets with a sufficiently large number of samples, for most samples, this term is almost uniform.  we conducted two experiments: a) We randomly selected 50 samples \\(\\{x^{(1)}, ..., x^{(i)}, ..., x^{(50)}\\}\\) and examined the \\(\\frac{loss(x_i, x_j)}{A(D \u2013 B)}\\) for each sample. b) To explore the impact of the number of sampled data on the original dataset, we randomly sampled a certain proportion of samples from the entire dataset, denoted as \\(B_v = r \\times D\\), where B is the set of samples, and we investigated the relationship between the value of \\(E_{x \\in B}\\frac{loss(x_i, x_j)}{A(D - B)}\\) and the proportionality factor r.\nAccording to the experiments, when a single sample is removed, the variation of \\(loss(x_i, x_j)\\) with \\(A(D \u2013 x)\\) is not significant. We attribute this to the model having already learned sufficient knowledge from most of the data, and thus is not sensitive to these individual samples. Therefore, we can fully consider that for different data, the variation of \\(\\frac{loss(x_i, x_j)}{A(D - B)}\\) can be neglected. Moreover, as the sampling ratio r increases, this value of \\(\\frac{loss(x_i, x_j)}{A(D - B)}\\) becomes more pronounced, which makes the first term in Equation (11) non-negligible. In extreme cases, the performance of the data pruning method will drop sharply, reflecting the difficulty of data pruning in CL at extreme pruning rates, which will be our future work target.\nBased on the above analysis, we disregard the first term of Equation (11), and drawing inspiration from the addition of a memory term to the scoring function in SL, we construct the scoring function as follows:\n\\[ score ((x^{(i)}, y^{(i)})) = NX (g ( f (x_i)), g (f (x_j))) \u2013 \\beta loss (x_i, x_j) \\]\nWhere \\(NX(\\bullet)\\) represents the NT-Xent loss, and \\(\\beta\\) is an adjustable hyperparameter."}, {"title": "4. EXPERIMENT", "content": "In the following sections, we validate the effectiveness of our theoretical results and the proposed dataset pruning method through experiments. In Section 4.1, we compare EMP with several other baseline methods on SL, including image classification tasks and a range of natural language tasks. In Section 4.2, we verify the high efficiency of EMP on SSL. In Section, we conduct a series of ablation experiments to validate the effectiveness of our theoretical results. In Section 4.4, we analyze the generalization performance of EMP using a one-dimensional linear interpolation method. It is worth noting that other baseline methods include static pruning: CD , Herding, DeepFool , Last Confidence , Glister and dynamic pruning: InfoBatch , e-greedy, and UCB . It is important to note that EMP is also a dynamic pruning method, so we mainly compare it with other dynamic pruning methods. To eliminate the influence of randomness, each of our experiments was run 5 times, and the average was taken. Our dataset introduction and experimental details are in the Appendix B and C."}, {"title": "4.1. Performance of EMP in SL", "content": "CIFAR. We conducted comparisons on both CIFAR-10 and CIFAR-100. In this work, similar to InfoBatch , we employed an annealing technique, specifically, using the entire dataset for training towards the end of the training process. More details are shown in Section 4.3. We presented results from other methods in recent years in Table (1). EMP achieved lossless performance on CIFAR-100, even slightly higher than the baseline. Notably, EMP significantly surpassed previous static pruning methods and led other dynamic pruning methods at high pruning rates. Specifically, on the CIFAR-100 dataset, EMP exceeded other methods by at least 1.3% under any pruning rate, and even by at least 2.1% at a 70% pruning rate.\nFurthermore, Figures (6) and (7) report the variation curves of EMP under different pruning rates. Surprisingly, at low pruning rates, EMP does not perform as well as expected. We believe that at low pruning rates, the model can utilize a larger amount of the dataset through LFL to learn general knowledge and demonstrate strong generalization capabilities. At this time, the model does not have a high demand for repeated learning of key samples. However, as the pruning rate increases and the number of times each sample is trained becomes limited, LFL shows limited performance. In contrast, EMP can extract key samples for the model to learn, strengthen the model's memory of general knowledge, and thus achieve efficient performance.\nImageNet-1K. To explore the performance of EMP on a large-scale dataset, we trained ResNet50 on ImageNet-1K, with results as shown in Table 2. Notably, the UCB and Greedy methods demonstrated performance similar to that of dynamic random pruning, and at high pruning rates, dynamic random pruning surpassed them. We believe that on ImageNet-1K, due to some noisy samples , they are difficult to fit correctly, hence LFL tends to select these noisy samples, leading to suboptimal performance of LFL. As expected, on the challenging ImageNet-1K dataset with high fitting difficulty, EMP showed efficient performance both at low and high pruning rates.\nGLUE. In addition to image classification tasks, natural language understanding tasks are also within the scope of our investigation. Specifically, we used the BERT-base pre-trained model for fine-tuning on the GLUE benchmark . The experimental results are reported in Table (3), and it is evident that EMP leads in most cases, even outperforming the case without pruning. Notably, at pruning rates of 50% and 70%, for datasets with a larger number of samples such as MNLI (393K) and QQP (363K), EMP demonstrates good performance. Specifically, at a 70% pruning rate, EMP leads other methods by at least 1% on the MNLI dataset. On the dataset QNLI with a moderate number of samples (108K), EMP also shows superior performance, exceeding other methods by 0.6%-1.9% at a 70% pruning rate. This proves that our method is also effective on natural language tasks."}, {"title": "4.2. Performance of EMP in SSL", "content": "Recently, Large Language Models (LLMs) have demonstrated significant performance , pretraining on vast datasets to obtain models with general knowledge. Visually, various visual pretrained models are also gaining popularity. To explore the performance of EMP in model pretraining, we experimentally verified it on CL using the SimCLR framework . In this work, we specifically pruned the data during the pretraining phase to investigate the practical effectiveness of EMP.\nWe conducted explorations on the CIFAR10/100 datasets, with results shown in Table (4). Specifically, for the CIFAR10 dataset, EMP outperformed other methods at different pruning ratios, especially at pruning ratios of 50% and 70%, where EMP achieved accuracies of 82.16% and 80.59%, respectively, significantly higher than other methods. In the CIFAR100 dataset, EMP also demonstrated superior performance across all pruning ratios, with accuracies of 56.37%, 51.99%, and 48.21% at pruning ratios of 30%, 50%, and 70%, respectively, showing a notable improvement over other methods such as InfoBatch.\nOverall, in CL, the EMP method is capable of maintaining high model performance while reducing the amount of training data, especially outstanding at high pruning ratios. This indicates that the EMP method has strong robustness and effectiveness in dynamic dataset pruning."}, {"title": "4.3. Ablation Experiment", "content": "Annealing Technique. In SSL, for the CIFAR10/100 datasets, we employed an annealing technique. Specifically, we determined a hyperparameter a to control the degree of annealing, and a is defined as follows:\n\\[ \u03b1 = \\frac{annealing \\, epochs}{total \\, epochs} \\]\nIt can be seen that the degree of annealing increases with the increase of a, but this also brings corresponding additional overhead. We believe that when the model has acquired sufficient knowledge in the non-annealing phase, the positive effects brought by the annealing phase will be less significant. In other words, the annealing phase not only exists as a technique to improve accuracy but also serves as an effective measure of the validity of the data pruning algorithm. Similarly, InfoBatch also uses this technique, and we mainly compare it with this method. Our results are reported in Figure (8). It is not difficult to see that at lower pruning rates, both methods are robust to annealing, and when the pruning rate increases, the shortcomings of LFL become apparent. Although EMP's accuracy drops without the annealing phase, InfoBatch's drop is more dramatic. Therefore, we can conclude that under any circumstances, EMP is robust to the annealing technique, which means that EMP enables the model to remember more samples and gain more knowledge during the early to mid-training phase.\nMemory Term Construction. In Section 3.4, we posit that the hidden layers before projection often contain richer representations, hence utilizing the pre-projection \\(f(x_i)\\) and \\(f(x_j)\\) to construct the memory term. In this work, we construct memory terms for both the hidden layer outputs before and after projection. Specifically, we compare the performance of two types of memory terms, \\(loss_f\\) and \\(loss_g\\), which are represented as follows:\n\\[ loss_f = || f(x_i) \u2212 f(x_j)||_2 \\]\n\\[ loss_g = ||g(f(x_i)) - g(f(x_j))||_2 \\]\nThis corresponds to Equation (11) in Section 3.4. Our results are reported in Table (5). It can be observed that using \\(loss_g\\) as the memory term results in a decrease in performance, which confirms our previous idea. In other words, since \\(f(x_i)\\) and \\(f(x_j)\\) form and maintain more information, constructing the memory term with the more informative \\(f(x_i)\\) and \\(f(x_j)\\) is more effective.\nLabel Noise. Label noise is a common challenge in real-world applications. Enhancing the robustness of algorithms to label noise during the data pruning process is a key issue . In this section, we investigate the robustness of EMP to label noise by conducting comparative experiments on CIFAR10 and CIFAR100 with synthetic label noise. Specifically, we introduce label noise into the two datasets by randomly replacing the labels of a certain percentage of training data with all possible labels, which mimics the real-life scenario where researchers may misjudge images . Our results are reported in Figure (9). It can be observed that EMP still maintains high performance even with the addition of 20% noise and leads other methods. Notably, the Greedy and UCB methods are not robust to noise, and are even outperformed by random dynamic pruning. We believe that these noisy data are difficult to fit during training, hence their scores remain high, leading to the retention of noisy data and resulting in low model performance. Overall, EMP maintains high performance in the presence of noise, indicating that EMP indeed retains key or general samples, thus demonstrating robustness to noise.\nHyperparameter \\(\\beta\\). In our method, both in SL and SSL, we have added a memory term to the scoring function, which is scaled by the hyperparameter \\(\\beta\\). Our experiments indicate that the best results are achieved when \\(\\beta\\) is set to 5. To explore the impact of \\(\\beta\\) on the experimental results, we conducted experiments with different values of \\(\\beta\\), and the results are reported in Figure (10). It is evident that even with different values of \\(\\beta\\), EMP still leads in most cases, demonstrating that the memory term we added can be effective. When \\(\\beta\\) exceeds the value we set, performance declines, which can be interpreted as the model having sufficient memory for these key samples but lacking in the overall information from the dataset, leading to a decrease in performance."}, {"title": "4.4. Generalization Analysis", "content": "To further investigate the effectiveness of EMP, we analyze its generalization capability. We use one-dimensional linear interpolation to examine how EMP affects the loss landscape while enhancing the model's memory ability. Previous work suggests that better model generalization is associated with flat minima in the loss landscape. Following the method proposed in , we inspect the loss landscape. Specifically, we assess the performance of models with parameters \\((1 \u2013 \\epsilon)\\theta_{init} + \\epsilon \\theta_{T}\\), where \\(\\theta_{init}\\) is the initial model, and \\(\\theta_{T}\\) is the converged model after optimization with different dataset pruning algorithms. In contrast to EMP, when we retain the samples with low scores in EMP (EMP-reverse), both the loss landscape and the accuracy curve present an opposite landscape to that of EMP. We have every reason to believe that EMP indeed enhances the model's generalization capability."}, {"title": "5. CONCLUSION", "content": "In this work, we proposed the EMP method. We started with the model's memory and theoretically explained the issue of insufficient memory in low-frequency learning. To address this, we identified the memory terms in the model's pre-training and fine-tuning based on theoretical and experimental analysis. EMP solves the problem of insufficient model memory at high pruning rates by adding a memory term to the scoring function. According to experiments, EMP leads the state-of-the-art methods in image classification tasks, natural language understanding tasks, and model pre-training tasks, showing a significant advantage at high pruning rates.\nLimitations and Future Works. Previous work has suggested that different layers within a network exhibit varying training dynamics, which also relates to model memory. Shallow layers tend to learn general knowledge, while deeper layers tend to learn task-specific knowledge, with memory primarily occurring in deeper layers. We have not yet explored this aspect. Designing or exploring the memory mechanisms and memory terms for structurally different models or different layers within a model is one of our future research directions."}, {"title": "A. Proof", "content": "In Section 3.2, we decomposed \\(I(\\theta; Y|X)\\) and derived the following:\n\\[ I (\\theta; Y|X) = H (\\theta) + H (Y|X) \u2013 H (\\theta,Y|X) \\] \nHere, we demonstrate that \\(H(\\theta,Y|X) \\leq H(\\theta)\\), thereby establishing a lower bound for \\(I(\\theta; Y|X)\\). The proof process is as follows.\nThe Data Processing Inequality states that for any random variables X, Y, Z, the following is true:\n\\[ H (Y | X) \\geq H (Y | X, Z) \\]\nSubstituting Z = \\(\\theta\\) into the above formula, we get:\n\\[ H (Y | X) \\geq H (Y | X, \\theta) \\]\nThe chain rule for joint entropy can be written as:\n\\[ H (\\theta,Y | X) = H (\\theta | X) + H (Y | \\theta, X) \\]\nCombining Equation (A.3) with Equation (A.4), we can obtain:\n\\[ H (Y | \\theta, X) \\leq H (Y | X) \\]\nAccording to the definition of joint entropy, expanding \\(H(\\theta, Y|X)\\) yields:\n\\[ H (\\theta, Y | X) = H (\\theta | X) + H (Y | \\theta, X) \\leq H (\\theta | X) + H (Y | X) \\]\nSince entropy is non-negative, therefore:\n\\[ H (\\theta, Y | X) \\leq H (\\theta | X) \\]\nSince \\(H (\\theta|X)\\) is the uncertainty of \\(\\theta\\) given X, it cannot be greater than the overall uncertainty of \\(\\theta\\), \\(H(\\theta)\\), thus we obtain:\n\\[ H (\\theta | X) \\leq H (\\theta) \\]\nCombining Equation (A.7) with Equation (A.8), we conclude:\n\\[ H (\\theta,Y | X) \\leq H (\\theta) \\]"}]}