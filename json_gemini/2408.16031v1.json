{"title": "EMP: Enhance Memory in Data Pruning", "authors": ["Jinying Xiao", "Ping Li", "Jie Nie", "Zhe Tang"], "abstract": "Recently, large language and vision models have shown strong performance, but due to high pre-training and fine-tuning costs, research has shifted towards faster training via dataset pruning. Previous methods used sample loss as an evaluation criterion, aiming to select the most \"difficult\" samples for training. However, when the pruning rate increases, the number of times each sample is trained becomes more evenly distributed, which causes many critical or general samples to not be effectively fitted. We refer to this as Low-Frequency Learning (LFL). In other words, LFL prevents the model from remembering most samples. In our work, we decompose the scoring function of LFL, provide a theoretical explanation for the inefficiency of LFL, and propose adding a memory term to the scoring function to enhance the model's memory capability, along with an approximation of this memory term. Similarly, we explore memory in Self-Supervised Learning (SSL), marking the first discussion on SSL memory. Using contrastive learning, we derive the memory term both theoretically and experimentally. Finally, we propose Enhance Memory Pruning (EMP), which addresses the issue of insufficient memory under high pruning rates by enhancing the model's memory of data, thereby improving its performance. We evaluated the performance of EMP in tasks such as image classification, natural language understanding, and model pre-training. The results show that EMP can improve model performance under extreme pruning rates. For example, in the CIFAR100-ResNet50 pre-training task, with 70% pruning, EMP outperforms current methods by 2.2%.", "sections": [{"title": "1. INTRODUCTION", "content": "Recently, deep learning technology has achieved significant breakthroughs in fields such as visual recognition (Shamshad, Khan, Zamir, Khan, Hayat, Khan and Fu, 2023; Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark et al., 2021; Chen, Yao, Chen, Zhang and Liu, 2023) and natural language processing (Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et al., 2023; Devlin, 2018; Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et al., 2023). Although these models exhibit exceptional performance, they often require training and fine-tuning on large datasets, especially in the pre-training of large language models (LLMs). This process not only demands substantial computational resources but also consumes a considerable amount of time. Therefore, reducing the burden of training models on large-scale datasets has become increasingly important for promoting the application of deep learning technology across a broader range of fields.\nData pruning accelerates model pre-training by retaining a core subset of typical or general samples, thereby reducing the number of training iterations. Recently, dynamic pruning methods that adjust the retained dataset in real-time have become popular (Qin, Wang, Zheng, Gu, Peng, Zhou, Shang, Sun, Xie, You et al.; Raju, Daruwalla and Lipasti, 2021; Xiao, Li and Nie, 2024). These methods dynamically score data at each checkpoint, allowing pruned data to be retrained. Since these methods do not require additional training of a proxy network, they have started to gain popularity. Some of these methods use sample loss to construct (Qin et al.; Raju et al., 2021), selecting the most \"difficult\" samples to train at each checkpoint. Raju et al. (2021) indicates that such methods can acquire more information and knowledge. However, we found that these methods are only effective at specific pruning rates. In other words, when the pruning rate is high, these methods exhibit poor performance. Moreover, on datasets that are difficult to fit, such as ImageNet-1K, the performance of low-frequency learning is comparable to, or even lower than, dynamic random pruning methods (see Table 2)."}, {"title": "2. RELATED WORK", "content": "Static Pruning. Static pruning aims to select a compact core subset before training. Toneva, Sordoni, des Combes, Trischler, Bengio and Gordon analyzed the forgetting of samples during the training process and assessed their forgetfulness in training to eliminate unforgettable samples, Xia, Liu, Yu, Shen, Han and Liu (2022) selected a moderate core set based on the distance of data points to the center, Aljundi, Lin, Goujaud and Bengio (2019) utilized the maximum diversity of samples in the gradient space for selection, Yang, Xie, Peng, Xu, Sun and Li ingeniously used the influence function, constructing a core subset based on the impact of samples on the model's generalization ability. Tan, Wu, Du, Chen, Wang, Wang and Qi (2024) designed a first-order gradient approximation to assess the impact of samples on the optimal empirical risk. Although these methods select an efficient core subset, the core subset, as a form of a posteriori knowledge, must be learned through the optimization of one or more proxy models to understand the characteristics and distribution of the data. For example, in Yang et al., SENet and ResNet18 are used as proxy models to accelerate the training of ResNet50. Although these proxy models are smaller in scale, they require training on the entire dataset, which inevitably brings additional overhead. More importantly, these methods result in a core subset with poor generalizability. For instance, in Yang et al., for different specifications of ResNet, the proxy model also requires additional selection and optimization.\nDynamic Pruning. To address the aforementioned additional overhead, Raju et al. (2021) argued that the optimal dynamic scoring is closely integrated with the training trajectory of the model. They categorized data into three types based on the number of times samples were selected and found that these samples are highly variable and can transition within the training dynamics. They first performed dataset pruning on the retained data's loss at each checkpoint during training without a proxy network, and from their conclusions, dynamic pruning is always superior to static pruning, even random selection. In Qin et al., soft pruning was proposed; they believed that for a dataset with N samples, hard pruning with an unchanged pruning rate requires a complexity of O(log N) for sorting the scores, while soft pruning only requires O(1). Although the sorting cost is reduced, experimentally, soft pruning cannot determine the true pruning ratio, thereby explicitly increasing the training cost of the model. It is worth noting that before us, Qin et al.; Raju et al. (2021) provided state-of-the-art performance in dynamic pruning."}, {"title": "3. METHODOLOGY", "content": null}, {"title": "3.1. Problem Description", "content": "Given a large-scale dataset $D = {X,Y}$ containing n training samples, where the input $X = {x^{(1)},...,x^{(n)}}$, labels $Y = {y^{(1)}, ..., y^{(n)}}$, and $f(Y|X,\\theta)$ represents the network output parameterized by $\\theta$. The goal of the data is to identify a subset $\\hat{D} = {\\hat{X},\\hat{Y}}$, where $\\hat{D} \\subseteq D$, thereby accelerating the model training process. It is important to note that the pruning rate s is expressed as $s = \\frac{||D-\\hat{D}||}{||D||}$. In this paper, the data is generated by the distribution p(x, y), and we need to use several information quantities such as entropy: $H(X) = -E[log p(x)]$, mutual information: $I(X; Y) = H(X) + H(Y) \u2013 H(X,Y)$, and Kullback-Leibler divergence: $KL(p(x)||q(x)) = E_{x~p(x)}[log(p(x)/q(x))]$."}, {"title": "3.2. LFL Leads to Poor Memory", "content": "Previous works (Qin et al.; Raju et al., 2021) used loss values to score samples, and at each checkpoint, they selected the portion with the highest loss values according to the pruning rate. We believe this is a form of low frequency learning. Specifically, at the first checkpoint, they selected $D_1$, and due to the model's gradient descent on $L(D_1,\\theta)$ during the training process, these data are very likely not to be retained at the next checkpoint. On the contrary, the algorithm tends to retain the data in $D_{\u00f4n}$, at which point the chance of data being retained is averaged, as shown in Figure (1). However, repeated learning can strengthen the model's memory (Wei et al., 2024), for example, in language models, the model is more inclined to capture familiar phrases, recognized knowledge (Zhang et al., 2023). Intuitively, LFL does not provide the model with the opportunity for repeated learning, leading to insufficient memory of the data by the model, especially under high pruning rates.\nIn our experiments, we tested the aforementioned viewpoint, as shown in Figure (3), where we established LFL under an extreme condition where each sample is trained the same number of times, which we refer to as Extreme Low-Frequency Learning (ELFL). It is not difficult to see from the graph that under high pruning rates, the model's training accuracy is not high. This situation is not unexpected, as it can be observed that in the early stages of training, due to the inconsistent gradient information from atypical and noisy examples, they may cancel each other out (Stephenson, Ganesh, Hui, Tang, Chung et al.; Gu and Tresp, 2019), at which point the model tends to learn general patterns (Wei et al., 2024), and the rise in training accuracy is not significant. In the later stages of training, the InfoBatch method, which uses loss as a criterion, and ELFL do not show a significant increase in training accuracy, which is particularly prominent in the challenging CIFAR100 dataset. Combining this with the long-tail theory (Feldman, 2020; Feldman and Zhang, 2020), we believe that in the later stages, important or typical samples appear infrequently, and these samples require more repeated memorization, while the model only remembers those long-tail samples that are easier to remember. Therefore, based on the experiments, pruning methods that use loss as a criterion struggle to memorize the data."}, {"title": "3.3. Enhancing Memory in SL", "content": "In Section 3.2, the inability of LFL to enable model memorization of data was analyzed. In the context of supervised learning, looking at Equation (1), we need to increase $I(\\theta; Y|X)$ to enable the model to remember the data. To achieve this goal, it is first necessary to estimate $I(\\theta; Y|X)$, which involves the independence of $\\theta$ and Y. Previous work has used gradients (Harutyunyan et al., 2020) and differential methods (Sordoni, Dziri, Schulz, Gordon, Bachman and Des Combes, 2021) to estimate mutual information. In our work, we decompose $I(\\theta; Y|X)$ as follows:\n$I (\\theta; Y|X) = H (\\theta) + H (Y|X) \u2013 H (\\theta,Y|X)$\nGenerally, $H(\\theta,Y|X) \u2264 H(\\theta)$, and our proof can be found in the Appendix A. Thus, we can establish a lower bound for $I(\\theta; Y|X)$:\n$I (\\theta; Y|X) \u2265 H (Y|X)$\nWhere the conditional entropy $H(Y|X)$ is a measure of how uncertain Y is given X. For a sample $(x^{(i)}, y^{(i)})$, we represent the conditional entropy of a sample by $H(f(x^{(i)},\\theta))$, where $f(x^{(i)}, \\theta)$ denotes the output of the model with parameters $\\theta$ for the input $x^{(i)}$.\nTherefore, we establish the scoring function for the supervised learning scenario. For the data $(x^{(i)}, y^{(i)})$, we use the following formula to score it:\n$score ((x^{(i)}, y^{(i)})) = L ((x^{(i)}, y^{(i)}), \\theta_k) + \\beta H (f (x, \\theta_k))$\nIn which, $\\theta_k$ represents the current model parameters, $L(\\bullet)$ generally refers to the cross-entropy function, and $\\beta$ is an adjustable hyperparameter that balances the model's learning of general patterns and memorization of data. The core idea of this scoring function is to use the sample loss to enable the model to obtain as much information as possible, allowing the model to learn not only general patterns but also, through the second term, to prefer samples that are easy to remember, achieving higher model performance."}, {"title": "3.4. Enhancing Memory in SSL", "content": "In SSL, we mainly discuss Contrastive Learning (CL). CL learns representations of data by focusing on positive samples that are close to each other (such as augmentations from the same image) and excluding negative samples (such as augmentations from different images) (Wu, Chen, Wu, Shi, Wang and He, 2024). By promoting the closeness of positive examples and maximizing the separation between negative examples in the latent space, it learns the representation of the data (Gui, Chen, Zhang, Cao, Sun, Luo and Tao, 2024).\nIn CL, a relatively simple and practical framework is SimCLR (Chen, Kornblith, Norouzi and Hinton, 2020), which is an easy-to-implement visual representation contrastive learning framework. We mainly discuss the training process of the basic encoder $f(\\bullet) : R^{L\u00d71} \u2192 R^{H\u00d71}$, where $f(\\bullet)$ typically uses a ResNet architecture (He, Zhang, Ren and Sun, 2016). For a sample $x \u2208 R^{L\u00d71}$, two independent data augmentation operators can be sampled to obtain $x_i, x_j \u2208 R^{L\u00d71}$. The basic encoder maps the two data to intermediate representations $f(x_i), f(x_j)$. During the training phase, a projection head $g(\\bullet)$ is often used to obtain the outputs $g(f(x_i)), g(f(x_j))$. And training is conducted using the NT-Xent loss to decrease.\nTo our knowledge, this is the first time discussing data memory in CL. As discussed in Section 3.2, under high pruning rates, it is necessary to strengthen the model's memory of data to achieve more efficient performance. Since $I(\\theta; Y|X)$ is extracted from the cross-entropy function, and CL training is independent of data labels Y, therefore, directly transferring the memory enhancement methods from SL to CL is not effective.\nPrevious work (Feldman, 2020) has explored model memory, where in SL, they compared the model's probability of a sample before and after the sample's participation in training. Since in CL, the model primarily learns the intrinsic features of the data without involving data labels in training, we extend this concept to a more general form:\n$mem (x) = \\frac{loss (x_i, x_j)}{A (D\u2212x)} = \\frac{loss (x_i, x_j)}{A (D)}$\nIn which, $A(D)$ represents the optimization algorithm utilizing the entire dataset, and the construction of the $loss(\\bullet)$ function is worth discussing.\nIt is worth noting that in CL, if the model fits the sample x well, it implies that the model will judge the two data generated from x, $x_i$ and $x_j$, as positive samples, and the NT-Xent loss function constructed by $x_i$ and $x_j$ will also decrease. In other words, when the model's memory of the sample x is enhanced, it means that for $x_i$ and $x_j$, the model's output tends to be the same, which is determined by the SimCLR framework. Chen et al. (2020) points out that whether it is a linear or nonlinear projection head, $f(x_i)$ and $f(x_j)$ form and maintain more information, and the hidden layers before projection are often richer representations. Therefore, we can explicitly consider that when the"}, {"title": "4. EXPERIMENT", "content": "In the following sections, we validate the effectiveness of our theoretical results and the proposed dataset pruning method through experiments. In Section 4.1, we compare EMP with several other baseline methods on SL, including image classification tasks and a range of natural language tasks. In Section 4.2, we verify the high efficiency of EMP on SSL. In Section, we conduct a series of ablation experiments to validate the effectiveness of our theoretical results. In Section 4.4, we analyze the generalization performance of EMP using a one-dimensional linear interpolation method. It is worth noting that other baseline methods include static pruning: CD (Agarwal, Arora, Anand and Arora, 2020), Herding (Welling, 2009), DeepFool (Ducoffe and Precioso, 2018), Last Confidence (Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis, Liang, Leskovec and Zaharia), Glister (Killamsetty, Sivasubramanian, Ramakrishnan and Iyer, 2021), and dynamic pruning: InfoBatch (Qin et al.), e-greedy, and UCB (Raju et al., 2021). It is important to note that EMP is also a dynamic pruning method, so we mainly compare it with other dynamic pruning methods. To eliminate the influence of randomness, each of our experiments was run 5 times, and the average was taken. Our dataset introduction and experimental details are in the Appendix B and C."}, {"title": "5. CONCLUSION", "content": "In this work, we proposed the EMP method. We started with the model's memory and theoretically explained the issue of insufficient memory in low-frequency learning. To address this, we identified the memory terms in the model's pre-training and fine-tuning based on theoretical and experimental analysis. EMP solves the problem of insufficient model memory at high pruning rates by adding a memory term to the scoring function. According to experiments, EMP leads the state-of-the-art methods in image classification tasks, natural language understanding tasks, and model pre-training tasks, showing a significant advantage at high pruning rates.\nLimitations and Future Works. Previous work (Yosinski, Clune, Bengio and Lipson, 2014; Raghu, Gilmer, Yosinski and Sohl-Dickstein, 2017) has suggested that different layers within a network exhibit varying training dynamics, which also relates to model memory. Shallow layers tend to learn general knowledge, while deeper layers tend to learn task-specific knowledge, with memory primarily occurring in deeper layers (Stephenson et al.). We have not yet explored this aspect. Designing or exploring the memory mechanisms and memory terms for structurally different models or different layers within a model is one of our future research directions."}, {"title": "A. Proof", "content": "In Section 3.2, we decomposed $I(\\theta; Y|X)$ and derived the following:\n$I (\\theta; Y|X) = H (\\theta) + H (Y|X) \u2013 H (\\theta,Y|X)$\nHere, we demonstrate that $H(\\theta,Y|X) \u2264 H(\\theta)$, thereby establishing a lower bound for $I(\\theta; Y|X)$. The proof process is as follows.\nThe Data Processing Inequality states that for any random variables $X, Y, Z$, the following is true:\n$H (Y | X) \u2265 H (Y | X, Z)$\nSubstituting $Z = \\theta$ into the above formula, we get:\n$H (Y | X) \u2265 H (Y | X, \\theta)$\nThe chain rule for joint entropy can be written as:\n$H (\\theta,Y | X) = H (\\theta | X) + H (Y | \\theta, X)$\nCombining Equation  with Equation , we can obtain:\n$H (Y | \\theta, X) \u2264 H (Y | X)$\nAccording to the definition of joint entropy, expanding $H(\\theta, Y|X)$ yields:\n$H (\\theta, Y | X) = H (\\theta | X) + H (Y | \\theta, X) \u2264 H (\\theta | X) + H (Y | X)$\nSince entropy is non-negative, therefore:\n$H (\\theta, Y | X) \u2264 H (\\theta | X)$\nSince $H (\\theta|X)$ is the uncertainty of $\\theta$ given $X$, it cannot be greater than the overall uncertainty of $\\theta$, $H(\\theta)$, thus we obtain:\n$H (\\theta | X) \u2264 H (\\theta)$\nCombining Equation  with Equation , we conclude:\n$H (\\theta,Y | X) \u2264 H (\\theta)$"}, {"title": "B. Dataset Introduction", "content": null}, {"title": "B.1. Image Classification Dataset", "content": "CIFAR10. The CIFAR-10 dataset (Krizhevsky, Nair and Hinton, 2010) consists of 60,000 32x32 color images, divided into 50,000 training images and 10,000 test images. The dataset is divided into 10 classes, including: airplanes, cars, cats, etc. Each class contains 6,000 images.\nCIFAR100. The CIFAR-100 dataset is similar in scale and image size to CIFAR-10 but contains 100 classes, with 600 images per class, totaling 60,000 images. These 100 classes are organized into 20 superclasses. The categories in CIFAR-100 are more granular; for example, what is categorized as \"cats\" in CIFAR-10 falls under the pet superclass in CIFAR-100.\nImageNet-1K. The ImageNet dataset was initially created to support the ImageNet Challenge (ILSVRC) and is one of the largest image databases to date, containing over 14 million annotated images spanning more than 20,000 categories."}, {"title": "B.2. Natural Language Datasets", "content": "GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang, 2018) consists of nine natural language understanding (NLU) tasks, all in English. The GLUE tasks include single-sentence tasks like CoLA and SST-2, similarity and paraphrase tasks such as MRPC, STS-B, and QQP, as well as natural language inference tasks including MNLI, QNLI, RTE, and WNLI. Well-known models like BERT (Devlin, 2018), XLNet (Yang, 2019), ROBERTa (Delobelle, Winters and Berendt, 2020), ERINE, T5, and others are tested on this benchmark. Specific details of each dataset are reported in Table (B.1)."}, {"title": "C. Implementation Details", "content": "In this section, we present the experimental details, including the selection of models and hyperparameters. In all experiments, we run using a single NVIDIA RTX 4090. It is worth noting, as mentioned in Section 4.3, we keep the hyperparameter for the memory term, $\\beta$, consistently at 5, without the need for additional optimization."}, {"title": "C.1. Supervised Learning", "content": "In supervised learning, we conducted experiments on the CIFAR-10/100 and ImageNet-1K datasets. In CIFAR, we used the ResNet18 and ResNet34 models, along with the LARS optimizer (You, Gitman and Ginsburg, 2017) and OneCycle learning rate schedule (Smith and Topin, 2017). For ImageNet-1K, we employed the ResNet50 model, and the learning rate was multiplied by 0.1 every 30 epochs. Our other hyperparameters are reported in Table B.2 and B.3."}, {"title": "C.2. Self-Supervised Learning", "content": "In self-supervised learning, we used the SimCLR framework. In the image augmentation module, for both CIFAR-10 and CIFAR-100, we performed the following operations:\n\u2022 Randomly cropped the image to a 32x32 region and resized it.\n\u2022 Randomly horizontally flipped the image with a 50% probability.\n\u2022 Converted the image to grayscale with an 80% probability.\n\u2022 Normalized the image.\nDuring the pre-training process, we added a linear head and used an exponential learning rate schedule. In the loss function, the temperature coefficient $\\tau$ was set to 0.5. The remaining hyperparameters are reported in Table (B.5)."}, {"title": "CRediT authorship contribution statement", "content": "Jinying Xiao: Conceptualization, Writing, Methodology, Experiment. Ping Li: Methodology, Writing, Supervision. Jie Nie: Validation, Software. Zhe Tang: Visualization."}]}