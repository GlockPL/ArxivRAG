{"title": "Mobile Edge Intelligence for Large Language Models: A Contemporary Survey", "authors": ["Guanqiao Qu", "Qiyuan Chen", "Wei Wei", "Zheng Lin", "Xianhao Chen", "Kaibin Huang"], "abstract": "On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest owing to their superior privacy, reduced latency, and bandwidth saving. Nonetheless, the capabilities of on-device LLMs are intrinsically constrained by the limited capacity of edge devices compared to the much more powerful cloud centers. To bridge the gap between cloud-based and on-device AI, mobile edge intelligence (MEI) presents a viable solution to this problem by provisioning AI capabilities within the edge of mobile networks with improved privacy and latency relative to cloud computing. MEI sits between on-device AI and cloud-based AI, featuring wireless communications and more powerful computing resources than end devices. This article provides a contemporary survey on harnessing MEI for LLMs. We first cover the preliminaries of LLMs, starting with LLMs and MEI, followed by resource-efficient LLM techniques. We then illustrate several killer applications to demonstrate the need for deploying LLMs at the network edge and present an architectural overview of MEI for LLMS (MEI4LLM). Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We aim to inspire researchers in the field to leverage mobile edge computing to facilitate LLM deployment in close proximity to users, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications.", "sections": [{"title": "I. INTRODUCTION", "content": "A. Background\nThe recent advent of large language models (LLMs) has been a milestone in artificial intelligence (AI) technology to enable general-purpose intelligence. LLMs excel in various domains, i.e., not only in the task they are built for, i.e., generating text responses, but also in tasks such as multimodal content analysis, summarization, and generalization. For instance, the GPT-4 multimodal model accepts image and text inputs, which produces text outputs exhibiting human-level performance on various professional and academic benchmarks. Apart from these general-purpose models, sometimes called foundation models, LLMs can be fine-tuned to downstream tasks, catering to specific industries and application scenarios. For example, medical LLM, such as Med-PaLM M [1], was designed by Google to provide high-quality answers based on rich data modalities spanning text, imaging, genomics, and more. Google DeepMind also developed Robotics Transformer 2 (RT-2) [2], a vision-language-action AI model for controlling robots. The broad spectrum of use cases highlights the profound impact of LLMs on everyday lives.\nDue to the associated computing, storage, and memory costs, the existing LLMs are mostly confined to cloud data centers for service provisioning. Regrettably, cloud-based LLM provisioning brings inherent drawbacks, including data privacy breaches, high bandwidth costs, and long service latency. Users must upload their data to utilize resources from cloud centers to access LLM services, which often results in significant communication delays. Moreover, uploading private data poses a serious risk to user privacy, especially in privacy-sensitive applications like smart health. Given these concerns, there is growing interest in on-device LLM deployment, which sparks a competitive race among major industry players. For instance, Google has launched Gemini Nano on Pixel 8 Pro smartphones with 1.8-billion and 3.25-billion parameters, respectively [3]. Qualcomm plans to launch Llama 2 support on Snapdragon-powered flagship smartphones and personal computers [4]. On-device LLM deployment enables local processing of sensitive personal data, such as end-to-end (E2E) encrypted messages and health data. It also provides low response time to delay-sensitive applications such as robot planning and autonomous driving. These salient advantages drive the continuing move of LLMs from cloud centers to mobile devices.\nB. Motivation: From Cloud LLMs to On-device LLMs to MEI LLMS\nAlthough on-device LLM is becoming a fast-growing field, the widespread deployment of on-device LLMs faces severe limitations. Specifically, the scarcity of computing, memory, and storage resources on edge devices substantially limits the scale of on-device LLM. On the one hand, the existing industrial efforts focus on sub-10B (10 billion parameters) LLMs due to the extensive resource requirements for the on-device deployment. For instance, Google's Gemini Nano, which relies on 4-bit models with 1.8B and 3.25B parameters, can only support relatively \"basic\" functionalities like summarizing text, suggesting smart replies in context, and checking grammar [3]. However, as the desired functionalities become more complex, deploying larger-scale LLMs on the device becomes necessary, which can significantly increase the overhead for LLM on-device inference. On the other hand, on-device fine-tuning paves the way for personalized and context-aware AI, serving as a fundamental block for superior Al performance. However, the existing on-device LLM products do not incorporate on-device training (fine-tuning) functionalities due to the extensive training cost, which is usually much more intensive than AI inference.\nTo address the aforementioned dilemma, mobile edge computing offers a promising solution. The 6G mobile network aims to deliver low-latency AI inference and training services for a wide range of mobile devices based on edge computing systems by leveraging the network-endowed computing capabilities, say, on base stations. This leads to a paradigm called \"mobile edge intelligence (MEI)\u201d. MEI sits between on-device AI and cloud-based AI, featuring wireless communications and a modest scale of computing resources. In other words, it is more potent than edge devices, yet not as powerful as cloud centers. Due to the short distance between edge devices and edge servers, large-scale LLMs can be supported with lower service latency. Meanwhile, the 6G edge can continuously fine-tune LLMs in ever-evolving environments by exploiting the more capable memory, energy, and computing power on edge servers. As such, the 6G mobile edge is expected to be essential in pushing LLMs to edge devices.\nC. Comparisons with Prior Surveys and Our Contributions\nThe deployment of LLMs is much more resource-intensive than conventional deep neural networks (DNNs), such as convolutional neural networks (CNNs), which is the main hurdle in bringing LLMs to the network edge. This survey paper aims to provide a contemporary survey of this converging trend, i.e., MEI and LLMs, primarily from the perspective of resource-efficient deployment, including storage efficiency, computing efficiency, and communication efficiency at the network edge.\nThis paper differs from the prior survey papers on efficient LLM training/fine-tuning and inference, such as [5], [7], [9], [11]\u2013[15]. These papers mostly focus on improving computing efficiency while overlooking the impact of communications on LLM training, inference, and caching and delivery, which is a significant bottleneck in mobile edge networks. This paper also differs from existing surveys/articles on LLM edge deployment, such as [6], [8], [10], [16]. These papers explore LLM-empowered AI service provisioning with cloud-edge synergy while not discussing resource-efficient deployments, such as parameter-efficient fine-tuning, split inference/learning, and efficient LLM caching and delivery and their interplay with wireless edge networks. Lastly, it is noted that this survey paper is fundamentally different from these papers on \"LLMs for networks [17], [18], where the design objective is to employ LLMs to optimize edge networks rather than leveraging edge computing to support LLMs. The comparisons with some relevant surveys/papers are provided in Table I. The major contributions of this paper are summarized as follows.\n\u2022 We present the application scenarios that drive the deployment of LLMs at the network edge. Although the use\n\u2022 We provide the first comprehensive survey on how 6G edge networks facilitate LLM caching and delivery, training, and inference, including edge LLM caching and delivery, edge LLM training, and edge LLM inference. We will particularly concentrate on the resource-efficient deployment of LLMs to improve the storage, communication, and computing efficiency of LLMs at the network edge.\n\u2022 We identify several crucial research directions for the integration of LLMs and mobile edge intelligence, including green edge AI and secure edge AI for LLMs.\nAs the outline illustrated in Fig. 1, the survey is organized as follows. Section II presents an overview of LLMs and MEI, and Section III introduces the latest resource-efficient LLM techniques. Section IV illustrates four key applications that demonstrate the necessity of deploying LLMs at the network edge. In Section V, we present the MEI for LLM (MEI4LLM) framework, which supports the deployment of LLMs at the network edge. This framework consists of AI-native architecture, parameter-sharing LLM caching and delivery, distributed LLM training/fine-tuning, and distributed LLM inference. Sections VI, VII, and VIII delve into efficient techniques for edge LLM caching and delivery, edge LLM training, and edge LLM inference, respectively, considering storage efficiency, computing efficiency, and communication efficiency. Finally, we outline the roadmap for future research opportunities in Section IX and present our conclusions in Section X."}, {"title": "II. PRELIMINARIES I: AN OVERVIEW OF LLMS AND \u039c\u0395\u0399", "content": "A. Large Language Models\n1) Transformers: LLMs are mostly built with Transformer-based architecture. Transformer [19] have sparked a significant paradigm shift in the domain of natural language processing (NLP), demonstrating exceptional performance on a broad range of language tasks, including text classification [20], machine translation [21] and question answering [22]. For example, Bidirectional Encoder Representations from Transformers (BERT) [23] have achieved state-of-the-art performance in question-answering tasks, showcasing superiority in capturing contextual information efficiently. The breakthroughs from Transformers have extended beyond NLP by achieving tremendous success in computer vision. Transformer models and their variants have been widely employed in various image processing tasks such as image recognition [24], object detection [25], and image segmentation [26]. For instance, the Vision Transformer (ViT) [24] segments images into non-overlapping patches and utilizes Transformer encoders to extract features, yielding superior detection accuracy over traditional CNNs [27].\nTransformers work as follows. Unlike the recursive connections for short-term context and sequential processing in Recurrent Neural Networks (RNNs), Transformers employ self-attention mechanisms to comprehensively capture intricate dependencies between sequence elements to learn long-range relationships. The core of the Transformer architecture design lies in the encoder-decoder architecture, consisting of stacked layers with multi-head self-attention mechanisms. These mechanisms prioritize processing different elements in the input sequence, enhancing the model's ability to effectively generate output tokens. Furthermore, each layer incorporates feed-forward networks (FFNs) and layer normalization. The encoder converts the input sequence into context-rich representations, whereas the decoder employs these representations to generate the output sequence, considering the input and previously generated tokens.\nSelf-attention lies in the heart of Transformers. The self-attention mechanisms embedded within Transformers overcome the limitation of short-term context inherent in RNNs, comprehensively grasping long-range dependencies and enhancing their ability to capture intricate relationships in sequences. Although attention modules have been widely used in feed-forward and recurrent networks [28], [29], Transformers exclusively rely on attention mechanisms and employ a unique implementation (i.e., multi-head attention (MHA)) for parallelization optimization, facilitating scalability on high-complexity models and large-scale datasets. Other alternatives, such as hard attention [30], are inherently stochastic, which necessitates Monte Carlo sampling for attention position sampling. Moreover, in contrast to convolutional or recursive counterparts [31]\u2013[33], Transformer requires minimal prior knowledge of problem structure. This characteristic renders it suitable for model pre-training via pretext tasks on large-scale unlabeled datasets [19], [23], enabling the encoding of highly expressive and generalizable representations. These representations effectively capture the relationships among entities in a given dataset, laying the groundwork for subsequent supervised fine-tuning in downstream tasks.\n2) LLMs: The scalability of Transformers has fuelled the rise of LLMs. A variety of LLMs have been built and evolved based on Transformer architecture. Presently, major players in the AI industry are dedicated to crafting their LLMs and applying them across various domains. For instance, OpenAI has developed the highly-regarded chat LLM, GPT-3 [34], demonstrating exceptional performance across various NLP tasks, such as text generation and machine translation. Google has introduced the medical LLM, Med-PaLM [35], capable of offering expert-level medical guidance and diagnoses. Facebook proposed an innovative image classification LLM, DEIT [36], which integrates self-supervised learning with the Transformer architecture to achieve race-level image classification performance with limited annotated data. These LLMs are trained on extensive and varied datasets available on the Internet [37].\nLLM architectures can be classified into three categories: encoder-only LLMs, encoder-decoder LLMs, and decoder-only LLMs. Encoder-only LLMs, e.g. ALBERT [38], exclusively consist of encoder components and are typically based on advanced architectures such as Transformer [19]. The encoder is responsible for processing input sequences to generate contextualized representations for each token. Despite lacking a decoder to produce output sequences, encoder-only LLMs still exhibit exceptional performance in various NLP tasks such as text classification, sentence similarity computation, and language understanding due to their efficient feature extraction capabilities and adaptable representations. Encoder-decoder LLMs, exemplified by models like T5 [39], represent a pivotal advancement in the NLP domain, integrating both encoder and decoder components within their architectures. The encoder processes input sequences to generate contextualized representations, while the decoder utilizes these representations to generate output sequences, typically in a sequence-to-sequence manner. Encoder-decoder LLMs find widespread application in tasks such as machine translation, text summarization, and question answering, owing to their ability to capture complex linguistic structures and contextual dependencies. Decoder-only LLMs, epitomized by the well-known GPT series [34], [40], constitute a significant branch of LLMs. Decoder-only LLMs adopt autoregressive decoding, which is widely used in both decoder-only and encoder-decoder LLMs, to generate output sequences based on previous tokens in the sequence. This architectural design makes them particularly well-suited for tasks where the model generates text sequentially, such as language generation, text completion, and dialogue response generation.\n3) Multimodal LLMs: Since traditional LLMs [41]\u2013[43] are mainly applied to textual data, the unimodal model training for LLMs limits their ability to comprehend other data types beyond text. For instance, traditional LLMs like GPT-3 and BERT [23] only rely on textual inputs. However, in numerous real-world scenarios, language comprehension is not limited to textual context but also visual cues, auditory signals, and contextual sensing information from diverse sensors.\nTo address the above issue, academia and industry extensively delve into the paradigm of multimodal LLMs shown in Fig. 2, amalgamating various modalities such as text, images, and audio, into a unified framework, unlocking the potential for handling diverse data types. For instance, GPT-4 [40] excels at simultaneously processing both image and text inputs, exhibiting human-comparable performance across various benchmark tests. In image description tasks, GPT-4 utilizes both images and associated textual data to generate more precise and vivid descriptions, while in speech recognition tasks, it merges speech signals with textual information to improve speech comprehension and conversion. multimodal perception plays a pivotal role in the pursuit of general AI, driven by the imperative to process complex real-world data [44]. This necessitates AI models capable of cross-modal information fusion and interactive learning, boosting training performance across multiple perceptual domains.\nMultimodal LLMs inherit powerful learning capabilities of LLMs to empower diverse and complex multimodal tasks by integrating foundation models of various modalities. The LLMs provide robust language generation, zero-shot transfer capabilities, and in-context learning, whereas foundation models of other modalities offer informative representations from other data types [45], [46]. Since foundation models of varied modalities are individually pre-trained, the primary challenge in constructing multimodal LLMs lies in how to connect these models to achieve high-performance collaborative training/inference. The predominant research in this domain focuses on refining modality alignment via multimodal pre-training [47], [48] and multimodal instruction-tuning [49], [50]. Multimodal pre-training learns a common representation across modalities by training the model with multimodal datasets, such as XText [51]. During training, the model learns to correlate information from diverse modalities by optimizing predefined objectives, thus achieving alignment across modalities. This alignment enhances the model's understanding of inter-modality relationships, leading to superior performance across various cross-modal tasks. Multimodal instruction-tuning is a method of fine-tuning based on pre-trained models, aimed at improving model performance on specific tasks. It combines the models with one or more modality-related tasks, then fine-tunes the model using modality-labeled data to improve its alignment with modality-specific tasks. Multimodal instruction-tuning enables models to learn to empower unseen tasks by following new instructions, thereby improving the model's zero-shot performance and generalization capability.\n4) Generative/Interactive AI: The rapid development of LLMs has a profound impact on various applications, particularly in generative AI (GAI) and interactive AI (IAI). GAI focuses on creating a wide range of content, including images, text, music, and video [52], collectively known as AI Generated Content (AIGC). By utilizing multimodal LLMs trained on high-quality datasets, GAI can effectively create superior AIGC based on input text [53]. On the other hand, IAI can be regarded as the next phase of GAI. IAI responds to user queries in applications like chatbots and virtual assistants while enabling Al agents to adapt through user interaction, thereby continually improving the accuracy [54], [55]. By leveraging powerful LLMs and the content generation strengths of GAI, IAI enables AI agents to emulate human interaction and generate meaningful and dynamic conversations with users [56], [57]. In this regard, LLMs are also regarded as the cornerstone of IAI because they facilitate complex interactive dialogues.\nTo enable AI agents to generate more accurate and up-to-date responses, retrieval augmented generation (RAG) can be integrated into LLMs to empower both IAI and GAI [58]. Specifically, LLMs use the input sequence to retrieve relevant data from the external knowledge sources when generating responses, thus improving content generation performance [59], [60]. For example, Google combines RAG with Gemini to enhance LLMs' ability to generate more accurate and contextually relevant responses for specific tasks [61]. The main advantages of integrating RAG into LLMs are twofold. First, by connecting to knowledge sources rich in the latest information, RAG grounds LLMs on the most factual, accurate, and up-to-date content, reducing the likelihood of \"hallucinations\u201d in generated outputs and eliminating the need for frequently adapting LLMs. Second, RAG enables users to verify the sources of the models' responses for improved trustworthiness [62].\n5) Industrial progress for LLMs: LLMs have seen significant advancements in the industry, driven by the maturation of deep learning algorithms [63]\u2013[65], increased computing capabilities, and the availability of large-scale datasets. Major technology companies, including OpenAI, Google, Microsoft, and Meta, have made substantial investments in LLM research and development, leading to the creation of prominent models like GPT series [34], [40] and BERT [23]. These models have demonstrated exceptional performance across a spectrum of NLP tasks, including language translation, text generation, question answering, and sentiment analysis. Furthermore, multimodal LLMs have expanded beyond their initial domain of NLP and are shining in diverse sectors, such as healthcare, autonomous driving, and smart cities. For instance, in healthcare, Med-PaLM [35] is devised for medical image analysis, clinical document processing, and patient diagnosis, facilitating accurate diagnoses and treatment decisions by healthcare professionals. In the realm of autonomous driving, DriveMLM [66] bridges the gap between language decisions and vehicle control commands, enabling closed-loop autonomous driving in realistic simulators. As can be seen, the proliferation of LLMs offers substantial value across multiple industries.\nRecent advancements in on-device LLMs have garnered attention from the industry. For instance, Meta proposed an on-device LLM, named MobileLLM, utilizing deep and thin architectures, embedding sharing, and grouped-query attention mechanisms [67]. Google introduced a new instruction-tuning approach for building a mobile-centric text rewriting LLM [68]. Nevertheless, on-device LLMs often underperform compared with powerful LLMs of larger model sizes. For example, Google's Gemini Nano-1, designed for on-device deployment, contains only 1.8 billion parameters in a 4-bit format, which is distilled from larger Gemini models [69]. Due to its compact size, when the capabilities of such a small LLM are insufficient for the requirements of edge devices, these devices may still need to upload data to access large-scale LLMs, i.e., on edge servers.\nB. Mobile Edge Intelligence\nMEI has emerged as a promising paradigm integrating AI with mobile edge computing, revolutionizing the landscape of mobile services and applications [70]\u2013[73]. The evolution of MEI comes from the convergence of various technological advancements, including the proliferation of Internet of Things (IoT) devices, the deployment of mobile networks, and the maturation of AI algorithms [70], [74]\u2013[76]. These developments have enabled MEI to surmount the limitations of traditional cloud-centric architectures by providing localized AI training/inference and data processing capabilities at the network edge.\nBy integrating AI and communications, the MEI framework enables mobile networks to provide services beyond communications, establishing a strong foundation for the Intelligence of Everything. Along this line, the usage case \"Integrated AI and Communication\" has been included in the IMT Framework Recommendation for 6G [77]. For standardization, the telecommunication standardization organizations 3GPP and ITU have depicted the prospects of edge intelligence in their white papers, respectively. ITU-3172 [78] elucidates the necessity of hosting machine learning (ML) functionalities closer to the network edge based on the latency-sensitive requirements of ML applications. In 3GPP release 18 for 5G standardization, MEI aims to support distributed learning algorithms, split AI/ML, and efficient AI model distribution [79]. The details are elaborated next. First, edge learning, such as federated learning, will be fully supported in edge networks, which enables edge servers to aggregate model updates and knowledge from multiple distributed edge devices, thereby improving the performance of AI/ML models. Second, the split AI/ML over 5G edge networks can facilitate the deployment of AI applications on devices with conflicting requirements, such as computation-intensive, energy-intensive, privacy-sensitive, and delay-sensitive requirements. For example, in the edge split inference, an AI model is partitioned into sub-models, and the computation-intensive and energy-intensive sub-models are offloaded to 5G edge servers (e.g., base stations). The edge server can execute the inference with the edge-side sub-model and the uploaded intermediate data from edge devices. At last, efficient AI model downloading ensures that an AI model can be delivered to edge devices with low latency when edge devices need to adapt to new AI tasks and environments. For example, autonomous driving vehicles need to download a new AI model within 1 second from a 5G edge server when the driving environments change. To integrate network-based AI algorithms into the 5G networks, the MEI framework needs to cater to the request for high-speed and stable data links between edge servers and edge devices. These links can enable high and constant uplink data rates for continuously uploading intermediate data/model updates to the edge server and high downlink data rates in a burst for downloading AI models to edge devices in a timely fashion. Moreover, the core of MEI is to capitalize on the proximity of data sources to edge computing devices (e.g., smartphones, laptops, and wearable gadgets) to enable intelligent decision-making closer to the data source. This distributed computing paradigm exhibits numerous advantages over conventional centralized architectures, including latency reduction, improved bandwidth utilization, data privacy preservation, and enhanced resilience to network failures.\nOn the application side, MEI carries substantial implications across various domains, such as smart healthcare, autonomous driving, and smart cities [80]. For instance, in the healthcare sector, MEI empowers real-time monitoring of patient health data and facilitates prompt interventions during emergencies. Likewise, in smart cities, MEI contributes to intelligent traffic management, environmental monitoring, and energy optimization, thereby fostering sustainability and enhancing the quality of life. Edge Intelligence has also witnessed notable industrial advancements, particularly with the proliferation of edge computing technologies and the advent of 5G networks. Leading enterprises, such as Microsoft, Google, Amazon, and NVIDIA, have developed edge AI platforms to support real-time Al services. For edge AI-empowered IoT applications, Microsoft \"Azure IoT Edge\", Google \"Cloud IoT\", Amazon \"Web Services IoT\", and NVIDIA \u201cEGX\u201d provide edge AI platforms to bring real-time Al services across a wide range of applications, spanning from live video analytics [81], smart home [82], and industrial IoT [83].\nC. Lessons Learned for MEI4LLM\nClearly, MEI4LLM is nothing but a special case of MEI. However, the need to train and deploy a massive number of LLMs at the edge can be a key stimulus for the evolution of MEI. On the one hand, the principles in next-generation MEI, including the push for full integration of AI and communications, align well with what edge LLM needs. On the other hand, the extreme resource demands of LLMs push forward the boundary of MEI. Specifically, MEI4LLM must feature the following characteristics: 1) the native support of model splitting and parallel training/inference across interconnected edge nodes to facilitate the deployment of extremely large-scale models; 2) the integrated design of wireless networking and resource-efficient LLM training/inference techniques such as parameter-efficient fine-tuning and token (representation) reduction (which will be introduced in Section VII and Section VIII), to make LLM deployment cost-effective. In essence, compared to traditional MEI, MEI4LLM primarily focuses on exploring the integrated design of resource management and efficient AI techniques to support LLMs under limited communication-computing resources, which will be the focus of this survey paper and the research theme in this field."}, {"title": "III. PRELIMINARIES II: RESOURCE-EFFICIENT LLM TECHNIQUES", "content": "Undoubtedly", "challenges": "n\u2022 Excessive computing overhead: It is reported that GPT-4 requires approximately 560 tera floating point operations per forward pass to generate one token [84", "85": ".", "86": "implying that on-device training would be even more challenging.\n\u2022 Huge storage/memory demands: On the one hand", "87": "which is unacceptable for mobile devices with limited memory. These factors indicate that deploying LLMs on edge devices for both training and inference poses stringent requirements on the storage and memory resources of edge devices.\n\u2022 High energy costs: The limited volume of battery in edge devices hinders the deployment of LLMs on edge devices. For instance", "88": ".", "compression": "LLM compression enables the deployment of compressed LLMs on edge devices", "below.\nQuantization": "Quantization converts LLM parameters from high-precision floating-point numbers (e.g.", "categories": "post-training quantization (PTQ) and quantization-aware training (QAT). PTQ involves directly converting the parameters of trained models into lower precision [90", "113": ".", "114": ".", "89": ".", "115": "."}, {"88": ".", "90": "."}, {"90": "propose the activation-aware weight quantization for uneven weight quantization. This method quantizes most of the weights in LLMs from FP16 to INT3/INT4. However", "error.\nPruning": "Unlike quantization", "116": "and unstructured pruning [117", "91": "utilizes gradient information and estimated Hessian matrices to make pruning decisions for coupled structures in LLMs", "112": "to recover model performance. The compressed LLMs pruned via structural pruning can be directly deployed and executed on standard computational frameworks without additional adjustments since the structured pruning removes entire structures in LLMs. 2) Unstructured pruning aims to remove individual weights", "92": "transforms the pruning problem into a series of large-scale sparse regression problems and addresses them with an innovative approximate solver. The proposed SparseGPT enables up to 60% sparsity for OPT-175B [118", "distillation": "Knowledge distillation (KD) [119", "11": [120], "93": "addresses the limitations of traditional KD loss functions", "120": ".", "94": "generate a set of 2.58 million instructions and use GPT-3.5 Turbo APIs to produce responses. These instructions are subsequently used to fine-tune various student language models. The distilled language model can achieve comparable performance or even outperform the 7B LLaMA [121", "LLMs": "Fast decoding methods can be used in on-device LLM deployment to save computing resources of edge devices in inference and facilitate on-device inference. Typical fast decoding methods can be categorized into speculative decoding", "decoding": "LLMs typically generate text outputs in an autoregressive manner", "95": "propose speculative decoding. In this method", "122": ".", "88": "."}, {"88": "the authors demonstrate that speculative decoding can halve both per-token generation latency and energy consumption while maintaining the quality of the generated content", "exit": "To minimize computing latency", "96": [123], "124": ".", "125": ".", "97": [126], "layer.\nMixture-of-experts": "Mixture-of-experts (MoE) can effectively scale up the LLM capacity and increase performance across various downstream tasks [127", "129": "."}]}