{"title": "Mobile Edge Intelligence for Large Language Models: A Contemporary Survey", "authors": ["Guanqiao Qu", "Qiyuan Chen", "Wei Wei", "Zheng Lin", "Xianhao Chen", "Kaibin Huang"], "abstract": "On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest owing to their superior privacy, reduced latency, and bandwidth saving. Nonetheless, the capabilities of on-device LLMs are intrinsically constrained by the limited capacity of edge devices compared to the much more powerful cloud centers. To bridge the gap between cloud-based and on-device AI, mobile edge intelligence (MEI) presents a viable solution to this problem by provisioning AI capabilities within the edge of mobile networks with improved privacy and latency relative to cloud computing. MEI sits between on-device AI and cloud-based AI, featuring wireless communications and more powerful computing resources than end devices. This article provides a contemporary survey on harnessing MEI for LLMs. We first cover the preliminaries of LLMs, starting with LLMs and MEI, followed by resource-efficient LLM techniques. We then illustrate several killer applications to demonstrate the need for deploying LLMs at the network edge and present an architectural overview of MEI for LLMS (MEI4LLM). Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We aim to inspire researchers in the field to leverage mobile edge computing to facilitate LLM deployment in close proximity to users, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications.", "sections": [{"title": "I. INTRODUCTION", "content": "A. Background\nThe recent advent of large language models (LLMs) has been a milestone in artificial intelligence (AI) technology to enable general-purpose intelligence. LLMs excel in various domains, i.e., not only in the task they are built for, i.e., generating text responses, but also in tasks such as multimodal content analysis, summarization, and generalization. For instance, the GPT-4 multimodal model accepts image and text inputs, which produces text outputs exhibiting human-level performance on various professional and academic benchmarks. Apart from these general-purpose models, sometimes called foundation models, LLMs can be fine-tuned to downstream tasks, catering to specific industries and application scenarios. For example, medical LLM, such as Med-PaLM M [1], was designed by Google to provide high-quality answers based on rich data modalities spanning text, imaging, genomics, and more. Google DeepMind also developed Robotics Transformer 2 (RT-2) [2], a vision-language-action AI model for controlling robots. The broad spectrum of use cases highlights the profound impact of LLMs on everyday lives.\nDue to the associated computing, storage, and memory costs, the existing LLMs are mostly confined to cloud data centers for service provisioning. Regrettably, cloud-based LLM provisioning brings inherent drawbacks, including data privacy breaches, high bandwidth costs, and long service latency. Users must upload their data to utilize resources from cloud centers to access LLM services, which often results in significant communication delays. Moreover, uploading private data poses a serious risk to user privacy, especially in privacy-sensitive applications like smart health. Given these concerns, there is growing interest in on-device LLM deployment, which sparks a competitive race among major industry players. For instance, Google has launched Gemini Nano on Pixel 8 Pro smartphones with 1.8-billion and 3.25-billion parameters, respectively [3]. Qualcomm plans to launch Llama 2 support on Snapdragon-powered flagship smartphones and personal computers [4]. On-device LLM deployment enables local processing of sensitive personal data, such as end-to-end (E2E) encrypted messages and health data. It also provides low response time to delay-sensitive applications such as robot planning and autonomous driving. These salient advantages drive the continuing move of LLMs from cloud centers to mobile devices.\nB. Motivation: From Cloud LLMs to On-device LLMs to MEI LLMS\nAlthough on-device LLM is becoming a fast-growing field, the widespread deployment of on-device LLMs faces severe limitations. Specifically, the scarcity of computing, memory, and storage resources on edge devices substantially limits the scale of on-device LLM. On the one hand, the existing industrial efforts focus on sub-10B (10 billion parameters) LLMs due to the extensive resource requirements for the on-device deployment. For instance, Google's Gemini Nano, which relies on 4-bit models with 1.8B and 3.25B parameters, can only support relatively \"basic\" functionalities like summarizing text, suggesting smart replies in context, and checking grammar [3]. However, as the desired functionalities"}, {"title": "C. Comparisons with Prior Surveys and Our Contributions", "content": "The deployment of LLMs is much more resource-intensive than conventional deep neural networks (DNNs), such as convolutional neural networks (CNNs), which is the main hurdle in bringing LLMs to the network edge. This survey paper aims to provide a contemporary survey of this converging trend, i.e., MEI and LLMs, primarily from the perspective of resource-efficient deployment, including storage efficiency, computing efficiency, and communication efficiency at the network edge.\nThis paper differs from the prior survey papers on efficient LLM training/fine-tuning and inference, such as [5], [7], [9], [11]\u2013[15]. These papers mostly focus on improving computing efficiency while overlooking the impact of communications on LLM training, inference, and caching and delivery, which is a significant bottleneck in mobile edge networks. This paper also differs from existing surveys/articles on LLM edge deployment, such as [6], [8], [10], [16]. These papers explore LLM-empowered AI service provisioning with cloud-edge synergy while not discussing resource-efficient deployments, such as parameter-efficient fine-tuning, split inference/learning, and efficient LLM caching and delivery and their interplay with wireless edge networks. Lastly, it is noted that this survey paper is fundamentally different from these papers on \"LLMs for networks [17], [18], where the design objective is to employ LLMs to optimize edge networks rather than leveraging edge computing to support LLMs. The comparisons with some relevant surveys/papers are provided in Table I. The major contributions of this paper are summarized as follows.\n\u2022 We present the application scenarios that drive the deployment of LLMs at the network edge. Although the use"}, {"title": "II. PRELIMINARIES I: AN OVERVIEW OF LLMS AND \u039c\u0395\u0399", "content": "A. Large Language Models\n1) Transformers: LLMs are mostly built with Transformer-based architecture. Transformer [19] have sparked a significant paradigm shift in the domain of natural language processing (NLP), demonstrating exceptional performance on a broad range of language tasks, including text classification [20], machine translation [21] and question answering [22]. For example, Bidirectional Encoder Representations from Transformers (BERT) [23] have achieved state-of-the-art performance in question-answering tasks, showcasing superiority in capturing contextual information efficiently. The breakthroughs from Transformers have extended beyond NLP by achieving tremendous success in computer vision. Transformer models and their variants have been widely employed in various image processing tasks such as image recognition [24], object detection [25], and image segmentation [26]. For instance, the Vision Transformer (ViT) [24] segments images into non-overlapping patches and utilizes Transformer encoders to extract features, yielding superior detection accuracy over traditional CNNs [27].\nTransformers work as follows. Unlike the recursive connections for short-term context and sequential processing in Recurrent Neural Networks (RNNs), Transformers employ self-attention mechanisms to comprehensively capture intricate dependencies between sequence elements to learn long-range relationships. The core of the Transformer architecture design lies in the encoder-decoder architecture, consisting of stacked layers with multi-head self-attention mechanisms. These mechanisms prioritize processing different elements in the input sequence, enhancing the model's ability to effectively generate output tokens. Furthermore, each layer incorporates feed-forward networks (FFNs) and layer normalization. The encoder converts the input sequence into context-rich representations, whereas the decoder employs these representations to generate the output sequence, considering the input and previously generated tokens.\nSelf-attention lies in the heart of Transformers. The self-"}, {"title": "C. Lessons Learned for MEI4LLM", "content": "Clearly, MEI4LLM is nothing but a special case of M\u0395\u0399. However, the need to train and deploy a massive number of LLMs at the edge can be a key stimulus for the evolution of MEI. On the one hand, the principles in next-generation M\u0395\u0399, including the push for full integration of AI and communications, align well with what edge LLM needs. On the other hand, the extreme resource demands of LLMs push forward the boundary of MEI. Specifically, MEI4LLM must feature the following characteristics: 1) the native support of model splitting and parallel training/inference across interconnected edge nodes to facilitate the deployment of extremely large-scale models; 2) the integrated design of wireless networking and resource-efficient LLM training/inference techniques such as parameter-efficient fine-tuning and token (representation) reduction (which will be introduced in Section VII and Section VIII), to make LLM deployment cost-effective. In essence, compared to traditional MEI, MEI4LLM primarily focuses on exploring the integrated design of resource management and efficient AI techniques to support LLMs under limited communication-computing resources, which will be the focus of this survey paper and the research theme in this field."}, {"title": "III. PRELIMINARIES II: RESOURCE-EFFICIENT LLM TECHNIQUES", "content": "Undoubtedly, due to the large sizes and computational complexity of LLMs, deploying LLMs on edge devices/servers for training/inference presents several critical challenges:\n\u2022 Excessive computing overhead: It is reported that GPT-4 requires approximately 560 tera floating point operations per forward pass to generate one token [84]. However, the advanced A100 GPU provides a computational capacity of only 19.5 tera floating point operations per second [85]. This indicates that each forward pass of GPT-4 would take about 28 seconds to generate one token with a single A100 GPU. Furthermore, the backward propagation typically requires more computational resources than a forward pass [86], implying that on-device training would be even more challenging.\n\u2022 Huge storage/memory demands: On the one hand, caching LLMs on edge devices consumes significant storage resources. LLMs specifically designed for on-device deployment even have billions of parameters, e.g., Google's on-device Gemini Nano-2 has 3.25 billion parameters. On the other hand, optimizing the model with the commonly used Adam optimizer during training usually requires 12 times the memory resources needed for inference [87], which is unacceptable for mobile devices with limited memory. These factors indicate that deploying LLMs on edge devices for both training and inference poses stringent requirements on the storage and memory resources of edge devices.\n\u2022 High energy costs: The limited volume of battery in edge devices hinders the deployment of LLMs on edge devices. For instance, running an LLM quantized to INT4 with 13 billion parameters using llama.cpp (one of the most lightweight on-device LLM engines) on a Xiaomi 11 smartphone results in an energy consumption of about 56 J/token [88]. This implies that a smartphone with a battery capacity of 3000 mAh and an output voltage of 3.7 V can generate only approximately 700 tokens if the LLM is deployed on the smartphone. The amount of data that can be processed would be even less if LLM training/fine-tuning were to be performed on edge devices.\nTo mitigate the above challenges, in this section, we will review the enabling techniques tailored for resource-efficient LLM deployment, which are summarized in Fig. 3. The comparison of related works is shown in Table II. It is worth noting that the methods discussed in this section can decrease the complexity of LLM deployment on edge devices, edge servers, or in device-server collaboration. Consequently, these key techniques serve as the foundation for MEI4LLM and all subsequent sections."}, {"title": "A. Resource-efficient Inference", "content": "On-device LLM inference eliminates privacy leakage and the need for Internet connections. However, it poses significant challenges for on-device deployment since LLMs require substantial computing, memory, and energy resources. In what follows, we briefly present how to mitigate these problems by introducing efficient on-device LLM inference techniques.\n1) LLM compression: LLM compression enables the deployment of compressed LLMs on edge devices, significantly alleviating both memory/storage and computing demands on edge devices. The design objective is to compress LLMS without substantially compromising inference accuracy. Although compression has been extensively studied in the field of traditional DNNs, the unique architectures and properties of LLMs necessitate the redesign of existing compression strategies. The compression techniques tailored for LLMs are detailed below.\nQuantization: Quantization converts LLM parameters from high-precision floating-point numbers (e.g., FP16) to low-precision numbers (e.g., INT4), thereby reducing storage usage, computing latency, and energy footprint during inference. Classic model quantization can be divided into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ involves directly converting the parameters of trained models into lower precision [90], while QAT considers quantization errors in the training phase to improve the performance of the quantized models [113]. Although QAT typically yields better performance, it requires model retraining, which is extremely resource-intensive for on-device LLMs. Therefore, most quantization methods for on-device LLMs rely on PTQ [114]. In contrast to traditional quantization strategies that target both weights and activations, LLM quantization primarily focuses on weight-only quantization [89]. The reasons are as follows. First, quantizing activations leads to more significant performance degradation for LLMs [115]. Second, the primary sources of latency and energy consumption when generating new tokens with LLMs are often due to loading model parameters from memory [88]. Therefore, weight-only quantization allows for more efficient loading of quantized weights from memory, making inference more efficient without significantly degrading inference accuracy.\nIn LLM quantization, uneven weight quantization can be adopted to preserve the inference accuracy of quantized LLMs since not all weights in LLMs contribute equally to the final inference results [90]. For example, the authors in [90] propose"}, {"title": "B. Resource-efficient Fine-tuning", "content": "Compared to inference, on-device LLM training demands significantly higher memory and computing resources. For example, computing the gradients of the LLM OPT-13B [118] consumes 12 times the memory needed for inference [87]. However, since LLM fine-tuning requires much less computing resources than full-parameter training, LLM fine-tuning is widely adopted in on-device LLM deployment. In this subsection, we will explore techniques that can effectively fine-tune LLM parameters under limited resources.\n1) Parameter-efficient fine-tuning: Parameter-efficient fine-tuning (PEFT) [13] has emerged as a prominent solution for LLM fine-tuning by updating only a small number of parameters in the fine-tuning process. Popular PEFT techniques can be categorized into three main types, i.e., additive PEFT, selective PEFT, and reparameterized PEFT, which are elaborated next.\nAdditive PEFT: To mitigate the computing burden of fine-tuning, the additive PEFT introduces trainable components with minimal parameters into LLMs while maintaining the pre-trained LLM parameters frozen. Additive PEFT can be further categorized into three types based on the locality of the introduced components, i.e., adapter tuning, prompt tuning, and prefix tuning, as shown in Fig. 4. 1) As first introduced by [107], adapter tuning inserts adapter modules into the Transformer layers and freezes the other parameters in the Transformer. In this approach, only the adapters are updated during fine-tuning. 2) Prompt tuning adds soft prompt tokens at the beginning of the input tokens for fine-tuning [108]. This method leverages the characteristic of LLMs that the encoding and token generation are based on the preceding tokens. 3) Prefix tuning adds trainable prefix parameters to the keys and values of the MHA in each Transformer layer [109]. Although this approach increases the number of trainable parameters compared with prompt tuning, it allows for direct modification of representations within LLMs, enabling LLMs to respond more precisely to specific tasks.\nSelective PEFT: Though additive PEFT enables fine-tuning LLMs with fewer trainable parameters, it introduces additional inference latency by adding more parameters [13]. To address this problem, selective PEFT preserves the model architecture by freezing most parameters and updating only a smaller subset of parameters. Selective PEFT can be broadly classified into unstructured selective PEFT and structured selective PEFT, which are shown in Fig. 4. 1) Unstructured selective PEFT determines the selection of trainable parameters individually, which can enhance the performance of fine-tuned models [13]. For example, the authors in [110] reformulate the trainable parameter selection in PEFT as an optimization problem with a second-order approximation of the loss function and provide a second-order approximation method to solve this problem. By choosing the sparse trainable parameters, the sparse fine-tuned models outperform the fully fine-tuned models. However, the sparsity of the trainable parameters may not be well supported by existing deep learning frameworks and hardware, introducing additional workload for on-device LLM deployment during fine-tuning [140]. 2) Structured selective PEFT selects regular combinations of parameters, such as specific model layers, to enhance hardware computational efficiency in on-device LLM deployment [140]. For example, the authors in [111] propose Structured Diff Pruning, which partitions parameters into groups based on matrix/bias vectors and strategically removes some groups. Then, only the parameters in the remaining groups are updated, thereby saving computing resources during LLM fine-tuning.\nReparameterized PEFT: Reparameterized PEFT techniques leverage low-rank matrices to reduce the number of trainable parameters during model fine-tuning. One of the most well-known methods in reparameterized PEFT for LLMs is LoRA [112]. For a pre-trained weight matrix in an LLM, LoRA introduces two additional trainable matrices with ranks much smaller than that of the pre-trained weight matrix, as illustrated in Fig. 4. In the fine-tuning process, the pre-trained weight matrix is frozen, and only the newly introduced two low-rank matrices are trainable. This approach allows for efficient model fine-tuning because updating the low-rank matrices requires significantly less computational capability than updating the pre-trained weight matrix. Besides, LoRA does not add any additional inference latency since LoRA's fine-tuned weights are merged into the original weights of LLMs during inference [13]. Due to its salient advantages, LoRA has inspired numerous subsequent research works. For example, Quantized LoRA (QLoRA) [141] aims to minimize memory consumption by combining quantization techniques with LoRA, enabling the fine-tuning of a language model with 65B parameters with a 48 GB GPU within 24 hours. The fine-tuned LLM achieves 99.3% of the ChatGPT's performance on the evaluated tasks, demonstrating the effectiveness of"}, {"title": "2) Zeroth-order optimization", "content": "Zeroth-order optimization [142] is a novel technique for model training, which estimates gradient updates only through the forward pass. This method considerably reduces the computational burden since the forward pass, equivalent to inference, demands far fewer computing resources than the backpropagation during training. Specifically, compared with prevalent first-order optimizers, such as Adam, zeroth-order optimizers do not need to store the intermediate results of the backpropagation during the training process, significantly reducing the memory consumption in LLM training. For example, the authors in [87] propose the zeroth-order optimizer, MeZO, which adopts simultaneous perturbation stochastic approximation to estimate model gradients only through the forward propagation and uses the estimated gradients to update model parameters. Compared with the fine-tuning with Adam, the model fine-tuned with MeZO demonstrates competitive performance on 7 out of 11 tasks, using only 1/12 of the running memory while only causing less than a 1% accuracy degradation. Moreover, to further enhance the efficiency of LLM fine-tuning, the zeroth-order optimization technique can be combined with PEFT methods, such as LoRA and prefix fine-tuning [87]."}, {"title": "IV. APPLICATION SCENARIOS", "content": "Although LLMs can be applied to a broad range of tasks, we focus on the application scenarios that motivate the deployment of LLMs at the network edge. Thus, as shown in Fig. 5, we present four killer LLM-empowered applications while concentrating on three aspects: latency requirements, bandwidth costs, and privacy requirements.\nMobile health: Healthcare is one of the most promising applications for LLMs. Google's Med-PaLM 2, for example, is an LLM fine-tuned on medical datasets, capable of delivering answers to medical inquiries [35]. Recently, Fitbit and Google Research have joined forces to build an LLM to support personalized health and wellness features, which aims to help people get summarization and recommendations from the data generated from their mobile devices [147]. Specifically, this model can deliver personalized coaching capabilities, like actionable messages and guidance, based on personal fitness goals. Apart from this, healthcare LLMs can also assist in medical question answering, diagnosis, treatment, and medical report generation [148]. With these exciting applications, mobile health has the following service requirements, making it better to deploy LLMs at the mobile edge:\n1) Latency requirements: Some LLM-empowered mobile health applications demand prompt warning messages to avert undesirable health consequences. For example, the state-of-the-art fall detection algorithms can achieve a latency of 37 ms [149]. Moreover, the tolerant audio/video conferencing latency for an emergency accident and a routine checkup varies from 0 to 150 ms and 150 to 400 ms, respectively [143]. LLMs should also achieve low latency to support the aforementioned applications, i.e., triggering warnings and generating advice upon fall detection. Since these scenarios require analytics of high-dimensional data/features to trigger warnings, uploading the data to cloud centers will experience long latency and high delay jitter through the backbone network.\n2) Bandwidth costs: Medical LLMs often possess multimodal processing capabilities. For instance, Google's Med-PaLM 2 has a multimodal version, called Med-PaLM M, which processes rich data modalities spanning text, imaging, genomics, and more, to interpret the biometrics of subjects. Emerging 5G-enabled mobile health applications also incorporate medical augmented reality/virtual reality (AR/VR), where"}, {"title": "V. AN OVERVIEW OF MEI4LLM", "content": "Pushing LLMs to the network edge is a continuing trend with the killer applications in Section IV and the enabling resource-efficient techniques as introduced in III. By deploying LLMs at the network edge, edge devices can collaborate with edge servers for collaborative learning and inference, which largely mitigates the resource scarcity of edge devices. In line with the \"NET4AI\" (network for AI) vision for the 6G era [159], this section presents an overview of the M\u0395\u0399 framework that supports the deployment of LLMs, called MEI4LLM, as shown in Fig. 6. MEI4LLM consists of the following basic components.\nA. AI-native Architecture\nThe next-generation edge networks will support AI services in an E2E manner. The goal of 6G should be to support AI, including LLMs, with superior performance with minimal communication, computing, storage, and energy requirements. For this reason, \u201ctask-oriented\" architecture is commonly envisioned for 6G. Instead of maximizing throughput or minimizing latency, the design goal can be minimizing the cross entropy of output tokens from LLMs by implementing the optimal distributed computing, feature extraction, and resource allocation schemes under diverse resource constraints.\nTo achieve this goal, network virtualization is of paramount importance for improving resource utilization, flexibility, and manageability. Following the design principle of software-defined networking, the MEI4LLM features a central controller that orchestrates network-wide computing resources and data transmissions, with the decoupled control and data plane. By collecting global network knowledge, such as the accuracy of LLMs, various quantization levels, user requirements on LLM services, channel conditions, battery status of users, and computing resource availability, the control partitions and coordinates model training/inference and delivery across the distributed edge computing systems, with intermediate smashed data (i.e., intermediate activations and back-propagated gradients), model parameters, or user data exchanged across edge routers and servers.\nFurther edge networks will evolve into the \"neural edge\" [159], where neural network layers are distributed across edge nodes for collaborative computing. Analogous to cloud data centers with many GPUs to support large-scale LLMS, MET4LLM must feature flexible and model splitting for training and inference across distributed edge devices and servers. Both air interfaces and network designs should have native support for federated learning, split learning, and split inference for AI models, including LLMs. Since model training and inference are robust to packet errors, task-oriented wireless transmissions, say, for smashed data at the cut layer, can be carried out with appropriate error control to achieve the best efficiency-reliability trade-off. The optimal model splitting, placement, and data routing for large-scale models should be concertedly supported over edge networks.\nAt last, information-centric networking can be implemented to ensure seamless model, feature, and data transmission across edge networks for efficient delivery of LLMs. In this respect, MEI4LLM should support LLM parameter block naming and name-based transmission protocol. By assigning names for each LLM parameter block, the central controller in the MEI4LLM architecture can forward the parameter request to where it is cached, reducing latency and bandwidth consumption for delivering large-scale models across networks and to end users.\nB. Parameter-sharing LLM Caching and Delivery\nConsidering the limited storage capacities of edge devices and frequent model fine-tuning, LLMs of interest should be delivered within edge networks from their location to where they are needed promptly. Moreover, considering RAG, external knowledge sources should also be cached at the network edge, ensuring timely acquisition of the data/knowledge once LLM applications need. The model/data delivery can be carried out in wired backhaul or radio access networks. The caching and delivery of LLMs must take advantage of the fact that parameter blocks can be shared among various downstream LLMs [112], [160] and even reused within the same LLM [38]. This unique characteristic enables efficient edge LLM caching and low-latency LLM delivery by reducing caching and delivery costs of duplicate LLM parameter blocks, as we will present in VI. To enable fast model delivery, MEI4LLM can construct a lookup table, assigning names for LLM parameter blocks to facilitate content search and management, following the principle of information-centric networking. By doing so, the MEI4LLM paradigm places LLMs at appropriate sites, retrieves LLMs of interest from nearby edge servers, and enables routing/multicasting of LLM parameter blocks to mobile users.\nC. Distributed LLM Training (Fine-tuning)\nIt is foreseen that 6G MEI systems can efficiently fine-tune LLMs to adapt to local environments. Edge LLM fine-tuning can be triggered whenever the inference accuracy decreases or after a certain period when local environments have changed. For example, LLM-empowered virtual assistants should be fine-tuned regularly to better adapt to new trends in news media, top local restaurants, and popular attractions, resulting in improved decision-making and interactions with users. LLM-empowered mobile health applications should be personalized to provide better predictions and health or fitness suggestions. In next-generation mobile networks, edge training for LLMs must answer two questions: 1) how to preserve user privacy and data ownership, and 2) how to support large-scale model training through the collaboration of edge nodes. To enhance the data privacy of users, federated learning (FL) and split learning (SL) serve as two promising distributed learning"}, {"title": "VI. EDGE CACHING AND DELIVERY FOR LLMS", "content": "Edge LLM caching and delivery play indispensable roles in both the training and inference of LLMs, serving as cornerstones of edge LLM deployment. For this reason, we begin with discussions on edge caching and delivery and then present edge training and inference in the subsequent sections. Compared with conventional edge service/content caching and delivery, the main distinction of edge LLM caching and delivery is the exploitation of parameter shareability, which is commonly observed in LLMs, aiming to increase storage and communication efficiency over edge networks. While parameter shareability can exist in traditional DNNs, it is much more prevalent and significant in LLMs due to the widespread adoption of PEFT techniques, requiring our special design attention. In what follows, we present the techniques arising from exploiting this characteristic of LLMs.\nEdge model caching can realize low model downloading latency by distributing AI models to wireless edge servers in advance. Distinct from service placement for computation offloading, edge model caching focuses on caching AI models for downloading from edge servers to end users. The design of AI model caching aims to provide models to more users within their quality-of-service (QoS) requirements [160], [165]. This paradigm enables users to fetch AI models from edge servers directly instead of accessing remote cloud data centers, which incurs excessive downloading latency [162], [164]. However, implementing edge LLM caching presents several challenges: 1) Limited storage capacity for LLM caching: Service providers aim to place as many popular LLMs as possible on edge servers to enhance the cache hit ratio and decrease model downloading latency for users. Nevertheless, the immense size of LLMs presents a significant challenge for their storage on edge servers; 2) High LLM edge cache (re)placement costs: Over time, previously cached LLMs may no longer align with the changing user requests. To address this, service providers may replace LLMs on edge servers to better accommodate up-to-date requests. However, the placement of these large-scale models leads to considerable communication overhead and imposes a substantial burden on mobile backhaul networks. In what follows, parameter-sharing model caching is presented to address the above challenges.\n1) Parameter-sharing LLM caching: Parameter-sharing model caching can be adopted to improve storage and transmission efficiency at the network edge. As discussed in Section III, PEFT, such as LoRA, is widely adopted for adapting LLMs to downstream tasks. In LoRA, the pre-trained LLM parameters are frozen, and only the newly introduced parameters are trainable, typically accounting for less than 1% of the original LLM parameters. Therefore, most parameters of various LLMs fine-tuned with LORA for downstream tasks are shared from pre-trained LLMs, which should be leveraged to enhance caching efficiency significantly. Take LoRA and GPT-2 as an example. Fig. 7 demonstrates that the inference performance almost remains unchanged even when 99.97% of parameters in the GPT-2 large model are frozen parameters from the pre-trained GPT-2 large. Based on this observation, in [162], we propose an AI model placement strategy, called TrimCaching, to maximize the cache hit ratio under server storage capacity and service latency constraints by exploiting the parameter-sharing properties of AI models, particularly LLMs. In the TrimCaching framework, only one copy of the shared parameter blocks across LLMs is cached on one edge server, thereby improving storage efficiency, as illustrated in Fig. 8. Compared with the Independent Caching strategy for edge LLM caching [166], [167], which does not consider the parameter sharing across LLMs, the TrimCaching strategy can significantly improve cache hit ratio as shown in Fig. 9.\nWhile the parameter-sharing model caching under multi-cell scenarios has been investigated in [162], this paradigm can be extended to consider many different scenarios in cellular networks, such as centralized-RAN (C-RAN) and heterogeneous networks (HetNets). Moreover, mobility-aware edge caching can also be developed by exploiting the knowledge of user mobility patterns. For instance, in [168], a distributed approximation algorithm based on large deviation inequalities is developed for content placement based on the assumption that users move randomly based on a discrete-time Markov chain model. Similar algorithms can be developed to address the parameter-sharing model caching problem with high user mobility.\n2) Edge LLM cache replacement: Since the popularity of models can evolve over time, another fundamental research problem in edge caching is LLM replacement. By replacing outdated content with new data, edge servers can continuously refresh their caches with new content to satisfy the ever-changing user requests [169]. The two most classic replacement strategies are the recency-based and the frequency-based strategies, which remove the least recently used (LRU) objects and least frequently used (LFU) objects and then replace them with updated content. However, such strategies fail to consider the shared parameter blocks across LLMs and the cooperative caching among different edge nodes. One direction to improve the replacement performance is to re-conduct the centralized proactive caching, say the scheme in [160], after a certain"}, {"title": "B. Edge LLM Delivery", "content": "An essential step in fetching the models from where they are cached to end users is delay-efficient model delivery. This process encompasses both model routing within backhaul/backbone networks and model downloading via wireless access links", "challenges": 1, "latency": "When the requesting LLMs are not cached on the associated edge server"}, {"latency": "AI model downloading needs to be finished with low latency to fulfill the QoS requirements of end users. As envisioned by 3GPP", "79": ".", "delivery": "To reduce the model delivery costs within the backhaul/backbone networks", "downloading": "To reduce the costs of wireless model downloading from base stations (edge servers) to users, it is essential to consider parameter-sharing wireless model downloading. As illustrated in Fig. 11, to decrease downloading latency, the key idea is to multicast the reusable parameter blocks, thereby achieving timely downloading. In [163", "164": [171], "90": ".", "172": "."}, {"90": [172], "173": "."}]}