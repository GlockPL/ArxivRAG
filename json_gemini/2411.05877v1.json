{"title": "GENERATIVE ADAPTER: CONTEXTUALIZING LANGUAGE MODELS IN PARAMETERS WITH A SINGLE FORWARD PASS", "authors": ["Tong Chen", "Hao Fang", "Patrick Xia", "Xiaodong Liu", "Benjamin Van Durme", "Luke Zettlemoyer", "Jianfeng Gao", "Hao Cheng"], "abstract": "Large language models (LMs) are typically adapted to improve performance on new contexts (e.g., text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff-fine-tuning incurs significant training cost and prompting increases inference overhead. We introduce GenerativeAdapter, an effective and efficient adaptation method that directly maps new contexts to low-rank LM adapters, thereby significantly reducing inference overhead with no need for finetuning. The adapter generator is trained via self-supervised learning, and can be used to adapt a single frozen LM for any new task simply by mapping the associated task or domain context to a new adapter. We apply GenerativeAdapter to two pretrained LMS (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models in three adaption scenarios: knowledge acquisition from documents, learning from demonstrations, and personalization for users. In StreamingQA, our approach is effective in injecting knowledge into the LM's parameters, achieving a 63.5% improvement in F1 score over the model with supervised fine-tuning (from 19.5 to 31.5) for contexts as long as 32K tokens. In the MetaICL in-context learning evaluation, our method achieves an average accuracy of 44.9 across 26 tasks, outperforming the base model. On MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to prompting with full conversation history. Together, these results suggest that GenerativeAdapter should allow for general adaption to a wide range of different contexts.", "sections": [{"title": "1 INTRODUCTION", "content": "Adaptation is essential for language models (LMs) to acquire new world knowledge (Jiang et al., 2024; Hu et al., 2023; Mecklenburg et al., 2024), learn new tasks (Min et al., 2022), and personalize to individual users (Salemi et al., 2024). Existing adaptation methods typically involve either prompting or fine-tuning (Brown et al., 2020). As the scale of LMs continues to increase, adapting them becomes increasingly difficult due to efficiency constraints during both training and inference (Hu et al., 2022).\nPrompting with task-specific demonstrations (i.e., in-context learning (Brown et al., 2020)) or background knowledge (i.e., retrieval-augmented generation (Lewis et al., 2020)) is one way to enable models to temporarily encode such relevant information, allowing flexible adaptation to various tasks. However, to maintain additional memory across sessions, some extra prompts must be added to the input, which incur an inference-time or storage overhead (Chevalier et al., 2023). Fine-tuning is another way to embed new information into the LM's parameters, retaining long-term memory. Nevertheless, it requires a training phase that is more computationally expensive than a single forward pass, and acquiring knowledge through continual pretraining has shown to be data-"}, {"title": "2 METHOD", "content": "We present GenerativeAdapter, an efficient and effective framework for directly generating additive weight updates to contextualize the pretrained LM (a frozen base LM) at test time. Unlike"}, {"title": "2.1 ADAPTATION WITH TEST-TIME CONTEXTUALIZATION", "content": "To contextualize a base model, \\(\\Theta_{base}\\), to a given context C, our goal is to obtain an updated model, \\(\\Theta_{C}\\), that can respond to user instructions using the information provided in the context C. In practice, the context can include different types of data, such as documents, dialogues, or task description and few-shot demonstrations.\nWe specifically focus on test-time contextualization, where context arrives incrementally as a stream of data, such as a continuous flow of documents or dialogue sessions. We represent this streaming context up to time step t as \\(\\sum(t) := (C_1, ..., C_t)\\), where \\(C_t\\) is the context chunk arriving at time step t. In this online adaptation scenario, the model must be efficiently adapted to each new context chunk as it becomes available.\nAs shown in Figure 1, we propose GenerativeAdapter as a framework that adapts the base model \\(\\Theta_{base}\\) to new contexts through a single forward pass as each context chunk arrives. Specifically, given test-time context \\(\\sum(t)\\), we adapt the base model \\(\\Theta_{base}\\) to a new model \\(\\Theta_{\\sum(t)}\\) using a context-dependent additive adapter \\(\\Delta_t\\), i.e., \\(\\Theta_{\\sum(t)} = \\Theta_{base}+\\Delta_t\\). More details regarding the adapter \\(\\Delta_t\\) will be provided in \u00a72.2. After this adaptation, the modified model \\(\\Theta_{\\sum(t)}\\) can be utilized for any test input relevant to the context \\(\\sum(t)\\) during inference. For example, if the context \\(\\sum(t)\\) consists of a user's past conversations, the modified model \\(\\Theta_{\\sum(t)}\\) can effectively summarize or answer questions about these conversations."}, {"title": "2.2 GENERATIVE ADAPTER", "content": "In this paper, we propose using a learned adapter generator \\(G\\) to directly produce the adapter \\(\\Delta\\) based on the streaming context \\(\\Sigma\\). The core idea is to use the adapter generator to project context token embeddings, encoded by the base language model (LM), into the matrices of each layer in the LM. Specifically, we consider only adapting the linear projection layers of the base Transformer model, i.e., the key/query/value/output layers of the multi-head attention unit and the down/up projection layers of the feed-forward network."}, {"title": "2.3 LEARNING TO UPDATE WITH SELF-SUPERVISED PRETRAINING", "content": "To preserve the language modeling capability of the adapted models \\(\\Theta_{\\sum(t)}\\) for \\(t \\in \\{1,2,...\\}\\), we pretrain the weight generator \\(G\\) using the next-token prediction loss of \\(\\Theta_{\\sum(t)}\\) in a self-supervised manner on web corpora. In other words, the adapter generator is trained on top of the frozen base model \\(\\Theta_{base}\\) in an end-to-end fashion. Specifically, we use two self-supervision pretraining tasks: reconstruction and completion."}, {"title": "2.4 NORMALIZATION FOR GENERATED WEIGHTS", "content": "In preliminary experiments, we find that using the naive outer product for generating weights led to instability during training, causing convergence issues. When multiplying the generated matrix with the input vector, the resulting output can either diminish to near-zero or grow excessively large.\nTo address this instability, we introduce normalization into the formulation, i.e.,\n\\[W_{\\Delta} \\leftarrow A_1 \\operatorname{norm}(S_t)B_2 = A_1 \\operatorname{norm}\\left(A_2 \\sum_{i=1}^t (H_i H_i^\\intercal) B_1\\right)B_2.\\]\nOur pilot experiments find that normalization based on singular value decomposition (SVD) is particular effective, among other normalization strategies.\nSVD Normalization The SVD normalization technique ensures the singular values of the outer product are normalized to 1. Given a matrix M, we define SVD normalization as:\n\\[\\operatorname{norm}(M) = UV^\\intercal,\\]\nwhere \\(M = U\\Sigma V^\\intercal\\) is the SVD factorization. This normalization resets the positive singular values of the matrix to one, preventing the vectors from excessively shrinking or exploding.\nLow-Rank SVD and LoRA An additional benefit of SVD normalization is that it can naturally produce low-rank matrices. Instead of performing a full-rank decomposition, we approximate the input matrix with a rank-\\(r\\) SVD decomposition, where \\(r\\) is a hyperparameter set in advance. Consequently, the matrix can be written as the product of two low-rank matrices, similar to a LoRA adapter (Hu et al., 2022):\n\\[\\begin{aligned}\nW_{\\Delta_t} &\\leftarrow A_1 \\operatorname{norm}(S_t) B_2 \\\\\n&= (A_1 U(H))(V^\\intercal(H)B_2),\n\\end{aligned}\\]\nwhere \\(U(H)\\) and \\(V(H)\\) are the matrices resulting from SVD normalization. This low-rank approximation reduces both computational cost and memory usage."}, {"title": "3 EXPERIMENTS SETTINGS", "content": "We experiment with using both Mistral-7B-Instruct (v0.2) (Jiang et al., 2023) and Llama2-7B-Chat (Touvron et al., 2023) as the base LMs. For efficiency, our main experiments train adapter generators to only update the output projection layers of the multi-head attention unit in Transformer. We study a more capable implementation in \u00a75 and defer the full exploration of other modules for future work.\nHyperparameters The intermediate dimension \\(d_r\\) and SVD rank \\(r\\) are set to 1,024 and 128, respectively. Approximately, this leads to 500 million parameters for the generator, with the generated adapter of 32 million parameters."}, {"title": "4 MAIN RESULTS", "content": "We evaluate GenerativeAdapter on three representative scenarios where contextualizing pretrained LMs is crucial, i.e., acquiring new factual knowledge (\u00a74.1), learning from demonstrations (\u00a74.2), and personalizing to individual users (\u00a74.3)."}, {"title": "4.1 DOCUMENT-BASED QUESTION ANSWERING WITH VARYING CONTEXT LENGTH", "content": "The factual knowledge stored in the parameters of a LM remains static after pretraining. Here, we consider the scenario where the model needs to adapt to new knowledge based on documents. After adaptation, it is expected to correctly answer information-seeking questions about these documents.\nSetup and Baselines To evaluate the fact recall ability of the adapted model, we use two question answering (QA) datasets, SQUAD (Rajpurkar et al., 2016) and StreamingQA (Liska et al., 2022), where each test case consists of a passage and a corresponding question about some information from that passage. To analyze the impact of context length on performance, we conduct an evaluation using contexts of varying lengths.\nWe divide the documents in the corresponding test set evenly into groups, with each group having an average length of k tokens (\\(k \\in \\{512, 1K, 2K, 4K, 8K, 16K, 32K\\}\\)). Thus, the model should contextualize on the article in each group and evaluate fact recall by the question associated with the articles. The QA accuracy is evaluated by comparing the generated output with the gold answer for all questions associated with the documents within the group. Following Rajpurkar et al. (2016), F1 score is used as the metric for evaluation.\nWe also analyze the computational and storage requirements of GenerativeAdapter, which comprises three phases: general-purpose pretraining, contextualization, and inference. The generator is pretrained once and can subsequently be used for any task. During the contextualization phase, GenerativeAdapter encodes the context into an adapter with a single forward pass. In the inference phase, the adapted model generates responses based on the input. Beyond the LM parameters, the extra storage required includes the parameters of the generative adapter.\nHere, we consider both full parameter fine-tuning and full context prompting using the same base model as baselines. For fine-tuning, we consider two variants. The first approach, supervised fine-tuning (SFT), trains the base model exclusively on a training set of question-answer pairs sourced from articles distinct from those in the test set. The second variant, known as continual pretraining (CPT), involves first training the base model on all documents in the test set, followed by further adaptation through SFT using the the training set of question-answer pairs. During inference, we evaluate the fine-tuned model in a closed-book manner, i.e., the model is tasked with directly producing the answer to a given question. For prompting, we simply concatenate all documents in the group as a single context and prompt the base model to respond accordingly. Specifically, for Llama2-7B-Chat, if the context length exceeds the maximum limit of 4K tokens, we truncate the prompt to include only the last 4K tokens. For GenerativeAdapter, we create an adapted model for each document group, which is similar to how the context is encoded as prompting. After that, the adapted model is asked to answer the question again in a closed-book fashion, akin to fine-tuning."}, {"title": "4.2 IN-CONTEXT LEARNING WITH VARYING IN-CONTEXT EXAMPLES", "content": "In the prompting paradigm, one emerging ability of pretrained LMs is that they can perform a task with a few task-specific input-output examples as context on unseen cases, also known as in-context learning (Brown et al., 2020). Here, we are interested to see whether GenerativeAdapter can provide further benefits in enhancing the base LM's in-context learning ability.\nSetup and Baselines We conduct experiments using MetaICL (Min et al., 2022), consisting of 26 test tasks. We also ensure that none of these test tasks were seen during the training of adapter generator. For each task, we use 1, 2, 4, 8, and 16 demonstrations randomly sampled from the"}, {"title": "4.3 PERSONALIZATION", "content": "Using LMs to analyze users' behaviours and memorize their preferences is the key to unlocking a tailored and engaging user experience, i.e., personalized LMs. Towards this goal, we focus on evaluating the LM's ability to memorize user information in conversations.\nSetup and Baselines We use the Multi-Session Conversation (MSC) dataset (Xu et al., 2022) for our experiments, following Packer et al. (2024). Each test case comprises a multi-session human-human conversation between two participants, along with a question regarding information mentioned within the conversation. The average length of the conversational context is 2.5K tokens, which makes it inefficient to prompt the model repeatedly with the entire conversation history for the same user. Similar to document-based QA (\u00a74.1), we evaluate the model quality using the F1 score by comparing the generated answers to the ground truth. We also report computation and memory costs. Here, we use Mistral-7B-Instruct as the base LM.\nAs baselines, we include both closed-book and full-conversation prompting based on the base LM, where the former involves random guesses and the latter incurs higher computation and memory costs by storing the entire long conversation. We also include the state-of-the-art prompt compression method, UltraGist (Zhang et al., 2024), which reduces the context into fewer token embeddings, thereby saving computation and memory costs."}, {"title": "5 ANALYSIS", "content": null}, {"title": "5.1 ABLATION STUDY", "content": "Here, we examine how different design choices with GenerativeAdapter affect model performance. Specifically, we train adapter generators for Mistral-7B-Instruct under various configurations and evaluate them using two metrics-reconstruction perplexity and completion perplexity-on a validation set drawn from the same distribution as the pretraining corpus. As we observe in our preliminary study, the quality of the resulting adapter generator is highly correlated with these metrics. The results are presented in Table 2, where the default setting is described in \u00a72.\nMix of both pretraining tasks is necessary. As we can see, relying solely on one task does not yield good perplexity on the validation set for both metrics. In particular, training with only reconstruction task results in a significant drop in completion perplexity, indicating a loss of general language modeling capabilities and overfitting to memorization. Thus, the completion task acts as data regularization for GenerativeAdapter, enabling the model to distill contexts into generated adapters and effectively utilize them in future predictions.\nSVD is a more effective normalization. We explore an alternative normalization for Equation 6 using the Frobenius norm, defined as \\(\\operatorname{norm}(M) = \\frac{M}{\\|M\\|_F}\\), where \\(\\|M\\|_F = \\sqrt{\\sum_{i,j} M_{ij}^2}\\). Compared to SVD used in our default setting, the Frobenius norm is computationally simple and helps bound the matrix scale. However, our results indicate that the Frobenius norm is not as effective. Observing substantial disparities in the scale of singular values, we hypothesize that applying the Frobenius norm may unnecessarily shrink certain directions, reducing the model's expressiveness.\nMore update parameters lead to better performance. By default, we add the adapter into the output projection layer of the attention module. We also experiment with adding the adapter to the down projection layer within the feedforward module, which introduces more update parameters (3x those of default). We observe that placing the adapter on the feedforward layer indeed leads to slightly improved reconstruction and completion perplexities. Due to computational constraints, a more thorough exploration was not feasible and we leave this for future investigation."}, {"title": "6 RELATED WORKS", "content": "Fast Weights: Our proposed method is closely related to the idea of \"fast weights\" (Hinton &\nPlaut, 1987; Ba et al., 2016; Schlag et al., 2021), which makes the model weights being adaptive to\nthe model input. Context-dependent fast weight programmers (FWPs) introduced by Schmidhuber\n(1992; 1993) use a slow network with slow weights to reprogram the fast weights of the correspond-\ning fast network. Schlag et al. (2021) point out that self-attention without softmax and other linear\nTransformer variants (Tsai et al., 2019; Katharopoulos et al., 2020; Choromanski et al., 2021; Peng\net al., 2021) can be viewed as FWPs. Clark et al. (2022) propose fast weight layers which are added\non top of the Transformer model after the last attention layer for language modeling. Different from\nprevious work mainly focusing on specific tasks, our goal is to enhance frozen pretrained LMs with\nfast associative memory for general language processing. Instead of using a slow network to pro-\ngram a separate fast model, our method can be viewed as a self-programming model, i.e., context\nencoded by the base LM is used to update the base LM itself.\nAdapting LMs via Meta-Learning: One line of work focuses on adapting pre-trained LMs for an\nonline stream of documents using meta-learning. Observing that naively fine-tuning on the docu-\nments using the negative log-likelihood loss is not effective for downstream question answering, Hu\net al. (2023) propose context-aware meta-learned loss scaling to re-weight the loss for individual to-\nkens based on their importance during the online fine-tuning. Tack et al. (2024) use a meta-learned\namortization network to directly predict parameter efficient fine-tuning modulations of the base LM\nfor individual context documents. The modulations are then aggregated into a single output for\ndownstream question answering. Unlike those methods that typically require a nested training loop,\nour adapter generator augments pretrained LMs and our model can be trained in an end-to-end fash-\nion with self-supervised objectives.\nParameter-Efficient Fine-Tuning (PEFT): GenerativeAdapter employs a low-rank adapter\nakin to LoRA (Hu et al., 2022), which was originally designed for PEFT. Several derivatives of\nLoRA exist such as AdaLoRA (Zhang et al., 2023) and DoRA (Liu et al., 2024), along with various\nother PEFT strategies such as serial adapters (Houlsby et al., 2019) and prefix tuning (Li & Liang,\n2021). A thorough survey of PEFT methods is presented by Han et al. (2024). Most work focuses\non task-specific fine-tuning scenarios. Instead, GenerativeAdapter is a general LM and does\nnot require a downstream dataset for adaptation."}, {"title": "7 CONCLUSION", "content": "In this work, we introduce GenerativeAdapter, a method for efficiently adapting pretrained LMs on-the-fly using test-time context through forward passes only. We design a adapter generator network on top of frozen pretrained LMs, transforming text contexts into updated model parameters. Our adapter generator network is trained end-to-end with the frozen pretrained LM using two self-supervised tasks on web corpora. We assess GenerativeAdapter across three scenarios that benefit from contextualizing pretrained LMs: acquiring new factual knowledge, learning from demonstrations, and personalizing to individual users. Our experiments indicate that GenerativeAdapter reduces information loss compared to full-context prompting in retaining factual knowledge from context documents. Additionally, the model effectively adapts to new task instructions when learning from demonstrations. Finally, GenerativeAdapter achieves comparable fact recall performance to efficient prompting methods while utilizing lower inference-time computation, showcasing its feasibility for user-specific adaptation in personalization scenarios. For"}, {"title": "A DATASET DETAILS", "content": "Pretraining. We pretrain our models using a randomly sampled subset of 1B tokens from the\nSlimPajama corpus. For validation, we sample an additional 100 segments, each containing 2K\ntokens, from the same corpus.\nInstruction Tuning. We perform instruction tuning using a combination of question answering,\nin-context learning, and instruction following datasets, following prior studies (Lin et al., 2024; Ge\net al., 2024; Zhang et al., 2024)."}, {"title": "B TRAINING SETUP", "content": "Implementation We empirically found that normalization is crucial for GenerativeAdapter\nto function effectively.\nFor SVD normalization, we implemented it using\ntorch.svd_lowrank(),setting the number of iterations to 1.\nGenerativeAdapter is able to generate the adaptors for prefixes of chunks simultaneously by\nprocessing the context chunks in parallel. The computation proceeds by processing the hidden\nstates of each Transformer block for all context chunks layer by layer. Given the hidden states of\n\u03a3(1),..., \u03a3(t) from the (l \u2013 1)-th Transformer block, denoted by \\(H_{1:t}^{(l-1)}\\), we first compute the\naccumulated outer product \\(S_t\\) using Equation 2. We then normalize this outer product to obtain\nthe additive matrix \\(W_{\\Delta_{1:t}}^{(l)}\\) using Equation 6, and finally get the output of the l-th Transformer block\n\\(H_{1:t}^{(l)}\\) by the base model.\nPretraining. For Mistral-7B-Instruct (hereafter referred to as Mistral), we use a learning rate of\n5 \u00d7 10\u22125, and for Llama2-7B-Chat (Llama2), we use 1 \u00d7 10\u22124. We apply a weight decay of 0.01\nand no dropout. The adapter added to the base model are scaled by 1/16 for Mistral and 1/8 for\nLlama2. We employ a WarmupDecayLR learning rate scheduler with a 100-step warmup and use\nthe Adam optimizer. The global batch size is set to 8. Pretraining the adapter generator on 1B tokens\ntakes approximately 20 hours using 8 NVIDIA H100 GPUs.\nInstruction Tuning. For instruction tuning, we largely follow the same configurations as in pre-\ntraining, with some adjustments. We set the learning rate to 5 \u00d7 10\u22125 for both Mistral and Llama2\nmodels. We train the models for 2 epochs and use a batch size of 32."}, {"title": "C EXPERIMENT SETUP", "content": "Document-based QA. We set up experiments for document-based question answering (QA) using\nboth supervised fine-tuning and continuous pretraining. For supervised fine-tuning on question-\nanswer pairs, we train on the training split of each dataset, evaluate on a validation set, and employ\nearly stopping when the validation loss increases. We use a learning rate of 1 \u00d7 10\u22125 and a global\nbatch size of 64. For continuous pretraining, we train for 8 epochs using the log-likelihood of the\ndocument as the training loss, with learning rates of 1 \u00d7 10\u22125 for Mistral and 3 \u00d7 10\u22125 for Llama2.\nEach passage is treated as a training sample, and we use a global batch size of 16.\nFor closed-book prompting and in-context prompting, we apply an instruction template to encourage\nthe model to generate a short answer.\nIn-Context Learning. We explore in-context learning using both fine-tuning and prompting meth-\nods. For fine-tuning, we conduct task-specific fine-tuning on 16 samples for each dataset. We use\na learning rate of 5 \u00d7 10\u22126 for Mistral and 1 \u00d7 10\u22125 for Llama2. A validation set of 16 samples,\ndisjoint from the training set, is collected from the same dataset. We train the model for a maximum\nof 40 epochs, employing early stopping if the validation loss increases for three consecutive epochs.\nFor in-context prompting, we observe that omitting additional instructions yields better performance\nfor Mistral, whereas adding an instruction template improves performance for Llama2."}]}