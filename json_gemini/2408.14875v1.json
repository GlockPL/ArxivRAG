{"title": "Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures", "authors": ["Pooja Krishan", "Rohan Mohapatra", "Saptarshi Sengupta"], "abstract": "The emergence of deep learning models has revolutionized various industries over the last decade, leading to a surge in connected devices and infrastructures. However, these models can be tricked into making incorrect predictions with high confidence, leading to disastrous failures and security concerns. To this end, we explore the impact of adversarial attacks on multivariate time-series forecasting and investigate methods to counter them. Specifically, we employ untargeted white-box attacks, namely the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs to the training process, effectively misleading the model. We also illustrate the subtle modifications to the inputs after the attack, which makes detecting the attack using the naked eye quite difficult. Having demonstrated the feasibility of these attacks, we develop robust models through adversarial training and model hardening. We are among the first to showcase the transferability of these attacks and defenses by extrapolating our work from the benchmark electricity data to a larger, 10-year real-world data used for predicting the time-to-failure of hard disks. Our experimental results confirm that the attacks and defenses achieve the desired security thresholds, leading to a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk datasets respectively after implementing the adversarial defenses.", "sections": [{"title": "1. INTRODUCTION", "content": "A time-series records a series of metrics over regular intervals of time as a sequence of values. Time-series forecasting is the task of estimating the output at a certain time step, given the previous values. It is used in a variety of domains such as finance (Sezer, Gudelek, & Ozbayoglu, 2020), power consumption prediction (Divina, Garc\u00eda Torres, Gom\u00e9z Vela, & V\u00e1zquez Noguera, 2019), health prediction of equipment (C.-Y. Lin, Hsieh, Cheng, Huang, & Adnan, 2019), healthcare (Kaushik et al., 2020), and weather forecasting (Karevan & Suykens, 2020). The widespread use of sensors and actuators has resulted in a proliferation of data, leading to the shift from traditional time-series forecasting methods to deep learning architectures (Siami-Namini, Tavakoli, & Siami Namin, 2018), which are more capable of gleaning insights and identifying long-term trends from the data. However, it is a double-edged sword as deep learning models can be easily compromised by attacks, causing the models to produce incorrect forecasts based on manipulated input data. This gullible nature of deep learning models to attacks paves the way for catastrophic failures in safety-critical applications and leads to the wastage of valuable resources, time, money, and productivity (Akhtar & Mian, 2018a). This opens up a new area of research to develop models resistant to these types of attacks.\nAdversarial attacks on deep learning models are classified into white-box or black-box attacks, and targeted or untargeted attacks depending on the ease of access, and the attacker's goal respectively. In white-box attacks, the attacker knows sensitive model-specific information such as inputs, targets, and gradients (Melis et al., 2021). Conversely, in black-box attacks, the model is viewed as an oracle that outputs values given input data and the attack is crafted based on observed model behavior (Oh, Schiele, & Fritz, 2019; Tsingenopoulos, Preuveneers, & Joosen, 2019). In targeted attacks, the adversary tries to not only delude the model but also prompts it to produce an output from a particular distribution (Fursov et al., 2021) whereas in untargeted attacks the attacker intends to trigger the model to generate incorrect outputs belonging to any distribution (Miller, Xiang, & Kesidis, 2020; J. Lin, Dang, Rahouti, & Xiong, 2021).\nAdversarial defense involves training the deep learning network with augmented data capturing the noise distribution that the attacker plans on using, or modifying the model ar-"}, {"title": "2. RELATED WORK", "content": "(Stergiou & Psannis, 2017) outline the interdependence between Internet of Things (IoT) and Big Data. With the proliferation of sensors that record and share information between devices on the Internet, there is no shortage of data, and developing deep learning models to predict future sensor values has become easy. This has led to a shift from traditional methods of time-series forecasting using model-driven methods to a data-driven method involving multiple deep learning models for time-series forecasting problems (Faloutsos, Gasthaus, Januschowski, & Wang, 2018). (Muzaffar & Afshari, 2019) found that LSTM models (Hochreiter & Schmidhuber, 1997) are better at predicting electricity consumed, given exogenous attributes such as temperature, humidity, wind speed, etc. (Wu, Liao, Miao, & Du, 2019) prove that using a Gated Recurrent Unit (GRU) (Cho et al., 2014) to predict power consumed in New South Wales in Australia led to much better forexcasting results than using traditional models. (Mishra, Basu, & Maulik, 2019) use a dilated temporal CNN to capture load consumption using multiple synthetic and real-world datasets accurately. citets4 prove that a Bidirectional Recurrent Neural Network (BRNN) is better at forecasting traffic flows using the GPS data collected from the Hohhot Bus Corporation than LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Cho et al., 2014) models. (Orimoloye, Sung, Ma, & Johnson, 2020) showcase the advantages of using deep networks in comparison to shallow ones while predicting stock prices. (Yan & Ouyang, 2018) compare conventional machine learning models to deep learning ones which use a combination of wavelet decomposition and LSTMs (Hochreiter & Schmidhuber, 1997) in stock market predictions. (Chen, 2024) use an LSTM working through the attention mechanism to predict the RUL of aircraft engines. (Al-Dulaimi, Zabihi, Asif, & Mohammadi, 2019) have used a neural network model combining LSTM (Hochreiter & Schmidhuber, 1997) and CNN (LeCun, Bottou, Bengio, & Haffner, 1998) to predict the RUL of aircraft engines. (Deutsch & He, 2018) show the advantages of using a deep learning model to predict the RUL of spinning parts.\nDespite these advances, deep learning models remain vulnerable to adversarial attacks. (Szegedy et al., 2014) reported vulnerabilities in deep learning models in image recognition, where Convolutional Neural Networks (CNNs) (LeCun et al., 1998) can be manipulated by injecting minute modifications into the data, thereby leading the network to miscalculate the input with high conviction. (Akhtar & Mian, 2018b) have presented a survey of all types of antagonistic manipulations and their impacts on image recognition such as self-driving cars (Eykholt et al., 2018), robotic vision (Melis et al., 2017), cyberspace attacks (Papernot et al., 2017), etc. With the development of multiple threat models to perform adversarial attacks, it is no surprise that a lot of effort went into developing adversarial defense strategies for these attacks in image recognition. (Akhtar, Mian, Kardan, & Shah, 2021b) summarize the adversarial attacks and defense mechanisms developed in computer vision in recent years.\nIn recent times, adversarial attacks and defense strategies used in time-series analysis have garnered the interest of researchers. (Rathore, Basak, Nistala, & Runkana, 2020) have demonstrated the effect of targeted, untargeted, and global attacks and adversarial fortification in time-series data."}, {"title": "3. EXPERIMENTAL SETUP", "content": "We first carry out our experiments on a smaller dataset which is used to predict the power consumed sometime in the future, given past readings. Once we have proved the success of the attacks and defense techniques in the electricity dataset, we repeat the experiments on a substantially larger dataset which is used to predict the RUL of Hard Disk Drives (HDDs) to prove the ease of transferability of these attacks and defenses. We describe the datasets used in this research and the preparation steps done to make the data more viable for ingestion by the deep learning models in this section."}, {"title": "3.1. Individual Household Power Consumption Dataset", "content": "Power consumption prediction is a vital task to estimate the amount of power that has to be supplied to various locations at any given time. It has a direct bearing on the environment and helps to cut costs. Motivated by the applications of power consumption prediction and to demonstrate the effects of antagonistic manipulation and fortification on a multivariate time-series dataset, we chose the Electricity dataset from the UCI repository (Hebrail & Berard, 2012) in this research.\nIt consists of over 2 million rows and 9 columns sampled by minute in a household for 4 years from the end of 2006 to the end of 2010. global active power represents the active power consumed in kW and global reactive power represents the reactive power consumed in kW, voltage and global intensity represents the mean voltage in volts and mean current in amperes respectively. sub metering 1, sub metering 2, and sub metering 3 is the active energy readings from the kitchen, laundry room, and electric water heating and air conditioning equipment respectively. The unit of all sub-metering readings is in watt-hour. The active term refers to the total actual power consumed whereas the reactive term corresponds to the unused power in the transmission wires.\nDuring preprocessing of the dataset, we treated the missing values (?) as null values (NaN) for simplicity. Each column has a NaN value, and we replace it with the mean of the respective column values. The values in the dataset are normalized using the smallest and largest values of the samples and they all lie in the range 0 and 1 to ensure consistency in predictions. We resampled the dataset daily, incorporating the average of the per-minute values thereby generating a dataset with 1400 samples to predict global active power. From Figure 1, which shows the distribution of global_active_power per day, per week, per month, and per quarter, it is evident that the periodicity of the distribution decreases as the time interval increases."}, {"title": "3.2. Backblaze Hard Disk Drive Dataset", "content": "Before the surge in big data, model-driven approaches to Prognostic Health Management (PHM) depended on reactive maintenance and preventative maintenance techniques. In reactive maintenance, the hard disks are replaced only after failure resulting in disruption of normal operations until the drive is fixed. In preventative maintenance, the hard disks are replaced well before failure leading to the replacement of a fully functioning disk and resulting in wastage of resources. In data-driven approaches, predictive maintenance is performed using the Remaining Useful Life (RUL) metric. RUL is an important metric used to indicate the time to failure of any equipment. To prove the transferability of the attacks and defenses on any real-world dataset, we employ the data store from Backblaze (2023) housing the hard drive sensor"}, {"title": "4. TRAINING PROCESSES", "content": "We use a vanilla LSTM model (Hochreiter & Schmidhuber, 1997) on the Electricity dataset and an Encoder-Decoder LSTM model (Sutskever, Vinyals, & Le, 2014) on the HDD dataset. The next two subsections consist of details about the experiments performed."}, {"title": "4.1. Individual Household Power Consumption Dataset", "content": "We propose a vanilla LSTM model due to the fixed number of attributes and target values and because it is also the most prevalent model for multivariate time-series regression tasks on the Electricity dataset (Alden, Gong, Ababei, & Ionel, 2020) (Ibrahim, Megahed, & Abbasy, 2021) (da Silva, Geller, dos Santos Moura, & de Moura Meneses, 2022).\nThe dataset is divided into 80%, 10%, and 10% for the training, validation, and test sets respectively. We trained the vanilla LSTM model consisting of a sequential layer followed by 100 hidden nodes with the ReLu activation function. ReLU overcomes the problem of vanishing gradients by outputting a value equal to the input if the input is positive. We also added a 10% dropout to regularize the network and prevent overfitting. Finally, we added a dense fully connected layer to the LSTM. Adam (Kingma & Ba, 2017) is used as the optimizer and the metric defined is Root Mean Squared Error (RMSE).\nWe divided the experiment into 3 parts to validate our findings and the behavior of the model:\n1) Without using cross-validation: We used the vanilla LSTM model to predict the global active power target variable by using the same validation set to inform training.\n2) Using walk-forward cross-validation: We repeated the experiment with 3, 5, and 10 cross-validation. Since classical k-fold and stratified cross-validation schemes shuffle the data and disrupt the order and seasonality of time-series input, we used walk-forward cross-validation. In walk-forward cross-validation, the first few data points in a finite window correspond to the training set and the next few data points correspond to the validation set. In the next iteration, more data points that were formerly in the validation set are included in the train set, and the window is expanded to include subsequent data points in the validation set. Based on the amount of folds, this process is repeated and the average RMSE score is reported as the final training and validation RMSE scores.\n3) Using look-back: The vanilla LSTM model trained so far looked back one day in the past to predict the future. We conducted experiments by increasing the look-back window size to allow the model to consider more samples from the past while making future predictions. The look-back window sizes we used in our experiments are 3, 6, 9, 12 and 15 days. The graph of the RMSE values for the train and test sets for"}, {"title": "4.2. Backblaze Hard Disk Drive Dataset", "content": "We propose an Encoder-Decoder LSTM (Sutskever et al., 2014) as the model to learn the underlying distribution of the HDD dataset from Backblaze. We chose this model architecture because of the varying nature of the input sequence due to the addition of S.M.A.R.T features over the years, and also since we are aiming to predict a sequence of RUL values given the sequence of inputs. This Encoder-Decoder model generates the most likely sequence given a sequence of data. The encoder scans the input and outputs a constant-size array called the context vector, and the decoder reads from the context vector.\nSimilar to the electricity dataset, this dataset is split following the 80-10-10 rule for the train, validation, and test sets respectively. Our Encoder-Decoder LSTM model consists of a Sequential Layer of one hundred hidden units with ReLu activation. We look 5, 15, 25, 35, and 45 days into the past to check the effect of feeding more past data to the model. Since we are looking back 't' periods, the output of the encoder is repeated 't' times before passing it through another ReLu layer with 100 units We added a 10% dropout to prevent overfitting and help in generalization. A dense layer is added to every period of the decoder's output series using the Time Distributed wrapper thereby enabling the model to predict the RUL for each time step. Adam optimizer is utilized to fine-tune the weights during training, and RMSE is used as metrics. For higher lookback time steps, the gradients explode and to prevent NaN metrics, the gradients are clipped if they exceed 0.5.\nWe divide our experiments into two to choose the top-performing model to execute the attacks and defenses as follows:\n1) Using look-back: We facilitate the learning of the Encoder-Decoder LSTM model by providing the five different kinds of datasets generated in Section 3.2. From Figure 5, it is clear that increasing the look-back window exacerbates predictions"}, {"title": "5. ATTACKS AND DEFENSE STRATEGIES", "content": "We illustrate the process of performing adversarial attacks and adversarial defenses in this section."}, {"title": "5.1. Overview of Process Flow", "content": "We demonstrate that the adversarial attacks are successful and exploit the sequential nature of deep learning networks and their susceptibility to adversarial perturbations using FGSM, and BIM attacks. Once the best LSTM models selected in Section 4.1 and Section 4.2 succumb to the two types of attacks mentioned above, we perform adversarial defenses using data augmentation at the data plane, and layer-wise perturbations at the gradient plane to inform the training process. This ensures model robustness to adversarial attacks. The entire process flow is summarized in Figure 7."}, {"title": "5.2. Adversarial Attacks", "content": "We perform two types of adversarial perturbations of the data."}, {"title": "5.2.1. Fast Gradient Sign Method (FGSM)", "content": "(Goodfellow, Shlens, & Szegedy, 2015) proposed FGSM to perform adversarial perturbations on CNNs for image data. The FGSM algorithm adds disturbances to the input in the direction of the gradients with regard to the loss function of the data. We have extended the algorithm to multivariate time-series datasets described earlier. The equation for FGSM attack is:\n$X_{adv} = X + \\epsilon sign(\\nabla J(f, X, y))$"}, {"title": "5.2.2. Basic Iterative Method (BIM)", "content": "(Kurakin, Goodfellow, & Bengio, 2017) proposed BIM which applies the FGSM attack multiple times. Since in each iteration, the attack forces the model to add noise or perturbation in the direction of the gradients with regard to the loss function, this attack mechanism is generally considered to be more powerful than FGSM. We have used BIM to attack the best-performing vanilla LSTM and Encoder-Decoder LSTM models selected earlier. Its equation is given as follows:\n$X_{adv} = X + \\alpha sign(\\nabla J(f, X, y))$\n$X_{adv} = min(X + \\epsilon, max(X \u2013 \\epsilon, X_{adv}))$"}, {"title": "5.3. Adversarial Defenses", "content": "We enumerate the two types of adversarial defenses performed, in this section."}, {"title": "5.3.1. Data Augmentation-based Adversarial Training (DAAT)", "content": "DAAT is a na\u00efve process of using adversarial attacks to create adversarial examples and augmenting the dataset to incorporate these examples during the training of the deep learning models (Goodfellow et al., 2015). This mechanism is used after anticipating the types of perturbations the adversary can use during the attack. DAAT provides the deep learning models with prior information necessary to stay resilient to attacks during the training process. We used the adversarial attacks introduced earlier to augment the dataset with adversarial examples for different values of perturbation magnitude epsilon. Then we trained a robust classifier on the augmented dataset. This robust classifier is more resistant to adversarial attacks since it has been trained to predict the right values, given the perturbed values in its training set. The system architecture of DAAT is shown in Figure 8."}, {"title": "5.3.2. Layer-wise Perturbation-Based Adversarial Training (LPAT)", "content": "(Zhang, Wang, He, Li, & Yu, 2018) proposed LPAT for hard drive health prediction, where they train an LSTM network to handle class imbalances better. This robust LSTM network is trained by going through two rounds of feed-forward and backpropagation in each iteration. The first round is similar to any neural network architecture where the outputs are computed in the forward propagation step, and the gradients are updated in the backpropagation step. In the second round, however, the gradients in each layer are perturbed"}, {"title": "6. RESULTS", "content": "In this section, we disclose the findings from carrying out the attacks and defenses on the final deep learning models selected in Section 3."}, {"title": "6.1. Individual Household Power Consumption Dataset", "content": null}, {"title": "6.1.1. Results of Adversarial Attacks", "content": "We contaminate the inputs to the vanilla LSTM model during the evaluation stage using the FGSM and BIM attacks mentioned in Section 5.2. For this experiment with the electricity dataset, we find that smaller orders of perturbation magnitude between 0.05 and 0.25, incremented in steps of 0.05 give test"}, {"title": "6.1.2. Results of Adversarial Defenses", "content": "We carry out two different types of adversarial defense strategies to harden the best-performing vanilla LSTM model on the electricity dataset. In DAAT, we augment the dataset with perturbed input attributes created by performing adversarial perturbations on the original feature samples. We train the vanilla LSTM network to learn the fluctuations in the features and predict the correct target. The different RMSE values obtained after performing DAAT on the electricity dataset are tabulated in Table 4. Although BIM is a stronger attack than FGSM, DAAT on the electricity dataset is slightly more resilient to BIM attack. The observed phenomenon can be ascribed to the broader range of variations in the test features"}, {"title": "6.2. Backblaze Hard Disk Drive Dataset", "content": null}, {"title": "6.2.1. Results of Adversarial Attacks", "content": "We poison the test inputs to the best-performing model identified in Section 4.2 using the FGSM and BIM attacks. We used epsilon ranging from 3 to 11 incremented in steps of 2 such as 3, 5, 7, 9, and 11 since the Encoder-Decoder LSTM model is inherently robust to lower \\epsilon resulting in only a 0.92% to 3.87% increase in error rate for FGSM and a 1.12% to 4.7% increase in error rate for BIM for \\epsilon between 0.05 and 0.2."}, {"title": "6.2.2. Results of Adversarial Defenses", "content": "We performed two types of adversarial defenses to make the best-performing Encoder-Decoder LSTM more resilient to"}, {"title": "7. LIMITATIONS AND FUTURE WORK", "content": "Research in adversarial attacks and defenses for multivariate time-series forecasting is nascent, with many areas unexplored. This study focuses on white-box attacks, which necessitate knowledge of model parameters and inputs. In the future, we will explore black-box attacks to construct an attack model using the estimates of the target model alone. We aim to develop a model that can discern the adversary's \\epsilon range based on prediction deviations and identify the maximum perturbation beyond which defense fails. Adversarial training with BIM as the perturbation method yields superior results, suggesting further exploration of the impact of \\epsilon on training, as BIM is a stronger attack than FGSM. We find that stochastic gradient update is highly dependent on the random values of \\epsilon picked during each iteration of backpropagation, warranting research into the selection of \\epsilon during the training process Investigating black-box transferability and integrating DAAT and LPAT into a hybrid approach could enhance model robustness against adversarial attacks."}, {"title": "8. CONCLUSION", "content": "Our study investigates the susceptibility of deep learning models in multivariate time-series forecasting to adversarial attacks and evaluates defense mechanisms. We show that models like vanilla LSTM and Encoder-Decoder LSTM when tested on the Individual Household Power Consumption and Backblaze Hard Disk Drive datasets, undergo significant performance degradation under adversarial perturbations like FGSM and BIM. The average error rate increases by 248.17% and 223% on the electricity and HDD datasets respectively, highlighting the impact of these attacks.\nWe also visualize the subtle changes to the input distribution post-attack, which are not easily detectable. To counter these vulnerabilities, we implement two robust defenses: Data Augmentation-based Adversarial Training (DAAT) and Layer-wise Perturbation-based Adversarial Training (LPAT). DAAT, particularly with BIM for augmentation, greatly enhances model resilience, reducing errors by up to 72.41% and 94.81% for the electricity and HDD datasets respectively.\nLPAT also shows effectiveness, with performance varying based on perturbation magnitude. These results emphasize the need for adversarial defense strategies in deep learning models for critical applications in smart and connected infrastructures, enhancing model reliability and ensuring secure, dependable predictions."}, {"title": "APPENDIX", "content": null}, {"title": "A. TIME-SERIES DATA CHARACTERISTICS", "content": "Time-series data are characterized into two types - stationary time-series where the mean or variance is a constant given by \\mu = c or \\sigma = c or non-stationary time-series in which the mean or variance varies with time given by \\mu = f(t) or \\sigma = f(t). Figure 26 shows a stationary time-series with constant mean and variance, and a non-stationary time-series where the mean increases with time as shown by the dotted line, and the variance representing the distance between consecutive peaks decreases with time."}, {"title": "B. SUPPLEMENTAL BACKGROUND", "content": "(Croce & Hein, 2019) propose a new black-box attack to apply sparse perturbations to image pixels leading to unnoticeable changes in the resultant image. (Dong et al., 2018) iteratively attacked the samples by introducing a momentum term that prevents the gradients from being stuck in a local maxima resulting in a much more powerful attack. A paper published by (Engstrom, Tran, Tsipras, Schmidt, & Madry, 2018), outlines the vulnerability of CNNs (LeCun et al., 1998) to benign rotations and transformations. (Finlay, Pooladian, & Oberman, 2019) use a log barrier-based optimization technique to solve the constrained optimization problem that aims to minimize the perturbation magnitude in adversarial attacks. (Huang et al., 2019) show that an intermediate-level attack ensures high transferability of adversarial attacks between architectures. (Moosavi-Dezfooli, Fawzi, Fawzi, & Frossard, 2017) identify the presence of global perturbations which are independent of the images and depend on the geometric modeling of the decision edges of deep learning algorithms. (Su, Vargas, & Sakurai, 2019) proved that CNNs are susceptible to attacks of lower dimensions by modifying only one pixel based on differential evolution to perturb images.\n(Sankaranarayanan, Jain, Chellappa, & Lim, 2018) suggest efficient layer-wise training to prevent overfitting in deep networks. (Mustafa et al., 2019) propose a convex polytope-based separation of features during learning, such that independent variables of various targets are maximally separated from one another. (Wang & Zhang, 2019) introduce a bilateral adversarial training framework by perturbing the labels and the features during the training process. (Madry, Makelov, Schmidt, Tsipras, & Vladu, 2017) perform adversarial training through robust optimization, exploring the universal transferability during training. (Jeddi, Shafiee, Karg, Scharfenberger, & Wong, 2020) introduce a framework that introduces perturbations during the training and inference and efficiently learns to detect noise in the input. (Dong, Deng, Pang, Zhu, & Su, 2020) outline a process called adversarial distribution training in which the internal maximization function seeks to learn the worst distribution possible, and the external minimization function seeks to minimize the loss over the distribution. (Madaan, Shin, & Hwang, 2020) put forth a novel loss function called vulnerability suppression loss that aims to minimize the latent space feature distortion. (Jang, Zhao, Hong, & Lee, 2019) develop an iterative stochastic generator to generate diverse adversarial examples capable of exposing the vulnerabilities in the target model. (Liu, Park, Hoang, Hasson, & Huan, 2022) propose an attack scheme that introduces imperceptible perturbations to create poisoned examples and training mechanisms based on randomized smoothing to enhance model robustness."}, {"title": "C. INDIVIDUAL HOUSEHOLD POWER CONSUMPTION DATASET", "content": null}, {"title": "C.1. Data Preprocessing", "content": "We conducted experiments using an LSTM which was designed to predict the global_active_power. To facilitate this, we resampled the dataset daily, incorporating the mean of the per-minute values. Figure 27 illustrates the distribution of the mean and sum of the per-minute values when the dataset is resampled daily. It is evident that whether we aggregate over the mean or sum while resampling over the day, the distribution remains consistent. Therefore, the pick of the clustering technique does not significantly impact the model estimates."}, {"title": "C.2. Architecture of vanilla LSTM", "content": "The vanilla LSTM unit shown in Figure 28 consists of:\n1) Forget Gate: The forget gate determines what piece of information to persist and what to delete. The input at a particular time step, i(t), and information from the preceding hidden state, s(t-1) act as inputs for the sigmoid activation, producing an output of 0 (forget), or 1 (remember). Its equation is:\n$f_g(t) = \\sigma(i(t)X_f + s(t \u2212 1)Z_f)$\n2) Input Gate: This gate decides the relevant information to be passed further from the present. The input vector and the previous hidden layer's output are multiplied element by element, after passing through a sigmoid and a tanh function:\n$j(t) = \\sigma(\u03af(t)X_j + s(t \u2212 1)Z_{ig})$\n$k(t) = tanh(i(t)X_k + s(t \u2212 1)Z_k)$\n$ig(t) = j(t) \u00b7 k(t)$\n3) Cell State:It reserves relevant long-term memory and performs element-by-element multiplication of the ouput of the forget gate and previous cell state to preserve only the relevant state of the network, and then performs element-by-element addition with the output of the input gate, given by:\n$C_s(t) = \\sigma(f_g(t) \u00b7 C_s(t \u2212 1) + ig(t))$\n4) Output Gate: This gate decides the resultant value at any period. The input vector, i(t), and the byproduct from the preceding hidden layer, s(t-1) are put into a sigmoid activation to calculate og(t), which is then multiplied with the tanh of the new cell state Cs(t), to pass on as intake of the next time step along with the cell state, C's(t).\n$0_g(t) = \\sigma(\u03af(t)X_{og} + s(t \u2212 1)Z_{og})$\n$s(t) = tanh(C_s(t)) \u00b7 o_g(t)$"}, {"title": "C.3. Training Process", "content": "Loss curve of the train and validation sets for the LSTM without using cross-validation or look back is shown in Figure 29."}, {"title": "C.4. Adversarial Defense Results", "content": "The overall percentage decrease in error on the electricity dataset after performing the different fortifications is tabulated in Table 9 and is illustrated in Figure 17."}, {"title": "D. BACKBLAZE HARD DISK DRIVE DATASET", "content": null}, {"title": "D.1. Architecture of Encoder-Decoder LSTM", "content": "The components of the Encoder-Decoder model used on this dataset are explained below and illustrated in Figure 31.\n1) Encoder: The encoder is composed of several RNNs, LSTMs, or GRUs stacked together accepting an input and propagating that information forward to the next units. The hidden state a(t) is computed from the input array, i(t), and the byproduct of the previous layer, a(t-1) based on the mapping of the chosen unit, if it is RNN, LSTM, or GRU. The final a(t) is composed of all the encoded information from the previous states and hidden layers. Its equation is given by:\n$a(t) = f(Z a(t \u2212 1) + X i(t))$"}, {"title": "2) Context Vector", "content": "The context vector represents the last hidden state output of the encoder and the first hidden state input to the decoder. It is an encoded latent space representation of the inputs and allows the decoder to forecast accurately."}, {"title": "3) Decoder", "content": "The decoder consists of similar stacked recurrent RNN, LSTM, or GRU units, receiving a hidden state from the preceding unit, and calculates its hidden state and the output.\n$b(t) = f(Z a(t \u2212 1))$\nSoftmax activation is applied on the hidden state, b(t), and the corresponding weight to output a probability vector as:\n$\\hat{o}(t) = softmax(Yb(t)) $"}, {"title": "D.2. Training Process", "content": "The loss function of the 5-day look back Encoder-Decoder model without cross-validation is shown in Figure 32."}, {"title": "D.3. Adversarial Defense Results", "content": "Table 10 shows the reduction in error after performing adversarial defense for different perturbation values and different adversarial attacks. and is visualized in Figure 25."}]}