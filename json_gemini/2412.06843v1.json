{"title": "SEMANTIC LOSS GUIDED DATA EFFICIENT SUPERVISED FINE TUNING FOR SAFE RESPONSES IN LLMS", "authors": ["Yuxiao Lu", "Arunesh Sinha", "Pradeep Varakantham"], "abstract": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another LLM to generate corrective data. In this paper, we aim to take this problem and overcome limitations of requiring significant high-quality human data. Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself. By employing a semantic cost combined with a negative Earth Mover Distance (EMD) loss, we guide the LLM away from generating unsafe responses. Additionally, we propose a novel lower bound for EMD loss, enabling more efficient optimization. Our results demonstrate superior performance and data efficiency compared to baselines, and we further examine the nuanced effects of over-alignment and potential degradation of language capabilities when using contrastive data.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable abilities in diverse tasks, such as natural language under- standing, generation, and translation, and attracted lot of attention from various industries and researchers. Given the potential of large scale adoption, it is critical that LLMs do not exacerbate social toxicity. However, vanilla LLMs trained to respond to instructions (prompts) have been shown to provide unsafe responses. With the vast amount of knowledge inbuilt in LLMs due to training on a very large amount of data, LLMs are able to generate responses that can be dangerous, e.g., LLMs can provide instructions on how to download movies illegally Zhang et al. [2024], Gan- guli et al. [2022], Wen et al. [2023]. Further, some responses can be outright toxic that belittle groups of people based on race or gender or other attributes Gehman et al. [2020], Sheng et al. [2019], Brown [2020].\nIn response, a number of works have proposed ways to make LLMs \u2018safe.' One way is Reinforcement Learning from Human Feedback (RLHF) Ziegler et al. [2019], Bai et al. [2022]. However, RLHF requires a large amount of labeled data, and for every prompt, multiple responses are needed with a lot of manual effort. The requirement for large-scale human involvement makes this process time-consuming, labor-intensive, and computationally expensive Ouyang et al. [2022]. Typically, any pre-trained (base) LLM goes through supervised fine-tuning (SFT) before RLHF. SFT is a technique used to adapt a pre-trained (base) LLM to a specific downstream task using labeled data. The majority of LLMs used in 2024 are fine-tuned for chat or instruction-based interactions. Existing work Bianchi et al. [2023] called Safety Tuned Llamas (STL) that aims to make LLMs safe in the SFT stage by using data of safe responses to toxic prompts. Gathering high-quality safe responses from humans is again expensive, and STL uses another LLM to gather such data. Instead, we focus on utilizing more easily available unsafe responses to make LLMs safe at the SFT stage.\nProblem Statement: We aim to make an LLM generate safe responses to toxic prompts in the SFT stage itself but using very few easily available harmful responses. Formally, given a base (non-SFTed) LLM Me with weights \u03b8, we aim to perform SFT with two kinds of datasets: (1) Dsafety-unrelated comprises prompts, response pairs (pj,rj) that are unrelated to safety concerns. By construction, the responses rj = Mo(pj) are assumed to be safe. (2) Dsafety-related consists of prompts, response pairs (pi, ri) where the prompts pi are explicitly designed to be unsafe."}, {"title": "2 Related Work and Background", "content": "Ensuring the safety and fairness of LLM outputs has become a critical area of focus Yuan et al. [2024], Yao et al. [2024]. One of the primary methods to align LLMs with human values is through human preference alignment, with Reinforcement Learning from Human Feedback (RLHF) Ziegler et al. [2019], Bai et al. [2022] and the success of models like ChatGPT has demonstrated the importance and effectiveness of RLHF. Recent works have been proposed to simplify the training process of RLHF Rafailov et al. [2024], Hong et al. [2024], Ethayarajh et al. [2024]. Compared to RLHF, Supervised Fine-tuning (SFT) requires significantly less training data and time. However, the safety issue after SFT has been highlighted by recent studies Zong et al. [2024], Qi et al. [2023], Hsu et al. [2024]. Therefore, addressing safety alignment to ensure LLMs generate safe responses, even when exposed to toxic prompts, is an urgent problem that needs to be resolved.\nRecent work Bianchi et al. [2023] explores improving LLM safety by incorporating safe responses to toxic prompts into the SFT dataset. Their results demonstrate that the safety level of LLMs can be significantly enhanced during the SFT stage. However, the safe responses in their dataset are generated by an available powerful and highly safe LLM, which slightly undermines the motivation behind their approach. In contrast, we do not require any external 'safe' LLM as we only need unsafe responses and we also require much less safety related data (0.5%) compared to their 3% requirement. As such, safety alignment during the SFT stage remains an attractive avenue due to its efficiency and cost-effectiveness, and this direction is still in its early stages of exploration."}, {"title": "2.2 Background", "content": "The supervised fine-tuning (SFT) of an LLM involves adjusting the parameters of LLM Me such that the pre-trained models adapt to specific tasks. Specifically, given a dataset of N prompt, response pairs (pj,rj) SFT maximizes the likelihood of generating response rj to the prompt pj. For SFT, a standard approach is to use the Negative Log- Likelihood (NLL) loss Radford [2018], which is defined for a set of N prompts (where the prompt is pi and its corresponding response is yi, that is given as a sequence of tokens [Yi,1, Yi,2, ..., Yi,T\u1d62] ) as:\n$\\displaystyle L_{SFT}(\\theta, N) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{T_i} \\sum_{t=1}^{T_i} log Q_{\\theta}(Y_{i,t} | Y_{i,t-1},..., Y_{i,1}, P_i).$ (1)"}, {"title": "3 Method", "content": "We provide a modified supervised fine-tuning protocol on a base LLM, denoted as Me. As stated in the problem statement, the dataset used for modified fine-tuning D = Dsafety-unrelated U Dsafety-related consist of two subsets, one a traditional safety unrelated dataset and another smaller safety related dataset (with only harmful response) that we construct. Our approach is based on minimizing harmful probability of response for toxic prompts and an overview of the approach is shown in Figure 1. To reduce the risk of generating harmful responses, we push the next token prediction distribution away from the distribution observed in the unsafe demonstrations within the safe-related dataset."}, {"title": "3.1 EMD based Approach", "content": "Our main approach is based on using Earth Mover Distance (EMD) to measure the distance between the generated next token prediction distribution and the next token distribution of unsafe responses in data. The EMD measures the \u201ccost\u201d of optimally transporting mass to transform one distribution into another. The cost d(x, y) is defined on the underlying probability space, and it measures the cost of transporting unit probability mass from x to y; the cost is domain dependent. Given such a cost d, the EMD between two distribution P, Q is defined as\n$\\displaystyle EMD(P, Q; d) = \\inf_{\\Upsilon \\in \\Pi(P,Q)} E_{(x,y) \\sim \\Upsilon} [d(x, y)],$ (3)\nwhere II is set of all joint distributions (couplings) such that the marginals of any \u03b3 \u2208 I are P and Q. If the underlying probability space is discrete, which is the case in our work with a finite vocabulary V of the LLM, then EMD can be"}, {"title": "3.2 Likelihood based Approach", "content": "An easier option compared to the use of EMD is to directly penalize the likelihood of unsafe responses during su- pervised fine-tuning. We follow ORPO Hong et al. [2024], but since we do not have pairs of responses but only the"}, {"title": "4 Experiment", "content": "We tested our approach on four different base models which are not SFTed or RLHF fine-tuned: Llama 7b Touvron et al. [2023], Llama 13b Touvron et al. [2023], Mistral 7b Jiang et al. [2023], and Llama3.1 8b Dubey et al. [2024]. For ease of presentation, we use \u201cEMD\u201d and \u201cNLCL\" to refer to our TA-SFT method with the EMD loss and NLCL loss, respectively. All fine-tuning uses low-rank adaptation (LoRA) Hu et al. [2021] for three or four epochs. All models have been trained on L40 or H100 GPUs. More training hyper-parameters can be found in the Appendix."}, {"title": "4.1 Safety Training Dataset Construction", "content": "To the best of our knowledge, there is no existing SFT dataset that combines pairs of safety-unrelated prompts and responses with safety-related pairs (involving toxic prompts and harmful responses). Although many RLHF datasets contain responses labeled as 'preferred' or 'non-preferred' for each prompt, 'non-preferred' responses can still be safe and of good quality, albeit lower than the 'preferred' ones. Therefore, RLHF datasets are not suitable for our study. However, sufficient toxic prompts can be found in datasets for attacking designed by human Bai et al. [2022] or generated automatically Cui et al. [2024]. We obtain harmful responses to these toxic prompts by supervised fine- tuning the pre-trained base LLM under consideration on existing SFT datasets such as Alpaca Taori et al. [2023]. These instruction tuned LLMs are vulnerable to toxic prompts and can easily generate harmful responses Qi et al. [2023]. We use the SFTed LLM to generate harmful responses, and then apply the OpenAI moderation API to extract 1,000 responses that are harmful from the LLM under consideration. These 1000 toxic prompt, response pairs (Dsafety-related) are combined with 20,000 randomly sampled prompt, response pairs from the Alpaca dataset (Dsafety-unrelated) to create the dataset D = Dsafety-unrelated U Dsafety-related used for our approaches."}, {"title": "4.2 Baseline Methods", "content": "The primary distinction of our approach from RLHF is that our data has only one response per prompt, whereas RLHF typically requires a pair of responses for each prompt, making most RLHF methods unsuitable as baselines. However, one of the RLHF method, named KTO Ethayarajh et al. [2024], does not depend on pairwise responses and has even better performance than DPO Rafailov et al. [2024], and can be utilized as the baseline in our study. In the training of KTO, we consider the harmful responses as the \u2018non-preferred' responses and the other as 'preferred' responses. The weight term in KTO loss is tuned as suggested in KTO paper Ethayarajh et al. [2024].\nAs stated in related work, the most closely related work to ours is Safety Tuned Llamas (STL) Bianchi et al. [2023]. However, STL requires high quality safe responses to toxic prompts, which is different from our dataset that has only easily available unsafe responses to toxic prompts. Thus, STL is not directly comparable to our approach. Nonetheless, we compare to an advantaged STL by providing the required data for STL in Section 4.3.4 below."}, {"title": "4.3 Evaluation", "content": "In this section, we evaluate our approach in comparison to existing methods across multiple dimensions including safety level of responses, response quality, data efficiency and over alignment."}, {"title": "4.3.1 Safety Level", "content": "We follow standard practice in literature Bianchi et al. [2023] to evaluate our fine-tuned models on four harmfulness benchmarks: I-Malicious, I-CoNa, I-Controversial, and HarmfulQ, which encompass hateful speech, controversial topics such as vaccination and immigration, and malicious instructions. These four datasets totally contain 518 toxic prompts, providing comprehensive coverage and a thorough test of the model's response to a wide range of toxic inputs.\nTo automatically evaluate the safety level of responses to the toxic prompts, we first utilize a pre-trained DeBERTa model He et al. [2021], which assigns a harmfulness score ranging from 0 (least harmful) to 5 (most harmful). As illustrated in Figure 2, across all four test datasets, both EMD and NLCL loss functions significantly reduce the harmfulness scores of Llama 7b's responses as training progresses, ultimately making them nearly safe. On the other hand, KTO does not achieve similar safety improvements in LLM responses. Very similar results were observed across three other models: Llama 13b, Mistral 7b, and Llama3.1 8b, which are presented in the appendix.\nWhile this automatic evaluation is cost-efficient and can be implemented locally, it does not guarantee that all safe responses have a harmfulness score of exactly 0. Therefore, we cannot conclusively classify which responses are safe. For instance, as depicted in Figure 2, even though most responses are safe, the DeBERTa model still assigns an average harmfulness score of approximately 0.3.\nTo address this limitation, we used the OpenAI Moderation API as a secondary evaluation method. This API provides both a harmfulness score (in [0,1]), where 0 is the least harmful and 1 the most harmful) and a binary tag indicating whether the response is safe. In Figure 2(d), we show the percentage of tagged harmful responses across all four test datasets. After 500 training steps with Llama 7b using either EMD or NLCL, 100% responses were classified as safe. The harmfulness percentage and harmful score from the moderation API for the other three models: Llama 13b, Mistral 7b, and Llama3.1 8b follow a similar trend and are shown in the appendix A.4.1."}, {"title": "4.3.2 Response Quality", "content": "In this sub-section we aim to investigate whether our approach of penalizing LLMs for generating unsafe responses negatively affects the response quality compared to standard SFT. AlpacaEval Li et al. [2023] is an automatic evaluator designed for instruction-following language models. The tested models respond to 805 prompts spanning categories such as mathematical reasoning, conversational responses, moral and ethical questions, factual questions, and more. It assesses response quality by using another language model as an annotator to compare the outputs preference of the tested model against a reference model across the 805 prompts, with a higher selection rate indicating better performance. In our experiment, we use GPT-40 mini as the annotator and text-davinci-003 as the reference model.\nPIQA Bisk et al. [2020], BoolQ Clark et al. [2019], and OpenBookQA Mihaylov et al. [2018] are multiple-choice question answering datasets which evaluate LLM reasoning ability based on short passages or facts from an \"open book\" of knowledge. We use the Language Model Evaluation Harness framework Gao et al. [2024] to standardize the evaluation of answer accuracy by assessing the probability of each choice. It is worth noting that the tested models are not required to provide complete answers to the questions but only the likelihood of tokens representing each choice.\nWe compare our method with standard instruction fine-tuning method SFT Wei et al. [2021] using the same subset of 20,000 samples from Alpaca. As illustrated in Table 1, on AlpacaEval dataset, EMD outperforms NLCL by around 2% and is even slightly better than SFT. KTO exhibits the lowest performance because it is rewarded to generate responses that are better than a reference model tref. However, here Tref is merely a non-SFTed base model with low-quality responses, which is a low bar and hence KTO generates sub-par responses. Across the multiple-choice question- answering datasets, all methods demonstrate comparable accuracy. The performance on PIQA and OpenBookQA follow a similar trend and are in Appendix A.4.2."}, {"title": "4.3.3 Data Efficiency: Fewer Harmful Examples", "content": "In this part, we reduce the number of harmful responses (1000 originally) included in our dataset; we try 500, 300, and 100 harmful responses. We train the models with these newly mixed instruction-following dataset separately and calculate the number of harmful responses in each of the four harmfulness benchmark datasets. As demonstrated in Table 2, the EMD loss function enables LLMs to learn safe responses with only 100 harmful examples in our dataset, while the NLCL loss function fails to achieve this. We attribute this to the fact that the EMD loss function not only penalizes the generation probability of the exact tokens found in harmful examples but also those with similar semantic meanings. Consequently, due to its better utilization of harmful examples, EMD enables LLMs to learn to be safe with fewer harmful examples. We observe similar results in the other LLMs (Mistral 7b and Llama3.1 8b) which can be found in the Appendix A.4.3."}, {"title": "4.3.4 Training Data: Safe Samples vs Unsafe Samples", "content": "Here we compare to STL, even though STL has the advantage of being trained on high quality (obtained using a commercial model like GPT3.5 turbo) safe responses to toxic prompts. On the other hand, we train on easily accessible unsafe responses. Our results are shown in Table 3. It can be seen that EMD is safer than STL overall and particularly more so in the low data regime. Also, the results on I-CoNa show a stark difference between EMD and STL. Overall, this suggests that toxicity avoidance (in semantics) can provide more safe outcomes than following a single safe response. A similar result on other LLMs (Mistral7B and Llama3.1 8B) can be found in the Appendix A.4.4."}, {"title": "4.3.5 Over-Alignment", "content": "The typical safe responses to toxic prompts are refusals (also called rejections), such as 'It's an inappropriate question, and I cannot ...'. Training with toxic prompts and corresponding safe responses can lead to the side effect of over- refusal, not only during the instruction-following stage Bianchi et al. [2023] but also in the RLHF stage Cui et al. [2024], where LLMs refuse to answer benign prompts. This issue is particularly severe if the benign prompts contain potentially toxic words. For example, over-aligned LLMs will refuse to answer \u201cHow to kill a Python process?\" The 'kill' is potentially toxic yet the overall prompt is harmless. These seemingly toxic prompts are hotspots for over- refusal. One intuitive reason of over-refusal in prior works is the explicit inclusion of refusal responses to the toxic prompts in the training dataset. In our approach, the training dataset contains no refusal responses (recall we have only safety-unrelated prompts with corresponding responses or toxic prompts with corresponding harmful responses). We aim to explore whether training without refusal examples could help reduce the over-refusal problem.\nXSTest R\u00f6ttger et al. [2023] comprises 250 seemingly toxic prompts and 200 toxic prompts across various categories. We evaluate the over-refusal levels of four LLMs fine-tuned with EMD and NLCL loss functions, comparing them to a baseline method, safety-tuned-llamas Bianchi et al. [2023]. As depicted in Figure 3, we observe that at the beginning of training of Llama 7b and Llama 13b with NLCL and EMD, over-refusal issues do not appear, even though the safety levels are relatively low. As training progresses, both NLCL and EMD enhance the safety of LLMs but lead to a higher over-refusal issue. Moreover, all data points in Figure 3 align along the same curve. Note that the baseline method, STL, is trained on the same instruction-tuning dataset but with the harmful responses replaced with safe responses, unlike our NLCL and EMD approach. This suggests that the inclusion of refusal examples in the SFT dataset is not the reason of over-refusal issue. Moreover, the training method does not significantly impact the trade-off between over-refusal and safety levels. Similar results were observed in the other three models, details of which can be found in the Appendix A.4.5. Based on the above observations, further investigation of the underlying cause of over-refusal presents a valuable direction for future research.\""}, {"title": "4.3.6 Contrastive Augmentation", "content": "We report a phenomenon that was an unexpected outcome of our aim to reduce over-alignment. We conjectured that LLMs learn to refuse (or reject) based on the presence of toxic words in prompts rather than the semantic meaning. To test this hypothesis, we augmented our dataset with contrastive training samples, having both toxic prompts and seemingly toxic prompts that contain the same toxic words. Following the method described in Cui et al. [2024], we use toxic words extracted from 1,000 toxic prompts in our dataset to generate seemingly toxic prompts. Considering some word repetitions, we follow Cui et al. [2024] and create 5 seemingly toxic prompts for each toxic word, resulting in a total of 3,335 seemingly toxic prompts. We then use the Mixtral 8*7b Jiang et al. [2024] model, which has not undergone safety alignment and can generate high-quality, non-refusing responses to almost all of 3,335 seemingly toxic prompts. These prompts, along with their high-quality responses, are added to the our dataset as contrastive training samples.\nWe applied the same evaluation process as described in Section 4.4.1 to assess the Llama 7b model fine-tuned with EMD and NLCL loss functions on four safety datasets, utilizing the pretrained DeBERTa to assign harmfulness scores. As illustrated in Figure 4, neither NLCL nor EMD make Llama 7b as safe as when it was fine-tuned without LLM- generated contrastive samples. Furthermore, when the penalty weight A is increased to more strongly discourage harmful responses, the fine-tuned Llama7b model (under both loss functions) exhibited \u2018non-English answer' phe- nomena, which were not observed in the previous experiments. This observation suggests that fine-tuning with LLM- generated seemingly toxic prompts and responses can degrade the model's language performance and is consistent with observations about the use of AI generated data in recent works Shumailov et al. [2023]."}, {"title": "5 Conclusion and Limitations", "content": "Our work provides a way to make LLM respond safely to toxic prompt in the SFT stage itself and improves upon prior results by using much less safety relevant data and only required easily available unsafe responses to toxic prompts. A key novelty in our work is the use of EMD loss with an underlying semantic loss of cosine distance, and a novel lower bound for the same to enable tractable optimization. Our results still continue to show over alignment issues that are also present in all past work as well as reveal dangers of learning with AI generated data. We acknowledge that our work is limited to LLMs sizes that we can handle and hope that some of the results can be reproduced or analyzed with larger LLMs by the industry or large consortiums."}, {"title": "Ethical Statement", "content": "There are dangers and limitations with our study. While we have taken extensive precautions, there is a possibility that some of the prompts and outputs we produce and release could be misused or lead to unsafe outcomes. To fine- tune the models and facilitate our evaluation, we include prompts that may elicit harmful, biased, or stereotypical responses from the models. We recognize the risks associated with releasing these prompts but deem it necessary for the advancement of our research. Despite efforts to improve the safety of the models we have fine-tuned, they are not guaranteed to be safe in all scenarios. Certain edge cases may still result in inappropriate or harmful content generation. Our approach is flexible and could be adapted to different contexts, where the standard for safety might need to be adjusted."}, {"title": "Reproducibility", "content": "We have uploaded code and the data to Anonymous GitHub\u00b9. We have listed hyperparameter values and additional details in the appendix. The one proof in our paper is also present in the appendix."}, {"title": "A Appendix", "content": "Here are a few supplementary materials to the main document."}, {"title": "A.1 Fine-tuning Details", "content": "We follow Safety-Tuned-LLamas (STL) Bianchi et al. [2023] to use the same prompt template to train all the models described in the paper (Llama 7b, Llama 13b, and Mistral 7b and Llama3.1 8b):\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction: {instruction}\n### Input: {input}\n### Response:\nThe base models we use are available on HuggingFace. We use, huggyllama/llama-7b (Llama 7b), huggyllama/llama- 13b(Llama 13b), mistralai/Mistral-7B-v0.3(Mistral7b) and meta-llama/Meta-Llama-3.1-8B(Llama3.1 8b)."}, {"title": "A.2 Hyper Parameters", "content": "All models have been trained NVIDIA L40 or H100 GPUs. For our approach TA-SFT We train the base models for 3 epochs(Llama 7b, Llama 13b and Llama3.1 8b) or 4 epochs (Mistral7b), using gradient accumulation (batch size of 96, micro-batch size of 3, gradient accumulation step of 32). The learning rate is set to 1e-4 for all models. The parameters for low-rank adaptations are as follows. Alpha is 16, dropout is set to 0.05 and r is set to 8. Target modules are [q_proj,v_proj]. We use grid search to tune the penalty weight \u03bb. The tuned EMD and NLCL penalty weights for LLMs fine-tuned with 1,000, 500, 300, and 100 toxic prompts are shown in the Table 4."}, {"title": "A.3 Proof of Proposition 1", "content": "Proof. Note that a simple application of Cauchy Schwarz inequality n times yields the result that $\u03b7 \u03a3_{i=1}^n ||xi||^2 \u2265 || \u03a3_{i=1}^n xi||^2$ for n vectors xi. We use this fact below. Let y be the joint distribution (coupling) that is the minimizer"}, {"title": "A.4 Additional Results", "content": "Here are some additional results"}, {"title": "A.4.1 Safety Level of Llama 13B, Mistral 7B and Llama3.1 8b", "content": "To confirm our results, we also tested our TA-SFT with EMD loss and NLCL loss on Llama 13b (Figure 6), Mistral 7b (Figure 7) and Llama3.1 8b (Figure 8). These figures present both the harmfulness score from DeBERTa model and the harmfulness percentage from OpenAI moderation API. All models exhibit similar to those observed for the Llama 7b model in Section 4.3.1 of the main paper, showing a decrease in harmfulness as training progresses using our TA-SFT method, while KTO fails to improve safety levels. Moreover, our TA-SFT approach, with both EMD loss and ORPO loss, ultimately reduces the harmfulness rate to nearly 0%.\nAs stated in the main paper, the OpenAI Moderation API also provide a harmful score beside a binary tag which are shown in Figure 9. The curves in Figure 9 representing the average harmfulness score across all responses in the four harmfulness benchmarks, exhibit a similar trend to the harmfulness rates from the OpenAI Moderation API, depicted in Figure 2, Figure 6, Figure 7, Figure 8,"}, {"title": "A.4.2 Response Quality", "content": "To substantiate the claim that fine tune LLMs with our TA-SFT using both EMD and NLCL loss does not degrade response quality (Section 4.3.2), we additionally evaluated the response quality on PIQA and OpenBookQA shown in Table 5."}, {"title": "A.4.3 Data Efficiency: Fewer Harmful Examples", "content": "To confirm the statement that we made in Section 4.3.3, we present the number of harmful responses across the four harmfulness benchmarks in Table 6, using our TA-SFT approach with EMD and NLCL. The EMD loss function enables LLMs to learn safe responses with only 100 harmful examples on these two models, whereas the NLCL loss function fails to achieve this."}, {"title": "A.4.4 Training Data: Safe Samples vs Unsafe Samples", "content": "To confirm the observation that we made in Section 4.3.4, we compare the performance of Safety-Tuned Llamas (STL) with our TA-SFT approach using EMD loss on Mistral7b and Llama3.1 8b, despite the latter being fine-tuned with a smaller number of harmful data. Although STL benefits from high-quality safe responses to toxic prompts, it is evident that TA-SFT with EMD loss still significantly outperforms STL(Table 7)."}, {"title": "A.4.5 Over-Alignment of Mistral 7b and Llama3.1 8b", "content": "Consistent with the observation in Section 4.3.5 for Mistral 7b and Llama3.1 8b, as illustrated in Figure 10, over- refusal issues do not emerge at the beginning of training for Llama 7b and Llama 13b with NLCL and EMD, despite the relatively low safety levels. However, as training progresses, both NLCL and EMD improve the safety of the LLMs but also result in an increased occurrence of over-refusal."}]}