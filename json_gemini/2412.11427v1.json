{"title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges", "authors": ["Chandan K Reddy", "Parshin Shojaee"], "abstract": "Scientific discovery is a complex cognitive process that has driven human knowledge and technological progress for centuries. While artificial intelligence (AI) has made significant advances in automating aspects of scientific reasoning, simulation, and experimentation, we still lack integrated AI systems capable of performing autonomous long-term scientific research and discovery. This paper examines the current state of AI for scientific discovery, highlighting recent progress in large language models and other AI techniques applied to scientific tasks. We then outline key challenges and promising research directions toward developing more comprehensive AI systems for scientific discovery, including the need for science-focused AI agents, improved benchmarks and evaluation metrics, multimodal scientific representations, and unified frameworks combining reasoning, theorem proving, and data-driven modeling. Addressing these challenges could lead to transformative AI tools to accelerate progress across disciplines towards scientific discovery.", "sections": [{"title": "Introduction", "content": "Scientific discovery - the process of formulating and validating new concepts, laws, and theories to explain natural phenomena - is one of humanity's most intellectually demanding and impactful pursuits. For decades, AI researchers have sought to automate aspects of scientific reasoning and discovery. Early work focused on symbolic AI approaches to replicate the formation of scientific hypotheses and laws in symbolic forms (Segler, Preuss, and Waller 2018; Mac-Coll 1897). More recently, deep learning and large language models (LLMs) have shown promise in tasks like literature analysis and brainstorming (Ji et al. 2024; Lu et al. 2024; Si, Yang, and Hashimoto 2024), experiment design (Boiko et al. 2023; Arlt et al. 2024), hypothesis generation (Wang et al. 2024; Ji et al. 2024), and equation discovery (Shojaee et al. 2024b; Ma et al. 2024).\nDespite this progress, we still lack AI systems capable of integrating the diverse cognitive processes involved in sustained scientific research and discovery. Most work has focused on narrow aspects of scientific reasoning in isolation. Developing more comprehensive AI discovery systems capable of supporting the full cycle of scientific in-"}, {"title": "Recent Advances in AI for Scientific Tasks", "content": "The past decade has witnessed remarkable progress in applying AI to various scientific tasks. This section highlights some of the most significant recent advances, demonstrating Al's growing capabilities in supporting and accelerating scientific discovery across multiple disciplines."}, {"title": "Literature Analysis and Brainstorming", "content": "The exponential growth of scientific publications has made it increasingly challenging for researchers to stay abreast of developments in their fields. Large language models (LLMs) pre-trained on vast scientific corpora have emerged as powerful tools to address this challenge, enhancing literature analysis and interaction. Researchers have developed specialized LLMs for various scientific domains. Models like PubMedBERT (Gu et al. 2021) and BioBERT (Lee et al. 2020) focus on biomedical literature, while SciBERT (Beltagy, Lo, and Cohan 2019) covers a broader range of scientific disciplines. More recent models such as BioGPT (Luo et al. 2022) and SciGLM (Zhang et al. 2024) have further pushed the boundaries of scientific language modeling, incorporating advanced architectures and training techniques. These models, trained on sources like PubMed and arXiv, excel at literature information retrieval, summarization, and question-answering. They enable efficient navigation of scientific knowledge by quickly finding relevant papers, distilling key findings, and synthesizing information to answer complex queries.\nBeyond analysis, recent works demonstrate LLMs' potential in generating novel scientific insights. For instance, SciMON (Ji et al. 2024) uses LLMs to generate new scientific ideas by analyzing patterns in the existing literature. These advancements show Al's capacity to not only aid in literature review but also contribute to identifying promising and novel research directions, potentially accelerating scientific discovery."}, {"title": "Theorem Proving", "content": "Automated theorem proving has recently gained attention in Al for science research due to its fundamental role in scientific reasoning. Recent years have seen remarkable progress in this field, particularly through the integration of LLMs with formal reasoning systems. The GPT-f framework (Polu and Sutskever 2020) pioneered this approach by training transformer-based language models on proof tactics, enabling navigation through complex mathematical proofs with the help of learned priors. Building on this, researchers have integrated proving techniques with LLMs and developed enhancements such as data augmentation (Han et al. 2021), retrieval augmentation (Yang et al. 2024), and novel proof search methods (Lample et al. 2022; Wang et al. 2023b). One of the key enhancements is the autoformalization approach, exemplified by the Draft-Sketch-Prove method (Jiang et al. 2023). This method uses LLMs to first draft informal proofs, translate them into formal sketches, and then complete proofs with additional proof assistant tools (B\u00f6hme and Nipkow 2010), mimicking the human process of moving from intuitive understanding to rigorous proof. As these systems become more adept at formalizing and proving complex statements, they could be applied to derive scientific theories, potentially accelerating the scientific process and leading to enhancements in fields where theoretical understanding lags behind empirical methods."}, {"title": "Experimental Design", "content": "Experimental design is a critical component of the scientific process, often requiring extensive domain knowledge and creative thinking. The automation of this process through generative models has the potential to accelerate scientific discovery across various fields. By leveraging LLM agents, researchers are recently developing systems that can design, plan, optimize, and even execute scientific experiments with minimal human intervention. These tools are particularly valuable in fields where experimental setup is costly, allowing researchers to explore a wider range of possibilities before physical implementation. For example, in physics, LLM-driven systems have demonstrated effectiveness in designing complex quantum experiments (Arlt et al. 2024) and optimizing parameters in high-energy physics simulations (Cai et al. 2024; Baldi, Sadowski, and Whiteson 2014).\nChemistry has also recently seen advancements in automated experimentation, with LLM agent systems capable of designing and optimizing chemical reactions (M. Bran et al. 2024). Moreover, in biology and medicine, LLM-driven experimental design has shown promise in optimizing gene-editing protocols (Huang et al. 2024), and designing more effective clinical trials (Singhal et al. 2023). These AI-driven approaches to experimental design allow researchers to tackle more complex problems and explore hypotheses that might otherwise be impractical due to time or resource constraints."}, {"title": "Data-driven Discovery", "content": "Data-driven discovery has become a cornerstone of modern scientific research, leveraging the ever-growing volumes of experimental, observational, and synthetic data to uncover new patterns, relationships, and laws. This paradigm shift has been particularly transformative in fields where complex systems and high-dimensional data are prevalent.\nIn drug discovery, data-driven approaches have significantly accelerated the identification of potential therapeutic compounds. For instance, recent works employed generative (Mak, Wong, and Pichika 2023; Callaway 2024) and multimodal representation learning (Gao et al. 2024) models to discover a novel antibiotic, effective against a wide range of bacteria, by searching and screening millions of molecules in the representation space (Gao et al. 2024). These enhancements demonstrate the power of AI in exploring vast chemical spaces that would be infeasible to search manually or in the huge and infinite combinatorial space of molecules.\nEquation discovery, commonly known as symbolic regression, is a data-driven task for uncovering mathematical expressions from data. Early neural methods like AI Feynman (Udrescu and Tegmark 2020) demonstrated the ability to rediscover fundamental physics laws from data alone, while later work incorporated physical constraints and structures for more interpretable models (Cranmer et al."}, {"title": "Key Challenges and Research Opportunities", "content": "First and foremost, evaluating AI systems for open-ended scientific discovery poses unique challenges compared to typical machine learning benchmarks. This challenge is particularly acute for large language models (LLMs) and other foundation models capable of storing and potentially \u201cmemorizing\" vast amounts of scientific knowledge (Brown 2020; Bommasani et al. 2021) in their parameters. Many existing benchmarks in the field of scientific discovery only focus on rediscovering known scientific laws or solving textbook-style problems. For instance, the AI Feynman dataset consists of 120 physics equations to be rediscovered from data"}, {"title": "Benchmarks for Scientific Discovery", "content": "However, these benchmarks may not capture the entire complexity of scientific discovery processes. More critically, they may be vulnerable to reciting or memorization by large language models, potentially leading to overestimation of true discovery capabilities (Carlini et al. 2021; Shojaee et al. 2024b). As (Wu et al. 2023) points out, LLMs can often solve scientific problems by pattern matching against memorized knowledge rather than through genuine reasoning or discovery. This concern is further emphasized by studies showing that LLMs can reproduce significant portions of their training data (Carlini et al. 2022). There is a pressing need for richer benchmarks and evaluation frameworks in this research area to better understand the gap between baselines and recent methods and to identify areas for improvement. Key directions include:\n\u2022 Developing benchmark datasets focused on novel scientific discovery rather than recovery: One promising approach is to create configurable simulated scientific domains where the underlying laws and principles can be systematically varied. This would allow testing discovery capabilities on new scenarios, mitigating the risk of models simply reciting memorized information observed in their training data. For example, (M. Bran et al. 2024) used a simulated chemistry environment to evaluate AI-driven discovery of novel chemical reactions. Similarly, (Shojaee et al. 2024b) designed simulated settings for different scientific domains such as material science, physics, and biology to evaluate AI-driven scientific equation discovery. A key challenge in this line of research is balancing the use of LLMs' prior scientific knowledge while avoiding mere recitation or memorization. This balance is crucial for advancing Al's role in scientific discovery.\n\u2022 Creating evaluation metrics for multiple facets of scientific discovery: To comprehensively assess scientific discovery capabilities, we need a multi-faceted evaluation framework. The key metrics include: (i) Novelty: Measures to quantify how different a discovered hypothesis or law is from existing knowledge. This could involve comparing against a corpus of known scientific literature (Ji et al. 2024); (ii) Generalizability: Assessing how well discovered laws or models predict out-of-distribution unobserved data. To do so, evaluation benchmarks should be developed that test discovered laws on scenarios significantly different from the training data distribution, highlighting how scientific theories should be generalizable to new contexts; (iii) Alignment with Scientific Principles: Evaluating whether discovered hypotheses are consistent with fundamental laws of physics or other well-established scientific knowledge. This could involve developing formal verification methods for scientific consistency (Cornelio et al. 2023; Cranmer et al. 2020a), as well as assessing the discovered laws' compat-"}, {"title": "Science-Focused Agents", "content": "Current work on scientific AI often treats models as passive tools rather than active agents pursuing discovery. There is a growing need to develop science-focused AI agents (Figure 2) that can leverage broad scientific knowledge, engage in reasoning, and autonomously verify their reasoning and hypotheses. Recently, LLMs have shown impressive capabilities in knowledge retrieval and reasoning (Huang and Chang 2023), making them promising candidates for developing such agents. These agents can integrate vast amounts of scientific knowledge embedded in LLMs, generate educated hypotheses, design experiments, verify their designs, and interpret the results. Also, their ability to interface with external tools and experimental data sources with the programming execution gate allows for real-world experimentation and validation. Recent work has demonstrated the potential of LLM-based agents in scientific domains. For example, (M. Bran et al. 2024) introduced ChemCrow, an LLM-augmented system for chemistry research. ChemCrow integrates GPT-4 with domain-specific tools for tasks such as reaction prediction, retrosynthesis planning, and safety assessment. This integration allows the system to reason about chemical processes and validate the hypotheses using specialized chemical tools. Similarly, (Ghafarollahi and Buehler 2024a) developed AtomAgents, a multi-agent system for alloy design and discovery. SciAgents (Ghafarollahi and Buehler 2024b) also uses multiple AI agents, each specializing in different aspects of materials science, to collaboratively design new bio-materials. The system incorporates physics-aware constraints and can interface with simulation tools to validate its predictions. However, developing effective science-focused agents also presents several challenges:\n\u2022 Domain-specific tool integration: Effective scientific agents require integration with specialized scientific tools and domain-specific knowledge. This challenge arises from the highly specialized nature of scientific instruments and methodologies, which are often underrepresented in LLMs' training data. (Bubeck et al. 2023) demonstrated that while LLMs like GPT-4 excel in general academic tasks, they struggle with specialized scientific reasoning, particularly in physics and chemistry. Potential research directions include developing modular architectures for integrating domain-specific knowledge bases and tool interfaces, and fine-tuning LLMs on curated scientific datasets. These approaches could enable LLMs to access domain-specific knowledge and interact effectively with specialized scientific tools, enhancing their capabilities in this setting.\n\u2022 Adaptive experimental design and hypothesis evolution: A significant challenge in scientific-focused agents is developing systems capable of long-term, iterative scientific investigations. Such agents must design experi-"}, {"title": "Multi-modal Scientific Representations", "content": "The landscape of scientific data is vast and diverse, encompassing far more than just textual information. While recent advancements in language models have significantly boosted our ability to process and reason with scientific literature, we must recognize that the majority of scientific data exists in forms quite different from natural language. From microscopy images to genomic sequences, from time series sensor data to structured databases and mathematical laws, scientific knowledge is inherently multi-modal (Topol 2023; Wang et al. 2023a). This diversity presents both challenges and opportunities for AI-driven scientific discovery. The challenge lies in developing integrated representation learning techniques that can effectively capture and unify these varied scientific data types. The opportunity, however, is immense: by creating AI systems capable of reasoning across these diverse modalities, we can accelerate scientific discovery in unprecedented ways.\nRepresentation learning offers the potential to distill complex, high-dimensional scientific data into more manageable continuous and low-dimensional forms. This is particularly crucial in scientific domains where high-quality data is limited or expensive to obtain through scientific experiments. By learning multi-modal robust representations with the help of pre-training techniques and synthetic simulation data, we can make more efficient use of limited data, potentially reducing the need for costly scientific experiments and accelerating the pace of discovery. Key directions in this line of research include:\n\u2022 Cross-modal scientific representation learning: Recent work has shown promising results in learning pre-trained joint representations across modalities for different sci-\n\u2022 scientific tasks. Notable successes include DrugCLIP (Gao et al. 2024) for joint representations of molecules and protein pockets in drug discovery, Text2Mol (Edwards, Zhai, and Ji 2021) bridging natural language and molecular structures, ProtST (Xu et al. 2023) unifying protein sequences and biomedical text in proteomics, and SNIP (Meidani et al. 2024) linking mathematical expressions with numeric data. These advances demonstrate the potential of cross-modal learning to enhance scientific tasks by leveraging complementary information across modalities. Despite these promising results, significant research opportunities remain (i) Expanding cross-modal representation learning to diverse and new scientific domains, (ii) Enhancing representation quality through recent integrated self-supervised and multi-modal pre-training; and (iii) Developing unified, modality-agnostic frameworks adaptable to heterogeneous scientific data types.\n\u2022 Latent space scientific hypothesis search: Many scientific discovery tasks involve searching through vast, combinatorial spaces of candidates. Current approaches to these problems often rely on evolutionary search or heuristic methods, which can be computationally expensive and inefficient (Sadybekov and Katritch 2023; Schmidt and Lipson 2009). Recent advances in representation learning offer a promising alternative: conducting scientific hypothesis optimization in learned latent spaces. By moving the search process into the latent space, we can potentially make the exploration of the hypothesis space more efficient and effective. This approach has shown potential across various domains, from drug discovery (Gao et al. 2024) to equation discovery (Meidani et al. 2024), molecular design (Abeer et al. 2024; Zheng, Li, and Zhang 2023), and protein engineering (Castro et al. 2022; Jumper et al. 2021). This emerging research direction has significant potential for scientific discovery. Future research avenues include (i) Integrating domain expert knowledge or feedback into the representations and discovery process, (ii) Enhancing interpretability of representations for scientific validation, and (iii) Advancing optimization techniques for nontrivial discovery objectives and more flexible hypothesis search in the latent space.\n\u2022 Multi-modal scientific reasoning frameworks: The advancement of AI-driven scientific discovery hinges on developing systems capable of multi-modal scientific reasoning. Recent works have shown promising results in this direction. For example, multi-modal retrieval augmented generation (RAG) systems have demonstrated potential in leveraging LLMs for scientific discovery (Park et al. 2024). Models like GIT-Mol (Liu et al. 2024a) showcase the integration of visual, textual, and graph reasoning for molecular discovery. In materials science, approaches combining textual reasoning with structural data have also shown promise in predicting material properties and guiding synthesis (Miret and Krishnan 2024). However, comprehensive multi-modal scientific reasoning frameworks remain an open challenge. Such frameworks must effectively integrate rea-"}, {"title": "Theory and Data Unification", "content": "Scientific discovery typically involves a complex interplay between theoretical reasoning, empirical observation, and mathematical modeling. However, most existing AI approaches to scientific tasks focus on just one of these aspects. There is a pressing need for unified frameworks that integrate logical and mathematical reasoning, formal theorem proving, data-driven modeling, experimental design, and causal inference. This integration is challenging but critical for capturing the full scientific discovery process. Recent advances in LLMs have shown promising results in both theorem-proving and data-driven scientific modeling. For instance, LLMs have demonstrated promising capabilities in automated theorem-proving and formal mathematical derivations from natural language problems (Yang et al. 2024; Jiang et al. 2023). On the data-driven side, (Shojaee et al. 2024b; Ma et al. 2024) have shown success in discovering equation hypotheses from data with the help of LLM-based program search. However, these approaches largely operate in isolation, and there is a significant gap in unifying these capabilities to mirror the holistic nature of scientific inquiry. Key challenges and research directions include:\n\u2022 Generating derivable hypotheses from empirical observations: Developing methods that can not only discover patterns in data but also produce rigorous mathematical derivations of these findings is crucial for ensuring the reliability and generalizability of AI-driven scientific discoveries to out-of-distribution data. Derivable theoretical results provide a level of confidence and understanding that goes beyond mere empirical correlation. Recent work, such as the AI-Descartes system (Cornelio et al. 2023), has shown promise by combining equation discovery tools (known as symbolic regression) with automated logical reasoning. However, integrating logical reasoning and data-driven frameworks that are adaptable across scientific discovery tasks still remains an open challenge. Research opportunities exist to automate proof verification, incorporate expert feedback, and embed derivability constraints in data-driven discovery algorithms.\n\u2022 Combining symbolic and neural approaches: How can we effectively integrate the strengths of symbolic reasoning (e.g., logical deduction, formal proofs) with the flexibility and learning capabilities of neural networks? Recent work on neuro-symbolic AI (Garcez and Lamb 2023; Sheth, Roy, and Gaur 2023) provides promising directions, but challenges remain in scaling these approaches to more complex settings and scientific tasks. Developing hybrid architectures that can transition between symbolic and neural representations is helpful in capturing the full spectrum of scientific reasoning.\n\u2022 Reasoning discovery uncertainty in formal frameworks: Scientific discoveries often involve uncertainties and probabilities, yet formal logical frameworks struggle to incorporate these aspects. Developing frameworks that can handle probabilistic reasoning while maintaining rigorous deduction capabilities is crucial for advancing AI-driven scientific discovery. Recent work, such as probabilistic logic systems (De Raedt and Kimmig 2015; De Raedt, Kimmig, and Toivonen 2007), and neuro-symbolic programming (Ahmed et al. 2022) has made progress in this direction. However, significant challenges remain for the use of these approaches in scientific discovery, including scalability to large-scale scientific problems, and expressiveness to capture complex scientific theories in specific scientific domains."}, {"title": "Conclusion", "content": "Developing unified Al systems for scientific discovery is an ambitious goal, but one with substantial potential impact. Success could dramatically accelerate progress across diverse scientific disciplines. This paper has outlined current progress as well as several key research challenges and opportunities toward this vision, including developing science-focused AI agents, creating improved benchmarks, advancing multimodal representations, and unifying diverse modes of scientific reasoning. Tackling these challenges will require collaboration between AI researchers, scientists across domains, and philosophers of science. While fully autonomous AI scientists may still be far off, nearer-term progress could produce powerful AI assistants to augment human scientific capabilities. Such tools could help scientists navigate the ever-growing scientific literature, brainstorm ideas, generate novel hypotheses, design experiments, and find unexpected patterns in complex experimental data. By pursuing this research agenda, the machine learning and AI community has an opportunity to develop systems that do not just automate product-related tasks, but actively push forward the frontiers of human scientific knowledge. The path will be challenging, but the potential rewards - both scientific and technological - are immense."}]}