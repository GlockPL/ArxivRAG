{"title": "Assessing SPARQL capabilities of Large Language Models", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Felix Brei", "Natanael Arndt"], "abstract": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.\nWe implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.\nWith this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.", "sections": [{"title": "1. Introduction", "content": "The combination of Large Language Models (LLMs) and Knowledge Graphs (KGs) is still gaining more traction as the rapidly growing number of research articles and the inclusion of LLMs in several KG related conference calls show. This includes different combinations [1] of LLMs and KGs, especially LLMs supported by KGs and LLMs supporting the work with KGs. An important interface for both combinations is the SPARQL query language as the structured retrieval and update interface for RDF KGs [2].\nWith the fast evolving list of LLMs available, the need for automated benchmarks increases.\nLLM performances is evaluated by BigBench [3], on the Open LLM Leaderboard [4], and the"}, {"title": "1.1. Research Questions", "content": "In this paper, we consider the following research questions.\nRQ 1. Can LLMs follow the syntactic rules of SPARQL SELECT queries?\nRQ 2. Can LLMs parse the semantics of SPARQL SELECT queries and act accordingly?\nRQ 3. Can LLMs write semantically correct SPARQL SELECT queries for a given question and KG, i.e. a query that yields the expected answer?\nRQ 4. Does the KG presentation have an influence on the ability to write SPARQL SELECT queries?\nThe research questions depend on each other and build on the previous questions as depicted in fig. 1. RQ1 focusses on the syntax, while RQ2 and RQ3 aim at semantics. RQ2 covers the interpretation and RQ3 the creation of said queries. RQ4 examines the influence of the way knowledge is presented on the results of RQ2 and RQ3."}, {"title": "1.2. Contribution", "content": "The contribution of the paper is both an evaluation of current LLMs as well as the extended evaluation framework itself. Our evaluation and framework is comprised of tasks that are executed in various configurations. In section 3.2 we introduce the tasks. The main task types are SPARQL Syntax Fixing (SSF), Text to SPARQL translation (T2S), SPARQL to Answer (S2A) and Text to Answer (T2A). These tasks evaluate relevant aspects to cover the dimensions along which the capabilities (see section 3.4) of the models are assessed according to our research questions:\n\u2022 Task type SSF focuses on working with the syntax of SPARQL SELECT queries, giving answers for RQ1.\n\u2022 Task type S2A is asking to work according to semantics of SPARQL SELECT queries, helping with RQ2.\n\u2022 In task type T2S the LLM should create syntactically and semantically correct SPARQL SELECT queries, giving us answers to RQ1, RQ3 and RQ4.\n\u2022 For answering task type T2A the LLM needs to get information from the KG, thus the results are relevant for RQ4.\nThe purpose of RQ4 is to understand the influence of the knowledge graph's syntax (Turtle, JSON-LD), the representation of the data (numerical identifiers, human readable identifiers), and the knowledge graph prompt inclusion (schema, size of the sub graph, and selection) in the answers to RQ2 and RQ3. The variants of execution with Turtle and JSON-LD as well as the selection of datasets (KG info type) and their subsets pay in on RQ4.\nThe remainder of the paper is structured as follows: In section 2 an overview on the related work is given and the paper is located with its contribution to the bigger picture. Section 3 presents the kind of tasks that were executed as part of our benchmark and which datasets were used. In section 4 we do an extensive evaluation of the obtained results and the conclusions that can be drawn. Section 5 summarizes the findings and gives an overview on the directions that our research will head next."}, {"title": "2. Related Work", "content": "Rangel et al. [10] propose a methodology for fine-tuning OpenLLaMA to generate SPARQL queries for question answering over life science knowledge graphs using data augmentation techniques, such as providing meaningful variable names and inline comments, improving the performance of the model in generating accurate SPARQL queries. Bustamante and Takeda [11] aim at improving the creation of SPARQL queries based on natural language questions. The authors use a GPT model to identify which property of the Text2SPARQL task is the hardest to solve, in order to select appropriate countermeasures. Avila et al. [12] evaluate the ability to answer natural language questions with ChatGPT on KGs. With the Auto-KGQAGPT approach the authors further investigate the ability to translate natural language question to SPARQL queries based on a prompt inclusion of KG fragments.\nLi et al. [13] address the challenge of declined quality of results in real-world scenarios where high-quality annotated data is insufficient. To target this challenge the authors present the FlexKBQA framework which employs templated SPARQL queries that are translated into natural language questions using LLMs to generate synthetic training data. This synthetic data allows to fine-tune a lightweight model for SPARQL generation along with further self-guided training on real queries to address a distribution shift between synthetic and real queries.\nDiallo et al. [14] give a comprehensive overview and performs a comparison of pre-trained LMs (PLMs), non-pre-trained LMs (NPLMs), and LLMs, testing various fine-tuning methods using LLMs. The error analysis of the models results in the finding, that the primary source of errors are incorrect URIs in SPARQL queries e.g. due to hallucination. Hirigoyen et al. [15] have found though that the hallucination can be prevented by using a copy mechanism.\nThe SPARQLGEN approach by Kovriguina et al. [8] is a one-shot generative approach to generate SPARQL queries using LLMs. It includes the relevant context in a single prompt, i.e. the question, an RDF sub graph with the relevant information, and an example of a SPARQL query and a question. Pliukhin et al. [16] present an approach for SPARQL query generation on the scholarly knowledge graph ORKG. Its setup is similar to SPARQLGEN but utilizes an advanced sub graph extraction. Zahera et al. [17] went on with leveraging chain of thougth prompting and utilizes the GERBIL QA system[18]. Lehmann et al. [19] suggest the usage of Controlled Natural Language as a human interface since it is closer to natural language. The Controlled Natural Language can then be unambiguously translated into a formal language such as SPARQL. They come to the conclusion that this approach substantially reduces the training data requirements.\nSeveral datasets of complex questions over knowledge graphs are available. Diefenbach et al. [20] present two datasets for training and benchmarking question answering systems using Wikidata. One is a translation of the SimpleQuestions dataset (cf. Bordes et al. [21]), the other is based on logs and user feedback. LC-QuAD 2.0 by Dubey et al. [22] is an advancement of the Large-Scale Complex Question Answering Dataset (LC-QuAD) [23]. It is compatible with both, the Wikidata and the DBpedia knowledge graphs. It contains \u201c30,000 questions, their paraphrases and their corresponding SPARQL queries"}, {"title": "3. Experiment Setup", "content": "For automated assessment of the capabilities of LLMs to work with SPARQL SELECT statements we extended the LLM-KG-Bench framework. In the following subsections we outline evaluation methods, the different task types, KGs used for testing and specific tasks.\nnomenclature that we used:\n\u2022 A task type is a class of tasks that an LLM needs to solve, like SSF or T2S.\n\u2022 A task is one specific instantiation of a task type, like T2A on LC-QuAD using JSON-LD syntax for the knowledge graph.\n\u2022 Task entry refers to one single data point that is used during a task, for example the third question in T2A for LC-QuAD. All tasks presented here have a list of five task entries which they choose from evenly distributed.\n\u2022 One execution of a task entry consists of all steps that are necessary to reach completion of this task entry. For example, the execution of one task entry in SSF entails the complete feedback dialog and ends with the final answer of the LLM and its evaluation."}, {"title": "3.1. Experiment Type Selection", "content": "SPARQL SELECT queries are connected to a related textual question, a knowledge graph the query is designed for, and an answer. For working with SPARQL SELECT queries one usually has to deal with a combination of these concepts. This leads us to the four different task types with inputs and outputs as shown in fig. 2."}, {"title": "3.2. Detailed Explanation of Task Types", "content": null}, {"title": "3.2.1. Task Type \u201cSPARQL Syntax Fixing\u201d (SSF)", "content": "The task SPARQL Syntax Fixing asks to fix a syntax error in a given SPARQL query. To be more precise, the LLM is given a syntactically malformed SPARQL query together with the resulting"}, {"title": "3.2.2. Task Type \u201cText to SPARQL\u201d (T2S)", "content": "The Text-to-SPARQL task type asks to create a SPARQL SELECT query for a KG and a textual question. Many KGQA approaches build upon this capability.\nThe following prompt is used with several variables(${kgName}, ${commonPrefixes}, ${question} and ${KgInfo}) filled in according to the specific task:"}, {"title": "3.2.3. Task Type \u201cSPARQL to Answer\u201d (S2A)", "content": "The SPARQL-to-Answer task asks to interpret a given SPARQL SELECT query on a given KG and answer with the binding values. The LLM is asked to give one value per line and the F1 measure is used to score the result, as described in more detail in section 3.3.2."}, {"title": "3.2.4. Task Type \u201cText to Answer\u201d (T2A)", "content": "This task type was added as a variation of the previous \"SPARQL to Answer\" task type for comparison. The task is to answer a given natural language textual question on a given KG and answer with the binding values. Expected answer structure and evaluation is the same as described above for task type S2A."}, {"title": "3.3. Evaluation Method", "content": "In this paper we are applying a quantitative approach. To support automated evaluation within in the LLM-KG-Bench framework we need to define the following evaluation methods for SPARQL SELECT queries as generated by task types SSF and T2S, and for answers given in task types S2A and T2A."}, {"title": "3.3.1. Evaluation of SPARQL SELECT queries (SSF and T2S)", "content": "SPARQL SELECT queries are evaluated regarding having correct syntax and yielding the expected result when executed on the KG. To be more precise, all entries of the result bindings are put into a set (duplicates removed) and are then compared to the expected result set. In case of syntax errors or an empty result set the LLM is given the opportunity to correct the SPARQL SELECT query in a feedback dialog session. Therefore a feedback message is generated and sent back to the LLM together with the previous dialog. This feedback dialog session is continued until up to three answers (one initial and up to two additional tries for corrections) are collected or at least one SPARQL SELECT query yields a result.\nIn case of syntax errors the following prompt template was used with values filled in for ${error} and ${sparql}:"}, {"title": "3.3.2. Evaluation of Answers given by Tasks S2A and T2A", "content": "In the task types S2A and T2A the LLM is asked to give the answer from the KG similar to binding values, one value per line. For comparing the given answer lines with the expected answer lines, precision, recall and f1 measure are calculated. As the LLMs answer does not always follow the expected structure perfectly we implemented a couple of scores for strict and more flexible answer parsing as well:\nexact (f1, precision, recall) \u2208 [0..1]: comparison on the exact lines given with expected answer entries\ntrimmed (trimF1, trimPrecision, trimRecall) \u2208 [0..1]: comparison after leading and trailing whitespace is removed\nfixed format (fixedF1, fixedPrecision, fixedRecall) \u2208 [0..1]: comparison after fixing simple format errors like http instead of https, prefix 0: (often given for JSON-LD) instead of : and removal of brackets (< and >) and quotation marks (' and \")\nrelaxed evaluation (relaxedF1, relaxedPrecision, relaxedRecall) \u2208 [0..1]: comparison with format fixed plus ignore case, removing default prefix : (makes instances similar to labels for semantic IRIs). In case a count is expected a list with the right length is also accepted as correct.\ncombinedF1 = $\\frac{f1+trimF1+fixedF1+relaxedF1}{4}$ \u2208 [0..1]: mean of the f1 scores above."}, {"title": "3.4. Task Aspects covered by the Task Types", "content": "The 4 task types described above in section 3.2 are covering a list of task aspects and are relevant for the research questions. They are mapped on task types in table 1 and described in the following list:\nread SPARQL SELECT query syntax: To work with a SPARQL SELECT query the syntax must be digested.\nPart of task types SSF and S2A, related to RQ 1.\ncreate correct SPARQL SELECT query syntax: Create or write syntactically correct SPARQL SELECT query syntax.\nPart of task types SFF and T2S, related to RQ 1.\nread semantics of a SPARQL SELECT query: Get the semantic meaning of a SPARQL SELECT query and act accordingly.\nPart of task type S2A, related to RQ 2.\ncreate a semantically correct SPARQL SELECT query: create a SPARQL SELECT query not only syntactically correct but with an appropriate semantic which yields the expected bindings.\nPart of task type T2S, related to RQ 3.\nKG info read: When dealing with a knowledge graph some kind of information on the KG structure and KG content is needed. The information on the structure could be anything from whole KG over schema to just a list of relevant or all entities and properties.\nPart of task types T2S, T2A and S2A, related to RQ 4."}, {"title": "3.5. Benchmark Datasets and KGs used for Task Implementations", "content": "There are several benchmark datasets available which contain pairs of SPARQL SELECT queries and textual natural language questions for specific knowledge graphs. We selected and implemented benchmark tasks for a couple of current datasets for smaller and bigger knowledge graphs. Only English textual questions were used as we focus on SPARQL here and not language capabilities. A total of five tuples, each consisting of a question and corresponding SPARQL query, was manually selected from each dataset. This allows to rerun the tasks more often to reduce the random noise in the results."}, {"title": "4. Results", "content": "We evaluated the LLMs from OpenAI, Anthropic and Google listed in table 3 with tasks listed in table 2. Each combination of LLM and task shown here was executed 50 times, resulting in 10 executions per task entry and LLM. All data generated is published at GitHub. The evaluation can be reproduced by anyone by using the --reeval parameter of the LLM-KG-Bench framework."}, {"title": "4.1. Results for SPARQL Syntax Fixing (SSF)", "content": "Fixing syntax errors in SPARQL SELECT queries seems to be easier for LLMs as can be seen in fig. 5a. Only 3 models (Claude 2.1, Claude 3 Haiku and GPT 3.5 2024/01) are not able to fix all errors within 3 feedback iterations, about 80% of the executions got a correct answer on first try. The most common error missed was a variable name containing a hyphen. E.g. ?foo-bar was not detected as an invalid variable name."}, {"title": "4.2. Results for SPARQL to Answer (S2A)", "content": "About 75% of the executions had perfect results and 90% had a relaxedF1 score of 1. The most common formatting problems were caused by insertion of additional spaces. About 10% had just wrong answers, often for task entries that required counting. Plots for the combinedF1 scores can be seen in figs. 3a and 3b."}, {"title": "4.3. Results for Text to SPARQL (T2S) Organizational Graph", "content": "The state-of-the-art LLMs seem to have little problems with generating SPARQL SELECT queries for the organizational graph, as can be seen in fig. 4a. But when we look at the results in fig. 4b for the numerical version with a translation table, we can see a different picture, where only Gemini 1.5 scores almost perfect."}, {"title": "4.4. Results for Text to SPARQL (T2S) LC-QuAD", "content": "In fig. 4c we can see that GPT4 in both versions and Claude 3 Opus were quite successful in generating correct SPARQL SELECT queries. They managed to generate syntactically correct queries in 99% of the cases on the first try, with all queries having correct syntax after the final feedback iteration. The f1measure score for these models increased from 0.699 for their first answer to 0.972 which means they really benefited from the provided feedback and knowledge graph information. Gemini 1.0 on the other hand reached only an f1 score of 0.274 after the final feedback loop, indicating that it still struggles to use the provided properties correctly. However the queries were syntactically correct in almost all cases. For the other models the property structure of Wikidata seems to be just as challenging. We often see generated graph patterns with properties used in a reversed direction (domain and range swapped)."}, {"title": "4.5. Results for Text to SPARQL (T2S) CoyPu", "content": "For the CoyPu-Mini graph, the LLMs struggled more to generate syntactically correct SPARQL queries in the first answer. When being given the IRIs, Haiku alone is responsible for 21 of the 45 errors, with Gemini generating 8 syntactically wrong queries and GPT4 just one (2023/11 model). The rest is shared between other flavors of Claude. The situation becomes worse when we provide the full KG; only 316 of the responses for JSON-LD and 401 for Turtle contained valid SPARQL (with 450 in total for each). We get the same picture if we give the Schema information, with 341 valid SPARQL queries for JSON-LD and 397 for Turtle. However, if we provide feedback and allow LLMs to correct their mistakes, the numbers increase to 391 for the JSON-LD graph and 434 for Turtle. All model families (GPT, Gemini, Claude) have instances where even the last answer they gave contained no valid SPARQL, so this is a phenomenon that can happen with each LLM. Looking at the last f1measure score (the final answer) of each LLM we found that the global average is about 0.3 for JSON-LD schema information and 0.4 for every other kind of information passed. This is valid across all models, except for one exception: The last f1measure score of the GPT4 models increases to 0.7 if we give it the full KG in JSON-LD format. Surprisingly this did not happen with the Turtle format. The plots for the max_combined in figs. 4d to 4h. show that the Gemini models, Claude 2.1 and Haiku perform worse for JSON-LD compared to using Turtle representations of the full KG or schema opposed to Opus and the GPT4 versions that perform better with JSON-LD."}, {"title": "4.6. Results for Text to SPARQL (T2S) Beastiary", "content": "Generating syntactically correct SPARQL queries was no problem for the LLMs (besides Haiku) as can be seen in figs. 4i to 41. No matter what kind of information was given about the KG, there were always about 420 of 450 queries that were parseable. Again, GPT4 performs best here with only answering two times with syntactically wrong queries (at the end of the dialog), and Haiku being responsible for about half of the errors, the rest being evenly distributed among the other flavors of Claude as well as Gemini. Looking at the f1 score we can see that the kind of"}, {"title": "4.7. Results for Text to Answer (T2A)", "content": "About 75% of the answers given were perfectly correct and about 97% good enough for the relaxed scoring. The most common formatting problem were the addition of spaces in the given answer. The few really wrong answers given were mainly on a question asking for a count, and the LLM answered with the wrong number. Plots for the combinedF1 score are given in figs. 3c and 3d."}, {"title": "4.8. Statistical Analysis", "content": "The above sections deal with the discussion of the results on a split-by-task basis, but we can also analyze the results based on different aspects that were covered, like the input format of a knowledge graph (JSON-LD vs Turtle) and the kind of information that was provided to the LLM (Full KG vs Schema information vs List of IRIs). We ran pairwise t-Tests and obtained the results shown in table 4. For each comparison we took the f1 scores (mean_f1measure) of two aspects across all tasks and performed a Welch's t-Test with the two resulting populations, labeled A and B. In each case the null hypothesis Ho was that $\\mu_A = \\mu_B$. Using the common threshold of p = 0.05 we can see that the null hypothesis can be rejected every time except for the difference between providing a list of relevant IRIs to the LLM vs the full KG. In the three remaining cases, we can see that there is a statistically significant difference in the data: Turtle vs JSON-LD, IRIS vs Schema, full KG vs Schema."}, {"title": "5. Conclusion and Outlook", "content": "Benchmarking LLMs on SPARQL SELECT query related tasks is difficult and more experiments are needed. Nonetheless with the work presented in this paper we contributed to the research questions defined and layed the ground for further automated evaluation.\nFrom the results gathered we can give the following answers to the research questions: Most LLMs evaluated had no problems with working with SPARQL SELECT query syntax (RQ1) or reading SPARQL SELECT query semantics (RQ2). Creating SPARQL SELECT queries with correct semantics (RQ3) seems to be still a difficult task for the LLMs evaluated here, at least when applying the approach to evaluate the answer generated. The results seem to depend on different aspects (RQ4). The varying performance between the 10 different variants of T2S tasks indicate, that we should further extend the tests for serialization formats and content that is provided as KG information to the LLMs (full graph vs. sub graph vs. schema vs. IRIs). Taking the CoyPu-Mini KG as an example, we found that Turtle leads to a higher number of syntactically correct queries, whereas JSON-LD improved the score of GPT4 tremendously and made it stand above every other model in this specific test.\nThe results of LLMs depend a lot on the concrete task setting as can be seen in fig. 4. Thus we found no clear winner. But when focusing on the mean value of f1 scores for T2S tasks Claude 3 Opus and GPT 4 scored best as shown in fig. 5b.\nFurther research could improve the evaluation of SPARQL SELECT queries created by LLMs, especially for queries returning no result when applied on the KG.\nIn the long term, the fine-tuning or training of LLMs would probably benefit from more SPARQL-related training data.\nUnfortunately, we found that existing KGQA benchmarks have ambiguity that hinder a proper automatic evaluation. Given the fact those publicly available benchmarks (like LC-QuAD) can be contained in the training dataset and memorized by the LLMs, emphasizes the need for new and diverse test datasets. Fortunately, with the extended framework presented in this work, it is easy to include new evaluation datasets."}, {"title": "Conflicts of interest", "content": "The authors have no competing interests to declare that are relevant to the content of this article. The authors used a free evaluation license for Claude and Gemini models, however due to the setup and technical nature of the evaluation this has no effect on the results."}]}