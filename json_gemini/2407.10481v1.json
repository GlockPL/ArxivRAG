{"title": "SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation", "authors": ["Jordan Juravsky", "Yunrong Guo", "Sanja Fidler", "Xue Bin Peng"], "abstract": "Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.", "sections": [{"title": "1 INTRODUCTION", "content": "Physics-based character animation offers the potential of synthesizing life-like and responsive behaviors from first principles. The use of reinforcement learning techniques for character control has led to a rapid growth in the corpus of motor skills that can be replicated by simulated characters, ranging from common behaviors, such as locomotion [Peng et al. 2017; Winkler et al. 2022; Xie et al. 2020], to highly athletic skills, such as gymnastics and martial arts [Liu and Hodgins 2018; Peng et al. 2018; Won et al. 2021; Xie et al. 2022]. As the capabilities of simulated characters continue to improve, interfaces that enable users to direct and elicit the desired behaviors from a character will be vital for the viability of these systems for practical applications. The most common interfaces for directing simulated characters often leverage compact control abstractions,"}, {"title": "2 RELATED WORK", "content": "Physics-based models for character animation has had a long history in computer graphics, with a large body of work devoted to constructing controllers that enable simulated characters to reproduce a large repertoire of motor skills [Clegg et al. 2018; da Silva et al. 2008; Hodgins et al. 1995; Lee et al. 2010a; Liu and Hodgins 2018; Tan et al. 2014; Wang et al. 2009, 2012]. While these efforts have led to substantial improvements in the motor capabilities of simulated characters, a barrier that has precluded the wider adoption of these models for practical applications has been the lack of accessible interfaces for users to direct the behaviors of these models. The majority of these models provide users with simple control interfaces, such as joystick commands or target waypoints [Agrawal and van de Panne 2016; Coros et al. 2009; Holden et al. 2017; Lee et al. 2021b,a, 2010b; Ling et al. 2020; Peng et al. 2018, 2022, 2021; Starke et al. 2019; Treuille et al. 2007; Zhang et al. 2020], which are easy to use but greatly restrict the control a user can exert over the character's behaviors. Alternatively, motion imitation models provide an expressive interface, where a user can specify target reference motions for a simulated character to imitate [Bergamin et al. 2019; Luo et al. 2023; Peng et al. 2018; Won et al. 2020]. While these models can provide users with versatile and granular control over a character's behavior, they require users to construct reference motions for every desired motion, which can itself be a costly and labour intensive process.\nText-Driven Generation: Natural language offers the potential to develop accessible and expressive interfaces for directing the behaviors of simulated characters. Recent advances in large-language models has lead to a proliferation of text-driven interfaces for generative models in a wide variety of domains [Devlin et al. 2018; Liu et al. 2019; Poole et al. 2023; Raffel et al. 2019; Saharia et al. 2022]. Similar techniques have also been adopted for motion synthesis, creating text2motion models that are able to generate motions according to natural language descriptions [Ahuja and Morency 2019; Lin et al. 2018; Plappert et al. 2017; Tevet et al. 2022, 2023]. These models have in part been made possible by the availability of large public datasets which contain tens of hours of text-labeled human motion data [Guo et al. 2022; Punnakkal et al. 2021]. A large body of work has also adapted generative modeling techniques for text-directed motion synthesis, such as sequence prediction models with RNNs [Lin et al. 2018; Plappert et al. 2017], variational auto-encoders [Athanasiou et al. 2022; Petrovich et al. 2022], contrastive coding [Ahuja and Morency 2019; Tevet et al. 2022], transformers [Jiang et al. 2023; Zhang et al. 2023a], and diffusion models [Dabral et al. 2023; Tevet et al. 2023]. While these text2motion models have demonstrated promising capabilities, the vast majority of this work focuses on kinematic motion models.\nLanguage-Directed Controllers: In this work, we aim to develop a large-scale framework for training language-directed models for physically simulated characters. Prior efforts in this domain have largely been limited in terms of the variety of motions that can be reproduced by a model, and the quality of the generated motions. Juravsky et al. [2022] proposed an adversarial imitation learning framework for training language-directed controllers for a simulated humanoid character. However, their system was only effective in learning from a relatively small dataset of approximately 9 minutes of motion data. A common approach for developing more versatile controllers is to combine a kinematic text2motion model with a low-level motion tracking model [Luo et al. 2023; Ren et al. 2023]. These methods first generate a kinematic reference motion using previously mentioned text2motion techniques, and the task of controlling a simulated character is then reduced to simple motion tracking. This approach is able to leverage the scalability of kinematic motion models that are trained on large motion datasets, but the capabilities of the simulated character are then greatly restricted to closely follow the behaviors dictated by the kinematic model. Furthermore, the motion quality of these composite models is still conspicuously lower compared to state-of-the-art physics-based character animation systems.\nThe goal of our work is to develop versatile end-to-end language-directed controllers for physically simulated characters that can be controlled using natural language to perform a large corpus of motor skills. Our model does not require an existing kinematic"}, {"title": "3 OVERVIEW", "content": "In this work, our goal is to train a single versatile control policy capable of responding to thousands of different text commands, while being able to naturally transition between motions. The approach we propose, which we call SuperPADL\u00b9, is inspired by two observations:\n\u2022 Existing reinforcement learning techniques for physics-based animation are able to train policies with many desirable properties, such as the ability to reproduce reference motions with high quality and naturally transition between skills. However, these approaches do not scale beyond at most several hundred motions.\n\u2022 Kinematic motion models, such as motion diffusion models, are able to scale to datasets containing thousands of motions using supervised learning objectives.\nIn light of these observations, we present a method that combines RL and supervised objectives, centered around the progressive distillation of motion controllers. Initially, we seek to train a large number of highly specialized expert policies using RL. We then iteratively distill these experts together with supervised techniques, progressively training more general-purpose and capable models. Concretely, our method is composed of three training stages:\n(1) We first train an independent expert tracking policy on every motion capture sequence in our dataset using DeepMimic"}, {"title": "3.1 Training Per-Motion Expert Tracking Policies", "content": "Recent work has assembled large datasets D = {(mi, Ci)} of motion capture sequences mi annotated with a set of one or more natural language labels Ci [Guo et al. 2022; Punnakkal et al. 2021]. However, these motion capture recordings are kinematic, preventing the direct application of supervised losses when training physically-simulated control policies. In order to address this incompatibility, in the first stage of SuperPADL we translate our motion capture dataset into the physical domain by training an expert tracking policy on every motion in our dataset.\nOur approach is inspired by the first stage of MoCapAct [Wagener et al. 2023], leveraging DeepMimic as our tracking method [Peng et al. 2018]. We train a DeepMimic expert policy \u03c0 (at|ot, \u03c6) on every motion capture sequence in our dataset, conditioned on the current state of character ot as well as a phase variable \u03c6 \u2208 [0, 1] that synchronizes the policy to the reference motion.\nWe train each policy for a maximum of 3000 epochs, corresponding to approximately 200M frames of experience. To reduce overall compute usage, we monitor the cartesian pose error throughout training and stop training early when the error falls below 3cm. If the epoch limit is reached and the pose error remains above 5cm, we discard the motion from our dataset. By leveraging a GPU-accelerated implementation of DeepMimic in NVIDIA Isaac Gym [Makoviychuk et al. 2021; Peng et al. 2018], the majority of tracking policies complete training in under an hour, while only 5% of motions are discarded (see Section 4.1). The discarded items in our dataset often correspond to physically implausible motions, such as those that involve third-party objects that are not recorded in the motion capture sequence (e.g. climbing up nonexistent stairs, sitting on nonexistent chairs, etc.). Additionally, our tracking environment resets whenever a character's body part, excluding the feet, touches the ground. This causes motions like crawling and lying prone to be discarded.\nAfter training each tracking policy, we record a dataset of trajectories containing 10000 observation-action frames from each expert:\nT\u2081 = (o\u2080, a\u2080, o\u2081, a\u2081, ...).\n(1)\nIn practice, in order to increase the diversity of states encountered during these rollouts, T\u1d62 is created in chunks by initializing the character 100 different times using random frames from the reference motion and rolling out each initialization for 100 frames. Additionally, in 90% of rollouts, stochastic actions are sampled from the policy's action distribution, while in the remaining 10% deterministic/greedy actions are taken from the mean of the action distribution. Note that while a combination of stochastic and deterministic actions are used to generate the rollouts, the actions recorded in the trajectory data always correspond to the deterministic action that the expert policy would have taken at each state. This is done to avoid adding noise to the trajectory action labels. We refer to this collection of trajectories D\u2081 = {T\u1d62} as our trajectory dataset."}, {"title": "3.2 Training Group Controllers with PADL+BC", "content": "Our ultimate goal is to train a single model that can perform a wide range of behaviors in response to user text commands and seamlessly transition between different behaviors as the user's command changes. However, at the end of the first stage of SuperPADL training, we are instead left with a collection of highly specialized expert tracking policies. Each expert can reproduce their corresponding motion capture clip with high quality, yet they cannot generate any other motion. Additionally, tracking policies lack the robustness to reliably recover from perturbations or initialization in out-of-distribution states, as might be necessary when transitioning between different motions. Therefore, the next step in our pipeline is to develop more general control policies that retain the motion quality of individual experts while also being much more robust.\nPrevious work has demonstrated that adversarial reinforcement learning can effectively train policies that exhibit these properties [Juravsky et al. 2022; Peng et al. 2022, 2021]. However, as we experimentally demonstrate in Section 4.3, these RL techniques do not directly scale to thousands of motions. We address this limitation by employing a progressive distillation approach in the second and third stages of SuperPADL training. In the second stage of SuperPADL, we train controllers on small groups of motions using an objective combining adversarial RL and behaviour cloning, before performing a second, purely-supervised distillation stage to train a final global policy. In this first distillation stage, we randomly partition our dataset into groups of 20 motions:\nPi = {(m\u2082\u2080\u1d62\u208a\u2081, C\u2082\u2080\u1d62\u208a\u2081), (m\u2082\u2080\u1d62\u208a\u2082, C\u2082\u2080\u1d62\u208a\u2082), ..., (m\u2082\u2080\u1d62\u208a\u2082\u2080, C\u2082\u2080\u1d62\u208a\u2082\u2080)} (2)\nand train a group controller \u03c0\u1d33(at|ot, I) on each partition Pi, parameterized by the current character state ot and the motion index I \u2208 {20i + 1, ..., 20i + 20}. The index I is encoded using a trainable, randomly-initialized embedding table.\nOur goal is for each group controller to:\n\u2022 Imitate the motions in its partition when conditioned on the corresponding motion.\n\u2022 Naturally transition between these motions when the input index changes.\n\u2022 Generally avoid falling over.\nWe optimize each group controller using PADL+BC, a novel objective that combines the adversarial RL setup of PADL with behaviour cloning:\nL = LPADL + 0.01 LBC.\nPADL introduces a motion-conditioned discriminator network:\nDisc(I, s, s') \u2192 [0, 1] (4)\nwhich is trained to distinguish between state transitions (s, s') from a reference motion m\u1d62 and state transitions generated by the policy when conditioned on I. The policy is optimized with PPO [Schulman et al. 2017], using a reward r\u1d62 that encourages the policy to \u201cfool\u201d the discriminator:\nrt = -log (1 - Disc(I, St-1, St)). (5)\nIn addition to this PPO loss, we introduce an additional behaviour cloning loss on the dataset of expert trajectories D\u2081. At each step of optimization, we sample (observation, action) pairs from the"}, {"title": "3.3 Distilling into a Global Text-Conditioned Policy", "content": "Group controllers mark a significant improvement in generalization over the tracking expert policies, enabling a single controller to reproduce multiple motions, naturally transition between motions, and operate without a phase variable. However, the group controllers are still constrained by the relatively small set of motions that each controller is trained on. As we demonstrate in Section 4.3, PADL+BC alone does not scale to the thousands of motions available in open-source motion capture datasets [Mahmood et al. 2019]. Moreover, our group controllers are conditioned using simple motion indices and cannot follow commands in natural language.\nTo train a global, language-conditioned policy \u03c0\u1d33 (at |ot, c) (where c denotes a text caption), we perform a second round of distillation, now leveraging the group controllers as teacher policies. Unlike PADL+BC, which combines RL and supervised training objectives, the training of the global policy is purely supervised, allowing us to scale to much larger datasets.\nTraining of the global policy begins similarly to group controllers with an offline, behaviour cloning warmup phase using the static trajectory dataset D\u2081 collected from the tracking policies. This initializes the state distribution of the global policy to be similar enough to those of the group controllers that they can provide effective teacher feedback.\nFollowing warmup, the global policy is trained to convergence using online imitation learning similar to DAGGER [Ross et al. 2011]. Every epoch, trajectories are rolled using the current global controller. Each observation in those trajectories is then annotated"}, {"title": "3.4 Experimental Details", "content": "3.4.1 Dataset Curation. The motion capture data that we use to train SuperPADL is a filtered subset of AMASS, an open-source aggregation of smaller motion datasets [Mahmood et al. 2019]. We first filter out any motion clips shorter than two seconds or longer than nine seconds. We also apply a series of filters that attempt to detect motions that are physically impossible, such as climbing a staircase or swinging from a bar (third party objects are not included in the motion capture recordings). The plausibility filters examine the heights of the character's limbs and extremities to look for signs that a character has not touched the ground for a prolonged period of time, filtering the motion out of our dataset if such an event is detected. We train DeepMimic experts on a dataset of 5866 filtered motions.\nTo augment this motion data with natural language annotations, we use the HumanML3D dataset of captions [Guo et al. 2022], which provides several captions for every motion in AMASS. To add additional diversity to this data, we use ChatGPT to generate paraphrases using the original set of annotations. When training our global controller on the 5587 motions that pass the expert tracking phase (totalling approximately 8.5 hours of data), there are a total of 48207 captions in the dataset.\n3.4.2 Network Architectures. All policies (tracking experts, group controllers, and global controllers) are trained using simple MLP architectures. While several existing works in adversarial RL use only the character's state at the the current frame as model observations [Juravsky et al. 2022; Peng et al. 2022, 2021], we observe benefits when training group and global controllers that are conditioned on a longer history. We maintain a context window looking 40 frames back into the past, and generate inputs for actor and critic networks by selecting every eighth frame from the window, totalling five total frames of observations. This approach balances providing models with a longer history with restricting the total observation dimension, since excessively large observations can slow down and potentially destabilize training."}, {"title": "4 RESULTS", "content": "We present examples of our global controller reproducing motions from its training data using text commands in Figure 7. We demonstrate that SuperPADL is able to generate an extremely diverse set of motions, ranging from basic locomotion skills and hand gestures to much more difficult martial arts and dancing behaviours. In contrast with kinematic motion diffusion models, where generating a single animation can take up to a minute [Tevet et al. 2023], we highlight that SuperPADL can generate motion in real time on a single consumer GPU, enabling interactive applications.\nSuperPADL is also able to successfully transition between skills, with examples shown in Figure 8. These transition abilities were initially learned by each group controller using adversarial RL, and have been inherited by the global controller through distillation. Note that even though each group controller was only trained to transition between the 20 motions in its group, the global SuperPADL controller is able to transition between any two motions, regardless of their group assignments."}, {"title": "4.1 Training Tracking Experts", "content": "In Figure 4 we visualize the distribution of training times for the expert tracking policies detailed in Section 3.1. All policies were trained on individual NVIDIA A40 GPUs. We see that ending training early based on the most recent tracking error significantly reduces the total cost of compute required to train all experts. A majority of experts finish training in under an hour, and over 30% complete in less than 30 minutes. However, since policies that do not reach the target error threshold are trained until the epoch limit of 3000 epochs, we see that the 5% of rejected policies have an oversized impact on cumulative training cost. This highlights the importance of strong dataset filters that can identify physically-implausible motions before training begins."}, {"title": "4.2 Measuring Controller Quality with Thresholded Precision and Recall", "content": "Measuring tracking error is difficult for policies lacking a phase variable input that synchronizes them to a reference motion. In order to evaluate the quality of motions produced by non-tracking policies (i.e. group and global controllers), we introduce the metrics of thresholded precision and recall. These metrics are inspired by the thresholded coverage metric used in [Juravsky et al. 2022], with our thresholded recall metric being almost identical to that work's construction of thresholded coverage. Thresholded recall measures the fraction of a reference motion that is reproduced by a policy when conditioned to generate that motion. To calculate the recall of a policy on a motion sequence m = (s\u2080, s\u2081, ..., sn), we first roll out a (deterministic) trajectory \u03c4 = (s'\u2080, s'\u2081, ..., s'k) from \u03c0. We then consider all ten-frame-long sliding windows from m and check whether any ten-frame window in \u03c4 is \u201csufficiently close\u201d, as determined by some threshold \u0454. Specifically, we define:\nRec(\u03c4, m, \u0454) = 1/(n-9) \u03a3\u1d62\u208c\u2080\u207f\u207b\u00b9\u2070  I[min\u2c7c\u2208{\u2080,...,k-10} ||s\u1d62:\u1d62\u208a\u2089 - s'\u2c7c:\u2c7c\u208a\u2089||\u2082 \u2264 \u0454] (7)\nwhere sx:y denotes the concatenation of frames (sx, sx+1, ..., sy), and I denotes an indicator variable. The key difference between this metric and thresholded coverage is that thresholded recall operates on windows of consecutive states, while thresholded coverage only considers individual frames. We choose to construct windows to better capture the temporal structure of the reference motion: for example, a hypothetical trajectory that perfectly imitated m in reverse would always produce a perfect thresholded coverage score, but this is not true for thresholded recall.\nComplementing this thresholded recall metric is a thresholded precision metric, which considers all the windows in \u03c4 and measures whether any window in m is sufficiently close:\nPrec(\u03c4, m, \u0454) = 1/(k-9) \u03a3\u1d62\u208c\u2080\u1d4f\u207b\u00b9\u2070  I[min\u2c7c\u2208{\u2080,...,n-10} ||s'\u1d62:\u1d62\u208a\u2089 - sj:j+9||\u2082 \u2264 \u0454] (8)\nWhile thresholded recall measures the fraction of m that the policy imitates, thresholded precision measures the fraction of \u03c4 that imitates a portion of m. For example, a trajectory that perfectly loops a subset of the reference motion would score very highly on precision, but low on recall. Conversely, a trajectory that perfectly imitated the entire reference clip, but then contained some bizarre additional motions, would have a very high recall score but a very"}, {"title": "4.3 Evaluating Global Controllers", "content": "We use thresholded precision and recall metrics to compare our global SuperPADL controller against two baselines trained on the same dataset of 5587 motions. Our first baseline directly applies PADL on the full dataset. Additionally, we train a PADL+BC controller on the entire dataset, instead of on a group of 20 motions. We use the same language encoder architecture as SuperPADL for both baselines and focus on evaluating the motion quality of each method when training on thousands of motions. The policy network sizes are held constant across all three methods. For the PADL and PADL+BC runs, the critic the discriminator are also appropriately scaled up in size.\nWe report our thresholded precision and recall metrics in Figure 5 and Table 1, observing that both baselines achieve lower precision and recall scores than SuperPADL. Qualitatively, these baseline networks are unable to do much more than stay upright and stumble around, appearing to respond very little to the user's text command. The PADL+BC network will occasionally reproduce short snippets of simple motions such as jogging. We emphasize that these baselines attempt to apply adversarial reinforcement learning objectives at scale, while SuperPADL only trains small-scale policies with RL. Instead, SuperPADL relies exclusively on supervised learning (through DAGGER) to train the global controller [Ross et al. 2011].\nWe assess the ability of the SuperPADL global controller to transition between skills in Appendix A. The global controller can successfully transition (i.e. not fall) over 90% of the time, even when transitioning between two skills from different motion groups. Additionally, we evaluate SuperPADL's ability to respond to language commands in Appendix B, showing that human raters are able to match animations to the appropriate caption a majority of the time."}, {"title": "4.4 Evaluating Group Controllers", "content": "We also assess the motion quality of our PADL+BC group controllers when compared against a pure-PADL baseline. We randomly select four groups of 20 motions from our dataset and train a controller using both approaches on each group. Note that unlike the PADL models trained in Juravsky et al. [2022], we train pure-PADL group controllers without any language conditioning, instead using the same simple motion index embedding as our PADL+BC controllers. Additionally, we measure the training time required for each network when using a single A40 GPU.\nWe report our results in Figure 6 and Table 2. We observe that group controllers trained with PADL+BC are able to generate higher-quality motions than vanilla PADL controllers while simultaneously requiring much less GPU time. Qualitatively, we observe that PADL+BC models are less prone to imitating only a subsection of a reference motion than PADL policies. Additionally, we find that PADL+BC models seem to be more successful at looping their generations and avoiding getting stuck. Note that the GPU training time"}, {"title": "5 DISCUSSION", "content": "In this work we presented SuperPADL, a framework for training physics-based text-conditioned animation models on large datasets. Our approach is predicated on the observations that kinematic motion models, using supervised learning objectives, are able to scale to datasets containing thousands of motions, while RL-based approaches can struggle beyond several hundred motions. In light of this, we employ a progressive distillation process, where we first train small expert policies using RL and then iteratively distill them into larger, more capable networks. Our final controller is able to reproduce skills from a dataset of over 5000 motions and naturally transition in response to changing user commands.\nWhile SuperPADL is able to reproduce many motions with high quality, the network can still struggle with some highly dynamic motions, such as ballet dances or jumps. A limitation of our current approach is that any poorly-reproduced skills (or other flaws) in a group controller will cascade into the global controller during distillation. Additionally, the global controller can still fall over, particularly when asked to transition during a difficult motion. For example, the character will often lose its balance when it is asked to transition mid-kick when it is balancing on one leg with the other leg extended. Altering the timing of motion transitions to avoid these sensitive regions can make the transitions much more reliable.\nWe are excited about future work that continues to scale physics-based text-to-motion models to even larger datasets. SuperPADL only uses a fraction of the motions in AMASS and in particular does not train on very long motion capture sequences. While the PADL policies trained in Juravsky et al. [2022] were only conditioned on the current character state, SuperPADL is trained on a history of past states. Future architectures that are given an even wider context may be able to learn from longer reference motions. Additionally, we are interested in exploring alternative combinations of RL and supervised learning for physics-based animation. SuperPADL's global controller is a relatively simple deterministic network, and a more sophisticated generative model setup might be better at modelling multi-modal motion distributions. Since the final distillation stage of SuperPADL is purely supervised, it should be possible to train the global controller as a diffusion model using a denoising objective on the target actions. This would give users access to many of the customization techniques that have been successful with text-to-image models, such as guidance and different noise schedulers [Ho and Salimans 2022]. Overall, we hope that our work contributes to the development of more capable physics-based animation models as well as more powerful, accessible animation tools."}, {"title": "A EVALUATING GLOBAL CONTROLLER TRANSITIONS", "content": "We evaluate the ability of the SuperPADL global policy to transition between skills without falling over. We assess the policy by rolling out \u201ctransition trajectories\u201d where the model is initially conditioned on one caption before the caption is changed to that of a different motion. We sample 4096 trajectories, each 10 seconds long with a transition point sampled uniformly between 3 and 7 seconds into the trajectory. In each trajectory, the character is initialized in a default standing position. We classify a transition as successful if the character does not fall over at any point in the trajectory.\nWe conduct two variations of this experiment. In the first version, we only sample pairs of captions that correspond to two motions from the same motion group (defined in Section 3.2). In the second version, the caption pair must come from motions in two different groups. By evaluating these cases separately, we can assess whether the global controller has learned general transition skills or whether it is limited by the group controllers it is distilled from (which are each only trained to transition between a small group of motions). Our results are summarized in Table 3. We see that the global controller succeeds at the transition over 90% of the time, regardless of whether caption pairs are sampled from the same group or not."}, {"title": "B EVALUATING RESPONSE TO LANGUAGE COMMANDS", "content": "We assess the faithfulness of SuperPADL\u2019s generated motions with respect to language commands using human evaluation\u00b2. We generate evaluation questions by sampling 100 captions from our training dataset and recording a 24-second animation from the global controller for each caption. In each animation, we initialize the character in a default standing pose.\nWe present three human raters with a rendered video of each animation and ask them to select the most appropriate caption from four options, where one answer is the correct caption and the others are randomly sampled from other motions in the dataset. We also present raters with options for \u201cNothing applies\u201d and \u201cMultiple options apply\u201d. The latter option is helpful to identify cases where the \u201cincorrect\u201d alternative captions come from similar motions, for example when both the true caption and an alternative option come from walking clips.\nThe evaluation results summarize our results in Table 4, with raters discerning the correct caption from SuperPADL\u2019s motion a majority of of the time. When examining looking at the evaluation results, we observed that many \u201cNothing applies\u201d responses correspond to motions where the character struggles to leave the initial standing pose and instead remains mostly idle."}, {"title": "C ARCHITECTURE AND TRAINING DETAILS", "content": "C.1 Physics Environment\nAll of our experiments are run using the NVIDIA Isaac Gym simulator [Makoviychuk et al. 2021]. Our character model and observation format matches that of Juravsky et al. [2022], except without a sword and shield. Our action space is 36-dimensional.\nC.2 Expert Tracking Policies\nEach expert tracking policy (and corresponding critic network) is an MLP with two hidden layers containing 1024 and 512 units, respectively. We use an ELU activation function. We encode the reference phase \u03c6 \u2208 [0, 1] using two scalars storing [sin(\u03c6), cos(\u03c6)]. Each network is trained using a single A40 GPU.\nC.3 Group Controllers\nWhen training group controllers, the actors, critics, and discriminators are separate MLP networks with a ReLU activation function and three hidden layers containing [1024, 1024, 512] units. For the actor and critic networks, we provide five frames of character states as input by maintaining a buffer of the 40 most recent frames and sampling every eighth. For the discriminator, we follow the convention of Peng et al. [2022] and use the 10 most recent frames as observations. We encode the motion index using an 128-dimensional embedding table. Each network is trained using a single A40 GPU. We train PADL+BC policies for a total of 10000 epochs (including the 2000-epoch warmup period where we only apply the behaviour cloning loss). When training PADL group controller baselines, we train the policy for 54000 epochs, corresponding to approximately 7B frames.\nC.4 Global Controllers\nAll networks used when training global controllers are MLPs with hidden layers containing [3072, 3072, 3072, 2048] units. We provide the policies (and, when applicable, critic networks) with five frames of history using the same method as the group controllers. We train controllers and baselines using eight A40 GPUs.\nWe train the SuperPADL global controller for 7900 epochs, corresponding to approximately 380K optimization steps, 6B frames of RL training (i.e. 6B online collected samples), and 12 hours of training. Like when training PADL+BC group controllers, we begin with a 2000 epoch BC-only warmup period. We use an ELU activation function and follow every activation layer with a LayerNorm [Ba et al. 2016].\nFor the PADL baseline, we train the model for 14600 epochs, corresponding to approximately 700K optimization steps, 15B frames, and 89 hours of training. For the PADL+BC baseline, we train the model for 14000 epochs, corresponding to approximately 670K optimization steps, 15B frames, and 98 hours of training. We do not use an initial BC-only warmup period (we do not observe it to have a significant impact). For both baselines, we use ReLU activations and no LayerNorm."}]}