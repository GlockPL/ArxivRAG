{"title": "Long-term Fairness in Ride-Hailing Platform", "authors": ["Yufan Kang", "Jeffrey Chan", "Wei Shao", "Flora D. Salim", "Christopher Leckie"], "abstract": "Matching in two-sided markets such as ride-hailing has re-cently received significant attention. However, existing studies on ride-hailing mainly focus on optimising efficiency, and fairness issues in ride-hailing have been neglected. Fairness issues in ride-hailing, including significant earning differences between drivers and variance of passenger waiting times among different locations, have potential impacts on economic and ethical aspects. The recent studies that focus on fairness in ride-hailing exploit traditional optimisation methods and the Markov Decision Process to balance efficiency and fairness. However, there are several issues in these existing studies, such as myopic short-term decision-making from traditional optimisation and instability of fairness in a comparably longer horizon from both traditional optimisation and Markov Decision Process-based methods. To address these issues, we propose a dynamic Markov Decision Process model to alleviate fairness issues currently faced by ride-hailing, and seek a balance between efficiency and fairness, with two distinct characteristics: (i) a prediction module to predict the number of requests that will be raised in the future from different locations to allow the proposed method to consider long-term fairness based on the whole timeline instead of consider fairness only based on historical and current data patterns; (ii) a customised scalarisation function for multi-objective multi-agent Q Learning that aims to balance efficiency and fairness. Extensive experiments on a publicly available real-world dataset demonstrate that our proposed method outperforms existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Ride-hailing systems have become increasingly prevalent as a mode of trans-portation, with platforms, such as Uber, utilising artificial intelligence (AI) al-gorithms to match drivers to passengers efficiently [11]. However, while these algorithms succeed in optimising earnings for drivers and reducing passenger waiting time, they often result in inequities, such as wage disparities based on gender or race [2]. As a result, there is a growing interest in the fair allocation of jobs to drivers in ride-hailing systems.\nA majority of the established research on creating equitable ride-hailing sys-tems relies heavily on heuristic or linear programming approaches [22,18,12,7,6], augmented by mechanisms that promote fairness. These traditional algorithms have benefits such as simplicity in structure and reasonable execution times. However, they fall short of guaranteeing non-myopic solutions that can make far-sighted decisions. In addition, existing studies do not utilise longitudinal his-torical data to identify future patterns for raised requests. These algorithms focus on the immediate future, which lack the predictive capacity for future demand trends and fail to consider historical discrepancies. Raman et al. proposed an allocation system based on a Markov Decision Process (MDP) to balance effi-ciency and fairness, which provides non-myopic allocation plans [14]. However, the approach to addressing equity issues in the ride-hailing system only consid-ers historical patterns for raised requests, without any consideration for future conditions.\nImportantly, in practical contexts, drivers in ride-hailing systems are gener-ally more concerned with their weekly earnings, which can be viewed as long-term earnings as opposed to daily earnings [21,4]. The challenge of maintaining long-term fairness can be attributed to three key factors: i) Short Sight: optimisation algorithms without look-ahead time fall short in harnessing long-term historical data and forecasting future patterns. The example shown in Fig. 1 il-lustrates the challenge. ii) Concept Drift: similar to concept drift in time-series forecasting, existing methods that consider fairness presuppose that future ride requests will adhere to previously observed patterns. However, this assumption is fundamentally flawed as the patterns of ride requests fluctuate continuously due to different factors, including weekday peak hours and public holidays. By its nature, the ride-hailing system necessitates real-time assignments, rendering it unfeasible to accumulate a sufficient volume of ride requests to explore dif-ferent patterns of raised requests before optimising assignments. iii) Disparity between Utility and Fairness Increase across Time. Multiple fairness def-initions have been proposed to evaluate fairness among drivers in ride-hailing"}, {"title": "3 Preliminaries", "content": "Studies revealed that there is a conflict between fairness and utility, and opti-mising fairness without considering utility will cause all the drivers not to get any assignment (no earnings for everyone is absolutely fair) [18,12,17]. Thus, to consider fairness in ride-hailing, the target is to balance between utility and fairness."}, {"title": "3.1 Problem Formulation", "content": "Assume there are $n$ drivers $v \\in V$ who aim to pick up different orders sent by riders, and all drivers and riders reside on a directed graph. The set of locations, $L$, are represented as nodes in this directed graph which can be locations for the $n$ drivers or either pickup or drop-off locations for rider-raised requests. The set of directed edges, $E$, where $e_{i,j} \\in E$, represents the travel distance between locations $l_i, l_j \\in L$. In the real-world scenarios, it is normal that the travel distance from a location (e.g. $l_i$) to another location (e.g., $l_j$) is different from travelling in the opposite direction. Thus, we define the real-world map as a directed graph. We define the utility of a trip between the location $l_i$ and $l_j$ based on $e_{i,j}$ according to existing studies [14,20].\nThe rider requests, driver states and time length are formulated as follow:\nA rider request $r \\in R$ and $r = (t_r, s_r, d_r)$ where the request $r$ is raised at time $t_r$ from locations $s_r$ to $d_r$; $s_r, d_r \\in L$.\nThe state of each of the $n$ drivers at time $t$ can be represented by the tuple $v_t \\in V_t$ and $v_t = (c_v, m, g_t, o_t)$, where $c_v$ represents the capacity of the vehicle that driver $v$ is driving, $m$ is the number of riders in the vehicle at time $t$, $g_t$ is the geographical location of $v$ at time $t$ ($g_t \\in L$) and $g_t$ is dynamically changing based on $t$ with $g_t = -1$ if the driver is travelling on an edge between two different nodes, $o_t(R_v)$ is the total utilities gained by $v$ from the requests that are assigned to $v$, represented by $R_v$, from starting time $t_0$ to $t$.\nThe time length $T = \\{\\{t_{-s}, t_{-s+1}, ..., t_{-1}\\}, t_0, \\{t_1, t_2, ..., t_n\\}\\}$ of the problem is formulated in three parts: the time length $\\{t_{-s}, t_{-s+1}, ..., t_{-1}\\}$ when the historical orders are already completed or being processed, the time step $t_0$ when the current orders have been raised but not been assigned to the drivers, and the time length $\\{t_1, t_2, ..., t_n\\}$ when the future orders will need to be assigned to the drivers.\nIt is important to note the utility of each request $r$ is calculated by $Geo(d_r, s_r) - Geo(s_r, g_t)$ where $Geo$ calculates the shortest geographical distance between two points. The utility reflects the balance between profit, defined as the distance from the start to end location for a request ($s_r$ to $d_r$) and cost, represented by the distance from the current location of the driver ($g_t$) to the start location of the request ($s_r$). The utility calculation is performed at the moment a request is assigned. We assume drivers opt for the shortest possible route, meaning the"}, {"title": "3.2 Efficiency", "content": "This study defines overall efficiency as the total utility acquired across all the drivers over a given timeframe. Given a set of requests $r \\in R$, efficiency aims to find an assignment $M$ that assigns each request to exactly one driver $v \\in V$ to maximise total utility shown as:\n$\\pi(M) = \\sum_{v \\in V} o_{t_n}(M(v))$\n$\\t_n = max(T)$\n(1)\nwhere $M(v)$ represents the set of requests that are assigned to the driver $v$ according to the assignment $M$."}, {"title": "3.3 Long-term Fairness", "content": "We define long-term fairness as the accumulated fairness that consider fairness calculated from the historical, current, and future allocation plan which will be the output from the proposed method using testing data. In this study, we use 1 week as the time horizon with the first three-days records treated as historical data, the fourth day treated as the current data, and the last three days treated as the future data which will be considered as testing data for the proposed method.\nThe fairness is calculated as the accumulated fairness through the whole week as drivers care more about weekly-based earnings instead of daily earnings in the real-world. The fairness is defined as the variance of utilities among the $n$ drivers according to existing studies [14], where utilities for each driver is defined as the total utilities through the whole week. Given a set of requests $r \\in R$, long-term fairness aims to find an assignment $M$ that assign each request to exactly one driver $v \\in V$ to minimise long-term fairness shown as:\n$F(M) = Var (o_{t_n}(M(v)))$\n$t_n = max(T)$\n(2)\nwhere $Var$ represents variance to calculate the variance of total utilities among different drivers."}, {"title": "3.4 Balance of Long-term Fairness and Total Utility", "content": "To maintain a balance between long-term fairness and total utility, we formulate the problem as an operation research question with the formula shown as:\n$\\underset{M}{\u0442\u0430\u0445} \\pi(M) - \\lambda F(M)$\n(3)\nsubject to\n$\\sum_{v \\in V, r \\in R} I_{rv} \\leq 1$\n(4)\nwhere $\\lambda$ is the weight for fairness, and $I_{rv}$ is defined as an indicator function:\n$I_{rv} =\\begin{cases} 1 & \\text{if request $r$ is assigned to vehicle $v$} \\\\ 0 & \\text{else} \\end{cases}$\n(5)"}, {"title": "4 Approach: Optimising Efficiency and Long-term Fairness for Ride-Hailing", "content": "In this section, we introduce a novel solution to both optimise the efficiency and longer-term fairness for ride-hailing applications. The proposed model adopts a multi-objective multi-agent Reinforcement Learning (MOMARL) algorithm to develop the allocation system, driven by three core considerations:\nReal-World Dynamics and Initial Conditions: The varying initial loca-tions of drivers in the real-world significantly influence their ability to serve ride requests, affecting their behaviour and accumulated utility. MOMARL allows different drivers learn different agent behaviours based on their start-ing locations, thereby optimising total income while promoting fairness.\nFairness and Equity Considerations: To address fairness, defined as the equitable comparison of utility gained by drivers, our system employs a central controller to acknowledge status of all agents. This ensures that allocation decisions consider the collective situation of all drivers, promoting a fair distribution of utility and opportunities across the network. The cen-tral controller allows the proposed method dynamically adjusts to real-time conditions and redistributes resources to maintain fairness and efficiency.\nWe designed the scalarisation function in MOMARL to balance utility and fairness and to approach Pareto Optimal. Additionally, a time-series prediction module is incorporated to provide future available actions in MOMARL to allow the proposed method to consider future requests' pattern. The prediction module is implemented to adapt the dynamics of the requests raised by riders based on time in an online manner."}, {"title": "4.1 Overview", "content": "The proposed model comprises a time-series prediction module, multi-objective Reinforcement Learning, and a scalarisation function.\nTime-series forecasting (Sec. 4.2). To encourage longer-term fairness, we utilise time-series forecasting in the proposed model to predict future re-quests as part of the input for MOMAQL. The time-series forecasting mod-ule exploits historical requests in different locations as input and the output is the number of requests that will be raised in the future from different locations.\nMulti-objective multi-agent Reinforcement Learning (Sec. 4.3). In this study, we exploit multi-objective multi-agent Q Learning (MOMAQL) to construct the fundamental part of the proposed model as Reinforcement Learning has been proven to be an efficient method to construct ride-hailing systems [8]. For each time step, the centralised controller assigns each request to an agent (a driver $v \\in V$) if $g_t \\neq -1$. Each objective function of MOMAQL focuses on maximising the utilities for each driver, where the utility of each request is calculated based on the geographical shortest distance. In this way, MOMAQL here output an allocation plan target on maximising the total utilities among different drivers.\nScalarisation function (Sec. 4.4). In order to transform the multi-objective problem into a standard single-objective problem, we then propose a scalar-isation function. The scalarisation function not only aims to transfer the problem, but by maximising the value of the scalarisation function, it also seeks for a balance between efficiency and fairness to approach Pareto Opti-mal.\nThe proposed model operates in four stages in each batch: predicting, evalu-ating, assigning and learning. We first predict the number of future requests in"}, {"title": "4.2 Time-series Forecasting", "content": "The request prediction module is defined based on time-series prediction for which we utilise Multi-Layer Perceptron in this study (MLP). MLP consists of an input layer of source nodes, one or more hidden layers, and an output layer. As an existing study stated that a single hidden layer is sufficient to approximate different continuous functions [3], we use a three-layer MLP in our proposed method. We first utilise the pairs of locations (start and destination locations from different requests) as features, then multiple measurements at time t, (t-1), ..., (t \u2212 n) are used to predict the requests that will happen in the future (the 7 days that we use to test the model), where each time step is set as 1 hour. The structure of the request prediction module has number of neurons in the hidden layer. By using the chosen dataset, we use the previous 1 month of data for training and output the number of requests that will happen based on each pair of locations in the next 7 days."}, {"title": "4.3 Multi-Objective Multi-Agent Q Learning", "content": "To convert the ride-hailing assignment problem to a Markov Decision Process (MDP), we define its foundational elements as follows:\nState: The states are derived from the various locations where drivers are initially positioned and where ride requests originate and conclude. These locations constitute a finite set of states, encapsulating both driver positions and request locations at any given time point t.\nAction: Within the MDP framework, actions at each time point t where $t \\in T = \\{\\{t_{-s}, t_{-s+1}, ..., t_{-1}\\}, t_0, \\{t_1, t_2, ..., t_n\\}\\}$, represent the strate-gic assignment of incoming ride requests to available drivers. The driver is requested to drive from the current location to the start location of the request to pick up the rider and drive to the destination. The proposed method allows an agent (driver) to accept multiple requests concurrently. The time series forecasting module is incorporated into the MDP framework to predict available actions at $t \\in T$, which enables the anticipation of future requests and their strategic incorporation into current decision-making. This forward-looking capability ensures that actions not only respond to immedi-ate demands but also adapt to predicted future conditions. Additionally, the"}, {"title": "4.4 Scalarisation Function", "content": "One of the approaches for multi-objective problems relies on single-policy algo-rithms [10] to learn Pareto Optimal solutions. Single-policy multi-objective Re-inforcement Learning algorithms exploit scalarisation functions over the vector-based reward functions, thereby reducing the multi-objective environment's di-mensionality to a single, scalar dimension. Maintaining a balance between opti-mising utility and improving fairness over a long time horizon is challenging as the two objectives increase at different speeds. Thus we define the scalarisation"}, {"title": "5 Experiments", "content": "5.1 Datasets\nWe exploit New York City Taxi dataset, a publicly available taxi trip dataset collected in New York City, which contains essential information on all requests"}, {"title": "5.2 Experimental Details", "content": "Baselines We select three existing fair ride-hailing methods and Greedy with the objective to balance efficiency and fairness as baselines:\nGreedy. We implement Greedy with the objective of balancing efficiency and fairness according to Eq. 3.\nREASSIGN. REASSIGN exploits traditional optimisation with the objective of balancing efficiency and fairness. To compare with our proposed method, the fairness definition is modified in REASSIGN according to Eq. 2. In the study, Lesmana et al. state that their proposed method can be applied with various fairness definitions [7].\nLAF. The study conducted by Shi et al. exploits a Markov Decision Pro-cess as a re-weighting module to refine the weight for each edge to promote fairness. LAF then utilise Hungarian algorithm to optimise total utility and output the final allocation plan. LAF is used as one of the baseline with the fairness definition modified according to Eq. 2 [17].\nBalance Ride-Pooling 6. The study conducted by Raman et al. exploits a Markov Decision process to optimise the number of rider requests serviced while maintaining fair earning among drivers. We use the method proposed by Raman et al. as one of our baselines [14]. The fairness definition they used in their study is similar to the fairness definition in our study. Hence, the implementation remains unchanged.\nExperimental Settings We selected data before 26/03/2016 as the training data and predicted the requests from 26/03/2016 to 01/04/2016. To reduce the training time, we extracted peak 2-hour data ranging from 19/03/2016 to 01/04/2016 and for the request prediction output. We then use the first seven days data and the extracted output from time-series forecasting to train our proposed model and test the remaining data. During the training process, we used a stratified sampling method with a sampling rate of 0.05 for the training data. We set $\\lambda = 1$ (a parameter shown in Eq. 7) to indicate no preference on utility or fairness, $\\omega = 0.6$ (a parameter shown in Eq. 7) to scale utility and fairness into the same range, $\\gamma = 0.9$ as the value for discount factor for"}, {"title": "5.3 Results and Analysis", "content": "As the range of variance varies significantly based on the attained total utility, we further utilise normalised fairness as another measurement to show experimen-tal results. In this study, we define normalised fairness as normalised standard deviation among the utilities for different drivers shown as:\n$F(M) = \\frac{\\sigma(U)}{\\overline{U}},$\n(8)\nwhere $U$ represents vector records accumulated utility by each driver and $\\overline{U}$ represents the mean utility across all drivers.\nTable 1 and Fig. 4 summarise the results of the proposed method compared to the baselines on the real-world dataset. We further tested the performance of each method by gradually increasing the prediction horizon to test the stability of fairness in terms of the time horizon. All the experiments are conducted under the same experimental settings.\nLong-term Fairness Performance Comparison Under this setting, each method attains an optimised allocation by using the whole seven days testing data. For Table 1, we can observe that: (1) The two objectives, fairness and efficiency, are contradicted to a certain level. For Greedy, with the optimised allocation focusing more on fairness, the total utility can even reach negative, which cannot be a solution in the real-world. In order to obtain a fair result, Greedy tends to sacrifice the utility of all the other drivers to achieve a fair result based on the driver with lowest utility instead of increasing the utility for the driver. Essentially, all the drivers are allocated to the requests with the low-est utility which leads to the final result with the negative value for efficiency. (2) Comparing REASSIGN (based on traditional optimisation) with the Pro-posed Method (based on Reinforcement Learning), we can see that the proposed method perform better, which further supports the statement made by Shah et al. [15] that traditional optimisation makes comparably more myopic decisions compared to Reinforcement Learning. (3) Balance Ride-Pooling proposed by [14] is also based on Reinforcement Learning, Compared with our proposed model, the method does not consider the patterns of requests in the future, which in-dicates the dependency of future patterns can improve the total utility for the drivers. (4) The proposed method achieves more balanced results in terms of efficiency and fairness compared to Greedy and outperforms other baselines.\nStability of Long-term Fairness We aim to compare existing methods with our proposed model in terms of fairness with various time horizons. Fig. 4 shows"}, {"title": "5.4 Ablation Study", "content": "Table 2 and Fig. 5 show the ablation study to answer two following questions:\nHow well does the request prediction module balance total utility and fairness? In this study, Table 2 shows the performance comparison of the proposed method, proposed method without prediction, and proposed method without fairness. The proposed method without fairness does not consider fair earning among different drivers but leads to the highest total utility. By con-sidering fairness, comparing the proposed method with the proposed method without fairness, it shows the request prediction module not only encourages fairer earning among different drivers but also increases the total utility. The Mean Squared Error of the request prediction module is 94.69, and the reason for the improvement on total utility and fairness is that the proposed method utilises the predicted future requests in the training process, which further up-dates the Q-Table based on future patterns of the requests.\nHow well does the request prediction module work on the stabil-ity of fairness with gradually increasing time horizons? Fig. 5 shows the performance of the stability of fairness with gradually increasing time hori-zons on the proposed method, proposed method without prediction, and pro-posed method without fairness. For the proposed method without fairness, as the method does not consider fairness in the allocation, the fairness is compa-rably unstable with high unfairness. The earnings instability among different drivers can cause issues in the real world. Comparing the proposed method and the proposed method without a prediction module, the proposed method with-"}, {"title": "6 Conclusion", "content": "In this paper, we formally proposed long-term fairness, which focuses on achiev-ing stable and comparably higher fairness over comparably longer time horizons. We argue that taxi drivers will care more about long-term earnings. To achieve the target, we introduce a request prediction module before allocation to allow look-ahead windows for the proposed allocation system and eliminate the as-sumption that requests always follow the same pattern. We exploit the output as part of the action space for the allocation model, which we designed using Multi-objective Multi-agent Q Learning. The experiments on real-world data demonstrated the effectiveness of our proposed method for maintaining overall fairness in the comparably longer time horizon and enhancing the stability of fairness when the time horizon gradually increases."}]}