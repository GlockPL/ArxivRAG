{"title": "AN EFFICIENT APPROACH TO REPRESENT ENTERPRISE WEB APPLICATION STRUCTURE USING LARGE LANGUAGE MODEL IN THE SERVICE OF INTELLIGENT QUALITY ENGINEERING", "authors": ["Zaber Al Hassan Ayon", "Gulam Husain", "Roshankumar Bisoi", "Waliur Rahman", "Dr. Tom Osborn"], "abstract": "This paper presents a novel approach to represent enterprise web application structures using Large Language Models (LLMs) to enable intelligent quality engineering at scale. We introduce a hierarchical representation methodology that optimizes the few-shot learning capabilities of LLMs while preserving the complex relationships and interactions within web applications. The approach encompasses five key phases: comprehensive DOM analysis, multi-page synthesis, test suite generation, execution, and result analysis. Our methodology addresses existing challenges around usage of Generative AI techniques in automated software testing by developing a structured format that enables LLMs to understand web application architecture through in-context learning. We evaluated our approach using two distinct web applications: an e-commerce platform (Swag Labs) and a healthcare application (MediBox) which is deployed within Atalgo engineering environment. The results demonstrate success rates of 90% and 70%, respectively, in achieving automated testing, with high relevance scores for test cases across multiple evaluation criteria. The findings suggest that our representation approach significantly enhances LLMs' ability to generate contextually relevant test cases and provide better quality assurance overall, while reducing the time and effort required for testing.", "sections": [{"title": "1 Introduction", "content": "Enterprise web applications constitute the fundamental infrastructure that orchestrates intricate organizational pro-\ncesses and facilitates multiple user interactions fulfilling thee processes [1]. These applications have transcended their\ntraditional role as mere digital interfaces to become critical determinants of operational excellence and market competi-\ntiveness for an enterprise. The exponential growth in application complexity, coupled with heightened user expectations,\nhas elevated quality engineering (QE) to a position of paramount significance in the software development lifecycle.\nThis emerging paradigm encompasses a comprehensive framework of methodologies and practices designed to ensure\nrobust functionality, seamless scalability, and sustainable maintainability of enterprise web applications. Within this\ncontext, the accurate representation and analysis of architectural intricacies in enterprise web applications has emerged\nas crucial elements in achieving holistic quality assurance objectives. When it comes to formal computational model\nof quality engineering processes, precise and formal structural representation of an enterprise application becomes\ncritical. There is a strong correlation between accurate architectural comprehension and an autonomous system's ability\nto provide acceptable level of quality assurance [2]. The evolution of enterprise web applications from monolithic\nstructures to complex, distributed systems has created an imperative for more sophisticated methods of automated\nstructural analysis before applying quality assurance processes. Software testing represents a critical aspect of a\nsoftware engineering lifecycle, serving as a determinant of system reliability, functional integrity, and overall quality\nof the software that will eventually support key processes of a business at scale. As contemporary software systems\ngrow increasingly sophisticated, the imperative for comprehensive testing methodologies becomes progressively more\npronounced. Beyond the conventional paradigm of defect identification and vulnerability remediation, robust testing\nframeworks validate system compliance across diverse operational environments while ensuring adherence to specified\nrequirements. This multifaceted approach serves as a crucial safeguard against system failures, security breaches, and\ncompromised user experiences. The evolution of AI augmented intelligent quality engineering practices transcends\nmere defect detection, managing the entire lifecycle independently and autonomously [3]. The recent applications of AI\ntechnologies have created significant interest from research as well as business community to explore how a formal and\ncomputational model of quality engineering can provide more efficient quality assurance [4]. Recent AI algorithms and\ntechnology stack are showing substantial promise to the testing ecosystem through automation capabilities, adaptive\nlearning mechanisms, and predictive analytics. This synergy between AI and testing methodologies has significant\npotential impact on traditional approaches by optimizing test procedures, minimizing manual intervention, and enabling\nintelligent test case generation coupled with automated anomaly detection systems. Natural Language Processing (NLP),\na specialized domain within AI, has initiated a transformation in software testing practices by introducing sophisticated\nlinguistic comprehension capabilities to the testing environment [5]. Generative AI based methodologies enable\nthe formal interpretation of the requirements written in natural language or semi formal format [6]. This facilitates\ncontext-aware testing strategies which are a paradigm shift from traditional testing approaches. By incorporating\nsemantic understanding into the testing framework, new approach enables the intelligent design of test cases based on\nspecifications and user feedback analysis autonomously. This not only expedites the identification of ambiguities and\ninconsistencies but also frees up the time of the software development team to be able to focus on other areas [7].\nNevertheless, as per our research work as part of developing a computational intelligence-based quality engineering\nplatform, we found that the task of effectively capturing and representing the intricate architectural patterns and\ninteraction paradigms within enterprise web applications presents formidable challenges. The heterogeneous nature of\ndevelopment frameworks, technological stacks, and architectural design patterns introduces substantial complexity in\nmodeling application structures with a level of precision we need such systems to work autonomously. Contemporary\nmethodologies frequently demonstrate limitations in providing dynamic and adaptive representations, thus constraining\ntheir efficacy in addressing the sophisticated requirements of modern enterprise systems as the system change occurs\nperiodically. This introduces growing challenges in the quality assurance area as existing script for testing might\nbreak and provide false result that might lead to false assumptions and failure to detect crucial bugs in the software\nsystems as the system gets updated periodically [8]. The NLP based solutions for test automation tried to solve this\nproblem with self-healing. Self-healing approaches were effective for small changes in the software system but failed\nwhen massive changes were introduced to software systems [9]. Also, NLP based solutions helped only on test script\nmaintenance but not in initial test script generation or test result reporting. The reason behind was the lack of overall\nsystem understanding of the NLP based systems [10]. The emergence of Large Language Models (LLMs), particularly\nthe Generative Pre-trained Transformer (GPT) architecture, has presented us with unprecedented opportunities within\nthe software engineering domain [11]. These models exhibit exceptional prowess in natural language processing and\ndemonstrate remarkable capabilities in generating contextually pertinent and semantically enriched content. Such\ncapabilities can be strategically leveraged to autonomously generate comprehensive representations of web application\nstructures through the systematic interpretation of user interactions, technical documentation, and source code artifacts.\nThe integration of LLMs enables the creation of intelligent, adaptive representations that can serve as a robust foundation\nfor quality engineering initiatives, encompassing automated testing frameworks, anomaly detection mechanisms, and"}, {"title": "2 Background and Related Work", "content": "Recent years have witnessed growing research interest in converting natural language requirements automatically into\nfunctional test scripts, driven by the increasing adoption of agile development and continuous integration practices.\nThis review examines key developments in the field, with a particular focus on systematic analyses, approaches, and\nautomation tools that enable requirements to be transformed into executable test scripts. The systematic literature\nreview by Mustafa et al. [15] represents seminal work in this domain. Their analysis presents a structured overview of\ndifferent automated test generation approaches derived from requirements specifications. A key finding emphasizes\nhow test generation approaches must be carefully matched to handle the inherent characteristics of requirements,\nincluding their potential ambiguity and incompleteness. Building on this foundation, some researchers [16] developed\nan extensive classification system for requirement-based test automation techniques, while also identifying critical\nresearch gaps and obstacles that need to be addressed to improve these methods' efficacy. Both research efforts\nhighlight the critical need to develop sophisticated frameworks capable of accurately processing natural language\nrequirements and producing corresponding test scripts. Taking a more practical perspective, Chrysalidis et al. developed\nan innovative semi-automated toolchain system that converts natural language requirements into executable code\nspecifically for flight control platforms [17]. Their work illustrates how domain knowledge can be effectively combined\nwith automation to streamline test script creation. The system enables engineers to structure requirements in modules\nthat can then be automatically transformed into executable tests, providing a concrete implementation of theoretical\nconcepts outlined in systematic reviews. The research by Koroglu & \u015een breaks new ground by applying reinforcement\nlearning techniques to generate functional tests from UI test scenarios written in human-friendly languages like Gherkin\n[18]. Their approach tackles the complex challenge of converting high-level declarative requirements into concrete\ntest scripts, effectively connecting natural language specifications with automated testing frameworks. Their research\nsuggests machine learning can substantially improve both the precision and efficiency of test script generation. The\nliterature also emphasizes the importance of automation tools like Selenium, with Rusdiansyah outlining optimal\npractices for web testing using Selenium, highlighting its capabilities for automating user interactions and enhancing\ntest accuracy [19]. This aligns with Zasornova's observations that automated testing enables improved efficiency and\nfaster detection of defects - crucial factors in contemporary software development [20]. Yutia presents an alternative\napproach through keyword-driven frameworks for automated functional testing. This methodology enables testers to\ndevelop succinct, adaptable test cases - particularly valuable when working with evolving natural language requirements\n[21]. The previous works on automated test script generation demonstrates consistent efforts to advance the script\ngeneration from natural language requirements. The combination of systematic reviews, practical toolchains, and\nsophisticated approaches including reinforcement learning and keyword-driven frameworks shows a comprehensive\nstrategy for addressing industry challenges. Malik et al. [22] made significant contributions through their work\non automating test oracles from restricted natural language agile requirements. Their proposed Restricted Natural\nLanguage Agile Requirements Testing (ReNaLART) methodology employs structured templates to facilitate test case\ngeneration. This research demonstrates how Large Language Models can effectively interpret and transform natural\nlanguage requirements into functional test scripts, addressing inherent language ambiguities. Additionally, K\u0131c\u0131 et al.\n[23] investigated using BERT-based transfer learning to classify software requirements specifications. Their research\nreveals that Large Language Models can substantially enhance requirements understanding through classification, which"}, {"title": "3 Methodology", "content": "Some applications of Large Language Models have been effective in understanding the natural language. However,\nchallenges remain with respect to the amount of data that can be used as a context to leverage the few shot learning\nof LLM. This makes the usage of LLMs in specialist domains such as test automation quite challenging. One of the\nsolutions is to fine tune the LLM to large amount of data related to a specific field which may result in better reasoning,\nbut then again, it is time consuming and costly process and does not work well for dynamic data. For specific application\nof enterprise test automation, in context learning is the best approach. This is particularly useful for feeding large\nweb DOM structures to utilize LLM for automation script generation, overall site sense making and to get important\ninsight from the website. After struggling with the limitations of in context learning and trying few approaches such as\nchunking, we have developed a novel approach to express overall site structure so that the hierarchy remains intact. This\nis a critical element in our intelligent quality engineering solution. This research introduces an innovative methodology\nto construct a hierarchical structural representation of enterprise web applications, optimized for few-shot learning in\nlarge language models (LLMs). The approach leverages state-of-the-art functional test automation principles to ensure\nscalability, modularity, and enhanced contextual understanding. The proposed methodology is divided into four phases,\neach targeting a specific aspect of web application analysis and representation."}, {"title": "3.1 Phase 1: Comprehensive DOM Analysis and Data Structuring", "content": "The first phase involves a comprehensive analysis of the Document Object Model (DOM) of the target web application.\nUtilizing a custom scraping agent, the methodology extracts all interactive and non-interactive elements from every\npage of the application, starting from the base URL. Key features of this phase include:\n\u2022 Preservation of Hierarchical Context: Parent-child and sibling relationships among elements are meticu-\nlously maintained, along with attributes like locators, URLs, and element types.\n\u2022 Semantic Segmentation: Elements are categorized into logical sections, such as navigation, forms, and\ncontent, based on their roles and attributes.\nThe extracted information is encapsulated into structured representations to facilitate downstream processing. This struc-\nturing preserves the integrity of the application's element hierarchy and contextual relationships, ensuring compatibility\nwith LLM-based reasoning."}, {"title": "3.2 Phase 2: Multi-Page Analysis and Site-Wise Synthesis", "content": "The methodology extends individual page analysis to a multi-page context by synthesizing relationships across the\napplication. This phase comprises:\n1. URL Pattern Analysis: Identifying dynamic segments, common patterns, and page categorizations.\n2. Cross-Page Relational Mapping: Establishing navigation flows, parent-child page hierarchies, and sibling\nrelationships.\n3. Contextual Augmentation: Enhancing page-level summaries with inter-page context and relational metadata.\n4. Page Type Identification: We employed pattern recognition and context aware LLM to find out page types by\nanalysing contents of the pages. The page types are members of a closed set,\nPT = {\"login\", \u201csignup\u201d, \u201caccount\u201d, \u201clisting\u201d, \u201cdetail\u201d, \u201cform\u201d, \"static\"}\nFor any page from any web application the type of page pt \u2208 PT.\n5. Hierarchical Structure Representation: To render the extracted data suitable for LLM few-shot learning, the\nmethodology employs a hierarchical encoding mechanism. The hierarchical representation captures:\n(a) Page-Level Summaries: High-level descriptions of each page, including its type (e.g., login, listing,\ndetail).\n(b) Section-Level Contexts: Logical segmentation of elements within a page, annotated with semantic and\nfunctional metadata.\n(c) Element-Level Details: Attributes, roles, and contextual relationships of individual elements.\n(d) Navigation Flow: Our approach analyses and determine the navigation flow and navigation priority\nbased on page content and element attributes\nThe outcome of this phase is a comprehensive site structure that encodes navigational pathways, interactive dependencies,\nand dynamic state transitions and type of pages. This hierarchical structure is crafted to maximize LLM interpretability\nwhile minimizing input size constraints. Advanced chunking algorithms ensure that the hierarchical integrity is preserved,\nenabling effective reasoning over multi-layered representations. Our approach to represent the web application with\nthis structure format maximize outcome by utilizing LLM's in context learning. As out test generation approach is\ninstruction driven, we can leverage this representation of web application structure to generate test cases which strictly\nfollows instruction."}, {"title": "3.3 Phase 3: Contextual and Relevant Test Suite Generation and Validation", "content": "The final phase demonstrates the utility of the hierarchical structure through the generation of context-aware test suites.\nThe overall site representation is being passed to LLM in a formatted prompt in an iterative manner for each page. Each\nprompt contains information about already generated test cases, extracted URL patterns, provided instructions, available\nelements, required test types. Required test types are combination of predefined test types, E and extracted test types\nfrom instruction, A. Required test types\nR=E \\cup A\nThis involves:"}, {"title": "3.4 Phase 4: Test Suite Execution", "content": "After we have generated a semantically rich formal representation of the web application in four phases above, we ca\nexecute test cases by driving a test framework. We also need to map specific test data to test steps under each test cases\nor alternatively generate synthetic data to execute autonomous test cases. This phase has two major parts:\n1. Test Data Mapping or Synthetic Data generation: Based on element pattern and test case, we employ LLM\nto map test data. The test data schema and test case with valid test steps are used as context for LLM to return\nproper mapping. If test data is not present, LLM's in context learning is being used to generate meaningful\nsynthetic data for given iteration to enable test suites for execution.\n2. Interpreter for Test Suite to Test Automation Framework Language: Test automation framework library\nor tool can be used by writing code in supported programming languages or by using tool's own language or\nrepresentations. Our approach involves training LLM with appropriate tool specific knowledge to overcome\nthis interpretation challenge. According to our experiments, LLM tends to understand codes better than tool\nspecific languages. For interpretation, our approach converts each test steps into tool specific representations.\nThe representation uses set of defined actions for the tool to be used."}, {"title": "3.5 Phase 5: Test Report and Analysis", "content": "All the executed test suites by the proposed system produce results. The results produced by the system can be easily\nunderstandable by test engineers but might not make much sense to individuals from non testing background. Thus our\napproach involves LLM based summaries on top of the result that is easy to understand to any individual with a bit\nof technical knowledge. Our representation of result also leverages LLM's in context learning as the result must co\nrelate with the web application on which the test cases got generated in the first place. To point out exact issue of the\nsystem from the test result, our representation of the web application again plays a role as a test report enhancer. This\nbehaviour mimics experienced test engineers who has extensive knowledge about the web application while producing\na reporting about test results.\nThe entire phases are represented in Fig 1. Figure 1 illustrated the process and data flow of the phases. Phase one starts\nwith scraper, then phase 2 is about formatting the data to the proposed representation. From phase 3 the formatted data\nstarts getting utilized and on phase 4 the represented format gets expanded. Finally on phase 5, the final reasoning step,\nthrough complete test reporting, the process flow ends."}, {"title": "4 Experiment", "content": "To evaluate our novel approach for developing hierarchical structure representation of enterprise web applications and\nits utility in enabling few-shot learning with large language models (LLMs), we conducted comprehensive experiments.\nThese experiments were designed to demonstrate the effectiveness of AI-driven methodologies, including web scraping,\nsite analysis, automated test case generation, and execution, across different web application contexts."}, {"title": "4.1 Objectives", "content": "1. Validation of scraped data for sense making from web applications"}, {"title": "4.1.1 Experimental Setup", "content": "The experiments were carried out in two different enterprise Web applications:\n1. Swag Labs: An platform, specifically focused on its login functionality hosted at https://www.saucedemo.\ncom/.\n2. MediBox A application, with emphasis on user signup functionality. The application is hosted in Atalgo\nEngineering development environment.\nBoth applications provided an opportunity to test our approach under different structural and functional paradigms,\nallowing a broad evaluation of the methodology."}, {"title": "4.2 Scope of Experiments", "content": "To evaluate our approach, we need to evaluate results of each phase individually. Each phase depends on our data\nrepresentation, and we have experimented each phase with our data representation to assess the correct representation.\nIn this section, we will discuss the experiments conducted."}, {"title": "4.2.1 Web Scraping and Data Extraction", "content": "For both applications, we utilized an AI-powered scraping module to extract metadata for each HTML element. This\nprocess was essential for building a structured hierarchical representation of the web applications. We have tried\nextracting all features and even tried extracting and passing the entire DOM tree. The earlier approaches induced\nproblems. There were lot of unnecessary data which was clouding LLM's decision and resulting in a number of\nirrelevant test cases. Also, the results of web application sense making through LLM was not up to the mark. Through\nour experiments, we found that elements need unique identifier apart from their locator. Locators for sibling elements\nlook almost similar sometimes, which can potentially mislead LLM and generated test case might end up with wrong\nelements. The overall DOM tree can be large which makes it impractical to provide overall data to LLM for web\napplication sense making through in context learning as LLMs have fixed context window. We cannot feed LLM with\nthe entire DOM tree, even essential extracted features only are impractical. We have to make sure our solution works"}, {"title": "4.2.2 Site Analysis and Hierarchical Representation", "content": "The scraped data was fed into a site analysis module to organize elements into logical sections and establish their\nrelationships. This module created a hierarchical representation of the web applications by:\n\u2022 Categorizing elements into sections such as navigation links, input forms, and feedback mechanisms.\n\u2022 Mapping relationships to construct a complete DOM tree, allowing downstream processing to focus on key\ninteractive regions of the applications.\nThe analysis facilitated by our approach effectively transformed unstructured web data into a hierarchical format,\nserving as a foundation for automated test case generation."}, {"title": "4.2.3 Automated Test Case Generation Using LLMs", "content": "Our methodology leveraged generative AI and LLMs to automatically generate functional test cases. The structured\nhierarchical data served as input, and the LLMs were prompted to create test cases covering diverse scenarios, including:\n1. Field Validation: Ensuring the presence and functionality of input fields.\n2. Error Handling: Validating error messages for invalid or missing inputs.\n3. Navigation and Redirection: Verifying transitions between application pages.\n4. Data Consistency: Ensuring logical consistency between dependent fields, such as password and confirmation\npassword.\nFor example, in MediBox, test cases included verifying unique email and mobile number registration, password strength\nvalidation, and successful user registration."}, {"title": "4.2.4 Test Case Execution and Data Handling", "content": "Once generated, the test cases were executed systematically against the respective applications. The execution process\nincluded:"}, {"title": "4.3 Applications and Scope", "content": "The hierarchical representation enabled effective utilization of LLMs in a few-shot learning setup, reducing the\ndependency on large labelled datasets. By focusing on structural and functional representations of web applications, the\napproach provided the following advantages:\n\u2022 Scalability: The pipeline could adapt to various web applications with minimal customization.\n\u2022 Automation: From data extraction to test case execution, the process minimized human intervention.\n\u2022 Precision: The hierarchical representation ensured that LLMs received contextually relevant prompts, leading\nto accurate and comprehensive test cases."}, {"title": "4.4 Experimental Design Highlights", "content": "For Swag Labs, 10 functional test cases focused on login validation were developed and executed. For MediBox,\nanother set of 10 test cases targeted the user signup process, ensuring robust field validation and error handling. Each\ntest case was categorized by priority, with high-priority cases addressing core functionalities such as login authentication\nand secure user registration."}, {"title": "4.5 Execution Workflow", "content": "The execution of test cases across both Swag Labs and MediBox applications followed a systematic workflow, leveraging\nthe hierarchical representation to guide the interactions:\n1. Test Case Initialization:\n\u2022 Generated test cases were categorized based on priority and functional grouping.\n\u2022 Each test case contained a detailed sequence of actions, expected outcomes, and validation criteria.\n2. Environment Setup:\n\u2022 A controlled testing environment was configured for both applications, ensuring consistent and repro-\nducible results.\n\u2022 Testing scripts were deployed to interact directly with the DOM elements as identified in the hierarchical\nstructure.\n3. Simulation of User Interactions:\n\u2022 The execution engine simulated user interactions, such as entering text in input fields, clicking buttons,\nand navigating between pages.\n\u2022 For Swag Labs, the interactions included logging in with various credential combinations (e.g., valid,\ninvalid, empty fields).\n\u2022 For MediBox, the tests involved filling out the signup form with different datasets to validate field-level\nconstraints and consistency.\n4. Real-Time Validation:\n\u2022 Each interaction was validated against the expected behaviour.\n\u2022 Success and failure conditions were logged, with detailed error messages captured for deviations.\n5. Dynamic Data Handling:\n\u2022 Synthetic data generation modules were employed to provide realistic test inputs, particularly for Medi-\nBox's signup form, covering edge cases like invalid email formats and weak passwords.\n\u2022 This ensured that the testing covered a wide spectrum of potential user inputs.\n6. Logging and Reporting:\n\u2022 Comprehensive logs, including timestamps, actions performed, and results, were generated.\n\u2022 Screenshots were captured for failed test cases to aid in debugging and root cause analysis."}, {"title": "4.6 Challenges Encountered", "content": "While the experimental pipeline was robust, certain challenges highlighted areas for further refinement:\n1. Element Accessibility:\n\u2022 Some UI elements required additional efforts for identification due to dynamic CSS selectors or hidden\nstates, particularly in MediBox's signup flow.\n2. Latency in Response Validation:\n\u2022 Delays in server responses caused intermittent issues during real-time validation, necessitating timeout\nadjustments.\n3. Dynamic Data Dependencies:\n\u2022 Handling dependencies between fields (e.g., password and confirmation password) required specific logic\nto ensure consistent behaviour."}, {"title": "4.7 Evaluation Metrics", "content": "To evaluate the effectiveness of the experiment, the following metrics were used:\n1. Coverage: The proportion of application functionalities covered by generated test cases.\n2. Execution Success Rate: The percentage of test cases successfully executed without errors.\n3. Error Categorization: Classification of failures to identify patterns and prioritize fixes (e.g., UI-related,\nserver-side issues).\n4. Efficiency: Time taken per test case execution and overall testing cycle duration."}, {"title": "5 Result and Analysis", "content": "As part of our experiments, we have generated and executed test suites. The ultimate goal is to be able to provide\nquality assurance on the software application using generative AI. For the purposes of this specific research, we wanted\nto confirm whether our unique approach of site representation for the purposes of enabling LLM to conduct in context\nlearning helps us achieve effective quality engineering. We have discussed as part pf methodology how this approach of\nweb application representation is foundational to achieve AI augmented quality engineering. To evaluate the overall\nefficiency of our data structure, we have assessed the quality of the generated test cases. The evaluation criteria we\nestablished for this assessment are:\n1. Test case execution success rate\n2. Relevance of the test cases based on instruction provided\n3. Relevance of the test cases for the web application under test\n4. Relevance and accuracy of test data mappings\n5. Quality of synthetic data generation\nThese criteria for evaluation are relevant for any enterprise test automation project and are regularly used by automation\nengineers, implicitly or explicitly. The final test execution results are produced by the Selenium test automation tool\nwhich is orchestrated by our AI platform, Flame. Based on the outlined evaluation criteria, the performance and\nefficiency of the developed approach for representing enterprise web application structures using Large Language\nModels (LLMs) in the service of intelligent quality engineering were analysed using two distinct use cases: the Swag\nLabs web application and the Atalgo Engineering's development/testing environment, known as MediBox platform. We\npresent below results of various components of our experiments, highlighting the relevance and impact of the novel\nstructure in overall functional testing. We will show case the generated test suites for both MediBox and Swag Labs\nplatforms then we will provide assessment against the evaluation criteria."}, {"title": "5.1 Evaluation", "content": "We have split the entire evaluation process into 5 steps. As the novel representation method is being used in 5 phases,\nwe had to evaluate each phase to get complete evaluation. The process or phases are sequentially dependent i.e the next\nstep always depends upon the output of previous step. Let's discuss the evaluation steps in this section:"}, {"title": "5.1.1 Test Case Execution Success Rate", "content": "The test execution success rate was calculated as the percentage of test cases that executed successfully without errors\n(and they were expected to behave the same way). We have used Selenium (Test Automation Platform) to perform\nautomated test execution from the generated test suites. The results for both applications are summarized in Table 3."}, {"title": "5.1.2 Relevance of Test Cases Based on Instructions", "content": "Swag Labs:\n\u2022 Instructions: Fetch all input fields, buttons, and labels from the homepage of Sauce Demo (https:\n//www.saucedemo.com/) and provide the details for each element, including their id, class, and attributes.\nGenerate a detailed test plan for logging in to the Sauce Demo application using valid credentials (stan-\ndard_user/secret_sauce) and invalid credentials. Include preconditions, steps, expected outcomes, and priority\nfor each test. Create a Selenium test script in Python to automate the login functionality of Sauce Demo.\nThe script should test both successful and failed login attempts and validate error messages. Execute the\ngenerated Selenium test script for Sauce Demo and provide a detailed test summary, including pass/fail results,\nscreenshots of failures, and execution time for each test.\n\u2022 Test cases directly addressed the login functionality's critical scenarios, confirming adherence to instructional\ngoals.\n\u2022 Coverage included variations in credentials, field emptiness, and user-specific conditions.\nMedibox:\n\u2022 Instructions: Create and execute a minimum of 10 functional test scripts specifically for the user signup\nprocess.\n\u2022 Deployed on our experiment environment\n\u2022 User Signup Endpoint URL: /UserSignup\nDetails:\n\u2022 Ensure the test cases cover a wide range of scenarios for comprehensive validation.\n\u2022 Test cases were comprehensive, targeting user registration with field validations, navigation checks, and logical\nconsistency.\n\u2022 Generated scenarios matched the specified instructions for ensuring accurate and secure user onboarding."}, {"title": "5.1.3 Relevance of Test Cases for the Web Application", "content": "Swag Labs:\n\u2022 Structural insights from the DOM allowed the generation of contextually relevant test cases tailored to the\napplication's architecture.\n\u2022 Example: Dynamic field validations and error feedback mechanisms were precisely targeted.\nMediBox:\n\u2022 Detailed analysis of navigation links and form sections ensured alignment with MediBox's hierarchical\nstructure.\n\u2022 Key observation: Integration of URL pattern recognition enabled mapping of related endpoints, enhancing\nnavigation validation."}, {"title": "5.1.4 Test Data Mappings Relevance", "content": "Swag Labs:\n\u2022 Mapped data consistently reflected the application's structural metadata, enabling accurate input-output\nscenarios.\n\u2022 Example: User-specific identifiers like \"locked-out user\" and \"problem user\" ensured realistic validation.\nMediBox:\n\u2022 Synthetic data generation effectively complemented user-provided data to fill missing fields during execution.\n\u2022 Data mappings accurately represented user inputs, confirming seamless integration of synthetic and real data."}, {"title": "5.1.5 Synthetic Data Generation Contextual Relevance", "content": "Swag Labs:\n\u2022 Synthetic data aligned with expected field formats, enabling smooth execution of test cases with varying\ncredentials.\n\u2022 Observations:\n\u2022 Data diversity enhanced robustness in edge case validations.\nMediBox:\n\u2022 Contextual relevance was evident in generated data for email, mobile number, and password fields.\n\u2022 Observations:\n\u2022 Password strength validation scenarios benefited significantly from the synthesized data.\nAlthough not directly related to the objective of this specific research, we have timeboxed the time taken in each of\nthese activities because in actual real-world projects, speed of execution is of high importance. This provides significant\nsavings at scale. We have concluded that once the initial setup and the configuration of the requirements, instructions\netc are complete, this approach provides significant time savings (upwards of 50%) compared to a traditional test\nautomation approach. This saving is more pronounced in the maintenance phase of the project and becomes significant\nas the software application scales."}, {"title": "6 Future Work and Conclusion", "content": "Our research demonstrates the successful application of LLM's few-shot learning capabilities in automated test script\ngeneration and execution, despite using models not specifically trained for the functional testing domain. The results\nvalidate our approach to hierarchical web application representation while also highlighting areas for future enhancement.\nFrom this point onward, towards the goals of computational model of quality engineering, two primary directions\nemerge for the future research. First, we plan to fine-tune LLMs specifically for the test automation domain using\ncurated datasets. This specialized training should address current limitations in handling large context windows and\ninput sizes for individual requests. Second, we propose developing a knowledge graph architecture to store and\nefficiently retrieve element-level data during run-time, potentially reducing the input size required for page-level test\ncase generation. Although our solution demonstrates robust performance across most web applications in generating\nquality test suites, we acknowledge certain limitations. Applications with exceptionally large DOM structures can\nchallenge our algorithm's ability to maintain hierarchical relationships. Furthermore, the substantial input sizes required\nby our representation method may lead to increased costs when using commercial LLM services, although the benefits\nof improved test coverage and maintenance efficiency will often justify this investment. The methodology's strength lies\nin its ability to enable LLMs to comprehend and interact with web application structures through in-context learning,\nproducing contextually relevant test cases. Looking ahead, we believe that the integration of domain-specific fine-tuning\nand knowledge graph-based element retrieval will significantly mitigate current limitations, further enhancing the\nscalability and cost-effectiveness of our approach. These improvements will pave the way for more efficient and\nautonomous quality engineering processes in enterprise web applications."}]}