{"title": "PHANTOM: SUBJECT-CONSISTENT VIDEO GENERATION VIA CROSS-MODAL ALIGNMENT", "authors": ["Lijie Liu", "Tianxiang Ma", "Bingchuan Li", "Zhuowei Chen", "Jiawei Liu", "Qian He", "Xinglong Wu"], "abstract": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.", "sections": [{"title": "1 INTRODUCTION", "content": "The rise of diffusion models (Peebles & Xie, 2023; Ho et al., 2020) is rapidly reshaping the field of generative modeling at an astonishing pace. Among them, the advancements in video generation brought by diffusion models are particularly remarkable. In the visual domain, video generation needs to pay more attention to the continuity and consistency of multiple frames compared to image generation, which poses additional challenges. Inspired by the scaling laws of large language models (OpenAI, 2024; Touvron et al., 2023; Yang et al., 2024a), the focus of video generation has shifted towards investigating foundational large models, such as those similar to Sora (OpenAI, 2023; Polyak et al., 2024; Yang et al., 2024b; Kong et al., 2024; Zheng et al., 2024), which have demonstrated exciting visual effects and are paving the way for a new paradigm in Artificial Intelligence Generated Content (AIGC). Nevertheless, up to now, various vertical applications of video generation tasks still lag behind image generation tasks.\nCurrently, foundational video generation models primarily focus on two major tasks: text-to-video(OpenAI, 2023) and image-to-video (Blattmann et al., 2023). Text-to-video (T2V) leverages language models to understand input text instructions and generate visual content describing the expected characters, movements, and backgrounds. While it allows for creative and imaginative content combinations, it often struggles with generating consistently predictable results due to inherent randomness. On the other hand, image-to-video (I2V) typically provides the first frame of an image along with optional text descriptions to transform a static image into a dynamic video. While more controllable, the content richness is often limited by the strict \"copy-paste\" (Polyak et al., 2024; Chen et al., 2025) nature of the first frame. Capturing the subject from the image and flexibly generating video based on text prompts, combining the diversity and controllability of image and text joint generation, we term this subject-consistent video generation as subject-to-video (S2V) (Huang et al., 2025; Chen et al., 2025; MiniMax, 2024). Its essence lies in balancing the dual-modal prompts of text and image, requiring the model to simultaneously align text instructions with the reference image content."}, {"title": "2 PHANTOM", "content": "In the subject-to-video task setting, existing works primarily focus on identity preservation for individuals. For example, ID-Animator (He et al., 2024), derived from the Adapter (Ye et al., 2023) concept in image tasks, injects facial information into the model to generate ID-preserving videos. ConsisID (Yuan et al., 2024) further distinguishes between high and low-frequency ID information for model injection. However, these works have been validated on small datasets (around 10k), which limits their ability to fully align facial information with text descriptions. MovieGen (Polyak et al., 2024) scales up to a larger dataset (around 10M) and trains on a larger model (30b parameters), demonstrating realistic and appealing results. The most aligned approaches for this task are demonstrated by the commercial tools Vidu (Vidu, 2024), Pika (Pika, 2024) and Keling (keling, 2024), which support generating subject-consistent videos using multiple reference images and text descriptions. The reference subject elements are not limited to characters, ID, or clothing but also include buildings, landscapes, and other components.\nIn summary, the subject-to-video task aims to deeply and simultaneously align the conditions of both text and image modalities. To achieve this, we first constructed a data structure consisting of text-image-video triplets. Unlike T2V, we have re-annotated the video captions (Team et al., 2023) to focus on describing the appearance and action of the subjects in the video. Additionally, the reference images for subjects are not naively taken from a single video frame but are sampled across multiple videos, ensuring that the generated video does not simply copy-paste the images like I2V. Furthermore, we redesigned the joint image-text injection model based on existing foundational video models to ensure effective learning of cross-modal data representations. Overall, we developed an algorithmic framework for subject-to-video, which is competitive with existing solutions on the market (Vidu, 2024; Pika, 2024; keling, 2024). Notably, it demonstrates superior performance in maintaining subject consistency compared to current ID-preserving expert models (Yuan et al., 2024; MiniMax, 2024)."}, {"title": "2.1 DATA PIPELINE", "content": "To achieve subject-to-video (S2V) alignment, we construct image, text, and video triplet data structures for cross-modal learning (Figure 3), requiring videos to be simultaneously paired with both images and text. Since our S2V capability is fine-tuned from a text-to-video (T2V) base model, we aim to reuse the T2V captioning scheme, leveraging vision-language models to directly comprehend videos. Inspired by MoiveGen (Polyak et al., 2024), we categorize image prompts into two types: in-paired and cross-paired. In-paired data involves selecting keyframes from videos as reference images. While this ensures consistency between the subjects in the images and videos, the high visual similarity may lead the model to overlook the text prompts, resulting in generated videos that simply copy-paste the input images. To mitigate this issue, we construct cross-paired data by matching"}, {"title": "2.2 FRAMEWORK", "content": "elements across different videos and filtering out clips with high visual similarity. Cross-paired data can come from different segments of the same long video or by retrieving reference subjects from a database.\nAfter constructing the cross-paired data pipeline, further segmentation is required based on application scenarios. We define subject-to-video (S2V) as extracting primary elements from an image and generating a video controlled by text. These primary elements include people, animals, objects, backgrounds, and more. Additionally, interactions between multiple elements can further categorize scenarios, such as multi-person interactions, human-pet interactions, and human-object interactions. By segmenting the data sources according to these application scenarios, we can quantitatively supplement missing data types. For example, virtual try-on applications require specific collections of model images and garment layouts. Furthermore, since our S2V capabilities are fine-tuned from the text-to-video (T2V) base model, bypassing the base model pre-training stage, we need to filter for higher data quality rather than merely increasing data quantity.\nThe Phantom architecture is illustrated in Figure 4, where the model is divided into an untrained input head and a trainable DiT (Peebles & Xie, 2023) module. For the DiT part, we refer to the MMDIT (Esser et al., 2024) structure, which is one of the mainstream choices for both image and video foundation models. The MMDiT module undergoes pre-training similarly to conventional T2V and first-last-frame I2V processes (Zeng et al., 2024), thus acquiring the necessary capabilities. In the input head, the video encoder (Yang et al., 2024b) and text encoder (Yang et al., 2024a) inherit weights from the base model, encoding input video and text prompts into corresponding latent features.\nCrucially, the reference image is encoded by a specific vision encoder and then concatenated with video features and text features separately. These concatenated features are input to the vision branch and text branch of DiT for computation. This approach modifies only the model's input without affecting the DiT structure itself. Specifically, the vision encoder is composed of Variational Auto-Encoder (VAE) (Esser et al., 2021) and CLIP (Zhai et al., 2023; Radford et al., 2021). Image features concatenated with video latents reuse the 3D VAE (Yang et al., 2024b) to maintain consistency in the visual branch input. Meanwhile, image features concatenated in the text branch use CLIP to provide high-level semantic information, thus compensating for the limitations of the low-level VAE features."}, {"title": "3 EXPERIMENTS", "content": "Phantom is fine-tuned from a video generation foundation model based on the DiT architecture. The T2V and I2V pre-training stages are excluded from this evaluation. We focus on assessing the subject consistency generation capability, with additional independent evaluations for face ID-based"}, {"title": "3.3 QUALITATIVE RESULTS", "content": "Here, we present the comparison results of several typical cases in Figures 6, 7, 8. Each generated video is displayed with four evenly sampled frames, including the first and last frames. Figures 6 and 7 respectively show the results of generating single and multiple subject consistency. It can be seen that Vidu (Vidu, 2024) and Phantom exhibit balanced performance in subject consistency, visual effect, and text response. Pika (Pika, 2024) performs poorly in subject consistency. Keling (keling, 2024) has a notable issue: some cases are very similar to I2V. For instance, the first frame of character videos almost matches the input reference image, leading to low success rates in virtual try-on scenarios. Additionally, the laptop case shows that the evaluated methods tend to cause deformations in rigid body movements. Figure 8 shows the results of video generation for facial ID preservation. The open-source method ConsisID (Yuan et al., 2024) tends to exhibit motion blur, and has weak text response. Hailuo (MiniMax, 2024) excels in visual aesthetics, but there is some loss in facial similarity. Our results are balanced across all dimensions, with particular advantage in ID consistency."}, {"title": "4 CONCLUSION", "content": "We propose Phantom, a method for subject-consistent video generation using a text-image-video triplet structure for cross-modal alignment. We redesigned the joint text-image model injection method based on existing video foundation models to effectively learn cross-modal data forms. This algorithm framework is highly competitive, particularly in unifying facial ID preservation tasks. Experimental results show that Phantom maintains an advantage over some commercial solutions."}]}