{"title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models", "authors": ["Zhiteng Li", "Mingyuan Xia", "Jingyuan Zhang", "Zheng Hui", "Linghe Kong", "Yulun Zhang", "Xiaokang Yang"], "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing (NLP) tasks, yet their substantial memory requirements present significant challenges for deployment on resource-constrained devices. Singular Value Decomposition (SVD) has emerged as a promising compression technique for LLMs, offering considerable reductions in memory overhead. However, existing SVD-based methods often struggle to effectively mitigate the errors introduced by SVD truncation, leading to a noticeable performance gap when compared to the original models. Furthermore, applying a uniform compression ratio across all transformer layers fails to account for the varying importance of different layers. To address these challenges, we propose AdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD introduces adaComp, which adaptively compensates for SVD truncation errors by alternately updating the singular matrices U and VT. Additionally, AdaSVD introduces adaCR, which adaptively assigns layer-specific compression ratios based on the relative importance of each layer. Extensive experiments across multiple LLM families and evaluation metrics demonstrate that AdaSVD consistently outperforms state-of-the-art (SOTA) SVD-based methods, achieving superior performance with significantly reduced memory requirements. The code and models will be available at https://github.com/ZHITENGLI/AdaSVD.", "sections": [{"title": "1. Introduction", "content": "Recently, large language models (LLMs) based on the Transformer architecture (Vaswani, 2017) have shown remarkable potential across a wide range of natural language processing (NLP) tasks. However, their success is largely driven by their massive scale, with models such as the LLaMA family (Touvron et al., 2023a) and the Open Pre-trained Transformer (OPT) series (Zhang et al., 2022) containing up to 70B and 66B parameters, respectively. The substantial memory requirements of these models present significant challenges for deploying them on mobile devices. Consequently, the widespread adoption of LLMs remains limited by their immense resource demands (Wan et al., 2023; Wang et al., 2024a; Zhou et al., 2024).\nRecent research on large language model (LLM) compression has explored various techniques, including weight quantization (Lin et al., 2024; Frantar et al., 2023), network pruning (Sun et al., 2024; Frantar & Alistarh, 2023), low-rank factorization (Wang et al., 2024b; Zhang et al., 2024; Yuan et al., 2024), and knowledge distillation (Zhong et al., 2024; Gu et al., 2024). Among these methods, low-rank factorization using Singular Value Decomposition (SVD) (Hsu et al., 2022a; Yuan et al., 2024; Wang et al., 2024b) stands out as a powerful approach for reducing both model size and computational cost. SVD achieves this by decomposing large weight matrices into smaller, low-rank components while preserving model performance. Since LLMs are often memory-bound during inference (Dao et al., 2022; Dao, 2024), SVD compression can effectively accelerate model inference by reducing the memory requirements, even when"}, {"title": "2. Related Works", "content": "2.1. LLM Compression Techniques\nRecent advancements in model compression techniques have significantly enhanced the efficiency of deploying LLMs while maintaining their performance. Widely explored approaches include weight quantization (Frantar et al., 2023; Lin et al., 2024), network pruning (Frantar & Alistarh, 2023; Ma et al., 2023; Yang et al., 2024; Gromov et al., 2024; Ashkboos et al., 2024), and hybrid methods (Dong et al., 2025). In unstructured pruning, SparseGPT (Frantar & Alistarh, 2023) prunes weights based on their importance, as determined by the Hessian matrix. However, it faces challenges in achieving optimal speedup, particularly due to hardware compatibility issues. Structured pruning methods, in contrast, are more hardware-friendly. LLM-Pruner (Ma et al., 2023) selectively removes non-critical coupled structures using gradient information. LaCo (Yang et al., 2024) introduces a layer-wise pruning strategy, where subsequent layers collapse into preceding ones. Gromov et al. (2024) explores the effectiveness of basic layer-pruning techniques combined with parameter-efficient fine-tuning (PEFT). Additionally, SliceGPT (Ashkboos et al., 2024) has pioneered post-training sparsification, emphasizing the importance of layer removal order for optimal performance. Quantization techniques offer another significant avenue for compression. GPTQ (Frantar et al., 2023) applies layer-wise quantization and reduces quantization errors through second-order error compensation. AWQ (Lin et al., 2024) introduces activation-aware weight quantization, employing a scale transformation between weights and activations. Moreover, BiLLM (Huang et al., 2024) and ARB-LLM (Li et al., 2025) achieve further compression to 1-bit while maintaining remarkable performance. More recently, STB-LLM (Dong et al., 2025) combines 1-bit quantization with pruning to achieve even greater memory reduction for LLMs. However, many of these compression techniques face challenges related to hardware compatibility,"}, {"title": "3. Method", "content": "Overview. As illustrated in Figure 2, our AdaSVD integrates adaptive compensation for SVD truncation (adaComp) with an adaptive importance-aware compression ratio method (adaCR). In Section 3.1, we first describe how adaComp compensates for SVD truncation. Next, in Section 3.2, we detail how adaCR determines the compression ratio based on layer importance. The pseudocode of AdaSVD is shown in Algorithm 1, and pseudocodes for adaComp and adaCR are provided in supplementary file.\n3.1. Adaptive Compensation for SVD Truncation\nSVD compression first applies SVD decomposition for matrix W, and then truncates the smallest singular values:\n$W = U\\Sigma V^T \\approx U_k\\Sigma_k V_k^T = \\hat{W}$,\nwhere $\\Sigma_k$ indicates the retaining top-k largest singular values, $U_k$ and $V_k$ represent the corresponding retaining singular vectors. Moreover, the diagonal matrix $\\Sigma_k$ can be further absorbed into $U_k$ and $V_k$ by\n$U_k^\\prime = U_k \\Sigma_k^{1/2}, V_k^\\prime = \\Sigma_k^{1/2} V_k$,\n$\\hat{W} = U_k\\Sigma_k V_k^T = U_k^\\prime(V_k^\\prime)^T$.\nfor the SVD compression error, defined as follows:\n$L_{SVD} = ||WX \u2013 \\hat{W}X||_F$\n$ = {||U_k^\\prime (V_k^\\prime)^T X \u2013 WX||_F}$.\nPrevious works (Hsu et al., 2022b; Yuan et al., 2024; Wang et al., 2024b) have made significant efforts to minimize LSVD. However, some of these methods involve complex and time-consuming preprocessing steps. Furthermore, they still face substantial challenges in effectively mitigating the large errors that arise under high compression ratios, particularly when truncating 50% or more of the parameters.\nTo compensate for the error attributed to SVD truncation, we need to optimize the following objective:\n$U_k^\\prime, V_k^\\prime = \\arg \\min_{U_k, V_k} {||U_k^\\prime (V_k^\\prime)^T X \u2013 WX||_F}$.\nA straightforward approach is to compute the partial derivatives of the SVD compression objective with respect to $U_k^\\prime$ and $V_k^\\prime$, resulting in the following expressions (additional details can be found in the supplementary file):\n$\\frac{\\partial L_{SVD}}{\\partial U_k^\\prime} = 0$\n$\\Rightarrow U_k^\\prime = WXX^T V_k^\\prime((V_k^\\prime)^TXX^TV_k^\\prime)^{-1}$,\n$\\frac{\\partial L_{SVD}}{\\partial V_k^\\prime} = 0$\n$\\Rightarrow V_k^\\prime = ((U_k^\\prime)^T U_k^\\prime)^{-1}(U_k^\\prime)^T W$.\nHowever, this method involves computing the matrix inverse, which can lead to unstable updates and significant compression errors, as shown in Figure 3 (a). To mitigate the issue of numerical instability, we propose a two-fold strategy to enhance the update quality of $U_k^\\prime$ and $V_k^\\prime$.\nFirst, the optimization objective for $U_k^\\prime$ is reformulated as"}, {"title": "a Least Squares Estimation (LSE) problem, where V", "content": "X is treated as the input and WX as the output:\n${U_k^\\prime}^T = \\arg \\min_{U_k^\\prime} {|| A(U_k^\\prime)^T \u2013 B||_F}^2$,\nwhere $A = X^T V_k^\\prime$ and $B = (WX)^T$. Since A is typically not a square matrix and may not be full rank, we first apply SVD to A to enhance numerical stability:\nA = $U_A \\Sigma_A V_A^T$,\nand then obtain the solution for $U_k^\\prime$ by using the Moore-Penrose pseudoinverse (Penrose, 1955) of A:\n${U_k^\\prime}^T = (A^{+}B) = (V_A \\Sigma_A^{+} U_A^T B)$,\nwhere $\\Sigma_A^{+}$ denotes the Moore-Penrose pseudoinverse of $\\Sigma_A$:\n$\\Sigma_A = diag(\\sigma_1, \\sigma_2,...,\\sigma_n)$,\n$\\Sigma_A^{+} = diag (\\sigma_1^{-1}1_{\\sigma_1 \\neq 0}, \\sigma_2^{-1}1_{\\sigma_2 \\neq 0},...,\\sigma_n^{-1}1_{\\sigma_n \\neq 0})$.\nSimilarly, we update $V_k^\\prime$ using the Moore-Penrose pseudoinverse of $U_k^\\prime$ to handle numerical instability:\n${V_k^\\prime}^T = \\arg \\min_{V_k^\\prime} {||U_k^\\prime (V_k^\\prime)^T X \u2013 WX||_F}$\n$= ((U_k^\\prime)^{+})^T W$.\nAs shown in Figure 3 (a), by reformulating the optimization objective as an LSE problem and solving for $U_k^\\prime$ and $V_k^\\prime$ using the Moore-Penrose pseudoinverse, we achieve a smooth curve that consistently reduces compression error stably.\nSecond, since the update rule incorporates the calibration data X, ideally, a large volume of X would yield better results. However, during our experiments, we found that extending X to just 32 samples on an 80GB GPU is challenging. To address this, we propose a stack-of-batch strategy that enables the utilization of more calibration data without increasing memory overhead. Specifically, given N calibration samples and a bucket size M (the maximum number of samples that can fit within the fixed GPU memory), we randomly sample mini_bsz = [] samples into one bucket by taking their mean value as follows:\n$X_{rand}$ = shuffle(X),\n$X^{\\prime}[k] = \\frac{1}{mini\\_bsz} \\sum_{i=1}^{mini\\_bsz} X_{rand}[(k-1) \\cdot mini\\_bsz + i]$,\nwhere k = 1, 2, ..., M, and cardinality |X'| = M.\nAs shown in Figure 3 (b), integrating stack-of-batch strategy further reduces the compression error.\nAs shown in Figure 2, to compensate for the error attributed to SVD truncation, we propose an adaptive method to subsequently update $U_k^\\prime$ and $V_k^\\prime$ with the above update rules. Moreover, the adaptation of $U_k^\\prime$ and $V_k^\\prime$ can be alternatively applied until convergence, where the update sequence over T iterations can be expressed as\n$(U_k^\\prime)^1 \\rightarrow (V_k^\\prime)^1 \\rightarrow (U_k^\\prime)^2 \\rightarrow (V_k^\\prime)^2 \\rightarrow \u2026 \\rightarrow (U_k^\\prime)^T \\rightarrow (V_k^\\prime)^T$,\nwhere $(U_k^\\prime)^T$ and $(V_k^\\prime)^T$ denote the updated singular matrices after T-th iteration, respectively. As shown in Figure 3 (c), the gap between the outputs of the compressed and original models narrows after alternative updates. The overlapping area rapidly increases after just a few iterations. More visual comparisons are shown in supplementary file.\nNotably, our adaptive compensation can be integrated with data whitening proposed by Wang et al. (2024b) and Liu et al. (2024), further reducing the SVD truncation error."}, {"title": "3.2. Adaptive SVD Compression Ratio", "content": "Previous studies on SVD compression typically apply a uniform compression ratio across all transformer layers of LLMs, overlooking the varying importance of different layers. Inspired by Men et al. (2024) and Dumitru et al. (2024), we propose adaCR, which adaptively determines the SVD compression ratio for each layer, considering each layer's impact on activations.\nThe importance of W can be measured by its impact on the input, which is quantified as the similarity between the input X and the output Y after passing through W.\nY = WX,\nI(W) = similarity(X, Y),\nwhere I(W) denotes the importance of W. The similarity metric used can vary, and for simplicity, we adopt cosine similarity in our method.\nThen, we normalize I(W) by mean centering to obtain the relative importance:\n$I_n(W) = \\frac{I(W)}{mean(I(W))}$.\nAfter mean normalization, the average importance is 1. A value of $I_n(W)$ greater than 1 indicates greater importance, while a value lower than 1 indicates lesser importance. The compression ratio of each layer will be adaptively adjusted based on the relative importance:\n$CR(W) = mrr + I_n(W) \\cdot (trr \u2013 mrr)$,\nwhere mrr and trr are the minimum and target retention ratios, respectively. Notably, CR(W) = mrr when $I_n(W) = 0$, and CR(W) = trr when CR(W) = 1.\nGiven the compression ratio for the i-th layer, we truncate the vectors of largest singular values from both $U_k^\\prime$ and $V_k^\\prime$ so that\n$CR(W) = \\frac{\\#params\\ of\\ U_k^\\prime + \\#params\\ of\\ (V_k^\\prime)^T}{\\#params\\ of\\ W_i}$.\nAs shown in Figure 4, the importance of different layers varies. It can be observed that the first layer always weighs the most importance, suggesting that we should retain more weight on it. For the Llama family, the relative importance curve approximates a bowl shape, highlighting the significance of both the initial and final layers."}, {"title": "4. Experiments", "content": "4.1. Setup\nWe compare the proposed AdaSVD against four baselines, including vanilla SVD and SOTA SVD-based LLM compression methods FWSVD (Hsu et al., 2022b), ASVD (Yuan et al., 2024) and SVD-LLM (Wang et al., 2024b).\nModels and Datasets. To demonstrate the generalizability of our method, we evaluate the performance of AdaSVD and the baselines on five models from three different LLM families, including LLaMA-7B (Touvron et al., 2023a), LLaMA2-7B (Touvron et al., 2023b), OPT-6.7B (Zhang et al., 2022), Mistral (Jiang et al., 2023), and Vicuna-7B (Chiang et al., 2023). We also use 10 datasets, including three language modeling datasets (WikiText-2 (Merity et al., 2016), PTB (Marcus et al., 1993), and C4 (Raffel et al., 2023)) and seven common-sense reasoning datasets (OpenbookQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2019), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2019), BoolQ (Clark et al., 2019), ARC-e, and ARC-c (Clark et al., 2018)). We use the LM-Evaluation-Harness framework (Gao et al., 2023) to evaluate the model performance on these zero-shot QA datasets.\nImplementation Details. To ensure a fair comparison, we followed ASVD (Yuan et al., 2024) and SVD-LLM (Wang et al., 2024b) to randomly select 256 samples from WikiText-2 as the calibration data and conduct data whitening before SVD truncation. All the experiments are conducted with PyTorch (Paszke et al., 2019b) and Huggingface (Paszke et al., 2019a) on a single NVIDIA A100-80GB GPU."}, {"title": "4.2. Main Results", "content": "We evaluate the overall performance of AdaSVD from two aspects: (1) performance under different compression ratios (40%, 50%, 60%, 70%, and 80%), (2) performance on different LLMs. Some generated contents by the compressed LLMs are provided in the supplementary file to provide a more straightforward comparison.\nPerformance under Different Compression Ratios. First, we evaluate the performance of LLaMA2-7B compressed by AdaSVD, vanilla SVD and the SOTA method SVD-LLM (Wang et al., 2024b) under compression ratios ranging from 40% to 80% on all 10 datasets, as shown in Table 1. On the three language modeling datasets, AdaSVD consistently outperforms vanilla SVD, and SVD-LLM across all the compression ratios. More importantly, AdaSVD exhibits significant advantages over the baselines under higher compression ratios. These results indicate that AdaSVD is more effective in compressing LLMs for more resource-constrained devices such as smartphones and IoT devices, which often have limited memory and processing capabilities. On the seven common sense reasoning datasets, AdaSVD also maintains its edge and performs better than the best-performing baseline on most of the datasets and consistently achieves higher average accuracy across all the compression ratios.\nPerformance on Different LLMs. To demonstrate the generability of AdaSVD across different LLMs, we compare AdaSVD and the baselines on four different models OPT-6.7B, LLAMA 2-7B, Vicuna-7B, and Mistral-7B \u2013 under 60% compression ratio on WikiText-2. As shown in Table 2,"}, {"title": "4.3. Ablation Study", "content": "We provide extensive ablation study results in Table 3 to show the effect of some key components in our work.\nEffectiveness of Adaptive Compensation. To validate the effectiveness of the proposed adaComp, we compare the PPL results of Llama2-7B with and without adaComp on Wikitest-2, PTB, and C4 datasets in Table 3a. Results of 70% and 80% compression ratios can be found in the supplementary file. It can be observed that AdaSVD consistently outperforms SVD-LLM after applying adaComp, and the performance gap is more significant under high compression ratios (i.e., 60%, 70%, and 80%).\nIteration Number. To investigate the impact of the number of adaComp iterations under different compression ratios, we perform an ablation study with 1, 3, and 15 iterations, as shown in Table 3c. Results for 70% and 80% compression ratios are provided in the supplementary file. At lower compression ratios (e.g., 40%, 50%, and 60%), it is observed that just 1 iteration of adaComp already outperforms the state-of-the-art method, SVD-LLM. However, increasing the number of iterations may lead to overfitting due to the limited calibration data, resulting in a performance drop. In contrast, at higher compression ratios (e.g., 70% and 80%), additional iterations lead to performance improvements, indicating that AdaSVD is more effective in high compression ratio scenarios where previous methods still struggle. This highlights the importance of balancing the number of iterations with the available data to avoid over-optimization, especially in low compression scales.\nEffectiveness of Adaptive Compression Ratio. To validate the effectiveness of our adaCR, we compared the results after removing adaCR (i.e., using constant compression ratios for all layers) from AdaSVD. As shown in Table 3b, AdaSVD already outperforms SOTA SVD-LLM without using adaCR, while integrating adaCR can further enhance the performance across all compression ratios.\nMinimum Retention Ratio. The minimum retention ratio (mrr) in adaCR is also crucial, and we investigate the impact of different mrr values in Table 3d for 40%, 50%, and 60% compression ratios. It can be observed that mrr remains relatively robust at lower compression ratios (40% and 50%), but requires more careful consideration at higher compression ratios (60%)."}, {"title": "4.4. Integrate with Weight Quantization", "content": "Similar to previous SVD-based compression methods (Hsu et al., 2022a; Yuan et al., 2024; Wang et al., 2024b), our AdaSVD is orthogonal to other types of compression techniques. Following Wang et al. (2024b), we integrate AdaSVD with the widely used weight quantization method GPTQ (Frantar et al., 2023). As shown in Table 4, we compare AdaSVD with SVD-LLM (Wang et al., 2024b) on the LLaMA2-7B model, using different compression ratios (40%, 50%, 60%, 70%, and 80%) across the WikiText-2, PTB, and C4 datasets. The results demonstrate that, when combined with the 4-bit weight quantization method GPTQ, AdaSVD also consistently outperforms SOTA baseline SVD-LLM across all compression ratios."}, {"title": "5. Conclusion", "content": "In this work, we propose AdaSVD, an adaptive SVD-based compression method for LLMs. AdaSVD first proposes adaComp, which adaptively compensates for the error caused by the truncation of singular matrices, efficiently reducing compression error without requiring additional training. Furthermore, AdaSVD proposes adaCR, which adaptively assigns compression ratios based on the importance of each layer, further enhancing performance while maintaining the same target compression rate. Both strategies effectively minimize SVD compression errors, particularly at high compression ratios. Our experiments on multiple open-source LLM families demonstrate that AdaSVD pushes the performance boundary beyond the current state-of-the-art SVD-based LLM compression methods."}]}