{"title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation", "authors": ["Yongxin Wang", "Meng Cao", "Haokun Lin", "Mingfei Han", "Liang Ma", "Jin Jiang", "Yuhao Cheng", "Xiaodan Liang"], "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress on various visual question answering and reasoning tasks leveraging instruction fine-tuning specific datasets. They can also learn from preference data annotated by human to enhance their reasoning ability and mitigate hallucinations. Most of preference data is generated from the model itself. However, existing methods require high-quality critical labels, which are costly and rely on human or proprietary models like GPT-4V. In this work, we propose Enhancing Alignment in MLLMs via Critical Observation (EACO), which aligns MLLMs by self-generated preference data using only 5k images economically. Our approach begins with collecting and refining a Scoring Evaluation Instruction-tuning dataset to train a critical evaluation model, termed the Critic. This Critic observes model responses across multiple dimensions, selecting preferred and non-preferred outputs for refined Direct Preference Optimization (DPO) tuning. To further enhance model performance, we employ an additional supervised fine-tuning stage after preference tuning. EACO reduces the overall hallucinations by 65.6% on HallusionBench and improves the reasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement over LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also shows the potential critical ability in open-source MLLMs, demonstrating that EACO is a viable path to boost the competence of MLLMs.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable success, largely driven by scaling up model size and enhancing data quality. In real-world applications, integrating inputs from other modalities, such as visual and auditory information, has propelled advancements in Multimodal Large Language Models (MLLMs). Existing MLLMs have demonstrated notable progress, especially within the research community, excelling in a range of downstream multimodal tasks such as visual question answering and image captioning. \nDespite these achievements, MLLMs still face critical challenges in reasoning and hallucination. For example, MLLMs sometimes generate descriptions that include counterfactual visual elements, reflecting inaccuracies in visual reasoning and comprehension in Figure 1. To mitigate reasoning errors and hallucinations, ongoing research has focused on several core strategies that enhance model performance across modalities. Creating high-quality annotated datasets for MLLMs is one of the most popular and effective approaches to improve model accuracy, reduce hallucinations, and enhance reasoning capabilities. However, such method is also highly resource-intensive, often presenting significant costs and logistical challenges.\nRecently, fine-tuning methods like Direct Preference Optimization [34] (DPO) refine model responses by aligning them more closely with human preferences. Utilizing preference data to guide the model in selecting preferred responses over less preferred alternatives. This approach discourages outputs that may be factually incorrect or lack logical coherence. While most existing methods rely on feedback from human annotators or proprietary models to generate preference data, these approaches are often costly and dependent on access to specialized resources. We seek to address these limitations by developing a scalable, critic-guided preference alignment strategy that uses self-generated preference data, thus reducing reliance on external feedback sources while maintaining high alignment quality.\nTo enhance reasoning abilities and reduce hallucinations in MLLMs, our work introduces a novel and economical critic-based method. Specifically, we collect and refine a critic dataset comprising 51,000 images and over 137,000 critic instructions. This dataset is designed to fine-tune the model, enabling it to assess and critique responses with greater accuracy. The optimization process starts with a seed model that undergoes training on the critic dataset.\nThis initial model is progressively fine-tuned to develop its critic abilities, allowing it to evaluate responses across multiple dimensions, such as Relevance, Basic Elements, and Clarity. After incorporating critic guidance into the response generation, the Critic then selects preferred and non-preferred outputs, which are used to guide refined Direct Preference Optimization [34] (DPO) tuning. The DPO process aligns the model more closely with high-quality responses, reducing tendencies toward hallucination and improving overall reasoning capabilities.\nThe primary contribution of this work is the development of a novel, critic-based training framework that enhances reasoning abilities and reduces hallucinations in MLLMs in an economical manner. Key contributions include:\n\u2022 We propose EACO, a novel critic-based approach that guides MLLMs toward generating more accurate, contextually relevant, and hallucination-free responses.\n\u2022 We refine a Large-scale Critic Dataset containing 51,000 images and over 137,000 detailed critique instructions. This dataset is designed to train the model's \"critic\" abilities, enabling it to assess responses across multiple dimensions.\n\u2022 EACO achieves an 8.5% average improvement on multiple benchmarks over baseline, which indicates effectiveness of the critic-based framework, especially on reasoning task and hallucination mitigation."}, {"title": "2. Related Work", "content": "2.1. Multimodal Large Language Models (MLLMs)\nMLLMs have dominated a wide range of multimodal tasks, achieving remarkable progress in vision-language understanding, reasoning and generation. Most of them rely on pre-trained unimodal models, where MLLMs use learnable projectors to connect visual encoders with language models. These projectors are typically categorized as either query-based, like Q-Former in MiniGPT-4 and Instruct-BLIP , which use cross-attention to capture visual signals, or projection layer-based, as seen in LLaVA and ShareGPT4V , where linear projection layers or Multi-Layer Perceptrons (MLPs) map visual features into the input space of language models. In contrast, Fuyu-8B and Gemini involve end-to-end training without pre-trained components. For example, Fuyu-8B processes raw image patches directly and transforms them into embeddings via linear projection, bypassing the use of pre-trained visual encoders and instead learning the vision-language relationship from scratch.\n2.2. Preference Alignment\nPreference alignment is a standard technique used in LLMs to strengthen the model's capabilities to improve the instruction-following ability. The goal of Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) is to align language models with user intent across a broad spectrum of tasks by fine-tuning them using human feedback."}, {"title": "2.3. Hallucination Mitigation", "content": "Hallucination often arises from the model's attempt to rely on its general knowledge base when it cannot confidently interpret the visual input , leading it to fill gaps with plausible but incorrect information. This can be particularly problematic in applications that require high accuracy, such as medical imaging or autonomous driving. To enhance the perception ability of MLLMs, several works scale up the resolution of the vision encoder, while others have adopted versatile vision encoders, for example, proposing mixing features from CLIP and DINO . There is one more line to mitigate hallucination by introducing the Reinforcement Learning from AI/Human Feedback. LLaVA-RLHF involves the human feedback to mitigate hallucination by maximizing the human reward. ViGOR designs a fine-grained reward model, which encompasses both human preferences and automatic metrics, to update the pre-trained model for hallucination mitigation."}, {"title": "3. Critic Model", "content": "Previous methods often rely on proprietary models, such as GPT-4V, or other large-scale models to build preference datasets, which are crucial for enhancing current Multilingual Language Models (MLLMs). In this section, however, we propose a training pipeline for critic models based on existing MLLMs, using open-source datasets. This approach aims to reduce dependency on proprietary systems while maintaining robust performance.\n3.1. Data Collection\nSilkie collected over 80,000 multimodal instructions from various datasets , annotated by GPT-4V . Each instruction includes four responses generated by different models, each evaluated across three dimensions: helpfulness, visual faithfulness, and ethical considerations. For our analysis, we randomly sampled 51,000 of these instructions and refined them for scoring evaluation. To enhance differentiation, we filtered out response pairs with similar scores for the same instructions and images, retaining only those pairs with a larger score gap to improve the robustness of our evaluation.\nAs shown in Figure 2, we finally select more than 137k instructions. The refined instruction template is below:\nId, Prompt, Response\nScore:[Score_{H}, Score_{VF},Score_{EC}]\nTotal Score\nwhere Score_{H}, Score_{VF}, and Score_{EC} denote the rating score of response for the image-question pair from perspectives of helpfulness, visual faithfulness, and ethical consid-"}, {"title": "4. Method: EACO", "content": "In this section, we introduce EACO, a framework composed of three key steps: response generation, critic-guided scoring, and preference learning. In Section 4.1, we first describe the self-generation process, where multiple responses are generated for subsequent selection. Next, Section 4.2 details the critic-guided scoring step, which utilizes a proprietary critic model to evaluate these responses. Finally, in Section 4.3, we explain the model updating process using preference data derived from the scoring step. To further enhance model capabilities, we incorporate an additional supervised fine-tuning (SFT) stage after preference tuning, similar to the approach in [10].\n4.1. Response Generation\nMost existing methods rely on costly or external models to generate responses for constructing preference datasets, which imposes a substantial computational and financial burden. In contrast, our approach leverages the current Multilingual Language Model (MLLM) to self-generate multi-"}, {"title": "Algorithm 1 EACO: Preference Tuning", "content": "INPUT: Unlabeled image dataset: {v^{(i)}\\}\\_{i\\in[N]} Image captioning prompt set: P = {x^{(i)}\\}\\_{i\\in[M]}; MLLM parameterized by \\theta_{0}: p_{\\theta_{0}}; Critic MLLM parameterized by \\theta_{\\theta^{\\'}}: p_{\\theta^{\\'}}; Critic Prompt: x_{critic}.\nfor i = 1, ... N do\nfor j = 1,...n do\nRandomly sample x \\sim {x^{(i)}\\}\\_{i\\in[M]};\nGenerate response y_{j} \\sim p_{\\theta_{0}} (\\cdot | v^{(i)}, x);\nGenerate score S_{iy_{j}} \\sim p_{\\theta^{\\'}} (\\cdot | v^{(i)}, x_{critic});\nend for\nSelect the preferred and non-preferred response y_{\\omega} \\sim arg \\underset{j\\in[n]}{max} S_{iy_{j}}, y_{l} \\sim arg \\underset{j\\in[n]}{min} S_{iy_{j}};\nAdd (x, v^{(i)}, y_{\\omega}, y_{l}) to dataset D;\nend for\nUpdate \\theta_{1} = argmin_{\\theta\\in\\Theta}\\sum_{(x,v,y_{\\omega},y_{l})\\in D} [log (\\sigma (\\beta log \\frac{p_{\\theta} (y_{\\omega}|x,v)}{p_{Pref} (y_{\\omega}|x,v)} - \\beta log \\frac{p_{\\theta} (y_{l}|x,v)}{p_{Pref} (y_{l}|x,v)}\n- \\alpha log p_{\\theta} (y_{\\omega} | x, v) - (\\alpha |y_{\\omega}| - \\alpha |y_{l}|))];\nOUTPUT: \\theta_{1}.\nple responses by pairing each image with its corresponding question. As shown in Figure 4, we obtain Perferred/Non-Preferred response pairs.\nThe generation process begins with a pre-trained MLLM, denoted as \\theta_{0}, as the initial checkpoint, along with an unlabeled image dataset v and an image captioning prompt set x. For each image-question pair (v, x), the current MLLM \\theta_{0} generates a set of n responses, {y_{1}, y_{2},..., y_{n}}."}, {"title": "4.2. Critic-guided Scoring", "content": "After self-generating n responses, we utilize a fine-tuned critic model to evaluate each response using a set of evaluation prompts, producing a score for each response. We then select the responses with the highest and lowest critic scores as the preferred and non-preferred responses, denoted as y_{\\omega} and y_{l}, respectively. These selections are used in the following DPO [34] training to further align the current MLLM. The selection process is outlined as follows:\ny_{\\omega} = arg \\underset{j\\in[n]}{max} S_{iy_{j}},\ny_{l} = arg \\underset{j\\in[n]}{min} S_{iy_{j}},\n(1)\nwhere S_{iy_{j}} represents the score assigned by the critic model to the j-th response in the i-th set."}, {"title": "4.3. Preference Tuning", "content": "Following the critic-guided scoring step to obtain preference and non-preference pairs, the current MLLM utilizes"}, {"title": "5. Experiments", "content": "Building on our previous work, this section explores preference alignment tuning using Direct Preference Optimization (DPO) with the assistance of a fine-tuned critic model. We begin by outlining the experimental setups in Section 5.1, including baseline methods and training details. Next, in Section 5.2, we describe the benchmarks used and present the main results along with a detailed analysis. Finally, Section 5.3 provides an ablation study to assess the impact of different components in our approach.\n5.1. Experimental Setup\nImplementation Details. In our experiments, we use LLaVA-v1.6-Mistral-7B [25], Bunny-8B [15], and MiniCPM-V2.6 8B [45] as backbone models. For critic model training, we employ low-rank adaptation (LoRA) fine-tuning on the 137k refined dataset described in Section 3.1. Detailed training hyperparameters are provided in Suppl. Section 9. To create the self-generated preference dataset, we randomly select 5k unlabeled images from the MSCOCO dataset , following the optimization process outlined in Section 4 for preference tuning. After preference learning, we apply an enhanced supervised fine-tuning (SFT) stage, inspired by [10]. For this SFT stage, we randomly subsample 5k instruction-tuning data from the models' original SFT datasets, constructing fine-tuning data with model-generated image captions. Here, we also adopt LORA fine-tuning for efficient training. Importantly, we ensure there is no overlap between the images in the preference data, enhanced SFT data, and the critic data by checking and filtering out any duplicates. The total tuning cost"}, {"title": "5.2. Quantitative Results and Analysis", "content": "The quantitative results presented in Table 1 reveals significant insights into the comparative performance of various visual language models across multiple benchmarks. Our proposed EACO consistently demonstrates an improvement in performance compared to other methods, suggesting that incorporating a Critic mechanism along with self-generated responses yields robust advancements across different evaluation metrics.\nFor the LLaVA-v1.6-Mistral-7B model, incorporating EACO leads to substantial gains across multiple benchmarks. In particular, EACO achieves remarkable results on the comprehensive benchmarks MMEP and MMEC, scoring 1532.8 and 376.4, respectively. These scores surpass all other methods, showcasing enhanced perception and cognitive capabilities within the model. Notably, our method also improves performance on both SEED and MathVista, with scores of 72.3 and 32.6, respectively. This improvement reflects an enhanced ability to understand and reason across both general and domain-specific visual question answering (VQA) tasks. In the hallucination benchmarks, our approach attains state-of-the-art scores on AMBER and HallusionBench, with scores of 80.6 and 48.2, respectively. These results indicate a significant reduction in model hallucinations, reinforcing EACO's effectiveness in controlling false or unsupported content generation.\nFor the Bunny-8B model, incorporating our approach yields improvements across nearly all metrics. Notably, scores on the AMBER and HallusionBench benchmarks reach 82.9 and 49.7, respectively, indicating a significant re-"}, {"title": "5.3. Ablation Study", "content": "To further analyze the impact of our approach, we conducted three ablation studies based on the LLaVA-v1.6-Mistral-7B model: (1). Scaling Up Data Quantity in preference tuning, (2). Iterative Alignment Performance, (3). Impact of Different Critic Prompts, and (4). Imapct of Critic Model.\nTo simplify the performance comparison process, we set the maximum score for each benchmark to 100 and take the average score of 7 benchmarks as the model's overall average performance in this section.\nScaling up dateset. We explore the performance when scaling up the preference dataset. The results in Figure 5 indicate that scaling up the preference dataset leads to perfor-"}, {"title": "6. Conclusion", "content": "Our proposed EACO framework offers a scalable and efficient method for guiding MLLMs towards responses that are more accurate, contextually relevant, and largely free of hallucinations. EACO demonstrates generalizability across different model architectures and benchmarks, effectively enhancing both reasoning abilities and reliability in various multimodal applications. This work underscores the potential of critic-based preference alignment as a pathway for optimizing multimodal models to better meet real-world, human-centric needs.\nDespite its strengths, EACO's critic capabilities are largely confined to straightforward tasks such as captioning and basic visual question answering (VQA) tasks . For more complex tasks that require chain-of-thought (CoT) reasoning , the critic model does not yet achieve performance on par with state-of-the-art language models like those in the GPT series . Nevertheless, we believe that EACO has substantial potential for advancing multimodal understanding and critique capabilities."}]}