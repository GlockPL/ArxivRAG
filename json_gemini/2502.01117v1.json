{"title": "Learning to Learn Weight Generation via Trajectory Diffusion", "authors": ["Yunchuan Guan", "Yu Liu", "Ke Zhou", "Zhiqi Shen", "Serge Belongie", "Jenq-Neng Hwang", "Lei Li"], "abstract": "Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Di's higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning. Our code is released at https://github.com/tuantuange/Lt-Di.", "sections": [{"title": "1. Introduction", "content": "Diffusion-based generative models have emerged as a breakthrough AI technology, achieving state-of-the-art performance in scenarios like audio, image, and video generation (Gozalo-Brizuela & Garrido-Merch\u00e1n, 2023). Recent advancements in diffusion models for high-dimensional data generation have introduced a novel application domain: the generation of neural network weights. This technology avoids the overhead associated with gradient-based training or fine-tuning, offering promising solutions for few-shot, multi-task, and multi-domain problems that require frequent weight updates.\nPrevious researchers leverage models such as Variational Autoencoder (VAE) (Kingma & Welling, 2014) and Hypernetwork (Ha et al., 2017) to learn the latent distribution of optimal weights for target tasks. OCD (Lutati & Wolf, 2023) and Meta-Diff (Zhang et al., 2024) attempt to use the diffusion model to simulate the weight optimization process. However, these approaches are constrained by single-level optimization frameworks and demonstrate limited capability for knowledge transfer between tasks, which subsequently impacts their generalization capacity for new tasks.\nMore importantly, current weight generation methods only utilize the optimal weights as training samples, overlooking the value of other weights along the optimization trajectory\u00b9. This limited utilization of available training resources results in suboptimal performance in terms of both accuracy and efficiency.\nIn this paper, (1) we propose to Learn to Learn Weight Generation via Trajectory Diffusion, i.e., Lt-Di. This method utilizes diffusion models within the framework of bi-level optimization. Benefiting from the generalization capability offered by bi-level optimization (i.e., learn to learn), Lt-Di"}, {"title": "2. Methodology", "content": "In this section, we introduce Lt-Di's workflow and provide a preliminary in Appendix C. The workflow of Lt-Di is shown in Figure 2, which consists of three stages: weight preparation, meta-training, and evaluation."}, {"title": "2.1. Weight Preparation", "content": "During the weight preparation stage, we collect weights and construct an optimization trajectory \\( Tra_i \\) corresponding to each task \\( T_i \\). In alignment with research demonstrating the significant influence of final layers in fine-tuning (Raghu et al., 2020; 2017), and to optimize computational efficiency, we focus our attention specifically on the downstream network head \\( \\theta^h \\) while maintaining the body \\( \\theta^b \\) freezing. Specifically, we use \\( T_i \\) in the training set to up-"}, {"title": "2.2. Meta-Training.", "content": "In the meta-learning stage, we use REPTILE (Nichol et al., 2018) as the framework to ensure efficiency. Our meta-objective is defined specifically as learning a diffusion model \\( \\epsilon \\) (i.e., denoiser) that can recover the Gaussian-initialized weights to the optimal weight \\( \\theta^{h*} \\) for the unseen task \\( T_{n+1} \\). We employ the same U-Net denoiser architecture given by Rombach et al. (2022), and maintain this setup across all experiments in this paper. To learn on multiple tasks and enhance meta-learning, the denoiser needs additional condition states represented by the task embedding \\( Embr_i = \\theta(T_i) \\)\u00b3. Considering empirical optimal weight \\( \\theta^h \\) as the diffusion target \\( x_0 \\), we follow the setup of vanilla diffusion algorithm Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020). One of an inner-loop loss \\( L_0 \\) can be written as:\n\\[ L_0 = \\sum_{t=1}^{T} ||\\epsilon_\\phi(Embr_i, t, x_t) - \\epsilon_0||^2, \\]\nand the inner-loop update equation can be written as:\n\\[ \\phi_i = \\phi_i - \\eta\\nabla L, \\]\nwhere \\( t \\) is the timestamp, \\( \\eta \\) is the inner-loop learning rate, \\( x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_0 \\), and \\( \\bar{\\alpha}_t = \\prod_{j=1}^{t}\\alpha_j \\). Note that \\( \\mathcal{L} \\) is the total loss of trajectory diffusion and the derivation of \\( \\mathcal{L} \\) is detailed in Section 3.1. According to REPTILE, the meta-update process can be written as:\n\\[ \\phi = \\phi + \\zeta \\sum_{i=1}^{n}(\\phi_i - \\phi), \\]"}, {"title": "2.3. Downstream Task Evaluation.", "content": "Based on the vanilla diffusion algorithm, the inference process of Lt-Di can be written as:\n\\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\phi). \\]\nThrough multiple inference processes, the well-trained denoiser \\( \\phi^* \\) can recover the optimal head \\( \\theta^{h*} \\) from Gaussian-initialized weights."}, {"title": "3. Trajectory Diffusion and Analysis", "content": "In this section, we introduce the trajectory diffusion and analyze its configuration, principles, and convergence."}, {"title": "3.1. Trajectory Diffusion", "content": "Although we have established a general framework to meta-train the diffusion model, directly using it leads to suboptimal performance. We denote the above method that combines meta-learning and vanilla diffusion as Lv-Di, i.e., Learning to Learn Weight Generation via Vanilla Diffusion. The right side of Figure 1 visualizes Lv-Di's inference chains in a 2D PAC-reduced weight landscape. In Omniglot's (Lake et al., 2011) 5-way 1-shot classification tasks, Lv-Di constrains only the inference endpoint, ignoring behavior along the inference chain. As a result, weights generated by Lv-Di will deviate from the ground truth (i.e., optimal weights). In Lt-Di, we use the whole optimization trajectory \\( Tra = {\\theta_m, ..., \\theta_h} \\) rather than a single optimal weight \\( \\theta_h \\) to guide the inference chain. According to the protocol given by DDPM, \\( x_T \\) equals \\( \\theta_0 \\), and \\( x_T \\) is Gaussian noise that equals initial weights \\( \\theta_m \\). Thus, we can denote"}, {"title": "3.2. Analysis", "content": "In this section, we will discuss the configuration and principle issues raised in Section 2.1 and Section 3.1, and the convergence of the weight generation paradigm:\n* How to configure trajectory diffusion?\n* How does trajectory diffusion enable higher accuracy with efficient training and inference processes?\n* The convergence properties of the weight generation paradigm and how to improve them."}, {"title": "3.2.1. TRAJECTORY DIFFUSION CONFIGURATION", "content": "First, we need to choose an index mapping function \\( r(\\cdot) \\) that maps the trajectory weights in \\( Tra_i = {\\theta_m, ..., \\theta_h} \\) to appropriate positions within the inference chain \\( Cinfer = {x_T, ..., x_0} \\). In this paper, we map uniformly. Let the length of the optimization trajectory be \\( m + 1 \\), the length of the inference chain be \\( T + 1 \\), and assume that \\( T + 1 \\) be divisible by \\( m + 1 \\), a trivial index mapping function \\( r(.) \\) can be written as:\n\\[ r(i) = i * \\frac{T}{m}. \\]\nSecond, we need to determine the length of the optimization trajectory. This issue entails a trade-off between loss function complexity and accuracy gain from more trajectory weights. Under the constraint of three GPU hours"}, {"title": "3.2.2. TRAINING AND INFERENCE ACCELERATION", "content": "We can loosely reason by treating \\( \\mathcal{L} \\) as a linear combination of multiple optimization objectives. When \\( m = 3 \\), Figure 4 records three optimization objectives \\( L_0, L_1, \\) and \\( L_2 \\), corresponding to three trajectory weights, \\( \\theta_0, \\theta_1 \\), and"}, {"title": "3.2.3. CONVERGENCE ANALYSIS AND IMPROVEMENT", "content": "Learning to generate weights is an indirect approach compared to learning directly from downstream task samples, and its convergence can be challenging to guarantee. By assuming an upper bound on the generative model's reconstruction error, the following analysis applies to all weight generation algorithms."}, {"title": "4. Experiment", "content": "Our experimental platform includes two A100 GPUs, one Intel Xeon Gold 6348 processor, and 512 TB of DDR4 memory. For all experiment results, we report the mean and standard deviation over 5 independent repeated experiments. In the following section, we present the basic experimental results and setups, with more details provided in D."}, {"title": "4.1. Ablation Study", "content": "The advantages of Lt-Di stem from three main components:\n* C1: Learning to generate optimal weights indirectly.\n* C2: Using diffusion models to generate high-quality weights.\n* C3: Using trajectory diffusion to guide the diffusion model.\nAs shown in Table 1, we validate the effectiveness of Lt-Di by ablating these components. When none of the components are used, Lt-Di degrades to the original REPTILE. When only C1 is used, we employ the vanilla VAE (Kingma & Welling, 2014) to generate weights, while we refer to this method as LLO-VAE. When using C1 and C2, Lt-Di degrades to Lv-Di. Results in Table 1 indicate that LLO-VAE demonstrates lower performance compared to REPTILE on the Omniglot dataset, suggesting the necessity for an enhanced generative algorithm. The comparison between REPTILE and Lv-Di shows that such an issue can be mitigated by using the diffusion model, which we attribute to its multi-step generation process that effectively captures the latent distribution of model weights. After adding the optimization trajectory, Lt-Di achieved the highest accuracy by modeling the entire optimization trajectory rather than just one optimal weight."}, {"title": "4.1.2. FUNCTIONAL COMPONENTS", "content": "Following the setup given by Finn et al. (2017), we conducted a case study to evaluate the accuracy improvement and overhead burden brought by each functional component. We incrementally added SAM and data augmentation components to Lt-Di's data preparation stage. In Omniglot and Mini-Imagenet datasets, we construct 5-way 1-shot tasks and record the model's accuracy on the meta-test set at each period of training. The results demonstrate that adding functional components can improve the model's performance without additional time overhead at any stage of training. As mentioned in 2.1, during meta-learning, the functional information is already included in the trajectory weights, allowing the meta-learning algorithm to focus solely on learning these weights. Therefore, the three models in the figure exhibit identical convergence rates and progressively increasing accuracy."}, {"title": "4.2. Comparison Experiments", "content": "Task. In this task, we train and evaluate models on disjoint pre-training and evaluation datasets. During evaluation, we do not use any labeled data to adjust the models. We evaluated the zero-shot transfer learning capability of the models using both accuracy and average per-sample evaluation la-"}, {"title": "4.2.1. ZERO-SHOT TRANSFER LEARNING", "content": "Task. In this task, we explore the multi-domain generalizability of Lt-Di. We follow the few-shot task given by Guan et al. (2023) to evaluate the model's performance."}, {"title": "4.2.2. FEW-SHOT LEARNING", "content": "Task. Following the setup provided by Finn et al. (2017), we train and evaluate models on disjoint meta-training and meta-testing tasks. During the evaluation stage, we use the support set in meta-test tasks to fine-tune the models, and then compare the accuracy and average per-sample evaluation latency on the query set."}, {"title": "4.2.3. MULTI-DOMAIN GENERALIZATION", "content": "Task. In this task, we explore the multi-domain generalizability of Lt-Di. We follow the few-shot task given by Guan et al. (2023) to evaluate the model's performance."}, {"title": "4.2.4. LARGE LANGUAGE MODEL FINE-TUNING", "content": "Task. In this section, we demonstrate that Lt-Di can be applied to the fine-tuning of LLM by learning to generate LORA (Hu et al., 2022) matrices for new tasks with low latency. We compared the algorithms in terms of their fine-tuning accuracy upon convergence and the latency required to achieve it."}, {"title": "5. Conclusion and Limitation", "content": "In this paper, we propose Lt-Di, which integrates the fast inference capability of weight generation and the cross-task transferability of bi-level optimization. Building on this, we further propose Trajectory Diffusion, enabling the model to capture the entire optimization trajectory, which enhances weight generation accuracy and efficiency. Finally, we theoretically and empirically demonstrate that the convergence of this indirect learning paradigm can be improved solely by constraining the eigenvalues of the Hessian matrix of the downstream tasks loss function, improving convergence efficiency without additional time overhead.\nOur method is not well-suited for single-task learning scenarios, as such settings do not require frequent weight updates. In these cases, the benefits of Lt-Di in fine-tuning and inference do not sufficiently outweigh the computational overhead of training a diffusion model. For relatively simple tasks, such as the Omniglot 5-way 5-shot task in Section 4.2.2, our method does not provide further accuracy improvements. Moreover, further validation is needed on tasks beyond computer vision and natural language processing."}, {"title": "Impact Statements", "content": "Theoretically, we extend the diffusion algorithm to Trajectory Diffusion and analyze the convergence of the weight generation paradigm. Practically, we validate our method across multiple multi-task scenarios."}, {"title": "A. Theorem and Proof", "content": "Readers can refer to the derivation process of DDPM (Luo, 2022) to understand the following derivation.\nTheorem 1. Given decay sequence {ao, ..., \u03b1\u03c4}, and trajectory weight xk. Let the inference equation align with the vanilla diffusion algorithm, i.e.,\n\\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\phi). \\]\nThen the diffusion model e& can recover the image xk from standard Gaussian noise x\u012b in T - k steps, when e is trained by\n\\[ L_k = \\sum_{t=k+1}^{T} ||\\sqrt{1-\\bar{\\alpha}}(x_t, t - k) - \\sqrt{1 - \\bar{\\alpha}}\\epsilon_k||^2, \\]\nwhere xt = \\sqrt{\\bar{\\alpha}}x_k + \\sqrt{1 - \\bar{\\alpha}}\\epsilon_k, \\bar{\\alpha}_t = \\prod_{j=1}^{t}\\alpha_j, \\alpha = \\prod_{j=k+1}^{t}\\alpha_j, and ek denotes standard Gaussian noise.\nProof. Consider the diffusion chain Cdiff = {xk, ..., XT} and let sequence {ak+1, ..., ar} be the corresponding decay schedule. According to vanilla diffusion model, to maximize the likelihood p(xk) of observed data xk, we need to minimize denoising matching term\n\\[ \\sum_{t=k+1}^{T} E_{q(x_t/x_k)} [D_{KL} (q(x_{t-1}|x_t, x_k) || p_\\phi(x_{t-1}|x_t))]. \\]\nIn the KL divergence bracket, the left term can be expended by the Bayesian Theorem. The right term is the inference process to be modeled with 4, whose expectation is given by Equation 1. According to Bayes Theorem,\n\\[ q(x_{t-1} | x_t, x_k) = \\frac{q(x_t | x_{t-1},x_k) q(x_{t-1} | x_k)}{q(x_t | x_k)} \\]\nAccording to the Markov Rule and standard diffusion process,\n\\[ q(x_t|x_{t-1},x_k) = q(x_t | x_{t-1}) \\]\n\\[ \\sim N(x_t; \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1 - \\alpha_t}I). \\]\nRecursively using the diffusion process on {xk, ..., Xt\u22121}, we have\n\\[ q(x_t/x_k) \\sim N(x_t; \\sqrt{\\bar{\\alpha}}x_k, (1-\\bar{\\alpha}) I), \\]\nwhere \\bar{a} = \\prod_{j=k+1}^{t} \\alpha_j. Note that the coefficients here differ from those in the vanilla diffusion algorithm. According to Equation 5, q(xt\u22121|xk) can be written as\n\\[ q(x_{t-1}|x_k) \\sim N(x_{t-1}; \\sqrt{\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}}}x_k, (1-\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}}) I). \\]"}, {"title": "B. Theorem and Proof", "content": "Assumption 1.\n1. Reconstruction error upper bound. The reconstruction error produced by the generative model is bounded by c.\n2. Loss function upper bound. Downstream task loss La(\u00b7) \u2264 \u03c8.\n3. l-smoothness. There exists a constant l such that for all weights 0, \u03b8' \u2208 Rn\n\\[ ||\\nabla L_a(0) - \\nabla L_a(0')|| \\leq l||0 - 0' || \\]\n4. \u00b5-strong convex. There exists a constant \u03bc > 0 such that for all 0, \u03b8' \u2208 R,\n\\[ L_a(0') \\geq L_a(0) + \\nabla L_a(0)^T (0' - 0) + \\frac{\\mu}{2}||0' - 0||^2. \\]\n5. Hessian matrix upper bound. The Hessian matrix eigenvalue around the neighborhood of optimal weight 0* is bounded by A.\nLemma 1. Assume that the loss function La() is l-smooth and satisfies \u00b5-strongly convex. Then, the sequence {\\theta_epoch} generated by the gradient descent update with step size satisfies\n\\[ ||\\theta_{epoch} - \\theta^* ||^2 \\leq \\frac{2}{ \\mu} [L_a(0) - L_a(0^*)] (1-\\frac{\\mu}{l})^{epoch} \\]\nProof. Since La(0) is l-smooth, for any 0 and \u03b8',\n\\[ L_a(0') \\leq L_a(0) + \\nabla L_a(0)^T (0' - 0) + \\frac{l}{2}||0' - 0||^2. \\]\nApplying this to the gradient descent update \\( \\theta^{k+1} = \\theta^k - \\frac{1}{l}\\nabla L_a(\\theta^k) \\), we have\n\\[ L_a(0^{k+1}) \\leq L_a(0^*) + \\nabla L_a(0^k)^T(0^{k+1} - 0^k) + \\frac{l}{2}||0^{k+1} - 0^k||^2 \\]\n\\[ = L_a(0^*) - \\frac{1}{2l}||\\nabla L_a(0^k)||^2. \\]\nSince La(0) is \u00b5-strongly convex, it satisfies the Polyak-Lojasiewicz condition:\n\\[ \\frac{1}{2} ||\\nabla L_a(0)||^2 \\geq \\mu [L_a(0) - L_a(0^*)]. \\]\nSubstituting this inequality into Equation 10, we have\n\\[ L_a(0^{k+1}) \\leq L_a(0^k) - \\frac{\\mu}{l} (L_a(0^*) - L_a(0^*)). \\]\nThe above equation can be reorganized to"}, {"title": "C. Preliminary", "content": "Potentially ambiguous symbols are described in Table 6 to enhance the reader's comprehension of this paper."}, {"title": "C.2. Diffusion Model", "content": "The core idea of the diffusion model is to model data through a two-stage process:\n* Diffusion Process: Starting with data xo, noise is added at each step to generate xr, eventually approaching a standard normal distribution.\n* Inference Process: Starting with noise x\u012b, a denoising model & generates XT-1, XT-2,...,xo step by step.\nBy precisely modeling the reverse process, the diffusion model & can generate new samples that match the original data distribution. Diffusion models define a decay schedule {a} to control the noise level at each step. In the diffusion process, noise is added at each step t, transforming the data xt-1 into xt\n\\[ q(x_t|x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I). \\]\nThe direct transition from xo to xt can be written as\n\\[ q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I), \\]\nwhere \\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i is the cumulative noise schedule, controlling the overall noise level from x0 to xt. In the inference process, the denoiser iteratively reconstructs the data by\n\\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\phi + \\sigma z), \\]\nWe omit additional \u03c3\u03c4\u03b5 for stable weight generation. The key point of training a variational model is to maximize the Evidence Lower Bound(ELBO). In the diffusion algorithm, optimizing the ELBO is essentially equivalent to minimizing the denoising match term\n\\[ \\sum_{t=k+2}^{T} E_{q(x_t|x_k)} [D_{KL} (q(x_{t-1}|x_t, x_k) || p_\\phi(x_{t-1}|x_t))], \\]\nwhich is also the objective optimized by our trajectory diffusion."}, {"title": "C.3. REPTILE", "content": "REPTILE is a first-order optimization-based meta-learning algorithm that simplifies training while retaining strong adaptability across tasks. It eliminates the need for second-order gradients, making it computationally efficient compared to algorithms like MAML. The training process consists of two loops: the inner-loop and the outer-loop.\nIn the inner loop, REPTILE performs gradient descent on a sampled task T\u2081 using the task's support set. Starting from the meta-parameters 0, the task-specific parameters \u03b8\u2081 are updated for K steps using\n\\[ \\theta_i^{(t+1)} = \\theta_i^{(t)} - \\eta \\nabla_{\\theta_i^{(t)}} \\mathcal{L}_{T_i} (\\theta_i^{(t)}), \\]\nwhere n is the inner-loop learning rate and LT, is the loss for task Ti.\nIn the outer-loop, the meta-parameters @ are updated by moving them toward the task-specific parameters \u03b8\u2081 obtained from the inner loop. This meta-update is given by\n\\[ \\theta \\leftarrow \\theta + \\zeta(\\theta_i - \\theta), \\]\nwhere is the outer-loop learning rate.\nBy iteratively repeating the inner and outer loops across multiple tasks drawn from the task distribution p(T), REPTILE optimizes the meta-parameters @ to find an initialization that enables fast adaptation to new tasks with minimal gradient steps. Its simplicity lies in avoiding second-order derivatives, while its effectiveness is demonstrated across diverse applications such as few-shot learning and domain generalization."}, {"title": "D. Experimental Detail", "content": "D.1. Dataset"}, {"title": "D.2. Model Configuration", "content": "Lt-Di involves hyperparameter configurations in two components: downstream network 0 and diffusion model 4. For the downstream network 0, we adopt different architectures for different tasks. In tasks on the Omniglot and Mini-ImageNet datasets, we follow the standard setup used by most meta-learning methods, employing 4 convolution blocks with batch normalization and ReLU as the body b and 2 fully connected layers as the head \u03b8h. For zero-shot tasks on CIFAR-10, CIFAR-100, STL-10, Aircraft, and Pets, we use the commonly adopted ResNet-12 as the body and still employ a 2-layer fully connected head. In multi-domain generalization tasks, we maintain the same setup. In the LLM multi-task fine-tuning scenario, the downstream network is the LLM model itself, i.e., RoBERTa-base with 125 million weights. For the diffusion model 4, we use the commonly adopted U-Net architecture. The number of inference steps in the diffusion model is set to 20."}, {"title": "D.4 Adaptability to Different Architectures", "content": "To verify the effectiveness of Lt-Di under network structures of different scales, we compare it with its degraded versions, i.e., REPTILE, LLO-VAE, and Lv-Di. Table 7 and Table 8 show the effectiveness of our method on Swin Transformer, ResNet18, and MobileNetV2 architectures. The results demonstrate that each component of our method remains effective across neural network architectures of different scales."}, {"title": "E. Related Work", "content": "Meta-Learning. Meta-learning often employs a bi-level optimization-based paradigm for better generalization performance on few-shot and reinforcement learning tasks. However, it incurs significant costs for weight fine-tuning, especially in multi-task scenarios. Methods like ANIL and Reptile improve training efficiency by minimizing updates to only essential task-specific layers or by approximating meta-gradients, respectively. However, these methods still rely on gradient computation and fail to achieve superior accuracy."}, {"title": "Network Weights Generation", "content": "Hypernetwork is the first method that uses one network to generate another's weights, leading to extensions like the HyperSeg for downstream task flexibility. Conditional diffusion models provide another approach, with OCD leveraging overfitting and Meta-Diff enhancing few-shot adaptability. Hyper-representations embed model characteristics to support weight generation for unseen tasks, while Image-free Classifier Injection achieves zero-shot classification via semantic-driven weights. These methods are constrained by their single-level optimization approach, which presents limitations in both cross-task knowledge transfer capabilities and potential adaptability to novel tasks. Most importantly, these methods overlook the role of other weights, limiting the model's efficiency and accuracy."}]}