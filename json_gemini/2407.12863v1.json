{"title": "Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models", "authors": ["Jung Hyun Lee", "June Yong Yang", "Byeongho Heo", "Dongyoon Han", "Kang Min Yoo"], "abstract": "Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities in mathematics through step-by-step reasoning chains. However, they are susceptible to reasoning errors that impact the quality of subsequent reasoning chains and the final answer due to language models' autoregressive token-by-token generating nature. Recent works have proposed adopting external verifiers to guide the generation of reasoning paths, but existing works utilize models that have been trained with step-by-step labels to assess the correctness of token-by-token reasoning chains. Consequently, they struggle to recognize discriminative details of tokens within a reasoning path and lack the ability to evaluate whether an intermediate reasoning path is on a promising track toward the correct final answer. To amend the lack of sound and token-grained math-verification signals, we devise a novel training scheme for verifiers that apply token-level supervision with the expected cumulative reward (i.e., value). Furthermore, we propose a practical formulation of the cumulative reward by reducing it to finding the probability of future correctness of the final answer and thereby enabling the empirical estimation of the value. Experimental results on mathematical reasoning benchmarks show that Token-Supervised Value Model (TVM) can outperform step-by-step verifiers on GSM8K and MATH with Mistral and Llama.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) pre-trained on massive data have achieved human-level performance across a wide range of tasks in natural language processing (Maslej et al., 2024). A notable exception to this trend is complex multi-step reasoning tasks such as mathematical problem solving, where current state-of-the-art LLMs still struggle to attain near-human performance. Previous studies have been focused on enhancing the reasoning capabilities of LLMs through: encouraging LLMs to generate step-by-step thought processes via few-shot or zero-shot prompting (Wei et al., 2022; Kojima et al., 2022); fine-tuning LLMs with question-solution pairs to generate intermediate reasoning steps before producing a final answer (Cobbe et al., 2021; Luo et al., 2023; Yu et al., 2023; Yuan et al., 2023); and employing aggregation techniques such as majority voting over final answers extracted from so-lutions generated by LLMs (Wang et al., 2023). However, when LLMs are left to their own devices to solve given problems, they remain error-prone due to their autoregressive nature in generating reasoning paths. If an LLM, by chance, produces a single error during generation, the reasoning path can be easily steered towards a wrong answer. This would worsen for LLMs when they face more complex reasoning tasks such as advanced-level mathematical problems in the MATH dataset (Hendrycks et al., 2021). To address this, researchers have focused on providing external aid to the LLM by training verifiers to assess the correctness of generated reasoning paths.\nExisting verifiers can be categorized into two types: outcome-supervised reward models (ORMS) and process-supervised reward models (PRMs). ORMS (Cobbe et al., 2021; Uesato et al., 2022; Yu et al., 2024) are trained to assess the correctness of a reasoning path by labeling each token as either correct or incorrect solely based on whether the final answer in the reasoning path is correct. PRMs (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2024) are trained with step-level labels to assess the correctness of each reasoning step, and they are generally preferred over ORMs due to the finer resolution of assessment in practice. Despite being proposed to assist LLMs, current verifiers may retain a fundamental misalignment with their per-token granularity. Since ORMs and PRMs employ uniform labels according to the correctness of either a whole reasoning path or step, respectively (Fig. 1), we argue that they were not designed to (i) learn the discriminative details of tokens within a reasoning path or (ii) evaluate whether an intermediate reasoning path is on a promising track toward the correct final answer.\nIn this paper, we propose the Token-supervised Value Model (TVM), a novel verifier that supervises each token in a reasoning path with a distinctive label, training each token with the expected cumulative reward. Unlike ORMs and PRMs, our token-level supervision with distinct per-token value labels along a reasoning path (Fig. 1) equips TVMs with the ability to capture the discriminative details of tokens within a reasoning path (see Fig. 2). Furthermore, providing a theoretical insight that the value of each token is equivalent to the probability of reaching the correct final answer from that token, we propose to label each token via empirical value estimation along sampled reasoning paths.TVM is trained to predict the probability of a per-token intermediate reasoning path being on a promising track toward the correct final answer. Therefore TVM could choose among candidate reasoning paths most likely to reach the correct final answer, whether they are partial or complete. Our contributions are threefold:\n\u2022 We propose the Token-supervised Value Model (TVM), a new verifier capable of capturing token-wise details via direct supervision with the expected cumulative reward (i.e., value) for each token along a reasoning path.\n\u2022 We generate per-token labels for verifier supervision via empirical value estimation, which allows TVM to predict the probability of an intermediate reasoning path reaching the correct final answer.\n\u2022 We show that TVM achieves performance improvements on GSM8K and MATH benchmarks across LLMs under 10B parameters, compared to ORMs and PRMs."}, {"title": "2 Background", "content": "This section reviews existing verifier frameworks for enhancing the mathematical reasoning capabilities of LLMs. Sec. 2.1 outlines the preliminary setups for training verifiers in mathematical reasoning verification. The subsequent sections revisit two existing types of supervision for verifier training: outcome supervision (Sec. 2.2) and process supervision (Sec. 2.3)."}, {"title": "2.1 Training Verifiers for Mathematical Reasoning", "content": "The mathematical reasoning capabilities of LLMs can be enhanced by employing reward models as external verifiers to assess the generated reasoning paths (Cobbe et al., 2021; Uesato et al., 2022; Lightman et al., 2023; Yu et al., 2024; Wang et al., 2024). The verifier is generally trained via supervised learning on a dataset obtained by sampling multiple reasoning paths per training problem using an LLM. Specifically, given a training problem qtr as an input, the LLM first generates Ntr reasoning paths, where n-th reasoning path is comprised of reasoning steps {$s_{n,j}$}$_{j=1}^{S_n}$ and a final answer $a_n$ for n = 1,..., Ntr. In token-level notation, the n-th reasoning path can also be expressed as a sequence of tokens {$t_{n,k}$}$_{k=1}^{T_n}$. Hereafter, {$s_{n,.}$} and {$t_{n,.}$} means {$s_{n,1},...,s_{n,j}$} and {$t_{n,1},...,t_{n,k}$}, respectively. The final answer $a_n$ is correct if it is equal to the ground truth answer $\\hat{a}$, and incorrect otherwise. Based on the correctness of the sampled reasoning paths, supervision is traditionally given in two ways: (i) outcome supervision (Cobbe et al., 2021; Uesato et al., 2022; Yu et al., 2024) and (ii) process supervision (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2024)."}, {"title": "2.2 Outcome Supervision", "content": "Prior works (Cobbe et al., 2021; Uesato et al., 2022; Yu et al., 2024) employ outcome supervision to label an entire reasoning path as correct if its final answer is correct (Fig. 1). The outcome reward function $r_o(\\cdot)$ is the correctness of the final answer:\n$r_o(a_n) = \\begin{cases}\n1 & \\text{if } a_n = \\hat{a} \\\\\n0 & \\text{if } a_n \\neq \\hat{a}\n\\end{cases}$\nfor n = 1,..., Ntr. An outcome-supervised reward model (ORM) $f_{ORM}$ is trained with every token in a reasoning path labeled as the outcome reward (Eq. 1). The ORM loss $L_{ORM}$ is defined as\n$L_{ORM} = \\sum_{n,k}^{N_{tr},T_n} l (r_o(a_n), f_{ORM} (q_{tr}, \\{t_{n,.} \\}^k_1)).$\nThe mean squared error is typically used as a loss function $l(\\cdot)$ in Eq. 2. Cobbe et al. (2021) demonstrated that a token-level verifier trained to judge the correctness after every token performs better than a solution-level verifier trained to determine the correctness only after the final token.\nInterestingly, Yu et al. (2024) showed that ORMs can be alternatively described as modeling the cumulative reward for each token, where all intermediate rewards are zero (i.e., $r(t_{n,k}) = 0$ for every n and k) and the discount factor $\\gamma$ is set to 1. The cumulative reward following an intermediate token $t_{n,k}$, $R(t_{n,k})$ is calculated as\n$R(t_{n,k}) = r(t_{n,k+1}) + \\cdots + r(t_{n,T_n}) + r_o(a_n)$\n$= \\begin{cases}\n0 + \\cdots + 0 + 1 = 1 & \\text{if } a_n = \\hat{a} \\\\\n0 + \\cdots + 0 + 0 = 0 & \\text{if } a_n \\neq \\hat{a},\n\\end{cases}$\nwhich is equivalent to $r_o(a_n)$ in Eq. 1. This entails that an intermediate reasoning path is labeled as correct if the final answer is correct, and vice versa. In this sense, ORMs can indirectly and implicitly learn the potential correctness of an intermediate reasoning path (Yu et al., 2024)."}, {"title": "2.3 Process Supervision", "content": "Process supervision enables a more accurate assessment of a reasoning path by explicitly training a verifier on the correctness of each step with step-level supervision (Lightman et al., 2023). The correctness of each reasoning step is either labeled via human annotation (Uesato et al., 2022; Lightman et al., 2023) or automation (Wang et al., 2024). Since acquiring human annotations is labor-intensive and costly, we mainly focus on process supervision without human annotations.\nFollowing Wang et al. (2024), an intermediate reasoning step $s_{n,j}$ can be labeled as correct if at least one of the reasoning paths starting from $s_{n,j}$ reaches the correct final answer $\\hat{a}$ (Fig. 1). In practice, $s_{n,j}$ is annotated by sampling a fixed number of reasoning paths conditioned on a sequence of intermediate reasoning steps {$s_{n,.} = {s_{n,1},...,s_{n,j}}$}. If at least one of the sampled reasoning paths reaches the correct final answer, $s_{n,j}$ is labeled as correct with the process reward $r_p(s_{n,j}) = 1$. Otherwise, $s_{n,j}$ is labeled as incorrect and $r_p(s_{n,j}) = 0$. Using the per-step labels obtained through automation, a Process-supervised Reward Model (PRM) is trained to provide a step-level assessment by minimizing the following loss:\n$L_{PRM} = \\sum_{n,j}^{N_{tr}, S_n} l (r_p(s_{n,j}), f_{PRM} (q_{tr}, \\{s_{n,.} \\}^j_1)),$\nwhere $l$ denotes the binary cross entropy loss."}, {"title": "3 Method", "content": "In this section, we introduce our proposed method coined Token-supervised Model (TVM), a novel verifier trained with a token-level supervision strategy to directly estimate the expected cumulative reward (i.e., value) for each token along a reasoning path. We also describe how to empirically estimate per-token value labels from Ntr generated reasoning paths for token-level supervision."}, {"title": "3.1 Motivation", "content": "As mentioned in Sec. 2, both outcome supervision (ORMS) and process supervision (PRMs) utilize homogeneous labels determined by the correctness of either the entire reasoning path or step (Fig. 1). Consequently, we hypothesize that they are likely to be neither explicitly nor directly trained to (i) learn the discriminative details of tokens within a reasoning path or (ii) evaluate whether an intermediate reasoning path is on a promising track toward the correct final answer."}, {"title": "3.2 Token-level Value Supervision", "content": "To overcome the issues above, we propose a new verifier based on token-level supervision with distinctive token-wise labels according to the potential of tokens in deducing the correct final answer. A natural choice to appropriately reflect the token-wise potential is prospective value modeling (Sutton and Barto, 2018), which is fine-grained and future-oriented compared to retrospective cumulative reward modeling (Eq. 3). Accordingly, we construct a supervision scheme for token tn,k in a reasoning path {qn,{tn,.}} = {tn,1,..., tn,k} with the expected cumulative reward (i.e., value):\n$V(t_{n,k}) = \\mathbb{E}[\\sum_{l=1}^{\\infty} \\gamma^{l-1} r(t_{n,k+l})|q_{tr}, \\{t_{n,.} \\}^k_1],$\nwhere $r(\\cdot)$ and $\\gamma$ denote a reward function and the discount factor, respectively.\nThe primary challenge in training value models as verifiers is estimating the value labels of a generated reasoning path (Yu et al., 2024). However, under the specific outcome reward formulation of Eq. 1 and no intermediate rewards, the expected cumulative reward (Eq. 5) reduces to the probability of reaching the correct final answer conditioned on the question qtr and intermediate reasoning path {tn,.}, which can be straightforwardly computed from generated reasoning paths and can indicate whether an intermediate reasoning path (i.e., {tn,.}) is on a promising track toward the correct final answer.\nProposition 3.1. Let the reward function r(tn,k) be defined as Eq. 1, which includes only the outcome reward with the discount factor $\\gamma = 1$ and no intermediate reward (i.e., r(tn,k) = 0 except the final answer). Then, the expected cumulative reward (Eq. 5) is equivalent to the probability of reaching the correct final answer conditioned on qtr and {tn,.} = {tn,1,\u2026, tn,k}:\n$\\mathbb{E}[\\sum_{l=1}^{\\infty} \\gamma^{l-1} r(t_{n,k+l})|q_{tr}, \\{t_{n,.} \\}^k_1]$\n$= P(\\text{the final answer will be } \\hat{a} | q_{tr}, \\{t_{n,.} \\}^k_1).$\nThe right-hand side of Eq. 6 can be empirically estimated from generated reasoning paths by calculating the proportion of correct reasoning paths starting from {tn,.} among total reasoning paths starting from {tn,.} (see Sec. 3.3).\nFollowing Proposition 3.1, we train the Token-supervised Value Model (TVM) by supervising each token with a value label empirically estimated as the probability of reaching the correct final answer given until that token. The objective of TVM is\n$L_{TVM} = \\sum_{n,k} l (p_{n,k}, f_{TVM} (q_{tr}, \\{t_{n,.} \\}^k)),$\nfor n = 1,..., Ntr and k = 1,..., Tn, where pn,k indicates the right-hand side of Eq. 6 and the loss function l is the mean squared error.\nCompared to existing verifiers, the resolution of assessment provided by the proposed token-level value supervision adequately matches the token-wise granularity of LLMs, thereby being able to capture the discriminative details of tokens within a reasoning path (Fig. 2). In contrast to ORMS, TVM is trained to directly estimate the probability of an intermediate reasoning path being on a promising track toward the correct final answer (Proposition 3.1). As a result, TVM can choose the reasoning path most likely to reach the correct final answer among candidate reasoning paths, whether they are partial or complete.\nDuring inference, TVM can be employed to either search the reasoning path most likely to be correct over complete reasoning paths generated from an LLM (Lightman et al., 2023) or distinguish prospective candidates likely to reach the correct final answer among partially generated reasoning paths. For the latter, we conduct a detailed study in Sec. 4.3 in the setting of verifier-guided step-wise beam search (Yu et al., 2024)."}, {"title": "3.3 Empirical Value Estimation", "content": "As discussed in Sec. 3.2, Proposition 3.1 alleviates the practical challenges of value estimation (Eq. 5) by formulating the value as the ratio of correct reasoning paths to total reasoning paths. Following Eq. 5 and Eq. 6, the estimated value for each token tn,k can be represented as\n$V(t_{n,k}) = \\mathbb{E}[\\sum_{l=1}^{\\infty} \\gamma^{l-1} r(t_{n,k+l})|q_{tr}, \\{t_{n,.} \\}^k]$\n$= P(\\text{the final answer will be } \\hat{a} | q_{tr}, \\{t_{n,.} \\}^k)$\n$= \\frac{P(\\{t_{n,.} \\}^k \\cap \\text{the final answer will be } \\hat{a}|q_{tr})}{P(\\{t_{n,.} \\}^k|q_{tr})}$\nIn practice, Eq. 8 can be empirically estimated from Ntr generated reasoning paths as the ratio of correct reasoning paths starting from {tn,.} among Ntr and total reasoning paths starting from {tn,.} among Ntr, respectively. The value label of each token V(tn,k) is assigned as\n$V(t_{n,k}) = \\frac{\\sum_{n'=1}^{N_{tr}} \\mathbb{I}(\\{t_{n',.} \\}^k = \\{t_{n,.} \\}^k \\cap a_{n'} = \\hat{a}) / N_{tr}}{\\sum_{n'=1}^{N_{tr}} \\mathbb{I}(\\{t_{n',.} \\}^k = \\{t_{n,.} \\}^k) / N_{tr}},$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function and Ntr cancels out. The overall procedure of empirical value estimation is described in Figure 3. The overall algorithm is deferred to Appendix C."}, {"title": "4 Experiments", "content": "To demonstrate the efficacy of TVM in improving the mathematical reasoning capabilities of LLMs, we conduct extensive experiments on the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks. Our experiments are based on the following large language models: 1) Mistral-7B (Jiang et al., 2023), Llama3-8B (AI@Meta, 2024); 2) those fine-tuned on MetaMATH (Yu et al., 2023) We use two existing verifier utilization strategies: (i) best-of-N search and (ii) step-by-step beam search.\nBest-of-N search. The best-of-N search strategy introduced in Lightman et al. (2023) is a conventional experimental setting to evaluate the performance of a verifier. For every test problem, an LLM first generates N complete reasoning paths. The reasoning path ranked highest by the verifier is chosen as the final candidate. For all experiments, we set N = 256 following Wang et al. (2024) unless specified otherwise.\nVerifier-guided step-level beam search (BS). To prevent errors in an intermediate reasoning step from propagating to subsequent steps, Yu et al. (2024) proposed guided decoding during intermediate reasoning steps via a verifier, a search strategy we call verifier-guided step-level beam search. For a test problem, after an LLM partially generates K reasoning paths each containing only the first intermediate reasoning step, the verifier-guided step-level beam search strategy alternates between the following two steps until all K partially generated reasoning paths are complete: (i) a verifier selects the top-b (<K) ranked partially generated reasoning paths, and (ii) the LLM generates K/b subsequent intermediate reasoning steps for each path chosen by the verifier. Among the K complete reasoning paths, the one scored highest by the verifier is selected. Thanks to verifier intervention in generating each intermediate reasoning step, with K much smaller than N, the performance of verifier-guided step-level beam search can be similar to that of best-of-N search in Table 1 and 2."}, {"title": "4.1 Grade School Mathematics (GSM8K)", "content": "Setups. An LLM is fine-tuned on the training dataset of GSM8K for two epochs with a batch size of 128 and a learning rate of 1e-5. Then, we sample Ntr = 100 reasoning paths per training problem with a temperature of 0.7 from the fine-tuned LLM and label each token in a reasoning path as Eq. 9. Finally, TVM initialized from either the same LLM or the fine-tuned LLM is trained on this dataset for one epoch with a batch size of 512 and a learning rate of either 2e-6 or 1e-5. More experimental details are deferred to Appendix E.\nResults. In the case of best-of-N search, we compare TVM with ORM (Cobbe et al., 2021) and Math-Shepherd (Wang et al., 2024), a PRM without human annotations, as explained in Sec. 2. As all experimental results in Wang et al. (2024) are only based on LLMs fine-tuned on MetaMATH, we also evaluate Math-Shepherd only for Mistral-7B-MetaMath and Llama3-8B-MetaMath. Despite using large N, Table 1 shows that TVM surpasses ORM and Math-Shepherd with improvements ranging from 0.6 to 2.6%p as well as self-consistency from 4.9 to 8.9%p, across the board.\nUnder the verifier-guided step-level beam search strategy, we primarily compare TVM against OVM (Yu et al., 2024) because Yu et al. (2024) confirmed that step-level beam search guided by a token-level verifier performs significantly better than that guided by a sentence-level value model (Feng et al., 2024) on GSM8K. Further comparison to Feng et al. (2024) is presented in Appendix B. In Table 1, TVM also consistently outperforms OVM ranging from 0.6 to 1.0%p.\nOne might wonder why the accuracy of OVM (K = 40) for Mistral-7B is much higher than that of OVM (K = 100) reported in Yu et al. (2024). This discrepancy arises because, in our experiments, some tokens (e.g., <<, >>) are correctly converted to token IDs by the Mistral-7B tokenizer."}, {"title": "4.2 Advanced Mathematics (MATH)", "content": "Setups. We employ fine-tuned LLMs on Meta-Math (Mistral-7B-MetaMath and Llama3-8B-MetaMath) without any further fine-tuning on the training dataset of MATH in order to sample reasoning paths in a newline-delimited format. Following Lightman et al. (2023); Wang et al. (2024), we also use 500 test MATH problems for evaluation, which is the same test dataset of Lightman et al. (2023), incorporating the remaining 4500 test problems into the training dataset of MATH. For each training problem, a fine-tuned LLM on MetaMath generates Ntr = 25 reasoning paths with a temperature of 0.7, with each token labeled as Eq. 9. Then, we train TVM initialized from the same fine-tuned LLM for one epoch on this dataset with a batch size of 512 and a learning rate of 2e-6. Further experimental details are given in Appendix E.\nResults. Similar to Sec. 4.1, Table 2 compares (i) TVM's best-of-N search performance with ORM and Math-Shepherd and (ii) TVM-guided step-level beam search to ORM-guided step-level beam search (i.e., OVM). In the former case, the performance of TVM is slightly superior or almost comparable to that of ORM and Math-Shepherd. This might be due to the fact that an LLM is extremely prone to producing errors in the process of generating N reasoning paths for difficult MATH problems. However, when capitalizing on the verifier-guided step-level beam search strategy, not only does TVM outperform the OVM ranging from 2.6 to 2.8%p, but TVM-guided step-level beam search also exhibits much better performance than best-of-N search by any verifier even if K = 40 is much smaller than N = 256."}, {"title": "4.3 Analyses on Verifier-guided Step-level BS", "content": "Case study. To validate the superiority of TVM over OVM in predicting whether an intermediate reasoning path is on a promising track toward the correct answer, for a test problem in the GSM8K benchmark, we compare OVM's and TVM's predictions. As illustrated in Fig. 4, in the third reasoning step, OVM incorrectly predicts a wrong intermediate reasoning path with the highest score while assigning a low score to a correct path. This occurs because OVM is inherently identical to ORM trained to implicitly and indirectly learn the potential correctness of an intermediate reasoning path. In contrast, TVM accurately predicts a correct intermediate path with the highest score and a wrong one with a low score. As TVM is trained to directly and explicitly estimate the probability of reaching the correct final answer for each token along a reasoning path, TVM can effectively predict at inference whether an intermediate reasoning path is on a promising track toward the correct answer.\nBeam size study. To investigate whether the accuracy of TVM improves with larger values of K and b in verifier-guided step-level beam search, we conduct experiments using TVM with varying sizes of K and b for Mistral-7B and Mistral-7B-MetaMath on the GSM8K benchmark. Table 3 shows that the accuracy of TVM on GSM8K increases as both K and b grow, but reaches a saturation point when K = 100 and b = 25."}, {"title": "5 Related Work", "content": "Best-of-N search. For N complete reasoning paths, a verifier Cobbe et al. (2021); Uesato et al. (2022); Lightman et al. (2023); Wang et al. (2024) ranks and picks the highest-scored reasoning path. Although best-of-N search using a verifier shows much superior performance compared to verifier-free strategies such as self-consistency (Wang et al., 2023), best-of-N search still possesses the same drawback as self-consistency as a large quantity of generated reasoning paths are required to solve challenging reasoning problems.\nStep-level beam search. In contrast to the selection among complete reasoning paths, several studies have focused on step-level beam searches for partial reasoning paths. Step-level beam search can be divided into (i) verifier-free step-level beam search and (ii) verifier-guided step-level beam search.\nUnder the verifier-free step-level beam search strategy, Yao et al. (2023); Hao et al. (2023) allow value estimation by prompting LLMs to sample or simulate long-term outcomes during inference. Alternatively, Feng et al. (2024); Yu et al. (2024) introduce step-level beam search guided by a sentence-level value model and an outcome-supervised reward model, respectively. Although Feng et al. (2024); Yu et al. (2024) show that verifier-guided step-level beam search achieves significant accuracy improvements over verifier-free one, each approach has its own weakness. As delineated in Yu et al. (2024), a sentence-level value model is unsuitable for step-level beam search. In addition, Yu et al. (2024) uses an outcome-supervised reward model, not a value model. As a result, there is still room for improvement in the performance of verifier-guided step-level beam search."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel verifier termed the Token-supervised Value Model (TVM). This model uses per-token value labels to guide LLMs toward promising mathematical reasoning paths. Unlike traditional verifiers, which lack token-level labels and thus cannot precisely evaluate intermediate steps in reasoning paths, TVM could estimate the expected cumulative reward for each token. This enables TVM to identify detailed token-level information and perform more precise reasoning at intermediate paths leading to the correct answer. Experimental results on benchmarks such as GSM8k and MATH have revealed that TVM outperforms previous verifiers across 7B-scale LLMs, including Mistral-7B and Llama3-8B, demonstrating its enhanced accuracy and effectiveness."}, {"title": "Limitations", "content": "Our method has demonstrated significant improvements over previous competing methods, but resource constraints limited us from running further experiments. Our TVM was primarily evaluated using 7B-scale models for mathematical reasoning, but it can be applied to larger models and extended to other domains. Additionally, our model could be utilized as a value model in reinforcement learning, such as in Proximal Policy Optimization training (Schulman et al., 2017; Zheng et al., 2023), to supervise LLMs."}]}