{"title": "Joint Optimal Transport and Embedding for Network Alignment", "authors": ["Qi Yu", "Zhichen Zeng", "Yuchen Yan", "Lei Ying", "R. Srikant", "Hanghang Tong"], "abstract": "Network alignment, which aims to find node correspondence across different networks, is the cornerstone of various downstream multi-network and Web mining tasks. Most of the embedding-based methods indirectly model cross-network node relationships by contrasting positive and negative node pairs sampled from hand-crafted strategies, which are vulnerable to graph noises and lead to potential misalignment of nodes. Another line of work based on the optimal transport (OT) theory directly models cross-network node relationships and generates noise-reduced alignments. However, OT methods heavily rely on fixed, pre-defined cost functions that prohibit end-to-end training and are hard to generalize. In this paper, we aim to unify the embedding and OT-based methods in a mutually beneficial manner and propose a joint optimal transport and embedding framework for network alignment named JOENA. For one thing (OT for embedding), through a simple yet effective transformation, the noise-reduced OT mapping serves as an adaptive sampling strategy directly modeling all cross-network node pairs for robust embedding learning. For another (embedding for OT), on top of the learned embeddings, the OT cost can be gradually trained in an end-to-end fashion, which further enhances the alignment quality. With a unified objective, the mutual benefits of both methods can be achieved by an alternating optimization schema with guaranteed convergence. Extensive experiments on real-world networks validate the effectiveness and scalability of JOENA, achieving up to 16% improvement in MRR and 20\u00d7 speedup compared with the state-of-the-art alignment methods.", "sections": [{"title": "1 Introduction", "content": "In the era of big data and AI [2, 3, 37], multi-sourced networks\u00b9 appear in a wealth of high-impact applications, ranging from social network analysis [5, 60, 61, 70], recommender system [14, 15, 30] to knowledge graphs [26, 51, 52, 56]. Network alignment, the process of identifying node associations across different networks, is the key steppingstone behind many downstream multi-network and Web mining tasks. For example, by linking users across different social network platforms, we can integrate user actions from multi-sourced sites to achieve more informed and personalized recommendation [9, 63, 70]. Aligning suspects from different transaction networks helps identify financial fraud [12, 64, 81, 82]. Entity alignment between incomplete knowledge graphs, such as Wikipedia and WorkNet, helps construct a unified knowledge base [7, 56, 66].\nMany existing methods approach the network alignment problem by learning low-dimensional node embeddings in a unified space across two networks. Essentially, these methods first adopt different sampling strategies to sample positive and negative node pairs, and then utilize a ranking loss, where positive node pairs (e.g., anchor nodes) are pulled together, while negative node pairs (e.g., sampled dissimilar nodes) are pushed far apart in the embedding space, to model cross-network node relationships [9, 28, 67, 81]. For example, as shown in Figure 1, the relationship between anchor node pair (a1, a2) is directly modeled by minimizing their distance $d(a_1, a_2)$ in the embedding space, while the relationship between (b1, b2) is depicted via an indirect modeling path $d(b_1, a_1)+ d(a_1, a_2) + d(a_2, b_2)$ [67].\nPromising as it might be, the indirect modeling adopted by embedding-based methods inevitably bear an approximation error between the path $d(b_1, a\u2081) + d(a\u2081, a\u2082) + d(a\u2082, b\u2082)$ and the exact cross-network node relationship $d(b_1, b\u2082)$, resulting in performance degradation. Besides, embedding-based methods largely depend on the quality of node pairs sampled by hand-crafted sampling strategies such as random walk-based [59], degree-based [9, 28]\n\u00b9In this paper, we use the terms 'network' and 'graph' interchangeably."}, {"title": "2 Preliminaries", "content": "Table 1 summarizes the main symbols used throughout the paper. We use bold uppercase letters for matrices (e.g., A), bold lowercase letters for vectors (e.g., s), and lowercase letters for scalars (e.g., a). The transpose of A is denoted by the superscript \u0442 (e.g., AT). An attributed network with n nodes is represented by G = (A, X) where A \u2208 Rn\u00d7n,X \u2208 Rn\u00d7d denote the adjacency matrix and node attribute matrix, respectively. We use V and & to denote the node and edge set of a graph, respectively. The semi-supervised attributed network alignment problem can be defined as follows:\nDEFINITION 1. Semi-supervised Attributed Network Alignment. Given: (1) two networks G1 = (A1, X1) and G2 = (A2, X2); (2) an anchor node set L = {(x, y)|x \u2208 G1, y \u2208 G2} indicating pre-aligned nodes pairs (x, y).\nOutput: alignment/mapping matrix S \u2208 Rn1\u00d7n2, where S(x, y) indicates how likely node x \u2208 G\u2081 and node y \u2208 G2 are aligned."}, {"title": "2.1 Embedding-based Network Alignment", "content": "Embedding-based methods learn node embeddings by pulling positive node pairs together while pushing negative node pairs apart in the embedding space via ranking loss functions [9, 28, 67, 81]. Specifically, given a set of anchor node pairs L, the ranking loss can be generally formulated as [67]:\n$T_{rank} = T_1 + T_2 + T_{cross}$\n$T_1 = \\sum_{x\\in L} \\sum_{x_p\\in G_1} (d(x, x_p) - d(x, x_n))$\nwhere $T_2 = \\sum_{y\\in L} \\sum_{y_p\\in G_2} (d(y, y_p) - d(y, y_n))$,\n$T_{cross} = \\sum_{(x, y) \\in L} d(x, y)$\n(1)\nwhere d(x, y) measures the distance between two node embeddings (e.g., L\u2081 norm), xp/yp denotes the positive node w.r.t. x/y, and xn/yn denotes the negative node w.r.t. x/y. In the above equation, T1, T2 are intra-network loss pulling sampled positive nodes (e.g., similar/nearby nodes) together, while pushing sampled negative nodes (e.g., disimilar/distant nodes) far part. Tcross is the cross-network loss, which aims to minimize the distance between anchor node pairs. In general, the objective in Eq. (1) indirectly models the node relationship between two non-anchor nodes (x', y') via a path through the anchor node pair (x, y), i.e., ((x', x), (x, y), (y, y'))."}, {"title": "2.2 Optimal Transport", "content": "OT has achieved great success in graph applications, such as network alignment [45, 57, 58, 77] and graph classification [11, 36, 76, 78]. Following a common practice [46], a graph can be represented as a probability measure supported on the product space of node attribute and structure, i.e., $\u03bc = \\sum_{i=1}^{n} h(i)\u03b4_{A(x_i),X(x_i)}$, where h\u2208 An is a histogram representing the node weight and \u03b4 is the Dirac function. The fused Gromov-Wasserstein (FGW) distance is the sum of node pairwise distances based on node attributes and graph structure defined as [43]:\nDEFINITION 2. Fused Gromov-Wasserstein (FGW) distance. Given: (1) two graphs G1 = (A1, X1), G2 = (A2, X2); (2) probability measures \u03bc1, \u03bc2 on graphs; (3) intra-network cost matrix C1, C2; (4) cross-network cost matrix M.\nOutput: the FGW distance between two graphs FGWq,a (G1, G2)\n$\\min_{\\sum \\in \\Pi(\\mu_1, \\mu_2)} (1-\u03b1) \\sum_{x \\in G_1, y\\in G_2} M^2(x, y)S(x, y) + \u03b1\\sum_{x,x' \\in G_1} |C_1(x, x') - C_2(y, y')|^2S(x, y)S(x', y').$\ny.y' EG2\n(2)\nThe first term corresponds to the Wasserstein distance measuring cross-network node distances, and the second term is the Gromov-Wasserstein (GW) distance measuring cross-network edge distances. The hyperparameter \u03b1 controls the trade-off between two terms, and q is the order of the FGW distance, which is adopted as q = 2 throughout the paper. The FGW problem aims to find an OT mapping S \u2208 \u03a0(\u03bc1, \u03bc2) that minimizes the sum of Wasserstein and GW distances, and the resulting OT mapping matrix S further serves as the soft node alignment."}, {"title": "3 Methodology", "content": "In this section, we present the proposed JOENA. We first analyze the mutual benefits between embedding and OT-based methods in Section 3.1. Guided by such analysis, a unified framework named JOENA is proposed for network alignment in Section 3.2. We further present the unified model training schema in Section 3.3, followed by convergence and complexity analysis of JOENA in Section 3.4."}, {"title": "3.1 Mutual Benefits of Embedding and OT", "content": "3.1.1 OT-Empowered Embedding Learning. The success of ranking loss largely depends on sampled positive and negative node pairs, i.e., (x, xp), (x, xn), (y, yp), (y, Yn) in Eq. (1), through which cross-network node pair relationships can be modeled. To provide a better sampling strategy, the OT mapping improves the embedding learning from two aspects: direct modeling and robustness. First (direct modeling), while embedding-based methods model cross-network node relationships via an indirect path (see Figure 1 for an example.) sampled by hand-crafted strategies, the OT mapping directly models such cross-network relationships, identifying positive and negative node pairs more precisely. Second (robustness), in contrast to the noisy embedding alignment, thanks to the marginal constraints in Eq. (2), the resulting OT mapping is noise-reduced [43, 77], where each node only aligns with very few nodes. Therefore, OT-based sampling strategy can be robust to graph noises.\n3.1.2 Embedding-Empowered OT Learning. The success of OT-based alignment methods largely depend on the cost design, i.e. C1, C2, and M in Eq. (2), which is often hand-crafted in existing works. To achieve better cost design, embedding learning benefits OT learning from two aspects: generalization and effectiveness. For one thing (generalization), building transport cost upon learnable embeddings opens the door for end-to-end training paradigm, thus, the OT framework can be generalized to different graphs without extensive parameter tuning. For another (effectiveness), neural networks generate more powerful node embeddings via deep transformations, enhancing the cost design for OT optimization."}, {"title": "3.2 Model Overview", "content": "The overall framework of JOENA is given in Figure 2, which can be divided into three parts: (1) RWR encoding for structure learning, (2) embedding learning via multi-level ranking loss with OT-based sampling, (3) OT optimization with learnable transport cost.\nPositional information plays a pivotal role in network alignment [67, 77], but most of the GNN architectures fall short in capturing such information for alignment [72]. Therefore, we explicitly encode positional information by conducting random walk with restart (RWR) [47]. By regarding a pair of anchor nodes (x, y) \u2208 L as identical in the RWR embedding space, we simultaneously perform RWR w.r.t. x \u2208 G\u2081 and y \u2208 G2 to construct a unified embedding space, where the RWR score vectors rx \u2208 Rn1 and ry \u2208 RN2 can be obtained by [47, 67]\n$r_x = (1 - \u03b2)W_1r_x + \u03b2e_x, r_y = (1 - \u03b2)W_2r_y + \u03b2e_y,$\n(3)\nwhere \u03b2 is the restart probability, W\u2081 = $(D^{-1}A_i)^T$ is the transpose of the row-normalized adjacency matrix, Di is the diagonal degree matrix of Gi, and ex, ey are one-hot encoding vectors with ex(x) = 1 and ey (y) = 1. The concatenation of RWR vectors w.r.t. different"}, {"title": "3.3 Unified Model Training", "content": "In this subsection, we present the model training framework under a unified objective function. Through a simple yet effective transformation, the FGW distance and multi-level ranking loss are combined into a single objective (Subsection 3.3.1), which can be efficiently optimized using an alternating optimization scheme with guaranteed convergence (Subsection 3.3.2).\n3.3.1 Unifying FGW Distance and Multi-level Ranking Loss. The FGW distance is a powerful objective for network alignment, and has been adopted by several works [6, 8, 45, 58] to supervise embedding learning. In general, based on the Envelop theorem [1], existing methods based on the FGW objective [6, 8, 45, 58] optimize the cost matrices under the fixed OT mapping S, whose gradients further guide the learning of feature encoder fe. However, due to the non-negativity of S, directly minimizing FGW distance leads to trivial solutions where cost matrices M, C1, C2 become zero matrices, hence leading to embedding collapse illustrated in Proposition 1.\nPROPOSITION 1. (EMBEDDING COLLAPSE). Given two networks G1, G2, directly optimizing feature encoder fe with the FGW distance leads to embedding collapse, that is E1 (x) = E2(y), \u2200x \u2208 G1, y \u2208 G2, where E1 = fe (G1), E2 = fe (G2).\nThe proof can be be found in Appendix B. In general, due to the non-negativity of FGW distance [46], its minimal value of zero is achieved by projecting all nodes to identical embeddings, hence significantly degrading embeddings' discriminating power.\nTo alleviate embedding collapse, we propose a transformation $g_\u03bb: \\mathbb{R}^{n_1 \\times n_2} \\rightarrow \\mathbb{R}^{n_1 \\times n_2}$ to transform the non-negative OT mapping S into an adaptive node sampling matrix $S_n = g_\u03bb(S)$ to discern the positive samples from the negative ones together with sampling weights. In this work, we adopt $g_\u03bb (S) = S - \u03bb1_{n_1\\times n_2}$, where \u03bb is a learnable transformation parameter. The rationale behind such design is to find the optimal threshold \u03bb to distinguish between positive and negative pairs automatically. Moreover, as the absolute value of Sn (x, y) indicates the certainty of sampling node pair (x, y) as positive/negative pairs, it helps distinguish easy and hard samples for the ranking loss. Equipped with such adaptive sampling matrix Sn, we quantitatively attribute the effectiveness of FGW distance to the following two aspects: node-level ranking and edge-level ranking.\nWasserstein Distance as Node-Level Ranking Loss. Equipped with the sampling strategy Sn, the Wasserstein distance term can be reformulated as a node-level ranking loss as follows\n$T_w = \\sum_{(x,y) \\in V_1XV_2} M(x, y)S_n(x, y)$\n$= \\sum_{(x,y_p) \\in R^+} M(x, y_p)|S_n(x, y_p)| - \\sum_{(x,y_n) \\in R^-} M(x, y_n)|S_n(x, y_n)| $\n(6)\nwhere R+= {(x, yp)|Sn(x, yp)\u22650}, R-= {(x, yn)|Sn(x, yn)<0}."}, {"title": "Gromov-Wasserstein Distance as Edge-Level Ranking Loss", "content": "R+ and R- are the sets of positive and negative node pairs, respectively. Therefore, Eq. (6) can be viewed as a weighted ranking loss function at the node level, where the sign of Sn (x, y) is used to distinguish between positive and negative node pairs and the sampling weight |Sn(x, y)| indicates the certainty of the sampled positive/negative node pair. For example, (x, y) is regarded as an uncertain pair and should contribute little to the ranking loss if S(x, y) is close to the threshold \u03bb. Similarly, if S(x, y) is far away from A, the relationship between (x, y) is more certain and should contribute more to the ranking loss. Therefore, Sn directly models all cross-network pairs (x, y) \u2208 V\u2081 \u00d7 V2 with noise-reduced certainty values. To this point, we provide a unified view of the Wasserstein distance and the node-level ranking loss.\nAnother limitation of the existing ranking loss is that it only considers node relationships while ignores the modeling of edge relationships, hence may fall short in preserving graph structure in the node embedding space [45, 59]. To address this issue, we introduce a novel ranking loss function at edge-level and unify it with the GW distance.\nGromov-Wasserstein Distance as Edge-Level Ranking Loss. The GW distance term can be reformulated as an edge-level ranking loss as follows,\n$T_{gw} = \\sum_{\\substack{x,x' \\in G_1,\\y,y' \\in G_2}} |C_1(x, x') \u2013 C_2(y, y')|^2S_n(x, y)S_n(x', y')$\n$= \\sum_{(e_{x,x'},e_{y_p,y_p}) \\in T^+} |d_e(e_{x,x'}, e_{y_p,y_p})||S_e(e_{x,x'}, e_{y_p,y_p})|-\\sum_{(e_{x,x'},e_{y_n,y_n}) \\in T^-} |d_e(e_{x,x'}, e_{y,y'})||S_e(e_{x,x'}, e_{y_n,y'_n})| $\n(7)\nwhere ex,x' is the edge between x and x', de measures the distance between two edges, and T\u207a, T\u00af are the sets of positive and negative edge pairs sampled by the edge sampling strategy Se. Similar to Eq. (6), Eq. (7) is a weighted ranking loss at the edge level, where the sign of $S_e (e_{x,x'}, e_{y, y'})$ distinguishes between positive and negative edge pairs and the sampling weight $|S_e(e_{x,x'}, e_{y,y'})|$ indicates the certainty of the sampled positive/negative edge pair. From the view of line graph [59], where edges in the original graph are mapped into nodes in the line graph and vice versa, the edge ranking loss in the original graph can be interpreted as the node ranking loss in the corresponding line graph."}, {"title": "3.3.2 Optimization", "content": "Combining the node-level ranking loss (Wasserstein distance) and edge-level ranking loss (GW distance) gives the unified optimization objective of JOENA for both embedding learning and OT optimization as Eq. (5). To optimize this objective, we adopt an alternating optimization scheme where the parameters of feature encoder fe, transformation parameter \u03bb, and OT mapping S are optimized iteratively.\nSpecifically, for the k-th iteration, we first fix the feature encoder $f_\\theta^{(k)}$ and the transformation parameter $\u03bb^{(k)}$, and optimize Eq. (5) w.r.t S by the proximal point method [57]. Due to the non-convexity of the objective, proximal point method decomposes the non-convex problem into a series of convex subproblems plus an additional Bregman divergence between two consecutive solutions, where each subproblem can be formulated as follows\n$S^{(t+1)} = arg min J(S; \u03bb^{(k)}, \\theta^{(k)}) + y_pDiv(S||S^{(t)}),$\n$\\Sigma \\in \\Pi(\\mu_1,\\mu_2)$\n(8)\nwhere t is the number of proximal point iteration, yp is the weight for the proximal operator, and Div is the Bregman divergence between two OT mappings. Then, the resulting subproblem in Eq. (8) can be transformed into an entropy-regularized OT problem as\n$min J^{(t+1)} = (C^{(t)},S) + y_p(logS,S) -\n$\\Sigma \\in \\Pi(\\mu_1,\\mu_2)$\n$\\frac{1}{y_p} \\sum_{(1-\\alpha)M + L_{total}}, logS + S),$\n(9)\nSince both and S are bounded, there exists a real number \u0454 \u2208 R satisfying\n$C^{(t)} = (1-\\alpha)M + L_{total},$\\n$L_{total} = \u03b1 L_{gw} - Y_p logS^{(t)},$\\n$L_{gw} = C_1^2 S^{(t)}1_{n_2\\times n_2} + 1_{n_1\\times n_1} S^{(t)} C_2  - 2C_1 C_2 S^{(t)}$\n$S^{(t)} = S^{(t)} - \\lambda^{(k)} 1_{n_1\\times n_2},$\nNote that S(t) is the OT mapping from last proximal point it- eration and remains fixed in the above equation. Therefore, the objective function of each proximal point iteration in Eq. (9) is essentially an entropy-regularized OT problem with a fixed transport cost $C^{(t)}$ minus a constant term that does not affect the optimization. Eq. (9) can be solved efficiently by the Sinkhorn algorithm [35].\nThen, we fix the feature encode $f_\\theta^{(k)}$ and OT mapping $S^{(k+1)}$, and optimize Eq. (5) w.r.t the transformation parameter \u03bb. Since the objective function is quadratic w.r.t. \u03bb, the closed-form solution for $\u03bb^{(k+1)}$ can be obtained by setting dJ |\u2202\u03bb = 0 as follows\n$\u03bb^{(k+1)} = \\frac{(1 \u2013 \u03b1) K_1 + \u03b1 K_2}{2\u03b1 K_3}$\nwhere\n(10)\n$K_1 = \\sum_{\\mathcal{X} \\in G_1, \\mathcal{Y} \\in G_2} M(x, y; \\theta^{(k)}),$\n$K_2 = \\sum_{\\substack{x,x' \\in G_1,\\\ny,y' \\in G_2}} d_e (e_{x,x'}, e_{y,y'}; \\theta^{(k)}) (S^{(k+1)}(x, y) + S^{(k+1)}(x', y') )$,$\n$K_3 = \\sum_{\\substack{x,x' \\in G_1,\\\ny,y' \\in G_2}} d_e (e_{x,x'}, e_{y,y'}; \\theta^{(k)})$\nFinally, to optimize the feature encoder fe, we fix the transformation parameter \u03bb(k+1) and the OT mapping S(k+1) to optimize Eq. (5) w.r.t \u03b8 via stochastic gradient descent (SGD), that is\n$\u0398^{(k+1)} = arg min_{\u03b8} J(\u03b8; S^{(k+1)}, \u03bb^{(k+1)}).$\n(11)\nAs we will show later, by iteratively applying Eq. (8)-(11), the objective function in Eq. (5) converges under the alternating optimization scheme. Besides, it is worth noting that alternating optimization is only used for model training, while model inference only requires one-pass, i.e., the forward pass of MLP and the proximal point method for OT optimization, allowing JOENA to scale efficiently to large networks."}, {"title": "3.4 Proof and Analysis", "content": "In this subsection, we provide theoretical analysis of the proposed JOENA. Without loss of generality, we assume that graphs share"}, {"title": "4 Experiments", "content": "In this section, we carry out extensive experiments and analyses to evaluate the proposed JOENA from the following aspects:\n\u2022 Q1. How effective is the proposed JOENA?\n\u2022 Q2. How efficient and scalable is the proposed JOENA?\n\u2022 Q3. How robust is JOENA against graph noises?\n\u2022 Q4. How do OT and embedding learning benefit each other?\n\u2022 Q5. To what extent does the OT-based sampling strategy surpass the hand-crafted strategies?\n4.1 Experiment Setup\nDatasets. Our method is evaluated on both plain and attributed networks summarized in Table 5. Detailed descriptions and experimental settings are included in Appendix C\u00b3.\nBaselines. JOENA is compared with the following three groups of methods, including (1) Consistency-based methods: IsoRank [40] and FINAL [80], (2) Embedding-based methods: REGAL [19], DANA [20], NetTrans [84], BRIGHT [67], NeXtAlign [81], and WL-Align [27], and (3) OT-based methods: WAlign [16], GOAT [39], PARROT [77], and SLOTAlign [45]. To ensure a fair and consistent comparison, for all unsupervised baselines, we introduce the supervision information in the same way as JOENA by concatenating the RWR scores w.r.t the anchor nodes with the node input features.\nMetrics. We adopt two commonly used metrics Hits@K and Mean Reciprocal Rank (MRR) to evaluate model performance. Specifically, given (x, y) \u2208 Stest where Stest denotes the set of testing node pairs, if node y \u2208 G2 is among the top-K most similar nodes to node u \u2208 G1, we consider it as a hit. Then, Hits@K is computed by $\\frac{\\# \\text{of hits}}{\\#\\text{Stest}}$. MRR is computed by the average of the reciprocal of alignment ranking of all testing node pair, i.e., $MRR = \\frac{1}{\\# \\text{Stest}} \\sum_{(x,y) \\in \\text{Stest}} \\frac{1}{rank(x,y)}$.\n4.2 Effectiveness Results\nWe evaluate the alignment performance of JOENA, and the results on plain and attributed networks are summarized in Table 2 and 3, respectively. Compared with consistency and embedding-based\n3Code and datasets are available at github.com/yq-leo/JOENA-WWW25."}, {"title": "4.3 Scalability Results", "content": "methods, JOENA achieves up to 31% and 22% improvement in MRR over the best-performing baseline on plain and attributed network tasks, respectively, which indicates that JOENA is capable of learning noise-reduced node mapping beyond local graph geometry and consistency principles thanks to the OT component. Compared with OT-based methods, JOENA achieves a significant outperformance compared with the best competitor PARROT [77] with up to 16% and 6% improvement in MRR on plain and attributed networks. Such outperformance demonstrates the effectiveness of the transport costs encoded by learnable node embeddings. Moreover, the performance improvement over WAlign [16] and SLOTAlign [45] indicates that JOENA successfully avoids embedding collapse thanks to the learnable transformation ga on OT mapping and the resulting adaptive sampling strategy Sn."}, {"title": "4.4 Robustness Results", "content": "To show the robustness of the proposed JOENA, we evaluate the performance of JOENA under structural and attribute noise.\n4.4.1 Robustness against Structural Noises. We first evaluate the robustness of JOENA against structural (edge) noises. Specifically, for edge noise level p, we randomly flip p% entries in the adjacency matrix, i.e., randomly add/delete edges [45]. Evaluations are conducted on plain Phone-Email network to eliminate potential interference from node attributes. The results are shown in Figure 4a.\nCompared to other baselines, the performance of JOENA consistently achieves the highest MRR in all cases. More importantly, thanks to the direct modeling and noise-reduced property of OT, we observe a much slower degradation of the MRR when the noise level increases, validating the robustness of JOENA against graph structural noises. Furthermore, embedding-based methods without OT (i.e., WLAlign [27], NeXtAlign [81], BRIGHT [67]) degrades much faster than methods with OT (i.e., JOENA, PARROT [77]), demonstrating that embedding-based methods are more sensitive to structural noise due to indirect modeling.\n4.4.2 Robustness against Attribute Noises. We also evaluate the robustness of JOENA against attribute noises. Specifically, for attribute level p, we randomly flip p% entries in the attribute matrix [48]. The results are shown in Figure 4b.\nCompared to baselines, the performance of JOENA consistently achieves the best performance, as well as the mildest degradation when attribute noise level increases, demonstrating the robustness of JOENA against node attribute noises. Besides, the performance of embedding-based methods degrades more severely than JOENA which further illuminates the deficiency of indirect modeling."}, {"title": "4.5 Further Analysis", "content": "4.5.1 Mutual Benefits of OT and Embedding Learning. To verify the mutual benefits of OT and embedding learning, we compare the performance of JOENA against the following variants on three datasets:\n(1) EMD infers node alignments by node embeddings learned under the sampling strategy from BRIGHT [67]; (2) EMD(OT) infers node alignments by node embeddings learned under our OT-based sampling strategy; (3) OT infers node alignments by the OT mapping with cost matrices based on RWR encoding; (4) JOENA infers node alignments by OT mapping with learnable cost matrices; (5) \u041e\u0422 EMB infers node alignments by the Hadamard product of OT mapping and the inner product of node embeddings; (6) \u041e\u0422+\u0415\u043c\u0432 infers node alignments by the sum of OT mapping and the inner product of node embeddings.\nThe results are shown in Table 4. Firstly, we observe a consistent outperformance of E\u043cB(OT) compared to EMB, showing that the proposed OT-based sampling strategy improves the quality of node embeddings compared to existing sampling strategies. Besides, comparing OT to JOENA, without learnable cost matrices, OT drops up to 16% in Hits@1 compared to JOENA, indicating that the cost design on learnable node embeddings improves the performance of OT optimization by a significant margin. Furthermore, we compare the performance of JOENA to OTOEMB and OT+\u0415\u043c\u0432, both of which naively integrate the OT and embedding alignments learned separately. It is shown that both \u041e\u0422 \u0415\u043c\u0432 \u0430nd \u041e\u0422+\u0415\u043c\u0432 achieves similar performance as OT and outperforms EMB. For one thing, this suggests that the outperformance of JOENA largely attributes to the OT alignment, which provides a more denoised alignment compared with embedding alignment. For another, naively combining the alignment matrices of embedding or OT-based method at the final stage hardly improves the alignment quality, and it is necessary to combine both components during training."}, {"title": "4.5.2 OT-based Sampling Strategy", "content": "We also carry out studies on the effectiveness of the OT-based sampling strategy ga (S). As shown in Figure 5, we report the MRR under different A with the learned \u03bb annotated. It is shown that JOENA achieves the best performance under the learned \u03bb. Besides, we observe a significant performance drop when A is not properly selected. This is due to the fact that when A is too small/large, most pairs will be sampled as positive/negative pairs exclusively, which further leads to embedding collapse. To validate this point, we visualize how the embedding space changes along optimization. As shown in Figure 6a, when setting \u03bb = 0, MRR gradually decreases and the learned embeddings collapse into an identical point along optimization. On the contrary, as shown in Figure 6b, JOENA is able to learn the optimal \u03bb, under which, MRR gradually increases and node embeddings are well separated in the embedding space."}, {"title": "5 Related Works", "content": "5.1 Network Alignment\nTraditional network alignment methods are often built upon alignment consistency principles. IsoRank [40] conducts random walk on the product graph to achieve topological consistency. FINAL [80] interprets IsoRank as an optimization problem and introduces consistency at attribute level to handle attributed network alignment. Another line of works [25, 50, 68] learn informative node embeddings in a unified space to infer alignment. REGAL [19] conducts matrix factorization on cross-network similarity matrix for node embedding learning. DANA [20] learns domain-invariant embeddings for network alignment via adversarial learning. BRIGHT [67] bridges the consistency and embedding-based alignment methods, and NeXtAlign [81] further balances between the alignment consistency and disparity by crafting the sampling strategies. WL-Align [27] utilizes cross-network Weisfeiler-Lehman relabeling to learn proximity-preserving embeddings. More related works on network alignment are reviewed in [12].\n5.2 Optimal Transport on Graphs\nOT has recently gained increasing attention in graph mining and network alignment, whose effectiveness often depends on the predefined cost function restricted to specific graphs. For example, [16, 32, 34, 62] represent graphs as distributions of filtered graph signals, focusing on one specific graph property, while other cost designs are mostly based on node attributes [6] or graph structures [39]. PARROT [77] integrates various graph properties and consistency principles via a linear combination, but requires arduous parameter tuning. More recent works combine both embedding and OT-based alignment methods. GOT [6] adopts a deep model to encode transport cost. GWL [58] learns graph matching and node embeddings jointly in a GW learning framework. SLOTAlign [45] utilizes a parameter-free GNN model to encode the GW distance between two graph distributions. CombAlign [8] further proposes to combine the embeddings and OT-based alignment via an ensemble framework.\n5.3 Graph Representation Learning\nRepresentation learning gained increasing attention in analyzing complex systems with applications in trustworthy ML [4, 13, 31, 71], drug discovery [41, 54, 55, 85] and recommender systems [29, 53, 75]."}, {"title": "6 Conclusions", "content": "In this paper, we study the semi-supervised network alignment problem by combining embedding and OT-based alignment methods in a mutually beneficial manner. To improve embedding learning via OT, we propose a learnable transformation on OT mapping to obtain an adaptive sampling strategy directly modeling all cross-network node relationships. To improve OT optimization via embedding, we utilize the learned node embeddings to achieve more expressive OT cost design. We further show that the FGW distance can be neatly unified with a multi-level ranking loss at both node and edge levels. Based on these, a unified framework named JOENA is proposed to learn node embeddings and OT mappings in a mutually beneficial manner. Extensive experiments show that JOENA consistently outperforms the state-of-the-art in both effectiveness and scalability by a significant margin, achieving up to 16% performance improvement and up to 20\u00d7 speedup.\nAcknowledgment"}]}