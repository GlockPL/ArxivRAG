{"title": "TEMPORAL ORDER PRESERVED OPTIMAL TRANSPORT-BASED CROSS-MODAL KNOWLEDGE TRANSFER LEARNING FOR ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to an acoustic model has been shown to greatly improve the performance of automatic speech recognition (ASR). However, due to the heterogeneous feature distributions in cross-modalities, designing an effective model for feature alignment and knowledge transfer between linguistic and acoustic sequences remains a challenging task. Optimal transport (OT), which efficiently measures probability distribution discrepancies, holds great potential for aligning and transferring knowledge between acoustic and linguistic modalities. Nonetheless, the original OT treats acoustic and linguistic feature sequences as two unordered sets in alignment and neglects temporal order information during OT coupling estimation. Consequently, a time-consuming pre-training stage is required to learn a good alignment between the acoustic and linguistic representations. In this paper, we propose a Temporal Order Preserved OT (TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for ASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are smoothly mapped to neighboring regions of linguistic sequences, preserving their temporal order relationship in feature alignment and matching. With the TOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained Chinese PLM for linguistic knowledge transfer. Our results demonstrate that the proposed TOT-CAKT significantly improves ASR performance compared to several state-of-the-art models employing linguistic knowledge transfer, and addresses the weaknesses of the original OT-based method in sequential feature alignment for ASR.", "sections": [{"title": "1. INTRODUCTION", "content": "The combination of a pretrained language model (PLM) with an end-to-end (E2E)-based acoustical model for automatic speech recognition (ASR) has made significant progress in recent years [1, 2, 3, 4, 5, 6]. The advantage of incorporating a PLM in ASR lies in the availability of unpaired large text corpora for training the PLM. Moreover, the linguistic knowledge encoded in the PLM can be utilized in ASR decoding. In most studies, the PLM is employed as an external language model (LM) for post-processing tasks such as beam search or rescoring in ASR [7, 8]. However, using an external LM for post-processing compromises the speed and sometimes parallel decoding capabilities of ASR. Addressing how to transfer linguistic knowledge to acoustic encoding during model training, and subsequently conducting speech recognition without relying on any external LM post-training, is an intriguing research topic.\nIn this study, our focus is on transferring linguistic knowledge from a PLM to a temporal connectionist temporal classification (CTC)-based ASR [9]. While there are several advanced end-to-end (E2E)-based ASR approaches that incorporate linguistic knowledge in acoustic model learning [10, 11], using a PLM, such as bidirectional encoder representation from transformers (BERT) [12]), facilitates linguistic knowledge transfer in ASR [13, 14, 15, 16], This knowledge transfer can also occur with a pretrained acoustic encoder, such as wav2vec2 [17], for both linguistic and acoustic knowledge transfer [18, 19, 20, 21, 22, 23, 24, 25]. However, due to the heterogeneous feature distributions in acoustic and linguistic spaces, it remains a challenging task to efficiently align feature representations between linguistic and acoustic modalities to facilitate knowledge transfer. In most studies, a cross-attention module is designed to integrate acoustic and text representations within a transformer decoder framework for combining acoustic and linguistic knowledge in ASR [26]. Yet, in the decoding stage, true text representations are unavailable, leading to the adoption of predicted text representations in decoding. This mismatch between training and testing phases weakens the benefits of linguistic information in ASR.\nFor efficient alignment and matching, an effective distance metric is needed to measure the difference between acoustic and linguistic feature representations. Considering this requirement, optimal transport (OT) emerges as a suitable tool for cross-modal alignment and linguistic knowledge transfer. OT, originally proposed for optimal allocating resources and later as a measure of discrepancies between probability distributions [27, 28], has found widespread ap-"}, {"title": "2. PROPOSED METHOD", "content": "The model framework of the proposed TOT-CAKT method is illustrated in Fig. 1. This model framework is modified based on a conformer-CTC-based ASR model, incorporating two key modifications. First, an 'Adapter' module is added as shown in the gray blocks in Fig. 1. Second, an temporal order preserved OT-based cross-modal matching module is introduced in the right branch of Fig. 1. Both the acoustic features extracted from the conformer encoder and linguistic features derived from a PLM (with BERT utilized in this paper) are involved in the cross-modal matching process. Further details are provided in the following sections."}, {"title": "2.1. Acoustic and linguistic feature representations", "content": "The acoustic feature is extracted from the acoustic encoder where a conformer-based encoder [37]) is adopted. The process in the 'Subsampling' module involves a two-layer convolution process with a downsampling operation (a downsampling rate of 4 was used in this paper). By incorporating a positional encoding from PEA, the initial input to conformer blocks is obtained as $H_0$. The output of the conformer encoder is represented as an acoustic representation $H_{ca}$.\n$H_0 = Subsampling (X) + PEA$ \n$H_{ca} = Conformer (H_0) \\in \\mathbb{R}^{l_a\\times d_a}$,\n(1)\nwhere $l_a$ and $d_a$ are temporal length and dimension of the acoustic feature vectors, respectively. Before engaging in cross-modal feature alignment, a linear projection termed 'FC2' in the 'Adapter' module is utilized to perform a feature dimension matching transform:\n$H_A = FC2 (H_{ca}) \\in \\mathbb{R}^{l_a\\times d_t}$ (2)\nIn this equation, $d_t$ corresponds to linguistic feature dimension. In the right branch of Fig. 1, the context-dependent linguistic feature representation is explored from a pretrained BERT model. The process is formulated as:\n$y_{token} = BERTTokenizer (y)$ \n$Z_0 = [CLS, y_{token}, SEP]$ \n$Z_i = BERT (Z_{i-1})$, (3)\nwhere 'BERT\u00bf' is the i-th transformer encoder layer of BERT model, i takes values from 1 to L, with L representing the total number of BERT encoder layers. 'BERTTokenizer' is a process to convert standard text to word piece based tokens [12]. Token symbols 'CLS' and 'SEP' represent the start and end of an input sequence. $Z_L \\in \\mathbb{R}^{l_t\\times d_t}$ is the final text representation which encodes context dependent linguistic information, $l_t$ denotes the sequence length, and $d_t$ represents feature dimension of text encoding representation."}, {"title": "2.2. Sinkhorn algorithm for cross-modal alignment", "content": "The original OT was formulated to transform from one probability distribution to another with minimum transport cost [27]. In this study, we applied OT for feature alignment on two sets. Given acoustic and linguistic feature sequences $H_A$ and $Z_L$ respectively, as:\n$H_A = [h_1, h_2, ..., h_i, ..., h_{l_a}]$\n$Z_L = [z_1, z_2, ..., z_j, ..., z_{l_t}]$, (4)\nwhere $l_a$ and $l_t$ are lengths of the two sequences. Suppose the two sequences in Eq. (4) are sampled from two probability distributions with weight vectors $a = [a_1, a_2, ..., a_i, ..., a_{l_a}]$ and $b = [b_1, b_2, ..., b_j, ..., b_{l_t}]$. ($a_i = 1/l_a, b_j = 1/l_t$ as uniform distributions if no prior information is available). The OT distance between the two sequences is defined as:\n$L_{OT} \\triangleq \\min\\limits_{\\Upsilon \\in \\Pi (H_A, Z_L)} <\\gamma, C>$, (5)\nwhere $\\gamma$ is a transport coupling set defined as:\n$\\Pi (H_A, Z_L)=\\{\\gamma \\in \\mathbb{R}^{l_a\\times l_t}_+ | \\gamma 1_l = a, \\gamma^\\top 1_l = b\\}$ (6)\nIn Eq. (6), $1_l$ and $1_l$ are vectors of ones with dimensions $l_a$ and $l_t$, respectively. In Eq. (5), C is a distance matrix (or ground metric) with element $c_{i,j}$ defined as pair-wised cosine distance:\n$c_{i,j} = C (h_i, z_j)=1 - cos (h_i, z_j)$ (7)\nA fast estimation of OT has been introduced through the celebrated entropy-regularized OT (EOT) [38] where the EOT loss is defined as:\n$L_{EOT} (H_A, Z_L) \\triangleq \\min\\limits_{\\Upsilon \\in \\Pi (H_A, Z_L)} <\\gamma, C> - \\alpha_1 H (\\gamma)$, (8)\nwhere $\\alpha_1$ is a regularization coefficient, and $H (\\gamma)$ is entropy of coupling matrix defined as:\n$H (\\gamma) = - \\sum_{i,j} \\gamma_{i,j} log \\gamma_{i,j}$. (9)\nThe solution of Eq. (8) can be implemented with Sinkhorn algorithm as [27]:\n$\\Upsilon_{\\alpha_1} = diag (u_1) * G * diag (u_2)$ (10)\nwhere $G = exp \\Big(-\\frac{C}{\\alpha_1}\\Big)$, $u_1$ and $u_2$ are two scaling (or renormalization) vectors."}, {"title": "2.3. Temporal order preserved OT", "content": "In the original estimation of OT in Eq. (8), the two sequences in Eq. (4) are treated as two sets without considering their temporal order relationship. In speech, temporal order information is crucial in OT coupling during cross-modal alignment, meaning that neighboring frames in an acoustic sequence should be progressively coupled with the neighboring tokens in a linguistic sequence. Therefore, as showed in Fig. 1, the temporal order information is input to the OT matching block. For the sake of clarity, the two sequences in Eq. (4) can be further represented with temporal order information as:\n$H_A = [(h_1, 1), (h_2, 2), ..., (h_i, i), ..., (h_{l_a}, l_a)]$\n$Z_L = [(z_1, 1), (z_2, 2), ..., (z_j, j), ..., (z_{l_t}, l_t)]$ (11)\nDuring the alignment of the two sequences for knowledge transfer, it is crucial to consider that elements with significant cross temporal distances might not be likely to be coupled. In other words, the coupling pairs with high probabilities between the two sequences should be distributed along the diagonal line of the temporal coherence positions. Based on this consideration, the temporal coupling prior could be defined as a two dimensional Gaussian distribution [39]. The fundamental concept is that the coupled pairs should not deviate significantly from the diagonal line of temporal coherence positions between the two sequences, which can be defined as:\n$p_{i,j} = \\frac{1}{\\sigma\\sqrt{2\\pi}} exp \\Big(-\\frac{d_{i,j}^2}{2\\sigma^2}\\Big)$ (12)\nwhere $\\sigma$ is a variation variable controlling the impact of the cross-temporal distance $d_{i,j}$ as defined in Eq. (13).\n$d_{i,j} = \\frac{|i/l_a - j/l_t|}{\\sqrt{\\frac{1}{l_a^2} + \\frac{1}{l_t^2}}}$ (13)\nIn Eq. (13), the cross-temporal distance is defined on the normalized sequence lengths in acoustic and linguistic spaces. In this definition, it is evident that the farther the distance between a paired position and the temporal diagonal line, the lower possibility of their correspondence in transport coupling. By incorporating this temporal coherence prior as regularization, the new OT is defined as:\n$L_{TOT}(H_A, Z_L)\\triangleq \\min\\limits_{\\Upsilon \\in \\Pi (H_A, Z_L)} < \\gamma, C > -\\alpha_1H(\\gamma)+\\alpha_2KL(\\gamma||P)$, (14)\nwhere $\\alpha_1$ and $\\alpha_2$ are two trade off parameters. In Eq. (14), $KL(\\gamma||P)$ is the Kullback-Leibler (KL) divergence between the transport coupling matrix $\\gamma$ and temporal prior correspondence matrix P with elements defined in Eq. (12). Building upon the definitions of KL-divergence and entropy, Eq. (14) can be further expressed to:\n$L_{TOT} (H_A, Z_L) \\triangleq \\min\\limits_{\\Upsilon \\in \\Pi (H_A, Z_L)} < \\gamma, \\tilde{C} > -\\tilde{\\alpha}H(\\gamma)$, (15)\nwhere $\\tilde{\\alpha} = \\alpha_1 + \\alpha_2$, and combined ground cost matrix as\n$\\tilde{C} = C - \\frac{\\alpha_2}{\\tilde{\\alpha}} log P$ (16)"}, {"title": "2.4. Loss function", "content": "The proposed TOT-CAKT involves two loss functions: the cross-modal alignment and matching loss (in the right branch of Fig. 1) and the CTC loss (in the left branch of Fig. 1). In cross-modal alignment, the acoustic feature can be projected onto the linguistic space using OT as:\n$\\tilde{Z}_{LOT} (H_A \\rightarrow Z_L) = \\gamma^* \\times H_A \\in \\mathbb{R}^{l_t\\times d_t}$, (20)\nwhere $\\gamma^*$ is the optimal transport coupling based on OT. Subsequently, the alignment loss is defined as:\n$L_{align} = \\sum_{j=2}^{l_t-1} 1 - cos(\\tilde{z}_j^*, z_j)$, (21)\nwhere $\\tilde{z}_j^*$ and $z_j$ are row vectors of feature matrices $\\tilde{Z}_L$ and $Z_L$ (matching on temporal dimensions), respectively. In Eq. (21), the usage of indices from 2 to $l_t - 1$ is for handling special symbols 'CLS' and 'SEP'. For efficient linguistic knowledge transfer to acoustic encoding, the following transforms are designed as indicated in Fig. 1:\n$\\hat{H}_{ca} = FC3 (LN(H_A)) \\in \\mathbb{R}^{l_a\\times d_a}$\n$H_{ca,t} = H_{ca}+ s\\cdot LN(\\hat{H}_{ca})$, (22)\nwhere s is a scaling parameter to adjust the importance of transferring linguistic projected feature. Based on this new representation $H_{ca,t}$ which is intended to encode both acoustic"}, {"title": "3. EXPERIMENTS", "content": "ASR experiments were conducted on the open-source Mandarin speech corpus AISHELL-1 [40] to evaluate the proposed algorithm. The data corpus comprises three datasets: a training set with 340 speakers (150 hours), a development (or validation) set with 40 speakers (10 hours), and a test set with 20 speakers (5 hours). Data augmentation as used in [40] was applied. Given the tonal nature of the Mandarin language in the ASR task, in addition to using 80-dimensional log Mel-filter bank features, three extra acoustic features related to fundamental frequency, i.e., F0, delta F0 and delta delta F0, were utilized as raw input features. These features were extracted with a 25ms window size and a 10ms shift."}, {"title": "3.1. Model architecture", "content": "In Fig. 1, the 'Subsampling' module consists of two CNN blocks with 256 channels, kernel size 3, stride 2, and ReLU activation function in each. The acoustic encoder is formed by stacking 16 conformer blocks [37], with each having a kernel size of 15, attention dimension $d_a = 256$, 4 attention heads, and a 2048-dimensional FFN layer. The 'bert-base-chinese' from huggingface is used as the pretrained PLM for linguistic knowledge transfer [41]. In this Chinese BERT model, 12 transformer encoders are applied, the token (or vocabulary) size is 21128, and the dimension of linguistic feature representation is $d_t = 768$."}, {"title": "3.2. Hyper-parameters in model learning", "content": "Several hyper-parameters are associated with the proposed model, and these parameters may have a joint (or correlated) effect in efficient linguistic knowledge transfer learning. In our preliminary experiments, for easy implementation, they were fixed as $\\beta = 0.5$ in Eq. (19), alignment trade off parameter $\\lambda = 0.3$ and scale parameter $w = 1.0$ in Eq. (24). $\\tilde{\\alpha}$ in Eq. (15) and s in Eq. (22) were varied in experiments. For optimization, Adam optimizer [42] is used with a learning rate (initially set to 0.001) schedule with 20,000 warm-up"}, {"title": "3.3. Results", "content": "In inference stage, only the left branch (blocks in dashed red box in Fig. 1) is utilized, maintaining the decoding speed similar to that of the CTC-based decoding. In our experiments, only CTC greedy search-based decoding was employed, and the results are presented in table 1. The results of the baseline system and several state-of-the-art systems that integrate BERT for linguistic knowledge transfer are also provided for comparison. In this table, 'Conformer+CTC' is the baseline system, trained without linguistic knowledge transfer. 'Conformer+CTC/AED' denotes a hybrid CTC/AED ASR system [3, 4, 5] which used a transformer decoder with attention to text representation during model training. 'NAR-BERT-ASR', 'KT-RL-ATT', and 'Wav2vec-BERT' are all based on integrating acoustic and linguistic features from BERT for ASR [22, 23, 14, 21], and even used a pretrained acoustic model (from wav2vec2.0 [17]) and PLM for knowledge transfer. In the OT based cross-modal learning, two experimental conditions were examined. One is that models with cross-modal learning were trained from scratch, i.e., without pretraining condition. The other is that the models were initialized with a pretrained acoustic model, then were fur-"}, {"title": "3.4. Visualization of transport coupling", "content": "In the proposed TOT-CAKT, the coupled pairs between the acoustic and linguistic feature sequences are explicitly designed to correspond to their temporal coherence, i.e., acoustic segments should match well with their linguistic tokens sequentially. Two examples of the coupling matrices are shown in Fig. 2. In Fig. 2-a, the coupling matrix is learned based on OT without temporal order constraint, and Fig. 2-b is the coupling matrix learned with temporal order constraint. From this figure, it is evident that clear temporal correspondences exist between the acoustic feature sequence and linguistic token sequence in both transport couplings. Moreover, several positions with incorrect couplings in Fig. 2-a were corrected in Fig. 2-b by our proposed method, which explicitly adds a temporal order constraint."}, {"title": "4. CONCLUSION", "content": "Acoustic and linguistic features belonging to different modalities require feature alignment as a crucial step in transfer-"}]}