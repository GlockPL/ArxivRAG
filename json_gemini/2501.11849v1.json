{"title": "Network-informed Prompt Engineering against Organized Astroturf Campaigns under Extreme Class Imbalance", "authors": ["Nikos Kanakaris", "Heng Ping", "Xiongye Xiao", "Nesreen K. Ahmed", "Luca Luceri", "Emilio Ferrara", "Paul Bogdan"], "abstract": "Detecting organized political campaigns, commonly known as astroturf campaigns, is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on X (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2x-3x improvements in terms of precision, recall and F1 scores.", "sections": [{"title": "1 Introduction", "content": "Social media have undoubtedly emerged as one of the most powerful mediums of information capable of influencing and even shaping public opinion [6, 22, 24]. They provide their users with an immediate, easy-to-use way of sharing and accessing rich content. Among other factors, their directness and convenience have definitely contributed to their wide adoption. Additionally, they facilitate and promote fruitful conversations among peers, allowing everyone to express their opinion freely [1, 9].\nDespite their widespread success, social media have also provided a fertile ground for malicious users to orchestrate and execute coordinated disinformation campaigns, commonly referred to as astroturf campaigns \u00b9 [21]. Such disinformation campaigns come to augment the negative effects of fake news by spreading persuasive but inaccurate content across a social network. They deliberately confuse and manipulate people and public opinion, usually in an attempt to influence the outcome of a political decision, for instance during a presidential election [2, 11, 12]. In most cases, the original coordinators of the campaigns camouflage their true identities by pretending to be trustworthy state representatives, towards promoting their political viewpoints [3, 7]. A common characteristic of such campaigns is their close connection to fake accounts. These fake accounts typically include fake, automated, state-sponsored accounts and social bots, which are created to artificially amplify the strength of a piece of information by artificial amplification.\nTheir objective is to distribute content in a way resembling that of organic users [11]. Adding to the complexity, it has been observed that specific groups of real users, such as conservative individuals and the elderly, are more susceptible to engaging with and re-sharing political disinformation [2, 9].\nMore recently, with the advent of the generative artificial intelligence (GenAI) era, the proliferation and spread of fake news on social media have become even more problematic in modern society [4]. GenAI can further exacerbate the spread of disinformation by equipping malicious individuals with sophisticated tools to create compelling fake content, thus making it increasingly difficult for users to distinguish between real and fake news [4, 15].\nIn an attempt to mitigate the effects of astroturf campaigns, existing approaches have primarily relied on graph mining and natural language processing (NLP) techniques. Graph neural networks (GNNs) and large language models (LLMs) have emerged as the default technology to perform graph mining and NLP tasks, respectively. The combination of LLMs and GNNs has also been proposed in the literature [10, 17]. GNNs extract structural and topological features associated with a propagation tree-shaped graph G formed by the (re-)posting actions of the users, while LLMs and other NLP methods deal with the textual content of the posts. The aforementioned propagation tree G usually consists of a set of nodes V denoting the posts and re-posts of the users and a set of edges E denoting the re-posting actions (see Section 3.2 for an extensive problem definition). Most of the existing approaches build on a GNN-based architecture wherein embeddings (vectors) from LLMs are attached to each node of the propagation tree. Briefly, the inclusion of textual embeddings enables GNNs to take into consideration both the content and the user interactions simultaneously.\nAlthough recent approaches have generally been successful, there is still space for improvement. In particular, they display several shortcomings: (i) Amounts of labeled data needed: To begin with, they require large amounts of labeled data for training, which may not be available, mainly due to policy restrictions of the considered social media platforms [23]; (ii) Class imbalance: Even though the necessary data are available, given that the number of organic conversations is way larger than that of the fake news ones, the existing datasets demonstrate an extreme class imbalance; as a result, the available data critically inhibit the performance of the proposed methods [17]; (iii) Biased and not easily transferable: The training process itself makes the models biased towards a specific dataset. Therefore, they are not easily transferable to new cases or environments; (iv) Outdated, fostering hallucinations: They are frequently trained on outdated data or data that promote LLM hallucinations [13]. (v) Unsustainable or infeasible training: Finally, training these methods, especially those including LLMs, is neither sustainable nor feasible due to their large number of trainable parameters or their closed-source code policies (e.g., OpenAI's GPT-4 model) [8, 14].\nTo address the above issues, in this paper, we propose a novel framework for identifying coordinated disinformation campaigns relying only on frozen LLMs. The proposed framework encodes both the structural and textual information of a propagation tree as a text, ready to be prompted to an LLM. Using prompt engineering and retrieval-augmented generation (RAG) allows LLMs to detect organized astroturf campaigns on social networks such as X. Our"}, {"title": "3 Proposed framework", "content": "In this section, we present our framework. Briefly, it is composed of four phases as illustrated in Figure 1. Initially, we generate a propagation tree based on the posting and re-posting \u00b2 actions of a social network. Then, using graph prompting techniques, we encode the generated propagation tree as a text, aiming to make graph information related to the social network available to a given LLM. In the case of few-shot prompting, a RAG component provides the overall pipeline with several labeled examples, potentially assisting in the final task. The text of the original tweet, the text-encoded graph and the examples from the RAG component are then combined with the initial prompt question to form the final prompt. Finally, an LLM predicts whether the given prompt concerns a coordinated disinformation campaign or not. We note that all the necessary background is presented in Section 2."}, {"title": "3.2 Problem definition", "content": "In this section, we define important notation and formulate our main problem.\nDefinition 3.1. (Users and tweets). Let $\\mathcal{U} = \\{u_1, u_2, ..., u_{|\\mathcal{U}|} \\}$ be the set of users of a social network. Let also $\\mathcal{T} = \\{t_1, t_2, ..., t_{|\\mathcal{T}|}\\}$ be the set of the textual documents of tweets and re-tweets. Each user $u \\in \\mathcal{U}$ is the author of a subset of tweets and re-tweets $\\mathcal{T}_u \\subseteq \\mathcal{T}$. Furthermore, each user u has a number of followers $k \\in \\mathbb{N}^+$.\nDefinition 3.2. (Propagation tree). We denote a propagation tree as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$. Essentially, $\\mathcal{G}$ is a tree-shape graph consisting of a set of nodes $\\mathcal{V} = \\{v_1, v_2, ..., v_{|\\mathcal{V}|} \\}$ and a set of edges $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$. Each node $v \\in \\mathcal{V}$ corresponds to a tweet or re-tweet and is related to a user $u \\in \\mathcal{U}$ and a textual document $t \\in \\mathcal{T}$. Each edge $e = (v_i, v_j) \\in \\mathcal{E}$ denotes a re-tweet action, where re-tweet $v_j$ re-tweeted (re-)tweet $v_i$. In our case, the root node $v_1$ of a propagation tree $\\mathcal{G}$ is always a tweet. Similarly, each node $v_i, i \\geq 2$ is a re-tweet. We also compute the time (delay) difference $d_j \\in \\mathbb{R}^+$ between two tweets $v_i$ and $v_j$. We note that the delay $d_1$ of the original tweet $v_1$ is 0. To sum up, a propagation tree represents re-posting (re-tweeting) actions among a tweet $v_1$ and a set of re-tweets $\\{v_2, ..., v_{|\\mathcal{V}|}\\}$.\nDefinition 3.3. (Main problem). We focus on the problem of identifying coordinated disinformation campaigns on social media (e.g. X) under extreme class imbalance using only a frozen LLM $f$ for inference as well as prompt engineering and RAG techniques. Given a propagation tree $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ and the text $t_1$ of the original tweet $v_1$, our goal is to predict whether the propagation tree $\\mathcal{G}$ is an organized disinformation campaign or not."}, {"title": "3.3 Propagation tree construction", "content": "The first phase of the proposed framework is to construct the propagation tree $\\mathcal{G}$. Depending on the social network under consideration this step can be omitted. Here, this step is critical, given that the information provided by X is a star graph where all the re-tweets $\\{v_2, ..., v_{|\\mathcal{V}|}\\}\\}$ are linked to the original tweet $v_1$. Therefore, to predict the re-tweeting actions $\\mathcal{E}$ of a propagation tree $\\mathcal{G}$, we adopt the method proposed in [17, 19]. We first sort the set of (re-)tweets $\\mathcal{V}$ by time in ascending order. Then, for each re-tweet $v_i \\in \\mathcal{V}$, where $2 \\leq i \\leq |\\mathcal{V}|$, we add an edge $e = (v_j, v_i) \\in \\mathcal{E}$ iff:\nCondition 1: The text $t_i$ of the re-tweet $v_i$ includes an RT mention to a specific user $u \\in \\mathcal{U}$ who is the author of a retweet $v_j \\in \\{v_1,..., v_{i-1}\\}.\nCondition 2: The author $u_i$ of (re-)tweet $v_i$ follows the author $u_j$ of a tweet $v_j \\in \\{v_1, ..., v_{i-1}\\}$. If multiple such users exist, we pick the most popular, i.e. the one with the most followers k.\nFinally, if none of the above conditions applies, we randomly select a (re-)tweet $v_j \\in \\{v_1,..., v_{i-1}\\}$, using the number of followers k of each u as a selection criterion. This process follows a discrete weighted probability distribution, where the probability of selecting tweet $v_j$ is proportional to the number of followers $k_j$ of user $u_j$ who posted that tweet. Formally, the probability $P \\in [0, 1]$ of selecting tweet $v_j \\in \\{v_1, v_2, ..., v_{i-1}\\}$ is given by\n$$P(v_j) = \\frac{k_j}{\\sum_{m=1}^{i-1} k_m}$$\n(1)"}, {"title": "3.4 Prompt engineering", "content": "We now describe the prompt engineering phase of the framework. This phase aims to encode both the structural and textual information of a propagation tree $\\mathcal{G}$ as a text, in a way that the overall performance of a given LLM $f$ is generally improved.\nGraph encoding. As opposed to existing approaches that employ GNNs to encode the topological characteristics of a propagation tree $\\mathcal{G}$, we decide to follow a new paradigm [5, 20] where both the graph and text are given as a textual prompt to an LLM. To that end, we introduce the graph encoding function $g : \\mathcal{G} \\rightarrow \\mathcal{W}$, where $\\mathcal{W} = \\{w_1, w_2, ..., w_{|\\mathcal{W}|}\\}$ is the set of the available tokens (words). The graph encoding function $g$ takes as input a propagation tree $\\mathcal{G}$ and converts it into a set of words $\\mathcal{W}_g \\subseteq \\mathcal{W}$. Following the encoding guidelines presented in [5], we found that the majority of the LLMs demonstrate an improved performance by simply encoding the retweet actions $\\mathcal{E}$ of a propagation tree $\\mathcal{G}$ as a text as follows: \"(2->1), (3->1), (4->3)\". In the previous example, $\\mathcal{E} = \\{(1, 2), (1, 3), (3, 4)\\}$ and $|\\mathcal{V}| = 4$. We note that other graph encoding functions can also be used based on the problem and the nature of the given graph $\\mathcal{G}$, including 'adjacency', 'incident' and 'friendship' encoding. We refer to [5] for an extensive list of graph encoding functions for LLMs.\nPrompting techniques. Most of the available prompting techniques are compatible with our framework, including zero-shot, few-shot, chain-of-thought (CoT) and role prompting. Considering zero-shot prompting, our approach combines the textual document $t_1$ of the original tweet $v_1$ and the text-encoded version of the structural information $\\mathcal{E}$ of a propagation tree $\\mathcal{G}$ to create the final prompt. As a result, zero-shot prompting relies only on the pre-trained knowledge of the LLM to generate a response.\nAlthough zero-shot prompting can be a sufficient technique, few-shot prompting is much preferable when labeled examples of positive (fake news) and negative propagation trees are available. These examples can also be incorporated into the final prompt and therefore allow the model to gain some knowledge from them, in an attempt to improve its performance. The prompted examples can be"}, {"title": "3.5 Balanced Retrieval-Augmented Generation (Balanced RAG)", "content": "Here we introduce the Balanced Retrieval-Augmented Generation (Balanced RAG) component of our proposed framework, which is pivotal in addressing the challenge of extreme class imbalance in datasets, particularly when the number of positive samples significantly differs from the number of negative samples.\nMotivation and rationale. Class imbalance is a prevalent issue in machine learning tasks, especially in scenarios like detecting coordinated disinformation campaigns where positive instances (e.g., fake news propagation trees) are scarce compared to negative instances (e.g., organic propagation trees). Traditional models trained on such imbalanced datasets tend to be biased towards the majority class, resulting in poor detection performance on the minority class. Balanced RAG aims to mitigate this issue by constructing pairs of samples that are highly similar in content and structure but have opposite labels. By presenting these contrasting yet similar examples to the LLM, we enable it to focus on subtle differences between classes, enhancing its discriminative capabilities and effectively addressing class imbalance.\nBasic overview. The Balanced RAG component operates by first retrieving the top n samples from the training dataset $\\mathcal{D}$ that are most similar to a given query propagation tree $\\mathcal{G}'$. For each retrieved sample $\\mathcal{G}$, regardless of its label (positive or negative), the method finds a sample with the opposite label that has the highest similarity to $\\mathcal{G}$. This process results in a set of balanced pairs $\\mathcal{P} = \\{(\\mathcal{G}_1, \\mathcal{G}_1^{\\text{opposite}}), (\\mathcal{G}_2, \\mathcal{G}_2^{\\text{opposite}}), ..., (\\mathcal{G}_n, \\mathcal{G}_n^{\\text{opposite}})\\}$, where each pair consists of two highly similar samples with contrasting labels.\nThese balanced pairs are then incorporated into the few-shot prompting approach used to query the LLM. By providing the LLM with representative examples that are both similar to the query and equally cover both classes, we enhance the model's ability to distinguish between positive and negative instances. This approach enables the LLM to learn the critical differences that define each class, thereby improving its ability to correctly classify new, unseen propagation trees.\nIndexing. To facilitate efficient retrieval, each propagation tree $\\mathcal{G}$ in the training dataset $\\mathcal{D}$ is represented as an embedding vector $e \\in \\mathbb{R}^d$ that captures both structural and textual features of the propagation tree. The indexing function is formally defined as:\n$$\\text{index} : \\mathcal{G} \\rightarrow e \\in \\mathbb{R}^d.$$\n(2)\nThese embeddings e can be obtained using pre-trained LLMs that encode textual information, or simple vectorization methods like bag-of-words. Structural features of the propagation trees, such as node degrees or graph motifs, can be concatenated to the textual embeddings to enrich the representation. The resulting embeddings are stored in an index structure to enable efficient similarity computations during the retrieval phase.\nRetrieval process. The retrieval process is a critical component of the Balanced RAG method, designed to select relevant and informative examples that aid the LLM in making accurate predictions, especially in the context of extreme class imbalance. It involves the following detailed steps:\nRetrieve top n similar samples to $\\mathcal{G}'$. Given a query propagation tree $\\mathcal{G}'$, our objective is to find the most similar samples in the training dataset $\\mathcal{D}$ to provide contextually relevant examples to the LLM. We compute the similarity between $\\mathcal{G}'$ and each sample $\\mathcal{G}$ in $\\mathcal{D}$. The similarity score between two propagation trees $\\mathcal{G}$ and $\\mathcal{G}'$ is calculated as:\n$$\\text{similarity}(\\mathcal{G}, \\mathcal{G}') = \\frac{\\text{index}(\\mathcal{G}) \\cdot \\text{index}(\\mathcal{G}')}{\\|\\text{index}(\\mathcal{G}) \\| \\|\\text{index}(\\mathcal{G}')\\|},$$\n(3)\nwhere $\\text{index}(\\mathcal{G}) \\cdot \\text{index}(\\mathcal{G}')$ denotes the dot product of the embedding vectors, and $\\|\\cdot \\|$ denotes the Euclidean norm. We then select the top n samples with the highest similarity scores to form the set $\\mathcal{D} = \\{\\mathcal{G}_1, \\mathcal{G}_2, ..., \\mathcal{G}_n\\}$, where each $\\mathcal{G}_i \\in \\mathcal{D}$ is ranked based on its similarity to the query $\\mathcal{G}'$.\nFor each $\\mathcal{G} \\in \\mathcal{D}$, find the most similar sample with opposite label. For each sample $\\mathcal{G} \\in \\mathcal{D}$, we aim to find a sample $\\mathcal{G}^{\\text{opposite}}$ in the training dataset $\\mathcal{D}$ that has the opposite label to $\\mathcal{G}$ and maximizes the similarity to $\\mathcal{G}$:\n$$\\mathcal{G}^{\\text{opposite}} = \\underset{\\mathcal{G}' \\in \\mathcal{D}^{\\text{opposite}}}{\\text{arg max}} \\text{similarity}(\\mathcal{G}, \\mathcal{G}'),$$\n(4)\nwhere $\\mathcal{D}^{\\text{opposite}} = \\{\\mathcal{G}' \\in \\mathcal{D} | \\text{label}(\\mathcal{G}') \\neq \\text{label}(\\mathcal{G})\\}$ is the subset of $\\mathcal{D}$ containing samples with labels opposite to that of $\\mathcal{G}$.\nConstruct balanced pairs. After identifying $\\mathcal{G}^{\\text{opposite}}$ for each $\\mathcal{G} \\in \\mathcal{D}$, we construct the set of balanced pairs:\n$$\\mathcal{P} = \\{(\\mathcal{G}_1, \\mathcal{G}_1^{\\text{opposite}}), (\\mathcal{G}_2, \\mathcal{G}_2^{\\text{opposite}}), ..., (\\mathcal{G}_n, \\mathcal{G}_n^{\\text{opposite}})\\},$$\n(5)\nwhere each pair $(\\mathcal{G}_i, \\mathcal{G}_i^{\\text{opposite}})$ consists of two samples that are highly similar in terms of their embeddings but belong to opposite classes.\nBy incorporating both the most similar samples and their most similar counterparts from the opposite class, we not only mitigate the class imbalance problem but also expose the model to nuanced differences between structurally similar but differently labeled instances."}, {"title": "3.6 Detecting astroturf campaigns", "content": "In the last phase, a frozen LLM $f$ predicts whether the considered propagation tree $\\mathcal{G}'$ concerns a coordinated disinformation campaign or not. Here, any LLM following a decoder-only generative architecture can be used. Formally, let $f : \\mathcal{W}\\rightarrow \\mathcal{W}$ be a"}, {"title": "4 Experiments", "content": "Our dataset is part of the FakeNewsNet dataset and concerns the 2016 United States presidential election. Each sample consists of a post from X and is linked to fake or real news based on Politifact. Since X does not provide information about the original re-tweeting actions, we predict the propagation tress $\\mathcal{G}$ as described in Section 3.3. The final dataset includes 10228 propagation trees, where 9.7% of them are considered as coordinated political fake news campaigns. It involves 504, 085 unique users, where 7.8% of them are malicious or susceptible to astroturf campaigns. The high degree of class imbalance makes this dataset suitable for evaluating the proposed framework."}, {"title": "4.2 Setup", "content": "We perform the necessary LLM inference of our experiments using the Ollama library. Our baseline models are implemented using the PyTorch Geometric library.\nEvaluation metrics. Since it is important to identify the positive samples of a dataset (i.e., fake news campaigns), we selected precision, recall, F1 score and ROC AUC scores as our main evaluation metrics."}, {"title": "4.3 Baselines and framework variants", "content": "Baselines. To test the performance of the proposed framework, we benchmark several of its variants against a list of common baseline methods. The baseline models may differ from our variant models with regard to the architecture they follow or the type of features they consider. Additionally, they may ignore components of the proposed framework or implement them differently. The main goal of the baselines is to evaluate our framework as far as the task of identification of coordinated campaigns is concerned. Finally, we note that the baseline models follow a GNN-based architecture, as proposed in [17, 24].\nFramework variants. We present the most important framework variants used to evaluate our framework. In all cases, we use the Llama 3.1 70B model. Our framework variants differ from one another in terms of the selected prompt engineering technique and the design of the RAG component. The first variant namely, Llama 70B (0-shot), does not incorporate a RAG component and uses zero-shot prompting. The second variant, Llama 70B (6-shot), does not include a RAG component but adapts a few-shot prompting architecture, where 6 propagation trees are given as examples to the Llama model. The examples are randomly chosen. The third framework variant, Llama 70B + RAG, employs few-shot prompting along with RAG. These six examples are the most similar to the propagation tree under investigation. The final framework variant, Llama 70B + Balanced RAG, employs few-shot prompting and RAG. We strategically retrieve the three most similar positive (organic propagation trees) and the three most similar three negative (astroturf propagation trees) examples. In that way, we provide Llama with a balanced version of propagation tree examples. We refer to this variant as 'balanced RAG' (see Section 3.5). A detailed list of framework variants is also presented in Section 5 for the ablation study."}, {"title": "4.4 Evaluation results", "content": "We compared our proposed framework against traditional graph-based baseline models, including GAT, GCN, GraphSAGE, and GraphConv, to evaluate its effectiveness in detecting coordinated disinformation campaigns under extreme class imbalance conditions. As detailed in Table 2, our framework significantly outperforms these baselines across all evaluation metrics- precision, recall, F1, and ROC AUC. Specifically, the incorporation of the Balanced Retrieval-Augmented Generation (Balanced RAG) component further enhances the accuracy of our approach. For instance, our framework achieves the best performance with a recall of 0.8507, represent a 1.5\u00d7-3\u00d7 improvement over the baseline models, which suffer from the negative effects of class imbalance, exhibiting low scores across all evaluation metrics.\nConsidering the above remarks, we conclude that the combination of network-informed prompt engineering and Balanced RAG can definitely assist a frozen LLM in improving its performance with respect to the task of identifying coordinated fake news campaigns under extreme class imbalance."}, {"title": "4.5 Hyperparameter tuning", "content": "We explore the impact of different hyperparameters on the performance of the proposed framework. To speed up the process, most"}, {"title": "5 Ablation study", "content": "We now investigate the contribution of each component to the performance of the proposed framework. We run the experiments for the ablation study with Llama 3.1 70B."}, {"title": "5.1 Impact of prompt engineering and Balanced RAG", "content": "To assess the impact of the RAG component on the overall performance of the framework, we run experiments with different RAG approaches. We also conduct experiments with numerous prompt engineering techniques to evaluate their overall impact. Results are shown in Table 5. We describe each ablation study in detail below. Overall, the results indicate that combining the proposed balanced"}, {"title": "5.2 Impact of the encoded information", "content": "We now investigate the influence of the encoded information on the performance of the proposed framework. Table 6 summarizes the overall results. As illustrated, providing an LLM with both the text of the tweets and the text-encoded propagation tree produces the best outcome for the task of identifying astroturf campaigns.\nNo graph information (w/o graph). We omit the graph textual encoding part. Thus, the LLM is not aware of the structure of the propagation tree. Instead, only textual information about the tweets and re-tweets is provided. We observe a decrease in performance. For instance, the recall score drops from 0.8507 to 0.795. Similar reductions are also observed for the rest of the metrics.\nNo textual information (w/o text). In this case, we omit the textual information of the tweets of a propagation tree. We observe a significant decrease in performance. For example, the recall score"}, {"title": "6 Discussion", "content": "LLMs against astroturf campaigns (RQ1). Frozen LLMs can effectively identify organized political disinformation campaigns. Our framework leverages prompt engineering and RAG and outperforms traditional GNN-based baselines. This indicates that even without fine-tuning, LLMs are able to identify coordinated disinformation efforts, given that they are provided with the appropriate prompts and relevant contextual information.\nEfficiency of text-encoded graphs (RQ2). Encoding graph information as text has proven to be an effective strategy for detecting astoturf disinformation campaigns. By transforming the structural data of the propagation trees into texts, we allow LLMs to analyze the relationships within the network. Utilizing graph textual encoding techniques eliminates the need for complex graph-based processing and simplifies the input while retaining critical information necessary for accurate detection.\nUtilizing only prompt engineering and RAG to identify astroturf campaigns (RQ3). It is feasible to utilize only prompt engineering and RAG techniques to identify astroturf campaigns using a frozen LLM. Our experiment results verify that this approach not only simplifies the detection process by avoiding the need for extensive model training but also achieves better performance compared to traditional methods.\nPrompt engineering and RAG under class imbalance (RQ4). By incorporating RAG examples, the framework provides the LLM with relevant instances that help balance the influence of negative samples during inference. It mitigates the bias towards real news, ensuring that the model remains sensitive to astroturf campaigns.\nGNNs for modeling graph-based data (RQ5). Our framework demonstrates that encoding graph information as text and utilizing LLMs can achieve better performance under class imbalance. With appropriate prompt engineering and RAG strategies, LLMs are competitive counterparts to GNNs for graph-related tasks, particularly in scenarios where training data is imbalanced.\nImportance of the proposed approach. The proposed framework offers several key advantages over traditional disinformation detection methods. Utilizing frozen LLMs with prompt engineering and RAG eliminates the need for labeled datasets. Additionally, the ability of the framework to effectively handle class imbalance ensures its applicability in real-world settings where astroturf campaigns are rare but important."}, {"title": "7 Conclusion", "content": "In this paper, we propose a novel framework for detecting coordinated political fake news campaigns on social media platforms like X. Our approach leverages prompt engineering and retrieval-augmented generation, enabling LLMs to identify such campaigns by incorporating both graph-based and textual information from the news propagation tree. We also introduce 'balanced RAG', a novel RAG component that considers both positive and negative examples during the retrieval step. We evaluate the effectiveness of our framework and its variants on a dataset concerning the 2016 United States presidential election. The experimental results demonstrate that the proposed framework performs better compared to traditional baseline methods, even under extreme class imbalance settings. Our framework highlights the importance of combining prompt engineering and balanced RAG with frozen LLMs to fight against organized disinformation campaigns."}, {"title": "A Implementation details", "content": "The source code is available at https://github.com/nkanak/brag-fake-news-campaigns."}, {"title": "B Experimental results", "content": "We provide the numerical results for the figures of the manuscript."}, {"title": "C Evaluation metrics", "content": "In this section, we expand on the main evaluation metrics used to assess the performance of our framework. The goal of these metrics is to evaluate the framework's ability to identify coordinated disinformation campaigns effectively, particularly under extreme class imbalance."}, {"title": "C.1 Precision", "content": "Precision is the fraction of correctly predicted positive cases out of all cases predicted as positive by the model. In the task of disinformation detection, precision is crucial because it measures the system's ability to minimize false positives (i.e., legitimate content incorrectly identified as part of a fake news campaign). High precision indicates that the model rarely misclassifies negative examples as positive. Mathematically, precision is defined as:\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n(7)\nwhere TP is the number of true positives (correctly identified disinformation campaigns) and FP is the number of false positives (legitimate content wrongly classified as disinformation)."}, {"title": "C.2 Recall", "content": "Recall is the fraction of true positive cases correctly identified by the model from the total actual positive cases. This metric is particularly important in disinformation detection tasks, where identifying false negatives (missed fake news campaigns) is critical. It is defined as:\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n(8)\nwhere TP is the number of true positives and FN is the number of false negatives. A high recall indicates that the model successfully identifies a large portion of positive examples."}, {"title": "C.3 F1 score", "content": "The F1-score provides a harmonic mean between precision and recall, balancing the trade-off between false positives and false negatives. In the context of disinformation detection, this metric helps determine the overall accuracy of the model, accounting for both the sensitivity to true positives and the specificity to false positives. The F1-score is given by:\n$$F1 = \\frac{2 \\times TP}{2x TP + FP + FN}$$\n(9)\nThis score is particularly useful in our case, where the dataset suffers from class imbalance, ensuring that both precision and recall are equally weighted."}, {"title": "C.4 ROC AUC score", "content": "The Receiver Operating Characteristic (ROC) curve is a plot that illustrates the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under this curve (AUC) measures the overall ability of the model to distinguish between positive and negative classes. The ROC AUC score is computed as:\n$$AUC = \\int_0^1 TPR(FPR)d(FPR)$$\n(10)\nwhere TPR is the true positive rate, and FPR is the false positive rate. A higher AUC indicates a better model performance in distinguishing between classes."}, {"title": "C.5 False positive rate (FPR)", "content": "The false positive rate is the ratio of incorrectly predicted positive instances among all actual negative instances. In the disinformation detection task, a high FPR would mean that legitimate content is mistakenly classified as part of a coordinated campaign, which can undermine the reliability of the system. FPR is calculated as:\n$$FPR = \\frac{FP}{FP + TN}$$\n(11)\nwhere FP is the number of false positives and TN is the number of true negatives. We aim to minimize this rate to reduce the impact of false alarms."}, {"title": "C.6 Class imbalance and metric interpretation", "content": "Due to the extreme class imbalance inherent in disinformation datasets, with coordinated disinformation campaigns forming a small fraction of the overall content, metrics like accuracy become less informative. Therefore, we focus on recall, F1-score, ROC AUC, and FPR, as these provide a clearer picture of the model's performance in identifying the minority class (disinformation campaigns)."}, {"title": "C.7 Metric performance in our framework", "content": "Our proposed framework outperforms traditional baselines, achieving high recall and F1 scores, even under significant class imbalance, as detailed in Section 3.4. The combination of network-informed prompt engineering and retrieval-augmented generation (RAG) allows our framework to maintain robust performance across all metrics without requiring additional training. The table in Section 3.4 showcases the full set of results for each metric."}, {"title": "D Prompt examples", "content": "In this section, we present a list of prompt examples using different prompting and RAG techniques. The list of prompting techniques includes zero-shot, few-shot and chain-of-thought prompting. These examples were retrieved during our experimentations."}, {"title": "D.1 Zero-shot prompting", "content": "Below is an example of zero-shot prompts used in the framework.\nPlease analyze the following text and its associated\npropagation chain that spreads on the internet\nlike a tree-based graph"}]}