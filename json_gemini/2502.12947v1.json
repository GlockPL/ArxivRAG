{"title": "Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models", "authors": ["Gyeongman Kim", "Gyouk Chu", "Eunho Yang"], "abstract": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years. However, their high memory requirements prevent their use in resource-constrained environments. While knowledge distillation (KD) has been a proven method for model compression, its application to MoE teacher models remains under-explored. Through our investigation, we discover that non-activated experts in MoE models possess valuable knowledge that benefits student models. We further demonstrate that existing KD methods are not optimal for compressing MoE models, as they fail to leverage this knowledge effectively. To address this, we propose two intuitive MoE-specific KD methods for the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR), both designed to effectively extract knowledge from all experts. Specifically, KA augments knowledge by sampling experts multiple times, while SAR uses all experts and adjusts the expert weights through router training to provide optimal knowledge. Extensive experiments show that our methods outperform conventional KD methods, demonstrating their effectiveness for MoE teacher models.", "sections": [{"title": "1 Introduction", "content": "Mixture-of-Experts (MoE) architecture (Jacobs et al., 1991; Shazeer et al., 2017) is one of the major contributing factors to the rapid advancements of Large Language Models (LLMs) (Jiang et al., 2024; Team, 2024; Liu et al., 2024). It allows the model to scale up while effectively improving the computational cost by utilizing only a subset of multiple experts during inference. Despite the advantages afforded by MoE architectures in scaling model capacity, several limitations persist. One such challenge is that it requires significant GPU memory compared to the dense model due to a number of non-active parameters. For this reason, the practical application of MoE models in resource-limited environments is generally challenging. Hence, research into effectively compressing recent large-scale MoE models becomes imperative, particularly for deployment in resource-constrained environments.\nOne of the notable compression techniques is knowledge distillation (KD) (Hinton, 2015). To facilitate student learning under teacher guidance, both the approach of using the teacher's output as supervised data (Kim and Rush, 2016; Peng et al., 2023; Fu et al., 2023) and the method to match the teacher's distribution with appropriate objective functions are widely adopted and actively researched. Specifically, concerning the second method, many works have focused on designing suitable objective functions (Wen et al., 2023; Ko et al., 2024; Agarwal et al., 2024; Wu et al., 2024) or on using student-generated output (Lin et al., 2020; Gu et al., 2024; Agarwal et al., 2024). Indeed, several models have successfully employed KD in practice, such as Phi (Abdin et al., 2024) and Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024).\nHowever, there has been no systematic development of KD methods specifically designed for the MoE teacher. Recent KD studies have largely overlooked scenarios where the model to be compressed is based on the MoE structure. While a few studies have applied KD to MoE teacher models (Artetxe et al., 2021; Fedus et al., 2022; Xue et al., 2022), they have used the conventional KD and have not thoroughly explored the effectiveness or challenges of distilling knowledge from MoE. Therefore, these generalized approaches might not fully exploit the potential of MoE as a teacher.\nIn this paper, we introduce MoE-specific knowledge distillation, which can effectively distill knowledge from the MoE teacher. To design such a specialized mechanism, we first conduct an in-depth analysis of MoE teacher during the basic KD process proposed by Sanh (2019). We found that even non-selected experts have a significant amount of potentially useful knowledge, which remains unutilized. Inspired by this observation, we propose two different intuitive solutions for effectively extracting knowledge from all experts (see Figure 3). The first method, knowledge augmentation (KA), employs sampling multiple times to decide which experts to activate based on their gate probabilities. Through this approach, a student can be provided with a variety of augmented knowledge from a single input data. The second method, student-aware router (SAR), optimizes the router based on student feedback before distillation, enabling the router to determine optimal weights to aggregate knowledge from all experts.\nWe apply our two approaches to Llama-MoE (Zhu et al., 2024) models with five instruction datasets. Considering the common practice of employing KD in memory-constrained settings, we utilize a dense student Sheared-Llama (Xia et al., 2023) rather than a MoE student. The experimental results show that when the teacher model is MoE, our method consistently outperforms the existing KD baselines. Additionally, the analysis of KA confirms that having a moderate amount of augmented knowledge is indeed beneficial. Moreover, in SAR, we confirm that router updates in fact induce subtle changes in gate values, and these changes demonstrably enhance the performance of KD.\nTo summarize, our contributions are three-fold:\n\u2022 We empirically found that non-activated experts from MoE teacher also possess knowledge that is of great benefit to a student, yet remains unexploited by existing methods.\n\u2022 We propose two novel methods, knowledge augmentation (KA) and student-aware router (SAR), effectively utilizing the distributed knowledge from the entire experts. To the best of our knowledge, these are the first KD methods specifically designed for MoE teacher.\n\u2022 We evaluate our framework on 5 instruction-following datasets. The result shows that K\u0391 and SAR outperform the existing KD methods, underscoring the effectiveness and importance of leveraging the architectural characteristics of MoE teacher."}, {"title": "2 Related Works", "content": "Knowledge distillation Knowledge distillation (KD) (Hinton, 2015) is a prevalent model compression technique, transferring knowledge from a large teacher model to a small student model. Most of the early works focused on applying KD to the text classification tasks by imitating all the possible things of the teacher model, from output distribution (Song et al., 2020; Liang et al., 2020) to hidden states (Jiao et al., 2020; Sun et al., 2019; Park et al., 2021b), attention scores (Wang et al., 2020), and so forth. However, these methods relied on a fixed teacher that generates knowledge without being aware of the student's learning characteristics, which often limited its effectiveness. Thus, several methods are also devised to provide student-friendly knowledge (Park et al., 2021a; Zhou et al., 2022; Ren et al., 2023).\nOn the other hand, various studies are actively examining its application to text generation tasks. The standard KD method minimizes the forward KL divergence between the output distributions of student and teacher at each time step (Sanh, 2019) or directly trains the student with the generated text from the teacher (Kim and Rush, 2016; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023). Recently, MiniLLM (Gu et al., 2024) explores a method to mix the distribution of the teacher with that of the student and use a policy gradient approach by optimizing the reverse KL divergence. GKD (Agarwal et al., 2024) utilizes the student-generated on-policy data to receive feedback from the teacher with a generalized Jensen-Shannon (JS) divergence objective. DistiLLM (Ko et al., 2024) applies skew KL divergence with their proposed adaptive off-policy mechanism. Although these methods have shown remarkable results, all of the experiments have used dense models, and whether they also show good results for distilling the Mixture-of-Experts model has not yet been studied.\nMixture-of-Experts Mixture-of-Experts (MoE) (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022) is an efficient way to increase the model size by replacing the feed-forward network (FFN) with multiple experts and a gating network. It dynamically activates different experts for each input token instead of using all parameters. Since it has been known that MoE provides advantages including more efficient training (He et al., 2022; Gale et al., 2023) and faster inference"}, {"title": "3 Preliminary", "content": "3.1 Knowledge Distillation\nKD minimizes the token-level distributional discrepancy between teacher and student. A standard approach to accomplish this minimization in the instruction-following setting is using the forward KL divergence (Sanh, 2019):\n$\\mathcal{L}_{KD} = D_{KL} (p_{\\theta}(y|x) || q_{\\phi}(y|x))$,\nwhere (x, y) \u2208 D, D denotes a dataset. \u00e6 and y represent the request and response, respectively, and this objective guides the student to learn by minimizing the distributional discrepancy in the only response part. p and qe denote the probability distributions of the teacher and student, respectively.\nRecently, MiniLLM (Gu et al., 2024) and GKD (Agarwal et al., 2024) suggest using reverse KL divergence and student-generated sequences to address the exposure bias problem. The objective reflecting these is as follows:\n$\\mathcal{L}_{student} = D_{KL} (q_{\\theta}(y|x) || p_{\\theta}(y|x))$,\nwhere (x,\u00b7) \u2208 D and y ~ qe(\u00b7|x).\n3.2 Mixture-of-Experts\nThe sparse MoE layer consists of N expert networks {E1,\uff65\uff65\uff65, EN} and a router network G. The router first computes the gate logits H(x) \u2208 RN for a single token representation x, which determines the likelihood of selecting each expert. After normalization with a softmax function, top k experts are selected based on this distribution, and their outputs are aggregated through a weighted sum. In this work, we only focus on the noisy Top-k gating introduced by Shazeer et al. (2017). This gating adds trainable Gaussian noise before Top-k experts selection. The process can be described as follows:\n$\\mathcal{H}(x)_i = (x \\cdot W_g)_i + StandardNormal() \\cdot Softplus((x W_{noise})_i)$,\n$\\mathcal{G}(x) = Softmax(KeepTopK(\\mathcal{H}(x), k))$,\n$y = \\sum_{i=1}^{N} \\mathcal{G}(x)_i E_i(x)$,\nwhere G(x)i denotes the probability of ith experts being selected and\n$KeepTopK(v, k)_i = \\begin{cases}v_i & \\text{if } v_i \\in TopK(v, k),\\\\ -\\infty & \\text{otherwise}.\\end{cases}$\nThe intrinsic characteristic of Top-k routing may lead to a scenario where certain experts are always favored in the selection process. In order not to negate the potential benefits of the MoE, distributing the workload across multiple experts to ensure their collective engagement is essential, which is called load balancing. The noise term in H(x) or the auxiliary loss as in Eq. (6) helps prevent the model from always relying on the same few experts, allowing a more balanced distribution of workload among experts. The auxiliary loss (Zhu et al., 2024) is as follows:\n$\\mathcal{L}_\\text{b} = CV(m)^2 + CV(P)^2$,\nwhere m \u2208 RN represents the set of token counts assigned to each of the N experts within a batch, and P\u2208 RN denotes the set of summed probabilities assigned to each expert in the batch. The coefficient of variation (CV) is defined as the ratio of the standard deviation o to the mean \u00b5, i.e., CV(x) = \u03c3(x)/\u03bc(x). Minimizing this encourages a more uniform distribution, which is desirable for balancing the expert load."}, {"title": "4 Method", "content": "4.1 Motivation\nTo investigate how the MoE teacher distills the knowledge during the classical KD process, we first analyze the distribution of gate probabilities. The gate probability refers to the normalized values of the gate logits H through the softmax function. The Top-k experts are selected based on these gate logits, and gate logits are also used to compute the weights during the weighted summation of expert outputs. Therefore, the gate probability can be considered an indicator of how useful each expert is. In this section, we use Llama-MoE (Zhu et al., 2024) models as teachers and do the conventional KD (Sanh, 2019) into Sheared-Llama (Xia et al., 2023) model which is a dense model. The training data is a subset of Dolly (Conover et al., 2023), and we evaluate our model on five instruction datasets. For further details, please see the Section 5.1.\nFigure 1 presents a visualization of the average of the sum of gate probabilities for used experts and that for unused experts in each layer across all training data during distillation. As shown in Figure 1, the sum of gate probabilities for the group of activated experts never exceeds 50% in most of the layers of all models. Although this may be an effect of the auxiliary loss for load balancing, considering that gate probability reflects how useful an expert is, it implies that a significant portion of potentially valuable knowledge from non-activated experts is not being leveraged. Thus, effective extraction and utilization of this unexploited knowledge could bring additional benefits to the student model during the distillation process, as more diverse and complementary knowledge would be incorporated into the learning.\nTo reflect this observation, we simply increase the number of selected experts k during the distillation process. Using the Llama-MoE-3.5B (4/16) model as the teacher model, we perform knowledge distillation by gradually increasing k from 4 to 16 and evaluate the performance of both the teacher and student models. The model performance is measured by the average ROUGE-L scores across five instruction-following datasets (Section 5.1 for more details). The results are shown in Figure 2. Based on the results, we observe that using more experts does not necessarily increase the performance of the teacher, but it certainly increases the performance of the student, except when all are used. This suggests that the improvement in the student's performance is not directly due to the teacher's performance enhancement. Nevertheless, we observe that using most of the non-activated experts proves to be practically beneficial for the student, and this implies that non-activated experts hold valuable knowledge. The reason for this could be that during the MoE training process, due to load balancing, different sets of experts are activated for the same input data, causing the knowledge to be distributed across multiple experts. However, conventional KD typically relies on using only the Top-k experts, which fails to account for this.\nTherefore, the core challenge in knowledge distillation for MoE teacher lies in effectively extracting and transferring the knowledge that is distributed across all experts to empower student learning. Successfully addressing this challenge is the key to fully leveraging the architectural characteristics of MoE teacher models in guiding student models."}, {"title": "4.2 Knowledge Augmentation", "content": "The first method to effectively utilize distributed knowledge across all experts is the knowledge augmentation (KA). Following the previous observation, we use N \u2013 1 experts for each layer where N is the total number of experts. Specifically, in each MoE layer, N \u2013 1 experts are selected by sampling from a gate probability distribution with probability \u03bb. Therefore, by selecting the Top N \u2013 1 experts with probability 1 \u2013 \u03bb, we can consistently generate knowledge that is similar to the Top-k selection while incorporating slightly different knowledge. This strategy allows the augmentation of diverse knowledge and balances the trade-off between consistency and diversity of knowledge with parameter \u03bb. The formulation of KA is as follows:\n$\\mathcal{E} = \\begin{cases}\\text{Sampled } N-1 \\text{ experts }& \\text{w.p. } \\lambda,\\\\ \\text{Top } N-1 \\text{ experts }& \\text{w.p. } 1 - \\lambda,\\end{cases}$\n$\\mathcal{KA}(v, \\mathcal{E})_i = \\begin{cases}v_i & \\text{if (ith expert)} \\in \\mathcal{E},\\\\ -\\infty & \\text{otherwise},\\end{cases}$\n$\\mathcal{G}^{\\mathcal{KA}}(x) = Softmax(\\mathcal{KA}(\\mathcal{H}(x), \\mathcal{E}))$,\nwhere E denotes the set of selected experts.\nIn each iteration, the teacher is forwarded M times for the same input using the KA method, augmenting M pieces of knowledge, which are transferred to the student. Following GKD (Agarwal et al., 2024), the response part y of the input is generated by the student, treating it as a pseudo-target, to mitigate exposure bias (Arora et al., 2022). Furthermore, the student's learning objective is the reverse KL divergence. We summarize the entire KA procedure in Algorithm 1."}, {"title": "4.3 Student-Aware Router", "content": "The second method is the student-aware router (SAR). Instead of merely selecting which experts to use, SAR takes a step further by directly optimizing the router to achieve an optimal weighted sum across all expert outputs. Inspired by the concept of student-friendly knowledge distillation, SAR updates the teacher's router using student feedback, ensuring that the generated knowledge is more useful to the student.\nSAR undergoes two stages in each iteration: router update and knowledge distillation. First, the router weights, Wg and Wnoise in Eq. (3), are optimized using student feedback (Kim et al., 2024) and auxiliary loss for load balancing. Only the router components of the MoE teacher are updated, while all other parameters remain fixed. After updating the router, the modified router is used to generate knowledge, which is then distilled into the student. At this stage, all experts are activated, and their outputs are aggregated through a weighted sum based on the modified router.\nSimilar to KA, SAR also uses pseudo-targets generated by the student and trains the router using reverse KL divergence:\n$\\mathcal{L}_{SAR} =DKL(P(y|x) || q_{\\theta}(y|x)) + \\beta \\mathcal{L}_\\text{b}$.\nHere, \u1e9e is a coefficient for the auxiliary loss, which is set to 0.01 following the teacher model (Zhu et al., 2024). The entire SAR process is summarized in Algorithm 2."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nSettings Following Gu et al. (2024), databricks -dolly-15k (Conover et al., 2023) is partitioned into 14k samples for the training set, 500 samples for the validation and test sets, respectively. In addition to the test set of Dolly, we evaluate 4 extra instruction-following datasets: SelfInst (Wang et al., 2023), 252 user-oriented instruction-following samples, Vicuna (Chiang et al., 2023), 80 questions used in the Vicuna evaluation, S-NI, 9k samples from the test set of SUPERNATURALINSTRUCTIONS (Wang et al., 2022), and UnNI, randomly sampled 10k samples from the core set of UNNATURALINSTRUCTIONS (Honovich et al., 2023). We adopt the ROUGE-L (Lin, 2004) score as the evaluation metric.\nModels To verify the effectiveness of proposed KD methods tailored for MoE, we need to compare the performance of KD from dense to dense with that from MoE to dense. For this comparison to be fair, dense teacher and MoE teacher should have comparable performances. This ensures that any performance improvements can be directly ascribed to the proposed method rather than the teacher's inherent capability. Additionally, the tokenizers of both models must be the same to compare token-level distributions.\nTo satisfy the above critical conditions, we use three Llama-MoE (Zhu et al., 2024) models as the MoE teachers, Sheared-Llama (Xia et al., 2023) 2.7B as the dense teacher, and Sheared-Llama 1.3B as the dense student. Sheared-Llama 2.7B exhibits comparable performance to Llama-MoE model, with a lower number of activated parameters. Both teacher models and the student model were initially fine-tuned with the Dolly training set before knowledge distillation, following the previous works (Agarwal et al., 2024; Gu et al., 2024).\nBaseline We compare our two approaches with three baselines: (1) supervised fine-tuning (SFT) directly fine-tunes the model on golden responses, which does not involve knowledge distillation; (2) KD (Sanh, 2019) uses the teacher's distribution with forward KL divergence; (3) GKD (Agarwal et al., 2024) uses the mixture of fixed data and on-policy student-generated outputs. Despite recent advancements and variants, GKD remains a representative study utilizing KL divergence or its variants and student-generated outputs, making it a suitable baseline for our experiment. Based on their reported performance, GKD computes reverse KL divergence with only student-generated outputs in this paper. For our methods, we set a sampling ratio x = 0.05 and the number of augmented samples M = 2 in the KA method. To validate our observation on the MoE teacher, we exclude the router update stage from SAR and simply activate all experts. This approach is referred to as ALL. Further details on the experimental setup are summarized in the Appendix A."}, {"title": "5.2 Results", "content": "We present the results of KA and SAR with baselines on 5 datasets in Table 1.\nFirst, when comparing the SFT results of three Llama-MoE models, the performance is better when there are more activated experts with the same total number of experts. Also, if the total activated parameters are similar, the performance is also comparable. Note that the dense teacher Sheared-Llama-2.7B indeed shows a similar performance compared to MoE teachers.\nSecond, we compare the performance between dense and MoE teachers for the two baselines, KD and GKD. Surprisingly, despite the MoE teacher having performance that is similar to or even slightly better than the dense teacher, both methods demonstrate that the dense model serves as a better teacher for the student. For KD, the student trained by the dense teacher always outperforms the student trained by the MoE teachers. This holds true under GKD as well, except for the Llama-MoE-3.5B (4/16) case. These results highlight that the existing KD methods are not optimized for extracting knowledge from the MoE teacher.\nThird, our proposed methods, knowledge augmentation and student-aware router, achieve higher performance than baselines when the teacher model is MoE. This result highlights that both methods are specifically designed for the MoE teacher. Therefore, when the teacher model is MoE, it is important to carefully consider the architectural characteristics of MoE and effectively extract knowledge that is distributed across all experts.\nLastly, the ALL approach, which simply activates all experts, outperforms the baselines in most cases but falls short of our proposed methods. This result aligns with the observation in Section 4.1, suggesting that while non-activated experts contain useful knowledge, simply utilizing all of them may not be the optimal strategy. Furthermore, the comparison with SAR demonstrates the effectiveness of router updates.\nThe qualitative results of our methods and the baselines are summarized in Appendix C, demonstrating that our methods produce responses most closely resembling the ground truth."}, {"title": "5.3 Analysis", "content": "Hyperparameters in KA We ablate various values of M, the number of augmented samples in KA. Figure 4 shows the performance for different numbers of samples, M. It indicates that the optimal M value varied across different models. Nevertheless, the appropriate value of M generally leads to beneficial augmentation. However, when M is excessively large, performance consistently degrades across all models. This is because too large values can lead to the generation of overly diverse knowledge for identical input due to the inherent randomness of sampling. Consequently, such excessive diversity can be detrimental to the overall performance, as it may introduce nonsense or unhelpful knowledge.\nWe also ablate various values of \u03bb, the probability of randomly sampling experts. The results are in Appendix B.\nShift of gate probability in SAR In Table 1, we compared the results of ALL and SAR and verified that training the routers of MoE teacher is indeed helpful. For a more rigorous analysis, we examine the changes in the gate probability distribution that occurred as the router network learned to be more student-aware.\nFigure 5 presents the layer-wise KL divergence of gate probabilities between the original teacher MoE and the teacher whose routers are trained with SAR. For all tokens of the training data, the maximum and average values are shown. For every teacher model, KL divergence increases with greater layer depth. The reason is that by learning the router in a student-friendly way, the modified gate probability affects the representation of the layer immediately following. This effect accumulates so that later layers have more different gate probabilities than the existing router. Eventually, these changes in gate probability have led to a more effective knowledge delivery to the student."}, {"title": "6 Conclusion", "content": "In this paper, we first observe that non-activated experts in MoE teachers contain valuable knowledge that can benefit the student model. Based on this observation, we assert that existing KD methods are suboptimal for distilling MoE models, as they do not fully utilize all experts. To address this issue, we propose two MoE-specific KD methods for the first time: knowledge augmentation and student-aware router. Our experimental results show that our methods outperform the baseline, clearly demonstrating the effectiveness of our approach in leveraging the full potential of MoE teacher models."}, {"title": "Limitations", "content": "We acknowledge the limitations arising from the rigorous experimental conditions. In addition to the common yet imperfect situation where teacher and student must use the same tokenizer, dense teacher and MoE teacher should have comparable performances. This condition is necessary to show that our method is an effective KD specialized for MoE. However, it is difficult to find a setup that satisfies these conditions other than the setting that we used in our experiment (Llama-MoE (Zhu et al., 2024) for the teacher and Sheared-Llama (Xia et al., 2023) for the student). We leave this for future work to explore, in conjunction with either emerging new methods (Boizard et al., 2024; Zhang et al., 2024) or by combining our method with existing ways (Xue et al., 2022)."}, {"title": "A Experimental Setup Details", "content": "For training, we utilize the AdamW optimizer (Loshchilov, 2017) with a batch size of 16. The learning rates for both the router and student models are set to 1e-5, and training is conducted for 10 epochs. The training and generation processes are conducted with a maximum sequence length of 512 and a maximum request length of 256. During generation, we apply top-k and top-p sampling with values of 0 and 1.0, respectively, while maintaining a fixed temperature of 1.0. All experiments in this study are conducted on 4 Intel Gaudi v2 accelerators using SynapseAI 1.18.0.\nTo ensure consistency in instruction-following tasks, all datasets are pre-processed by converting instruction-response pairs into a standardized sentence structure, following the approach used in previous studies (Gu et al., 2024). Model evaluation is performed using the ROUGE-L score (Lin, 2004), which has been shown to correlate well with human preferences in instruction-following assessments (Wang et al., 2022). The best-performing checkpoint on the validation set, determined by the ROUGE-L score, is selected for final evaluation. All evaluations are performed across five different random seeds, and the reported results reflect the average performance."}, {"title": "B Effects of A in KA", "content": "Table 2 shows the performance for different values of \u03bb, which represent the probability of sampling experts. In this experiment, we use the Llama-MoE-3.5B (4/16) model as a teacher and fix the value M, the number of augmented samples, as 2. The result indicates that too large \u00e0 leads to performance degradation. This result is similar to the pattern observed in Figure 4, likely due to the analogous reason. In other words, the proper value of A generally makes augmentation helpful, whereas the excessive value of A compromises the knowledge."}, {"title": "C Qualitative Results", "content": "For the qualitative results, we present samples generated by student models trained using various methods. The samples are drawn from the S-NI dataset and utilize LLaMA-MoE-3.5B (4/16) as the teacher model, with Sheared-LLaMA-1.3B employed as the student model. Results are shown in Table 3. It is shown that our proposed methods generate responses most similar to the ground truth."}]}