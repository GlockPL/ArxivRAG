{"title": "Transformer-based Language Models for Reasoning in the Description Logic ALC Q", "authors": ["Angelos Poulis", "Eleni Tsalapati", "Manolis Koubarakis"], "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order logic sentences with only a few logical operators and quantifiers. We construct the natural language dataset, DELTAD, using the expressive description logic language ACCQ. DELTAD comprises 384K examples and increases in two dimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we systematically investigate the logical reasoning capabilities of a supervised fine-tuned DeBERTa-based model and two large language models (GPT-3.5, GPT-4) with few-shot prompting. We show that the DeBERTa-based model fine-tuned on our dataset can master the entailment checking task. Moreover, the performance of GPTs can improve significantly even when a small number of samples is provided (9 shots). We open-source our code and datasets.", "sections": [{"title": "Introduction", "content": "Description Logic (DL) languages (Baader et al., 2003) are fragments of first-order logic (FOL) that have evolved into one of the main formalisms for the representation of conceptual knowledge in a precise and well-defined manner. An expressive and decidable DL language that supports existential, universal, and numerical constraints besides the standard Boolean operators is ALCQ. For instance, in ALCQ one can formally express sentences like the ones appearing in Fig. 1.\nThe formal apparatus of DLs allows us to perform deductive reasoning tasks, such as entailment checking, i.e., deciding whether a sentence or a set of sentences, logically implies another. Recent advancements in transformer-based language models (TLMs) have sparked new research into whether TLMs can learn to perform such tasks over contexts expressed in natural language (Clark, Tafjord, and Richardson, 2020; Han et al., 2022; Tang et al., 2023; He et al., 2023). However, in most cases, the contexts used were either"}, {"title": "Background on Description Logics", "content": "We can use ALC Q (Baader et al., 2003) to represent knowledge about a domain by defining three types of entities: individuals (e.g., John), concepts (e.g., Postdoc, i.e., the concept describing the entities that are postdocs) and roles (e.g., teaches). A concept expression C can be formed using these entities, Boolean constructors (\u03a0, \u222a, \u00ac), quantifiers (\u2200, \u2203), and number restrictions (<, >) recursively as follows:\n C,D := A |T|\u22a5|\u00acC|C\u03a0D|C\u222aD|\u2200R.C | \u2203R.C |\u2265 nR.C |\u2264 nR.C, where A is an atomic concept, R an atomic role, T the top concept, which has every individual as an instance, and the dual of T. In this way, one can represent formally complex concept expressions, such as all entities that \u201chave a Ph.D., teach at most two postgraduate courses and are not academics\" (\u2203hasDegree.PhD < 2teaches.PostgrCourse \u25a1 \u00acAcademic). Subsumption axioms in ALCQ have the form C \u2291 D and describe relationships between concept expressions. For example, one can describe formally that all postdocs are described by the aforementioned concept as Postdoc \u2291 \u2203owns.PhD < 2teaches.PostgrCourse\u220f\u00acAcademic. We denote with LHS (left-hand side) the concept expression that appears on the left of the subsumption symbol (\u2291) in a subsumption axiom and with RHS (right-hand side) the concept expression that appears on the right. Assertional axioms or, simply, facts describe knowledge about named individuals, i.e., that are instances of some concept (expression) and have the form C(a) or R(a, b), where a, b individuals. Using complex expressions one can construct very complex facts. An ALCQ knowledge base (KB) is a set of subsumption axioms and a set of facts.\nDelta-closure(K, t) of a KB K is the set of subsumption axioms and facts that are inferred from K within time t seconds using the InferredOntologyGenerator"}, {"title": "Dataset Generation", "content": "We investigate the ability of transformers to perform textual entailment checking over ALCQ KBs expressed in natural language with respect to two dimensions: i) the depth D of the sentences (i.e., subsumption axioms/facts in question), henceforth mentioned as queries, with respect to the corresponding KB, ii) the linguistic complexity level L (defined in Section 3.1) of the knowledge required to answer the queries. To achieve this, each example in the dataset DELTAD is a 5-tuple (T, Q, A, D, L), where T is the context containing ALCQ axioms (subsumption axioms/facts) expressed in natural language, Q the query expressed in natural language, henceforth mentioned as question, A is the answer which can be either true, false, or unknown, and D the depth of Q, if A is true or false, otherwise it is denoted as na. L is the linguistic complexity of the KB\nThe pipeline for the generation of the dataset is presented in Fig. 2. For the generation of an example (described in detail in Section 3.1) of linguistic complexity level n (L < n) and depth m (D < m), we first generate a KB K using a specially crafted probabilistic context-free grammar (denoted in Fig. 2 with ALC Q-n PCFG) for producing subsumption axioms and facts of maximum linguistic complexity n. Then, Delta-closure(K, t) is calculated for t = 5 sec. KBs that required more than 5 seconds to calculate the Delta-closure, were discarded.\nFrom the Delta-closure we calculate, as described in Section 3.2, true (answer=true), false (answer=false) and unknown (answer=unknown) queries, which eventually will formulate the sentences in question. A KB is kept only if it can produce queries with all three types of answers at all depths up to m, otherwise a new one is generated. Once this process is completed, the generated queries (subsumption axioms/facts) along with the original K, are translated into natural language statements Q and into the context T, respectively, by utilizing a set of natural language templates, as described in Section 3.3."}, {"title": "KB Generation", "content": "To create diverse contexts and to avoid overfitting to a specific vocabulary, we have defined two different pools of terms, Pool A and Pool B. Pool A contains 14 atomic concepts, 5 roles, and 8 individuals, mostly taken from Rule-Taker dataset (Clark, Tafjord, and Richardson, 2020)(in RuleTaker the subsumption axioms are simple conjunctive implications, where the concept names are named \"attributes\", the roles \"relations\" and the individual names \"entities\"). Pool B contains 8 atomic concepts, 8 roles, and 8 individuals. Both pools can be found in A.1.\nFrom each pool, we generate 20 datasets (40 in total) of 1000 KBs each, of various inference depths and axiom lengths.\nTo obtain KBs of different linguistic complexity levels, we have manually crafted four types (L = 0, 1, 2, 3) of PCFGs, based on the number of constructors and quantifiers appearing in their axioms. In general, a concept of linguistic complexity L contains L Boolean constructors and at most L + 1 quantifiers.\nAn L-type PCFG produces KBs of linguistic complexity level L with axioms that their one side (e.g., LHS) is of linguistic complexity L and their other side (e.g., RHS) of at most L 1, but also contains simpler axioms, of smaller linguistic complexity levels. Specifically, A KB of level:\n\u2022 L = 0 contains axioms of the form Level\u2080 \u2291 Level\u2080\n\u2022 L = 1 contains axioms of the form Level\u2080 \u2291 Level\u2081, Level\u2081\u2291 Level\u2080, Level\u2081 \u2291 Level\u2081,\n\u2022 L = 2 contains axioms of the form Level\u2080 \u2291 Level\u2082, Level\u2081 \u2291 Level\u2082, Level\u2082\u2291 Level\u2080, Level\u2082 \u2291 Level\u2081, Level\u2082 \u2291 Level\u2082,\n\u2022 L = 3 contains axioms of the form Level\u2080 \u2291 Level\u2083, Level\u2081 \u2291 Level\u2083, Level\u2082 \u2291 Level\u2083, Level\u2083 \u2291 Level\u2080, Level\u2083 \u2291 Level\u2081, Level\u2083 \u2291 Level\u2082.\nFor instance, KBs of level L = 0 contain only very simple facts or subsumption axioms that do not contain any Boolean constructors but can contain one quantifier, such as Enthusiastic \u2291 \u2203supports.Enthusiastic (translated in NL as \"Enthusiastic people support someone enthusiastic\"), but KBs of level L = 3 can contain subsumption axioms as complex as the first one appearing in Fig. 1.\nIt is important to discern the notion of linguistic complexity of a sentence from its length. We do not focus here only on sentences that contain, for instance, multiple conjunctions but rather on sentences with a more complex structure (with quantifiers as well), leading to increased linguistic complexity.\nTo keep the KBs processible by the reasoners, the subsumption axioms can contain up to seven atomic concepts and up to two nested quantifiers (e.g., \u2203likes.(\u2203loves.Cat)), which describes the entities that like some entity that loves some cat). All KBs are rather small (with a minimum of 3"}, {"title": "Query Generation", "content": "For an inference depth D, a true query q is an axiom or fact selected from the Delta-closure of a consistent K, such that depth(q, K) = D. An unknown query (answer=unknown) is generated by creating a random fact or statement (using the corresponding PCFG) such that it does not belong to the Delta-closure of K and is consistent with K. A false query (answer=false) can be generated in three ways:\n\u2022 From an inconsistent K: for every a \u2208 K if K \\ {a} is consistent then a is a false query over the KB K\\ {a}.\n\u2022 From a consistent K: i) By negating a true query q with depth(q, K) = D (and applying De Morgan's laws). ii) By automatically generating an appropriate axiom or fact a such that KU {a} is inconsistent and depth(a, K) = D.\nFor instance, suppose that a KB K\u2081 contains the axioms (Vadmires.)(Anne) and Vadmires.\u22a5 \u2291 \u2200likes.Quiet which in natural language are translated into: \"Anne admires none\", \"All people that admire none like only quiet people\". Then, the fact (\u2203likes.\u00acQuiet)(Anne) stating that \"Anne likes someone who is not quiet\" forms a false query for K.\nThe disadvantage of the first approach is that it requires calling the reasoner multiple times, a time-consuming process, especially in KBs with long axioms (e.g., L=3 KBs). Hence, we used the two latter approaches.\nWe set the reasoning depth limit to five (i.e., D = 0, 1, 2, 3, 5) following the literature (Clark, Tafjord, and Richardson, 2020). Additionally, extending this further would require longer times for the dataset generation."}, {"title": "Data Translation to NL", "content": "The KBs and queries were translated to NL with the use of templates. The templates were created based on the user-friendly Manchester syntax for ALCQ (Horridge et al., 2006). Following this syntax, the intersection (\u03a0) and union (\u222a) operators, are translated as \"and\" and \"or\", respectively, the existential (\u2203) quantifier is translated as \"someone\u201d or \"something\" (depending on whether the pool is about people or things), the universal (\u2200) as \"only\", and the number restrictions <, > as \"at most\" and \"at least\". Also, we use the word \"that\" for intersections and nested quantifiers. For instance, the fact (\u2203likes.(\u2200likes.Kind))(Bob) is translated as \"Bob likes someone that likes only kind people\".\nFollowing the template-based approach suggested by Tafjord, Dalvi, and Clark (2021), the axioms of the form C \u2291 D are, roughly, translated into NL in four different ways: i) \"If C then D\u201d; ii)\u201cPeople/Things that are C' are D\u201d, iii)\"All people/things that are C are D\"; iv) If C = T and D = \u2200R.C' this is translated as \"Someone/something can R only people/things that are C'\u201d. A fact C'(a) is translated as \"a is C\". To ensure that the resulting NL sentences are grammatically correct we have used a grammar checker"}, {"title": "The Dataset DELTA D", "content": "At the end, the examples of the same depth and level from both pools are merged. This results in 20 datasets of 2000 KBs each, with each resulting dataset containing sentences from both vocabularies. From each KB we generated three queries (true, false, unknown) for each depth (D = {0,1,2,3,5}), i.e., from each KB we generated 3 \u00d7 (d+1), d \u2208 D, queries. So, in total, the dataset contains"}, {"title": "Statistical Features", "content": "As it is thoroughly discussed by Zhang et al. (2022), it is impossible to eliminate all statistical features that exist in data, besides, some of them inherently exist in logical reasoning problems.\nHowever, DELTAD is balanced with respect to some of the most obvious features: i) KB size: From the same KB we extract all three types of questions (true, false, unknown); ii) Inference depth: We keep a KB only if it can provide all three types of questions with the same inference depth; iii) Formulation of the question: The translation to natural language is implemented in such a way that the word \"not\" appears almost equal number of times in true questions (52.39%), false questions (50.71%) and unknown questions (46.60%); iv) Average length in words: True questions 10.85, false questions 8.97, unknown questions 10.34."}, {"title": "Experiments", "content": "We systematically tested the entailment checking ability of supervised fine-tuned DeBERTaV3-large, due to its recent advancements in NLU tasks (He, Gao, and Chen, 2023). We also tested in zero-shot and few-shot prompting the models GPT-3.5 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4) from OpenAI, as they have demonstrated strong performance across various reasoning benchmarks (OpenAI, 2023). Our limited resources did not allow us to test the performance of other models, like the Llama family; we plan to do this in future work."}, {"title": "DeBERTa-based Models", "content": "Evaluation Setup We fine-tuned the DeBERTaV3-large to predict true/false/unknown (i.e., multi-class sentence classification) for each example. A context-question pair was supplied to the model as [CLS] context [SEP] question [SEP]. We used accuracy as the evaluation metric. The test data has an equal balance of true/false/unknown answers, hence the baseline of random guessing is 33.3%. The specifics of the chosen hyper-parameters, which we maintained consistently throughout our experiments, can be found in A.2.\nFor each combination of depth and level, we trained different models on subsets of DELTAD. A model DELTAi,j is trained in examples of reasoning depth up to i and of linguistic complexity level up to j. For instance, the model DELTA3,2 has been trained to depths up to 3 and linguistic complexity levels up to 2. The final model DELTAM has been trained to all depths and all linguistic complexity levels, i.e., DELTAM=DELTA5,3. For all datasets, we partitioned the data into 70%/10%/20% splits for train/validation/test sets."}, {"title": "Tests on Symbolic Data", "content": "To test the effect of the semantics of the words on the performance of DELTAM we created the dataset SoftSymbolic, generated by replacing consistently in the test set the words appearing in pools A and B with symbols. Specifically, all individuals (e.g., Anna) were replaced with an a\u017c symbol (e.g., a3), all classes (e.g., smart) with an C; symbol (e.g., C2), and all roles (e.g., likes) with an Rk symbol (e.g., R5), where i, j,k is some ID number. The average performance of DELTAM over the SoftSymbolic dataset was 95.3%, hence in contrast to Tang et al. (2023), we conclude that DELTAM is not affected by the lack of semantics.\nTo check if the models can perform over purely logical examples, we generated the HardSymbolic dataset, which resulted from the SoftSymbolic by also utilizing the DL terminology: the word \"some\" (corresponding to the existential quantifier) was translated as \u201cexists\u201d, the \u201cif . . . then . . .\u201d (corresponding to subsumption) as \u201cis subsumed by\", etc.\nThe performance of DELTAM on both Soft and Hard Symbolic datasets is presented in Table 4. We observe that the average performance of DELTAM over the HardSymbolic dataset dropped to 58.5%. This is an expected result as the structure of the tested sentences was very different from the sentences in which DELTAM had been trained. We tested also the GPT models on (100 examples of) the HardSymbolic dataset where they showed similar performance to DELTAM. The results are shown in Table 5. The average accuracy of GPT-3.5 0-shot was 57%, 9-shot 61%; and GPT-4 20%, 65%, respectively. Hence, TLMs seem to struggle with purely logical datasets. Both the Soft and HardSymbolic datasets are openly available in the provided URL."}, {"title": "Related Work", "content": "Multiple surveys (Yang et al., 2023; Huang and Chang, 2023; Yu, Zhang, and Wang, 2023) in the literature describe the most recent research developments on the use of transformers for reasoning tasks. One of the first datasets generated for this purpose was from Clark, Tafjord, and Richardson (2020) with RuleTaker, demonstrating the potential of transformers to perform logical question answering under CWA by training TLMs on synthetic datasets. However, their approach was limited to short expressions of simple conjunctive subsumption axioms. Tafjord, Dalvi, and Clark (2021), generated the ProofWriter datasets (under CWA and OWA) and with a T5 (Raffel et al., 2020)-based model fined-tuned on ProofWriter showed that TLMs can generate proofs with high accuracy (94.8% for depth 5). We generated DELTAD based on the approach for the generation of the datasets RuleTaker and ProofWriter, i.e., using PCFGs. However, DELTAD is different from these datasets as i) ALCQ is a much more expressive logic language hence we produced new PCFGs; ii) we have defined different PCFGs for each linguistic complexity level (which has not been done for any other dataset in the literature); iii) it is balanced regarding the aspects discussed in Section 3.5.\nIn more expressive contexts, Onta\u00f1\u00f3n et al. (2022) showed that TLMs perform well (up to 90.5%) over contexts generated by propositional logic and a small subset of FOL. Han et al. (2022), with the FOLIO dataset (1.4K), generated from FOL sentences -but without number restrictions- tested the ability of various TLMs for the same reasoning task and concluded that RoBERTa (Liu et al., 2019) performed best among all tested models (including GPT-3 and Codex) but still, the performance was low. Tian et al. (2021) introduced the much richer synthetic dataset LogicNLI (30K), under OWA for diagnosing TLMs' ability in FOL reasoning, showing that even their best-performing model does not learn to perform reasoning tasks and cannot generalize to different scenarios. Schlegel, Pavlov, and Pratt-Hartmann (2022) generated a very simple dataset (containing a single conjunction) for satisfiability checking and showed that models that perform well on hard problems do not perform equally well on easier ones, concluding that transformers cannot learn the underlying reasoning rules rather than they tend to overfit to patterns in the generated data. Also, Zhang et al. (2022), and Tian et al. (2021) achieved similar results. Bang et al. (2023) studied ChatGPT's (Liu et al., 2023) deductive reasoning ability on bAbi task 16 (Weston et al., 2016) and EntailmentBank (Dalvi et al., 2021), performing merely well. In addition, differently from our results (where the performance decrease was small), Tang et al. (2023) showed that TLMs perform significantly better when using natural language instead of symbolic representations of logical facts and subsumption axioms."}, {"title": "Conclusions and Future Work", "content": "We generated the only large dataset (384K) in the literature that targets expressive DLs (namely, ALCQ), enjoys both high expressivity and high linguistic complexity, and is publicly available for further understanding of the functionality of TLMs. We showed that our DeBERTa-based model, DELTAM, can carry out entailment checking over expressive synthetic datasets with very high accuracy, regardless of the linguistic complexity of the context. Differently from recent results in the literature, we showed that our model has learned to generalize on unseen reasoning depths, smaller or greater. Zero-shot tests showed that DELTAM is mostly robust to other distributions. Tests with the GPT family showed that GPT-4 can have significant performance with only a few shots. The high accuracy of zero-shot testings in a real-world scenario demonstrates the potential of TLMs for performing reasoning tasks bypassing the necessity for domain experts to be familiar with formal representations. Our qualitative tests revealed the need for the development of systematic evaluation techniques of synthetically generated datasets. Hence, this will be our next step in future work. Furthermore, we plan to explore the upper limit of the expressivity of the logic language so that a TLM will be able to perform reasoning tasks with high accuracy.\nFinally, we will expand our evaluation section with other state-of-the-art generative models."}, {"title": "Acknowledgement", "content": "This work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program."}, {"title": "Appendix", "content": "The appendix consists of the following content:\n\u2022 A.1: Dataset generation\n\u2022 A.2: Additional training details\n\u2022 A.3: Evaluation results on datasets for Lo, L1, L2"}, {"title": "Dataset Generation", "content": "We generated our synthetic dataset DELTAD, using four probabilistic context-free grammars. To ensure that we will produce datasets with inference depth D > 0, i.e., datasets resulted from some inferencing, we generated a new statement C \u2291 D, if C' either appeared in some already generated fact or the RHS of some already generated statement.\nProbabilistic Context-Free Grammars Each grammar is based on some vocabulary of terms. Pool A and Pool B are defined next."}, {"title": "Additional Training Details", "content": "We used PyTorch 2.0 to set up our training and testing (inferencing). We use the microsoft/deberta-v3-large model from the transformers library, along with the accelerate framework. We fine-tuned the DeBERTaV3-large model (304M parameters) using the AdamW optimizer on two A100 GPUs. We used mixed precision (FP16) for our calculations to save memory and speed up the process. The specific set of hyperparameters used for all our models' training is given in Table 7. The model showed significant performance with this set of hyper-parameters, so there was no reason to proceed with any further hyper-parameter tuning, especially given our limited resources. The model output corresponds to the truth value 0 for False, 1 for True, and 2 for Unknown labels."}, {"title": "Evaluation Results on Datasets for Lo, L1, L2", "content": "The performance of the intermediate models DELTAi,j, for i\u2208 {0,1,2,3,5}, j\u2208 {0,1,2} on their corresponding datasets (of D < i and L < j) are illustrated in Tables 8, 9, 10. We observe that the pattern of the models' performance across various linguistic complexity levels is similar. However, as the models progress to higher linguistic complexity"}]}