{"title": "Multimodal Multihop Source Retrieval for Web Question Answering", "authors": ["Navya Yarrabelly", "Saloni Mittal"], "abstract": "This work deals with the challenge of learning and reasoning over multi-modal multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn multi-source reasoning paths and find the supporting facts across both image and text modalities for answering the question. In this paper, we investigate the importance of graph structure for multi-modal multi-hop question answering. Our analysis is centered on WebQA. We construct a strong baseline model, that finds relevant sources using a pairwise classification task. We establish that, with the proper use of feature representations from pre-trained models, graph structure helps in improving multi-modal multi-hop question answering. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph structure can be leveraged to improve the retrieval performance for the task. Experiments and visualized analysis demonstrate that message propagation over graph networks or the entire graph structure can replace massive multimodal transformers with token-wise cross-attention. We demonstrated the applicability of our method and show a performance gain of 4.6% retrieval F1score over the transformer baselines, despite being a very light model. We further demonstrated the applicability of our model to a large scale retrieval setting.", "sections": [{"title": "1. Introduction", "content": "Web search is fundamentally multimodal and multihop. Here multimodal refers to the fact that the web has text, images, tabular and speech modality data, and when we query, the response should be based on the features extracted from all these modalities. However, most web search engines treat the web as a text-only landscape which results in not leveraging many highly informative sources. In general, when humans search the web, they go through multiple sources, and effectively aggregate information based on reasoning to answer complex questions . So, essentially we need data that is multihop - a question requires reasoning over information scattered over multiple sources in order to obtain the correct answer. In this project, we aim to target the problem of multimodal multihop source retrieval for open-domain question answering as shown in Fig. 1. Specifically, we aim to develop an AI system which for a given text question, can select relevant sources of information from different modalities required to generate a correct answer.\nGraph Convolutional Networks (GCNs) are designed to share information across different nodes of a graph and effectively utilize them to make decisions. They are hence well suited for the task of multihop reasoning. However, to the best of our knowledge, there is no existing work that aims to explore GCNs for solving multimodal multihop source retrieval. In this work, we explore independent approaches each one designed to explore different aspects of GCNs and showcase their impact on our specific task. Through this work, we demonstrate the comparable performance of GCNs to complex transformer-based baselines (Chang et al., 2021) even with lightweight sub-optimal input representations. We suspect this is critically due to the inductive biases of GCNs making them well suited for information aggregation and multimodal retrieval tasks.\nThe primary contributions of this work are as follows:\n\u2022 A novel graph-based framework to solve multimodal and multihop source retrieval that can scale to open-domain question-answering in the wild.\n\u2022 Generate question-conditioned node representations of sources that are informed by the information present in other sources.\n\u2022 Explore various graph architectures for the source retrieval problem.\n\u2022 Present the computation and latency advantages of our proposed approach over expensive transformer-based models which makes it scalable for a web-scale retrieval task.\nThis is the link to our Github repository for the project: https://github.com/smittal10/Multimodal-Multihop-QA"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. multimodal visual Q/A", "content": "The seminal work of (Antol et al., 2015) released a large-scale dataset for Visual Question Answering. This work extracted uni-modal deep features and fused (point-wise multiplication) them for answering. (Das et al., 2017) introduced visual dialog task. The aim of this problem is to develop a conversational AI agent which can also process inputs from visual modality. However, there is a gap between the quantitative (reported) performance and the actual ability of models to generate realistic and diverse inputs.\nThe work from (Murahari et al., 2019) aims to improve the generative ability of models for real-life scenarios. They introduce two competing agents Q-BOT and A-BOT. Q-BOT is trained to ask diverse questions which forces A-BOT to explore a larger state space and jointly answer more informatively. The work of (Goyal et al., 2017) was aimed to balance the VQA dataset by having almost twice the original image-question pairs. The pairs often contained conflicting question-answers for the same visual cues to prevent overfitting. Interestingly, extensive works still make a weak assumption that all relevant information is often limited to a single image. Although the context needed to answer many real-life questions can span across multiple images."}, {"title": "2.2. Multihop QA", "content": "There has been substantial work in recent years on building QA models that can reason over multiple sources of evidence. In 2018, (Yang et al., 2018) introduced a text-based QA dataset, HotPotQA that required reasoning over multiple supporting documents. They used an RNN-based architecture that could produce \u201cyes\u201d/\u201cno\u201d or span-based answers. This seminal work is still uni-modal in nature.\nRecent works have expanded multihop QA for multimodal inputs. The work from (Talmor et al., 2021) proposed MultiModalQA, which uses inputs image, text, and table to answer questions. They propose a multihop decomposition (ImplicitDecomp) method where the answer is iteratively generated by parsing through individual QA modules. However, they generate questions from a fixed template and hence task reduces to filling of missing entries once the template is identified. Hence it cannot scale well for zero-shot generations. MIMOQ (Singh et al., 2021) proposes a QA system that can reason and also respond in multiple modalities. The authors propose a novel multimodal framework called MEXBERT (Multimodal Extractive BERT) that uses joint attention over input textual and visual streams for extracting multimodal answers given a question. They observe noticeable improvements when compared with methods that independently extract answers from unimodal QA modules. However, the key contribution of MIMOQ is still to generate multimodal answers and not information aggregation across modalities."}, {"title": "2.3. Cross modality representations", "content": "Significant advances in Visual QA and multimodal QA can be attributed to the advances in Transformer-based methods initially proposed by (Vaswani et al., 2017) and their large-scale pre-training methods. There are two major directions of pre-training: (a) Parallel streams of encoders one for each modality followed by fusion (Tan & Bansal, 2019)(Lu et al., 2019); (b) Unified encoder-decoder representations that can take both the language or image modalities (Zhou et al., 2019). (Anderson et al., 2017) proposed a new form of representation for images. They extract objects from the image and represent an image as a set of embeddings for the Regions of Interest (ROI). The spatial positional features (bounding box locations) are also added to the encoding. This representation is semantically closer to language representations. Hence refining these models and ways of representations for multihop reasoning is expected to work well on WebQA (Chang et al., 2021) benchmark.\nTo the best of our knowledge, this is the first dataset that introduces open-domain question-answering in a multimodal and multihop setting. Success on WEBQA requires a system to retrieve relevant multimodal sources first and aggregate information from text and vision modality. Our approach significantly differs from all the prior work in this domain, where we leverage GNNs to solve the problem of multimodal retrieval and ranking. One major limitation of transformer-based models is that they cannot process all the sources together while making a decision during retrieval as the input length is limited. In an open-domain QA system that typically has hundreds of candidate sources to choose from, this problem is amplified manifold. Our approach solves this fundamental problem as it can leverage information from all the sources at the same time."}, {"title": "3. Problem Statement", "content": "For multimodal multihop QA the problem is defined such that the QA system gets a text question Q and a set of sources si \u2208 S as input. It is important to note that source si can be either an image Zi or a text snippet Ti such that SCIUT. The benchmark in (Chang et al., 2021) divides the problem into two separate tasks:\nTask (A) is a source separation task. Here, for a given question Q system has to divide the sources SQ into positive and distractors (S5, S5). Hence formally the problem is defined as :\n$S = f_{source}(Q, S_Q|$\u03b8$) where S^+\\subseteq S_Q \\subseteq S$  (1)\nTask (B) is the Question Answering task. Here for a given Q and So the task is to learn a function to generate Apred such that\n$A_{pred} = f_{qa}(Q, S^+)$  (2)\nApred is the generated answer (not classification). The functions fqa and fsource are parameterized by $ and @ which are the weights of neural models. This project primarily focus on Task (A) i.e. the problem of multimodal multihop source retrieval."}, {"title": "4. Baseline Models", "content": ""}, {"title": "4.1. VLP + VinVL", "content": "The authors from WebQA (Chang et al., 2021) introduced this baseline for multimodal source retrieval. VLP (Zhou et al., 2019) is a pre-trained multimodal transformer well suited for both understanding and generative tasks. The source retrieval baseline is fine-tuned on the VLP check-points. Text modality inputs are tokenized by Bert-base-cased tokenizers. For images, the baseline extracts ~ 100 region proposals using latest state-of-the-art model VinVL (Zhang et al., 2021). The authors conducted experiments with various image encoders like x101fpn etc. however demonstrated VinVL (Zhang et al., 2021) to work best. In this work, we hence draw comparisons with the baseline's best VLP + VinVL architecture.\nFor a given question Q, every source SQ,i \u2208 Sq is fed into the baseline model one by one. The input to the model is hence a pair of (Source, Question) i.e. < [CLS], SQ,i, [SEP], Q, [SEP] > and the model predicts the probability of it being a positive source. This approach hence makes a critical assumption that prediction over a source is independent of other sources in the dataset. We find this to be a weak assumption and not in the spirit of multihop reasoning which the dataset (Chang et al., 2021) critically demands."}, {"title": "4.2. CLIP + Sentence-BERT based baseline", "content": "As mentioned in the above section, the VLP baseline introduced by the authors uses very expensive input features. For example, the image features for WebQna dataset alone account for 500 GB of disk space (as mentioned by the author in thier official github repo 1) as it takes in 100 region proposals for every image. For our proposed method, we use very lightweight input features (discussed in the later sections) from pre-trained models like CLIP (Radford et al., 2021) and Sentence BERT (Reimers & Gurevych, 2019) which fit under 2 GB of memory. In the spirit of fair comparison, we train a baseline model with the same pairwise classification objective as the VLP baseline keeping the input feature space same with our proposed graph-based model. The motivation to build this baseline is to determine whether a graph-based approach has merit over a pairwise objective (as in VLP baseline) keeping the input feature space same. For this purpose, we encode the text modality (question, text snippets and image captions) with Sentence-BERT (sBERT) and images with CLIP's vision encoder. For a question-source pair we obtain encoded vectors after passing through CLIP and sBERT. We apply a few linear layers on top of concatenation of question-source encoded vector and predict the probability of it being a positive source for that question. We fine-tune the weights of CLIP and sBERT while training on this task."}, {"title": "5. Proposed Approach", "content": null}, {"title": "6. Methods", "content": "Our proposed Hierarchical Graph Network (HGN) consists of four main components: (i) Graph Construction Module to construct a hierarchical graph to include connections from questions to sources and edges to connect question guided knowledge extraction from different sources; (ii) Context Encoding Module, where initial representations of graph nodes are obtained via various pre-trained transformer models for both images and textual sources ; (iii) Graph Reasoning Module, where graph-convolution-based message passing algorithm is applied to jointly update node representations; and (iv) Graph Prediction Module, where we combine different training objectives to train out graph-encoder modules to retrieve relevant sources for the given question. We investigated different architectures and graph structures and training objectives to identify the best combination for effectively identifying the relevant sources retrieval for the problem of multi-modal multi-hop question answering. We briefly describe each of the components and include the empirical analysis in the section below."}, {"title": "6.1. Graph Construction Module", "content": ""}, {"title": "6.1.1. STAR NODE STRUCTURE", "content": "Figure 6 shows the graph structure where each source is only connected to the question and the information aggregation across source happens in a multi-hop setting ion to all source node. For this approach, all relevant information to identify a particular source as positive is within two hops."}, {"title": "6.1.2. FULLY CONNECTED GRAPH STRUCTURE", "content": "Figure 4 shows the fully connected strcuture where we add additional edges between all the source nodes. This strcutures enables for the model to enable direct communication between the sources but has the risk of aggregating irrelevant information from different sources."}, {"title": "6.1.3. HIERARCHICAL SEMANTIC GRAPH NETWORKS", "content": ""}, {"title": "Entity based GNN", "content": "We rely on textual part of both images and text sources to further build a fine-grained heirarchical network for our task. Each question and source are linked via the entities that are being addressed. Thus our graph is naturally encoded in a hierarchical structure, and motivates our graph construction based on the hierarchical connections across sources and source to question. For each source node, we add edges between the node and all the sentences in the source. We further extract all the entities in the sentence and add edges between the sentence node and these entity nodes. Each question also has entities and the entities across different sources and question are connected if the mention refers to the same entity. Each type of these nodes captures semantics from different information sources. Thus, the hierarchical graph effectively exploits the structural information across all different granularity levels to learn fine-grained representations, which can locate supporting facts and answers more accurately than simpler graphs with homogeneous nodes. Figure 4 describes the construction of the heterogeneous entity graph. The entity \"Minnesota State Highway\" is refered in both sources S1, S3 and Question Q. Thus, we form edges between S\u2081 and S3 and also between each of the entity mentions across different sources. Since each source captures its own contextual representation of the shared entities, by creating edges we enable for our graph reasoning modules to allow for an information flow path from question to sources and between sources to answer multi-hop queries (Fang et al., 2019)."}, {"title": "Semantic Role Labelling based GNN", "content": "Incorporating semantic informnation has shown to improve the performance of multihop question answering systems (Zheng & Kordjamshidi, 2020). We further experimented to inlcude the semantic structure of the source to build a heterogeneous graph that contains document-level sub-graph S and argument predicate SRL sub-graph Arg for each data instance. In the graph construction process, the document level sub-graph S includes question q, and sources S1, ..., n which includes both textual and image modalities. The argument-predicate SRL sub-graphs Arg, including arguments as nodes and the predicates as edges, are generated using AllenNLP-SRL model. We extract the arguments corresponding to each predicate and create a node which is a concatenation of the argument phrase and argument type, including \u201cTEMPORAL\u201d, \u201cLOC\u201d, etc. Figure 4 describes the construction of the heterogeneous graph. The heterogeneous graph's edges are added as follows: 1) There will be an edge between a source and an argument if an argument appears in this source 2) Two sentences si and sj are connected by an edge if they share an argument by approximate textual matching (the red dashed lines); 3) Two argument nodes Argi and Argj will have an edge if a predicate exists between Argi and Argj 4) There will be an edge between the question and source if they share an argument.\nWe build a predicate-based semantic edge matrix K and a heterogeneous edge weight matrix A. The semantic edge matrix K is a matrix that stores the word index of the predicates. If two argument nodes Argi and Argj related to the same predicate, we add that predicate word index to K(Argi , Argj). Sometimes, Argi and Argj are related to more than one predicate. These predicate features can be used as edge features for various graph encoding modules. In the meantime, the heterogeneous edge weight. matrix A is a matrix that stores different types of edge weights. We divide the edges into three types: source-argument edges, argument-argument edges, question-source, question-argument and source-source edges. The weight of a sentence-sentence edge is 1 when two sentences share an argument. Meanwhile, the weight of a sentence-argument edge is 1 if there exists an edge between a sentence and an argument. The edge weight matrix can also be modified to include the prior knowledge of relevance of sources to a given question.\nNode Representations For each of the sources S1, ..., \u03b7, we initialize the node features for textual sources with a 768 dimensional BERT embedding and for image sources, we concatenate the image representations obtained using different pre-trained models and the caption embedding. Since each argument occurs in multiple sources, each of these arguments are initialized with their phrase embeddings obtained from the corresponding source. This allows for different sources to exchange information about"}, {"title": "6.2. Context Encoding Module", "content": "To encode each modality we use modality-specific encoders. All the text information i.e. text snippets, image captions, and questions are encoded into a 768-dimensional vector using sBERT (Reimers & Gurevych, 2019). We used the pre-trained variant of Sentence BERT on MSMARCO Passage Ranking Dataset 2. Please note that by using such an encoding scheme we lose token-level features and embed the entire document/sentence in one vector. This can be seen as a downside of our approach over the baseline VLP model that takes token-level features and uses the attention mechanism on them.\nTo encode the vision modality, we experiment with two pre-trained models; (i) ResNet-152 (He et al., 2015) network - 2048 dimensional vector and (ii) CLIP-ViT-L/14 (Radford et al., 2021) - 768 dimensional vector. ResNet has a CNN-based model architecture and is pre-trained on an image recognition task. CLIP is a multi-modal model that is trained on millions of text-image pairs on a contrastive similarity objective. While ResNet is a unimodal encoder, the Vision Transformer in CLIP is tuned towards jointly understanding text and images. We experiment with these input encoders in the following settings:"}, {"title": "6.3. Graph Reasoning Module", "content": "We construct a multimodal heterogenous graph where a node either represents a question or a source. The edges between these nodes are added based on the graph strutures explained in the previous section.\nTo learn the graph embedding we leverage from the learning of GraphSAGE (Hamilton et al., 2017). This work proposed a method to inductively learn node embeddings and the method generalizes well to previously unseen data. Specifically we use the following function to aggregate information across nodes:\n$x'_i = W_1x_i + W_2 \\cdot mean_{j \\in N(i)} x_j$ (3)\nHere N(i) denotes the 1 hop-neighborhood of node with index i. We use multiple layers of GraphSAGE that facilitates information aggregation from the distant neighbors as well. We train the GraphSAGE module with different training objectives described in the next section. When the graph converges, we obtain multi-modal node embeddings that map image and text sources in the same space."}, {"title": "6.4. Graph Training Objectives", "content": ""}, {"title": "6.4.1. NODE CLASSIFICATION", "content": "Our Graph Encoder Gp(.) module maps each source and question to a d dimension real valued vector. Each question node qi is connected to the positive sources 81 + and negative sources 81-. Hence, the transformed embedding of each node is a contextual representation with question knowledge encoded in it. We aimed to learn whether each source is a positive or negative source from its embedding vector. We train an MLP on top of the graph encoder module to predict the binary class of each source and used a binary cross entropy based loss function for all the nodes for a question."}, {"title": "6.4.2. EDGE CLASSIFICATION", "content": "To further guide the Graph Encoder module, we also tried to explicitly predict the edge connection type (whether an edge exists or not) between question and the sources and also between sources to reinforce the multihop connections signal among sources to the Graph Encoder training. We use cosine similarity of the transformed node embeddings to determine the probability of an edge between two nodes and use a binary cross entropy based loss function for each edge in the graph."}, {"title": "6.4.3. CONTRASTIVE LOSS", "content": "Since we construct a heterogeneous graph encoding module, the embeddings of nodes of different types are transformed using different feature transformation but in the same vector space. Dot-product similarity as a ranking function to train the the graph encoders for the problem of retrieval is considered a metric learning problem (Kulis et al., 2013). The objective is to embed all the nodes in a vector space where the similarity between a question and a positive passage is higher than the question and it's corresponding dis-tractor sources and we aim to do this by learning a better embedding space.\nTraining the graph encoders so that the dot-product similarity (Eq. (1)) becomes a good ranking function for retrieval is essentially a metric learning problem (Kulis, 2013). The goal is to create a vector space such that relevant pairs of questions and passages will have smaller distance (i.e., higher similarity) than the irrelevant ones, by learning a better embedding function.\nLet D = {(qi, St, Si,1,..., Pin)} be a training data that contains a question qi and positive sources s1 + and distractor sources 811. We optimize the loss function as the loss function as the negative log likelihood of the positive sources.\n$\\hat{L} (q_i, s_i^+, s_{i,1}, ..., s_{i,n}) = -log \\frac{e^{sim(q_i, s_i^+)}}{\\sum_{j=1}^n e^{sim(q_i, s_{i,j})}}$ (4)"}, {"title": "7. Experimental Setup", "content": ""}, {"title": "7.1. Dataset", "content": "We use the WebQA (Chang et al., 2021) dataset to solve the problem of multimodal multihop source retrieval. The dataset contains input from two different modalities: Images and Text. The dataset contains a set of sources So associated with each question. Each element in the set So can either be a positive source (with label +1) or a negative source (with label 0). Note that the number of sources (|SQ|) is not fixed and hence is different for each question Q. The questions with a positive source from visual modality are further sub-divided into six categories. Table 5 contains these question categories. We do not specifically use this sub-division while training however we do use them for quantitative analysis of performance."}, {"title": "7.2. Evaluation Metrics", "content": "The performance for source retrieval is measured using the F1-score."}, {"title": "7.2.1. F1-SCORE", "content": "There is a severe imbalance in the dataset and hence accuracy of source selection is not the best metric. For example, the base model can achieve ~ 92% accuracy by predicting every source as negative. Hence we use the F1-score to measure performance. F1-score is measured as a harmonic mean of Precision and Recall.\n$F1_{score} = 2* \\frac{Pr * Recall}{Pr + Recall}$ (5)\nwhere Precision (Pr) is defined as $\\frac{TP}{TP+FP}$ and Recall is defined as $\\frac{TP}{TP+FN}$. Further, TP are the true positives, FP are false positives and FN are false negatives. F-score enables us to measure of well the model is in predicting positive and negative sources even with imbalance."}, {"title": "7.3. Experiment details", "content": "Section 4 describes our baseline models. Our baselines operate in a pairwise setting, they take a question and a source and predict the relevance of each source individually, as the transformer based architectures cannot handle large input lengths."}, {"title": "8. Results and Discussion", "content": "Table 2 lists our main results on the WebQA dataset in a restricted setting where we rank only 50 relevant sources for each question. Row 1-3 lists the baselines which are all pairwise classification models that take a question and candidate source and determine the relevance of each source independently. VLP baseline (row 1) uses a multi-modal transformer with token level cross-attention between the modalities by taking 100 region proposal features for an image and BERT features for representing question and passage tokens. Since these features are very expensive, we constructed two additional baselines that use CLIP(Radford et al., 2021) for image features and Sentence-BERT(Reimers & Gurevych, 2019) for textual features. Our GNN with star graph using fine-tuned CLIP and fine-tuned sBERT features gave the best combined F1 among all the settings and about +4.6% increment over the pairwise VLP baseline for image queries. This shows the efficacy of our approach despite using much cheaper features (source level features instead of token level) and a much lighter model that can rank all the sources in a single pass.\nTable 5 shows F1 score for different categories of queries. Despite smaller number of training examples for the categires \"shape\" and \"others\", we are able to achieve an F1 score comparable with that of other categories with large number of traning instances available."}, {"title": "8.1. Graph Structures Analysis", "content": "We experimented with three kinds of graph structures. 1. Starnode structure where we include connections from question to all source node. 2. Fully Connected structure where we add additional connections between all source nodes. 3. Hierarchical graph networks as discussed in section 6.1. Row 5 refers to the fully connected graph structure and rows 5,8,9 refer to the F1 score results with the starnode structure and rows 6 and 7 refer to the Hierarchical Graph Networks (HGN). We observed that adding additional negative connections among source nodes leads to irrelevant information aggregation and since the GCN model cannot dynamically determine the edge weights, fully connected graph structure led to a decreased performance. Overall we saw an improvement of about 2.3% overall gain in F1 score and about 8.7% increase in the F1 score for the text queries.\nFrom our fully connected vs starnode results analysis, we believe that Graph attention networks which can dynamically adjust the weights for the neighbors for query dependent source aggregation would further improve the performance. This establishes that having fine-grained nodes and incorporating a rich semantic structure helps in the retrieval performance. We believe that having a richer input features and more fine-grained nodes for images would have improved for image queries also. We plan to further explore this direction for improving image-retrieval accuracy."}, {"title": "8.2. Context Encoding Module -Features", "content": "Table 2 shows the impact of our different modality-specific features input to our GNN model. From rows 6 and 8, it is evident that zero-shot CLIP features for images is more informative to graph learning than zero-shot ResNet features. We get an improvement of +3 points in combined F1 score by using CLIP features as we expected earlier since CLIP is trained on a multimodal objective. Using CLIP features not only improves the Image F1 but also improves Text F1 by +2.65 points. We believe this happens because of the better text-aligned image features from CLIP that makes the model discard the negative image sources relatively easier now. Jointly fine-tuning CLIP's vision encoder with the GNN module results in degradation of performance by -1.4 points. This result aligns with one of the conclusions in this work (Shao et al., 2020) where authors say that the graph structure is helpful when pre-trained models are used in a feature-based manner and not jointly fine-tuned. In row 9 of table 2, we report the results for GNN model with image and text features extracted from fine-tuned CLIP and Sentence BERT respectively from our second baseline model (row 3). This is the best model that we attain in all our setting giving a boost of +1.6 points in F1 over zero-shot CLIP and sentence BERT features. It also helps to give a massive improvement in image queries over the baseline by +4.6 points."}, {"title": "8.3. Question-Source similarity Visualizations", "content": "Through the histogram plots in figure 5 we wish to show the power of our graph-based method (in orange) in learning effective representations for question and sources over pre-trained models like sentence BERT (in blue). The above and below graphs shows the dot product distribution of (question (.) negative text source) pairs and (question (.) postive text source) pairs respectively. By plotting the dot product, we want to compute a similarity score between question and source. A good embedding representation would show high similarity between question and positive sources (close to 1), while low similarity between question and negative sources (close to 0). We can see in the above graph how the similarity scores obtained from graph node embeddings (in orange) have the majority chunk between 0.6 and 0.8 while the Sentence BERT-based similarity scores peak in the range of 0.4-0.6. Similarly, in the graph below we observe the efficacy of graph node-based embeddings (orange) as the distibution of question and negative source similarity scores move below 0 close to -0.75 while sentence-BERT based scores mainly lie between 0.25 and 0.6."}, {"title": "8.4. Retrieval in a Restricted vs Full Setting", "content": "For handling web scale retrieval, we further investigate the effect of retrieval scale where the QA system has to rank all the available sources (of order millions) to determine the positive sources that contain the information to answer a given query. For a pairwise classification baseline like VLP (Chang et al., 2021), it would take 3 years to rank 1 Million sources for a given query and hence such a model cannot work for full scale retrieval. We measure the performance impact by filtering top 20 sources using inexpensive similarity metrics and re-ranking the top-20 sources using our models.\nTable 3 shows the results for dense (BM25) versus sparse retrieval approaches. For the VLPbased model, top 20 sources are retrieved based on CLIP embedding similarity and then these sources are re-ranked using VLP baseline, sources are selected if its binary classification confidence is above a specified threshold. For our GNN based model, we follow a similar procedure to filter top 20 sources based on SBERT similarity score and re-rank them using our GNN model. We observed that while sparse retrieval approaches worked well for restricted setting, the performance dropped for an increased scale. While our GNN model could have handled larger number of candidate sources, it suffers from over squashing from negative sources. However, it holds the promise for Graph Attention Networks which can prune negative edges more efficiently."}, {"title": "8.5. Computation and Latency Analysis", "content": "We also analysed the computation requirements of our model vs the baseline transformer based VLP model. Table 4 shows the latency to process 50 sources for a question for each of the models. Our GNN based model provides a speedup of about 250 times over the pairwise transformer baseline (Chang et al., 2021) and also is a much lighter model requiring 2 GB gpu memory. Thus our model is more suited for large scale retrieval systems."}, {"title": "8.6. Qualitative Results", "content": "We analysed a few examples where our model made wrong predictions and other challenging examples where our model was able to predict correctly.\nAs shown in Figure 6 we can see that for the question in row 1, the positive image-based sources which contain the pillars are captured by the model. Even though pillar is not an explicit class in the image-based model, we see that the model can identify the correct image sources. Our model was also able to retrieve the other image source though it's caption does not have any lexical overlap with the query."}, {"title": "9. Key Insights", "content": "\u2022 Pruning out irrelevant source connections leads to better graph learning as seen in HGNN+SRL model. Full connections to high number negative sources leads to the problem of over-squashing (phenomenon in which information from the exponentially-growing receptive field is compressed into fixed-length node vectors) and irrelevant information flow.\n\u2022 Token level cross attention is more powerful to understand text modality sources as seen in the baseline VLP model which uses token-level features combined with attention. We believe that to be the main reason why we cannot beat the baseline Text F1 as we compress all the text modality information into a single fixed-size vector.\n\u2022 GNN training with pre-trained features yields better performance than jointly fine-tuning the pre-trained model as discussed in ??.\n\u2022 The graph-based architecture helps substantially to improve on Image queries over the baseline VLP model by +4.6 points in F1. However, the performance on text queries degrades. We hypothesize this happens because we compress all text information in one single vector while the baseline uses token level features. However, the HGNN method comes close to the baseline Text F1 despite sub-optimal. We believe combining that graph structure with features used in our best model will beat the baseline on text queries also. We leave that for future investigation."}, {"title": "10. Future Directions", "content": "We further plan to leverae Graph information flow paths for answer generation might be helpful. Answers can be selected either from entities in the constructed entity graph or from spans in documents by fusing entity representations back into token-level document representation."}]}