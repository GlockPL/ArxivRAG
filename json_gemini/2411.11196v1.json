{"title": "PickScan: Object discovery and reconstruction from handheld interactions", "authors": ["Vincent van der Brugge", "Marc Pollefeys", "Joshua B. Tenenbaum", "Krishna Murthy Jatavallabhula", "Ayush Tewari"], "abstract": "Reconstructing compositional 3D representations of scenes, where each object is represented with its own 3D model, is a highly desirable capability in robotics and augmented reality. However, most existing methods rely heavily on strong appearance priors for object discovery, therefore only working on those classes of objects on which the method has been trained, or do not allow for object manipulation, which is necessary to scan objects fully and to guide object discovery in challenging scenarios. We address these limitations with a novel interaction-guided and class-agnostic method based on object displacements that allows a user to move around a scene with an RGB-D camera, hold up objects, and finally outputs one 3D model per held-up object. Our main contribution to this end is a novel approach to detecting user-object interactions and extracting the masks of manipulated objects. On a custom-captured dataset, our pipeline discovers manipulated objects with 78.3% precision at 100% recall and reconstructs them with a mean chamfer distance of 0.90cm. Compared to Co-Fusion, the only comparable interaction-based and class-agnostic baseline, this corresponds to a reduction in chamfer distance of 73% while detecting 99% fewer false positives.", "sections": [{"title": "I. INTRODUCTION", "content": "Constructing detailed and informative maps of 3D envi-\nronments is a fundamental task in robotics and augmented\nreality, and crucial for many downstream applications. A\nparticularly important aspect of such maps is the degree to\nwhich they break down their surroundings into meaningful\nparts. For example, while many 3D reconstruction techniques\nreconstruct their surroundings as one \"contiguous\" geom-\netry, many applications would benefit from a breakdown\ninto several manipulatable objects and a background that\nis considered to be static. Think of a robot finding and\nmanipulating tools in an otherwise static environment, or an\naugmented reality experience where individual objects can\nbe selected and hologram copies thereof edited, rearranged,\netc.\nIn this work, we aim to design an easy-to-use pipeline that\ncan scan environments in such a compositional way, resulting\nin reconstructions where different objects are represented\nwith their own complete 3D model. We want said pipeline\nto work robustly on any type of object, as long as it can\nbe grasped and held in a user's hand. Notably, we do not\nwant to be restricted by the scanned objects having to fall\nwithin limited training distributions, as is typically the case\nwhen object discovery is driven by pre-trained segmentation\nnetworks.\nNearly all existing methods to such compositional scene\nreconstruction fall short of these specifications. One critical\naspect here is the way in which individual objects are iden-\ntified and tracked. One prominent approach (cf. [1], [2], [3],\n[4]) is to assume static scenes and identify objects using pre-\ntrained segmentation networks. However, such approaches\nare inherently restricted to objects that fall within a limited\ntraining distribution. Moreover, the static assumption has the\nshortcoming that objects can not be manipulated to scan them\nfully from all sides, and provably makes object discovery\nimpossible for some object constellations, for example for\ncongregations of objects with similar texture but different\nshapes and sizes see figure 2 for an example. While a\nnumber of works have been proposed that allow for object\nmovement in the scene (cf. [5], [6], [7], [8]), hence allowing"}, {"title": "II. RELATED WORK", "content": "Reconstructing 3D models of scenes has been widely\nstudied, with a lot of early work done on structure from\nmotion (SfM) and simultaneous localization and mapping\n(SLAM). While initial works focused on reconstructing a\nscene's geometry as a whole, some later works also break\ndown the scene into individual objects. Many of these com-\npositional scene reconstruction methods assume a scene to be\nstatic, i.e. do not allow for any object movement or dynamic\ninteraction. Some notable SLAM-based examples include\n[1], [2], which reconstruct a static scene and break it down\ninto semantic classes, and [3], [4] which do the same on the\ninstance-level. Such methods, which combine SLAM with\nsome type of semantic information, belong to the category\nof semantic visual SLAM see [10] for a comprehensive\nsurvey. Recently and outside the SLAM context, a number of\nneural-representation-based methods have also tackled static\ncompositional scene reconstruction, see for example [13],\n[14].\nMore relevant to us are methods like [5], [6], [7], [8] that\ndo not assume a scene to be static, but instead allow for\nobjects to be moved around and hence scanned fully. Most\nof these methods still rely on semantic information, usually\nprovided by a pre-trained segmentation network, to propose\nobject instances, and thus are not class agnostic. The notable\nexception here is Co-Fusion [5], which can discover objects\nsolely based on their rigid motion. To our knowledge, this\nis the only existing method that fulfills both the constraint\nof being class-agnostic as well as allowing for objects to be\nmoved around by the user and hence scanned fully.\nLastly, two other lines of work are also relevant to our\nmethod: Firstly, handheld object reconstruction methods like\n[12], [15], [16], [17], [18] deal with extracting 3D models\nof objects that are manipulated in a user's hand. However,\nthese assume either a static camera or object masks to be\ngiven, and hence do not allow to reconstruct entire scenes\nin a compositional manner. Secondly, motion segmentation\ntechniques like [19], [20], [21], [22], [23], [24] identify\nindividual objects based on them moving in a rigid fashion,\nalthough they do not reconstruct these objects. Related to this\nare methods like [25], [26] which extend dynamic neural"}, {"title": "III. METHODOLOGY", "content": "The main idea behind our pipeline is that, building on\nexisting works, we can reduce the compositional scene recon-\nstruction problem to finding the mask of each manipulated\nobject in some frame of the input RGB-D stream. Using a 2D\nmask tracker like XMem [11], we can then track this mask to\nall other frames; and with these propagated masks, we can\nreconstruct and track the respective object in 3D using an\nunknown object reconstruction and tracking technique like\nBundleSDF [12].\nThe resulting pipeline is depicted in figure 3 and can be\nthought to consist of two phases: In a first phase, we first\nestimate, for each frame, the mask of a potential manipulated\nobject (\"candidate object mask\") - regardless of whether\nmanipulation takes place in that frame. For this, we estimate\na per-frame segmentation of the moving scene parts by\ncomparing a given frame's reprojected point cloud with a\npoint cloud reconstruction of the initial scene (section III-\nA), which we then refine to per-frame candidate object masks\n(section III-B). In a second, \"object interaction\" phase, the\ncandidate object masks are in turn used to estimate the\nmasks for each respective manipulated object. Specifically,\nwe first detect individual user-object interactions (section\nIII-C). For each such interaction, we pick one particularly\nrepresentative (\"best\") mask (section III-D), track it in 2D,\nand suppress possible duplicate object detections (section III-\nE). After these two phases, we finally use the tracked masks\nto reconstruct and track the manipulated objects in 3D using\nBundleSDF [12]."}, {"title": "A. Moving mask heuristic", "content": "In our egocentric setup, the only possibly moving objects\nare the arm and potentially a manipulated object; our ap-\nproach to estimating the manipulated object mask is thus to\nestimate the mask of all moving scene parts (\"moving mask\")\nin each frame before removing an estimated arm mask.\nTo estimate the moving mask, we follow a displacement-\nbased heuristic, where points are estimated to be moving\nif they are sufficiently far away from the initial scene at\ntime t = 0. To reconstruct a point cloud of the initial\nscene ('initial point cloud\"), we require the user to first\nscan the scene without manipulating any objects; after this\nstatic scan is completed, they can start manipulating and\nscanning objects. We use a pre-trained arm segmentation\nnetwork [28] to identify the end of this static scanning phase,\nnamely as the moment the arm mask stops being non-empty.\nNote that arm segmentation is the only task for which we\nuse a pre-trained segmentation network; we do not use pre-\ntrained segmentation networks to discover objects directly.\nThe initial point cloud is extracted from the frames in the\nstatic scanning phase: Using initial pose estimates from a\nvisual-inertial odometry (VIO) pipeline, we refine the camera\nposes and fuse the static phase's RGB-D frames into a point\ncloud with the differentiable SLAM pipeline gradSLAM\n[29]. We also run this pipeline (VIO + gradSLAM) on all\nframes of the scan (including ones where manipulation takes\nplace) to extract all camera poses. With this, we can reproject\na pixel (u, v) at time t into the canonical coordinate frame Fo\nof the initial point cloud using perspective camera geometry:\n\n  \\qquad tP(u,v) = \\pi^{-1}(u, v, d[t, u, v], K) = K^{-1}. d[t, u, v] \\cdot \\upsilon\n  \\qquad  OP(u,v)=\n\\begin{bmatrix}\nu\\\\1\\end{bmatrix}  = To,t\n\\begin{bmatrix}tP(u,v)\\\\1\\end{bmatrix}\n\nWhere \u00bfP(u,v) is the point's 3D coordinate in coordinate\nframe Fi, K is the camera intrinsic matrix, d[t] the depth\nframe at time t, and To,t the 4x4 homogeneous transforma-\ntion from coordinate frame F\u2081 to Fo. We detect a pixel (u, v)\nas moving if its reprojection is further away than a tunable\nthreshold radius Pmoving from any point in the initial point\ncloud.\""}, {"title": "B. Candidate object mask", "content": "Building on the estimated per-frame moving mask, we\nestimate a per-frame manipulated object mask next. For\neach frame, we remove the arm mask from the moving\nmask, divide the remainder into contiguous areas of pixels\n(\"blobs\"), and ignore blobs below a tunable threshold area\nin size. We then choose the blob with the reprojected points\nthat are closest to the points in the user's hand. We estimate"}, {"title": "C. Interaction detection", "content": "To detect individual objects and hence factorize the scene,\nwe detect the beginnings and ends of individual object\ninteractions. For this, we focus on two different distances that\nevolve distinctively during a picking-up-and-laying-down\nmaneuver. Firstly, the distance between the hand points and\nthe initial point cloud (\"hand-initial distance\" hereinafter)\nis small as the interaction starts, becomes large as the user\nremoves the object from its support (e.g. a table), and\nbecomes small again as their hand once again approaches\nsome support area while laying down the object. Meanwhile,\nthe distance between the hand points and the candidate object\nmask's points (\"hand-object distance\u201d hereinafter) remains\nsmall during the entire interaction as the object is lodged in\nthe user's hand. Following these patterns, we detect object\ninteractions as periods starting with the hand-initial distance\ncrossing above the hand-object distance, and ending with the\nformer crossing below the latter once again. To minimize\nthe effect of noise, we apply a median filter to both distance\ntrajectories before extracting interactions. Lastly, we filter\nout interactions below a tunable threshold in duration for\ndenoising. See figure 4 for a visualization of this interaction\ndetection step."}, {"title": "D. Best frame selection", "content": "Next, for each detected object (interaction), we pick a\n\"best frame\" in which the estimated object mask is deemed to\nbe of high quality. For this, we employ the intersection-over-\nunion (IoU) between subsequent frames' candidate object\nmasks as a stability measure. Periods in which this cross-\nframe IoU remains above a tunable threshold Tiou are deemed\nto be stable periods in which the mask is likely to be accurate,\nin contrast to periods with small cross-frame IoU in which\nthe mask is changing drastically between frames. For the best\nframe, we focus on the longest stable period, and therein pick\nthe frame with the highest cross-frame IoU with its previous\nobject mask."}, {"title": "E. Non-maximum suppression", "content": "After tracking each object's best frame with the mask\ntracker XMem [11], we have a refined per-frame mask of\neach detected manipulated object. At this point, we remove\nduplicate object detections using a kind of non-maximum\nsuppression, based on mask overlap: For each ordered pair\n(tuple) of detected objects (A, B), we calculate the mean\npercentage (over all frames) of object A's mask that is\ncontained in object B's mask; we denote this (generally\nasymmetric) fraction as contains(B). As long as a tuple\nof objects (A, B) exists for which containsa(B) > Tnms,\ni.e. object A is contained to a sufficiently large extent\nin object B, we remove the object which is contained\nto the largest degree in some other object's mask, i.e.\narg max max contains(B). At this point, the detected\nobjects can be reconstructed and tracked using BundleSDF."}, {"title": "IV. EXPERIMENTS", "content": "To validate our method, we answer the following ques-\ntions:\n1) How does our pipeline perform compared to Co-Fusion,\nthe only baseline that is both class-agnostic and allows\nfor scanning objects fully?\n2) Why use our interaction-based approach to object dis-\ncovery rather than just finding objects using pre-trained\nsegmentation networks?"}, {"title": "A. Dataset", "content": "We evaluate on a custom dataset of 3 different scans\nin a tabletop setting, each containing 3 different objects\nthat are manipulated and hence form the foreground under\nreconstruction (see figure 5). The manipulated objects are\ntaken from a set of 5 different foreground objects for which\nwe have scanned high-fidelity 3D ground truth models with\na static scanning setup [30]. We capture our scenes with\nan iPhone 12 Pro at a resolution of 192x256 pixels and\na framerate of 60fps. A typical scan in our dataset lasts\nbetween 60 and 90 seconds, of which the first 10 to 30\nseconds are made up of the static scanning phase for the\nreconstruction of the initial point cloud."}, {"title": "B. Processing details", "content": "We process our scans on a machine with an NVIDIA\nRTX A5000 GPU (24GB GPU memory), 64GB RAM, and\nan Intel Core i9-10900X CPU running at 3.70GHz. For\ndense reconstruction of a detected object with BundleSDF,\nwe limit ourselves to processing every 5th frame in the\nspan of the detected interaction to reduce the computational\nworkload. With these settings, reconstructing one object"}, {"title": "E. Advantage over segmentation-based object discovery", "content": "Given the prominence of semantic segmentation for object\ndiscovery tasks, we want to compare our method with a\nsimple baseline where objects are discovered using semantic\nsegmentation. For a given foreground object, we manually\nchoose one frame in which the object is clearly visible, and"}, {"title": "F. Limitations", "content": "We observe several limitations of our pipeline. Firstly, it\nstill yields false positive object detections in some cases,\nwhich can generally be tracked to an inaccurate estimation\nof the distance between the hand point cloud and the initial\npoint cloud, both of which exhibit some noise. Secondly,"}, {"title": "V. CONCLUSION", "content": "In this work, we have presented an interaction-\nguided, class-agnostic approach to compositional scene\nreconstruction. Specifically, our pipeline allows a user\nto move around a scene with an RGB-D camera, pick\nup and lay down objects, and outputs one 3D model\nfor each picked-up object. Our main contribution is a\nmechanism to interaction-based object discovery that detects\nindividual object interactions and outputs one mask for\neach manipulated object. Combined with existing works in\nneural-representation-based unknown object reconstruction\nas well as 2D mask tracking, this results in a fully\nautomated, compositional scene reconstruction pipeline.\nIn our evaluations, we have shown the significance of\nthis pipeline: Firstly, we have shown that we outperform\nCo-Fusion, the only existing technique for compositional\nscene reconstruction that allows to scan objects fully while\nbeing class agnostic in its object discovery. Secondly, we\nhave demonstrated our advantage over segmentation-based\napproaches, showing that our discovered object masks yield\nreconstructions of comparable quality downstream while\nhaving the advantage of being class-agnostic as well as\nbeing capable of leveraging object interactions to break\ndown challenging scenes."}]}