{"title": "MMAU: A MASSIVE MULTI-TASK AUDIO UNDERSTANDING AND REASONING BENCHMARK", "authors": ["S Sakshi", "Utkarsh Tyagi", "Sonal Kumar", "Ashish Seth", "Ramaneswaran Selvakumar", "Oriol Nieto", "Ramani Duraiswami", "Sreyan Ghosh", "Dinesh Manocha"], "abstract": "The ability to comprehend audio\u2014which includes speech, non-speech sounds, and music is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction\u00b9 and reasoning 2 questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.", "sections": [{"title": "INTRODUCTION", "content": "The recent advancements in Large Language Models (LLMs) have fueled discussions around the development of generalist AI agents, often referred to as Artificial General Intelligence (AGI), capable\nWe define an information extraction question as one that requires a deep understanding of audio, detailed content analysis, and the application of external world knowledge when necessary.\n\u00b2We define a reasoning question as one that requires intentional, complex thinking beyond basic content understanding, analysis, and knowledge application, simulating expert-level cognitive processes."}, {"title": "RELATED WORK", "content": "Audio-Language Models. Recent years have seen significant progress in audio understanding, driven by advances in (large) language models that enhance cross-modal interactions between audio and language. Early research focused on developing cross-modal audio-language encoders (ALE) that learn shared representations between the two modalities. Notable models include AudioCLIP (Guzhov et al., 2022), CLAP (Elizalde et al., 2023), and CompA (Ghosh et al., 2023). CompA makes a first attempt to address reasoning in audio-language encoders by improving compositional reasoning through a novel training paradigm. More recently, efforts have shifted toward integrating audio understanding with LLMs, leading to the emergence of Large Audio-Language Models (LALMs). These include models such as Pengi (Deshmukh et al., 2023), Qwen-Audio (Chu et al., 2023), LTU (Gong et al., 2023c), and GAMA (Ghosh et al., 2024c). Leveraging the advanced reasoning capabilities of LLMs, LALMs can respond to complex queries involving audio inputs. For instance, GAMA demonstrates that instruction-tuned models can accurately interpret intricate questions about acoustic scenes. However, unlike humans who can perceive and reason across diverse audio types, LALMs have largely evolved in isolation, with specialized models focusing separately on sounds (e.g., Pengi, LTU, GAMA, etc.), speech (e.g., SALM (Chen et al., 2024), AudioPalm (Rubenstein et al., 2023), etc.), or music (LLark (Gardner et al., 2023), MERT (Li et al., 2023) and others (Liu et al., 2024b; Doh et al., 2023; Won et al., 2024)). Few models are capable of comprehensively understanding all three (e.g., Qwen-Audio (Chu et al., 2024), Audio Flamingo (Kong et al., 2024)).\nAudio Benchmarks. With the rapid rise of multimodal LLMs, there has been a significant surge in the development of comprehensive benchmarks for text and vision modalities to assess expert-level domain knowledge and advanced reasoning capabilities, including subject knowledge (Clark et al., 2018; Hendrycks et al., 2020), safety (Zhang et al., 2023b; Seth et al., 2023), multilingual proficiency (Ahuja et al., 2023), multidisciplinary understanding (Yue et al., 2024; Hu et al., 2024), perception tests (Yuan et al., 2023), mathematical reasoning (Li et al., 2024; Zhang et al., 2024), and video understanding (Li et al., 2023; Ning et al., 2023; Fu et al., 2024a). However, despite this progress, there is still a notable lack of similarly complex benchmarks for the audio modality. To address this gap, a few attempts have been made to build audio-language benchmarks for speech (e.g., OpenASQA (Gong et al., 2023b)), sound (e.g., CompA (Ghosh et al., 2023), CompA-R (Ghosh et al., 2024c)), music (e.g., MusicBench (Melechovsky et al., 2023), MuChin (Wang et al., 2024b), MuChoMusic (Weck et al., 2024)), and their combinations (e.g., AIR-Bench Yang et al. (2024), AudioBench Wang et al. (2024a)). These benchmarks, however, either focus on limited reasoning tasks like compositional or temporal reasoning Ghosh et al. (2023) or rely on fundamental audio tasks like ASR and acoustic scene classification Yang et al. (2024). To the best of our knowledge, no existing benchmark fully addresses the breadth and depth of reasoning required to evaluate advanced audio understanding, leaving a critical gap in the assessment of LALMs' capabilities."}, {"title": "THE MMAU BENCHMARK", "content": "We introduce the Massive Multi-Task Audio Understanding and Reasoning Benchmark (MMAU), a novel benchmark designed to evaluate the expert-level multimodal reasoning and knowl-edge-retrieval capabilities of large audio-language models (LALMs). MMAU comprises of carefully"}, {"title": "DATA CURATION AND ANNOTATION", "content": "We follow a rigorous 7-step pipeline for curating MMAU, described below:"}, {"title": "COMPARISON WITH OTHER BENCHMARKS", "content": "To highlight the distinction between current benchmarks and MMAU, we break down the informa-tion processing steps of a Large Audio-Language Model (LALM):\n\u2192 Knowledge Extraction (optional) \u2192 Reasoning (optional)\nAudio Understanding\nPerception\nMost existing benchmarks focus solely on audio understanding, assessing models on basic audio processing tasks like ASR, Speech Emotion Recognition, and other foundational tasks. These tasks primarily evaluate whether the model can comprehend the audio content\u2014such as spoken words,"}, {"title": "EXPERIMENTAL SETUP", "content": "LALMs. We compare a range of open-source, open-access, and closed-source LALMs, including (i) Qwen2-Audio-Chat (Chu et al., 2024): A open-access model (only checkpoints are available; train-ing data and details is unknown) with strong capabilities in sound, speech, and music understanding and reasoning. Qwen2-Audio-Instruct is a fine-tuned version with chat abilities based on Qwen2-7B as its LLM. (ii) GAMA (Ghosh et al., 2024c): A leading fully open-source model focused on sound and music understanding, built on LLaMa-2-7B. (iii) GAMA-IT is its fine-tuned variant for complex reasoning. (iv) SALAMONN (Tang et al., 2023): One of the first open-source LALMs, excelling in speech and sound understanding. (v) LTU (Gong et al., 2023c): A fully open-source model with"}, {"title": "RESULTS AND DISCUSSION", "content": "Table 3 compares the results of various LALMs on the MMAU benchmark. Our key findings are:\n1. MMAU poses a significant challenge. The MMAU benchmark is highly demanding for current models, both open-source and proprietary. The top-performing LALM achieves only 53% accuracy, while the best-cascaded captioning + LLM approach reaches just 59%. In comparison, human performance achieves 82%.\n2. Minimal gap between open-source and proprietary models. Unlike other domains, we observe only a small performance gap between the best open-source and proprietary LALMs. As shown in Table 3, Qwen2, the leading open-access model, performs almost on par with the proprietary Gemini-Pro, with just a 0.47% difference in average performance. However, the top fully open-source model, GAMA, trails significantly behind, with a larger performance gap of 21% compared to Gemini-Pro.\n3. Generalized vs. Specialized Models. Generalized models trained across multiple do-mains\u2014speech, sounds, and music\u2014such as Qwen2-Audio, LTU-AS, and Gemini, exhibit strong overall performance. This indicates that larger, more diverse training data leads to a more comprehensive understanding of audio.\n4. Models perform best on sound and worst on speech. With average scores of 18%, 30%, 23% across speech, sound, and music, models perform best on sound-related tasks and struggle the most with music. This suggests that while spoken language understanding has advanced, reasoning over spoken language\u2014especially perception beyond mere content\u2014remains a challenge. On the other hand LALMs have mastered music reasoning, a skill generally not possed non-experts.\n5. Cascaded approaches outperform others. Captioning audios first and then prompting LLMs yields the best results. Enhancing the quality of the captions further improves over-all performance. This demonstrates the potential of scaling audio-language understanding through separate advancements in audio and language reasoning."}, {"title": "ARE LALMS REALLY LISTENING?", "content": "Figure 5 compares the performance of vari-ous models on the MMAU test set, where theoriginal audio input is replaced with randomGaussian noise. This experiment helps assesswhether models are truly attending to the audioinputs or just respond using language-priors.As shown, the performance of MuLLaMa andSALMONN remains largely unaffected, indi-cating that these models may not always rely onthe audio input to generate responses. In con-trast, models like GAMA, Qwen2-Instruct, andGemini Pro v1.5 exhibit a significant drop in per-formance, suggesting that they are more atten-tive to the audio content. We provide examplesof incorrect outputs in Appendix I."}, {"title": "CAN CAPTIONS BRIDGE THE GAP FOR TEXT-ONLY MODELS?", "content": "Figure 5 compares the performance of various models on the MMAU test set, where the origi-nal audio input is replaced with captions. We present results using two types of captions: weak captions (generated using EnCLAP (Kim et al., 2024) for sounds, MuLLaMa for music, and Whis-per base (Radford et al., 2023) for speech transcripts) and strong, detailed captions (generated using Qwen2-Audio-Instruct with prompts detailed in Appendix L). As the results show, strong captions can indeed help bridge the audio understanding gap for text-only models, with GPT4o achieving the highest accuracy at 59%. Additionally, we demonstrate that enhancing the quality of captions signif-icantly boosts the performance of text-only LLMs (i.e., when captions effectively capture acoustic"}, {"title": "DEEP DIVE: SKILL-SPECIFIC MODEL PERFORMANCE", "content": "The average scores for Gemini Pro across easy,medium, and hard questions are 39.60, 43.82,and 36.03, respectively (detailed results forother models in Table 5). While it suggeststhat models perform consistently across diffi-culty levels, we wanted to dive deeper into skill-specific performance. Figure 6 illustrates theaccuracy distribution across easy, medium, andhard questions for eight skills with the highestnumber of questions in the benchmark. Sur-prisingly, the reason for the uniformity acrossdifficulty levels is that models excel in cer-tain skills across all difficulties (e.g., PhonemicStress Pattern Analysis), but consistently strug-gle with others (e.g., Temporal Reasoning), re-gardless of the question's difficulty. This ob-servation highlights that rather than focusingon improving model performance in a singleskill, future work should focus on developinga broader range of competencies, ensuring they can handle complex reasoning across various tasks."}, {"title": "PINPOINTING LALM WEAKNESSES: WHERE ARE THEY FALLING SHORT?", "content": "Figure 7 provides a breakdown of the error types made by Qwen2-Audio-Instruct and Gemini Pro v1.5across 500 instances. The dominant error category for both models is Perceptual Errors, withQwen2-Audio-Instruct showing 55% and Gemini Pro v1.5 at 64%. This indicates that both modelsstruggle primarily with understanding and accurately perceiving the audio inputs. Reasoning Er-rors and Answer Extraction Errors (Errors where model outputs and ground-truth answers aresame but the model provided an ill-formatted response) account for a significant portion of mis-takes, particularly for Qwen2-Audio-Instruct, where 18% of errors are reasoning-based, suggestingthat even when models correctly perceive the audio, they often fail to apply the necessary complexreasoning. For Gemini 1.5 Pro, reasoning errors account for 11%, while answer extraction errorsremain consistent between both models. Interestingly, Knowledge Errors and Annotation Errorsform smaller percentages. Overall, our error analysis highlights that improving perceptual under-standing is crucial for better performance. This can be done through more training data (Liu et al.,2023a), better architectures (Ghosh et al., 2024c) or other methods (Fu et al., 2024b)."}, {"title": "CONCLUSION, LIMITATIONS AND FUTURE WORK", "content": "In this paper, we introduce MMAU, a novel large-scale benchmark designed to comprehensively evaluate multimodal audio understanding and reasoning in Al models. MMAU challenges models with a diverse set of tasks that assess 27 distinct skills, emphasizing advanced perception and domain-specific reasoning. The benchmark presents tasks akin to those faced by experts, making it a rigorous test for AI systems. Our evaluations of 18 open-source and proprietary LALMs reveal that even the overall best model achieves only 59% accuracy on MMAU, highlighting the significant challenges it poses. We also provide a detailed analysis of the unique hurdles presented by this benchmark.\nAs part of future work, we aim to address in future iterations of MMAU: (i) Currently, we treat skills required for information extraction and reasoning as disjoint sets. As part of future work, we plan to incorporate tasks that require skills from both types. (ii) There is a risk of biases introduced during the human or LLM-driven annotation process. We aim to further refine the dataset to minimize any potential biases. (iii) MMAU focuses on multiple-choice tasks and does not evaluate open-ended generation, which allows models to reason more freely and exhibit errors such as language hallucinations. Including open-ended tasks will help us better understand these kinds of errors. (iv) Lastly, we plan to broaden the range of tasks and skills covered by MMAU to enhance its challenge and relevance to future models."}, {"title": "APPENDIX", "content": "Table of Contents:\n1. B Additional Results\n2. C Annotation Details\n3. D Model Details\n4. E Dataset Details\n5. F Annotation Tool\n6. G Comparison\n7. H Question Categories\n8. I Failure Cases"}, {"title": "ADDITIONAL RESULTS", "content": "B.1 AUDIO-LANGUAGE ENCODERS (ALES)\nALES To asses how CLAP-like Audio-Language Encoders (ALEs) perform on MMAU as shown in Table 4, we evaluate several open-source ALEs, including (i) CLAP, a fully open-source model designed primarily for sound and music comprehension. We tested different variants of CLAP, such"}]}