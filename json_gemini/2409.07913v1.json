{"title": "UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints", "authors": ["Inzamamul Alam", "Muhammad Shahid Muneer", "Simon S. Woo"], "abstract": "In the wake of a fabricated explosion image at the Pentagon, an ability to discern real images from fake counterparts has never been more critical. Our study introduces a novel multi-modal approach to detect Al-generated images amidst the proliferation of new-generation methods such as Diffusion models. Our method, UGAD, encompasses three key detection steps: First, we transform the RGB images into YCbCr channels and apply an Integral Radial Operation to emphasize salient radial features. Secondly, the Spatial Fourier Extraction operation is used for a spatial shift, utilizing a pre-trained deep learning network for optimal feature extraction. Finally, the deep neural network classification stage processes the data through dense layers using softmax for classification. Our approach significantly enhances the accuracy of differentiating between real and AI-generated images, as evidenced by a 12.64% increase in accuracy and 28.43% increase in AUC compared to existing state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Rapid advancements in Generative AI have significantly impacted the digital realm, especially with the proliferation of content generation tools. This evolution has made creating and disseminating fake images easier, posing a challenge in distinguishing them from real ones and heightening the risk of misinformation. The advent of generative adversarial networks (GANs) and Stable Diffusion (SD) has notably enhanced AI's capability in photo-realistic content generation, presenting both creative opportunities and challenges, particularly in assuring content authenticity. As AI-generated content increasingly mimics reality, serious concerns have been raised over its potential misuse in activities. To address this, our paper focuses on detecting AI-generated fake images within the evolving AI landscape. Various deep-learning approaches, including pre-processing strategies to obtain power spectra for classification [9, 69], have been proposed to tackle this challenge. However, they fall short due to advancements in AI generation methods. Additionally, different forensic methods have been explored [10]. However, the accuracy and architectural adequacy of many existing fake image authentication methods need to be improved to effectively detect the latest AI-generated images. Benchmark datasets such as FaceForensics++ [54], CelebDF [41], and FakeAVCeleb [24] have been instrumental in classifying images from varied generation methods. However, they fall short because they do not include the latest Al-generated datasets and may not be able to cope with those methods.\nIn this work, we propose UGAD, which combines spectral forensic analysis with deep learning classification to detect the latest Al-generated fake images effectively. The overview of our approach is presented in Fig. 1. As shown in Fig. 1, we start by pre-processing images through two parallel processes. First, we convert RGB images to YCbCr color space and apply FFT on each pixel, merging them into a 2D image represented in an XY plane. Next, a Radial Integral Operation (RIO) operation is then applied to create a 1D array of vector values for different radii, capitalizing on the distinct power spectra of real and AI-generated images. The RIO output for fake Al-generated images is constant, while the fluctuations in the real images are not. In the next phase, we transform RGB images per channel and merge them to form a 3D image. FFT is applied to each channel, split, and concatenated into one image. We then employ Spatial Feature Extraction (SFE) to extract prominent features, followed by a spatial shift transformation. The processed image is fed into a ResNet152 architecture, culminating in softmax classification. In summary, our contributions include:\n\u2022 We introduce a novel Radial Integral Operation (RIO) in YCbCr color space, enhancing object recognition in diverse lighting components.\n\u2022 We propose a new Spatial Fourier Extraction (SFE) method to convert spatial features into the spectral domain, and globally update spectral data.\n\u2022 Our approach was rigorously tested with images from the latest Al-generative methods such as faces, scenes, and objects, and outperformed existing methods."}, {"title": "2 RELATED WORK", "content": "Several deepfake detection methods have been proposed in the past [2, 8, 18\u201320, 24\u201340, 48, 55\u201361, 64\u201368]. Wu et al. [69] developed a general classification approach, while Corvi et al. [9] focused on pre-processing techniques to extract power spectra for improved classification. Radford et al. [50] proposed language supervision-based perceptual learning, and Wang et al. [63] introduced DIRE for detecting diffusion-generated images. Cozzolino et al. [11] developed a GAN image detection method using a ResNet152 backbone, and Zhang et al. [70] achieved high accuracy using Discrete Cosine Transform (DCT) for power spectra analysis. However, their focus was limited to a single stable diffusion dataset. Jeong et al. [21] use power spectrum and train the generated models on a generator and discriminator network. At the same time, we have applied RIO to accumulate the density of the power spectrum. In every radius, the density of the power spectrum for the fake images is mostly constant. We aim to use this phenomenon to classify real and fake images effectively. Other Forensic techniques have also been explored, with Corvi et al. [10] employing noise prints for camera fingerprint extraction. Mandelli et al. [46] used a forensic approach for distinguishing real and fake Western blot images, albeit limited to a specific image type. Ma et al. [43] applied statistical and neural network-based methods to detect fingerprints in real vs. fake images, exploiting unique properties of the image generation process. However, our work differs from the above approaches because we have extracted fake footprints and distinct features in the frequency domain to more accurately and effectively classify the model fake methods."}, {"title": "3 OUR APPROACH", "content": "The initial stage of our method, UGAD is designed to pre-process input RGB images to extract critical spectral information, where a dataset comprising inputs as real X and AI-generated images Y are defined as follows:\nX x Y = {(x_i, y_j) | x_i \u2208 X, y_j \u2208 Y and i \u2208 {1, ..., M}}, (1)\nwhere each (x, y) is a pair of images containing real and fake images within our pre-processing pipeline, and M is the last index.\nConversion to YCbCr Color Space. To prepare the images for spectral analysis, our initial step involves the conversion of input RGB images into the YCbCr color space. This transformation provides distinct channels for Luminance (Y) and Chrominance (Cb and Cr). The Y channel, in particular, encapsulates essential image details, which are later used in extracting FFT features. Stage 1 in Fig. 1 presents the conversion of RGB images to YCbCr images, while constants in the below equations are used to extract Luminance and Chrominance. The constants in the RGB to YCbCr conversion Eq. 2 originate from the ITU-defined transformation matrix, reflecting perceived luminance and chrominance. They ensure accurate representation of brightness and color information in the YCbCr color space. Specifically, the conversion equations are"}, {"title": "3.1 Radial Integral Operation (RIO)", "content": "defined as follows:\nY(i, j) = 0.299 R(i, j) + 0.587 G(i, j) + 0.114. B(i, j)\nC_p(i, j) = 128 -0.168736 R(i, j) - 0.331264. G(i, j) +0.5. B(i, j)\nC_r(i, j) = 128 +0.5R(i, j) \u2013 0.418688 G(i, j) 0.081312 B(i, j),\n(2)\nwhere Y(i, j) represents the luminance (Y) component of the YCbCr image, Cb(i, j) corresponds to the blue chrominance (Cb) component, and Cr(i, j) denotes the red chrominance (Cr) component, respectively. Furthermore, R(i, j), G(i, j), and B(i, j) represent the red, green, and blue channel values of the pixel at position (i, j) within the original RGB image, respectively.\nFast Fourier Transformation (FFT). Our next step involves applying the FFT operation to each pixel of the YCbCr image. We choose FFT, because the frequency information can effectively extract patterns for Al-generated fake methods. The FFT operation is defined as follows: FFT(k) = \\sum_{n=0}^{N-1} (Re(x(n))+j.Im(x(n)))e^{-jkn}.\nAnd, it is applied to each channel of the YCbCr in the following way: Y'(i, j) = FFT(Y(i, j)), Cb\u2032(i, j) = FFT(Cb(i, j)), and Cr' (i, j) = FFT(Cr(i, j)).The outcome of the FFT operation encompasses both real (Re) and imaginary (Im) components for each pixel, effectively transforming the image into the frequency domain FFT(k) on the basis of complex discrete-time signals x(n).\nMerging Spectral Information. The real Re and imaginary Im components derived from all channels are subsequently merged into a singular 2D image, where Stage 2 in Fig. 1 characterizes the result from the operations. This combined image serves as a representation of spectral information within the XY coordinates and is mathematically expressed as follows:\nXY(i, j) = FFT (Y (i, j)) + FFT(Cb(i, j)) + FFT (Cr(i, j)) (3)"}, {"title": "3.1.1 Quadrant Analysis and Radial Integral Operation (RIO).", "content": "To further enhance the valuable features from spectral content, we apply an RIO, which draws a circle across the 2D plane from each radius of the image spanning from (0,0) point and employs an integration function to compute spectral information at varying radii. The simplified radial integral operation (RIO) in Stage 3 in Fig. 1 is defined as follows:\nf(r) = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} f (r, \\theta) d\\theta (4)\nThis RIO culminates in a 1D array capturing RIO value across multiple radii, encompassing numerous pixels. And, finding all pixel's Fourier weight within the individual radius is defined as follows:\nM = \\sum_u\\sum_v |F(u, v)|, (5)\nwhere F(u, v) represents the Fourier coefficient at spatial frequency coordinates (u,v) and \\sum\\sum is taken over the Fourier co-efficient with the circular region radius Ri, and (u,v) needs to satisfy the following condition:\n(u \u2013 u_{center})^2 + (v \u2013 v_{center})^2 <= R^2 (6)\nFrom Eq. 4, our expected f (r, 0) can be derived by computing the integral of the squared magnitude of the Fourier transform of the image over a range of radii ri. This operation captures the spatial frequency distribution of the image, enabling analysis of its structural characteristics and frequency components.\nf(r, \\theta) = \\| \\int_{r_i}^{R_i} \\frac{3M}{4\\pi r} r_i dr_i (w_k cos(\\theta), w_k sin(\\theta)) \\|^2, (7)\nwhere R_i = r_0, r_1, ... r_n and W_k = \\frac{2\\pi k}{N}, and k is the magnitude value, and N is the total number of pixels on that r_i, respectively. In Section 4.2.5, we present that, for different fake images, the spectrum lines in RIO do not overlap each other. The advantage of non-overlapping frequency information is that fake images fluctuate constantly, which differs from real images. This indicates and captures that RIO provides valuable information and can assist in representing features to distinguish real vs. fake images."}, {"title": "3.2 Spatial Fourier Unit (SFU)", "content": "Next to extract the spatial features from the frequency domain of the images, which involves the following steps: First, let IRGB (i, j, c) represent the input RGB image for each color channel R, G, and B, and let IFFT (i, j, c) be the image in the frequency domain after FFT, where each channel from RGB individually transforms with FFT in the following way:\nI_{FFT} (i, j, c) \u2192 FFT (I_{RGB} (i, j, c)) (8)\nAfter that, we split a Fourier transformed image IFFT (i, j, c) into four quadrants as shown in Eq. 9: Top Left (TL), Top Right (TR), Bottom Left (BL), and Bottom Right (BR). This split operation in Eq. 9 plays a pivotal role in extracting essential spatial frequency features for Spatial Fourier Extraction (SFE), which halves both spatial dimensions and renders four smaller feature maps for retaining valuable artifacts (See Section 4.2.3 for more explanation).\nI_{split} \u2192 SPLIT (I_{FFT} (i, j, c))\nI_{splitTL} = I_{FFT} (i, j)  for 1 \u2264 i \u2264 \\frac{H}{2}, 1 \u2264 j \u2264 \\frac{W}{2}\nI_{splitTR} = I_{FFT} (i, j + \\frac{W}{2}) for 1 \u2264 i \u2264 \\frac{H}{2}, \\frac{W}{2} + 1 \u2264 j \u2264 W\nI_{splitBL} = I_{FFT} (i+\\frac{H}{2}, j) for \\frac{H}{2} + 1 \u2264 i \u2264 H, 1 \u2264 j \u2264 \\frac{W}{2}\nI_{splitBR} = I_{FFT} (i+\\frac{H}{2}, j+\\frac{W}{2}) for \\frac{H}{2} + 1 \u2264 i \u2264 H, \\frac{W}{2} + 1 \u2264 j \u2264 W, (9)\nwhere the SPLIT function performs the split operation in four parts: I_{splitTL},I_{splitTR}, I_{splitBL}, and I_{splitBR} as shown in Stage 6 in Fig. 1, and, the channel C represents the number of channels in each of the splitting parts.\nAlso, we define I_{Concat} to be a new image formed by concatenating the above four parts along the channel dimension in Eq. 10, where I_{Concat} stacked feature maps from Eq. 9 for increasing nonlinearities, And, I_{Concat} constrcuts deeper layers, which is less prone to overfitting, as they can capture more diverse and discriminative features. The new image, I_{Concat} has 4C channels depicts in Stage 7 in Fig. 1. This operation results in an image with the height and width (H/2 and W/2), respectively, with an increased number of channels, combining information from all four parts into a single image with richer channel-wise information as shown in Stage 7.\nI_{Concat} = Concat([I_{splitTL},I_{splitTR}, I_{splitBL}, I_{splitBR} ], axis = 2) (10)"}, {"title": "3.2 Spatial Fourier Unit (SFU)", "content": "Furthermore, we apply the Spatial Fourier Extraction (SFE) operation in Stage 8 as shown in Fig.1. The SFE transforms spatial features into a frequency domain, conducting efficient global updates on frequency data. The SFE is used in Eq. 11 to obtain the ILatent, where ILatent contains all the important spectral feature information from IConcat. Deriving ILatent from Eq. 11 is further discussed in Section 3.2.\nI_{Latent} (i, j, c) \u2192 Spatial\\_Fourier\\_Extraction (I_{Concat} (i, j, c)) (11)\nSpatial Fourier Extraction (SFE). Our proposed method SFE in Stage 8 is shown in Figure 1, which extracts spectral features from the augmented IConcat image. The SFE is conducted in the following three steps:\nStep 1. The spatial operation applies a depthwise convolution kernel K to a feature map X as follows:\nY_{i,j}^{(k)} = \\sum_{l=1}^M X_{i+m,j+n}^{(l)} * K_{m,n}^{(l)}, (12)\nwhere i and j represent spatial indices, and Y_{i, j}^{(k)} represents the output feature map at position (i, j) with depth k, and X_{i+m,j+n}^{(l)} represents the input feature map at position (i+m, j+n) with depth l. And, K_{m,n}^{(l)} represents the depthwise convolution kernel at position (m, n) with depth l, and C represents the augmented height from Eq. 10, and the symbol * denotes the convolution operation. Step 2. The next step of our SFE method is to normalize the output from Eq. 12. First, let us define that we have a feature map (Y) after the previous step with a shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the spatial dimension, respectively.\nNext, we introduce a covariance-based normalization approach to calculate the second-order statistics (covariance matrix) Couc within each channel c for each mini-batch as follows:\nCov_c = \\frac{1}{N} \\sum_{i=1}^N (Y[i, c, :, :] \u2212 \u03bc_c) (Y[i, c, :, :] \u2212 \u03bc_c), (13)\nwhere \u03bcc represents the batch mean for channel c. For each channel c, we normalize the activations using the inverse square root of the covariance matrix to stabilize the operation as follows:\nZ[i, c, :, :] = \\gamma_c (Y[i, c, :, :] \u2212 \u03bc_c) (Cov_c + \u03f5)^{-1/2} + \u03b2_c, (14)\nwhere \u03b3 is the shift and \u03b2 is the scale parameter, and \u03f5 is a small constant added for numerical stability, which is in the range of \u03f5 and is typically in the order of magnitude of 1 \u00d7 10-5 to 1 x 10-6. This range is small enough to avoid numerical instability while not affecting the normalization process significantly. This covariance normalization in Eq. 14 stabilizes the activations and standardizes the feature distributions across channels and spatial locations. Specifically, subtracting the channel mean centers the data while normalizing by the square root of the covariance matrix scales the variances. The learnable parameters \u03b3c and \u03b2c further tune the normalized activations.\nOverall, this data-dependent normalization in Step 2 adaptively standardizes the representations spatially and across channels. We can model more complex second-order statistics for effective feature normalization than simple channel-wise means and variances. The normalized output Z can also serve as more robust intermediate representations for subsequent processing in Step 3."}, {"title": "3.2 Spatial Fourier Unit (SFU)", "content": "Step 3. The last step in SFE is to apply a modified ReLU activation function is applied after the depthwise normalization process, which can be represented as follows:\nReLU(Z) = max(0, Z);\nFReLU(Z) = ReLU(Re(Z)) + iReLU(Im(Z)), (15)\nwhere Z is an element in the feature map. Assuming that the positive and negative values of the complex-valued images are represented in the four quadrants, FReLU has the advantage that information can be obtained from three quadrants among four. Applying the inverse FFT on both real and imaginary components, Eq. 15 guarantees the generation of a Hermitian matrix characterized by its symmetry. It facilitates the production of real-valued outputs suitable for subsequent neural network computations [3].\nLastly, in SFU, let I\u2081 (i, j, c), I\u2082 (i, j, c), I\u2083 (i, j, c), and I\u2084 (i, j, c) be the four copies of the ILatent. And, we stack them in the specific arrangement via a spatial shift method as follows:\nI_{Spatial\\_Shift} (i, j, c) = \n  \\begin{cases}\n    I\u2081 (i, j, c) & I\u2081 \u2208 \\{(i, j, c) | 1 \u2264 i \u2264 \\frac{H}{2}, 1 \u2264 j \u2264 \\frac{W}{2},1 \u2264 c \u2264 4C\\}\\\\\n    I\u2082 (i, j + \\frac{W}{2}, c-4C) & I\u2082 \u2208 \\{(i, j,c) | 1 \u2264 i \u2264 \\frac{H}{2}, \\frac{W}{2} + 1 \u2264 j \u2264 W, 4C + 1 \u2264 c \u2264 8C\\}\\\\\n    I\u2083 (i+\\frac{H}{2}, j, c-8C) & I\u2083 \u2208 \\{(i, j, c) | \\frac{H}{2} + 1 \u2264 i \u2264 H, 1 \u2264 j \u2264 \\frac{W}{2}, 8C +1 \u2264 c \u2264 12C\\}\\\\\n    I\u2084(i + \\frac{H}{2}, j + \\frac{W}{2}, c \u2013 12C) & I\u2084 \u2208 \\{(i, j, c) | \\frac{H}{2} +1 \u2264 i \u2264 H, \\frac{W}{2} + 1 \u2264 j \u2264 W, 12C + 1 \u2264 c \u2264 16C\\},\\\\\n  \\end{cases} (16)\nwhere\nIn the above representation, Ispatial_Shift is the stacked image created by arranging I1, I2, I3, and I4 in a specific manner: I\u2081 in the top-left, I\u2082 in the top-right, I\u2083 in the bottom-left, and I\u2084 in the bottom-right positions, as shown in Stage 9 in Fig. 1. The height and width dimensions can be perfectly restored to the original size before any downsampling or manipulations by generating four shift-copied versions of each activation in this precise layout. Propagating this quadruplicated information facilitates lossless transmission of all visual details encoded within the initial feature map. Irrecoverable losses can occur due to misalignment between original and transformed feature grid dimensions without this tailored spatial shifting procedure to copy and rearrange components. Overall, this deliberate copying and positioning enables dimensional restoration without permanent losses of spatial information. Further explanation for the significance of split, concat, and shift is demonstrated in Section 4.2.3."}, {"title": "4 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "Datasets. Our dataset consists of both existing popular datasets from Wu et al. [69] and self-generated images using the latest open source methods such as Stable diffusionV1.2 [52] by StableAI [47], DreamBooth [45], and Latent Diffusion by CompVis models. Fake images in the test dataset contain samples from eleven generation methods such as ProGAN [22], StyleGAN2 [23], StyleGAN3"}, {"title": "4.1 Comparison with SOTA Methods", "content": "We extensively evaluated our approach on different datasets, utilizing various pre-trained ResNet models for feature extraction. These models include ResNet18, ResNet32, ResNet50, ResNet101, and ResNet152, respectively. We show that ResNet152 is the most suitable choice for the ablation study. Hence, we use ResNet152 to evaluate other SOTA methods in Table 1. As shown in the last row in Table 1, our method achieves the highest average AUC and accuracy, compared to the existing SOTA approaches: Wang et al., [63], Chandrasegaran et al., [7], Chai et al., [5], Grag et al., [15], and Xu Zhang et al. [71]. We find that our approach outperforms other SOTA methods in the majority of cases, except BigGAN, which is the second best. In addition, we conducted an experiment involving the testing of 10k images, as delineated in Table 2, and found that our proposed method performs better in classifying the real vs. fake in terms of average accuracy compared to CLIP on all methods."}, {"title": "4.2 Ablation Studies", "content": "We analyzed the accuracy performance of various ResNet architectures (see Table 3). Across all ResNet architectures, we consistently noticed increased performance with our SFU and RIO component. Specifically, for ResNet50, mean accuracy improves from 67% without additional components to 82.1%, with RIO + SFU. Similarly, the performance of ResNet101 increases from 75.2% to 87.2%. ResNet152 showed increased performance from 78.7% to 93.0%."}, {"title": "4.2.1 Effect of Different ResNet architectures.", "content": "SFU enhances spatial feature extraction with advanced up-scaling techniques. This SFU component resulted in high-resolution datasets and increased performance across all ResNet architectures. The most significant gains are observed in high-resolution datasets such as Artist & Danbooru, illustrating SFU's effectiveness in enhancing spatial details. Also, RIO enhances the regions of interest, reducing computational overhead by deriving a 1D array from an image while increasing accuracy across all models, especially in ResNet152 combined with SFU. This results in 3% increase. This shows RIO improves feature extraction for accurate classification. Moreover, SFU + RIO integration yields accuracy improvements, notably 3% increment in ResNet152, which is effective in complex datasets such as Artist & Danbooru. This improvement across different ResNet architectures indicates the robustness of the combined approach.\nAlso, SFU without Split and Shift (SFU) shows decreased accuracy across all architectures, with ResNet50 dropping from 77.6% to 72.42% and ResNet152 from 90% to 84.82%. The absence of split and shift leads to less effective feature extraction. Lastly, RIO without YCbCr Conversion (RIO) results in reduced accuracy for ResNet50 from 82.1% to 79.72%, which is more pronounced in color-critical datasets such as AD/GLIDE and AD/LD. ResNet152's accuracy falls from 93% to 91.58%, affirming the importance of YCbCr conversion in maintaining high performance across architectures."}, {"title": "4.2.2 Effect of Our Proposed Components", "content": "Incorporating shifting operations is essential for efficient information exchange among neighboring pixels. The shift operation strategically replicates critical features across top-right, top-left, bottom-right, and bottom-left positions, ensuring the retention of crucial information when feeding SFU-extracted features into the ResNet architecture and mitigating information loss. As assessed by accuracy (Acc), in Table 4, we present the outcomes delineating the efficacy of split and shift operations on the practical dataset Tgen. We can observe that the preeminent methods. It is noteworthy that all methodologies were trained on the ProGAN subset and subsequently assessed for generalization across the remaining 6 subsets. The variants denoted as Wang-0.1 and Wang-0.5 signify models trained with 10% and 50% data augmentation, respectively.\nOur proposed methodology, designated as \"Ours\", attains the highest accuracy across all test datasets, achieving 0.936 for DreamBooth [45], 0.899 [17] for MidjourneyV4, 0.853 for MidjourneyV5 [17], 0.922 for NightCafe [53], 0.878 for StableAI [47], and 0.731 for YiJian [44]. These results demonstrate our approach's efficacy in enhancing performance on practical datasets with split and shift operations."}, {"title": "4.2.3 Effectiveness of Split and Shift Operation in SFU", "content": "Data Augmentation evaluates how well each detector handles post-processing on images with various adjustments. We consider four operations based on [62]: (1) No augmentation; (2) Gaussian blur with a 50% chance, blur strength (sigma) from 0 to 3; (3) JPEG compression with a 50% chance, compression quality from 30 to 100; (4a) Blur and JPEG compression with a 0.5 probability for each; (4b) Similar to (4a) but with a 10% probability. We train our model with the augmented dataset and apply it to the practical dataset Tgen. Results in Fig. 3 show detector performance with various augmentations. DreamBooth [45] maintains high accuracy but decreases"}, {"title": "4.2.4 Effectiveness of Data Augmentation.", "content": "distinct images are presented to demonstrate the efficacy of spectrum analysis with our RIO. Specifically, utilizing FFT facilitates clear differentiation between lines generated through various methods. Notably,"}, {"title": "4.2.5 Spectrum Analysis in RIO Process.", "content": "For effectiveness of real-world scenario, we took more than 3,000 real-world images consisting FaceSwap generation methods, where some samples are presented in Fig. 5. Our proposed approach benchmarks at shortest inference time of about 400 milliseconds on average.\nOverall, our ablation study reveals that both SFU and RIO components play a pivotal role in enhancing the model's performance. This enhancement is particularly noticeable due to their integration, which proves highly effective in deeper networks, such as ResNet152. We identify the split and shift mechanism within SFU and the YCbCr conversion in RIO as critical sub-components. Removing these components results in significant drops in accuracy. This comprehensive ablation study shows the importance of these components in improving spatial feature processing and region-specific analysis, affirming our architectural choices."}, {"title": "4.2.6 Inference Time for Unknown Images.", "content": "Currently, we have deployed our demo system in a live web server and it has been deployed since Oct. 2023 to help detect real-world deepfakes. Users can upload videos or images, and our approach can provide the results through an analysis tool containing detection results with a CSV file, a frame detection graph, a pie chart, etc. We hope that our tools can be used more widely for people to use to detect Al-generated content."}, {"title": "5 DEPLOYMENT AND REPRODUCIBILITY", "content": "In this work, we introduce UGAD to combat new types of AI-generated fake images, where we observed a consistent pattern of frequency distribution among those fake images. Our approach focuses on extracting frequency domain features from the YCbCr color space and introduces Spatial Feature Extraction (SFE) to enhance the frequency features. Our extensive experimental results demonstrate that our approach surpasses the performance of other SOTA methods over various types of AI-generated content, including the latest generative model. Our work shows a promising avenue for robustly classifying fake images in practical scenarios."}, {"title": "6 CONCLUSION", "content": "This work was partly supported by Institute for Information & communication Technology Planning & evaluation (IITP) grants funded by the Korean government MSIT: (RS-2022-II221199, RS-2024-00337703, RS-2022-II220688, RS-2019-II190421, RS-2023-0023 0337, RS-2024-00356293, RS-2022-II221045, and RS-2021-II212068)."}]}