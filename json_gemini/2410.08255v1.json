{"title": "Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning", "authors": ["David D. Baek", "Yuxiao Li", "Max Tegmark"], "abstract": "Motivated by interpretability and reliability, we investigate how neural networks represent knowledge during graph learning, We find hints of universality, where equivalent representations are learned across a range of model sizes (from 102 to 109 parameters) and contexts (MLP toy models, LLM in-context learning and LLM training). We show that these attractor representations optimize generalization to unseen examples by exploiting properties of knowledge graph relations (e.g. symmetry and meta-transitivity). We find experimental support for such universality by showing that LLMs and simpler neural networks can be stitched, i.e., by stitching the first part of one model to the last part of another, mediated only by an affine or almost affine transformation. We hypothesize that this dynamic toward simplicity and generalization is driven by \"intelligence from starvation\": where overfitting is minimized by pressure to minimize the use of resources that are either scarce or competed for against other tasks.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs), despite being primarily trained for next-token predictions, have shown impressive reasoning capabilities (Bubeck et al., 2023; Anthropic, 2024; Team et al., 2023). However, despite recent progress reviewed below, it is not well understood what knowledge LLMs represent internally and how they represent it. Improving such understanding could enable valuable progress relevant to transparency, interpretability, fairness and robustness, for example\n\u2022 discovering and correcting inaccuracies to improve model reliability.\n\u2022 discovering and correcting bias,\n\u2022 revealing and removing dangerous knowledge (relevant to bioweapon design, say),\n\u2022 detecting deceptive behavior where models deliberately output information inconsistent with its knowledge.\nThe goal of this paper is to deepen our our understanding of learned knowledge representations by focusing specifically on the representations learned of knowledge graphs (KGs), which are loosely speaking a discrete set of entities with various relations between them. KGs provide a valuable test-bed because, although they are simpler and more structured than the totality of implicit knowledge LLM training corpora, they can nonetheless capture a massive amount of valuable human knowledge. We focus on applying the approach of mechanistic interpretability not to learned algorithms, but to learned knowledge.\nThe rest of this paper is organized as follows: We relate our approach to prior work in Section 2. In Section 3, we formally describe our problem settings, and Section 4 explains various methods of measuring representation alignment, including model stitching. Section 5 presents hints of universality via LLM stitching, and Section 6 takes a further glimpse into the geometrical structure of LLMs' representations. We discuss our starvation hypothesis for hints of universality in Section 7, and summarize our conclusions in Section 8."}, {"title": "RELATED WORK", "content": "Following the drastic enhancement of LLMs' capabilities, understanding the inner workings of Large Language Models have become increasingly important to ensure the safety and robustness of AI systems (Tegmark & Omohundro, 2023; Dalrymple et al., 2024).\nMechanistic Interpretability Neural Networks have demonstrated a surprising ability to generalize (Liu et al., 2021; Ye et al., 2021). Recently, there have been increasingly more efforts on trying to reverse engineer and interpret neural networks' internal operations (Zhang et al., 2021; Bereska &\nGavves, 2024; Baek et al., 2024). Such methods include using structural probes and interventions\nat the level of entire representations (Hewitt & Manning, 2019; Pimentel et al., 2020), and studying\nneuron activations at the individual neuron level (Dalvi et al., 2019; Mu & Andreas, 2020). Our\nwork contributes to these efforts, aiming to understand the knowledge representations in LLMs.\nKnowledge Representations in Language Models Several studies have presented evidence for op-\ntimism about LLMs' intelligence, showing that LLMs are capable of forming conceptual representa-\ntions in spatial, temporal, and color domains (Gurnee & Tegmark, 2023; Abdou et al., 2021; Li et al.,\n2021). Some studies focused primarily on examining the linearity of LLMs' feature representations\n(Gurnee & Tegmark, 2023; Hernandez et al., 2023). Several works have found multi-dimensional\nrepresentations of inputs such as lattices (Michaud et al., 2024) and circles (Liu et al., 2022; Engels\net al., 2024), one-dimensional representations of high-level concepts and quantities in large language\nmodels (Gurnee & Tegmark, 2023; Marks & Tegmark, 2023; Heinzerling & Inui, 2024; Park et al.,\n2024).\nLanguage model representations could be viewed as extensions of early word embedding methods\nsuch as GloVe and Word2vec, which were found to contain directions in their vector spaces corre-\nsponding to semantic concepts, e.g. the well-known formula f(king) - f(man) + f(woman) = f(queen)\n(Drozd et al., 2016; Pennington et al., 2014; Ma & Zhang, 2015). Language model representations\ncould also be viewed as a generalization of traditional knowledge graph embedding models such\nas TransE (Wang et al., 2014), ComplexE (Trouillon et al., 2016), and TransR (Lin et al., 2015),\ntypically embed both entities and relations in the latent space, and optimize the score function to\nperform link prediction task.\nRepresentation Alignment and Model Stitching There are active discussions in the literature about\nstrengths and weaknesses of different representation alignment measures (Huh et al., 2024; Bansal\net al., 2021; Sucholutsky et al., 2023). Several works have considered stitching to obtain better-\nperforming models, such as stiching vision and language models for image and video captioning\ntask (Li et al., 2019; Iashin & Rahtu, 2020; Shi et al., 2023), and stitching BERT and GPT for\nimproved performance in look ahead section identification task (Jiang & Li, 2024). Some works have\nconsidered stitching toy transformers to understand the impact of activation functions on model's\nperformance (Brown et al., 2023). Our work considers stitching LLMs to examine the hints of\nuniversality across scales, and to better understand the knowledge processing process of LLMs."}, {"title": "PROBLEM FORMULATION", "content": "In this section, we define our notation and terminology.\nKnowledge Graph Learning: Consider a general knowledge graph (KG) consisting of m binary relations R(1), R(2), ... R(m) between n objects (nodes) X1, ..., Xn. A KG simply generalizes the concept of a directed graph (a single binary relation R(xi, xj) \u2192 {0,1} with directed edges linking nodes R(xi, xj) = 1) by allowing multiple types of edges R(i) which we may imagine having different colors. Our machine-learning task is link prediction: to predict the probability Pijk that R(i) (xj, xk) = 1 by training on a random data subset. If desired, we can further generalize from binary relations to allow relations that take fewer or more arguments (unary or ternary relations, say). A trivial example is where xi = i, m = 1 and R(1)(xi, xj) = 1 iff i > j. A richer example\nwe will study involves family trees where each node xi is a person and there are relations such as\n\"sister\", \"mother\u201d, \u201cdescendant\" and \"spouse\u201d.\nKG representations: Many of the most popular KG-learning-algorithms embed both the relations\nR\u2081 and the elements xj in a vector space Rd and train a link prediction function Pijk of some clever"}, {"title": "MEASURING REPRESENTATION ALIGNMENT VIA STITCHING", "content": "There are active discussions in the literature about strengths and weaknesses of different representation alignment measures (Huh et al., 2024; Bansal et al., 2021; Sucholutsky et al., 2023). In this paper, we quantify representation equivalence as the accuracy that one model's decoder gets on a trained affine transformation or almost affine transformation (AAT) of the output from another model's encoder. This AAT also enables visual comparison of the two learned representations in the same space. We refer to this Equivalence Score (ES) as AAT-ES. This is a generalization of model stitching, where a small quadratic term is introduced to absorb any superfluous nonlinearities that may emerge during nonlinear model training.\nDefinition 1. (AAT, Almost Affine Transformation) f: E \u2192 G is an AAT if there exists b, c, d such that f(Ei) = bi+\u2211kckEik+\u20ac\u2211k,l dk,lEi,kEi,l, where a small quadratic term e \u226a 1 is introduced to absorb any superfluous nonlinearities that may emerge during nonlinear model training.\nWhile AAT-ES is a parametric measure that requires training (and hence more computational re- sources) as opposed to nearest-neighbor-based metrics such as CKA (Centered Kernel Alignment), we believe that AAT-ES is a better measure of representation alignment, as it directly compares the embedding with the decoder. Due to representation symmetries, there are many degenerate repre-"}, {"title": "UNDERSTANDING ALIGNMENT OF LLM REPRESENTATIONS VIA\nSTITCHING", "content": "Could we stitch two different LLMs to obtain another LLM whose performance is comparable to\noriginal models? Specifically, we consider (a) stitching LLMs of different sizes within the same\nmodel family, and (b) stitching two LLMs from different model family. Note that models from\ndifferent model family typically have different tokenizers.\nWe use OPT-{1.3B, 2.7B, 6.7B}, Pythia\u2013{410M, 1.4B, 2.8B}, Mistral-7B-Instruct, and LLaMA-\n3.1-8B-Instruct for the stitching experiment. We used open-source models available in Huggingface,\nand used Huggingface datasets monology/pile-uncopyrighted and monology/pile-test-val for training\nand evaluating test loss. Each sample was truncated at the 2048th token, and 2000 randomly chosen\nsamples from the test set were used to evaluate the test loss.\nFigure 1 shows the result of stitching LLMs. We found that stitching is possible across a range\nof stitching positions, with text generation capabilities slightly worse than the original model, as\nmeasured by the test loss.\nTo evaluate the equivalence of representations at different stages of model inference, we also experi- mented stitching various layers of one model onto a fixed layer of another model. Results are shown\nin Figure 2 and 3. We observe that the embedding layer is extremely hard to stitch to any other parts"}, {"title": "EXPLORING UNIVERSALITY: A GLIMPSE INTO THE GEOMETRIC\nSTRUCTURE OF LLM REPRESENTATIONS", "content": "In this section, we take a further glimpse into the geometric structure of LLM Representations by\nstitching LLMs to MLPs and hand-crafted decoders, using family tree representations as an example.\nWe will find intriguing hints of universality whereby equivalent representations are learned in these"}, {"title": "STARVATION HYPOTHESIS: UNIVERSALITY OF REPRESENTATIONS", "content": "Figure 9 shows the Goldilocks zone in the genealogical tree learning. When the decoder's size is\ntoo small, it cannot even fit the training data well, let alone generalize to unseen test data. When\nthe decoder's size is too large, the model overfits and fails to generalize: while the training accuracy\nremains nearly 100%, the test accuracy drops. The plots suggest that the generalization occurs\nfrom starvation, i.e., the necessity to discover generalizable and universal representations within the\nlimited number of trainable weights.\nMore generally, if the decoder is too dumb, it never outperforms chance. If the decoder is too\nsmart, it can simply memorize (overfit) the data. Either way, the encoder loses its incentive to\nlearn a clever compact representation. In the Goldilocks zone, however, the decoder is incentivized\nto learn the most clever representation, defined as the one that's easiest to decode. Starvation is\npresumably caused not by small overall resources, but by fierce competition for resources against\nother LLM tasks. In today's world with ever-growing amount of data, our LLMs will never be\ncapable enough to memorize all the data, so they are incentivized to learn the most clever (hence\nuniversal) representation, compressing the data in a way that allows generalization."}, {"title": "CONCLUSION", "content": "In this paper, we find hints of universality, where equivalent representations are learned across a\nrange of model sizes (from 102 to 109 parameters) and contexts (MLP toy models, LLM in-context\nlearning and LLM training). We find experimental support for such universality by showing that\nLLMs and simpler neural networks can be stitched, i.e., by stitching the first part of one model to\nthe last part of another, mediated only by an affine or almost affine transformation. We hypothesize\nthat this dynamic toward simplicity and generalization is driven by \"intelligence from starvation\":\nwhere overfitting is reduced by pressure to minimize the use of resources that are either scarce or\ncompeted for against other tasks."}]}