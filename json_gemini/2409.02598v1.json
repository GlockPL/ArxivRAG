{"title": "SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments", "authors": ["Wenwu Guo", "Jinlin Wu", "Zhen Chen", "Qingxiang Zhao", "Miao Xu", "Zhen Lei", "Hongbin Liu"], "abstract": "Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models for 3D registration. To solve these challenges, we propose the SurgTrack, a two-stage 3D instrument tracking method for CAD-free and robust real-world applications. In the first registration stage, we incorporate an Instrument Signed Distance Field (SDF) modeling the 3D representation of instruments, achieving CAD-freed 3D registration. Due to this, we can obtain the location and orientation of instruments in the 3D space by matching the video stream with the registered SDF model. In the second tracking stage, we devise a posture graph optimization module, leveraging the historical tracking results of the posture memory pool to optimize the tracking results and improve the occlusion robustness. Furthermore, we collect the Instrument3D dataset to comprehensively evaluate the 3D tracking of surgical instruments. The extensive experiments validate the superiority and scalability of our SurgTrack, by outperforming the state-of-the-arts with a remarkable improvement. The code and dataset are available at https://github.com/wenwucode/SurgTrack.", "sections": [{"title": "1 Introduction", "content": "Developing computer-assisted surgery systems can improve the quality of interventional healthcare for patients [4,14,6,1,25,7,5], offering significant benefits, such as reduced operational times and minimized risk of surgical complications. In particular, surgical navigation systems have become an indispensable component in modern surgery [15,13,2], and ascertain the exact positioning of surgical instruments by tracking distinctive sections of the tools. As a critical element of surgical navigation systems, including electromagnetic-based [18], optical-based [20], and vision-based systems [27]. Among these, vision-based systems have garnered considerable interest due to non-invasive, cost-effective, flexible, and not subject to line-of-sight limitations or electromagnetic disturbances [26,27].\nThe 3D tracking algorithm is essential in vision-based surgical navigation systems [20]. However, most existing methods of instrument tracking are based on object-tracking algorithms, detecting the region of interest object and corresponding matching the detected region across different frames. Early works [29] required markers of surgical instruments, and achieved instrument tracking by recognizing and matching markers across different frames. This method causes invasion of surgical instruments, lacking scalability. Later works [3] proposed marker-freed tracking methods, detecting instruments with handcraft visual features and then tracking instruments through the Kalman filter algorithm. Limited by the generalizability of handcraft visual features, these marker-freed methods did not perform well in real-world applications. Recently, Fathollahi et al. [9] proposed a highly accurate instrument tracking method, which introduces Yolo-v5 [12] to improve the accuracy of instrument detection and applies ReID [28] technology to improve the accuracy of cross-frame matching. However, these methods focused on developing 2D tracking of instruments, which can only perceive 2 degrees of freedom, which is not enough to provide sufficiently accurate information for surgical navigation.\nExisting 2D tracking systems [29] are restricted to the x and y planes, accommodating in-plane rotations for a total of three degrees of freedom. In comparison, 3D object tracking approaches [30,19,17,22,23] match detected objects with pre-established computer-aided design (CAD) models to ascertain their 3D orientation. Represented through six degrees of freedom-spanning the x, y, and z axes, and including the rotational dimensions of pitch, yaw, and roll this detailed spatial understanding is vital for vision-based navigational systems. However, the application of these 3D tracking methods to surgical environments is fraught with challenges. A primary challenge is the inaccessibility of CAD models for surgical instruments, as they are often proprietary due to patent protections. The absence of CAD models hinders most 3D tracking techniques in the realm of surgical instrument tracking. Additional obstacles are the low textural features and frequent occlusions of surgical instruments, which complicate their detection and sustained tracking.\nInspired by existing works [16,19,24], we design a novel 3D surgical instrument tracking method, named SurgTrack, which is capable of accurately tracking the 6 degrees of freedom of surgical instruments in real 3D space. To solve the problem of missing CAD models, we incorporate an Instrument Signed Distance Field (SDF) model generating the 3D representation of the surgical instrument with RGB-D video frames. We also propose an Instrument SDF model to further accurately learn the 3D shape and texture of instruments. Through Instrument SDF, SurgTrack completes the registration of 3D tracking without CAD models. To solve tracking problems caused by occlusion and weak textures, we apply a posture memory pool to provide historical tracking results as a reliable reference. We also utilize a posture graph optimization module to optimize the"}, {"title": "2 Method", "content": null}, {"title": "2.1 Overview of SurgTrack", "content": "An overview of our SurgTrack framework is shown in Fig. 1. To achieve CAD-free registration, we first model the 3D shape of the surgical instrument using SDF (2.2). Then, we track the 6-DoF pose of the instruments through the Posture Memory Pool and Posture Graph Optimization (2.3)."}, {"title": "2.2 CAD-free Instrument Registration", "content": "Instrument SDF Modeling. Given the 3D point cloud {\\v \u2208 R\u00b3} captured by a RGB-D camera, we adapt the Signed Distance Function (SDF) to model the 3D representation of the surgical instrument as follows:\n\nS = {v|\u03a8(v) = 0},\n\nwhere (v) = 0 represents the points on the surface of the instrument. Therefore, we can derive the 3D model of the instrument from point cloud data, eliminating the need for a pre-existing Computer-Aided Design (CAD) model. This 3D model facilitates the registration process for 3D tracking. However, the SDF methodology faces inherent limitations when dealing with complex scenarios, such as occlusions and low-texture regions.\nOcclusion and Texture Optimization. To address this, we incorporate the occlusion constraint and shape constraint in the SDF model. For occlusions, we introduce a positive value d to alleviate boundary ambiguities between background and instrument caused by partial occlusions:\n\nL_{occ} =  - \\frac{1}{|V_{occ}|} \\sum_{v \u2208 V_{occ}} (\u03a8(v) - \u03b4)^2.\n\nFor surfaces with weak textures, we consider points near the surface in the SDF modeling process, enabling our SurgTrack to better capture the surface geometry and handle areas with weak textures, as follows:\n\nL_{surf} =  \\frac{1}{|V_{surf}|} \\sum_{v \u2208 V_{surf}} (\u03a8(v) + d - d\u2206)^2 .\n\nIn this way, the total loss function L is defined as follows:\n\nL = aL_{occ} + BL_{surf},\n\nwhere a and \u1e9e balance the contributions of the two components."}, {"title": "2.3 Instrument Tracking", "content": "Tracking Initialization. In the tracking stage, we estimate a coarse pose t by matching the current frame and its adjacent frames with RANSAC algorithm as follows:\n\n\\widehat{t} = arg \\min_{R,t} \\sum_{i} ||Rp_i + t - q_i||^2 .\n\nIn the above equation, RANSAC algorithm minimizes the distance between the reconstructed results pi and their corresponding scene points qi and estimates the coarse pose ft. R and t represent the rotation and translation matrix.\nTracking Optimization. Following the initial rough pose estimation obtained using RANSAC, the pose Et serves as the initial estimate in the subsequent optimization phase. This pose is further refined by integrating the pose memory pool with the pose graph to improve accuracy and robustness. First, to address challenges such as long-term tracking drift, data loss, and occlusions, it is crucial to preserve the pose data from previous frames. We implement a posture memory pool P that stores this information as follows:\n\nP = {(\\xi_i, M_i) | i = 1, 2, . . ., N },\n\nwhere i \u2208 SE(3) represents the optimized pose of the ith frame, Mi contains the 3D point cloud data associated with the ith frame, and N is the number of keyframes currently stored in the posture memory pool.\nWith the initial pose \u03bet, we construct a posture graph using selected relevant frames from the posture memory pool. The selection is based on criteria such as the RANSAC matching threshold and frame overlap to ensure reliable references. Then, the posture graph is constructed as follows:\n\nG = (\u03bd, \u03b5),\n\nwhere the nodes V consist of the current frame Ft and the selected reference frames Ppg, as V = Ft UPpg with |V| = K + 1.\nBased on the posture graph, we refine the tracking results of the current frame through the following loss function, resulting in the final optimized pose \u03bet \u2208 SE(3):\n\nE_{t} \\leftarrow arg \\min_{\\xi_{\u03c4}} \\sum_{i \u2208 V,j \u2208 V,i\u2260j} [w_{f}L_{3D} (i, j) + w_{p}L_{2D}(i, j)] + w_{s}L_{SDF}(t) ,\n\nwhere L3D (i, j) is the 3D distance loss, L2D (i,j) is the 2D projection loss, LSDF(t) is the instrument SDF depth loss, and the scalar weights wf, wp, ws are empirically set to 1. Specifically, the 3D distance loss L3D is calculated as:\n\nL_{3D}(i, j) =  \\frac{1}{(P_m,P_n)EC_{i,j}} \\sum \u03c1 (||P_m-P_n||^2).\n\nThis 3D distance loss measures the Euclidean distance between corresponding RGB-D features pm, Pn \u2208 R\u00b3, using the Huber loss function \u03c1 to enhance the robustness of our SurgTrack.\nOn the other hand, the 2D projection loss L2D is calculated as:\n\nL_{2D} (i, j) = \\sum_{p \u2208 I_{i}} \u03c1 (n_i (p). ((T_i_j p)) -p)).\n\nThis 2D projection loss assesses the pixel-wise point-to-plane distance after projection and transformation, comparing node i to the plane in node j.\nFinally, the instrument SDF depth loss LSDF is calculated as follows:\n\nL_{SDF}(t) = \\sum_{p \u2208 I_t} \u03c1(|\u03a8(\u03be^{-1}(\u03c0_0^{-1}(p)))|).\n\nThis instrument SDF depth loss measures the distance between the current frame and the implicit surface defined by the Instrument SDF, where (\u00b7) is the signed distance function indicating proximity to the surface. Note that this loss is considered only after the initial training of the object field has converged.\nIn this way, the optimization strategy for our SurgTrack, starting from the rough pose Et and resulting in the final optimized pose t \u2208 SE(3), integrates 3D spatial information, instrument shape, and depth data from a single viewpoint to complete pose optimization, improving robustness against reflections, weak textures, and long-term tracking challenges."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Settings", "content": "Datasets. We collect a 3D tracking dataset of surgical instruments in RGB-D videos, named Instruments3D. The Instruments3D dataset consists of 13 videos across 5 surgical instruments, including ultrasound bronchoscopes, flexible and rigid endoscopes, thoracoscopes, and ultrasound probes. The Instruments3D dataset presents RGB-D videos by capturing human hands manipulating YCB objects, recorded at close range using an Intel RealSense camera. The ground truth data is derived through multi-view registration. We also conduct experiments on the general object 3D tracking dataset, HO3D [10,11].\nEvaluation metrics. We follow the classical evaluation protocol of 3D object tracking [10,11]. We use the ADD and ADD-S as the accuracy metric of 3D tracking, with their values ranging from 0 to 1, where higher values signify better accuracy. We use the Chamfer Distance (CD) as a measure of reconstruction error, where a smaller value indicates a more precise reconstruction."}, {"title": "3.2 Comparison Results on Instrument3D and HO3D", "content": "Comparison on Instrument3D. The Instrument3D dataset presents a complex challenge due to the frequent occlusions and severe motion blur encountered during the manipulation of surgical instruments. Furthermore, the inherent characteristics of these instruments such as their weak texture, reflective surfaces, and slender profiles compound the difficulty. Despite these difficulties of the Instrument3D dataset, our SurgTrack maintains the capability of robust, long-term tracking in most cases, as shown in Fig. 2. The comparison results in Table 1"}, {"title": "3.3 Ablation Study", "content": "To comprehensively evaluate our SurgTrack framework for 3D tracking of surgical instruments, we investigate the impact of each module. These modules include occlusion and texture Optimization, posture memory pool, and posture graph. As shown in Table 2, the occlusion and texture optimization is helpful for tracking optimization, which can increase ADD-S by 12.43% and ADD by 21.56%. When constructing the posture graph, selecting the most matching pose subset instead of randomly selecting can reduce the CD error by nearly 3cm and increase the ADD by 41.29%. In this way, these comparisons further validate the effectiveness of our SurgTrack with tailored modules."}, {"title": "4 Conclusion", "content": "In this study, we collect a new multi-category surgical instrument 3D tracking data set, conduct a comprehensive study on 3D surgical instrument tracking, and propose a framework for 3D instrument tracking. We use Instrument SDF to generate the 3D representation of surgical instruments, achieving CAD-free 3D tracking registration. In the tracking stage, we use the posture memory pool and combine it with the posture graph for pose optimization, which greatly improves the 3D tracking accuracy. We also use the Instrument SDF to further improve the robustness to occlusion, weak texture, and long-term tracking. Experiments show that our method has significant superiority and scalability over public data sets and surgical instrument 3D tracking datasets."}]}