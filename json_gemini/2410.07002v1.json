{"title": "CURSORCORE: ASSIST PROGRAMMING THROUGH ALIGNING ANYTHING", "authors": ["Hao Jiang", "Qi Liu", "Rui Li", "Shengyu Ye", "Shijin Wang"], "abstract": "Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the rise of large language models (LLMs), AI-assisted programming technology has developed rapidly, with many powerful LLMs being applied in this field Zan et al. (2022); Liang et al. (2024); Yang et al. (2024). The technology mainly takes two forms. One form involves completing a specified code snippet at the end or inserting corresponding code at a designated position, typically"}, {"title": "2 ASSISTANT-CONVERSATION: NEW CONVERSATION FRAMEWORK FOR PROGRAMMING ASSISTANTS", "content": "In this section, we introduce a new conversational framework, Assistant-Conversation, aimed at simplifying the programming process. The framework leverages all available information during programming to streamline work for programmers."}, {"title": "2.1 FRAMEWORK FORMULATION", "content": "We introduce the constituent elements of Assistant-Conversation: System (S), History (H), Current (C), User (U), and Assistant (A). The Assistant (A) represents the output of the model, while the inputs consist of the System (S), History (H), Current (C), and User (U). Figures 1 and 2 shows several examples of them. These definitions will be referenced throughout the rest of this work."}, {"title": "2.2 COMPARISONS OF ASSISTANT-CONVERSATION", "content": "Completion and insertion modes face challenges when modeling both C and H Although they can utilize C, they fail to capture H, limiting the modeling of future changes in C, and are incapable of deleting or editing code. Although user instructions and reflection information can be used through comments and assert statements, this capability is weak and unstable.\nChat models are not ideal for all programming assistance tasks These models focus on user input rather than the code content, while the input should primarily be centered on C instead of just user instructions. In traditional conversational frameworks, the sole input source is U, which works for chatbots but not for application assistants. Input sources should include C, H, and U, as both H and U are related to C. Although instruction models can represent the interaction history between users and assistants, they struggle to capture the historical changes in the application's content. Prompt engineering can integrate some of this information into existing models, but the"}, {"title": "2.3 SPECIFICATIONS AND IMPLEMENTATION", "content": "To represent a piece of code like C, we can either use it directly or wrap it in a markdown code block. However, representing code changes, such as H or changes in A, is more complex. We can either use the whole code, patches that alter the code, or records of both the modification locations and the specific changes. Some methods work well but experience issues when handling longer texts, such as outputting the entire modified code, which can be slow. Other methods output minimal content, like providing only the modification locations and changes. These are faster but still not optimal in terms of performance. We represent code changes in the experiments of the main body using the whole code format, and we investigate different ways to represent these modifications, as detailed in Appendix B. Additionally, we explore methods for compressing historical code changes in Appendix F.\nIn some cases, programmers assign assistants to focus on specific areas of code. They might use the cursor to mark a general location or directly select a range of code, as shown in Figure 2. We handle this by treating them as special tokens (see Appendix D for further details).\nWe structure conversations in the order of S-H-C-U-A to match the actual workflow. This mirrors the chronological sequence in which information is generated during the programming process. By doing so, we maximize prefix overlap across multiple requests, utilizing prefix caching to reduce redundant kv-cache computations and improve efficiency Zheng et al. (2023a). A is organized in code-chat order, prioritizing code edits due to their importance in real-time applications where speed is crucial."}, {"title": "3 APEVAL: BENCHMARK FOR ASSISTED PROGRAMMING", "content": "Past benchmarks assessing LLM code capabilities have effectively evaluated tasks like program synthesis Chen et al. (2021); Austin et al. (2021), code repair Muennighoff et al. (2024); Jimenez et al. (2024), and instructional code editing Cassano et al. (2023b); Paul-Gauthier (2024); Guo et al. (2024b). However, they fall short in fully assessing how models use various types of information to assist in programming. This gap calls for a new benchmark."}, {"title": "3.1 BENCHMARK OVERVIEW", "content": "As discussed in Section 2.1, programming assistance can involve different types of information, with H and U being optional. Thus, there are four possible combinations of information: H, C, U; H, C; C, U; and only C. HumanEval Chen et al. (2021) is a well-known benchmark for evaluating code completion. It has been extended to assess other tasks such as code insertion Bavarian et al. (2022), instruction-based tasks CodeParrot (2023); Muennighoff et al. (2024), and multilingual generation Zheng et al. (2023b); Cassano et al. (2023a). We refer to these works and further extend it to comprehensively evaluate the model's ability to assist programming. We randomly categorize each task into one of the four types, then manually implement the functions and simulate the potential instructions that programmers might give to an LLM during the process, collecting all interactions. We invite programmers with varying levels"}, {"title": "3.2 EVALUATION PROCESS AND METRICS", "content": "In all tasks, we use the classic Pass@1 metric to execute the generated code, which is the simplest version of the Pass@k metric Chen et al. (2021). Since APEval is an extension of HumanEval, we use the test set created by EvalPlus Liu et al. (2023). We report the results from both the basic and extra tests. We provide the model with relevant information during the programming process, and the model immediately returns the modified code. Some methods may improve performance by increasing the number of output tokens to model the thinking process; we discuss this further in Appendix E."}, {"title": "4 PROGRAMMING-INSTRUCT: COLLECT ANY DATA DURING PROGRAMMING", "content": "To align models with programming-related data, relevant training data must be collected. While large amounts of unsupervised code Kocetkov et al. (2023) and instruction data Wei et al. (2023b); Luo et al. (2024b) have been gathered, there remains a significant lack of data on the coding process. Manually annotating the coding process is expensive, so we propose Programming-Instruct, a method to automate this data collection."}, {"title": "4.1 DATA SOURCES", "content": "To ensure both quality and diversity in the coding process data, we collect information from three different sources: Alprogrammer, Git commit, and Online Submit.\nAlprogrammer For each code snippet, we use LLMs to generate the corresponding coding history. Since human coding approaches vary widely, we utilize several LLMs, each guided by three distinct prompts, representing novice, intermediate, and expert programmers. The LLMs then return their version of the coding process. Prompts used are shown in Appendix K.\nGit Commit Some software can automatically track changes, such as Git. We use Git commit data from Github, which captures users' code edits and modification histories.\nOnline Submit Many online coding platforms like Leetcode allow users to submit code for execution and receive feedback. During this process, users continuously modify their code until it is finalized. We also make use of this data.\nThrough these sources, we obtain a large number of samples, each consisting of multiple code snippets. The last snippet in each sample is referred to as the final snippet (F). Examples of data sources are shown in Figure 3."}, {"title": "4.2 DATA PROCESSING", "content": "After collecting a large number of coding processes, we process them to meet the requirements of Assistant-Conversation. Figure 4 shows the steps of data processing. First, we randomly select a time"}, {"title": "5 CURSORCORE: FINE-TUNE LLMS TO ALIGN ANYTHING", "content": ""}, {"title": "5.1 BASE MODELS", "content": "We fine-tune existing base LLMs to assist with programming tasks. Over the past few years, many open-source foundation models have been trained on large code corpora sourced from GitHub and other platforms, demonstrating strong performance in coding. We choose Deepseek-Coder Guo et al. (2024a), Yi-Coder AI et al. (2024) and Qwen2.5-Coder Hui et al. (2024) series, which we refer to as CursorCore-DS, CursorCore-Yi and CursorCore-QW2.5 series after training. Deepseek-Coder has achieved state-of-the-art performance on numerous coding-related benchmarks over the past year, gaining wide recognition. Yi-Coder and Qwen2.5-Coder are the most recently released models at the start of our experiments and show the best performance on many benchmarks for code now. These models are widely supported by the community, offering a good balance between size and performance, making them suitable for efficient experimentation. For ablation experiments, we use the smallest version, Deepseek-Coder-1.3B, to accelerate the process. We use a chat template adapted from ChatML OpenAI (2023) to model Assistant-Conversation during training, as detailed in Appendix I."}, {"title": "5.2 TRAINING DATA", "content": "We use Programming-Instruct to collect data. For Alprogrammer, we gather code snippets from datasets such as the stack Kocetkov et al. (2023) and oss-instruct Wei et al. (2023b), then prompt LLMs to generate the programming process. For Git commit data, we collect relevant information from editpackft Cassano et al. (2023b) (a filtered version of commitpackft Muennighoff et al. (2024)) and further refine it through post-processing and filtering. Regarding online submission data, we source the programming process from the Codenet dataset Puri et al. (2021). First, we group all submissions by user for each problem, then exclude invalid groups without correct submissions to"}, {"title": "5.3 TRAINING DETAILS", "content": "Our models are trained for 2 epochs using the Transformers library Wolf et al. (2020). We enhance memory efficiency and speed with techniques such as Deepspeed ZeRO3 Rajbhandari et al. (2019), ZERO Offload Ren et al. (2021), FlashAttention2 Dao (2024), and triton kernels Hsu et al. (2024). We calculate the maximum sequence length that can be processed per batch based on the available VRAM. Using the First-Fit Decreasing algorithm Kundu et al. (2024), we pack training samples to ensure that each batch reaches its maximum sequence length, thereby optimizing training speed. The training process employs the Adafactor optimizer Shazeer & Stern (2018) with a learning rate of 5e-5, coupled with a cosine scheduler featuring 15 warm-up steps."}, {"title": "6 EVALUATION AND RESULTS", "content": "In this section, we evaluate the CursorCore models. We begin by describing the experimental setup and then present and analyze the results."}, {"title": "6.1 EXPERIMENTAL SETUP", "content": "We conduct the data selection ablation and primary evaluation on our APEval benchmark, and provide results on well-known benchmarks such as Python program synthesis, automated program repair, and instructional code editing, which are detailed in Appendix G. We choose prominent open-source and closed-source LLMs as our baselines. For all benchmarks, we use greedy decoding to generate evaluation results. CursorCore natively supports various inputs in APEval, whereas base and instruction LLMs require additional prompts for effective evaluation. We design few-shot prompts separately for base and instruction models, as detailed in Appendix J."}, {"title": "6.2 DATA SELECTION ABLATION", "content": "We train the smallest model Deepseek-Coder-1.3B on different combinations of datasets to determine the optimal data mix. The results of the ablation study are shown in Figure 9.\nAlprogrammer has the highest data quality Among the various data sources, the model trained on the Alprogrammer dataset achieve the best per- formance on APEval. We believe this is primar- ily because the data aligns well with the required format of APEval. Moreover, unlike other data sources such as Git Commit, the AIprogrammer data is almost entirely synthesized by LLMs, ex- cept for the initial code. As LLMs have advanced, the quality of their generated data has generally surpassed that of data collected and filtered from human-created sources.\nImportance of mixing data with different in- formation types We find that using high-quality chat-style data alone, such as the Evol-Instruct dataset, does not achieve the desired performance; it underperforms compared to the AIprogrammer dataset. However, when combining both datasets, the model shows a notable improvement. This indicates that to better align the model with a variety of data and information, it is necessary to use datasets containing diverse types of information.\nOur final selection We combine data from all sources for training. Since our focus is on Python, and training on multilingual data leads to a decrease in APEval scores, we use only the Python part of the Git Commit and Online Submit datasets. As a result, we get CursorCore series models."}, {"title": "6.3 EVALUATION RESULTS ON APEVAL", "content": "In Table 4, we present the results of evaluating CursorCore series models and other LLMs on APEval. It includes both the average results and the results across four different types of information within the benchmark, each item in the table is the score resulting from running the base tests and extra tests. We also report the evaluation results of other well-known models on APEval, which can be found in Appendix H."}, {"title": "7 CONCLUSION", "content": "This work explores how LLMs can maximize the use of any available information during program- ming process to assist coding. We introduce Assistant-Conversation to model the diverse types of information involved in programming. We present APEval, a new benchmark that includes various historical edits and instructions, providing a comprehensive evaluation of the model's programming assistance capabilities. Additionally, we propose Programming-Instruct, which is designed to collect data for training LLMs to assist programming, along with their corresponding data sources. Further- more, we train CursorCore, which demonstrate outstanding performance in assisting programming tasks while achieving a good balance between efficiency and cost. We also conduct extensive ablation experiments and analyzes. Beyond enhancing traditional approaches of programming assistance, we plan to extend this approach to support models capable of assisting with repository-level development as well as other applications."}, {"title": "A RELATED WORK", "content": ""}, {"title": "A.1 AI-ASSISTED PROGRAMMING", "content": "AI-assisted programming has a long history, encompassing various tasks such as clone detection Lu et al. (2021), code summarization Sun et al. (2024), program synthesis Chen et al. (2021); Austin et al. (2021), automatic program repair Gulwani et al. (2016), code editing Wei et al. (2023a), and code optimization Shypula et al. (2024). In the past, these tasks were typically addressed by custom-built models, which were difficult to scale across different tasks. With the rise of LLMS, AI- assisted programming increasingly leverages LLMs to handle multiple types of tasks simultaneously. Numerous high-quality open-source and closed-source products, such as Continue Continue-Dev (2024), Aider Paul-Gauthier (2024), Copilot Github-Copilot (2022) and Cursor Cursor-AI (2023), are based on this approach."}, {"title": "A.2 CODE MODELS", "content": "Recently, LLMs have attracted significant attention in the research community for their impact on enhancing various aspects of code intelligence. Open-source code LLMs like CodeLlama Rozi\u00e8re et al. (2023); Touvron et al. (2023), Deepseek-Coder Guo et al. (2024a); DeepSeek-AI et al. (2024), StarCoder Li et al. (2023); Lozhkov et al. (2024), Codegemma Team et al. (2024), Codestral Mistral- AI (2024a), Codegeex Zheng et al. (2023b), Yi-Coder AI et al. (2024), and Qwen-Coder Hui et al. (2024) have made substantial contributions by utilizing large code corpora during training. Some models, such as WizardCoder Luo et al. (2024b), OctoCoder Muennighoff et al. (2024), CodeLlama-Instruct, Deepseek-Coder-Instruct, MagiCoder Wei et al. (2023b), Yi-Coder-Chat, and Qwen-Coder-Instruct, have been fine-tuned using instruction data collected through methods like Self- Instruct Wang et al. (2023); Taori et al. (2023), Evol-Instruct, and OSS-Instruct. These models are specifically trained on code-related instructions, improving their ability to follow coding instructions. They have made significant breakthroughs in tasks like code completion and editing."}, {"title": "A.3 CODE BENCHMARKS", "content": "HumanEval Chen et al. (2021) is one of the most well-known benchmarks in the code domain, featuring several variants that extend it to different programming languages, extra tests, and broader application scenarios. Other notable benchmarks include MBPP Austin et al. (2021) for program synthesis, DS1000 Lai et al. (2022) for data science tasks, SWE-Bench Jimenez et al. (2024) for real-world software engineering problems, and CanItEdit / CodeEditorBench Cassano et al. (2023b); Guo et al. (2024b) for code editing. Additionally, LiveCodeBench Jain et al. (2024) focuses on contamination-free evaluations, while Bigcodebench Zhuo et al. (2024) and Naturecodebench Zhang et al. (2024b) provide comprehensive program synthesis assessments. CRUXEval Gu et al. (2024) targets reasoning, CrossCodeEval Ding et al. (2023) focuses on repository-level code completion, and Needle in the code Hui et al. (2024) is designed for long-context evaluations."}, {"title": "B CODE MODIFICATION REPRESENTATION", "content": "As discussed in Section 2.3, there are various ways to represent code modifications. Many previous works have explored techniques for instruction-based code editing Wei et al. (2023a); Muennighoff et al. (2024); Paul-Gauthier (2024); Sweep-AI (2024). We build upon these works with the following formats, as shown in Figure 10:\nWhole file format (WF) We use the entire code, allows for a straightforward representation of the modifications. However, when only small parts of the code are changed, this method leads to redundancy, especially for long code files. Certain mitigation can be achieved through technologies such as retrieval-based speculative decoding Yang et al. (2023); He et al. (2024).\nUnified diff format (UD) The diff format is a common way to represent code changes, widely adopted for its efficiency and readability. Among various diff formats, unified diff is one of the most popular, as it efficiently shows code changes while reducing redundancy. It is commonly used in software tools such as git and patch."}, {"title": "C ADDITIONAL DETAILS ABOUT PROGRAMMING-INSTRUCT", "content": "In our code editing records, we place no limits on the granularity or number of edits. Changes between two code versions may involve anything from a single character to multiple extensive modifications. However, data collected from various sources may be compressed, resulting in incomplete records. This compression can lead to a higher proportion of large-scale edits, particularly in Git Commit data. To address this issue, we propose a decomposition strategy: when there are multiple changes between versions, we break them down into single-step modifications, with the steps ordered randomly. For Git Commit data, we apply this decomposition strategy with a 90% probability, while for Alprogrammer and Online Submit data, we apply it with a 50% probability."}, {"title": "D TARGET AREA REPRESENTATION", "content": "To modify code, programmers often specify the parts requiring changes, typically in one of two ways: either by clicking with the cursor to indicate a general area or by selecting a specific text range with defined start and end points. We model both cases using special tokens: \u201c< | target | >\" for cursor positions, and \u201c< | target_start | >\u201d and \u201c< | target_end | >\" to mark the selected region's boundaries. While collecting training data, we determine modification locations based on the code differences before and after changes. In real-world applications, the decision to provide explicit locations\u2014and their granularity- varies among programmers. To account for this variability, we introduce randomized choices for determining the form and location, integrating this approach into the Programming-Instruct pipeline.\nWe evaluate CursorCore-DS-1.3B on APEval both with and without location in- formation to assess its impact on performance. The results in Figure 13 show that including location information has minimal effect, likely because most APEval examples are relatively short, enabling LLMs to easily infer modification loca- tions, much like humans do without a cursor. Previous works, such as those on automated program repair Zhang et al. (2024a), have emphasized the importance of identifying the modification location. We believe this emphasis stems from traditional code completion and insertion paradigms, as well as the natural align- ment of specifying modification points with human thought processes. However, with the advancement of LLMs, the benefit of providing location information diminishes when generating code at the function or file level. This may need further exploration in longer contexts, such as repository-level editing tasks."}, {"title": "E DISCUSSION ABOUT THOUGHT PROCESS", "content": "Incorporating reasoning processes in prompts has been shown to improve model performance, as demonstrated in various works like CoT Wei et al. (2022) and ReACT Yao et al. (2023). Some studies have even integrated these processes into the training phase to further enhance effectiveness Zelikman et al. (2022). In this work, we also explore a self-taught approach, where we prompt LLMs to reverse-generate the reasoning process from outputs and incorporate them into the model's output during training. Our model and data setup follow the same configuration as described in Appendix B to enable quick experiments. The evaluation results are shown in Figure 14.\nAfter incorporating reasoning into training, the model shows slight performance improvements, but the output length increases sig- nificantly. The tokens used for reasoning often exceed those in the modified code. Since many programming-assist applications require real-time responses, longer reasoning times may be im- practical, so we do not integrate this process into CursorCore. We believe that the decision to use reasoning processes should be"}, {"title": "F CONVERSATION RETRIEVAL FOR ASSISTANT-CONVERSATION", "content": "Not all code editing records are necessary for inferring user intent and predicting output. Some past modifications, such as simple typos corrected shortly after, offer little value to future predictions, and thus can be safely removed. Additionally, if a programmer continuously interacts with the model without delet- ing these records, the editing history will accumulate and grow until it exceeds the model's maximum context length. This could negatively affect performance and speed.\nTo address this, it is essential to compress the editing history or retrieve only the relevant portions. Similar to how many conver- sation retrieval techniques, such as memory modules Packer et al. (2023), prompt compression Jiang et al. (2023) and query rewrit- ing Ye et al. (2023), are used to manage dialogues for chatbots, these methods can be adapted for handling code editing records. In this work, we explore a basic approach, sliding window, to in- vestigate possible solutions. When the number of historical editing records surpasses a predefined threshold, the model automatically discards the oldest entries.\nWe evaluate this method on APEval, as shown in Figure 15. The impact of setting a sliding window of a certain size on the results is minimal, indicating that compressing the historical records effectively balances performance and efficiency."}, {"title": "G EVALUATION RESULTS OF OTHER BENCHMARKS", "content": "We also evaluate CursorCore on other well-known benchmarks. We use HumanEval+ and MBPP+ Liu et al. (2023) to evaluate Python program synthesis, CanItEdit Cassano et al. (2023b) for instructional code editing, and the Python subset of HumanEvalFix from OctoPack Muennighoff et al. (2024) for automated program repair. All benchmarks are based on their latest versions, and HumanEvalFix uses the test-based repair version as described in the original paper. To generate results, we consistently use vLLM Kwon et al. (2023) due to its versatility and support for customized conversation formats. Evaluations are conducted within each benchmark's execution environment.\nUnlike previous LLMs, CursorCore supports multiple input formats, and different formats may produce different results. To comprehensively showcase this, we categorize input formats based on specific assisted programming scenarios into three cases:\n* Chat: Similar to the chat format of ChatGPT Ouyang et al. (2022), we wrap the query before passing it to the model, which returns a response in a chat style. The final result is obtained after post-processing.\n* Inline: Similar to Copilot Inline Chat Github-Copilot (2022) and Cursor Command K Cursor- AI (2023) scenarios, corresponding to the combination of C and U in Assistant-Conversation. Compared to the Chat mode, it is more tightly integrated with the IDE and returns less additional content.\n* Tab: Similar to the use case of Copilot++ Cursor-AI (2023), it is the most automated of all scenarios. We provide only the C to the model. For instructional code editing and automated code repair, no explicit instructions are passed.\nEvaluation results are shown in Table 5. Our model outperforms the corresponding instruction-tuned and base models across several benchmarks. However, the performance of the 6B+ model, when compared to its corresponding models, is not as strong as that of the 1B+ model. Notably, with the recent release of Qwen2.5-Coder-7B at the start of our experiments, we outperform it on only one"}, {"title": "H ADDITIONAL EVALUATION RESULTS ON APEVAL", "content": "We also report the evaluation results of various versions of other well-known models on APEval, as shown in Table 6."}, {"title": "I CHAT TEMPLATE", "content": "Our model's chat template OpenAI (2023) is adapted from the ChatML template, where each message in the conversation is restricted to one of the following roles: system, history, current, user, or assistant. The assistant's output includes both code modifications and chat interaction with the user. To indicate code changes, we use two special tokens \u201c< | next_start | >\u201d and \u201c< | next_end | >\u201d to wrap the code modification parts. This approach models Assistant-Conversation effectively and is compatible with standard ChatML templates and chatbot applications. Figure 16 illustrates an example of our chat template, while Figure 17 presents examples of the chat template when using the LC and SR modes described in Appendix B."}, {"title": "J PROMPTS FOR EVALUATION", "content": "We report the prompts used to evaluate base LLMs on APEval in Table 13, while the prompts used for evaluating instruct LLMs are presented in Table 14."}, {"title": "K PROMPTS FOR DATA COLLECTION", "content": "We design specific system prompts and few-shot examples to collect high-quality training data, as we find that many examples are very difficult to complete with current LLMs, and only a few of them can be successfully completed using rough prompts. For Alprogrammer, we utilize LLMs to simulate programmers at three different skill levels, with each level using a distinct set of prompts as shown in Tables 7 to 9. Additionally, prompts used for evaluating whether the outputs align with user intent, generating user instructions, and facilitating chat interactions between models and users are outlined in Tables 10 to 12. Partial few-shot examples are shown in Figures 18 to 23."}, {"title": "L LIMITATIONS AND FUTURE WORK", "content": "Repo-level development assistance In this work, we focus on supporting the development of single files or function-level code. However, real-world development operates at the repository level, involving multiple files and greater interaction with IDEs. Previous research has made notable advances in repository-level tasks such as code completion Zhang et al. (2023), issue fixing Jimenez et al. (2024), and documentation generation Luo et al. (2024a). Repository-level code assistance deals with larger datasets, and achieving optimal performance and speed will require more effort. We leave the exploration of multi-file repository-level programming assistance and leveraging additional IDE interactions for future work.\nMore scenarios and criteria for evaluation We have only tested our models' code assistance capabilities on Python-specific benchmarks. While multi-language program synthesis benchmarks like Multipl-E Cassano et al. (2023a) can evaluate coding abilities across languages, dedicated benchmarks are still needed to assess programming assistance for each language. Additionally, our benchmark is relatively small and based on an extension of HumanEval, making it insufficient to cover all development scenarios. Beyond using the classic Pass@k metric to evaluate accuracy, other criteria should also be considered, such as evaluating the model's efficiency, security, and redundancy Huang et al. (2024); Pearce et al. (2021); Li et al. (2024).\nPreference-based optimization Methods like PPO Schulman et al. (2017) and DPO Rafailov et al. (2023), which optimize models based on human preferences, have been widely used in LLMs. In programming assistance, programmers can provide feedback on predicted outputs for identical or similar coding processes, further optimizing the model Shinn et al. (2023). To enable this, a significant amount of feedback data from programmers using AI-assisted tools should be collected or synthesized.\nEnhance performance with API calls We aim to integrate function calls Patil et al. (2023) into the model to further enhance its capabilities. One potential application is incorporating function calls into the thinking process, such as retrieving information or executing partial code for feedback. Although our final models excludes this thinking step due to performance and speed considerations, we are exploring hybrid approaches to introduce this process while maintaining speed and combine it with other strategies for searching how to edit. Another application is leveraging function calls in output, where calling a Python script for tasks like variable replacement might be more efficient than manually generating code blocks or search-and-replace strategies. For repository-level changes, using terminal commands or IDE APIs could sometimes be a more convenient solution.\nExpand to other applications Our framework is designed for programming assistance applications, but the alignment approach can also be applied to other types of AI assistants. For example, in designing an art assistant, it should be able to predict the next drawing step based on the artist's previous drawing patterns, the current state of the canvas, and the artist's instructions. Extending this approach to design assistants for other applications is an interesting research direction."}]}