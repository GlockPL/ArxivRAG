{"title": "Fine-Tuning Strategies for Continual Online EEG Motor Imagery Decoding: Insights from a Large-Scale Longitudinal Study", "authors": ["Martin Wimpff", "Bruno Aristimunha", "Sylvain Chevallier", "Bin Yang"], "abstract": "This study investigates continual fine-tuning strategies for deep learning in online longitudinal electroencephalography (EEG) motor imagery (MI) decoding within a causal setting involving a large user group and multiple sessions per participant. We are the first to explore such strategies across a large user group, as longitudinal adaptation is typically studied in the single-subject setting with a single adaptation strategy, which limits the ability to generalize findings. First, we examine the impact of different fine-tuning approaches on decoder performance and stability. Building on this, we integrate online test-time adaptation (OTTA) to adapt the model during deployment, complementing the effects of prior fine-tuning. Our findings demonstrate that fine-tuning that successively builds on prior subject-specific information improves both performance and stability, while OTTA effectively adapts the model to evolving data distributions across consecutive sessions, enabling calibration-free operation. These results offer valuable insights and recommendations for future research in longitudinal online MI decoding and highlight the importance of combining domain adaptation strategies for improving BCI performance in real-world applications.\nClinical Relevance\u2014Our investigation enables more stable and efficient long-term motor imagery decoding, which is critical for neurorehabilitation and assistive technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "A brain-computer interface (BCI) measures brain activity and translates it into control commands for computers or other external devices [1]. This provides a direct alternative to natural neural pathways, enabling BCIs to replace, restore, enhance, supplement, or improve the brain's interaction with its external or internal environment [2], [3].\nA widely used method for controlling BCIs is the motor imagery (MI) paradigm. In this paradigm, the user imagines the movement of a body part without physically performing the action. This imagined movement engages neural mechanisms similar to those involved in actual execution [4], making MI-BCIs particularly effective for promoting motor recovery in chronic stroke patients [2].\nHowever, novice users often struggle to elicit the correct brain patterns, a challenge known as BCI inefficiency [5]- [7], also referred to as BCI illiteracy. Unlike paradigms such as P300 or steady-state visually evoked potentials, which rely on responses to external stimuli, MI requires users to endogenously modulate their brain rhythms, i.e., actively regulate their neural activity which is known to be more challenging. Potential solutions to this BCI inefficiency fall mostly into two categories: either promoting user learning or improving the decoder [5], [6].\nAs with almost any skill acquisition process, effective BCI usage depends on practice guided by feedback [8]. Research even indicates that implicit learning, where users develop skills through self-regulation guided by feedback, may be more effective than explicitly guiding or instructing the user [9]-[11]. To facilitate such implicit learning, closed-loop systems that provide real-time feedback are essential.\nThe other possible solution for successful BCI usage is to increase the quality of the decoder. While this has been the subject of a large research effort for several decades [12], one important - although less investigated issue is the adaptiveness of the decoder. While users must develop the ability to generate the correct brain patterns, the system must also adjust to the evolving neural activity of the users. Decoder adaptation is essential in this process, ensuring that the BCI remains effective despite evolving brain patterns over time.\nImportantly, user learning and decoder adaptation are not independent processes but are tightly interconnected. This dynamic interaction can be conceptualized as a two-learner problem, where both the user and the decoder adapt to each other's changing trajectories over time [13]\u2013[15]. Through mutual learning, they try to find an optimal communication strategy. In longitudinal settings, this interdependence be- comes particularly significant as the user's neural patterns may shift substantially over time due to factors such as learning, neuroplasticity, or changes in the environment.\nResearch has demonstrated that decoder adaptation can enhance the user's ability to control the BCI, leading to overall performance improvements [15]\u2013[20]. However, de- termining the optimal strategy for recalibrating decoders over time remains an open question. Current approaches vary primarily in the frequency of recalibration [17], and the data composition used for recalibration [16], [19].\nWithin the realm of deep learning, the process of gradually adapting to an incoming stream of data from dif- ferent domains can be described as domain-incremental continual learning [21], [22]. However, continual learning in MI decoding presents unique challenges compared to its applications in other fields. In traditional settings, continual learning methods prioritize efficient adaptation to new tasks or domains while retaining knowledge of previous ones, thereby avoiding catastrophic forgetting.\nIn MI decoding, the latter consideration is irrelevant, as the data distribution continuously evolves over time, and revisiting previous distributions is neither necessary nor feasible, considering the non-stationary nature of the data. This differs from applications such as automotive systems, where recurring conditions (e.g., varying weather) require continual learning to handle repeated scenarios. However, it is worth mentioning that a certain level of decoder stability tends to benefit user learning in MI decoding [18].\nAnother distinctive aspect of continual MI decoding across multiple sessions is the potential absence of calibration data for the upcoming (target) session [16]. As a result, offline adaptation between sessions must rely solely on the most recent data, i.e., the data from previous session(s).\nTo address the distribution shift between consecutive sessions, i.e., the most recent session and the upcoming target session, online test-time adaptation (OTTA) [23], [24] emerges as a viable approach. OTTA leverages the incoming sample-wise stream of unlabeled target data after deployment to adapt the model dynamically to the evolving unknown target distribution.\nWhile decoder adaptation and continual learning have shown promise, key questions remain about effectively in- tegrating fine-tuning strategies in longitudinal MI decoding across large user groups. The dynamic interplay between user learning and decoder adaptation, alongside EEG's non- stationary nature and limited target data, demands broader studies beyond single-subject or short-term settings.\nTo address these challenges, we systematically investigate continual learning for online MI decoding in a large-scale longitudinal setting. Our contributions are as follows:\n\u2022 We are the first to investigate deep learning-based continual learning for MI decoding in a longitudinal setting across a large user group (61 subjects).\n\u2022 We examine the impact of different fine-tuning strate- gies on the performance and stability in a realistic causal pseudo-online [25] manner.\n\u2022 We demonstrate the effectiveness of combining offline fine-tuning together with online test-time adaptation to establish a comprehensive, fully adaptive calibration- free decoding framework. This framework effectively leverages new data as it becomes available to adapt the decoder to users' evolving neural patterns, addressing the domain shifts naturally present in biosignals. It not only ensures sustained performance and stability across sessions and subjects but also enables continuous performance improvements."}, {"title": "II. MATERIALS & METHODS", "content": "We employ the dataset published by Stieger et al. in 2021 [26], which includes data from 61 subjects with 7-11 sessions per user. To our knowledge, this is the only publicly available motor imagery (MI) dataset that captures longitudinal user learning within a large population with online feedback [27]. In contrast, commonly used BCI datasets, such as those from the BCI competitions [28], focus on small user groups (typically around 10 subjects) and neglect user learning, as they include only a few sessions (typically \u2264 2). More recent databases [7], [29] have expanded to include significantly larger subject pools, enabling more robust analyses across users but still offer limited sessions per user. While data recorded for the Cybathlon competitions [17] addresses this limitation by including multiple sessions over extended pe- riods [18], it focuses solely on a single patient, i.e., the pilot competing in the event. The only other dataset comparable to the one used in this study is provided by Forenzo et al. [19]. However, it includes significantly fewer subjects and sessions than the Stieger2021 dataset. Moreover, since their study alternates between different decoders, they were not able to observe any user learning over time. While this potentially provides valuable insights into the impact of decoder stability on user learning, it raises questions about the dataset's suitability to investigate (mutual) learning dynamics.\nStieger2021 dataset: The dataset was collected at a sam- pling rate of 1000 Hz using 64 EEG channels. Data collection spanned 7 to 11 sessions, with an 8-week gap between the first two sessions, followed by sessions recorded every 2- 3 days. For our analysis, we selected 24 channels centered around the motor cortex, resampled the data to 250 Hz, and applied a band-pass filter between 8 Hz and 30 Hz to capture the \u00b5 and \u1e9e rhythm. Each session consisted of 450 trials, equally divided among three paradigms: left/right movement, up/down movement, and combined 2D movement. Participants in the left/right (LR) movement paradigm imagined opening and closing their right (left) hand to move the cursor to the right (left). In the up/down (UD) movement paradigm, they imagined opening and closing both hands to move the cursor upward and voluntary rest to move it downward. The combined 2D movement paradigm required participants to integrate both types of imagery for two-dimensional cursor control. For this study, we focused exclusively on the binary paradigms (LR and UD), as the four-class task typically results in accuracies too low for practical control [26], [30]. This results in 150 trials per session per paradigm.\nAt the beginning of each trial, the target appeared on the screen for 2s, indicating the desired direction of cursor movement. During the subsequent feedback phase, partici- pants had up to 6s to steer the cursor toward the target. The trial ended earlier if any target, i.e., an edge of the screen, was reached. The position of the cursor, serving as visual feedback, was determined using an autoregressive (AR) model of order 16, and was updated every 40 ms. This AR model serves as the baseline for our investigations, representing a widely established online decoding method."}, {"title": "Model", "content": "We employ the BaseNet [31] architecture, which can be considered a modern evolution of the shallow convolutional neural networks ShallowNet [32] and EEGNet [33]. \u03a4\u03bf make the architecture suitable for online decoding, we use real-time adaptive pooling (RAP) [34], which modifies the pooling layers to enable the decoding of sliding windows. We select sliding windows of length 1s (as done in, e.g., [19], [30], [34]) and an update frequency of 25 Hz to match the original online setting. This leads to the RAP parameters $k_1 = 5, s_1 = 5, k_2 = 50, s_2 = 2$. The complete source code is available on GitHub 1."}, {"title": "Training strategies", "content": "Each subject participates in up to 11 sessions, with each session containing $N_t = 150$ trials $X_i$ per paradigm paired with corresponding labels $y_i$. The dataset for a single session is represented as $D^{subject}_{session} = \\{(x_i, y_i)\\}_{i=1}^{N_t}$. During supervised pre-training, we employ a cross-subject leave-one-subject- out strategy to learn subject-invariant representations as done in [35], providing an effective initialization for subsequent subject-specific fine-tuning. The source dataset is constructed by aggregating data from the first session of the remaining $N-1 = 60$ subjects, denoted as $D_{source} = \\cup_{j \\in \\{1, ..., N\\}\\{i\\}} D_1$. This approach enables calibration-free online decoding for unseen subjects by leveraging data from the other participants. Moreover, it reflects a practical scenario in which only limited data from multiple subjects is available at the initial stage of source model training. Subsequent subject-specific applications can then be highly personalized.\nAfter pre-training, the model undergoes supervised fine- tuning using subject-specific data under a causal constraint, ensuring that only data recorded prior to the test session is used for fine-tuning. This fine-tuning step is essential as it enables the model to adapt to subject-specific patterns, which can vary widely across users. By refining the pre-trained, subject-invariant model with personalized data, the model becomes better suited to each user's unique characteristics, resulting in improved decoding performance.\nWe investigate two distinct data settings typically exam- ined in continual learning: exemplar-free and joint. In the exemplar-free setting, only the data from the prior session is used for fine-tuning, whereas the joint setting incrementally incorporates data from all prior sessions.\nAdditionally, we evaluate two fine-tuning strategies: in- dependent and sequential. The independent strategy reini- tializes fine-tuning from the pre-trained source model for each new session, while the sequential strategy builds upon the most recently fine-tuned model of the target subject. Combining both, there are four different settings in to- tal: exemplar-free independent, exemplar-free sequential, joint independent and joint sequential, which are compared to the baseline from [26] and the non-adapted, subject- invariant source model."}, {"title": "Task vector notation", "content": "A widely used representation for describing the variations in fine-tuned models is the task vector notation [36]. For simplicity, we will omit the target subject index in the fol- lowing explanation. Here, $\\Theta_{src}$ represents the model weights after pre-training on the source data, while $\\theta_t$ denotes the weights following the t-th fine-tuning iteration. The task vector is defined as $\\tau_t = \\theta_t - \\Theta_{src}$, which specifies a direction within the weight space. The task vector notation of the four different settings together with the corresponding fine-tuning trajectory is visualized in Fig. 1.\nFollowing the causal constraint of only using the previ- ous sessions for fine-tuning, the fine-tuned weights $\\theta_t$ are evaluated using the datset $D_{t+1}$ from the session $t+1$ ."}, {"title": "Test-time adaptation", "content": "As rebiasing the decoder between different domains [34], [37] is a very important step in MI but recording addi- tional calibration data for each new domain is costly [16], we perform online test-time adaptation (OTTA) [23], [24]. Specifically, we use Euclidean alignment (EA) [38], [39] in an online fashion [34] to mitigate the domain shift of the input data between sessions. Additionally, we use online Adaptive batch norm (AdaBN) [40], [41] to account for the shifts in the batch normalization statistics in the intermediate layers of the model. Both adaptation processes are carried out using only the current (sliding) window of the target session, making this a single-sample OTTA approach. The use of OTTA makes it possible to dispense domain- specific calibration data without losing performance. Another advantage is that, since as we only account for the overall distribution shift, the decision boundary does not change within one session, ensuring stability and thus facilitating user learning [18]."}, {"title": "Metrics", "content": "To compare our different approaches, we use the trial-wise accuracy [29] as our primary metric. This means that a trial is considered to be successful if more than 50% of all windows of that trial are correctly classified. For simplicity, we will refer to this as accuracy in the following.\nReported single values with standard deviations correspond to the mean and standard deviation of individual subject performances. This is achieved by first averaging sessions per subject, ensuring equal representation regardless of the number of sessions completed by each subject.\nTo compare the similarity between task vectors, we em- ploy the cosine distance $d(\\tau_i, \\tau_j) = 1 - \\frac{\\tau_i^T \\tau_j}{\\|\\tau_i\\|\\cdot\\|\\tau_j\\|} \\in [0,1]$ ."}, {"title": "III. RESULTS & DISCUSSION", "content": "Figures 2a and 2b present the session-wise test accuracy for the different approaches corresponding to the LR and UD paradigms, respectively. For the baseline and our source model, which remain constant (apart from rebiasing) across the sessions for each subject, the performance increase over time can only be attributed to the user exhibiting more discriminative patterns over time. It is, however, worth noting that as our experiments are carried out in a pseudo-online fashion [25], potential user adaptation to our decoders can not be explicitly examined.\nBoth figures clearly demonstrate that incorporating decoder adaptation enhances the average test accuracy and increases the extent of performance improvement over time. This trend is indicative of successful decoder adaptation. We speculate that in an online experiment, the improvement over time could be even greater, as users would have the opportunity to adjust to the adapting decoder.\nThe average performance, together with the pairwise p- values (paired two-sided t-test against baseline and source model), are displayed in Fig. 3. For the LR paradigm, all our models outperform the baseline, and all fine-tuning approaches outperform the source model with p < 0.001. For the UD paradigm, the source and baseline are not statistically different (p = 0.748), and the difference between the baseline and the exemplar-free independent setting is smaller (p = 0.0124).\nThese findings confirm the decoder adaptation's effec- tiveness while highlighting key differences between the ap- proaches. Notably, leveraging previously acquired subject- specific knowledge, whether explicitly through joint fine- tuning or implicitly by using the previously fine-tuned de- coder, leads to improvements in overall performance.\nThe joint sequential setting achieves the highest accu- racy, as it benefits from cumulative progress by reusing the previously fine-tuned model, in contrast to the inde- pendent approach. Furthermore, this setting ensures stable fine-tuning by incorporating all previously recorded subject- specific data, unlike the exemplar-free approach.\nTo further understand the relationship between the fine- tuning approaches, we visualize the session-wise test accu- racy for each fine-tuned model in Fig. 4. In each matrix, the first column represents the source performance, while the diagonal entries on the right correspond to the fine-tuning results reported in Fig. 2. Since we adhere to a strictly causal data setting, sessions are not evaluated using models fine- tuned with data recorded in later sessions. Thus, these entries are marked with an X.\nFor three out of four settings, we observe a clear trend: performance improves with both the progression of sessions (user learning) and increased fine-tuning steps (decoder adap- tation). When comparing rows, selecting the most recently fine-tuned model (i.e., the field on the right) exhibits the highest test accuracy.\nIn the exemplar-free independent scenario, these trends are still apparent but show a diminished degree of improvement across sessions and fine-tuning steps. We attribute this to reduced stability, as fine-tuning relies only on data from the most recent session, which can vary significantly due to factors such as user concentration, motivation, or environ- mental influences. Consequently, a single session may not adequately represent the current state of the user. Moreover, in this scenario, the decoder cannot leverage previously acquired subject-specific knowledge and must begin adapting to the user from scratch for each new session, limiting its ability to achieve cumulative progress.\nThe displayed matrices represent averages across subjects and paradigms, and subject-specific matrices might deviate from the overall trend. Thus, to further assess the validity of selecting the most recently fine-tuned decoder, we conducted a theoretical experiment. For each subject, we identified the best-performing decoder per session to establish a theoretical upper bound. For Fig. 4, this corresponds to picking the highest value per row in each subject-specific matrix.To examine the stability between fine-tuned models more closely, we calculated the cosine distance between task vectors, as shown in Fig. 5. The distance matrix for the exemplar-free independent scenario supports our earlier con- clusions. Since fine-tuning restarts from scratch for each session, the process is less stable, resulting in larger distances between consecutive task vectors compared to the other three settings. Nevertheless, the cosine distance remains well below 1, indicating a degree of relationship and some level of stability between consecutive task vectors. In contrast, for task- or class-incremental fine-tuning, the cosine distance ap- proaches one [36], signifying nearly orthogonal task vectors.\nFor the other three settings, especially the later task vectors exhibit greater similarity to one another, suggesting increased stability between fine-tuning steps, which tends to benefit user learning [18], [19]. Additionally, across all four settings, the distance to the first task vector is noticeably larger. This may be attributed to greater data discrepancies, given the 8- week gap between the first two sessions compared to only a few days between each of the subsequent ones [26].\nThe previously presented results demonstrate the advan- tages of training models in a joint sequential manner. How- ever, since the joint data setting increases the memory and time requirements for fine-tuning, we examined how the number of previous sessions used during sequential fine- tuning influences performance. The results are shown in Table II, with p-values calculated against the joint setting (first row). For the LR paradigm, performance differences are significant across all data settings, though a trend emerges where accuracy approaches the joint setting when incor- porating four previous sessions. In contrast, for the UD paradigm, the differences are generally smaller. Interestingly, fine-tuning with only three or four previous sessions slightly outperforms the joint setting. We speculate that this may be due to greater user learning over time (see Figure 2b baseline), where older sessions could hinder rapid adaptation to new data. Nonetheless, the differences are still small, and this hypothesis is speculative as we can not examine the user's behavior in the different settings. Therefore, we still recommend using the joint setting for its stability. However, if memory constraints or a large number of sessions become a concern, this setting could likely be relaxed without a (large) loss in performance.\nTo eliminate the need for session-specific calibration data while maintaining performance, we utilize OTTA throughout this study, which integrates online EA and online AdaBN. To evaluate the individual contributions of these components, we conducted an ablation study using the source model. The results are presented in Table III, where p-values are calculated against the fully enabled configuration (first row). Notably, disabling EA alone contributes to a ~ 10% loss in performance compared to the source model with both com- ponents active. Similarly, disabling AdaBN also decreases performance, though its statistical significance (p < 0.05) is only observed for the UD paradigm. This may be due to the generally higher level of user learning observed in the UD paradigm, meaning the impact of OTTA is more pronounced when the distribution shift is greater. Ultimately, while EA has a more substantial effect, both components play a crucial role in ensuring reliable and robust decoding performance."}, {"title": "IV. CONCLUSION", "content": "This study explored various fine-tuning strategies for pseudo-online longitudinal EEG MI decoding in a causal setting for a large user group. We investigated the impact of different training strategies to incorporate previously acquired subject-specific knowledge implicitly or explicitly during fine-tuning with various training designs. The results demonstrate that leveraging previously acquired subject-specific information enhances performance and improves stability. Overall, the most effective strategy, joint sequential fine-tuning, involved incorporating all previously recorded subject-specific data while continuously building on the last subject-specific fine-tuned model.\nFurthermore, the use of OTTA enables calibration-free op- eration for new sessions or subjects, making our approach well-suited for real-world applications and directly integrable into both applicative and experimental contexts.\nAlthough these results are obtained through offline analysis, we make every effort to replicate a pseudo-online setting that closely approximates real conditions. While our experimental analysis only accounts for one half of the two-learner system, we are confident that these results are robust and will hold in online experiments as this study is conducted on a large user group and across several sessions."}]}