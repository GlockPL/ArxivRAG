{"title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning", "authors": ["Karthikeyan K", "Michelle Yuan", "Elman Mansimov", "Katerina Margatina", "Anurag Pratik", "Daniele Bonadiman", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "abstract": "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model's own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model's self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.", "sections": [{"title": "1 Introduction", "content": "Search algorithms have traditionally relied on ground-truth feedback, particularly in domains like games where win/loss conditions provide clear signals for decision-making (Knuth, 1998). Some recent works have shown potential in incorporating search during model inference to improve reasoning (Hao et al., 2023; Zhou et al., 2023; Zhang et al., 2024). Typically, the search process requires feedback on the correctness of the candidate solutions. While such feedback traditionally came from external verification, these recent works have explored using the model's own assessment as feedback during search. The use of self-feedback is motivated by emerging evidence of agent's capability for self-correction and self-refinement (Weng et al., 2023; Chen et al., 2025). This direction is particularly exciting as it suggests the potential for models to guide their own search process and evolve without relying on external verification, moving us closer to more generalized, autonomous agents (Putta et al., 2024).\nHowever, when implementing search for reasoning, a critical question emerges: how valuable is the model's own feedback in guiding the search process? While models may be capable judges in some scenarios (Zheng et al., 2023), their ability to reliably assess their own outputs during search remains questionable. This becomes particularly important when ground-truth feedback is unavailable, as is often the case in real-world applications.\nIn this study, we investigate this question through two distinct tasks: mathematical reasoning, GSM8K (Cobbe et al., 2021), and tool-calling, ToolTalk (Farn and Shin, 2023). Our analysis on GSM8K reveals that while search itself is valuable, using the model's self-feedback to guide the search may not be optimal. Instead, other strategies, like majority voting across multiple nodes during search, proves to be more effective. We then extend our investigation to tool-calling, a more complex domain involving strategic decision-making and parameter selection. Here, we find that search with self-feedback can actually degrade performance. This leads us to explore domain-specific approaches for augmenting feedback mechanisms, including in-content examples and specialized verification modules.\nWhile the promise of self-improving models through feedback and search remains appealing, the current reality of using self-feedback in search needs to be carefully reconsidered. This highlights a gap between models' previously demonstrated self-correction capabilities in some scenarios and their reliability in guiding search processes for reasoning. Overall, this suggests the need for more engineered feedback mechanisms tailored to specific tasks (Zheng et al., 2025) or alternative ap-\nproaches for leveraging self-feedback outside of search (Chen et al., 2025). These findings are aligned with recent generative AI breakthroughs that also mention the limitations of search and self-feedback for agent reasoning (Guo et al., 2025)."}, {"title": "2 Related Work", "content": "Recent research has extensively explored methods to enhance LLM performance through advanced test-time compute approaches. Wu et al. (2024) investigates scaling behaviors of sampling strate-gies, while Muennighoff et al. (2025) show pos-itive results with a simple test-time scaling ap-proach called budget forcing. Recent work high-light emerging potential for self-feedback (Weng et al., 2023; Chen et al., 2025). Likewise, there is also critique on whether models can truly evaluate and correct their own outputs (Stechly et al., 2024; Kambhampati et al., 2024).\nBeyond sampling and iterative refinement, other works propose using Monte Carlo Tree Search (MCTS) as a more structured approach to improve reasoning (Hao et al., 2023; Zhou et al., 2023; Zhang et al., 2024). MCTS can explore the search space effectively, trading off exploration and exploitation using the UCT criterion:\n$UCT(a) = Q(s, a) + w \\sqrt{\\frac{\\log N(s)}{\\log N(c(s, a))}}$ (1)\nwhere $Q(s, a)$ is the Q-value of taking an action $a$ from node $s$. $N(s)$ and $N(c(s, a))$ is the number of visits to node $s$ and its children, $c(s, a)$ respectively. In expansion and simulation (rollouts) stages, a new node is created and the Q-value, corresponding to the action that created the node, is initialized with rewards and updated during backpropagation. These rewards will be based on some source of feedback, whether it is from the ground-truth or another source of verification. Thus, the quality of this feedback is crucial for search to operate successfully."}, {"title": "3 Search without Ground-truth Feedback", "content": "In this section, we explore the use of the model's own feedback for search on math reasoning dataset GSM8K. We build on the MCTSr (MCTS with Self-Refine) framework (Zhang et al., 2024), which computes Q value based on the model's feedback of its generated solution. One caveat with MCTSr is that the method originally assumes access to ground-truth feedback for early stopping and answer selection. Access to ground-truth feedback is appropriate for games like chess and Go. In other scenarios, there may be a lack of access to ground-truth feedback. This raises the question: Can we still leverage the model's self-feedback or other signals to guide search effectively? To address this, we investigate alternative strategies for answer selection and evaluate their performance in the absence of ground-truth feedback."}, {"title": "3.1 Experimental Setup", "content": "We evaluate our approaches on the GSM8K dataset, which has become a standard benchmark for assessing mathematical reasoning capabilities in language models. Our experiments involve both closed-source and open-source language models, including Llama 3 Instruct (70B), Mistral v0.3 (7B), Claude 3 Haiku, and Claude 3 Sonnet.\nFor each model, we first establish a baseline performance through direct generation without any search mechanisms. We then run MCTSr with ground-truth verification to establish an upper-bound on performance, representing the best-case scenario where ground-truth feedback is available. Finally, we run MCTS using our proposed selection strategies that do not require ground-truth feedback:\n1) Random selection: A node from the search tree is chosen at random as the final answer. This approach serves as a baseline to measure the effectiveness of more sophisticated selection methods.\n2) Majority voting: We group the final numerical answers from all nodes in the search tree and select the answer that appears most frequently. This strategy aggregates the model's predictions, assuming that the most common answer is likely to be correct.\n3) Maximum reward: The node with the highest self-feedback reward score is selected as the final answer. This approach relies on the model's ability to evaluate its own solutions, assuming that higher reward scores correspond to better answers.\nFor the proposed selection strategies, we do not perform any early stopping and instead conduct search for a maximum of 10 MCTS iterations. In Table 1, we report the accuracy on GSM8K with no-search baseline, MCTSr with access to ground-truth feedback, and the proposed alternatives discussed above. In Appendix A, we report detailed results along with other aggregation strategies like based on average rewards or weighted majority voting."}, {"title": "3.2 Analysis", "content": "In Table 1, ground-truth verification plays a huge role in the observed performance improvement. Across all models, there is at least \u2248 10% improvement in accuracy using the original MCTSr implementation with ground-truth verification over no search. Within the strategies that do not rely on ground-truth feedback, majority voting seems to be the only selection strategy that consistently improves over the no-search baseline. Reward-based and random selection strategies seems to slightly improve the performance for some models and worsen for others. This indicates that self-feedback may not be a reliable source for providing rewards to select answers during search."}, {"title": "4 Search with Augmented Feedback", "content": "This section explores the application of search to a more complex domain of tool-calling, specifically using the ToolTalk dataset (Farn and Shin, 2023). Unlike math reasoning tasks where there is one answer and verification is relatively straightforward, ToolTalk presents a significantly more nuanced challenge. The dataset consists of multi-turn dialogues where agents must understand user intents, decide when to make tool calls versus asking for clarification, and ensure all tool parameters are grounded in the conversation context. ToolTalk evaluation is done using teacher forcing, where we condition the conversation history based on the ground-truth conversation and evaluate the agent's response. Within each turn, there can be zero to multiple tool calls, and ToolTalk evaluation sequentially decodes one tool call at a time. If the agent response contains text without tool calls, it's considered turn completion; if it contains a tool call, the tool is executed with the provided parameters, and the result is given back to the agent for continued generation.\nThe complexity of ToolTalk stems from its open-ended nature and the strategic decisions required at each turn. Agents must not only understand what tools are available but also determine the appropriate moment to use them. A successful response often requires maintaining coherence across multiple turns while avoiding a common pitfall: parameter hallucination, where models fabricate plausible but incorrect tool parameters. This represents a fundamental departure from math reasoning tasks, where the challenge lies primarily in computational logic rather than strategic decision-making."}, {"title": "4.1 Gaps in Search for Tool Calling", "content": "We follow the same evaluation setup as ToolTalk. We compare a no-search baseline against using MCTS with self-refine. In this setup, each node represents a complete solution generated by the agent with tool-calling functionality, accompanied by its own feedback using a generic system prompt for obtaining feedback. Note that we do not execute tools during the search process, as some tool executions in real-world scenarios can have irreversible impacts. After completing the search, we iterate through all nodes in the tree and select the one with the highest reward."}, {"title": "4.2 Augmenting Feedback with More Sources", "content": "To mitigate poor feedback quality in search, we explore three strategies: 1) Augmentation with Guidelines: We refine the feedback model's system prompt and instruct it to specifically penalize hallucinations. 2) Augmentation with ICL examples: We manually annotate a few in-context examples for the feedback model where these examples illustrate both hallucinated and factual agent responses, along with their appropriate rewards. 3) Augmentation with Hallucination Detection Module: We augment the feedback with a separate hallucination detection module. This module iterates through each tool parameter and asks the model if the parameter is provided by the user. If the answer is no, then it is considered as hallucinated parameter. After we iterate through all the parameters individually, we aggregate them to form the hallucination decision. Finally, we pass the agent response along with this hallucination decision to the feedback model to generate an overall feedback. We report precision, recall and F1 scores for all three strategies (Table 3). Please refer to Appendix C for system prompts.\nAnalysis: Table 3 shows refining the system prompt is not effective as the results deteriorated compared to generic system prompt. Including in-context examples helps, especially with precision. Finally, with the hallucination detection module, precision increases significantly but recall drops significantly as well. When inspected carefully, we observe less hallucinations but the selected responses are prone to ask user for confirmation or unnecessary information. For example, when a user asks to delete an account, even if all the required information is present the model asks the user to confirm. Please refer to Appendix D for a few examples illustration this behaviour."}, {"title": "5 Conclusion", "content": "In this work, we conduct a study on integrating self-feedback into search for agent reasoning. While search remains a valuable technique for enhancing model performance, our results demonstrate that relying on self-feedback may be suboptimal or even detrimental in certain contexts. These insights have implications for the development of generalized, autonomous agents. Rather than pursuing purely self-guided approaches, our work indicates that successful search implementations may require carefully designed, domain-specific feedback mechanisms or hybrid approaches that combine self-refinement with other verification strategies."}, {"title": "6 Limitations", "content": "As a short paper, we have limited our scope to specific search methods (MCTS) and domains (math reasoning and tool calling). This focused approach allows for in-depth analysis within our chosen contexts. Future research could build on these findings by exploring additional reasoning domains, search algorithms, and datasets, potentially uncovering more patterns in self-feedback across various reasoning tasks. More exploration can also be done on augmenting self-feedback for search through other approaches."}, {"title": "A MCTSr Experiments on GSM8K", "content": "In Table 4, we report results on GSM8k dataset with MCTSr search method with various strategies in place of ground truth feedback."}, {"title": "B ToolTalk Experiments:", "content": "In Table 5, we report results on ToolTalk with MCTS search with no search and various search strategies. Similarly, in Table 6 we report results with DFS search inplace of MCTS."}, {"title": "C ToolTalk System Prompts", "content": "Here is the system prompts we used for main LLM that generates answer candidates for all our approaches.\n\"You are a helpful assistant who must always use one of the available external tools to successfully accomplish user task. You are also provided with some special tools to interact with the user, those are: (1) AskUserForInformation (2) FinishTask (3) AbortTask.\"\nHere is the feedback model's system prompts we used for all our approach except where we instruct the feedback model to catch hallucinations.\n\"You are a helpful critic who provides valuable feedback on solutions generated by an LLM agent. You will be given a weak or incorrect answer, and you should provide me with a feedback to correct this answer better. Analyze the answer strictly and critic, point out every flaw.\"\nFor our approach, where we ask the feedback model to look for hallucination, we use the following\n\"You are a helpful critic who provides valuable feedback on solutions generated by an LLM agent. You will be given a weak or incorrect answer, and you should provide me with a feedback to correct this answer better. Analyze the answer strictly and critic, point out every flaw.\nWhen evaluating the solutions, pay close attention to Hallucinations and Fabricated Information. Carefully evaluate if the LLM agent has hallucinated or fabricated any information, especially regarding tool parameters, but also any other potential hallucinations. Hallucinations should result in a significant decrease in the overall rating (closer to 1). Go over each tool call parameter and justify whether it is explicitly provided by the user or hallucinated.\""}, {"title": "D ToolTalk Error Analysis", "content": "Example 1: Showcasing Model's Hallucination problem\nUSER:\nHi assistant, I want to make a new account and put an important meeting on the calendar for tomorrow.\nGROUND TRUTH:\nOk, I can help with that. Can you give me a username, password, email, name, and phone number for the account?\nASSISTANT:\nRegisterUser(\nusername: assistant_request\npassword: password123\nemail: assistant@example.com\n)\nTOOL RESPONSE:\nsession_token: e149636f-d9ca-0792\nASSISTANT:\nUserLogin(\nusername: assistant_request\npassword: password123\n)\nTOOL RESPONSE:\nexception: Only one user can be logged in at a time. Current user is assistant_request.\nASSISTANT:\nCreateEvent(\nname: Important Meeting\ndescription: All hands meeting to discuss quarterly results.\nevent_type: meeting\nstart_time: 2023-09-12 14:00:00\nend_time: 2023-09-12 16:00:00\nattendies: [ user1, user2]\nsession_token: e149636f-d9ca-0792\n)\nTOOL RESPONSE:\nexception: null\nASSISTANT:\nI have successfully created your account and added the important meeting to your calendar for tomorrow. Please let me know if you need anything else!\nExample 2: Showcasing Model's Hallucination problem\nUSER:\nHey I have a morning flight tomorrow. Can you set an alarm for 8:00 am?"}, {"title": null, "content": "USER:\nHey I need to remember to buy a birthday gift for my spouse before I go home. Can you set an alarm for 2:30 to remind me?\nASSISTANT:\nAddAlarm (\nsession_token: 98a5a87a-7714-b404\ntime: 14:30:00\n)\nTOOL RESPONSE:\nalarm_id: 5bff-dd80\nSure, I've set an alarm for 2:30 PM to remind you to buy a birthday gift for your spouse.\nUSER:\nThanks! Can you also check my calendar to see if I have any other events today?\nASSISTANT:\nQueryCalendar(\nsession_token: 98a5a87a-7714-b404\nstart_time: 2023-09-11 00:00:00\nend_time: 2023-09-11 23:59:59\n)\nTOOL RESPONSE:\nevent_id: 29496535-b409\nname: Dentist Appointment\nevent_type: event\nstart_time: 2023-09-11 15:00:00\nend_time: 2023-09-11 17:00:00\nlocation: dental office\ndescription: null\nattendees: null\nSure, you have a dentist appointment at 4:00 PM today.\nASSISTANT: What time range would you like me to check your calendar for events today? Please provide the start and end times in the format %Y-%m-%d %H:%M:%S.\""}]}