{"title": "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models", "authors": ["Rebekka G\u00f6rge", "Michael Mock", "H\u00e9ctor Allende-Cid"], "abstract": "Social categories and stereotypes are embedded in language and can introduce data bias into the training of Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights the into formation and spread of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a given sentence. For this, we derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in human language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a given sentence, where the language model examines the linguistic properties of a sentence and provides a basis for a fine-grained stereotype assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these linguistic indicators are present in these sentences and explain the strength of a stereotype using the scoring function. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated described behaviors and features. However, using more few-shot examples within the prompts, significantly improves performance. Furthermore, model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct_4bit.", "sections": [{"title": "1 Introduction", "content": "Content Warning: This paper presents textual examples that may be offensive or upsetting.\nLanguage reflects and transmits the social categories and stereotypes that humans use to quickly perceive and interact within their complex social environment. Stereotypes are defined as \"cognitive representation people hold about a social category consisting of beliefs and expectancies about their probable behavior, feature and traits\" [13]. Although social categorization is a fundamental human tendency [4], it often leads to oversimplified perceptions of groups, exaggerating similarities within the group, while amplifying differences with those outside. Harmful consequences, such as discrimination or unfair decisions, arise when individuals are judged based on broad social category associations rather than their unique traits. This might even be enforced by the use of negative stereotypes, which are denoted as prejudice\u00b9. Encoded in human language, also large language models (LLMs) trained on massive amount of aggregated and crawled text data are learning, reproducing and disseminating stereotypes [32]. This perpetuation of stereotypes and its potential harmful impact on society and individuals is a significant concern regarding AI [27].\nCurrent research investigates in the detection and mitigation of stereotypes as part of the research on fairness and bias\u00b2 of AI models [18]. Within the context of bias, the presence of stereotypes in training data constitutes a form of data bias\u00b3. Likewise, the reproduction of stereotypes by AI models is seen as a harmful consequence of bias, often referred to as representational harm [18]. To mitigate representational harm, state-of-the-art (SOTA) LLMs are equipped with guardrails to prevent stereotypical output. Although these measures are often effective against explicit reproduction of stereotypes, they often fail when confronted with slight variations in the prompting [42]. This indicates that intrinsic biases continue to manifest in model behavior, highlighting the need to reduce stereotypes as much as possible at the data level. Also sociolinguistics has long examined the emergence and linguistic form of stereotypes, Blodgett et al. finds that most research on bias in natural language processing (NLP) is poorly aligned with interdisciplinary studies. Existing work in stereotype detection primarily relies on human-constructed and annotated benchmarking datasets [3, 16, 30, 31], which serve as the foundation for training classifiers for text-based stereotype detection and for benchmarking widely used LLMs. These datasets and resulting detection methods typically categorize sentences as either stereotypical or anti-stereotypical based on subjective human assessments, leading to pitfalls such as misalignment or uncleanness of the stereotypes themselves [5]. To address this issue, Liu argues that a purely binary separation of stereotypes is insufficient, as stereotypes can be expressed in various forms. Instead, they propose a fine-grained quantification of the strength of a stereotype using a continuous scoring system grounded in human stereotype\n\u00b9\"Negative affective evaluations of social categories and members\" are denoted as prejudice [4, 39]\n\u00b2Definition according to [22]: \"Systematic difference in treatment of certain objects, people, or groups in comparison to others\"\n\u00b3Definition according to [22]: \"Data properties that, if unaddressed, lead to AI systems that perform better or worse for different groups\""}, {"title": "2 Related work", "content": "As introduced above, research on stereotyping in NLP is an integral part of the wider investigation into fairness and bias in artificial intelligence (AI). According to Hovy and Prabhumoye, the key sources of bias in NLP are the data used to train models and the models themselves. While biased data reflects societal stereotypes and inequities, model architectures and training processes can amplify or introduce additional biases, compounding the problem. Stereotypes in language systems intersect with social hierarchies and human cognition, highlighting the need to engage with foundational literature from psychology, sociology, and sociolinguistics to understand stereotype formation, its harms, and to guide the development of more equitable NLP methodologies. While, few works in NLP build up on this foundation [5], numerous studies have shown that bias, including stereotyping, is a problem in NLP [7, 9, 42]. This includes more recently pre-trained language models, especially masked language models (MLMs) [2, 31, 38], which, while showing considerable success in various NLP tasks, also show considerable evidence of inheriting and perpetuating cultural biases embedded in the training corpora, which can lead to harm through biased representations. Therefore, most recent studies have focused primarily on identifying stereotypes in model outputs as a form of representational harm as a consequence of model bias [18].\nTo this end, several datasets such as the Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark [31], StereoSet [30], Multi-Grain Stereotype (MGS) [43] and FairPrism [16] have been introduced comprising stereotypes across different social categories. StereoSet and CrowS-Pairs in particular are widely used, but also inherit significant weaknesses regarding the construction of stereotypes [6], which is why [33] presents an improved version of CrowS-Pairs. Moreover, both datasets contain so-called anti-stereotypes, which are artificially constructed and unlikely occur in regular discourse [34].\nIn contrast to stereotype as a harm of model bias, the exploration of stereotypes as a form of data bias introduced through human language bias is less explored [17]. Existing methods can be divided into statistical methods and model-based techniques. Statistical approaches such as embedding-based metrics [18] focus on analyzing the distribution and co-occurrence patterns of words, phrases, or demographic categories within datasets. In contrast, model-based approaches often trained on the aforementioned datasets leverage AI models to detect and assess stereotypes by analyzing contextual relationships and latent representations in text. To this end, these models can uncover the explicit sources of bias in text, but may also reflect the biases present in their own training data. Notably, these approaches can also be used as a filter to detect stereotypes in LLM outputs.\nAmong the model-based approaches, there are several studies that specifically adapt and train pre-trained language models for stereotype detection [34] or evaluation [17, 25, 36], using different methods and assessing stereotypes according to different aspects. In the context of stereotype detection, Pujari et al. address the subtle manifestations of stereotypes by creating a focused evaluation dataset that includes explicit stereotypes, implicit stereotypes, and non-stereotypes. They leverage multi-task learning and reinforcement learning to enhance the accuracy of stereotype detection. Often model-based stereotype detection methods lack explainability. To address this, Wu et al. and King et al. propose explainability tools such as Shap and Lime [35] to explain the decisions of stereotype detectors. In terms of stereotype evaluation, Sap et al. develop and implement Social Bias Frames, a formalism capturing the pragmatic implications of stereotypes, supported by a large annotated corpus that emphasizes the need for combining structured inference with commonsense reasoning. Similar, Fraser et al. builds on interdisciplinary work and presents a pre-trained embedding model that models the Stereotype Content Model [15] from psychological research to analyze stereotypes along the dimensions of warmth and competence, leading to an intrinsically interpretable approach. Also Liu focuses on stereotype evaluation and advocates fine-grained evaluation of stereotypes instead of binary recognition by introducing a human ranking of stereotypes and pre-training LLMs on it to investigate correlations between stereotypes and broader social topics.\nWith the recent advances of LLMs, current approaches also propose the use of fine-tuned language models [11, 21, 40] to detect stereotypes in text data. Tian et al. and Dige et al. assess zero-shot stereotype identification using reasoning and Chain-of-Thought (CoT) prompts. Here, Tian et al. finds that reasoning plays a pivotal role in enabling language models to exceed the limitations of scaling on out-of-domain tasks like stereotype detection. However, Dige et al., report limited performance, with Alpaca 7B achieving the highest stereotype detection accuracy of 56.7%, while suggesting that increasing model size and data diversity could lead to further performance gain. This finding is approved by the findings of Huang et al., who evaluate the trustworthiness of current"}, {"title": "3 The social category and stereotype communication framework", "content": "Sociolinguistics has long been researching how stereotypes are shared and maintained in language. To explicate these linguistic processes, the Social Categories and Stereotypes Communication (SCSC) framework [4] integrates different aspects of the research on stereotyping and biased language use in one framework. It is based on the theory that stereotypical expectations are reflected in language through subtle, largely implicit linguistic biases, such as minor changes in syntactic and semantic structures. It is based on the definition of a stereotype introduced in Section 1, which consists of an addressed 'social category' and the associated beliefs and expectations about 'behavior and characteristics'. To explain how theses stereotypes are maintained and disseminate in language, the SCSC framework provides three main aspects: (1) the shared cognition of social categories, (2) the communicated target situation, and (3) the types of bias in language use. Shared cognition of social categories (1) explains, from a cognitive perspective, three factors influencing how people perceive social categories: the extent to which a category is perceived as a meaningful, and coherent group (the perceived category entitativity), the beliefs and expectations about the likely behaviors, features and traits associated with that category (the stereotype content), and the extent to which these characteristics are seen as immutable for its members, and stable across time and situations (the perceived category essentialism). The target situation (2) refers to the information level of the communicated content, which is determined by the generalization of the described content concerning the addressed social target category and the associated situation. The lowest level of information describes situational behaviors of individuals, the intermediate level captures enduring characteristics of individuals, and the highest level outlines enduring characteristics of the group as a whole. While the aggregation of low-level information contributes to stereotype formation, stereotypes are primarily conveyed and maintained at the highest level, making it particularly relevant for stereotype detection.\nFrom particular interest to us is the bias in language use (3), which describes the linguistic biases that are used in language when expressing and maintaining stereotypes\u2074. These linguistic biases mean that certain linguistic forms and patterns are used in the communication of stereotypes in language. Notably, these linguistic biases express, on a linguistic level, the factors of shared cognition of social categories (1), as well as the information level of the target situation (2). Based on the stereotype definition, there are linguistic biases referring to the social category and linguistic biases pertaining to the associated content (characteristics and behaviors). We illustrate this with the sample sentence \"Women can't drive in the rain\". To create or spread a stereotype through language, communication must relate to a specific social category or an individual representing that group. This is achieved linguistically by using a label that identifies the targeted social group, referred to as \"category label\" (e.g., Women). By differing in semantic content (denotation and connotation) and linguistic form (grammatical form and generalization), this category label conveys various meanings and perceptions that influence shared social category cognition. The semantic content of the label transports the categories boundaries, and hierarchies influencing the perceived category entitativity. Moreover, it might even already activate stereotypical content (e.g., referring to women using the label ladies versus feminists). Similar, the generalization of the label and its grammatical form influence the perceived category entitativity and the level of information. For\n\u2074Notably, the language use is embedded in a communicative context determined by factors such as social context, medium, or communication partners."}, {"title": "4 Development of a categorization scheme for linguistic indicators", "content": "4.1 Categorization scheme\nThe SCSC framework [4] uses a variety of examples to describe how linguistic biases in language influence the cognitive perception of stereotypes. However, it does not provide a clear and structured scheme that captures and defines all the different linguistic biases and their potential forms. Therefore, we derive a categorization scheme based on the SCSC framework, which we extend to assess stereotypes automatically at the sentence level.\nAt the highest level, we adopt the division proposed by [4] into the two primary categories, category label and associated content, taking into account both language meaning and linguistic form for each category. From there, we derive from the information level (2) and linguistic biases (3) described in Section 3 a set of n linguistic indicators that signal stereotypes in communication, which we denote as linguistic indicator A\u00a1. We only include linguistic indicators that can be assessed based on linguistics as objectively as possible and without the need for interpretation. For each of these indicators, we define a set of k potential values. Depending on its value, most of the attributes has a strengthening (\u2191) or weakening effect (\u2193) on one of the aspects of shared social category cognition (1) (category entitativity, category essentialism, and stereotype content). The aggregation of linguistic indicators reflects the potential linguistic strength of the stereotype.\nThe categorization scheme is illustrated in Table 2. For both primary categories, we define a linguistic indicator as basic decision criteria to check whether the sentence at all contains a) a category label (has category label) and b) an information on associated behavior or characteristics of the category label (information level (situation)). Both are prevalent for the existence of stereotypical content. While our approach is applicable for the detection of stereotypes against arbitrary social groups, we restrict the identification of a category labels within one analysis to labels referring to predefined sensitive attributes such as race or gender to ensure a meaningful evaluation [5]. If a category label and, if applicable, associated behaviors and characteristics about a behavior or a property are available, both are further categorized. Regarding the category label, it encompasses the following aspects: at the level of meaning, it includes the content of the category label, its connotation, and the information level (target), at the level of linguistic form, it involves the grammatical form and generalization (category label). In terms of shared information, the meaning encompasses the associated content, while the linguistic form includes generalization (content), the use of explanation of behaviors or characteristics, and the use of signal words. To increase objectivity, especially when automating the process with LLMs, we do not include irony bias and negation bias as further linguistic indicators of stereotype inconsistency."}, {"content": "4.2 Dataset and manual ground truth annotation\nTo validate the functionality of the categorization scheme and establish a reference ground truth, we manually annotate a subset of the CrowS-Pairs dataset. The Crowdsourced Stereotype Pairs (CrowS-Pairs) benchmark is a widely used dataset that addresses stereotypes across nine types of social bias (such as race, gender, religion, or physical appearance). It contains 1,508 examples divided into stereotypes (demonstrating a stereotype against a socially disadvantaged group) and anti-stereotypes (violating a stereotype against a socially disadvantaged group). Each example is a pair, with a sentence about a disadvantaged group alongside a minimally different sentence about a contrasting advantaged group. The sentences were obtained via crowd-sourcing with Amazon Mechanical Turk. While CrowS-Pairs is broadly used, Blodgett et al. reveals serious shortcomings in the conceptualization and operationalization of the dataset. Of particular relevance to our work is the pitfall described in relation to anti-stereotypes: The anti-stereotypes found in CrowS-Pairs are usually negations or contrasts of created stereotypes. In some cases, true statements are found, but in many cases irrelevant statements are made about a target group [6]. Due to the artificial construction of the anti-stereotypes, they do not reflect the linguistic patterns for formulating stereotype-inconsistent statements as described in [4], but mainly change the content of the statement about a group [34]. While this might be less critical for benchmarking the preferences of a language model, it cannot"}, {"title": "5 Model development and validation", "content": "The next step is to automate the categorization scheme that has been developed in order to analyze linguistic indicators of stereotypes at sentence level. To implement the categorization scheme, the linguistic indicators A\u00a1 have to be detected and classified. Therefore, it is necessary to solve several classical NLP tasks such as relation extraction or sentiment analysis. For this purpose, we use instruction-finetuned LLMs with an in-context learning (ICL) approach using LLMs as judges, leveraging their strong overall text comprehension capabilities leading to SOTA performance in classical NLP tasks [41]. In-context learning [12] allows LLMs to perform tasks without parameter updates by leveraging contextual information at inference time, making it useful in scenarios with scarce labeled data. Its effectiveness relies on the model's scale and pretraining, with larger models exhibiting superior ICL capabilities. In particular, we exploit few-shot learning [8], which improves model performance by providing labeled examples within in-context learning [29]. The \"LLM-as-a-Judge\" approach [19] employs LLMs as evaluators for complex tasks, addressing subjectivity and variability in traditional evaluations while remaining adaptable to various sensitive attributes without the need for training or fine-tuning. To this end, we follow a promptbased approach that should be effectively applicable to a wide range of sentences. To cover different sizes and architectures, we include Llama-3.1.-8B-Instruct and Llama-3.3-70B-Instruct from the Llama family [14], Mixtral-8x7B-Instruct from Mistral AI [23], GPT-4o-mini and GPT-4 [1] of OpenAI. In the following, we first describe how the categorization scheme is implemented through prompts and incrementally refined through prompt engineering. From this, we validate model performance of various LLMs in detecting and classifying the linguistic indicators."}, {"title": "5.1 Prompt engineering", "content": "In terms of prompt engineering, we manually create and adapt a prompt template by adhering to best practice prompt engineering strategies. We seek to optimize the prompt through iterative refinement, using both Llama-models as reference points. The final prompt for the fine-grained evaluation of linguistic indicators can be found in Appendix A.\nWe construct a few-shot prompt that includes a role description, a task description, and several examples covering different scenarios. The task description describes the categorization scheme introduced in Section 4 and is designed to guide the model through a decision flow to detect the linguistic indicators A\u00a1 and classify its correct value. Following the annotator's guidelines, we formulate a basic prompt by exploring different formulations, that includes the definition of each attribute and an example for each value. This basic prompt incorporates one or several interchangeable sensitive attributes for which stereotypes are to be identified (e.g., Your task is to identify, in a given sentence, a category label referring to ). To facilitate the task, we integrate COT components, prompting the model to extract and repeat relevant content before addressing questions about the category label and associated content (e.g., Extract the exact information shared about the category label). To determine the best number of examples to be used in the few-shot learning, we compare the models' mean performance over the attributes related to the category label and related to the associated content. As shown in Figure 3, we find that by increasing the number of examples in several steps from one to six that the accuracy strongly increases. For the last three examples, the performance decreases slightly, which we assume is due to the fact that they cover corner cases (compare Appendix A). Nevertheless, we keep these examples as they are important for a broad application.\nWe compare single-stage and multi-stage prompt formulations. The single-stage approach uses one prompt for the entire categorization scheme, while the multi-stage approach divides into two sub-tasks: the first prompt focuses on the category label, followed by a second prompt that addresses the associated content using the category label found as context. While Llama-3.3-70B-Instruct achieves similar performance in both stages, the performance of Llama-3.1.-8B-Instruct with respect to the linguistic properties of the content drops drastically in the multi-stage approach. This is consistent with the findings of [37], so we continue with the single-stage approach. Similar to [40], to facilitate deterministic answer detection we ask the model to give answers related to the categorization in a structured way. Here, we find that asking for structured JSON-format {\"has_category_label\": \"yes\", \"full_label\": \"these English gentlemen\"} encourages both models to quote only the requested output in the exact scheme which is not the case by using just a numbering ((1)yes, (2) these English gentlemen)."}, {"title": "5.2 Model validation", "content": "Using this prompt, we evaluate the performance of Llama-3.1.-8BInstruct, Llama-3.3-70B-Instruct, GPT-4, GPT-4o-mini and Mixtral-8x7B-Instruct in the detection and classification of linguistic indicators using a temperature of t = 0.7. All experiments are conducted on a sample of CrowS-Pairs, utilizing our existing human annotations as ground truth. After running the models, we post-process the outputs to extract the linguistic indicators from the JSON outputs. However, GPT-4o-mini and Mixtral-8x7B-Instruct do not consistently adhere to the JSON format, adding extra spaces or backslashes, which we remove in post-processing.\nWe evaluate each model's performance on the linguistic indicators using accuracy and F1-Score, performing multi-class classification as each indicator must be detected and categorized. Detailed results for each linguistic indicator can be found in Appendix C. We summarize the results in Figure 4, illustrating the accuracy of indicators related to the category label in Figure 4a and those related to the associated content in Figure 4b. Performance scores are averaged across individual class performances. Overall, larger models like Llama-3.3-70B and GPT-4 outperform smaller models and Mixtral-8x7B-Instruct across all linguistic properties, though distinct trends emerge across all models: All models perform significantly better in detecting and classifying linguistic indicators of the category label compared to those of the associated content. Indicators such as has category label, the information level, and the grammatical form of the category label are generally well recognized due to their clarity and unambiguity. A closer examination of the F1-score of generalization (target) shows that while the values generic and individual are detected effectively, the intermediate label subset poses challenges for the models. Additionally, nearly all models struggle to accurately identify the connotation of the label, which is unexpected given their strong performance in sentiment recognition. Further analysis indicates that the models often include the sentiment of the entire sentence instead of focusing solely on the category label. In terms of the performance on the content properties, the gap between large and small models widens. While larger models maintain acceptable performance, smaller models experience a significant decline. A noticeable trend across the models is the difficulty in detecting generalization (content), as the models struggle to focus solely on the verbs and adjectives used in the sentences. For other attributes, there is greater variability in the models' performance.\nOverall, the evaluation confirms the potential of LLMs for automatically realizing the categorization scheme. However, only larger models such as Llama-3.3-70B and GPT-4 are able to provide consistent performance across attributes. Challenges remain in accurately interpreting attributes such as connotation and generalization, suggesting areas for potential improvement in future work. For the further experiments, we continue to use Llama-3.3-70B, which achieves a mean accuracy of 81%, comparable to GPT-4's 83%. However, Llama-3.3-70B is open source, making the results easier to reproduce. With these results on LLM performance in detecting and categorizing linguistic indicators of a stereotype, we can already enhance existing stereotype detection mechanisms with providing annotations of the stereotype in terms of an approved sociolinguistic framework. These annotations, which can serve as human-understandable explanations, can facilitate fine-grained analysis of a given text database or as further vehicle for analyzing the LLM outputs. We present an example on this in Appendix B. In the following, we focus on utilizing these annotations to generate a numerical score that quantifies the strength of a stereotype, in the sense of the work presented by Liu[25]."}, {"title": "6 Assessing linguistic stereotype indicators", "content": "6.1 Weighting of linguistic indicators and scoring function\nUsing the categorization scheme and in-context learning, we can effectively derive the linguistic indicators based on the experiments performed. As can be seen in Table 2, each linguistic indicator has a strengthening or weakening effect on category entitativity, stereotype content and category essentialism, and thus on how we perceive stereotypes in language. However, this does not include an explicit weighting of the different factors relative to each other. In order to assess the linguistic indication of stereotypes, we propose a weighting of the linguistic indicators and, based on this, a scoring function for the aggregation of the linguistic indicators.\nTo obtain this weighting, we use the work of [25], which computes a fine-grained stereotype score on CrowS-Pairs based on human ranking of stereotypes. In this study, human annotators have repeatedly compared a tuple of four varying stereotypes to each other using Best-Worst-Scaling [26]. The ranking of each sentence was then converted to a real-value score from 1 to 1 using Iterative Luce Spectral Ranking [28], where 1 indicates a sentence with a large stereotype and -1 indicates a sentence with a small or no-stereotype. If our linguistic indicators are actually present in stereotypical sentences, they should also have been unconsciously employed by these annotators.\nTo estimate the importance of different linguistic indicators in human language, we seek to approximate the score of [25] based on our linguistic indicators. Since [25] also uses CrowS-Pairs, we can map each of the sentences in our annotated subsample of CrowSPairs to its corresponding value of the score proposed by [25], which is publicly available. We normalize the score and denote it as scorebws. We train a linear regression model using scorebws as target and our linguistic indicators A\u00a1 as features.\nscorebws = \u03b2o + \u03b21A1 + ... + \u03b2\u03b7\u0391\u03b7 (1)\nWe include the linguistic indicators A\u00a1, which have a fixed set of values and exclude the open-text attributes content of the category"}, {"title": "6.2 Evaluation of the approach", "content": "In this section, we evaluate our complete approach (see Figure 1) for automated scoring of stereotypes and discuss its effectiveness in assessing the linguistic strength of a stereotype. The evaluation is performed on our sample of annotated stereotypical sentences from the CrowS-Pairs dataset to evaluate the scoring function as such, as well as on the full set of stereotypical sentences from CrowSPairs related to race or gender to evaluate the automated application to new data. The scorebws from [25] is used as reference in both cases.\nWe compute the scoring function for each sentence in our sample dataset based on the predicted linguistic indicators from Llama-3.3-70B, denoted as scorescsc. We evaluate it against the scorescsc from our ground truth annotations of the linguistic indicators to understand how well it is approximated by the scorescsc. Additionally, we compare it to the scorebws, as we aim to understand how well our score aligns with human ratings. The scorescsc differs from the ground truth scorescsc by MAE = 0.03, confirming that LLM effectively captures the most important linguistic indicators. The scorescsc has a minimum absolute error of MAE = 0.06 to the scorebws, which is slightly higher than the scorescsc. As shown in Figure 7, all three scores correlate well but are concentrated between 0.3 to 0.7. However, socrebws exhibits a broader range, particularly downward, compared to scorescsc and scorescsc, which are constrained by the linearity of the scoring function. This suggests the hypothesis that the subjective human ranking is more strongly influenced by perceptions and stereotypical content, leading to stronger values than those captured only by the linguistic indicators.\nTo investigate this hypothesis, we conduct a qualitative analysis of the sample sentences, comparing the ground truth scorescsc with scorebws. We first order the sentences according to the scorescsc and examine those with identical linguistic indicators, but with high deviations in the scorebws. Six examples targeting the same 'social category' are illustrated in Table 3 (more examples in Appendix A, Table 4). The first four and the last two samples share the same linguistic indicators. The first four sentences, differ only in their stereotypical content, where subjectively sentence 1 and 2 seem to be 'less' harmful. Sentences 3 and 4 convey nearly identical and highly prejudicial content, and while they share the same scorescsc, their scorebws differs. Notably, sentence 4 has even a lower scorebws than sentence 2, which is not intuitively explainable. These examples demonstrate that scorebws lacks the interpretability needed to understand the source of these different judgments, and in these cases appears to be heavily influenced by human subjectivity, a limitation also noted by [25]. In contrast, sentences 5 and 6 show that the evaluation of the stereotypical content itself is not reflected in the scorescsc, sometimes leading to under- or overestimates. Here, the human-evaluated scorebws rates the stereotype content in sentence 6 as much stronger and in sentence 5 as much weaker than our score. Overall, the qualitative analysis indicates that scorebws and scorescsc differ more for sentences about individuals (MAE = 0.07) for generic groups (MAE = 0.05), as our score rates low-level information about individuals generally lower. Due to this, it fails to capture (implicit) stereotype deduction regarding specific individuals (e.g., She said to not try and steal anything, me being black and all.)."}, {"title": "7 Conclusion", "content": "In this paper, we present a novel framework for detecting and quantifying linguistic indicators of stereotypes in text, using the concepts of the SCSC framework as guidelines to establish a comprehensive categorization scheme. Based on manual annotations, we find that most of these linguistic indicators are indeed present in widely used stereotypical data. We demonstrate that LLMs and their in-context learning capabilities enable the automatic evaluation of linguistic indicators related to stereotypes. By limiting the models to the evaluation of linguistic factors, we mitigate the risk of introducing model bias into the results. Our results indicate that larger LLMs such as LLama-3.3-70B are particularly effective at detecting these indicators, with an average performance of 82%, especially when provided with a greater variety of few-shot learning examples. However, challenges remain in assessing nuanced aspects like connotation and generalization, as the model does not seem to focus solely on the relevant sentence components. To enhance this, we will focus future work again on employing multi-stage prompts that delve into these aspects in greater detail.\nTo allow fine-grained quantification of stereotypes based on linguistic indicators, we go beyond the SCSC framework and approximate the importance of different linguistic indicators based on an empirical evaluation of human stereotype ranking [25]. Using this weighting, we introduce our scoring function, which aggregates linguistic indicators into a continuous score. Our score partially aligns with human stereotype ranking, but does not fully explain them. While we demonstrate that our scoring function is intrinsically interpretable, and consistent in evaluating linguistic indicators, it does not account for the implications and harmfulness of stereotypical content. In contrast, human-based stereotype rankings, though subject to variability due to individual perception and subjectivity, do capture this harmfulness. This is particularly evident in more difficult linguistic formulations and implicit stereotypes. Nevertheless, our automated scoring function achieves a MAE of 0.07 compared to the human-annotated scoring of [25] but does neither require human annotation nor fine-tunings of models. Additionally, it enables the automated generation of annotations for linguistic indicators of stereotypical statements, which can serve as explanations and a foundation for further analyses.\nWe acknowledge the following limitations of our work: First, our analysis relies solely on the publicly accessible CrowS-Pairs dataset. While the results presented in this paper showcase the effectiveness of our methodology, we recognize the need for future enhancements through the incorporation of additional data. Second, to apply our approach to large amounts of (mostly unrelated) training data the approach is currently not efficient enough and need to be adapted as described in our conclusion. Third, our results have not been evaluated across multiple runs due to budgetary constraints and may exhibit minor variations arising from the use of LLMs.\nIn future work, our function could be extended by including an indicator that evaluates the sentiment associated with the behaviors or characteristics, similar to how the connotation of the label is evaluated. This would still be in accordance with the SCSC framework, that also defines the stereotype content as one of the variables of shared cognition of stereotypes. We will investigate this further, but it is important to note that this goes beyond a purely linguistic assessment, so it is necessary to avoid introducing model bias. Moreover, we will embed the presented approach in a larger framework that also addresses stereotype detection, allowing the method to be used for arbitrary training data evaluation or model output evaluation. For this purpose, we plan to use a two-step approach, where in a first step a detection of potential stereotypes can be performed based on the existence of a category label and associated behaviors and characteristics, and in a second step the scoring function developed in the current approach can be used for a fine-granular assessment of the stereotype strength, raising the opportunity to filter out false positive detections. Currently, we focus only on the sentence level, but future approaches will need to include the broader context in which a sentence is situated."}, {"title": "Ethical statement", "content": "In this paper, we analyze stereotypes derived from a publicly available dataset from the U.S. We acknowledge that these stereotypes are not representative of all cultures. In addition, our assessment of stereotype strength based on linguistic indicators may not fully capture the perceptions of the individuals involved and simplifies a complex issue. Our work is influenced by our own cultural background and we recognize that aspects of fairness beyond our experience may not be adequately represented."}, {"title": "Acknowledgement", "content": "The development of this publication was supported by the Ministry of Economic Affairs, Industry, Climate Action and Energy of the State of North Rhine-Westphalia as part of the flagship project ZERTIFIZIERTE KI. The authors would like to thank the consortium for the successful cooperation."}, {"title": "A Appendix", "content": "Prompt template\nRole description\nYou are a helpful assistant answering only in JSON-Output.\nTask Description\nA category label in a sentence is a linguistic label used to identify a given group of persons that refers to a demographic group or a specific or unspecific person that exist in reality. Your task is to identify (if possible) in a given sentence a category label referring to race or gender and the information that is shared about this category label.\nInstructions\nA category label in a sentence is a linguistic label used to identify a given group of persons that refers to a demographic group or a specific or unspecific person that exist in reality. Your task is to identify (if possible) in a given sentence a category label referring to race or gender and the information that is shared about this category label.\nGiven a sentence", "JSON-Format": "n(1) Evaluate whether the sentence has a category label that refers to a demographic group of people or an individual using only 'yes' or 'no'. Consider gender and race as a demographic group. Be aware that also pronouns and names can give indication on gender and race. If 'yes', the sentence has a category label, answer the following questions with respect to the category label. If the sentence has no category label answer for all following questions using 'not-applicable'\n(2) Name the full category label (including article) used in the sentence to identify the group of people or the person. If more than one category labels are given, name only the most important one.\n(3) Name if, the category label refers to a 'specified target' (it addresses a specified individual or specific individual(s), e.g.Mary, a black man) or to a 'generic target' (it addresses a generic social category, e.g. Asians).\n(4) Name the connotation of the category label without considering the other content, using 'negative' (negative or"}]}