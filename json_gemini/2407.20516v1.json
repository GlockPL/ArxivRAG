[{"title": "4.1.4 Data Sharding", "authors": ["Zheyuan Liu", "Guangyao Dou", "Zhaoxuan Tan", "Yijun Tian", "Meng Jiang"], "abstract": "Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning (ML) grew out of the quest of artificial intelligence (AI) for building a model that learns patterns from a dataset $D = \\{(x, y)\\}$. The model uses the patterns to predict the output y for an unseen input x, known as a classifier or regressor, depending on whether y is a categorical or numeric variable. An interesting question is, after the model is built, what if the developers find that the dataset contains a set of data points $D_f = \\{(x_f, y_f)\\} \\subset D$ that should NOT be learned? For example, we might not want the model to leak a customer's income though it might have seen lots of bank data. Machine unlearning (MU) methods aim to help the model \u201cforget\u201d the data points in $D_f$ (which is named forget set) so that ideally the model would be the same or similar as a new model trained on $D \\backslash D_f$, and MU would save a lot of development cost than re-training the model.\nMU has been studied and reviewed in conventional AI. Recently, generative AI (GenAI), based on generative ML models, has exhibited its capabilities of data generation which may be used as and not limited to classification and regression. For example, large language models (LLMs) [1, 151, 155, 191] can generate tokens right after an article, about its sentiment, topics, or number of words, if prompted properly. That means, there could be a prompt that made the LLM trained on bank data to generate \"[Customer Name]'s income is [Value].\u201d In this case, people want the model to \u201cforget\" so deeply that it would not generate this output with any prompts: the prompt could be strongly related, like \u201cwhat is the income of [Customer Name]?\u201d, or loosely related, like \u201ctell me the income of someone you know.\u201d Therefore, the forget set must be re-defined in GenAI, and many concepts such as objectives, evaluation methods, and MU techniques must be reviewed carefully. Meanwhile, GenAI includes not only LLMs but also many other types of models such as vision generative models [78, 136, 193] and multimodal large language model (MLLMs) [91, 92]. These motivate us to write this survey, while MU surveys for the conventional AI exist [10, 94, 103, 134, 146, 176].\nThe forget set in GenAI can be defined as $D_f = \\{(x, y_f)|\\forall x\\}$ or simplified as $D_f = \\{y_f\\}$, where $y_f$ can be any undesired model output, including leaked privacy, harmful information, bias and discrimination, copyright data, etc., and x is anything that prompts the model to generate $y_f$. Due to the new definition, it is much less expensive to collect a large forget set which contains many data points (i.e., $y_f$) that the training set D does not contain. So, while MU in conventional AI mainly focused on tuning a model to be the same as trained only on $D \\backslash D_f$, people have at least three expectations on the machine-unlearned GenAI, considering three sets of data points:\n\u2022 The target forget set $D_f = \\{(x, y_f) \\in D\\} \\subset D_f$,\n\u2022 The retain set $D_r = D \\backslash D_f$,\n\u2022 The unseen forget set $D_f' = D_f \\backslash D_f$.\nSuppose the original GenAI model is denoted by $g : \\mathcal{X} \\rightarrow \\mathcal{Y}$, and the unlearned model is $g^*$. The three quantitative objectives of optimizing (and evaluating) $g^*$ are as follows (higher is better):\n\u2022 Accuracy: the unlearned model should not generate the data points in the seen forget set:\n$\\text{Accuracy} = \\sum_{y_f \\in \\tilde{D_f}} \\sum_{x \\in \\mathcal{X}} I(g^*(x) \\neq y_f, \\forall x \\in \\mathcal{X}) / |\\tilde{D_f}| \\in [0, 1],$\nwhere I(stmt) is an indicator function \u2013 it returns 1 if stmt is true and otherwise returns 0. Note that we do not expect the original model to always fail on $D_f$, i.e., $g(x) = y_f$. So, it is possible that both g and $g^*$ do not generate $y_f \\in \\tilde{D_f}$ with any prompt x.\n\u2022 Locality: the unlearned model should maintain its performance on the retain set:\n$\\text{Locality} = \\sum_{(x,y) \\in D_r} I(g^*(x) = g(x)) / |D_r| \\in [0, 1].$\nNote that Locality does not require the unlearned model $g^*$ to generate the expected output but the output from the original model g.\n\u2022 Generalizability: the model should generalize the unlearning to the unseen forget set:\n$\\text{Generalizability} = \\sum_{y_f \\in D_f'} \\sum_{x \\in \\mathcal{X}} I(g^* (x) \\neq y_f, \\forall x \\in \\mathcal{X}) / |D_f'| \\in [0, 1].$\nSignificance. Around these three objectives, many MU techniques have been designed for GenAI and found useful in AI applications. GenAI becomes increasingly data-dependent, concerned parties and practitioners may request the removal of certain data samples and their effects from training datasets and already trained models due to privacy concerns and regulatory requirements,"}, {"title": "1.1 Related Surveys", "content": "Recently, several studies have provided valuable insights into various aspects of machine unlearning techniques in LLMs. These include general surveys on machine unlearning, such as [10, 94, 130, 146, 176], which cover a wide range of topics from basic concepts and diverse techniques to potential challenges in the field. Additionally, some works focus on specific aspects of LLM MU; for example, Zhang et al. [182] explores the implementation challenges of Right To Be Forgotten (RTBF) in LLM MU and provides insights on designing suitable methods, while Lynch et al. [103] highlights the importance of comprehensive unlearning evaluations in LLM MU and offers insights on designing robust metrics. Furthermore, Liu et al. [99] emphasizes security and privacy concerns in LLM MU, focusing on threats, attacks, and defensive methods. Lastly, Ren et al. [134] highlights the distinctions between various types of techniques, including both MU and non-MU methods, in generative models and their implementations for copyright protections.\nTo the best of our knowledge, there remains a significant gap in comprehensive investigations that incorporate the existing literature and ongoing advancements in the field of GenAI. While previous surveys primarily focus on LLMs or specific aspects of GenAI MU, they do not expand the categorization to include a broader range of generative models, such as generative image models and multimodal (large) language models. Moreover, these surveys lack a thorough examination of the technical details of GenAI MU, including categorization, datasets, and benchmarks. Additionally, the specific objectives of GenAI unlearning, crucial for guiding effective practices, have not been thoroughly investigated. Uniquely, our survey addresses this by formulating these objectives, providing a clear and structured framework that delineates the goals and expectations for effective unlearning practices. By defining the objectives as Accuracy, Locality, and Generalizability, we establish a robust and systematic approach to evaluate and advance GenAI unlearning methods.\nGiven the rapid advancement of GenAI MU techniques, it is crucial to conduct a detailed examination of all representative methodologies and align them with the objectives of GenAI MU. This includes summarizing commonalities across different types, highlighting unique aspects of each category and modality, and discussing open challenges and future directions in the domain of GenAI MU. Our survey aims to fill this gap by providing a holistic overview of the field, encompassing a wide spectrum of generative models and offering an in-depth analysis of the state-of-the-art techniques, datasets, and evaluation metrics."}, {"title": "1.2 Main Contributions", "content": "This paper provides an in-depth analysis of various aspects of GenAI MU, including technical details, categorizations, challenges, and prospective directions. We first provide an overview of the categorization of GenAI MU strategies. Specifically, we classify existing strategies into two categories: Parameter Optimization, and In-Context Unlearning. Importantly, these two categories not only offer thorough coverage of all contemporary approaches, but we also break down each category into sub-categories based on the characteristics of different types of unlearning designs. Additionally, we provide a comprehensive analysis of each category, with special emphasis on its effectiveness and potential weaknesses, which can serve as a foundation for future research and development in the field. In concrete, our contributions can be summarized as follows:\n\u2022 Novel Problem Objectives: We formulate the task of GenAI MU as a selective forgetting process where methods from different categories can be viewed as various approaches to removing different types of knowledge. We highlight the biggest difference between GenAI MU and traditional MU lies in the focus on forgetting specific outputs rather than specific input-output mappings. We uniquely highlight that an effective GenAI MU method should be evaluated based on three important objectives: Accuracy, Locality, and Generalizability. This structured framework provides clear and measurable goals for unlearning practices in generative models.\n\u2022 Broader Categorization: We cover the full spectrum of existing unlearning techniques for generative models, including generative image models, (Large) Language Models (LLMs), and Multimodal (Large) Language Models (MLLMs). Specifically, our categorization is based on how the target knowledge is removed from the pre-trained generative models, incorporating two distinct categories: Parameter Optimization, and In-Context Unlearning. The advantages and disadvantages of each method are comprehensively discussed in the survey.\n\u2022 Future Directions: We investigate the practicality of contemporary GenAI MU techniques across various downstream applications. Additionally, we thoroughly discuss the challenges present in the existing literature and highlight prospective future directions within the field.\nThe remainder of the survey is structured as follows: Section 2 introduces a comprehensive summary of evaluation metrics for GenAI MU strategies, aligning with newly formulated objectives. Section 3 introduces the background knowledge of machine unlearning and generative models. Section 4 provides a comprehensive categorization of existing unlearning strategies for different types of generative models, with a thorough emphasis on their relationships and distinctions. Section 5 presents the commonly used datasets and prevalently used evaluation benchmarks for GenAI MU in various downstream applications. Section 6 offers a comprehensive overview of realistic applications of unlearning techniques. Section 7 discusses potential challenges in GenAI MU and offers several opportunities that may inspire future research. Finally, Section 8 concludes the survey."}, {"title": "2 EVALUATION METRICS", "content": "Before delving into the taxonomy of GenAI MU techniques in details, in this section, we first introduce various evaluation metrics commonly used to evaluate the effectiveness of different GenAI MU strategies from varied perspectives. We align the metrics with our pre-defined objectives for a better demonstration. An overall assessment of a harmful unlearning application on LLM is displayed in Figure 2.\n2.1 Accuracy\nAccuracy is a fundamental evaluation metric for measuring the effectiveness of unlearning techniques in Gen AI models. It assesses the extent to which the model successfully forgets specific unwanted target knowledge. This metric captures the expectation that the unlearned model $g^*$ does not produce outputs that it should have forgotten regardless of the input x. By maximizing $Acc(g^*; \\tilde{D_f})$, we ensure that the unlearned model does not generate any outputs associated with the target forget dataset, demonstrating high accuracy. Accuracy can be easily defined to evaluate the performance of GenAI MU techniques under various scenarios. For example, in safety alignment applications [52, 96, 178], accuracy can be defined to assess the harmfulness or appropriateness of the output y using models such as GPT-4, moderation models [66], and NudeNet detector [6]. In the context of hallucination reduction [57, 174, 178], accuracy is typically evaluated by comparing the output with the ground truth on factual datasets or benchmarks [80, 87, 135, 180]. For bias alleviation [68, 179], accuracy can be measured using Equalized Odds (EO) or StereoSet metrics, which are calculated by comparing the probability assigned to contrasting portions of each sentence, conditioned on the shared portion of the sentence. Finally, for copyright protection and privacy compliance [15, 33, 161, 172, 178], accuracy is evaluated by the memorization level, the accuracy on selected forget set, or the success rate of membership inference attacks, which detect whether a target data was used to train the model.\n2.2 Locality\nLocality is the second objective of GenAI MU, measuring the unlearned model's capability to preserve knowledge unrelated to the unlearned content. In traditional unlearning, locality is typically measured by retained accuracy, indicating the model's performance on the retaining dataset $D_r$, which is separated from the known dataset D. However, defining a specific $D_r$ in GenAI MU can be challenging due to the large volume and diversity of the pre-trained dataset. Therefore, in the context of GenAI models, $D_r$ denotes any arbitrary set of data that is not part of the forget set $D_f$, on which the model's performance needs to be preserved. Locality ensures that the unlearned model retains its ability to produce the same outputs as the original model g for the retain dataset, thereby demonstrating high locality. Depending on the modality, unlearned model $g^*$ may need to preserve different types of knowledge. For instance, Receler [58] highlights the importance of locality in text-to-image models by evaluating whether erasing one concept affects the synthesis of unrelated concepts. Large Language Models (LLMs), on the other hand, need to maintain performance across various tasks beyond the targeted unlearned skill. Studies such as [64, 65, 162, 189] measure performance on benchmarks like MMLU [51], MathQA [3], and GLUE [160] after unlearning to evaluate model locality on unrelated datasets.\n2.3 Generalizability\nGeneralizability is a crucial metric for evaluating the effectiveness of GenAI unlearning approaches, aligning with the third objective of GenAI MU. It measures the capability of the unlearned model $g^*$ to handle similar unseen forgetting datasets $D_f'$ encountered after unlearning, ensuring that the unlearned knowledge extends to other similar inputs not present in the target forgetting dataset $D_f$. This metric ensures that the unlearned model does not produce outputs related to the forgotten"}, {"title": "3 BACKGROUND", "content": "In this section, we first provide the basics knowledge of generative models as background knowledge and an overview of machine unlearning of non-generative models to further facilitate the understanding of technical details in GenAI MU.\n3.1 Generative Models\n3.1.1 Generative Image Models. Deep learning has extensively explored various generative image models, including Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and Text-to-Image Diffusion Models. In particular, autoencoders [71, 158] comprise an encoder that transforms an image into a latent vector and a decoder that generates new images from these vectors. GANs [46, 111, 168] use a min-max game where the generator creates images and the discriminator distinguishes real from generated ones, refining both through adversarial training. Diffusion Models [53, 136] generate images via a forward process that adds noise to an image and a reverse process that denoises it to reconstruct the original. Text-to-Image Diffusion Models [78, 193] like Stable Diffusion [136], MidJourney, and DALL-E2, generate images from textual descriptions using Latent Diffusion Models (LDMs) combined with CLIP [131]. Advanced techniques like Textual Inversion [36] and DreamBooth [139] have recently enhanced these models for customized image editing.\n3.1.2 (Large) Language Models. Language models often refer to probabilistic models that generate the likelihood of word sequences to predict future tokens [81]. On the other hand, Large Language Model (LLMs) are deep neural language models with billions of parameters, pre-trained on extensive text corpora to understand natural language distribution and structure [190]. Almost all LLMs use the transformer architecture for its efficiency [157]. In particular, there are three types of language model designs: encoder-only, decoder-only, and encoder-decoder. Encoder-only models like BERT [26] use bidirectional attention and are pre-trained on tasks like masked token prediction, enabling them to adapt quickly to various downstream tasks. Decoder-only models like GPT [132] are trained on next-token prediction tasks using the transformer decoder architecture. Encoder-decoder models like T5 [133] are trained on text-to-text tasks, with encoders extracting contextual representations and decoders mapping these representations back to text. In the context of LLMs, most models contain the decoder-only architecture for simplicity and efficient inference.\n3.1.3 Multimodal (Large) Language Models. Multimodal (Large) Language Models (MLLMs) refers to the models that enable LLMs to perceive and integrates information from various data modalities. One key branch of MLLMs is vision-language pre-training, which aims to enhance performance on vision and language tasks by learning multimodal foundation models. Vision Transformer (ViT) [28] introduced an end-to-end solution by applying the Transformer encoder to images, while CLIP [131] used multimodal pre-training to convert classification into a retrieval task, enabling zero-shot recognition. Recent advancements in LLMs like LLaMA [155], and GPT [1] have benefited from scaled-up training data and increased parameters, leading to substantial improvements in language understanding, generation, and knowledge reasoning. These developments have popularized the use of auto-regressive language models as decoders in vision-language tasks, facilitating knowledge sharing between language and multimodal domains, thereby promoting the development of advanced MLLMs like GPT-4v and LLaVA [91].\n3.2 Machine Unlearning for Non-Generative Models\nThe concept of Machine Unlearning (MU) was first raised by [13] in response to privacy regulations like the General Data Protection Regulation of the European Union [54] and the California Consumer Privacy Act [124] have established the right to be forgotten [12, 24, 42], which mandates the elimination of specific user data from models upon removal requests. In particular, previous works have considered MU for non-generative models in two criteria: Exact Unlearning and Approximate Unlearning.\n3.2.1 Exact Unlearning. The most straightforward approach to \"unlearn\" a data point is to remove it from the training set and then retrain the model from scratch, which can be both expensive and inefficient. Exact Unlearning aims to eliminate all information related to the selected data so that the unlearned model performs identically to a retrained model. [12] first introduced the SISA framework, which partitions data into shards and slices, with each shard serving as a weak learner that can be quickly retrained upon an unlearning request. [42] presented unlearning approaches for k-means clustering. However, exact unlearning does not allow algorithms to trade privacy for utility and efficiency due to its stringent privacy criteria.\n3.2.2 Approximate Unlearning. On the other hand, approximate unlearning [19, 20, 45, 116] only requires the unlearned model to perform similarly to the retrained model, allowing for a better balance between utility and efficiency. For example, ConMU [97] introduces a controllable unlearning pipeline that aims to address the trilemma within the realm of MU, focusing on balancing privacy, utility, and runtime efficiency. Additionally, Koh and Liang [73] approximates model perturbation towards empirical risk minimization on the retaining datasets using techniques like the inverse of the Hessian matrix. Golatkar et al. [44] employs Fisher-based unlearning techniques to estimate the impact of removing specific information and propose methods to effectively remove this information from intermediate layers of DNNs. These strategies highlight the potential of approximate unlearning to achieve efficient and effective unlearning without the need for complete retraining.\n3.3 Relationship with Model Editing\nAnother closely related concept to GenAI unlearning is model editing, which focuses on the local or global modification of specific pre-trained model behaviors by replacing outdated, incomplete, or inappropriate information with new knowledge. There are several similarities between these two domains. Firstly, they overlap when the objective of model editing is to erase certain information, as editing techniques can be considered a form of or an aid to the unlearning process in in-context"}, {"title": "4 METHODOLOGY CATEGORIZATION", "content": "In this section", "D_r$": "Parameter Optimization", "Overview": "The unlearning approach through parameter optimization aims to adjust specific model parameters to selectively unlearn certain behaviors without affecting other functions. This method aims to perform minimal alterations to parameters linked with target forget data $D_f$ influences or biases to preserve essential model performance. However", "loss": "The gradient ascent (GA) based approach is originate from a typical optimization-based technique raised by [153]. Given a target forget set $D_f$ and an arbitrary loss function $L(\\theta)$", "t": "n$\\theta_{t+1"}, "leftarrow \\theta_t + \\lambda \\nabla_{\\theta_t} L(\\theta)$\nwhere $\\lambda$ is the unlearning rate and $\\theta_t$ denotes model parameters at step t. The objective of such approach reverts the change of the gradient descent without reverse loss term during the training with its opposite operation. KUL [65] implements the gradient ascent (GA) method to intentionally maximize the loss function, thereby shifting the model's predictions for specific target samples in the opposite direction. Notably, during the unlearning process, KUL updates only a small subset of $\\theta$ to unlearn the target unlearn sample $D_f$. However, KUL may be impractical in scenarios where sensitive information is not explicitly defined. Building on this concept, UnTrac [64] employs the GA approach but extends its application to estimate the influence of a training dataset on a test dataset through unlearning. Nevertheless, though GA based approach can obtain an excellent unlearning performance, this usually comes with a large sacrifice of the model utility with non-target samples (i.e. catastrophic collapse), as it elaborated in [96, 178]. To address this utility degradation, NPO[185] leverages the principles of preference optimization but uniquely applies it using only negative samples. By modifying the standard preference optimization to exclude positive examples, NPO aims to make the model forget target undesirable data $D_f$ by decreasing their prediction probability. Additionally, the work theoretically shows that the progression toward catastrophic collapse can be exponentially slower when using NPO loss than pure GA. Specifically, NPO approach utilizes the following loss function:\n$L_{NPO,\\beta}(\\theta) = - \\frac{2}{\\beta |D_f|} \\mathbb{E}_{x \\in D_f} \\left[ \\log \\sigma\\left( \\frac{q^*_{\\theta}(y|x)}{q_{\\theta}(y|x)}\\right) \\right]$\nIn this formulation, $\\beta$ is a scaling parameter that adjusts the sharpness of the preference modifications. The term $\\log \\frac{q^*_{\\theta}(y|x)}{q_{\\theta}(y|x)}$ measures how the predictions of the current unlearned model deviate from those of the pre-trained model. This loss function aims to minimize the likelihood of the model predicting outcomes linked to the $D_f$, effectively diminishing the model's recall of this data.\n    },\n    {\n      \"title\": \"4.1.3 Knowledge Distillation\",\n      \"content\":", "Knowledge distillation approaches typically involve a teacher-student model configuration and treat the unlearned model $g^*$ as the student model, aiming to mimic the desirable behavior from a teacher model. KGA [161] introduces a novel framework to align knowledge gaps, defined as differences in prediction distributions between models trained on different data subsets. The KGA framework adjusts the model to minimize the discrepancy in knowledge between a base model trained on all data and new models trained on an external set $D_n$ similar to D but excluding $D_f$. This is formulated as:\n$f^* = \\underset{f}{argmin} |dis(\\mathcal{D}_n)(g, g_n) - dis(\\mathcal{D}_{f})(g, g_f)|$.\nHere, g is the original model trained on D, while $g_n$ and $g_f$ are models trained on $D_n$ and $D_f$, respectively. The function dis measures the distributional differences using KL divergence. KGA treats the original model g as a teacher model and minimizes the distance of output distributions when feeding samples in D to $g^*$ and g. Moreover, [27] introduces a novel LLM unlearning approach named deliberate imagination (DI) to further maintain model's generation and reasoning capabilities. DI employs a self-distillation technique in which a teacher model guides an LLM to generate creative and non-harmful responses, rather than merely forgetting unwanted memorized information. The process begins by strategically increasing the probability tokens within the teacher model that serve as alternatives to the memorized ones, thereby encouraging the production of novel and diverse outputs. Subsequently, DI fine-tunes the student model using the predicted output probabilities from the teacher, enabling the student models to generate imaginative responses that are less dependent on memorized data.\nSimilar to many white-box approaches, the KGA approach depends on accessing the model's internal weights, which makes it inapplicable to black-box models. To address this issue, $\\delta$ learning [60] introduces a novel framework for unlearning in black-box LLMs without accessing to the model's internal weights. Specifically, $\\delta$ learning utilizes a pair of smaller, trainable models to compute a logit offset that is then applied to the black-box LLM to adjust its responses. These smaller trainable models, referred to as offset models, are trained to predict how the logit outputs of the black-box LLM should be modified to exclude the influence of $D_f$. The offset is calculated as the difference in logits between these two models and added to the black-box LLM's logits, guiding the final output away from sensitive information."], "content": "Data sharding approaches usually divide the training data into multiple shards, where each corresponds to a subset of the overall data D. Inspired by SISA [12], separate models are trained for each data shards that can effectively remove target data in different shard based on request. As it mentioned in Section 3, SISA provides an exact unlearning guarantee due to the data to be forgotten have no impacts on the retrained version of the model, making it suitable to wide range of model architectures, including GenAI. However, direct implementation of SISA on GenAI is infeasible due to its the high computational cost in model saving, retraining, and inference. Hence, various work have adapted the similar approach of SISA and made more suitable to GenAI depends on its corresponding unlearning scenario. FairSISA [68] proposes a post-processing unlearning strategy for bias mitigation in ensemble models produced by SISA. In particular, this post-processing function f aims to enforce fairness in the outputs of $g^*$, particularly ensuring that the model's predictions do not exhibit bias related to a sensitive attribute A. This can be formulated as follows: f post-processes the outputs of $g^*$ to satisfy fairness constraints, such as equalized odds, across groups defined by A without significantly affecting the overall model accuracy:\n$\\min_{f} \\mathbb{E}[l(f(g^*(x)), Y)]$\nsubject to:\n$Pr(f(g^*(x)) = 1 | A = 0, Y = y) = Pr(f(g^*(x)) = 1 | A = 1, Y = y), \\forall y \\in \\{0,1\\}.$\nHere, l is a loss function, and E denotes the expected value, reflecting the objective to minimize any deviation from the true labels Y while adhering to the fairness constraints.\nNext, to reduce retraining costs while preserving utility during inference, Liu and Kalinli [98] introduces the LOO ensemble method to unlearn target token sequences from LMs. This method uses a teacher-student framework where multiple teacher models are trained on partitioned data segments. When data removal is requested, the student model (i.e. base LM) is supervised by the remaining teachers, excluding the one trained on the segment to be unlearned. The recalibration is formalized as:\n$g^{LOO-E}_{T(\\omega| \\omega^{-k})} = \\frac{1}{M-1} \\sum_{i=1}^{M} 1\\{D_k \\notin D_m\\}  \\cdot  p_{\\theta_{m}}(\\omega | \\omega^{t-1})$\nwhere $g^{LOO-E}_{T(\\omega| \\omega^{-k})}$ is the aggregated prediction excluding the teacher trained with $D_k \\subset D_f$. $\\theta_{m}$ denotes the parameters of the m-th teacher model, and $\\omega_{t-1}$ reflects the input sequence up to step t-1. The student model fine-tunes its parameters based on KL divergence, ensuring sensitive data is unlearned while maintaining performance on $D_r$. Despite improved utility performance compared to SISA, LOO lacks a theoretical guarantee for teacher models and may impose significant computational overheads on GenAI like LLMs.\nThe data sharding approach can be applied to vision generative models and LLMs for measuring training data influence for unlearning. Dai and Gifford [23] presents temporary unlearning using ensemble ablation. The core idea is to use an ensemble of diffusion models, each trained on a subset of data, to assess the influence of individual data points by excluding models exposed to the data point and observing changes in outputs. The work provides a theoretical guarantee that the resulting model has not seen the removed training sample. The ensemble is represented as $f_e$:\n$f_e(x,t) = \\mathbb{E}_{S \\sim \\mathcal{X}} [ \\mathbb{E}_{r \\sim R} [f(x, t, A(S,r))]]$\nwhere x is the input, t is the diffusion time, $\\mathcal{X}$ represents the uniform distribution over $2^D$, and A(\u00b7) denotes the training procedure with training samples and exogenous noise r. To assess the influence of a specific point x, the ensemble without x, $f_e^{-x}$, is evaluated as:\n$f^{-x}_e(x,t) =  \\frac{1}{\\mathbb{P}r(x \\in S | S \\sim \\mathcal{X})}  \\mathbb{E}_{S \\sim \\mathcal{X}} [ \\mathbb{E}_{r \\sim R} [f(x, t, A(S,r)) 1_{x \\notin S}]]$.\nThis method aligns with machine unlearning by providing a more efficient way to disregard specific data influences without extensive retraining. Using an ensemble approach ensures effective data attribution and compliance with privacy and fairness."}, {"title": "4.1.5 Extra Learnable Layers", "content": "Another parameter optimization approach is to introduce additional parameters or trainable layers in the model and train them to actively forget different sets of data from $D_f$. This approach negates the necessity of modifying the model's inherent parameters", "objective": "n$\\min_{\\theta_m"}, {"as": "n$\\theta_m = \\left( \\sum_i X_i^T X_i \\right)^{-1"}, [58], {}]