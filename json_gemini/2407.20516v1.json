{"title": "Machine Unlearning in Generative AI: A Survey", "authors": ["Zheyuan Liu", "Guangyao Dou", "Zhaoxuan Tan", "Yijun Tian", "Meng Jiang"], "abstract": "Generative Al technologies have been deployed in many places, such as (multimodal) large language models\nand vision generative models. Their remarkable performance should be attributed to massive training data\nand emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or\ndangerous information originated from the training data especially those from web crawl. New machine\nunlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects\nfrom the models, because those that were designed for traditional classification tasks could not be applied for\nGenerative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new\nproblem formulation, evaluation methods, and a structured discussion on the advantages and limitations of\ndifferent kinds of MU techniques. It also presents several critical challenges and promising directions in MU\nresearch.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning (ML) grew out of the quest of artificial intelligence (AI) for building a model that\nlearns patterns from a dataset \\(D = {\\{(x, y)}\\}\\). The model uses the patterns to predict the output y for\nan unseen input x, known as a classifier or regressor, depending on whether y is a categorical or\nnumeric variable. An interesting question is, after the model is built, what if the developers find\nthat the dataset contains a set of data points \\(D_f = {\\{(x_f, y_f)}\\}\\) c D that should NOT be learned? For\nexample, we might not want the model to leak a customer's income though it might have seen lots\nof bank data. Machine unlearning (MU) methods aim to help the model \u201cforget\u201d the data points\nin \\(D_f\\) (which is named forget set) so that ideally the model would be the same or similar as a new\nmodel trained on \\(D \\\\ D_f\\), and MU would save a lot of development cost than re-training the model.\nMU has been studied and reviewed in conventional AI. Recently, generative AI (GenAI), based on\ngenerative ML models, has exhibited its capabilities of data generation which may be used as and not"}, {"title": "1.1 Related Surveys", "content": "Recently, several studies have provided valuable insights into various aspects of machine unlearning\ntechniques in LLMs. These include general surveys on machine unlearning, such as [10, 94, 130, 146,\n176], which cover a wide range of topics from basic concepts and diverse techniques to potential\nchallenges in the field. Additionally, some works focus on specific aspects of LLM MU; for example,\nZhang et al. [182] explores the implementation challenges of Right To Be Forgotten (RTBF) in LLM\nMU and provides insights on designing suitable methods, while Lynch et al. [103] highlights the"}, {"title": "1.2 Main Contributions", "content": "This paper provides an in-depth analysis of various aspects of GenAI MU, including technical\ndetails, categorizations, challenges, and prospective directions. We first provide an overview of\nthe categorization of GenAI MU strategies. Specifically, we classify existing strategies into two\ncategories: Parameter Optimization, and In-Context Unlearning. Importantly, these two\ncategories not only offer thorough coverage of all contemporary approaches, but we also break\ndown each category into sub-categories based on the characteristics of different types of unlearning\ndesigns. Additionally, we provide a comprehensive analysis of each category, with special emphasis"}, {"title": "2 EVALUATION METRICS", "content": "Before delving into the taxonomy of GenAI MU techniques in details, in this section, we first\nintroduce various evaluation metrics commonly used to evaluate the effectiveness of different\nGenAI MU strategies from varied perspectives. We align the metrics with our pre-defined objectives\nfor a better demonstration. An overall assessment of a harmful unlearning application on LLM is\ndisplayed in Figure 2."}, {"title": "2.1 Accuracy", "content": "Accuracy is a fundamental evaluation metric for measuring the effectiveness of unlearning tech-\nniques in Gen AI models. It assesses the extent to which the model successfully forgets specific\nunwanted target knowledge. This metric captures the expectation that the unlearned model g*\ndoes not produce outputs that it should have forgotten regardless of the input x. By maximizing\nAcc(g*; \\(D_f\\)), we ensure that the unlearned model does not generate any outputs associated with the\ntarget forget dataset, demonstrating high accuracy. Accuracy can be easily defined to evaluate the\nperformance of GenAI MU techniques under various scenarios. For example, in safety alignment\napplications [52, 96, 178], accuracy can be defined to assess the harmfulness or appropriateness of\nthe output y using models such as GPT-4, moderation models [66], and NudeNet detector [6]. In\nthe context of hallucination reduction [57, 174, 178], accuracy is typically evaluated by comparing"}, {"title": "2.2 Locality", "content": "Locality is the second objective of GenAI\nMU, measuring the unlearned model's ca-\npability to preserve knowledge unrelated\nto the unlearned content. In traditional un-\nlearning, locality is typically measured by\nretained accuracy, indicating the model's\nperformance on the retaining dataset \\(D_r\\),\nwhich is separated from the known dataset\nD. However, defining a specific \\(D_r\\) in\nGenAI MU can be challenging due to the\nlarge volume and diversity of the pre-\ntrained dataset. Therefore, in the context\nof GenAI models, \\(D_r\\) denotes any arbi-"}, {"title": "2.3 Generalizability", "content": "Generalizability is a crucial metric for evaluating the effectiveness of GenAI unlearning approaches,\naligning with the third objective of GenAI MU. It measures the capability of the unlearned model\ng* to handle similar unseen forgetting datasets \\(D_f\\) encountered after unlearning, ensuring that the\nunlearned knowledge extends to other similar inputs not present in the target forgetting dataset\n\\(D_f\\). This metric ensures that the unlearned model does not produce outputs related to the forgotten"}, {"title": "3 BACKGROUND", "content": "In this section, we first provide the basics knowledge of generative models as background knowl-\nedge and an overview of machine unlearning of non-generative models to further facilitate the\nunderstanding of technical details in GenAI MU."}, {"title": "3.1 Generative Models", "content": "3.1.1 Generative Image Models. Deep learning has extensively explored various generative image\nmodels, including Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and\nText-to-Image Diffusion Models. In particular, autoencoders [71, 158] comprise an encoder that\ntransforms an image into a latent vector and a decoder that generates new images from these vectors.\nGANs [46, 111, 168] use a min-max game where the generator creates images and the discriminator\ndistinguishes real from generated ones, refining both through adversarial training. Diffusion\nModels [53, 136] generate images via a forward process that adds noise to an image and a reverse\nprocess that denoises it to reconstruct the original. Text-to-Image Diffusion Models [78, 193]\nlike Stable Diffusion [136], MidJourney, and DALL-E2, generate images from textual descriptions\nusing Latent Diffusion Models (LDMs) combined with CLIP [131]. Advanced techniques like Textual\nInversion [36] and DreamBooth [139] have recently enhanced these models for customized image\nediting.\n3.  1.2 (Large) Language Models. Language models often refer to probabilistic models that generate\nthe likelihood of word sequences to predict future tokens [81]. On the other hand, Large Language\nModel (LLMs) are deep neural language models with billions of parameters, pre-trained on extensive\ntext corpora to understand natural language distribution and structure [190]. Almost all LLMs use\nthe transformer architecture for its efficiency [157]. In particular, there are three types of language\nmodel designs: encoder-only, decoder-only, and encoder-decoder. Encoder-only models like BERT\n[26] use bidirectional attention and are pre-trained on tasks like masked token prediction, enabling\nthem to adapt quickly to various downstream tasks. Decoder-only models like GPT [132] are trained\non next-token prediction tasks using the transformer decoder architecture. Encoder-decoder models\nlike T5 [133] are trained on text-to-text tasks, with encoders extracting contextual representations\nand decoders mapping these representations back to text. In the context of LLMs, most models\ncontain the decoder-only architecture for simplicity and efficient inference.\n4.  1.3 Multimodal (Large) Language Models. Multimodal (Large) Language Models (MLLMs) refers\nto the models that enable LLMs to perceive and integrates information from various data modalities.\nOne key branch of MLLMs is vision-language pre-training, which aims to enhance performance on\nvision and language tasks by learning multimodal foundation models. Vision Transformer (ViT)\n[28] introduced an end-to-end solution by applying the Transformer encoder to images, while CLIP\n[131] used multimodal pre-training to convert classification into a retrieval task, enabling zero-shot\nrecognition. Recent advancements in LLMs like LLaMA [155], and GPT [1] have benefited from\nscaled-up training data and increased parameters, leading to substantial improvements in language\nunderstanding, generation, and knowledge reasoning. These developments have popularized the"}, {"title": "3.2 Machine Unlearning for Non-Generative Models", "content": "The concept of Machine Un-\nlearning (MU) was first raised by\n[13] in response to privacy reg-\nulations like the General Data\nProtection Regulation of the Eu-\nropean Union [54] and the Cal-\nifornia Consumer Privacy Act\n[124] have established the right\nto be forgotten [12, 24, 42], which\nmandates the elimination of spe-\ncific user data from models upon\nremoval requests. In particular,\nprevious works have considered\nMU for non-generative models\nin two criteria: Exact Unlearning\nand Approximate Unlearning."}, {"title": "3.2.1 Exact Unlearning", "content": "The most\nstraightforward approach to \"unlearn\" a data point is to remove it from the training set and then\nretrain the model from scratch, which can be both expensive and inefficient. Exact Unlearning\naims to eliminate all information related to the selected data so that the unlearned model performs\nidentically to a retrained model. [12] first introduced the SISA framework, which partitions data\ninto shards and slices, with each shard serving as a weak learner that can be quickly retrained upon\nan unlearning request. [42] presented unlearning approaches for k-means clustering. However,\nexact unlearning does not allow algorithms to trade privacy for utility and efficiency due to its\nstringent privacy criteria."}, {"title": "3.2.2 Approximate Unlearning", "content": "On the other hand, approximate unlearning [19, 20, 45, 116] only\nrequires the unlearned model to perform similarly to the retrained model, allowing for a better bal-\nance between utility and efficiency. For example, ConMU [97] introduces a controllable unlearning\npipeline that aims to address the trilemma within the realm of MU, focusing on balancing privacy,\nutility, and runtime efficiency. Additionally, Koh and Liang [73] approximates model perturbation\ntowards empirical risk minimization on the retaining datasets using techniques like the inverse of\nthe Hessian matrix. Golatkar et al. [44] employs Fisher-based unlearning techniques to estimate\nthe impact of removing specific information and propose methods to effectively remove this infor-\nmation from intermediate layers of DNNs. These strategies highlight the potential of approximate\nunlearning to achieve efficient and effective unlearning without the need for complete retraining."}, {"title": "3.3 Relationship with Model Editing", "content": "Another closely related concept to GenAI unlearning is model editing, which focuses on the local\nor global modification of specific pre-trained model behaviors by replacing outdated, incomplete, or\ninappropriate information with new knowledge. There are several similarities between these two\ndomains. Firstly, they overlap when the objective of model editing is to erase certain information,\nas editing techniques can be considered a form of or an aid to the unlearning process in in-context"}, {"title": "3.4 Relationship with RLHF", "content": "One popular technique for aligning model behaviors with human values is RLHF [9, 66, 149, 180].\nSome existing GenAI unlearning works [96, 178] have demonstrated that datasets created from RLHF\ncan be beneficial in the unlearning or evaluation process. However, RLHF has several drawbacks:\nfirstly, collecting human inputs and outputs is expensive, as it typically requires a large number of\nprofessional annotators. Secondly, RLHF approaches are computationally intensive, demanding\nextensive GPU resources for further training the model, often for hours or even days. In contrast,\nGenAI unlearning offers an effective alternative to RLHF. It requires only the collections of target\nnegative examples \\(\\tilde{D_f}\\) and unseen samples \\(O_f\\) that practitioners intend the model to unlearn,\nwhich are easier and cheaper to collect through user reporting or red teaming compared to the\npositive samples needed for RLHF. Additionally, it is computationally efficient, as unlearning can\nusually be accomplished with the same resources required for fine-tuning."}, {"title": "4 METHODOLOGY CATEGORIZATION", "content": "In this section, we summarize the current approaches for generative model unlearning. In particular,\nwe first classify all the approaches into two categories based on their actions to \\(D_f\\) and \\(D_r\\):\nParameter Optimization, and In-Context Unlearning, as it displayed in Table 3. Then, we"}, {"title": "4.1 Parameter Optimization", "content": "4.1.1 Overview: The unlearning approach through parameter optimization aims to adjust spe-\ncific model parameters to selectively unlearn certain behaviors without affecting other functions.\nThis method aims to perform minimal alterations to parameters linked with target forget data \\(D_f\\)\ninfluences or biases to preserve essential model performance. However, such direct modifications\nto parameters can unintentionally impact unrelated parameters, potentially reducing model utility.\nParameter optimization for unlearning can be implemented through various methods, includ-\ning gradient-based adjustments, knowledge distillation, data sharding, integrating extra\nlearnable layers, task vector, and parameter efficient module operation."}, {"title": "4.1.2 Gradient Based", "content": "Gradient based unlearning approaches aim to adjust the model parameters\nin a way that selectively forgets the knowledge associated with specific data points or patterns.\nThis is achieved by optimizing the model's loss function in reverse (for gradient ascent) or forward\n(for gradient descent) directions to effectively remove or mitigate the learned associations without\nsignificantly impacting the model's overall performance.\nGradient-based with reverse loss: The gradient ascent (GA) based approach is originate from a\ntypical optimization-based technique raised by [153]. Given a target forget set \\(D_f\\) and an arbitrary\nloss function L(\\(\\theta\\)), the GA algorithm iteratively update the model at each training step t:\n\\(\\theta_{\\tau+1} \\leftarrow \\theta_{\\tau} + \\lambda \\nabla_{\\theta_{\\tau}} L(\\theta)\\)\nwhere \\(\\lambda\\) is the unlearning rate and \\(\\theta_\\tau\\) denotes model parameters at step t. The objective of such\napproach reverts the change of the gradient descent without reverse loss term during the training\nwith its opposite operation. KUL [65] implements the gradient ascent (GA) method to intentionally\nmaximize the loss function, thereby shifting the model's predictions for specific target samples in\nthe opposite direction. Notably, during the unlearning process, KUL updates only a small subset of\n\\(\\theta\\) to unlearn the target unlearn sample \\(D_f\\). However, KUL may be impractical in scenarios where\nsensitive information is not explicitly defined. Building on this concept, UnTrac [64] employs\nthe GA approach but extends its application to estimate the influence of a training dataset on a\ntest dataset through unlearning. Nevertheless, though GA based approach can obtain an excellent\nunlearning performance, this usually comes with a large sacrifice of the model utility with non-\ntarget samples (i.e. catastrophic collapse), as it elaborated in [96, 178]. To address this utility\ndegradation, NPO[185] leverages the principles of preference optimization but uniquely applies\nit using only negative samples. By modifying the standard preference optimization to exclude\npositive examples, NPO aims to make the model forget target undesirable data \\(D_f\\) by decreasing\ntheir prediction probability. Additionally, the work theoretically shows that the progression toward\ncatastrophic collapse can be exponentially slower when using NPO loss than pure GA. Specifically,\nNPO approach utilizes the following loss function:\n\\(L_{NPO,\\beta}(\\theta) = -\\frac{2}{\\beta |D_f|} E_{x \\epsilon \\tilde{D_f}} log \\sigma(-\\beta log\\frac{q^*(\\y|x)}{q(y|x)})\\)\nIn this formulation, \\(\\beta\\) is a scaling parameter that adjusts the sharpness of the preference modifica-\ntions. The term \\(log \\frac{q^*(\\y|x)}{q(y|x)}\\) measures how the predictions of the current unlearned model deviate\nfrom those of the pre-trained model. This loss function aims to minimize the likelihood of the\nmodel predicting outcomes linked to the \\(\\tilde{D_f}\\), effectively diminishing the model's recall of this data."}, {"title": "4.1.3 Knowledge Distillation", "content": "Knowledge distillation approaches typically involve a teacher-\nstudent model configuration and treat the unlearned model g* as the student model, aiming to\nmimic the desirable behavior from a teacher model. KGA [161] introduces a novel framework to\nalign knowledge gaps, defined as differences in prediction distributions between models trained\non different data subsets. The KGA framework adjusts the model to minimize the discrepancy in\nknowledge between a base model trained on all data and new models trained on an external set\n\\(D_n\\) similar to D but excluding \\(D_f\\). This is formulated as:\n\\(f^* = argmin_f |dis_{\\Omega(\\tilde{D_n})}(g,g_n) - dis_{\\Omega(\\tilde{D_f})}(g,g_f)|\\).\nHere, g is the original model trained on D, while \\(g_n\\) and \\(g_f\\) are models trained on \\(D_n\\) and \\(D_f\\),\nrespectively. The function dis measures the distributional differences using KL divergence. KGA\ntreats the original model g as a teacher model and minimizes the distance of output distributions\nwhen feeding samples in D, to g* and g. Moreover, [27] introduces a novel LLM unlearning ap-\nproach named deliberate imagination (DI) to further maintain model's generation and reasoning\ncapabilities. DI employs a self-distillation technique in which a teacher model guides an LLM to\ngenerate creative and non-harmful responses, rather than merely forgetting unwanted memorized\ninformation. The process begins by strategically increasing the probability tokens within the teacher\nmodel that serve as alternatives to the memorized ones, thereby encouraging the production of\nnovel and diverse outputs. Subsequently, DI fine-tunes the student model using the predicted output\nprobabilities from the teacher, enabling the student models to generate imaginative responses that\nare less dependent on memorized data.\nSimilar to many white-box approaches, the KGA approach depends on accessing the model's\ninternal weights, which makes it inapplicable to black-box models. To address this issue, \\(d\\)- learn-\ning [60] introduces a novel framework for unlearning in black-box LLMs without accessing to\nthe model's internal weights. Specifically, \\(d\\) learning utilizes a pair of smaller, trainable models\nto compute a logit offset that is then applied to the black-box LLM to adjust its responses. These\nsmaller trainable models, referred to as offset models, are trained to predict how the logit outputs\nof the black-box LLM should be modified to exclude the influence of \\(D_f\\). The offset is calculated as\nthe difference in logits between these two models and added to the black-box LLM's logits, guiding\nthe final output away from sensitive information."}, {"title": "4.1.4 Data Sharding", "content": "Data sharding approaches usually divide the training data into multiple\nshards, where each corresponds to a subset of the overall data D. Inspired by SISA [12], separate\nmodels are trained for each data shards that can effectively remove target data in different shard\nbased on request. As it mentioned in Section 3, SISA provides an exact unlearning guarantee\ndue to the data to be forgotten have no impacts on the retrained version of the model, making it\nsuitable to wide range of model architectures, including GenAI. However, direct implementation\nof SISA on GenAI is infeasible due to its the high computational cost in model saving, retraining,\nand inference. Hence, various work have adapted the similar approach of SISA and made more\nsuitable to GenAI depends on its corresponding unlearning scenario. FairSISA [68] proposes a\npost-processing unlearning strategy for bias mitigation in ensemble models produced by SISA. In\nparticular, this post-processing function f aims to enforce fairness in the outputs of g*, particularly\nensuring that the model's predictions do not exhibit bias related to a sensitive attribute A. This can\nbe formulated as follows: f post-processes the outputs of g* to satisfy fairness constraints, such\nas equalized odds, across groups defined by A without significantly affecting the overall model"}, {"title": "4.1.5 Extra Learnable Layers", "content": "Another parameter optimization approach is to introduce addi-\ntional parameters or trainable layers in the model and train them to actively forget different sets of\ndata from \\(D_f\\). This approach negates the necessity of modifying the model's inherent parameters,\nthereby preventing interference to its original knowledge. EUL [14] integrates an additional un-\nlearning layer, \\(U^{(i)}\\), into the transformer following the feed-forward networks. Throughout the\ntraining, this unlearning layer is exclusively engaged to learn to forget the specified data, while\nthe rest of the model parameters, denoted as \\(\\theta\\), remain frozen. Upon receiving a deletion request,\nthe model first trains a distinct unlearning layer \\(U^{(i)}\\) with parameters \\(\\theta_i\\) tailored to that request."}, {"title": "4.1.6 Task Vector Methods", "content": "Inspired by recent work of weight interpolations [2, 35, 62, 108, 170],\nIlharco et al. [61] first proposes the concept of task vector, which can be obtained by taking the\ndifference between the original model weights of a pre-trained model and its weights fine-tuned on\na specific task. In particular, if we let \\(\\theta_f\\) be the corresponding weights after fine-tuning on task\nt, the task vector is then denoted as \\(\\tau_t = \\theta_T - \\theta_o\\). Then, taking the element-wise negation of the\ntask vector \\(\\tau_t\\) can enable \\(\\theta_o\\) to forget target knowledge on task t without jeopardizing irrelevant\nknowledge, resulting in an unlearned model that has weight of \\(\\theta_u = \\theta_o - \\lambda \\tau\\) with \\(\\lambda\\) as a scaling\nterm. One exemplar work is SKU [96], which designs a novel unlearning framework to eliminate\nharmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted\nof two stages where the first stage aims to identify and acquire harmful knowledge within the\nmodel, whereas the second stage targets to remove the knowledge using element-wise negation\noperation. Different from pure gradient ascent approaches where the model locality is largely"}, {"title": "4.1.7 Parameter Efficient Module Operation Methods", "content": "Inspired by works on merging model\nparameters under full fine-tuning [67, 107, 169], Zhang et al. [184] explores composing parameter-\nefficient modules (PEM) like LoRA [56] and (IA)\u00b3 [93] for flexible module manipulation. Unlike task\nvector methods, which modify the global weight vectors, PEM operation methods apply localized\nadjustments within specific modules. Similar to task vector methods, the negation operator (\\(\\ominus\\))\nin PEM methods unlearns stored knowledge within adapter modules. For example, in a LoRA\nmodule denoted as \\(\\theta_{lora}\\) = {A, B}, A is initialized following a random Gaussian distribution, and B\nis initialized to all zeros to recover the pre-trained model at the beginning. We could negate B or A\nwhile keeping the other unchanged to facilitate unlearning or forgetting certain skills (e.g., toxic\ndata). This process can be written as:\n\\(\\theta_{lora}^{negation} = \\ominus \\theta_{lora}\\ = {A, -B}\\).\nThis negation operator changes the intermediate layer's activation values in the opposite direction\nof gradient descent, aiding in unlearning targeted knowledge.\nTo further enhance model truthfulness and detoxification, Ext-Sub [57] introduces \"expert\" and\n\"anti-expert\" PEMs. The \"expert\" PEM, trained on retaining data \\(D_r\\), represents desired behaviors,\nwhile the \"anti-expert\" PEM, trained on unlearned data \\(\\tilde{D_f}\\), represents harmful behaviors. Ext-Sub\nidentifies commonalities between these PEMs to determine shared capabilities and then subtracts\nthe deficiency capability responsible for untruthful or toxic content. The Ext-Sub operation is\ndefined as:\n\\(\\theta_u = \\theta_{expert} \\oplus \\lambda \\cdot Ext(\\theta_{anti-expert})\\),\nwhere \\(\\theta_{expert}\\) and \\(\\theta_{anti-expert}\\) are the parameters of the expert and anti-expert PEMs, respectively,\nand Ext() is the extraction function isolating the deficiency capability. This process removes harmful\neffects while preserving and enhancing the beneficial capabilities of the LLM.\nAdditionally, PEM can be utilized during model training to prevent the acquisition of harmful\ninformation. For instance, Zhou et al. [192] introduces security vectors, enabling LLMs to be\nexposed to harmful behaviors without modifying the model's original, safety-aligned parameters.\nSpecifically, these vectors allow the LLM to process and respond to harmful inputs during training,\nensuring that the core parameters remain unaltered. The security vector \\(\\theta_s\\) is optimized to minimize\nthe loss associated with harmful data:\n\\(arg \\min_{\\theta_o} E_{(X,Y)~D_u} \\min_{\\theta_s} L(f(X; \\theta; \\theta_s), Y)\\)"}, {"title": "4.1.8 Summary", "content": "The parameter optimization strategies focus on adjusting specific model param-\neters to selectively unlearn certain behaviors without affecting other functions. These approaches\ninvolve precise alterations to parameters associated with unwanted data influences or biases, ensur-\ning the preservation of essential model performance. Gradient-based approaches with reversed loss\nare effective for unlearning accuracy and generalizability but can negatively impact model locality\nby inadvertently affecting unrelated parameters. In contrast, gradient-based methods without\nreversed loss can maximally preserve locality but may not excel in unlearning accuracy and gener-\nalizability. Extra learnable layers provide highly targeted unlearning but may demand significant\ncomputational resources. Data sharding methods excel in maintaining locality by partitioning the\ntraining data and ensuring specific data points can be unlearned without extensive retraining,\nalthough they might struggle with generalizability in very large models. Knowledge distillation\nis effective in maintaining locality by transferring knowledge to a new model trained to exclude\nspecific data, thus retaining essential performance while unlearning undesired knowledge. However,\nit can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task\nvector and parameter-efficient module operations may perform well in terms of unlearning accuracy\nand generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches\nleading to instability due to significant model degradation, resulting in poor locality performance."}, {"title": "4.2 In-Context Unlearning", "content": "5.  2.  1 Overview . Unlike parameter optimization approaches, which actively modifies parameters\neither locally or globally via different techniques, in-Context unlearning techniques retain the\nparameters in their original state and manipulate the model's context or environment to facilitate\nunlearning. In particular, these strategies result in an unlearned model \\(g_\\theta\\) where \\(\\delta_u = 0\\), but with\nchanges in how the model interacts with its input or inferences.\n6.  2.  2 In-Context Unlearning. In-context unlearning utilizes the approach of in-context learning\nto selectively erase targeted knowledge during inference, treating the model as a black box. This kind\nof method is resource-efficient but has inherent limitations due to the nature of in-context learning.\nSpecifically, it modifies only the model's immediate outputs without fundamentally eradicating\nthe unwanted knowledge embedded within the model's internal parameters. ICUL [125] first\nintroduces the idea of in context unlearning, which alters input prompts during the inference phase\nto achieve targeted unlearning in the situation where model's API is the only access to the model.\nThe technical process involves several key steps:\n(1) Label Flipping, where the label of the data point that needs to be forgotten is flipped\nto contradict the model's learned associations, resulting in the template \"[Forget Input]\\(\\lhd\\)[Flipped Label],\"\n(2) Context Construction, where additional correctly labeled examples are sampled and ap-\npended to the flipped example, creating a mixed input sequence, resulting in the template\n\"[Forget Input]\\(\\lhd\\) [Flipped Label]\\(\\lhd\\) \\\\n; [Input]\\(_1\\) [Label]\\(_1\\) \\\\n;... [Input]\\(_s\\) [Label]\\(_s\\)"}, {"title": "4.2.3 Summary", "content": "Parameter frozen methods retain the model's parameters in their original state\nwhile manipulating the model's context or environment to facilitate unlearning. A notable advantage\nof this method is its resource efficiency, as it does not require retraining or modification of the\nmodel's internal parameters, making it suitable for scenarios where direct access to the model's\ninternals is limited (e.g. black-box models). However, a significant drawback is that parameter\nfrozen methods only modify the model's immediate outputs without fundamentally eradicating\nthe unwanted knowledge embedded within the model's parameters. This can lead to incomplete\nunlearning, as the underlying knowledge remains intact."}, {"title": "5 DATASETS AND BENCHMARKS", "content": "5.1 Datasets\nIn this section, we summarize the datasets commonly used in the field of Generative AI, as outlined\nin Table 4, to benefit future MU research. Instead of merely categorizing the datasets by task\n(i.e., generation and classification), they are organized according to their intended unlearning\nobjectives. We specifically focus on those datasets primarily utilized as target datasets during the\nunlearning process, excluding object removal datasets such as CIFAR10 and MNIST, as well as\ngeneric evaluation benchmark datasets like Hellaswag [181] and Piqa [8]."}, {"title": "5.1.1 Safety Alignment", "content": "The Civil Comments dataset [11] is comprised of public comments from\nvarious news websites, each labeled with a level of toxicity to represent the degree of harmfulness\nin the content. Studies such as [61] and [184] have utilized subsets of these highly toxic samples\nto extract harmful knowledge from pre-trained models. Complementing this, the Anthropic red\nteam dataset [5, 39] includes human preference data and annotated dialogues for evaluating red\nteam attacks on language models. This dataset aids in reducing harm in generative models, as\ndemonstrated by ForgetFilter [189], which classifies conversations into safe and unsafe categories to\nfacilitate unlearning harmful responses, and by [192], which generates security vectors to address\nharmful knowledge.\nNext, the PKU-SafeRLHF dataset [66] consists of 30,000 expert comparison entries with safety\nmeta-labels, initially proposed by BeaverTails to train moderation models preventing LLMs from\ngenerating harmful outputs. This dataset has been instrumental in various studies; for instance,\nLLMU [178] leverages harmful samples for gradient ascent, while SKU [96] extracts harmful\nknowledge from multiple perspectives in pre-trained models. Lastly, moving to the multimodal\ndomain, the LAION dataset [143] contains 5.85 billion CLIP-filtered image-text pairs, supports\nlarge-scale multi-modal model training. Despite its democratizing potential, it poses challenges,\nsuch as the generation of NSFW content by diffusion models. To mitigate these risks, recent studies\nlike [183] and [37] have identified and targeted subsets of NSFW data within LAION for content\nremoval."}, {"title": "5.1.2 Copyrights Protection", "content": "The Harry Potter [138] has been a focus for studies aiming to elimi-\nnate a model's ability to generate Harry Potter-related content while preserving performance. [32]"}, {"title": "5.1.3 Hallucination Eliminations", "content": "The HaluEVAL dataset [80", "178": ".", "110": "includes 21,919 factual relations with prompts and paired true and\nfalse responses, making it suitable for examining how models process and recall factual knowledge.\nLarimar [25"}]}