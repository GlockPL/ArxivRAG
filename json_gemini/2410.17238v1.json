{"title": "SELA: TREE-SEARCH ENHANCED LLM AGENTS FOR AUTOMATED MACHINE LEARNING", "authors": ["Yizhou Chi", "Yizhang Lin", "Sirui Hong", "Duyi Pan", "Yaying Fei", "Guanghao Mei", "Bangbang Liu", "Tianqi Pang", "Jacky Kwok", "Ceyao Zhang", "Bang Liu", "Chenglin Wu"], "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.", "sections": [{"title": "INTRODUCTION", "content": "Automated Machine Learning (AutoML) is a rapidly evolving field that seeks to automate the process of designing reliable machine learning solutions with minimal human intervention. Traditional AutoML frameworks, such as Auto-WEKA (Thornton et al., 2013), Auto-Sklearn (Feurer et al., 2015; 2020), AutoGluon (Tang et al., 2024b), and H2O AutoML (LeDell & Poirier, 2020), rely on predefined search spaces and routines. These frameworks primarily focus on optimizing hyperparameters and model ensembling to find the best model configuration. However, this fixed and static approach often lacks the adaptability needed to handle diverse and dynamic data scenarios, resulting in suboptimal performance in more complex settings. Additionally, the traditional focus on model training leaves other crucial stages of the machine learning pipeline, such as data preprocessing and feature engineering, underexplored, thereby limiting the overall effectiveness of these systems.\nRecently, large language model (LLM)-based agents have emerged as promising tools for automating machine learning tasks by leveraging natural language processing capabilities to generate code. These systems typically begin with a natural language prompt describing the dataset and the problem, after which an LLM generates an end-to-end solution. Early efforts, such as Zhang et al."}, {"title": "2 RELATED WORKS", "content": "Tree Search and Its Integration with LLMs Tree search algorithms have significantly advanced problem-solving in artificial intelligence, with Monte Carlo Tree Search (MCTS) emerging as a leading technique. These algorithms have been successfully applied across various domains, including robotics (Wu et al., 2015; Clary et al., 2018; Best et al., 2019), chemistry (Segler et al., 2018), and gaming (Silver et al., 2016; 2017), where MCTS is used to navigate vast solution spaces and solve complex problems. More recently, research has focused on integrating tree search with Large Language Models (LLMs) to enhance reasoning and decision-making. Studies such as Krishnamurthy et al. (2024) and Dwaracherla et al. (2024) explored LLMs' capacities for efficient exploration, while Tang et al. (2024a) and Hui & Tu (2024) developed strategies for exploiting previously learned knowledge. Zhou et al. (2024) and Chi et al. (2024) applied MCTS for planning with external or self-evaluated feedback, while Feng et al. (2023); Wang et al. (2024) adapted AlphaZero-style tree search to LLM-based tasks. These advancements underscore the potential of combining tree search methods with LLMs, balancing exploration of new solutions with exploitation of prior knowledge to enhance decision-making.\nAdvances and Limitations in AutoML Systems Automated Machine Learning (AutoML) frameworks were introduced to reduce the need for expert knowledge in designing machine learning pipelines. Early AutoML efforts, such as (Thornton et al., 2013; Olson & Moore, 2016; Jin et al., 2019; Feurer et al., 2020; Erickson et al., 2020; LeDell & Poirier, 2020; Wang et al., 2021), focused primarily on automating key pipeline components like hyperparameter optimization, model selection, stacking, and ensembling. These frameworks achieved notable progress by integrating meta-learning and hyperparameter search strategies to automatically select and tune machine learning models. Furthermore, extensions into multi-modal data settings (Tang et al., 2024b; Jin et al., 2023) have broadened AutoML's applicability.\nRecently, there has been growing interest in leveraging LLMs within AutoML systems to enhance pipeline flexibility. Studies such as Hollmann et al. (2024); Li et al. (2024) applied LLMs to automate feature engineering, while Liu et al. (2024) introduced LLMs for hyperparameter tuning. In addition, Luo et al. (2024) proposed embedding LLMs at each stage of the machine learning workflow. Despite these advancements, traditional AutoML systems remain constrained by rigid pipelines and limited flexibility to adapt to unique datasets or specific task requirements.\nLLM Agents for Dynamic Machine Learning Pipelines In contrast to static pipelines, LLM-based agents offer a more dynamic solution for addressing complex machine learning challenges. Hong et al. (2024a;b) introduced an LLM agent with hierarchical graph modeling and programmable node generation, enabling the creation of sophisticated, adaptable pipelines for diverse data scenarios. Similarly, Zhang et al. (2024) demonstrated that LLMs could effectively interpret structured inputs and apply past experiences to solve new machine learning tasks. Guo et al. (2024) expanded on this by introducing a data science agent that leverages case-based reasoning; however, it faces challenges when generating solutions from scratch due to its reliance on existing codebases. Schmidt et al."}, {"title": "3 METHOD", "content": "As illustrated in Figure 2, SELA consists of three key components: an LLM-based insight proposer, a search module using MCTS, and an LLM agent as the experiment executor. First, the LLM generates insights from the problem description and dataset, defining a search space. The search module then organizes this space into a tree structure and uses MCTS to explore promising paths. During each cycle, the selected path is passed to the LLM agent, which translates the configuration into an executable pipeline. The agent plans, codes, and executes the experiment, feeding the results back to refine future searches. This iterative process continues until the termination criterion is met. The following sections provide a detailed explanation of each component."}, {"title": "3.1 INSIGHT PROPOSAL AND SEARCH SPACE CREATION", "content": "To enable SELA to explore a wide range of machine learning strategies, we introduce an insight proposer that generates diverse methods tailored to different stages of the machine learning workflow. Each proposed insight suggests either a single technique or a combination of methods aimed at enhancing performance. For instance, a feature engineering insight might recommend creating interaction features from existing variables, while a model training insight could propose a specific algorithm or suggest running a grid search to improve accuracy.\nThe insight proposer takes as input the problem description $p$ and dataset information $d$, such as metadata and sample records, and generates $m$ insights $A$ for each stage of the machine learning process using a large language model $M$. These insights are stored in an insight pool, forming a search space $A$ for SELA to explore. We decompose the machine learning process into five stages: Exploratory Data Analysis ($\\tau_1$), Data Preprocessing ($\\tau_2$), Feature Engineering ($\\tau_3$), Model Training ($\\tau_4$), and Model Evaluation ($\\tau_5$). For simplicity, we denote the entire set of stages as $T$ and refer to"}, {"title": "3.2 PIPELINE EXECUTION AND CODE GENERATION", "content": "We employ an LLM agent, referred to as the experiment executor $E$, to conduct each trial by building practical experimental pipelines from natural language requirements. The agent takes two main steps in this process. First, given an experiment configuration $c$, which is a set of insights provided by the search module (introduced in Section 3.3.2), the experiment executor translates these insights into a detailed plan. This plan consists of a sequence of task instructions $I_{\\tau \\in T}$ corresponding to each stage of the machine learning process. This step is referred to as $E_{\\text{plan}}$.\nNext, following the plan, the agent writes and executes code $\\sigma_{\\tau}$ for each task $\\tau$ based on the respective instruction $I_{\\tau}$, producing the code $\\sigma_{\\tau \\in T}$ for the full pipeline, along with the final execution score $s$. The complete set of code outputs $\\sigma_{\\tau \\in T}$ is concatenated into a full solution $\\sigma_{\\text{sol}}$ to address the problem. This phase is referred to as $E_{\\text{code \\& execute}}$.\n$E_{\\text{plan}}(p, d, c, M) \\rightarrow I_{\\tau \\in T}$ (2)\n$E_{\\text{code \\& execute}}(I_{\\tau \\in T}, D, M) \\rightarrow (\\sigma_{\\tau \\in T}, s)$ (3)"}, {"title": "3.3 TREE SEARCH IN MACHINE LEARNING EXPERIMENTS", "content": "In order to systematically explore the different configurations in machine learning experiments, we model the search space as a hierarchical tree. This structure allows us to apply tree search algorithms, where each path through the tree represents a different experiment configuration. Algorithm 1 also provides an overview of this searching process."}, {"title": "3.3.1 EXPERIMENT NODE", "content": "To facilitate the exploration of various strategies, we model the proposed search space as a hierarchical tree that is well-suited for applying search algorithms. Each node in the tree, denoted as $x$, represents one insight $A$ in the search space $A$ and contains the following attributes:\n\u2022 Insight $\\lambda(x)$: Represents the specific insight $\\lambda \\in A$ associated with this node, where $\\tau$ denotes the stage of the machine learning pipeline.\n\u2022 Depth $\\delta(x)$: Indicates the stage of the machine learning process the node corresponds to (e.g., depth 1 might represent data preprocessing, depth 2 for feature engineering, and depth 3 for model training).\n\u2022 Value $v(x)$: The cumulative score from simulations for this node and all its descendants.\n\u2022 Number of Visits $n_{\\text{visits}}(x)$: The total number of simulations conducted for this node and its descendants.\n\u2022 Simulation Score $s(x)$: The score for simulating this node.\n\u2022 Solution Code $\\sigma_{\\text{sol}}(x)$ The final code produced after the node simulation.\n\u2022 Stage Code $\\sigma_{\\text{stage}}(x)$: The code generated up to the node's current stage, a part of the solution code\nBy modeling the search space as a tree, each path from the root to a node $x$ represents an experiment configuration $c(x) = {\\lambda(x_1), \\lambda(x_2), ..., \\lambda(x)} \\in A$, where $x_1, x_2, ..., x$ are nodes along the path. The task of finding the optimal solution can therefore be viewed as a path search within the tree, where each path corresponds to a potential configuration of the experiment."}, {"title": "3.3.2 TREE SEARCH FOR ML EXPERIMENTS", "content": "We apply Monte Carlo Tree Search (MCTS) to systematically explore and identify optimal machine learning solutions within our framework. MCTS allows us to efficiently navigate the search space across multiple stages of the machine learning pipeline\u2014from data preprocessing to model selection\u2014by balancing exploration and exploitation."}, {"title": "4 EXPERIMENTS", "content": "Datasets We evaluate SELA alongside several baselines on 20 datasets, which include 13 classification tasks and 7 regression tasks from the AutoML Benchmark (AMLB) (Gijsbers et al., 2024) and Kaggle Competitions.\nTable 4 provides detailed information on the datasets used. All datasets are split into training, validation, and test sets with a 6:2:2 ratio. Each framework utilizes the training and validation sets to train models and makes predictions on the test set labels.\nEvaluation Metrics For the AMLB datasets, we use the default target column provided by OpenML. For Kaggle competition datasets, we rely on the target column specified in the competition description. Performance is measured using root mean squared error (RMSE) for regression tasks, F1 score for binary classification, and F1-weighted score for multi-class classification. To ensure comparability across datasets with varying metrics, we introduce a Normalized Score (NS), which maps RMSE into the range from 0 to 1."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets We evaluate SELA alongside several baselines on 20 datasets, which include 13 classification tasks and 7 regression tasks from the AutoML Benchmark (AMLB) (Gijsbers et al., 2024) and Kaggle Competitions.\nTable 4 provides detailed information on the datasets used. All datasets are split into training, validation, and test sets with a 6:2:2 ratio. Each framework utilizes the training and validation sets to train models and makes predictions on the test set labels.\nEvaluation Metrics For the AMLB datasets, we use the default target column provided by OpenML. For Kaggle competition datasets, we rely on the target column specified in the competition description. Performance is measured using root mean squared error (RMSE) for regression tasks, F1 score for binary classification, and F1-weighted score for multi-class classification. To ensure comparability across datasets with varying metrics, we introduce a Normalized Score (NS), which maps RMSE into the range from 0 to 1."}, {"title": "4.2 RESULTS", "content": "As shown in Table 2, SELA achieves the highest average Normalized Score (NS) and average best rank among all frameworks. Notably, SELA excels in producing the highest number of top predictions, as indicated in the \"Top 1\" column across all datasets. Furthermore, the \"Losses\""}, {"title": "4.3 ABLATION STUDY", "content": "For the rest of the study, we employ a subset of datasets to evaluate SELA under various settings. Our selection process involves choosing the first two datasets alphabetically for each machine learning task. Specifically, we use boston, colleges, credit-g, Click_prediction_small, GesturePhaseSegmentationProcessed, and mfeat-factors to conduct the ablation study."}, {"title": "Effectiveness of Search", "content": "To evaluate the effectiveness of Monte Carlo Tree Search (MCTS) in improving the solution search process, we conducted an ablation study. In this study, we compared the performance of our method using MCTS against a variant that randomly samples insights from each stage's insight pool. As shown in Table 3, the MCTS version achieves a higher average normalized score across datasets and a better overall ranking compared to the random sampling approach. Moreover, even the random sampling variant of our method outperforms Data Interpreter, the base experimenter. This suggests the presence of an appropriate search space and an experiment agenda is vital for improving a machine learning agent. Our insight proposer generates relevant and useful insights, facilitating such improvement, regardless of the selection method."}, {"title": "Number of Rollouts", "content": "Figure 4 illustrates that the average performance of SELA improves as the number of permitted rollouts increases. The trend demonstrates the strong scalability of SELA, as it efficiently leverages additional opportunities to explore the search space, improving the normalized score by 4.7% after 10 rollouts and 6.4% after 20, compared to the initial rollout."}, {"title": "LLM Adaptability", "content": "To evaluate the robustness of our framework, we conduct experiments using different Large Language Models (LLMs). Specifically, we compare the performance of SELA with Claude-3.5-Sonnet (Anthropic, 2024) and GPT-40 (OpenAI, 2024) against DeepSeek V2.5 which we primarily use for evaluation. This comparison enables us to assess how the choice of LLM affects the overall effectiveness of our approach."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced SELA, a novel framework that integrates LLM-based agents with Monte Carlo Tree Search (MCTS) to automate machine learning workflows. Our experimental results, conducted on 20 machine learning datasets, demonstrate SELA's effectiveness and highlight its distinct advantages over both traditional AutoML frameworks and existing LLM-based approaches. The proposed methodology is not limited to machine learning but could be adapted to a wide range of sequential decision-making problems, provided they can be represented as tree structures with scalar rewards derived from their leaf nodes. Looking ahead, future work could explore extending this framework to other domains, including software engineering, scientific discovery, game playing, and robotics. Furthermore, improving the efficiency and scalability of the tree search process for larger solution spaces remains an important area for investigation. Another promising direction is developing techniques to provide interpretable explanations of the search process and solution rationale, enhancing the transparency and trustworthiness of the system. SELA represents a significant advancement in automated machine learning, demonstrating the potential of combining traditional search algorithms with the flexibility of LLMs."}, {"title": "B.1 TASK PROMPT", "content": "All LLM-based methods start by receiving the same base requirement prompt at the beginning of the task. The prompt specifies the dataset's name, the target label column, the evaluation metric to be used, and the dataset's file path. Furthermore, the prompt include a path to a text file containing the dataset's metadata."}, {"title": "B.2 INSTRUCTION PROMPT", "content": "The instruction prompt would direct the framework to save the final prediction file for evaluation."}, {"title": "B.3 INSIGHT PROPOSAL PROMPT", "content": "Insight Proposer uses this prompt to generate a search space of insights for different stages of the machine learning task."}, {"title": "E.1 OVERVIEW OF SELA'S SEARCH PROCESS", "content": "In this case study, we demonstrate how SELA conducts a search cycle using MCTS:\nPre-search Step: Initialization\nSELA begins by defining high-level stages, such as exploratory data analysis, data preprocessing, feature engineering, and model training, which structure the overall machine learning workflow. During the search, SELA populates these stages with specific insights, which act as experimental configurations for simulation.\nStep 1 & 2: Selection and Expansion\nSELA leverages MCTS to explore specific stages like feature engineering and model training. For example, in one iteration, SELA selects Node 0-1. This node corresponds to a stage insight that generates time-based features, expanding into five child nodes representing various model specifications and training strategies, such as Random Forests, Support Vector Machines, Neural Networks, Gradient Boosting, or Grid Search.\nStep 3: Simulation\nNext, SELA samples one of the expanded child nodes for simulation. For instance, when Node 0-1-1 is chosen, SELA runs a complete experiment where time-based feature engineering (Node 0-1) is followed by training a Support Vector Machine (SVM) with a kernel specified by Node 0-1-1. The simulation yields an evaluation score.\nStep 4: Backpropagation\nAfter the simulation, the resulting performance score is propagated back through the tree. For example, after simulating Node 0-1-1, MCTS updates the numeric feedback for its parent nodes, such as Node 0-1 and Node 0. The search cycle repeats from Steps 1 to 4 until a stopping condition is reached.\nPost-search Step: Best Node Selection\nIn the final phase, SELA selects the node representing the best-performing solution. In this example, Node 0-1-1, using an SVM with an RBF kernel, achieved the highest score in the current dataset by combining effective feature engineering with advanced model training. SELA then presents the code associated with this node as the optimal solution."}], "equations": ["\\text{InsightProposer}(p, d, M) \\rightarrow A := {\\lambda_{\\tau i} | \\tau \\in T, i = 1, ..., m} \\tag{1}", "E_{\\text{plan}}(p, d, c, M) \\rightarrow I_{\\tau \\in T} \\tag{2}", "E_{\\text{code \\& execute}}(I_{\\tau \\in T}, D, M) \\rightarrow (\\sigma_{\\tau \\in T}, s) \\tag{3}", "\\text{UCT-DP}(x) = \\frac{v(x)}{n(x)} + \\lambda_{\\text{explore}} \\sqrt{\\frac{\\ln n_{\\text{visits}}(x_{\\text{parent}})}{n(x)}} \\tag{4}", "n(x) = \\begin{cases} \\lambda_{\\text{unvisited}} & \\text{if } n_{\\text{visits}}(x) = 0 \\\\ n_{\\text{visits}}(x) & \\text{otherwise}. \\end{cases} \\tag{5}", "\\text{NS}(s_{\\text{raw}}) = \\begin{cases} 1 - \\frac{1}{1 + \\log(1 + s_{\\text{raw}})} & \\text{if the metric is RMSE}. \\\\ s_{\\text{raw}} & \\text{otherwise}. \\end{cases} \\tag{6}", "\\text{Rescaled NS}(f) = \\frac{\\text{NS}_f}{\\text{NS}_{SELA}} \\tag{7}"]}