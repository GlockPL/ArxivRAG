{"title": "DINT Transformer", "authors": ["Yueyang Cang", "Yuhang Liu", "Xiaoteng Zhang", "Erlu Zhao", "Shi Li"], "abstract": "DIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.", "sections": [{"title": "1. Introduction", "content": "Transformer(Vaswani, 2017), as one of the most popular models in the field of artificial intelligence today, is widely used in natural language processing, computer vision, and other fields, especially with the application of decoder-only architectures in large language models (LLMs). Its core lies in the attention mechanism based on softmax, which assigns importance to different tokens in a sequence. However, recent research(Lu et al., 2021) has found that LLMs face the challenge of attention noise when accurately focusing on key information in the context.\nTo address the issue of attention noise, DIFF Transformer(Ye et al., 2024) introduces a differential attention mechanism that effectively suppresses the impact of irrelevant context by computing DIFFerence between two independent attention distributions. However, DIFF Transformer still has a significant limitation: DIFFerential operation causes the resulting attention matrix to fail in guaranteeing that the sum of each row equals one. This introduces numerical instability into the model's internal computations and may adversely affect the performance of downstream tasks.\nIn our study, we observe that many tokens within a sequence often rely on a few globally critical tokens for their semantic interpretation. For example, in a sentence, key elements such as the subject or main predicate verb often serve as semantic anchors, playing a crucial role in shaping the overall meaning of the sentence (as shown in Figure 1). Based on this observation, we propose DINT Transformer, which extends DIFF Transformer by introducing an integral mechanism. This integral component computes global importance scores, enhancing the model's focus on critical tokens. Our proposed DINT Transformer not only reduces attention noise further by strengthening the focus on globally important tokens but also ensures row-normalized attention matrices through the parameter setup, resolving the numerical instability issue present in DIFF Transformer, thereby significantly improving model performance.\nWe conducted extensive experiments on tasks such as long-context language modeling and key information retrieval to evaluate the effectiveness of DINT Transformer. The results demonstrate that DINT Transformer consistently outperforms both the Transformer and DIFF Transformer, especially in long-sequence tasks, where its integral mechanism effectively captures global dependencies and further reduces attention noise. By ensuring row-normalized attention distributions, DINT Transformer provides an interpretable and robust attention mechanism, addressing key limitations of prior approaches. Furthermore, DINT Transformer enhances performance in downstream tasks like key information retrieval while maintaining scalability. These findings establish DINT Transformer as a powerful and efficient foundation for future advancements in sequence modeling and large language models."}, {"title": "2. DINT Transformer", "content": "DINT Transformer is designed as a robust architecture for sequence modeling, particularly for large language models (LLMs). The model consists of L stacked layers, where each layer applies a DINT attention module followed by a feedforward network. Starting from token embeddings $X_o \\in \\mathbb{R}^{N\\times d_{model}}$, the input is progressively transformed through L layers to produce the final output $X_L$. The key innovation lies in the addition of an integral mechanism within the attention module, which enables effective modeling of global dependencies while preserving numerical stability. The overall structure aligns with common practices, incorporating pre-RMSNorm(Zhang & Sennrich, 2019) and SwiGLU(Ramachandran et al., 2017; Shazeer, 2020) for enhanced performance following LLaMA(Touvron et al., 2023). A diagram of the model architecture is shown in Figure 2."}, {"title": "2.1. DIFF Attention", "content": "DIFF attention introduces a differential attention mechanism that reduces attention noise by leveraging the difference between two attention distributions. Specifically, given the input $X \\in \\mathbb{R}^{N \\times d_{model}}$, it is projected to query, key, and value matrices:\n$[Q_1; Q_2] = XW_Q, [K_1; K_2] = XW_K, V = XW_V, \\qquad (1)$\nwhere $Q_1, Q_2, K_1, K_2 \\in \\mathbb{R}^{N\\times d}$ and $V \\in \\mathbb{R}^{N\\times 2d}$ are the projected matrices, and $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model}\\times 2d}$ are learnable parameters. The differential attention operator computes the output as:\n$\\text{DiffAttn}(X) = (\\text{softmax} (\\frac{Q_1K_1}{\\sqrt{d}}) - \\lambda \\cdot \\text{softmax} (\\frac{Q_2K_2}{\\sqrt{d}}))V \\qquad (2)$\nwhere $\\lambda$ is a learnable scalar parameter. This differential mechanism effectively suppresses irrelevant context, enhancing the robustness of the attention scores by canceling common-mode noise, analogous to the operation of differential amplifiers in electrical engineering. To synchronize learning dynamics, $\\lambda$ is re-parameterized as:\n$\\lambda = \\text{exp}(\\lambda_{q1} \\cdot \\lambda_{k1}) - \\text{exp}(\\lambda_{q2} \\cdot \\lambda_{k2}) + \\lambda_{init}, \\qquad (3)$\nwhere $\\lambda_{q1}, \\lambda_{k1}, \\lambda_{q2}, \\lambda_{k2} \\in \\mathbb{R}^{d}$ are learnable vectors, and $\\lambda_{init} \\in (0, 1)$ is a constant used for initialization. Empirical results show that setting $\\lambda_{init} = 0.8 - 0.6 \\times \\text{exp}(-0.3\\cdot (1 - l))$, where $l \\in [1, L]$ represents the layer index, works well in practice."}, {"title": "2.2. DINT Attention", "content": "DINT attention extends DIFF attention by introducing an integral mechanism, enhancing the model's ability to capture globally important information while maintaining numerical stability through row normalization in the final attention matrix. The signal attention matrix $A_1$ is computed using $Q_1$ and $K_1$:\n$A_1 = \\text{softmax}(\\frac{Q_1K_1^T}{\\sqrt{d}}) \\qquad (4)$\nThe integral component computes global importance scores by averaging the signal attention weights column-wise:\n$G = \\frac{1}{N} \\sum_{m=1}^N A_1 [m, n], \\qquad (5)$\nwhere $G \\in \\mathbb{R}^{1\\times N}$ is then expanded to match the dimensions of differential component:\n$G_{expanded} = \\text{repeat}(G, N), \\qquad (6)$\nwhere $G_{expanded} \\in \\mathbb{R}^{N\\times N}$ is obtained by repeating G across N rows.\nDINT attention operator computes the output as:\n$\\text{DINTAttn}(X) = (A_{diff} + \\gamma \\cdot G_{expanded}) V, \\qquad (7)$\nwhere $\\gamma$ is a learnable scalar following DIFF Transformer, $A_{diff}$ is DIFF attention component, and $G_{expanded}$ is the expanded global importance scores matrix.\nUnified Parameter Setting. By setting $\\lambda$ and $\\gamma$ to the same value, we ensure that the final attention matrix $A_{final}$ has rows that sum to 1. This row normalization guarantees numerical stability and consistency across the model, thus maintaining data stability throughout the layers. This unified setting follows the parameterization method used in DIFF Transformer, further enhancing training stability."}, {"title": "2.3. Multi-Head Differential Attention", "content": "We also use the multi-head mechanism in DINT Transformer. Let h denote the number of attention heads. We use different projection matrices $W_q^i, W_k^i, W_v^i, i \\in [1, h]$ for the heads. The scalar $\\lambda$ is shared between heads within the same layer. Then the head outputs are normalized and projected to the final results as follows:\n$\\text{head}_i = \\text{DiffAttn}(X; W_q^i, W_k^i, W_v^i, \\lambda) \\qquad (8)$\n$\\text{head}_i = \\text{LN}(\\text{head}_i) \\qquad (9)$\n$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_h)W_o \\qquad (10)$\nwhere $W_o \\in \\mathbb{R}^{d_{model}\\times d_{model}}$ is a learnable projection matrix, LN(\u00b7) uses RMSNorm for each head, and Concat(\u00b7) concatenates the heads together along the channel dimension. Unlike DIFF Transformer, we do not apply an additional multiplier to the outputs of each head, as the unified parameter setting in DINT Transformer already ensures numerical stability and consistency. The number of heads is set as $h = d_{model}/2d$, where d is the head dimension of the Transformer, to ensure that the parameter count and computational complexity are aligned."}, {"title": "2.4. Overall Architecture", "content": "The overall architecture stacks L layers, where each layer contains a multihead differential attention module and a feedforward network module. We describe DINT Transformer layer as:\n$Y^l = \\text{MultiHead}(\\text{LN}(X^l)) + X^l \\qquad (11)$\n$X^{l+1} = \\text{SwiGLU}(\\text{LN}(Y^l)) + Y^l \\qquad (12)$\nwhere LN(\u00b7) is RMSNorm, and SwiGLU(X) is defined as:\n$\\text{SwiGLU}(X) = (\\text{swish}(XW_G) XW_1)W_2,$\nwhere $W_G, W_1 \\in \\mathbb{R}^{d_{model}\\times d_{model}}$, and $W_2 \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ are learnable matrices."}, {"title": "3. Experiments", "content": "In this study, we evaluate DINT Transformer through a series of experiments, comparing it with DIFF Transformer and other baseline models. Since DINT Transformer does not introduce new learnable parameters, only increasing computational complexity, its parameter count remains unchanged. Therefore, the model configurations used in the comparison were chosen to be the same as those of DIFF Transformer. Our experiments show that by enhancing attention to globally significant tokens, DINT Transformer effectively reduces attention noise. Additionally, DINT Transformer exhibits stronger stability compared to DIFF Transformer, leading to improved performance across tasks such as long-sequence modeling, key information retrieval, and in-context learning."}, {"title": "3.1. Language Modeling Evaluation", "content": "We trained a 3B DINT Transformer language model using the same configuration settings as the 3B DIFF Transformer language model. The model settings are shown in Table 1."}, {"title": "4. Conclusions", "content": "We propose DINT Transformer, which integrates global attention statistics into DIFF Transformer to reduce noise and enhance focus on key words. This improves the model's ability to capture global information, ensuring better stability and scalability. Experiments show DINT Transformer excels in long-sequence modeling, key information retrieval, and in-context learning, making it highly promising for NLP tasks requiring global context awareness."}]}