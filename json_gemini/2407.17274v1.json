{"title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation", "authors": ["Yongqi Li", "Hongru Cai", "Wenjie Wang", "Leigang Qu", "Yinwei Wei", "Wenjie Li", "Liqiang Nie", "Tat-Seng Chua"], "abstract": "Text-to-image retrieval is a fundamental task in multimedia processing, aiming to retrieve semantically relevant cross-modal content. Traditional studies have typically approached this task as a discriminative problem, matching the text and image via the cross-attention mechanism (one-tower framework) or in a common embedding space (two-tower framework). Recently, generative cross-modal retrieval has emerged as a new research line, which assigns images with unique string identifiers and generates the target identifier as the retrieval target. Despite its great potential, existing generative approaches are limited due to the following issues: insufficient visual information in identifiers, misalignment with high-level semantics, and learning gap towards the retrieval target. To address the above issues, we propose an autoregressive voken generation method, named AVG. AVG tokenizes images into vokens, i.e., visual tokens, and innovatively formulates the text-to-image retrieval task as a token-to-voken generation problem. AVG discretizes an image into a sequence of vokens as the identifier of the image, while maintaining the alignment with both the visual information and high-level semantics of the image. Additionally, to bridge the learning gap between generative training and the retrieval target, we incorporate discriminative training to modify the learning direction during token-to-voken training. Extensive experiments demonstrate that AVG achieves superior results in both effectiveness and efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-image retrieval, as a fundamental task in multimedia processing, has garnered significant attention over the past decade [33, 39, 40]. Its goal is to offer users cross-modal content that goes beyond just text. Despite the fundamental nature of this task, it faces the challenge of bridging the gap between two distinct modalities, text and image, which presents specific research problems.\nCurrent approaches to text-to-image retrieval can be categorized into two groups: 1) The first category, known as the one-tower framework [3, 6], utilizes a cross-attention mechanism to model fine-grained interactions. Recently developed large vision-language models, such as BLIP [16], have shown remarkable capabilities in accurately ranking a small list of images. However, the one-tower architecture falls short in terms of efficiency, making it less suitable for large-scale image retrieval scenarios. 2) The two-tower framework [4, 40], e.g., CLIP [25], independently maps visual and textual samples into a joint embedding space to calculate cross-modal similarity. This framework trades off some accuracy for increased efficiency, making it excel at retrieving relevant images from extensive image sets. In practice, retrieving a certain number of images from a large-scale set is the initial and essential step to accurately rank a small list of images. Therefore, we concentrate on the fundamental step, to accomplish effective cross-modal retrieval while keeping high efficiency.\nGenerative cross-modal retrieval, a new retrieval paradigm, has recently emerged as a novel research line to cross-modal retrieval [19]."}, {"title": "2 RELATED WORK", "content": "Cross-modal retrieval (text-image matching) [7, 18, 22, 34] is a fundamental task in multimedia processing, which can be categorized into one-tower framework and two-tower framework according to the modality interactions. The one-tower framework [3, 6, 14, 24] incorporates fine-grained cross-modal interactions to achieve matching between fragments, such as objects and words. For example, the pioneering work SCAN [14] utilized cross-modal interaction to infer latent region-word alignments. Diao et al. [6] inferred multi-level semantic correlations in a similarity graph and used the attention"}, {"title": "2.1 Cross-modal Retrieval", "content": "mechanism to filter out noisy alignments. In the two-tower framework [4, 9, 23, 40], images and texts are independently mapped into a joint feature space, where semantic similarities are calculated using cosine functions or Euclidean distances. For example, Frome et al. [10] proposed a method that represents visual and textual instances in a modality-agnostic space to assess their semantic relevance. To enhance discriminative power, Zheng et al. [40] argued that the commonly used ranking loss is ineffective for large-scale multi-modality data, and they introduced a new instance loss that exploits intra-modal data distribution in an end-to-end manner.\nIt is important to note that the one-tower framework and two-tower framework are not directly comparable. The one-tower framework prioritizes fine-grained interaction at the expense of efficiency, making it less suitable for retrieval with large-scale image sets. Both the one-tower framework and the two-tower framework formulate cross-modal retrieval as a discriminative problem, which relies on discriminative loss and negative samples [15]. Differently, we formulate the text-to-image retrieval task as the token-to-voken generation problem. Our work is designed to be applicable to large-scale image sets with high efficiency, making it comparable to the two-tower framework."}, {"title": "2.2 Generative Retrieval", "content": "Generative retrieval is an emerging paradigm in text retrieval that involves generating identifier strings of passages as the retrieval target. Identifiers play an important role in generative retrieval, which could reduce the volume of irrelevant information and facilitate easier memorization and learning for the model [20]. Different types of identifiers have been explored in various search scenarios, including passage titles (Web URLs), numeric IDs, and substrings of passages, as shown in previous studies [2, 5, 28, 30]. Although the text is naturally discrete, there are studies [29, 37] demonstrating that relearning a codebook to discretize the documents is effective for text generative retrieval. Regrettably, the above identifiers are ineffective for images in the context of cross-modal retrieval, due to the unique challenges of bridging the gap between two distinct modalities in this task.\nBeyond the text retrieval, Li et al. [19] first proposed the generative cross-modal retrieval paradigm. However, they used predefined strings as identifiers of images, which lacked the visual information of images. IRGen [38] applied the generative retrieval into the image-to-image retrieval task and also adopted the image tokenization technique. However, IRGen is ineffective in text-to-image retrieval due to the misalignment with semantics in the image tokenization stage. Besides, they also overlooked the learning gap between generation training and the retrieval target."}, {"title": "2.3 Image Tokenization", "content": "In 2017, Van Den Oord et al. [32] proposed the Vector Quantized Variational Autoencoder (VQ-VAE), a model designed to learn the low-dimensional discrete representation of images and autoregressively generate the images. Building upon this work, VQ-VAE2 [27] incorporated a hierarchy of discrete representations. Subsequently, VQ-GAN [8] further enhanced the perceptual quality of reconstructed images by incorporating adversarial and perceptual loss. Additionally, ViT-VQGAN [36] introduced the powerful transformer"}, {"title": "3 METHOD", "content": "Our proposed AVG consists of two stages:\nCross-modal Aligned Image Tokenization: This step aims to develop an effective image tokenizer, which discretizes an image into a sequence of vokens based on not only its visual content but also the high-level semantics.\nDiscrimination Modified Token-to-Voken Generation: By discretizing images into vokens, the image retrieval task is formulated as the token-to-voken generation problem. To mitigate the gap between generation and retrieval, a discriminative loss is introduced to modify the learning goal of the generative training."}, {"title": "3.1 Cross-modal Aligned Image Tokenization", "content": "Unlike text, which naturally breaks down into discrete units, images are continuous. To generate an image in an autoregressive manner, the first step is to discretize the image. With the success of autoregressive generation, the concept of image tokenization has been investigated through methods like VQ-VAE [32] and RQ-VAE [13]. Essentially, these approaches utilize a variational autoencoder to learn a visual codebook, which is then used to map image patches onto a sequence of vectors corresponding to entries in the visual codebook.\nDespite the success of image tokenizers in autoregressive image generation, they are not well-suited for cross-modal retrieval tasks for several practical reasons: 1) The tokenization process results in an excessively long sequence of tokens. For instance, VQ-VAE divides a 128*128 image into 32*32 patches, creating a total of 1,024"}, {"title": "3.2 Discrimination Modified Token-to-Voken Generation", "content": "Since we could discretize an image i into a sequence of vokens and these tokens could well represent the image's visual content and high-level semantics, the task of text-to-image retrieval can naturally be reinterpreted as token-to-voken generation problem.\nGenerative training. The autoregressive language models, such as T5 [26] and LLaMA [31], denoted as AM, only support token-to-token generation. Therefore, we first expand the language models' vocabulary by adding the vokens in the learned visual codebook C into the original token list. It is notable that we also copy the voken's embedding into the vocabulary to take full advantage of the embedded semantics, which will benefit the following generative training. We then continue training the generative language model using the next \"voken\" prediction, formulated as follows,\n\\(L_{gen} = -\\sum_{j=1}^{N} log p_{\\Theta}(voken_j|token_{<L};voken_{<j})\\),\nwhere \\(voken_{<j}\\) denotes the vokens from an image i, \\(voken_{<L}\\) denotes the tokens after tokenizing the text t, and L is the length of the sequence of tokens. \u0398 is the parameters of the generative language model. After the generative training, the autoregressive language model AM could conduct associations between text (tokens) and image (vokens).\nDiscriminative training. As we claimed in the Introduction, there is a notable discrepancy between generative training and the intended retrieval target. To mitigate this learning gap, we introduce a discriminative loss to modify the learning direction of the autoregressive model AM. We first prompt the language model to generate several images (vokens) via beam search, as follows,\n\\[I_r = AM(token_{<L}; b),\\]\\[I_r = \\{I_1, I_2, ..., I_b \\},\\]"}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments to answer four research questions:\nRQ1: How does our proposed AVG compare in performance to previous generative cross-modal retrieval methods?\nRQ2: How does the AVG compare in effectiveness and efficiency to the classical one-tower and two-tower frameworks?\nRQ3: How do different components contribute to the performance of AVG?\nRQ4: How does AVG perform with different key settings, such as the visual codebook, voken length, and model size?"}, {"title": "4.1 Datasets and Baselines", "content": "Datasets. We evaluated the proposed autoregressive voken generation method, AVG, on two widely-used datasets: Flickr30K [35] and MS-COCO [21]. Flickr30K comprises 31,783 images, each paired with five human-annotated sentences. We utilized the data split employed by Li et al. [17], with 29,783 images for training, 1,000 for validation, and 1,000 for testing. MS-COCO consists of 123,287 images, each accompanied by five annotated sentences. We followed the dataset split proposed in [14], using 113,287 images for training, 5,000 for validation, and 5,000 for testing. In line with previous studies [4, 35], we evaluated our method using the standard recall metric R@K, with K set to 1, 5, and 10.\nBaselines. As generative cross-modal retrieval is a relatively new paradigm, there are only a few methods that serve as baselines. The first method, GRACE [19], is the pioneering generative"}, {"title": "4.3 Overall Performance (RQ1)", "content": "To answer the question RQ1, we conducted experiments on Flickr30K and MS-COCO (5k) to compare AVG with previous generative cross-modal retrieval methods. The results are presented in Table 1, from which we have the following observations:\nAmong the three generative cross-modal retrieval approaches, GRACE performs poorly compared to IRGen and AVG. This is because GRACE determines an image's content without taking its"}, {"title": "4.4 Paradigm Comparison (RQ2)", "content": "As a new approach, we also compared AVG with the classical one-tower framework and two-tower framework. We summarize the three different paradigms (BLIP, CLIP, AVG) from the aspects of performance, large-scale retrieval, and efficiency (latency). The performance is evaluated on Flickr30K in terms of R@1, R@5, and R@10. The \"large-scale retrieval\" means the paradigm could be applicable to large-scale image sets or not. Efficiency is measured by the number of queries it can process per second using an image set containing 0.3 million images. The results are summarized in Table 2, and we gained the following findings."}, {"title": "4.5 Ablation Study (RQ3)", "content": "In AVG, the cross-modal alignment loss is introduced in the training of the image tokenizer to inject semantics into vokens, and the voken embeddings are transformed to expand the token vocabulary. Do the cross-modal alignment and voken embedding work? In the token-to-voken generative training phase, the discriminative loss is used to modify the solely generative training. We conducted ablation studies by evaluating the following variants:\n1) \"w/o cross-modal alignment\u201d. We removed the \\(L_{align}\\) from the image tokenizer's training. 2) \"w/o voken embedding\". While expanding the language model's vocabulary, we randomly initialized embedding for tokens rather than copying the trained voken embeddings. 3) \"w/o discriminative loss\u201d. We eliminated the loss \\(L_{dis}\\) and trained the language model only via \\(L_{gen}\\). By analyzing the"}, {"title": "4.6 In-depth Analysis", "content": "Discretizing images into vokens is a complex task that should not be underestimated. It involves representing a large number of images using a limited number of vokens, which is also could be regarded as a compression problem. For instance, in Flickr30K, there are approximately 30,000 images, and the codebook size M needs to be determined. We reported the performance versus the codebook size M in Table 4."}, {"title": "4.6.1 Analysis on visual codebook", "content": "By analyzing the results, we gained the following findings. When the codebook size N is set to 256, there is a significant drop in performance. This is likely due to the challenge of representing 30,000 images with only 256 vokens. This is supported by the observation that increasing the codebook size N leads to improved performance, as a larger visual codebook provides more flexibility to represent various images. However, when the codebook size increases from 1,024 to 2,048, the performance decreases. We believe that a larger visual codebook may reduce compression difficulty, allowing the tokenizer to find a shortcut to satisfy tokenization loss, but it may not be suitable for subsequent token-to-voken generation."}, {"title": "4.6.2 Analysis on voken length", "content": "In section 4.6.1, we primarily focus on determining the optimal visual codebook size for a collection of images. Similarly, we also address the question of how many vokens should be used to represent an image. To investigate this, we presented the performance of the voken length in Table 5. The results presented in Table 5 indicate that voken length 2 performs the worst compared to other variants, suggesting that only two vokens are insufficient to effectively represent an image's content. An increase in voken length to 4 results in a significant improvement, with voken length 6 achieving a better performance. This is attributed to the ability of more vokens to better capture the image's content. Although the voken length of 8 results in a slight performance decrease, a voken length of 10 consistently achieves the best performance. However, due to resource constraints, we did not further increase the voken length. It is worth mentioning"}, {"title": "4.6.3 Analysis on autoregressive model size", "content": "In the token-to-voken training stage, we expanded the vocabulary of generative language models by adding vokens and continued training the language model with text-image pairs. To evaluate the impact of different language models on performance, we conducted experiments using various backbones, such as T5-base, T5-large, T5-XXL, and LLaMA-7b. We utilized the LoRA [11] technique to fine-tune LLAMA due to limited computing resources. The results are summarized in Table 6, and we obtained the following findings.\nIt has been observed that increasing the model size leads to a certain performance improvement among the T5 series. It is reasonable to expect that a larger language model generally possesses a more powerful semantic understanding ability. In addition to the pretrained knowledge, larger model sizes provide more space"}, {"title": "4.6.4 Analysis on beam search", "content": "AVG relies on beam search to generate a ranked list of images, and it is important to investigate how the size of the beam affects the retrieval metrics R@1, 5, and 10. We summarized the performance of AVG at different beam sizes in Table 7. When using beam search, only the number of sequences smaller than the beam size are returned. Therefore, a beam size of 1 only provides the R@1 result. Our findings show that the greedy search (beam size=1) performs poorly, but performance gradually improves as the beam size increases. This is because a larger beam size reduces the possibility of missing the correct image. The results suggest that a larger beam size is needed to achieve a better R@10, but there is not much difference between beam sizes of 30, 40, or 50. And a larger beam size also reduces efficiency. Therefore, a beam size of 20 or 30 is suitable, considering both effectiveness and efficiency."}, {"title": "4.6.5 Analysis on efficiency", "content": "As we explore a new approach to cross-modal retrieval, it is crucial to consider efficiency as a key evaluation factor. In our study, we conducted a comparison of AVG with CLIP (two-tower) and GRACE (generative) using different image set sizes. Efficiency was measured by query latency, specifically the number of queries the model could process per second. The results are depicted in Figure 4.\nAs the size of the image set increases, it is observed that the efficiency of CLIP decreases. This is because CLIP needs to rank images by matching the textual query with each image in the set. In contrast, AVG and GRACE maintain consistent efficiency regardless of the image size. This demonstrates a significant advantage of the generative paradigm for cross-modal retrieval. Additionally, within the generative paradigm, AVG shows a notable efficiency advantage over GRACE. This is because GRACE relies on a multimodal"}, {"title": "4.6.6 Cases of image tokenization", "content": "We showed cases in Figure 5 to demonstrate the image tokenizer in AVG. It is observed that similar images are assigned similar vokens. For instance, the two images in Figure 5 (a) are assigned completely different vokens, whereas the two images in Figure 5 (b) share the same initial voken. Additionally, we observed that AVG's image tokenizer takes into account the high-level semantics of the image. The two images in Figure 5 (c) depict a kayaker in a boat and share similarities, such as the subject matter. However, there are some differences as detailed in captions, such as the color of the boat and clothes, as described in the caption. As a result, the two images are assigned similar vokens \"659, 566, 629, 227\" and \"659, 566, 629, 653\" but not the same. The two images in Figure 5 (d), despite having some differences in background, color, and action, are assigned the same vokens. This is because the two images have the same caption, and our image tokenizer is able to focus on the high-level semantic content of the images and filter out irrelevant visual information for retrieval."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented a novel autoregressive voken generation method, AVG, designed for the text-to-image retrieval task. Unlike previous discriminative methods that focus on matching text with images, AVG takes a different approach by reformulating text-to-image retrieval as token-to-voken generation. To tackle the unique challenges of cross-modal retrieval, AVG introduces cross-modal alignment image tokenization, which involves discretizing images into vokens while considering both low-level visual information and high-level semantics. Additionally, to address the learning gap in generative training for retrieval targets, AVG incorporates a discriminative loss to adjust the learning direction during the training stage. Our experiments demonstrate that AVG significantly outperforms previous generative paradigms in cross-modal retrieval, thanks to the powerful image tokenizer and modified learning loss of AVG. Furthermore, AVG exhibits superior effectiveness and efficiency, showcasing advantages over both one-tower and two-tower frameworks.\nIn the future, we plan to further explore this topic from the following perspectives. Firstly, AVG involves two separate stages, image tokenization, and generation training, to facilitate generative cross-modal retrieval. However, these two disconnected stages may result in certain drawbacks. For instance, the assigned vokens may not be optimal for the subsequent generative training. One potential solution is to integrate the two training stages, creating a more seamless end-to-end process. Secondly, image tokenization for generative retrieval entails compressing a collection of images into a sequence of vokens using a visual codebook of a specific size. There may be a strict mathematical relationship among these three variables, which could help determine the optimal parameters for different image sets."}]}