{"title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Xuan Shen", "Pu Zhao", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "abstract": "Abstract-Fine-tuning helps large language models (LLM)\nrecover degraded information and enhance task performance.\nAlthough Low-Rank Adaptation (LoRA) is widely used and\neffective for fine-tuning, we have observed that its scaling factor\ncan limit or even reduce performance as the rank size in-\ncreases. To address this issue, we propose RoRA (Rank-adaptive\nReliability Optimization), a simple yet effective method for\noptimizing LoRA's scaling factor. By replacing a/r with \u03b1/\u221ar,\nRORA ensures improved performance as rank size increases.\nMoreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of\naccuracy recovery when fine-tuning pruned models. Extensive\nexperiments demonstrate the effectiveness of RoRA in fine-tuning\nboth uncompressed and pruned models. RoRA surpasses\nthe state-of-the-art (SOTA) in average accuracy and robustness\non LLAMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically\noutperforming LoRA and DoRA by 6.5% and 2.9% on LLAMA-7B, respectively. In pruned model fine-tuning, RoRA shows\nsignificant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4% pruning, RoRA achieves 5.7% higher average\naccuracy than LoRA and 3.9% higher than DoRA.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) are typically trained on\nbroad datasets during pretraining, which enables the model\nto develop general language understanding capabilities. Fine-\ntuning allows the model to perform better on specific tasks\nor domains. For example, fine-tuning can help the model\nbetter handle text in specialized areas such as healthcare\nor law. Fine-tuning helps reduce biases and generate more\nrelevant and natural text. Moreover, Large-scale deep learning\nmodels [1], [2], [3], [4], [5], [6], [7], [8] which often\ncomprise billions or even hundreds of billions of parame-\nters, face limitations in deployment on resource-constrained\ndevices [9], [10], [11], [12], [13], [14], such as mobile phones.\nPruning techniques are commonly applied to address these\nchallenges by reducing model size and computational overhead\nwhile maintaining performance [15], [16], [17]. Parameter-\nEfficient Fine-Tuning (PEFT) [18] has also gained promi-\nnence as a strategy to reduce the high computational cost\nof full model fine-tuning. This method allows for efficient\ntask-specific [19], [20] fine-tuning of large language models\n(LLMs) without the need to retrain all parameters.\nOur goal is to maximize fine-tuning performance under\nresource constraints. Fully fine-tuning is costly, and Low-\nRank Adaptation (LoRA) [21] offers an efficient Parameter-\nEfficient Fine-Tuning (PEFT) approach for language and vi-\nsion [22], [23], [24], [25], [26] models. The rank r determines\nthe dimensionality of low-rank weight updates, balancing\nresource efficiency and performance, with LoRA suggests that\nincreasing the rank does not necessarily enhance subspace. We\nobserved that the performance of both LoRA and its improved\nSOTA version, Weight-Decomposed Low-Rank Adaptation\n(DoRA) [27], declines beyond r = 32, consuming more GPU\nwithout gains, as shown by the blue and green line [27] in\nFig. 1.\nTo address this problem, we propose a method that uses\nan optimized scaling factor (OpS) \u03b1/\u221ar for fine-tuning both\nuncompressed and pruned [28], [29], [30], [31] [32], [33]\nLLMs. While rsLoRA [34] employs a similar scaling factor to\nstudy the impact of the scaling factor on the learning process,\nour motivation, theoretical derivation, and experimental design\nare independent. This scaling factor mitigates the impact\nof rank, ensuring that gradient updates remain independent\nof rank. Our proposed method, Reliability Optimization for"}, {"title": "II. BACKGROUND AND PROBLEM FORMULATION", "content": "Pre-trained models like LLaMA [35] possess broad linguis-\ntic knowledge but may lack domain [36], [37], [38] special-\nization. Users often fine-tune them with task-specific data to\nenhance performance while maintaining overall language.\nLORA [21] is an efficient and widely used fine-tuning\nmethod. In LoRA, the update of $m_o$ is constrained by a\nlow-rank decomposition: $m_o + \u2206m = m_o + YBA$, where\n$B \u2208 R^{P_{out}Xr}$, $A \u2208 R^{r\u00d7P_{in}}$, and \u03b3 is the scaling factor. During\ntraining, $m_o$ remains fixed and does not receive gradient\nupdates, while B and A are trainable parameters. The forward\npass is given by:\n$H(x) = m_o x + \u2206mx = (m_o + \u03b3BA)x$,\nwhere $x \u2208 R^{P_{in}}$ is the input, $H(x) \u2208 R^{P_{out}}$ is the output, $m_o \u2208$\n$R^{P_{out} P_{in}}$. The scaling factor \u03b3 is set to a/r in Equation (1)."}, {"title": "III. THE PROPOSED METHOD", "content": "To address the accuracy drop with increasing LoRA rank\nr, we analyzed the relationship between gradient variance and\nrank r. We optimized the scaling factor from a/r to \u03b1/\u221ar to\nensure that the gradient variance remains unaffected by rank r.\nThis optimized scaling factor is defined as OpS (Optimization\nScaling) and the method is referred to as Reliability Optimiza-\ntion for Rank Adaptation (RORA)."}, {"title": "A. Mathematics Analysis for the Weight Variance", "content": "The relationship between the gradient and its variance\nis critical: the gradient indicates the loss function's rate of\nchange, while the variance reflects stability. We analyze how\nrank r affects both using mathematical techniques.\nMathematics Analysis. We represent the increment part of\nthe output H in eq. (1) by $w\u2208 R^{P_{out}}$, $x \u2208 R^{P_{in}}$ is the input,\nand replace the scaling factor a/r with \u03b3. Each term $w_i$ can\nbe expressed as:\n$w_i = \\sum_{j=1}^{P_{in}}\\sum_{k=1}^{r} B_{ik}A_{kj}x_j$.\nUsing the chain rule to compute the partial derivatives of\nthe loss function L with respect to $B_{ik}$ and $A_{kj}$, we have\n$\\frac{\u2202L}{\u2202B_{ik}} = \u03b3\\frac{\u2202L}{\u2202w_i}\\frac{\u2202w_i}{\u2202B_{ik}} = \u03b3\\frac{\u2202L}{\u2202w_i}A_{kj}x_j$, and $\\frac{\u2202L}{\u2202A_{kj}} = \u03b3\\frac{\u2202L}{\u2202w_i}\\frac{\u2202w_i}{\u2202A_{kj}} = \u03b3\\frac{\u2202L}{\u2202w_i}B_{ik}x_j$.\nLORA [21] sets the learning rate \u03b7 and initializes $B_{ik} = 0$ [39],\n[40], a common optimization assumption to analyze early\ntraining and parameter scaling effects. After the first step\nupdate, $A_{kj}$ remains unchanged, and $B_{ik}$ is updated as:\n$B_{ik}^{(t+1)} = B_{ik}^{(t)} - \u03b7\\frac{\u2202L}{\u2202B_{ik}} = B_{ik}^{(t)} - \u03b7\u03b3 \\sum_{j=1}^{P_{in}}\\frac{\u2202L}{\u2202w_i}A_{kj}^{(t)}x_j$,\nwhere $B_{ik}^{(t)}$ represents the before updated value of $B_{ik}$, and\n$B_{ik}^{(t+1)}$ represents its updated value after the previous step.\nSubstituting Equation (3) into Equation (2), and replacing\n$\u2202L/\u2202w_i$ with $\u03b4_i$, we have:\n$w_i^{(t+1)} = -\u03b7\u03b4_i\u03b3^2\\sum_{j=1}^{P_{in}}\\sum_{k=1}^{r}\\sum_{l=1}^{P_{in}}A_{kl}x_l^{(t)} A_{kj}x_j^{(t+1)}$.\nAssuming that $\u03b4_i$ is bounded and independent of r, and\nthat the elements in A and the inputs $x_i^{(t)}$ and $x_i^{(t+1)}$ are\nindependently and identically distributed normal variables with\nmean 0 and variance 1, the variance of $w_i^{(t+1)}$ is:\n$Var[w_i^{(t+1)}] = E[(w_i^{(t+1)})^2] \u2013 (E[w_i^{(t+1)}])^2$.\nSince the elements in the matrix A and the two inputs\n$x_i^{(t)}$ and $x_i^{(t+1)}$ are assumed to be independently, the expected\nvalue of $w_i^{(t+1)}$ is:\n$E[w_i^{(t+1)}] = -\u03b7\u03b4_i\u03b3^2\\sum_{j=1}^{P_{in}}\\sum_{k=1}^{r}\\sum_{l=1}^{P_{in}}E[A_{kl}]E[x_l^{(t)}]E[A_{kj}]E[x_j^{(t+1)}]$.\nSince the expected values of A and x are both zero, the\nexpected value of $w_i^{(t+1)}$ is 0. The variance is equal to the\nexpected value squared:\n$Var[w_i^{(t+1)}] = E[(w_i^{(t+1)})^2]$.\nTherefore, we need to compute $E[(w_i^{(t+1)})^2]$. Substituting\nthe expression for $w_i^{(t+1)}$ and using the linearity of expecta-\ntions have:\n$E[(w_i^{(t+1)})^2] = \u03b7^2\u03b4_i^2\u03b3^4\\sum_{j=1}^{P_{in}}\\sum_{k=1}^{r}\\sum_{l=1}^{P_{in}}E[(A_{kl})^2]E[(x_l^{(t)})^2]E[(A_{kj})^2]E[(x_j^{(t+1)})^2]$.\nSince the variance of A and x is 1, we have:$E[A^2_{kl}] =$\n$E[A^2_{kj}] = 1$, $E[(x_l^{(t)})^2] = E[(x_j^{(t+1)})^2] = 1$, thus, we derived\nthe expression:\n$Var[w_i^{(t+1)}] = E[(w_i^{(t+1)})^2 ] = \u03b7^2\u03b4_i^2\u03b3^4r^2P_{in}.$"}, {"title": "B. Reliability Optimization for Rank Adaptation", "content": "By substituting the scaling factor \u03b3 = a/r, which suggested\nby [21], we can obtain:\n$||W||_2 \u2248 c \u00b7 \u03b1^2 \u00b7 O_r(1/r)$,\nwhere $O_r(1/r)$ means that as r increases, its magnitude is\napproximately around 1/r. The formula shows that the training\noutput increment H in Equation (1) slows down as rank r\nincreases. If replace a/r with \u03b1/\u221ar, The following can be\nobtained:\n$||W||_2 \u2248 c \u00b7 \u03b1^2 \u00b7 O_r(1)$."}, {"title": "C. Comparison with LoRA", "content": "In Fig. 2, RoRA distinguishes itself from LoRA by thor-\noughly examining gradient variance. Our analysis shows that\nthe rank r in LoRA can cause gradient instability. Using the\noptimization scaling factor (OpS) \u03b1/\u221ar, we ensure that rank\ndoes not affect gradient changes, effectively mitigating this\nproblem."}, {"title": "IV. EXPERIMENTS", "content": "The performance of RoRA was evaluated on LLaMA-\n7B/13B [35], LLaMA2-7B, LLaMA3-8B [41], and the pruned\nmodel SHEARED-LLAMA-1.3B [5] on commonsense rea-\nsoning tasks using one NVIDIA A6000 48G GPU. RORA\nwas compared with LoRA [21], DORA [27], and various\nbaselines, including Prompt Learning (Prefix) [42], Series\nAdapter [43], and Parallel Adapter [44]. The commonsense\nreasoning tasks included BoolQ [45], PIQA [46], SIQA [47],\nHellaSwag [48], WinoGrande [49], ARC-e [50], ARC-c [50],\nand OBQA [51].\nTable I shows that RoRA average accuracy outperforms all\nbaselines on LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-\n8B. In commonsense reasoning tasks with ranks r from 4 to\n128, RORA steadily improves, peaking at 81.3% accuracy at\nrank 128, surpassing LoRA (74.7%) and DoRA (78.4%) by\n6.5% and 2.9%, respectively (Fig. 1). We also tested r =\n256 on LLaMA-7B, LORA and DORA drop 2%, while RORA\nimproves by 0.3%, confirming its advantage as r approaches fine-tuned it using RoRA, DORA, and LoRA. Table 2 shows\nfull fine-tuning with diminishing returns.\nThe loss curve in Fig. 3 illustrates the performance of three\nfine-tuning methods applied to LLaMA-7B, specifically with a\nrank of 128. RoRA shows a rapid initial drop in loss, followed\nby a steady decrease after step 60. Notably, RoRA achieves the\nlowest loss among all methods. On a single NVIDIA A6000\nGPU, LORA and RoRA require about 3h 45m for Rank r=8,\nDORA takes 5h 35m, and all methods add 6m for Rank r=128,\nwith inference latency about 29s.\nFine-tuning pruned models is more challenging than un-\npruned ones due to information loss and reduced flexibility,\nmaking hyperparameter tuning crucial. Comparing RoRA,\nDORA, and LoRA in fine-tuning pruned models highlights\ntheir effectiveness in addressing these challenges. ShearedL-\nLaMA [5] 1.3B is a pruned version of LLaMA2-7B, with an\n81.4% pruning rate, reducing it to 1.3 billion parameters. We\nthat LoRA and DoRA perform best at rank 32, while RORA\npeaks at rank 128, achieving 3.9% higher performance than\nDORA and 5.7% higher than LoRA at their optimal ranks.\nThis demonstrates RoRA's significant advantage in fine-tuning\npruned models compared to LoRA and DoRA."}, {"title": "V. CONCLUSION", "content": "We introduce RoRA (Rank-adaptive Reliability Optimiza-\ntion), a sample yet effective method for optimizing the scaling\nfactor in LoRA. By substituting a/r with \u03b1/\u221ar, RoRA\nimproves performance as rank size increases, enhancing the\nsubspace of low-rank adaptation matrices. This approach ex-\ncels in fine-tuning both uncompressed and pruned models.\nThrough extensive experiments, RoRA demonstrates effec-\ntiveness, achieving superior average accuracy and robustness\ncompared to current state-of-the-art methods."}]}