{"title": "Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services", "authors": ["Ali Doosthosseini", "Jonathan Decker", "Hendrik Nolte", "Julian M. Kunkel"], "abstract": "The increasing adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open-source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of AI models on HPC systems. By offering a web service using our HPC infrastructure to host LLMs, we leverage the trusted environment of local universities and research centers to offer a private and secure alternative to commercial LLM services. Our solution natively integrates with Slurm, enabling seamless deployment on HPC clusters and is able to run side by side with regular Slurm workloads, while utilizing gaps in the schedule created by Slurm. In order to ensure the security of the HPC system, we use the SSH ForceCommand directive to construct a robust circuit breaker, which prevents successful attacks on the web-facing server from affecting the cluster. We have successfully deployed our system as a production service, and made the source code available at https://github.com/gwdg/chat-ai", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have sparked an adoption of AI systems in a wide range of applications. This puts many institutions and companies in the difficult position of having to come up with a strategy to adopt these new technologies and deciding to what extent their data can be handled by (potentially foreign) companies such as OpenAI and Google or alternatively attempt to self-host an LLM service in order to preserve the privacy of user data.\nHigh-quality LLM models have recently become widely available due to many companies and institutions providing free access to their model weights, commonly via Hugging Face.\nUsing a runtime such as llama.cpp, vLLM or Text-Generation-Inference one can run smaller versions of these models, e.g., 7B or 8B parameters, on high-end consumer GPUs with 10 to 20GB of VRAM. However, for larger models such as Llama3-70B and Qwen2-72B that more closely rival the performance of state-of-the-art closed source models such as OpenAI's ChatGPT4, significant investment into GPU resources is required.\nIn our testing we found that a single instance of Meta's LLama3-70B model fits into the VRAM of two of NVIDIA's H100 GPUs when using FP8 quantization via VLLM 0.5.0 or even a single H100 GPU when significantly reducing the context size. While this allows effective inference of LLama3-70B and models of similar size, it limits access to private inference of these models to those who can afford H100 GPUs or similarly powerful hardware.\nMany research centers, universities and institutions either operate their own HPC infrastructure or have access to HPC infrastructure via a research network. Typically, HPC centers operate a batch-scheduler such as Slurm and put strong emphasis on the security of their systems.\nHPC infrastructure is an attractive target for attackers to steal research data, interrupt operationability or hijack compute resources for crypto mining. To ensure system security, on HPC multi-user systems, admins often rely on battle-tested security mechanisms such as POSIX permissions and SSH for access management. This significantly restricts the possibility of using HPC resources to expose services without compromising these security frameworks.\nFurthermore, a batch-scheduler such as Slurm requires users to submit jobs, which it then schedules at a suitable time to one or more nodes depending on the expected job duration, resource demands and availability of resources. Considering that a user of an HPC service, e.g. chatting with an LLM, would expect their request to be processed"}, {"title": "1.1 Outline", "content": "In Section 2 we outline the background and motivation. In Section 3 we explore related work. In Section 4 we share our challenges and solutions. In Section 5 we describe our architecture in more detail. In Section 6 we evaluate our service with regards to security, performance and explore its adoption by users and institutions, followed by a discussion of the limitations and future prospects in Section 7. Finally, we summarize our contributions in Section 8 and offer our source code and acknowledgements in Section 9."}, {"title": "2 Background", "content": "In this section we describe the existing tools and paradigms that are relevant to our approach.\nSlurm The Simple Linux Utility for Resource Management, or short Slurm , is a workload or resource manager for HPC clusters. It offers a solution for running batch jobs on a cluster through the use of shell scripts. Slurm assigns jobs to nodes depending on the availability of the current nodes and requirements of the job, ensuring that the resources of the cluster are utilized efficiently.\nScheduling Paradigm When assigning a job that requests a number of resources such as multiple nodes, Slurm will only assign and start a job when all resources are available at once for the requested duration. This is commonly referred to as gang scheduling. While it is desirable for a compute job such as an MPI job that can only start when all nodes are available to use gang scheduling, it is a different case for services. For a service that is supposed to serve user requests, multiple instances might be required to sufficiently serve all requests but if not enough resources are available, the service could still operate with reduced performance and dynamically add instances when more resources become available. For one-time jobs that are not time-sensitive, Slurm is a suitable choice, however, one would have to adapt their resources to a service paradigm.\nvLLM vLLM is a state-of-the-art, open source LLM runtime implementation. It uses an attention mechanism inspired by memory paging techniques in operating systems, achieving near-zero waste in key-value cache memory. Since its first release in"}, {"title": "3 Related Work", "content": "Providing access to LLMs is in itself a service offering, and running such services in a production environment is commonly associated with existing frameworks such as Kubernetes. However, most HPC infrastructures rely on batch-scheduling systems such as Slurm, Torque or HTCondor as their main resource manager. Moreover, modern LLMs require so much compute power not only for training but also for inference that an inference service would benefit from access to powerful hardware such as high-end GPUs installed in HPC infrastructure.\nIn order to use these HPC resources, various approaches exist.\n\u2022 Zhang et al. proposed a unified framework for Apache Hadoop YARN and Slurm to create a mobile AI service platform.\n\u2022 Wan et al. proposed a Slurm-based service platform to optimize startup times and storage usage for multi-user applications on HPC infrastructure.\n\u2022 Shainer et al. proposed a service architecture with a focus on supporting specialized HPC hardware such as DPUs and Infiniband.\n\u2022 Zhou et al. proposed an architecture that utilizes both Torque to run compute jobs submitted via an interface running on a Kubernetes cluster.\n\u2022 Metje proposed an approach and studied various existing approaches for running Kubernetes cluster on top of Slurm without relying on root access.\nOn a given HPC system using Slurm as its resource manager, trying to employ some of these approaches would require replacing Slurm with another system such as Kubernetes. This would disrupt existing users and workflows of such a system and force users to migrate to another system. By keeping the existing resource manager, the inference service could operate side by side with user workloads and utilize gaps in the Slurm scheduling . While this approach has the potential to greatly improve the resource utilization of the HPC hardware, for example by serving inference from gaps the in the scheduling besides training jobs, this can also result in a reduced user experience for users of the inference service. Especially when serving large LLMs, long start-up times can be expected when loading these models into memory. Where serverless functions commonly take only seconds to load , loading an LLM can take multiple minutes, which could be the time a user needs to wait for a response to their inference request if no instance of a desired model was ready. This issue becomes especially pressing when considering the high need for GPU resources from LLM inference and training workloads, which could result in training jobs consuming all GPUs in a cluster and leaving no resources for inference services or vice versa.\nAnother major aspect of running services on HPC systems is security as sensitive research data and valuable compute resources are concentrated in HPC centers. A common access protocol for HPC clusters is SSH. SSH itself allows for creating a remote terminal but can also be used to forward arbitrary ports or commands to allow remote access through it . Using SSH to tunnel traffic to create a web service with a secure connection to a backend running on HPC infrastructure is an approach for creating a HPC-based inference platform, but storing an SSH key in an exposed server could pose a security risk and incentivize bad actors to penetrate the server in order to gain access to the HPC cluster, and thus may not be allowed in HPC centers with strict security standards such as SSH access only from individual users on local devices.\nOne should also consider the possibilities for the usage of LLMs in education as many HPC centers are closely connected to universities and could offer inference capabilities to students as an alternative to commercial offerings.\nBesides the efforts described in this paper towards a private LLM inference, other research centers also made efforts to develop a private chat interface including FhGenie, HAWKI and Minverva Messenger. Their approaches to privacy are mainly to implement user management on their side and to use a single OpenAI API key to send all requests to ChatGPT in order to obfuscate the attribution of a given request to a specific user, essentially acting as a wrapper or middleman for ChatGPT. The privacy provided by this approach is, however, limited as all requests and responses are still send to a third party, e.g., OpenAI or Microsoft Azure. Support for alternative endpoints via self-hosted backends that provide an OpenAI-compatible AI, which is in development for HAWKI and FhGenie, could provide a fully private solution."}, {"title": "4 Challenges", "content": "In this section we present the challenges and concerns of hosting web services on an HPC infrastructure that employs Slurm as its resource manager.\nParadigm Differences The purpose of HPC clusters is commonly to run large-scale and demanding batch jobs with a fixed start and end point, but not continuous and scalable web-services. The batch job and service paradigms are strictly different from one another and have different expectations and goals. Slurm is designed to handle the batch job paradigm and will schedule incoming jobs according to it. In order to adapt it to the service paradigm, a job that runs an instance of a service must continuously be replaced or extended. This requires some other scheduling control flow to operate on top of Slurm to implement this functionality, e.g., keeping a pool of one or more instances active for a given service by ensuring that one or more jobs are active for that service at a given time.\nSecurity From a security perspective, hosting a web service on an HPC cluster opens up additional attack surfaces. An HPC cluster is a valuable target for an attacker as it offers massive amounts of compute resources, which could be misappropriated for crypto mining and are hosts for potentially sensitive research data.\nAs a web service has a much wider audience than an HPC cluster, it may be desirable to make it accessible from the internet, requiring some components of the service to be exposed from behind the firewalls of a given HPC center. SSH in particular is a commonly allowed access protocol and in some cases is the sole option to access the necessary HPC components, thereby forcing access patterns to build on top of SSH to remain compatible with the existing security framework. Moreover, as an SSH connection is established using cryptographic keys, which would need to be stored on a potentially exposed machine in order to connect to the HPC backend, extra caution must be taken to ensure the damage caused by a potential compromise of this key is limited, such that it cannot be used as an entry point to the HPC cluster and its compute power by an attacker.\nPerformance HPC infrastructure commonly includes specialized hardware such as HPC interconnects, e.g., Infiniband, and parallel file systems to optimize the performance of batch jobs. While HPC hardware and Ethernet interconnects are normally powerful enough to support most services, when pushing the performance especially with the workarounds that have been placed in order to adapt the paradigm, the performance may be reduced compared to an infrastructure tailored for running production services. Most notably, performance differences can be expected in throughput and latency.\nScalability Slurm does not inherently provide an option to scale up the number of jobs as the number of requests increase, requiring additional efforts to implement automatic scaling. In order to support a service where request volumes vary over time, it is necessary to support automatic scaling, where the number of instances for a given service is adjusted based on request volume. This requires the implementation of a functionality to measure the current request volume as well as to calculate and enforce the desired amount of service jobs. Moreover, this also introduces the problem of service discovery, as incoming"}, {"title": "Reliability", "content": "Slurm itself is robust against nodes failing by ignoring problematic nodes and assigning jobs to other nodes until an administrator fixes the issue. However, Slurm does not implement mechanisms to ensure constant availability such as health checks, which are crucial for a service to be reliably accessible and available at all times. These mechanisms would therefore need to be implemented on top of Slurm."}, {"title": "Privacy", "content": "For a service that offers an interface to chat with LLMs, essentially receiving user inputs and responding with outputs generated by LLMs that process these conversations, one can consider all user inputs and responses to be sensitive user data that must be kept private, thereby limiting the options for storing user data. Some approaches include using cryptography and key management mechanisms to keep user data encrypted and inaccessible while it is not used directly.\nHowever, in the context of highly sensitive information such as medical data, privacy is critical and should be prioritized, as leakage of such data could potentially be catastrophic. Depending on the type of service, stricter privacy requirements can be achieved by enforcing the use of encryption keys for users to be able to download and decrypt their data. The other option, which is feasible and arguably even more secure, is to never store sensitive user data, such as past and ongoing conversations with LLMs, on the server side and instead store all such data only in the local device, for example in the browser's cache.\nAnother consideration to data privacy is the encryption of data in transit in order to prevent a scenario where an attacker performs man-in-the-middle attacks after compromising part of the service to read user inputs and responses from the services.\nIt is possible to implement end-to-end-encryption and to keep data encrypted during computation using Trusted-Execution-Environments, which are currently available for NVIDIA GPUs. These additional measurements can be considered for system hardening but go beyond our basic requirements, as these serve to protect incoming data while parts of the system have already been compromised. Nevertheless, when following defense-in-depth security paradigms, these additional measurements are well worth pursuing."}, {"title": "Monitoring and Accounting", "content": "When submitting jobs via Slurm, it is able to perform accounting based on the compute hours used by a given job and attribute them to the user who submitted the job. However, when offering a service, it is to be expected that multiple users will be served by the same job instance.\nFurthermore, it can also be expected that many service users do not have access privileges to the HPC systems, such that the service user management is independent from the HPC center. Therefore, the service jobs should be submitted via a functional account that has no correspondence to an individual user or administrator. User-based monitoring, accounting and moderation instead has to be handled in an authentication layer in front of the service or integrated within the service itself. Therefore, individual requests can be attributed to logged-in user accounts, and the timestamp for a given request can be stored in order to monitor the service usage without violating the privacy assumptions from the previous paragraph."}, {"title": "5 Chat AI Architecture", "content": "The architecture of Chat AI is shown in Figure 1 with its two major components, namely the web server labeled as ESX Server and the HPC infrastructure labeled as KISSKI Platform. Fundamentally, the HPC platform is only expected to provide Slurm itself.\nEach of the two major components is split up into further sub-components, which are individually explained in the following. It should also be noted that all sub-components running on the ESX Server have been containerized and operate via docker compose."}, {"title": "5.1 Authentication", "content": "We employ an Apache webserver as a reverse proxy for our web interface to provide integration with our Single Sign-On (SSO) provider Academiccloud. This is effectively an OAuth2 authentication mechanism using the Apache webserver module OpenIDC and could be used via any other SSO provider that implements the same protocols.\nOnce a user has signed into the SSO provider, they are forwarded through the API Gateway to the Chat AI web interface. Furthermore, when forwarding the requests, the Apache webserver attaches the email address of a given user as the user id to all requests."}, {"title": "5.2 API Gateway", "content": "We deployed the Kong Open Source edition as the API gateway to route incoming requests to the corresponding upstreams. Kong was chosen as it is open source and provides all the features we require from an API gateway including creation and management of API routes, rate limiting, API key management and observability.\nContinuing from the Apache webserver authentication component, a user request is forwarded by the Kong API Gateway to the Chat AI web interface via its respective route. If a user then submits an inference request to an LLM via the web interface, the request is sent through the API Gateway to the corresponding route depending on the selected model,\nMoreover, the API Gateway also exposes an access point for API users. Users with valid API keys can bypass the Apache webserver and connect to the API Gateway directly, which performs authentication via the provided API key. This unifies the path for web and API users past the API gateway such that the incoming route and user is abstracted from the backend service."}, {"title": "5.3 Web Interface", "content": "The web interface illustrated in Figure 2 serves as a user-friendly way for users to interact with a selection of LLMs. We chose to implement our own web interface instead of reusing existing open source software such as Text-Generation-Webui or LibreChat in order to have full control over the handling of user data as well as the routing of requests.\nInitially, we built the web interface using gradio but this resulted in significant load on our server for large user numbers. Therefore, we decided to rewrite the web interface using React and Vite, which is the design that is shown in Figure 2, significantly reducing the load on our server."}, {"title": "5.4 HPC Proxy", "content": "To enable the submission of inference requests from the web server to the HPC cluster without compromising its tight security, we wrap this communication in an SSH connection. SSH is the only protocol allowed to access the login node of our HPC center, labelled as Service Node in Figure 1, and automatic logins via SSH to user accounts are also forbidden.\nStoring an SSH key on the web server which could potentially be used by an attacker to gain access on the HPC cluster was only allowed by using the ForceCommand directive of SSH. This directive is used to restrict an SSH key to a specific command such that it cannot be used to run generic shell commands on the remote machine. Moreover, as this is configured via the authorized_keys file in a user's directory, it can be configured without root access. In our case, the SSH key belongs to a functional account which is also responsible for the submission of service jobs to Slurm.\nWe have developed the HPC Proxy as an application that ensures that the SSH connection between itself and the HPC login node remains open and is reestablished in case an interruption breaks the SSH connection. Interruptions are detected by attempting to send keep alive pings once every 5 seconds or so. Besides that, the HPC Proxy serves as a regular proxy that forwards requests to a specific custom script running on the HPC service node. This script is designed to forward authorized inference-related HTTP requests to a service node running the desired model and ready to respond. Additional steps were taken to ensure the proxy is fast, efficient and generalizable with support for various types of requests and responses, including streaming.\nIn Figure 1 we included two HPC Proxy instances, as this architecture decouples the web server from the HPC platform, allowing a single web server to potentially utilize multiple HPC platforms by starting an HPC Proxy instance for each of them and load balancing via the API Gateway. On the other hand, it is also possible to have multiple web servers utilize the same HPC platform, each with its own individual SSH connection. With this configuration, we are able to provide custom web services that require HPC resources without directly exposing the components in the cluster."}, {"title": "5.5 Scheduler", "content": "Previously, the role of the HPC Proxy was described in establishing and maintaining an SSH connection to the HPC login node, such that a custom script is called for every valid incoming request to route it to a healthy service instance. We also developed a scheduler script, which is run periodically on every incoming keep alive ping from the HPC Proxy, with the goal of ensuring the availability of the service instances.\nThe scheduler script can be configured with a set of services it should maintain along with the specifics of running their respective jobs, such as the job script and scalability factor. The scheduler script obtains a list of running jobs for each service from Slurm via the squeue command, compares it against the given configuration, and takes necessary action to maintain the services, such as submitting new jobs via sbatch. In order to avoid race conditions, we ensure only one instance of the scheduler is running at a time by means of a lock file.\nThe scheduler script maintains a routing table, containing an entry for each active job, along with its associated service, node and port number. This table is used to route each incoming request to a specific node and port number associated with a service instance, selected randomly out of all eligible instances, effectively performing random load balancing.\nAs Slurm provides no network virtualization, if two jobs on the same node try to occupy the same port, it will fail for the second job as the port is already occupied by the first job. To avoid this, the scheduler script picks a random port when submitting a new job for that job to use and checks the routing table to ensure that the port is not already occupied.\nOnce a service job has been submitted, it takes an uncertain amount of time before the instance is ready to serve requests, as Slurm needs to assign it to a node and the model may need to be loaded into GPU memory. Therefore, the scheduler script periodically probes the newly submitted jobs until they are ready, before marking them as ready to serve requests in the routing table.\nTo scale up and down the number of service instances according to demand, calculating the request volume for a given service is necessary. This is performed by storing the average number of concurrent requests to each service within a predefined time window, which is then recalculated and updated on each scheduling run. If this average is higher than a certain threshold, the scheduler spawns multiple instances of that service to prevent the formation of a backlog. Likewise, when the average is too low, the scheduler allows the excess jobs to expire without resubmitting them, effectively reducing the number of instances and freeing up resources.\nIn an alternative design, the request volume could be measured by the API Gateway or HPC Proxy and forwarded to the scheduler script, but we decided against this option in order to minimize the coupling between the web server and the HPC platform, and instead pursued a solution to calculate the request volume directly on the HPC platform."}, {"title": "5.6 LLM Server", "content": "As Chat AI is an LLM chat service, we also require a runtime to serve LLM inference requests. For that purpose, one can assume the individual services mentioned in the previous subsection to refer to various LLMs hosted on the HPC infrastructure, but can also be any server runtime that benefits from HPC resources.\nTo run an LLM model with GPU acceleration, we employ the open source runtime VLLM. VLLM has the advantages of offering an OpenAI-compatible API and an active community to quickly implement support for many popular open source LLMs. From our experience, vLLM was several times more efficient than our unoptimized LLM runtime implementation."}, {"title": "5.7 External Proxy", "content": "The External Proxy serves as an optional extension to the architecture to provide access to external models such as ChatGPT4 as an additional route in the API Gateway, effectively becoming a wrapper for additional services. As ChatGPT4 requires paid access, we placed strict rate limits via the API Gateway and restricted access to certain user groups."}, {"title": "5.8 Monitoring", "content": "The monitoring components are not shown on Figure 1 as they are optional but nevertheless important for capturing metrics and detecting issues. We employ an external Grafana service, which constantly captures logging and monitoring data from a Prometheus server that is integrated into the API gateway via a plugin. This is performed via standardized monitoring endpoints and ensures only authorized access to the monitoring system."}, {"title": "6 Evaluation", "content": "Our primary goals for the LLM web service were to ensure its privacy, security and practicality. Due to the nature of our infrastructure and demands of our users, security and privacy took the highest priority in our design. In this section, we evaluate the architecture for Chat AI based on the achieved standards for security and data privacy as well as performance measurements regarding throughput and latency. Furthermore, we assess its user adoption as a factor in determining the architecture's real-world viability and overall success."}, {"title": "6.1 Security", "content": "In order to evaluate the security of our architecture we assume different attack scenarios. These should serve to visualize that our designs follow the principals of defense-in-depth, minimizing permission and not storing anything that does not need to be stored."}, {"title": "6.1.1 Security of the Web Interface", "content": "Our Chat AI webinterface employs standard security mechanism via TLS and in order to access it a user must have a valid account and login to authenticate with the SSO provider.\nAssuming a breach in the webinterface, which gives an attacker a shell in the webinterface container, this would allow the attacker to spy on users of the webinterface and set up man-in-the-middle attacks until the breach is discovered and patched. However, as no user data is stored, no immediate data can be stolen by an attacker.\nAdministrative tasks on the web server are handled via an SSH connection, which is only possible from the company internal VPN when utilizing two-factor authentication.\nDue to the containerized nature of the setup, an attacker would need to escape the container to gain a shell on the host system and then gain root access in order to compromise other parts of the system. Even in this scenario, an attacker would not be able to use the compromised web server as an entry point into other parts of the HPC infrastructure."}, {"title": "6.1.2 Security of the HPC System", "content": "In our view, compromising the HPC system would be the worst case scenario that needs to be prevented at all costs. As users do not have direct access to the HPC infrastructure in this architecture, any attacks on the HPC infrastructure would necessarily originate from the web server, meaning that some component within the web server would have to be already compromised. In this case an attacker may attempt to locate and read the SSH key used to establish a connection to the HPC system. However, as the SSH key is configured with ForceCommand and corresponds to a functional account with no administrative permissions on the cluster, its effectiveness in penetrating the HPC platform is significantly reduced.\nA potential alternative to the attack scenario above would be if an attacker discovers flaws in the custom scripts defined as the ForceCommand, and attempts to perform injection attacks by submitting requests that instead of being routed to a service, end up executing shell commands. For this purpose we bring extra attention to the implementation of the input parsing in the custom script to protect against injection attacks, restricting any request to follow a preset of determined paths, and avoiding any potentially dangerous commands such as eval.\nEssentially, an attacker would need to breach multiple layers of security to first gain a significant foothold on the web server, and then breach multiple layers again to get some form of shell access on the HPC system. Even if these security layers fail, the previous conversations and messages of our users physically cannot be stolen from us as they were never stored in the first place. While it is impossible to claim that a system's security is perfect, we consider having multiple layers that an attacker would need to breach as a marker of an architecture's security."}, {"title": "6.2 Data Privacy", "content": "As for data privacy, we regard compliance with GDPR and the strict data privacy measures it necessitates as our main benchmark. Sensitive and personalized data must be transmitted and stored securely to ensure that third parties cannot access it. We take privacy a step further and minimized the data collection according to GDPR article 5(1) lit. c to the degree, that no user prompts are being stored at any circumstance, i.e., prompts and responses, are not stored on the server. To still provide a stateful conversation, the conversation history is solely stored within the user's browser, which can simply be deleted with a button provided in the web interface or by resetting their browser history.\nIt follows that conversational data is also not used for any purpose such as training models. Some non-conversational usage statistics are gathered solely for monitoring and accounting purposes. This includes the user's account identifier, timestamps for requests, and selected model. With this information, we can monitor the load on the individual models and identify potential misbehaving users.\nAs a result of these measures, an attacker on the web interface or on the HPC system cannot access past conversations as they are only kept locally on the users' devices and never stored on the server."}, {"title": "6.3 Performance", "content": "For our architecture to be able to support increasing numbers of users, we need to assess its performance. To evaluate the effectiveness of incorporating the HPC cluster as the backbone of our service, we conducted a series of tests. These tests measured the throughput and latency of each component in the architecture and can be used to identify bottlenecks in the pipeline. Latency measurements were performed using a custom UNIX shell script, and for throughput measurements we employed the Load testing framework Locust."}, {"title": "6.3.1 Latency", "content": "The latency or response time is the amount of time it takes for a client to receive a response to a request, i.e., the duration between the time a request is sent and the point in time when the response is received. To identify potential bottlenecks, we performed latency measurements for every individual component in the architecture on the path for user requests and calculated the time spent in each step. For statistical validity, we took the average over 50 measurements with identical conditions.\nThe setup that the tests were performed on consists of a VM web server running Ubuntu 22.04.4 LTS with a 16-core AMD EPYC processor and 8GB of RAM, and a Slurm-based HPC cluster consisting of one login node and 10 GPU nodes, each with 4 Nvidia H100/80GB GPUs and a 52-core Intel(R) Xeon(R) Platinum 8470 processor with 500GB of RAM running Rocky Linux 9.2.\nUnder normal conditions when the system is not overloaded, a user can expect to receive their first response token after approximately 50ms, of which more than 27ms are the compute time of the underlying LLM. Therefore, the additional latency overhead introduced by our architecture was approximately 23ms, which is not noticeable in most cases and can be considered acceptable."}, {"title": "6.3.2 Throughput", "content": "The throughput or load experiments were conducted to assess the capacity of our architecture, i.e., its capability to handle a large number of requests from users. This enables us to assess the efficiency and scalability of our architecture and its implementation. Similar to the latency measurements, we individually tested the throughput for each of the individual components and steps in the architecture on the path of a user request.\nThe hardware setup was identical to that of the latency tests, but we used Locust for the throughput measurements.\nThe bottleneck is the capacity of the LLMs to respond to prompts. If the model instances are scaled up by a factor of 10, it is possible that the infrastructure, especially the SSH connection, would need to be optimized or expanded to increase the capacity of the service. Extending the HPC Proxy implementation to establish more than one SSH connection with the cluster and having it load balance requests would be feasible to overcome this limitation."}, {"title": "6.4 User Adoption", "content": "Finally, we explore the user adoption and growth in popularity of Chat AI. Since its initial release on February 22nd, 2024 to users of AcademicCloud, the popularity of the service has increased rapidly and consistently. As shown in Figure 3, in the first three months, over 6000 users were registered in the service, and by June 2024 this number increased to 9000. This includes users from over 160 universities and over 120 research institutions with about 70 of the institutions being part of the Max-Plank-Gesellschaft (MPG) .\nOn average, about 500 to 600 users actively use Chat AI on a typical work day as can be seen in Figure 4, of which approximately 100 are new to the service. In total, Chat AI has received and responded to more than 230'000 messages as of June 2024. These numbers are expected to increase in the future as we continue to add features and improve the service.\nOver time, several features and models were added to Chat AI, wich can be seen in the timeline given in Figure 5. The ability to interact with OpenAI's GPT-4 model was included shortly after the initial release, followed by a myriad of state-of-the-art open source models in the subsequent weeks.\nIn May 2024, the redesign of the user interface was released, moving from our old gradio-based web interface to the new React and Vite-based interface. With the redesign, several new functions and features were added such as custom system prompts. A key feature that was introduced in response to popular demand, was the possibility to store conversations onto local devices via an export and import functionality. This enables the users to restore and resume previous conversations without compromising data privacy.\nAdditionally, we began offering OpenAI-compatible API access to our open-source models, for researchers and students to integrate custom apps and run experiments with Chat AI. The API access, which is provided on request, proved to be massively popular with over 70 api users and drastically increasing the number of requests and demand for the open-source models.\nDespite offering free access to GPT-4, a massively popular commercial model, many users still chose to interact with open-source LLMs in Chat AI as can be seen in Figure 5. The major advantages which could explain this demand is the data privacy, security, customization, inference speed, and more recently the ability to interact via the API. This indicates that despite the apparent superiority of commercial models, there is sufficient demand for open-source LLMs that it is possible to offer a competitive service without depending on commercial LLMs, by leveraging these advantages. It also demonstrates the capability of existing HPC infrastructures to host computationally demanding web services using the proposed architecture at large scale."}, {"title": "7 Discussion", "content": "As the adoption of Chat AI continues to grow throughout universities and institutions in Germany, our contribution as well as its advantages are evident in enabling the ability to host a practical and secure web service on existing Slurm-based HPC clusters. In this section we discuss the limitations of our approach, and attempt to visualize the future outlook of our service."}, {"title": "7.1 Limitations", "content": "Throughout our development efforts and the lifetime of the service, we encountered several problems and challenges, as well as identified limitations of our design, some of which were resolved or could be resolved through additional development efforts but some remain open and would require significant changes."}, {"title": "7.1.1 Reliability", "content": "Some reliability issues are rooted in the architecture due to its many points of failure, such as the SSH connection from the web server to the HPC service node, which if broken can cause an outage of the entire service. This was resolved by improving the implementation of the HPC Proxy to quickly and automatically restore the connection.\nAnother issue we encountered were thenumerous failures of the LLM service on the HPC nodes, some of which were caused by bugs and incompatibilities of vLLM running on our GPU nodes, while some were due to the incapability of Slurm to handle cases where a node would fail, and would, for example, need to be rebooted. While these issues are not specific to our architecture, our scheduler script was not equipped to properly reconcile from these failure states without manual intervention.\nFurthermore, in some cases during high-demand and multiple node failures, it was possible that Slurm could not find the required resources to start a new model instance, and a given model became unavailable in the meantime. To mitigate these issues, we introduced some countermeasures in the scheduler script, but the reliability and stability still need further improvement."}, {"title": "7.1.2 Automation", "content": "Many of the problems that a resource manager for a service paradigm has to solve and that are already solved through a platform such as Kubernetes had to be re-implemented in our solutions. This meant at first more manual setup and configuration tasks, which were gradually being automated. For example, the detection and handling of common error states, which are solved through health checks and automatic restarts had to be implemented from scratch.\nThe deployment of new LLM models on the HPC platform still requires manual effort to set up the service for the scheduler script and to create the respective routes in the API Gateway. However, to facilitate the adoption of our solution, more automation tools must be implemented and added to the service."}, {"title": "7.1.3 Scale to zero", "content": "With our current design it is currently not possible for a service to appropriately handle scale to zero. For a proper implementation of scale to zero, when no requests arrive for a given service for some time, it would reduce the number of desired active service jobs to zero. If a request would then arrive for a service that has scaled to zero, the request would need to be held in a queue until a single instance of service job is ready to process the accumulated requests. Depending on the service job, the cold start time for it might be multiple minutes, for example, to start a 70B LLM, which could result in timeouts for users. Currently, with our architecture and its implementation, it would be possible to scale a model down to zero instances but there is no component that maintains a queue of requests while the model is loading. While this could be implemented it would require significant changes, as the HPC Proxy and all components in the web server are also unaware of the status of the HPC platform and the available service instances, such that only the custom script can detect that a request cannot be answered until the model is ready. Therefore, as this is a script and not an active component, the queue would need to be held in the web server, and it must query the availability status of the services by constantly probing the custom script."}, {"title": "7.2 Encryption", "content": "In our current implementation, it would be possible for an attacker to perform man-in-the-middle attacks if that attacker manages to breach the Chat AI web interface as the user messages are not encrypted internally. This could be solved by implementing end-to-end-encryption via asymmetric cryptography. The Chat AI web interface could provide users with the public key of an asymmetric key pair for the user to automatically encrypt the payload of their requests. Moreover, the user client could also generate a key pair and attach the public key to the payload.\nThe private key would be placed on the HPC login node for the service jobs to decrypt the user requests, process them and then re-encrypt them using the attached public key from the user. The user would then receive a response and be able to decrypt it using their private key.\nImplementing this Diffie-Hellmann inspired encryption schema would protect against man-in-the-middle attacks in the case of an attacker taking over the Chat AI web server at the cost of slightly increased latency for requests. However, an attacker could still swap out the public key provided by the Chat AI web interface and provide their own public key, resulting in them being able to read user requests but the backend failing to process the requests and the breach being quickly discovered."}, {"title": "7.3 Future Work", "content": "Chat AI and its underlying architecture have proven to be a viable solution for hosting HPC-based web services. As the popularity of Chat AI has grown, so did the demand for more features, models and services. There are a multitude of additional AI services that can be offered on our infrastructure, within and extending the scope of Chat AI.\nAs users are requesting to be able to run custom LLMs, which only a subset of the overall user base would use, we plan to further investigate how we can implement scale to zero such that we can support a wide range of models without each of them constantly occupying valuable GPU resources.\nFurthermore, with the release of vision language models (VLMs) [Zha+24] both in the open source [Wan+23b; Che+24] and the commercial space via GPT4o [Ope+24], we are also looking to expand our implementation to support VLMs as well. Moreover, as our architecture is not specific to LLMs we are also planning to implement support for other AI services that benefit from GPU acceleration via our HPC infrastructure such as audio transcription and text-to-speech services.\nFinally, in order for our services to scale out more and to improve automation while relying on well-established software, we are considering switching out the underlying infrastructure for a Kubernetes-based platform."}, {"title": "8 Conclusion", "content": "In this paper we proposed a solution for securely hosting high-performance web services on Slurm-based HPC infrastructure and demonstrated its capabilities through our implementation of Chat AI - a high-performance responsive web service that provides access to state-of-the-art open-source and commercial Large Language Models (LLMs). Notably, our solution leverages existing Slurm installations with no special hardware or software requirements, creating a low barrier to entry for other institutions with similar infrastructure and security concerns.\nFurthermore, we demonstrated the success and popularity of Chat AI through its user adoption throughout the academic community. We inferred that the popularity of the service, especially the open-source models, stems from the full control we provide to users over their data, conversations and messages, as well as a powerful web interface with features such as custom system prompts. The OpenAI-compatible API access to the open-source models proved to be popular and drastically increased the demand for these models.\nWe believe that the Chat AI service and its underlying architecture have the potential to democratize secure and private access to state-of-the-art LLMs, including open-source models. We hope that our contribution facilitates progress in the development of high-performance, secure web services, accelerating innovation and research in various fields."}, {"title": "9 Code Availability & Acknowledgements", "content": "Source Code We provide the web interface as a stand-alone software, including our Chat AI API service, which can be set up and run on a local machine or a web server at https://github.com/gwdg/chat-ai. We also provide the software setup of our web server consisting of multiple containers, including the API gateway, the proxies, and monitoring tools at https://github.com/gwdg/saia-hub. Finally, we provide the custom script that ForceCommand runs, along with the scheduler and tools to run the LLM servers at https://github.com/gwdg/saia-hpc.\nAcknowledgements This work was supported by the Federal Ministry of Education and Research (BMBF), Germany under the AI service center KISSKI (grant no. 01IS22093C) as well as many colleagues at the GWDG, the joint data center of Max Planck Society for the Advancement of Science (MPG) and University of G\u00f6ttingen."}]}