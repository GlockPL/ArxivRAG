{"title": "Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services", "authors": ["Ali Doosthosseini", "Jonathan Decker", "Hendrik Nolte", "Julian M. Kunkel"], "abstract": "The increasing adoption of large language models (LLMs) has created a pressing need for an efficient, secure and private serving infrastructure, which allows researchers to run open-source or custom fine-tuned LLMs and ensures users that their data remains private and is not stored without their consent. While high-performance computing (HPC) systems equipped with state-of-the-art GPUs are well-suited for training LLMs, their batch scheduling paradigm is not designed to support real-time serving of AI applications. Cloud systems, on the other hand, are well suited for web services but commonly lack access to the computational power of clusters, especially expensive and scarce high-end GPUs, which are required for optimal inference speed. We propose an architecture with an implementation consisting of a web service that runs on a cloud VM with secure access to a scalable backend running a multitude of AI models on HPC systems. By offering a web service using our HPC infrastructure to host LLMs, we leverage the trusted environment of local universities and research centers to offer a private and secure alternative to commercial LLM services. Our solution natively integrates with Slurm, enabling seamless deployment on HPC clusters and is able to run side by side with regular Slurm workloads, while utilizing gaps in the schedule created by Slurm. In order to ensure the security of the HPC system, we use the SSH ForceCommand directive to construct a robust circuit breaker, which prevents successful attacks on the web-facing server from affecting the cluster. We have successfully deployed our system as a production service, and made the source code available at https://github.com/gwdg/chat-ai", "sections": [{"title": "Introduction", "content": "Recent advances in large language models (LLMs) have sparked an adoption of AI systems in a wide range of applications [Par; Ope+24]. This puts many institutions and companies in the difficult position of having to come up with a strategy to adopt these new technologies and deciding to what extent their data can be handled by (potentially foreign) companies such as OpenAI and Google [Ope+24; Cho+22] or alternatively attempt to self-host an LLM service in order to preserve the privacy of user data [Seb23; Ksh23].\nHigh-quality LLM models have recently become widely available due to many companies and institutions providing free access to their model weights, commonly via Hugging Face\u00b9. Notable recent releases include Llama3 via Meta [AIM24], Qwen2 via Alibaba Cloud [24e] and Mixtral via Mistral AI [Jia+23].\nUsing a runtime such as llama.cpp\u00b2, vLLM [Kwo+23] or Text-Generation-Inference\u00b3\none can run smaller versions of these models, e.g., 7B or 8B parameters, on high-end\nconsumer GPUs with 10 to 20GB of VRAM. However, for larger models such as Llama3-\n70B [AIM24] and Qwen2-72B [24e] that more closely rival the performance of state-of-the-\nart closed source models such as OpenAI's ChatGPT4 [Ope+24], significant investment\ninto GPU resources is required.\nIn our testing we found that a single instance of Meta's LLama3-70B [AIM24] model\nfits into the VRAM of two of NVIDIA's H100 GPUs when using FP8 quantization via\nvLLM 0.5.0 or even a single H100 GPU when significantly reducing the context size.\nWhile this allows effective inference of LLama3-70B and models of similar size, it limits\naccess to private inference of these models to those who can afford H100 GPUs or similarly\npowerful hardware.\nMany research centers, universities and institutions either operate their own HPC\ninfrastructure or have access to HPC infrastructure via a research network4. Typically,\nHPC centers operate a batch-scheduler such as Slurm [YJG03] and put strong emphasis\non the security of their systems.\nHPC infrastructure is an attractive target for attackers to steal research data, interrupt\noperationability or hijack compute resources for crypto mining. To ensure system security,\non HPC multi-user systems, admins often rely on battle-tested security mechanisms such\nas POSIX permissions and SSH for access management. This significantly restricts the\npossibility of using HPC resources to expose services without compromising these security\nframeworks.\nFurthermore, a batch-scheduler such as Slurm requires users to submit jobs, which it\nthen schedules at a suitable time to one or more nodes depending on the expected job\nduration, resource demands and availability of resources. Considering that a user of an\nHPC service, e.g. chatting with an LLM, would expect their request to be processed"}, {"title": "Outline", "content": "In Section 2 we outline the background and motivation. In Section 3 we explore related\nwork. In Section 4 we share our challenges and solutions. In Section 5 we describe our\narchitecture in more detail. In Section 6 we evaluate our service with regards to security,\nperformance and explore its adoption by users and institutions, followed by a discussion of\nthe limitations and future prospects in Section 7. Finally, we summarize our contributions\nin Section 8 and offer our source code and acknowledgements in Section 9."}, {"title": "Background", "content": "In this section we describe the existing tools and paradigms that are relevant to our\napproach.\nSlurm The Simple Linux Utility for Resource Management, or short Slurm [YJG03], is\na workload or resource manager for HPC clusters. It offers a solution for running batch\njobs on a cluster through the use of shell scripts. Slurm assigns jobs to nodes depending\non the availability of the current nodes and requirements of the job, ensuring that the\nresources of the cluster are utilized efficiently.\nScheduling Paradigm When assigning a job that requests a number of resources such\nas multiple nodes, Slurm will only assign and start a job when all resources are available\nat once for the requested duration. This is commonly referred to as gang scheduling.\nWhile it is desirable for a compute job such as an MPI job that can only start when\nall nodes are available to use gang scheduling, it is a different case for services. For\na service that is supposed to serve user requests, multiple instances might be required\nto sufficiently serve all requests but if not enough resources are available, the service\ncould still operate with reduced performance and dynamically add instances when more\nresources become available. For one-time jobs that are not time-sensitive, Slurm is a\nsuitable choice, however, one would have to adapt their resources to a service paradigm.\nvLLM vLLM [Kwo+23] is a state-of-the-art, open source LLM runtime implementa-\ntion. It uses an attention mechanism inspired by memory paging techniques in operating\nsystems, achieving near-zero waste in key-value cache memory. Since its first release in"}, {"title": "Related Work", "content": "Providing access to LLMs is in itself a service offering, and running such services in a\nproduction environment is commonly associated with existing frameworks such as Ku-\nbernetes [Liu23]. However, most HPC infrastructures rely on batch-scheduling systems\nsuch as Slurm, Torque or HTCondor [Liu23; Hil+20] as their main resource manager.\nMoreover, modern LLMs require so much compute power not only for training but also\nfor inference [Wan+24; Wan+21] that an inference service would benefit from access to\npowerful hardware such as high-end GPUs installed in HPC infrastructure.\nIn order to use these HPC resources, various approaches exist.\n\u2022 Zhang et al. [Zha+19] proposed a unified framework for Apache Hadoop YARN8\nand Slurm to create a mobile AI service platform.\n\u2022 Wan et al. [Wan+23a] proposed a Slurm-based service platform to optimize startup\ntimes and storage usage for multi-user applications on HPC infrastructure.\n\u2022 Shainer et al. [Sha+22] proposed a service architecture with a focus on supporting\nspecialized HPC hardware such as DPUs and Infiniband."}, {"title": "Challenges", "content": "In this section we present the challenges and concerns of hosting web services on an HPC\ninfrastructure that employs Slurm as its resource manager.\nParadigm Differences The purpose of HPC clusters is commonly to run large-scale\nand demanding batch jobs with a fixed start and end point, but not continuous and\nscalable web-services. The batch job and service paradigms are strictly different from one\nanother and have different expectations and goals. Slurm is designed to handle the batch\njob paradigm and will schedule incoming jobs according to it. In order to adapt it to the\nservice paradigm, a job that runs an instance of a service must continuously be replaced\nor extended. This requires some other scheduling control flow to operate on top of Slurm\nto implement this functionality, e.g., keeping a pool of one or more instances active for a\ngiven service by ensuring that one or more jobs are active for that service at a given time.\nSecurity From a security perspective, hosting a web service on an HPC cluster opens\nup additional attack surfaces. An HPC cluster is a valuable target for an attacker as it\noffers massive amounts of compute resources, which could be misappropriated for crypto\nmining and are hosts for potentially sensitive research data.\nAs a web service has a much wider audience than an HPC cluster, it may be desirable\nto make it accessible from the internet, requiring some components of the service to be\nexposed from behind the firewalls of a given HPC center. SSH in particular is a commonly\nallowed access protocol and in some cases is the sole option to access the necessary HPC\ncomponents, thereby forcing access patterns to build on top of SSH to remain compatible\nwith the existing security framework. Moreover, as an SSH connection is established using\ncryptographic keys, which would need to be stored on a potentially exposed machine in\norder to connect to the HPC backend, extra caution must be taken to ensure the damage\ncaused by a potential compromise of this key is limited, such that it cannot be used as\nan entry point to the HPC cluster and its compute power by an attacker.\nPerformance HPC infrastructure commonly includes specialized hardware such as HPC\ninterconnects, e.g., Infiniband, and parallel file systems to optimize the performance of\nbatch jobs. While HPC hardware and Ethernet interconnects are normally powerful\nenough to support most services, when pushing the performance especially with the\nworkarounds that have been placed in order to adapt the paradigm, the performance\nmay be reduced compared to an infrastructure tailored for running production services.\nMost notably, performance differences can be expected in throughput and latency.\nScalability Slurm does not inherently provide an option to scale up the number of jobs\nas the number of requests increase, requiring additional efforts to implement automatic\nscaling. In order to support a service where request volumes vary over time, it is necessary\nto support automatic scaling, where the number of instances for a given service is adjusted\nbased on request volume. This requires the implementation of a functionality to measure\nthe current request volume as well as to calculate and enforce the desired amount of\nservice jobs. Moreover, this also introduces the problem of service discovery, as incoming"}, {"title": "Chat AI Architecture", "content": "The architecture of Chat AI is shown in Figure 1 with its two major components,\nnamely the web server labeled as ESX Server and the HPC infrastructure labeled as\nKISSKI Platform. Fundamentally, the HPC platform is only expected to provide Slurm\nitself.\nEach of the two major components is split up into further sub-components, which are\nindividually explained in the following. It should also be noted that all sub-components\nrunning on the ESX Server have been containerized and operate via docker compose.\nAuthentication\nWe employ an Apache webserver as a reverse proxy for our web interface to provide\nintegration with our Single Sign-On (SSO) provider Academiccloud13. This is effectively\nan OAuth2 authentication mechanism using the Apache webserver module OpenIDC and\ncould be used via any other SSO provider that implements the same protocols.\nOnce a user has signed into the SSO provider, they are forwarded through the API\nGateway to the Chat AI web interface. Furthermore, when forwarding the requests, the\nAPI Gateway\nWe deployed the Kong Open Source edition14 as the API gateway to route incoming re-\nquests to the corresponding upstreams. Kong was chosen as it is open source and provides\nall the features we require from an API gateway including creation and management of\nAPI routes, rate limiting, API key management and observability.\nContinuing from the Apache webserver authentication component, a user request is\nforwarded by the Kong API Gateway to the Chat AI web interface via its respective route.\nIf a user then submits an inference request to an LLM via the web interface, the request\nis sent through the API Gateway to the corresponding route depending on the selected\nmodel,\nMoreover, the API Gateway also exposes an access point for API users. Users with\nvalid API keys can bypass the Apache webserver and connect to the API Gateway directly,\nwhich performs authentication via the provided API key. This unifies the path for web\nand API users past the API gateway such that the incoming route and user is abstracted\nfrom the backend service.\nWeb Interface\nThe web interface illustrated in Figure 2 serves as a user-friendly way for users to\ninteract with a selection of LLMs. We chose to implement our own web interface instead\nof reusing existing open source software such as Text-Generation-Webui15 or LibreChat16\nin order to have full control over the handling of user data as well as the routing of\nrequests.\nHPC Proxy\nTo enable the submission of inference requests from the web server to the HPC cluster\nwithout compromising its tight security, we wrap this communication in an SSH con-\nnection. SSH is the only protocol allowed to access the login node of our HPC center,\nlabelled as Service Node in Figure 1, and automatic logins via SSH to user accounts are\nalso forbidden.\nStoring an SSH key on the web server which could potentially be used by an attacker\nto gain access on the HPC cluster was only allowed by using the ForceCommand directive\nof SSH17. This directive is used to restrict an SSH key to a specific command such that\nit cannot be used to run generic shell commands on the remote machine. Moreover, as\nthis is configured via the authorized_keys file in a user's directory, it can be configured\nwithout root access. In our case, the SSH key belongs to a functional account which is\nalso responsible for the submission of service jobs to Slurm.\nWe have developed the HPC Proxy as an application that ensures that the SSH con-\nnection between itself and the HPC login node remains open and is reestablished in case\nan interruption breaks the SSH connection. Interruptions are detected by attempting to\nsend keep alive pings once every 5 seconds or so. Besides that, the HPC Proxy serves as a\nregular proxy that forwards requests to a specific custom script running on the HPC ser-\nvice node. This script is designed to forward authorized inference-related HTTP requests\nto a service node running the desired model and ready to respond. Additional steps were\ntaken to ensure the proxy is fast, efficient and generalizable with support for various types\nof requests and responses, including streaming.\nIn Figure 1 we included two HPC Proxy instances, as this architecture decouples the\nweb server from the HPC platform, allowing a single web server to potentially utilize\nmultiple HPC platforms by starting an HPC Proxy instance for each of them and load\nbalancing via the API Gateway. On the other hand, it is also possible to have multiple\nweb servers utilize the same HPC platform, each with its own individual SSH connection.\nWith this configuration, we are able to provide custom web services that require HPC\nresources without directly exposing the components in the cluster.\nScheduler\nPreviously, the role of the HPC Proxy was described in establishing and maintaining an\nSSH connection to the HPC login node, such that a custom script is called for every valid\nincoming request to route it to a healthy service instance. We also developed a scheduler\nscript, which is run periodically on every incoming keep alive ping from the HPC Proxy,\nwith the goal of ensuring the availability of the service instances.\nThe scheduler script can be configured with a set of services it should maintain along\nwith the specifics of running their respective jobs, such as the job script and scalability\nfactor. The scheduler script obtains a list of running jobs for each service from Slurm via\nthe squeue command, compares it against the given configuration, and takes necessary\naction to maintain the services, such as submitting new jobs via sbatch. In order to avoid\nrace conditions, we ensure only one instance of the scheduler is running at a time by\nmeans of a lock file.\nThe scheduler script maintains a routing table, containing an entry for each active\njob, along with its associated service, node and port number. This table is used to\nroute each incoming request to a specific node and port number associated with a service\ninstance, selected randomly out of all eligible instances, effectively performing random\nload balancing.\nAs Slurm provides no network virtualization, if two jobs on the same node try to\noccupy the same port, it will fail for the second job as the port is already occupied by the\nfirst job. To avoid this, the scheduler script picks a random port when submitting a new\njob for that job to use and checks the routing table to ensure that the port is not already\noccupied.\nOnce a service job has been submitted, it takes an uncertain amount of time before the\ninstance is ready to serve requests, as Slurm needs to assign it to a node and the model\nmay need to be loaded into GPU memory. Therefore, the scheduler script periodically\nprobes the newly submitted jobs until they are ready, before marking them as ready to\nserve requests in the routing table.\nTo scale up and down the number of service instances according to demand, calculating\nthe request volume for a given service is necessary. This is performed by storing the\naverage number of concurrent requests to each service within a predefined time window,\nwhich is then recalculated and updated on each scheduling run. If this average is higher\nthan a certain threshold, the scheduler spawns multiple instances of that service to prevent\nthe formation of a backlog. Likewise, when the average is too low, the scheduler allows\nthe excess jobs to expire without resubmitting them, effectively reducing the number of\ninstances and freeing up resources.\nIn an alternative design, the request volume could be measured by the API Gateway\nor HPC Proxy and forwarded to the scheduler script, but we decided against this option\nin order to minimize the coupling between the web server and the HPC platform, and\ninstead pursued a solution to calculate the request volume directly on the HPC platform.\nLLM Server\nAs Chat AI is an LLM chat service, we also require a runtime to serve LLM inference\nrequests. For that purpose, one can assume the individual services mentioned in the\nprevious subsection to refer to various LLMs hosted on the HPC infrastructure, but can\nalso be any server runtime that benefits from HPC resources.\nTo run an LLM model with GPU acceleration, we employ the open source runtime\nvLLM18. vLLM has the advantages of offering an OpenAI-compatible API and an active\ncommunity to quickly implement support for many popular open source LLMs. From our\nexperience, vLLM was several times more efficient than our unoptimized LLM runtime\nimplementation.\nExternal Proxy\nThe External Proxy serves as an optional extension to the architecture to provide access to\nexternal models such as ChatGPT4 [Ope+24] as an additional route in the API Gateway,\neffectively becoming a wrapper for additional services. As ChatGPT4 requires paid access,\nwe placed strict rate limits via the API Gateway and restricted access to certain user\ngroups.\nMonitoring\nThe monitoring components are not shown on Figure 1 as they are optional but neverthe-\nless important for capturing metrics and detecting issues. We employ an external Grafana\nservice, which constantly captures logging and monitoring data from a Prometheus server\nthat is integrated into the API gateway via a plugin. This is performed via standardized\nmonitoring endpoints and ensures only authorized access to the monitoring system."}, {"title": "Evaluation", "content": "Our primary goals for the LLM web service were to ensure its privacy, security and\npracticality. Due to the nature of our infrastructure and demands of our users, security\nand privacy took the highest priority in our design. In this section, we evaluate the\narchitecture for Chat AI based on the achieved standards for security and data privacy as\nwell as performance measurements regarding throughput and latency. Furthermore, we\nassess its user adoption as a factor in determining the architecture's real-world viability\nand overall success.\nSecurity\nIn order to evaluate the security of our architecture we assume different attack scenarios.\nThese should serve to visualize that our designs follow the principals of defense-in-depth,\nminimizing permission and not storing anything that does not need to be stored.\nSecurity of the Web Interface\nOur Chat AI webinterface employs standard security mechanism via TLS and in order\nto access it a user must have a valid account and login to authenticate with the SSO\nprovider.\nAssuming a breach in the webinterface, which gives an attacker a shell in the webin-\nterface container, this would allow the attacker to spy on users of the webinterface and\nset up man-in-the-middle attacks until the breach is discovered and patched. However,\nas no user data is stored, no immediate data can be stolen by an attacker.\nAdministrative tasks on the web server are handled via an SSH connection, which is\nonly possible from the company internal VPN when utilizing two-factor authentication.\nDue to the containerized nature of the setup, an attacker would need to escape the\ncontainer to gain a shell on the host system and then gain root access in order to compro-\nmise other parts of the system. Even in this scenario, an attacker would not be able to use\nthe compromised web server as an entry point into other parts of the HPC infrastructure.\nSecurity of the HPC System\nIn our view, compromising the HPC system would be the worst case scenario that needs\nto be prevented at all costs. As users do not have direct access to the HPC infrastructure\nin this architecture, any attacks on the HPC infrastructure would necessarily originate\nfrom the web server, meaning that some component within the web server would have\nto be already compromised. In this case an attacker may attempt to locate and read\nthe SSH key used to establish a connection to the HPC system. However, as the SSH\nkey is configured with ForceCommand and corresponds to a functional account with\nno administrative permissions on the cluster, its effectiveness in penetrating the HPC\nplatform is significantly reduced.\nA potential alternative to the attack scenario above would be if an attacker discov-\ners flaws in the custom scripts defined as the ForceCommand, and attempts to perform\ninjection attacks by submitting requests that instead of being routed to a service, end\nup executing shell commands. For this purpose we bring extra attention to the imple-\nmentation of the input parsing in the custom script to protect against injection attacks,\nrestricting any request to follow a preset of determined paths, and avoiding any potentially\ndangerous commands such as eval.\nEssentially, an attacker would need to breach multiple layers of security to first gain a\nsignificant foothold on the web server, and then breach multiple layers again to get some\nform of shell access on the HPC system. Even if these security layers fail, the previous\nconversations and messages of our users physically cannot be stolen from us as they were\nnever stored in the first place. While it is impossible to claim that a system's security\nis perfect, we consider having multiple layers that an attacker would need to breach as a\nmarker of an architecture's security.\nData Privacy\nAs for data privacy, we regard compliance with GDPR and the strict data privacy mea-\nsures it necessitates as our main benchmark. Sensitive and personalized data must be\ntransmitted and stored securely to ensure that third parties cannot access it. We take\nprivacy a step further and minimized the data collection according to GDPR article 5(1)\nlit. c to the degree, that no user prompts are being stored at any circumstance, i.e.,\nprompts and responses, are not stored on the server. To still provide a stateful conversa-\ntion, the conversation history is solely stored within the user's browser, which can simply\nbe deleted with a button provided in the web interface or by resetting their browser\nhistory.\nIt follows that conversational data is also not used for any purpose such as training\nmodels. Some non-conversational usage statistics are gathered solely for monitoring and\naccounting purposes. This includes the user's account identifier, timestamps for requests,\nand selected model. With this information, we can monitor the load on the individual\nmodels and identify potential misbehaving users.\nAs a result of these measures, an attacker on the web interface or on the HPC system\ncannot access past conversations as they are only kept locally on the users' devices and\nnever stored on the server.\nPerformance\nFor our architecture to be able to support increasing numbers of users, we need to assess its\nperformance. To evaluate the effectiveness of incorporating the HPC cluster as the back-\nbone of our service, we conducted a series of tests. These tests measured the throughput\nand latency of each component in the architecture and can be used to identify bottlenecks\nin the pipeline. Latency measurements were performed using a custom UNIX shell script,\nand for throughput measurements we employed the Load testing framework Locust19.\nLatency\nThe latency or response time is the amount of time it takes for a client to receive a\nresponse to a request, i.e., the duration between the time a request is sent and the point\nin time when the response is received. To identify potential bottlenecks, we performed\nlatency measurements for every individual component in the architecture on the path for\nuser requests and calculated the time spent in each step. For statistical validity, we took\nthe average over 50 measurements with identical conditions.\nThe setup that the tests were performed on consists of a VM web server running\nUbuntu 22.04.4 LTS with a 16-core AMD EPYC processor and 8GB of RAM, and a\nSlurm-based HPC cluster consisting of one login node and 10 GPU nodes, each with 4\nNvidia H100/80GB GPUs and a 52-core Intel(R) Xeon(R) Platinum 8470 processor with\n500GB of RAM running Rocky Linux 9.2.\nUnder normal conditions when the system is not overloaded, a user can expect to\nreceive their first response token after approximately 50ms, of which more than 27ms\nare the compute time of the underlying LLM. Therefore, the additional latency overhead\nintroduced by our architecture was approximately 23ms, which is not noticeable in most\ncases and can be considered acceptable.\nThroughput\nThe throughput or load experiments were conducted to assess the capacity of our archi-\ntecture, i.e., its capability to handle a large number of requests from users. This enables\nus to assess the efficiency and scalability of our architecture and its implementation. Sim-\nilar to the latency measurements, we individually tested the throughput for each of the\nindividual components and steps in the architecture on the path of a user request.\nUser Adoption\nFinally, we explore the user adoption and growth in popularity of Chat AI. Since its initial\nrelease on February 22nd, 2024 to users of AcademicCloud21, the popularity of the service\nhas increased rapidly and consistently. As shown in Figure 3, in the first three months,\nover 6000 users were registered in the service, and by June 2024 this number increased\nto 9000. This includes users from over 160 universities and over 120 research institutions\nwith about 70 of the institutions being part of the Max-Plank-Gesellschaft (MPG) 22.\nOn average, about 500 to 600 users actively use Chat AI on a typical work day as can\nbe seen in Figure 4, of which approximately 100 are new to the service. In total, Chat\nAI has received and responded to more than 230'000 messages as of June 2024. These\nnumbers are expected to increase in the future as we continue to add features and improve\nthe service.\nOver time, several features and models were added to Chat AI, wich can be seen in\nthe timeline given in Figure 5. The ability to interact with OpenAI's GPT-4 model was\nincluded shortly after the initial release, followed by a myriad of state-of-the-art open\nsource models in the subsequent weeks."}, {"title": "Discussion", "content": "As the adoption of Chat AI continues to grow throughout universities and institutions in\nGermany, our contribution as well as its advantages are evident in enabling the ability\nto host a practical and secure web service on existing Slurm-based HPC clusters. In this\nsection we discuss the limitations of our approach, and attempt to visualize the future\noutlook of our service.\nLimitations\nThroughout our development efforts and the lifetime of the service, we encountered several\nproblems and challenges, as well as identified limitations of our design, some of which were\nresolved or could be resolved through additional development efforts but some remain open\nand would require significant changes.\nReliability\nSome reliability issues are rooted in the architecture due to its many points of failure, such\nas the SSH connection from the web server to the HPC service node, which if broken can\ncause an outage of the entire service. This was resolved by improving the implementation\nof the HPC Proxy to quickly and automatically restore the connection.\nAnother issue we encountered were the numerous failures of the LLM service on the\nHPC nodes, some of which were caused by bugs and incompatibilities of vLLM running\non our GPU nodes, while some were due to the incapability of Slurm to handle cases\nwhere a node would fail, and would, for example, need to be rebooted. While these issues\nare not specific to our architecture, our scheduler script was not equipped to properly\nreconcile from these failure states without manual intervention.\nFurthermore, in some cases during high-demand and multiple node failures, it was\npossible that Slurm could not find the required resources to start a new model instance,\nand a given model became unavailable in the meantime. To mitigate these issues, we\nAutomation\nMany of the problems that a resource manager for a service paradigm has to solve and\nthat are already solved through a platform such as Kubernetes had to be re-implemented\nin our solutions. This meant at first more manual setup and configuration tasks, which\nwere gradually being automated. For example, the detection and handling of common\nerror states, which are solved through health checks and automatic restarts had to be\nimplemented from scratch.\nThe deployment of new LLM models on the HPC platform still requires manual effort\nto set up the service for the scheduler script and to create the respective routes in the\nAPI Gateway. However, to facilitate the adoption of our solution, more automation tools\nmust be implemented and added to the service.\nScale to zero\nWith our current design it is currently not possible for a service to appropriately handle\nscale to zero. For a proper implementation of scale to zero, when no requests arrive for\na given service for some time, it would reduce the number of desired active service jobs\nto zero. If a request would then arrive for a service that has scaled to zero, the request\nwould need to be held in a queue until a single instance of service job is ready to process\nthe accumulated requests. Depending on the service job, the cold start time for it might\nbe multiple minutes, for example, to start a 70B LLM, which could result in timeouts for\nusers. Currently, with our architecture and its implementation, it would be possible to\nscale a model down to zero instances but there is no component that maintains a queue\nof requests while the model is loading. While this could be implemented it would require\nsignificant changes, as the HPC Proxy and all components in the web server are also\nunaware of the status of the HPC platform and the available service instances, such that\nonly the custom script can detect that a request cannot be answered until the model is\nready. Therefore, as this is a script and not an active component, the queue would need\nto be held in the web server, and it must query the availability status of the services by\nconstantly probing the custom script.\nEncryption\nIn our current implementation, it would be possible for an attacker to perform man-in-\nthe-middle attacks if that attacker manages to breach the Chat AI web interface as the\nuser messages are not encrypted internally. This could be solved by implementing end-to-\nend-encryption via asymmetric cryptography. The Chat AI web interface could provide\nusers with the public key of an asymmetric key pair for the user to automatically encrypt\nthe payload of their requests. Moreover, the user client could also generate a key pair and\nattach the public key to the payload.\nThe private key would be placed on the HPC login node for the service jobs to decrypt\nthe user requests, process them and then re-encrypt them using the attached public key\nfrom the user. The user would then receive a response and be able to decrypt it using\ntheir private key.\nImplementing this Diffie-Hellmann [Mer78] inspired encryption schema would protect\nagainst man-in-the-middle attacks in the case of an attacker taking over the Chat AI web\nserver at the cost of slightly increased latency for requests. However, an attacker could\nstill swap out the public key provided by the Chat AI web interface and provide their own\npublic key, resulting in them being able to read user requests but the backend failing to\nprocess the requests and the breach being quickly discovered.\nFuture Work\nChat AI and its underlying architecture have proven to be a viable solution for hosting\nHPC-based web services. As the popularity of Chat AI has grown, so did the demand for\nmore features, models and services. There are a multitude of additional AI services that\ncan be offered on our infrastructure, within and extending the scope of Chat AI.\nAs users are requesting to be able to run custom LLMs, which only a subset of the\noverall user base would use, we plan to further investigate how we can implement scale\nto zero such that we can support a wide range of models without each of them constantly\noccupying valuable GPU resources.\nFurthermore, with the release of vision language models (VLMs) [Zha+24] both in the\nopen source [Wan+23b; Che+24] and the commercial space via GPT4o [Ope+24], we are\nalso looking to expand our implementation to support VLMs as well. Moreover, as our\narchitecture is not specific to LLMs we are also planning to implement support for other\nAI services that benefit from GPU acceleration via our HPC infrastructure such as audio\ntranscription and text-to-speech services.\nFinally, in order for our services to scale out more and to improve automation while\nrelying on well-established software, we are considering switching out the underlying in-\nfrastructure for a Kubernetes-based platform."}, {"title": "Conclusion", "content": "In this paper we proposed a solution for securely hosting high-performance web services\non Slurm-based HPC infrastructure and demonstrated its capabilities through our imple-\nmentation of Chat AI - a high-performance responsive web service that provides access to\nstate-of-the-art open-source and commercial Large Language Models (LLMs). Notably,\nour solution leverages existing Slurm installations with no special hardware or software\nrequirements, creating a low barrier to entry for other institutions with similar infrastruc-\nture and security concerns.\nFurthermore, we demonstrated the success and popularity of Chat AI through its\nuser adoption throughout the academic community. We inferred that the popularity of\nthe service, especially the open-source models, stems from the full control we provide to\nusers over their data, conversations and messages, as well as a powerful web interface\nwith features such as custom system prompts. The OpenAI-compatible API access to the\nopen-source models proved to be popular and drastically increased the demand for these\nmodels."}]}