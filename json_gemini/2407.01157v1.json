{"title": "Unaligning Everything: Or Aligning Any Text to Any Image in Multimodal Models", "authors": ["Shaeke Salman", "Md Montasir Bin Shams", "Xiuwen Liu"], "abstract": "Utilizing a shared embedding space, emerging multimodal models exhibit unprecedented zero-shot capabilities. However, the shared embedding space could lead to new vulnerabilities if different modalities can be misaligned. In this paper, we extend and utilize a recently developed effective gradient-based procedure that allows us to match the embedding of a given text by minimally modifying an image. Using the procedure, we show that we can align the embeddings of distinguishable texts to any image through unnoticeable adversarial attacks in joint image-text models, revealing that semantically unrelated images can have embeddings of identical texts and at the same time visually indistinguishable images can be matched to the embeddings of very different texts. Our technique achieves 100% success rate when it is applied to text datasets and images from multiple sources. Without overcoming the vulnerability, multimodal models cannot robustly align inputs from different modalities in a semantically meaningful way. Warning: the text data used in this paper are toxic in nature and may be offensive to some readers.", "sections": [{"title": "Introduction", "content": "Built on large pre-trained foundation models (Bommasani et al. 2022), applications have exhibited unprecedented capabilities for a wide range of tasks, setting new state-of-the-art on benchmark datasets, acing standard exams, and passing professional exams (OpenAI 2023; Brandes et al. 2022; Kung et al. 2023; Choi et al. 2023). Such models, however, are not well understood due to their complexity, even though the need for understanding and the risks of lacking is widely recognized and acknowledged (Bommasani et al. 2022). For example, transformers have become a hallmark component in models for many applications and have led to significant improvements in performance on benchmark datasets (Vaswani et al. 2017; Dosovitskiy et al. 2021; Devlin et al. 2019). By transforming inputs from different modalities (such as texts and images) to a common embedding space, emerging multimodal models provide new capabilities and new applications are being developed by exploiting the shared space (Radford et al. 2021).\nAt the same time, it is well known that neural networks exhibit an intriguing property in that they are subject to adversarial attacks: some small changes to an input could result in substantial changes in model responses and outputs (Goodfellow, Shlens, and Szegedy 2015; Szegedy et al. 2014; Chakraborty et al. 2021). While studies have shown adversarial examples exist to break even aligned models (Zou et al. 2023), it is not clear whether the shared space could be exploited to establish arbitrary associations between images and texts, or between two different modalities, therefore breaking the alignments that many of the models rely on in order to function properly.\nIn this paper, using a gradient-descent-based optimization procedure as detailed in our prior work (Salman, Shams, and Liu 2024), we show that perturbing an input image to a deployed model in unnoticeable ways can alter the resulting representation to match any chosen text and, therefore, reveal an inherent vulnerability of joint vision-text models. Since such multimodal models are being deployed, the identified vulnerability should be considered for these models. Furthermore, we show that the resulting inputs can dramatically change classification results with no modifications to the classifiers.\nTo highlight the main advantages of our framework, we present our results using multiple models, including the ImageBind (Girdhar et al. 2023).\nThese and additional results shown in the Experiments section, along with the fact we have obtained the same findings on all the image-text pairs we have used, demonstrate convincingly that there are visually indistinguishable inputs corresponding to the embeddings of very different texts, and yet there are very different images corresponding to the embeddings of identical texts. By analyzing the equivalence classes (Salman, Shams, and Liu 2024) of the embeddings"}, {"title": "Related Work", "content": "The transformer architecture (Vaswani et al. 2017) revolutionized NLP by effectively capturing long-range dependencies, resulting in powerful pre-trained models like BERT (Devlin et al. 2019) and GPT (Brown et al. 2020) that excel in various tasks. This advancement extends to computer vision with the Vision Transformer (ViT) (Dosovitskiy et al. 2021), showcasing the transformative impact of attention mechanisms across domains.\nThe recent prompting-based models and multimodal models have further accelerated the trend. The joint multimodal models have demonstrated significant benefits by employing a shared embedding space across various modalities. For example, CLIP (Radford et al. 2021) aligns vision and text representations. The model is trained to predict the coherence of image-text pairs, which enables it to understand complex relationships between the two modalities. Several recent works extend the technique of shared embedding space beyond text and vision by employing a unified embeddings space in various modalities are: GPT-4 (OpenAI 2023), MiniGPT-4 (Zhu et al. 2023), Flamingo (Alayrac et al. 2022), Bard (Pichai 2023), LLaVA (Liu et al. 2023) and, ImageBind (Girdhar et al. 2023).\nAnother line of research aims to comprehend models by probing them to unveil new properties. A well-studied problem is adversarial attacks, where unnoticeable changes to the input can cause the models, primarily classifiers, to change their predictions. Most adversarial attacks are applied to commonly used (deep) neural networks, including multiple-layer perceptrons and convolutional neural networks (CNNs), demonstrating their vulnerability and sensitivities to such adversarial changes (Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015). Croce and Hein propose AutoAttack, an ensemble of parameter-free attacks that combines multiple methods to provide a robust assessment of a model's vulnerability (Croce and Hein 2020). Recent studies have explored the vulnerability of multimodal models to adversarial attacks, which can potentially jailbreak aligned large language models (LLMs) or Vision Language models (VLMs) (Carlini et al. 2023; Qi et al. 2023; Zou et al. 2023). Bhojanapalli et al. investigate the robustness of ViTs against attacks where the attacker has access to the model's internal structure (Bhojanapalli et al. 2021; Shao et al. 2022). Notably, these methods revolve around generating adversarial examples based on the classifier's methodology rather than focusing on the representation level. However, our approach is different. Rather than crafting an adver-"}, {"title": "Preliminaries", "content": "sarial example tailored to a particular classifier, our method is designed to generate examples that conform to a specified representation.\nA closely related study by Kazemi et al. examines the behavior and vulnerabilities of the CLIP model (Kazemi et al. 2024). Their work is centered on inverting the CLIP model embeddings to understand the semantic information of these embeddings. However, our work focuses on demonstrating the vulnerabilities within the shared embedding spaces of multimodal models through adversarial attacks. We show through extensive experiments that visually indistinguishable images can be mapped to arbitrary text, revealing an inherent vulnerability that is classifier agnostic.\nAs this paper focuses on vision-language models that are based on transformers, here we first describe the transformers mathematically and then describe the vision-language models. Transformers can be described mathematically succinctly, consisting of a stack of transformer blocks. A transformer block is a parameterized function class $f_e: \\mathbb{R}^{n \\times d} \\to \\mathbb{R}^{n \\times d}$. If $x \\in \\mathbb{R}^{n \\times d}$ then $f_e(x) = z$ where $Q^{(h)}(x_i) = W_q x_i$, $K^{(h)}(X_i) = W_k x_i$, $V^{(h)}(x_i) = W_u x_i$, $W_{h,q}, W_{h,k}, W_{h,v} \\in \\mathbb{R}^{d \\times k}$. The key multi-head self-attention is a softmax function applying row-wise on the inner products.\n$\\alpha_i^{(h)} = softmax_j(\\frac{\\langle Q^{(h)}(x_i), K^{(h)}(x_j) \\rangle}{\\sqrt{k}})$                                                                     (1)\nThe outputs from the softmax are used as weights to compute new features, emphasizing the ones with higher weights given by\n$u_i = \\sum_{h=1}^{H} W_{c,h} \\sum_{j=1}^{n} \\alpha_{i,j}^{(h)} V^{(h)}(x_j)$, $W_{c,h} \\in \\mathbb{R}^{k \\times d}$.                    (2)\nThe new features then pass through a layer normalization, followed by a ReLU layer, and then another layer normalization. Typically transformer layers are stacked to form deep models.\nWhile transformer models are widely used for natural language processing tasks, recently, they are adapted to vision tasks by using image blocks on the basic units, and spatial relationships among the units are captured via the self-attention mechanism (Dosovitskiy et al. 2021). A vision-language model based on transformers incorporates a dedicated transformer model for each input modality. The resulting representations from these modalities are mapped to a shared embedding space.\nFor the ImageBind model, we denote the model for image $x$ as $f_1(x)$ and for text $t$ as $f_r(t)$. Fig. 2 shows that the image embeddings and text embeddings share the same vector space. Given image $x$ and C text labels, $t_0, ..., t_{C-1}$,"}, {"title": "Embedding Alignment Procedure", "content": "As described in our previous work (Salman, Shams, and Liu 2024), the proposed approach for embedding alignment focuses on aligning the representation of an input with that of a target input. The core of the method is an iterative gradient optimization procedure, similar to how most of the neural networks are trained, except the gradient is calculated with respect to the input variables. Since we need to match two vectors, we define the loss for finding an input matching a given representation as\n$\\mathcal{L}(x) = \\mathcal{L}(x_o + \\Delta x) = \\frac{1}{2} ||f_1(x_o + \\Delta x) - f_r(t_{tg}) ||^2$,   (4)\nwhere $x_0$ is an initial image and $f_r(t_{tg})$ specifies the target embedding for a specified text sequence $t_{tg}$. Approximately, the gradient is given by\n$\\frac{\\partial \\mathcal{L}}{\\partial x} |_{x=x_o} \\approx (\\frac{\\partial f_1}{\\partial x})^T|_{x=x_o} (f_1(x_o + \\Delta x) - f_r(t_{tg}))$.    (5)\nIn each step, the algorithm first calculates the loss as defined by the loss function, between the image embedding with the target embedding as the one given by the specified text. Then using PyTorch, it computes the gradient by doing backward computation. After the gradient is calculated, we update the pixel values by doing gradient descent.\nEq. (5) shows how the gradient of the mean square loss function is related to the Jacobian of the representation function at $x = x_0$. In other words, the gradient is related to the differences between the current and target text representations. When the differences are large, the gradient should be significant as well. Consistent with this analysis, on all the examples we have experimented with, we are able to minimize the loss, and details are provided in the Experimental Results section and appendix.\nWhile local optimal solutions could be obtained by solving a quadratic programming problem or linear programming problem, depending on the norm to be used when minimizing Ax, the gradient function works effectively for all the cases we have tested due to the Jacobian of the transformer.\nOne of the practical issues using the gradient descent-based procedure is how to determine the learning rate. In the case of the transformers, the model can be approximated by a linear model when it moves within one activation region; note that it is approximate due to the nonlinearity of the softmax. The algorithm is able to find the matching representations for a wide range of learning rates; see the Experimental Results section for more details."}, {"title": "Experiments", "content": "In this section, we first outline the specifics of our experimental settings and implementation details. Our designed framework is systematically applied across various datasets and multiple multimodal models; in the subsequent subsections, we present both the experimental outcomes and quantitative results. Our findings showcase the capability to align any distinguishable text with an image through imperceptible adversarial attacks within a joint image-text model. More importantly, we show that our framework exhibits versatility, being agnostic to both the model architecture and dataset characteristics."}, {"title": "Datasets and Settings", "content": "Datasets. We conduct extensive experiments to evaluate our proposed framework on widely recognized publicly available vision datasets, namely ImageNet (Deng et al. 2009) and MS-COCO (Lin et al. 2015). For the text aspect, we adopt a methodology inspired by the work of Jones et al. (2023) and Borkan et al. (2019), obtaining a dataset comprising 68, 332, and 592 toxic comments with 1, 2, and 3 tokens respectively. Additionally, our framework undergoes evaluation using the Jigsaw toxic dataset available at Kaggle (van Aken et al. 2018).\nImplementation Details. To demonstrate the feasibility of the proposed method on large multimodal models, we have used the pretrained model publicly available by ImageBind, which in turn uses a CLIP model. More specifically, Image-Bind utilizes the pre-trained vision (ViT-H 630M params) and text encoders (302M params) from the OpenCLIP (Ilharco et al. 2021; Girdhar et al. 2023). The input size is 224 \u00d7 224 x 3, and the dimension of the embedding is 1024. We perform all our experiments on a lab workstation featuring two NVIDIA A5000 GPUs. We will provide source code for all our experiments in GitHub.\nAdditional Models. To demonstrate the broader applicability of the models, we thoroughly evaluate with several other multimodal models, including CLIPSeg (L\u00fcddecke and Ecker 2022), AltCLIP (Chen et al. 2022), BLIP-2 (Li et al. 2023), etc. An example with CLIPSeg is presented in the following subsection. The size of the input is determined by the models. For all the multimodal models we have used, a preprocessing step is used to resize the input image to 224 \u00d7 224 x 3 for subsequent processing. Therefore, the method works equally well regardless of the resolution of the original input images.\nEmbedding Projections. To obtain the low-dimensional projections of the images shown in Fig. 1 and other similar figures, the largest principal components are computed from a subset of images from the ImageNet dataset. Then, we project an embedding to be displayed along the six principal components with the largest eigenvalues. Note that the projections are used to illustrate the differences between embeddings, and details of the principal components would not impact the results significantly in that similar embeddings will have similar projections, and different embeddings will have different projections."}, {"title": "Experimental Results", "content": "To demonstrate the effectiveness of our method on deployed models such as the CLIP model, a key is to be able to match a given representation given by a phrase, a sentence, or any text sequence that can be encoded by the text transformer. We have tested the embedding matching procedure using many image and text pairs and Fig. 4 shows a typical example. The left of Fig. 4 shows the evolution of the loss when matching the embedding of an image to a specified target embedding. Similar to gradient descent, where the loss could become higher or lower, the high peaks indicate noise during the optimization process because the loss is non-linear. We use a small step size to make sure it converges. The right panel shows that cosine similarity increases steadily. We also show the average pixel value difference between the new input and the original image at each step; one can see the values remain very small even though they increase as well. The algorithm is not sensitive to the learning rate and works effectively across a broad range of values, spanning from 0.001 to 0.09. For instance, with a learning rate of 0.001, convergence is achieved in around 40,000 iterations, while 0.09 requires around 8,000 iterations. The visual differences in the resulting images are not noticeable. Eqn. 4 and 5 provide an explanation, as the gradient for our loss is insensitive to the learning rate.\nSystematic evaluation. To further demonstrate the effectiveness of the gradient procedure to match embeddings, we have applied them to numerous images and texts from different sources. Understanding the algebraic and geometric structures of the embedding space allows us to explore the"}, {"title": "Quantitative evaluation", "content": "space effectively. For example, we can find adversarial attacks to the embedding of any given image or text using the proposed gradient procedure. Fig. 1 shows two examples. To demonstrate the universal applicability of the procedure and the adversarial examples that exist almost everywhere, Fig. 3 shows more examples from different categories from the ImageNet dataset. See the appendix for additional examples on ImageNet and MS-COCO datasets. The efficacy of the procedure is model-agnostic. To substantiate and confirm this, Fig. 6 illustrates an example of a different multimodal model using CLIPSeg, showcasing the consistent application and effectiveness of the approach irrespective of the specific model employed. In addition, Fig. 7 shows real-world scenarios, demonstrating the practical relevance of our findings. All these examples convincingly demonstrate our method is model and dataset-agnostic.\nFig. 5 depicts the distribution of cosine similarities: the red and green curves represent cosine similarity values corresponding to pairs of text embeddings from the two toxic datasets under consideration respectively. The blue curve illustrates the distribution of cosine similarities between embeddings of image and text pairs aligned through our embedding process from the ImageNet data"}, {"title": "Adversarial Modification Detection", "content": "and toxic dataset. Essentially, the absence of overlap indicates that we can subtly modify an image corresponding to any selected text. In other words, with our approach, if we are provided with two or more texts, we can generate multiple visually indistinguishable images, one for each text, ensuring that a classifier will classify every image to the assigned text, regardless of the semantics of the images. Due to this characteristic, Table 1 demonstrates a 100% success rate (Carlini et al. 2023) in accurately matching the images with the toxic texts. To define the success rate, we first establish criteria for a successful image alignment. After aligning an image with the embedding of a specific text, we utilize\nWe have observed that the embedding-matched images exhibit much higher sensitivity to Gaussian noise than the original ones. Leveraging this insight, we have designed a detection algorithm introduced in our previous work (Salman et al. 2024). As demonstrated in that study, the detection algorithm performs reliably and consistently across a wide range of standard deviations. The process is as follows: we add Gaussian noise of a specified standard deviation to a given image and then classify them. If the labels of the two images agree, the image is unmodified; otherwise, the image is modified."}, {"title": "Discussion and Future Work", "content": "It may be attempting to categorize our framework as an adversarial attack technique. Our primary focus is on analyzing the embedding space; we utilize the ImageBind solely as a classifier to validate our findings and is not used otherwise. While our embedding matching procedure can be used to generate effective adversarial examples, it is fundamentally different. Our technique is classifier agnostic and does not exploit features specific to classifiers. Consequently, our examples with matched embeddings will appear to be the same to any classifier or downstream model that builds on embeddings. On the other hand, traditional adversarial attacks are specific to classifiers and applications, focusing on altering their outputs by changing the input.\nIdentifying adversarial attacks on multimodal models is very active (Qi et al. 2023; Schlarmann and Hein 2023; Evtimov et al. 2021). In general, all of them focus on how small changes in inputs can alter the final output (such as captions or classification labels) across various models. In contrast, our work identifies a new representation vulnerability. For instance, as shown in Fig. 1, three strawberry and cauliflower pairs can be made to be associated with three different texts, highlighting a more foundational vulnerability.\nThe plausible root cause of such adversarial examples and also semantically different images with identical embeddings is that transformers do not require the inputs to be aligned to have similar embeddings. By adding alignment-sensitive components to the embedding could mitigate the problem, which is being investigated further.\nGiven that the models are susceptible to such adversarial attacks, a logical question is if there is an effective method to mitigate the attacks. One potential way to do so is to train a model further to reduce the vulnerabilities. For deep neural networks, robust adversarial training has been used with success (Bai et al. 2021). It is unclear how much an adversarially trained model will affect the algorithm's ability to match images with text, and this is currently being investigated.\nThe results shown in this paper seem not to be consistent with the impressive results demonstrated by such models. Note that almost all existing results are measured on benchmark datasets. Due to the high dimensionality of the embedding space and the input space, even the largest dataset will cover the spaces very sparsely. We believe that systematic evaluations such as ours are necessary if one likes to evaluate models to be able to predict their behaviors in the entire space rather than on samples."}, {"title": "Conclusion", "content": "In this paper, using a gradient descent-based procedure, we have revealed a new vulnerability in multimodal models, where semantically unrelated inputs can have similar representations, and, at the same time, semantically identical images can have very different representations. Therefore, aligning different inputs to shared embedding space in a semantically meaningful way may not be viable. As multiple models are being developed, one must consider the vulnerabilities in multimodal models for secure applications. As the proposed technique can associate any image with any chosen text, one must understand the implications of this inherent vulnerability."}, {"title": "Appendix", "content": "Vision Transformers\nVery recently, several multi-modal models have been introduced. By using a shared embedding space among different modalities, such joint models have shown to have advantages. Vision transformers have been successful in various vision tasks due to their ability to treat an image as a sequence of patches and utilize self-attention mechanisms.\nhigh-dimensional vector representation. These patch embeddings are then supplied into the transformer blocks to be processed further (Dosovitskiy et al. 2021).\nAdditional Results\nHere we provide more details and additional information about the results we have included in the main text."}, {"title": "Image Quality Evaluation", "content": "+ .04 x\n+ .04 x\nFigure 9: Pixel differences between the original and corresponding embedding-aligned images in Fig. 1 (b) and (f); they are multiplied by 25 for visualization.\n. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are commonly used metrics to quantify the differences between the original and modified images (Hor\u00e9 and Ziou 2010; Morales, Klinghoffer, and Lee 2023). PSNR effectively measures the detailed quality of an image, whereas SSIM provides an intuitive assessment of its structural integrity. We present the average PSNR and SSIM values between the original and manipulated (i.e., embedding-aligned) images across all the datasets under consideration in Table 2. These metrics indicate that the image quality does not significantly degrade with minimal distortion. Due to resource and time constraints, we restricted the results to 800 examples for Table 2. We followed the approach by Szegedy et al. (Szegedy et al. 2014), where they used a smaller set (64 images) from ImageNet when calculating the average distortion of adversarial examples.\nMore Results. In the main paper, the results are mostly generated using the ImageNet and 1,2,3-tokens toxic dataset. To showcase the versatility of our framework across different vision and text datasets, the subsequent figures also present"}]}