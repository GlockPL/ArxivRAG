{"title": "Unaligning Everything: Or Aligning Any Text to Any Image in Multimodal Models", "authors": ["Shaeke Salman", "Md Montasir Bin Shams", "Xiuwen Liu"], "abstract": "Utilizing a shared embedding space, emerging multimodal models exhibit unprecedented zero-shot capabilities. However, the shared embedding space could lead to new vulnerabilities if different modalities can be misaligned. In this paper, we extend and utilize a recently developed effective gradient-based procedure that allows us to match the embedding of a given text by minimally modifying an image. Using the procedure, we show that we can align the embeddings of distinguishable texts to any image through unnoticeable adversarial attacks in joint image-text models, revealing that semantically unrelated images can have embeddings of identical texts and at the same time visually indistinguishable images can be matched to the embeddings of very different texts. Our technique achieves 100% success rate when it is applied to text datasets and images from multiple sources. Without overcoming the vulnerability, multimodal models cannot robustly align inputs from different modalities in a semantically meaningful way. Warning: the text data used in this paper are toxic in nature and may be offensive to some readers.", "sections": [{"title": "Introduction", "content": "Built on large pre-trained foundation models (Bommasani et al. 2022), applications have exhibited unprecedented capabilities for a wide range of tasks, setting new state-of-the-art on benchmark datasets, acing standard exams, and passing professional exams (OpenAI 2023; Brandes et al. 2022; Kung et al. 2023; Choi et al. 2023). Such models, however, are not well understood due to their complexity, even though the need for understanding and the risks of lacking is widely recognized and acknowledged (Bommasani et al. 2022). For example, transformers have become a hallmark component in models for many applications and have led to significant improvements in performance on benchmark datasets (Vaswani et al. 2017; Dosovitskiy et al. 2021; Devlin et al. 2019). By transforming inputs from different modalities (such as texts and images) to a common embedding space, emerging multimodal models provide new capabilities and new applications are being developed by exploiting the shared space (Radford et al. 2021).\nAt the same time, it is well known that neural networks exhibit an intriguing property in that they are subject to adversarial attacks: some small changes to an input could result in substantial changes in model responses and outputs (Goodfellow, Shlens, and Szegedy 2015; Szegedy et al. 2014; Chakraborty et al. 2021). While studies have shown adversarial examples exist to break even aligned models (Zou et al. 2023), it is not clear whether the shared space could be exploited to establish arbitrary associations between images and texts, or between two different modalities, therefore breaking the alignments that many of the models rely on in order to function properly.\nIn this paper, using a gradient-descent-based optimization procedure as detailed in our prior work (Salman, Shams, and Liu 2024), we show that perturbing an input image to a deployed model in unnoticeable ways can alter the resulting representation to match any chosen text and, therefore, reveal an inherent vulnerability of joint vision-text models. Since such multimodal models are being deployed, the identified vulnerability should be considered for these models. Furthermore, we show that the resulting inputs can dramatically change classification results with no modifications to the classifiers.\nTo highlight the main advantages of our framework, we present our results using multiple models, including the ImageBind (Girdhar et al. 2023).\nThese and additional results shown in the Experiments section, along with the fact we have obtained the same findings on all the image-text pairs we have used, demonstrate convincingly that there are visually indistinguishable inputs corresponding to the embeddings of very different texts, and yet there are very different images corresponding to the embeddings of identical texts. By analyzing the equivalence classes (Salman, Shams, and Liu 2024) of the embeddings"}, {"title": "Related Work", "content": "The transformer architecture (Vaswani et al. 2017) revolutionized NLP by effectively capturing long-range dependencies, resulting in powerful pre-trained models like BERT (Devlin et al. 2019) and GPT (Brown et al. 2020) that excel in various tasks. This advancement extends to computer vision with the Vision Transformer (ViT) (Dosovitskiy et al. 2021), showcasing the transformative impact of attention mechanisms across domains.\nThe recent prompting-based models and multimodal models have further accelerated the trend. The joint multimodal models have demonstrated significant benefits by employing a shared embedding space across various modalities. For example, CLIP (Radford et al. 2021) aligns vision and text representations. The model is trained to predict the coherence of image-text pairs, which enables it to understand complex relationships between the two modalities. Several recent works extend the technique of shared embedding space beyond text and vision by employing a unified embeddings space in various modalities are: GPT-4 (OpenAI 2023), MiniGPT-4 (Zhu et al. 2023), Flamingo (Alayrac et al. 2022), Bard (Pichai 2023), LLaVA (Liu et al. 2023) and, ImageBind (Girdhar et al. 2023).\nAnother line of research aims to comprehend models by probing them to unveil new properties. A well-studied problem is adversarial attacks, where unnoticeable changes to the input can cause the models, primarily classifiers, to change their predictions. Most adversarial attacks are applied to commonly used (deep) neural networks, including multiple-layer perceptrons and convolutional neural networks (CNNs), demonstrating their vulnerability and sensitivities to such adversarial changes (Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015). Croce and Hein propose AutoAttack, an ensemble of parameter-free attacks that combines multiple methods to provide a robust assessment of a model's vulnerability (Croce and Hein 2020). Recent studies have explored the vulnerability of multimodal models to adversarial attacks, which can potentially jailbreak aligned large language models (LLMs) or Vision Language models (VLMs) (Carlini et al. 2023; Qi et al. 2023; Zou et al. 2023). Bhojanapalli et al. investigate the robustness of ViTs against attacks where the attacker has access to the model's internal structure (Bhojanapalli et al. 2021; Shao et al. 2022). Notably, these methods revolve around generating adversarial examples based on the classifier's methodology rather than focusing on the representation level. However, our approach is different. Rather than crafting an adver-"}, {"title": "Preliminaries", "content": "As this paper focuses on vision-language models that are based on transformers, here we first describe the transformers mathematically and then describe the vision-language models. Transformers can be described mathematically succinctly, consisting of a stack of transformer blocks. A transformer block is a parameterized function class $f_e : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. If $x \\in \\mathbb{R}^{n \\times d}$ then $f_e(x) = z$ where $Q^{(h)}(x_i) = W_q x_i$, $K^{(h)}(X_i) = W_k x_i$, $V^{(h)}(x_i) = W_u x_i$, $W_{h,q}, W_{h,k}, W_{h,v} \\in \\mathbb{R}^{d \\times k}$. The key multi-head self-attention is a softmax function applying row-wise on the inner products.\n$a^{(h)}_{ij} = softmax_j(\\frac{(Q^{(h)}(x_i), K^{(h)}(x_j))}{\\sqrt{k}})$         (1)\nThe outputs from the softmax are used as weights to compute new features, emphasizing the ones with higher weights given by\n$u_i = \\sum_{h=1}^{H} \\sum_{j=1}^{n} \\alpha^{(h)}_{ij} V^{(h)}(x_j), W_{c,h} \\in \\mathbb{R}^{k \\times d}$.          (2)\nThe new features then pass through a layer normalization, followed by a ReLU layer, and then another layer normalization. Typically transformer layers are stacked to form deep models.\nWhile transformer models are widely used for natural language processing tasks, recently, they are adapted to vision tasks by using image blocks on the basic units, and spatial relationships among the units are captured via the self-attention mechanism (Dosovitskiy et al. 2021). A vision-language model based on transformers incorporates a dedicated transformer model for each input modality. The resulting representations from these modalities are mapped to a shared embedding space.\nFor the ImageBind model, we denote the model for image x as $f_1(x)$ and for text t as $f_T(t)$.\nGiven image x and C text labels, $t_0,..., t_{C-1}$,"}, {"title": "Methodology", "content": "Understanding the structures of the representation space is crucial for determining how the model generalizes. As introduced in our earlier work (Salman, Shams, and Liu 2024), we have proposed a framework to explore and analyze the embedding space of vision transformers, uncovering intriguing equivalence structures and their implications for model generalization and robustness. Generally, we model the representation given by a (deep) neural network (including a transformer) as a function $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$. A fundamental question is to have a computationally efficient and effective way to explore the embeddings of inputs by finding the inputs whose representation will match the one given by $f(x_{tg})$, where $x_{tg}$ is an input whose embedding we like to match. Informally, given an image of a strawberry, all the images that share its representation given by a model will be treated as a strawberry."}, {"title": "Embedding Alignment Procedure", "content": "As described in our previous work (Salman, Shams, and Liu 2024), the proposed approach for embedding alignment focuses on aligning the representation of an input with that of a target input. The core of the method is an iterative gradient optimization procedure, similar to how most of the neural networks are trained, except the gradient is calculated with respect to the input variables. Since we need to match two vectors, we define the loss for finding an input matching a given representation as\n$L(x) = L(x_0 + \\Delta x) = \\frac{1}{2} ||f_1(x_0 + \\Delta x) - f_T(t_{tg}) ||^2$,   (4)\nwhere $x_0$ is an initial image and $f_T(t_{tg})$ specifies the target embedding for a specified text sequence $t_{tg}$. Approximately, the gradient is given by\n$\\frac{\\partial L}{\\partial x} (\\frac{\\partial f_1}{\\partial x}|_{x=x_0})^T (f_1(x_0 + \\Delta x) - f_T(t_{tg}))$.          (5)\nIn each step, the algorithm first calculates the loss as defined by the loss function, between the image embedding with the target embedding as the one given by the specified text. Then using PyTorch, it computes the gradient by doing backward computation. After the gradient is calculated, we update the pixel values by doing gradient descent.\nEq. (5) shows how the gradient of the mean square loss function is related to the Jacobian of the representation function at $x = x_0$. In other words, the gradient is related to the differences between the current and target text representations. When the differences are large, the gradient should be significant as well. Consistent with this analysis, on all the examples we have experimented with, we are able to minimize the loss, and details are provided in the Experimental Results section and appendix.\nWhile local optimal solutions could be obtained by solving a quadratic programming problem or linear programming problem, depending on the norm to be used when minimizing Ax, the gradient function works effectively for all the cases we have tested due to the Jacobian of the transformer.\nOne of the practical issues using the gradient descent-based procedure is how to determine the learning rate. In the case of the transformers, the model can be approximated by a linear model when it moves within one activation region; note that it is approximate due to the nonlinearity of the softmax. The algorithm is able to find the matching representations for a wide range of learning rates; see the Experimental Results section for more details."}, {"title": "Experiments", "content": "In this section, we first outline the specifics of our experimental settings and implementation details. Our designed framework is systematically applied across various datasets and multiple multimodal models; in the subsequent subsections, we present both the experimental outcomes and quantitative results. Our findings showcase the capability to align any distinguishable text with an image through imperceptible adversarial attacks within a joint image-text model. More importantly, we show that our framework exhibits versatility, being agnostic to both the model architecture and dataset characteristics."}, {"title": "Datasets and Settings", "content": "Datasets. We conduct extensive experiments to evaluate our proposed framework on widely recognized publicly available vision datasets, namely ImageNet (Deng et al. 2009) and MS-COCO (Lin et al. 2015). For the text aspect, we adopt a methodology inspired by the work of Jones et al. (2023) and Borkan et al. (2019), obtaining a dataset comprising 68, 332, and 592 toxic comments with 1, 2, and 3 tokens respectively.\nImplementation Details. To demonstrate the feasibility of the proposed method on large multimodal models, we have used the pretrained model publicly available by ImageBind , which in turn uses a CLIP model. More specifically, ImageBind utilizes the pre-trained vision (ViT-H 630M params) and text encoders (302M params) from the OpenCLIP (Ilharco et al. 2021; Girdhar et al. 2023). The input size is 224 \u00d7 224 x 3, and the dimension of the embedding is 1024. We perform all our experiments on a lab workstation featur- ing two NVIDIA A5000 GPUs.\nAdditional Models. To demonstrate the broader applicability of the models, we thoroughly evaluate with several other multimodal models, including CLIPSeg (L\u00fcddecke and Ecker 2022), AltCLIP (Chen et al. 2022), BLIP-2 (Li et al. 2023), etc. An example with CLIPSeg is presented in the following subsection. The size of the input is determined by the models. For all the multimodal models we have used, a preprocessing step is used to resize the input image to 224 \u00d7 224 x 3 for subsequent processing. Therefore, the method works equally well regardless of the resolution of the original input images.\nEmbedding Projections. To obtain the low-dimensional projections of the images shown in Fig. 1 and other similar figures, the largest principal components are computed from a subset of images from the ImageNet dataset. Then, we project an embedding to be displayed along the six principal components with the largest eigenvalues. Note that the projections are used to illustrate the differences between embeddings, and details of the principal components would not impact the results significantly in that similar embeddings will have similar projections, and different embeddings will have different projections."}, {"title": "Experimental Results", "content": "To demonstrate the effectiveness of our method on deployed models such as the CLIP model, a key is to be able to match a given representation given by a phrase, a sentence, or any text sequence that can be encoded by the text transformer. We have tested the embedding matching procedure using many image and text pairs and Fig. 4 shows a typical example. The left of Fig. 4 shows the evolution of the loss when matching the embedding of an image to a specified target embedding. Similar to gradient descent, where the loss could become higher or lower, the high peaks indicate noise during the optimization process because the loss is non-linear. We use a small step size to make sure it converges. The right panel shows that cosine similarity increases steadily. We also show the average pixel value difference between the new input and the original image at each step; one can see the values remain very small even though they increase as well. The algorithm is not sensitive to the learning rate and works effectively across a broad range of values, spanning from 0.001 to 0.09. For instance, with a learning rate of 0.001, convergence is achieved in around 40,000 iterations, while 0.09 requires around 8,000 iterations. The visual differences in the resulting images are not noticeable. Eqn. 4 and 5 provide an explanation, as the gradient for our loss is insensitive to the learning rate.\nSystematic evaluation. To further demonstrate the effectiveness of the gradient procedure to match embeddings, we have applied them to numerous images and texts from different sources. Understanding the algebraic and geometric structures of the embedding space allows us to explore the"}, {"title": "Quantitative evaluation", "content": "Fig. 5 depicts the distribution of cosine similarities: the red and green curves represent cosine similarity values corresponding to pairs of text embeddings from the two toxic datasets under consideration respectively. The blue curve illustrates the distribution of cosine similarities between embeddings of image and text pairs aligned through our embedding process from the ImageNet"}, {"title": "Discussion and Future Work", "content": "It may be attempting to categorize our framework as an adversarial attack technique. Our primary focus is on analyzing the embedding space; we utilize the ImageBind solely as a classifier to validate our findings and is not used otherwise. While our embedding matching procedure can be used to generate effective adversarial examples, it is fundamentally different. Our technique is classifier agnostic and does not exploit features specific to classifiers. Consequently, our examples with matched embeddings will appear to be the same to any classifier or downstream model that builds on embeddings. On the other hand, traditional adversarial attacks are specific to classifiers and applications, focusing on altering their outputs by changing the input.\nIdentifying adversarial attacks on multimodal models is very active (Qi et al. 2023; Schlarmann and Hein 2023; Evtimov et al. 2021). In general, all of them focus on how small changes in inputs can alter the final output (such as captions or classification labels) across various models. In contrast, our work identifies a new representation vulnerability. For instance, as shown in Fig. 1, three strawberry and cauliflower pairs can be made to be associated with three different texts, highlighting a more foundational vulnerability.\nThe plausible root cause of such adversarial examples and also semantically different images with identical embeddings is that transformers do not require the inputs to be aligned to have similar embeddings. By adding alignment-sensitive components to the embedding could mitigate the problem, which is being investigated further."}]}