{"title": "XNet v2: Fewer Limitations, Better Results and Greater Universality", "authors": ["Yanfeng Zhou", "Lingrui Li", "Zichen Wang", "Guole Liu", "Ziwen Liu", "Ge Yang"], "abstract": "XNet introduces a wavelet-based X-shaped unified architecture for fully- and semi-supervised biomedical segmentation. So far, however, XNet still faces the limitations, including performance degradation when images lack high-frequency (HF) information, underutilization of raw images and insufficient fusion. To address these issues, we propose XNet v2, a low- and high-frequency complementary model. XNet v2 performs wavelet-based image-level complementary fusion, using fusion results along with raw images inputs three different sub-networks to construct consistency loss. Furthermore, we introduce a feature-level fusion module to enhance the transfer of low-frequency (LF) information and HF information. XNet v2 achieves state-of-the-art in semi-supervised segmentation while maintaining competitve results in fully-supervised learning. More importantly, XNet v2 excels in scenarios where XNet fails. Compared to XNet, XNet v2 exhibits fewer limitations, better results and greater universality. Extensive experiments on three 2D and two 3D datasets demonstrate the effectiveness of XNet v2.", "sections": [{"title": "I. INTRODUCTION", "content": "Biomedical image segmentation has achieved remarkable success with the development of deep neural networks (DNNs) [1]\u2013[3]. Fully-supervised training is a common learning strategy for semantic segmentation, which is trained with labeled images, using manual annotations as supervision signals to calculate supervised losses with segmentation predictions. Efficient encoder-decoder architecture is the mainstream paradigm in fully-supervised models. This architecture can accurately preserve the boundary information of segmented objects and alleviate overfitting on limited labeled images. Furthermore, some studies extend this architecture to 3D to meet the needs for volumetric segmentation. Recently, sequence-to-sequence transformers have become popular for biomedical image segmentation [4]\u2013[6]. Some research attempts to combine convolutional neural networks (CNNs) with transformers [3], [7], allowing the model to take advantage of both the low computational cost of CNNs and the global receptive field of transformers.\nCompared with fully-supervised model with the same number of labeled images, semi-supervised training has superior performance. It learns with a few labeled images and additional unlabeled images, which alleviates the need for laborious and time-consuming annotations. Semi-supervised models use the perturbation consistency of segmentation pre-dictions to construct the unsupervised loss and use it together with the supervised loss of labeled images as supervision signals to guide model training [8], [9]. Different perturbation strategies and different calculation methods of unsupervised loss produce various semi-supervised segmentation models, such as SASSNet [10], MC-Net [11], SPC [12], etc.\n[13] propose an X-shaped network architecture XNet, which can simultaneously achieve fully- and semi-supervised biomedical image segmentation. XNet uses LF and HF images generated by wavelet transform as input, then separately encodes LF and HF features and fuses them. For fully-supervision, XNet extracts and fuses the complete LF and HF information of the raw images, which helps XNet focus on the semantics and details of segmentation objects to achieve higher pixel-wise accuracy and better boundary contours. For semi-supervision, XNet constructs the unsupervised loss based on dual-branch consistency difference. This difference comes from different attention to LF and HF information, which alleviates the learning bias caused by artificial perturbations.\nHowever, XNet still shows performance degradation when images have little HF information. Furthermore, it also has the limitations in insufficient fusion and underutilization of raw image information.\nIn this study, we first analyze the limitations of XNet, then make targeted improvements and propose XNet v2. Different from directly using LF and HF images generated by wavelet transform as input, XNet v2 performs image-level complementary fusion of LF and HF images. The fusion results along with the raw images are fed into three different networks (main network, LF network and HF network) to generate segmentation predictions for consistency learning. Furthermore, similar to XNet, we introduce the feature-level fusion modules to better transfer LF and HF information between different networks. XNet v2 achieves state-of-the-art in semi-supervised segmentation while maintaining superior results in fully-supervised learning. More importantly, it still achieves competitive results in some scenarios where XNet cannot work (such as on the ISIC-2017 [14] and P-CT [15] datasets). Extensive benchmarking on three 2D and two 3D public biomedical datasets demonstrates the effectiveness of XNet v2."}, {"title": "II. METHOD", "content": "We analyze the limitations of XNet in Section II-A. Then we propose XNet v2 with fewer limitations and greater universality in Section II-B. Finally, we further introduce the components of XNet v2, including image-level and feature-level fusion in Section II-C and Section II-D, respectively."}, {"title": "A. Limitations of XNet", "content": "Performance degradation with hardly HF information.\nAs mentioned in [13], XNet is negatively impacted when images hardly have HF information. To intuitively illustrate this phenomenon, we compare the class activation map (CAM) [16] of HF encoder of XNet on CREMI [17] and ISIC-2017 [14]. From Figure 1, we can see that CREMI has rich HF information and HF encoder can better focus on these texture and edge details. In contrast, ISIC-2017 has less HF information, which prompts HF encoder to fail to extract recognizable information and locate specific segmentation objects.\nUnderutilization of raw image information. XNet uses LF and HF images generated by wavelet transform as input, and the raw images are not involved in training. Although LF and HF information can be fused into complete information in fusion module, the raw image may still contain useful but unappreciated information.  Insufficient Fusion. XNet only uses deep features for fusion. Shallow feature fusion and image-level fusion are also necessary."}, {"title": "B. Reduce Limitations and Increase Universality", "content": "In view of the limitations of XNet, we propose XNet v2 and show its overview in Figure 2. XNet v2 consists of three sub-networks: main network M, LF network L and HF network H. M, L and H are based on UNet [1] (3D UNet [18]). We use L and H to fuse with M and use their respective shallow and deep features to construct L&M and H&M fusion modules, which enables M to better absorb semantics and details. It also allows L and H to generate segmentation predictions with more perturbations.\nDifferent from directly using LF and HF images generated by wavelet transform as input, XNet v2 performs image-level complementary fusion of LF and HF images, which further reduces limitations and improves universality (we discuss it in detail in Section II-C). The fusion results along with the raw images are fed into L, H and M to generate segmentation predictions for consistency learning.\nXNet v2 uses LF and HF outputs to construct consistency loss with the output of M respectively, which avoids the instability of training loss when LF or HF information is insufficient. To be specific, XNet v2 is optimized by minimizing supervised loss on labeled images and triple output complementary consistency loss on unlabeled images. The total loss $L_{total}$ is defined as:\n$L_{total} = L_{sup} + L_{unsup}$,\nwhere $L_{sup}$ is supervised loss, $L_{unsup}$ is unsupervised loss, i.e., triple output complementary consistency loss, $\u03bb$ is a weight to control the balance between $L_{sup}$ and $L_{unsup}$. Same as [13], $\u03bb$ increases linearly with training epochs, $\u03bb = \u03bb_{max} * epoch/max\\_epoch$. We compare the performance of different $\u03bb_{max}$ in ablation studies of Section III-D.\nThe supervised loss $L_{sup}$ is defined as:\n$L_{sup} = L_{sup}(p^M, Y_i) + L_{sup}(p^L, Y_i) + L_{sup}(p^H, Y_i)$,\nwhere $p^M$, $p^L$ and $p^H$ represent segmentation predictions of M, L and H for the i-th image, respectively. $Y_i$ represents ground truth of the i-th image. The unsupervised loss $L_{unsup}$ is defined as:\n$L_{unsup} = L_{unsup}^{LM}(p^M, p^L) + L_{unsup}^{MH}(p^M, p^H)$,"}, {"title": "C. Image-Level Fusion", "content": "Different from [13], after using wavelet transform to generate LF image $I_L$ and HF image $I_H$, we fuse them in different ratios to generate complementary image $x^L$ and $x^H$. $x^L$ and $x^H$ are defined as:\n$x^L = I_L + \u03b1I_H$,\n$x^H = I_H + \u03b2I_L$,\nwhere $\u03b1$ and $\u03b2$ are the weights of $I_H$ and $I_L$, respectively. We can see that the input of XNet is a special case when $\u03b1 = \u03b2 = 0$ while our definition is a more general expression. \nSimple but Effective. This strategy is simple but achieves image-level information fusion. More importantly, it solves the limitation of XNet not working with less HF information. To be specific, when hardly have HF information, i.e., $I_H \u2248 0$:\n$x^L = I_L + \u03b1I_H \u2248 I_L$,\n$x^H = I_H + \u03b2I_L \u2248 \u03b2I_L \u2248 \u03b2x^L$.\n$x^H$ degenerates into a perturbation form of $x^L$, which can be regarded as consistent learning of raw images with two different LF perturbations. It effectively overcomes the failure to extract features when HF information is scarce.\nWe set $\u03b1$ and $\u03b2$ to change randomly within the range $[a, b]$ during training stage, which increases the diversity and randomness of training samples to further improve training quality."}, {"title": "D. Feature-Level Fusion", "content": "We use fusion module to transfer feature-level complementary information between L and M, H and M. Taking L&M fusion module as an example, we describe its structure. We use $E_n^M$ and $E_n^L$ to represent the n-th layer features of M and L, respectively. The fusion between $E_n^M$ and $E_n^L$ is shown in Figure 3. $E_n^M$ and $E_n^L$ perform channel concatenation to acquire features with twice the number of channels. Then we use 3x3 convolution to fuse features and concatenate the fused features to the decoders of M and L.\nFor M and L, we use deep (3rd and 4th) features for fusion. For M and H, we use shallow (1st and 2nd) features for fusion. The design of two fusion modules is asymmetric, which is also equivalent to introducing feature-level perturbations into the model."}, {"title": "III. EXPERIMENTS", "content": "We evaluate our model on three 2D datasets (GlaS [20], CREMI [17] and ISIC-2017 [14]) and two 3D datasets (P-CT [15] and LiTS [21])."}, {"title": "C. Comparison with State-of-the-art Models", "content": "Semi-Supervision. We compare XNet v2 extensively with 2D and 3D models on semi-supervised segmentation, including UAMT [22], URPC [23], CT [24], MC-Net+ [25], etc. From Table II and Table III, we can see that XNet v2 significantly outperforms previous state-of-the-art models in both 2D and 3D. Furthermore, because of the introduction of image-level complementary fusion and the effective utilization of raw images, XNet v2 has more competitive performance than XNet and is capable of handling scenarios where XNet cannot work (such as on the ISIC-2017 and P-CT datasets), which addresses the limitation of XNet in handling insufficient HF information.\nFully-Supervision. As previous experiments, XNet v2 still shows superior performance compared to XNet."}, {"title": "D. Ablation Studies", "content": "To verify effectiveness of each component, we perform the following ablation studies in semi-supervised learning."}, {"title": "Comparison of Range Combinations of \u03b1 and \u03b2", "content": "As shown in Equation (5), different range combinations of \u03b1 and \u03b2 produce different LF and HF complementary fusion images. To determine the optimal range combination , we conduct comparative experiments on GlaS. We set 3 value ranges for \u03b1 and \u03b2 to generate 9 combinations. Table V shows the results for GlaS and we find that larger \u03b1 and \u03b2 achieves better performance. According to the analysis in Section II-C, this may be because larger \u03b1 and \u03b2 alleviate the performance degradation of insufficient LF or HF information."}, {"title": "Effectiveness of Image-Level Complementary Fusion", "content": "We compare the performance of different image-level fusion strategies, including without fusion (\u03b1&\u03b2 = 0), single-sided fusion (\u03b1\u03b2 = 0) and complementary fusion (\u03b1&\u03b2 \u2260 0).  In contrast, complementary fusion can improve performance by a large margin, because it realizes the mutual complementation of missing frequency information."}, {"title": "Effectiveness of Raw Images", "content": "As mentioned in Section II-A, the information of raw images is also crucial for segmentation. In Table IX, we show the performance improvement of XNet v2 by introducing raw images. Furthermore, introducing additional branches for raw images can further improve performance, so we design additional main network M for raw images in XNet v2."}, {"title": "Effectiveness of Wavelet Perturbation", "content": "We compare the wavelet perturbation with other common perturbations, including Gaussian noise, network initialization, image smoothing and sharpening. We find that wavelet perturbation achieved better results. We also note that smoothing and sharpening can also enhance LF semantics and HF details but have a negative impact. Furthermore, we also apply various perturbations to MT [8] and acquire consistent conclusions."}, {"title": "Comparison of Model Size and Computational Cost", "content": "To illustrate that the performance improvement comes from well-designed components rather than the additional parameters brought by multiple networks. We compare the performance of semi-supervised models with the similar scale on GlaS and the results are shown in Table X. We find that the increase in the number of parameters (Params) and multiply-accumulate operations (MACs) cannot bring positive effects to these semi-supervised models. Furthermore, as in [13], we reduce the number of channels of XNet v2 to half and quarter to generate XNet v2\u00af and XNet v2\u00af\u00af. These lightweight models still have superior performance than lightweight XNet with similar scale (XNet\u00af and XNet\u00af\u00af). The above experiments strongly prove that the performance improvement comes from various designs rather than the increase in model size and computational cost."}, {"title": "Effectiveness of Components", "content": "To demonstrate the improvement of different components, we conduct step-by-step ablation studies on Glas and the results are shown in Table XI. Using raw images as input and training the semi-supervised model based on three independent UNet, we achieve a baseline performance of 78.80% in Jaccard. Using LF and HF complementary fusion images as input improves the baseline by 1.86% and 2.71% in Jaccard, respectively. Using them together further improves the baseline to 81.65% in Jaccard. Introducing L&M and H&M fusion modules improves the baseline by 3.88% and 3.31% in Jaccard, respectively. By introducing all components, we finally improve the baseline to 83.17% in Jaccard."}, {"title": "IV. CONCLUSION", "content": "We proposed XNet v2 to solve various problems of XNet, enabling it to maintain superior performance in scenarios where XNet cannot work. XNet v2 has fewer limitations, greater universality, and achieves state-of-the-art performance on three 2D and two 3D biomedical segmentation datasets. Extensive ablation studies demonstrate the effectiveness of various components.\nImages are essentially discrete non-stationary signals while wavelet transform can effectively analyze them. We believe that wavelet-based deep neural networks are a novel way for biomedical image segmentation."}]}