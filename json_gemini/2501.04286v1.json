{"title": "Mapping the Edge of Chaos:\nFractal-Like Boundaries in The Trainability of\nDecoder-Only Transformer Models", "authors": ["Bahman Torkamandi"], "abstract": "In the realm of fractal geometry, intricate structures emerge from simple iterative processes that\npartition parameter spaces into regions of stability and instability. Likewise, training large language\nmodels involves iteratively applying update functions, such as Adam, where even slight hyperpa-\nrameter adjustments can shift the training process from convergence to divergence. Recent evidence\nfrom miniature neural networks suggests that the boundary separating these outcomes displays fractal\ncharacteristics [1]. Building on these insights, this study extends them to medium-sized, decoder-only\ntransformer architectures by employing a more consistent convergence measure and examining the\nlearning rate hyperparameter landscape for attention and fully connected layers. The results show\nthat the trainability frontier is not a simple threshold; rather, it forms a self-similar yet seemingly\nrandom structure at multiple scales, with statistically consistent and repeating patterns. Within this\nlandscape, a region of stable convergence is surrounded by a complex chaotic border, illustrating the\nsensitive nature of the underlying training dynamics.", "sections": [{"title": "Introduction", "content": "Fractals are intricate and infinitely complex patterns that can emerge from simple, repeating processes. They are\ncharacterized by self-similarity, meaning their structure appears similar at any level of magnification. Fractals can be\nfound both in nature such as in the branching of trees, the formation of snowflakes, and coastlines and in mathematical\nconstructs. Their unique ability to model irregular and fragmented shapes makes them invaluable in various fields,\nincluding computer graphics, biology, and physics [2].\nA fractal can be described as a set that exhibits self-similarity at every scale and possesses a non-integer Hausdorff\ndimension, which quantifies its complexity. Fractal F typically has features that satisfies the following conditions:\n1. Similarity Transformations: There exists a number of similarity transformations (scalings, translations,\nrotations) that cover the fractal F with scaled-down copies of itself.\n2. Fine Structures: Fractal F contains detailed structure at arbitrary scales.\n3. Hausdorff Dimension: Most fractal have a Hausdorff dimension D that exceeds their topological dimension.\nThe Hausdorff dimension is defined using the Hausdorff measure, which extends the concept of measuring\nlengths, areas, and volumes to non-integer dimensions [3].\nMany fractals are generated through iterative processes. Iterated Function Systems (IFS) utilize the repeated application\nof contraction mappings to construct self-similar structures, as exemplified by the Sierpinski triangle [2]. Some fractals,\nhowever, do not display exact self-similarity but rather exhibit quasi-self-similar patterns. Another class of fractals is\nproduced by iterating complex functions; for instance, the Multibrot set is generated by iterating the complex function"}, {"title": null, "content": "f(z, c) = zd + c, where integer d > 2, and determining whether the sequence diverges or remains bounded based\non different values of the complex parameter c starting with 20 = 0. Likewise, Multijulia sets are defined by fixing\nthe parameter c and varying the initial values of z in the function f(z, c) = zd + c. The boundary in the complex\nplane, separating initial z values whose associated sequences remain bounded from those that diverge, exhibits a fractal\nstructure.\nAn intriguing real-world fractal example appears in chapter 5 of [4], Fractal Systems Generated by Randomwalks in\nTwo-Dimensional Space, fine particles arriving at a filter surface can be deposited on top of those collected earlier,\nproducing clusters that significantly affect respirable dust assessments. These seemingly random deposits often exhibit\npronounced clustering behavior. A straightforward way to visualize this effect is through a Monte Carlo simulation, in\nwhich entries from a random number table represent dust fineparticles, with both the random number table and resulting\nfractal structures shown in figure 1. Despite the underlying randomness, the resulting two-dimensional patterns can\nshow fractal-like clusters reminiscent of random walks, as repeated random placements can yield clumping structures.\nSuch overlaps raise the crucial question of whether a cluster on the filter consists of one large non-respirable agglomerate\nor multiple smaller respirable fineparticles; a misclassification can introduce considerable errors into estimations of\nrespirable dust hazards [4].\nNewton fractals provide another striking example of fractal geometry emerging from iterative processes. These fractals\narise from applying Newton's method to find roots of complex functions. Iterating from different initial points partitions\nthe complex plane into basins of attraction, each converging to a different root. The intricate, fractal boundaries between\nthese basins highlight the sensitivity and complexity inherent in such iterative processes [5].\nTraining neural networks involves iterative updates of model parameters\u2014commonly through optimization algorithms\nlike gradient descent or Adam-which can lead to either convergent or divergent outcomes. This process is highly\nsensitive to hyperparameters, and small adjustments can significantly impact the stability of training.\nWhen training a neural network, we iterate a function (e.g., a gradient descent step) involving many variables (the\nparameters of the neural network). For instance, if we perform full-batch gradient descent with a fixed learning rate \u03b7,\nwe update the parameters W by iterating the function:\n$W_{t+1} = W_t - \\eta \\nabla L(W_t)$\nwhere $\\nabla L(W_t)$ is the gradient of the training loss at iteration t. More advanced optimization algorithms like Adam [6]\nincorporate adaptive learning rates and momentum terms. The Adam optimizer updates the parameters using estimates\nof the first and second moments of the gradients:"}, {"title": null, "content": "$\\begin{aligned}\nm_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) \\frac{\\partial L(W_t)}{\\partial w_i} \\\\\nv_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) (\\frac{\\partial L(W_t)}{\\partial w_i})^2 \\\\\n\\hat{m}_{t+1} &= \\frac{m_{t+1}}{1 - \\beta_1^{(t+1)}} \\\\\n\\hat{v}_{t+1} &= \\frac{v_{t+1}}{1 - \\beta_2^{(t+1)}} \\\\\nw_{t+1} &= w_t - \\eta (\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon})\n\\end{aligned}$\nwhere $\\hat{m}$ and $\\hat{v}$ are the estimates of the first and second moments, $\\beta_1$ and $\\beta_2$ are exponential decay rates, and $\\epsilon$ is a\nsmall constant for numerical stability.\nRecently, transformer models have revolutionized natural language processing (NLP) by enabling efficient handling\nof sequential data through self-attention mechanisms. As these models scale to billions of parameters, training\nthem effectively becomes increasingly challenging. Issues such as vanishing or exploding gradients, sensitivity to\nhyperparameters, and non-convex optimization landscapes complicate the training process. Recent work by Sohl-\nDickstein [1] demonstrated that the boundary between hyperparameters leading to successful and divergent training in\nvery small neural networks exhibits fractal characteristics. This implies that small changes in hyperparameters can\nlead to significant differences in training outcomes, highlighting the complex and sensitive nature of the trainability\nlandscape. Understanding this boundary is crucial for developing robust training methodologies.\nIn this paper, inspired by the iterative processes used to generate fractals, I examine the fractal characteristics of the\nhyperparameter boundaries that separate stable from divergent training regimes in large decoder-only transformer\nmodels optimized by algorithms such as Adam. Drawing on the parallels between fractal generation and neural network\ntraining, I present empirical evidence of repetitive structures embedded within the hyperparameter landscape."}, {"title": "Related work", "content": null}, {"title": "Fractal Boundaries in Neural Networks", "content": "Sohl-Dickstein [1] demonstrated that the boundary between hyperparameters leading to successful and divergent\ntraining in shallow neural networks exhibits fractal characteristics. By visualizing training outcomes over grids of\nhyperparameters, the study revealed that the boundary is intricate and self-similar across different scales and training\nconditions [1].\nIn his experiments, the author trained a single hidden layer neural network comprising 16 neurons using gradient\ndescent and MSE loss, utilizing activation functions such as tanh or ReLU. The training dataset contained the same\nnumber of examples as the number of free parameters in the network, totaling 272. The inputs were initialized from a\nstandard normal distribution.\nTo assess convergence, the loss was scaled to start at 1. Training was considered to have converged if the mean of the\nlast 20 loss values fell below 1 [7]. The convergence measure was defined as follows:\n$\\text{convergence\\_measure} = \\begin{cases}\n \\frac{\\sum l_i}{l_i'} & \\text{if converged,} \\\\\n 1 & \\text{otherwise.}\n\\end{cases}$\nHe reported fractal dimensions ranging from 1.17 to 1.98 across various configurations. Specifically, a fractal dimension\nof 1.55 was observed when using a minibatch with the tanh activation, and learning rates for both the input and hidden\nlayers as hyperprameters."}, {"title": "Transformer Models", "content": "Introduced by Vaswani et al. [8], transformer architectures have reshaped language models by enabling parallel\nprocessing of sequential data through self-attention mechanisms. This paradigm shift allows models to capture long-\nrange dependencies more effectively than traditional recurrent architectures. Despite their success, training large\ntransformers remains challenging due to their scale and the complexity of their optimization landscapes. Building on\nthe foundational transformer architecture, various adaptations and enhancements have been proposed to address specific\nlimitations and extend their capabilities. Notably, decoder-only transformer models, such as the Generative Pre-trained\nTransformer (GPT) series [9, 10, 11], have gained significant attention for their prowess in generative tasks.\nAlthough transformers are most prominently used for text, they have been extended to other domains, including\ncomputer vision, speech recognition, and multimodal tasks. In computer vision, Vision Transformers (ViT) have\nachieved competitive results by treating image patches as \u201ctokens\" and applying attention across them, demonstrating\nperformance on par with or exceeding traditional convolutional networks [12]. Similarly, transformer-based approaches\nin speech recognition, such as Speech-Transformer, leverage the self-attention mechanism to model acoustic sequences\neffectively [13].\nDespite their architectural advantages, scaling transformer models to handle vast datasets and complex tasks introduces\nsignificant challenges. Large-scale transformers, encompassing billions of parameters, demand substantial computa-\ntional resources and memory bandwidth, making their training both time-consuming and economically demanding.\nAdditionally, the optimization landscape of such expansive models is fraught with difficulties, including numerous local\nminima and saddle points that can hinder effective convergence.\nTo address these challenges, several strategies have been employed:\n\u2022 Distributed Training and Model Parallelism: Techniques such as tensor parallelism, pipeline parallelism, and\ndata parallelism enable the distribution of model training across multiple GPUs or even clusters of machines,\nthereby alleviating memory constraints and reducing training times.\n\u2022 Advanced Optimization Algorithms: Beyond standard optimization methods like Adam, newer algorithms\nsuch as LAMB (Layer-wise Adaptive Moments optimizer for Batch training) [14] have been proposed to\nenhance convergence rates and stability during the training of large-scale transformers.\n\u2022 Regularization Techniques: Incorporating regularization methods like dropout and weight decay helps prevent\noverfitting and improves the generalization capabilities of transformer models."}, {"title": "Network Architecture", "content": "In this study, I employ a decoder-only transformer architecture tailored for autoregressive character prediction. Unlike\nthe standard transformer model, which consists of both encoder and decoder stacks, the model utilizes exclusively\ndecoder layers to generate the next character in a sequence based on the preceding context. Each decoder layer\nincorporates multi-head self-attention mechanisms and feedforward networks, complemented by layer normalization\nand residual connections to ensure stable and efficient training. The model comprises 95, 973 trainable parameters.\nThe network configuration is as follows:\n\u2022\n\u2022 Context Length: 64 tokens/characters.\n\u2022 Temperature: 0.3.\n\u2022 Mini Batch Size: 256.\n\u2022 Attention Key-Query Dimension: 64 (matching the model embedding dimension).\n\u2022 Positional Encoding: Utilizes a non-trainable sinusoidal positional encoder to preserve the sequential\ninformation of input characters, enabling the transformer to process data in an autoregressive manner.\n\u2022 Initialization: The weights in the neural network are initialized using default initializers provided by Flax\nlibrary, with the same initialization used each time the network is trained.\nLoss function : Loss function is the softmax cross-entropy loss."}, {"title": "Data", "content": "The dataset comprises 1,071, 890 strings, each containing 64 characters, extracted from the Complete Works of William\nShakespeare available on Project Gutenberg [15]. I use characters as tokens, resulting in a vocabulary size of 101\nunique tokens."}, {"title": "Convergence Measure", "content": "The convergence criterion and the convergence measure in Sohl-Dickstein [1] work is not sufficient nor consistent,\ntherefore the model is considered to have converged when:\n\u2022 The mean of recent losses is below 0.4 (determined based on the data and generation quality).\n\u2022 The mean of recent losses is at least 0.1 lower than the mean of early losses.\n\u2022 The variance of recent losses is below a threshold of 0.01.\nI use the first and last 5% of the training steps to calculate early and late losses, respectively.\nTo investigate the trainability boundary, I evaluate whether training runs converge or diverge across various hyperparam-\neter ranges, focusing on the learning rates for fully connected layers and attention mechanisms. First, I establish a more\nconcrete definition of convergence based on the loss function derived from hypothetical training scenarios that represent\nthe most convergent and divergent behaviors. I then convert the convergence metric to a color map, which visually\nrepresents the degree of convergence. This approach allows us to identify abrupt state changes and discern any chaotic\nor transitional phases in trainability. By mapping these variations, I can effectively visualize and analyze the stability of\nthe training process under different learning rate settings."}, {"title": null, "content": "Let:\nN = len(normalized_losses)\nC = cut off = 0.4\nM = Max"}, {"title": null, "content": "then 1 + $\\frac{\\sum_{i=1}^{N} l_i}{(N - 1)C}$ is the discrete approximation to the area under f3 . The convergence measure $\\mu$ is defined as:\n$\\mu = \\begin{cases}\n\\frac{\\sum_{i=1}^{N} l_i - (1 + (N - 1)C)}{(N-1)C} & \\text{if converged and } \\sum_{i=1}^{N} l_i \\geq 1 + (N - 1)C, \\\\\n\\frac{\\sum_{i=1}^{N} l_i - (1 + (N - 1)C)}{1 + (N - 1)(\u041c - \u0421)} & \\text{otherwise.}\n\\end{cases}$\nwhere $l_i$ is each element in normalized losses.\nThis formulation constrains the convergence measure to be between -1 and 1, where -1 represents the most divergent and\n1 represents the most convergent. This consistent measure allows us to use a color map proportional to the convergence\nmeasure to examine the trainability. I assign the most intense blue to 1 and the most intense red to -1, with white\nrepresenting 0.\nI also calibrated this convergence measure using the output of the language model. Due to predicting next character,\nlimited computational resources, fewer parameters, and less training data, developing a fully-fledged large language\nmodel (LLM) for English was not the focus. However, the model's ability to generate English-like words instead of\nrandom or repetitive characters indicates that it has achieved meaningful convergence and has learned from the data.\nExamples of generated response and the corresponding convergence measures for the language model tasked to complete\n\"To be or not to be\":\n\u2022\n\u2022\n\u2022\n$\\mu$: -0.2494\nTo be or not to be oq;\u00ebpy;;E*E;\u00ebEpppAW1\u00eb\u00ebEqX\u00ebWpE((\u00ebqqEq(W\u0153p*Wqq\u00ebqqq\u0153q\u00eb((\u00eb:\u00eb\u00ebqqqqWq1\n$\\mu$: -0.1313\nTo be or not to be ther the the the and that the and the the the athe at I ah\n$\\mu$: 0.1714\nTo be or not to be the painter the the are fartions the the so the ounlIIIIIIIIIIII\n$\\mu$: 0.4769\nTo be or not to be the partician of the soul and may and so some and thainomalovima\n\u2022 $\\mu$: 0.4883\nTo be or not to be the partice.\nCHIEF JUSTICE.\nI will be so your court trastristri"}, {"title": "Results", "content": "Using the more rigorous convergence criterion and a consistent convergence measure, I trained the language model (from\nsratch) across a range of learning rates, varying both the attention layers and all other layers. This analysis revealed\nself-similar patterns at multiple scales, with repeating statistical characteristics at different levels of magnification.\nIn figure 4, the regions that converged are shown in black and figure 5 presents the corresponding borders; these intricate\nboundaries have a dimension of 1.9772. The intricate patterns observed throughout the hyperparameter convergence\nlandscape underscore the complexity of trainability dynamics at a granularity of 10-5.\nFigure 6-a shows the hyperparameter convergence landscape for the learning rates of the attention layers and their\nlayer normalization ($\\mu_{att}$) versus all other layers ($\\mu_{fc}$). In this figure, a stable region is surrounded by chaotic patterns.\nZooming into a specific region (Figure 6-b and 4) reveals intricate divergent and convergent behaviors, indicating\na chaotic boundary. Boundaries in this zoomed region have a fractal dimension of 1.9772. Further magnification\n(Figure 7-b) uncovers similar patterns; this area, with a fractal dimension of 1.9715, shows statistical self-similarity\nacross scales and chaotic tendencies. Figure 8 likewise, demonstrates self-similar characteristics\u2014as evidenced by\nsimilar textures and convergence-measure histograms at a Granularity of 10-10\u2014which also has edges with a fractal\ndimension of 1.9649. Similarly, figure 9 reproduces these textures and statistical properties, the edges show a box-count\ndimension of 1.9783 (Figure 9-b).\nThe statistical characteristics of the ($\\mu_{att}$)-($\\mu_{fc}$) landscape are nearly identical across different scales, as confirmed by\nthe convergence-measure histograms. Figures 10, 11, 12, and 13 illustrate this property at resolutions of 10\u22125,10-8,\n10-10, and 10-11, respectively."}, {"title": null, "content": "For completeness, I conducted the same experiments in two additional boundary regions. The findings again revealed\nsimilar statistical self-similarity, with non-integer fractal dimensions, at the upper portion of the region separating\ndivergent and convergent behaviors (Figures 14, 15, 16, and 17). Boundaries in this area have fractal dimensions of\n1.8118 and 1.8218, respectively. Further inspection of the lower portion (Figures18, 19, 20, and 21) also confirmed the\npresence of the same phenomenon, with dimensions 1.5810 and 1.5413. Overall, these results suggest a highly chaotic\nboundary between convergence and divergence in transformer-based language models with statistical self similarity at\ndifferent scales."}, {"title": "Conclusion", "content": "This study investigated the nature of hyperparameter-space boundaries separating stable from divergent training regimes\nin a decoder-only transformer architecture. Drawing parallels to fractal-generating iterative processes observed in\nsmaller networks, a more consistent convergence measure was introduced to visualize the training landscape over various\nlearning rates for attention and fully connected layers. The results revealed repeating patterns across multiple scales,\nexhibiting chaotic features and self-similarity, nearly identical statistical characteristics, as evidenced by histograms of\nconvergence measure and non-integer box-counting dimensions.\nOwing to constrained training conditions, data, and computational resources, exploring more expansive hyperparameter\nspaces or larger models was not feasible. Nonetheless, these findings open promising avenues for further research.\nExpanding these experiments to more complex model architectures, larger datasets, and more diverse optimization\nalgorithms could help determine whether such intricate and scale-invariant boundaries are intrinsic features of modern\nlarge-scale deep learning."}, {"title": "Appendix", "content": "I use box-counting method to estimate fractal dimensions and Sobel operator to detect edges. For verification porpuses,\nI applied the box-counting technique to Sierpinski fractal with dimension 1.585 and tested edge detection by identifying\nthe edges of the Mandelbrot set. Both results are presented in figures 22, 23, 24 and 25.\nBy applying those techniques to a black and white version of the hyperprameter landscape, I was able to calculate the\nfractal dimension for different levels of magnification."}]}