{"title": "Fully tensorial approach to hypercomplex neural networks", "authors": ["Agnieszka Niemczynowicz", "Rados\u0142aw Antoni Kycia"], "abstract": "Fully tensorial theory of hypercomplex neural networks is given. The key point is to observe that the algebra multiplication can be represented as a rank three tensor. This approach is attractive for neural network libraries that support effective tensorial operations.", "sections": [{"title": "1 Introduction", "content": "The fast progress in applications of Artificial Neural Networks (NN) promotes new directions of research and generalizations. This involves advanced mathematical concepts such as group theory [19], differential geometry [5, 6], or topological methods in data analysis [7].\nThe core of NN implementations lies in linear algebra usage. In most popular programming libraries (TensorFlow [1], PyTorch [15]), the most popular architecture"}, {"title": "2 Methods", "content": "This section provides an overview of the mathematical theory behind the operations used in implementing hypercomplex neural networks. This is a tenet of methods used in this paper. These are classical notions explained in detail in standard references, e.g., [2]."}, {"title": "2.1 Tensors", "content": "The primary object that is used in NN implementations is a tensor. It relies on the tensor product described in the following definition"}, {"title": "Definition 1.", "content": "The tensor product of two vector spaces $V_1$ and $V_2$ over the field $F$ is the vector space denoted by $V_1 \\otimes V_2$ and defined as a quotient space $V_1 \\times V_2/L$, where $L$ is a subspace of $V_1 \\times V_2$ that is spanned by\n\n$(v + w, x) - (v, x) - (w,x)$,\n$(v, x + y) - (v, w) - (v, x)$,\n$(\\lambda v, x) - \\lambda(v, x)$,\n$(v, \\lambda x) - \\lambda(v, x)$,\n\nwhere $v, w \\in V_1, x, y \\in V_2, \\lambda \\in F$.\nBy induction, it can be defined for $k$ vector spaces ${V_i}_{i=1}^k$ and denoted by $V_1 \\otimes \\dots \\otimes V_k$ that can be dented by $ \\bigotimes_{i=1}^k V_i$.\nThe tensor product can also be defined for duals spaces $V^*$ of a vector spaces $V_i$ for $i \\in {1, ... k}, 0 < k < \\infty$, and we can define mixed tensor product made for vector spaces and their duals.\nThe tensor product of vector space is a vecotr space, so we can define a base, e.g., if the base of $V$ is ${e_i}_{i=1}^{N_1}$ and $W$ is ${f_j}_{i=1}^{N_2}$, then the base of $V \\otimes W$ is ${e_i \\otimes f_j}_{i,j=1}^{N_1,N_2}$.\nThe tensor space can be used to decompose any multilinear mapping. It is expressed in the universal factorization theorem for tensor product. It states that for a bilinear mapping $F: V \\times W \\rightarrow X$ of vector spaces $V, W, X$ can be uniquely factorized by a new mapping $\\hat{F}: V \\otimes W \\rightarrow X$ according to Fig. 1. Then the map $F$ is called the tensor."}, {"title": "Example 1.", "content": "In the base ${e_i}$ of $V$ and ${f_j}$ of $W$ the bilinear mapping $F : V \\otimes W \\rightarrow \\mathbb{R}$ has the form\n\n$F = F_{ij}e_i \\otimes f_j$,\n\nwhere $F_{ij} \\in \\mathbb{R}$ for all $i, j$, are the coefficients of a numerical matrix that is a representation of the multilinear mapping (tensor) $F$ in the fixed base of $V$ and $W$. The matrix collects the components of the tensor in a fixed tensor base.\nThe matrix $[F_{ij}]$ is implemented in TensorFlow and PyTorch library as a tensor class. The critical difference is that the mathematical tensors have specific properties of transformations under the change of basis of underlying vector spaces. The libraries implement the tensors as a multidimensional matrix of numbers. Moreover, they do not keep the upper (contravariant) or lower (covariant) position of indices.\nThe example can be extended to the tensor product of multiple vector spaces and their duals."}, {"title": "Example 2.", "content": "The linear mapping $A: V \\rightarrow W$ can be written as a mapping $A \\in V^*\\otimes W \\rightarrow \\mathbb{R}$ that can be written in the base ${e^i}$ of $V^*$ and ${f_i}$ of $W$ as $A= A^i_j e^i f_j$.\nThe vector space of linear operators is denoted as $L(V,W)$.\nFor tensors we also often use abstract index notation where we provide only components of the tensor, e.g., $A_{ij}$, understanding them not as fixed base numerical values but as a full tensor $A_{ij}e^i e_j$, see [16],\nSince the tensor product is a functor in the category of linear spaces [2], therefore, for a linear mapping $A: V \\rightarrow W$, we can define the extension of the mapping for a tensor product space $\\otimes_{i=1}^n A: \\otimes_{i=1}^n V \\rightarrow \\otimes_{i=1}^n W$ by acting on product as $A(v_1 \\otimes \\dots \\otimes v_n) = Av_1 \\otimes ... \\otimes Av_n$ and extending by linearity for all combinations.\nThis is similar behaviour as for the Cartesian product of vector spaces. The Carte- sian product is also a functor, and therefore, all the above operations apply in this case.\nWe can define a few linear algebra operations realized in tensor libraries."}, {"title": "Broadcasting:", "content": "it is defined on for a linear operator $A : V \\rightarrow W$ to be an multilinear extension\n\n$\\mathfrak{b}: L(V, W) \\rightarrow L(\\times_{i=1}^n V, \\times_{i=1}^n W)$,\n\nthat is $\\mathfrak{b}(A)(v_1, ..., v_n) = (Av_1,..., Av_n)$. Similar broadcasting is realized for the tensor product. Both operations relies on functoriality of Cartesian product and tensor product."}, {"title": "Transposition/Permutation:", "content": "The transposition of two components relies on the following fact: there is the unique mapping $\\tau:V\\otimes W\\rightarrow W\\otimes V$ that simply reverses the order of factors $\\tau(v \\otimes w) = w \\otimes v$. We can extend it to arbitrary permutation of $n$ numbers, $p : {1,..., n} \\rightarrow {1,..., n}$, we have $T_p(V_1 \\otimes \\dots \\otimes V_n) = V_{p(1)} \\otimes ... \\otimes V_{p(n)}$.\nWe can define a similar operation for the Cartesian product.\nIn the abstract index notation, we have $T_pA_{i_1...i_k} = A_{i_{p(1)}...i_{p(k)}}$."}, {"title": "Reshaping:", "content": "For a pair of indices $i, j$ of the range $0 < R(i) < \\infty$ and $0 < R(j) < \\infty$ we can define a reshape operation, which is the new sole index $Rs(i, j)$ that value depends on the values of $i, j$ given by the function: $Rs(i, j) = iR(j) + j$. We can extend the operation for the pair $(p, p + 1)$ of neighbour indices of a tensor by $Rs_pA_{i_1...i_p i_{p+1}...i_k} = A_{i...Rs(i_p,i_{p+1})...i_k}$, where $p + 1 < k$. This operation changes only the way of indexing; however, it is useful in applications. In the abstract index notation we can write $Rs_pA_{i_1...i_p i_{p+1}...i_k} = A_{i_1...i_{p-1}Rs(i_p,i_{p+1})i_{p+2}...i_k}$."}, {"title": "Contraction:", "content": "For a two tensors $A$ and $B$, and a fixed base ${e^i}$ of $V$, the contraction of indices $p$ (related to $V$) and $q$ (related to $V^*$) is (note implicit sum): $C_{p,q}(A, B) = A(\\dots e_i \\dots) B(\\dots e^i \\dots)$ where $A \\in ... V \\otimes \\dots \\otimes V^* ...$ and $B \\in ... V^* \\otimes \\dots \\otimes V ...$\nThe contraction can also be defined for a single tensor in the same way, e.g., in abstract index notation for a single tensor $T$, ones get $C_{pq}T = T_{\\dots i_p \\dots i_q \\dots}$, where implicit summation was applied."}, {"title": "Concatenation:", "content": "joins the tensors ${A^{(i)}}_{i=1}^n$ of the same shape along given dimension $j$, i.e., $K_j({A^{(i)}}_{i=1}^n) = A^{(1)} \\dots A^{(i_{j-1})} A^{(i_j)} \\dots A^{(i_k)}$."}, {"title": "2.2 (Hypercomplex) Algebras", "content": "In this part we introduce mathematical concepts related to hypercomplex and general algebras, as in the following definition."}, {"title": "Definition 2.", "content": "The algebra over a field $F$ is a vector space $V$ equipped with a product a binary operation $\\cdot : V \\times V \\rightarrow V$ with the following properties:\n\n$(x + y) \\cdot z = x \\cdot z + y \\cdot z$,\n$z \\cdot (x + y) = z \\cdot x + z \\cdot y$,\n$(\\alpha x) \\cdot (\\beta y) = \\alpha\\beta (x \\cdot y)$,\n\nfor $x, y, z \\in V$ and $\\alpha, \\beta \\in F$.\nMoreover, the algebra is commutative if $x \\cdot y = y \\cdot x$ for $x, y \\in V."}, {"title": "Example 3.", "content": "For real numbers $\\mathbb{R}$ we have $V = {e_0}$, $F = \\mathbb{R}$ with $e_0 \\cdot e_0 = 1$.\nComplex numbers $\\mathbb{C}$ can be obtained from commutative algebra with $V = {e_0, e_1}$, $F = \\mathbb{R}$ and $e_0 \\cdot e_0 = e_0$, $e_0 \\cdot e_1 = e_1$ and $e_1 \\cdot e_1 = -e_0$.\nBy convention we will always assume that the neutral element of the algebra mutliplication is $e_0$.\nEfficient use of algebras in computations based on tensors (TensorFlow, PyTorch) relies on converting the product within the algebra into a tensor operation.\nTreating algebra as a vector space with additional structure of vector mutliplication we have the following definition, which is essential to the rest of the paper."}, {"title": "Definition 3.", "content": "For an algebra $V$ over $F$ the product can be defined as a tensor $A \\in V^* \\otimes V^* \\otimes V$. Selecting the base of $V = span{e_i}_{i=0}^{n-1}$ and the dual base $V^* = {e^i}_{i=1}^{n-1}$ with $e^i(e_j) = \\delta^i_j$, the product has the form\n\n$A = A^k_{ij} e^i \\otimes e^j \\otimes e_k$.\n\nThen the multiplication table entry is presented in (5).\n\n$\\begin{array}{c|c}\ne_j & \\\\\ne_i & A^k_{ij}\\end{array}$\n\nThe tensor coefficients (abstract index notation), $A^k_{ij}$, play the same role as structure constants for a group [8]. These coefficients in a fixed base can be represented as a multidimensional matrix (called tensors in TensorFlow and PyTorch). When the algebra is commutative, then $A(x, y) = A(y,x)$ or $A^k_{ij} = A^k_{ji}$."}, {"title": "3 Results", "content": "In this section, we provide mathematical details of the implementation of hypercomplex dense and convolutional neural networks."}, {"title": "3.1 Hypercomplex Dense layer", "content": "We start with a description of the dense layer. It is a general-purpose layer that operates on the data with additional dimensionality, which is a multiple algebra dimensions. We assume that the input data are of dimension $b \\times al \\times in$, where $b$ is the batch size, $al$ - the algebra size, and $in$ - the positive integer multiplier. The last two numbers determine the input data size. The input tensor $X = X_{isRs(ia,iin)}$, where $is$ - batch index, $ial$ is the dimension of algebra, $iin$ is the multiplicity index of algebra dimension. Moreover, we use learning parameters (weights/kernel) $K = K_{ialiiniu}$, where $iu$ is the index over units/neurons. The bias $b = b_{Rs(ial,iu)}$ is used if needed. Kernel and bias are usually initialized with numbers taken from specific distributions [9].\nWe now provide the algorithm of hypercomplex dense network in Algorithm 1. We offer both tensorial and abstract index notations (AIN). We need two flags bias - if bias is included and activation if activation function $\\sigma$ is used. The Keras with"}, {"title": "Algorithm 1 Hypercomplex dense NN", "content": "Require: $X, A, \\sigma, bias, activation$\nEnsure: $K, b$ - initialized\n$W \\leftarrow C_{1,0}(A, K) [AIN: W_{iaka_{liniu}} \\leftarrow \\sum_j A_{iajk} K_{ji_{liniu}}]$\n$W \\leftarrow T_{p=(0\\rightarrow0,1\\rightarrow2,2\\rightarrow1,3\\rightarrow3)} (W) [AIN: W_{iatinKa_{Linlu}} \\leftarrow W_{p(i_{aka}i_{liniu})}]$\n$W \\leftarrow Rs_{1,0}( Rs_{0}(W)) [AIN: W_{i_{1}i_{2}} \\leftarrow W_{Rs(i_{a},i_{in})Rs(k_{a},i_{u})}]$\n$Output \\leftarrow C_{1,0}(X, W) [AIN: Output_{sibi_z} \\leftarrow \\sum_k X_{isi_{k}=Rs(i_{a},i_{in})} W_{ki_{2}}]$\nif bias is True then\n  $Output \\leftarrow Output + b$\nend if\nif activation is True then\n  $Output \\leftarrow b(\\sigma)(Output)$\nend if"}, {"title": "3.2 Hypercomplex Convolutional layer", "content": "In this part the hypercomplex convolutional neural network will be described. We present general $k$-dimensional ($k = 1, 2, 3$) layers. They differ by the shape of the input data and kernel size."}, {"title": "Algorithm 2 Hypercomplex k-dimensional convolutional NN", "content": "Require: $X, A, \\sigma, bias, activation$\nEnsure: $K, b$ - initialized\n$W \\leftarrow C_{1,0}(A, K) [AIN: W_{iakai_{l_1...l_ki_{in}i_f}} \\leftarrow \\sum_j A_{iajk} K_{ji_{1}...i_{k}i_{in}i_f}]$\n$W \\leftarrow T_{p=(0\\rightarrow k+2)} (W) [AIN: W_{i_{l_1...l_ki_{atin}i_f}} \\leftarrow W_{p(i_{aka}i_{1}...i_{k}i_{in}i_f)}]$\n$W \\leftarrow Rs_{k+2}(W) [AIN: W_{kai_{1}...l_ki_{j}i_f} \\leftarrow W_{kai_{1}...l_k Rs(i_{a}, i_{in})i_f}]$\n\ntemp = []\nfor $i \\in {0, . . ., al}$ do $temp = temp + [conv_{k \\text{D}}(X,W_{i...})]$\nend for\n$Output_{ibil...i_{l_k}h} \\leftarrow K_{k+1}(temp)$\nif bias is True then\n  $Output \\leftarrow Output + b$\nend if\nif activation is True then\n  $Output \\leftarrow b(\\sigma)(Output)$\nend if"}, {"title": "4 Conclusion", "content": "In this paper, we introduced the mathematical details of the implementation of dense and convolutional NN based on hypercomplex and general algebras. The critical point in this presentation is to associate algebra multiplication with rank-three tensor. Thanks to this observation, all the NN processing steps can be represented as tensorial operations.\nThe fully tensorial operations applied to algebra operations simplifies neural networks operations, and allows to support for fast tensorial operations in modern packages as TensorFlow and PyTorch. Implementation of the above generalized hypercomplex NN is described elsewhere [13]."}]}