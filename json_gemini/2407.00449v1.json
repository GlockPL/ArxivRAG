{"title": "Fully tensorial approach to hypercomplex neural networks", "authors": ["Agnieszka Niemczynowicz", "Rados\u0142aw Antoni Kycia"], "abstract": "Fully tensorial theory of hypercomplex neural networks is given. The key point is to observe that the algebra multiplication can be represented as a rank three tensor. This approach is attractive for neural network libraries that support effective tensorial operations.", "sections": [{"title": "1 Introduction", "content": "The fast progress in applications of Artificial Neural Networks (NN) promotes new directions of research and generalizations. This involves advanced mathematical concepts such as group theory [19], differential geometry [5, 6], or topological methods in data analysis [7].\nThe core of NN implementations lies in linear algebra usage. In most popular programming libraries (TensorFlow [1], PyTorch [15]), the most popular architecture is feed forward NN, which is based on a stack of layers where the data passes between them unidirectionally. Optimized tensorial operations realize the flow of the data.\nThere are different algebraic extensions. One of these paths is Algebraic Neural Networks [14], where the additional endomorphism operations on data are performed. The other algebra-geometry direction is the neural networks based on Geometric Alge- bra/Clifford algebra [4, 17]. Recently, Parametrized Hypercomplex Neural Networks were invented [10] for convolutional layers. They can learn optimal hypercomplex algebra adjusted to data, exploring optimized Kronecker product. However, in some applications keeping hyperalgebra of even more general algebra parameters as (fixed) hyperparameters is needed. In such a way we can optimize algebra structure at the metalevel.\nIn this paper we discuss implementation in which we change classical real algebra computations into various hypercomplex or even more general algebras computations. This approach presented, e.g., in [3] is not new. However, there is a revival of interest of this direction due to better complexity properties in such areas as image processing [18] or time series analysis [11]. In these contributions the Open Source code [18] for specific four-dimensional hypercomplex algebras was used.\nThe implementation explained in this article significantly expands the ideas from [18] for arbitrary algebras, including hypercomplex ones. The algorithms described here agree with the NN presented in [18] for specific 4-dimensional hypercomplex algebras. However, implementation of [18] was obtained by constructing an additional multiplication structure from the multiplication table for the hypercomplex algebra, which is treated as additional step in setting up neural network. Our approach permit us to omit this complexity and generalize to arbitrary algebras. This is very important contribution form theoretical treatment of general algebraic approach to hypercomplex neural networks.\nThe main contribution of this paper is following:\n\u2022 summarize basic concepts on tensorial operations in terms of hypercomplex and more general algebras, especially, we noted that the algebra multiplication can be expressed as a third-rank tensor,\n\u2022 provide general algorithm for computations within hypercomplex dense layer,\n\u2022 provide general algorithms for 1-, 2-, and 3-dimensional hypercomplex convolutional layer computations."}, {"title": "2 Methods", "content": "This section provides an overview of the mathematical theory behind the operations used in implementing hypercomplex neural networks. This is a tenet of methods used in this paper. These are classical notions explained in detail in standard references, e.g., [2].\n2.1 Tensors\nThe primary object that is used in NN implementations is a tensor. It relies on the tensor product described in the following definition"}, {"title": "2.1 Tensors", "content": "Definition 1. The tensor product of two vector spaces $V_1$ and $V_2$ over the field $F$ is the vector space denoted by $V_1 \\otimes V_2$ and defined as a quotient space $V_1 \\times V_2/L$, where $L$ is a subspace of $V_1 \\times V_2$ that is spanned by\n\\begin{equation}\n\\begin{array}{c}\n(v + w, x) \u2013 (v, x) \u2013 (w,x), \\\\\n(v, x + y) \u2013 (v, w) \u2013 (v, x), \\\\\n(\\lambda v, x) \u2013 \\lambda (v, x), \\\\\n(v, \\lambda x) \u2013 \\lambda (v, x),\n\\end{array}\n\\label{eq:tensorproductdef}\n\\end{equation}\nwhere $v, w \\in V_1$, $x, y \\in V_2$, $\\lambda\\in F$.\nBy induction, it can be defined for $k$ vector spaces $\\{V_i\\}_{i=1}^k$ and denoted by $V_1 \\otimes \\dots \\otimes V_k$ that can be dented by $\\bigotimes_{i=1}^k V_i$.\nThe tensor product can also be defined for duals spaces $V^*$ of a vector spaces $V_i$ for $i \\in \\{1, \\dots k\\}$, $0 < k < \\infty$, and we can define mixed tensor product made for vector spaces and their duals.\nThe tensor product of vector space is a vecotr space, so we can define a base, e.g., if the base of $V$ is $\\{e_i\\}_{i=1}^{N_1}$ and $W$ is $\\{f_j\\}_{j=1}^{N_2}$, then the base of $V \\otimes W$ is $\\{e_i \\otimes f_j\\}_{i,j=1}^{N_1,N_2}$.\nThe tensor space can be used to decompose any multilinear mapping. It is expressed in the universal factorization theorem for tensor product. It states that for a bilinear mapping $F: V \\times W \\rightarrow X$ of vector spaces $V, W, X$ can be uniquely factorized by a new mapping $\\hat{F}: V \\otimes W \\rightarrow X$ according to Fig. 1. Then the map $\\hat{F}$ is called the tensor.\nMoreover, in the map $\\hat{F}$, we can move $X$ to the domain of the map, i.e., we can define $\\tilde{F} : V \\times W \\times X \\rightarrow F$ instead of $F$, where $F$ is a field common to the vector spaces.\nExample 1. In the base $\\{e_i\\}$ of $V$ and $\\{f_j\\}$ of $W$ the bilinear mapping $F : V \\otimes W \\rightarrow \\mathbb{R}$ has the form\n\\begin{equation}\nF = F_{ij}e_i \\otimes f_j,\n\\label{eq:tensorproduct}\n\\end{equation}\nwhere $F_{ij} \\in \\mathbb{R}$ for all $i, j$, are the coefficients of a numerical matrix that is a representa- tion of the multilinear mapping (tensor) $F$ in the fixed base of $V$ and $W$. The matrix collects the components of the tensor in a fixed tensor base.\nThe matrix $[F_{ij}]$ is implemented in TensorFlow and PyTorch library as a tensor class. The critical difference is that the mathematical tensors have specific properties of transformations under the change of basis of underlying vector spaces. The libraries implement the tensors as a multidimensional matrix of numbers. Moreover, they do not keep the upper (contravariant) or lower (covariant) position of indices.\nThe example can be extended to the tensor product of multiple vector spaces and their duals."}, {"title": null, "content": "Example 2. The linear mapping $A : V \\rightarrow W$ can be written as a mapping $A \\in V^*\\otimes W \\rightarrow \\mathbb{R}$ that can be written in the base $\\{e^i\\}$ of $V^*$ and $\\{f_i\\}$ of $W$ as\u00b9 $A = A_{i}^j e^i \\otimes f_j$.\nThe vector space of linear operators is denoted as $L(V,W)$.\nFor tensors we also often use abstract index notation where we provide only com- ponents of the tensor, e.g., $A_{ij}$, understanding them not as fixed base numerical values but as a full tensor $A_{ij}e^i \\otimes e_j$, see [16],\nSince the tensor product is a functor in the category of linear spaces [2], therefore, for a linear mapping $A : V \\rightarrow W$, we can define the extension of the mapping for a tensor product space $\\bigotimes_{i=1}^n A: \\bigotimes_{i=1}^n V \\rightarrow \\bigotimes_{i=1}^n W$ by acting on product as $A(v_1 \\otimes ... \\otimes v_n) = Av_1 \\otimes ... \\otimes Av_n$ and extending by linearity for all combinations.\nThis is similar behaviour as for the Cartesian product of vector spaces. The Carte- sian product is also a functor, and therefore, all the above operations apply in this case.\nWe can define a few linear algebra operations realized in tensor libraries.\n\u2022 Broadcasting: it is defined on for a linear operator $A : V \\rightarrow W$ to be an multilinear extension\n\\begin{equation}\nb: L(V, W) \\rightarrow L(\\times_{i=1}^n V, \\times_{i=1}^n W),\n\\label{eq:broadcasting}\n\\end{equation}\nthat is $br(A)(v_1, ..., v_n) = (Av_1, ..., Av_n)$. Similar broadcasting is realized for the tensor product. Both operations relies on functoriality of Cartesian product and tensor product.\n\u2022 Transposition/Permutation: The transposition of two components relies on the following fact: there is the unique mapping $\\tau : V \\otimes W \\rightarrow W \\otimes V$ that simply reverses the order of factors $\\tau(v \\otimes w) = w \\otimes v$. We can extend it to arbitrary permutation of $n$ numbers, $p : \\{1,..., n\\} \\rightarrow \\{1,..., n\\}$, we have $T_p(V_1 \\otimes \\dots \\otimes V_n) = V_{p(1)} \\otimes ... \\otimes V_{p(n)}$. We can define a similar operation for the Cartesian product.\nIn the abstract index notation, we have $T_pA_{i_1...i_k} = A_{i_{p(1)}...i_{p(k)}}$.\n\u2022 Reshaping: For a pair of indices $i, j$ of the range $0 < R(i) < \\infty$ and $0 < R(j) < \\infty$ we can define a reshape operation, which is the new sole index $Rs(i, j)$ that value depends on the values of $i, j$ given by the function: $Rs(i, j) = iR(j) + j$. We can extend the operation for the pair $(p, p + 1)$ of neighbour indices of a tensor by $Rs_pA_{i_1...i_p i_{p+1}...i_k} = A_{i_1...Rs(i_p, i_{p+1})...i_k}$, where $p + 1 < k$. This operation changes only the way of indexing; however, it is useful in applications. In the abstract index notation we can write $Rs_pA_{i_1...i_p i_{p+1}...i_k} = A_{i_1...i_{p-1}Rs(i_p, i_{p+1})i_{p+2}...i_k}$.\n\u2022 Contraction: For a two tensors $A$ and $B$, and a fixed base $\\{e_i\\}$ of $V$, the contraction of indices $p$ (related to $V$) and $q$ (related to $V^*$) is (note implicit sum): $C_{p,q}(A, B) = A(\\dots e_i\\dots)B(\\dots e^i\\dots)$ where $A \\in \\dots \\otimes V \\otimes \\dots \\otimes V^* \\otimes \\dots$ and $B \\in \\dots \\otimes V^* \\otimes \\dots \\otimes V \\otimes \\dots$\nThe contraction can also be defined for a single tensor in the same way, e.g., in abstract index notation for a single tensor $T$, ones get $C_{pq}T = T_{\\dots i_p \\dots}^\\dots i_q \\dots$, where implicit summation was applied."}, {"title": null, "content": "\u2022 Concatenation: joins the tensors $\\{A^{(i)}\\}_{i=1}^{A(2)}$ of the same shape along given dimension $j$, i.e., $K_j(\\{A^{(i)}\\}_{i=1}^{A(2)}) = A_{i_1...i_{j-1}k_j i_{j+1}...i_k}$.\n2.2 (Hypercomplex) Algebras\nIn this part we introduce mathematical concepts related to hypercomplex and general algebras, as in the following definition.\nDefinition 2. The algebra over a field $F$ is a vector space $V$ equipped with a product a binary operation $\\cdot : V \\times V \\rightarrow V$ with the following properties:\n\u2022 $(x + y) \\cdot z = x \\cdot z + y \\cdot z$,\n\u2022 $z \\cdot (x + y) = z \\cdot x + z \\cdot y$,\n\u2022 $(\\alpha x) \\cdot (\\beta y) = \\alpha \\beta (x \\cdot y)$,\nfor $x, y, z \\in V$ and $\\alpha, \\beta \\in F$.\nMoreover, the algebra is commutative if $x \\cdot y = y \\cdot x$ for $x, y \\in V$.\nExample 3. For real numbers $\\mathbb{R}$ we have $V = \\{e_0\\}$, $F = \\mathbb{R}$ with $e_0 \\cdot e_0 = 1$.\nComplex numbers $\\mathbb{C}$ can be obtained from commutative algebra with $V = \\{e_0, e_1\\}$, $F = \\mathbb{R}$ and $e_0 \\cdot e_0 = e_0$, $e_0 \\cdot e_1 = e_1$ and $e_1 \\cdot e_1 = -e_0$.\nBy convention we will always assume that the neutral element of the algebra mutliplication is $e_0$.\nEfficient use of algebras in computations based on tensors (TensorFlow, PyTorch) relies on converting the product within the algebra into a tensor operation.\nTreating algebra as a vector space with additional structure of vector multi- plication\u00b2 we have the following definition, which is essential to the rest of the paper.\nDefinition 3. For an algebra $V$ over $F$ the product can be defined as a tensor $A \\in V^* \\otimes V^* \\otimes V$. Selecting the base of $V = span\\{e_i\\}_{i=0}^{d-1}$ and the dual base $V^* = \\{e^i\\}_{i=1}^{d-1}$ with $e^i(e_j) = \\delta_{j}^{i}$, the product has the form\n\\begin{equation}\nA = A_{ij}^k e^i \\otimes e^j \\otimes e_k.\n\\label{eq:algebra product}\n\\end{equation}\nThen the multiplication table entry is presented in (5).\n\\begin{equation}\n\\begin{array}{c|c} & e_j \\\\\n\\hline e_i & A_{ij}^k e_k\n\\end{array}.\n\\label{eq:table}\n\\end{equation}\nThe tensor coefficients (abstract index notation), $A_{ij}^k$, play the same role as struc- ture constants for a group [8]. These coefficients in a fixed base can be represented as a multidimensional matrix (called tensors in TensorFlow and PyTorch). When the algebra is commutative, then $A(x, y) = A(y, x)$ or $A_{ij}^k = A_{ji}^k$."}, {"title": "3 Results", "content": "In this section, we provide mathematical details of the implementation of hypercom- plex dense and convolutional neural networks."}, {"title": "3.1 Hypercomplex Dense layer", "content": "We start with a description of the dense layer. It is a general-purpose layer that oper- ates on the data with additional dimensionality, which is a multiple algebra dimensions. We assume that the input data are of dimension $b \\times al \\times in$, where $b$ is the batch size, $al$ - the algebra size, and $in$ - the positive integer multiplier. The last two numbers determine the input data size. The input tensor $X = X_{i_s R_s(i_{al},i_{in})}$, where $i_s$ - batch index, $i_{al}$ is the dimension of algebra, $i_{in}$ is the multiplicity index of algebra dimen- sion. Moreover, we use learning parameters (weights/kernel) $K = K_{i_{al} i_{in} i_u}$, where $i_u$ is the index over units/neurons. The bias $b = b_{R_s(i_{al},i_u)}$ is used if needed. Kernel and bias are usually initialized with numbers taken from specific distributions [9].\nWe now provide the algorithm of hypercomplex dense network in Algorithm 1. We offer both tensorial and abstract index notations (AIN). We need two flags bias - if bias is included and activation if activation function $o$ is used. The Keras with\nAlgorithm 1 Hypercomplex dense NN\nRequire: $X, A, \\sigma$, bias, activation\nEnsure: $K, b$ - initialized\n$W \\leftarrow C_{1,0}(A, K)$ [AIN: $W_{i_{akaliniu}} \\leftarrow \\Sigma_j A_{aijka} K_{jiiniu}$]\n$W \\leftarrow T_{p=(0 \\rightarrow 0, 1 \\rightarrow 2, 2 \\rightarrow 1, 3 \\rightarrow 3)}(W)$ [AIN: $W_{i_{iatinkatu}} \\leftarrow W_{p(i_{akaliniu})}$]\n$W \\leftarrow Rs_10 R_s(W)$ [AIN: $W_{i_1 i_2} \\leftarrow W_{Rs(i_a,i_{in})Rs(k_a,i_u)}$]\n$Output \\leftarrow C_{1,0}(X, W)$ [AIN: $Output_{i_{sibiz}} \\leftarrow \\Sigma_k X_{i_s k=Rs(i_{al},i_{in})} W_{ki_2}$]\nif bias is True then\n$Output \\leftarrow Output + b$\nend if\nif activation is True then\n$Output \\leftarrow b(\\sigma)(Output)$\nend if\nTensorFlow implementation and PyTorch implementations are given in [13]."}, {"title": "3.2 Hypercomplex Convolutional layer", "content": "In this part the hypercomplex convolutional neural network will be described. We present general k-dimensional (k = 1, 2, 3) layers. They differ by the shape of the input data and kernel size.\nThe additional 'image channels' of the data can be packed as an element of algebra. For instance, the two-dimensional image can be decomposed as a matrix of four- dimensional algebra elements, i.e., color channels plus alpha, making a single pixel an element of four-dimensional algebra.\nThe general idea of NN action, as in [18], is to use algebra constants $A$ tensor to separate the coefficients of the algebra base component and then apply the traditional convolution for each algebra component.\nThe dimension of the input data $X = X_{ibi_1...i_k R_s(i_{al},i_{in})}$ of dimension $b \\times n_1 \\times \\dots \\times n_k \\times al \\times in$, where $b$ is the batch size, $n_i$, $i \\in \\{1,...,k\\}$ - the size of data sample in each dimension, $al$ - the algebra dimension, and $in$ - the positive multiplier\u00b3.\nThe kernel size is $al \\times L_1 \\times ... L_k \\times in \\times F$, where: $L_i$, $i \\in \\{1, ..., k\\}$ is the kernel dimension in each dimension, and $F$ the number of filters. We therefore have the kernel $K = K_{i_{al} i_{l_1}...i_{l_k} i_{in} i_f}$. The flag bias indicates the bias $b$ usage. If it is used, it has dimension $al \\times F$. Kernel and bias can be initialized by arbitrary distributions [9]. We use standard k-dimensional convolution $convkD(X, K, strides, padding)$ for convoluting algebra components, there are standard optimized convolution operations [12]. The algorithm is presented in the Algorithm 2.\nAlgorithm 2 Hypercomplex k-dimensional convolutional NN\nRequire: $X, A, \\sigma$, bias, activation\nEnsure: $K, b$ - initialized\n$W \\leftarrow C_{1,0}(A, K)$ [AIN: $W_{i_{akai_1 i_1...i_{l_k} i_{in} i_f}} \\leftarrow \\Sigma_j A_{aijka} K_{ji_1...i_{l_k} i_{in} i_f}$]\n$W \\leftarrow T_{p=(0 \\rightarrow k+2)}(W)$ [AIN: $W_{ai_1...i_{l_k} i_{in} i_f} \\leftarrow W_{p(i_{akai_1 i_1...i_{l_k} i_{in} i_f})}$]\n$W \\leftarrow Rs_{k+2}(W)$ [AIN: $W_{kai_1...i_{l_k} jf} \\leftarrow W_{kai_1...i_{l_k} Rs(ia, Vin) f}$]\ntemp = []\nfor $i \\in \\{0, . . ., al\\}$ do temp = temp + $[convkD(X,W_{i...})]$ end for\n$Output_{ibi_1...i_1kh} \\leftarrow K_{k+1}(temp)$\nif bias is True then\n$Output \\leftarrow Output + b$\nend if\nif activation is True then\n$Output \\leftarrow b(\\sigma)(Output)$\nend if"}, {"title": "4 Conclusion", "content": "In this paper, we introduced the mathematical details of the implementation of dense and convolutional NN based on hypercomplex and general algebras. The critical point in this presentation is to associate algebra multiplication with rank-three tensor. Thanks to this observation, all the NN processing steps can be represented as tensorial operations.\nThe fully tensorial operations applied to algebra operations simplifies neural networks operations, and allows to support for fast tensorial operations in mod- ern packages as TensorFlow and PyTorch. Implementation of the above generalized hypercomplex NN is described elsewhere [13]."}]}