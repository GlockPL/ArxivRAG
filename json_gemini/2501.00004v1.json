{"title": "NewsHomepages: Homepage Layouts Capture Information Prioritization Decisions", "authors": ["Ben Welsh", "Arda Kaz", "Michael Vu", "Naitian Zhou", "Alexander Spangher"], "abstract": "Information prioritization plays an important role in how humans perceive and understand the world. Homepage layouts serve as a tangible proxy for this prioritization. In this work, we present NewsHomepages, a large dataset of over 3,000 new website homepages (including local, national and topic-specific outlets) captured twice daily over a three-year period. We develop models to perform pairwise comparisons between news items to infer their relative significance. To illustrate that modeling organizational hierarchies has broader implications, we applied our models to rank-order a collection of local city council policies passed over a ten-year period in San Francisco, assessing their \"newsworthiness\". Our findings lay the groundwork for leveraging implicit organizational cues to deepen our understanding of information prioritization.", "sections": [{"title": "1 Introduction", "content": "The way humans prioritize and organize information is central to attention and understanding. Prior work has explored the need for effective prioritization to manage information overload (Miller, 1956) and including case-studies of prioritization approaches (Rosenfeld, 2002; Hays, 2018). However, larger scale analyses, specifically with an aim towards predictive modeling, are often limited by lack of available data and methods.\nIn this work, we seek to rectify this by focusing on a prominent source of publicly available, highly curated examples of prioritization decisions. News organizations' homepages are meticulously crafted by professional human editors: their layouts offer manifestations of the news organization's prioritization principles (Boukes et al., 2022). We use homepage layouts to study information organization at scale. We present NewsHomepages, to our knowledge the first large-scale homepage"}, {"title": "2 Background and Dataset", "content": "Visual cues for editorial importance on homepages have a deep history in the design principles of physical newspapers (Barnhurst and Nerone, 2001) and result from deliberate editorial processes. At The New York Times, for example, top editors and designers convened daily in the renowned Page One meeting (Usher, 2014) to determine the most important articles for the print newspaper the next day. In the digital era, meetings like this evolved into Homepage Meetings (Sullivan, 2016), influencing the design and content placement on the website's homepage for the upcoming day. As such, homepages continue to be distillations of professional judgement and priorities.\nOne visual cue editors use is positional placement, with articles positioned towards the top and left of a page considered more important (Nielsen, 2006). This stems from observations that readers naturally begin scanning from the top-left corner (Bucher and Schumacher, 2006). Secondly, the space articles occupy is considered: larger articles or headlines are perceived as more important (Garc\u00eda, 1987). In print media, prominence is conveyed through more column space; in digital media, longer headlines, featured images, and extended summaries. Finally, graphics and design also play a pivotal role in signaling the importance of news stories. Articles accompanied by photographs, videos, or other multimedia elements are often deemed more significant (Zillmann et al., 2001). The use of capital letters, bold fonts, and color further enhances a story's prominence.\nWe find few large-scale computational analyses studying these attributes. To enable a more precise study of editorial judgement, we construct a large corpus of news homepage layouts, over which we can track these indicators of relative importance."}, {"title": "2.1 Compilation of News Homepages", "content": "We compiled a list of 3,489 news homepages, as of the time of this writing, which we scraped twice daily on an ongoing basis over a period of three years. From 2019-2024, we have collected a total of 363,340 total snapshots. Our dataset collection is actively maintained and facilitated by a large contributing community of over 35 activists, developers and journalists. We collect homepages"}, {"title": "2.2 Data Collection Pipeline", "content": "Our dataset collection runs in a chron job twice a day, and uploads data to Internet Archive. For each snapshot, we store the following information:\n1. All links on the page: We store a flat-list of hyperlinks on every homepage and associated text.\n2. Full-page screenshots: We store JPGs of each complete homepage as we render it.\n3. Complete HTML snapshots (subset of pages): For a subset of homepages, we save a compressed version of the webpage, including all CSS files and images, using SingleFile.\nIn addition to our Internet Archive storage, we also synchronize with Wayback Machine to store these homepages, providing a secondary backup and ensuring long-term preservation."}, {"title": "3 Dataset Processing", "content": "In order to robustly extract visual attributes for each article on a homepage (i.e. size, position, presence of graphics), we need to determine bounding boxes for all articles on a homepage. Examples of bounding boxes are shown in Figure 1: each bounding box, also referred to as article card, covers all information directly associated with that article.\nLayout parsing is a well-researched field (Shen et al., 2021; Li et al., 2020). However, homepages"}, {"title": "3.1 Bootstrapping a Bounding Box Detector", "content": "Following other bootstrapping approaches (Amini et al., 2022), we: (1) develop a simple deterministic algorithm to generate candidate data, (2) apply a filtering step to exclude low-quality data, (3) use our high-precision dataset to train a more robust classifier. Figure 7, in the Appendix, provides an overview of the pipeline."}, {"title": "Step 1: Find Bounding Boxes Deterministically", "content": "We design a deterministic algorithm, called the DOM-Tree algorithm, to start our bootstrapping process. At a high level, the algorithm traces each <a> tag in the Document Object Model (DOM) and extracts the largest subtree in the DOM that contains only a single <a> tag (illustrated in Figure 4, Appendix). This method can extract the maximal bounding box for each article, however it faces robustness challenges, for example, if a link exists within an article card (e.g. a link to an authors page, as shown in Figure 4b, Appendix.)\nWe apply this algorithm to a subset of the NewsHomepages dataset, combining 15 homepages each from all outlets for which we have HTML files, JPEG snapshots, and hyperlink json files (approximately 15,000 homepages). Since each outlet typically maintains a consistent layout on their homepages across samples, we include more outlets for generalizability."}, {"title": "Step 2: Filter Low-Quality Bounding Box Extractions", "content": "We take several filtering steps to prevent \"drift\" (Amini et al., 2022). (1) First, we exclude non-news article links (e.g. log-in pages) by training simple text classifier to distinguish between URLs to news articles and others. We manually labeled over 2,000 URLs. The model achieves an accuracy of 96%. (2) Then, we exclude bounding boxes that did not overlap highly with link text. We determine this by first rendering the HTML pages as images and overlaying bounding boxes, then running OCR to extract the bounding-box text. (3) Finally, we exclude bounding boxes with im-"}, {"title": "Step 3: Train a More Robust Classifier", "content": "Now with our dataset in hand, we trained a Detectron2 model (Wu et al., 2019). Our model uses ResNet-101 as a backbone with a Feature Pyramid Network (FPN) for extracting multi-scale features and Smooth L1 loss for bounding box regression. During training, we used a base learning rate of 0.02 with a linear warmup over the first 1000 steps. We trained the model for 10,000 steps in total, with learning rate reductions after 5000 steps. A weight decay of 0.0001 and momentum of 0.9 were also employed. The training ran on 4\u00d7A40 GPUs for 24 hours."}, {"title": "3.2 Evaluation and Results", "content": "To evaluate the quality of our bounding box detection, we conducted manual validation for four types"}, {"title": "4 Homepage Modeling", "content": "Given precise layout information for the 363k homepages in our dataset, we arrive again at the core question of this research: how regular and predictable is an outlet's layout decisions? We initially hypothesized that modeling homepage placement would be challenging. As shown in Figures 3, 5, 6 and described by (Ang\u00e8le, 2020), certain areas of many homepages lack clear editorial consistency. This introduces noise and makes it difficult to learn a uniform policy guiding article placement decisions. Further, learning a single set of policies is challenged by the changing news cycle; some days have lots of news while others have less."}, {"title": "4.1 Modeling Approach", "content": "A homepage is intended to present a collection of articles as a cohesive bundle; individual articles do not exist in isolation (Tufte, 1990). Predicting the placement of a single article without considering the context of other articles would be overly noisy and potentially ineffective (Salganik et al., 2006). Conversely, attempting to predict the placement of all articles simultaneously poses a combinatorial challenge that is computationally infeasible.\nTo address this issue, we formulate our modeling task as a pairwise preference problem. Specifically, we consider pairs of articles (a1, a2) and train models to predict a binary preference variable p, where\n$P_o(a_1 > a_2) = 1$ if article $a_1$ is preferred over article $a_2$ for outlet o, and $P_o(a_1 > a_2) = 0$ otherwise."}, {"title": "4.2 Modeling Variations", "content": "We explored different modeling variations on the New York Times homepages, as they have a variety of content, shares and functionalities on their site (Spangher, 2015). We test 5 different models: {distilbert-base-uncased, flan-t5-base, flan-t5-large, roberta-base, roberta-large} and constructed a training dataset of 74,857 article-pairs and a test dataset consisting of 18,715 datapoints consisting of pairs of NYTimes articles from same homepages.\nWe experienced exploding gradients in the flan-t5-large and RoBERTa-large models, motivating us to use a learning rate limit of 5e-5 for all the models, for the sake of equal comparison."}, {"title": "4.3 Dataset Selection and Processing", "content": "From our list of 3,000 outlets, we select 31 outlets for detailed analysis. We selected well-known outlets in various categories, including different political leanings (left-leaning vs. right-leaning), local and national levels, and varied subject matters such as science, chess and aviation. For each outlet, we collected between 200 and 300 homepage snapshots, resulting in 1,000 to 50,000 pairs of articles. We created an 80/20 train/test split and trained distilbert-base-uncased models for each outlet. We trained each model with 5e-5 learning rate limit, 3 epochs, 0.01 weight decay.\nEach article in our dataset includes its textual representation as it appeared on the homepage. To enhance the reliability of our models, we undertake several data processing steps informed by preliminary experiments: (1) we only sample pairs of articles that are adjacent on the homepage, to curate preference pairs that are more likely to be challenging and topically similar. Secondly, we clean the textual representations by stripping out any times, dates, and formatting elements. We also remove author names to prevent the models from learning biases based on authors who might be favored by the organization. Please refer to Appendix A for a detailed list of the outlets used and the specific number of data points associated with each."}, {"title": "4.4 Results", "content": "We show our results in Table 4. While some models (e.g. Breitbart) perform noticeably poorly, we note that the majority of our models score above f1 > .6. We do not find a significant correlation between model performance and training set size. We were surprised to observe the tractability of this task; this indicates that many of the concerns we had about noise were either handled by our preprocessing steps, or not as important as we believed."}, {"title": "5 Demonstrations", "content": "To evaluate the practical utility of our models, we design two downstream tasks: (1) analyzing newsworthiness agreement between publishers, and (2) using newsworthiness models to rank corpora of interest to journalists."}, {"title": "5.1 Task 1: Newsworthiness Agreement Between Publishers", "content": "In this task, we aim to rank-order lists of news items drawn from a larger pool of articles to calculate the agreement rates for newsworthiness decisions between different news outlets. Previous research has observed surprising overlaps in sentiment and preferences between right-leaning and left-leaning outlets (Gentzkow and Shapiro, 2010), and we wish to quantitatively test this phenomenon using our preference models.\nWe selected 9 of the 31 outlets for which we trained preference models in the previous section. From each outlet, we sampled 1,000 articles, matching on variables such as topic, length, publication date, and other potential confounders. These 9 outlets were chosen because they represent a range of political viewpoints.\nFor each model no; (corresponding to outlet oi), we used it to sort lists of 1,000 articles {a1, a2,..., a1000}19 from outlets {o}1. In other words, the output of applying model no; to the article list from outlet oj is a fully sorted list no(Aj). We used the size \u00d7 position model for this experiment, as performance was similar to the size-only model, and we believed that the multi-variable models capture more newsworthiness information than the single-variable models.\nWe calculated Kendall's \u03c4, a correlation measure for ordinal data, between each pair of sorted lists (noi(Ak), noj(Ak)) for all i, j, k, and averaged the correlations across j. The resulting correlation matrix is displayed in Figure 2a. Some surprising insights emerge from this analysis. Notably, Breitbart, a right-leaning outlet, and Mother Jones, a left-leaning outlet, have one of the highest rates of agreement.\nTo establish a baseline and ensure we are not merely capturing topic overlap (despite matching on topics), we conducted a simple SBERT embedding experiment (Reimers and Gurevych, 2019). We sampled a set of 100 articles per outlet, generated embeddings using SBERT, and averaged these embeddings to create single outlet-level embed-"}, {"title": "5.2 Task 2: Surfacing Potentially Newsworthy Leads", "content": "In this task, we explore how well these newsworthiness judgments transfer outside of the news domain. In this task, we build on the work of Spangher et al. (2023a). The authors introduced the task of newsworthiness prediction as a detection and alerting system for journalists: utilizing a list of San Francisco Board of Supervisors' policies (a typical source of stories for journalists), they attempted to detect which policies were more newsworthy in order to alert journalists.\nHere, we suspect that editorial cues from different homepages will help us surface especially newsworthy content based on the preferences of each outlet. We applied the models from each outlet to sort the list of Board of Supervisors' policies. Then, we selected the top 10 items from the ordered lists no, and used a large language model (LLM) to summarize the key points raised in each policy.\nThe LLM's summarization results and examples are shown in Table 5. Themes emerge, with subject-specific outlets like The Weather Channel highlighting policies related to environmental issues. We presented these results to a group of journalists, and 81% of respondents indicated they were impressed and would consider using such a system in their workflow. These findings demonstrate the potential of our models to assist journalists in identifying newsworthy leads from large corpora of documents, thereby supporting investigative journalism and timely reporting."}, {"title": "6 Discussion", "content": "Our demonstrations show two core findings: first, editorial priorities and decision-making can be inferred simply by examining the layout decisions made on homepages. This decision-making is distinct from simple topic preferences, as we show. In fact, commonalities about decision-making can be observed between outlets that appear distinct topically. Second, newsworthiness judgements have potential to be used in tools for reporting.\nStepping back, these results indicate that homepage editorial cues provide an interesting, novel angle for news analysis, as well as a tantalizing direction in newsworthiness detection (Spangher et al., 2023a; Diakopoulos et al., 2010). Both of these applications are premised by the assumption that editorial cues learned from one outlet's homepage can be transferred to other domains, be it another outlet's articles, or non-news content. This is an important assumption: the intuitive findings that we have made in our demonstrations provide some degree of proof that this transfer is robust.\nWe experimented with different ways of making this transfer even more robust. We attempted to train additional models to serve as in-domain and out-of-domain classifiers, and then multiplied the probabilities: $P(a_1 > a_2) = p_o(in\\_domain|a_1, a_2) \\times p_o(a_1 > a_2)$, where for outlet o, $p_o(in\\_domain|a_1, a_2) = 1$ if $a_1$ belongs to o while $a_2$ does not. However, our results in"}, {"title": "7 Related Work", "content": "Understanding how information is prioritized and presented in news media has long been a subject of scholarly interest. This section situates our work within the broader literature on information prioritization, visual cues in editorial decision-making, layout parsing, modeling editorial judgments, and data-driven studies of news content and bias.\nInformation Prioritization and Newsworthiness The concept of newsworthiness is central to journalism studies and media sociology. Classic works by Galtung and Ruge (1965) introduced a set of news values that determine the selection and presentation of news stories. These news values have been revisited and updated by scholars such as Harcup and O'Neill (2001) and Harcup and O'Neill (2017), who identified factors like relevance, timeliness, and unexpectedness as key determinants of newsworthiness. Prior research has explored the cognitive and organizational processes behind news selection. Shoemaker (1991) examined the gatekeeping role of editors and journalists in filtering news content. Herman and Chomsky (2021) discussed how media organizations' structures influence news production and prioritization.\nOur work contributes to this literature by providing a computational approach to inferring newsworthiness judgments from homepage layouts, offering a large-scale empirical perspective on editorial prioritization decisions.\nVisual Cues and Editorial Decision-Making Visual presentation plays a crucial role in shaping readers' perceptions of news importance. Studies have shown that elements such as headline size, article position, and the use of images significantly affect reader attention and recall (Brooks and Pinson, 2022; Nass and Mason, 1990).\nEye-tracking research has provided insights into how users interact with news websites. Nielsen"}, {"title": "8 Conclusion", "content": "In this work, we introduced NewsHomepages, a comprehensive dataset capturing editorial prioritization decisions through the lens of homepage layouts across thousands of news organizations. By applying weakly-supervised models, we demonstrated the ability to robustly infer editorial judgments through pairwise comparisons of article positioning."}, {"title": "9 Contributions", "content": "Co-first author contributions: Ben Welsh founded the News-Homepages Internet Archive Project, built and maintained the pipelines to perform data collection over a period of 8 years. Alexander Spangher was the primary writer, he was the advisor for Arda and Michael, and he performed the analysis in Section 5. Arda Kaz did the modeling in Section 4. Secondary author contributions: Michael Vu did most of Section 3. Naitian performed initial exploration in Section 5 and editing."}, {"title": "10 Limitations", "content": "This work, while advancing the study of editorial prioritization on homepages, comes with several limitations. First, the dataset, although large and diverse, predominantly focuses on English-language news outlets from the U.S., which may limit the generalizability of our models to international or non-English outlets. Despite the inclusion of some non-U.S. and non-English homepages, the models have not been explicitly evaluated on a broader range of languages or cultural contexts. This focus may overlook regional editorial conventions and biases that differ significantly from the U.S. context.\nAnother limitation is that the study focuses primarily on visual cues of newsworthiness, such as size, position, and graphical elements. While these cues are significant, they are not the only factors that influence editorial decisions. The models do not account for less visible but equally critical considerations, such as journalistic ethics, editorial mandates, or audience engagement metrics, which may influence homepage layouts but remain unquantified in this dataset.\nAdditionally, the weakly-supervised learning methods employed for layout parsing may struggle with more complex or irregular homepage designs. As described, the model had trouble generalizing to homepages with obscure HTML structures, leading to imperfect bounding box detections in some cases. This could result in misinterpretations of editorial significance, especially for websites with non-traditional or highly dynamic layouts.\nLastly, the results may have some bias due to the reliance on pairwise article comparisons. This method, while efficient, reduces the complexity of editorial decision-making into binary relationships, potentially overlooking more nuanced or multifaceted prioritization strategies that editors use in practice."}, {"title": "10.1 Computational Budget", "content": "The computational resources required for this project were substantial. The model training involved 4\u00d7A40 GPUs for initial phases and 16\u00d7A100 GPUs for more extensive model fine-tuning and deployment. Training the custom Detectron2 model alone took 24 hours, with additional time required for fine-tuning and model testing across multiple outlets. While the experiments could be completed within this budget, more extensive experiments across all 3,000 outlets would require significantly more resources, especially when scaling to different languages and regional variations."}, {"title": "10.2 Use of Annotators", "content": "The dataset used in this work was primarily compiled automatically through web scraping, with minimal manual annotation. However, annotators were employed to manually label URLs to distinguish between news articles and non-news articles during the preprocessing phase. A set of 2,000 URLs was manually labeled, contributing to a more refined and accurate dataset. The authors performed this task themselves. However, beyond this, no human annotators were used to manually assess newsworthiness, as the models relied on position and size as proxies for editorial decisions."}]}