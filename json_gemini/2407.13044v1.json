{"title": "DROPKAN: REGULARIZING KANS BY MASKING POST-ACTIVATIONS", "authors": ["Mohammed Ghaith Altarabichi"], "abstract": "We propose DropKAN (Drop Kolmogorov-Arnold Networks) a regularization method that prevents co-adaptation of activation function weights in Kolmogorov-Arnold Networks (KANs). DropKAN operates by randomly masking some of the post-activations within the KANs computation graph, while scaling-up the retained post-activations. We show that this simple procedure that require minimal coding effort has a regularizing effect and consistently lead to better generalization of KANs.\nWe analyze the adaptation of the standard Dropout with KANs and demonstrate that Dropout applied to KANs' neurons can lead to unpredictable performance in the feedforward pass. We carry an empirical study with real world Machine Learning datasets to validate our findings. Our results suggest that DropKAN is consistently a better alternative to Dropout, and improves the generalization performance of KANs.", "sections": [{"title": "1 Introduction", "content": "Kolmogorov-Arnold Networks (KANs) [1] are recently proposed as an alternative to Multi-Layer Perceptrons (MLPs). The computation graph of Kolmogorov-Arnold Networks (KANs) is different form the standrad Multi-Layer Perceptrons (MLPs) in two fundamental ways: 1) On edges: KANs use trainable activation functions, unlike MLPs that rely on linear weights. 2) On neurons (\"nodes\"): KANs sum the incoming signals, different to MLPs that apply non-linear activation functions e.g., ReLU. This change in the computation graph indicates that many of the techniques used in MLPs might not be directly transferable to KANs, or at least may not necessarily give the same desired effect. In this work we explore whether KANs could benefit from using Dropout [2] for regularization, and propose DropKAN based on our analysis as a method to regularize KANs by randomly masking post-activations. DropKAN is efficient at regularizing KANs and is easy to incorporate within any implementation of KANs using a single line of code.\nOur contributions in this paper can be categorized in two folds. First, we analyze the behavior of Dropout applied to KANs and show that it behaves differently from how it was originally designed to operate with MLPs. Our second and main contribution is proposing DropKAN, an alternative to Dropout applied to KANs, as we show that DropKAN consistently outperforms Dropout with KANs using a number of real world datasets."}, {"title": "2 Motivation", "content": "We start by formalizing the definition of a KAN layer and the adaptation of Dropout to KANs. We denote the input dimension of the lth layer of a KAN model consisting of L layers by $n_l$. The activation functions connecting the layer l to the following layer l + 1 can be expressed as a 1D matrix of functions:\n$\\Phi_l = \\{\\phi_{l,j,i}\\}, l = 1, 2, . . ., L, i = 1, 2, . . ., n_l, j = 1, 2, . . ., n_{l+1}$\nwhere $\\phi_{l,j,i}$ is an arbitrary function represented as a spline with trainable parameters. We define $x_{l,i}$ as the pre-activation (input) of the function $\\phi_{l,j,i}$; the post-activation of $\\phi_{l,j,i}$ is denoted by $\\widehat{x}_{l,j,i} = \\phi_{l,j,i}(x_{l,i})$. The neurons in the KAN layer performs a sum of all incoming post-activations to output $x_{l,j,i}$.\n$x_{l+1,j} = \\sum_{i=1}^{N_{in}} \\widehat{x}_{l,j,i} = \\sum_{i=1}^{N_{in}} \\phi_{l,j,i} (x_{l,i}), j = 1, 2, ..., N_{l+1}$\nBy adding a Dropout layer between the KAN layers l and l + 1 as in Figure 1, a binary dropout mask $m_j$ is applied to the l layer outputs' $x_{l+1,j}$ in training time. The mask is usually sampled independently from a Bernoulli distribution with a probability p of being 1 (indicating that the neuron j is retained) and 1 \u2013 p of being 0 (indicating that the neuron j is dropped). The output of the KAN layer with the inclusion of Dropout becomes:\n$\\widehat{x}_{l+1,j} = \\frac{m_j x_{l+1,j}}{1-p}, j = 1, 2, ..., n_{l+1}$\nThe output of the Dropout layer in Equation 3 is usually scaled-up by a factor of $\\frac{1}{1-p}$, we denote this factor by s. This procedure is done to compensate the effect of dropping out some nodes, with the rational of ensuring that a similar level of signal will continue to propagate through the network with the presence of Dropout in training time."}, {"title": "2.1 Why is Dropout problematic with KANS?", "content": "The key motivation of Dropout is to prevent co-adaptations [3] of weights by sampling a thinned network through dropping out nodes. If a node in a MLP is dropped by a Dropout layer then all the weights connected to it will take no part in the feedforward and backward passes during the training step. The weights of the dropped neuron in a MLP have no impact on feedforward because they are multiplied by an input of zero from the dropped node, while on the backward pass they have no influence on the loss calculation, and will consequently get a gradient of zero in that training step.\nHowever, applying Dropout to the outputs of a KAN layer is not enough to exclude the masked nodes from actively participating in the feedforward and backward passes. The zero outputs from the masked nodes will be used as inputs to the activation functions $\\Phi_{l+1}$ in the following layer l + 1, given that for an arbitrary activation function $\\phi'$ from $\\Phi_{l+1}$, the output of $\\phi'(x = 0)$ is not necessarily zero, the nodes will still propagate the corresponding value of $\\phi'(x = 0)$ into the network during feedforward. Consequently, the weights of activation function (the spline coefficients) $\\phi'$ in the layer 1 + 1 following the dropout will also be updated in the backward pass.\nAnother key issue with applying Dropout to KANs is the compensation of dropping out some nodes by scaling-up. As we have observed in Equation 3, the outputs of the kept nodes are scaled by a factor of s. This procedure is ineffective with KANs as the arbitrary activation function from $\\Phi_{l+1}$ are not necessarily homogeneous function of degree 1, $\\phi'(sx) \\neq s\\phi'(x)$. The behavior of $\\phi'(sx_{l+1,j})$ is unpredictable at training time, and it is not trivial to identify s that could lead to the proper scaling-up effect, as this procedure must be done for each layer using Dropout in the KANs."}, {"title": "3 Methods", "content": "We design DropKAN to address the previous issues of Dropout by applying a binary dropout mask $M_l$ to the post- activations $\\widehat{x}_{l,j,i}$ as observed in Figure 1 in training time. This is different to Dropout where the mask is applied to the output of the neurons $x_{l+1,j}$. Similar to Dropout, we sample the mask independently from a Bernoulli distribution with a probability p of being 1 (indicating that the post-activation is retained) and 1 p of being 0 (indicating that the post-activation is dropped). The output of the KAN layer when DropKAN is applied becomes:\n$x_{l+1,j} = \\frac{1}{1-p} \\sum_{i=1}^{N_{in}} M_i \\widehat{x}_{l,j,i} = \\frac{1}{1-p} \\sum_{i=1}^{N_{in}} M_i \\phi_{l,j,i} (X_i), j = 1, 2, ..., N_{out}$\nIn DropKAN the output is scaled-up by a factor of $\\frac{1}{1-p}$, to compensate for the dropped post-activations. In test time DropKAN behaves exactly as a regular KAN layer as in Equation 2, effectively serving as an identity function.\nApplying the mask on the post-activations in DropKAN combined with scaling-up ensures that the expected values of the sums in KAN nodes during training are approximately the same with and without DropKAN:\n$E[\\frac{1}{1-p} \\sum_{i=1}^{N_{in}} M_i \\widehat{x}_{j,i}] = E[\\sum_{i=1}^{N_{in}} \\widehat{x}_{j,i}]$\nUnlike with Dropout, DropKAN doesn't mask the nodes after the sum, instead some of the post-activations are masked while the kept ones are scaled-up, which ensure the expected value of the summation taking place in the node with the presence of DropKAN is about the same without DropKAN as observed in Equation 5."}, {"title": "4 Results and Discussion", "content": "This section describes the experimental design we have used to evaluate DropKAN, along with the results obtained in our experiments. The first experiment aimed to evaluate the expected value of a KAN function using DropKAN layers in training mode, and compare to a network with Dropout enabled between the KAN layers. In the second experiment, we compare the performance of a network using DropKAN layers against a regular KANs and KAN regularized with Dropout using a number of classification problems."}, {"title": "4.1 Experimental Setup", "content": "Our experiments involve 10 popular [4, 5] data sets from the UCI Machine Learning Database Repository*. We have included datasets of varying sizes from different domains. Table 1 provides a summary of the number of instances, features and classes of all data sets used in our experiments.\nEach data set is divided into training (60%), validation (20%), and testing (20%) splits. A unified approach of prepossessing is adopted for all data sets, including categorical features encoding, imputation of missing values, and shuffling of instances. Accuracy of the model is the metric we used for the evaluation in all experiments. All reported accuracies are the ones realized on the testing set.\nAs for the KANs hyperparameters we have adopted the default hyperparameters' values, beside setting the grid to 3, and turning off the trainable parameters of the base activation function. The KAN architecture is unified across all experiments for all datasets [nin, 10, 1], where nin is the number of input features in the dataset, unless explicitly"}, {"title": "4.2 Experiment I - Forward Pass Evaluation", "content": "In this experiment we evaluate the impact of using DropKAN and Dropout on the forward pass of backpropagation for a KAN network. We train the KAN network [6, 2, 2, 1] five times for a total of 100 steps. We carry five forward passes using the validation split on every 10 steps using the five settings of: No-Drop, Dropout w/ scale, Dropout w/o scale, DropKAN w/ scale, and DropKAN w/o scale. We fix the rate of drop to 0.5 for all drop modes on each layer. We average the value at the output neuron (23,1) to estimate the expected value of the output using the validation split. The plots in Figure 2 show the results of this experiment, and while it is clear that the propagated signals for both Dropout w/o scale and DropKAN w/o scale become weaker due to the process of zeroing-out, the scaling is allowing DropKAN w/ scale to pretty accurately approximate the strength of the signal comparing to the No-Drop mode consistent with Equation 5. On the other hand, the procedure of scaling-up the input for Dropout is consistently off, and in this case is causing to over-scale the signal. As we have explained earlier, scaling up the input of the non-linear activation function ' would cause an unpredictable behaviour at training time since $\\phi'(sx) \\neq s\\phi'(x)$."}, {"title": "4.3 Experiment II - Classification Problems", "content": "In this experiment, we compare KANs equipped with DropKAN layers against KANs with standard KAN layers and KANs regularized using Dropout. The goal of the experiment is to validate the regularizing effect of DropKAN and to compare it against using standard Dropout with KANs.\nWe use a random search to optimize the rates of drop for the DropKAN and Dropout settings, we ran the search for 50 evaluation per setting, and choose the setting with the highest accuracy performance on the validation split. For evaluation we train every setting five times and report the average of the runs.\nThe results in Table 2 suggest that regularization methods improved the standard KAN performance (No-Drop) in 8 scenarios apart from two datasets semeion, and car. DropKAN w/ scale is clearly the most successful setting recording the highest test accuracy in six of the ten datasets, while the standard Dropout w/ scale was only the best choice in one dataset.\nThe results suggest that scaling is showing a positive effect with DopKAN as the scaled settings performed better than w/o scaling in seven scenarios, consistent with the results of the previous experiment and Equation 5. However, scaling is not as successful in the case of Dropout showing better performance comparing to no scaling in only 4 scenarios."}, {"title": "5 DropKAN Implementation", "content": "DropKAN can not be implemented as a standalone layer, but instead it must be applied to the post-activations within the KAN layer. Luckily, the implementation is straightforward, and could be realized with a single line of code, allowing DropKAN to be easily incorporated into any custom KAN implementation. We demonstrate here the line of code to be added assuming torch library is used for the KAN layer implementation:\npostspline = postspline * torch.empty(postspline.shape, device=postspline.device).bernoulli_(1 - rate)/ (1 - rate)\nWhere postspline is the post-activations (spline) variable, and the variable rate is the dropping probability of DropKAN.\nIt would be interesting to implement and test DropKAN for the newly-emergeing KAN-based architectures such as graph [6, 7, 8], convolutional [9], and transformer [10]. For instance in FourierKAN-GCF (Graph Collaborative Filtering) [11], regularization strategies like node and message dropout were tested, we believe DropKAN could be directly extended to such architectures."}, {"title": "6 Related Work", "content": "Expanding upon the Dropout technique, various methodologies [12, 13, 14] have been proposed to refine the training regularization of Deep Neural Networks (DNNs) for supervised learning. The fundamental idea is to introduce noise into intermediate layers during training. Recent advancements have focused on enhancing regularization in Convolutional Neural Networks (CNNs) through the introduction of structured noise. For instance, the SpatialDropout method [15] selectively removes entire channels from activation maps, the DropPath scheme [16] opts to discard entire layers, and the DropBlock algorithm [14] zeros out multiple contiguous regions within activation maps.\nTechniques inspired by Dropout are not limited to ones during the feedforward stage of backpropagation, as in Gradient Dropout proposed in the context of meta-learning by [17], and later generalized by [18] to the supervised training setting, [18] showed that masking the gradient during the backward pass prevents the network from memorizing the data and learning overly simple patterns early in the training, similar ideas could potentially be extended to KANS ."}, {"title": "7 Conclusions", "content": "We present DropKAN an effective method to regularize KANs leading to reliable generalization improvement over the standard KANs, and KANs regularized with Dropout. Our analysis of adopting Dropout to KANs indicates that using it between KAN layers is problematic due to the speciality of the computation graph of KANs. We design DropKAN to mask the post-activations instead of masking the neurons/nodes. We show that a KAN network constructed using DropKAN layers consistently outperforms a KAN of the same architecture using standard KAN layers, even when the later is regularized with Dropout between its layers."}]}