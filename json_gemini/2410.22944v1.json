{"title": "FOCUS ON THIS, NOT THAT! STEERING LLMS WITH\nADAPTIVE FEATURE SPECIFICATION", "authors": ["Tom A. Lamb", "Adam Davies", "Alasdair Paren", "Philip H.S. Torr", "Francesco Pinto"], "abstract": "Despite the success of Instruction Tuning (IT) in training large language models\n(LLMs) to perform arbitrary user-specified tasks, these models often still leverage\nspurious or biased features learned from their training data, leading to undesired\nbehaviours when deploying them in new contexts. In this work, we introduce\nFocus Instruction Tuning (FIT), which trains LLMs to condition their responses\nby \"focusing on\u201d specific features whilst ignoring others, leading to different be-\nhaviours based on what features are specified. Across several experimental set-\ntings, we show that focus-tuned models can be adaptively steered by focusing on\ndifferent features at inference-time: for instance, robustness can be improved by\nfocusing on task-causal features and ignoring spurious features, and social bias\ncan be mitigated by ignoring demographic categories. Furthermore, FIT can steer\nbehaviour in new contexts, generalising under distribution shift and to new unseen\nfeatures at inference time, and thereby facilitating more robust, fair, and control-\nlable LLM applications in real-world environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Instruction Tuning (IT) (Zhang et al., 2023), a specialised form of supervised fine-tuning (SFT),\nhas become an essential step in the process of developing effective instruction-following large lan-\nguage models (LLMs) (Ouyang et al., 2022; Touvron et al., 2023; Chen et al., 2024). While ex-\ntensive pre-training to perform next token prediction allows LLMs to extract common patterns and\nknowledge from large text corpora, IT fine-tunes these models on input-output pairs complemented\nby natural-language task instructions, teaching them to perform open-ended language-based tasks\ngiven instructions (Huang et al., 2023). However, despite the improvements observed in zero-shot\ngeneralisation from IT, recent studies suggest that some of these gains may be superficial, stemming\nfrom the models' ability to learn task template formats or spurious input/output correlations rather\nthan a more generalisable instruction-following capability (Kung & Peng, 2023; Ghosh et al., 2024).\nAs a result, LLMs may fail to generalise to new contexts where the same templates or spurious cor-\nrelations are not present (Kung & Peng, 2023).\nTo address these limitations, we propose Focus Instruction Tuning (FIT), an extension of traditional\nIT where LLMs are also fine-tuned with respect to an instruction regarding which features to \"focus\non\" or \"not focus on\". Training LLMs to condition responses on the provided focus specification,\nwhere responses to the same input differ based on the specified features, allows end users to dy-\nnamically steer model behaviour by indicating which features should play a role or be ignored in\nperforming a task. For instance, in Figure 1, we show how FIT can be used to steer a model to ig-\nnore gender stereotypes and focus instead on task-relevant information in order to correctly solve a\nquestion-answering task. In our experiments, we demonstrate that FIT is effective at steering models\nto ignore known spurious features while focusing on causal (task-relevant) features (see Section 4)\nacross a variety of features and tasks such as sentiment analysis and natural language inference, and\ncan be used to mitigate social bias by ignoring demographic features in question-answering. We find\nthat FIT is also robust to distribution shifts over feature values, and can generalise to new, held-out\nfeatures not encountered during training. In sum, our primary contributions are as follows:"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1 SPURIOUS FEATURE LEARNING", "content": "Deep neural networks, such as foundation models like LLMs, are susceptible to relying on spurious\nfeatures present in the training dataset \u2013 i.e., input features that are correlated with outputs in the\ntraining distribution, but are not correlated in all test distributions (Ye et al., 2024). Relying on spu-\nrious features leads models to fail to generalise under distribution shifts where such correlations may\nno longer hold Wang et al. (2023a). Spurious features have been extensively studied in computer\nvision, encompassing features such as background colour (Xiao et al., 2021; Venkataramani et al.,\n2024; Arjovsky et al., 2019) or texture (Geirhos et al., 2018; Baker et al., 2018), and are also preva-\nlent in many widely used NLP benchmarks (Sun et al., 2024b; Borkan et al., 2019). For instance, the\ntoken SPIELBERG is spuriously correlated with positive sentiment in datasets like SST-2 (Socher\net al., 2013b), meaning that models trained on SST-2 may learn to predict sentiment by leveraging\nthese spurious features instead of more general sentiment features (Wang & Culotta, 2020). This\nreliance on non-causal features undermines the robustness of models in generalising to distribution\nshift.\nA variety of approaches have been explored to detect and mitigate the effects of spurious feature\nlearning, particularly under distribution shifts. Traditional approaches include prompt engineering\n(Sun et al., 2024b), regularisation techniques (Arjovsky et al., 2019; Chew et al., 2024), and di-\nrectly incorporating causal inference strategies (Wang & Culotta, 2020; 2021; Udomcharoenchaikit\net al., 2022). Substantial work in mechanistic interpretability has also aimed to discover models'\nrepresentation and use of task-causal or spurious features: for instance, causal probing (which trains\nprobing classifiers to recognise and modify supervised feature representations encoded by founda-\ntion models; see Belinkov, 2022; Canby et al., 2024; Davies & Khakzar, 2024) has been used to\nstudy how models leverage causal versus spurious features features in the context of a given task"}, {"title": "2.2 CONTROLLING LLMS", "content": "Instruction Tuning. Due to the next-word prediction training objective, large language models\n(LLMs) often struggle by default to generate outputs that align with human instructions in down-\nstream applications (Huang et al., 2023). Instruction-tuning (IT) mitigates this issue by fine-tuning\npre-trained LLMs on datasets composed of instruction-response pairs (Zhang et al., 2023), aiming to\nalign the responses of the fine-tuned model more closely with the distributions preferred by humans\n(Ouyang et al., 2022). There are several popular approaches for collecting IT training data, such as\nusing human-annotated data (Dolly, 2023), extracting datasets from existing collections (Longpre\net al., 2023; Mishra et al., 2022), or gathering data from internet sources (Zhou et al., 2024). IT\ndatasets can also be synthesised with LLMs, either by bootstraping them from the same model that\nwill be instruction-tuned on them (Wang et al., 2023c; Chen et al., 2024), or by distilling from a\nlarger or more powerful model to instruction-tune smaller models (Taori et al., 2023; Mitra et al.,\n2023; Xu et al., 2023).\nDespite the success of IT in zero-shot generalisation, Gudibande et al. (2023) find that improve-\nments on many downstream benchmark tasks may be largely due to coverage of task data within\nIT training datasets; and bootstrapping IT methods (which, in principle, might not be subject to this\nissue provided they synthesise novel IT task instances) require a robust and effective LLM for fine-\ntuning to avoid degenerate training cycles (Zhang et al., 2023). Furthermore, Kung & Peng (2023)\nshow that some of the downstream performance gains from IT can be attributed to models' ability\nto learn surface-level patterns, such as the required answer format, rather than acquiring more gen-\neralisable instruction-following skills. These limitations underscore the need for advancements in\nsupervised fine-tuning (SFT) methods beyond IT to facilitate more predictable and reliable control\nof downstream model behaviours.\nRefocusing LLMs. Refocusing LLMs. Several methods have been proposed to better control\ninstruction-tuned models both during and after training. Llama-Guard (Inan et al., 2023) fine-tunes\nLLMs to detect predefined risk features in inputs and outputs based on a user-specified taxonomy,\nsuch as identifying sexual content in inappropriate contexts. JsonTuning (Gao et al., 2023) enhances\ntraditional instruction tuning by enforcing structured input and output formats in JSON, clarifying\ntask requirements and reducing sensitivity to paraphrasing (Sun et al., 2024a). In contrast, Focus In-\nstruction Tuning (FIT), as introduced in this work, provides a more flexible and powerful approach.\nWhile Llama-Guard operates only post-training and is limited to the safety domain, FIT enables\nfine-grained control both during and after training, conditioning models on a broader range of fea-\ntures across arbitrary domains via natural-language specifications. Moreover, unlike JsonTuning,\nwhich is restricted to enforcing output structure, FIT allows users to specify input features, enabling\nthe model to ignore spurious correlations or highlight task-relevant attributes."}, {"title": "3 METHODOLOGY", "content": "Preliminaries. We consider a pre-trained, decoder-only large language model (LLM) p\u03b8 that models\nthe probability distribution over its vocabulary V autoregressively. For an input sequence x =\n[x1,...,xL] \u2208 VL, the joint probability of x is given by $p_{\\theta}(x) = \\prod_{l=1}^{L} p_{\\theta}(x_i|x_{<i})$, with $p_{\\theta}(x_1|\\emptyset) =$\np\u03b8 (x1). In traditional supervised learning, for a sample (x, y) ~ D, the conditional likelihood of the\noutput y given input x is p\u03b8(y|x) = \u03a0Li=1 p\u03b8(yi|x, y<i), with p\u03b8(y1|0) = p\u03b8(Y1); and in supervised\nfine-tuning (SFT) of LLMs, this manifests as minimising the negative log-likelihood (NLL) of y\ngiven x."}, {"title": "Focus Instruction Tuning (FIT).", "content": "We introduce Focus Instruction Tuning (FIT), a specialised form\nof instruction tuning that trains LLMs to adjust their responses based on user-specified features\nprovided in natural language.\nLet F denote the set of possible features (e.g., specific keywords, sentiment, verb tense, demographic\ninformation, etc.) that the model can be instructed to focus on or ignore when generating responses.\nWe consider a set of natural language instructions to focus or rule out specified features in F which\nwe term the focus instruction set Ifocus.\u00b9 Explicitly, we define Ifocus as\n$I_{focus}={\\emptyset, focus(F_i), ignore(F_i), focus(F_i) \\land ignore(F_j) | F_i, F_j \\in F}$    (1)\nwhere: \u00d8 denotes an empty focus instruction with no features to focus on or to ignore; focus (Fi) is an\ninstruction to focus on feature F\u2081; ignore (F\u2081) is an instruction to ignore feature F\u2081; and focus (Fi) \u2227\nignore (F) is an instruction to focus on feature F\u2081 whilst ignoring feature F\u2081. We include the default\nprompt in order to aid the model in learning the underlying task as well as the ability to refocus its\nattention on specified features during FIT.\nConsider a sample (x, y)  ~ Pdata (x, y) drawn from an underlying data distribution and\na focus instruction Ifocus drawn from a distribution Pfocus over the set of focus instruc-\ntions Ifocus. Then the likelihood of response y conditioned on input x, task instruc-\ntion I (as in standard IT), and focus-instruction Ifocus is modelled as pe(y|I, Ifocus, x)."}, {"title": "FIT Training.", "content": "Consider a clas-\nsification task\u00b2 with finite label\nspace y, where a single causal\nfeature C\u2208 F is fully predic-\ntive of label y \u2208 Y given input\nx at both training time and un-\nder distribution shift (Koh et al.,\n2021). We also consider spu-\nrious features S \u2208 S \u2286 F\nfrom a subset of spurious fea-\ntures S, where feature values\u00b3\ns\u2208 Image(S) for some spuri-\nous feature S\u2208 S correlate with\na label ys \u2208 V, where this corre-\nlation may change under distri-\nbution shift (Ming et al., 2022).\nFinally, we define F as the set of\nfeatures that may be included in focus instructions during training, consisting of the causal feature\nand the set of spurious features F = {C} US.\nFor a sample (x,y) ~ Pdata(x, y), we specify the focus label Yfocus = Yfocus(x, y, Ifocus) \u2208 Y that\ndepends on the ground truth label y and focus instruction Ifocus \u2208 Ifocus. Intuitively, we define focus\nlabel yfocus as Yfocus = y when either no focus features are specified (i.e., using the empty focus\ninstruction), when the focus is on the underlying causal feature C, or when ignoring a spurious\nfeature S; but when either the focus is on a spurious feature or the causal feature is ignored, Yfocus\nis defined as the label spuriously correlated with a particular value of the spurious feature present\nin input x. This changing target yfocus trains the model to learn to adjust its responses based on"}, {"title": "Evaluating FIT under spurious correlations.", "content": "After introducing FIT above, we now turn to settings\nwhere we can empirically train and evaluate it. A key aspect of our evaluation is the use of known\nspurious correlations, which simulate real-world scenarios where models can be misled by features\nthat are spuriously predictive of the output label. By adjusting the co-occurrence rate between\nspurious features and their associated labels, we can test FIT's ability to dynamically steer a model's\nresponses depending on the features on which it is focusing or ignoring.\nWe define the co-ocurrence rate, or predictivity (Hermann et al., 2024), between spurious feature\nvalues and the label with which they are spuriously correlated by Pspurious. Specifically:\nDefinition 1. (Defining pspurious). Let S \u2208 S \u2286 F denote a spurious feature. Suppose that a value\nof S, say s, is spuriously correlated with label ys. Then we define pspurious(s) as\n$P_{spurious}(s) = P(Y = y_s|X, S = s, s \\in X)$   (4)\nfor some dataset sample (x, y) \u2208 D, where S \u2208 X denotes the presence of feature S in example X.\nBy varying Pspurious(s), we can control the predictivity of spurious features and observe the model's\nbehaviour when focusing on or ignoring these features as well as causal features.\nGiven a task with N classes, we require pspurious = 1/N within the training set, ensuring that the\nunderlying label distribution, p(y|I, Ifocus, x), is of maximum entropy when focusing on spurious\nfeatures. This allows the model to better distinguish between causal and spurious features, as effec-\ntively minimising Equation (3) would require the model to make predictions without relying on the\nunderlying causal feature when its attention is specified to focus on on spurious features. A more\ndetailed exploration of this setting of pspurious during training can be found in Appendix A.1.\nNext, we evaluate FIT across several test sets that capture different conditions of spurious correla-\ntions and distribution shifts:\n\u2022 Diid: Held-out test samples with the same pspurious as in the training set.\n\u2022 Dhigh: Test samples with a higher pspurious than in the training set.\n\u2022 Dlow: Test samples with a lower pspurious than in the training set.\n\u2022 Dflipped: Test samples where spurious feature values are flipped to co-occur with different\nlabels than in the training set, with the same high pspurious as in Dhigh.\nWe further evaluate FIT under distribution shifts, where the specific values taken by spurious features\ndo not overlap between the training and test sets, by introducing one additional test set:\n\u2022 Dshift (Pspurious): Test datasets where the spurious feature values are distinct from those\nwithin the training set.\nWe evaluate over these datasets specifically on our SMNLI datset (c.f. Section 4.2)."}, {"title": "4 EXPERIMENTS", "content": "In this section we empirically validate the effectiveness of FIT across a range of popular LLMs\nof varying sizes and on different NLP datasets, including classification and multi-choice question-\nanswering tasks.\nBefore reporting the main results, we introduce the evaluation metric (focus accuracy) that we re-\nport, baselines, models, and training settings used throughout the experiments. In Section 4.1, we\nfirst verify that FIT performs well on the simpler SS dataset, a synthetic sentiment analysis dataset\nderived from SST-5 (Socher et al., 2013b). We then demonstrate in Section 4.2 that FIT generalises\nto more complex features and handles distribution shifts on the SMNLI dataset, a sub-sampled ver-\nsion of the MNLI dataset (Williams et al., 2018). Finally, in Section 4.3, we show that FIT has\npractical, real-world impact by effectively mitigating bias in the BBQ dataset (Parrish et al., 2022),\nwhere we further illustrate FIT's ability to generalise to new features seen for the first time when\nperforming inference.\nMetrics. We define the focus accuracy for a focus instruction Ifocus \u2208 Ifocus as the proportion of\nsamples where the model's prediction aligns with the focus label, yfocus, as specified in Equation (2).\nSpecifically, for each sample (x, y) \u2208 D, the model produces a prediction \u0177 ~ \u0440\u03b8(y | I, Ifocus, X)\nbased on a fixed focus instruction Ifocus \u2208 Ifocus. The focus label, Yfocus = Yfocus (x, y, Ifocus), cor-\nresponds to the target output given the focus instruction for the input x with ground truth label y.\nFocus accuracy for focus instruction Ifocus, denoted Afocus (Ifocus), is computed as the fraction of\ncorrect predictions with respect to the focus label:\n$A_{focus}(I_{focus}) = \\frac{1}{|D|} \\sum_{(x,y) \\in D} \\mathbb{1}(\\hat{y} = y_{focus}),$ (5)\nwhere 1(y = Yfocus) is the indicator function that equals 1 if the model's prediction \u0177 matches the\nfocus label yfocus, and 0 otherwise.\nWe report focus accuracy for each model on all dataset splits, using the prompt types and focus\ninstructions detailed in Appendix A.3. Generations are evaluated through simple pattern matching\ndue to the use of constrained beam decoding. Further details are provided in Appendix A.2.\nModels and training settings.\nWe evaluate FIT using three popular LLMs that\nspan a range of model sizes: Llama-3.1-8B-Instruct (Dubey et al., 2024),\nMistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Vicuna-13B-v1.5 (Chiang et al.,\n2023). The models are fine-tuned using parameter-efficient SFT with LoRA (Hu et al., 2021), lever-\naging Hugging Face's SFTTrainer (Wolf et al., 2020) with default hyperparameters. Early stop-\nping is applied based on validation loss, as defined in Equation (3). For generation, we use con-\nstrained beam decoding (Anderson et al., 2017) and use fully verbalised (natural language) labels\nduring both training and testing, except for the multi-choice BBQ dataset. For further training de-\ntails, refer to Appendix A.1.\nBaselines. We consider two baselines to compare against FIT: a few-shot baseline (Manikandan\net al., 2023) and a SFT baseline. The SFT baseline follows the same setup as the FIT method, but\nwithout the focus instructions during training. This ensures a fair comparison between FIT and the\nbaseline, as both methods are trained on the same examples and labels (i.e., focus labels Yfocus), with\nthe only difference being the inclusion of focus instructions in FIT. This setup allows us to isolate\nand evaluate the specific impact of incorporating focus instructions in FIT. The few-shot baseline\ninvolves using 5 in-context examples uniformly sampled at random from the training set for each\ntest example, where we use the same focus instruction for each in-context sample as for the test\nsample. Further details of baselines can be found in Appendix A.4."}, {"title": "4.1 VALIDATION OF FIT ON THE SS DATASET", "content": "Spurious Sentiment dataset (SS). We first evaluate FIT on a synthetic binary sentiment analysis\ndataset. Starting with SST-5 (Socher et al., 2013a), a 5-class sentiment analysis dataset, we use\nLlama-3.1-70B-Instruct (Dubey et al., 2024) to inject the spurious keywords Pineapple"}, {"title": "4.2 FIT PERFORMS WELL WITH MORE COMPLEX FEATURES ON THE SMNLI DATASET AND\nGENERALISES UNDER DISTRIBUTION SHIFT", "content": "Spurious MNLI dataset (SMNLI). Next, we evaluate our method on a more complex dataset\nwith subtler features. Specifically, we construct an NLI dataset by sub-sampling from MNLI\n(Williams et al., 2018), where we induce a spurious correlation between text genres and labels by\nsub-sampling accordingly. We refer to this dataset as SMNLI, where the feature set is defined as\nF = {NLI relationship, genre}. The co-occurrence rate of genres and their spuriously associated\nlabel is governed by pspurious, which varies across the test sets discussed in Section 3. We ensure that"}, {"title": "4.3 FIT STEERS BEHAVIOUR IN THE PRESENCE OF SOCIAL BIAS DATA AND GENERALISES TO\nUNSEEN FEATURES", "content": "Bias Benchmark for QA (BBQ) dataset. Finally, we experiment with BBQ Parrish et al. (2022), a\nwidely-utilised multiple-choice question-answering benchmark annotated with nine forms of so-\ncial bias that are relevant to any given answer, such as stereotypes that would imply a given\nanswer to an otherwise ambiguous question (see Figure 1). The feature set contains F =\n{question context, gender identity, race/ethnicity, ..., disability status}, which contains one causal\nfeature (question context) and 9 bias features. Of the n = 9 bias features, we focus-tune mod-\nels with respect to 6, and test on these 6 features plus the remaining 3 bias features in order to\ntest how well FIT generalises to features that are not seen during focus tuning. Here, we consider\nthe spurious features to be the presence of a particular social group (e.g., men or women) in the\nquestion context, and spurious answers to be those that would be indicated by relying on social\nstereotypes rather than the specific question context (e.g., see Figure 1). The stereotyped response\nused to determine spurious answers for these bias features are provided as part of the BBQ dataset.\nResults. Figure 6 shows the focus accuracy results of the three models on the BBQ dataset, visualis-\ning performance on features seen during training and unseen, held-out features. The models demon-\nstrate high and comparable focus accuracy across both seen and unseen bias features, indicating that\nFIT generalises well to unseen features, including nuanced reasoning about group stereotypes. This\nhighlights the usefulness of FIT in mitigating social biases in LLM responses. Specifically, FIT can"}, {"title": "5 ABLATION", "content": "Generalisation to different test-time prompt formats. As observed in the IT literature,\ninstruction-tuned models sometimes memorise instruction formats and struggle to follow para-\nphrased instructions at test time (Ghosh et al., 2024). In Appendix A.5 (Figure 7), we compare\nthe performance of models on the SMNLI dataset when using the same focus instructions at training\nand test time versus using paraphrased instructions at test time. We generate 10 different test-time\nfocus instructions of each instruction type defined in Equation (1) by paraphrasing the existing focus\ninstruction using ChatGPT (OpenAI, 2022). The results show minimal variation in focus accuracy\nacross different dataset splits and focus features, even when testing on paraphrased prompts, indi-\ncating that FIT indeed teaches models a general capacity to focus on or ignore features regardless of\nthe specific way that focus instructions are phrased."}, {"title": "6 CONCLUSIONS", "content": "In this work, we introduce Focus Instruction Tuning (FIT), a method designed to steer the behaviour\nof LLMs by focusing on or ignoring specific features when formulating responses. Across a range\nof tasks and settings, we demonstrate that FIT can be used to steer LLM behaviours at inference\ntime, even in the context of distribution shifts over feature values or when generalizing to unseen\nfeatures at inference time. Additionally, we show that our method can mitigate biases by identifying\nand factoring out known stereotypes that might otherwise influence responses. Thus, FIT represents\na step toward enabling more robust, fair, and controllable LLMs.\nWe recommend that future work explore the effectiveness of FIT across a broader variety of tasks,\nincluding open-ended, free-form natural language generation tasks such as summarization or trans-"}, {"title": "ETHICAL CONSIDERATIONS AND LIMITATIONS", "content": "The ability to steer model behaviours by dynamically focusing on or ignoring arbitrary features such\nas demographic categories, as enabled by FIT, carries the potential to reduce algorithmic discrimi-\nnation and mitigate related harms. Indeed, practitioners can measure the discrepancy in behaviour\nwhen a model focuses or ignores some features in order to identify and correct discriminatory be-\nhaviour. Furthermore, the ability to attribute model responses to specific input features (as enabled\nby, e.g., observing changes in model behaviour resulting from different focus specifications) is also\nan important goal in explainable AI, indicating a potential role for FIT in facilitating more direct and\nproductive interactions between humans and AI systems in collaborative decision processes, where\ninsights about the features that caused a model to make a particular prediction may be helpful for\nassessing whether or not this prediction is an ethical, legal, or responsible basis for a real-world\ndecision. Finally, the ability to focus on user-specified features carries implications for robustness,\nas focus-tuned models can be steered to focus on stable (causal) features whose relationships with\nlabels are expected to hold across multiple test domains while ignoring (spurious) features whose\nassociations with labels may not generalise beyond the training domain.\nOn the other hand, one possible negative impact of FIT is that it could potentially be used by bad\nactors to intentionally bias models to, e.g., make discriminatory predictions based on social bias\nfeatures. However, this opportunity would already be available by directly fine-tuning the model on\na biased dataset. Another possible concern raised by our findings reported in Appendix A.9 is that\nFIT may not be as effective when focusing on or ignoring features that heavily overlap with each\nother. While this may be consideration in certain narrow contexts, it does not detract from FIT's\ngeneral promise across arbitrary natural-language tasks (see Appendix A.9 for further discussion)."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 FT TRAINING AND OPTIMISATION SETTINGS", "content": "FT Optimisation. Algorithm 1 gives precise details on how we implement FIT in practice when per-\nforming SFT of a model on a given training set. In particular, it shows how we approach optimising\nthe FIT training objective given in Equation (3).\nFT training settings. We use the SFTTrainer class from HuggingFace (Wolf et al., 2020) and use\nall of the default training settings for performing SFT of LLMs. Furthermore, we define p(Ifocus)\nby placing a small probability (in our experiments, 0.05) on the empty focus instruction \u00d8. We then\nuniformly distribute the remaining probability mass over the non-empty focus instructions.\nWe implement early stopping on a held-out validation set based on the cross-entropy loss over focus\nlabels Yfocus corresponding to randomly sampled focus instructions - this matches the context in\nwhich the models will be evaluated. We obtain this set by splitting our training set in an 80/20% for\ntraining and validation. We use a patience of 4 validation evaluation steps, which occur after a fixed\nnumber of steps.\nWe use LORA (Hu et al., 2021) for parameter-efficient fine-tuning. We target the query and value\nprojection matrices within each LLM and use LoRA r = 16 and a = 32 across models.\nChoice of pspurious during training. Consider a classification problem with N classes so that |Y| =\n6. During FIT training we want the model to learn to change it's behaviour depending on which\nfeatures are specified to be focused on or ignored.\nTo achieve this, consider when the focus instruction requires that the model focus on a spurious\nfeature during training, which means that Ifocus \u2208 {focus(S), focus(S) ^ ignore(Fj) | F; \u2208 F \\\n{S}}, we choose pspurious = 1/N. The definition of Pspurious given in Equation (4) implies that in this\nsetting Pspurious = P(Y = ys|X, I, Ifocus, s \u2208 X) for a feature value s of spurious feature S in input\nX. Therefore, setting pspurious = 1/N for the training dataset induces a uniform distribution over the\nset the set of class labels conditioned on an input X and focus instruction Ifocus during training."}, {"title": "A.2 EVALUATION METRICS", "content": "Generation settings. We generate responses from our FT model using constrained beam-decoding\n(Anderson et al., 2017) with 8 beams. This ensures that the answer labels for each classification task\nthat we investigate appear in the model's output. We limit the maximum number of newly generated\ntokens to be 5 to stop any unnecessary text given after the model's initial classification prediction."}, {"title": "A.3 FIT FOCUS INSTRUCTIONS AND PROMPT TEMPLATES", "content": "Focus instructions. We consider the following focus instruction formats for the different focus\ninstructions introduced in Equation (1) which are used for FIT training and evaluation:"}, {"title": "A.6 SPURIOUS SENTIMENT (SS) DATASET", "content": "We take a pre-existing dataset, in this case SST-5 (Socher et al., 2013a), and modify it in order to\ninduce a known spurious feature and create a spurious binary sentiment analysis dataset.\nData-generating process (DGP). We frame our DGP using a graphical model to describe the syn-\nthetic dataset that we create. We follow a similar model to that described in (Arjovsky et al., 2019),\nspecifically the model used for generating their coloured MNIST dataset. We use the following\nvariables within our graphical model:\n\u2022 C - true underlying sentiment, the causal feature within this task, sampled from the original\ndataset."}, {"title": "A.7 SPURIOUS NLI DATASET (SMNLI)", "content": "We generate a tertiary NLI dataset, SMNLI, with a known spurious feature. We do this considering\nthe MNLI dataset Williams et al. (2018). This is a NLI dataset with three labels: entailment (0),\nneutral (1) and contradiction (2), where data is sampled from 5 underlying categories or genres"}, {"title": "A.8 SPURIOUS HANS DATASET (SHANS)", "content": "We generate a binary NLI dataset, SHANS, with a known spurious feature. We do this considering\nthe HANS dataset McCoy (2019). This is an NLI data set with two labels: entailment (0) and\ncontradiction (1). This is an adversarial dataset designed to assess different NLI models' reliance on\nspurious heuristics rather than on the underlying relationship between the premise and the hypothesis\nwhen making predictions. Specifically, the author's consider three major categories of heuristics:\nlexical overlap heuristic (assuming that a premise entails from words within the hypothesis), sub-\nsequence heuristic( assuming that the premise entails all any of its contiguous sub-sequences of\nwords) and constituent heuristic (assuming that a premise entails a hypothesis that is any constituent\nwithin it's syntactic parse tree)."}, {"title": "A.9 FIT ON SHANS", "content": "Here we give the results of performing SFT and FIT on the SHANS datasets.\nSpurious HANS (SHANS) dataset. We generate binary NLI dataset sub-sampled from HANS (Mc-\nCoy, 2019), a dataset designed to challenge NLI models by exposing common heuristics they rely on,\nsuch as lexical overlap (whether the hypothesis shares many words with the premise), sub-sequence\n(whether the hypothesis is a contiguous sub-sequence of the premise), and constituent (whether the\nhypothesis is a grammatical sub-structure of the premise). The presence of these heuristics are spu-\nriously correlated with labels through sub-sampling the presence of each of the heuristics from the\noriginal dataset. The degree of co-occurrence is governed by Pspurious, which varies according to the\ntest sets described in Section 3. We ensure that pspurious is the same for all feature values within each\ndataset split. In particular, we set Pspurious to be 0.5, 0.5, 0.9, 0.25 and 0.9 on Dtrain, Diid, Dhigh, Dlow\nand Dflipped respectively."}]}