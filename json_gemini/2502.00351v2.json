{"title": "Multi-Order Hyperbolic Graph Convolution and Aggregated Attention for Social Event Detection", "authors": ["Yao Liu", "Tien-Ping Tan", "Zhilan Liu", "Yuxin Li"], "abstract": "Social event detection (SED) is a task focused on identifying specific real-world events and has broad applications across various domains. It is integral to many mobile applications with social features, including major platforms like Twitter, Weibo, and Facebook. By enabling the analysis of social events, SED provides valuable insights for businesses to understand consumer preferences and supports public services in handling emergencies and disaster management. Due to the hierarchical structure of event detection data, traditional approaches in Euclidean space often fall short in capturing the complexity of such relationships. While existing methods in both Euclidean and hyperbolic spaces have shown promising results, they tend to overlook multi-order relationships between events. To address these limitations, this paper introduces a novel framework, Multi-Order Hyperbolic Graph Convolution with Aggregated Attention (MOHGCAA), designed to enhance the performance of SED. Experimental results demonstrate significant improvements under both supervised and unsupervised settings. To further validate the effectiveness and robustness of the proposed framework, we conducted extensive evaluations across multiple datasets, confirming its superiority in tackling common challenges in social event detection.", "sections": [{"title": "1. Introduction", "content": "Social Event Detection (SED) entails the identification of interconnected message clusters from social media data streams to signify certain real-world events. This procedure can be regarded as part of the larger social event data stream. SED is extensively utilized in diverse domains, including sentiment analysis, disaster response, voter trend prediction, and the identification of harmful occurrences. These applications are prevalent in the majority of mobile apps featuring social functionalities, encompassing prominent platforms such as Twitter, Weibo, and Facebook. Consequently, social event detection yields significant insights for both enterprises, aiding in the evaluation of consumer preferences, and public services, facilitating assistance during emergencies and disaster management. Due to its extensive applicability, SED has attracted considerable interest and discourse from both academic and industrial domains. SED presents unique challenges compared to traditional text classification tasks, primarily due to the nature of social media data. These challenges include short text length, unique data structures, and complex heterogeneous relationships [1, 2, 3, 4, 5, 6, 7, 8]. The first challenge arises from the brevity of social media content. Time constraints, platform settings, or user preferences often constrain posts, resulting in short text that is challenging to process effectively. Creating high-quality labeled datasets from such content is also resource-intensive. Topic-based co-occurrence models were used in the early stages of SED research because they were effective and could show complex word relationships. However, these statistical methods suffered from sparse word representations and overlooked critical semantic information. To fix this problem, more recent research has used unsupervised or self-supervised methods like HISEvnet [9] and HCRC [10] to improve performance on high-quality datasets and added semantic context using tools like Word2Vec [11].\nThe second challenge stems from the tree-like structure of social media data. This structure arises from interactions like replies, mentions, and shares, often influenced by key opinion leaders. In Euclidean spaces, it's hard to model these kinds of hierarchical relationships well, as shown by how poorly they can capture tree-like patterns (see Figure 1). To overcome this, researchers have turned to hyperbolic spaces, which are better suited for representing hierarchical and tree-structured data.\nThe third challenge involves the multilayered and heterogeneous relationships inherent in social media. Social interactions generate diverse and complex connections, requiring sophisticated modeling techniques. Meta-path-based approaches, ranging from simple to advanced, have been employed to capture these intricate relationships effectively. Despite the excellent outcomes of current methodologies, they frequently neglect the higher-order interactions among event feature nodes. This research introduces a novel framework named Multi-Order Hyperbolic Graph Convolution with Aggregated Attention (MOHGCAA) to address prevalent issues in social event detection (SED) and the limitations of existing methodologies.\nThe proposed framework leverages the strengths of hyperbolic space to capture hierarchical structures effectively. It begins by projecting node features from Euclidean space into hyperbolic space, capitalizing on its natural suitability for representing hierarchical relationships. In the tangent plane of hyperbolic space at the origin, the framework encodes both first-order and higher-order syntactic relationships simultaneously. This design eliminates the need for excessively deep graph convolutional layers while preserving the richness of learned features. A dynamic attention mechanism further aggregates these multi-order representations, adaptively emphasizing the most task-relevant relationships. The final aggregated features are mapped back into hyperbolic space, ensuring consistency and maintaining the hierarchical integrity of the data for down-stream tasks such as classification.\nTo evaluate the generalizability of our method, we conducted extensive experiments not only on social network datasets but also on multiple general graph neural network datasets. Our framework demonstrated superior performance across diverse tasks. Additionally, we validated its effectiveness in both supervised and unsupervised settings, achieving consistently strong results. This work makes significant contributions to the fields of Social Event Detection (SED) and hyperbolic representation learning, addressing key challenges and advancing current methodologies."}, {"title": "2. Relate Work", "content": "Recent years have witnessed a surge of research focused on modeling graphs in hyperbolic space. The primary motivation lies in the unique properties of hyperbolic space, which features negative curvature and geometrical advantages well-suited for learning hierarchical representations of graphs. Gromov demonstrated that hyperbolic space is inherently better than Euclidean space for representing tree-like structures. As noted earlier, GCN-based models improve syntactic and semantic modeling compared to conventional neural networks. Researchers [35] have extended graph neural networks to hyperbolic space by leveraging tangent space. For example, [36] constructed spatial-temporal graph convolution networks in hyperbolic space for dynamic graph sequences, further exploring projection dimensions within tangent space using neural architecture search (NAS). [37] decoupled the message-passing process of GCN into three distinct operations: feature transformation, neighborhood aggregation, and activation. They then adapted each operation to hyperbolic space. Similarly, [38] introduced k-GCN, a mathematically robust generalization of GCN for constant curvature spaces. In social event detection, hyperbolic space has been applied primarily to social event detection. [8] demonstrated success using a simple HNN combined with MLP for social event detection. Building on this, [39] proposed the GraphHAM model, which integrates hyperbolic space with an automatic meta-path selection mechanism for heterogeneous information graphs. This framework optimizes meta-path weights and converts them into vectors, significantly reducing the reliance on labeled data. Additionally, the authors designed a Hyperbolic Multi-Layer Perceptron (HMLP) to better capture semantic and structural information in social media contexts. Despite these advancements, current studies often overlook multi-order representations in hyperbolic space, leaving room for further innovation in event detection."}, {"title": "2.1. Event Detection", "content": "Early event detection research focused on feature engineering to improve model performance. [12] utilized constituent and dependency parsers to extract linguistic features like word forms, lemmas, and entity paths, supplemented by contextual information such as semantic roles. POS tagging provided grammatical cues for event triggers, while [13] demonstrated the utility of unsupervised topic models in uncovering latent connections between events and entities, particularly in addressing imbalanced datasets like ACE-2005. [14] emphasized contextual features, using lexical and semantic patterns to enhance trigger-entity relationships. Despite their success, these approaches relied heavily on manual feature design, limiting scalability and paving the way for automated neural network methods. The advent of deep learning significantly advanced event detection. [15] introduced convolutional neural networks (CNNs) for event detection, emphasizing feature extraction through pooling techniques. [16] refined this with dynamic multi-pooling (DMCNN), segmenting sentences around event triggers to enhance semantic representation. Subsequent innovations, such as skip-window CNNs ([17]) and parallel multi-pooling CNNs ([18]), improved compositional feature capture. [19] extended CNNs to document-level contexts using a bootstrapping model. However, CNNs often struggled with long-range dependencies, as their localized focus and pooling operations sometimes led to information loss. To address these limitations, recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) networks ([20]), became prominent. [21] and [22] highlighted LSTMs' ability to model sequential dependencies. [23] integrated syntactic information using dependency bridges, blending hierarchical structures with sequential modeling. [24] advanced this further with a Tree-LSTM framework enriched by external ontologies, enhancing long-range dependency modeling and event representation. Graph Convolution Neural Networks (GCNs), introduced by [25], extended event detection beyond sequential representations by leveraging graph structures. GCN-ED ([26]) and JMEE ([27]) used dependency trees to model syntactic relationships. However, reliance on first-order syntactic edges limited these models' ability to capture diverse relations. [28] addressed this with the Multi-Order Graph Attention Network (MOGANED), enhancing representation diversity. [29] proposed GatedGCN, introducing gating mechanisms to improve layer differentiation, while [30] developed Edge-Enhanced GCN (EE-GCN) to incorporate dependency label information. Similarly, [31] improved event detection by integrating dependency edges in GTN-ED. The progress in social event detection builds upon the advancements in event detection research outlined above. By leveraging these foundational methodologies and adapting them to the unique challenges of social contexts, researchers have refined techniques to identify and analyze events within dynamic, interconnected social environments."}, {"title": "2.2. Social Event Detection", "content": "Social Event Detection (SED) has emerged as a challenging task that continues to garner substantial attention from researchers. Existing approaches to SED generally fall into two categories: topic-based methods and techniques leveraging heterogeneous information networks (HINs). Topic-Based Methods: Topic-based methods have been widely applied to social media data but face limitations due to the brevity and sparsity of text. For example, Latent Dirichlet Allocation [32] relies on word co-occurrence statistics, which are often insufficient in short-text scenarios. GPU-DMM [33] mitigates this limitation by integrating Word2Vec with LDA and DMM, adding semantic background information to enhance performance. However, it overlooks relational structures between data nodes. To address this, SGNS [34] models focus on learning contextual semantic relationships, effectively mitigating keyword sparsity in short texts. Inspired by SGNS, the Semantic-Assisted Non-Negative Matrix Factorization (SeaNMF) model was developed to capture semantics through word-document and word-context relevance in short-text corpora. While these methods have advanced topic modeling for SED, they generally fail to leverage the richness of HINs, which are capable of integrating multiple attribute representations to improve analytical depth. Heterogeneous Information Networks: The application of HINs to SED has shown considerable promise. Hao's pioneering work introduced an event-based meta-pattern to capture semantic relevance among events, incorporating external knowledge bases to enrich event representation. Building on this, Hao proposed PP-GCN [2], a heterogeneous graph convolution network that integrates event metaschemas and external knowledge to achieve fine-grained event categorization. This model's knowledge-instantiated similarity measure significantly enhanced its ability to identify semantic correlations among events, demonstrating exceptional performance in both detection and clustering tasks. Subsequent research has further exploited graph neural networks (GNNs) for SED. For instance, KPGNN, proposed by [4], utilizes incremental learning on HINs to improve detection accuracy. Despite these advances, existing methods struggle to accurately capture the hierarchical and tree-like structures inherent in social media data when constrained to Euclidean space. As highlighted earlier, this limitation poses a significant challenge for modeling multi-level relationships effectively. Our work addresses this gap by adopting hyperbolic space for node representation, enabling the modeling of multi-order relationships while preserving the hierarchical nature of social media data. This novel approach not only resolves structural representation issues but also offers a robust framework for advancing SED methodologies."}, {"title": "2.3. Hyperbolic Representation Learning", "content": "Recent years have witnessed a surge of research focused on modeling graphs in hyperbolic space. The primary motivation lies in the unique properties of hyperbolic space, which features negative curvature and geometrical advantages well-suited for learning hierarchical representations of graphs. Gromov demonstrated that hyperbolic space is inherently better than Euclidean space for representing tree-like structures. As noted earlier, GCN-based models improve syntactic and semantic modeling compared to conventional neural networks. Researchers [35] have extended graph neural networks to hyperbolic space by leveraging tangent space. For example, [36] constructed spatial-temporal graph convolution networks in hyperbolic space for dynamic graph sequences, further exploring projection dimensions within tangent space using neural architecture search (NAS). [37] decoupled the message-passing process of GCN into three distinct operations: feature transformation, neighborhood aggregation, and activation. They then adapted each operation to hyperbolic space. Similarly, [38] introduced k-GCN, a mathematically robust generalization of GCN for constant curvature spaces. In social event detection, hyperbolic space has been applied primarily to social event detection. [8] demonstrated success using a simple HNN combined with MLP for social event detection. Building on this, [39] proposed the GraphHAM model, which integrates hyperbolic space with an automatic meta-path selection mechanism for heterogeneous information graphs. This framework optimizes meta-path weights and converts them into vectors, significantly reducing the reliance on labeled data. Additionally, the authors designed a Hyperbolic Multi-Layer Perceptron (HMLP) to better capture semantic and structural information in social media contexts. Despite these advancements, current studies often overlook multi-order representations in hyperbolic space, leaving room for further innovation in event detection."}, {"title": "3. Preliminaries", "content": "This study explores the use of hyperbolic space to capture hierarchical relationships in tree-structured data, highlighting its advantages in node classification tasks. To set the stage, this section introduces foundational concepts in graph structures and hyperbolic geometry."}, {"title": "3.1. Definition of Graph Structure", "content": "A graph G = (V, E, N, E) consists of node sets V, edge sets E, node type sets N, and edge type sets E. A graph is classified as heterogeneous if |N| > 1 or |E| > 1, and homogeneous otherwise. Traditional Graph Neural Networks (GNNs) often use Euclidean space to model structural relationships, which can fall short in representing hierarchical information inherent to tree-like structures. Hyperbolic space, with its natural capacity for embedding hierarchical data, provides a more robust alternative, although it may still overlook complex inter-node relationships. This research addresses these limitations by adopting hyperbolic models, with the following section detailing relevant hyperbolic geometry concepts as a foundation."}, {"title": "3.2. Hyperbolic Models", "content": "Hyperbolic geometry, a type of Riemannian manifold with negative curvature, contrasts with the zero curvature of Euclidean space and the positive curvature of spherical geometry. Several models represent hyperbolic space, including the Poincar\u00e9 Ball Model P, the K\u00e4lin Model K, and the Lorentz Model L, as illustrated in Figure 2. While each model has distinct mathematical characteristics, they share structural equivalence. The Poincar\u00e9 Ball Model and Lorentz Model are particularly prevalent in graph representation research, and their definitions are provided below. Here, || || denotes the Euclidean norm and (,) denotes the Minkowski inner product.\nDefinition 3.1 (Poincar\u00e9 Ball Model): The Poincar\u00e9 Ball Model Pr, characterized by negative curvature, is defined as the Riemannian manifold $(B^n, g)$, which $B^n = \\{x \\in \\mathbb{R}^n : ||x||^2 < -1/c\\}$ represents an open n-dimensional ball with radius $1/\\sqrt{|c|}$. The metric tensor is given by $g = \\lambda(x)^2 g^E$, with $\\lambda(x) = 2/(1 + c||x||^2)$ as the conformal factor and $g^E$ as the Euclidean metric.\nDefinition 3.2 (Lorentz Model): Also known as the hyperbolic model due to its advantageous visualization prop- erties, the Lorentz Model is defined as the Riemannian manifold $(L^n, g_\\mathfrak{L})$ with negative curvature, where $L^n = \\{x \\in \\mathbb{R}^{n+1} : (x, x)_\\mathfrak{L} = 1/c\\}$ and $g_\\mathfrak{L} = diag([-1,1,...,1])_n$ denotes the Minkowski metric."}, {"title": "3.3. hyperbolic space representation", "content": "This section investigates node representations for graph neural networks within hyperbolic space.\nIn Euclidean space, node features are generally derived from pre-trained embeddings, node-specific attributes, random sampling, or one-hot encodings. In hyperbolic space, however, these features are projected into either the Poincar\u00e9 ball model or the Lorentz model to accommodate the curvature of the space.\nIn the Poincar\u00e9 ball model, the node representation $x^B = exp_o^\\mathfrak{P}(x)$ involves projecting features onto the tangent space at the origin. This projection is defined by:\n$exp_o^\\mathfrak{P}(v) = x \\frac{tanh \\left(\\frac{\\sqrt{|c|}||v||}{2}\\right)}{\\sqrt{|c|}||v||}v$ (1)\nwhere c denotes the curvature, x is a point in hyperbolic space, and v is a Euclidean vector.\nIn the Lorentz model, the mapping is defined as: $x^\\mathfrak{L} = exp_o^\\mathfrak{L}((0,x^E))$, where a zero is prepended $x^E$ to satisfy the condition imposed by the Minkowski inner product. The exponential mapping in the Lorentz model is expressed as:\n$exp_o^\\mathfrak{L}(v) = cosh(\\sqrt{c}||v||)x + \\frac{sinh (\\sqrt{c}||v||)}{\\sqrt{c}||v||}v$. (2)\nAfter obtaining the initial hyperbolic space representation, it becomes necessary to perform feature transformation for graph neural networks. The most straightforward method is to complete Euclidean feature operations by projecting the existing hyperbolic space onto the tangent space at a specific point. The notations $log_o^\\mathfrak{P}(x)$ and $log_o^\\mathfrak{L}(x)[1:n]$ representation operations in the Poincar\u00e9 ball model and the Lorentz model, respectively.\nIn the case of the Poincar\u00e9 ball model, the mapping to the tangent space is defined as:\n$log_o^\\mathfrak{P}(y) = \\frac{2}{\\sqrt{|c|}}tanh^{-1}(\\sqrt{|c|} \\frac{||-x \\ominus_c y||}{2})\\frac{-x \\ominus_c y}{||-x \\ominus_c y||}$"}, {"title": "4. Methodology", "content": "This section describes our methodology, beginning with an overview of multi-order graph convolution and aggregation in hyperbolic space (Section 4.1). We then apply this method to unsupervised learning event detection (Section 4.2) and validate its effectiveness in supervised learning settings (Section 4.3)."}, {"title": "4.1. Multi-Order Graph Convolution and Aggregated in Hyperbolic Space", "content": "To perform multi-order graph convolution aggregation in hyperbolic space, our method follows four key steps: First, we initialize the node features from Euclidean space into hyperbolic space and map these features to the tangent space at the origin. Second, we perform convolution operations of different orders within the tangent space to obtain high-order information about these features. Third, we aggregate the convolution representations from various orders using an attention mechanism. This ensures that the most relevant features are emphasized during the aggregation process. Finally, the aggregated representation in the tangent space is mapped back to a new hyperbolic space using the exponential map. This step obtains in the final output a node feature representation of a high-order convolution aggregation of hyperbolic space. This structured approach combines the advantages of hyperbolic geometry and graph convolution, enabling efficient and meaningful feature learning. The overall framework can refer the Figure5."}, {"title": "4.1.1. Overview", "content": "To perform multi-order graph convolution aggregation in hyperbolic space, our method follows four key steps: First, we initialize the node features from Euclidean space into hyperbolic space and map these features to the tangent space at the origin. Second, we perform convolution operations of different orders within the tangent space to obtain high-order information about these features. Third, we aggregate the convolution representations from various orders using an attention mechanism. This ensures that the most relevant features are emphasized during the aggregation process. Finally, the aggregated representation in the tangent space is mapped back to a new hyperbolic space using the exponential map. This step obtains in the final output a node feature representation of a high-order convolution aggregation of hyperbolic space. This structured approach combines the advantages of hyperbolic geometry and graph convolution, enabling efficient and meaningful feature learning. The overall framework can refer the Figure5."}, {"title": "4.1.2. Initial Hyperbolic Layer and Tangent Space Transformation", "content": "To initialize node features in hyperbolic space, we project Euclidean node features using the exponential map (Equation 1 and Equation 2 in Section 3.3). These features are then mapped to the tangent space at o via the logarithmic map (Equation 3 and Equation 4 in Section 3.3), enabling compatibility with Euclidean operations for graph neural network (GNN) processing."}, {"title": "4.1.3. Multi-Order Graph Convolution Network", "content": "We capture multi-order relationships through adjacency matrices A for each node, consisting of three submatrices of dimension n \u00d7 n: $A_{along}, A_{rev}$, and $A_{loop}$.\n\u2022 $A_{along}(i, j) = 1$ if there is a directed edge from $x_i$ to $x_j$;otherwise, it is 0.\n\u2022 $A_{rev} = A^T_{along}$ captures the reverse direction.\n\u2022 $A_{loop}$ is an identity matrix for self-loops.\nOur multi-order GCN module aggregates node features across these matrices, yielding feature vectors $h_i^l$ for each node:\n$h_i^l = f(x_i, a_{along}) \\oplus f(x_i, a_{rev}) \\oplus f(x_i, a_{loop})$ (5)\nwhere $f()$ is the graph convolution function in hyperbolic space, k is the orders to aggregate, and $\\oplus$ denotes element-wise addition. The convolution function $f(p_i, a_k)$ is defined as:\n$f(x_i, a_k) = \\sigma \\left(\\sum_{j=1}^n \\alpha_{ij} (W_{a,k} x_j + \\epsilon_{a,k})\\right)$ (6)\nHere, $\\alpha_{ij}$ represents local attention weights based on Ollivier Ricci curvature, computed as:"}, {"title": "4.1.4. Attention-Based Aggregation", "content": "To aggregate multi-order representations effectively, we apply a weighted attention mechanism:\n$h_i = \\sum_{k=1}^K v_i^k h_i^k$, (8)\nwhere $v_i^k$ is the attention weight for the k-order representation of node i, computed as:\n$v_i = softmax(s_i) = \\frac{exp(W_y tanh(Wx_i))}{\\sum_{j=1}^N exp(W_y tanh(Wx_j))}$ (9)\nHere, $W$ and $W_y$ serve as linear transformations that project node features into scalar attention scores, thereby highlighting the most relevant orders for each node's representation in hyperbolic space."}, {"title": "4.1.5. Mapping from Tangent Space to Hyperbolic Space", "content": "After aggregating in the tangent space, the representation is projected back to hyperbolic space using the exponential map (Equation 1 and Equation 2 in Section 3.3), aligning the representation with hyperbolic curvature. This projection optimizes the features for downstream tasks such as node classification and link prediction."}, {"title": "4.2. Multi-Order Unsupervised Hyperbolic Graph Convolution and Aggregated Attention for Social Event Detection", "content": "High-quality annotated datasets, whether for general or specialized domains, are resource-intensive to create. Contrastive learning, as a self-supervised approach, has gained popularity as it offers a cost-effective alternative. In this work, we employ unsupervised methods to validate the generality, scalability, and robustness of our proposed approach, as depicted in the overall framework diagram (Figure 6). The process consists of three main steps: graph data augmentation, multi-order hyperbolic space node feature graph convolution and aggregation encoding, and contrastive loss optimization."}, {"title": "4.2.1. Overall Framework", "content": "High-quality annotated datasets, whether for general or specialized domains, are resource-intensive to create. Contrastive learning, as a self-supervised approach, has gained popularity as it offers a cost-effective alternative. In this work, we employ unsupervised methods to validate the generality, scalability, and robustness of our proposed approach, as depicted in the overall framework diagram (Figure 6). The process consists of three main steps: graph data augmentation, multi-order hyperbolic space node feature graph convolution and aggregation encoding, and contrastive loss optimization."}, {"title": "4.2.2. Graph Data Augmentation", "content": "Graph data augmentation is essential for contrastive learning, especially in domains like computer vision (CV) and natural language processing (NLP). In the preliminary sections, we introduced the construction of graphs with nodes and relationships. Here, we use graph augmentation techniques to assess how our method affects node representation in hyperbolic space. Specifically, we select feature corruption as the primary augmentation technique for node features. Given a graph G = (V, A) and its augmented version G' = (V', A'), the node set changes (V \u2260 V') while the adjacency matrix remains the same (A = A')."}, {"title": "4.2.3. Multi-order Hyperbolic Space Node Feature Graph Convolution and Aggregation Encoding", "content": "Following the method we propose in 4.1.2, we map node features from Euclidean space to hyperbolic space. The transformation is defined as:\n$h_i^{1,H} = (W^1 \\oplus^H x_i^{0,H})$, (10)\nwhere $W^1 \\oplus^H x_i^{0,H}$ represents the hyperbolic tangent space operation described in Equation 1:\n$W x^B := exp_o^\\mathfrak{P}(W log_o^\\mathfrak{P}(x))$, (11)\nor alternatively in Equation 2,\n$W x^\\mathfrak{L} := exp_o^\\mathfrak{L}(0, W log_o^\\mathfrak{L}(x)[1 : n])$. (12)\nThe bias is Equation 13:\n$x_b^{H,H,H} = exp_o^H (P_{o\\rightarrow x^H} (log(b^H)))$ (13)\nwhere $b^H$ is the bias in hyperbolic space $H^c$ with curvature c and the parallel transport $P_{o\\rightarrow x^H}$ is the function from the origin o to the hyperbolic point $x^H$ under curvature c. By using the method in Section 4.1.3, we calculate the attention-weighted representation:\n$h_i^{1,H,k} = \\sigma (\\sum_{j=1}^n \\alpha_{ij} (W_{a,k} h_j^{0,H} + \\epsilon_{a,k}))$, (14)\nand apply the method in Section 4.1.4:\n$h_i^{1,H} = \\sum_{k=1}^K softmax(\\alpha_{i,k}^{A,N}) h_i^{k,1,H}$. (15)"}, {"title": "4.2.4. Contrastive Loss", "content": "Before expressing in hyperbolic space, we map the features to the tangent space with Equation 3 and 4, resulting in:\n$x = log(H)$, (18)\nand\n$x' = log(H')$. (19)\nthe contrastive loss function is defined as:\n$L_{MOUHGA} = \\frac{1}{N+M} \\sum_{i=1}^N \\mathfrak{R}(X) [log D(e_i, Y)] + \\sum_{j=1}^M \\mathfrak{R}(X') [log (1 - D(e'_j, Y'))])$, (20)\n, where Y = R(X) is derived using a readout function R(.) defined as:\n$R(X) = \\sigma (\\sum_{i=1}^N e_i )$, (21)\nwhere $e_i \\in X$. Finally, a discriminator D(\u00b7) [40] maximizes the mutual information, formulated as:\n$D(e_i, Y) = p (e_i^T W Y)$ (22)\n, where there W is a learnable scoring matrix, and $p$ denotes the nonlinear logistic sigmoid function."}, {"title": "4.3. Multi-Order Supervised Hyperbolic Graph Convolution and Aggregated Model for Social Event Detection", "content": "To validate the scalability and robustness of our proposed method, we applied the multi-order hyperbolic graph convolution aggregation to supervised learning tasks. The overall framework is illustrated in Figure 7. Specifically, the node features represented in Euclidean space are first initialized and mapped into hyperbolic space using the logarithmic map (Equation 3 and Equation 4). Subsequently, the features are projected onto the tangent space at the origin o via the exponential map (Equation 1 and Equation 2) to perform multi-order convolution aggregation. After aggregation, the updated features are mapped back into hyperbolic space using the logarithmic map (Equation 3 and Equation 4). Finally, the processed features are passed through a linear decoder and output to the classifier for loss computation."}, {"title": "4.3.1. Overall Framework", "content": "To validate the scalability and robustness of our proposed method, we applied the multi-order hyperbolic graph convolution aggregation to supervised learning tasks. The overall framework is illustrated in Figure 7. Specifically, the node features represented in Euclidean space are first initialized and mapped into hyperbolic space using the logarithmic map (Equation 3 and Equation 4). Subsequently, the features are projected onto the tangent space at the origin o via the exponential map (Equation 1 and Equation 2) to perform multi-order convolution aggregation. After aggregation, the updated features are mapped back into hyperbolic space using the logarithmic map (Equation 3 and Equation 4). Finally, the processed features are passed through a linear decoder and output to the classifier for loss computation."}, {"title": "4.3.2. Multi-Order Graph Convolution and Aggregation in Hyperbolic Space and Decoder Design", "content": "We utilized the encoding approach proposed in Section 4.1 to obtain representations in hyperbolic space:\n$H = exp(h_i^{1,H})$. (23)\nWe then project these embeddings back to Euclidean space using the logarithmic map (Equation 3 or Equation 4):\n$X = log(H)$. (24)\nAfter the projection, we compute the final linear decoding in Euclidean space as:\ny = W x + b. (25)\nFinally, we obtained the hyperbolic space feature representations of the nodes after multi-order convolution aggregation."}, {"title": "4.3.3. Classification and Loss Function", "content": "We define our task as a multi-node classification problem, where the given set $y = \\{y_i \\in R|1 \u2264 i \u2264 N\\}$ represents N message labels. The predicted probability for each class $p_i$ is computed using the softmax function:\n$p_i = \\frac{e^{y_i}}{\\sum_{j=1}^{n} e^{y_j}}$ (26)\nFinally, we minimize the cross-entropy loss function for training:\n$L_{MOHGCA} = -\\sum_{i=1}^{n} l_i log(p_i)$, (27)\nwhere $p_i$ is the predicted probability for class i, and $l_i$ represents the ground truth label."}, {"title": "5. Experiments", "content": "The experimental design was structured to address the following key research questions:\n\u2022 Q1: Does the proposed method outperform foundational approaches in prior works?\n\u2022 Q2: How do variations in the number of node relation attention aggregation steps and dimensional settings influence experimental outcomes?\n\u2022 Q3: Can experiments with multi-order configurations and dimensional settings empirically demonstrate that hyperbolic space outperforms Euclidean space?\n\u2022 Q4: What are the effects of different models within the hyperbolic space on the performance of the proposed method?\nThese questions aim to provide a comprehensive evaluation of our model's effectiveness, robustness, and theoretical advantages over existing methodologies."}, {"title": "5.1. Datasets", "content": "The datasets used in this experiment include: Our model primarily targets social media data, so we selected the real-world Twitter [41] dataset as our primary dataset. However, since the Twitter dataset is a large-scale dataset, contrastive learning with it may lead to out-of-memory errors. To address this, we used a balanced, smaller \"mini-Twitter\" dataset derived from the original dataset. Additionally, we used the Cora and the Citeseer [42] datasets to evaluate the effectiveness of modeling in hyperbolic space. Table 2 provides an overview of these datasets."}, {"title": "5.2. MOHGCAA Experiments in Unsupervised Settings", "content": "In this subsection, we evaluate the MOHGCAA model under unsupervised settings. Section 5.2.1 introduces the baseline experiments, and Section 5.2.2 provides a detailed description of the experimental environment and parameter settings. The remainder of this section addresses questions Q1 and Q2 with respect to the Multi-Order Supervised Hyperbolic Graph Convolution and Aggregated Model for Social Event Detection (MOUHGCAASED) model."}, {"title": "5.2.1. Baseline Models", "content": "To evaluate our model, we conducted extensive experiments using the mini-Twitter, Cora, and Citeseer datasets.We compared the performance against several state-of-the-art baselines, as detailed below:\n\u2022 DGI [43]: A single-branch graph contrastive learning model. It generates negative samples by either augmenting data or corrupting the original graph structure, excelling in unsupervised learning of node representations through mutual information maximization.\n\u2022 GraphCL [44]: A dual-branch graph contrastive learning framework. It performs augmentations on the input graph to generate two distinct views, capturing structural and semantic similarities to enhance representation learning.\n\u2022 GCN [25]: A foundational graph neural network model leveraging spectral graph theory to generalize convolution operations to graph structures. By utilizing graph Laplacians, GCN performs layer-wise aggregation of features from local neighborhoods, enabling efficient semi-supervised learning.\n\u2022 HGCN [45]: An extension of GCN to hyperbolic space, designed for hierarchical and complex graph data. HGCN capitalizes on the unique properties of hyperbolic geometry to represent hierarchical structures with exponential capacity, achieving superior results in tasks like node classification and link prediction.\n\u2022 HNN [46]: A hyperbolic neural network that operates directly in hyperbolic space to model hierarchical and structured datasets. By redefining core neural operations in hyperbolic geometry, HNN provides enhanced performance in classification and representation learning of complex data.\n\u2022 HyboNet [47]: A sophisticated hyperbolic neural framework fully utilizing the Lorentz model of hyperbolic geometry. HyboNet avoids inefficient mappings between Euclidean and hyperbolic spaces, incorporating hyperbolic rotations and attention mechanisms to achieve compact, powerful embeddings for relational and hierarchical data.\n\u2022 UHSED [8]: An unsupervised hyperbolic graph-based model for social event detection, specifically tailored for heterogeneous and unlabelled social media datasets. By constructing a unified social message graph and employing contrastive learning, UHSED captures both semantic and structural information effectively.\nThe performance of these models was benchmarked on multiple tasks, highlighting the strengths and limitations of each in various contexts. This comparison underscores the advancements introduced by our proposed approach."}, {"title": "5.3. MOHGCAA Experiments in Supervised Settings", "content": "This section presents the experimental results of MOHGCAA. Section 5.3.1 introduces the baseline experiments, while Section 5.3.2 describes the experimental setup and parameter configurations. The remaining subsections address the key research questions Q1 and Q2."}, {"title": "5.3.1. Baseline Models", "content": "To validate the robustness of our method, we conducted experiments in supervised scenarios. Intuitive experiments"}]}