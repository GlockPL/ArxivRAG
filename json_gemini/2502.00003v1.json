{"title": "Defending Compute Thresholds Against Legal Loopholes", "authors": ["Matteo Pistillo", "Pablo Villalobos"], "abstract": "Existing legal frameworks on artificial intelligence (\u2018AI') rely on training computational (or 'compute') thresholds as a proxy to identify potentially-dangerous Al models and trigger increased regulatory attention. In the United States, Section 4.2(a) of Executive Order 14110 instructs the Secretary of Commerce to require extensive reporting from developers of Al models above a certain training compute threshold. In the European Union, Article 51 of the AI Act establishes a presumption that AI models above a certain compute threshold have high impact capabilities and hence pose systemic risk, thus subjecting their developers to several obligations including capability evaluations, reporting, and incident monitoring. The bedrock of training compute thresholds lies in scaling laws\u2014the observation that the compute used to train an AI model is correlated with its performance and, hence, its base capabilities.", "sections": [{"title": "I. Introduction", "content": "The term 'compute thresholds' refers to a regulatory threshold devised for the governance of frontier Al models.\u00b9 Compute thresholds offer a simple and clean, albeit imprecise,\u00b2 solution to categorizing Al models and their potential for risk. Relying on scaling laws, these thresholds aim at pre-selecting potentially-dangerous Al models by taking the compute used for their training as a proxy for their capabilities and potential ability to cause harm, and subjecting these models to greater regulatory attention including capability evaluations, reporting obligations, and incident monitoring.\nAt the time of writing, two compute thresholds have been successfully established: at the federal level in the United States through Executive Order 14110,4 and in the European Union through the AI Act.5 A third compute threshold came very close to existence at the state level in the United States, through California Senate Bill 1047.6 The bill was approved by the California legislature but then vetoed by the State's Governor.\u201d Despite reasonable criticism being leveled at compute thresholds, it is possible that this type of regulatory threshold will become more prevalent across the globe in the coming years, given that it provides an \u2018easy\u2019solution to categorize frontier AI models. For instance, the upcoming United Kingdom's frontier AI Bill might include a form of training compute threshold. Furthermore, the establishment of compute thresholds has been advocated for in the People's Republic of China by a group of influential scholars.9\nExecutive Order 14110 requires the Secretary of Commerce\u2014in consultation with the Secretary of State, the Secretary of Defense, the Secretary of Energy, and the Director of National Intelligence to define within 90 days \u201cthe set of technical conditions\u201d under which AI models must be subject to reporting obligations as set forth under Section 4.2(a).10 Until then, the Secretary of Commerce must require that \u201cany model that was trained using a quantity of computing power greater than 10^26 integer or floating-point operations\u201d comply with Section 4.2(a) reporting requirements.\nAt the time of writing, no known Al model crosses the 1e26 OP or FLOP training compute threshold, which means that no existing model is subject to these proposed oversight measures through this pathway. 12\nThe AI Act establishes that a general-purpose AI model poses systemic risk if \u201cit has high-impact capabilities\u201d as determined by \"appropriate technical tools and methodologies, including indicators and benchmarks,\u201d or \u201ca decision of the Commission.\u201d13 In this respect, a general-purpose Al model is presumed to have high-impact capabilities if \u201cthe cumulative amount of computation used for its training measured in floating point operations is greater than 10^25.\"\nIn light of \u201cevolving technological developments, such as algorithmic improvements or increased hardware efficiency,\u201d the Commission can amend the threshold through delegated acts, as well as \u201csupplement benchmarks and indicators.\u201d15 At the time of writing, around 20 models appear to cross the 10^25 FLOP threshold.16\nThe vetoed California Senate Bill 1047 defined \u201ccovered model[s]\u201d as AI models that: (i) were trained with a quantity of compute greater than 1e26 OP or FLOP, whose cost exceeded 100 million dollars; or (ii) were created by fine-tuning a \u201ccovered model\u201d with 3e25 FLOP or OP, or more, for a cost exceeding 10 million dollars.18 The threshold was supposed to be updated by the Government Operations Agency on and after January 1, 2027.\nThe vetoed Bill also included the category of \u201c[c]overed model derivative,\u201d which was defined as: (i) \u201c[a]n unmodified copy of a covered model;\u201d (ii) \u201c[a] copy of a covered model\u201d that has been \u201csubjected to post-training modifications unrelated to fine-tuning;\" (iii) \u201ca copy of a covered model\u201d that has been fine-tuned with less than 3e25 FLOP or OP (or the threshold to be determined by the Government Operations Agency on and after January 1, 2027), for a cost exceeding 10 million dollars; or (iv) \u201ca copy of a covered model that has been combined with other software.\u201d20\nIn this paper, we highlight four capability-enhancing techniques that might lead to legal loopholes in the mentioned training compute thresholds: (i) significant fine-tuning: (ii) model reuse, such as knowledge distillation, kickstarting, and reincarnation; (iii) model expansion; (iv) above-optimal inference compute use. These techniques enable AI developers to increase their models' capabilities while simultaneously conserving training compute resources, thereby potentially allowing them to circumvent the additional regulatory compliance triggered by the compute thresholds set forth under Section 4.2(a) of Executive Order 14110 and Article 55 of the AI Act. For instance, these loopholes can enable developers to elude the obligations to provide the U.S. Government with \u201cinformation, reports, or records\u201d on \u201cthe results of any developed dual-use foundation model's performance in relevant AI red-team testing,\"21 or to perform model evaluations, assess and mitigate systemic risks, collect information about serious incidents, and ensure adequate cybersecurity protection under the AI Act.\nThis fallacy in compute thresholds has been recently raised by other scholars, who observed that \u201cmany improvements dramatically improve performance but are currently completely ignored by compute thresholds since they don't contribute to training FLOP\u201d23 and \u201c[t]he FLOPS threshold incentivizes providers to optimize models to fall under the threshold without necessarily making those models less dangerous.\"24\""}, {"title": "II. Legal Loopholes and Policy Suggestions", "content": "This Section examines four illustrative capability-enhancing and compute-saving techniques that might constitute legal loopholes to existing training compute thresholds.\n\u2022\tII.A: Fine-tuning above a certain threshold.\n\u2022\tII.B: Model reuse, such as knowledge distillation, kickstarting and reincarnation.\n\u2022\tII.C: Model expansion.\n\u2022\tII.D: Above-optimal inference compute use.\nEach Section offers a description of:\n(i) The techniques.\n(ii) The potential legal loopholes to the existing training compute thresholds.\n(iii) How the potential legal loopholes interact with the existing legal frameworks.\n(iv) Our policy recommendations to patch the described loopholes.\nWe believe that more research should be pursued on this topic. First, more techniques than the ones described in this paper are capable of enhancing the capabilities of trained models, including scaffolding, compression, and solution choice. Second, additional research could reach a more precise estimate of compute equivalent savings, as current estimates are based on observations from experiments that were not explicitly designed to measure them."}, {"title": "A. Fine-tuning", "content": "Large language models (\u2018LLMs') are usually trained in at least two different stages\u2014pre-training and fine-tuning. These stages rely on the same fundamental operations at the technical level, and are only distinguished by the amount of training, the type of data, and the effects. Pre-training is more computationally intensive and endows the model with powerful general-purpose capabilities. Fine-tuning is less demanding in terms of compute and relies on specially crafted data to refine a model's capabilities.26\nFine-tuning is a crucial component of modern LLMs, and most frontier models use some form of it.\nFine-tuning is generally used to adapt AI models to specific use-cases (like math28), to align Al models with human preferences,29 and for instruction tuning\u2014which is regarded as an essential process to elicit model capabilities.30 As Figure 1 below shows, fine-tuning can have a significant impact on the capabilities of models, particularly in narrow tasks. However, it is generally accepted that fine-tuning in most cases does not significantly increase general capabilities, but merely modulates them.\nGiven its broad usefulness, large diffusion, and limited impact on general capabilities, fine-tuning is often considered as not warranting immediate monitoring or regulatory oversight.\nNonetheless, there is a risk that fine-tuning could circumvent existing compute thresholds. Consider the following example, visually represented in Figure 2 below. A developer has an existing Al model (\u2018Model A'), which currently sits below the regulatory threshold. The developer wants to produce a new, more capable model using additional compute. If the developer trains a new model from scratch exceeding the threshold (\u2018Model B'), it will be subject to regulatory compliance. The developer could then opt for a less-burdensome route: fine-tuning the original model (Model A) on general-purpose data with an additional amount of compute that is lower than the compute threshold, but that, if cumulated with the compute used for the initial training, would be greater than the compute threshold (or lower than the compute threshold but with comparable capabilities). This maneuver has the potential to exempt developers from regulatory compliance and possibly compromise the intended safety goals of the regulatory framework.\nExecutive Order 14110 does not contain any specification around how fine-tuning should be considered for the purposes of the compute threshold set forth under Section 4.2(a)\u2014making it unclear whether and to what extent fine-tuning compute counts towards the 1e26 OP or FLOP compute thresholds set forth therein (see Figure 2 above).\nArticle 51(2) of the AI Act specifies that the 1e25 FLOP compute threshold\u2014which triggers a presumption of systemic risk\u2014includes the \u201ccumulative amount of computation used for [] training.\u201d35 Recital 111 of the AI Act clarifies that \u201c[t]he cumulative amount of computation used for training\u201d includes \u201cthe computation used across the activities and methods that are intended to enhance the capabilities of the model prior to deployment, such as pre-training, synthetic data generation and fine-tuning.\u201d36 Therefore, in the European Union, fine-tuning compute likely counts towards the 1e25 FLOP compute threshold (see Figure 2 above).37\nUnder the vetoed California Senate Bill 1047, the definition of \u2018covered model' would have included Al models that were \u201ccreated by fine-tuning a covered model using a quantity of computing power equal to or greater than three times 10^25 integer or floating-point operations.\u201d38 In other words, covered models would have included models above 1e26 OP or FLOP39 that were then fine-tuned with 3e25 OP or FLOP or more. If a developer had retrained a covered model (above 1e26 OP or FLOP) with less than 3e25 OP or FLOP, the model would have fallen within the category of \u2018covered model derivatives.\u201940 California Senate Bill 1047 defined fine-tuning as \u201cadjusting the model weights of a trained covered model or covered model derivative by exposing it to additional data.\u201d41 Therefore, fine-tuning would have counted towards compute thresholds in California, but only if the starting model\u2014i.e., the model before fine-tuning-had already exceeded 1e26 OP or FLOP. Whereas it appears that fine-tuning would not have counted towards compute thresholds if the starting model had not already been a 'covered model,' such as for instance if the starting model was at 1e25 OP or FLOP (see Figure 2 above).\nThe United States could still patch this potential legal loophole. As mentioned above, the Secretary of Commerce has yet to define the \"set of technical conditions\u201d for models that would be subject to the reporting requirements.\nCompute thresholds might also be subject to judicial review at a later point, which could offer courts an opportunity to interpret the gaps. Adding a specification that the 1e26 OP or FLOP threshold is \u2018cumulative,' along the lines of the AI Act, is certainly one option for making the compute threshold set forth under Executive Order 14110 more comprehensive. However, there may be a better alternative. In fact, the expression 'cumulative' might include within the threshold any and all compute used to fine-tune AI models, including for the purposes of alignment with human preferences and capability elicitation. In addition to being undesirable, the compute necessary to fine-tune an Al model and the relative costs are only a very small fraction of training compute\u2014typically < 1% and sometimes < 0.01% of the pre-training cost.\nThis makes fine-tuning very widespread and thus impractical for regulators to track. Consider, for instance, that Hugging Face's model hub lists more than one million models, of which more than 80,000 are fine-tuned or modified versions of Llama.\nA policy solution targeting fine-tuning should, therefore, distinguish legitimate uses of fine-tuning from disguised model retraining. Since fine-tuning rarely alters an AI model's general capabilities, one could distinguish the two based on the generation of new general capabilities.\nIn this way, the demarcation line would be set at the minimum amount of compute necessary to create new capabilities\u2014in other words, at the minimum detectable change. When fine-tuning is so extensive that an Al model develops new capabilities, it should be counted within the compute threshold. Based on the data from the Chinchilla paper, a 14.4% increase in training compute is sufficient to improve an Al model's test loss by 2%, a statistically significant improvement.\nTherefore, the minimum detectable change sits around 14.4% of training compute\u2014which can be rounded up to 15%, to avoid false positives. With a 15% increase in training compute, either in one solution or in multiple installments, one can confidently increase capabilities by at least 1%.\nIn light of the above, we suggest that fine-tuning compute is counted towards the existing compute threshold when\u2014in one instance or in the aggregate\u2014it is greater than 15% of the compute used to train the AI model. By contrast, fine-tuning compute should not be counted whenever it is lower. The effect of this policy recommendation would be to also include within the existing thresholds AI models that are developed with a lower amount of initial training compute and that are subsequently fine-tuned with compute exceeding 15% of the training compute. For instance, assuming a fine-tuning threshold at 15% of training compute, AI models developed with 1.5e25 OP or FLOP would fall within the threshold established in the United States.\nThis policy suggestion would be desirable for three reasons. First, it emphasizes fairness. AI models with the same capabilities would be treated equally, regardless of the capacity-enhancing technique used. At the same time, this solution utilizes and relies on the compute threshold already identified in Executive Order 14110, at 1e26 OP or FLOP. It does not aim to expand this threshold to include Al models with lower capabilities than the frontier Al models already identified as requiring additional oversight. Instead, it only seeks to explain how this threshold could be better applied in a context in which AI developers utilize extensive fine-tuning. Second, enactment and enforcement of this policy solution would be practical for regulators. At the time of writing, less than 10 developers appear to have developed Al models above 1.5e25 OP or FLOP globally, making enforcement of this policy-including the relevant monitoring-feasible for regulators. Third, regulatory compliance would not burden individuals, small organizations, or research centers that fine-tune models. The cost to train a model at 1e26 FLOP or OP is estimated around 100 million USD49\u2014which means that the cost of fine-tuning an Al model with compute above 15% of the model's initial training compute (over 15 million USD) would likely be sustainable only for large organizations.\nOur policy suggestion would be best complemented by reporting requirements triggered once fine-tuning compute crosses the 15% threshold. For instance, Article 52 of the AI Act-according to which \u201cthe relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement\u201d (i.e., the crossing of the 1e25 FLOP training compute threshold) \u201cis met or it becomes known that it will be met\u201d\u2014could be adapted to include fine-tuning compute above the relevant threshold.50"}, {"title": "B. Model Reuse: Knowledge Distillation, Kickstarting, and Reincarnation", "content": "Knowledge distillation, kickstarting and reincarnation are three techniques that fall within the realm of model reuse.51 The term 'model reuse' refers to the practice of \u201creus[ing] pre-trained models to help further model building.\"\nIn other words, \u201crather than building a model from scratch,\" AI developers \u201cconstruct a model by utilizing existing available models, mostly trained for other tasks.\nKnowledge distillation consists in having a newer, smaller AI model learn from an older, larger Al model or ensemble of AI models.\nA larger model or multiple small models generate data to train a new model, which will then have the same capabilities as those initial models. In other words, knowledge distillation distills knowledge from \"a large teacher model\" into a \"small student model.\"\nThe student model \u201clearns from the outputs of an old teacher model.\u201d\nRather than developing two \u201ccumbersome model[s]\u201d from scratch, the second AI model can learn from the first. In this way, AI developers can improve \u201csmaller and cheaper models by distilling knowledge from expensive and proprietary models, such as those behind ChatGPT.\u201d57\nKickstarting \u201cemploy[s] already trained agents as teachers to share demonstrations with a student who has to learn the same task from scratch.\"\nHowever, in contrast to distillation, the kickstarted 'student' AI model is allowed to surpass its \u2018teacher' in performance.\nReincarnation refers to techniques in reinforcement learning that leverage previously trained agents to quickly train a new model, instead of training it from scratch.\nAll these techniques of model reuse-knowledge distillation, kickstarting, and reincarnation-constitute important scientific development. However, they could also represent a legal loophole to existing training compute thresholds as they can enable a significant reduction of compute consumption by exploiting already-existing models61 and expedited learning in real-world tasks.62 These techniques \"leverage existing computational work\"63 so that the \"[t]raining compute expended on earlier models can partially substitute for the compute needed in subsequent training runs,\u201d64 thus cutting down computational costs.65 In other words, model reuse aims at increasing \u2018adjusted compute,' which has been defined as the \u201chypothetical amount of compute that would be required to train a model from scratch to reach the same level of performance that the actual model reached during training while employing model recycling.\u201d\nAs shown in Figure 3 above, compute savings from model reuse can be substantial.\nA distilled model can be up to ~10x smaller than its teacher. For instance, Orca learned from GPT-4 and ChatGPT. Its size is only 13B parameters, and its compute is 10x smaller than that of ChatGPT. BERT's knowledge has been distilled into Tiny-BERT, a smaller \u201cstudent\u201d model that is 7.5x smaller than BERT. DistilBERT reduced BERT's size by 40%. MiniMA was distilled from LLaMA2-7B using 50x less compute.\nSimilarly, it has been estimated that a kickstarted AI model can match the performance of an AI model trained from scratch in ~10x fewer steps.\nWith respect to reincarnated models, scholars have shown that the compute savings are 10x-15x for matching the performance of the previous model, and 2x-5x for surpassing it.\nIf we assume that distilled, kickstarted, and reincarnated models can be ~10x smaller than their teachers, it means that AI developers can use up to roughly ~10x less compute to develop an AI model with capabilities comparable to the teacher model, or even better capabilities.\nTherefore, AI developers interested in scaling up an existing model that does not yet surpass an existing compute thresholds (\u2018Model A'), could either: (i) develop a new model (\u2018Model B') using more compute, thereby crossing the compute threshold and ensuring the model be subject to regulatory compliance; or (ii) use knowledge distillation, kickstarting, or reincarnation on an existing model below the threshold (Model A) and potentially obtain an AI model (\u2018Model A+') that has the same capabilities of the scaled-up model (Model B) but with a computational expenditure that falls below the compute threshold. As shown in Figure 4 above, this second route would enable AI developers to avoid regulatory compliance triggered by compute thresholds.\nA third option for AI developers could be to: (iii) develop a 'teacher model' above the compute threshold, and never launch it on the market ('Incognito' Model A); then use knowledge distillation, kickstarting, or reincarnation to obtain a marketable AI model that sits below the threshold (Model A+). Similarly to option (ii), this third option could spare AI developers the burden of threshold-triggered regulatory compliance, as shown in Figure 4 above. However, this route heavily depends on how thoroughly compute reporting is verified by regulatory authorities and whether they check for AI models that are not deployed externally.\nThe Executive Order on AI does not address model reuse.\nSimilarly, model reuse is not addressed by the AI Act.\nThe proposed definition of \u2018covered model derivative' included \u201ca copy of a covered model that had been subjected to post-training modifications unrelated to fine-tuning,\u201d75 or \"that had been combined with other software.\u201d76 This could have included model reuse. However, California Senate Bill 1047 only considered post-training modifications applied to \u2018covered models.' Therefore, reused Al models below the threshold (1e26 OP or FLOP) would not have been regulated.\nTwo policy solutions could patch this legal loophole. First, the model derived through knowledge distillation, kickstarting, or reincarnation (Model A+) could be subject to regulatory compliance every time the starting \u2018teacher' model (Model A)\u2014whether incognito or not\u2014is greater than the compute threshold, regardless of whether Model A+ sits below or above the compute threshold.\nSecond, when knowledge distillation, kickstarting, or reincarnation are used, the relevant compute thresholds could be lower by a number of times equal to the compute savings."}, {"title": "C. Model Expansion", "content": "Model expansion refers to a technique in which \u201ca smaller model's pre-trained parameters are used to initialize a subset of a larger model's parameters,\u201d80 either by \u201cderiving new neurons from the existing ones\u201d or \u201cinitializing new parameters separately.\"\nIn other words, new learnable parameters are added to an already-trained AI model to make it larger and more capable. Simplifying the concept with a metaphor, model expansion is comparable to enhancing a human brain by adding additional gray matter that contains no knowledge and is ready to be trained. The larger model learns faster than a model of the same size trained from scratch by virtue of having parts of it already trained.\nAs shown in Figure 5 below, model expansion enables considerable compute savings. Such savings are estimated in a range between 20 and 76%, depending on the exact technique used.\nHence, instead of taking an AI model below the compute threshold (\u2018Model A') and scaling it up above the threshold (\u2018Model B'), AI developers could add new untrained modules to Model A and train only these new parts with a quantity of compute that is lower than the compute thresholds. This process could lead to two potential loopholes, visually represented in Figure 6 below.\nI.\tThe resulting model could have capabilities that are equivalent to Model B, while using an amount of compute that is smaller than the compute thresholds. This could allow AI developers to avoid the regulatory compliance triggered by training compute thresholds.\nII.\tWhile the training compute and the growth compute are each smaller than the compute thresholds, the resulting model might exceed compute thresholds in the aggregate. Depending on how countries consider the compute used for model expansion, this could also allow AI developers to elude regulatory compliance.\nExecutive Order 14110 does not regulate the first potential loophole described above (I). Similarly, with respect to the second potential loophole described above (II), it is unclear whether the compute used for model expansion counts towards the compute thresholds established under Executive Order 14110 (see Figure 6 above).\nThe AI Act does not regulate the first potential loophole described above (I). Instead, with respect to the second potential loophole described above (II), both the training compute and the compute used for model expansion would likely be counted towards the \u201ccumulative\u201d compute threshold in the European Union and the resulting model would thus likely fall within the scope of compute thresholds in the European Union (see Figure 6 above).\nThe California Senate Bill 1047 does not regulate the first potential loophole described above (I). With respect to the second potential loophole (II), California Senate Bill 1047 would have only regulated cases in which: (i) \u201cpost-training modifications unrelated to fine-tuning\u201d such as model expansion\u2014are applied to \u201c[a] copy of a covered model;\u201d or (ii) \u201cother software\u201d is combined with \"[a] copy of a covered model.\u201d If the starting model (Model A) were not already a 'covered model\u2019\u2014i.e., it fell below the compute threshold\u2014then model expansion would not have been regulated by the proposed California Senate Bill 1047 compute threshold (see Figure 6 above).\nFollowing the rationale already laid out with respect to fine-tuning and model reuse (see previous Sections II.A-B), a possible solution to the model expansion loophole could be to factor the relevant compute savings in the calculation of compute thresholds. Assuming a compute saving in the range of 20-76%, AI developers could be subject to the regulatory compliance triggered by compute thresholds when: (a) they use model expansion; and (ii) the resulting Al model crosses 5e25 OP or FLOP in the United States (or 5e24 FLOP in the European Union). For a conservative approach that takes into account a possible 76% compute saving, a model expansion threshold could even be set at 2e25 OP or FLOP in the United States (or 2e24 FLOP in the European Union)."}, {"title": "D. Inference", "content": "Inference is the process through which a trained AI model is used to make new predictions based on users' input, such as in the form of a prompt. Each of the users' requests entails a compute expenditure, which can be defined as \u2018inference compute per request.\u2019\nWhen we refer to inference compute in this paper, we mean inference compute per request.\nThe current common practice is to train AI models with a dataset size proportional to the number of parameters. The compute per inference token is proportional to the model size. Hence, in a model trained compute-optimally, inference compute is proportional to or slightly less than the square root of the training compute.\nFor example, the compute-optimal inference compute for a model trained on 1e24 OP or FLOP is around 1e11 OP or FLOP.\nWhen inference compute is higher than these levels, inference compute is \u2018above compute optimality.' For example, if an Al developer increases the inference compute from 1e11 OP or FLOP to 1e14 OP or FLOP, there is an increase of 3 orders of magnitude (\u2018OOM') above the compute-optimal inference compute.\nThere is a proven relationship between above compute-optimal inference compute and capabilities, as shown in Figure 7 below.\nSpecifically, an increase of 2-3 OOM above compute-optimal inference compute is estimated to allow developers to save ~2 OOM in training compute, while maintaining performance. Capabilities usually plateau after that, with the exception of math and coding. In the latter case, an increase of 5-6 OOM above compute-optimal inference compute corresponds to a saving of 3-4 OOM in training compute.\nWithin these limits, it is possible to increase a model's capabilities at the cost of additional inference compute per request.\nHence, AI developers could shift compute from training to inference, thus obtaining equally-capable systems while utilizing less training compute. This would allow developers to stay within established training compute thresholds while obtaining more capable Al models, as shown in Figure 8 below.\nInference compute is not considered by Executive Order 14110. Therefore, it remains unclear if and how inference compute will be considered for the purposes of identifying covered models, and Al developers could exploit this gap to game training compute thresholds (see Figure 8 above).\nThe AI Act also does not appear to take into account inference compute in the cumulative training compute threshold, opening the door to the potential loophole described above (see Figure 8 above).\nSimilarly, inference compute was not considered in the vetoed California Senate Bill 1047 (see Figure 8 above).\nAs Al models continue to scale, we recommend that above-optimal inference compute should be considered when calculating training compute thresholds. In particular, training compute thresholds should be deemed as crossed if the above the compute-optimal inference compute is equal to an increase in training compute that would have caused the AI model to cross the training compute threshold.\nConsider for example a le24 OP or FLOP model, whose compute-optimal inference compute sits around le11 OP or FLOP. If an AI developer uses 1e15 OP or FLOP instead of le11 OP or FLOP, the amount of inference compute \u2018in excess' (+3 OOM, equivalent to ~2 OOM in training compute) should be added to the training compute used to develop the model. In this example, the total would be 1e26 FLOP or OP (1e24 FLOP or OP + 2 OOM), and the AI developer would have crossed the compute threshold in the European Union.\nThis recommendation encounters two main uncertainties. First, trying to bypass compute thresholds using above-optimal inference compute could be economically inconvenient for Al developers. Increasing inference compute harms the scalability of AI systems, as each interaction with the system becomes more computationally expensive. Hence, it is possible that this technique will pose less concerns than expected. Second, enforcing this type of policy approach could be challenging for math and coding, where an increase of 5-6 OOM above compute-optimal inference compute corresponds to a saving of 3-4 OOM in training compute. There are hundreds of models above 1e23 OP or FLOP, and even more above 1e22 OP or FLOP,100 making it difficult for regulators to monitor above-optimal inference compute use."}, {"title": "III. Conclusion", "content": "Despite the criticisms reasonably leveled at training compute thresholds, this form of regulatory threshold is relied on in existing legal frameworks, such as Executive Order 14110 and the AI Act and, and might appear in upcoming frameworks as well, such as the UK frontier AI Bill. The main reason for its use in Al legal frameworks is the ease with which it can, albeit imperfectly, pre-identify potentially dangerous systems of concern and trigger additional regulatory attention.\nConsidering the importance of training compute thresholds for AI governance at the time of writing, in this paper we examined some potential legal loopholes to compute thresholds, assessed how they interact with existing legal frameworks, and recommended policy solutions to increase the robustness of this regulatory tool."}]}