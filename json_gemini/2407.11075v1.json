{"title": "A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)", "authors": ["Yuntian Hou", "Di Zhang", "Jinheng Wu", "Xiaohang Feng"], "abstract": "Through this comprehensive survey of Kolmogorov-Arnold Networks (KAN), we have gained a thorough understanding of its theoretical foundation, architectural design, application scenarios, and current research progress. KAN, with its unique architecture and flexible activation functions, excels in handling complex data patterns and nonlinear relationships, demonstrating wide-ranging application potential. While challenges remain, KAN is poised to pave the way for innovative solutions in various fields, potentially revolutionizing how we approach complex computational problems.", "sections": [{"title": "Introduction", "content": null}, {"title": "Research Background", "content": "Kolmogorov-Arnold Networks (KAN) represent an emerging neural network architecture founded on the theorems of Kolmogorov and Arnold. These theorems assert that any continuous multivariable function can be represented as a superposition of a finite number of single-variable functions. This discovery provides a robust mathematical basis for the design of KAN, allowing it to replace the fixed linear weights in traditional neural networks with learnable single-variable functions, thereby enhancing the model's flexibility and interpretability.\nIn 1957, Kolmogorov's theorem demonstrated that any continuous multivariable function could be represented by a finite number of single-variable functions [13]. Arnold extended this work in 1958, further proving the theorem's applicability to three-variable functions [2]. These foundational studies have paved the way for the development of KAN, enabling it to tackle more complex function representation and learning tasks [13][2]. The application of KAN benefits from classical literature in numerical analysis and partial differential equation (PDE) solving. For instance, Babuska and Rheinboldt [3] discussed the application of adaptive finite element methods in PDE solving, providing a theoretical background for KAN's use in numerical analysis. The importance of the CFL condition proposed by Courant, Friedrichs, and Lewy [6] also supports understanding KAN's application in PDE solving. Additionally, the regularization method proposed by Tikhonov [17] has played a crucial role in handling ill-posed problems and PDE solving, significantly impacting KAN's application in data fitting techniques [3][6][17].\nIn recent years, KAN's application in machine learning has garnered widespread attention. Its primary advantages lie in model flexibility and interpretability. Unlike traditional multilayer perceptrons (MLP), \u039a\u0391\u039d does not have fixed activation functions but achieves this through learnable single-variable functions. This characteristic makes KAN excel in data fitting and complex function learning. For example, Liu et al. demonstrated KAN's superior performance in data fitting and PDE solving, showing that even smaller KAN models could match or exceed MLP performance in these tasks [14]. Another significant advantage of KAN is its performance in handling high-dimensional data. Xu et al. [20] applied KAN in time series analysis, proving KAN's capability in capturing complex temporal dependencies [20]. Furthermore, KAN's applications in graph-structured data processing and hyperspectral image classification have showcased its wide applicability and exceptional performance [8]."}, {"title": "Research Objectives", "content": "This paper aims to systematically review the theoretical foundations, implementation methods, and performance of Kolmogorov-Arnold Networks (\u039a\u0391\u039d) in various applications. The specific objectives include:\n1. Elucidate the Basic Structure and Working Principles of KAN:\nTo provide a comprehensive understanding of the fundamental architecture and operational mechanisms of KAN.\n2. Explore Implementation Methods and Challenges in Various Tasks:\nTo investigate the diverse methodologies for implementing KAN across different tasks and the associated challenges.\n3. Analyze Performance in Practical Applications and Comparison with Other Models: To evaluate the performance of KAN in real-world applications and compare it with other models to highlight its strengths and weaknesses.\n4. Summarize Current Research Progress and Propose Future Directions: To summarize the advancements in KAN research and suggest potential future research directions.\nThrough this review, readers will gain a thorough understanding of the current state and developmental potential of KAN, providing valuable insights for future research and applications."}, {"title": "Overview of Kolmogorov-Arnold Networks", "content": null}, {"title": "Theoretical Foundation", "content": "The Kolmogorov-Arnold theorem forms the theoretical foundation of Kolmogorov-Arnold Networks (KAN). This theorem, proposed by the Russian mathematician Andrey Kolmogorov in 1957, addresses the representation of continuous multivariable functions. Kolmogorov's theorem states that any continuous multivariable function can be represented as a superposition of a finite number of continuous single-variable functions. Specifically, for an n-dimensional continuous function $f(x_1,x_2,...,x_n)$, there exists a set of continuous single-variable functions $g_i$ and $h_{ij}$ such that:\n$f(x_1, x_2,...,x_n) = \\Sigma_{i=1}^{2n+1} g_i(\\Sigma_{j=1}^{n} h_{ij} (x_j))$\nThis theory indicates that complex multivariable functions can be represented through combinations of simpler single-variable functions, offering a new perspective for neural network design [13]. Subsequently, Vladimir Arnold extended Kolmogorov's work in 1958, further proving the theorem's application to three-variable functions. Arnold's research not only validated the universality of Kolmogorov's theorem but also demonstrated its effectiveness in representing higher-dimensional functions [2]. These foundational studies provided a solid mathematical basis for the development of KAN.\nBased on the Kolmogorov-Arnold theorem, KAN employs learnable single-variable functions to replace the fixed weights in traditional neural networks. This innovative design allows KAN to dynamically adapt to different data patterns, resulting in higher flexibility and accuracy when handling complex data relationships [14]. Specifically, each weight parameter in KAN is represented as a single-variable function, typically parameterized as a spline function. This approach not only enhances the network's flexibility but also improves its ability to model nonlinear relationships [8].\nThe design of KAN significantly enhances model interpretability. By using learnable single-variable functions, researchers and users can more intuitively"}, {"title": "\u039a\u0391N Architecture", "content": "The architecture of Kolmogorov-Arnold Networks (KAN) is uniquely characterized by its core concept of replacing traditional fixed linear weights with learnable single-variable functions, thereby achieving greater flexibility and adaptability. The following sections detail the fundamental structure and operational principles of \u039a\u0391\u039d.\nNetwork Hierarchical Structure The KAN architecture is composed of multiple layers, each containing several nodes and edges. Each node receives input signals from the previous layer and applies nonlinear transformations to these signals through learnable single-variable functions on the edges. Typically, these single-variable functions are parameterized as spline functions to ensure smoothness and stability during data processing [14].\nIn KAN, each weight parameter is represented as a single-variable function, the specific form of which can be adjusted during the training process. This design allows each layer's output to depend not only on the input signals but also on optimized parameters that adapt to different data patterns, thereby enhancing the model's expressive power [8].\nActivation Functions and Spline Functions A key feature of KAN is the use of learnable activation functions. These activation functions are usually constructed from spline functions. Spline functions are piecewise polynomial functions defined by a set of control points that determine their shape. These control points can be independently adjusted, influencing the local shape of the function without altering its global characteristics. This property ensures that spline functions maintain the smoothness of the activation functions while flexibly adapting to complex patterns in the data [5].\nBy utilizing spline functions, KAN can apply complex nonlinear transformations on each edge, leading to superior performance in handling nonlinear relationships. Specifically, the diversity and flexibility of spline functions enable the model to remain efficient and stable when processing various types of data [14].\nTraining and Optimization The training process of KAN is similar to other neural network models, typically employing the backpropagation algorithm. First, the network parameters are initialized, followed by a forward pass to compute the output. Next, the loss function is calculated to measure the difference between the predicted and actual results, and the gradients of the loss function with respect to each parameter are computed using the backpropagation algorithm. Finally, the parameters are updated using gradient descent or other optimization algorithms [8].\nSince KAN's activation functions are learnable, the training process must optimize both the activation functions on the edges and the weights between nodes. This process requires fine-tuning to ensure stability and convergence."}, {"title": "Comparison with Traditional Neural Networks", "content": "Kolmogorov-Arnold Networks (KAN) exhibit several significant advantages over traditional neural networks such as Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The following sections provide a detailed comparison in terms of architecture design, parameter optimization, computational efficiency, model performance, and interpretability.\nArchitecture Design The unique design of KAN is based on the Kolmogorov-Arnold theorem, introducing learnable single-variable functions in place of fixed linear weights. This design allows KAN to apply dynamically adjusted activation functions on the network edges rather than using fixed activation functions at the nodes as in traditional neural networks. Specifically, each weight parameter in KAN is represented by a single-variable function, typically parameterized as a spline function. This architecture enhances the network's flexibility, enabling it to better adapt to different data patterns [14][8].\nParameter Optimization In traditional neural networks, parameter optimization mainly focuses on fixed weights and biases. In contrast, KAN extends the scope of optimization parameters by introducing learnable activation functions. This means that during the optimization process, both the network's weights and biases as well as the parameters of the activation functions need to be adjusted. While this process increases the complexity of training, it also provides greater flexibility, allowing KAN to more effectively capture complex data relationships [5].\nComputational Efficiency Traditional neural networks, especially large CNNs, typically require a substantial number of parameters and computational resources, which is a significant limitation in resource-constrained environments. KAN significantly reduces the number of parameters by using learnable single-variable functions. For example, studies have shown that Convolutional KAN (Convolutional Kolmogorov-Arnold Networks) achieves high accuracy with significantly fewer parameters compared to traditional CNNs, thereby reducing computational costs [5]. This reduction not only improves computational efficiency but also enhances the model's scalability, making it more suitable for resource-limited environments.\nModel Performance KAN has demonstrated superior performance over traditional neural networks across various tasks. Its ability to dynamically adjust activation functions makes it particularly effective in handling nonlinear and complex data relationships. For instance, in applications such as hyperspectral image classification and time series analysis, KAN has achieved higher accuracy and efficiency due to its more flexible model architecture [14][20]. These studies indicate that KAN possesses broad applicability and outstanding performance in practical applications.\nInterpretability The design of KAN not only improves model performance but also enhances its interpretability. Traditional neural networks with fixed activation functions limit the understanding of the model's decision-making process. In contrast, KAN's use of learnable activation functions allows researchers and users to more intuitively understand and interpret the internal mechanisms and decision processes of the model. This is particularly important for applications requiring high interpretability, such as medical diagnosis and financial forecasting [8]. Enhanced interpretability helps build trust and transparency in the model's decisions, especially in critical application areas."}, {"title": "Applications of KAN", "content": null}, {"title": "Symbolic Regression", "content": "Symbolic regression is a method that seeks to find mathematical formulas or expressions that best represent data. Unlike traditional regression analysis, symbolic regression aims not just to fit data points but to discover the underlying mathematical relationships generating the data. This method is particularly important in scientific research because it can uncover the physical laws and mathematical principles behind the data.\nSymbolic regression, while theoretically powerful, faces several practical challenges. Firstly, symbolic regression requires searching for the best mathematical expression within a vast formula space, a process that is computationally intensive. Traditional symbolic regression algorithms, such as those based on genetic programming, can generate highly interpretable models but often struggle with computational resource constraints, making it difficult to handle large-scale and high-dimensional datasets [7].\nSecondly, symbolic regression performs poorly when dealing with noisy data. Real-world data typically contains various types of noise and outliers, making it challenging to accurately extract underlying mathematical relationships. Traditional methods are prone to overfitting or failing to find robust formulas under such conditions [15].\nAdvantages of KAN in Symbolic Regression\nKolmogorov-Arnold Networks (KAN) exhibit significant advantages in the application of symbolic regression, particularly in addressing the aforementioned challenges. KAN's unique architecture and flexible activation functions allow it to effectively perform symbolic regression tasks, as detailed below:\n1. High Accuracy and Interpretability: By using spline functions as activation functions, KAN can more accurately fit complex nonlinear relationships. Unlike traditional neural networks, KAN can dynamically adjust its activation functions to better capture complex patterns in the data. For example, Liu et al. [14] demonstrated KAN's exceptional performance in symbolic regression, achieving high accuracy in formula discovery by effectively modeling complex data.\n2. Innovations and Improvements:\n\u2022 Path Signature Integration: Path signatures are tools that capture the geometric features of time series data. By integrating path signatures, KAN can better understand and predict complex time series patterns. Path signatures effectively capture the geometric shape and structural information of data, providing a more comprehensive perspective for modeling time series data [10].\n\u2022 Dynamic Adaptation: KAN can dynamically adapt to different data patterns by learning adjustable parameters of path signatures, improving performance in multi-task time series prediction. The introduction of path signatures allows KAN to dynamically adjust according to different time series characteristics, enhancing the model's adaptability and robustness [18].\nThe emergence of KAN has had a profound impact on the field of symbolic regression. Firstly, KAN's high accuracy and interpretability enable researchers to better understand the mathematical relationships underlying the data, thereby promoting scientific discovery. For example, Vaca-Rubio et al. [18] showed KAN's superior performance in symbolic regression, demonstrating through experiments that KAN is more accurate and efficient than traditional methods when dealing with complex datasets. Secondly, KAN's flexibility and robustness make it particularly effective in handling noisy data. By dynamically adjusting activation functions, KAN can accurately identify the symbolic formulas generating data even in high-noise environments, providing greater reliability and stability for practical applications [14]."}, {"title": "Time Series Prediction", "content": "Kolmogorov-Arnold Networks (KAN) demonstrate significant potential in the domain of time series prediction. This involves using past data to predict future values and finds applications in various fields such as financial market analysis, weather forecasting, and medical monitoring. KAN, with its unique architecture and flexible activation functions, can capture complex patterns and both short-term and long-term dependencies in time series data.\nT-KAN and MT-K\u0391\u039d\nT-KAN (Time-Kolmogorov-Arnold Network)\nThe T-KAN model is specifically designed to handle time series data. It leverages the flexibility of KAN by using learnable single-variable activation functions to capture nonlinear relationships and complex patterns in time series. Compared to the basic KAN model, T-KAN achieves higher performance through the following improvements:\n1. Dynamic Activation Functions: Traditional neural networks typically use fixed activation functions like ReLU or Sigmoid, whereas T-KAN employs learnable single-variable activation functions. This allows the model to dynamically adjust its activation patterns, better accommodating the changes in time series data [14].\n2. Nonlinear Relationship Capture: T-KAN effectively captures nonlinear relationships in time series data, excelling in tasks such as financial market prediction and energy consumption forecasting. Xu et al. [20] demonstrated T-KAN's superior performance in stock market prediction, accurately modeling historical price and trading volume data to achieve precise short-term price predictions.\nT-KAN's innovation lies in its ability to adapt to nonlinear changes in time series data through learnable activation functions, making it exceptionally adept at capturing complex temporal dependencies. Additionally, T-KAN has optimized the learning mechanism of activation functions compared to the basic KAN model, enabling more precise reflection of dynamic changes in time series data.\nMT-KAN (Multi-Task Kolmogorov-Arnold Network)\nThe MT-KAN model extends T-KAN's capabilities to multi-task learning, allowing it to handle multiple related tasks simultaneously. Compared to the basic KAN model and T-K\u0391\u039d, \u039c\u03a4-\u039a\u0391N achieves improvements through the following methods:\n1. Shared Network Structure: MT-KAN enhances overall prediction performance by sharing network layers and parameters across different tasks, leveraging the interrelationships between tasks. For instance, in multi-task time series prediction tasks like electricity load forecasting and air quality monitoring, MT-KAN models the interactions within multivariate time series, achieving higher prediction accuracy.\n2. Parameter Sharing and Task Interaction: By sharing some model parameters, MT-KAN can share useful feature representations across multiple tasks, improving the prediction performance of each task. This approach not only enhances model accuracy but also reduces the amount of training data required.\nSigKAN (Signature-Weighted Kolmogorov-Arnold Network)\nSigKAN further enhances KAN's performance in time series prediction by incorporating path signatures. Path signatures capture crucial geometric features of time series paths, and by combining this information with KAN's output, SigKAN can more comprehensively represent time series data. Specifically:\n1. Path Signature Integration: Path signatures are tools that capture the geometric features of time series data. By integrating path signatures, SigKAN can better understand and predict complex time series patterns.\n2. Dynamic Adaptation: SigKAN dynamically adapts to different data patterns by learning adjustable parameters of path signatures, improving performance in multi-task time series prediction. Inzirillo and Genet (2024) proposed that SigKAN outperforms traditional methods in multi-task time series prediction.\nIn various experiments, T-\u039a\u0391\u039d, \u039c\u03a4-\u039a\u0391N, and SigKAN have shown outstanding performance in time series prediction. For example, experiments on financial time series data indicate that T-KAN and MT-KAN excel in predicting stock implied volatility, significantly outperforming traditional MLP and RNN models. These results demonstrate that even in complex and highly volatile financial data environments, KAN models can achieve high-accuracy predictions. SigKAN, by combining the advantages of path signatures and KAN, further improves performance in multi-task time series prediction, excelling in tasks such as market trading volume prediction and absolute return prediction.\nThe research by Vaca-Rubio et al. (2024) also highlights KAN's superior performance in satellite traffic prediction. Through experiments, KAN models have outperformed traditional MLP models in handling complex satellite traffic data. KAN not only surpassed MLP in prediction accuracy but also exhibited excellent parameter efficiency, achieving higher prediction performance with fewer trainable parameters.\nKAN demonstrates significant advantages in time series prediction. Its unique architecture and flexible activation functions effectively capture complex temporal dependencies and nonlinear patterns. These characteristics enable KAN to showcase broad application potential and exceptional performance in fields such as financial forecasting and energy management."}, {"title": "Graph-Structured Data Processing", "content": "Kolmogorov-Arnold Networks (KAN) exhibit significant advantages in the processing of graph-structured data, particularly in the fields of graph learning and Graph Neural Networks (GNNs). Graph-structured data are prevalent in various domains, including social networks, molecular structures, and biological networks, requiring robust models to capture the complex relationships and structural information among nodes. The challenges in processing graph-structured data include:\nComplex Inter-Node Relationships: Graph-structured data feature complex relationships and dependencies between nodes, making it difficult for traditional neural networks to directly process these intricate structures.\nHigh-Dimensional Data and Large-Scale Graphs: Graph-structured data often have high-dimensional features and large-scale structures, posing significant demands on computational resources and model scalability.\nHeterogeneity and Diversity: Different graphs may exhibit significant structural and feature variations, necessitating models with strong generalization capabilities to handle diverse graph data.\nGraph Kolmogorov-Arnold Networks (GKAN) extend the principles of KAN to graph-structured data. GKAN introduces learnable single-variable functions on graph edges, replacing the fixed convolutional architecture in traditional GCNs, thereby innovatively transforming the way information is processed in graph structures.\nNode Feature Aggregation: GKAN adopts two primary architectures, differing in the sequence of aggregation and activation function application. The first architecture involves aggregating node features (e.g., summation or averaging) before applying KAN layers. This approach emphasizes applying learnable activation functions after aggregation, enabling the model to better capture complex relationships within local graph structures [12].\nEmbedding Layer Processing: The second architecture involves applying KAN layers between node embeddings at each layer, followed by aggregation. By introducing learnable single-variable activation functions between each layer, GKAN can flexibly adapt to feature variations across different graph structures. This method allows the model to dynamically adjust its representational capacity between different layers, enhancing its ability to process complex graph data [12].\nDynamic Adaptation: GKAN can dynamically adapt to different graph structures and features, enhancing the model's adaptability and robustness. This capability makes GKAN particularly effective in handling heterogeneous and diverse graph data. Compared to the basic KAN model, GKAN demonstrates stronger scalability and adaptability when processing large-scale and high-dimensional graph-structured data [14][18].\nExperiments and Results: In various experiments, GKAN has shown superior performance in processing graph-structured data. For instance, in experiments on the Cora dataset, GKAN achieved higher accuracy in node classification tasks compared to traditional GCN models. Specifically, when using 100 features, traditional GCNs achieved an accuracy of 53.5\nFurthermore, GKAN's experiments on other graph datasets also indicate its superior performance in processing large-scale and high-dimensional graph-structured data. In tasks such as social network analysis and molecular graph analysis, GKAN has exhibited exceptional performance and wide-ranging application potential."}, {"title": "Hyperspectral Image Classification", "content": "Kolmogorov-Arnold Networks (KAN) exhibit significant advantages in hyperspectral image classification, particularly in handling high-dimensional data and complex spatial-spectral correlations. Hyperspectral images (HSI) contain rich spectral information that can distinguish different materials, making them widely applicable in agriculture, environmental monitoring, and military fields. Hyperspectral image classification faces several challenges, including:\nHigh-Dimensional Data: Hyperspectral images typically consist of hundreds of spectral bands, requiring effective dimensionality reduction and feature extraction methods [16].\nComplex Spatial-Spectral Correlations: Each pixel in hyperspectral images contains both spectral and spatial information, increasing the complexity of classification [19].\nData Sparsity: In many applications, labeled hyperspectral image data are very limited, making effective classification with small samples a crucial issue [16].\nAdvantages of KAN in Hyperspectral Image Classification Kolmogorov-Arnold Networks (KAN) can effectively address these challenges through their unique architecture and flexible activation functions. Specifically:\nWavelet-based KAN (Wav-KAN): Wav-KAN introduces wavelet functions as learnable activation functions within the KAN framework to achieve nonlinear mapping of input spectral features. This method can effectively capture multi-scale spatial-spectral patterns, significantly enhancing classification performance. For instance, Seydi et al. [16] demonstrated Wav-KAN's exceptional performance on benchmark datasets such as Salinas, Pavia, and Indian Pines, showing notable improvements in classification accuracy over traditional multilayer perceptron (MLP) and Spline-KAN models.\nSpectralKAN: SpectralKAN uses B-spline functions as activation functions to represent multivariable continuous functions, extracting hyperspectral image features for classification. SpectralKAN introduces a spatial-spectral KAN encoder, where the spatial KAN encoder extracts spatial features and compresses spatial dimensions, while the spectral KAN encoder extracts spectral features and classifies them into changing and non-changing categories. This method not only improves classification accuracy but also reduces model parameters, floating-point operations (FLOPs), GPU memory, and training and testing time, improving computational efficiency [19].\nDetailed Analysis:\n1. High-Dimensional Data Processing: Wav-KAN and SpectralKAN use learnable activation functions to effectively address the high-dimensional data problem in hyperspectral images. Wav-KAN utilizes wavelet functions for multi-scale decomposition of spectral data, capturing features at different scales, allowing the classification model to comprehensively understand the spatial and spectral information in the data [16].\n2. Spatial-Spectral Correlations: SpectralKAN introduces a spatial-spectral KAN encoder to separately process spatial and spectral features, effectively capturing complex spatial-spectral correlations in hyperspectral images. The spatial KAN encoder first extracts spatial features and performs dimensionality reduction, followed by the spectral KAN encoder extracting spectral features and classifying them. This two-stage processing method significantly improves the model's ability to capture complex spatial-spectral patterns [19]."}, {"title": "Current Research Progress", "content": null}, {"title": "Recent Research Overview", "content": "Research on Kolmogorov-Arnold Networks (KAN) has made significant progress across multiple domains. Here are some recent important studies:\nFundamental Research and Extensions: Liu et al. [14] proposed the basic KAN framework, using learnable single-variable activation functions to replace fixed activation functions in traditional neural networks. Their study demonstrated that KAN outperforms traditional multilayer perceptrons (MLP) in terms of accuracy and interpretability, especially in data fitting and partial differential equation (PDE) solving [14][11].\nSymbolic Regression: Liu et al. [14] explored the application of KAN in symbolic regression tasks, showcasing its advantages in discovering the mathematical relationships and physical laws underlying data. Experimental results indicated that KAN accurately identifies the symbolic formulas generating data even in high-noise environments [14][20].\nTime Series Prediction: Xu et al. [20] investigated the application of KAN in time series prediction, proposing two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift in time series and interpret nonlinear relationships between predictions and previous time steps through symbolic regression. MT-KAN improves prediction performance by effectively utilizing complex relationships in multivariate time series [20][18].\nGraph-Structured Data Processing: Liu et al. [14] introduced GKAN (Graph Kolmogorov-Arnold Networks), extending the principles of KAN to graph-structured data. GKAN replaces the fixed convolutional architecture in traditional GCNs with learnable single-variable functions on graph edges, achieving higher accuracy in node classification tasks [14][4].\nWavelet KAN (Wav-KAN): Bozorgasl et al. [4] proposed Wav-KAN, integrating wavelet functions into the KAN framework to enhance model interpretability and performance. Wav-KAN utilizes wavelet transforms to capture high-frequency and low-frequency components of input data, improving classification accuracy, training speed, and robustness, especially in high-dimensional and complex pattern scenarios [4][16]."}, {"title": "Improvements and Optimizations", "content": "Recent research has continuously introduced improvements and optimizations to KAN, including:\nRational KAN (rKAN): Afzalaghaei et al. [1] proposed rKAN, which incorporates rational function bases into KAN. Using Pad\u00e9 approximants and rationalized Jacobi functions, rKAN significantly improves performance in regression and classification tasks. This approach enhances activation function representation and parameter update mechanisms, increasing model accuracy and computational efficiency [1][4].\nConvolutional KAN (Conv-KAN): Conv-KAN applies KAN principles to convolutional neural networks by introducing learnable single-variable functions into convolutional kernels. This method has shown excellent performance in image recognition and processing tasks, particularly in handling complex spatial patterns and high-dimensional image data [4][20].\nKAN-based Adversarial Training (KAT): Huang et al. [9] introduced KAT, leveraging KAN's mathematical properties to improve the robustness of deep neural networks through adversarial training. Experimental results demonstrated that KAT significantly enhances model robustness against adversarial examples [9][18].\nKAN in Quantum Architecture Search (KANQAS): Recent studies have explored the application of KAN in quantum architecture search, showing its potential in quantum computing and optimization problems. KANQAS optimizes quantum circuit design, significantly improving the efficiency and performance of quantum algorithms [14][18]. These recent research advancements and optimization methods highlight KAN's broad potential and exceptional performance in various applications. Continuous optimization and expansion are expected to further enhance KAN's role in deep learning and data analysis."}, {"title": "Future Research Directions", "content": null}, {"title": "Potential Improvements", "content": "Structural Optimization: Further optimization of the KAN structure is an important direction for future research. Existing studies have shown that learnable single-variable activation functions significantly enhance model flexibility and performance. However, there is still room for optimization. For example, Afzalaghaei and Kiamari [1] proposed improving the representation capability of activation functions by introducing rational function bases (rKAN), thus enhancing accuracy and computational efficiency in regression and classification tasks [1]. Future research can explore the introduction of other function bases and how to adaptively select the best activation functions for different tasks.\nTraining Method Improvements: Improving training methods is also an important research direction. Existing studies indicate that adversarial training (KAT) can significantly enhance model robustness [9]. Future research can explore more training strategies, such as hybrid adversarial training, transfer learning, and self-supervised learning, to further improve model performance when handling noisy data and small sample sizes.\nParallel Computing and Hardware Acceleration: Due to the high computational complexity of KAN, future research can focus on parallel computing and hardware acceleration technologies. For example, utilizing graphics processing units (GPU) and tensor processing units (TPU) for model acceleration, or designing dedicated hardware architectures to enhance KAN's computational efficiency."}, {"title": "New Application Scenarios", "content": "Medical Diagnosis: The successful application of KAN in hyperspectral image classification indicates its great potential in medical diagnosis. For example, KAN can be applied to medical image analysis, improving disease detection and diagnosis accuracy by capturing complex spatial-spectral patterns [16].\nFinancial Forecasting: KAN's ability to capture nonlinear relationships makes it promising in financial market prediction. Future research can explore KAN's application in stock price prediction, risk management, and quantitative trading, modeling complex market dynamics to improve prediction accuracy and decision support [20].\nEnvironmental Monitoring: Hyperspectral images have important applications in environmental monitoring, such as land use classification, water quality monitoring, and weather prediction. KAN's advantages in handling high-dimensional and complex data make it an ideal tool for environmental monitoring [18]."}, {"title": "Potential for Multidisciplinary Integration", "content": "Integration with Quantum Computing: Quantum computing is becoming a new research hotspot. The integration of KAN with quantum computing (KANQAS) is expected to significantly enhance the efficiency and performance of quantum algorithms. Future research can explore how to utilize quantum computing's powerful computational capabilities to further optimize KAN's training and inference processes [14].\nIntegration with Bioinformatics: Bioinformatics data are complex and high-dimensional, and KAN's flexibility and powerful modeling capabilities make it highly promising in genomics, protein structure prediction, and biological network analysis. Future research can explore KAN's application in bioinformatics to advance research in life sciences [14]."}, {"title": "Conclusion", "content": "Through this comprehensive survey of Kolmogorov-Arnold Networks (KAN), we have gained a thorough understanding of its theoretical foundation, architectural design, application scenarios, and current research progress. KAN, with its unique architecture and flexible activation functions, excels in handling complex data patterns and nonlinear relationships, demonstrating wide-ranging application potential.\nFirstly, the theoretical foundation of KAN, based on the Kolmogorov-Arnold theorem, provides it with exceptional performance in data fitting and partial differential equation (PDE) solving. This solid mathematical basis allows KAN to replace fixed weights in traditional neural networks with learnable single-variable functions, thereby enhancing model flexibility and interpretability.Secondly, KAN's architecture, which introduces learnable single-variable functions, improves model adaptability and interpretability, making it highly suitable for applications requiring high interpretability, such as medical diagnosis and financial forecasting. The dynamic adjustment of activation functions enables KAN to excel in capturing complex data relationships, demonstrating significant advantages in symbolic regression, time series prediction, graph-structured data processing, and hyperspectral image classification.\nCurrent research advancements highlight continuous optimization of KAN'S architecture and training methods, enhancing its performance across various tasks. Future research directions include further optimizing KAN's structure and training methods, exploring its potential in new application scenarios such as medical diagnosis, financial forecasting, and environmental monitoring, and integrating KAN with other disciplines like quantum computing, bioinformatics, and physics. In summary, as an emerging neural network architecture, KAN demonstrates immense research and application potential with its theoretical foundation, flexible architecture, and broad application prospects. With ongoing optimization and expansion, KAN is expected to play an increasingly important role in scientific research and practical applications in the future."}]}