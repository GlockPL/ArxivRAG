{"title": "Foundation Models for CPS-IoT: Opportunities and Challenges", "authors": ["Ozan Baris", "Yizhuo Chen", "Gaofeng Dong", "Liying Han", "Tomoyoshi Kimura", "Pengrui Quan", "Ruijie Wang", "Tianchen Wang", "Tarek Abdelzaher", "Mario Berg\u00e9s", "Paul Pu Liang", "Mani Srivastava"], "abstract": "Methods from machine learning (ML) have transformed the implementation of Perception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS) and the Internet of Things (IoT), replacing mechanistic and basic statistical models with those derived from data. However, the first generation of ML approaches, which depend on supervised learning with annotated data to create task-specific models, faces significant limitations in scaling to the diverse sensor modalities, deployment configurations, application tasks, and operating dynamics characterizing real-world CPS-IoT systems. The success of task-agnostic foundation models (FMs), including multimodal large language models (LLMs), in addressing similar challenges across natural language, computer vision, and human speech has generated considerable enthusiasm for and exploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics pipelines, promising to reduce the need for costly task-specific engineering.\nNonetheless, a significant gap persists between the current capabilities of FMs and LLMs in the CPS-IoT domain and the requirements they must meet to be viable for CPS-IoT applications. In this paper, we analyze and characterize this gap through a thorough examination of the state of the art and our research, which extends beyond it in various dimensions. Based on the results of our analysis and research, we identify essential desiderata that CPS-IoT domain-specific FMs and LLMs must satisfy to bridge this gap. We also propose actions by CPS-IoT researchers to collaborate in developing key community resources necessary for establishing FMs and LLMs as foundational tools for the next generation of CPS-IoT systems.", "sections": [{"title": "1 Introduction", "content": "Cyber-Physical Systems and the Internet-of-Things (referred to as CPS-IoT Systems henceforth) operate through Perception-Cognition-Communication-Action (PCCA) loops, leveraging multimodal and multiview sensor data to comprehend physical states, predict future and remote conditions, exchange information with other systems, and execute timely interventions. Recently, CPS-IoT engineering has shifted from mechanistic and statistical models rooted in human expertise to high-dimensional, data-driven models.\nAdvances in machine learning (ML) and artificial intelligence (AI) research in language, vision, and speech have led to specialized neural models and training algorithms tailored for the CPS-IoT domain. These models demonstrate notable performance gains across common signal modalities like IMU, radar, lidar, and wireless, addressing tasks such as human activity recognition, acoustic event detection, spatio-temporal tracking, and signal generation. They leverage unique characteristics like spectral domain information while tackling challenges such as sensor placement sensitivity and varying sampling rates.\nHowever, the first generation of ML-based CPS-IoT systems faces several critical limitations. First, the trained models are task-specific, designed for particular tasks and requiring dedicated retraining for each (new) task the system must perform. Second, this task-specific supervised training strategy requires copious amounts of labeled sensor data, which is difficult to obtain because many sensing modalities are not human-interpretable, making retrospective labeling challenging. Moreover, contemporaneous labeling is intrusive, privacy-invading, and low-quality, particularly as many CPS-IoT systems operate in real-time out in the wild; and due to the limited scope of deployments/studies, most labeling strategies typically lead to data imbalance issues. Lastly, these models are often dependent on specific sensor sampling rates, placements, and configurations that vary between deployments.\nEmergence of Foundation Models: In broader AI/ML research and industry, there is a clear trend toward homogenizing model functionalities. Supervised discriminative models, limited by the high cost and scarcity of labeled data, are increasingly being replaced by generative models trained through self-supervised learning on generic pretext tasks. This shift has led to the rise of Foundation Models (FMs), introduced in [12], which are defined as models trained on broad datasets using large-scale self-supervision and adaptable to a wide range of downstream tasks. FMs, such as Large Language Models (LLMs), have revolutionized Al system design by enabling task adaptation via FM-based pipelines rather than bespoke task-specific ones. The success of FMs is driven by three key factors: self-supervised pretraining, scale (of both data and computation), and innovative architectures like transformers [112], structured state-space models [23, 41], and hybrid approaches [66]. While early FMs like BERT emphasized unifying representations requiring task-specific stages, later models with natural language interfaces, such as LLMs and Large Vision Language models, can specify and perform semantically unrelated tasks. Methods for adapting FMs to downstream tasks-ranging from full fine-tuning and task-specific adapters to input prompt tuning, in-context learning, and zero-shot prompting-have further broadened their applicability.\nFoundation Models for CPS-IoT Domain: Mirroring the rise of FMs in NLP, vision, and speech, the CPS-IoT domain has seen parallel advancements in developing FMs tailored to its unique challenges. Indeed, the factors driving the emergence of FMs in broader AI/ML are even more critical in the CPS-IoT domain: sensor data types are more diverse, labeling is harder, unlabeled sensor data is far more abundant both absolutely and relatively, tasks are more varied with stricter performance and safety requirements, systems scale more extremely, and platforms are more resource-constrained. Many have recognized the potential of FMs for CPS-IoT, spurring significant efforts to develop task-independent sensor time series representations (e.g., LIMU-BERT [124], ImageBind [38], FOCAL [68]), leverage pretrained LLMs with their world knowledge, sequence processing, and spatiotemporal reasoning for general sensor analysis tasks (e.g., Penetrative AI [123], IoT-LM [79], LLMSense [86]), and address specific tasks (e.g., Chronos [4], MOMENT [39] for forecasting) and applications (e.g., RT-2 [13] for robot control and LSM [82] for wearable health tracking).\nAbout the Paper: While there is considerable activity on CPS-IoT-related FMs and the use of LLMs for CPS-IoT, much of it directly extends models, architectures, training methods, and applications from general AI/ML domains like language and vision. At the same time, CPS-IoT systems have distinctive characteristics and needs - arising from their embodied and embedded nature, and their tight coupling with the physical world \u2013 which FMs must address. Unlike language, sensor data are discretized samples of continuous spatiotemporal physical signals rather than ordered symbolic sequences. Factors such as sampling rate, policy, quantization strategy, gain factor, location, and orientation affect both the quality and content of the physical world information in the sensor data stream. Likewise, CPS-IoT tasks must consider the spatiotemporal properties of sensor measurements and output actions, as well as latency in producing outputs.\nMotivated by these observations and guided by an in-depth analysis of state-of-the-art work and insights from our preliminary research, this paper explores the desiderata for CPS-IOT FMs and lays out a research agenda. This agenda identifies key technical challenges to address and calls for collaborative efforts to create the necessary artifacts to realize it. The ideas in this paper draw on the collective expertise of the multi-institutional authoring team, spanning the entire CPS-IoT system stack and multiple applications, including prior contributions to FMs and LLMs for CPS-IoT.\nThe paper is divided into three main sections. Section 2 analyzes the state of the art in CPS-IoT-focused FMs and the use of LLMs for this purpose. Section 3 presents preliminary findings from our research, addressing challenges such as resource-constrained platforms, sensor viewpoints, and tasks beyond prediction. Finally, Section 4 synthesizes insights from the earlier sections into a vision for future research and community-driven artifacts."}, {"title": "2 State of the Art in CPS-IoT FMs", "content": "Existing research in FMs (including LLMs) relevant to CPS-IoT has primarily focused on perception, with limited attention to cognition, communication, and action of the PCCA loops. To assess the state-of-the-art (SOTA), we first examine perception-related FMs along two axes: sensor modalities handled (single, multiple, and flexible) and tasks performed (fixed, configurable, selectable, and run-time specifiable). We then explore FMs beyond the perception tasks for CPS-IoT."}, {"title": "2.1 CPS-IoT FMs for Perception: A Sensor Modality Perspective", "content": "Much of the research on CPS-IoT FMs for perception focuses on projecting raw sensor signals into compact representations, or embeddings, that encode underlying physical phenomena and are generalizable to various downstream tasks. Self-supervised learning (SSL) is commonly used to learn such representations without requiring task-specific annotations. The trained FM is then adapted to compute desired outputs for a specific downstream task, such as event classification or spatiotemporal localization. This stage is either learned from limited annotated data or designed from first principles and human knowledge. However, learning robust representations for CPS-IoT systems is challenging due to the spatiotemporal characteristics of sensor signals [53, 70] and the significant heterogeneity of sensor modalities across application domains [65]. We categorize prior works on SSL for CPS-IoT FMs into three groups, focusing on single modality, a fixed set of modalities, or a flexible set of modalities.\n2.1.1 Unimodal Models. Most CPS-IoT FMs focus on mapping a single sensor modality (e.g., image, IMU, sound) into embeddings. SSL frameworks for these FMs either contrast temporally close samples to capture temporal consistency [109, 133] or reconstruct masked portions of time-series [27, 83, 124]. For human activity recognition, the FM in [131] uses a multi-task self-supervision approach employing the arrow of time, permutation, and time warping to enhance generalization. Similarly, RelCon [125] applies a learnable distance measure and softened contrastive loss to capture motif similarity and domain-specific semantics. Alternatively, many SSL approaches explore frequency representations of raw signals to capture spatiotemporal features. TimesNet [119] transforms 1D time series into 2D tensors to capture multi-periodicity. TFC [137] enforces time-frequency consistency through contrastive learning. AudioMAE [48] builds on MAE [45] architectures to reconstruct masked spectrograms of acoustic signals. PhyMask [54] introduces physics-informed masking for MAE pretraining. Beyond learning techniques, alternative sensing embedding encoder architectures of FMs have been explored, such as frequency transformers [55], mixer models [18, 30], and recently emergent state-space models [11, 99, 134].\n2.1.2 Multimodal Models. FMs that integrate heterogeneous signals from a fixed set of sensor modalities into joint embeddings have emerged for multimodal CPS-IoT applications [57, 79, 82, 107]. Cosmo [85] enhances modality representations with augmented fusion for contrastive learning. Cocoa [26] improves cross-modal coherence with discriminative objectives on temporally distant samples. FOCAL [68] factorizes the representation into orthogonal shared and private subspaces to capture complementary and shared modality information. Parallel works have explored aligning modalities to handle missing data. Imagebind [38] binds images, text, audio, depth, thermal, and IMU data into a joint embedding space using only image-paired data. Babel [22] proposes expandable networks to incrementally integrate modalities. Similarly, MMBind [87] constructs pseudo-paired data from incomplete multimodal sources for multimodal pretraining.\n2.1.3 Flexible Modality Models. Existing works have explored FMs that adapt to flexible modalities, as available sensor modalities in CPS-IoT applications often vary due to resource constraints, platform heterogeneity, and operating environment. One approach involves models that infer modality availability at test time. For example, missing modality can be indicated with special input value [89] or with a mask vector [92]. Transformers can use prompt tokens (similar to position embeddings) to mark sensor modality and adapt to missing data [60, 64, 111]. Models can also handle missing modalities at test time by performing probabilistic modality dropout or cross-modal attention masking during training [72]. Another approach trains reconstruction models with modality dropout to generate missing data from the available modalities [118]. Alternatively, models like Perceiver [51] adopt new architecture to handle arbitrary configurations of modalities. Lastly, FMs for univariate time-series forecasting [4, 52, 69] treat all sensor data as timestamped, scaled real-numbered values and can adapt to arbitrary modalities by handling input domain shift with scaling training data, though often unsuccessfully [81]."}, {"title": "2.2 CPS-IoT FMs for Perception: A Task Perspective", "content": "The trend towards increasing homogenization of model functionality in broader AI as reflected in LLMs is also influencing CPS-IOT FMs resulting in an evolution from unifying sensor modality representations to unifying CPS-IoT tasks. Below we analyze the SOTA in CPS-IoT FM from the perspective of breadth of tasks they handle, organizing them into four groups: fixed task, design-time configurable task, run-time selectable task, and run-time specifiable task.\n2.2.1 Fixed Task Models. These models perform a single specific task but can do so on a broad spectrum of sensor data types either in a zero-shot manner or with some fine-tuning for performance. The most prominent example of such FMs is time-series forecasting models, often referred to as Time Series FMs (TSFMs) despite their single-task focus, that have recently emerged in academic literature and commercial offerings. Examples include Amazon's Chronos [4], a family of open-source probabilistic forecasting models based on the T5 architecture [94], Moirai [117], which employs an encoder-only transformer for multivariate time series, LagLlama [95], which leverages the decoder-only Llama architecture [110] for probabilistic forecasting, and TimesFM [24], which uses a decoder-based transformer with residual blocks for multivariate time-series forecasting.\n2.2.2 Configurable Task Models. FMs focused on representing raw sensor data into embedding vectors fall under this category, such as those discussed in the preceding subsection (e.g., LIMU-Bert [124], FOCAL [68], TimesNet [119] etc.). Pretrained via SSL on a broad spectrum of data, they can be configured for specific tasks by pairing with a downstream stage. Task-specific labeled sensor data is used to train the downstream stage, which may also fine-tune the embedding FM. For many tasks, the downstream stage can be as simple as a linear probe, though more sophisticated models are often used. An excellent example is MOMENT [39], a family of open-source FMs for time-series analysis. It serves as a versatile representation learning FM that, when paired with a fine-tuned linear probe, can handle tasks like imputation, anomaly detection, and long-horizon forecasting, or, with a fine-tuned classification head, can perform classification tasks.\n2.2.3 Selectable Task Models. Selectable CPS-IOT FMs provide greater flexibility by allowing users to choose from a predefined set of tasks at run-time without requiring any additional fine-tuning. These models use a metadata channel to specify the desired task from the fixed set. The MOMENT FM mentioned above can also function as a selectable task model, performing anomaly detection, imputation, short-horizon forecasting, and classification in a zero-shot manner without parameter updates. Similarly, TimeGPT [36] utilizes an encoder-decoder structure to perform zero-shot forecasting and anomaly detection without additional training. UniTS [34], a unified multi-task time-series model, processes input data as tokens and employs a shared architecture for tasks like forecasting, classification, anomaly detection, and imputation. At deployment, prompt tokens-learnable embeddings fine-tuned for specific datasets or tasks-are appended to input tokens to provide task-specific context, enabling adaptation to new tasks or datasets without modifying the frozen pretrained model.\n2.2.4 Run-time Specifiable. Run-time specifiable tasks enable users to define new tasks at run-time via input specification (e.g., text). In NLP, text-to-text models use task-specific prefixes [94] or text description in modern LLMs [14] for task definitions. In the CPS-IoT domain, recent work leverages LLMs' internal pretrained world knowledge to ingest task descriptions and exhibit emergent reasoning, planning [43], and optimization [126] abilities. The key difference lies in whether input data (e.g., sensor measurements) and outputs (e.g., state estimates, control actions) are textually embedded, or mapped via adapters to or from the embedding space. Examples of the first approach include PromptCast [127], LLMTime [40], IoT-LLM [3], Penetrative AI [123] and LLMSense [86]. These works demonstrate that LLMs, with their world knowledge and reasoning abilities [71, 77], excel in zero- and few-shot settings, handling tasks from forecasting to complex inference that go beyond mere sequence processing and pattern matching [42]. The second approach, such as IoT-LM [79] and Time-LLM [52], trains adapters to align a CPS-IoT system's input, output, or intermediate variables to the backbone LLM's semantic space, outperforming the first approach but requiring more complex design and fine-tuning. Lastly, some recent works [128, 135, 136] have explored leveraging label semantics for sensing classification without any task-specific training."}, {"title": "2.3 Beyond Perception", "content": "While existing research towards CPS-IoT FMs has primarily focused on encoding sensory data for perception, there is incipient work on FMs for other stages of the PCCA loops with the emergence of task- and platform-specific AI/ML models. The successes of deep reinforcement learning in robotics [44], autonomous vehicles [58], drones [115], pan-tilt-zoom cameras [97], and building energy and HVAC management [130] catalyzed interest in FMs adaptable to deployments (e.g., different buildings, robotic bodies, etc.) without requiring additional training. Particularly, the robotics community has a burgeoning body of FM research [32, 47] developing general-purpose robots and models generalizable across new platforms. These FMs are often trained as vision-language action (VLA) models [13, 56, 73], or finetuned Vision Language Models (VLMs), to learn policies for tasks like manipulation and navigation from massive vision-language demonstration data with diverse robots [84] and can be used for variety of tasks such as planning [101] in the PCCA loops of robots. Alternatively, LLMs and VLMs are increasingly used via prompting as FMs for CPS-IoT tasks in robotics [29, 113], HVAC control [98], and industrial control [102], as well as tools for generating control code [62, 113], annotating sensor data [46], tasking CPS-IoT resources [67], managing sensor privacy [114], and Q&A over sensor data [50]."}, {"title": "3 New Insights from Our Research", "content": "Given the quickly expanding landscape of FMs for CPS-IoT applications, an interesting question becomes: Are FMs for the CPS-IoT domain simply an adaptation of more general research in AI/ML to new datasets and tasks (such as the approaches summarized in the previous section) or are there fundamental differences in how we should think of FMs for CPS-IoT applications? Several distinguishing characteristics of the CPS-IoT domain impact FM design:\n1. Tight resource/quality trade-offs: CPS-IoT applications significantly shift acceptable resource/quality trade-offs for practical deployment of AI/ML. For example, recent statistics indicate that the cost of converting a standard car to an autonomous one is significantly higher than the cost of the original vehicle [25]. This is unacceptable and would hinder the pervasive use of mobile intelligence.\n2. Spatial embodiment: CPS-IoT applications are concerned with world state that evolves in physical time and space. Sensors sample that state from various vantage points with a range of observation modalities. FMs must abstract away from sensor properties, modalities, and locations to representations of the observed phenomena in space and time.\n3. Historical context: Historical context is critical to interpreting current phenomena. Today's LLMs are limited in the amount of context they can ingest. While this amount may be sufficient for understanding large passages of text, sensors can be sampled at much larger rates, with arbitrarily old events playing a significant role in interpreting the present. Novel solutions are needed to preserve important context.\n4. Structural constraints: The physical embodiment of CPS-IoT applications into a world that abides by laws of nature entails the existence of significant amounts of prior knowledge, including laws of physics, system dynamics, and other constraints imposed on the data at multiple levels of abstraction. FMs, in a way, are an expression of the underlying knowledge in a domain. Thus, training and/or inference should allow explicit representation and exploitation of domain constraints to improve reasoning efficiency.\nMotivated by the above differences, we discuss a preliminary exploration of challenges arising from these distinguishing properties, solution requirements to address these challenges, and remaining open issues."}, {"title": "3.1 Tight Resource/quality Trade-offs", "content": "The integration of future CPS-IoT FMs with private sensor data and user context will favor implementations where the intelligence resides on the edge and not in the cloud. To what extent are improvements required in resource efficiency and inference quality for FMs to run on the mobile device?\n3.1.1 A Preliminary Experiment. To address the above question, we test two existing general FMs on task-specific IoT data: a time-series FM (TSFM), MOMENT (of which we use two different sizes), and an LLM, Llama-3.2, with 1 billion parameters. Figure 1 illustrates the experimental setup for these models. To assess their effectiveness, we consider two common mobile IoT tasks: extrapolation and imputation, executed on electrocardiogram (ECG) and photoplethysmogram (PPG) signals, downsampled to 50Hz, inspired by mobile health monitoring applications. We also compare these approaches to two classical baselines (variations of ARIMA models using [100]). The first baseline, called ARIMA-pretraining (ARIMA-PT), fits an ARIMA model using an 80-20 training-evaluation split, after which the model remains fixed for evaluation. The second approach, termed ARIMA-online-training (ARIMA-OT), leverages the lightweight nature of ARIMA models to perform test-time fitting. We implement the models on a Google Pixel 8 Pro based on an open-source Java library [106] and the ExecuTorch framework [75]. Memory footprint is assessed by measuring the memory usage change before and after loading and running the model. Cold-start latency is defined as the total duration encompassing both the model loading phase and the execution of a single inference, while warm-start latency measures the time taken for a single inference after the model is already loaded."}, {"title": "3.1.2 Experimental Results and Recommendations", "content": "Table 1 shows that the largest of the time-series FMs, MOMENT-L did outperform both versions of ARIMA on both tasks. However, this was accomplished at the expense of 2-3 order of magnitude increase in latency, and over 3 orders of magnitude increase in memory footprint. For example, Moment-L required 1.2 GB of memory and incurred a cold-start latency of 2.3 seconds, compared to 0.74 MB and 2ms cold-start latency for ARIMA-PT. Moreover Llama failed to outperform ARIMA, despite its higher resource consumption than even MOMENT-L. This observation highlights the limitations of today's general LLMs in processing low-level sensory data directly, suggesting that their strengths lie in handling metadata, high-level reasoning, or language-based analytics, rather than raw time series analysis.\nThe experiment motivates developing a new generation of FMs specifically designed and optimized for CPS-IoT applications of interest. Such FMs, if successful, might become new architectural abstractions a part of the mobile OS that allow handling myriads of user-specific intelligent tasks. But first, one must understand more carefully how CPS-IoT applications differ and what these differences imply in terms of FM design and customization."}, {"title": "3.2 Spatial Embodiment", "content": "CPS-IoT applications distinguish world state (of monitored phenomena) from the observed (or measured) views as sampled by sensors at specific vantage points. There is usually the notion of a channel that propagates state from the source phenomenon to the observers, incurring various distortions along the way. This creates an inconsistency: Ingested FM data are sensor data representing distributed local observations. The goal, however, is to represent the actual world state. For example, in acoustic monitoring, one may want to recover the real sound, location, and speed of a moving object from projections perceived by individual sensors at different vantage points. CPS-IoT FMs must therefore abstract from the time-series data of individual sensors to representations of the underlying observed phenomena. How can the training of FMs be nudged to represent the observed environment, not sensor data? Said differently, how to ensure the invariance of the representation from the observation instruments and their locations?\n3.2.1 Challenges. To allow FM training to decouple local signal projections from the unified latent semantic representation of the observed phenomena, it needs to understand the spatial structure of observations. Arbitrarily placed sensors simply perform an irregular local sampling of the scene. Their positions must be encoded by the FM to understand how the scene is sampled. Traditional positional embeddings, such as those common for text and images, are typically designed to encode the relative order of input tokens in sequential or grid-based data. In contrast, sensors are not deployed in regular lines or grids. Thus, in multi-view or multi-vantage sensing applications, sensor geo-location embedding (that directly encodes the specific location information of each vantage point) is desirable. Location embeddings should further generalize to differences in absolute positioning that do not impact spatial reasoning. For example, they can encode relative distances, not absolute GPS locations. Alternatively, they can use augmentations that abstract away the absolute locations to ensure that the model focuses on relative positioning features. In addition to the geo-location embedding, timestamp embeddings should also be incorporated to specify the temporal metadata of observations. A multi-vantage model must also understand the relations between vantage points. For example, in vision, geometric transformations and/or neural network-based approaches (e.g., NeRF [76]) can be applied to construct an image from a new angle or vantage point given existing images from other vantage points. To attain the same effect with arbitrary sensors, one training approach could be to mask a portion of a sensor's time series and reconstruct it from signals at other vantage points, forcing the model to learn signal dependencies on location.\n3.2.2 A Preliminary Experiment. For a proof of concept, we used a multimodal MAE-based framework that consists of an encoder and a decoder. Input signals from different vantage points are transformed into visual representations (namely, spectrograms). A dedicated encoder processes each modality, and the resulting embeddings are combined in a joint encoder to enable a cross-modal representation. The overall architecture is illustrated in Figure 2.\nTo encode positional information, the model employs a 3-dimensional learnable positional embedding that integrates time, frequency, and spatial vantage point information. These embeddings are added to the masked input before being processed by the encoder. Notably, relative coordinates of vantage points are encoded instead of absolute locations to ensure the translation invariance of the spatial positional embeddings. To uncover spatiotemporal dependencies, the raw inputs are divided into patches, with a subset selectively masked across specific vantage points, time steps, or frequency bands. The auto-encoder is then trained to reconstruct the complete stacked spectrograms. This reconstruction process enables the framework to extract spatiotemporal relations in a self-supervised manner by inferring signals across related vantage points, times, and frequency bands."}, {"title": "3.2.3 Experimental Results and Recommendations", "content": "To test if the newly-trained model can recover physical states of observed phenomena, we use it to perform target tracking. Unlike classification, where data from any one sensor might be sufficient for identifying the target, tracking requires collective use of multiple sensor signals and positions to learn the equivalent of localization. We evaluate it on a self-collected dataset using a network of six Raspberry Pi nodes strategically positioned along vehicle routes to collect acoustic and seismic signals. During the data collection, civilian vehicles were driven by the sensors at varying speeds and directions, with the vehicle's real-time GPS coordinates recorded for ground-truthing purposes. We present the trained model's classification and tracking performance in Figure 3 compared to state-of-the-art contrastive baselines [19, 26, 68, 91, 108]. Our method consistently achieves the highest accuracy and F1 score for classification and the lowest Mean Squared Error (MSE) and Mean Absolute Error (MAE) for tracking.\nThe experiment demonstrates the importance of investigating novel approaches to FM pretraining that nudge the FM to learn representations of physical world state, as opposed to merely encoding the projections of that state, comprising sensory time-series signals. This is often called the inverse problem [104], as distinguished from forecasting future observations. Many challenges must be solved to develop general mechanisms for distilling unified sensor-agnostic latent representations of environmental state (collected from a wide range of sensing modalities and vantage points), and mechanisms for automating state interpretation, given world knowledge. The problem is more challenging in cluttered environments, calling for solutions that augment sensor measurements with prior knowledge to disambiguate among multiple competing inverse hypotheses regarding complex state from limited available observations. The final representation should also be generalized across different sensor configurations, abstracting away from specifics such as sensor calibration details, gains, and sampling rates."}, {"title": "3.3 Historical Context", "content": "The importance of temporal events (that extend over time) to the interpretation of observed physical phenomena makes historical context of present observations an important part of their semantic representation. Most existing work on CPS-IoT applications has focused on short-time perception tasks, such as human activity recognition or object detection, which typically require only a few seconds of sensor data for inference. However, to achieve a human-like understanding of the world, a system must capture high-level contextual information over extended periods of time, an aspect often overlooked in current work.\nFor example, for an outside observer to ascertain that an office building is empty (say, at the end of a work day), it needs the accumulated record of all entry and exit events since the building was last unlocked. Considering a camera with a 30 frame/second rate, an 8-hour work-day generates nearly one million frames, far more than the context window of modern LLMs. It is therefore important to know what needs to be remembered. Unfortunately, the key events to remember depend on the situation. To describe different situations, it is useful to define the concept of a complex event. A complex event represents a high-level scenario with spatiotemporal rules and patterns that require aggregating and reasoning over numerous short-term activities, which we call atomic events. For example, Fig. 4(a) defines a complex event where an intelligent assistant on a mobile device understands a sanitary protocol and alerts users of potential violations. Fig. 4(b) shows a scenario in a smart facility, where a surveillance system detects potentially suspicious activity, such as an unusual parcel hand-off, by analyzing data across distributed cameras.\n3.3.1 Challenges. Complex event detection (CED) introduces several challenges. First, the pattern of interest (e.g., the trained self-attention matrix of the FM's encoder) must identify key atomic event occurrences relevant to the complex event while ignoring irrelevant activities. Let us call the latter, \"don't care\" elements, or \"X.\" For example, the sanitary protocol can be represented as \"Use restroom \u2192 X \u2192 Wash hands \u2192 X \u2192 Eat,\" where \"X\" includes other irrelevant activities like \"walking\" or \"sitting.\" Incorporating \"X\" broadens the range of possible matching sequences, and the temporal duration further amplifies this space exponentially. Second, complex events have much longer time dependencies. In the sanitary protocol example, violation occurs when the person skips \"Wash hands\" after \"Use restroom\" and before \"Eat\". However, the time gap between those atomic events can be significant. Third, complex events often require immediate attention. For instance, in a nursing monitor system, we shouldn't wait to analyze data until the end of the day. Instead, we need immediate alerts when safety is violated."}, {"title": "3.3.2 A Preliminary Experiment", "content": "To investigate the performance of existing LLMs on a real-time CED task, we designed a multimodal complex event dataset in a smart health monitoring setting, and created a stochastic simulator that mimics daily human behaviors and synthesizes the corresponding sensor traces using existing IMU and Audio datasets for the underlying atomic activities, WISDM[116] and ESC50[90]. Fig. 4(c) illustrates the task. To help the LLM, we assumed a perfect labeling tool that detects the ground truth text label of the atomic activity and passes it to the LLM. We then explained in the text prompt what constitutes a complex event. We use three metrics to evaluate the LLM's performance. The Length Accuracy metric determines if the the complex event labels given by LLMs have the same length as the input sequence. The Conditional F1 Score evaluates element-wise (time-wise) F1 score for three complex event labels, conditioned on the case when the LLM outputs a complex event sequence with the correct length T. Coarse F1 Score is a sample-wise coarse F1 score that evaluates complex event labeling at high-level. It does not require a precise match between the predicted and ground-truth complex event labels at every timestamp. It only requires the LLM to recognize a complex event type in the 5-minute sample correctly. Table 2 shows the results. It can be seen that the LLMs performed somewhat poorly at complex event detection for all types of complex events considered.\nTwo approaches were then considered to building an improved real-time Complex Event Detection (CED) framework. The first uses sequential neural network models like CNNs, Transformers, and RNNs to learn complex event rules in a data-driven manner. The second employs neurosymbolic architectures with hand-coded rules, such as DeepProbLog[74] and NeurASP [129], utilizing neural network outputs for probabilistic symbolic computation. The neurosymbolic method we designed integrates neural networks for recognizing atomic activities with user-defined rules for complex event detection. We compare three model types for complex event detection: (1) End-to-End Models: Includes Transformer, LSTM, TCN, and Mamba, trained on sensor latent embedding sequences and complex event labels; (2) Concept Bottlenecked Models: Consists of AE + Transformer, AE + LSTM, AE + TCN, and AE + Mamba. These models use a pretrained classifier (AE) to obtain atomic activity labels, which are then used to train the neural backbones; (3) Neurosymbolic Model: The AE+FSM model employs the most probable atomic activity label as input for rule-based finite state machines (FSMs)."}, {"title": "3.3.3 Experimental Results and Recommendations", "content": "Figure 5 presents our preliminary results for various models. We also tested these models on out-of-distribution (OOD) complex events lasting 3 minutes, 15 minutes, and 30 minutes, all adhering to the same rules but varying in temporal span. The Mamba and AE+FSM models outperformed others on average. The AE + FSM model incorporates correct complex event rules; its performance declines greatly with longer traces due to cumulative errors from imperfect atomic event inference. While the Mamba model showed the best generalization on the OOD test sets, we still noted a performance drop as the temporal span increased.\nThe above experiments show that FMs capable of reasoning over long and complex temporal patterns are still lacking. Though LLMs have potential to perform well on CED tasks, the current models still suffer from hallucinations and poor ability in long-chain reasoning. In our preliminary experiments on different model architectures and methods, we also find that state-based methods such as the Mamba model and FSM engines are more suitable when dealing with complex event patterns that may span a long time, as they can compress information into states efficiently. However, CED tasks remain challenging in many ways. For example, it remains to design efficient means to define new types of complex events, allowing the FM to detect them in a zero-shot of a few-short manner. Input tokenization is another important concern. With meaningful events occurring at multiple granularities, the best size for input time-series sensor data tokens is unclear. Another challenge is to scale context window size and develop mechanisms to efficiently remember old (but important) events in continuous and long sensor streams. In particular, the ability to incorporate human knowledge and the use of neurosymbolic architecture to guide attention seem like key architectural requirements."}, {}]}