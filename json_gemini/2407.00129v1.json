{"title": "Multimodal Learning and Cognitive Processes in Radiology: MedGaze for Chest X-ray Scanpath Prediction", "authors": ["Akash Awasthi", "Ngan Le", "Zhigang Deng", "Rishi Agrawal", "Carol C. Wu", "Hien Van Nguyen"], "abstract": "Predicting human gaze behavior within computer vision is integral for developing interactive systems that can anticipate user attention and address fundamental questions in cognitive science. While methodologies exist for modeling gaze behavior on natural images, scanpath prediction from radiographic images remains unexplored.\nTo develop an Al system that can model the cognitive processes of the radiologist and predict the scanpaths on the CXR images.\nThis retrospective study utilized publicly available datasets: REFLACX[1], comprising eye-tracking data from multiple radiologists, and EGD-CXR[2], which includes data from a single radiologist. MedGaze employs a two-stage training process using Large-Multimodal models to generate human-like scanpaths and fixations duration using the radiology reports and CXR images. Evaluation includes metrics such as loU score, CC score, and Multimatch score, comparing MedGaze against a state-of-the-art method. Human evaluation assessed similarity to human-generated patterns and coverage of the region of interest.\nMedGaze outperformed the state of the art on both EGD-CXR and REFLACX datasets. On EGD-CXR, MedGaze achieved IoU, CC (Correlation Coefficient), and mean Multimatch score (mMM) of 0.41 [95% CI 0.40,0.42 ] vs 0.27 [95% CI 0.26,0.28 ], 0.50 [95% CI 0.48,0.51 ] vs 0.37 [95% CI 0.36,0.41 ], and 0.80 [95% CI 0.79,0.81 ] vs 0.71 [95% CI 0.70,0.71 ] compared to the state of the art. On REFLACX, MedGaze scored 0.45 [95% CI 0.44,0.46 ] vs 0.30 [95% CI 0.29,0.30 ], 0.53 [95% CI 0.50,0.55 ] vs 0.40 [95% CI 0.38,0.42 ], and 0.84 [95% CI 0.83,0.85 ] vs 0.76 [95% CI 0.75,0.77 ]. MedGaze also demonstrated its ability to assess case difficulty through fixation duration, showing a significant Spearman rank correlation of 0.65 (p=0.00) with true case difficulty ranks on EGD-CXR. In human evaluation, 13 out of 20 MedGaze-predicted scanpath videos resembled human-generated patterns, and 18 out of 20 achieved a comprehensive score of 4 (60-80% region coverage). Additionally, MedGaze-predicted scanpaths showed minimal redundancy (redundancy score = 1) compared to human-generated ones (9 out of 20 vs 5 out of 20).\nModeling scanpaths on radiology images is crucial for understanding and anticipating radiologist's eye movements, enhancing training standardization, and improving diagnostic accuracy.", "sections": [{"title": "Introduction", "content": "The modeling of human gaze behavior is a critical problem in computer vision, with significant implications for designing interactive systems that can anticipate a user's attention. In medical imaging, particularly with chest X-rays (CXR), predicting scanpaths is essential for enhancing diagnostic accuracy and efficiency. By analyzing how expert radiologists navigate these images, we can develop advanced training programs to help novices adopt effective viewing strategies, thereby reducing errors and improving their diagnostic skills.\nPredicting human scanpaths on medical images presents unique challenges compared to natural images due to the presence of abnormal regions with varying shapes, sizes, and contrasts [6]. Previous research has focused on predicting scanpaths in natural images by targeting specific objects or goals[3,4,5]. Our study introduces MedGaze, a novel system tailored to model scanpaths aligned with radiology reports containing multiple abnormalities. MedGaze predicts fixation points and durations crucial for identifying abnormalities, aiming to enhance human-Al collaboration and refine training modules for novice radiologists.\nAs shown in Figure 1a, our methodology involves two-stage training: Vision to Radiology Report Learning (VR2) and Vision Language Cognitive Learning (VLC), utilizing large publicly available datasets. Given the limited availability of eye gaze tracking data [1,2], we leverage the MIMIC dataset[7,8] for representation learning to extract medically relevant multimodal features, which are then used to model eye gaze movements. Our model employs Large Multimodal Models (LMMs) to extract text-enriched multimodal embeddings. Unlike previous computer vision efforts that focus on predicting scanpaths based on specific objects or categories, our approach addresses a broader context of modeling scanpath sequences for searching multiple abnormalities in CXR images. Specifically, our method scales up the prediction by an order of magnitude compared to existing state-of-the-art methods.\nTo validate our approach, we compare its performance to current state-of-the-art methods in computer vision for predicting scanpaths on natural images, using statistical metrics. Additionally, we assess our model's ability to generalize across different radiologists. An expert thoracic radiologist provides ratings based on the comprehensiveness and redundancy of predicted scanpaths to evaluate their clinical relevance."}, {"title": "Materials and Methods:", "content": "Figure 1a outlines our two-stage training approach, Vision to Radiology Report Learning (VR2) and Vision-Language Cognition Learning (VLC), aimed at extracting the text-enriched multimodal embeddings to model cognitive processes in CXR diagnosis. Figure 1b expands on our architectural framework, which comprises three pivotal components: the Visual Backbone, the Multimodal Space, and the Gaze Prediction Module.\nVisual Backbone: The Visual Backbone is essential for extracting contextualized visual embeddings. It includes ResNet-50[9] as a frozen feature extractor that extracts visual features from images. Following this, 6 standard transformer encoder [20] layers are incorporated to generate a contextualized feature embedding, denoted as Z\u2081. Additionally, we employ 2D sinusoidal positional embeddings to denote the location of each patch [10]. Our ablation study experiments reveal that substituting the ResNet-50 (feature extractor) and transformer encoder block with a CLIP-based vision [11]transformer results in increased training duration and computational costs, as well as relatively inferior performance. We include the ablation study results in the supplementary material.\nMedFormer: During the initial training phase of the VR2, we propose a transformer-based module pre-trained specifically on the MIMIC data, called MedFormer. This module aims to bridge the gap between the frozen image encoder and the large language model, facilitating the extraction of a fixed number of image features irrespective of the input image resolution. It consists of two transformer submodules: one called an image transformer which interacts with the frozen image encoder, and the other one called a text transformer which can function both as a text encoder and decoder. MedFormer filters out unnecessary visual details, providing focused and refined visual context. This reduces the LLM's burden of aligning visual and language data from scratch, making the training process more efficient.\nLarge Language Model: This component serves as the cornerstone of our architecture, tasked with modeling the complex interplay between refined contextualized image embeddings and text embeddings. Consequently, it equips the gaze prediction module with robust multimodal embeddings enriched by textual context. By employing the frozen decoder-based LLM known as OPT[12], we integrate MedGaze's output with text embeddings and input this concatenated representation to the LLM.\nMultimodal Space: In contrast to the previous Gazeformer [3] model, which employed simple linear projections to create image-text joint embeddings in the visual-semantic space, our experiments demonstrate that this approach falls short for detailed radiology reports. Therefore, we propose to connect MedGaze with a large language model to capture the complex interplay between image and text embeddings. Radiology reports are extensive, detailing numerous diseases or abnormalities that radiologists look for. Thus, simplistic modeling within the visual-semantic space may prove inadequate. The radiology reports show long dependencies since they begin searching for various diseases from the start of the image. Consequently, the sequence in which diseases are identified may involve complex cognitive processes. For example, detecting a patchy opacity of various sizes and shapes could lead to the diagnosis of pneumonia or edema. Our ablation experiments found that the optimal configuration for multimodal space requires integrating both MedGaze and the LLM.\nGaze Prediction Module: This module is responsible for predicting both fixation coordinates and fixation duration, and it consists of a fixation decoder and a scanpath prediction network. Specifically, the fixation decoder adopts a transformer decoder-like architecture[20], processing F fixation queries. These learnable fixation queries, which are randomly initialized, encode information about the fixation timestep. The maximum length of the fixation sequence is denoted as F. If the output fixation length is shorter than the maximum sequence length, padding is used to adjust the length to F. We have used 6 standard Transformer decoder layers in the Fixation decoder block. The latent fixation embeddings interact through self-attention and engage with the multimodal embedding (M) via encoder-decoder attention. Furthermore, fixed 2D position encoding is added to the multimodal embedding to provide positional information about the patches.\nIn the fixation prediction module, fixation coordinates are directly regressed from the output of the fixation decoder $Z_{fd}$, which has a size of batch size \u00d7 F \u00d7 model dimensions, with F indicating the timestep information. Radiologists exhibit variability in gaze sequence patterns, reflecting individual approaches to diagnosing diseases from CXR images, leading to inter-subject variability in fixation patterns. To ensure the model's generalizability across multiple radiologists and avoid learning spurious correlations, fixation coordinates, and durations are modeled using a Gaussian distribution. This involves regressing the mean and log-variance of the 2D coordinates and fixation duration using six distinct MLP layers, employing the reparametrization trick [13]to ensure a fully differentiable network. Padding is employed for fixation sequences shorter than the maximum length set (F), and a separate MLP classifier with a softmax classifier is utilized to predict whether a specific step in the F slices of the multimodal embedding is a valid fixation or a padding token. During inference, (X, Y, T, V) are predicted, where X, Y represent the fixation coordinates, T represents the fixation duration and V represents the probability of this fixation quad being a valid fixation or a padding token. Sequence termination occurs when V>0.5, signaling the start of the padding tokens.\nTraining Procedure: In the initial phase (VR2), we train the MedGaze on the MIMIC data to acquire text-informed vision representation. During this stage, the Qformer[14] is connected with the frozen image encoder to facilitate training using techniques such as Image-Text matching loss [15], Image-Text contrastive loss [14], and Image-Text grounding loss [16]. Moving to the second training stage (VLC), depicted in Figure 1 a, we integrate the MedGaze with the visual backbone (consisting of a frozen image encoder and a transformer encoder) and the frozen LLM to execute the Vision-Language Cognitive Learning. In this phase, the total loss (L\u2081) is calculated by summing the spatio-temporal loss and the cross-entropy loss for token classification across N samples in the minibatch, as described in Equation 1."}, {"title": "Results:", "content": "Our results section is structured into three distinct parts. Comparison with the state-of-the-art, prediction visualization and human evaluation.\nComparison with the State of the Art: It is essential to highlight that static fixation heatmaps are generated based on predicted fixation coordinates and fixation duration for each case. The intensity around each fixation coordinate is adjusted by scaling it with the fixation duration. For Table 2, we set the intensity spread around each fixation coordinate to 50. However, we also evaluated performance across all pixel spread levels and provided the comparison in Figure 3.\nAs shown in Table 2, when trained and tested on the same dataset (same radiologist), MedGaze shows significant improvements over Gazeformer [3]. Specifically, for the EGD-CXR dataset, MedGaze achieves a mloU of 0.41 [95% CI 0.40,0.42 ], mCC of 0.50 [95% CI 0.48,0.51], mMM of 0.80 [95% CI 0.79,0.81 ], and mD-MM of 0.50 [95% CI 0.46,0.52 ], compared to Gazeformer's 0.27 [95% CI 0.26,0.28 ], 0.37 [95% CI 0.36,0.41], 0.71 [95% CI 0.70,0.71], and 0.06 [95% CI 0.048, 0.0839], respectively. On the REFLACX dataset, MedGaze achieves a mloU of 0.45 [95% CI 0.44,0.46], mCC of 0.53 [95% CI 0.50,0.55 ], mMM of 0.84 [95% CI 0.83,0.85 ], and mD-MM of 0.66 [95% CI 0.65,0.68 ], while Gazeformer achieves 0.30 [95% CI 0.29,0.30 ], 0.40 [95% CI 0.38,0.42 ], 0.76 [95% CI 0.75,0.77], and 0.29 [95% CI 0.27,0.33], respectively. This substantial performance gain highlights MedGaze's superior ability to predict radiologists' scanpaths and fixation durations accurately.\nAdditionally, we assess performance based on dataset transferability to understand how well the model generalizes across different datasets. Since the EGD-CXR and REFLACX datasets are recorded by different radiologists, it is crucial to comprehend how well the model identifies abnormal regions corresponding to text, rather than solely overfitting to a specific dataset. When trained on EGD-CXR and tested on REFLACX, MedGaze achieves a mloU of 0.39 [95% CI 0.38, 0.40] and an mCC of 0.42 [95% CI 0.40, 0.43], outperforming Gazeformer, which scores 0.26 [95% CI 0.25, 0.27] and 0.33 [95% CI 0.31, 0.34], respectively. Conversely, when trained on REFLACX and tested on EGD-CXR, MedGaze scores 0.41 (95% CI 0.40, 0.43) for mloU, 0.50 (95% CI 0.47, 0.51) for mCC surpassing Gazeformer's 0.28 (95% CI 0.27, 0.29), 0.38 (95% CI 0.36, 0.41) respectively.\nWe also trained and tested our proposed model on the combined REFLACX+EGD-CXR dataset to evaluate whether a larger dataset would enhance scanpath predict [95% CI 0.48, 0.51], mMM of 0.85 [95% CI 0.84, 0.86], and mD-MM of 0.73 [95% CI 0.72, 0.74], significantly surpassing Gazeformer's scores of 0.30 [95% CI 0.29, 0.31], 0.42 [95% CI 0.40, 0.43], 0.78 [95% CI 0.77, 0.79], and 0.43 [95% CI 0.41, 0.45], respectively.\nIn Figure 2A, it is evident that MedGaze outperforms Gazeformer across all spread levels when both models are trained and tested on the same radiologist's data. In Figure 2B, MedGaze also surpasses Gazeformer across all spread levels when trained and tested on different radiologists' eye gaze datasets. Notably, the blue curve, representing MedGaze trained on a combination of both EGD-CXR and REFLACX, consistently outperforms all other curves. The orange curve, slightly below, represents MedGaze trained on EGD-CXR and tested on REFLACX. The difference between these curves (EGD_REF_MedGaze and EGD+Ref MedGaze) is more pronounced than the difference between the curves representing EGD+REF_Gazeformer and REF_EGD_Gazeformer. This indicates that MedGaze exhibits greater effectiveness and generalization when data augmentation is performed."}, {"title": "Discussion:", "content": "This study introduces MedGaze, a novel system designed to model the complex cognitive processes of radiologists when interpreting chest X-ray (CXR) images. MedGaze employs a two-stage training strategy: Vision-Language Representation Learning and Vision Cognitive Learning. Initially, MedGaze is pre-trained on the publicly available MIMIC dataset to learn medically relevant multimodal features. Subsequently, the pre-trained MedGaze undergoes end-to-end training with the EGD-CXR and REFLACX datasets, aiming to predict scanpaths over CXR images. Our system is thoroughly evaluated using statistical metrics and human evaluation."}]}