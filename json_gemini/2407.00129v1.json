{"title": "Multimodal Learning and Cognitive Processes in Radiology: MedGaze for Chest X-ray Scanpath Prediction", "authors": ["Akash Awasthi", "Ngan Le", "Zhigang Deng", "Rishi Agrawal", "Carol C. Wu", "Hien Van Nguyen"], "abstract": "Predicting human gaze behavior within computer vision is integral for developing interactive systems that can anticipate user attention and address fundamental questions in cognitive science. While methodologies exist for modeling gaze behavior on natural images, scanpath prediction from radiographic images remains unexplored. To develop an Al system that can model the cognitive processes of the radiologist and predict the scanpaths on the CXR images. This retrospective study utilized publicly available datasets: REFLACX[1], comprising eye-tracking data from multiple radiologists, and EGD-CXR[2], which includes data from a single radiologist. MedGaze employs a two-stage training process using Large-Multimodal models to generate human-like scanpaths and fixations duration using the radiology reports and CXR images. Evaluation includes metrics such as loU score, CC score, and Multimatch score, comparing MedGaze against a state-of-the-art method. Human evaluation assessed similarity to human-generated patterns and coverage of the region of interest. MedGaze outperformed the state of the art on both EGD-CXR and REFLACX datasets. On EGD-CXR, MedGaze achieved IoU, CC (Correlation Coefficient), and mean Multimatch score (mMM) of 0.41 [95% CI 0.40,0.42 ] vs 0.27 [95% CI 0.26,0.28 ], 0.50 [95% CI 0.48,0.51 ] vs 0.37 [95% CI 0.36,0.41 ], and 0.80 [95% CI 0.79,0.81 ] vs 0.71 [95% CI 0.70,0.71 ] compared to the state of the art. On REFLACX, MedGaze scored 0.45 [95% CI 0.44,0.46 ] vs 0.30 [95% CI 0.29,0.30 ], 0.53 [95% CI 0.50,0.55 ] vs 0.40 [95% CI 0.38,0.42 ], and 0.84 [95% CI 0.83,0.85 ] vs 0.76 [95% CI 0.75,0.77 ]. MedGaze also demonstrated its ability to assess case difficulty through fixation duration, showing a significant Spearman rank correlation of 0.65 (p=0.00) with true case difficulty ranks on EGD-CXR. In human evaluation, 13 out of 20 MedGaze-predicted scanpath videos resembled human-generated patterns, and 18 out of 20 achieved a comprehensive score of 4 (60-80% region coverage). Additionally, MedGaze-predicted scanpaths showed minimal redundancy (redundancy score = 1) compared to human-generated ones (9 out of 20 vs 5 out of 20). Modeling scanpaths on radiology images is crucial for understanding and anticipating radiologist's eye movements, enhancing training standardization, and improving diagnostic accuracy.", "sections": [{"title": "Introduction", "content": "The modeling of human gaze behavior is a critical problem in computer vision, with significant implications for designing interactive systems that can anticipate a user's attention. In medical imaging, particularly with chest X-rays (CXR), predicting scanpaths is essential for enhancing diagnostic accuracy and efficiency. By analyzing how expert radiologists navigate these images, we can develop advanced training programs to help novices adopt effective viewing strategies, thereby reducing errors and improving their diagnostic skills.\nPredicting human scanpaths on medical images presents unique challenges compared to natural images due to the presence of abnormal regions with varying shapes, sizes, and contrasts [6]. Previous research has focused on predicting scanpaths in natural images by targeting specific objects or goals[3,4,5]. Our study introduces MedGaze, a novel system tailored to model scanpaths aligned with radiology reports containing multiple abnormalities. MedGaze predicts fixation points and durations crucial for identifying abnormalities, aiming to enhance human-Al collaboration and refine training modules for novice radiologists.\nAs shown in Figure 1a, our methodology involves two-stage training: Vision to Radiology Report Learning (VR2) and Vision Language Cognitive Learning (VLC), utilizing large publicly available datasets. Given the limited availability of eye gaze tracking data [1,2], we leverage the MIMIC dataset[7,8] for representation learning to extract medically relevant multimodal features, which are then used to model eye gaze movements. Our model employs Large Multimodal Models (LMMs) to extract text-enriched multimodal embeddings. Unlike previous computer vision efforts that focus on predicting scanpaths based on specific objects or categories, our approach addresses a broader context of modeling scanpath sequences for searching multiple abnormalities in CXR images. Specifically, our method scales up the prediction by an order of magnitude compared to existing state-of-the-art methods.\nTo validate our approach, we compare its performance to current state-of-the-art methods in computer vision for predicting scanpaths on natural images, using statistical metrics. Additionally, we assess our model's ability to generalize across different radiologists. An expert thoracic radiologist provides ratings based on the comprehensiveness and redundancy of predicted scanpaths to evaluate their clinical relevance."}, {"title": "Materials and Methods:", "content": "Figure 1a outlines our two-stage training approach, Vision to Radiology Report Learning (VR2) and Vision-Language Cognition Learning (VLC), aimed at extracting the text-enriched multimodal embeddings to model cognitive processes in CXR diagnosis. Figure 1b expands on our architectural framework, which comprises three pivotal components: the Visual Backbone, the Multimodal Space, and the Gaze Prediction Module.\nVisual Backbone: The Visual Backbone is essential for extracting contextualized visual embeddings. It includes ResNet-50[9] as a frozen feature extractor that extracts visual features from images. Following this, 6 standard transformer encoder [20] layers are incorporated to generate a contextualized feature embedding, denoted as $Z_I$. Additionally, we employ 2D sinusoidal positional embeddings to denote the location of each patch [10]. Our ablation study experiments reveal that substituting the ResNet-50 (feature extractor) and transformer encoder block with a CLIP-based vision [11]transformer results in increased training duration and computational costs, as well as relatively inferior performance. We include the ablation study results in the supplementary material.\nMedFormer: During the initial training phase of the VR2, we propose a transformer-based module pre-trained specifically on the MIMIC data, called MedFormer. This module aims to bridge the gap between the frozen image encoder and the large language model, facilitating the extraction of a fixed number of image features irrespective of the input image resolution. It consists of two transformer submodules: one called an image transformer which interacts with the frozen image encoder, and the other one called a text transformer which can function both as a text encoder and decoder. MedFormer filters out unnecessary visual details, providing focused and refined visual context. This reduces the LLM's burden of aligning visual and language data from scratch, making the training process more efficient.\nLarge Language Model: This component serves as the cornerstone of our architecture, tasked with modeling the complex interplay between refined contextualized image embeddings and text embeddings. Consequently, it equips the gaze prediction module with robust multimodal embeddings enriched by textual context. By employing the frozen decoder-based LLM known as OPT[12], we integrate MedGaze's output with text embeddings and input this concatenated representation to the LLM.\nMultimodal Space: In contrast to the previous Gazeformer [3] model, which employed simple linear projections to create image-text joint embeddings in the visual-semantic space, our experiments demonstrate that this approach falls short for detailed radiology reports. Therefore, we propose to connect MedGaze with a large language model to capture the complex interplay between image and text embeddings. Radiology reports are extensive, detailing numerous diseases or abnormalities that radiologists look for. Thus, simplistic modeling within the visual-semantic space may prove inadequate. The radiology reports show long dependencies"}, {"title": "", "content": "since they begin searching for various diseases from the start of the image. Consequently, the sequence in which diseases are identified may involve complex cognitive processes. For example, detecting a patchy opacity of various sizes and shapes could lead to the diagnosis of pneumonia or edema. Our ablation experiments found that the optimal configuration for multimodal space requires integrating both MedGaze and the LLM.\nGaze Prediction Module: This module is responsible for predicting both fixation coordinates and fixation duration, and it consists of a fixation decoder and a scanpath prediction network. Specifically, the fixation decoder adopts a transformer decoder-like architecture[20], processing F fixation queries. These learnable fixation queries, which are randomly initialized, encode information about the fixation timestep. The maximum length of the fixation sequence is denoted as F. If the output fixation length is shorter than the maximum sequence length, padding is used to adjust the length to F. We have used 6 standard Transformer decoder layers in the Fixation decoder block. The latent fixation embeddings interact through self-attention and engage with the multimodal embedding (M) via encoder-decoder attention. Furthermore, fixed 2D position encoding is added to the multimodal embedding to provide positional information about the patches.\nIn the fixation prediction module, fixation coordinates are directly regressed from the output of the fixation decoder $Z_{fd}$, which has a size of $batch \\ size \\times F \\times model \\ dimensions$, with F indicating the timestep information. Radiologists exhibit variability in gaze sequence patterns, reflecting individual approaches to diagnosing diseases from CXR images, leading to inter-subject variability in fixation patterns. To ensure the model's generalizability across multiple radiologists and avoid learning spurious correlations, fixation coordinates, and durations are modeled using a Gaussian distribution. This involves regressing the mean and log-variance of the 2D coordinates and fixation duration using six distinct MLP layers, employing the reparametrization trick [13]to ensure a fully differentiable network. Padding is employed for fixation sequences shorter than the maximum length set (F), and a separate MLP classifier with a softmax classifier is utilized to predict whether a specific step in the F slices of the multimodal embedding is a valid fixation or a padding token. During inference, (X, Y, T, V) are predicted, where X, Y represent the fixation coordinates, T represents the fixation duration and V represents the probability of this fixation quad being a valid fixation or a padding token. Sequence termination occurs when V>0.5, signaling the start of the padding tokens.\nTraining Procedure: In the initial phase (VR2), we train the MedGaze on the MIMIC data to acquire text-informed vision representation. During this stage, the Qformer[14] is connected with the frozen image encoder to facilitate training using techniques such as Image-Text matching loss [15], Image-Text contrastive loss [14], and Image-Text grounding loss [16]. Moving to the second training stage (VLC), depicted in Figure 1 a, we integrate the MedGaze with the visual backbone (consisting of a frozen image encoder and a transformer encoder) and the frozen LLM to execute the Vision-Language Cognitive Learning. In this phase, the total loss (L\u2081) is calculated by summing the spatio-temporal loss and the cross-entropy loss for token classification across N samples in the minibatch, as described in Equation 1."}, {"title": "", "content": "$\\frac{1}{N} \\sum_{k=1}^{N} (L_{spa}^{k} + L_{val}^{k})\\qquad \\qquad (1)$ \nWhere $\\qquad L_{spa}^{k} = \\frac{1}{t} \\sum_{i=0}^{t} (|\\hat{x}_{i}^{k} - x_{i}^{k}| + |\\hat{y}_{i}^{k} - y_{i}^{k}| + |\\hat{t}_{i}^{k} - t_{i}^{k}|) $ \n$\\frac{1}{L} \\sum_{i=1}^{L-1} (\\hat{v}_{i}^{k}log v_{i}^{k} + (1 - \\hat{v}_{i}^{k})log(1 - v_{i}^{k})) $\nHere, $L$ represents the total loss, $L_{spa}$ is the spatio-temporal loss, which is an L1 loss between the predicted and ground truth fixation sequences, including duration. The predicted scanpath, denoted as $s = \\{(x^{k}_{i}, y^{k}_{i}, t^{k}_{i})\\}_{i=0}^{L-1}$ has a maximum length L, while $t$ is the length of the ground truth scanpath$\\hat{s} = \\{(\\hat{x}^{k}_{i},\\hat{y}^{k}_{i},\\hat{t}^{k}_{i})\\}_{i=0}^{L}$ $L_{val}$ signifies the validity prediction loss, calculated as the negative log-likelihood loss for validity prediction for each token.\nFor the VLC training phase, we adopted a batch size of 32 and implemented Disjoint Optimization \\cite{mondal2023gazeformer} with the Adam optimizer \\cite{kingma2014adam}. This optimization technique employs variable learning rates for different network parameter groups. MedGaze underwent training for 200 epochs to achieve optimal performance.\nDatasets: In this study, we utilized two datasets: EGD-CXR[2] and REFLACX [1]. These datasets consist of CXR images with synchronized eye-tracking and transcription pairs, annotated by different radiologists. We utilized both datasets to assess the generalization capability of our proposed system. Additionally, we merged both datasets to create a larger dataset, enabling us to evaluate the system's performance comprehensively. Table 1 presents details about the training and testing samples utilized across different datasets. The key hyperparameter we considered was the maximum fixation length, set to 50. This choice was"}, {"title": "", "content": "made based on the observation that most cases had a total of 50 scanpaths, indicating that doctors typically concluded their diagnosis within this range. This length is an order of magnitude larger than that of state-of-the-art gaze modeling in natural images [3]. In the supplementary material, we include the distribution plot showing the most common fixation sequence lengths.\nStatistical Metrics: We assessed our model using two categories of metrics: fixation heatmap-based and scanpath similarity-based evaluations. For fixation heatmaps, we employ Intersection over Union (IoU) and Correlation Coefficient (CC)[17]. IoU quantifies the percentage overlap between the target and prediction masks, while CC gauges the correlation between normalized predicted and human fixation maps. Regarding scanpath similarity, we utilize the mean Multimatch Match Score (MM)[18,19], which aggregates scores for shape, direction, length, position, and duration. Additionally, we present the mD-MM (mean Duration Multimatch score), representing the duration aspect of the MM score and indicating the accuracy of fixation duration predictions. We provide 95% Confidence Intervals derived from the bootstrapped method to ensure the robustness of our findings. For the analysis of case complexity, we compute the Pearson Correlation coefficient for true and predicted total fixation durations, and the Spearman rank correlation coefficient for case difficulty ranks. All statistical calculations were performed using the Scikit-learn package (version 1.2.1) in Python v3.8."}, {"title": "Results:", "content": "Our results section is structured into three distinct parts. Comparison with the state-of-the-art, prediction visualization and human evaluation.\nComparison with the State of the Art: It is essential to highlight that static fixation heatmaps are generated based on predicted fixation coordinates and fixation duration for each case. The intensity around each fixation coordinate is adjusted by scaling it with the fixation duration. For Table 2, we set the intensity spread around each fixation coordinate to 50. However, we also evaluated performance across all pixel spread levels and provided the comparison in Figure 3.\nAs shown in Table 2, when trained and tested on the same dataset (same radiologist), MedGaze shows significant improvements over Gazeformer [3]. Specifically, for the EGD-CXR"}, {"title": "", "content": "dataset, MedGaze achieves a mloU of 0.41 [95% CI 0.40,0.42 ], mCC of 0.50 [95% CI 0.48,0.51], mMM of 0.80 [95% CI 0.79,0.81 ], and mD-MM of 0.50 [95% CI 0.46,0.52 ], compared to Gazeformer's 0.27 [95% CI 0.26,0.28 ], 0.37 [95% CI 0.36,0.41], 0.71 [95% CI 0.70,0.71], and 0.06 [95% CI 0.048, 0.0839], respectively. On the REFLACX dataset, MedGaze achieves a mloU of 0.45 [95% CI 0.44,0.46], mCC of 0.53 [95% CI 0.50,0.55 ], mMM of 0.84 [95% CI 0.83,0.85 ], and mD-MM of 0.66 [95% CI 0.65,0.68 ], while Gazeformer achieves 0.30 [95% CI 0.29,0.30 ], 0.40 [95% CI 0.38,0.42 ], 0.76 [95% CI 0.75,0.77], and 0.29 [95% CI 0.27,0.33], respectively. This substantial performance gain highlights MedGaze's superior ability to predict radiologists' scanpaths and fixation durations accurately.\nAdditionally, we assess performance based on dataset transferability to understand how well the model generalizes across different datasets. Since the EGD-CXR and REFLACX datasets are recorded by different radiologists, it is crucial to comprehend how well the model identifies abnormal regions corresponding to text, rather than solely overfitting to a specific dataset. When trained on EGD-CXR and tested on REFLACX, MedGaze achieves a mloU of 0.39 [95% CI 0.38, 0.40] and an mCC of 0.42 [95% CI 0.40, 0.43], outperforming Gazeformer, which scores 0.26 [95% CI 0.25, 0.27] and 0.33 [95% CI 0.31, 0.34], respectively. Conversely, when trained on REFLACX and tested on EGD-CXR, MedGaze scores 0.41 (95% CI 0.40, 0.43) for mloU, 0.50 (95% CI 0.47, 0.51) for mCC surpassing Gazeformer's 0.28 (95% CI 0.27, 0.29), 0.38 (95% CI 0.36, 0.41) respectively.\nWe also trained and tested our proposed model on the combined REFLACX+EGD-CXR dataset to evaluate whether a larger dataset would enhance scanpath predict [95% CI 0.48, 0.51], mMM of 0.85 [95% CI 0.84, 0.86], and mD-MM of 0.73 [95% CI 0.72, 0.74], significantly surpassing Gazeformer's scores of 0.30 [95% CI 0.29, 0.31], 0.42 [95% CI 0.40, 0.43], 0.78 [95% CI 0.77, 0.79], and 0.43 [95% CI 0.41, 0.45], respectively.\nIn Figure 2A, it is evident that MedGaze outperforms Gazeformer across all spread levels when both models are trained and tested on the same radiologist's data. In Figure 2B, MedGaze also surpasses Gazeformer across all spread levels when trained and tested on different radiologists' eye gaze datasets. Notably, the blue curve, representing MedGaze trained on a combination of both EGD-CXR and REFLACX, consistently outperforms all other curves. The orange curve, slightly below, represents MedGaze trained on EGD-CXR and tested on REFLACX. The difference between these curves (EGD_REF_MedGaze and EGD+Ref MedGaze) is more pronounced than the difference between the curves representing EGD+REF_Gazeformer and REF_EGD_Gazeformer. This indicates that MedGaze exhibits greater effectiveness and generalization when data augmentation is performed."}, {"title": "Analyzing the Case Difficulty based on the fixation duration:", "content": "Our investigation into case difficulty, inferred from fixation duration, reveals insightful findings regarding the model's comprehension of case difficulty. When trained and tested on the EGD-CXR dataset with recordings from a single experienced radiologist, the Pearson correlation coefficient (CC) between the radiologist's time duration and the model's predicted time duration was 0.54 (p=0), as illustrated in Figure 4, Column 1. Conversely, the REFLACX dataset, with recordings from five radiologists of varying experience levels, yielded a lower CC of 0.36 (p=0). To further assess case difficulty, we ranked cases based on total predicted fixation durations, with longer durations indicating higher difficulty (longer visual attention), and plotted these ranks against the ground truth. Furthermore, for the EGD-CXR dataset, a significant positive correlation between predicted and ground ranks was evident, with a Spearman rank correlation coefficient of 0.64 (p=0), depicted in Figure 4, Column 2. In the REFLACX dataset,"}, {"title": "", "content": "the analysis showed a lower Spearman rank correlation coefficient of 0.36 (p=0), likely due to the dataset's inherent noise from multiple radiologists with varying expertise levels.\nFigure 5 illustrates cases from the EGD-CXR test set positioned at both extremes (lowest and highest) of the distribution shown in Figure 4, Column 2, which represents the rank correlation. The cases ranked highest, indicating the most difficult scenarios, typically feature multiple abnormalities, thus enhancing their difficulty. In contrast, cases ranked lowest, denoting the simplest scenarios, frequently involve no abnormalities or represent normal cases."}, {"title": "Human Radiologist Evaluation:", "content": "We conducted a randomized control study where a board-certified radiologist was asked to score scanpaths without the knowledge of whether they are from human radiologists or MedGaze. The results, as shown in Table 3, indicate MedGaze's strong alignment with human gaze patterns. For identifying machine-generated versus human gaze patterns, MedGaze's predictions were rated as human-like in 13 out of 20 instances by a radiologist, compared to 19 for the ground truth, demonstrating a high degree of human-likeness. In terms of comprehensiveness, MedGaze showed robust coverage of important regions, with 8 predictions scoring a 4 (61-80% coverage) and 10 achieving a 5 (81-100% coverage), closely matching the ground truth, which had 8 and 12 predictions in these categories, respectively. Furthermore, MedGaze exhibited minimal redundancy, with most"}, {"title": "", "content": "scores at 1 or 2, indicating efficient coverage with less overlap compared to human patterns, which had more instances of moderate redundancy. Overall, MedGaze effectively mimics the human gaze while maintaining efficiency and thorough coverage of significant regions. We also provide the randomly selected 40 video and expert radiologist evaluations in the source code repository."}, {"title": "Discussion:", "content": "This study introduces MedGaze, a novel system designed to model the complex cognitive processes of radiologists when interpreting chest X-ray (CXR) images. MedGaze employs a two-stage training strategy: Vision-Language Representation Learning and Vision Cognitive Learning. Initially, MedGaze is pre-trained on the publicly available MIMIC dataset to learn medically relevant multimodal features. Subsequently, the pre-trained MedGaze undergoes end-to-end training with the EGD-CXR and REFLACX datasets, aiming to predict scanpaths over CXR images. Our system is thoroughly evaluated using statistical metrics and human evaluation."}, {"title": "", "content": "The table presents a performance comparison between MedGaze and the state-of-the-art (SOTA) method Gazeformer across different train/test combinations. Notably, when trained and tested on the same datasets (either EGD-CXR or REFLACX), MedGaze consistently outperforms Gazeformer in all metrics: mean Intersection over Union (mloU), mean Correlation Coefficient (mCC), mean Multimatch Metric (mMM), and mean Duration Multimatch Metric (mD-MM). For instance, on the REFLACX dataset, MedGaze achieves a mloU of 0.45 [95% CI 0.44, 0.46], an mCC of 0.53 [95% CI 0.50, 0.55], an mMM of 0.84 [95% CI 0.83, 0.85], and an mD-MM of 0.66 [95% CI 0.65, 0.68], significantly higher than Gazeformer's 0.30 [95% CI 0.29, 0.30], 0.40 [95% CI 0.38, 0.42], 0.76 [95% CI 0.75, 0.77], and 0.29 [95% CI 0.27, 0.33], respectively. The substantial difference in mD-MM scores for both datasets, EGD-CXR (MedGaze: 0.50 [95% CI 0.46, 0.52] vs. Gazeformer: 0.06 [95% CI 0.048, 0.0839]) and REFLACX (MedGaze: 0.66 [95% CI 0.65, 0.68] vs. Gazeformer: 0.29 [95% CI 0.27, 0.33]), highlights MedGaze's superior ability to predict fixation duration, crucial for understanding case difficulty. This performance can be attributed to our two-stage training approach, which effectively captures the intricate visual attention patterns of radiologists.\nOur results also highlight the impact of dataset size on model performance. Both MedGaze and Gazeformer exhibit enhanced performance when trained on the larger REFLACX dataset compared to the smaller EGD-CXR dataset. This discrepancy is particularly evident in the metrics, with MedGaze's performance on REFLACX (mMM of 0.84 [95% CI 0.83, 0.85]) surpassing that on EGD-CXR (mMM of 0.80 [95% CI 0.79, 0.81]). This finding underscores the importance of large, diverse training datasets in improving model accuracy and generalizability.\nAnother crucial aspect of our study is MedGaze's ability to generalize across different radiologists. When trained on one dataset and tested on another (e.g., trained on REFLACX and tested on EGD-CXR), MedGaze still demonstrates robust performance, albeit with a slight decrease compared to training and testing on the same dataset. For example, MedGaze's mloU drops from 0.45 (95% CI 0.44, 0.46) when trained and tested on REFLACX to 0.41 [95% CI 0.40, 0.43] when trained on REFLACX and tested on EGD-CXR.\nTo further validate MedGaze's effectiveness, we created a larger dataset by combining the REFLACX and EGD-CXR datasets. MedGaze achieved a mloU of 0.41 [95% CI 0.40, 0.42], an mCC of 0.49 [95% CI 0.48, 0.51], an mMM of 0.85 [95% CI 0.84, 0.86], and an mD-MM of 0.73 [95% CI 0.72, 0.74], significantly higher than Gazeformer's scores of 0.30 [95% CI 0.29, 0.31], 0.42 [95% CI 0.40, 0.43], 0.78 [95% CI 0.77, 0.79], and 0.43 [95% CI 0.41, 0.45], respectively. Although there is a slight decrease in performance when combining data from different radiologists compared to training and testing on the same radiologist's data, our model still showed good performance. This suggests that combining data from multiple radiologists acts as a regularizer, introducing noise into the training process and aiding in generalizing the model across multiple datasets.\nRadiologists' interpretation of CXRs entails varying fixation durations, influenced by multiple factors such as the complexity of findings, the number of abnormalities present, and their level of expertise, etc. To evaluate a model's ability to grasp case difficulty, we analyzed the Pearson correlation coefficient between predicted and ground truth total fixation durations. When tested"}, {"title": "", "content": "on the EGD-CXR dataset (collected on experienced radiologist), the model demonstrated a significant positive correlation (CC=0.56, p=0), indicating its tendency to predict longer durations for challenging cases. Conversely, testing on the REFLACX dataset, which features recordings from radiologists with varying expertise levels, resulted in a lower correlation coefficient (CC=0.35, p=0), reflecting the dataset's noise due to differing levels of radiologist experience. This noise impacted the model's ability to accurately learn case difficulty, despite its capacity to discern important regions amidst noisy data.\nExpanding our investigation, we ranked case difficulty based on total fixation duration, with longer fixation durations corresponding to more difficult cases, thus representing higher ranks. For the EGD-CXR dataset, a significant positive correlation between predicted and ground truth ranks was evident, with a Spearman rank correlation coefficient of 0.56 (p=0). The rank-order plot illustrated a discernible trend, demonstrating how predicted ranks align well with ground truth ranks, particularly for cases with higher fixation durations. Subsequently, our analysis extended to the REFLACX dataset. Despite a lower Spearman rank correlation coefficient of 0.36 (p=0), likely attributed to the dataset's inherent noise from multiple radiologists of varying expertise levels. Additionally, we plotted the most difficult (highest rank) and simplest cases. The most difficult cases typically exhibit multiple abnormalities requiring careful attention for accurate diagnosis, whereas the simplest cases often depict normal conditions without abnormalities. This ranking approach can effectively guide the development of training programs for novice radiologists by presenting cases in increasing order of difficulty. Beginning with straightforward cases( normal cases with no abnormality) allows beginners to grasp normal anatomy and basic abnormalities, progressing to more challenging cases with longer fixation durations to refine their skills in identifying subtle or atypical findings. Such structured training enhances diagnostic accuracy and confidence, equipping radiologists to effectively manage diverse clinical scenarios while fostering continuous professional development.\nThe evaluation of MedGaze using human-likeness and comprehensiveness criteria reveals insightful findings about its performance in predicting gaze patterns. MedGaze's predictions were rated as human-like in 13 out of 20 cases, compared to 19 out of 20 for the ground truth, indicating a high degree of accuracy in emulating human gaze behavior. In terms of comprehensiveness, MedGaze demonstrated strong coverage of important regions, with 8 predictions scoring a 4 (61-80% coverage) and 10 predictions achieving a perfect score of 5 (81-100% coverage). This performance is comparable to the ground truth, where 8 and 12 predictions scored 4 and 5, respectively. However, the redundancy scores suggest that MedGaze predictions are less redundant than human gaze patterns, with a majority of its scores falling between 1 and 2 (minimal to some minor redundancy), while human patterns had more instances of moderate redundancy. This indicates that MedGaze not only effectively identifies crucial regions but also does so more efficiently, avoiding unnecessary fixation on redundant areas. Overall, the results underscore MedGaze's ability to closely mimic human gaze patterns while enhancing efficiency in gaze prediction.\nDespite the promising results, several limitations must be acknowledged. The datasets used (REFLACX and EGD-CXR) are limited in size and diversity, potentially affecting the model's generalizability. The eye-tracking data, derived from a small number of radiologists, may not"}, {"title": "", "content": "fully capture the complexity of human visual behavior. Additionally, MedGaze currently focuses solely on chest X-rays, and its applicability to other medical imaging modalities remains to be explored. Furthermore, the computational cost and complexity associated with large multimodal models could limit real-time clinical deployment.\nIn conclusion, MedGaze represents a significant advancement in predicting scanpaths on medical images, particularly chest X-rays (CXR). Through a two-stage training process leveraging large publicly available datasets, MedGaze accurately models the cognitive processes of radiologists to predict fixation coordinates and durations. Our system demonstrates superior performance compared to the state-of-the-art Gazeformer, as evidenced by higher loU, CC, and Multimatch scores across different datasets. This improvement is consistent whether MedGaze is trained and tested on data from the same radiologist or across multiple radiologists, indicating robust generalizability. Furthermore, human evaluations affirm that MedGaze's predicted scanpaths closely resemble expert search patterns, with higher comprehensiveness and lower redundancy. These findings underscore the potential of MedGaze to enhance diagnostic accuracy, improve training programs for novice radiologists, and optimize clinical workflows, ultimately contributing to better patient outcomes. Future work will focus on expanding dataset diversity, exploring applicability to other imaging modalities, and optimizing the model for real-time clinical use."}]}