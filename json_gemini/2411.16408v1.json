{"title": "Low-Data Classification of Historical Music Manuscripts: A Few-Shot Learning Approach", "authors": ["Elona Shatri", "Daniel Raymond", "Gy\u00f6rgy Fazekas"], "abstract": "In this paper, we explore the intersection of technology and cultural preservation by developing a self-supervised learning framework for the classification of musical symbols in historical manuscripts. Optical Music Recognition (OMR) plays a vital role in digitising and preserving musical heritage, but historical documents often lack the labelled data required by traditional methods. We overcome this challenge by training a neural-based feature extractor on unlabelled data, enabling effective classification with minimal samples. Key contributions include optimising crop preprocessing for a self-supervised Convolutional Neural Network and evaluating classification methods, including SVM, multilayer perceptrons, and prototypical networks. Our experiments yield an accuracy of 87.66%, showcasing the potential of AI-driven methods to ensure the survival of historical music for future generations through advanced digital archiving techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "The digitisation of historical sheet music is crucial for preserving musical heritage. Optical Music Recognition (OMR) techniques convert these documents into machine-readable formats like MusicXML and MIDI, allowing integration into modern platforms. However, recognising symbols in these manuscripts presents challenges due to document degradation and variability [19].\nExisting deep learning approaches to OMR typically rely on large, annotated datasets, but the scarcity of labelled data for historical documents presents a major obstacle [2]. Without sufficient labels, traditional OMR systems struggle to generalise across diverse notational styles found in historical archives. This paper addresses these limitations by proposing a self-supervised learning framework that facilitates robust music symbol classification with minimal labelled data, thus supporting the broader goal of preserving and digitising musical heritage.\nBuilding on the foundations of previous research by [15], we hypothesise that a self-supervised Convolutional Neural Network (CNN), paired with optimised classification algorithms, can effectively classify music symbols using only a few annotated examples. A key focus of this work is optimising the preprocessing of musical \u201ccrops\u201d\u2014small sections of manuscripts-where we apply transformations that enhance the model's robustness to the varied forms of degradation typically found in historical documents. The extracted features are subsequently classified using methods like Support Vector Machines (SVM), multilayer perceptrons (MLP), and prototypical networks, ensuring accuracy despite limited sample sizes."}, {"title": "II. BACKGROUND", "content": "As mentioned in Section I, the digitisation of cultural artefacts, such as historical sheet music, is an essential component of modern efforts to preserve and share cultural heritage. However, this process presents significant challenges, particularly in fields like OMR, where the diversity and degradation of historical documents require more advanced approaches than those used for modern printed music. Deep learning models have proven effective in image classification tasks, but they typically rely on large volumes of annotated data-data that is often scarce for historical manuscripts [17].\nFew-shot learning addresses this limitation by enabling models to generalise from only a few training examples. This is particularly valuable for tasks like OMR in historical contexts, where obtaining extensive labelled data is impractical. The few-shot learning paradigm, often utilising an N-way-K-shot approach, empowers models to recognise and classify symbols based on a minimal number of labelled samples [17]. Current OMR systems for modern, printed scores achieve high accuracy through deep learning techniques such as transformer [25] and object detection [22], [23] largely because of the uniformity and well-annotated nature of the data. However, when applied to handwritten or historical music manuscripts, these models struggle due to variability in musical notation, handwriting styles, and document degradation over time [19].\nTo mitigate these challenges, few-shot learning techniques can be divided into two broad categories: metric-based methods and meta-learning approaches. Metric-based methods, such as Siamese networks, project samples into a feature space where similar items cluster together by minimising the distance between them [18]. Prototypical networks extend this concept by computing a prototype for each class, allowing new samples to be classified based on their proximity to these prototypes [9]. These approaches are well-suited for the task of recognising symbols in historical music manuscripts, where only a few labelled examples may be available."}, {"title": "III. METHODOLOGY", "content": "This study builds on the work by [15], adapting a self-supervised Convolutional Neural Network (CNN) to encode relevant features for music symbol classification in historical manuscripts using few-shot learning. Our goal is to provide a framework that not only achieves high classification accuracy but also aids in the digital preservation of musical heritage by overcoming the limitations of scarce labelled data. The key stages of the pipeline are detailed below (see Figure 1:\nHistorical music sheets often exhibit varying symbol sizes, spacing, and document degradation, making it essential to preprocess the documents carefully before classification. To extract individual musical symbols, we divided the manuscripts into subsections or crops, each ideally containing a single symbol. This was done using a sliding window approach, which processes the manuscript in overlapping segments. Binarisation and entropy-based algorithms filtered out blank or irrelevant sections, improving computational efficiency. We employed the Sauvola method [20] for adaptive binarisation to handle the uneven lighting and degradation typically found in historical documents.\nSince historical manuscripts often lack extensive labelled data, self-supervised learning enables feature extraction from unlabelled data. We applied the VICReg method [21], which trains the CNN by distorting each crop twice and ensuring that these distortions map to the same point in the feature space. This approach is well-suited for handling the variability in symbol appearance caused by document ageing and handwritten inconsistencies. To further enhance network convergence, an expander block was included, and the loss function was structured to balance variance, invariance, and covariance terms, promoting robust feature learning.\nOnce trained, the CNN produced a high-dimensional feature vector for each symbol, which was subsequently classified using methods such as SVM, MLP, and prototypical networks. These classifiers were chosen for their complementary strengths in handling high-dimensional and few-shot data. Key improvements were made to both the image transformation and classification stages to enhance robustness and accuracy:\n\u2022 We tested additional transformations, such as salt-and-pepper noise, elastic distortion, and fade, simulating typical degradation in historical manuscripts [12]. This ensured consistent feature extraction across varying image qualities.\n\u2022 The pipeline originally used k-nearest neighbors (kNN) for classification, but we experimented with SVM, MLP, and prototypical networks, which better suited the high-dimensional feature space produced by the CNN."}, {"title": "C. Crop preprocessing", "content": "To improve the robustness of the feature extraction process, we applied several image transformations, simulating the typical forms of degradation seen in historical music manuscripts."}, {"title": "D. Classification", "content": "We evaluated several classification algorithms, with MLP outperforming others. The MLP was configured with five layers, reducing the input feature dimensions from 1600 to 128 through gradual dimensionality reduction. Regularisation was applied at each stage using batch normalisation and dropout to prevent overfitting, which is particularly important given the high-dimensional feature space generated by the CNN. Additionally, a prototypical classification layer was explored, with additional fully connected layers to improve the embedding's adaptability, though MLP consistently showed superior performance."}, {"title": "E. Data Augmentation", "content": "To further enhance the classifiers' generalisation capabilities, we employed various data augmentation techniques during training (see Section IV for details), as shown in Figure 2. These transformations ensured that the CNN extracted consistent features regardless of document quality."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this section, we outline the dataset used, the pipeline configuration, and the application of data augmentation during the classification stage. The setup was designed to ensure that the model can effectively handle the variability and degradation typical of historical music manuscripts, contributing to their long-term preservation through accurate digital recognition.\nWe employed the Capitan dataset [24], which contains mensural notation extracted from historical music manuscripts. The dataset comprises 98 pages, totalling 17,112 labelled music symbols across 28 distinct classes. This dataset presents a realistic challenge, as the symbols exhibit variability in style and quality due to the age and condition of the manuscripts. Its diversity makes it an ideal testbed for evaluating few-shot learning techniques in the context of cultural preservation."}, {"title": "A. Configuration", "content": "To ensure consistency with prior work and allow for meaningful comparison, we maintained a similar pipeline configuration. The crop extraction process was performed using a sliding window of 64 pixels, with an entropy threshold of 0.8 to filter out blank or uninformative regions. The self-supervised CNN generated a 1600-dimensional feature vector for each crop, using VICReg regularisation parameters set to $\\lambda$ = 10, $\\mu$ = 10, and $\\zeta$ = 1 [16].\nFor classification, we used the N-way-K-shot method, which allows the model to generalise from a small number of labeled samples. We experimented with K values of 1, 2, 5, and 10 to simulate varying degrees of data scarcity. The prototypical classifier was tested with different combinations of support (S) and query (Q) sets: S = 1, Q = 2; S = 2, Q = 3; and S = 6, Q = 4. These configurations were chosen to assess the model's ability to handle different levels of training data availability, which is critical for digitising rare historical documents."}, {"title": "B. Data Augmentation", "content": "Data augmentation is crucial in improving the model's robustness to the wide range of image qualities found in historical manuscripts. We applied augmentation at different levels (A = [0, 1, 2, 5, 10, 20]), where A represents the number of augmented samples generated per original crop. The transformations used during augmentation were carefully selected to simulate common types of degradation, including resized crops, colour jitter, and Gaussian blur. These transformations reflect the typical wear and tear of historical manuscripts.\nHowever, during testing, we observed that using all transformations from the self-supervised training stage led to overfitting, reducing classification accuracy. To mitigate this, we opted for a reduced set of augmentations\u2014random resized crop, colour jitter, and Gaussian blur\u2014during the classification phase. This approach balanced enhancing generalisation and preventing the model from becoming overly reliant on specific transformation patterns."}, {"title": "V. RESULTS AND DISCUSSION", "content": "This section presents the classification results, focusing on comparing different classifiers and evaluating the impact of data augmentation. Additionally, the results are contextualised with respect to the kNN-based baseline established by Alfaro-Contreras et al. [15] and provide a discussion on trade-offs between data augmentation and overfitting.\nThe MLP consistently outperformed the other classifiers across multiple settings, as shown in Table I. It achieved an accuracy of 87.66% with five samples per class, surpassing the kNN baseline by 5.66% from Alfaro-Contreras et al. (82.0%) [15]. This demonstrates the strength of MLP's multi-layered architecture in effectively handling the high-dimensional features extracted by the self-supervised CNN. The MLP's ability to learn complex, non-linear decision boundaries made it particularly well-suited for classifying symbols in this few-shot learning context. This outperformance is consistent with findings from [8] and [9], where similar architectures were demonstrated to perform well in feature-rich scenarios.\nCompared to meta-learning approaches such as Prototypical Networks, the MLP displayed more robust generalization in cases with larger support sets (L > 3). Prototypical Networks, while effective in low-shot settings (e.g., 75.96% at L = 1), showed diminishing accuracy as the augmentation increased, particularly with limited support-query splits, dropping to 68.68% at A = 20. This suggests that while meta-learning techniques offer advantages in extremely data-scarce contexts, MLP-based methods can scale better as data availability increases. Incorporating additional meta-learning frameworks such as Matching Networks or MAML in future studies could provide further insights into their scalability in OMR tasks.\nBy contrast, kNN, as highlighted in [15], performed well in simpler settings but struggled in high-dimensional spaces, where the concept of \"nearest neighbors\u201d is weakened by the curse of dimensionality [17]. SVM, although occasionally competitive with MLP, exhibited limitations in handling complex, multi-class scenarios due to its reliance on hyperplanes. These results align with previous work showing that non-linear classifiers, such as MLP, are more adept at capturing intricate patterns in feature-rich datasets.\nData augmentation played a key role in improving the classifiers' performance, especially for MLP, as shown in Figure 3. Moderate augmentation levels led to consistent accuracy improvements, but excessive augmentation resulted in diminishing returns for several classifiers, including SVM, kNN, and the prototypical network.\nFor SVM, over-augmentation may have introduced overly complex decision boundaries, while kNN was likely affected by the cluttered feature space, which hampered the \"nearest neighbour\u201d principle. The prototypical network was particularly sensitive to augmentation, with performance dropping from 75.96% to 68.68% when too many augmented variations were introduced. This was likely due to the prototypical network's reliance on smaller support sets, making it vulnerable to noise.\nHowever, MLP benefited from higher augmentation levels, showing improvements even with larger sets of augmented samples. This suggests that MLP's deep architecture was better equipped to generalise from augmented data, learning robust features from noisy or altered representations.\nDespite the overall performance, all classifiers struggled with certain musical symbol classes, particularly beamed notes, see Figure 4. These symbols were often confused due to their similar visual structure. This issue indicates a broader challenge in OMR, where subtle visual differences between symbols can be difficult to distinguish with limited training data. These challenges are further compounded in historical sheet music, where inconsistencies in symbol shapes and document degradation create additional obstacles for classifiers."}, {"title": "VI. CONCLUSION", "content": "This study demonstrated significant improvements in OMR for historical music manuscripts using few-shot learning techniques. The MLP classifier outperformed both the kNN baseline from Alfaro-Contreras et al. and alternative classifiers, showcasing its robustness in feature-rich, low-data scenarios. Additionally, moderate data augmentation enhanced performance, while excessive augmentation highlighted the importance of balancing augmentation strategies to avoid overfitting. The integration of Prototypical Networks provided a comparison with meta-learning approaches, demonstrating complementary strengths but emphasising the scalability of MLP in larger support set scenarios.\nFuture work will expand evaluations to additional datasets, enabling broader comparisons with other few-shot learning approaches such as MAML and Matching Networks. Furthermore, investigating augmentation strategies tailored to specific symbol complexities, such as adaptive augmentation based on symbol class, could enhance classifier performance in challenging OMR tasks. This continued exploration aims to further bridge the gap between cutting-edge AI techniques and the preservation of cultural heritage through improved OMR systems."}]}