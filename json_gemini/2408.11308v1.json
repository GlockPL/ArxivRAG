{"title": "EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models", "authors": ["Chongwen Zhao", "Zhihao Dou", "Kaizhu Huang"], "abstract": "Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation. In an effort to mitigate such risks, the concept of \"Alignment\" technology has been developed. However, recent studies indicate that this alignment can be undermined using sophisticated prompt engineering or adversarial suffixes, a technique known as \"Jailbreak.\" Our research takes cues from the human-like generate process of LLMs. We identify that while jailbreaking prompts may yield output logits similar to benign prompts, their initial embeddings within the model's latent space tend to be more analogous to those of malicious prompts. Leveraging this finding, we propose utilizing the early transformer outputs of LLMs as a means to detect malicious inputs, and terminate the generation immediately. Built upon this idea, we introduce a simple yet significant defense approach called EEG-Defender for LLMs. We conduct comprehensive experiments on ten jailbreak methods across three models. Our results demonstrate that EEG-Defender is capable of reducing the Attack Success Rate (ASR) by a significant margin, roughly 85% in comparison with 50% for the present SOTAs, with minimal impact on the utility and effectiveness of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are garnering unprecedented attention and application in the field of artificial intelligence, with chatbots such as Chat-GPT (Achiam et al., 2023) and Llama (Touvron et al., 2023a) standing out as notable examples. However, an inherent challenge arises due to the fact that these models could generate inappropriate and potentially harmful content, including biased, unlawful, pornographic, and fraudulent material (Weidinger et al., 2021). To mitigate the risks associated with such content and to steer LLM-generated responses away from these issues, researchers have innovated a series of alignment algorithms (Ouyang et al., 2022; Wei et al., 2022; Song et al., 2024). Through the implementation of these algorithms, chatbots have been empowered to discern and tactfully refuse to generate outputs in response to prompts that naively seek to elicit potentially harmful content.\nMore recently, it has been however discovered that well-designed jailbreak prompts can circumvent such alignment, posing new challenges for building stricter safety barriers (Zou et al., 2023; Liu et al., 2024; Wei et al., 2024). Meanwhile, efforts to defend against jailbreaks are ongoing. Prompt-based methods (Zhang et al., 2024; Xie et al., 2023; Jain et al., 2023; Wei et al., 2023; Inan et al., 2023a) approach defense by manipulating or detecting user prompts. However, these methods are impractical since they degrade significantly in utility (Xu et al., 2024a). As a result, researchers turn to decoding-based defense methods (Robey et al., 2024; Cao et al., 2024; Xu et al., 2024a; Zhao et al., 2024b). Instead of directly accessing prompts, decoding-based defense methods leverage the model's internal properties. Since these methods can maintain high model functionality, decoding-based defense methods have shown promise in defending against jailbreak attacks.\nUnfortunately, current decoding-based defense technologies are insufficient. Studies show that present defense methods could only reduce the Attack Success Rate (ASR) by around 50% against jailbreak prompts (Xu et al., 2024b). Approaches like RA-LLM (Cao et al., 2024) and Smooth-LLM (Robey et al., 2024) propose generating responses multiple times with random dropouts to defend against character-sensitive adversarial suffix attacks. However, they are less effective against prompt crafting attacks, which typically involve character-insensitive prompts. SafeDecoding (Xu et al., 2024a) aims to increase the likelihood of disclaimer generation artificially, but in practice, it fails to effectively reduce ASR in models with stronger safety barriers.\nIn response to the drawbacks of existing decoding-based defense methods, we revisit the functions of different layers in LLMs. Todd et al. (2024) reveal that the initial layers specialize in triggering specific tasks. The middle layers act as repositories of knowledge and shape the emotional tone of the output (Zhou et al., 2024; Zhao et al., 2024a). Subsequent layers are where the refinement of the language output occurs (Fan et al., 2024). Given that language only affects how we deliver, but not the semantics of expression (Fedorenko et al., 2024), we postulate that LLMs process jailbreak and harmful prompts similarly when recognizing functions in the initial layers and accessing stored knowledge in the middle layers.\nTo validate our postulation, we conduct a series of analysis. First, our results in Section 3.2 demonstrate that the classifiers trained on the initial layers achieve over 80% accuracy in detecting fail-to-refuse harmful prompts. More intuitively, as illustrated in Figure 1a, our empirical visualization shows that starting from the early layers of models (e.g., layer 6 and layer 8), embeddings of jailbreak prompts aligned with harmful prompts. In the middle layers (e.g., layer 12), where LLMs retrieve information, jailbreak embeddings shift towards benign embeddings slightly, and by the later layers (e.g., layers 28 and 32), they become increasingly aligned with benign embeddings. Ultimately, the jailbreak embeddings are either distributed throughout the space (as seen with Llama2) or distributed with the decision boundary (as seen with Vicuna and Guanaco), complicating the model in recognizing jailbreak status.\nRemarkably, the process by which large language models generate responses closely mirrors how humans organize language. To structure language output, humans first form an idea (Piaget, 1926), then draw upon experiences and memories (Corballis, 2019; Tulving et al., 1972). Finally, language serves as a conduit for conveying information (Brandt, 2010; Fedorenko et al., 2024). As such, we argue that the focus may be placed on the early or intermediate layers rather than the latter or even final layers, which are overemphasized by current defense methods.\nBased on this insight, we propose a simple yet novel framework for defending jailbreak, utilizing Early Exit Generation to defend against jailbreak, namely EEG-Defender. Specifically, we exploit benign prompts and rejected harmful prompts as anchors for each layer's output. If the embeddings from the early and middle layers are sufficiently similar to the harmful anchor, the model will refuse the user's request. We evaluate three popular LLMs: Llama2, Vicuna, and Guanaco. Despite its simplicity, our results show that EEG-Defender significantly outperforms all five baselines under most conditions, achieving approximately an 85% reduction rate in ASR while maintaining high functionality on benign prompts. Notably, EEG-Defender requires no fine-tuning of the original LLM and incurs minimal additional computational cost compared to existing defense methods, making it seamlessly integrable into current workflows.\nIn summary, our contributions are three-fold:\n\u2022 Human-like generation process of LLMs. Our study reveals that the generation process of LLMs parallels human language organization, a notable phenomenon not addressed in previous research.\n\u2022 Latent space mechanism of jailbreak. We empirically demonstrate that embeddings of jailbreak prompts in the early and middle layers closely resemble those of harmful prompts, but shift towards benign prompts in the later layers.\n\u2022 Defend jailbreak through early exit. Building on our insights into LLM jailbreak, we propose EEG-Defender. EEG-Defender reduces Attack Success Rate (ASR) by approximately 85% against existing jailbreak methods, with near-zero computational cost."}, {"title": "2 Background and Related Work", "content": "We first define the key notations used in this paper.\nEmbeddings. In LLMs, the embedding $e$ refers to the outputs produced by the transformer layers. Let $x_{1:s}$ denote a $s$-length user prompt, the LLM will generate output starting from $x_{s+1}$. In the final layer $n$, the embedding $e_n$ is used to generate the probability of the next token $x_{s+1}$ to $x_{1:s}$ by:\n$P_\\theta(x_{s+1} | x_{1:s}) = \\text{softmax}(We_n)$,\nwhere $\\theta$ denotes a language model and $W$ represents the $k \\times m$ projector matrix that maps the embedding space $\\mathbb{R}^m$ to the token space $\\mathbb{R}^k$.\nJailbreak. Jailbreak process aims to construct an adversarial prompt to elicit a harmful output of LLMs. Let $h$ denote a harmful question, and $\\theta$ denote a language model. The process of jailbreak is to find $x_{1:s}$ by solving:\n$\\max_{x_{1:s}} \\prod_{i=0}^{s+1} P_\\theta(x_{s+i} | x_{1:s+i})$,\nwhere $i, j$ such that $X_{i:j} = h$ and $x_{s+1:}$ starting with \"Sure, here is ...\" instead of a disclaimer or rejection response."}, {"title": "2.2 LLM Jailbreak", "content": "Jailbreak attacks are generally categorized into prompt crafting and token optimizing.\nPrompt Crafting. Wei et al. (2024) found that LLMs are often vulnerable to jailbreaks due to competing objectives and mismatched generalizations. They proposed 30 jailbreak methods to elicit harmful responses from GPT and Claude. To reduce the manual effort involved in crafting prompts, Yu et al. (2024); Mehrotra et al. (2024); Chao et al. (2024) developed several automatic frameworks for jailbreaking LLMs. These frameworks typically create a virtual context and suppress the denying output, which utilize the result founded in Wei et al. (2024).\nToken Optimizing. In a white-box setting, attackers have access to the gradients of LLMs, allowing them to optimize prompts to increase the likelihood of generating affirmative responses. Zou et al. (2023) achieved jailbreak by optimizing an adversarial suffix to minimize the loss of the desired prefix of outputting. The AutoDAN attack constructed prompts that can pass perplexity testing (Liu et al., 2024). Additionally, Qiang et al. (2024) combined In-Context Learning (ICL) with model gradients to distract the model's attention and generate harmful content."}, {"title": "2.3 Jailbreak Defense", "content": "Defense strategies against jailbreaks can be broadly categorized into prompt-based methods and decoding-based methods.\nPrompt-based Defense. Directly detecting content within prompts can help prevent harmful content generated by LLMs. Therefore, Inan et al. (2023a), OpenAI (2023b), and Jigsaw (2017) have proposed several APIs for content detection. In addition to filtering harmful prompts, manipulation of prompts can be incorporated to reinforce safety measures. Zhang et al. (2024) proposed adding prompts that instruct the model to prioritize safety. Xie et al. (2023) leveraged psychological principles by incorporating self-reminder prompts in system messages, encouraging LLMs to respond responsibly and thereby reducing the success rate of jailbreak attacks. Additionally, Jain et al. (2023) outlined three defensive strategies: perplexity detection, paraphrasing, and reorganization. However, this approach suffers from a high false positive rate, limiting its effectiveness in real applications.\nDecoding-based Defense. Some jailbreak prompts can be highly sensitive to character-level changes. Therefore, introducing random perturbations and dropouts can help mitigate attack effects (Robey et al., 2024). Cao et al. (2024) developed RA-LLM, which leverages the inherent robustness of LLMs and applies Monte Carlo sampling with dropout as a defense strategy. Xu et al. (2024a) revealed that safety disclaimers often remain among the top tokens in the outputs generated by jailbreak prompts. They proposed amplifying these safety token probabilities to reduce the risk of jailbreaks. Besides, Zhao et al. (2024b) identified several safety-critical layers within LLMs and re-aligned these layers to improve overall safety. Overall, these defense methods effectively balance utility and safety, but their effectiveness diminishes with models that have stronger safety barriers."}, {"title": "2.4 Language Production", "content": "One of the most widely accepted theories about how language is organized in humans is Piaget's theory, which suggests that thought forms first, and then language develops (Piaget, 1926). When individuals have a concept in mind, they draw upon their memories (Corballis, 2019) and personal experiences (Tulving et al., 1972; Sherwood, 2015). Conversely, language is optimized for communication, where people use signs to express and share their thoughts with others; this system of signs has gradually evolved into complex languages (Brandt, 2010). In summary, language is often seen as a bridge between communication and cognition in humans, with ideas forming first and language being structured based on memories and experiences.\nOur work is inspired by the process of language production, a phenomenon also reflected in LLMs. After receiving a prompt, the LLM first identifies the purpose of the prompt and triggers a function within the model (Todd et al., 2024). Then, it accesses and processes stored information (Meng et al., 2022) and manages emotional tone (Zhao et al., 2024a; Zhou et al., 2024) for prompts in the early and middle layers. Several studies found that by truncating (Fan et al., 2024), skipping (Elhoushi et al., 2024), and pruning (Men et al., 2024) some deeper layers, models can respond faster while maintaining correctness. This observation reveals that later layers are responsible for organizing languages. Due to the shared semantic similarities between jailbreak and harmful prompts, we believe that LLMs tend to perform similarly when identifying functions and accessing information."}, {"title": "3 A Closer Look into Jailbreak", "content": "Although concurrent work (Lin et al., 2024) demonstrates that well-aligned LLMs can effectively distinguish between benign and harmful prompts within the model's latent space, the mechanisms behind jailbreaks remain under debate. To gain a deeper understanding of jailbreak, we further investigate the representation of prompts.\nMotivated by the human-like generation process of the language model and the observation that well-aligned LLMs can reject malicious and some jailbreak prompts, our aim is to understand how jailbreak prompts manage to bypass safety barriers. Previous attack methods (Zou et al., 2023; Wei et al., 2024) suggest that the first token of response influenced the overall responses. Rejection responses always start with an apology or a disclaimer, while helpful responses to benign prompts typically begin with an affirmation. Given that jailbreak prompts share semantic similarities with harmful prompts but resemble benign prompts in their response patterns, we first conjecture that jailbreak embeddings progressively transit from harmful to benign as the layers go deeper."}, {"title": "3.1 Embedding of Jailbreak: A Toy Example", "content": "We conduct a toy example to examine how jailbreak prompts are positioned in the embedding space. We collected 60 benign prompts from Alpaca Eval (Li et al., 2023b), and 60 harmful prompts from AdvBench (Zou et al., 2023). Then, we evaluated 60 prompts generated by GCG (Zou et al., 2023), AutoDAN (Liu et al., 2024), GPTFuzz (Yu et al., 2024), and Tap (Mehrotra et al., 2024), all of which are effective at jailbreak models. As Figure 1a shows, in the final layer, the harmful prompts and benign prompts embedding are linearly separable after PCA, with jailbreak embeddings positioned between them, making detection and defense against jailbreaks more challenging. However, we found that in the earlier layers of LLMs (e.g., layer 6), embeddings for benign and harmful prompts are clearly separated, with jailbreak embeddings more closely aligned with harmful prompts. As we move to deeper layers, although benign and harmful embeddings remain distinct, jailbreak embeddings incline toward the center of benign embeddings. With this intriguing phenomenon, we also hypothesize that early and middle layers of transformers inherently possess the ability to discern jailbreak prompts."}, {"title": "3.2 Shallow Layers can Distinguish Jailbreak", "content": "To simulate real-world chatbot applications, we adapted the toxic-chat training dataset (Lin et al., 2023) to validate our hypothesis. The dataset includes 5,082 user prompts from the Vicuna online demo, with 384 identified as harmful. Specifically, we re-evaluated the harmful prompts in the dataset using Llama and Vicuna. We identified 302 harmful prompts for Llama2 and 140 harmful prompts for Vicuna that model can successfully reject using keyword matching. Second, we collected the embedding of all layers for benign prompts and rejected harmful prompts and trained 32 MLP classifiers as well as 32 prototype classifiers corresponding to the output of each layer, respectively. We use these two classifier sets to identify jailbreak prompts that the model cannot reject.\nAs shown in Figure 2, classifiers collected from the early layers perform much better than those from the later layers. The accuracy in distinguishing jailbreak prompts exceed 80% for both models up to the twelfth layer, strongly supporting our second hypothesis. This indicates that we should likely focus on the early and intermediate layer space rather than the output space.\nTo summarize, we empirically demonstrate that the mechanism for jailbreak is their embedding moves away from \"harmful\" and toward \"benign\" in the outputting space. Building on our analysis and observations that the shallow layers of LLMs can distinguish jailbreak prompts, we propose using the model's early and intermediate layer space as a bridge to defend against jailbreak attacks."}, {"title": "4 Proposed Method", "content": "In this section, we introduce our EEG-Defender in detail. The overview of our framework is illustrated as Figure 3. Based on our observation that shallow layers can distinguish jailbreak prompts, we build classifiers through the transformers."}, {"title": "4.1 Early Exit Generation and Classifiers", "content": "We primarily develop the EEG-Defender framework by three key steps in the following.\nStep I. Constructing Prompt Pool. Given a set of prompts $P = \\{P_1, P_2, ..., P_q\\}$, we first need to identify the harmfulness of each prompt $Y = \\{Y_1, Y_2, ..., Y_q\\}$, where $Y_i = 0$ for benign prompts and $y_i = 1$ for harmful prompts. Then, for harmful prompts, we use the given aligned LLM $f$ to generate corresponding responses $\\{a_1, a_2, ..., a_k\\}$. We then identify the prompts that are successfully rejected, resulting in the set $R = \\{r_1, r_2, ..., r_m\\}$. For benign prompts, we can directly use them to form a set $B = \\{b_1, b_2, ..., b_k\\}$. Finally, we get prompt set $P' = R \\cup B$ and corresponding $Y'$.\nStep II. Training Classifiers. We collect the embeddings from each layer of the LLM for prompts by generating the first token. Assuming that the LLM has $n$ layers in total, the embedding of a prompt $p_i$ could be represented as $E_i = \\{e_{i1}, e_{i2},...,e_{in}\\}$. Given the relatively small number of rejected prompts, we choose to implement prototype classifiers in our framework. The prototype $g_{ki}$ of class $k$ is computed by the mean embedding within this class (Snell et al., 2017). Let $P_k$ denote the set of samples of class $k$ in set $P'$. At the $i$th layer, $g_{ki}$ is represented by:\n$g_{ki} = \\frac{1}{|P_k|} \\sum_{x_j \\in P_k} e_{ji}$,\nwhere $e_{ji}$ is the embedding of $x_j$ at $i$th layer. The classification result $c_i$ of a sample embedding $e$ at layer $i$ is determined by:\n$c_i = \\text{arg} \\min_k d(e_i, g_{ki})$\nwhere $d$ represents the cosine distance as below:\n$d(e_i, g_{ki}) = 1 - \\frac{e_i \\cdot g_{ki}}{||e_i|| ||g_{ki}||}$"}, {"title": "Step III. Safe Generation.", "content": "We use the classifiers trained in Step II to classify prompts. Based on our observations, classifiers in the early layers demonstrate higher accuracy in detecting jailbreak prompts. Consequently, the EEG framework maintains a cumulative positive counter, referred to as the Harmfulness score, which tracks the total occurrences of positive classifications (i.e., prompts identified as harmful) by the classifier. Two hyperparameters, $\\alpha$ and $t$, control the shallow layer usage ratio and the harmfulness score threshold, respectively. Suppose the given LLM has a total of $n$ transformer layers and generates the response $x_{s+1:}$. The output of the LLM with EEG-Defender, $x'_{s+1:}$ can be accessed by:\n$x'_{s+1:} = \\begin{cases} \\text{Refuse to answer}, & \\text{if } \\sum_{i=1}^{\\lfloor \\alpha \\cdot n \\rfloor} c_i > t \\\\ x_{s+1:}, & \\text{otherwise} \\end{cases}$"}, {"title": "4.2 EEG-Defender", "content": "Based on the classifier and configuration set in Section 4.1, EEG-Defender can be integrated with any transformer-based LLM by monitoring the internal representation of the model. When a user inputs a prompt, EEG-Defender calculates the harmfulness score using embeddings starting from the first layer to the $\\lfloor \\alpha \\times n \\rfloor$th layer before generating the first token. If the harmfulness score (i.e., cumulative positive count) reaches the threshold $t$, the LLM can immediately halt generation and output a standard refusal response. Essentially, EEG-Defender evaluates the internal representations of prompts without requiring additional fine-tuning or retraining of the original model, making it a plug-and-play component for any LLM."}, {"title": "5 Experiment", "content": "In this section, we evaluate the effectiveness of EEG-Defender in defending against jailbreak prompts. We assess the effectiveness of EEG-Defender using 10 attack methods and 5 baseline defenses. Finally, we analyze the impact of adjusting hyper-parameters and prototype centers on the defense performance."}, {"title": "5.1 Experimental Setup", "content": "In this experiment, we use the prototype centers of rejected prompts and benign prompts calculated from the toxic-chat training dataset (Lin et al., 2023). We then calculate the embedding distance (i.e., cosine similarity) of the targeted prompt to the two prototypes to establish the decision boundary.\nModels and Settings. We conduct our experiment with three LLMs: Vicuna-7b, Llama-2-7b-chat, and Guanaco-7b. We use an early layer ratio of $\\alpha = 0.75$ for all models. The harmfulness score limit is set to $t = 12$ for Vicuna and Guanaco, and $t = 11$ for Llama2.\nDatasets and Baseline. We evaluate EEG-Defender on ten state-of-the-art attack methods: GCG (Zou et al., 2023), AutoDAN (Liu et al., 2024), GPTFuzz (Yu et al., 2024), TAP (Mehrotra et al., 2024), Pair (Chao et al., 2024), as well as 5 methods identified in jailbroken (Wei et al., 2024). We finf that Llama2 and Vicuna are unable to parse base64 encoding, therefore we select five Competing Objectives attack methods from Wei et al. (2024) (AIM, Wikipedia, Distractor, Refusal Suppress, Distractor and Negated). First, 50 harmful questions are randomly selected from Zou et al. (2023). For each harmful question, two prompts are generated using GCG, GPTFuzz, AutoDAN, Pair, and Tap, and one prompt is constructed using each of the five methods from Wei et al. (2024). This process results in a total of 750 jailbreak prompts. We then select three prompt-based defending methods (PPL (Jain et al., 2023), ICD (Wei et al., 2023), and Self-Reminder (Xie et al., 2023)) and two decoding-based defending methods (SafeDecoding (Xu et al., 2024a) and RA-LLM (Cao et al., 2024)) as baselines to evaluate these jailbreak prompts. To assess the model helpfulness with EEG-Defender, we collect 300 benign prompts from Li et al. (2023b). For the configurations of the attack method and defense baseline, please refer to Appendix A.\nEvaluation Metric. We adopt the Attack Success Rate (ASR) and Benign Answering Rate (BAR) as our main comparison metric following the prior work (Cao et al., 2024). The ASR refers to the ratio of jailbreak prompts $f$ successfully bypasses the defense mechanism to the total number of inputs $m$. If the model does not respond to the jailbreak prompt with a refusal answer but with a meaningful response, we consider it a successful jailbreak. The BAR is the ratio of the number of non-malicious inputs $s$ that successfully navigate through the defense filter to the total benign prompts $t$. We also calculate the average ASR Reduction Rate for these two models, demonstrating the generalizability of defense methods. Our defense goal is to reduce the ASR while preserving the LLM's usability by maintaining a high BAR."}, {"title": "5.2 Experimental Results", "content": "We present the ASR, Average ASR, BAR, and Average ASR Reduction Rate for Llama and Vicuna in Table 1. Our results show that EEG-Defender can mitigate about 85% of ASR while maintaining a high BAR. In contrast, prompt-based defense methods (e.g., PPL, ICD, Self-Reminder) significantly degrade the utility of the Llama2 model, limiting their applicability. Conversely, decoding-based methods preserve the model's utility but are less effective in defending the Llama2 model. Overall, EEG-Defender maintains a high BAR across both well-aligned models and significantly reduces ASR compared to other methods.\nWe defer the experiments on the Guanaco model in Appendix B, and the result is provided in Table 5. The computation process for the computational budget is detailed in Appendix D. Additionally, we assess the transferability of EEG-Defender by swapping the prototype classifiers of Llama and Vicuna. We also conduct classification experiments on the toxic-chat test dataset, with the results presented in Table 4. It is worth noting that even without fine-tuning, the classification result of EEG-Defender with Llama2 outperforms all state-of-the-art harmful content detection methods in terms of F1-score."}, {"title": "5.3 Analysis", "content": "In this section, we first analyze the results of various decoding-based defense methods. Next, we explore the sensitivity of hyper-parameters through an experiment conducted on Vicuna, with the results presented in Figure 4. Finally, we evaluate the effectiveness of selecting different prototypes.\nAnalysis on Decoding-based Methods. We observe that decoding-based defense methods perform well in terms of BAR for Llama and ASR for Vicuna, but not as effectively for ASR in Llama and BAR in Vicuna. This intriguing result may be attributed to the characteristics of the output embedding space. As shown in Figure 1a, Llama's benign and harmful embeddings, depicted in the two-dimensional PCA plot, are more diverse than those of Vicuna in the last layer. Consequently, increasing the rejection probability (e.g., SafeDecoding) or sampling multiple times with random dropout (e.g., RA-LLM) makes it less likely for benign prompts to produce rejection responses, resulting in better BAR performance for Llama compared to Vicuna. Additionally, the jailbreak prompts in Llama are more varied and less aligned with the decision boundary, making them less likely to be rejected if they are close to benign prompt centers. We believe that the challenge in balancing BAR and ASR with existing decoding-based methods is due to their heavy reliance on final layer embeddings, which neglect the early and intermediate layers of LLMs. In contrast, EEG-Defender focuses on shallow layer embeddings, allowing for a more effective balance between BAR and ASR.\nAnalysis on Hyper-parameter $\\alpha$. We maintain the BAR of Vicuna at approximately 90% while evaluating the ASR of jailbreak prompts. We observe that ASR initially decreases and then increases as the hyperparameter $\\alpha$ increases. Notably, when the classifier trained on the final layer is included ($\\alpha = 1$), the average ASR increases by 5% compared to $\\alpha = 0.75$. This observation aligns with our findings in Figure 1a and 2, where jailbreak embeddings in the final layer are closer to benign prompts, and later layer classifiers exhibit lower accuracy. Despite this, EEG-Defender is not highly sensitive to $\\alpha$, as ASR decreases significantly with our defense, regardless of the $\\alpha$ value.\nAnalysis on Hyper-parameter $t$. We analyze the impact of the parameter $t$, which controls the strictness of EEG-Defender, with $\\alpha$ fixed at 0.75 in the experiment. As the harmfulness score increases, both BAR and ASR rise. Once a certain threshold is surpassed, the rate of increase in BAR slows, while the rate of increase in ASR accelerates. This may suggest that the optimal value for $t$ has been reached for EEG-Defender.\nAnalysis on Impact of Prototype. The selection of prototypes also impacts defense performance. To simplify the experiment and illustrate the effect of prototypes on defense efficacy, we omit the classification of prompts into rejection and jailbreak categories when constructing the prompt pools $B$ and $R$. Instead, we use the original prompt pool $P$ to construct classifiers. This version is referred to as EEG-JPS (Jailbreak Prompt Simplified). As shown in Table 2, EEG-JPS performs less effectively in both ASR and BAR than EEG-Defender. This is likely because including jailbreak prompts in the prompt pool may shift the center of the harmful prototype closer to the benign one, potentially making it more challenging to distinguish between the two categories."}, {"title": "6 Conclusion", "content": "In this paper, we introduced EEG-Defender, a simple yet effective framework for defending against jailbreak attacks. Drawing inspiration from the human-like generation process of language models, we investigated the mechanism behind jailbreaking. Our experiments revealed that in shallow transformer layers, jailbreak prompt embeddings are closer to those of harmful prompts, but as layer depth increases, these embeddings shift toward benign ones. These insights led to the development of a more robust defense mechanism against jailbreaking through early exit generation. Our results show that EEG-Defender reduces the ASR of jailbreak methods by approximately 85%, compared to 50% for current SOTAs, with minimal impact on the utility and effectiveness of LLMs."}, {"title": "7 Limitation", "content": "Scope of application of EEG-Defender. This work primarily focuses on existing single-turn jailbreak attack methods. However, multi-turn jailbreak attacks may become more prevalent in the future, and we have not yet evaluated these in multi-turn conversations. Additionally, we will focus on developing defense mechanisms for Multi-Modal LLMs (MLLMs), as existing defending methods for these models are inadequate (Luo et al., 2024).\nPerformance of EEG-Defender. For certain attack methods, our results are not as significant as others (e.g., GCG for Vicuna and Pair for Llama). Although the BAR decrease rate for the model is better than other defense methods, there is still some impact on the original functionality. Future work could explore additional strategies, such as random erasing and rephrasing, to further strengthen the safety barrier."}, {"title": "8 Ethical Impact", "content": "We emphasize that EEG-Defender can be developed using only publicly available jailbreak attack prompts, without the need to create new attack methods. We demonstrate that some jailbreak prompts for LLMs contain harmful sentences but do not include original inappropriate responses from the LLMs. We will release the code and demonstrations to support future red-teaming efforts and prevent misuse. Additionally, we will continue to investigate and develop improved defense mechanisms to counteract jailbreak attacks."}]}