{"title": "Forecasting VIX using interpretable Kolmogorov-Arnold networks", "authors": ["So-Yoon Cho", "Sungchul Lee", "Hyun-Gyoon Kim"], "abstract": "This paper presents the use of Kolmogorov-Arnold Networks (KANs) for forecasting the CBOE Volatility Index (VIX). Unlike traditional MLP-based neural networks that are often criticized for their black-box nature, KAN offers an interpretable approach via learnable spline-based activation functions and symbolification. Based on a parsimonious architecture with symbolic functions, KAN expresses a forecast of the VIX as a closed-form in terms of explanatory variables, and provide interpretable insights into key characteristics of the VIX, including mean reversion and the leverage effect. Through in-depth empirical analysis across multiple datasets and periods, we show that KANs achieve competitive forecasting performance while requiring significantly fewer parameters compared to MLP-based neural network models. Our findings demonstrate the capacity and potential of KAN as an interpretable financial time-series forecasting method.", "sections": [{"title": "1. Introduction", "content": "Deep learning based on the universal approximation theorem and implemented via multi-layer perceptrons (MLPs) has substantially improved over the past few decades and has been applied across a wide range of fields and industries. This progress can be attributed to its advantages in, for example, feature extraction and representation [1], function approximation [2], and forecasting performance [3]. However, in finance where interpretability is crucial for decision-making, the black-box nature of MLPs often makes decision-makers hesitant to fully adopting deep learning. In particular, for financial time-series forecasting, the lack of interpretability raises concerns about their robustness to future market dynamics.\nIn financial time-series data, volatility is regarded as one of the most important measures along with returns. Among the various measures of volatility, the Chicago Board Options Exchange (CBOE) Volatility Index (VIX) represents an important indicator of expected market fluctuation. The VIX, often referred to as the \"fear index\", reflects the market's expectations of volatility for the S&P 500 index over the next 30 days. It is calculated by combining the weighted prices of S&P 500 index call and put options across various strike prices, thereby providing a forward-looking indicator of market uncertainty. Unlike historical volatility measures, which rely entirely on past data, options-implied volatility measures such as the VIX incorporate the forward-looking views of option market participants. As a result, the VIX often provides practitioners with a more valuable perspective on market risk than historical backward-looking metrics.\nBeyond reflecting the market's expectation of volatility, the VIX also serves as the underlying asset for various tradable products following the launch of VIX derivatives such as VIX futures and VIX options in the 2000s. Notably,"}, {"title": "2. Kolmogorov-Arnold Networks: Representation, Structure, and Training", "content": "The remainder of this paper is organized as follows. Section 2 reviews the Kolmogorov-Arnold representation theorem and describes the structure and training procedure of KANs. Section 3 analyzes empirical results based on the interpretability of KAN and evaluates its forecasting performances through comparisons with traditional time-series models and MLP-based neural networks. Section 4 concludes the paper."}, {"title": "2.1. Kolmogorov-Arnold Representation Theorem", "content": "The Kolmogorov-Arnold representation theorem [27, 28] provides a foundational framework for expressing complex multivariate functions through compositions of univariate functions and additive operations. Formally, the theorem states that any continuous multivariable function $f: [0, 1]^n \\rightarrow R$ can be represented as a finite sum of compositions of continuous univariate functions:\n$$f(x) = f(x_1, x_2,..., x_n) = \\sum_{q=1}^{2n+1} \\Phi_q(\\sum_{p=1}^{n} \\phi_{q,p}(x_p))$$ \nwhere $\\phi_{q,p}: [0, 1] \\rightarrow R$ and $\\Phi_q : R \\rightarrow R$ are continuous univariate functions. Note that these functions may exhibit non-smooth or fractal-like behavior.\nThis theorem is often regarded as highly promising in the field of deep learning, as it reduces the challenge of approximating multivariable functions to the optimization of univariate functions. Nevertheless, its original formulation has certain limitations, notably a shallow two-layer structure and restricted flexibility in the intermediate nodes."}, {"title": "2.2. Kolmogorov-Arnold Networks", "content": "KANs are neural networks inspired by the Kolmogorov-Arnold representation theorem. They address the constraints of the original design-which is limited to two layers and 2n + 1 nodes in the middle layer- by stacking multiple KAN layers, each with an arbitrary number of nodes. Formally, a single KAN layer is defined as\n$$\\Phi = {\\phi_{q,p}}, q = 1, 2, ..., N_{in}, p = 1, 2, ..., N_{out},$$ \nwhere the functions $\\phi_{q,p}$ are univariate trainable activation functions, and $n_{in}$ and $n_{out}$ denote the dimensions of its inputs and outputs, respectively. Given an $n_{in}$-dimensional input vector $x = (x_1, x_2,..., x_{n_{in}})$, the output of this KAN layer is defined by\n$$\\Phi(x) = (\\sum_{q=1}^{N_{in}}\\phi_{q,1}(x_q), \\sum_{q=1}^{N_{in}}\\phi_{q,2}(x_q), ..., \\sum_{q=1}^{N_{in}}\\phi_{q, N_{out}}(x_q)).$$ \nThen a generalized KAN with L layers is expressed as:\n$$KAN(x) = (\\Phi^{(L-1)} \\circ ... \\circ \\Phi^{(0)})(x),$$ \nwhere each $\\Phi^{(l)} := {\\phi_{q,p}^{(l)}}$ represents the l-th KAN layer. Notably, the structure derived from the Kolmogorov-Arnold representation theorem can be seen as a special case of a KAN with two layers and 2n + 1 nodes in the middle layer. By allowing arbitrary depths and widths, KANs substantially extend this framework's flexibility.\nActivation functions of KANs. Conventional MLP-based neural networks typically employ a single activation function (e.g., ReLU, Sigmoid) shared across all nodes after a global linear transformation. In contrast, KANs introduce residual activation functions that combine a smooth basis function b(x) with a B-spline component. Specifically, each node's activation function in KAN is given by\n$$\\phi(x) = w_b b(x) + w_s spline(x),$$"}, {"title": "3. VIX Forecasting using KAN", "content": "In this paper, we focus on forecasting the CBOE VIX using KANs. Since the VIX calculation method was revised in September 20032, we employ the VIX data from January 2004 to December 2023 to ensure intertemporal consistency. Alongside the VIX data, we also utilize the S&P 500 daily excess returns over the same period, using the 1-month U.S. Treasury bill yield obtained from the Federal Reserve Economic Data (FRED) as the risk-free rate. To align the scale with the VIX, the S&P 500 daily excess returns are multiplied by 100.\nWe construct three datasets from historical VIX data to use as explanatory variables, each designed to examine whether particular characteristics of volatility can be observed. The first dataset uses five consecutive lagged values, {$V_{t-1}, V_{t-2}, V_{t-3}, V_{t-4}, V_{t-5}$}, to predict the next VIX value $V_t$. By focusing on the preceding five trading days, this dataset highlights short-term momentum. The second dataset consists of non-uniform lags, {$V_{t-1}, V_{t-5}, V_{t-10}, V_{t-21}$}, to investigate whether the short- (one day), medium- (one or two weeks), or long- (one month) range dependencies remain and provide predictability. Finally, the third dataset comprises the previous VIX value, $V_{t-1}$, with weekly average $V^w$, monthly average $V^m$, and quarterly average $V^q$, defined by\n$$V_t^w = \\frac{1}{5} \\sum_{i=1}^{5} V_{t-i}, V_t^m = \\frac{1}{21} \\sum_{i=1}^{21} V_{t-i}, V_t^q = \\frac{1}{63} \\sum_{i=1}^{63} V_{t-i},$$ \nrespectively. We use this aggregated-statistics approach to determine whether the mean-reverting property of volatility across multiple horizons can be captured and, if so, to clarify the effects of short-, medium-, long-, and very long-term averages on volatility dynamics. For convenience, we denote these datasets as Dataset 1, Dataset 2, and Dataset 3, respectively, in the remainder of this paper.\nTo ensure the robustness of our experiments, we vary the proportions of training, validation, and test sets across three different configurations. Period 1 allocates the data from 2004 to 2015 for training, 2016\u20132017 for validation, and 2018-2023 for testing, corresponding to a 6:1:3 ratio. Period 2 applies a 7:1:2 ratio, and Period 3 uses 8:1:1. Each of the three datasets is tested over all three periods, resulting in nine total experimental setups. We denote a setup with Dataset i and Period j by $D_iP_j$, where $i \\in {1, 2, 3}$ and $j\\in {1, 2, 3}$."}, {"title": "3.2. Empirical Results", "content": "Before examining our empirical findings in detail, we describe the hyperparameters used for KAN in this study. All experiments, across every dataset and period, are conducted with the same hyperparameter settings. Specifically, we use a two-layer KAN with two hidden nodes in the hidden layer. We adopt this parsimonious configuration because increasing the depth or the number of hidden nodes does not lead to any significant improvement or even any noticeable difference after the pruning and symbolification steps. The activation functions include B-spline components of order k = 3 with a grid size of 3. For optimization, we employ the L-BFGS algorithm with a learning rate of 0.04. If the validation loss fails to improve for five consecutive epochs, we reduce the learning rate by a factor of 0.1; if it does not improve for ten consecutive epochs, we stop training activation functions and prune unimportant edges and nodes. We then replace the activation functions with symbolic functions that incorporate learnable affine parameters. Finally, these affine parameters are trained for 30 epochs with a learning rate of 0.0004. The regularization coefficient A for sparsification is set to zero during training, as unimportant edges and nodes are pruned effectively even without the help of this regularization.\nFor comparison, we employ both statistical time-series models and MLP-based neural networks as benchmark methods. The statistical models consist of a naive forward-filling method, ARMA and ARIMA approaches, and two"}, {"title": "3.2.2. Analysis of KAN forecasting results", "content": "We begin by investigating the results of the trained KANs and the effects of symbolification without yet comparing them to other benchmark models.\nA notable observation from the top-row figures is that, in all cases including other periods in Appendix B, KAN's B-spline-based activation functions are trained to exhibit nearly linear shapes. Recall that KAN's activation functions (the black curves in the top-row figures) are designed to approximate arbitrary univariate continuous functions via splines. However, under the configurations and explanatory variables used in this study, linear forms consistently emerge. This suggests that, for predicting the VIX, every explanatory variable we tested primarily exerts linear influence on forecast performance. It also aligns with prior research indicating that simpler and more parsimonious models often suffice to capture essential characteristics of noisy financial time-series data [40, 8, 41]. Moverover, this observation indirectly supports the linearity assumptions commonly made by statistical time-series models such as ARMA, ARIMA, and HAR, while simultaneously highlighting KAN's distinctive capability. Whereas these statistical models presuppose linearity, KAN identifies such structures directly from data, imposing no prior assumptions on its functional form. This advantage allows researchers to uncover data-driven insights."}, {"title": "3.2.3. Statistical assessment of KAN as a forecasting model", "content": "To examine the statistical properties and significance of our fitted prediction model, we employ two tests commonly used in financial time-series forecasting-the Mincer-Zarnowitz test [44] and the Durbin-Watson test [45, 46]- and present the results in Table 3. The Mincer-Zarnowitz approach examines whether a forecast is unbiased by regressing\n$$V_t = a + \\beta \\hat{V_t} + u_t,$$ \nfor residual $u_t$, and constant $a$ and $\\beta$, and testing whether $\\alpha = 0$ and $\\beta = 1$ [44]. In our setting, the F-statistics remain well below the usual critical thresholds, and p-values consistently exceed conventional significance levels (e.g., 0.10), indicating insufficient evidence to reject the null hypothesis ($\\alpha, \\beta$) = (0, 1). In other words, there is no strong indication that $\\alpha \\neq 0$ or $\\beta \\neq 1$. Meanwhile, the Durbin-Watson statistic ranges between approximately 1.799 and 2.180 for all datasets and forecast periods. This statistic typically falls between 0 and 4, with values near 2 indicating no first-order autocorrelation. Empirical studies often use 1.5-2.5 as a practical threshold range for concluding that no substantial first-order serial correlation remains. In our results, the statistic falls comfortably within that interval, suggesting no problematic first-order autocorrelation in the residuals. Overall, these results suggest that the model's forecasts do not deviate significantly from unbiasedness, nor do the residuals exhibit problematic autocorrelation."}, {"title": "3.2.4. Capturing the leverage effect", "content": "Having established that KAN effectively replicates volatility clustering and mean-reverting behavior, we next examine its capacity to capture the leverage effect. In particular, we aim to determine whether excess returns of the underlying asset (S&P 500) help explain remaining variability in VIX forecasts once past VIX data have been accounted for. To this end, we consider the augmented model:\n$$V_t = \\hat{V_t} + e_t, $$\nHere, $\\hat{V_t} := KAN(V_{t}, R_{t-1})$.\nwhere $\\hat{V_t}$ is the KAN-based forecast derived exclusively from past VIX observations using Datasets 1\u20133, $R_{t-1}$ is the excess return of the underlying asset at t \u2212 1, and $e_t$ is a residual term associated with $\\hat{V_t}$.\nUnder the hypothesis that the KAN models in Section 3.2.2 are trained optimally on their given explanatory variables (i.e., $\\hat{V_t}$ represents the best possible prediction from historical VIX alone), we attempt to explain the remaining variability stemming from the stock market via $R_{t-1}$. If this hypothesis holds, then the KAN model in (3.6) would return $\\hat{V_t}$ unchanged and the effect of the excess return would be expressed through a suitable function during training. In other words, the VIX may be written as\n$$V_t = \\hat{V_t} + e_t = (\\hat{V_t} + \\phi(R_{t-1})) + e_t,$$ \nwhere $\\phi$ is a function KAN learns for excess returns, and $e_t$ remains the irreducible noise. From the perspective of the Mincer-Zarnowitz test results in Section 3.2.3, which did not reject the null hypothesis $\\alpha = 0$ and $\\beta = 1$ in (3.5), this expression suggests that the error term $u_t$ in (3.5) can be decomposed into $\\phi(R_{t-1})$ and the irreducible error $e_t$.\nWe design a straightforward KAN to explore how $R_{t-1}$ affects the remaining forecast error. Specifically, we feed both $\\hat{V_t}$ and $R_{t-1}$ into a one-layer KAN, where each passes through its own B-spline-based activation function to construct $V_t.$"}, {"title": "3.2.5. Comparison with traditional and deep learning benchmark methods", "content": "We compare forecasting performances of KAN and various benchmark methods: traditional statistical time-series models (forward filling, ARMA, ARIMA, HAR), and neural network models (MLP, LSTM). For notational convenience, the neural network model (KAN, MLP, LSTM) trained on Dataset i is denoted as model-D; for i = 1, 2, 3. To ensure a robust and comprehensive assessment of model performance, we employ several metrics: mean squared error (MSE), mean absolute error (MAE), mean absolute percentage error (MAPE), $R^2$, and quasi-likelihood (QLIKE). Their definitions are given by\n$$MSE = \\frac{1}{N_{test}} \\sum_{t=1}^{N_{test}}(V_t - \\hat{V_t})^2, MAE = \\frac{1}{N_{test}} \\sum_{t=1}^{N_{test}}|V_t - \\hat{V_t}|, MAPE = \\frac{1}{N_{test}} \\sum_{t=1}^{N_{test}}\\frac{|V_t - \\hat{V_t}|}{V_t} \\times 100,$$\n$$R^2 = 1 - \\frac{\\sum_{t=1}^{N_{test}} (V_t - \\hat{V_t})^2}{\\sum_{t=1}^{N_{test}} (V_t - \\bar{V})^2}, QLIKE = \\frac{1}{N_{test}} \\sum_{t=1}^{N_{test}} [ln(\\frac{\\hat{V_t}}{V_t}) + (\\frac{V_t}{\\hat{V_t}})-1],$$\nwhere $N_{test}$ is the number of test data points, $\\bar{V}$ denotes the mean of the VIX index in the test period, and $V_t$ and $\\hat{V_t}$ are the actual and forecasted VIX values at time t, respectively.\nTables 5-7 summarize the out-of-sample forecasting accuracy for KAN and various benchmark methods. Across the three Periods, KAN consistently delivers competitive results, often achieving the best or second-best performances. Although the naive forward filling method performs surprisingly well-likely owing to the strong persistence inherent in the VIX-ARMA(1,1) and ARIMA(1,1,1) reveal the limitations of a purely linear framework devoid of longer-range dependence or multiple timescale averages. Meanwhile, the HAR(3) and HAR(4) models, which assume a linear additive structure over short-, medium-, long-, and very-long-term averages, generally achieve decent results.\nRegarding neural-network models, MLP and LSTM often demonstrate strong predictive power. However, they require approximately 300 or 1,000 times more parameters compared to KAN while offering comparable accuracy. Moreover, a closer look at the metrics suggests that KAN tends to excel in MSE rather than MAE or MAPE. This indicates that KAN produces fewer extreme errors or outliers. Remarkably, KAN's parsimonious design consistently achieves performance on par with these more parameter-intensive MLP-based networks, underscoring its robustness and stability in noisy financial time-series data.\nTurning to the different dataset configurations, Dataset 3 typically yields the best results for KAN, MLP, LSTM, and HAR models alike. Notably, KAN-D3 demonstrates particularly strong accuracy in all three periods. This suggests that effectively capturing key characteristics of the VIX, such as mean-reversion and high-persistence, leads to reliable forecasting results. Models using Dataset 1 also perform reasonably well. The decent results when using Dataset 1 are presumably because consecutive lags capture crucial short-term dependencies (e.g., volatility clustering, momentum). By contrast, Dataset 2 often fails to provide significant multi-horizon features.\nTaken together, these findings confirm that KAN offers a robust and interpretable alternative to both classical time-series approaches and parameter-intensive MLP-based neural networks. This illustrates that a parsimonious structure can yield high forecasting accuracy in noisy financial time-series such as the VIX."}, {"title": "4. Conclusion", "content": "MLP-based neural networks have exhibited remarkable forecasting performances in numerous studies, but their black-box nature often restricts their practical use in financial time-series forecasting such as the VIX. In this study, we employ Kolmogorov-Arnold Networks (KANs) to address these limitations. Each edge in KAN extracts important features from the data and describes them with symbolic functions, enabling the output of the network to be expressed as a closed-form in terms of explanatory variables. This interpretability allows us to gain valuable insights into the VIX's inherent characteristics such as mean reversion and the leverage effect. Furthermore, our empirical analysis across multiple periods and datasets indicates that KAN achieves robust and competitive accuracy compared to MLPs and LSTMs while requiring significantly fewer parameters and maintaining interpretability. These findings suggest that KAN can serve as a powerful and parsimonious framework for analyzing noisy financial time-series data, including the VIX, and may be extended to broader contexts where clear, data-driven insights are essential."}, {"title": "Appendix A. Training results for deep and wide KAN", "content": "In this paper, all experiments were conducted with a two-layer KAN using two nodes in the hidden layer. This section examines the impact of increasing either the depth or the number of nodes. Specifically, we analyze two cases: (1) a two-layer KAN with 10 hidden nodes, and (2) a three-layer KAN with 4 hidden nodes each. The training results are presented in Figure A.1. While depth and width can be adjusted to various values, the results remain consistent and are therefore omitted. As shown in the figure, even with increased width or depth, the learned activation functions for each layer predominantly exhibit linear characteristics. The actual number of active nodes in the figure may appear smaller (e.g., 2 or 6) than the predefined configuration with a larger number of nodes (e.g., 10). This is because, during training, unimportant nodes are pruned as part of the optimization process, leaving only significant nodes to contribute to the final model. All these activation functions are replaced with linear functions $y = x$ or $y = -x$ at the symbolification step, and summed at the node level. Consequently, the outcomes of each layer become a linear combination of input variables. Even after passing through multiple layers, the result remains a linear combination. Thus, we chose the simplest configuration, [ni, 2, 1], where n\u2081 = 4 for Dataset 1 and n\u2081 = 5 for Dataset 2 and Dataset 3, as the structure of the KAN model."}, {"title": "Appendix B. KAN training result before and after symbolification in Period 1 and Period 2", "content": "In Section 3.2.2, only the training results of KANs in Period 3 are presented. The results in Period 1 and Period 2 are illustrated in Figure B.2 and Figure B.3, respectively. As shown in the figures, it is evident that the trained activation functions before symbolification exhibit a linear form across all Datasets and Periods."}]}