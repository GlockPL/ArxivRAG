{"title": "Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review", "authors": ["Andrii Zahorodnii", "Jasper J.F. van den Bosch", "Ian Charest", "Christopher Summerfield", "and Ila R. Fiete"], "abstract": "This study proposes a data-driven framework for enhancing the accuracy and efficiency of scientific peer review through an open, bottom-up process that estimates reviewer quality. Traditional closed peer review systems, while essential for quality control, are often slow, costly, and subject to biases that can impede scientific progress. Here, we introduce a method that evaluates individual reviewer reliability by quantifying agreement with community consensus scores and applying Bayesian weighting to refine paper quality assessments. We analyze open peer review data from two major scientific conferences, and demonstrate that reviewer-specific quality scores significantly improve the reliability of paper quality estimation. Perhaps surprisingly, we find that reviewer quality scores are unrelated to authorship quality. Our model incorporates incentive structures to recognize high-quality reviewers and encourage broader coverage of submitted papers, thereby mitigating the common \u201crich-get-richer\" pitfall of social media. These findings suggest that open peer review, with mechanisms for estimating and incentivizing reviewer quality, offers a scalable and equitable alternative for scientific publishing, with potential to enhance the speed, fairness, and transparency of the peer review process.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern technologies have empowered the sharing of information at scale, as well as commentary and feedback on the shared content. However, distilling this collective feedback into a reliable collective assessment of shared information has remained a thorny challenge. The well-known and ubiquitous problem of mis- and dis-information on social media platforms is a testament to this difficulty (Vosoughi et al., 2018; Kitchens et al., 2020).\nScientific communication, the primary focus of the present work, suffers from related problems. Partially for historical reasons (Birukou et al., 2011) and partly to head off the kinds of misinformation problems found in open media, scientific review is a closed, slow, top-down, expensive process with incentives for high-profile publishing that might also distort science (Noorden, 2013; Young et al., 2008; Buranyi, 2017; da Silva and Dobr\u00e1nszki, 2014). A scientific report is submitted to one journal, whose editors select whether to desk-reject it or send it out for review to a small number of hand-picked reviewers. The resulting reviews emanate from a small number of reviewers and are thus highly stochastic. If the paper is rejected, the process repeats until the paper is accepted at some journal. This friction-filled process slows scientific progress in a way that can impede junior scientists' careers. The exponentially growing volume of scientific output makes these challenges more daunting. The concept of \"publish then review\", with bottom-up review, offers a potential antidote (Eisen et al., 2020; Ginsparg, 1997; Eisen, 2016; Kravitz and Baker, 2011; Kriegeskorte et al., 2012; Nosek and Bar-Anan, 2012; Stern and"}, {"title": "2 RESULTS", "content": "2.1 High variability: A core challenge for collective paper review\nWe consider data from two conferences that employed a transparent review process and used explicit scoring for determining paper acceptance decisions: ICLR 2023 and the Conference on Cognitive Computational Neuroscience (CCN) 2023. In the CCN2023 peer review process, each of the 589 reviewers assigned \"Impact\" and \"Clarity\" numerical scores to the 527 submitted abstracts. All reviewers were authors of submitted abstracts. Each abstract received an average of 9.1 reviews. In ICLR2023, each of the 3798 submitted papers received anywhere from 2 to 9 reviews by reviewers selected through a separate track, with an average of 3.8 reviews per submission. Each review consisted of multiple dimensions of scores, with the overall rating and confidence scores (used in analyses in this paper).\nIn both datasets, the correlation in scores given by pairs of reviewers to the same paper was surprisingly low (Figure 1). The correlation was very weak in the community-based review process of CCN (r =\n0.161 \u00b10.014; Figure 1A). Though somewhat higher for ICLR2023 (r = 0.361 \u00b10.014; Figure 1C), the correlations were still small despite the fact that the submissions being assessed were full-length standalone papers. Common possible causes of low correlation, such as variations in what range of scores each reviewer actually utilizes, are sometimes addressed by score normalization techniques (e.g., z-scoring, ranking, mean removal, or distribution inversion). Applying any of these methods only marginally increased the correlation (r < 0.19 for CCN2023; Figure 1B). Although the correlation increased with the self-reported confidence scores in ICLR2023 data (from r = 0.24 for confidence=2 up to r = 0.49\nfor confidence=5; Figure 1D), only 10.0% of reviewers reported having this high confidence. Moreover, the confidence of ratings was inversely correlated with the ratings scores (Supplementary Figure S1), implying that on average, better papers receive less confident reviews. In sum, our analysis reveals a striking lack of agreement among reviewers when assessing the same paper."}, {"title": "2.2 Bayesian paper quality estimation using reviewer quality estimatates", "content": "We first consider a very simple model (Figure 2; a slightly richer model is presented below). Assume that each submission (paper) j has a hidden ground truth quality qj. Reviewers are assumed to give scores to that paper drawn from a distribution centered at qj, with a user-specific standard deviation 6\u00a1, where i is the index of the reviewer (Figure 2A). We can think of the inverse standard deviation of a reviewer as a ground-truth reviewer quality measure. The goal is to obtain good estimates of qj without knowledge of the paper ground truth values. This simple model leaves out possible systematic reviewer biases (see Shah et al. (2017); Helmer et al. (2017); Ross (2017); Stelmakh et al. (2019); Goldberg et al. (2024);\nStelmakh et al. (2023) for discussion of biases in the review process), assumes there are no bots or bad actors (we add bots below), and doesn't constrain the scores to lie in a bounded interval, however it suffices to demonstrate the core idea. Traditionally, the scores of all reviewers are averaged to obtain a final estimated quality score of the paper, on which decisions are made. This calculation produces an estimator of the paper quality score with mean squared deviation (MSD) equal to\n$MSD(simple mean) = \\frac{\\sum \\sigma_i^2}{n^2}$ (1)"}, {"title": "2.3 Assessing the Bayesian approach in CCN 2023 review data", "content": "The above result motivates applying a Bayesian weighting of paper reviews by estimated reviewer quality in a real world review process. To assess outcomes, we again turn to the CCN2023 process (Figure 3). In the experiments that follow, we used the CCN2023 \"Impact\" scores.\nUnfortunately, in such real world datasets we lack ground truth knowledge of paper quality to assess the robustness of different measures. To construct a proxy, we assume that the full community average score (CAS) for a paper is the ground truth. The CAS represents the score that would be assigned if many scientists in the community were to review the paper. With an estimate for individual reviewer quality, we can assess the efficacy of different metrics on subsampled sets of reviews for each paper.\nCCN2023 data contained unique reviewer IDs for each reviewer, across papers, thus we were able to compute reviewer standard deviations relative to the CAS and generate an estimate of reviewer quality. Unlike in our simple model, scientists in CCN2023 play two roles: they are reviewers and authors. A quality value can be assigned to each role. Given the sparsity of the data (each individual writes relatively few reviews and each paper is reviewed by relatively few reviewers), a natural question is whether quality as an author is related to quality as a reviewer and thus whether it might be possible to arrive at a better per-individual reviewership score by combining both.\nIs authorship quality predictive of reviewer quality? We considered the mean squared deviation of an individual's scores from the CAS each paper reviewed by that individual (i.e., their quality as a reviewer), and compared it with the CAS they received for their own submissions (i.e., their quality as an author)."}, {"title": "2.4 How things could be: an open framework for peer review", "content": "Based on the findings above, we propose a new open framework for scientific peer review (Figure 4) and assess its soundness in a computational model. In this framework, all papers are immediately published (Eisen et al., 2020). Post-publication, users on the platform self-select themselves to review the paper. In this framework, users may give quality ratings not only to papers but also to others' reviews (Walther and van den Bosch, 2012). The quality of the given publication is estimated from these reviews and the ratings of the reviews. Authors of new submissions, reviewers, and raters of reviewers all come from the same user pool.\nGenerative model We implement a generative model of this process (Figure 4A): The model consists of a set of papers and a set of reviewers. Each generated paper i has a hidden ground truth quality $q_i \\in (0, 1)$. Each reviewer has a ground truth reviewer quality $p_i \\in (0,1)$. Whenever reviewer i writes a review for paper j, they assign it a score $\\sim N(q_j, a/p_i)$: in other words, the score is based on the intrinsic paper quality, with a standard deviation that is inversely proportional to reviewer quality. Whenever user i rates a review of another user j, they assign a score to them which is $\\sim N(p_j, a/p_i)$. We set a = 0.18 to match the reviewer correlation levels to those we found in CCN2023 data. We set the standard deviations of the review scores and paper scores to be equal, based on the finding from a NeurIPS 2022 randomized control trial (Goldberg et al., 2024) that disagreement between reviewers assessing review quality are comparable to the disagreement rates of paper reviewers. Varying a does not change the results qualitatively. In addition, we consider the existence of consistently low-quality, unreliable reviewers or \u201cbots\". These unreliable users are modeled as assigning paper review scores and ratings of other reviews uniformly at random in[0, 1], regardless of paper or review quality.\nAll assigned scores must lie in the interval [0, 1], and user-assigned scores are the only data available for the system to derive its quality estimators. As a worst-case conservative scenario, we consider a batch/offnline process, in which users have provided reviews of papers and other users, and we must"}, {"title": "3 DISCUSSION", "content": "Summary We evaluated two real-world peer-review datasets to determine how well reviewer scores correlate with each other, and found very low levels of agreement between reviewers. In one of these datasets, we made an estimate of paper quality for a subset of papers that had a large number of reviews based on the community average score, and used this as a metric for assessing both authorship quality and reviewer quality for each individual reviewer. We found that the authorship quality of an individual is not predictive of their reviewer quality.\nGiven these quantified challenges in extracting meaningful paper quality scores from small numbers of noisy reviewers, we proposed and assessed a Bayesian weighting of reviewer scores based on empirically estimating reviewer quality. In the low number of reviews per paper regime, the proposed measure significantly outperforms standard averaging methods. We mitigated the problem of estimating reviewer quality given the small numbers of reviews written by each individual with a method for binned estimation of reviewer quality.\nFinally, we showed that generating reviewer scores can incentivize reviewers to produce high-quality reviews in two ways: a desire for impact (a reviewer's assessment of a paper is weighted more heavily if they have a higher reviewer score), and a desire for recognition (a reviewer's work can be visibly"}]}