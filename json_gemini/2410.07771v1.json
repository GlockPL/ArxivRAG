{"title": "Full-Rank No More: Low-Rank Weight Training for Modern Speech Recognition Models", "authors": ["Adriana Fernandez-Lopez", "Shiwei Liu", "Lu Yin", "Stavros Petridis", "Maja Pantic"], "abstract": "This paper investigates the under-explored area of low-rank weight training for large-scale Conformer-based speech recognition models from scratch. Our study demonstrates the viability of this training paradigm for such models, yielding several notable findings. Firstly, we discover that applying a low-rank structure exclusively to the attention modules can unexpectedly enhance performance, even with a significant rank reduction of 12%. In contrast, feed-forward layers present greater challenges, as they begin to exhibit performance degradation with a moderate 50% rank reduction. Furthermore, we find that both initialization and layer-wise rank assignment play critical roles in successful low-rank training. Specifically, employing SVD initialization and linear layer-wise rank mapping significantly boosts the efficacy of low-rank weight training. Building on these insights, we introduce the Low-Rank Speech Model from Scratch (LR-SMS), an approach that achieves performance parity with full-rank training while delivering substantial reductions in parameters count (by at least 2\u00d7), and training time speedups (by 1.3\u00d7 for ASR and 1.15\u00d7 for AVSR).", "sections": [{"title": "I. INTRODUCTION", "content": "Training large machine learning (ML) models for computer vision, natural language processing, and speech recognition has become increasingly challenging due to the exponential growth in the number of parameters, scaling from millions [1] to billions [2], [3], and even reaching trillions [4]. To expedite the training process and alleviate memory constraints, researchers have explored numerous techniques aimed at reducing the parameter count of modern neural networks.\nOne avenue of research has focused on the design of resource-efficient networks, exemplified by models such as MobileNet [5] and EfficientNet [6]. While these models reduce size, they often come at the expense of performance. Another widely adopted approach is network pruning, which seeks to eliminate redundant or less significant components within a model. Unstructured pruning, for example, removes individual weights and achieves high compression rates, but is not hardware-friendly [7]\u2013[9]. Conversely, structured pruning, which removes entire structures such as channels or attention heads, offers real speedups on common hardware, though it typically achieves lower compression ratios [10].\nRecently, low-rank matrix factorization has garnered significant attention, particularly with the advent of Low-Rank Adaptors (LoRA) [11]. This approach dramatically reduces memory usage and computation time, making it highly effective for parameter-efficient fine-tuning of large language models (LLMs). Following this development, the ML community has increasingly focused on low-rank fine-tuning, leading to the emergence of several enhanced methods [12]\u2013[16]. Given that base models are pre-trained on massive corpora, it is unsurprising that a few low-rank adaptors can yield satisfactory fine-tuning performance.\nHowever, when it comes to training large-scale neural networks from scratch, directly applying low-rank structures often compromises performance [17]. Some degree of full-rank training appears necessary to achieve comparable performance, as evidenced by approaches that employ short full-rank \"warm-up\" phases before transitioning to low-rank training [17]\u2013[20], or strategies that maintain full-rank weight matrices while applying low-rank gradients [21]\u2013[23]. Despite these advancements, it remains unclear whether modern networks can be trained directly with low-rank weights from scratch, without compromising performance, and while retaining both parameter and memory efficiency. Additionally, the potential of low-rank architectures has been underexplored in the context of speech recognition, where prior efforts have been confined to relatively small model sizes, typically under 100 million parameters [24]\u2013[28]. The efficacy of low-rank training in large-scale speech recognition models remains largely unexamined, especially with Conformer-based models [29], [30].\nTo address this gap, we redirect our attention from pursuing state-of-the-art performance to systematically investigate low-rank training from scratch for large-scale Automatic Speech Recognition (ASR) and Audio-Visual Speech Recognition (AVSR) models. Our study begins with an in-depth analysis of low-rank emergence during full-rank weight training, revealing that the entire model gradually learns a low-rank structure during training, and some layers exhibit lower ranks than others, as shown in Figure 1. Encouraged by these findings,"}, {"title": "II. METHODOLOGY", "content": "Let us examine a widely used architecture for ASR/AVSR, as described in [29]. This architecture accepts audio or audio-visual data as input and generates the corresponding grapheme transcription. We'll focus on ASR for illustration purposes, but the concepts can be applied to any speech model. Figure 2-(a) illustrates an ASR model consisting of a ResNet frontend, a Conformer encoder and a Transformer decoder that is jointly trained using Connectionist Temporal Classification (CTC loss) and Cross-Entropy (CE Loss). Our objective is to minimize the model's size and accelerate the training process while preserving its complexity and performance. We choose to low-rank the linear layers in the Conformer encoder and Transformer decoder, which includes the linear projections in both FFNs and MHSA. Illustrations of these parameterized modules are provided in Figures 2-(b) and 2-(d) for the Conformer encoder, but the same applies to the Transformer decoder.\n### A. Simplifying linear layers with low-rank factorization\nA linear layer processes an n-dimensional input vector x and produces an m-dimensional vector z = Wx. Our aim is to factorize the weight matrix \\(W \\in \\mathbb{R}^{m \\times n}\\) into a lower-rank representation, specifically, a product of two matrices \\(U \\in \\mathbb{R}^{m \\times r}\\) and \\(V^T \\in \\mathbb{R}^{r \\times n}\\), where r is much smaller than both m and n. Therefore, the parameterized linear layer is given as \\(z \\approx U(V^Tx)\\) (shown in Fig. 2-(c)). Computation and memory costs are reduced from \\(O(mn)\\) to \\(O(r(m + n))\\).\n### B. Effective initialization\nThe initialization of model parameters is a crucial step in achieving optimal performance, and it requires careful consideration. Glorot et al. [31] and He et al. [32] proposed two widely used initialization methods for FFNs that adhere to two key principles: i) the mean of the activation should be zero; and ii) the variance of the activation should remain consistent across every layer. By ensuring a zero mean and maintaining the variance of the input to each layer, these methods guarantee a stable signal that neither explodes nor vanishes. To achieve low-rank training, it is essential to maintain the variance of the original matrix consistently across each layer after decomposing it into the product of two matrices, i.e., \\(var(W) \\approx var(UV^T)\\). Several attempts have been made to preserve these principles, as discussed in [33]\u2013[35]. We follow [34], who presented spectral initialization as a technique that simulates the behavior of existing initialization. To initialize the model weights, we follow conventional initializations as described in [31] and [32], and then for each factorized layer, we use the spectral SVD initialization scheme [34], as shown in Eq. (1).\n\\(SVD_{r}(W) = \\hat{U} \\Sigma \\hat{V}^T, U = \\hat{U}_{:,r} \\sqrt{\\frac{\\Sigma_{r}}{r}}, V^T = \\sqrt{\\frac{\\Sigma_{r}}{r}} \\hat{V}^T\\) (1)\n### C. Optimizing rank for different layers\nDetermining distinct ranks for different layers is optimal, as layers contribute unequally to the overall model performance [9]. However, identifying the appropriate rank for each layer remains a significant challenge.\nFormally, assuming that our model M contains L layers, we intend to factorize a subset S of them. For each layer \\(l \\in S\\), we assign a scaling factor \\(\\alpha_l \\in [0,1]\\). The rank \\(r_l\\) for each layer is then determined by multiplying the minimum dimension of the weight matrix \\(W_l \\in \\mathbb{R}^{m \\times n}\\) by the scale factor \\(\\alpha_l\\), i.e., \\(r_l = \\alpha_l \\cdot \\text{min}\\{m, n\\}\\), for all \\(l \\in S\\). Smaller \\(\\alpha\\) represents a lower rank.\n1) *Uniform Layer-wise Rank*: Previous works usually assign a uniform rank for all layers without considering layer importance, i.e., \\(\\alpha_l = \\alpha\\), for all \\(l \\in S\\), e.g., [24], [27]. However, given the fact that layers in neural networks are not equally important, this rank assignment may lead to suboptimal performance.\n2) *Linear Layer-wise Rank*: In fact, different layers within a model can exhibit distinct low-rank patterns. To illustrate this, we analyze the emergence of low-rank structures during the training of a Conformer block. As depicted in Fig. 1, the entire model gradually learns a low-rank structure as training progresses, with some layers consistently displaying lower ranks than others. This observation naturally prompts further investigation into the behavior of these layers across varying depths. Figures 3-(a) and 3-(b) present the compression ratios"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Dataset. For ASR, we conduct experiments on two datasets: Librispeech [36] and LRS3 [37]. For Librispeech, we use \u201ctrain-clean-100\u201d, \u201ctrain-clean-360\", and \u201ctrain-other-500\u201d subsets, totalling 960 hours of training data and evaluate our performance on the \u201ctest-clean\u201d set with a total of 5.1 hours of audio. LRS3 consists of 439 hours of video clips, with 118,516 (408 hours), 31,982 (30 hours) and 1,321 (0.9 hours) clips in the pre-training, training-validation, and test sets, respectively. For AVSR, we use the same LRS3 splits for training and test. Additionally, following previous works [9], [29], [38], we use VoxCeleb2 [39] and AVSpeech [40] audio-visual datasets for training, resulting in 1,307 and 1,323 hours. We report results using Word Error Rate (WER).\nFor ASR, we only apply adaptive time masking [41] to the raw audio stream. Specifically, we select a number of masks that is proportional to the length of the utterance and a maximum masking duration of up to 0.4 seconds. For AVSR, in addition to adaptive time masking, we also apply horizontal flipping and random cropping.\n*Pre-processing*. For ASR, raw audio waveforms are used as input to the model and undergo z-normalization per utterance before being fed into the model [42]. For AVSR, in addition, we crop a 96\u00d796 region centered around the mouth and then convert each frame into a greyscale image [41].\n*Model architecture*. Instead of focusing on achieving state-of-the-art performance, our main objective is to explore low-rank training from scratch. To this end, we adapt the open-source architectures presented in [29]. Our ASR model comprises a 1D ResNet front-end, followed by a Conformer encoder, a Transformer decoder and a CTC layer, resulting in 243M parameters. Following [9], our AVSR model comprises a ResNet frontend for audio and video modalities, a multi-layer perceptron for early fusion of multi-domain features, a single Conformer encoder, a Transformer decoder and a CTC layer, resulting in 268.5M parameters.\n*Training details*. The models are trained for 75 epochs using the AdamW optimiser [43]. A constant learning rate with square root cooldown scheduler [44] is used for ASR, while a cosine scheduler is used for AVSR. The peak learning rate is 0.0006/0.001 for ASR/AVSR, 15 epochs of cooldown for ASR and a warm-up of 5 epochs for both. The ASR/AVSR models are trained with 32/64 A100 GPUs, respectively.\""}, {"title": "IV. RESULTS", "content": "Low-Rank Initialization Comparison. Figure 4 illustrates the performance of low-rank ASR models initialized using two different methods: the traditional Kaiming initialization [32] (explored before for relatively small speech models [24], [27]) and SVD initialization (as detailed in Section II-B). In this analysis, uniform low-rank training was applied across all FNNs. The results clearly demonstrate that SVD initialization yields more stable training dynamics and minimizes performance degradation across varying scaling factors \\(\\alpha\\). These findings underline the efficacy of SVD initialization, prompting its adoption in all subsequent experiments.\nLayer Factorization Experiments. In Table I, we present a comparison of low-rank ASR models with different layer factorization strategies. We examine three scenarios: i) only linear layers in MHSA blocks are factorized; ii) only linear layers in FFNs are factorized; iii) both linear layers in MHSA blocks and FFNs are factorized.\nOur experiments yield several notable insights. First, we observe that uniformly reducing the rank of the FFNs results in a significant decline in model's performance. For example, reducing the feature dimension by 50%, which leads to an approximate 20% reduction in model size, results in an absolute increase of 0.41% in WER on LRS3 and 0.26% in WER on Librispeech. This indicates that FFNs are more resistant to low-rank factorization. Second, we find that the linear layers in MHSA blocks can be effectively factorized from scratch with minimal performance degradation. Interestingly, the application of low-rank structures in these layers can even improve performance. For instance, applying a scaling factor of 0.12 to the linear layers in MHSA blocks improved the ASR model's performance compared to the baseline (1.92% WER vs. 1.99% WER on LRS3), and similarly on Librispeech (2.61% vs. 2.66%). Third, applying a uniform low-rank structure to both the MHSA and FFN linear layers led to unstable training, often resulting in exploding gradients and model degradation.\nNote that our objective is to reduce model size while maintaining performance. While factorizing the MHSA layers can slightly enhance performance, it does not yield substantial memory reductions or speed-ups, as the majority of parameters are located in the FFN layers. Conversely, factorizing the FFN layers results in smaller models, but at the cost of performance loss. These findings suggest that applying a uniform low-rank pattern across all layers is suboptimal, and a more tailored strategy is required for efficient low-rank training.\nLayer-Wise Rank Comparison. Figure 3 illustrates that the blocks of the Conformer and Transformer models exhibit different behaviors based on their location in the model. Specifically, early blocks are more amenable to low-rank approximation, while later blocks are less suitable for low-rank approximation. Additionally, MHSA layers tend to have lower rank than FFN layers (around a 25% lower rank), which is consistent with the findings from the previous section. In Table II, we investigate a linear scaling factor that increases the rank as the depth of the model increases (as explained in Section II-C2). Specifically, we examine multiple low-rank projections of the ASR model on both LRS3 and Librispeech. Our findings indicate that a linear mapping can significantly reduce the number of model parameters while maintaining the model's performance. Notably, reducing the model size by more than 50% (115 M parameter model) results in a model that is 1.3\u00d7 faster and uses around 10% less memory, without compromising its performance. This was not observed when using a uniform rank, as a comparable model (113 M) requires larger batch size and sometimes results in unstable training. Additionally, it increases WER by 0.48% on LRS3 and 0.17% on Librispeech. Therefore, we can effectively low-rank linear layers by assigning an appropriate ratio based on their location within the network. Furthermore, when the ranks are accurately assigned based on both layer type and depth, there is no need for a warm-up period.\nLow-Rank Training for AVSR. Table III showcases a comparison of various low-rank AVSR models evaluated on LRS3. Our linear rank mapping with 130 M parameters stands out for its ability to reduce parameters count by 50%, leading to a notable 1.15\u00d7 speed-up and 10% memory savings. Notably, this is achieved while maintaining performance levels, with only a slight degradation observed in extremely noisy scenarios (0.8% WER increment at a SNR of -7.5 dB)."}, {"title": "V. CONCLUSIONS", "content": "This study explores the emergence of low-rank structures during the training of speech recognition models and uncovers two key findings: (i) as training progresses, the entire model gradually adopts a low-rank structure, with certain layers consistently exhibiting lower ranks than others; (ii) early blocks tend to have lower ranks compared to later blocks, following a near-linear pattern. We found that SVD initialization and linear layer-wise rank enhance low-rank training. By leveraging these techniques, we propose LR-SMS, an effective linear rank mapping that enables the training of large-scale speech models with low-rank weights from scratch. LR-SMS achieves substantial reductions in parameter count, memory usage and training time while matching the performance of full-rank training, making it a promising solution for efficient and scalable speech recognition."}]}