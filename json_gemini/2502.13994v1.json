{"title": "Generative Detail Enhancement for Physically Based Materials", "authors": ["SAEED HADADAN", "BENEDIKT BITTERLI", "TIZIAN ZELTNER", "JAN NOV\u00c1K", "FABRICE ROUSSELLE", "JACOB MUNKBERG", "JON HASSELGREN", "BARTLOMIEJ WRONSKI", "MATTHIAS ZWICKER"], "abstract": "We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to increase the visual fidelity of existing materials by adding, for instance, signs of wear, aging, and weathering that are tedious to author. To obtain realistic appearance with minimal user effort, we leverage a generative image model trained on a large dataset of natural images. Given the geometry, UV mapping, and basic appearance of an object, we proceed as follows: We render multiple views of the object and use them, together with an appearance-defining text prompt, to condition a diffusion model. The generated details are then backpropagated from the enhanced images to the material parameters via inverse rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce spatial consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic to the used material model, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. We demonstrate prompt-based material edits exhibiting high levels of realism and detail.", "sections": [{"title": "1 INTRODUCTION", "content": "Depicting rich 3D worlds is a driving goal of computer graphics. While achieving this goal is possible today for experienced artists with expert tools, the pareto principle applies: The creative step of authoring the overall look of an asset takes little time in comparison to the disproportionate effort of infusing details and imperfections of the real world. Our goal is therefore to create a tool that enhances 3D objects with appearance details requiring comparatively minimal effort from the artist.\nFor this, we turn to diffusion models [Ho et al. 2020] that are capable of producing realistic visuals and can be conditioned using text prompts and guiding images. A key consideration, however, is the amount of training data available. While datasets containing 3D objects and materials exist [Deitke et al. 2022; Vecchio and Deschaintre 2024], they cannot compete with natural image datasets in size and diversity, which directly impacts the model capabilities. We therefore build our tool using an off-the-shelf diffusion model that was trained on an internet-scale image set.\nWe combine the diffusion model with a physically based renderer to enable two key editing features: 1) specifying the initial look of the object, and 2) outputing a material representation that is complient with traditional authoring workflows. Our algorithm works as follows. We start by rendering the original 3D asset from multiple views. Then we condition the diffusion model on a concatenation of these views, and a text prompt describing the desired detail enhancements. Since our goal is to merely enhance the appearance, we propose a specific way of using two publicly available ControlNets [Zhang et al. 2023] to condition the model on the asset geometry and initial appearance. Finally, the differences between the original renderings and the diffusion-generated views are back-propagated to the material parameters via inverse rendering.\nThe main challenge of multi-view generation with diffusion models is the consistency of individual details in all relevant views. We address this challenge with two contributions. First, we seed the diffusion model with noise that is itself consistent across the views. We adopt the idea of integral noise [Chang et al. 2024] and project a common UV-space noise pattern into each view in a variance-preserving manner. Second, we bias the attention maps in the diffusion model to encourage pixels to attend to their corresponding locations in different views. We compute the correspondences by reprojecting points between the views using ray tracing operations.\nOur approach has several benefits. It avoids the impractical creation of a new dataset and/or retraining a large diffusion model; we rely on an off-the-shelf model that can be easily replaced by another one. We preserve the original geometry and artistic intent, while modifying the material texture maps according to the user's target prompts. Because we only enhance the input material, the inverse rendering is more likely to succeed than if starting the optimization from scratch. Lastly, the input and output of our model are in the form of a classical 3D representation (e.g, triangles, textures) and thus perfectly multi-view consistent. This also allows users to further edit the appearance, integrate it into larger scenes, and render with common renderers."}, {"title": "2 PRIOR WORK", "content": "For brevity, we only cover methods directly related to enforcing multi-view consistency in diffusion denoisers and to editing of physically based materials using generative models.\nMulti-view consistency via latents sharing. TexPainter [Zhang et al. 2024b] denoises multi-view latents, and correlates the views in a shared texture space. Tex4D [Bao et al. 2024] extends the idea to the temporal domain using a video diffusion model. These methods however do not easily generalize to view dependent PBR materials. Patashnik et al. [2024] and Pandey et al. [2023] instead operate on intermediate features of the network to enforce 3D consistent transformations in the output images.\nAnother approach is to reuse the diffusion model's input noise accross views as proposed by [Chang et al. 2024; Daras et al. 2024]. Our work extends this idea by anchoring the noise field in UV space for a more robust handling of disocclusions.\nMulti-view consistency via view correspondences. Cross-frame attention modules have been devised for known depth maps [Tang et al. 2023], poses [Cerkezi et al. 2023], or epipolar constraints [Kant et al. 2024]. These methods however require large-scale training. Our method exploits known geometry and is training-free.\nText-guided 3D generation. Poole et al. [2023] pioneered generation of 3D models using text-to-image diffusion and score distillation sampling (SDS). The method has been extended to various representations [Lin et al. 2023; Yi et al. 2023], with improved objective functions [Wang et al. 2023], sampling [Zhu and Zhuang 2023], and material decomposition [Chen et al. 2023a; Youwang et al. 2024]. Early methods [Cao et al. 2023; Chen et al. 2023b; Richardson et al. 2023] suffer from over-blurring due to the lack of view consistency. Follow-up work improved this using spatial attention [Shi et al. 2023], video-models [Voleti et al. 2024; Wu et al. 2024], and tiled inputs [Deng et al. 2024]. We use the latter idea in our work.\nFlashTex [Deng et al. 2024], DreamMat [Zhang et al. 2024a], and MaPa [Zhang et al. 2024c] specialize in material reconstruction for a known scene. They leverage known priors by training a controlnet [Zhang et al. 2023] from geometry buffers (e.g. depth and normal), and lighting rendered with known, constant, materials (e.g. fully diffuse and specular). This helps greatly with view dependent shading effects, and separating shadows from material albedo. Vecchio et al. [2024] train a generative model to directly synthesize material maps. All these methods are costly as they require training with specialized object/material datasets, which we avoid.\nImage and appearance editing. Text-guided diffusion models are often applied to image editing while preserving the semantics of the source image. This is achieved by manipulating the self-attention layers [Tumanyan et al. 2023], often combined with DDIM inversion [Mokady et al. 2023; Parmar et al. 2023]. RGB\u2194X [Zeng et al. 2024] allows editing the content of images by first extracting irradiance and material maps, manually editing them, and then generating the corresponding realistic image. We consider a dual problem, where the inputs and outputs are PBR maps, and the intermediate step that enhances details involves generation of an image.\nMaterial upscaling. Gauthier et al. [2024] consider a subset of detail enhancement, focusing on increasing the resolution of PBR material textures by inverse rendering upscaled images. However, they operate on flat-geometry renderings and are therefore unable to synthesize detail in the context of the object geometry and the surrounding scene; this is one of our goals.\nVideo diffusion models. Video diffusion models [Blattmann et al. 2023; Hong et al. 2023; Yang et al. 2024] generate (view-consistent) video from text and image conditioning. Notably, SV3D [Voleti et al. 2024] adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation. However, these models come at significant computational cost, and the generated frames are typically not as detailed as text-to-image models."}, {"title": "3 BACKGROUND", "content": "Diffusion models. Denoising diffusion leverages a bidirectional process where the forward pass gradually corrupts training data by iteratively adding Gaussian noise until the data becomes pure noise. The reverse process then learns to denoise the corrupted data through a neural network, which predicts and removes noise step-by-step. We use a latent space diffusion model [Rombach et al. 2022], which extracts the latent space using a variational autoencoder and performs the denoising steps with a U-Net architecture operating at different scales (see Figure 3 in Rombach et al. [2022]).\nControlNet. In order to guide the denoising process, the Control-Net model [Zhang et al. 2023] implements a dual-network architecture. The first network-a pretrained diffusion model-is locked down to perform the usual denoising task and cloned. The clone network is connected to the locked network using zero-initialized convolution layers, and enables precise spatial conditioning of the pretrained denoiser using images.\nAttention. The attention operation [Vaswani et al. 2017] has been incorporated to diffusion models to capture relationships between different activations in the denoising process. The operation takes its inputs and embeds them in three learned linear spaces, referring to them as key, query, and value. Denoting the matrices of these embeddings Q, K, and V, respectively the attention formula\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V$\nprovides a mechanism for capturing the similarity between queries and keys (where dk is the dimensionality of the key embedding). A typical U-Net diffusion model features two types of attention: cross-attention layers that guide the denoising of image regions using a given text-prompt, and self-attention layers that allow regions within the image to influence each other. In the self-attention module, the QKT product forms a large attention score matrix, where entry [i, j] describes how strongly region i attends to region j."}, {"title": "4 METHOD", "content": "Our method (see Figure 2) comprises three stages: forward rendering, detail generation, and inverse rendering discussed below. We then present our three technical contributions in Sections 4.1 to 4.3.\nForward rendering. We begin with a user-provided asset comprising of known geometry and the material to be enhanced, as well as a 3D scene providing context for the asset. We render the scene from a small number of views (between 9 and 16 views in practice) in an orbit around the asset. The output of the renderer consists of the renderings as well as auxiliary buffers of surface normals, which serve to condition the diffusion model during the next stage.\nDetail generation. The renderings from the previous stage are passed to an off-the-shelf diffusion model to add detail to the renderings, conditioned on a text prompt and the auxiliary buffers. Although the rendered views could be enhanced individually, we find that mutual view consistency is improved if we use multi-view visual prompting [Deng et al. 2024], in which we concatenate all views into a grid (e.g. 3\u00d73 or 4\u00d74) and enhance them simultaneously. While multi-view prompting improves consistency across the views at a coarse scale, there remains enough variation in fine-scale detail between views to make reconstruction of highly detailed materials challenging. To remedy this, we propose two modifications to the diffusion model: using view-correlated input noise that is anchored in 3D space (Section 4.2) and biasing the attention layers with pixel-to-pixel correspondence information (Section 4.3).\nInverse rendering. We finally propagate the detail generated by the diffusion model back to the original material of the 3D asset, leveraging a differentiable renderer [Jakob et al. 2022] to minimize the difference between the enhanced views and the rendered material in a stochastic gradient optimization. In all our results, we optimize the spatially-varying albedo, normal, and roughness textures of a typical PBR material [Burley 2012], but any differentiable material definition can be used in principle. We initialize the optimization state with the original textures; this improves the convergence likelihood."}, {"title": "4.1 Structure-preserving detail enhancement", "content": "To enhance the rendered views while preserving the character of the input material, we follow the approach of Meng et al. [2022] and add a user-controlled amount of noise to the rendered views to get the initial state for diffusion. This is not enough, however, to preserve the original material and geometry. Hence, we additionally condition the diffusion model with two publicly available ControlNets [Zhang et al. 2023]: ControlNet tile, trained to do super-resolution, which we repurpose to respect the input view while enhancing details, and ControlNet normal that helps preserve lighting and curvature details using our auxiliary normal buffer as input.\nTogether with our other modifications (Sec. 4.2 and 4.3) we find this to be effective at achieving consistency with the original material and across views, while avoiding expensive training of task-specific ControlNets as used in prior work [Deng et al. 2024]."}, {"title": "4.2 View-correlated noise prior", "content": "Although the relationship between the initial noise input and the image output in diffusion is highly non-linear, the two are correlated. This has previously been exploited for temporal consistency by warping noise by a motion field, and we take a similar approach. Because diffusion models are highly sensitive to the noise statistics, the noise we produce must be uncorrelated within each view and have uniform variance, or we risk significant artifacts.\nBased on Chang et al. [2024], we propose a simple method to correlate the initial noise of the diffusion model across views while preserving its statistics. In contrast to their application, we deal with a sparse set of views that do not undergo smooth motion. It would be challenging to warp an initial noise from a reference view due to the significant amount of disocclusion between views. Instead, we exploit the known geometry of the asset and anchor a reference noise field in the UV space of the asset.\nFor each view, we then project the noise from UV space (we use 1024 \u00d7 1024 noise textures) into image space and use this as the initial state for diffusion. Compared to the analytic integration of Chang et al. [2024], we use a simpler but effective supersampling approach. We subdivide each pixel into a grid of subpixels (4 \u00d7 4 in our implementation) and project the corners of each subpixel into the UV space of the object:\nWe then sample the noise field at the center of each projected subpixel, and compute an area-weighted average of the sampled noise values: Si fi Ai, where A\u00a1 is the area of each projected subpixel i, and fi its noise value. Neighboring pixels don't overlap and generally average distinct sets of noise texels, and the resulting noise is independent within each view.\nThe variance of the projected noise is highly non-uniform, depending on the projected area of each pixel. To correct this, we could normalize the noise field by an estimate of its variance: \u221a\u03a3 \u0391. However, because multiple subpixels may map to the same noise texel, we need to additionally account for the covariance between subpixels. We estimate this with Covi = max(Atexel/Ai-1, 0) where Atexel = 1024-2 is the area of a noise texel. Intuitively, this counts how many times a distinct noise value is overcounted on average. The final normalization factor is then \u221a\u00a1A(1 + Covi). This matches the variance of projected- and reference noise.\nIn the case of extreme magnification of the noise texture, individual noise texels may project to multiple pixels and correlate noise within the image. This is rare and usually caused by missing or degenerate UVs, but it can negatively impact the quality of diffusion. As a safeguard, we smoothly blend the projected noise value with independent white noise when the pixel area in UV space, \u03a3\u03af \u0391\u03af, approaches the area of a noise texel."}, {"title": "4.3 Pixel-correspondence attention bias", "content": "The second technique for improving the multi-view consistency amounts to biasing the self-attention mechanism of the diffusion model according to a reprojection prior.\nAs described in Section 3, the self-attention modules allow image regions to influence each other. We refer to these regions as latent pixels to emphasize that they map to pixels in the input/output image. The QKT product in the self-attention module forms a large NX N score matrix where entry [i \u2208 Q, j\u2208 K] describes how strongly latent pixel i attends to latent pixel j; N is the total number of latent pixels.\nOur goal is to increase the attention scores between latent pixels in different views that observe the same surface patch. This will increase the chance that these latent pixels will denoise to consistent visuals in the resulting images. Conceptually, we construct an N \u00d7 N bias matrix B, where any positive value B[i, j] will boost the attention of pixel i to pixel j.\nWe determine the values of B as follows. For each pair of pixels i and j (where i \u2260 j) in the 3 \u00d7 3 latent image grid, we cast a ray through the center of latent pixel j into the 3D scene, finding the first hit point p. We then project p onto the image plane containing pixel i and check that p and the projection q are mutually visible.\nIf the projection q is within a neighborhood I of pixel i, we set the value in the bias matrix to a user-defined constant: B[i, j] = w.\nThe matrix is used to alter the attention operation as:\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T + B}{\\sqrt{d_k}})V$.\nIn practice, constructing the full N \u00d7 N matrix is often prohibitive, and the bias term has to be evaluated on the fly (see Sec. 5).\nThe U-Net applies self-attention at multiple scales, and we adjust the size of neighbordhood I (92, 52, 32, 12 for layers 1-4, respectively) to map to the same size patch in the original image. Figure 3 visualizes exemplary attention scores before and after adding the bias for a single specific surface point. The increment w is a user-defined constant analysed in Figure 4. Increasing w improves view consistency but eventually generates less compelling appearance as the diffusion starts to lose its global view of the image."}, {"title": "5 IMPLEMENTATION", "content": "We implemented our system in PyTorch [Paszke et al. 2019] using publicly available models: the tile [Liylasviel 2025b] and normal [Liylasviel 2025a] variants of ControlNet (for color and normal inputs respectively) and the corresponding Hugging Face implementation [Wolf et al. 2020] of Stable Diffusion 1.5.\nWe use the Mitsuba 3 system [Jakob et al. 2022] for GPU accelerated (differentiable) rendering. During inverse rendering, we apply a tonemapping operator [Reinhard et al. 2002] to the high-dynamic range renderings before comparing them with the (low-dynamic range) diffusion-generated target images using a relative L2 loss. It is important not to clip high values in order to preserve smooth specular highlights and avoid zero-valued gradients during back-propagation.\nThe ControlNet occasionally fails to preserve the exact silhouette of the rendered 3D geometry causing some background pixels to \"bleed into the object\" during the diffusion. We therefore stop gradient propagation for pixels that are within a fixed distance of the (precomputed) object boundary and downscale the loss at grazing incident angles based on a cosine-factor. Masked points still receive coverage from other views and are not removed from optimization.\nMemory considerations and scalability. Both the main diffusion process and our attention biasing can easily exhaust available GPU memory when operating on large images. One memory bottleneck is the variational autoencoder used to convert between images and the spatially-downsampled latent representation. For this conversion we found it necessary to split images back into tiles consisting of the individual views. The latent sampling inside the autoencoder and the main diffusion & denoising processes can then internally operate on the (re)concatenated grids."}, {"title": "6 EXPERIMENTS", "content": "Comparisons. While our goal of enhancing given 3D assets with existing materials is distinct from recent work leveraging image models for view-consistent editing and generation, it is important to evaluate whether existing works can address our problem. Figure 5 justifies our technique by comparing it to related methods applied in our problem setting. The top half shows image generators that do not rely on the full 3D geometry of a given asset, including SPAD [Kant et al. 2024], Diffusion Handles [Pandey et al. 2023], and RGB X [Zeng et al. 2024]. Because SPAD and Diffusion Handles are not designed to work with the given 3D geometry of an input asset, they struggle to render the asset accurately from multiple viewpoints. On the other hand, RGB\u2192X takes accurate scene intrinsics (normals, albedo, roughness, etc.) as input, but it is not equipped to ensure multi-view consistency.\nThe bottom part of Figure 5 compares our approach to DreamMat [Zhang et al. 2024a] and TexPainter [Zhang et al. 2024b], two state-of-the-art material generators for given 3D assets. While they focus on material generation from scratch, our primary goal is to enhance existing materials. Hence our result is more faithful to the initial asset provided as input (shown in Figure 4), while also providing more realistic fine grained details.\nVisual results. Figure 10 shows results of our complete pipeline, starting with basic assets, all the way to the recovered material parameters and the corresponding renderings. The air conditioner result in particular highlights the advantage of using a diffusion model trained on natural images. The rusting of the blades is distinct from that of the box itself, which aligns with the expectation these components would age differently. In Figure 6 we show how classifier free guidance [Ho and Salimans 2022] enables the user to control the magnitude of the detail enhancements. We show further examples in the supplementary document and video.\nAblation. We validate our contributions and algorithmic choices to achieve view consistency of a pure 2D generative diffusion model"}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "Additional consistency improvements. Although our contributions improve multi-view consistency, occasional deviations between views still occur at small scale. The inverse rendering typically resolves them by superimposing the conflicting visuals in the material maps. The final representation is view consistent, by design, but high-frequency view-dependent effects (e.g., mirror reflections) may end up baked in the albedo texture; see Figure 7. Since multi-view consistency of diffusion generators is an actively sought property, we believe future work will alleviate this either in a data-driven fashion, e.g., by employing multi-step optimization and video models, or by imposing additional geometric priors, such as identifying pixel correspondences using manifold walks on specular surfaces [Jakob and Marschner 2012].\nControlling the diffusion model. We currently expose only a text prompt as the control over the generated detail. Future work could explore more fine-grained user control, such as CLIP priors [Face 2025; Ramesh et al. 2022] or manipulations using texture exemplars [Guerrero-Viu et al. 2024]. The recently published Stable Diffusion 3.x [Esser et al. 2024; Stability AI 2025] supports more sophisticated text encoders and ControlNets, and swapping them in place of our current diffusion model provides a near-term avenue for better control. Importantly, artists still retain full editability of the material produced by our tool using traditional workflows.\nEnhancing macro geometry. We focused on visual enhancements that can be captured using texture maps without attempting to refine the input macro geometry. While inverse rendering is in principle capable of updating the mesh, we leave this for future work.\nManual hyperparameter tuning. The attention bias w is currently a manually tuned hyperparameter. Although the range of reasonable values is limited ([0 \u2013 3.5] in our experiments), and w can be tuned quickly on low-resolution images, a parameter-free biasing would make the technique more practical."}, {"title": "8 CONCLUSION", "content": "We presented a method for enhancing the detail of a classically authored material using a diffusion model. Our method renders the provided material from multiple views, adds details to the renderings using a diffusion model, and then backpropagates the changes to the material using inverse rendering. Inverse rendering requires detail to be consistent across views, and we achieve this with two technical contributions: noise correlation by projecting from a reference noise anchored in the UV space, and attention biasing using the known geometry of the object. This requires no new datasets or expensive retraining and is largely built from off-the-shelf, pre-trained components.\nThe resulting method serves the important use case of human-in-the-loop authoring: Rather than entirely replacing the artist and generating materials from scratch, we allow the artist to maintain creative control using traditional workflows, while reducing the time spent on tedious detailing of assets-analogous to \u201cauto-complete\u201d for material detail. Because the input and output of our method are traditional materials, our method can be used at any stage in the authoring process, and the produced enhacements arbitrarily post-processed, blended, and combined. We believe our work builds a solid foundation for future practical tools that will further improve the robustness and controllability of the generative process."}, {"title": "A ADDITIONAL RESULTS", "content": "We present full results of materials from the main paper as well as additional results in Figures 11-13. The presentation follows the main paper and shows results of our complete pipeline starting from basic assets all the way to the recovered material parameters.\nFigure 14 shows an uncropped version of Figure 3 in the main paper, and visualizes exemplary attention scores before and after adding the bias for a single specific surface point.\nFinally, Figure 15 is an extended version of Figure 6 from the main paper and demonstrates the full set of user-controlled parameters and their impact on the generated visuals. The classifier-free guidance parameter controls the amount of adherence to the prompt; the CNet Tile scale and the added noise parameters control how much the generated material is allowed to deviate from the input views, and trades off the amount of detail vs preservation of the original design intent."}]}