{"title": "Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach", "authors": ["Tanvir Hossain", "Khaled Mohammed Saifuddin", "Muhammad Ifte Khairul Islam", "Farhan Tanvir", "Esra Akbas"], "abstract": "Graph Neural Network (GNN) achieves great suc-cess for node-level and graph-level tasks via encoding meaningfultopological structures of networks in various domains, rangingfrom social to biological networks. However, repeated aggregationoperations lead to excessive mixing of node representations,particularly in dense regions with multiple GNN layers, resultingin nearly indistinguishable embeddings. This phenomenon leads to the oversmoothing problem that hampers downstream graphanalytics tasks. To overcome this issue, we propose a noveland flexible truss-based graph sparsification model that prunesedges from dense regions of the graph. Pruning redundant edgesin dense regions helps to prevent the aggregation of excessiveneighborhood information during hierarchical message passingand pooling in GNN models. We then utilize our sparsificationmodel in the state-of-the-art baseline GNNs and pooling models,such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL,DMonPool, and AdamGNN. Extensive experiments on differentreal-world datasets show that our model significantly improvesthe performance of the baseline GNN models in the graphclassification task.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, graph neural networks (GNN) have givenpromising performance in numerous applications over differ-ent domains, such as gene expression analysis [19], traffic flowforecasting [28], fraud detection [22], and recommendation system [7]. GNN effectively learns the representation of nodesand graphs via encoding topological graph structures intolow-dimensional space through message passing and aggregations mechanisms. To learn the higher-order relations betweennodes, especially for large graphs, we need to increase thenumber of layers. However, creating an expressive GNN modelby adding more convolution layers increases redundant receptive fields for computational nodes and results in oversmooth-ing as node representations become nearly indistinguishable.Several research works illustrate that due to oversmoothing,nodes lose their unique characteristics [5], [9], adverselyaffecting GNNs' performance on downstream tasks, includingnode and graph classification. Different models have been pro-posed to overcome the problem, such as skip connection [6],drop edge [17], GraphCON [18]. While many of these methodsfocus on node classification, they often overlook the impactof oversmoothing on the entire network's representation. Ad-ditionally, only a limited number of studies have investigatedthe influence of specific regions causing oversmoothing [6],[9] in GNNs. These studies show that the smoothness in GNNvaries for complex connections in different graph areas, andan individual node with high degrees converges to stationarystates earlier than lower-degree nodes. Hence, the networks'regional structures affect the phenomenon because repeatedmessage passing occurs within the dense neighborhood regions ofthe nodes. Therefore, we observe the impact of congestedgraph regions on oversmoothing.We conduct a small experiment to demonstrate the earlyoversmoothing at highly connected regions on a toy graph(Figure 1(a)). To calculate the density on the graph, we utilizethe k-truss [1], one of the widely used cohesive subgraphextraction models based on the number of triangles each edgecontains. To show the smoothness of the node features, weutilize the average node representation distance (ANRD) [11].We measure the ANRD of different k-truss regions and presenthow it changes through the increasing number of layers inGNN. We present the toy graph and ANRD values with respectto the number of layers in Figure 1(b). While the toy graph inthe figure is a 4-truss graph, it has 6, 7, and 8-truss subgraphs.Nodes and edges are colored based on their trussness. Asknown, k-truss subgraphs have hierarchical relations, e.g., 7and 8-truss subgraphs are included in the 6-truss subgraph.Even at layer 2, we observe the ANRD of 7 and 8-trusssubgraphs substantially degrades compared to the lower truss(k = 4, 6) subgraphs.While oversmoothing is observed at the node level, it mayalso result in losing crucial information for the graphs' repre-sentation to distinguish them. Furthermore, to learn the graphrepresentation, GNNs employ various hierarchical poolingapproaches, including hierarchical coarsening and message-"}, {"title": "II. RELATED WORKS", "content": "Graph Classification. Early GNN models leverage simplereadout functions to embed the entire graph. GIN [26] intro-duces their lack of expressivity and employs deep multisetsums to represent graphs. In recent years, graph poolingmethods have acquired excellent traction for graph represen-tation. They consider essential nodes' features instead of allnodes. Flat pooling methods utilize the nodes' representationwithout considering their hierarchical structure. Among them,GMT [3] proposes a multiset transformer for capturing nodes'hierarchical interactions. Another approach, SOPool [25], cap-italizes vertices second-order statistics for pooling graphs.There are two main types of hierarchical pooling methods:clustering-based and selection-based. Clustering-based meth-ods assign nodes to different clusters: computing a clusterassignment matrix [27], utilizing modularity [21] or spectralclustering [4] from the node's features and adjacency. On theother hand, selection-based models compute nodes' impor-tance scores up to different hop neighbors and select essentialnodes from them. Two notable methods are SAGPool [12],which employs a self-attention mechanism to compute thenode importance, and HGP-SL [30], which uses a sparse-max function to pool graphs. KPLEXPOOL [2] hierarchicallyleverages k-plex and graph covers to capture essential graphstructures and facilitates the diffusion between contexts ofdistance nodes. Some approaches combine both hierarchicalpooling types to represent graphs. One model, ASAP [16],adapted a new self-attention mechanism for node sectioning,a convolution variant for cluster assignment. Another model,AdamGNN [32], employs multi-grained semantics for adapt-ing selection and clustering for pooling graphs.Oversmoothing: While increasing number of layers for aregular neural network may results better learning, it maycause an oversmoothing problem in which nodes get sim-ilar representations during graph learning because of theinformation propagation in GNN. To tackle this, researcherspropose different approaches: DROPEDGE [17] randomlyprunes edges like a data augmentor that reduces the mes-sage passing speed, DEGNN [14], [23] applies connectivityaware decompositions that balance information propagationflow and overfitting issue, MADGap [5] measures the averagedistance ratio between intra-class and inter-class nodes whichlower value ensures over-smoothing. However, these methodsoverlook networks' regional impact on oversmoothing. Thek-truss [10] algorithm primarily applies to community-basednetwork operations to identify and extract various dense re-gions. It has been employed in different domains, such ashigh-performance computing [8] and graph compression [1]."}, {"title": "III. PRELIMINARIES", "content": "This section discusses the fundamental concepts for GNNand pooling, and also formulate the oversmoothing problemincluding the essential components for our solution to thisproblem. We begin with discussing graph neural networks andgraph pooling techniques. Then, define the issue, includingthe task. Finally, we delve into the foundation concept ofour model (k-truss), which plays a crucial role in solving theproblem."}, {"title": "A. GNN and Graph Pooling", "content": "Graph Neural Network (GNN) [20] is an informationprocessing framework that defines deep neural networks ongraph data. Unlike traditional neural network architecturesthat excel in processing Euclidean data, GNNs are expertsin handling non-Euclidean graph structure data. The principalpurpose of GNN is to encode node, subgraph, and graph intolow-dimension space that relies upon the graph's structure. InGNN, for each layer, K in the range 1,2,...k, the computa-tional node aggregates (1) messages $m_{N(v)}^{(k)}$ from its K-hopneighbors and updates (2) its representation $h_v^{(k+1)}$ with thehelp of the AGGREGATE function.\n$m_{N(v)}^{(k)} = AGGREGATE^{(k)}(\\{h_u^{(k)}, \\forall u \\in N(v)\\})$\n$h_v^{(k+1)} = UPDATE^{(k)}(h_v^{(k)}, m_v^{(k)})$\nIn the context of graph classification, GNNs must focus onaggregating and summarizing information across the entiregraph. Hence, the pooling methods come into play.\nGraph Pooling. [13] Graph pooling performs a crucialoperation in encoding the entire graph into a compressedrepresentation. This process is vital for graph classificationtasks as it facilitates capturing the complex network structureinto a meaningful form in low-dimensional vector space.During the nodes' representation learning process at different"}, {"title": "B. Oversmoothing", "content": "According to [29], continual neighborhood aggregation ofnodes' features gives an almost similar representation to nodesfor an increasing number of layers K. simply, without consid-ering the non-linear activation and transformation functions,the features converge as:\n$h^{\\infty} = AX, A_{i,j} = \\frac{(d_i + 1)^{-r} (d_j + 1)^{1-r}}{2m + n}$\nwhere, $v_i$ and $v_j$ are source and target nodes, $d_i$ and $d_j$ aretheir degrees respectively, \u00c2 is the final smoothed adjacencymatrix and r\u2208 [0,1] is the convolution coefficient. Theequation (6) shows for an infinite number of propagations,the final features are blended and only rely upon the degreesof target and source nodes. Furthermore, through spectral andempirical analysis [6] shows: nodes with higher-dree are morelikely to suffer from oversmoothing.\n$h_k(j) = \\sqrt{d_j + 1}(\\frac{\\sqrt{d_j + 1}}{2m + n}) = \\sqrt{d_j + 1}(\\frac{\\sum_{i=1}^n \\sqrt{d_i + 1}}{2m + n})$\nIn the equation (7), Ag is the spectral gap, m is the numberof edges, and n is the number of nodes. It represents thefeatures convergence relied upon the spectral gap AG andsummation $\\sum_{i=1}^n$ of feature entries. When the number oflayers K goes to infinity, the second term disappears (after\u00b1). Hence, all vertices' features converge to steady-state foroversmoothing, which mainly depends on the nodes' degrees."}, {"title": "C. Problem Formulation", "content": "This research aims to alleviate oversmoothing by effectivelysimplifying graphs to balance global and local connections,resulting in better graph classification results. Formally, AGraph is denoted as G = (V, E, X), where V is the setof nodes and E is the set of edges. Symbol X \u2208 $R^{N\u00d7d}$represents the graph's feature matrix of dimension d, whereN = |V| is the number of nodes in G and xv \u2208 $R^d$,xv \u2208X and v \u2208 Vis ad dimensional feature of a particularnode in the graph. The neighborhood of a node u is denotedas N(u), and its degree is represented as d(u) = |N(u)|.For a dataset D = (G, Y) consisting of a set of graphsG = {$G_1, G_2\u2026G_N$}, label pair Y = {$Y_1, Y_2, \u2026.Y_N$},our truss-based sparsification algorithm introduces a set ofsparsified graphs as Gs = {$Gs_1, Gs_2\u2026GS_N$}. The algorithmis designed to remove redundant graph connections and retainthe graph's essential structural information. Subsequently, Thissparsified graphs set is analyzed using GNN models to learna function f: Gs \u2192 Y leveraging the reduced complexity ofthe graphs. The principal objective is to enhance the accuracy(Acc) of GNN models in graph classification tasks."}, {"title": "D. K-truss", "content": "Identifying and extracting cohesive subgraphs is a pivotaltask in the study of complex networks. The k-truss subgraphextraction algorithm is instrumental as it isolates subgraphsbased on a specific connectivity criterion. The root of thecriterion is the term support, which refers to the enumerationof triangles in which an edge participates. The supportserves as the cornerstone to measure the cohesiveness of asubgraph. The following two definitions explain the criterionfor extracting specific tightly interconnected subgraphs froma complex network.\nDefinition 1: Support: In graph G = (V, E), the supportof an edge e = (u, v) \u2208 E is denoted as $sup_G(e)$ the numberof triangles where e involves, i.e $sup_G(e) = |{ \u0394_{uvw} :w \u2208 V } |$.\nDefinition 2: k-truss subgraph: A subgraph S = ($V_s, E_s$)where, S\u2282G, $V_s$ \u2286 V and $E_s$ \u2286 E is a k-truss subgraphwhere every edge e \u2208 $E_s$ has at least k - 2 support, wherek\u2265 2.\nNotably, the concept of k-truss is inherently dependent onthe count of triangles within the graph, establishing that anygraph can be considered a 2-truss subgraph. The hierarchicalstructure of k-truss subgraphs implies that a 3-truss subgraphis a subset of the 2-truss subgraph (the original graph) denotedas $G_3 \u2286 G_2$. Similarly, $G_4 \u2286 G_3\u2026\u2026G_k \u2286 G_{k-1}$.\nDefinition 3: Edge Trussness: For a given graph G, for k >2, an edge, E(u, v), can exist in multiple k-truss subgraphs.The trussness of the edge, denoted as Tr(u, v), is quantifiedfrom the highest k value for which the edge is included in thatsubgraph. That is, Tr(u, v) = k and (u, v) \u2209 $G_{k+1}$."}, {"title": "IV. TRUSS BASED GRAPH SPARSIFICATION", "content": "In this section, we explain the proposed truss-based graphsparsification model (TGS) to overcome the oversmoothingproblem for graph classification. In graph analytics, classifyinggraphs is challenging due to their large size and complex struc-ture. Graph sparsification- a technique that reduces the numberof graph connections by preserving crucial graph structures, isan emerging technique to address these challenges. We aim toget an effective simplified graph that keeps essential short- andlong-distance graph connections through graph sparsification,which produces the optimal graph classification result. Theoverall architecture of the proposed model is presented inFigure 3. The model consists of 2 parts: truss-based graphsparsification and graph learning on the sparsified graph.We observe four phases to develop the truss-based graphsparsification framework.\nPhase 1: Compute edge trussness: At first, we apply the k-trussdecomposition algorithm on an unweighted graph to computeits edges' trussness as weight. Next, we split all edges intogroups based on their truss values: high-truss edges and low-truss values for a given threshold \u03b7.\nPhase 2: Measure node strenght: TGS focuses on high-trussedges for sparsification. As high-truss edges have higherdegrees, they massively contribute to the oversmoothing phe-"}, {"title": "A. Dense Region Identification", "content": "To learn the structure of the graph, GNN applies messagepassing between nodes through edges. Through repeated mes-sage passing, nodes in the dense regions get similar neighbors'feature information, which causes oversmoothing. As a result,the features of those regions' nodes become indistinguishable.Many different density measures exist, including k-truss, k-core, and k-edge. This paper uses k-truss, defined based ontriangle connectivity, to identify the dense regions.\nOur approach employs a truss-decomposition [1] algorithm,as detailed in Algorithm 1, to compute edge trussness and"}, {"title": "B. Pruning Redundant Edges", "content": "Ascertaining the high-truss edges is crucial for under-standing the density level in different parts of the graph.However, directly pruning these edges may break up essentialconnectivity between nodes. For example, low-degree nodescould be connected with a dense region node, and pruning anincident high-truss edge may not provide adequate informationto that low-degree node. To balance the connectivity betweennodes, we determine the nodes' strength of edge high-trussedges and then proceed to the next step. To measure nodes'(\u03b7 \u2208E, E\u2208 EH) strength, TGS calculates the averagetrussness $T_N(n)$, n \u2208 V. This score ensures the density depthof a node and implies its important connectivity.\nDefinition 4: The strength of a node is measured as thesummation of all of its incident edge weights. However, In thisresearch, node strength is applied as the average of nodes'incident edges' trussness.\n$T_N(n) = \\frac{1}{|N(n)|}\\sum_{v \\in N(n)} T_r(n, v)$\nFor a candidate edge E = (u,v), after measuring thenode strength of u and v (8), their minimum value (9) hasbeen taken. Notably, a node may be included in different k-truss subgraphs. Hence, its neighborhood's trussness providesmore connectivity information. The minimum node strengthof an edge's two endpoints signifies the least density of itssurroundings. As we aim to reduce the density of highlyconnected regions to combat oversmoothing, TGS follows atechnique to decide to prune edges. For this purpose, theminimum node strength of the edge E is compared to athreshold 8. In condition (2), This comparison ensures theedge's presence in a prunable dense region. The conditionindicates that if any end of the candidate edge is sparse$T_N(E) < \u03b4$, TGS avoids cutting it because that connectionserves as an essential message-passing medium in the GNNsaggregation step. In contrast, when the minimum score equals"}, {"title": "IV. EXPERIMENT DESIGN AND ANALYSIS", "content": "This section validates our technique on different real-worlddatasets by applying standard graph pooling models. First, weprovide an overview of the datasets. Then, we briefly describethe parameters of various methods. Finally, we compare theperformance of our TGS algorithm's enhancement with theoriginal baselines in the graph classification tasks includinganalysis of parameters, deeper networks, and ablation study."}, {"title": "A. Datasets and Baselines", "content": "We experiment with our model on eight different TUDortmund [15] datasets: Five of them are biomedical do-main: PROTEINS, NCI1, NCI109, PTC, and DD andthree of them from social network domain: IMDB-BINARY,IMDB-MULTI, and REDDIT-BINARY. We extend theTGS algorithm with seven state-of-the-art backbone graphpooling models. Among them, three are node clustering-based pooling methods: DiffPool [27], DMonPool [21] andMinCutPool [4]. Two models, SAGPool [12] and HGP-SL [30], utilize a node selection approach for pooling thegraphs. Of the remaining two, one learns graph representationthrough flat-pooling (GMT [3]), and another one utilizes anadaptive pooling approach by applying both node selectionand clustering for the pooling procedure: AdamGNN [32].We report the statistics of the datasets in the Table I."}, {"title": "B. Experimental Settings", "content": "To compare fairly, we executed the existing standard im-plementations of the baselines and incorporated them withour model. For evaluation, we split the datasets into 80%for training, 10% for validation, and 10% testing. Mostly,we stopped learning early for 50 consecutive same validationresults in training. We measured the performance using theaccuracy metric by ruining each model 10 times for 10 randomseeds and reported their mean. The batch size was 128 for mostof the models. The effectiveness of our pruning method mostlydepends on two crucial parameters: the cutoff parameter \u03b7and the edge pruning threshold 8. For all experiments, weset n = 3, which means any edge with a trussness scorebelow 3 cannot be pruned from the graph. On the otherhand, we experimented with various d values across thedatasets. Specifically, we used & values of {3,4,5,6,7} forIMDB-BINARY, IMDB-MULTI, and {3,3.5, 4} for REDDIT-BINARY datasets. For PROTEINS and DD datasets & was setas {3, 3.25, 3.5, 3.75, and 4} and for NCI1, NCI109, we used\u03b4 values of { 2.5, and 3} while for PTC only 2.5."}, {"title": "C. Result Analysis", "content": "Table II reports the experiment results, providing a compara-tive analysis between our established model and original base-lines across various datasets. TGS integrated with backbonegraph pooling models consistently outperforms the baselines,and demonstrates its robustness in graph classification tasks.On selection-based models, with the incorporation of theSAGPool model, TGS achieves a 1.5-5.5% gain(G) (11) overthe original models. Notably, on DD and IMDB-BINARYdatasets, the gains are 3.34% and 5.17%, respectively. In theexperiment with the HGP-SL model, TGS attains a sustainableimprovement of nearly 4.5% on the IMDB-BINARY datasetand on the PTC dataset, which is over 2.5%. AdaptingTGS along with the flat pooling model GMT acquires asignificant gain (nearly 7%) over the NCI109 dataset andmaintains consistent performance on other datasets.In experiments with cluster-based modes, TGS equipped toDiffpool model achieves a magnificent accuracy gain on thePTC dataset, which is nearly 19%. It also demonstrates strongperformance with the DMonPool model over all datasets.Notably, it achieves the highest accuracy on the REDDIT-BINARY dataset, which is 85.75%.\n$Gain(G) = \\frac{TGS - Original}{Original} \u00d7 100%$\nExtended Experiment: In addition to the pooling method,we incorporate the TGS model with the fundamental GNNmodels: two versions of graph isomorphic networks (GIN-0 and GIN-\u20ac) and the simple graph convolution networkfor graph classification. During the experiment with GINnetworks, we follow 10-fold cross-validation to evaluate thevalidity of our model. On the other hand, we assess theGCN as other pooling models (section V-B). In most con-texts (in Table II), our technique outperforms these modelson every dataset. Especially on the PTC and IMDB-BINARY"}, {"title": "D. Analysis in Deeper Network", "content": "We examine the impact of the TGS in deeper layers with thebackbone models. In this analysis along with the SAGPool, wechoose three GNN models: GCN, GIN-\u20ac, and GIN-0. Besides,we select two datasets from the biomedical domain (DD andPROTEINS) and one from the social network domain (IMDB-BINARY). Figure 4 shows the best two ranked (Table IVand V in the section VII) TGS variants for the threshold\u03b4 compared to the original model performance. Columns 1(TGS(GCN)) and 4 (TGS(SAGPool)) reveal that for increasingthe number of layers, in most cases the TGS outperforms theoriginal models on all three datasets in multiple layers. Figure 4(j) and 4(k) illustrate the similar trends in the context ofTGS(GIN-\u20ac) and TGS(GIN-0) on the IMDB-BINARY dataset.However, both of these models show fluctuations in accuracyon the DD and PROTEINS datasets. One interesting fact isthat the accuracy trend TGS(GIN-0) has dis-proportionallyincreased in deeper networks on DD. A possible reason couldbe the dense nature of the networks in the dataset."}, {"title": "E. Sensitivity Analysis", "content": "In Figure 5, we demonstrate our model's performance forvariations of hyperparameters' values on the IMDB-BINARYand PROTEINS datasets. Notably, in most cases, the pruningrate decreases as much as the delta value increases. Figure 5(a) shows at 8 = 3 value, our equipped TGS modelsperform well. We observe that for lower & value, the accuracyof TGS with AdamGNN increases, whereas degrades forTGS(SAGPool). On the other hand, in the PROTEINS dataset(Figure 5(c)), for changing the 8 value, TGS decorated with"}, {"title": "F. Ablation Study", "content": "This section observes the strategical and functional signif-icance of TGS with the backbone graph pooling models. Wechose four datasets and six pooling methods. In Table III,for each dataset at the first two rows, we change the edge'sconnectivity strength measuring equations (8), and (9). In thefirst row, the equation (8) remains the same but at equation (9)instead of minimum the average of two end nodes' strengthhas been taken. On the other hand, in the second row, nodestrength measuring equation (8) is modified whereas the otherequation remains unchanged. In the last two rows, we changethe pruning procedure, examining to prune two (prune 2*) andthree (prune 3*) edges without updating the edge trussness.Due to the change of equations, the model's performance onthe PROTEINS dataset increases with its extension to MinCut-Pool and DiffPool methods. However, on the NCI1 dataset, theperformance of AdamGNN dramatically decreases (47.36%and 61.22%). Regarding examining 2 and 3 edges for pruning,sometimes more than one edge is pruned without updating the"}, {"title": "VI. CONCLUSION", "content": "In this paper, we have proposed an effective k-truss-basedgraph sparsification model to facilitate graph learning of thegraph neural networks (GNN). Through the sparsification ofdense graph regions' overflowed message passing edges, ourmodel includes more variability to the input graph for alle-viating oversmoothing. Comprehensive experiments on eightrenowned datasets verify that TGS is consistent in performanceover popular graph pooling and readout-based GNN models.We expect our research to show some interesting directions:Learning the edge pruning threshold during training, applyingparallelization during pruning edges at different k-truss sub-graphs, and joint learning to measure edge importance duringgraph sparsification."}, {"title": "VII. SUPPLEMENT", "content": "A. Result Details\nThis section reports the experiment details of the TGS modelpipelined with the backbone graph pooling and GNN models.In all the tables, the social networks and biomedical domains'datasets' results are shown with the accuracy (%) metric.We measure the average accuracy for each dataset and rankthem for different variations of the edge pruning threshold \u03b4compared to the original backbone model's scores. Table IVand V report all the results of different TGS-variants forseparate 8 values. Due to limited space, we represent the(REDDIT-BINARY & PTC) and (NCI1 & NCI109) datasets'results together in sub-tables."}]}