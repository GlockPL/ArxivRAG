{"title": "Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach", "authors": ["Tanvir Hossain", "Khaled Mohammed Saifuddin", "Muhammad Ifte Khairul Islam", "Farhan Tanvir", "Esra Akbas"], "abstract": "Graph Neural Network (GNN) achieves great success for node-level and graph-level tasks via encoding meaningful topological structures of networks in various domains, ranging from social to biological networks. However, repeated aggregation operations lead to excessive mixing of node representations, particularly in dense regions with multiple GNN layers, resulting in nearly indistinguishable embeddings. This phenomenon leads to the oversmoothing problem that hampers downstream graph analytics tasks. To overcome this issue, we propose a novel and flexible truss-based graph sparsification model that prunes edges from dense regions of the graph. Pruning redundant edges in dense regions helps to prevent the aggregation of excessive neighborhood information during hierarchical message passing and pooling in GNN models. We then utilize our sparsification model in the state-of-the-art baseline GNNs and pooling models, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and AdamGNN. Extensive experiments on different real-world datasets show that our model significantly improves the performance of the baseline GNN models in the graph classification task.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, graph neural networks (GNN) have given promising performance in numerous applications over different domains, such as gene expression analysis [19], traffic flow forecasting [28], fraud detection [22], and recommendation system [7]. GNN effectively learns the representation of nodes and graphs via encoding topological graph structures into low-dimensional space through message passing and aggregation mechanisms. To learn the higher-order relations between nodes, especially for large graphs, we need to increase the number of layers. However, creating an expressive GNN model by adding more convolution layers increases redundant receptive fields for computational nodes and results in oversmoothing as node representations become nearly indistinguishable.\nSeveral research works illustrate that due to oversmoothing, nodes lose their unique characteristics [5], [9], adversely affecting GNNs' performance on downstream tasks, including node and graph classification. Different models have been proposed to overcome the problem, such as skip connection [6], drop edge [17], GraphCON [18]. While many of these methods focus on node classification, they often overlook the impact of oversmoothing on the entire network's representation. Additionally, only a limited number of studies have investigated the influence of specific regions causing oversmoothing [6], [9] in GNNs. These studies show that the smoothness in GNN varies for complex connections in different graph areas, and an individual node with high degrees converges to stationary states earlier than lower-degree nodes. Hence, the networks' regional structures affect the phenomenon because repeated message passing occurs within the dense neighborhood regions of the nodes. Therefore, we observe the impact of congested graph regions on oversmoothing.\nWe conduct a small experiment to demonstrate the early oversmoothing at highly connected regions on a toy graph (Figure 1(a)). To calculate the density on the graph, we utilize the k-truss [1], one of the widely used cohesive subgraph extraction models based on the number of triangles each edge contains. To show the smoothness of the node features, we utilize the average node representation distance (ANRD) [11]. We measure the ANRD of different k-truss regions and present how it changes through the increasing number of layers in GNN. We present the toy graph and ANRD values with respect to the number of layers in Figure 1(b). While the toy graph in the figure is a 4-truss graph, it has 6, 7, and 8-truss subgraphs. Nodes and edges are colored based on their trussness. As known, k-truss subgraphs have hierarchical relations, e.g., 7 and 8-truss subgraphs are included in the 6-truss subgraph. Even at layer 2, we observe the ANRD of 7 and 8-truss subgraphs substantially degrades compared to the lower truss (k = 4, 6) subgraphs.\nWhile oversmoothing is observed at the node level, it may also result in losing crucial information for the graphs' representation to distinguish them. Furthermore, to learn the graph representation, GNNs employ various hierarchical pooling approaches, including hierarchical coarsening and message-passing, resulting in oversmoothing via losing unique node features [24], [33]. Consequently, dense regions' identical node information affects the graph's representation learning.\nWe extend the preliminary investigation on the toy graph given in Figure 1(a). We first apply the SAGPool model. After each pooling layer's operation, we measure the coarsened graph's nodes' embedding space matrix (ESM) with 12 norm, then present the results for the first 2 pooling layers in Figure 2(a) and 2(b). We observe that embedding distances are getting smaller for nodes within the dense regions, significantly reducing the final graph's representation variability. These node and graph representation characteristics through GNN models inspire us to work at different levels of dense regions in the network to mitigate oversmoothing.\nOur Work. To tackle the challenge, we develop a truss-based graph sparsification (TGS) model. Earlier sparsification models apply supervised techniques [31] and randomly drop edges [9], [17] which may result in losing meaningful connections. However, our model selects the initial extraneous information propagating edges by utilizing edge trussness. It operates on the candidate edge's nodes' neighborhood and measures the connectivity strength of nodes. This connectivity strength assists in understanding the edge's local and global impact on GNN's propagation steps. Based on their specific node strength limits, we decide which edges to prune and which to keep. Removing selected redundant edges from dense regions reduces noisy message passing. That decreases oversmoothing and facilitates the GNN's consistency in performance during training and inference time. As we see in Figure 2(c) and 2(d), the sparsified graph exhibits greater diversity in node distances than the original, enhancing better representation learning. In a nutshell, the contributions of our model are listed as follows.\n\u2022\tWe observe the prior stationary representation learning of nodes emerging in the network's high-truss region, which denotes a new perspective on explaining oversmoothing.\n\u2022\tWe develop a unique truss-based graph sparsification technique to resolve this issue.\n\u2022\tIn the edge pruning step, we measure the two nodes' average neighborhood trussness to detect the regional interconnectedness strength of the nodes. During the message passing steps in GNN, as we trim down the dense connections within subgraphs, nodes in less dense areas at varying hop distances acquire diverse hierarchical neighbor information. Conversely, nodes in highly dense regions receive reduced redundant information. This provides smoothness to the node representation as well as to the graph representation.\n\u2022\tWe provide a simple but effective model by pruning noisy edges from graphs based on their nodes' average neighborhood trussness. The effectiveness of our model has been evaluated in comparison with standard GNN and graph pooling models. Extensive experiments on different real-world graphs show that our approach outperforms most of those baselines in graph classification tasks.\nThe rest of this paper is organized as follows. Section II discusses the related work that informs our research. Section III introduces the model's preliminaries, whereas Section IV describes the model itself. Next, Section V represents our model's experiment results and an analysis of its performance on different datasets. Finally, we conclude the paper with a discussion of future research directions."}, {"title": "II. RELATED WORKS", "content": "Graph Classification. Early GNN models leverage simple readout functions to embed the entire graph. GIN [26] introduces their lack of expressivity and employs deep multiset sums to represent graphs. In recent years, graph pooling methods have acquired excellent traction for graph representation. They consider essential nodes' features instead of all nodes. Flat pooling methods utilize the nodes' representation without considering their hierarchical structure. Among them, GMT [3] proposes a multiset transformer for capturing nodes' hierarchical interactions. Another approach, SOPool [25], capitalizes vertices second-order statistics for pooling graphs.\nThere are two main types of hierarchical pooling methods: clustering-based and selection-based. Clustering-based methods assign nodes to different clusters: computing a cluster assignment matrix [27], utilizing modularity [21] or spectral clustering [4] from the node's features and adjacency. On the other hand, selection-based models compute nodes' importance scores up to different hop neighbors and select essential nodes from them. Two notable methods are SAGPool [12], which employs a self-attention mechanism to compute the node importance, and HGP-SL [30], which uses a sparse-max function to pool graphs. KPLEXPOOL [2] hierarchically leverages k-plex and graph covers to capture essential graph structures and facilitates the diffusion between contexts of distance nodes. Some approaches combine both hierarchical pooling types to represent graphs. One model, ASAP [16], adapted a new self-attention mechanism for node sectioning, a convolution variant for cluster assignment. Another model, AdamGNN [32], employs multi-grained semantics for adapting selection and clustering for pooling graphs.\nOversmoothing: While increasing number of layers for a regular neural network may results better learning, it may cause an oversmoothing problem in which nodes get similar representations during graph learning because of the information propagation in GNN. To tackle this, researchers propose different approaches: DROPEDGE [17] randomly prunes edges like a data augmentor that reduces the message passing speed, DEGNN [14], [23] applies connectivity aware decompositions that balance information propagation flow and overfitting issue, MADGap [5] measures the average distance ratio between intra-class and inter-class nodes which lower value ensures over-smoothing. However, these methods overlook networks' regional impact on oversmoothing. The k-truss [10] algorithm primarily applies to community-based network operations to identify and extract various dense regions. It has been employed in different domains, such as high-performance computing [8] and graph compression [1]."}, {"title": "III. PRELIMINARIES", "content": "This section discusses the fundamental concepts for GNN and pooling, and also formulate the oversmoothing problem including the essential components for our solution to this problem. We begin with discussing graph neural networks and graph pooling techniques. Then, define the issue, including the task. Finally, we delve into the foundation concept of our model (k-truss), which plays a crucial role in solving the problem.\nA. GNN and Graph Pooling\nGraph Neural Network (GNN) [20] is an information processing framework that defines deep neural networks on graph data. Unlike traditional neural network architectures that excel in processing Euclidean data, GNNs are experts in handling non-Euclidean graph structure data. The principal purpose of GNN is to encode node, subgraph, and graph into low-dimension space that relies upon the graph's structure. In GNN, for each layer, K in the range 1,2,...k, the computational node aggregates (1) messages \n\\(m_{N(v)}^{(k)}\\) from its K-hop neighbors and updates (2) its representation \\(h^{(k+1)}\\) with the help of the AGGREGATE function.\n\\(m_{v}^{(k)} = AGGREGATE^{(k)}(\\{h_{u}^{(k)}, \\forall u \\in N(v)\\})\\)\n(1)\n\\(h_{v}^{(k+1)} = UPDATE^{(k)}(h_{v}^{(k)}, m_{v}^{(k)})\\)\n(2)\nIn the context of graph classification, GNNs must focus on aggregating and summarizing information across the entire graph. Hence, the pooling methods come into play.\nGraph Pooling. [13] Graph pooling performs a crucial operation in encoding the entire graph into a compressed representation. This process is vital for graph classification tasks as it facilitates capturing the complex network structure into a meaningful form in low-dimensional vector space. During the nodes' representation learning process at different layers, one or more pooling function(s) operate on them. These pooling layers are pivotal in enhancing the network's ability to generalize from graph data through effective graph summarization. In general, pooling operations are categorized into two types: Flat pooling and Hierarchical pooling.\nFlat pooling [13] is a straightforward graph readout operation. It simplifies the encoding by providing a uniform method to represent graphs of different sizes in a fixed size.\n\\(h_g = READOUT(\\{h_{v}^{(k)}|v\\in V\\})\\)\n(3)\nHierarchical pooling [13] iteratively coarsens the graph and encodes comprehensive information in each iteration, reducing the nodes and edges of the graph and preserving the encoding. It enables the graph's representations to achieve short and long-sighted structural details. In contrast to Flat Pooling, it gives deeper insights into inherent graph patterns and relationships.\nBetween the two types of hierarchical graph pooling methods, the selection-based methods emphasize prioritizing nodes by assigning them a score, aiming to retain the most significant nodes in the graph. They employ a particular attention function for each node to compute the node importance. Based on the calculated scores, top k nodes are selected to construct a pooled graph. The following equation gives a general overview of the top k selection graph pooling method:\n\\(S = score(G, X); idx = topK(S, [\\alpha \\times N])\\)\n\\(A^{(l+1)} = A_{idx,idx}\\)\n(4)\nwhere \\(S \\in R^{N \\times 1}\\) is the scores of nodes, \u03b1 is the pooling ratio, and N is the number of nodes. Conversely, clustering-based pooling methods form supernodes by grouping original graph nodes that summarize the original nodes' features. A cluster assignment matrix \\(S \\in R^{N \\times K}\\) using graph structure and/or node features are learned by the models. Then, nodes are merged into super nodes by \\(S \\in R^{N \\times K}\\) to construct the pooled graph at (l + 1)th layer as follows\n\\(A^{(l+1)} = S^{(l)^T} A^{(l)} S^{(l)}\\)\n\\(H^{(l+1)} = S^{(l)^T} H^{(l)}\\)\n(5)\nwhere \\(A \\in R^{N \\times N}\\) is the adjacency matrix and \\(H \\in R^{N \\times d}\\) is the feature matrix with d dimensional node feature and N is the number of nodes. Note that, the AGGREGATE, UPDATE and READOUT operations are different operational functions, commonly including min, max, average, and concat."}, {"title": "B. Oversmoothing", "content": "According to [29], continual neighborhood aggregation of nodes' features gives an almost similar representation to nodes for an increasing number of layers K. simply, without considering the non-linear activation and transformation functions, the features converge as\n\\(h^{\\infty} = AX, A_{i,j} = \\frac{(d_i + 1)^r (d_j + 1)^{1-r}}{2m + n}\\)\n(6)\nwhere, \\(v_i\\) and \\(v_j\\) are source and target nodes, \\(d_i\\) and \\(d_j\\) are their degrees respectively, \\(\\hat{A}\\) is the final smoothed adjacency matrix and \\(r \\in [0,1]\\) is the convolution coefficient. The equation (6) shows for an infinite number of propagations, the final features are blended and only rely upon the degrees of target and source nodes. Furthermore, through spectral and empirical analysis [6] shows: nodes with higher-dree are more likely to suffer from oversmoothing.\n\\(h_k(j) = \\sqrt{d_j+1}(\\frac{\\sqrt{d_j+1}}{2m+n}) = \\frac{\\sqrt{d_j+1}}{2m+n} \\sum_{i=1}^{n} (\\uprho_{i}^{k} \\frac{\\sqrt{d_j+1}}{n})\\)\n(7)\nIn the equation (7), \\(\\lambda_G\\) is the spectral gap, m is the number of edges, and n is the number of nodes. It represents the features convergence relied upon the spectral gap \\(\\lambda_G\\) and summation \\(\\sum_{i=1}^{n}\\) of feature entries. When the number of layers K goes to infinity, the second term disappears (after \\(\\pm\\)). Hence, all vertices' features converge to steady-state for oversmoothing, which mainly depends on the nodes' degrees."}, {"title": "C. Problem Formulation", "content": "This research aims to alleviate oversmoothing by effectively simplifying graphs to balance global and local connections, resulting in better graph classification results. Formally, A Graph is denoted as \\(G = (V, E, X)\\), where V is the set of nodes and E is the set of edges. Symbol \\(X \\in R^{N \\times d}\\) represents the graph's feature matrix of dimension d, where \\(N = |V|\\) is the number of nodes in G and \\(x_v \\in R^d, x_v \\in X\\) and \\(v \\in V\\) is ad dimensional feature of a particular node in the graph. The neighborhood of a node u is denoted as N(u), and its degree is represented as \\(d(u) = |N(u)|\\). For a dataset \\(D = (G, Y)\\) consisting of a set of graphs \\(G = \\{G_1, G_2 ....G_N\\}\\), label pair \\(Y = \\{Y_1, Y_2, ....Y_N\\}\\), our truss-based sparsification algorithm introduces a set of sparsified graphs as \\(G_s = \\{G_{s1}, G_{s2}....G_{SN}\\}\\). The algorithm is designed to remove redundant graph connections and retain the graph's essential structural information. Subsequently, This sparsified graphs set is analyzed using GNN models to learn a function f: \\(G_s \\rightarrow Y\\) leveraging the reduced complexity of the graphs. The principal objective is to enhance the accuracy (Acc) of GNN models in graph classification tasks."}, {"title": "D. K-truss", "content": "Identifying and extracting cohesive subgraphs is a pivotal task in the study of complex networks. The k-truss subgraph extraction algorithm is instrumental as it isolates subgraphs based on a specific connectivity criterion. The root of the criterion is the term support, which refers to the enumeration of triangles in which an edge participates. The support serves as the cornerstone to measure the cohesiveness of a subgraph. The following two definitions explain the criterion for extracting specific tightly interconnected subgraphs from a complex network.\nDefinition 1: Support: In graph \\(G = (V, E)\\), the support of an edge \\(e = (u, v) \\in E\\) is denoted as \\(sup_G(e)\\) the number of triangles where e involves, i.e \\(sup_G(e) = |\\{\\Delta_{uvw} : w \\in V\\}|\\).\nDefinition 2: k-truss subgraph: A subgraph \\(S = (V_s, E_s)\\) where, \\(S \\subseteq G, V_s \\subseteq V\\) and \\(E_s \\subseteq E\\) is a k-truss subgraph where every edge \\(e \\in E_s\\) has at least k - 2 support, where \\(k \\geq 2\\).\nNotably, the concept of k-truss is inherently dependent on the count of triangles within the graph, establishing that any graph can be considered a 2-truss subgraph. The hierarchical structure of k-truss subgraphs implies that a 3-truss subgraph is a subset of the 2-truss subgraph (the original graph) denoted as \\(G_3 \\subseteq G_2\\). Similarly, \\(G_4 \\subseteq G_3\u2026\u2026G_k \\subseteq G_{k-1}\\).\nDefinition 3: Edge Trussness: For a given graph G, for k > 2, an edge, E(u, v), can exist in multiple k-truss subgraphs. The trussness of the edge, denoted as \\(Tr(u, v)\\), is quantified from the highest k value for which the edge is included in that subgraph. That is, \\(Tr(u, v) = k\\) and \\((u, v) \\notin G_{k+1}\\)."}, {"title": "IV. TRUSS BASED GRAPH SPARSIFICATION", "content": "In this section, we explain the proposed truss-based graph sparsification model (TGS) to overcome the oversmoothing problem for graph classification. In graph analytics, classifying graphs is challenging due to their large size and complex structure. Graph sparsification- a technique that reduces the number of graph connections by preserving crucial graph structures, is an emerging technique to address these challenges. We aim to get an effective simplified graph that keeps essential short- and long-distance graph connections through graph sparsification, which produces the optimal graph classification result. The overall architecture of the proposed model is presented in Figure 3. The model consists of 2 parts: truss-based graph sparsification and graph learning on the sparsified graph. We observe four phases to develop the truss-based graph sparsification framework.\nPhase 1: Compute edge trussness: At first, we apply the k-truss decomposition algorithm on an unweighted graph to compute its edges' trussness as weight. Next, we split all edges into groups based on their truss values: high-truss edges and low-truss values for a given threshold \u03b7.\nPhase 2: Measure node strenght: TGS focuses on high-truss edges for sparsification. As high-truss edges have higher degrees, they massively contribute to the oversmoothing phenomenon (Section III-B). Thus, strategic pruning of those edges helps to reduce oversmoothing. However, at the same time, important structural connections need to be maintained. To do so, we measure the minimum node strength of its two end nodes for each candidate high-truss edge, indicating the edge's surroundings' density status.\nPhase 3: Prune and update: When that minimum value exceeds the standard density assuring threshold, we prune the edge from the graph and update all the edge's trussness values. Due to the cascading effect of the network, pruning affects other edges' trussness. Therefore, edge trussness needs to be updated after each pruning operation. The process continues the pruning step until all high-truss edges are examined.\nPhase 4: Learning: At the end of the sparsification, we first feed the processed graph to GNN models for graph learning. Finally, we experiment the entire graph's representation with a multi-layer perceptron (MLP) network.\nNote that our model follows some strategies during the graph sparsification steps: (a) sorts the high-truss edges in descending order to prune more dense regions' edges earlier, and (b) examines each edge only once. Removing an edge from the graph might affect other edges; then, in further exploration, one edge might satisfy the pruning condition. The phenomenon negligibly happens as TGS starts to prune from more dense edges. Hence, the technique avoids recursion.\nA. Dense Region Identification\nTo learn the structure of the graph, GNN applies message passing between nodes through edges. Through repeated message passing, nodes in the dense regions get similar neighbors' feature information, which causes oversmoothing. As a result, the features of those regions' nodes become indistinguishable. Many different density measures exist, including k-truss, k-core, and k-edge. This paper uses k-truss, defined based on triangle connectivity, to identify the dense regions.\nOur approach employs a truss-decomposition [1] algorithm, as detailed in Algorithm 1, to compute edge trussness and discover all k-trusses from G. At first, it takes an unweighted graph as input and computes the supports of all edges. Then initialize the value of k as 2 and select the edge e* with the lowest support (line 5). Next, the value k is assigned as edge weight W, and the edge is removed (line 12). Removing an edge decreases other edges supports. Hence, we reorder edges according to their new support values (line 9). The process continues until the edges that have support no greater than (k - 2) are removed from the graph. Next, the algorithm checks whether any edge exists to access or not. If one or more exist(s), it increments k by one and goes to the line 4 again to measure their trusseness (line 14-17). Edge trussness facilitates understanding the highest dense region within which the edge exists. After calculating the edge trussness, to identify highly dense areas, TGS separates the edges in \\(G_r\\) into two sets: High-Truss Edges and Low-Truss Edges. Following condition (1), it compares all edges' trussness with the given threshold value, \u03b7, and determines the High Truss Edges \\(E_H\\). For example, in Figure 3, given \u03b7 = 3, the blue (T(E) = 4) and golden (Tr(E) = 3) colored edges are high-truss edges. Pruning \\(\\ E \\in E_H\\}\\) reduces the load of high-degree nodes in dense regions, which assists in mitigating oversmoothing in GNN.\nCondition 1: High Truss Edges \\(E_H\\): In any graph for a specific variable \u03b7, if an edge's trussness value is greater than or equal to \u03b7 then the edge is considered as a high truss edge and their set is denoted as \\(E_H\\).\nB. Pruning Redundant Edges\nAscertaining the high-truss edges is crucial for understanding the density level in different parts of the graph. However, directly pruning these edges may break up essential connectivity between nodes. For example, low-degree nodes could be connected with a dense region node, and pruning an incident high-truss edge may not provide adequate information to that low-degree node. To balance the connectivity between nodes, we determine the nodes' strength of edge high-truss edges and then proceed to the next step. To measure nodes' \\((\u03b7 \\in E, E \\in E_H)\\) strength, TGS calculates the average trussness \\(T_N(n), n \\in V\\). This score ensures the density depth of a node and implies its important connectivity.\nDefinition 4: The strength of a node is measured as the summation of all of its incident edge weights. However, In this research, node strength is applied as the average of nodes' incident edges' trussness.\n\\(T_N(n) \\leftarrow \\frac{1}{|N(n)|} \\sum_{u \\in N(n)} Tr(n,u)\\)\n(8)\nFor a candidate edge \\(E = (u,v)\\), after measuring the node strength of u and v (8), their minimum value (9) has been taken. Notably, a node may be included in different k-truss subgraphs. Hence, its neighborhood's trussness provides more connectivity information. The minimum node strength of an edge's two endpoints signifies the least density of its surroundings. As we aim to reduce the density of highly connected regions to combat oversmoothing, TGS follows a technique to decide to prune edges. For this purpose, the minimum node strength of the edge E is compared to a threshold \u03b4. In condition (2), This comparison ensures the edge's presence in a prunable dense region. The condition indicates that if any end of the candidate edge is sparse \\(T_N(E) < \u03b4\\), TGS avoids cutting it because that connection serves as an essential message-passing medium in the GNNs aggregation step. In contrast, when the minimum score equals or exceeds the value of \u03b4, we assume the edge is part of a highly dense region, and there is a high chance of excessive messages passing between that region's nodes. That may cause them to blend their representations, leading to oversmoothing during graph learning through GNN (section III-B). From the condition, the model understands which edges contribute to undesirable density levels that foster oversmoothing in GNNs.\n\\(T_N(E) \\leftarrow minimum(T_N(u), T_N(v))\\)\n(9)\nCondition 2: An Edge \\(e = (u, v)\\) is eligible for pruning when the minimum average neighborhood edge weight between u and v equals or exceeds the threshold \u03b4.\n\\(T_N(E) \\geq \u03b4\\)\n(10)\nFor example, at the lower-left in phase 3 (in Figure 3), the edge (\\(v_2, v_{10})\\), where the degrees of \\(v_2\\) and \\(v_{10}\\) are 5 and 2, respectively. The node strengths, \\(T_N(v_2)\\) is \\(\\{(3 \\times 3) + (2 \\times 2)\\} / 5 = 2.6\\), and \\(T_N(v_{10})\\) is \\(\\{(2 + 2)\\} / 2 = 2\\). Given \u03b4 = 2.5, and the minimum node strength, \\(T_N(v_2,v_{10}) = min (2.6, 2) = 2\\). Hence, the pruning condition is unsatisfactory, and the edge will stand in the graph. If TGS pruned the edge, the neighborhood of \\(v_{10}\\) would be sparser than before and miss its crucial global information. On the other hand, at the upper-right in phase 3, in context of \\(E = (v_1, v_3)\\), \\(T_N(v_1) = 3\\) and \\(T_N(v_3) = 3\\). Hence, comparing to the value of \u03b4 they are already in the dense region and \\(T_N(v_1,v_3) = 3 \\geq 2.5\\). In this case, the pruning will help to prevent blended node representation in GNN, especially between highly interconnected subgraphs. According to our model, it considers the nodes will still stay in enough dense regions to receive meaningful local and global neighborhood information after pruning."}, {"title": "V. EXPERIMENT DESIGN AND ANALYSIS", "content": "This section validates our technique on different real-world datasets by applying standard graph pooling models. First", "15": "datasets: Five of them are biomedical domain: PROTEINS", "domain": "IMDB-BINARY", "methods": "DiffPool [27", "21": "and MinCutPool [4", "12": "and HGP-SL [30", "3": "and another one utilizes an adaptive pooling approach by applying both node selection and clustering for the pooling procedure: AdamGNN [32", "parameters": "the cutoff parameter \u03b7 and the edge pruning threshold \u03b4. For all experiments", "Experiment": "In addition to the pooling method", "models": "two versions of graph isomorphic networks (GIN-0 and GIN-\u20ac) and the simple graph convolution network for graph classification. During the experiment with GIN networks"}, {"models": "GCN, GIN-\u20ac, and GIN-0. Besides, we select two datasets from the biomedical domain (DD and PROTEINS) and one from the social network domain (IMDB-BINARY). Figure 4 shows the best two ranked (Table IV and V in the section VII) TGS variants for the threshold \u03b4 compared to the original model performance. Columns 1 (TGS(GCN)) and 4 (TGS(SAGPool)) reveal that for increasing the number of layers, in most cases the TGS outperforms the original models on all three datasets in multiple layers. Figure 4(j) and 4(k) illustrate the similar trends in the context of TGS(GIN-\u20ac) and TGS(GIN-0) on the IMDB-BINARY dataset. However, both of these models show fluctuations in accuracy on the DD and PROTEINS datasets. One interesting fact is that the accuracy trend TGS(GIN-0) has dis-proportionally increased in deeper networks on DD. A possible reason could be the dense nature of the networks in the dataset.\nE. Sensitivity Analysis\nIn Figure 5, we demonstrate our model's performance for variations of hyperparameters' values on the IMDB-BINARY and PROTEINS datasets. Notably, in most cases, the pruning rate decreases as much as the delta value increases. Figure 5(a) shows at \u03b4 = 3 value, our equipped TGS models perform well. We observe that for lower \u03b4 value, the accuracy of TGS with AdamGNN increases, whereas degrades for TGS(SAGPool). On the other hand, in the PROTEINS dataset (Figure 5(c)), for changing the \u03b4 value, TGS decorated with SAGPool, MinCutPool, DMonPool, and HGP-SL, showing near"}]}