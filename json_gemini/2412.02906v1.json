{"title": "Does Few-Shot Learning Help LLM Performance in Code Synthesis?", "authors": ["Derek Xu", "Tong Xie", "Botao Xia", "Haoyu Li", "Yunsheng Bai", "Yizhou Sun", "Wei Wang"], "abstract": "Large language models (LLMs) have made significant strides at code generation through improved model design, training, and chain-of-thought. However, prompt-level optimizations remain an important yet under-explored aspect of LLMs for coding. This work focuses on the few-shot examples present in most code generation prompts, offering a systematic study on whether few-shot examples improve LLM's coding capabilities, which few-shot examples have the largest impact, and how to select impactful examples. Our work offers 2 approaches for selecting few-shot examples, a model-free method, CODEEXEMPLAR-FREE, and a model-based method, CODEEXAMPLAR-BASE. The 2 methods offer a trade-off between improved performance and reliance on training data and interpretability. Both methods significantly improve CODELLAMA's coding ability across the popular HUMANEVAL+ coding benchmark. In summary, our work provides valuable insights into how to pick few-shot examples in code generation prompts to improve LLM code generation capabilities.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) (Radford et al., 2019) have demonstrated impressive capabilities outside of natural language processing, including in mathematics (Touvron et al., 2023; Luo et al., 2023), time series forecasting (Ansari et al., 2024; Woo et al., 2024; Das et al., 2023), tabular data understanding (Hegselmann et al., 2023; Hollmann et al., 2022; Xu et al., 2024), and multi-modal understanding (Liu et al., 2024a). Among these capabilities, LLMs' application to software engineering is particularly exciting. In just a few months, LLMs exhibit zero-shot abilities in code completion (Peng et al., 2023), code generation (Roziere et al., 2023; Guo et al., 2024; Dubey et al., 2024), test case generation (Vikram et al., 2023; Lemieux et al., 2023; Sch\u00e4fer et al., 2023), debugger interaction (Islam et al., 2024; Ho et al., 2024), and repository-level generation (Shrivastava et al., 2023; Bui et al., 2024; Wang et al., 2024a).\nIn this work, we study the important task of code generation (Roziere et al., 2023), where an LLM agent generates code described by a prompt consisting of a natural language function description and few-shot input-output examples. Current LLMs exhibit code generation capabilities through improved model design (Guo et al., 2024), training (Roziere et al., 2023), and chain-of-thought (Li et al., 2023). Our work aims to enhance existing LLMs for code generation by improving the prompt itself, which remains an under-explored research problem. In fact, existing techniques evaluate on predefined prompt templates with little to no modifications (Austin et al., 2021; Liu et al., 2024b).\nTo improve the prompt itself, we break down the prompt template into 2 components: (1) a natural language description, providing a high-level description of the code, and (2) few-shot input-output examples, describing function input outputs to disambiguate the natural language description. For example, the description \"return a list with elements incremented by 1\" can be disambiguated by the example \"incr_list([1,2,3]) == [2,3,4],\u201d which shows that each numerical element of the list should be incremented by 1.\nInspired by existing work (Liu et al., 2021, 2024c) that show LLM's in-context learning (ICL) ability is shaped by which examples are included in the ICL prompt, we hypothesize LLM's coding ability is also shaped by which few-shot examples are included in the code generation prompt. We confirm our hypothesis on several language models: T5-SMALL, T5-BASE, MISTRAL, and CODELLAMA on the HUMANEVAL+ benchmark."}, {"title": "Related Work", "content": "Recent advancements in large language models (LLMs) have led to significant progress in code generation and understanding. State-of-the-art models like GPT-4 (OpenAI, 2023), Claude 3.5 (Anthropic, 2024), Llama 3 (Dubey et al., 2024), and Mistral (AI, 2023) have demonstrated remarkable code synthesis capabilities across various programming languages. These models leverage vast amounts of code data and natural language descriptions to generate contextually relevant and syntactically correct code snippets. More specialized code LLMs, such as CodeT5 (Wang et al., 2021), InCoder (Fried et al., 2022), DeepSeek-Coder (Guo et al., 2024), StarCoder2 (Lozhkov et al., 2024), Qwen2.5-coder (Hui et al., 2024), Magicoder (Wei et al., 2023), Artigenz-Coder (Agarwal et al., 2024) etc. have been fine-tuned specifically for programming tasks, showing improved performance in code completion and bug fixing. Recent work has also explored the use of LLM-powered agents for more complex coding tasks. For instance, Agent-Coder (Huang et al., 2023) and OpenCodeInterpreter (Zheng et al., 2024) introduce frameworks that decompose coding problems into subtasks or leverage execution feedback and iterative refinement, tackling challenging programming problems more effectively."}, {"title": "In-Context Learning", "content": "In-context learning enables LLMs to adapt to new tasks by providing examples within the input prompt (Brown, 2020). Research focuses on optimizing example selection and ordering (Lu et al., 2021; Liu et al., 2021; Wang et al., 2024b; Peng et al., 2024), including using curriculum learning (Liu et al., 2024d) and comparing it with fine-tuning (Mosbach et al., 2023). This has driven advances in example construction (Chan et al., 2022), mechanism exploration (Rubin et al., 2021), and reliability improvement (Gao and Das, 2024). However, challenges persist in performance stability and prompt sensitivity (Zhao et al., 2021), motivating research on meta-learning (Coda-Forno et al., 2023) and model scale effects (Sun et al., 2024). To enhance inductive reasoning, researchers are developing techniques like Case2Code (Shao et al., 2024) and SolverLearner (Cheng et al., 2024) to evaluate and improve LLMs' ability to learn from input-output examples. These efforts, along with frameworks like Modelizer (Mammadov et al., 2024) and AcTracer (Huang et al., 2024), underscore the importance of selecting informative input-output pairs for strong reasoning and generalization."}, {"title": "Problem Setting", "content": "Our work studies how to generate effective prompts for large language model (LLM) code generation. We denote the textual prompt as $x_m \\in X_m$ and corresponding code (in text form) as $y_m \u0454 Y_m$. Each prompt consists of a natural language description, $u_{NL} E U_{NL}$, and N few-shot demonstration examples, $D_j = [(x_{c}^{(j,i)}, y_{c}^{(j,i)})]_{i=0}^{N-1}$, where $y_{c}^{(j,i)} \\in Y_c$ is the expected output object when the code is run on example input object $x_{c}^{(j,i)} \\in X_c$. A template, T, converts the natural language description"}, {"title": "Model-Free Ranker", "content": "As mentioned in Section 4, the goal of $f_p$ is to approximately rank the Pass@1 metric by ranking the PERPLEXITY (TARGET) metric. Inspired by recent work showing foundation models learn more from data they cannot generate (Shumailov et al., 2024), our model-free ranker makes the following qualitative assumptions: (1) all few-shot examples will improve performance by disambiguating the natural language prompt, and (2) few-shot examples that the LLM cannot generate are more effective at disambiguating the natural language prompt. We show the degree to which these assumptions hold in Section 5.2.\nUnder these assumptions, the model-free algorithm selects few-shot examples that the LLM cannot self-generate, which we measure with PERPLEXITY (SOURCE) as described in Equation 2.\n$PP_{source}((x_{c}^{(j,i)}, y_{c}^{(j,i)}); f_m) = PP (T(u_{NL}, [(x_{c}^{(j,i)},y_{c}^{(j,i)}]); f_m, \\emptyset)$ (2)\nWe show that PERPLEXITY (SOURCE)'s ranking is a good estimator for PERPLEXITY (TARGET)'s ranking in Section 5.3.2. Intuitively, if the LLM does not expect to read a certain few-shot example, then this example will have a large effect on the LLM's generated output. As long as this effect is positive (the model does not hallucinate), PERPLEXITY (SOURCE) remains a reliable metric for selecting impactful few-shot examples. CODEEXEMPLAR-FREE, $f_p^{(free)}$, ranks the examples based on descending PERPLEXITY (SOURCE)."}, {"title": "Model-Based Ranker", "content": "To generate rankings with less assumptions, we introduce a model-based algorithm for $f_p$. Our model-based algorithm determines whether examples improve or deteriorate performance by directly learning trends from a labelled training dataset. Specifically, we collect a dataset of few-shot example, $[(x_{c}^{(j,i)}, y_{c}^{(j,i)})]$, to PERPLEXITY (TARGET) pairs and train a neural network, $f_b$, to predict the PERPLEXITY (TARGET) based on LLM representations of the labelled example: $f_b(f_m(\\cdot);\\theta)$. CODEEXAMPLAR-BASE, $f_p^{(based)}$ ranks the examples based on descending outputs of the neural network.\nCollecting the Dataset: We collect the training dataset by splitting the HUMANEVAL+ dataset into the training and testing splits in a 80:20 split, ensuring prompts in the test set do not leak into the training set. For each training set prompt, we form new labelled pairs from each few-shot example, where inputs are $x_m^{(j)} = T(u_{NL}, [(x_{c}^{(j,i)},y_{c}^{(j,i)}])$ and outputs are $y_m^{(j,i)} = PP_{target}(y_m^{(j)}; f_m, (x_{c}^{(j,i)}, y_{c}^{(j,i)}))$.\nExtracting Embeddings: We pass the input prompt to CODELLAMA (Roziere et al., 2023) to obtain hidden representations. Because intermediate layers of LLMs contain more semantic information, we extract the [EOS] token from the 16th layer, which we denote as $h_0^{(j,i)} = h(f_m(T(u_{NL}, [(x_{c}^{(j,i)}, y_{c}^{(j,i)}]])))$. This embedding contains rich semantic information on each test case, as shown in Section 5.3.3."}, {"title": "Motivational Experiments", "content": "We evaluate the effect of adding few-shot examples on 5 language models: T5-SMALL (60M) (Raffel et al., 2020), T5-BASE (220M) (Raffel et al., 2020), MISTRAL (7B) (Jiang et al., 2023), LLAMA (8B) (Dubey et al., 2024), and CODELLAMA (7B) (Roziere et al., 2023). We evaluate CODEEXEMPLAR-FREE ($f_p^{(free)}$) and CODEEXAMPLAR-BASE ($f_p^{(based)}$) on the best LLM among the above choices, CODELLAMA ($f_m$) on the popular HUMANEVAL+ benchmark (Liu et al., 2024b). As described in Section 4.2, we divide the prompts into 2 sets, one for training CODEEXAMPLAR-BASE and one for validation/testing in an 60:20:20 split. In this work, we adopt GPT (Radford et al., 2019) to generate new few-shot examples for each test prompt. However, because the HUMANEVAL+ benchmark (Liu et al., 2024b) already uses GPT to generate its test cases, to prevent data leakage, the large pool of examples is obtained from HUMANEVAL+ benchmark. Specifically, we split the unit tests into (1) few-shot examples used for prompt optimizations ($D_j = [(x_{c}^{(j,i)}, y_{c}^{(j,i)})]_{i=0}^{M-1}$) and (2) few-show examples used for benchmarking in an 60:20:20 split.\nWe note that the Pass@1 metric is too coarse for our use-case. Specifically, state-of-the-art models (Guo et al., 2024; Huang et al., 2023) show minimal performance differences between different algorithms. To better measure the effect of prompt design, we use the more finegrained PERPLEXITY (TARGET) metric, described in Section 4. We show the correlation between Pass@1 and PERPLEXITY (TARGET) in Section 5.3.1. We show that few-shot example selection improves Pass@1 results in Section 6."}, {"title": "Whether Few-Shot Examples Help", "content": "We first evaluate whether few-shot example help or hurt downstream performance. First, we test whether adding single few-shot examples improves the LLM's probability of generating the ground truth solution, PERPLEXITY (TARGET), by iterating through all possible single test cases, $PP_{target}(y_m^{(j)}; f_m, [(x_{c}^{(j,i)}, y_{c}^{(j,i)})]) \\forall i \\forall j$. Next, we test whether adding multiple few-shot examples improves the LLM's probability of generating the ground truth solution, $\\Delta$PERPLEXITY (TARGET), formally defined as $PP_{target}(y_m^{(j)}; f_m, D_j)-PP_{target}(y_m^{(j)}; f_m, \\emptyset)$.\nOur study shows prompt design has a large influence on the downstream performance. Specifically, by adding just a single one-shot example, LLM coding capabilities can improve by the same margin as model architecture or training dataset design choices (Table 1). Following neural scaling laws (Kaplan et al., 2020), larger models, such as MISTRAL, LLAMA, and CODELLAMA, exhibit much better zero-shot coding capabilities than T5-SMALL and T5-BASE. We find most examples improve PERPLEXITY (TARGET), because the median test case improves PERPLEXITY (TARGET). However, not all test cases are created equal. The best test case for a given problem improves PERPLEXITY (TARGET) much more than the median test case, motivating a need to rank candidate examples, $f_p$, by their helpfulness. Among the LLMs tested, CODELLAMA (Roziere et al., 2023) benefits the most from good test case selection, hence we focus this work around CODELLAMA.\nGiven the improvement from single few-shot examples, we extend our analysis to multiple test cases. By adding more few-shot examples, CODELLAMA coding performance significantly improves then saturates around a log perplexity improvement of around 5.0 after 6 examples (Figure 2). While performance monotonically improves, longer prompts require much more compute costs (Dao et al., 2022). Specifically, the LLM is approximately 2\u00d7 slower on prompts including 10 few-shot examples compared to"}, {"title": "Which Few-Shot Examples Help", "content": "Because most few-shot examples improve CODELLAMA coding capabilities, we shift our focus to studying which examples improve its coding capabilities the most."}, {"title": "Target Perplexity", "content": "As explained in Section 4, Pass@1 is a discrete metric, hence cannot capture the more fine-grained performance gains from different design choices. Specifically, the Pass@1 metric is binary on a single coding problem. Thus, Pass@1 does not effectively measure the effect of including individual one-shot examples in the prompt of a single problem. Hence, we propose a more fine-grained surrogate metric, PERPLEXITY (TARGET),"}, {"title": "Source Perplexity", "content": "CODEEXEMPLAR-FREE estimates the PERPLEXITY (TARGET) ranking by few-shot examples by their PERPLEXITY (SOURCE) scores. Following the one-shot experimental setup from Section 5.2, our analysis verifies PERPLEXITY (SOURCE) somewhat approximates the PERPLEXITY (TARGET) scores (Figure 4). We highlight the benefit of PERPLEXITY (SOURCE) is it does not require access to any ground truth code to compute. Hence this metric can directly be used for model-free inference, as elaborated in Section 4.1."}, {"title": "Embedding-Level Signals", "content": "While PERPLEXITY (SOURCE) roughly correlates PERPLEXITY (TARGET), we discover LLM embeddings themselves also provides a reliable signal on a prompt's PERPLEXITY (TARGET) score. Specifically, we plot the t-SNE 2D visualization of the 16th layer of CodELLAMA. Our investigation (Figure 3) reveals LLM embeddings themselves are a suitable signal to measure PERPLEXITY (TARGET). These results motivate for our learning-based approach, CODEEXAMPLAR-BASE, which encodes semantic representations of one-shot examples to estimate PERPLEXITY (TARGET)."}, {"title": "Case Study Analysis", "content": "To understand which examples boost LLMs coding capabilties, we plot the top 3 most informative and least informative prompts for 2 random"}, {"title": "Main Experiments", "content": "Given the motivating results in Section 5, we now evaluate whether our proposed ranking function, $f_p$ improves model performance. We adopt the same dataset splitting strategy as in Section 5.1."}, {"title": "Main Results", "content": "Our experiments show both CODEEXEMPLAR-FREE and CODEEXAMPLAR-BASE improves CODELLAMA's likelihood of generating ground truth code by up to 5.0 through solely optimizing the prompt (Figure 5). Specifically, under the fixed N-shot setting, CODEEXEMPLAR-FREE extracts more effective prompts than both the original prompt (Human Eval), randomly selecting few-shot examples (Random). Figure 5 also demonstrates our methods improve perplexity over the natural language prompt on its own.\nNot only does our approach improve the PERPLEXITY (TARGET) score, but both CODEEXEMPLAR-FREE and CODEEXAMPLAR-BASE improves CODELLAMA's raw Pass@1 coding performance by 5.70 \u00b1 2.17% and 5.05 \u00b1 1.70% on average respectively through solely optimizing the prompt (Figure 6). We highlight CODEEXAMPLAR-BASE performs better than the random choice under all settings of N. These results indicate that our prompt optimization techniques have practical application."}, {"title": "Distribution Shift Bottleneck", "content": "We provide further analysis into future directions for CODEEXAMPLAR-BASE. Specifically, we find CODEEXAMPLAR-BASE can be improved by a better training dataset. We arrive at this conclusion by splitting across all few-shot examples instead of splitting across all prompts. Under this setting, the same prompt can occur between training and testing. Hence, the model only needs to learn the importance of individual examples, rather than learn the importance of individual examples and generalize to new prompts. By removing the need to generalize to new pormpts, CODEEXAMPLAR-BASE greatly improves in its ranking predictions (Figure 7). This indicates better dataset design which improves the prompt generalization of CODEEXAMPLAR-BASE can further improve its performance, encouraging development of larger coding datasets. We highlight all the experiments in this work adopt the harder setting, where the train and test set have disjunct natural language descriptions ."}, {"title": "Conclusion", "content": "In this work, we analyzed the effects of few-shot examples on the coding capabilities of large language models (LLMs). Our work identified interesting properties of few-shot examples, including the example complexity, embedding representation, and perplexity. To this end, we proposed 2 effective strategies of picking in-context examples, a model-free algorithm based on perplexity, CODEEXEMPLAR-FREE, and a model-based algorithm trained on data, CODEEXAMPLAR-BASE. We show that both CODEEXEMPLAR-FREE and CODEEXAMPLAR-BASE meaningfully improves CODELLAMA's coding capabilties."}, {"title": "Limitations", "content": "This work studies several aspects of few-shot examples in the prompt. The main experiments are performed on CODELLAMA (Roziere et al., 2023). For future work, we wish to extend to more state-of-the-art models such as DeepSeeker (Guo et al., 2024). This work does not tackle chain of thought prompting, only the original prompt in a zero-shot manner. This work focuses on code generation. While the techniques are applicable to general in-context learning, we leave such study to future work. This work tests on the HUMANEVAL+ (Liu et al., 2024b) benchmark. We leave incorporating harder benchmarks, such as BigCodeBech (Zhuo et al., 2024), to future work."}]}