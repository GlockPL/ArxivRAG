{"title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion", "authors": ["Samarth N Ramesh", "Zhixue Zhao"], "abstract": "As text-to-image models grow increasingly powerful and complex, their burgeoning size presents a significant obstacle to widespread adoption, especially on resource-constrained devices. This paper presents a pioneering study on post-training pruning of Stable Diffusion 2, addressing the critical need for model compression in text-to-image domain. Unlike previous work focused on language models or traditional image generation, our study tackles the complex multimodality generation models, and particularly examines the pruning impact on the textual component and the image generation component separately. We conduct a comprehensive comparison on pruning the model or the single component of the model in various sparsities. Our results yield previously undocumented findings. For example, contrary to established trends in language model pruning, we discover that simple magnitude pruning outperforms more advanced techniques in text-to-image context. Furthermore, our results show that Stable Diffusion 2 can be pruned to 38.5% sparsity with minimal quality loss, achieving a significant reduction in model size. We propose an optimal pruning configuration that prunes the text encoder to 47.5% and the diffusion generator to 35%. This configuration maintains image generation quality while substantially reducing computational requirements. In addition, our work uncovers intriguing questions about information encoding in text-to-image models: we observe that pruning beyond certain thresholds leads to sudden performance drops (unreadable images), suggesting that specific weights encode critical semantics information. This finding opens new avenues for future research in model compression, interoperability, and bias identification in text-to-image models. By providing crucial insights into the pruning behavior of text-to-image models, our study lays the groundwork for developing more efficient and accessible AI-driven image generation systems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and diffusion models often face the challenge of having exceptionally large model sizes [2, 19]. While these extensive parameters allow them to understand high-quality text and generate images, they also result in substantial computational demands and resource consumption [2, 25, 39]. With billions of parameters, these models become prohibitively expensive to operate, limiting their use primarily to large corporations. To address this issue, significant research has been devoted to model compression techniques for deep learning models. Compressing these models can make them deployable on edge devices [11, 27, 28], reduce response times [14, 22], enable real-time applications [26, 27], and greatly increase their accessibility.\nQuantization [10] and pruning [12, 13, 21] are two primary compression approaches. Quantization focuses on reducing the precision of a model's weights, while pruning seeks to identify and eliminate redundant weights. Both methods have been well studied for LLMs but much less has been done for vision language models [24]. Further, recent advancements have introduced post-training pruning methods for LLMs, which offer compressed models without the need for retraining [8, 37].\nHowever, there has been relatively little research on post-training pruning for vision language models. As far as we know, no prior work has addressed post-training pruning of text-to-image models. This paper seeks to fill this gap by investigating the impact of post-training pruning on Stable Diffusion 2 [33]. Additionally, this study examines how pruning impacts the textual encoder and the image generator of the model separately, and identifies the optimal sparsity levels for each component."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-to-Image Models", "content": "The field of image generation was initially dominated by Generative Adversarial Networks (GANs) [1, 9] and Diffusion models [35]. While GANs produce high-quality images, their variability is constrained by data used in the adversarial training process and they require substantial effort to scale for diverse, complex applications [18]. On the other hand, Diffusion models, built"}, {"title": "2.2 Pruning", "content": "Pruning works on the principle that some connections in a network are of less importance than others [21]. The method of finding these connections is of utmost importance for the post-training pruning. We introduce state-of-the-art pruning methods experimented in LLMs and their main difference is how they identify the weights to be pruned.\nMagnitude Pruning. Magnitude-based pruning [12] is one of the simplest and most commonly used techniques for pruning neural networks. It operates under the assumption that a connection's importance correlates with the absolute value of its weight. As described in Equation 1, the weights are ranked according to their magnitude, and the least important weights are pruned by setting them to zero. While this method may not provide optimal performance compared to more advanced techniques, it is easy to implement, computationally efficient, and serves as a strong baseline for further refinement.\nLow Importance Connections = $\\underset{i}{\\text{argmin }} |w_i|$ (1)\nSparseGPT. SparseGPT [8] tackles the task by attempting to solve the problem of identifying the optimal pruning mask. The goal is to find a sparse subset of weights that minimizes reconstruction error. While the exact solution to this problem requires O($d^4$) time complexity [8], where d is the dimension of the hidden layer, SparseGPT achieves an approximate solution in O($d^3$), offering a more efficient approach. This method has consistently outperformed magnitude pruning across LLMs of various sizes.\nWanda. Weight and Activation Pruning (Wanda) [37] is a more recent method that surpasses SparseGPT in performance and has become widely adopted in various text-based models. Wanda is grounded in the observation that emergent properties in LLMs are often associated with the presence of \"outliers\u201d [4]. These outliers"}, {"title": "3 Experimental Setup", "content": "This paper investigates the pruning of Stable Diffusion 2, a widely used text-to-image generation model. Stable Diffusion 2 has 1.2 billion parameters of which 340 million parameters (28%) are in the CLIP text encoder and 860 million parameters (72%) are in the U-Net diffusion generator. Our initial experiments focus on examining the effects of independently pruning individual components of the model while maintaining the integrity of the other parts. For example, only prune the text encoder component and keep the diffusion generator untouched. Following this, full model pruning is explored across multiple axes to determine the optimal balance of sparsities between the text encoder and the diffusion generator. The evaluation of the best configuration for full model pruning is informed by the results obtained from the individual component pruning experiments."}, {"title": "3.1 Pruning Single Component", "content": "For text-encoder-only pruning, four different techniques are tested, namely magnitude pruning, Wanda, magnitude pruning with OWL and Wanda with OWL. For each of these techniques, multiple sparsities are tested in steps of 10% and more granular tests are performed at intervals of interest."}, {"title": "3.2 Full Model Pruning", "content": "Two distinct approaches are employed for the experiments to find the optimal configuration for full model pruning. The first approach aims to determine the best ratio of sparsities between the text encoder and image diffusion generator if a specific full model sparsity is desired. \nThe second approach to full model pruning relies on the findings from the individual component pruning experiments. Both the text encoder and the diffusion generator exhibit identifiable drop-off points in performance. Assuming that the sub-models remain largely unaffected until reaching these drop-off points, the full model is pruned to align with the drop-off thresholds of both sub-models. This leads to a recommended configuration for maximal pruning that balances performance and sparsity effectively."}, {"title": "3.3 Dataset", "content": "We use the Microsoft COCO: Common Objects in Context (MSCOCO) 2017 dataset [23] which has a large number of real images and corresponding captions. MSCOCO is a commonly used dataset for"}, {"title": "3.4 Evaluation", "content": "FID. The Fr\u00e9chet Inception Distance (FID) [16] defines a distance metric between two sets of images and measures how different the two sets are. A lower FID indicates that the two sets of images are very similar.\nIn our experiments, we use the FID to compare images generated from MSCOCO captions with corresponding real images. For each model we generate 10,000 images to calculate the FID.\nCLIP Score. The CLIP Score [15] uses a multi-modal text and image model to directly compare a generated image with the provided prompt. It uses the CLIP model to calculate the similarity and a lower CLIP Score indicates a higher correlation in the semantic content of the prompt and image.\nIn our experiments, for each pruned model, we generate 10,000 images and calculate the average similarity of each image with its prompt using the CLIP Score."}, {"title": "4 Results and Analysis", "content": "The primary objective of this study is to analyze the impact of pruning on text-to-image models, specifically focusing on the individual components of Stable Diffusion 2, namely the CLIP text encoder and the U-Net diffusion generator. Our aim is to explore the optimal trade-off between model performance and computational resource efficiency. We initially approach this problem by examining the effects of pruning when targeting only one component at a time. The insights gained from this analysis subsequently inform our strategy for full model pruning.\nAll experimentation is quantitatively evaluated using FID [16] and CLIP Score [15] metrics on 10,000 images generated from captions present in the MSCOCO 2017 dataset [23]. These metrics are reinforced by qualitative evaluation of generated image quality."}, {"title": "4.1 CLIP Pruning Only", "content": "We experiment with two pruning methods on the CLIP text encoder and extend these by applying OWL to test whether outlier weighting improves performance in text-to-image models. Figures 1, 3, 5, 7 show the FID and CLIP scores for each method. At lower sparsity levels, all techniques result in minimal performance degradation. However, as sparsity increases, each method experiences a sharp performance decline at specific thresholds. Qualitative evaluations of the generated images corroborate these quantitative results.\nMagnitude Pruning. As the simplest and most straightforward method, magnitude pruning serves as an important baseline. When pruning up to a sparsity of 60%, the model performs similarly to the original, with only a minimal decline in performance. However, at 62.5% sparsity, the model experiences a sudden drop in performance.\nWanda Pruning. Despite being one of the most effective pruning techniques for many language models, Wanda appears to be poorly suited for pruning the text component in this text-to-image model.\nComparison of Pruning Techniques. The finding that Wanda pruning is outperformed by magnitude pruning is quite surprising and underscores the need for pruning techniques tailored specifically to text-to-image models.\nAnother key observation is that Outlier Weighted Layerwise (OWL) pruning, when applied on top of both magnitude and Wanda pruning, results in only marginal improvements."}, {"title": "4.2 U-Net Pruning Only", "content": "For the diffusion component of the model, we observe a more gradual decline in performance as pruning increased. The model maintains strong performance at lower sparsity levels, but its performance steadily decreases as sparsity exceeded 40%."}, {"title": "4.3 Comparison between Text and Image Pruning", "content": "To address the question of which component-the text encoder or the image diffusion generator-yields better results when pruned, we compare the best-performing pruned models for each approach. This comparison is presented in Table 2.\nComparing the two models, which qualitatively produce similar results, reveals some intriguing findings. First, pruning the image diffusion generator achieves greater overall sparsity due to its larger size, contributing more significantly to the full model's sparsity. Additionally, pruning the text encoder results in a worse CLIP score, while pruning the image diffusion generator leads to a worse FID score. This raises interesting questions about what these metrics are actually measuring."}, {"title": "4.4 Full Model Pruning", "content": "First Approach. Given the computational demands of evaluating models, conducting an exhaustive grid search to identify the optimal pruning configuration is impractical. All experiments used magnitude pruning for both components of the model.\nTables 3, 4, 5 demonstrate that the model performs better when the majority of the sparsity is allocated to the image component.\nSecond Approach. We examine the model pruned to both thresholds established in the individual pruning experiments."}, {"title": "5 Conclusions", "content": "This research presents the first comprehensive study on post-training pruning of text-to-image models, specifically focusing on Stable Diffusion 2. Our findings reveal unexpected behaviors and provide crucial insights into the compression of these complex models. We demonstrate that Stable Diffusion 2 can be effectively pruned to 38.5% sparsity with minimal quality loss, achieving a significant reduction in model size. Contrary to established trends in language model pruning, we found that simple magnitude pruning outperforms more advanced techniques in this context."}, {"title": "6 Future Work", "content": "Our results and analyses suggest several future research directions. Our findings indicate that both Wanda and SparseGPT do not effectively extend to these models, while magnitude pruning-typically regarded as the baseline method for pruning-emerges as the best-performing approach in our study."}, {"title": "A Other Results", "content": ""}]}