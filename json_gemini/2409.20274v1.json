{"title": "Probabilistic Answer Set Programming with Discrete and Continuous Random Variables", "authors": ["Damiano Azzolini", "Fabrizio Riguzzi"], "abstract": "Probabilistic Answer Set Programming under the credal semantics (PASP) extends Answer Set Programming with probabilistic facts that represent uncertain information. The probabilistic facts are discrete with Bernoulli distributions. However, several real-world scenarios require a combination of both discrete and continuous random variables. In this paper, we extend the PASP framework to support continuous random variables and propose Hybrid Probabilistic Answer Set Programming (HPASP). Moreover, we discuss, implement, and assess the performance of two exact algorithms based on projected answer set enumeration and knowledge compilation and two approximate algorithms based on sampling. Empirical results, also in line with known theoretical results, show that exact inference is feasible only for small instances, but knowledge compilation has a huge positive impact on the performance. Sampling allows handling larger instances, but sometimes requires an increasing amount of memory. Under consideration in Theory and Practice of Logic Programming (TPLP).", "sections": [{"title": "1 Introduction", "content": "Almost 30 years ago (Sato 1995), Probabilistic Logic Programming (PLP) was proposed for managing uncertain data in Logic Programming. Most of the frameworks, such as PRISM (Sato 1995) and ProbLog (De Raedt et al. 2007), attach a discrete distribution, typically Bernoulli or Categorical, to facts in the program and compute the success probability of a query, i.e., a conjunction of ground atoms. Recently, several extensions have been proposed for dealing with continuous distributions as well (Azzolini et al. 2021; Gutmann et al. 2011a;b), that greatly expand the possible application scenarios.\nAnswer Set Programming (Brewka et al. 2011) is a powerful formalism for representing complex combinatorial domains. Most of the research in this field focuses on deterministic programs. There are three notable exceptions: LPMLN (Lee and Wang 2016), P-log (Baral et al. 2009), and Probabilistic Answer Set Programming under the credal semantics (Cozman and Mau\u00e1 2016). The first assigns weights to rules while the last two attach probability values to atoms. However, all three allow discrete distributions only."}, {"title": "2 Background", "content": "In this section we review the main concepts used in the paper."}, {"title": "2.1 Answer Set Programming", "content": "In this paper, we consider Answer Set Programming (ASP) (Brewka et al. 2011). An answer set program contains disjunctive rules of the form $h_1; ... ; h_m :- b_1, ..., b_n.$ where the disjunction of atoms $h_i$ is called head and the conjunction of literals $b_i$ is called body. Rules with no atoms in the head are called constraints while rules with no literals in the body and a single atom in the head are called facts. Choice rules are a particular type of rules where a single head is enclosed in curly braces as in ${a} :- b_1,...,b_n,$ whose meaning is that a may or may not hold if the body is true. They are syntactic sugar for $a; not\\_a :- b_1, ..., b_n$ where $not\\_a$ is an atom for a new predicate not appearing elsewhere in the program. We allow aggregates (Alviano and Faber 2018) in the body, one of the key features of ASP that allows representing expressive relations between the objects of the domain of interest. An example of a rule r0 containing an aggregate is: $v(A) :- #count{X : b(X)} = A.$ Here, variable A is unified with the integer resulting from the evaluation of the aggregate $#count{X : b(X)}$, that counts the elements X such that b(X) is true.\nThe semantics of ASP is based on the concept of stable model (Gelfond and Lifschitz 1988). Every answer set program has zero or more stable models, also called answer sets. An interpretation I for a program P is a subset of its Herbrand base. The reduct"}, {"title": "2.2 Probabilistic Answer Set Programming", "content": "Uncertainty in Logic Programming can be represented with discrete Boolean probabilistic facts of the form $\\Pi :: a$ where $\\Pi \\in [0,1]$ and a is an atom that does not appear in the head of rules. These are considered independent: this assumption may seem restrictive but, in practice, the same expressivity of Bayesian networks can be achieved by means of rules and extra atoms (Riguzzi 2022). Every probabilistic fact corresponds to a Boolean random variable. With probabilistic facts, a normal logic program becomes a probabilistic logic program. One of the most widely adopted semantics in this context is the Distribution Semantics (DS) (Sato 1995). Under the DS, each of the n probabilistic facts can be included or not in a world w, generating $2^n$ worlds. Every world is a normal logic program. The DS requires that every world has a unique model. The probability of a world w is defined as $P(w) = \\prod_{f_i \\in w} \\Pi_i \\cdot \\prod_{f_i \\notin w}(1 - \\Pi_i)$.\nIf we extend an answer set program with probabilistic facts, we obtain a probabilistic answer set program that we interpret under the credal semantics (CS) (Cozman and Mau\u00e1 2016; 2020). In the following, when we write \"probabilistic answer set program\", we assume that the program follows the credal semantics. Similarly to the DS, the CS defines a probability distribution over worlds. However, every world (which is an answer set program in this case) may have 0 or more stable models. A query q is a conjunction of ground literals. The probability P(q) of a query q lies in the range $[\\underline{P}(q), \\overline{P}(q)]$ where\n$\\overline{P}(q) = \\sum_{w_i \\text{ such that } m \\in AS(w_i), m \\models q} P(w_i)$,\n$\\underline{P}(q) = \\sum_{w_i \\text{ such that } \\forall m \\in AS(w_i), m \\models q} P(w_i).$ (1)\nThat is, the lower probability is given by the sum of the probabilities of the worlds where the query is true in every answer while the upper probability is given by the sum of the probabilities of the worlds where the query is true in at least one answer set. Here, as usual, we assume that all the probabilistic facts are independent and that they cannot appear as heads of rules. In other words, the lower (upper) probability is the sum of the probabilities of the worlds from which the query is a cautious (brave) consequence. Conditional inference means computing upper and lower bounds for the probability of a query q given evidence e, which is usually given as a conjunction of ground literls. The"}, {"title": "2.3 ProbLog and Hybrid ProbLog", "content": "A ProbLog (De Raedt et al. 2007) program is composed by a set of Boolean probabilistic facts as described in Section 2.2 and a set of definite logical rules, and it is interpreted under the Distribution Semantics. The probability of a query q is computed as the sum of the probabilities of the worlds where the query is true: every world has a unique model so it is a sharp probability value.\nGutmann et al. (2011a) proposed Hybrid ProbLog, an extension of ProbLog with continuous facts of the form\n$(X, \\phi) :: b$\nwhere b is an atom, X is a variable appearing in b, and $\\phi$ is a special atom indicating the continuous distribution followed by X. An example of continuous fact is $(X, gaussian(0, 1)) :: a(X)$, stating that the variable X in a(X) follows a gaussian distribution with mean 0 and variance 1. A Hybrid ProbLog program P is a pair (R,T) where R is a set of rules and $T = T_c \\cup T_d$ is a finite set of continuous ($T_c$) and discrete ($T_d$) probabilistic facts. The value of a continuous random variable X can be compared only with constants through special predicates: $below(X, c_0)$ and $above(X, c_0)$, with $c_0 \\in R$, that succeed if the value of X is respectively less than or greater than $c_0$, and $between(X, c_0, c_1)$, with $c_0, c_1 \\in R, c_0 < c_1$, that succeeds if $X \\in [c_0, c_1]$.\nThe semantics of Hybrid ProbLog is given in (Gutmann et al. 2011a) in a proof theoretic way. A vector of samples, one for each continuous variable, defines a so-called continuous subprogram, so the joint distribution of the continuous random variables defines a joint distribution (with joint density f(x)) over continuous subprograms. An interval $I \\in R^n$, where n is the number of continuous facts, is defined as the cartesian product of an interval for each continuous random variable and the probability $P(X \\in I)$ can be computed by integrating f(x) over I, i.e.,\n$P(X \\in I) = \\int_I f(x) dx$.\nGiven a query q, an interval I is called admissible if, for any x and y in I and for any truth value of the probabilistic facts, the truth value of the q evaluated in the program obtained by assuming X takes value x and y is the same. In other words, inside an admissible interval, the truth value of a query is not influenced by the values of the"}, {"title": "3 Hybrid Probabilistic Answer Set Programming", "content": "Probabilistic logic programs that combine both discrete and continuous random variables are usually named hybrid\u00b9. In this paper, we adopt the same adjective to describe probabilistic answer set programs with both discrete and continuous random variables, thus\n\n\u00b9 In ASP terminology, the word \"hybrid\u201d is usually adopted to describe an extension of ASP, such as the one by (Janhunen et al. 2017). Here we use the word hybrid exclusively to denote the presence of discrete and continuous random variables."}, {"title": "4 Algorithms", "content": "In this section, we describe two exact and two approximate algorithms for performing inference in HPASP."}, {"title": "4.1 Exact Inference", "content": "Modifying the PASTA solver. We first modified the PASTA solver (Azzolini et al. 2022)\u00b2, by implementing the conversion of the hybrid program into a regular probabilistic answer set program (Algorithm 1). PASTA performs inference on PASP via projected answer set enumeration. In a nutshell, to compute the probability of a query (without loss of generality assuming here it is an atom), the algorithm converts each probabilistic fact into a choice rule. Then, it computes the answer sets projected on the atoms for the probabilistic facts and for the query. In this way, PASTA is able to identify the answer set pertaining to each world. For each world, there can be 3 possible cases: i) a single projected answer set with the query in it, denoting that the query is true in every answer set, so this world contributes to both the lower and upper probability; ii) a single answer set without the query in it: this world does not contribute to any probability; iii) two answer sets, one with the query and one without: the world contributes only to the upper probability. There is a fourth implicit and possible case: if a world is not present, this means that the ASP obtained from the PASP by fixing the selected probabilistic facts is unsatisfiable. Thus, in this case we need to consider the normalization factor of Equation 9. PASTA already handles this by keeping track of the sum of the probabilities of the computed worlds. The number of generated answer sets depends on the number of Boolean probabilistic facts and on the number of intervals for the continuous random variables, since every interval requires the introduction of a new Boolean probabilistic fact. Overall, if there are d discrete probabilistic facts and c continuous random variables, the total number of probabilistic facts (after the conversion of the intervals) becomes $T = d + \\sum_{i=1}^{c}(k_i - 1)$, where $k_i$ is the number of intervals for the i-th continuous fact. Finally, the total number of generated answer sets is bounded above by $2^{T+1}$, due to the projection on the probabilistic facts. Clearly, generating an exponential number of answer sets is intractable, except for trivial domains.\n\n\u00b2 Code and datasets available on GitHub at https://github.com/damianoazzolini/pasta and on zen-odo at https://doi.org/10.5281/zenodo.11653976."}, {"title": "4.2 Approximate Inference", "content": "Approximate inference can be performed by using the definition of the sampling semantics and returning the results after a finite number of samples, similarly to what is done for programs under the DS (Kimmig et al. 2011; Riguzzi 2013). It can be performed both on the discretized program and directly on the hybrid program.\nSampling the discretized program. In the discretized program, we can speed up the computation by storing the sampled worlds to avoid calling again the answer set solver in case a world has been already sampled. However, the number of discrete probabilistic facts obtained via the conversion is heavily dependent on the types of constraints in the program.\nSampling the hybrid program. Sampling the hybrid program has the advantage that it allows general numerical constraints, provided they involve only continuous random variables and constants. In this way, we directly sample the continuous random variables and directly test them against the constraints that can be composed of multiple variables and complex expressions. In fact, when constraints among random variables are considered, it is difficult to discretize the domain. Even if the samples would be always different (since the values are floating point numbers), also here we can perform caching, as in the discretized case: the constraints, when evaluated, are still associated with a Boolean value (true or false). So, we can store the values of the evaluations of each constraint: if a particular configuration has already been encountered, we retrieve its contribution to the probability, rather than calling the ASP solver.\nApproximate algorithm description. Algorithm 2 illustrates the pseudocode for the sampling procedure: for the sampling on the discretized program, the algorithm discretizes the program by calling DISCRETIZE (Algorithm 1). Then, for a number of samples s, it samples a world by including or not every probabilistic fact into the program (function SAMPLEWORLD) according to the probability values, computes its answer sets with function COMPUTEANSWERSETS (recall that a world is an answer set program), checks whether the query q is true in every answer set (function QUERYINEVERYANSWERSET) or in at least one answer set (function QUERYINATLEASTONEANSWERSET), and updates the lower and upper probability bounds accordingly. At the end of the s iterations, it returns the ratio between the number of samples contributing to the lower and upper probability and the number of samples taken. The procedure is analogous (but without discretization) in the case the sampling on the original program. A world is sampled with function SAMPLEVARIABLESANDTESTCONSTRAINTS: it takes a sample for each continuous random variable, tests that value against each constraint specified in the program, and removes it if the test succeeds, otherwise it removes the whole rule containing it. The remaining part of the algorithm is the same.\nWe now present two results regarding the complexity of the sampling algorithm. The first provides a bound on the number of samples needed to obtain an estimate of the upper (or lower) probability of a query within a certain absolute error. The second result provides a bound on the number of samples needed to obtain an estimate within a certain relative error."}, {"title": "5 Experiments", "content": "We ran experiments on a computer with 8 GB of RAM and a time limit of 8 hours (28800 seconds). We generated five synthetic datasets, t1, t2, t3, t4, and t5, where every world of all the discretized versions of the programs has at least one answer set. We use the SciPy library (Virtanen et al. 2020) to sample continuous random variables. The following code snippets show the PASTA-backed programs; the only difference with the ones for aspcs is in the negation symbol, not for the former and \\+ for the latter. For all the datasets, we compare the exact algorithms and the approximate algorithms based on sampling, with an increasing number of samples. We record the time required to parse the program and to convert the HPASP into a PASP together with the inference time. The time for the first two tasks is negligible with respect to the inference time.\nDataset t1. In t1 we consider instances of increasing size of the program shown in Example 4. Every instance of size n has n/2 discrete probabilistic facts $d_i$, n/2 continuous random variables $c_i$ with a Gaussian distribution with mean 0 and variance 1, n/2 pair of rules $q0 :- below(c_i, 0.5), not q1$ and $q1 :- below(c_i, 0.5), not q0$, i = {1, ..., n/2}, and n/2 rules $q0 :- below(c_i, 0.7), d_i$, one for each i for i = {1,...,n/2}. The query is q0.\nExact inference results. The goal of benchmarking the exact algorithms is threefold: i) identifying how the number of continuous and discrete probabilistic facts influences"}, {"title": "6 Related Work", "content": "Probabilistic logic programs with discrete and continuous random variables have been the subject of various works. Gutmann et al. (2011a) proposed Hybrid ProbLog, an extension of ProbLog (De Raedt et al. 2007) to support continuous distributions. There are several differences with Hybrid ProbLog: first, Hybrid ProbLog focuses on PLP while our approach on PASP. Thus, the syntax and semantics are different. For the syntax, in PASP we can use rich constructs such as aggregates, that greatly increase the expressivity of the programs. For the semantics, at high level, PLP requires that every world (i.e., combination of probabilistic facts) has exactly one model while PASP does not. Another difference with Hybrid ProbLog is in the discretization process: Hybrid ProbLog discretizes the proofs of a program while we directly discretize the program. Moreover, their inference algorithm is based on the construction of a compact representation of the program via Binary Decision Diagrams, while we use both ASP solvers with projective solutions and knowledge compilation targeting NNF.\nDistributional Clauses (Gutmann et al. 2011b) and Extended PRISM (Islam et al. 2012) are two other proposals to handle both discrete and continuous random variables. The semantics of the former is based on a stochastic extension of the immediate consequence Tp operator, while the latter considers the least model semantics of constraint logic programs (Jaffar and Maher 1994) and extends the PRISM framework (Sato 1995). Michels et al. (2015) introduced Probabilistic Constraint Logic Programming whose semantics is based on an extension of Sato's Distribution Seman-"}, {"title": "7 Conclusions", "content": "In this paper we propose Hybrid Probabilistic Answer Set Programming, an extension of Probabilistic Answer Set Programming under the credal semantics that allows both discrete and continuous random variables. We restrict the types of possible numerical constraints and, to perform exact inference, we convert the program containing both discrete probabilistic facts and continuous random variables into a program containing only discrete probabilistic facts, similarly to (Gutmann et al. 2011a). We leverage two existing tools for exact inference, one based on projected answer set enumeration and one based on knowledge compilation. We also consider approximate inference by sampling either the discretized or the original program. We tested the four algorithms on different datasets. The results show that the exact algorithm based on projected answer set enumeration is feasible only for small instances while the one based on knowledge compilation can scale to larger programs. Approximate algorithms can handle larger instances and sampling the discretized program is often faster than sampling the original program. However, this has a cost in terms of required memory, since the discretization process adds a consistent number of rules and probabilistic facts. In the future, we plan to extend our framework to also consider comparisons involving more than one continuous random variable and general expressions, as well as considering lifted inference approaches (Azzolini and Riguzzi 2023b) and handle the inference problem with approximate answer set counting (Kabir et al. 2022)."}]}