{"title": "Keep what you need : extracting efficient subnetworks from large audio representation models", "authors": ["David Genova", "Philippe Esling", "Tom Hurlin"], "abstract": "Recently, research on audio foundation models has witnessed notable advances, as illustrated by the ever improving results on complex downstream tasks. Subsequently, those pretrained networks have quickly been used for various audio applications. These improvements have however resulted in a considerable increase both in size and complexity of these models. Along the environmental concerns this issue raises, this prevents the deployment of such networks on consumer-level devices, and precludes their use for real-time applications. Moreover, this appears contradictory with the specificity of the tasks for which these models are used, which are often simpler compared to extracting a rich, multi-purpose representation from any type of audio data. In this paper, we address this issue with a simple, yet effective method to extract lightweight specialist subnetworks from large foundation models. Specifically, we introduce learnable binary masks in-between the layers of a pretrained representation model. When training the end-to-end model on a downstream task, we add a sparsity-inducing loss to the overall objective, hence learning a compact subnetwork specialized on a single task. Importantly, the weights of the foundation model are kept frozen, resulting into low additional training costs. Once trained, the masked computational units can then be removed from the network, thus implying significant performance gains. We assess our method on three widespread audio foundation models, each based on a different backbone architecture, and illustrate its effectiveness on common audio representation evaluation tasks, as well as its versatility on both speech, music, and general audio. Code for reproducing the results and supporting webpage are available at https://github.com/gnvIRCAM/Audio-representation-trimming.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have seen the emergence of deep neural networks as powerful and flexible solutions to address complex tasks in various domains. The development of foundation models has allowed to extract rich representations from unlabelled data, which have found numerous applications in several fields, such as natural language processing [1] or computer vision [2]. In audio, these models have led to significant progress in multiple areas of research, from audio information retrieval [3] to generative modelling [4]. After pre-training these models on vast corpuses of sounds taken from various audio domains, they are then adapted to downstream tasks on smaller datasets with limited data available, where fitting a randomly initialized network would likely result in overfitting. Hence, models such as HuBERT [5], CLAP [6], or MERT [7] have demonstrated state-of-the-art performance in transfer learning on speech, music, and general audio-related tasks, e.g. speech recognition, music auto-tagging, or environmental sounds classification. However, foundation models have necessitated a continuous increase both in terms of number of parameters and computational complexity, which seems compulsory to encode sounds from highly variable audio domains into informative representations. Notably, this prevents their use for embedded applications on devices with limited computational resources. Moreover, despite being trained on simpler tasks with homogeneous data, the resulting models have even more parameters, due to the classification head, as well as potential additional modules [8]. Hence, it is reasonable to assume smaller subnetworks could be extracted from the large foundation model, by keeping only the parts relevant for a single task, on a restrained set of sounds.\nIn order to mitigate such computational burdens, extensive research has carried on neural network compression. Among existing methods, network pruning [9] consists in identifying the less important weights of a network, which can then be removed without altering the performance of the model. While many pruning approaches allow for a substantial reduction in number of parameters [10], [11], they usually require either fine-tuning or retraining the full model to achieve high compression ratios, hence being computationally expensive, and requires to have access to the original training dataset, which might not be publicly available. In this paper, we address the structured pruning of audio representation models in a computationally-efficient way. Specifically, we introduce learnable binary masks after each intermediate layer of a pretrained foundation model. When training the model on a downstream task, we add a sparsity-inducing loss to the overall objective, hence learning a compact subnetwork specialized on a single task. Importantly, the weights of the foundation model are kept frozen, resulting into lower training costs than fine-tuning. Once trained, the masked computational units can then be removed from the network, thus implying inference speed-up and disk size reduction. We assess our method on three widespread audio foundation models and illustrate its effectiveness on common audio representation evaluation tasks for both music, speech, and general audio."}, {"title": "II. PROPOSED APPROACH", "content": "Our goal is to remove computational blocks (i.e. columns for linear layers, convolution channels and attention heads) in large foundation models when training them on downstream tasks with smaller datasets. To this end, we introduce learnable masks within the representation model, and introduce a sparsity-inducing loss to the overall objective.\nRepresentation models are a class of neural networks that aim at extracting compact embeddings from high-dimensional data. These models are usually trained in a self-supervised way by either building discriminable representations using contrastive learning [12], or by enforcing predictability of the embeddings in mask token modelling [7]. We base our work on three audio foundation models, each being trained on distinct audio domains (music, sound events or speech) and with different architectures (convolutional, transformer and conformer). Specifically, CLAP [6] is a multimodal network trained on paired text-audio data from multiple datasets [13]-[16]. Using contrastive learning, the model builds a joint embedding space in which matching texts and sounds are encoded into aligned projections. Authors experiment with two different architectures, one using a 4-layer transformer network identical to HTSAT [17], the other being a 14-layer convolutional network introduced in PANNs [18]. Whereas CLAP embeds an entire sound into a single vector, MusicFM [19] and Wav2Vec 2.0 [20] predict temporal sequences of codes. Both models are trained using masked token modeling, by randomly masking parts of the input, and then training the network to either predict the masked values or to identify them from a set of negative samples. MusicFM relies on the architecture of BEST-RQ [21] (12 conformer layers [22]), and is trained on musical data from both Free Music Archive [19] and an internal dataset. Wav2Vec 2.0 is composed of a 7 layers deep convolutional feature extractor, followed by 12 or 24 (base/large) transformer layers, and is trained on speech data from either Librispeech [23] or LibriVox [24]. As both Wav2Vec2 and MusicFM rely on a transformer backbone, we chose to use the convolutional CLAP model to cover a broader scope of architectures.\nOnce trained to extract informative representations, foundation models can then be trained on more specific tasks, usually by adding a smaller network (such as a multi-layer perceptron or a recurrent network), denoted as the head, at the end of the encoder. The resulting model is then trained using either linear probing, which tunes only the parameters of the head, or finetuning, where all the weights of the model are trained. Whereas linear probing is more compute-efficient, but can lead to subpar performances, fine-tuning often yields better results, but involves higher training costs and is more prone to overfitting. Hybrid approaches can be found in parameter-efficient transfer learning (PETL), which provides both flexible and efficient strategies to transfer the knowledge of pretrained models.\nAdapter-based methods [8] introduce processing modules inbetween the layers of foundation models. Closer to our work, Scaling-and-Shifting your Features (SSF) [25] adds learnable affine transforms modulating the intermediate features of the encoder, namely $f^{(l, \\theta)}(x) = \\gamma_l f^{(l, \\theta)}(x) + \\beta_l$, where $\\gamma_l, \\beta_l \\in \\mathbb{R}^{d_{out}}$ are respectively scale and shift coefficients, $f^{(l, \\theta)}$ is the l-th layer of the encoder, and $d_{out}$ is the embedding dimension of $f^{(l, \\theta)}$. Hence, our approach can be seen as a variant of SSF by enforcing $\\beta_l = 0$ and $\\gamma_l \\in \\{0, 1\\}^{d_{out}}$, while allowing reduction of the model by removing zeroed units once the training is complete.\nDespite all of the benefits of representation learning, foundation models tend to be notably large, as they must have enough capacity to build rich representations for highly diverse data. Hence, such networks can not be used for offline applications, especially on embedded devices with limited computational resources.\nEver since the advent of large neural networks, researchers have dedicated efforts to make models smaller and faster. Early works on neural network pruning [9], [26] already showed that smaller networks can be found within larger ones, with improved generalization and reduced training costs. Follow-up research [10], [27] established that most of the parameters of overparameterized networks could be removed (up to 95%) while reaching equivalent (often even higher) accuracy. However, identifying these subnetworks requires multiple training cycles, and lead to sparse weight matrices which can not be converted into computational speed-up. Follow-up research proposed several approaches to prune networks in a single training run [28], [29], as well as removing parameters in a structured fashion [30], thus truly leading to model reduction. However, these approaches do not match the performance of those relying on repeated training cycles, especially for structured pruning, which produces lower compression ratios than unstructured variants at matching accuracy.\nGiven a pretrained foundation model, we consider linear probing of the network (i.e. keeping its weights frozen) on a given downstream task. In this context, we hypothesize that many units of the model can be removed, while keeping a sufficiently informative representation for the considered task and dataset. Indeed, we believe that, while models with large capacity are necessary to build informative representations of audio data, downstream tasks do not leverage all of the available information contained in these embeddings, hence could be solved with a well-chosen subset of the network units. We define the end-to-end model as $f(x) = g(e(x, \\psi), \\theta)$, where $g(., \\theta)$ is the classification head and $e(., \\psi)$ is the encoder (trainable parameters are indicated in bold). We focus on structural units of the encoder that can be entirely removed, such as convolution channels or attention heads, and note $l(., \\psi_i), i \\in \\{1, ..., L\\}$ the computational blocks composing the encoder, with L being the number of such"}, {"title": "III. EXPERIMENTS", "content": "Datasets and tasks: We chose 9 publicly available datasets for downstream evaluation, corresponding to 3 tasks per audio domain (speech, sound event, music). For each dataset, we follow the official training/validation/test splits. Librispeech [23] is a corpus of audiobooks sampled at 16kHz, which we use for Automatic Speech Recognition (ASR). Fluent Speech Commands [31] is a dataset of 16kHz recordings designed for Intent Classification (IC). It is composed of 23,000 audios, each utterance corresponding to a specific action, object, and location. Speech Commands [32] comprises 65,000 1-second audio from thousands of speakers, used for keyword-spotting. ESC50 [33] is a dataset of 2000 environmental sounds, distributed in 50 classes (e.g. dog, vacuum cleaner or human laugh), each recording being 5 seconds long. Urbansound8k [34] is a collection of 8372 4 seconds-long samples from 10 classes (such as gunshot or jackhammer) for sound event classification. FSD50k [13] is a sound event dataset for multilabel classification, in which each sound is associated to a set of labels from 200 diverse categories (notably including speech and music). GTZAN [35] is a dataset composed of 1000 30-seconds-long tracks, divided into 10 musical genres. To avoid biases from the original dataset, we use the fault-filtered split\u00b9.\nMagnaTagTune [36] is composed of more than 25,000 30-seconds-long musical clips labelled from a set of 188 tags. We keep the 50 most frequents ones for music auto-tagging. NSynth [37] is a dataset of 305,979 musical notes sampled at 16kHz, which we use for pitch estimation.\nFor evaluation metrics, we use weighted-accuracy (w-Acc.) for classification tasks, mean-Average precision (mAP) for autotagging, and word-error rate (WER) for ASR.\nImplementation details: We adapt the architecture of the head to the downstream task. Specifically, for classification and auto-tagging, we use a 2-layer MLP with hidden dimension 1024 and ReLU activation. For ASR, we add a 2-layer BiLSTM with hidden dimension 256. During training, the weights of the encoder are frozen and we train only the probing head and the masks. All models are trained for 100,000 steps using the Adam optimizer and a learning rate of le-3, using cross-entropy loss for classification, binary cross-entropy for music auto-tagging, and CTC loss for speech recognition. We set the weight of the sparsity loss to match the magnitude of the task objective. For each model, we report the results for trimming ratios (i.e number of parameters removed from the network) of 25%, 50% and 75%, which corresponds to different values of $t \\in [0.3,0.7]$ for each model and task.\nBaselines and ablations: For each model, we train a model of equivalent size from scratch, with the same capacity as the trimmed model. Specifically, given a specific task, we use the mask obtained using our method to trim the foundation model. We then re-initialize the remaining weights and train the newly obtained model from scratch. For MusicFM and Wav2Vec2, we apply learning rate warmup for 25,000 steps, which is known to play a critical role when training transformers, and lower the final learning rate value to le-4. We also evaluate our approach against the SSF approach, by removing the sparsity loss and replacing the mask modules with modulation layers."}, {"title": "IV. RESULTS", "content": "In Table I, we present the results of our approach for the aforementioned models and datasets. We found that, for most models, our approach outperforms regular linear probing up to 25% of removed parameters, and still nearly matches the performance of the full model for even larger sparsity ratios (40-50%). For most tasks, our method consistently produces better results than training a network of equivalent size from scratch (which almost always results in overfitting), while being significantly cheaper to train. In most cases, SSF outperforms our approach on accuracy, which is to be expected as this approach can adapt the network features with more flexibility than ours. Yet, SSF does not lead to any reduction of the model. When training Wav2Vec2 for ASR, we note performance drops at relatively small trimming ratios (compared to the other models and tasks), which can be attributed to the complexity of this task (being both a classification and sequence alignment problem). Yet, the trimmed networks still reach competitive performance (7.2% WER for 40% sparsity) and could still be used for efficient ASR with only a small accuracy loss. We note a similar phenomenon with CLAP for mutilabel classification on FSD, where trimming the model always results in lower mAP, which can be explained by the high diversity of sounds and labels within the dataset. Generally, we note that the optimal trimming ratio (i.e. number of removed units associated to the best evaluation score) is strongly dependent on the task. Indeed, as illustrated in the two rightmost columns of Table I, models trained for complex tasks (e.g. seq2seq, multiple labels or diverse dataset) will require larger subnetworks than when trained on simpler tasks such as single label classification.\nIn Table II, we present the computational gains when trimming foundation models using our method. As these gains will depend on the percentage of removed units, for each model, we set a maximal performance drop of 5% (relatively to the full model), and report the metrics for the fastest model within this range and averaged across tasks, namely classification (classif.), audio tagging and ASR. Hence, the values in Table II should be read as expected gains for tasks of comparable complexity."}, {"title": "V. CONCLUSION", "content": "In this paper, we showed that foundation models can be drastically reduced when used in downstream tasks setups. We illustrated our approach in a wide variety of cases and different architectures and found that, for most cases, a smaller model can achieve similar (and even in some cases better) performance than the full network. We also found that trimmed models effectively yield faster inference and decreased disk size. Hence, our approach provides a step towards making such models suitable for on-device applications. For future works, we seek to extend our method to a wider set of applications, notably for audio generative modelling, which has greatly benefited from the improvements in representation learning. Moreover, we seek to better understand the structure of these learned masks, and how we could leverage them to better understand the features learned by audio foundation models."}]}