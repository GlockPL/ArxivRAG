{"title": "LOBG: Less Overfitting for Better Generalization in Vision-Language Model", "authors": ["Chenhao Ding", "Xinyuan Gao", "Songlin Dong", "Yuhange He", "Qiang Wang", "Alex Kot", "Yihong Gong"], "abstract": "Existing prompt learning methods in Vision-Language Models (VLM) have effectively enhanced the transfer capability of VLM to downstream tasks, but they suffer from a significant decline in generalization due to severe overfitting. To address this issue, we propose a framework named LOBG for vision-language models. Specifically, we use CLIP to filter out fine-grained foreground information that might cause overfitting, thereby guiding prompts with basic visual concepts. To further mitigate overfitting, we developed a structural topology preservation (STP) loss at the feature level, which endows the feature space with overall plasticity, allowing effective reshaping of the feature space during optimization. Additionally, we employed hierarchical logit distilation (HLD) at the output level to constrain outputs, complementing STP at the output end. Extensive experimental results demonstrate that our method significantly improves generalization capability and alleviates overfitting compared to state-of-the-art approaches.", "sections": [{"title": "1. Introduction", "content": "Vision-language models (VLMs), such as CLIP [27] and ALIGN [12], have shown exceptional generalization capabilities for downstream tasks [6, 17, 20]. Prompt learning has emerged as a more efficient alternative to fine-tuning VLMs, such as CoOp [44], introducing learnable prompt vectors to adapt models for downstream tasks. However, since prompts are optimized for specific tasks, prompt models tend to overfit as training progresses, losing the original CLIP model's ability to generalize to unseen tasks. Thus, maintaining generalization to unseen tasks while learning specific tasks is a crucial challenge in prompt learning, known as Base to Novel (B2N) [43] prompt learning.\n\nThe present mainstream approaches for the B2N task can be divided into categories: (i) Learning class-agnostic or independent prompts to reduce prompt overfitting [14, 43]. For example, CoCoOP [43] generates input-conditioned tokens for each instance rather than specific classes, thus making it less sensitive to class shift. (ii) Maintain consistency between the prompt model and the frozen CLIP features to achieve the original generalization ability for new tasks [15, 39]. PromptSRC [15], for instance, introduces knowledge distillation techniques to maintain consistency between the prompt model and the original CLIP features and logits. Although these methods help prevent prompt overfitting and improve model generalization to some extent, the performance improvements for unseen classes are quite limited. As shown in Figure 1 (b), the latest SOTA method, TCP, achieves only a 0.22% performance improvement on new classes compared to other SOTA methods (e.g., MaPLe). Therefore, this motivates us to explore the underlying reasons for the decline in generalization ability in B2N tasks.\n\nWe visualized the attention maps of the original CLIP model and the CoOp [44] on base and novel class in Figure 1 (a). We can observe that zero-shot generalization ability is reduced primarily due to overfitting the prompt model to base class data during the few-shot fine-tuning stage. Specifically, after fine-tuning on the base classes, the CoOp prompt model (right one) tends to focus excessively on fine-grained foreground details (e.g., concentrating attention on specific areas of the cat) while neglecting coarse-grained structural elements such as contours, shapes, and poses (e.g., the original CLIP model has a broader attention span). These structural attributes are crucial for preventing overfitting and enhancing generalization to unseen new classes. Therefore, during the prompt learning process, we aim to focus on the structural information of the image rather than fine-grained foreground details to address the B2N problem.\n\nWe propose the Less Overfitting for Better Generalization (LOBG) framework, as a way for learning structural information from the data, which can effectively adapt the prompt model to unseen tasks.\n\nFirst, we propose the foreground information filtering (FIF) module, which removes fine-grained information from images to enable the model to focus on overall structural information rather than excessively on foreground details. Specifically, we employ the attention maps from the frozen CLIP module as masks and set appropriate thresholds to eliminate foreground details in the images. Our research shows that this moderate refinement of fine-grained information has little impact on base class training but significantly enhances the model's generalization performance. Secondly, mainstream methods (ii) aim to restore generalization by aligning models with the original CLIP features. However, they often use strict distillation, which results in the model absorbing an excess of fine-grained information from CLIP, thereby limiting the learning capacity of the prompts and hindering improvements in generalization ability. As shown in Figure 1 (b), PromptSRC only improved performance on new classes by 0.80% compared to CLIP. In contrast, we propose the structural topology preservation (STP) constraint, which ensures consistency with the original CLIP only in terms of overall structure and our method achieves a significant improvement of 2.82% over CLIP on new classes. While preserving CLIP's original generalization ability by maintaining its topological structure, this approach does not constrain the prompt's adaptability to downstream tasks. Our research indicates that focusing on extracting structural information from CLIP, rather than its details, can more effectively enhance the prompt model's generalization ability for unseen tasks. Furthermore, we employ hierarchical logit distilation (HLD) at the output layer to complement the STP constraint, ensuring the preservation of structural information. In summary, our method makes the following main contributions:\n\n\u2022 We explore the decline of generalization in B2N tasks caused by overfitting to fine-grained details and propose the LOBG framework to address this problem.\n\u2022 We use FIF to remove fine-grained details of images, shifting the focus to overall structure and enhancing generalization without affecting base class training.\n\u2022 We introduce STP and HLD constraints, which preserve CLIP's overall structure and generalization ability, leading to significant performance improvements on new classes compared to CLIP.\n\u2022 Experiments on 11 benchmark datasets demonstrate that our method effectively mitigates overfitting of base-to-novel generalization."}, {"title": "2. Related Works", "content": "Vision-Language Model Pre-training. In recent years, vision-language models have garnered widespread attention from researchers as a new tool for visual recognition. Vision-language models such as CLIP [27], BLIP [18], ALIGN [12], and FILIP [40] leverage image-text pairs from the web to train multimodal networks. During pre-training, these models use a self-supervised approach by pulling closer the matched image-text pairs and pushing apart the unmatched ones. This allows the models to understand the semantics of images and their corresponding textual descriptions. Due to their strong generalization capabilities, vision-language models have been widely applied in various downstream visual tasks [1, 24, 28, 41, 45],.\n\nPrompt Tuning. To quickly transfer VLM to downstream tasks, some methods have adopted the concept of prompt tuning from the NLP field [14, 43, 44]. This approach introduces a small number of additional parameters to enable the rapid transfer of VLMs to downstream tasks. CoOp [44] and CoCoOp [43] fine-tune the frozen CLIP model by adding prompts to the text branch. Maple [14] and IVLP [29] enhance transferability by adding prompts to both the visual and language branches of CLIP. PromptSRC [15], KgCoOp [38] and TCP [39] attempt to improve generalization performance by incorporating knowledge distillation, introducing the knowledge of the frozen CLIP into the prompts. However, these methods still struggle to prevent CLIP from overfitting to base classes during transfer to downstream tasks, failing to meet the generalization requirements for novel classes. It is worth noting that some methods [34, 36, 42] introduced external data or ex-"}, {"title": "3. Method", "content": "3.1. Preliminary\n\nContrastive Language-Image Pretraining (CLIP). CLIP [27] consists of two sub-networks, referred to as the image encoder and the text encoder. By understanding image-text pairs, CLIP constructs a shared embedding space $\\mathbb{R}^d$, where d denotes the feature embedding dimension. The image encoder and text encoder of CLIP are denoted as $f(\\theta_f)$ and $g(\\theta_g)$, where $\\theta_f$ and $\\theta_g$ represent the parameters of $f$ and $g$. Given an image $x$ and class labels $Y = \\{y_1, y_2,..., y_i,......,y_N\\}$ ($N$ is the number of classes), the image encoder generates the corresponding image feature $z = f(x,\\theta_f) \\in \\mathbb{R}^d$. The text encoder then generates the corresponding text features $V = \\{v_i = g(t_i, \\theta_g) \\in \\mathbb{R}^d\\}_{i=1}^{N}$ using handcrafted text templates $t_i$ = \u201ca photo of [CLASSNAME]\u201d. Finally, using cosine similarity, CLIP predicts the class for a given image $x$:\n\n$p(y = i|x) = \\frac{exp(sim(z, v_i)/T)}{\\sum_{j=1}^{N} exp(sim(z, v_j)/T)}$ (1)\n\nhere, $sim(z, v_j) = \\frac{z \\cdot v_j}{||z|| ||v_j||}$ denotes the cosine similarity, and $T$ is the temperature coefficient.\n\nSoft Prompt Tuning. Based on IVLP [29], we add learnable visual and text prompts to the image and text encoders, denoted as $P_v = \\{p_v^k\\}_{k=1}^{K}$ and $P_t = \\{p_t^l\\}_{l=1}^{L}$, respectively. Here, $K$ and $L$ represent the number of learnable tokens. Therefore, the inputs to the image and text encoders become: $x = [P_v, x]$ and $t_i = [P_t, t_i]$. The resulting image feature is $\\tilde{z} = f(\\tilde{x},\\theta_f) \\in \\mathbb{R}^d$, and the text features are $V = \\{v_i = g(\\tilde{t_i}, \\theta_g) \\in \\mathbb{R}^d\\}_{i=1}^{N}$. Similarly, the prediction"}, {"title": "3.2. Foreground Information Filtering", "content": "VLMs are transferred to downstream tasks through prompt tuning, which enhances the model's focus on fine-grained foreground information within the training data, facilitating effective transfer. However, in B2N tasks involving unseen downstream challenges, prompt tuning can cause the model to overly fit the training data, leading to the neglect of essential underlying visual information and ultimately degrading performance on unseen data.\n\nTo address the overfitting issue mentioned above, we propose a mask mechanism to reduce excessive attention to local information. We use the attention map of each training image to capture its foreground information and construct a mask to filter part of them. Specifically, we apply a threshold to the attention maps: attention blocks with values greater than the threshold are set to 0, while the rest are set to 1, to create a mask.\n\nThen, we use the obtained mask to filter the foreground fine-grained information of the image:\n\n$\\tilde{x} = mask \\bigodot x$ (4)\n\nwhere the $\\bigodot$ denotes element-wise multiplication. By using a mask, we can filter out the parts of the input image that are most likely to cause overfitting. This forces the model to focus more on the underlying visual information when transferring to downstream tasks. This approach helps improve the model's ability to generalize to unknown data in downstream tasks."}, {"title": "3.3. Structural Topology Preservation", "content": "Our model aims to guide the overall feature space towards downstream tasks while preserve the original information for maintaining the generalization ability. By retaining angular relationships between samples instead of point-to-point constraints, we preserve the topological structure of original CLIP. This maintains CLIP's generalization ability without limiting prompt adaptability to downstream tasks. Given any three samples $x_i, x_j, x_k$ in the feature topology space, they form a local angular relationship A, where A is used for the frozen CLIP, and $\\tilde{A}$ is used for the model after prompt tuning.\n\nDue to the different semantic information contained in various layers of ViT, we perform weighted combinations of features from different ViT layers to further supplement the lost low-level visual information during the model fitting process. The fused image feature is obtained as follows:\n\n$Z_w = \\sum_{i=1}^{H} w_i z_i$ (5)\n\nwhere $z_i$ is the feature from the i-th ViT layer of the image encoder, H is the number of ViT layers, and $w_i$ is the weight obtained through Gaussian sampling, with $\\sum w_i = 1$.\n\nTherefore, for the frozen CLIP model, the local angular relationship A of samples $x_i, x_j, x_k$ is expressed as:\n\n$A (x_i, x_j, x_k) = \\frac{e_{ij} \\cdot e_{kj}}{||e_{ij}|| ||e_{kj}||}$ (6)\n\nwhere $e_{ij} = Z_{w,i} \u2013 Z_{w,j}$ and $e_{kj} = Z_{w,k} \u2013 Z_{w,j}$ represent the distance vectors between two samples in the feature space.\n\nSimilarly, for the model after prompt tuning, the local angular relationship $\\tilde{A}$ of samples $x_i, x_j, x_k$ is expressed as:\n\n$\\tilde{A} (x_i, x_j, x_k) = \\frac{\\tilde{e_{ij}} \\cdot \\tilde{e_{kj}}}{||\tilde{e_{ij}}|| ||\\tilde{e_{kj}}||}$ (7)\n\nwhere $\\tilde{e_{ij}} = Z_{w,i} \u2013 Z_{w,j}$ and $\\tilde{e_{kj}} = Z_{w,k} \u2013 Z_{w,j}$. The local angular relationship function between samples reflects the angular structure of the feature topology space. We aim to maintain this structure during the CLIP's transfer to downstream tasks. Therefore, our structural topology preservation loss is as follows:\n\n$L_{vision} (T_i, x_j, x_k) = ||A - \\tilde{A}||$ (8)\n\nDue to the sparse and relatively fixed nature of textual information, to maintain consistency in the text space, we use the L1 distance to constrain the text space for a given text t:\n\n$L_{text} = ||g(\\tilde{t}, \\theta_g) - g(t,\\theta_g)||_1$ (9)\n\nwhere the $g(\\theta_g)$ is the text encoder, $\\tilde{t}$ is the text input with the prompt added, and t is the original text input.\n\nOur STP does not directly constrain the movement of individual samples in the feature space. Instead, it constrains the change in feature space structure by maintaining angular relationships, allowing reasonable displacement of samples in the feature space during the optimization process. This soft constraint is easier to optimize and imparts a degree of plasticity to the feature space, making its structure flexible. Overall, our STP loss is defined as follows:\n\n$L_{STP} = L_{vision} + L_{text}$ (10)"}, {"title": "3.4. Hierarchical Logit Distillation", "content": "To further complement STP at the model output and inherit CLIP's knowledge of the relationships between different samples, we constrain the model output at different layers of the logits. We believe logits for a single example contain specific information, and distilling knowledge from them may alter the encompassing topological structure. Hence, we use hierarchical logit distillation for model consistency supplementation.\n\nInstance-aware Distillation. Following the traditional knowledge distillation approach [11], we use KL divergence to achieve alignment with the frozen CLIP space at the instance awareness as follows:\n\n$L_{ikd} = D_{KL}(P(y|x) || p(y|x))$\n\n$= \\sum_i p(y = i|x) log (\\frac{p(y = i|x)}{P(y = i|x)})$ (11)\n\nhere, $p$ represents the logits from the frozen CLIP, while $\\tilde{p}$ represents the logits from the prompt-tuned model. The loss function $L_{ikd}$ aligns the model with the frozen CLIP at the instance awareness, providing fundamental spatial alignment.\n\nClass-aware Distillation. We consider that the output of the prompt-tuned model and the output of the frozen CLIP model should exhibit similar distribution descriptions across categories.\n\nWe denote M and $\\tilde{M}$ as the class relationship matrices for the frozen CLIP and the model after prompt tuning, respectively. Given the logits p of the frozen CLIP and the logits $\\tilde{p}$ of prompt-tuned model, they are computed as follows:\n\n$M = p^T \\cdot p, \\  \\ \\ \\  \\tilde{M} = \\tilde{p}^T \\cdot \\tilde{p}$ (12)\n\nhere, M and $\\tilde{M}$ are C \u00d7 C matrices, where C is the number of classes. $M_{ij}$ and $\\tilde{M_{ij}}$ represent the probabilities that a sample belongs simultaneously to class i and class j. Given the class relationship matrices, the following constraints apply:\n\n$L_{ckd} = ||M - \\tilde{M}||_2$ (13)\n\nThrough $L_{ckd}$, we further supplement the lost class information during the model transfer process, achieving model consistency at the class awareness.\n\nOur hierarchical logit distillation is:\n\n$L_{HLD} = L_{ikd} + L_{ckd}$ (14)\n\nOverall, our final loss function is as follows:\n\n$L = L_{cls} + \\lambda L_{HLD} + \\gamma L_{STP}$ (15)\n\nwhere, $\\gamma$ and $\\lambda$ are hyperparameters."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\n\nBase-to-Novel Generalization. In this setup, we evaluate the generalization of LOBG. Following CoOp [44], we divide the dataset evenly into two parts: base classes and novel classes. We conduct 16-shot learning on the base classes and perform zero-shot evaluation on the novel classes. This setup aims to evaluate the model's generalization capability when transferring to downstream tasks.\n\nCross-Dataset Transfer. Following CoOp [44], in cross-dataset transfer, we conduct few-shot training on ImageNet1K [5] and then perform zero-shot testing on multiple heterogeneous datasets.\n\nDomain Generalization. We evaluated the robustness of our method on out-of-distribution datasets. Similar to cross-dataset evaluation, we perform few-shot learning directly on ImageNet1K [5] and then test our ImageNet-trained model on four other ImageNet datasets that contain different types of domain shifts.\n\nDatesats. For base-to-movel generalization and cross dataset transfer setting, we use ImageNet [5], FGVCAircraft [21], EuroSAT [8], UCF101 [33], DTD [4], Caltech101 [7], Oxford-Pets [25], Stanford-Cars [16], Oxford-Flowers [23], Food101 [2], and SUN397 [37]. For domain generalization, we use ImageNet1k [5] as the source domain and ImageNet-A [10], ImageNet-R [9], ImageNet-V2 [30], and ImageNet-Sketch [35] as the target domains. For base-to-novel generalization, based on the performance of CoOp, we classify DTD [4], FGVCAircraft [21], EuroSAT [8], and UCF101 [33] as challenging datasets. These four datasets have fine-grained characteristics, making them more prone to overfitting compared to other datasets.\n\nImplementation Details. Our implementation is based on the ViT-B/16 variant of the CLIP model, utilizing prompts of length 4 in both the visual and text branches. We trained for 20 epochs across all three benchmarks. For Base-to-Novel Generalization, prompts were added in the first 7 transformer layers, while for the other two benchmarks, this number was 3. We employed the Adam optimizer with a learning rate set to 2.5e-3 and a batch size of 4. Performance metrics included base class accuracy, novel class accuracy, and harmonic mean accuracy, averaged over 3 experimental runs to determine final accuracy. We use Independent Vision-Language Prompting (IVLP) [29] as our baseline method.\n\n4.2. Base-to-Novel Generalization\n\nWe compared our method with zero-shot CLIP, CoOp, CoCoOp, Maple, PromptSRC, Dept and TCP."}, {"title": "5. Conclusion and Limitation", "content": "In this work, we introduce the LOBG framework to tackle overfitting in Base-to-Novel task, improving generalization in VLM transfer. By filtering fine-grained foreground information, we reduce overfitting and enhance attention to underlying visual details. We also develop structural topology preservation (STP) loss and hierarchical logit distilation (HLD) to preserve CLIP's structural information while adapting to downstream tasks. Our method shows significant improvements across 11 datasets and outperforms existing approaches. Future work will focus on refining foreground filtering to minimize its impact on base class learning and developing more effective methods for capturing structural information to further address overfitting in VLM prompt tuning."}]}