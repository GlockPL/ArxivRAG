{"title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches", "authors": ["Sifan Wu", "Amir Hosein Khasahmadi", "Mor Katz", "Pradeep Kumar Jayaraman", "Yewen Pu", "Karl Willis", "Bang Liu"], "abstract": "Parametric Computer-Aided Design (CAD) is central to contemporary mechanical design. However, it encounters challenges in achieving precise parametric sketch modeling and lacks practical evaluation metrics suitable for mechanical design. We harness the capabilities of pre-trained foundation models, renowned for their successes in natural language processing and computer vision, to develop generative models specifically for CAD. These models are adept at understanding complex geometries and design reasoning, a crucial advancement in CAD technology. In this paper, we propose CadVLM, an end-to-end vision language model for CAD generation. Our approach involves adapting pre-trained foundation models to manipulate engineering sketches effectively, integrating both sketch primitive sequences and sketch images. Extensive experiments demonstrate superior performance on multiple CAD sketch generation tasks such as CAD autocompletion, CAD autoconstraint, and image conditional generation. To our knowledge, this is the first instance of a multimodal Large Language Model (LLM) being successfully applied to parametric CAD generation, representing a pioneering step in the field of computer-aided mechanical design.", "sections": [{"title": "1 Introduction", "content": "Parametric Computer-Aided Design (CAD) is crucial in mechanical engineering, playing a key role in developing modern objects such as lamps, cars, and buildings. Its importance lies in its ability to accurately depict complex designs, essential for the integrity and functionality of engineered products interacting with humans and the environment. CAD sketches, consisting of basic 2D shapes, are foundational for parametric CAD modeling [3]. As illustrated in Fig 1, these sketches comprise primitive geometric elements (e.g., lines, arcs, points) defined by various relationships (e.g., coincident, parallel, tangent). However, the required precision for 3D construction in parametric CAD can be both challenging and time-consuming [15]. Consequently, there is a significant push to improve CAD generation to simplify and accelerate the manual design process. Leveraging deep learning to identify patterns in CAD sketches could automate the completion of geometry and constraints in engineering drawings [22, 23, 28].\nIntegrating deep learning into CAD offers a significant opportunity to transform the field. Recent research [10, 14, 19, 22, 23, 29, 31] has concentrated on the generative modeling of engineering sketches, particularly focusing on Transformer-based architectures [26]. These approaches often utilize graph structures [22] or attention mechanisms [23] to process sketch information. However, the effectiveness of these methods is limited by the availability of data and a lack of common-sense understanding, which restricts their capacity to perform complex geometric reasoning in engineering sketches. Additionally, rendered sketch images (such as in Fig. 1(a) and 4) have great potential to represent geometry patterns from image modality, which could be complementary for parametric sketches language. While none of current CAD generative models take sketch image into consideration. Our research presents a novel Multimodal Transformer-based Generative model for CAD generation tasks, pioneering in its use of both language and image for sketch modeling. By merging these modalities, we aim to increase the accuracy of CAD generation, setting a new benchmark by simultaneously modeling text-style sketch parametric and image-style sketch renderings.\nLarge Language Models (LLMs) have gained widespread success for their powerful commonsense reasoning and problem-solving abilities in a variety of domains, such as natural language processing, mathematics, and medical sciences [1,5,7,18,24,25,33]. The capabilities of LLMs align well with engineering sketches, which can be understood as sequences representing points and connections. Commonsense knowledge in pre-trained LLM can also offer valuable hints for sketch patterns. Furthermore, combining sketch images with textual descriptions can greatly improve the performance of language models in CAD sketch generative models. In the realm of parametric CAD design, which requires a thorough understanding of complex interactions(e.g. parallel, coincident, symmetry), the advantages of LLMs become even more pronounced.\nIn this work, we leverage the capabilities of pre-trained LLMs to develop a generative model tailored for parametric CAD, addressing the limitations of the existing methods. We introduce CadVLM, a CAD Vision Language Model, featuring a multimodal encoder-decoder framework. CadVLM stands out by pro-"}, {"title": "2 Related Work", "content": "Vision Language Pre-Training. The great success of large language models (LLMs) such as BERT [11] and GPT [2] has drawn significant attention within the field of natural language processing and code generation domain. CodeT5+ [27] is a family of open-source encoder-decoder LLMs for code tasks, ranging from 220 million to 16 Billion parameters. Beyond single modality LLMs, multimodal foundation models have also exhibited enhanced performance in various vision-and-language tasks [6,12,20]. Depending on downstream tasks, Various pre-training objectives have also been proposed like image-text contrastive learning [20], image-text matching [12], and (masked) language modeling [12]. Most recently, developing domain-specific multimodality LLMs has gained momentum such as MedBLIP [4] and TableGPT [33].\nCAD Sketch Generation 2D engineering sketches are fundamental in CAD for designing and manufacturing mechanical parts. The recent availability of extensive engineering sketch datasets has facilitated the development of generative models that enhance traditional CAD workflows. The SketchGraphs [22] dataset, featuring 15 million parametric CAD sketches with detailed geometric constraint graphs, showcases primitive interrelations. The paper also introduces a baseline model using autoregressive message-passing networks for constraint graph generation, but it lacks primitive coordinate output, relying solely on constraint graphs and the Onshape solver for sketch configuration, thus limiting its utility in CAD autocompletion tasks. Ganin et al. [8] proposed another large-scale CAD dataset containing over 4.7M parametric sketches from the Onshape public repository. However, limited by sketch format, this dataset has not been widely researched in the realm of CAD generative models.\nAnother line of work for generative modeling of CAD sketches is based on Transformer [23, 28-30]. Vitruvion [23] models both primitives and constraints,"}, {"title": "3 Background", "content": "Engineering Sketch Definition. In the field of parametric CAD, engineering sketches are fundamental, serving as detailed blueprints for designing a range of objects from small components to large structures. These sketches are essential for converting conceptual ideas into precise, manufacturable designs. To ensure a standardized representation, sketches of varying dimensions, spanning from millimeters to meters, are uniformly positioned within a bounding box of 1-meter width at the origin. Each sketch consists of two key sequences: a sequence of primitives, S, and a sequence of constraints, C.\nWe adopt the normalization and quantization methods for primitive parameters as outlined by [23]. A sketch primitive is expressed as $S = (e_1, e_2, ..., e_m)$, where m represents the number of entities in the sketch. Each entity $e_i$ is characterized by positional parameters $(P_1,..., P_k)$, where $p$ represents the normalized coordinates of points that constitute the entity. Sketches typically include three types of entities lines, arcs, and circles. As illustrated in Figure 1, all sketches are centrally positioned and normalized within a range of [1,64]. Lines, shown in blue, are depicted using two points: start and end point coordinates. Curves, shown in green, are characterized by three points: the start, the end, and a mid-point on the curve. Circles, illustrated in red, are portrayed with four uniformly distributed points on the circle's circumference. This format allows for the interpretation of each sketch as a sequence of tokens. Constraints are similarly represented as primitives in $C = (c_1, ..., c_n)$, with each constraint entity c being defined by its type and the reference indices of parameters to which the primitives must conform.\nCAD AutoCompletion Task. In addressing the existing CAD AutoCompletion task, which aims to automate routine CAD design procedures, we propose a new autoregressive generative model. This task involves completing a partially given sketch to form a fully realized design. The mathematical formulation of"}, {"title": "4 CadVLM: A Vision Language Model for CAD Generation", "content": "In this section, we delve into the specifics of our CAD Vision Language Model (CadVLM), employing the CAD autocompletion task as an illustrative example. In our approach, the sequence of sketch primitives S is interpreted akin to language text, while the corresponding visual representation of the sketch is treated as image I. It should be noted that the text-grounded image decoder is not utilized in the CAD auto-constraint task.\n4.1 Network Framework\nCadVLM is structured as an asymmetrical encoder-decoder architecture, featuring a two-stream encoder, a text decoder based on a language model, and a lightweight image decoder for constructing the autocompleted image. This design is depicted in Figure 2. The two-stream encoder is tailored to process dual-modal inputs: a vision sub-encoder for the sketch image I and a text sub-encoder for the prefix sequence of sketch primitives $S_p$. For text decoding, we employ a causal language transformer model, which autoregressively generates"}, {"title": "5 Experiments", "content": "We conduct extensive experiments and ablation studies on CadVLM for the CAD autocompletion and autoconstraint tasks. Quantitative assessment is conducted by measuring Entity Accuracy, Sketch Accuracy, and CAD F1.\n5.1 Experimental settings\nDataset We use the SketchGraphs [22] dataset for all our experiments, consisting of 15 million CAD sketches collected from Onshape\u00b3. Referring to [23], we use the filtered version and further deduplicate the dataset. After deduplication, the training, validation, and test set have 626,236, 22,031, 21,979 unique sketches correspondingly. The filtered sketches are restricted to sketches comprised of arcs, circles, and lines. To ensure a fair comparison, we normalize each sketch as in [23]; specifically the quantized range of numerical tokens in the primitive sequence is [1,64].\nBaselines We evaluate several ablation and conditional variants of CadVLM, as well as three SOTA CAD generative model baselines: Vitruvion [23], Deepcad [29] and SketchGraphs [22]. Since SketchGraphs does not incorporate continuous primitive parameters such as coordinate value, we only compare with the CAD autoconstraint task. And DeepCAD only considers parametrized command sequences, which are incompatible with CAD constraints. We primarily evaluate its performance on CAD autocompletion tasks.\nSettings For the image encoder and decoder, we choose the encoder and decoder of state-of-the-art pre-trained ViT-MAE-base from [9]. Following [9] we set the patch size and stride both as 32. We render all sketch images with size of 224 x 224 x 3. For the text encoder and text decoder, we use the encoder and decoder of pre-trained CodeT5+ [27] correspondingly, which is an instruction-tuned code LLM with 770M parameters trained on various code datasets. We train the autocompletion model and autoconstraint model both for 30 epochs, using the Adam optimizer with decoupled weight decay regularization [17], with the learning rate set according to the CosineAnnealingLR [16] scheduler. The initial learning rate is set to 3e \u2013 4, with batch size 32. All our experiments are run using 8 NVIDIA A100 GPUs, and our models take between 30-40 hours to train. In the CAD Autocompletion experiments, we randomly sample 20%-80%"}, {"title": "5.2 Quantitative comparison with baseline sketch generative models", "content": "Table 1 reports results on the CAD Autocompletion task. We find that Vitruvion achieves 3% on Sketch Accuracy and 40.7% on Entity Accuracy, meaning it can accurately predict 3% of sketches and correctly identifies at least one entity in 40.7% of sketches. Deepcad performs worst since it only have 4 layers of encoder and decoder, makes it hard to represent complex CAD sketches. Our CadVLM outperforms Virtruvion on Sketch Accuracy, Entity Accuracy, and CAD-F1 by 20.8%, 28.3%, and 39.9% respectively. These results demonstrate the effectiveness of pre-trained foundational models and image modality. A closer examination of the qualitative results in Figure 4 reveals that Vitruvion produces many invalid results missing symmetric curves or lines. Sketches from CadVLM are better in terms of quality with more complex shapes and stronger symmetry.\nChatGPT Series Model Finetune To evaluate the ability of a general LLM on the CAD autocompletion task, we fine-tuned ChatGPT on three subsets"}, {"title": "CadVLM variants", "content": "To better understand the source of performance gains, we conducted a comparative analysis of different variants of our proposed CadVLM: CadVLM-text, CadVLM-w/o-ITC, CadVLM-w/o-IDL&ITC, and CadVLM-w/o-IDL. CadVLM-text relies solely on the sketch primitive sequence text as its input, optimizing for language modeling loss. In contrast, CadVLM-w/o-IDL utilizes ITC and LM loss for its training objectives, CadVLM-w/o-ITC focuses on IDL and LM, and CadVLM-w/o-IDL&ITC prioritizes only the LM objective. The findings reveal that CadVLM-Text underperforms compared to all other CadVLM variants across all three metrics, which underlines the value of the sketch image modality in providing diverse geometric information alongside sequence text.\nInterestingly, CadVLM-w/o-IDL emerges as the superior performer among the CadVLM variants, highlighting the significance of aligning image and text embeddings in multimodal modeling. Incorporating Image Decoding Loss (IDL) further enhances the alignment between image and text embeddings, leading CadVLM to show a notable improvement in all three metrics.\nFor the CAD Autoconstraint task, as shown in Table 2, SketchGraphs struggles to predict the whole sketch correctly, with only 0.48% Sketch Accuracy. This suggests that relying solely on graph structures of sketches is insufficient for handling long sequences of sketch primitives. In contrast, Vitruvion achieves 10.7% on Sketch Accuracy, showing the necessity to model primitives. For the CAD Autoconstraint task it is less challenging to predict at least one constraint correctly but more challenging to accurately predict all constraints, easily caused"}, {"title": "5.3 Ablation Study", "content": "We conducted several ablation studies to assess the performance of CadVLM on image-conditioned primitive generation tasks and investigate the effectiveness of each component within CadVLM.\nImage Conditional Generation. Following [23], we conduct a series of experiments to evaluate the effectiveness of CadVLM modeling primitive sequences conditioned on the images of sketches. As designers may begin their"}, {"title": "Entity Level Modelling.", "content": "As described earlier, each CAD sketch is comprised of multiple entities. We considered adding this inductive bias to the model by processing each entity separately and then combining their embeddings with the initial sequence of all sketch tokens. For this purpose, we first prepend a special token (ENTITY\u3009 to each of the entity input sequences. Subsequently, the entities from all the sketches in the batch are fed into the model's encoder in parallel. Next, the embeddings of each entity are gathered for each sketch. For each sketch, we concatenate these embeddings with the original sequence tokens using another special token (TOKEN) as a delimiter for the second part of the sequence. The decoder remains unchanged. As shown in Table 4, incorporating this inductive bias increases computation and memory demands while only marginally enhancing the performance.\nImpact of different input entity ratios for CAD autocompletion. To explore the impact of different input entity ratios for CAD autocompletion, we further test CAD autocompletion results with different input entity ratios: 20%, 40%, 60%, and 80%. Compared with the experiments in Sec 5.4, we specify the sampled ratio of input entities. As shown in Figure 5, CadVLM outperforms Vitruvion on all input entity ratios. In particular, CadVLM achieves 73.8% Entity Accuracy, 49.5% Sketch Accuracy, and 62.7% CAD F1 when given 80% of entities as input. For the hardest setting when given only 20% of entities as input, CadVLM can still achieve 43.9% Entity Accuracy and 16.9% CAD-F1, which proves that CadVLM has great potential to be used in real world settings.\nEffect of using different language models for text encoder and decoder. To better compare the performance of different text encoder-decoder, we experiment using different pre-trained foundation models as text encoder and decoder. To eliminate the influence of the image encoder and decoder, we only consider the CadVLM-Text variant. For the choice of text encoder and decoder, we also compare the performance of different Encoder-Decoder pre-trained"}, {"title": "6 Conclusion", "content": "In this study, we introduce CadVLM, an innovative end-to-end vision-language model tailored for modeling parametric CAD sketches, which are pivotal in modern mechanical design. CadVLM harnesses the commonsense knowledge and reasoning abilities inherent in large language models, enabling it to adeptly handle the complex geometric reasoning required for CAD sketches. Our experimental results demonstrate that CadVLM outperforms existing state-of-the-art models in both autocompletion and autoconstraint tasks, highlighting the potent enhancement that pre-trained models bring to CAD construction."}, {"title": "A Sketch Parameters", "content": "As mentioned in the main text, all primitives are represented as numerical tokens, and all constraints are constructed by at least one primitive, indicated by the corresponding edge's primitive tokens, followed by the constraint type token."}, {"title": "B Additional Evaluation", "content": "As shown in Figure 6 we provide more CAD generative model output samples.\nB.1 Multiple sampling outputs\nAs shown in Figure 7, CadVLM can also generate multiple outputs using nucleus sampling with the cumulative probability parameter of p = 0.9."}, {"title": "C Experimental Details", "content": "This section provides further details of the model architecture and training regime.\nModel Architecture. Our CadVLM all share a main transformer-based pre-trained language model as text encoder and decoder, which is responsible for processing the sequence of primitives or constraints. And a Vision transformer as image encoder and decoder for processing the sketch image. The text and image transformers architecture are identical across all the models. The text encoder and decoder are both composed of 24 layers transformer with 16 attention heads of each layer, and a total embedding dimension of 1024. No significant hyperparameter optimization was performed. The image encoder is composed of 12 layers transformer with 16 attention heads and 768 hidden size."}]}