{"title": "Identifying General Mechanism Shifts in Linear Causal Representations", "authors": ["Tianyu Chen", "Kevin Bello", "Francesco Locatello", "Bryon Aragam", "Pradeep Ravikumar"], "abstract": "We consider the linear causal representation learning setting where we observe a linear mixing of d unknown latent factors, which follow a linear structural causal model. Recent work has shown that it is possible to recover the latent factors as well as the underlying structural causal model over them, up to permutation and scaling, provided that we have at least d environments, each of which corresponds to perfect interventions on a single latent node (factor). After this powerful result, a key open problem faced by the community has been to relax these conditions: allow for coarser than perfect single-node interventions, and allow for fewer than d of them, since the number of latent factors d could be very large. In this work, we consider precisely such a setting, where we allow a smaller than d number of environments, and also allow for very coarse interventions that can very coarsely change the entire causal graph over the latent factors. On the flip side, we relax what we wish to extract to simply the list of nodes that have shifted between one or more environments. We provide a surprising identifiability result that it is indeed possible, under some very mild standard assumptions, to identify the set of shifted nodes. Our identifiability proof moreover is a constructive one: we explicitly provide necessary and sufficient conditions for a node to be a shifted node, and show that we can check these conditions given observed data. Our algorithm lends itself very naturally to the sample setting where instead of just interventional distributions, we are provided datasets of samples from each of these distributions. We corroborate our results on both synthetic experiments as well as an interesting psychometric dataset. The code can be found at https://github.com/TianyuCodings/iLCS.", "sections": [{"title": "1 Introduction", "content": "The objective of learning disentangled representations is to separate the different factors that contribute to the variation in the observed data, resulting in a representation that is easier to understand and manipulate [3]. Traditional methods for disentanglement [e.g., 19, 20, 7, 9, 26] aim to make the latent variables independent of each other.\nConsider the setting of linear independent component analysis (ICA) [19], that is, the observed variables $X \\in \\mathbb{R}^P$ are generated through the process $X = GZ$, where $Z \\in \\mathbb{R}^d$ are latent factors, and $G\\in \\mathbb{R}^{P\\times d}$ is an unknown \u201cmixing\u201d matrix. Under the key assumption that Z has statistically independent components, and under some additional mild assumptions, landmark results in linear ICA show that it is possible to recover the latent variables Z up to permutation and scaling [13, 19]."}, {"title": "2 Related Work", "content": "Causal representation learning. In contrast to our setting, which focuses on identifying shifted nodes in the latent representation, existing methods in CRL aim to recover both the latent causal graph and the mixing function. Previous works have studied identifiability in various settings, such as latent linear SEMs with linear mixing [40], and with nonlinear mixing [4]; latent nonlinear SEMs with finite degree polynomial mixing [1], and with linear mixing [48]; and nonlinear SEMs with nonlinear mixing [50, 49, 23, 22]. Although these studies ensure the identifiability of causal graphs (up to permutation and scaling ambiguities), they generally rely on the assumption that each latent variable is intervened upon in at least one environment, necessitating access to at least d interventional distributions. Moreover, the aforementioned works assume specific types of interventions, such as hard/soft interventions and single-node interventions, and restrict changes in interventional distributions, disallowing edge reversals or the addition of new edges. The most"}, {"title": "3 Problem Setting", "content": "Consider a random vector $X$ in $\\mathbb{R}^P$ that is a linear mixing of d latent variables $Z = (Z_1,..., Z_d)$:\n$X = GZ$.\nHere the latent variables in $Z$ follows a linear SCM [37, 38], that is,\n$Z = AZ + \\Omega^{1/2}\\epsilon$\nwhere $A \\in \\mathbb{R}^{d\\times d}$ corresponds to a DAG $G$ such that $A_{jk} \\neq 0$ iff there exists an edge $j \\rightarrow k$ in the DAG $G$; $\\Omega \\in \\mathbb{R}^{d\\times d}$ is a diagonal matrix with positive entries, and $\\epsilon \\in \\mathbb{R}^d$ is a random vector with independent components with mean zero and variance one, i.e., that $Cov(\\epsilon) = I_d$. Denoting $B = \\Omega^{-1/2}(I_d \u2013 A)$, we have that:\n$Z = B^{-1}\\epsilon$."}, {"title": "4 Identifying Shifts in Latent Causal Mechanisms", "content": "Following the setup outlined in the previous section, our focus now turns to developing an algorithm to identify latent shifted nodes, given data from multiple environments. First, note that we can write the overall model as a linear ICA problem, where, for any environment k, the observation $X^{(k)}$ is a linear combination of independent components $\\epsilon^{(k)}$. Specifically, we have\n$X^{(k)} = GZ^{(k)} = G(B^{(k)})^{-1}\\epsilon^{(k)}$\nUnder the mild conditions given in Assumption A, from classical ICA identifiability results stated in Theorem 1, we can identify $G(B^{(k)})^{-1}$ up to permutation and sign flip. Let $M^{(k)} = B^{(k)}H$ where $H = G^{+}$. Then, we can only identify $M^{(k)}$ up to permutation and sign flip, which does not suffice to identify the latent SCM encoded in $B^{(k)}$. In sum, what we can only obtain from ICA is\n$\\hat{M}^{(k)} = P^{(k)}D^{(k)}B^{(k)}H$\nwhere $P^{(k)}$ is a permutation matrix, and $D^{(k)}$ is a diagonal matrix with \u20131 or +1 on its diagonal. As Seigal et al. [40] points out, it is not possible to identify $B^{(k)}$ further given generalized interventions. Our first result is that our present mild assumptions suffice to infer shifted nodes.\nTheorem 2 (Identifiability). Given access to $K \\geq 2$ environments, assume that A and B hold for all environments. Then, all latent shifted nodes are identifiable.\nAn interesting facet of our identifiability result is that it is constructive. In the next subsection we will provide an explicit algorithm to infer the shifted nodes and prove the main theorem above."}, {"title": "4.1 Constructive identifiability", "content": "Consider $\\hat{\\epsilon}^{(k)} = \\hat{B}^{(k)}HX^{(k)}$ and $\\hat{M}^{(k)} = \\hat{B}^{(k)} HX^{(k)} = P^{(k)}D^{(k)}B^{(k)}HX^{(k)} = P^{(k)}D^{(k)}\\epsilon^{(k)}$, where $\\hat{\\epsilon}^{(k)}$ and $\\hat{M}^{(k)}$ are the output of ICA, which contain the permutation and sign flip ambiguities given by $P^{(k)}D^{(k)}$.\nObtaining a consistent ordering of the noise components across all environments is equivalent to finding $P^{(k)}$. Under Assumption B, and without loss of generality, we consider that $(\\epsilon^{(k)}_1, ..., \\epsilon^{(k)}_d)$ are in increasing order with respect to their $\\psi$ values. Since $\\psi$ is invariant to sign flip, we can calculate $\\psi(\\epsilon^{(k)}_i)$ for all $i \\in [d]$ and sort the calculated $\\psi$ values in increasing order. Let $P^{(k)}$ denote the sorting permutation with respect to $\\psi$, so that post-sorting, we get $P^{(k)}\\hat{\\epsilon}^{(k)}$.\nProposition 1. $P^{(k)} = (P^{(k)})^{-1}$, i.e., $P^{(k)}$ is the inverse permutation of the ICA scrambling.\nFrom Proposition 1, we thus find that we can unscramble the permutation $P^{(k)}$ by sorting with respect to $\\psi$. We get $P^{(k)}\\epsilon^{(k)} = P^{(k)}P^{(k)}D^{(k)}\\epsilon^{(k)} = D^{(k)}\\epsilon^{(k)}$ from the above proposition. In other words, we can extract $\\epsilon^{(k)} = D^{(k)}\\epsilon^{(k)}$ via $\\hat{M}^{(k)} = P^{(k)}\\hat{M}^{(k)} = D^{(k)}B^{(k)}H = D^{(k)}M^{(k)}$ after ICA and sorting by $\\psi$."}, {"title": "4.2 Finite-sample algorithm", "content": "Thus far, we have considered the population setting where we are given the entire interventional distributions. In practice, we are given samples from each of these interventional distributions, so that we have K datasets, one for each of the interventional distributions. The overall algorithm is given next in Alg. 1 with detailed explanations following the algorithm."}, {"title": "5 Experiments", "content": "In this section, we investigate the performance of our method in synthetic and real-world data."}, {"title": "5.1 Synthetic Data", "content": "In our setup, each noise component $\\epsilon_i$ is sampled from a generalized normal distribution with the probability density function given by $p(\\epsilon_i) \\propto exp{-|\\epsilon_i|^{1/2} }$, where $i = 1,2,...,d$. In this noise generation process, the noise vector $\\epsilon$ adheres to the condition $\\psi(\\epsilon_i) < \\psi(\\epsilon_j)$ for all $i < j$ if we choose $\\psi(y) = P(|y| \\leq 1)$. Following the methodology similar to that in [40], we start by sampling either an Erd\u0151s-R\u00e9nyi (ER) or Scale-Free (SF) graph with d nodes and an expected edge count of md, where $m \\in \\{2,4,6\\}$, denoted as ERm or SFm. The observed space dimension p is set to 2d. For each graph, the weights are independently sampled from Unif $\u00b1 [0.25, 1]$ and the diagonal entries of $\\Omega$ from Unif[2, 4]. In each environment k, 15% of the nodes are randomly selected for shifting. The new weights $A^{(k)}_{ij}$ for the shifted node i, and the new entries of $\\Omega^{(k)}$, specifically $\\Omega^{(k)}_{ii}$, are independently sampled from Unif[6, 8]. The mixing function G is independently generated from Unif[-0.25, 0.25].\nEmpirically, we have observed that the following formulation of $L_i^{k,k'}$ leads to improved results:\n$L_i^{k,k'} = \\frac{min{\\mid\\mid M_i^{(k)} - M_i^{(k')}\\mid\\mid_1, \\mid\\mid M_i^{(k)} + M_i^{(k')}\\mid\\mid_1}}{\\mid\\mid M_i^{(k)} \\mid\\mid_1 + \\mid\\mid M_i^{(k')}\\mid\\mid_1}$"}, {"title": "6 Concluding Remarks", "content": "In this study, we demonstrated that latent mechanism shifts are identifiable, up to a permutation, within the framework of linear latent causal structures and linear mixing functions. Furthermore, we introduced an algorithm, grounded in ICA, designed to detect these shifts. Our method offers a broader applicability to various types of interventions compared to CRL framework. Unlike shift detection methods where node variables are directly observable, our approach extends to scenarios where latent variables remain unobserved. A promising future direction consists of adapting our methodology to nonlinear transformations, which could address more complex, practical challenges, such as identifying latent mechanism shifts in real-world image data."}, {"title": "A Limitations and Broader Impacts", "content": "Limitations of this work include the need to relax the noise assumption and to consider similar settings under nonlinear mixing functions. These are promising directions to explore in the CRL field. The broader impact of this work is that CRL methods can be used to identify mechanism shifts and determine root causes, which can be utilized in the biological field to find disease genes or biomarkers. Currently, the negative impacts of this method are not clear."}, {"title": "B Illustration of our algorithm", "content": ""}, {"title": "C Discussion on Test Function", "content": "In Assumption B, we assume that there exists a test function $\\psi$ and that we can access it. Here we discuss ways to relax it. Recall that in Section 4.1, $\\psi$ is utilized to sort the noise component $\\epsilon^{(k)}$ to ensure that the post-sorting noise vector $\\hat{\\epsilon}^{(k)}$ has a consistent order across all environments."}, {"title": "D Discussion on Sample Complexity", "content": "The sample complexity of our method must be considered from two perspectives: one involves using ICA to estimate $\\epsilon^{(k)}$ and $M^{(k)}$, and the other pertains to utilizing $\\epsilon^{(k)}$ and a test function to sort the rows of $M^{(k)}$. Since the sorting step depends on the choice of test function, we assume for simplicity that $M^{(k)}$ is already sorted by the ground truth order. Thus, we only focus on the asymptotic behavior of $\\hat{M}^{(k)}$, which closely relates to the properties of the ICA estimator.\nThere are various algorithms for solving ICA [18, 17, 41]; each algorithm exhibits different asymptotic statistical properties. If we apply the findings in Auddy and Yuan [2], we assume that the estimated ICA unmixing function has the following statistical accuracy:\nTheorem 4. If the sample size $n \\geq g(\\delta, d)$, then with probability at least $1 \u2013 h(n, \\delta, d, \\epsilon)$, we have:\n$l(\\hat{M}^{(k)}_i - M^{(k)}_i) \\leq C\u00b7p(\\delta, d)f(d)$,\nwhere $\\hat{M}^{(k)}_i$ represents the i-th row of the estimated unmixing function $M^{(k)}$, C is a constant, and p, f, g, and h are known functions. For instance, in Auddy and Yuan [2], $p(\\delta,n) = \\sqrt{d/n}$ and $f(\\delta) = log(1/\\delta)$. Here, l denotes the loss function, and the $L_2$ norm can serve as an option.\nUnder this theorem, for two environments k and k', if node i does not shift, we have:\n$||\\hat{M}^{(k)}_i - \\hat{M}^{(k')}_i||_2 \\leq ||\\hat{M}^{(k)}_i - M^{(k)}_i||_2 + ||M^{(k')}_i - \\hat{M}^{(k')}_i||_2 \\leq 2\u00b7C\u00b7p(\\delta, n)f(d)$\nwith a probability of at least $1 \u2013 2h(n, \\delta, \\epsilon)$. Thus, by setting the threshold a as $2\u00b7C\u00b7p(\\delta, n)f(d)$, we can control the false discovery rate to be at most $2h(n, \\delta, \\epsilon)$. A similar sample complexity theorem can be extended to cases involving more than two environments, as long as the statistical properties of the ICA solution are known."}, {"title": "E Detailed Proofs", "content": ""}, {"title": "E.1 Proof of Proposition 2", "content": "Lemma 1. Under problem setting, for any $x, y \\in \\mathbb{R}^{d\\times 1}$, the equation $x^TH = y^T H$ holds if and only if x = y.\nProof. Given that G possesses full column rank, it follows that H = G\u2020 has full row rank. Consequently, the null space of HT is {0}. Therefore, if $x^T H = y^T H$, it implies $H^T (x \u2013 y) = 0$. This leads to the conclusion that x \u2013 y = 0, which in turn implies x = y.\nProof of Proposition 2. Recall that $B^{(k)} = (\\Omega^{(k)})^{-1/2} (I_d \u2013 A^{(k)})$. Since $A^{(k)}$ is a weighted adjacency matrix, its diagonal entries are zero. Thus,\n$B^{(k)}_{ij} = (\\Omega^{(k)}_{ii})^{-1/2} (A^{(k)}_{ij})$ if i $\\neq$ j,\n$B^{(k)}_{ii} = (\\Omega^{(k)}_{ii})^{-1/2}$ if i = j.\nUnder Definition 1, if node Zi is shifted, it implies either 1) $\\Omega^{(k)}_{ii} \\neq \\Omega^{(k')}_{ii}$, 2) $A^{(k)}_{ij} \\neq A^{(k')}_{ij}$, or 3) both conditions hold. In scenarios 1) and 3), $B^{(k)}_{ij} \\neq B^{(k')}_{ij}$, resulting in $B^{(k)} \\neq B^{(k')}$. In scenario 2), while $\\Omega^{(k)}_{ii} = \\Omega^{(k')}_{ii}$, there exists a $j \\in [d]$ such that $A^{(k)}_{ij} \\neq A^{(k')}_{ij}$, leading to $B^{(k)}_{ij} \\neq B^{(k')}_{ij}$. If node Zi is not shifted, then $A^{(k)}_{ij} = A^{(k')}_{ij}$ and $\\Omega^{(k)}_{ii} = \\Omega^{(k')}_{ii}$, implying $B^{(k)} = B^{(k')}$. Therefore, Zi is shifted if and only if $B^{(k)} \\neq B^{(k')}$. According to Lemma 1, $B^{(k)} \\neq B^{(k')}$ if and only if $B^{(k)} H \\neq B^{(k')} H$, which is equivalent to $M^{(k)}_i \\neq M^{(k')}_i$.\nIn conclusion, Zi is shifted if and only if $M^{(k)}_i \\neq M^{(k')}_i$."}, {"title": "E.2 Proof of Theorem 3", "content": "Lemma 2. Under problem setting, it is not possible for an intervention on the latent node Zi to result in $M^{(k)}_i = \u2013M^{(k')}_i$.\nProof. We prove this by contradiction. Suppose that $M^{(k)}_i = \u2013M^{(k')}_i$. According to Lemma 1, this would imply $B^{(k)}_i = \u2013B^{(k')}_i$. However, we know $B^{(k)} = (\\Omega^{(k)})^{-1/2}(I_d \u2013 A^{(k)})$ where $A^{(k)}$ is the weight matrix for a DAG. Since $A^{(k)}_{ii} = 0$, it follows that $B^{(k)}_{ii} = (\\Omega^{(k)}_{ii})^{-1/2}$. Therefore, both $B^{(k)}_{ii}$ and $B^{(k')}_{ii}$ are positive. It is impossible for $B^{(k)}_{ii}$ to be equal to \u2013$B^{(k')}_{ii}$. Consequently, the scenario where $M^{(k)}_i = \u2013M^{(k')}_i$ cannot occur.\nProof of Theorem 3. Recall from the data generation process that\n$\\hat{M}^{(k)}X^{(k)} \\approx \\epsilon^{(k)}$\nWhen input $X^{(k)}$ to ICA, we have $\\hat{M}^{(k)} = P^{(k)} D^{(k)} M^{(k)}$ and $\\hat{\\epsilon}^{(k)} = P^{(k)} D^{(k)} \\epsilon^{(k)}$. Without loss of generality, we assume that $\\epsilon^{(k)}$ is ordered increasingly with respect to $\\psi$. Thus, post sorting with respect to $\\psi$, we eliminate the ambiguity of $P^{(k)}$, and we get $\\hat{M}^{(k)} = D^{(k)} M^{(k)}$ and $\\hat{\\epsilon}^{(k)} = D^{(k)} \\epsilon^{(k)}$.\nWe are now ready to prove that Zi is not shifted if and only if $\\hat{M}^{(k)}_i = \\pm \\hat{M}^{(k')}_i$. This immediately implies that if Zi is not shifted, then $M^{(k)}_i = M^{(k')}_i$, thus satisfying $\\hat{M}^{(k)}_i = \\pm \\hat{M}^{(k')}_i$.\nIf $\\hat{M}^{(k)}_i = \\pm \\hat{M}^{(k')}_i$, there are two cases: $\\hat{M}^{(k)}_i = \\hat{M}^{(k')}_i$ or $\\hat{M}^{(k)}_i = \u2013\\hat{M}^{(k')}_i$. We prove in Lemma 2 that the scenario $\\hat{M}^{(k)}_i = \u2013\\hat{M}^{(k')}_i$ cannot exist. The only surviving situation is $\\hat{M}^{(k)}_i = \\hat{M}^{(k')}_i$, which indicates that Zi is not shifted."}, {"title": "F Experiments on Synthetic Data Compared with DCI", "content": "As described in Section 5.1, instead of generating the mixing function G from Unif[-0.25, 0.25], we set G = I, such that X = Z and Z can be directly observed. In this setup, finding general interventions in linear causal representations reduces to identifying general interventions in linear SEM, a setting for which the existing method DCI [52] is designed."}, {"title": "G Additional Information on Real Data", "content": "This section provides detailed information on the procedures employed in analyzing the real dataset.\nPreprocessing The initial dataset comprised 19,719 observations.In the preprocessing phase, any observation with a missing value in any column was excluded, leaving a total of 19,710 observations for further analysis. Subsequently, we applied max-min value normalization to the scores of each question, ensuring that all scores were normalized to fall within the range [0, 1]. This normalization step is crucial for achieving uniformity in the data scale, thereby facilitating accurate analysis and comparison across the dataset.\nLabeling the Noise To derive meaningful psychological insights, it is crucial to assign semantic labels to all latent nodes. Given that the noise components are pairwise distinct and unique to the latent node Zi, we consider intervening on each noise component, then remixing and observing the changes in the observational space. This approach enables us to assign semantic labels to both the noise components and their corresponding latent nodes. We utilize observations from the male dataset as the reference context, which comprises 7,603 observations. Following the initial step of our method, we obtain the sorted $\\hat{M}^{male}$ and $\\hat{\\epsilon}^{male}$. The mixing function G is derived from $(\\hat{M}^{male})^\u2020$."}]}