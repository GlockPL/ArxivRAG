{"title": "Concept Complement Bottleneck Model for Interpretable Medical Image Diagnosis", "authors": ["Hongmei Wang", "Junlin Hou", "Hao Chen"], "abstract": "Models based on human-understandable con- cepts have received extensive attention to improve model interpretability for trustworthy artificial intelligence in the field of medical image analysis. These methods can provide convincing explanations for model decisions but heavily rely on the detailed annotation of pre-defined concepts. Consequently, they may not be effective in cases where concepts or annotations are incomplete or low-quality. Although some methods automatically discover effective and new visual concepts rather than using pre-defined concepts or could find some human-understandable con- cepts via large Language models, they are prone to veering away from medical diagnostic evidence and are challeng- ing to understand. In this paper, we propose a concept complement bottleneck model for interpretable medical im- age diagnosis with the aim of complementing the existing concept set and finding new concepts bridging the gap between explainable models. Specifically, we propose to use concept adapters for specific concepts to mine the con- cept differences and score concepts in their own attention channels to support almost fairly concept learning. Then, we devise a concept complement strategy to learn new concepts while jointly using known concepts to improve model performance. Comprehensive experiments on med- ical datasets demonstrate that our model outperforms the state-of-the-art competitors in concept detection and dis- ease diagnosis tasks while providing diverse explanations to ensure model interpretability effectively.", "sections": [{"title": "I. INTRODUCTION", "content": "The progress of Artificial Intelligence (AI) has a profound impact on the development of image processing methods, where deep learning techniques provide excellent solutions for computer-aided diagnosis in Medical Image Analysis (MIA) such as pneumonia diagnosis [1], [2] and cancer detection [3], [4]. Despite the impressive performance of deep learn- ing models, they face various challenges in practical imple- mentations. One crucial challenge is that black-box models lack transparency and interpretability during model end-to- end training. As for AI in MIA, it is crucial for doctors and patients to comprehend the significance and impact of data within the model's feature space in MIA, which allows them to place deeper trust in the model's predictions and support AI to further improve human well-being [5].\nInterpretable deep learning models can be divided into post-hoc models and ante-hoc models [6]. Post-hoc models provide final explanations for model decisions after the model training process, such as gradient-based methods [7], [8] and counterfactual-based methods [9], [10]. Although post- hoc explanations are expected to provide strong evidence for model decisions in many applications [11], [12], these models are sensitive to input data, so their decisions are more easily influenced by changes in training data, indicating that the explanations are unreliable [13]. Ante-hoc models ensure inherent interpretability throughout the entire process of model training and inference. In the field of MIA, ante-hoc explainable models mainly include attention-based [14], [15], example-based [16]\u2013[18] and concept-based models [19]\u2013[22].\nAmong these models, concept-based models have caught the most eyes of researchers since they rely on human- understandable textual concepts or human-friendly visual con- cepts to explain model decisions [23]. Some studies focus on predicting concepts contained in the images and detailedly annotated by doctors or experts to make decision. For example, the Concept Bottleneck Model (CBM) [24] is one of the most representative works. CBM predicts concept scores by minimizing the concept classification cross-entropy loss. Then, it jointly or independently trains a classifier with a bottleneck layer to make decision in which step it allows externally adjusting the contribution of concepts to modify the final decision. Furthermore, many variants of CBM have been extensively studied in disease diagnosis based on X-ray, ultra- sound and other medical images [19], [25]. Although CBM- based methods effectively enhance model interpretability, they require fully concept annotations of training data. However, in real-world scenarios, detailed annotations are extremely time-consuming and labor-consuming. There are some approaches that could automatically learn visual concepts [26], [27], but they finds concepts which are difficult to correspond to clinical concepts in a general way. With the development of Large Lan- guage Models (LLMs) or Visual Language Models (VLMs), although many researches could automatically discover and generate textual concepts for images by using LLMs or VLMs"}, {"title": "II. RELATED WORK", "content": "Model interpretability is a crucial issue, especially in the field of MIA. The decisions of deep learning models in this high-risk area directly relate to human life and health. Therefore, for doctors and patients, any model decision must be reasonably explained, which could promote humna trust model and make the next medical decision based on the model prediction. To enhance users' confidence in computer-aided or automated diagnostic systems, concept-based interpretable models for disease diagnosis have been studied widely.\nWith the development of CBM, some research has been proposed to enhance the interpretability of disease diagnosis models based on the concept bottleneck. For instance, Yuk- sekgonul et al. [30] propose a Post-hoc Concept Bottleneck Model (PCBM) which flexibly obtains concept vectors from datasets with annotated concepts to generate concept subspace. Then, they map the feature of target domain data to the concept subspace to derive concept scores. Additionally, they also incorporate a residual network to enhance classification performance at the expense of model transparency. Chauhan et al. [19] introduce an interactive strategy that enables the model to seek input from human collaborators for labeling specific concepts, thereby enhancing the model's accuracy. Furthermore, some studies leverage textual concept encod- ing to aid in concept identification in images. Patricio et al. [31] propose a two-stage skin disease diagnosis model. Initially, they deriv lesion segmentation masks by a trained semantic segmentation model to masking invalid features. Subsequently, they append a concept encoding layer after the feature extractor to pinpoint the presence of specific concepts in the image and generate corresponding scores for classifier training. During training, they minimize the consistency loss between final visual concept encoding and textual concept encoding. Bie et al. [32] present a skin disease diagnosis method based on multi-level alignment of image and textual concepts encoded by the pretrained LLM. They capture more detailed semantic information in the images through three levels of alignment: image-level, token-level, and concept- level. Similarly, they employ a multilayer perceptron with two fully connected layers (FCLs) to compute concept scores and diagnose skin diseases from aligned features.\nThe methods mentioned above have greatly ameliorated the transparency of medical image diagnosis models, but they rely entirely on given medical concepts and fine-grained concept annotations. To reduce the dependence on concept annotations, some researchers present to discover image areas which could be defined as concepts [26], [27]. Fang et al. [26] propose to clustering the regular image blocks which are effective for the model decision-making. Despite that the model could find the areas of the discover concepts are consistent with the doctors' explanations, the method is hard to be generalized to other diseases, like skin disease diagnosis due to the highly over- lapping medical visual concepts. Additionally, some models have explored concept-based interpretable disease diagnosis methods using LLMs to generate concepts. For example, Patr\u00edcio et al. [21] propose an embedded learning strategy to enhance the model's performance. They add trainable layers on top of the frozen CLIP to align categories with images. During inference, they predict image categories by calculating the similarity scores between text concepts generated by ChatGPT and the images. To further improve the semantic relevance between generated concepts and the input images, Kim et al. [20] first generate candidate concepts using a LLM and then detect whether a given concept was present based on concept- based visual activation scores to remove invalid concepts. They only utilize visually meaningful concepts to guide and explain model decisions. However, all concepts share the same image features, which places higher demands on the feature extractor and limits the exploration of the different roles of various concepts in skincon disease diagnosis. Moreover, there still is the performance gap between black-box models and the existing methods based on fine-grained medical concepts and"}, {"title": "III. METHODOLOGY", "content": "In this paper, we present a Concept Complement Bottle- neck Model (CCBM). Firstly, we utilize textual concepts to guide the model to learn high-dimensional medical concepts. Secondly, by configuring the concept adapters, the model ex- tracts features that are conducive to learning specific concepts and aggregates concept scores from cross-attention modules. Finally, we propose a concept complement strategy to learn new and effective concepts, which ensures the interpretability of the model while reducing the performance gap between the interpretable model and the black-box model. The overall framework of the model is shown in Fig. 1. We construct CCBM by the following steps below.\nWe first extract the textual known concept embeddings and then specifically learn visual features through the concept adaptersvin this step.\n1) Encoding textual known concepts: The textual known concept set is defined as $K$ which includes $n_k$ labeled con- cepts, and the textual known concept encoder $E_T$ is used to encode the textual known concepts. In our settings, $E_T$ is a frozen text encoder to map the concepts to $d_k$-dimensional concept subspace: $E_T : K \\rightarrow \\mathbb{R}^{d_k}$. Furthermore, these known concept embeddings will be used as the keys $K_i$ $(i = 1, 2, ..., n_k)$ and values $V_i$ $(i = 1, 2, ..., n_k)$ in the cross- attention module.\n2) Encoding visual known concepts independently: We set up concept adapters to extract key features from a shared image encoding for each concept, considering their unique differences. Detailedly, we select a Convolutional Neural Network (CNN) $E_I$ as image encoder to map the input image data $X$ including $N$ samples from $n_c$ classes to the fundamental feature space: $E_I : X \\rightarrow \\mathbb{R}^d$, where $d$ represents the feature dimension. After that, we set $n_c$ concept adapters $C_i$ $(i = 1, 2, ..., n_k)$ to extract specific concept features from the fundamental features to be the quaries $Q$ of the following cross-attention module. Intuitively, we hope the fundamental features extracted by the image encoder can include all of crucial features for all different concepts, so we had better to choose a strong image encoder. For simplicity, in our setting, each concept adapter is set as a FCL that maps the $d$-dimension fundamental feature space to the $d_k$-dimension concept subspace. It is worth noting that we need to ensure that the subspace dimensions of all concept adapters are consistent with the subspace dimension of text encoder. We fomulate the image encoder and concept adapters as follows:\n$Q_i = C_i(E_I(X)), i = 1, 2, ..., n_k$,                                                      (1)\nin which $Q_i$ is the specific concept features extracted by the i-th concept adapter.\nThrough the encoders and concept adapters, we can obtain the image features of all known concepts and the embedings of all textual known concepts. At this step, we use the multi- head cross-attention module to obtain the attention score. For all known concepts, we use the embeddings of the textual concept as the keys and values, and the outputs of the concept adapters as the queries to calculate the attention weights where each output is a query. All textual concept embeddings are"}, {"title": "B. Concept Score Prediction", "content": "represented as the matrix $K \\in \\mathbb{R}^{m \\times d_k}$ and $V \\in \\mathbb{R}^{m \\times d_k}$, and all visual concept embeddings are denoted as the matrix $Q \\in \\mathbb{R}^{m \\times d_k}$. In this paper, we only fomulate the expression of single-head cross-attention output:\n$A(Q, K) = softmax(\\frac{QK^T}{\\sqrt{d_c}}),$                                                                                (2)\n$A_w(Q, K, V) = A(Q, K)V,$                                                                                               (3)\nwhere $A \\in \\mathbb{R}^{m \\times d_k}$ is the attention map matrix whose elements represents the weights of different concept pairs and $A_w \\in \\mathbb{R}^{m \\times d_k}$ is the weighted sum of all concepts. According to the setting of the concept adapters, we can not average these attention as final features to calculate concept scores by a FCL but need to aggregate them in another way to get concept scores independently. Specifically, we can get the attention $A_w(Q_i, K, V)$ for the i-th concept. Furthermore, we need to calculate the concept scores based on these attention weights. Different from the previous bottleneck models who directly use a FCL to project the common feature to get the concept scores, we could calculate these concept scores independently by any score calculation module based on their specific concept features. In our model, the aggregator for known concepts includes $n_k$ FCLs, and we use each FCL $f_i$ to project the attention weights to get the concept score for each concept. Hence, the concept scores are calculated as follows:\n$S_i = f_i(A_w(Q_i, K, V)) \\in \\mathbb{R}^{n_k}, i = 1, 2, ..., n_k$,                                                        (4)\nwhere $S_i$ is the final concept score for the i-th concept."}, {"title": "C. Concept Complement Bottleneck Model for Medcial Image Diagnosis", "content": "1) Explainable Diagnosis Decision using Pre-defined Con- cept Set: If we do not set unknown concept learning branch in our model, based on these concept scores, we can directly predict the diagnosis results by a decision layer $f_d$. The final prediction is:\n$\\hat{Y} = f_d(S) \\in \\mathbb{R}^{n_c}$,                                                                                             (5)\nwhere $S = [S_1, S_2, ..., S_{n_k}]$ is the concept score vectors of input images and $\\hat{Y}$ is the final disease prediction.\nDuring the training process, we jointly train the model to perform well on the concept detection task and disease diag- nosis task. In particular, we require model decisions to more explicitly depend on these concept scores to ensure model interpretability. Therefore, we leverage the cross-entropy loss for the classification task and the concept-learning loss for the concept detection task. The total loss of our CCBM is:\n$\\underset{\\hat{Y},S}{min} \\; (\\lambda_1L_{ce}(Y, \\hat{Y}) + L_{cep}(S, C)),$                                                                                               (6)\nwhere $\\hat{Y}$ is the classification prediction of the model, $Y$ is the ground truth of image diagnosis, $C$ is the matrix of the ground truth of the concept detection task, and $\\lambda_1$ is the hyperparameter to balance the two tasks. $L_{ce}$ is the cross- entropy loss for classification task:\n$L_{ce} = - \\sum_{i=1}^{N_C} \\sum_{j=1}^{n_c} Y_{ij} \\log(\\hat{y}_{ij}),$                                                                                           (7)\nand $L_{cep}$ depends on the concept learning task is a classifi- cation task or a regression task. As for the former, we use the multi-label classification cross-entropy loss, which can be formulated as:\n$L_{cep} = - \\sum_{i=1}^{N} \\sum_{j=1}^{n_k} (C_{ij} \\log(s_{ij}) + (1 - C_{ij}) \\log(1 - s_{ij})),$                                                        (8)\nand as for the latter, if the concept annotation is scored but not binary, we use the Mean Square Error (MSE) loss $L_{cep}$:\n$L_{cep} = \\sum_{i=1}^{N} \\sum_{j=1}^{n_k} (C_{ij} - s_{ij})^2,$                                                                     (9)\nwhere $s_{ij}$ and $C_{ij}$ is the concept label and the ground truth of the j-th concept of the i-th sample, respectively. $s_{ij} = \\sigma(S_{ij})$, and $\\sigma(\\cdot)$ is the sigmoid function.\n2) Concept Complement Strategy: Although existing mod- els based on concept bottleneck can maintain high classifi- cation performance while providing interpretability by train- ing on concept detection tasks and classification tasks, the model performance needs to be balanced between concept detection and classification tasks. In addition, there is still a gap between explainable model and black-box models in classification tasks. In order to reduce this performance gap while maintaining model transparency, we propose a concept complement strategy to learning unknown concepts which are helpful to diagnosis.\nThe concept complement strategy includes two parts: con- cept complement and concept score adjustment. Concept com- plement means that we add $n_u$ additional unknown concept adapters which are set as $n_u$ FCLs projecting features from $d$-dimension to $d_u$-dimension space to learn $n_u$ new concepts. The general processing of unknown concepts is similar to that of known concepts in the other steps. However, we have no textual representions of these new concepts. We set $n_u$ learnable embedding vectors in the model to uniquely represent the learned unknown concepts which will be used as the keys $K^u$ and values $V^u$ of unknown concept cross- attention module. If we denote the visual encoding of unknown concepts from concept adapters as $Q_u \\in \\mathbb{R}^{n \\times d_u}$, the concept scores of unknown concepts are calculated as:\n$l_j = g_j(A_w(Q, K^u, V^u)) \\in \\mathbb{R}^{n_u}, j = 1, 2, ..., n_u,$                                                             (10)\nwhere $l_j$ is the final concept score for the j-th unknown concepts and $g_j$ is the FCL for aggregating the j-th unknown concept. It is worth noting that these scores are only used to help adjust the importance of known concepts so that the model can better approach the performance of the black-box model, since we have no label of those concepts. The final diagnosis of model can be represented as:\n$\\hat{Y} = f_l([S, L]) \\in \\mathbb{R}^{n_c}$,                                                                                                (11)\nwhere $L = [l_1, l_2, ..., l_{n_u}]$ is the concept scores of unknown concepts, $[S, L]$ is the concatenation of the concept scores of known and unknown concepts, and $f_l$ is the FCL for the final prediction. Additionally, we add a similarity loss term to the total loss to ensure the unknown concepts are as different as"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we introduce our experiment datasets, set- tings and comparison algorithms, and we report and analyze our experimental results to verify the effectiveness of CCBM.\nWe conducted comprehensive experiments on two dermo- scopic image datasets Derm7pt and Skincon, an ultrasound breast image dataset BrEaST, and a lung nodule CT dataset LIDC-IDRI. Derm7pt [35] contains 1011 dermoscopic images including 20 specific skin disease diagnosis and detailed labels of 7 clinical concepts based on the seven-point skin lesion malignancy checklist. We only considered 827 images in which the diagnosis belongs to Nevus and Melanoma. Skincon [36] contains 3230 skin images of Malignant, Benign, and Non-neoplastic categories in the Fitzpatrick 17k dataset [37]. We choose 22 concepts where there are at least 50 images con- taining the concept from 48 general medical clinical concepts densely annotated by two dermatologists. BrEaST [38] is an ultrasound breast image dataset with detailed annotations via 7 concepts from BI-RADS descriptors, which contains 256 images with 3 different types of breast diagnosis, including Benign, Malignant and Normal. We only use 254 abnormal breast images in our experiments. LIDC-IDRI [39] contains 1018 CT scans with detailed annotations for 8 concepts from experienced radiologists. We extract the regions of interest of 2635 lung nodules with a diameter over 3 mm from 2D CT images for experiments. To address the annotation disagreement among the radiologists, we calculate the average of the benign-malignant ranking as the malignant score for the disease diagnosis task. Then, we assign binary labels to nodules based on their 0-5 averaged malignancy score. If the malignancy score is over 3, we define the nodule as Malignant; otherwise, it is Benign. Similarly, for each concept, we normalized scores to a range of 0 to 1 and averaged the different scores from radiologists to obtain the final concept scores. The used concepts are detailedly shown in Table I.\nAs for dermoscopic image datasets Derm7pt and Skincon, we follow PCBM [30] and choose the trained Inception-v3 model [40] as the image encoder, and we use pretrained ResNet50 as the image encoder for other two datasets. The frozen pre-trained ClinicalBERT [41] is utilized as the text encoder. In our experiments, the concept adapters and aggre- gators are set as FCLs. Additionally, we use Adam optimizer for model training. We use the grid method to choose model hyperparameters. For the Derm7pt and BrEaST dataset, the hyperparameters are set as $\u03bb_1 = 0.2, \u03bb_2 = 10$. For the Skincon dataset, $\u03bb_1 = 0.1, \u03bb_2 = 5$, and $\u03bb_1 = 0.5, \u03bb_2 = 10$ for LIDC- IDRI dataset. As for the number of unknown concepts, we can set $n_u$ as the number of categories of datasets, since intuitively we need at least the same number of data features as the sample categories to distinguish all categories. To evaluate model performance, we use the Area Under Curve (AUC), ACCuracy (ACC) and F1-score as disease diagnosis evaluation metrics, and the first two metrics are also used to evaluate the concept detection tasks of Derm7pt, Skincon and BrEaST. Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are used for evaluating the concept Regression task of LDIC-IDRI dataset. All of mean and standard deviation results are obtained by five-fold cross-validation in our experiments.\nTo verify the effectiveness and advancement of the model, we compare CCBM with the state-of-the-art methods, in- cluding CBM [24], PCBM (-H) [30], an Ante-hoc Explain- able Concept-based model (AEC) [34], Concept-Based In- terpretability using Vision-Language Models (CBIVLM) [21] and Energy-based CBM (ECBM) [33]. We also test the backbone models to evaluate the gap between our explainable model and black-box models. For methods that do not support concept detection, we use \u201cN/A\" to indicate invalid data."}, {"title": "C. Experimental Results and Analysis", "content": "1) Disease Diagnosis and Concept Detection: To verify the effectiveness of our model on concept detection task and skin"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a concept complement bottleneck model for interpretable medical image analysis by jointly learning unknown concepts while using known concepts to predict disease. Our model incorporates concept adapters and aggregators with a visual-text concept cross-attention mod- ule, creating an fair concept bottleneck model that enhances the precision and effectiveness of disease predictions using known concepts. We also present an effective strategy for learning unknown concepts, aimed at extracting more sig- nificant information to enhance model performance. Through comprehensive experiments, we demonstrate that our model achieves superior classification performance in concept de- tection and disease diagnosis tasks, providing more faithful and understandable explanations. Detailed experiment analysis showcases the effectiveness of the learned unknown concepts. While our study does not delve into the development of new concept textualization, specialization, and generalization, these areas present promising avenues for future exploration."}]}