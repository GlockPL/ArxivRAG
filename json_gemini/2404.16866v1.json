{"title": "Functional Protein Design with Local Domain Alignment", "authors": ["Chaohao Yuan", "Songyou Li", "Geyan Ye", "Yikun Zhang", "Long-Kai Huang", "Wenbing Huang", "Wei Liu", "Jianhua Yao", "Yu Rong"], "abstract": "The core challenge of de novo protein design lies in creating proteins with specific functions or properties, guided by certain conditions. Current models explore to generate protein using structural and evolutionary guidance, which only provide in-direct conditions concerning functions and properties. However, textual annotations of proteins, especially the annotations for protein domains, which directly describe the protein's high-level functionalities, properties, and their correlation with target amino acid sequences, remain unexplored in the context of protein design tasks. In this paper, we propose Protein-Annotation Alignment Generation (PAAG), a multi-modality protein design framework that integrates the textual annotations extracted from protein database for controllable generation in sequence space. Specifically, within a multi-level alignment module, PAAG can explicitly generate proteins containing specific domains conditioned on the corresponding domain annotations, and can even design novel proteins with flexible combinations of different kinds of annotations. Our experimental results underscore the superiority of the aligned protein representations from PAAG over 7 prediction tasks. Furthermore, PAAG demonstrates a nearly sixfold increase in generation success rate (24.7% vs 4.7% in zinc finger, and 54.3% vs 8.7% in the immunoglobulin domain) in comparison to the existing model. We anticipate that PAAG will broaden the horizons of protein design by leveraging the knowledge from between textual annotation and proteins.", "sections": [{"title": "Introduction", "content": "Protein design Marshall et al. (2019) is a crucial task for its immense potential on drug discovery Sli-woski et al. (2014), enzyme engineering Sterner et al. (2008), immunongineering Swartz et al. (2012) and so on. The generation of proteins with specific properties, behaviors, or functions, such as optimizing the binding affinity to given molecules Houk et al. (2003) or incorporating a particular ion-binding site Regan (1995), is known as de novo protein design. This process presents a significant challenge due to the vast space of protein sequences and the complexity of protein functions. Recently, machine learning models have shown profound potential for protein design. The existing studies mostly rely on the structural Watson et al. (2023) or evolutionary information Alamdari et al. (2023a) as the guidance to design proteins. However, in many cases, these conditions can only offer indirect"}, {"title": "Preliminaries", "content": null}, {"title": "Protein and Its Textual Annotations", "content": "The primary structure of a protein can be represented as an amino acid sequence $S = (s_1,s_2,\\dots, s_n)$, where $s_i$ is the i-th amino acids chosen from 20 different amino acids which are represented as 20 characters.\nGiven a protein S, the annotation-sequence pair set $D_s = \\{(T_k, S_{kj})\\}_{k=1}^{K_s}$, is constructed by extracting the correspondence between textual annotations and protein domains & properties from protein database, such as UniProtKB Consortium (2022), where $T_k$ is the textual annotations of the k-th domain $S_k$ of the protein and $K_s$ is the number of annotation-domain pairs of the protein S. For the property annotation, the corresponding domain is the entire sequence, i.e, $S_{1:|s|}$. This annotation-domain pair set $D_s$ provides comprehensive knowledge about the protein S and, therefore, plays an important role in training the protein generation model."}, {"title": "Encoders for Proteins and Annotations", "content": "We adopt a protein encoder (PE) to generate both the protein and its domain representations based on the amino acid sequence as $z_S = f_{PE}(S)$, where $f_{PE}$ is the protein encoder and $z_S$ is the embedding of S.\nFor textual annotations, we generate their representations using the pre-trained language model $f_{LM}$ as $z_A = f_{LM}(G(A_s))$, where $A_s \\subseteq \\{T_k | (T_k, S_j) \\in D_s\\}$ is a subset of the annotations in the annotation-domain pair set for a protein S, G(\u00b7) is a template function which converts a set of annotations to a textual description, and $z_A$ is the embedding of the annotation set. Note that the annotation subset can cover all annotations in $D_s$ or just one. If $A_s$ covers only one annotation $T_o$, we can skip the template function and directly generate the embedding of $T_o$ using $f_{LM}$.\nWe employ transformer-based protein encoder and text encoder as our base model and initialize them with pre-trained models. Specifically, we employ SciBERTBeltagy et al. (2019), which is pre-trained on computer science and biological datasets, to initialize the text encoder $f_{LM}$. The pre-trained protein encoder, ProtBERTElnaggar et al. (2021), is used to initialize the protein encoder $f_{PE}$."}, {"title": "Methodology", "content": "In this section, we propose a multi-modal framework, Protein-Annotation Alignment Generation(PAAG), which enables the flexible annotations-guided protein design."}, {"title": "Conditional Protein Decoding", "content": "Our ultimate goal is generating functional proteins conditioned on a set of annotations $A = \\{T_k\\}_{k=1}^{|A|}$. We will first employ template function G to translate these annotations as textual descriptions and then utilize a language model to extract the representation of the annotation set $z_A = f_{LM}(G(A))$. Then the protein sequence is generated using a decoder $f_D$ based on this representation as $S = f_D(z_A)$.\nProtein Decoder. We adopt an auto-regressive protein decoder $f_D$ that can receive the condition from the language model. Specifically, the decoder will first process the protein sequence through causal attention layers to enable auto-regressive generation, then integrate the information from annotations via cross-attention layers followed by feed-forward layers. Note that the causal attention layers are initialized using the same weights as the self-attention layers in the protein encoder and the feed-forward layers share the same parameters as the protein encoder. Here, the parameter-sharing mechanism enables higher training efficiency Li et al. (2023). The cross-attention layers are randomly initialized and trained from scratch.\nProtein Modeling (PM) Loss. To guide the learning of the model, we estimate the PM loss as\n$L_{PM} = -\\sum_{i=1}^{l} log p(S_i|S_{<i}; A)$,\nwhere l is the length of the protein S and $p(S_i|S_{<i}; A)$ is the predicted probability of the i-th amino acid given all previous amino acids and the annotation set."}, {"title": "Multi-level Protein and Annotation Alignment", "content": "To better integrate the information between proteins and annotations, we aim to align the multi-level representations of proteins and annotations. Specifically, we conduct local alignment and global alignment by performing contrastive learning at domain level and protein level, respectively."}, {"title": "Local Alignment", "content": "Given a protein S, and its annotation-domain pairs set $D_s$, in domain-level, we aim to align the representation of the domain $S_h$; and its corresponding annotation $T_k$ and use all annotation-domain pairs in $D_s$ as positive pair. To construct the negative pairs, we randomly sample the sub-regions outside the domain $S_j$, i.e. $S_{1:j-1} \\cup S_{j+1:l}$, as the negative samples $S_h^{neg}$ of the domain.\nAnnotation-Domain Contrastive(ADC) Loss: We adopt InfoNCE loss as the ADC loss to align the representations of local annotation $z_i^k$ and functional domain $z_{Si}^k$ as\n$L_{ADC} = -\\frac{1}{K_s} \\sum_{k=1}^{K_s} log \\frac{exp(s(S_j^k, T_k)/\\tau)}{\\sum_{n=1}^{N} exp(s(S_j^k, T_n)/\\tau)}$,\nwhere $K_s$ is the number of functional domains for the protein S, N is the number of negative samples, and $\\tau$ is a learnable temperature parameter.\nThe local alignment enables PAAG to explicitly learn the relation between the functional domain and its annotation. Therefore, we can use the model to controllably generate the specific functional domain given annotations."}, {"title": "Global Alignment", "content": "To enable global alignment, we utilize template function G(\u00b7) to construct protein-level textual de-scription and form the positive protein-description pairs as {(G(As), S)}. Since multiple annotations and the entire protein will be more complex, to enlarge the number of negative samples for global alignment, we follow the setting of MOCO He et al. (2020) to construct momentum encoders fm as\n$f_m \\leftarrow mf_m + (1 - m)f$,\nwhere the encoder f can be either protein encoder fPE or text encoder fLM, and m is the momentum hyperparameter. We follow implementation details in Li et al. (2021) and Li et al. (2022) to construct the momentum encoders for both encoders.\nThe momentum encoders extract consistent features to increase the number of negative samples, and the dynamic dictionaries will store these features.\nAnnotation-Protein Contrastive (APC) Loss is designed to align the representations of global properties $z_A$ with protein $z_S$. Specifically, for each protein sequence and annotation set, we calculate the softmax-normalized sequence-to-annotation and annotation-to-sequence similarity as:\n$p_S^{s2a}(S) = \\frac{exp(s(S, A_m)/\\tau)}{\\sum_{m=1}^{M} exp(s(S, A_m)/\\tau)}$\n$p_A^{a2s}(A) = \\frac{exp(s(A, S_m)/\\tau)}{\\sum_{m=1}^{M} exp(s(A, S_m)/\\tau)}$\nwhere $\\tau$ is a learnable temperature parameter, Am and Sm indicate their representations will be extracted by respective momentum encoders.\nDenote $\\tilde{y}^{a2s}(A)$ and $\\tilde{y}^{s2a}(S)$ as the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1. The annotation-protein contrastive loss is defined as the cross-entropy H between p and $\\tilde{y}$:"}, {"title": "Training Objectives", "content": "In the training of PAAG, we optimize four objective functions. These functions are designed to align the representations between annotations and protein sequences, integrate the two modalities, and reconstruct the protein sequence. In contrast to ProteinDT Liu et al. (2023a), which splits the training process into three separate stages, PAAG jointly optimizes these four objective functions in an end-to-end manner.\nThe overall pretraining objective of PAAG is:\n$L = L_{ADC} + L_{APC} + L_{APM} + L_{PM}$."}, {"title": "Annotation-guided Protein Design", "content": "After obtaining the model F of PAAG, we can use F to design the proteins with given textual annotations. In the following, we given the definition of annotation-guided protein design.\nDefinition 3.1 (Annotation-guided Protein Design) Given an annotation set $A=\\{T_k\\}_{k=1}^{K}$ with K annotations, a generative model F can leverage the information from $\\{T_k\\}_{k=1}^{K}$ to generate the protein S which satisfies the condition described in $\\{T_k\\}_{k=1}^{K}$. Quantitatively, this task aims to maximize the following objective function:\n$max \\sum_{T \\in A} M_T(S), s.t. S = F(A)$.\nwhere $M_T()$ is corresponding metric function for the annotation T.\nIn this paper, we mainly focus on two type of measures, functional-domain measure and global-property measure.\nFunctional-domain metric: for the annotation describing a certain functional domain, $M_T()$ will invoke a profile hidden Markov models from Pfam Mistry et al. (2021) to search for the optimal match within protein S regarding to the domain described by annotation T and assign an e-value s to this match. Given an e-value threshold e, we have:\n$M_{T,e}(S) = 1_{S}(s < e)$,\nwhere $1_{S} (cond)$ outputs 1 when cond = True, otherwise, outputs 0.\nGlobal-property metric: for the annotations describing protein's global properties, $M_T()$ employ a pre-defined oracle to determine if the given S has the property described by the annotation T and output a score s in the range of [0, 1]. Here, 0 denotes the absence of the property, while 1 signifies its presence."}, {"title": "Experiment", "content": "In this section, we extensively evaluate PAAG from two aspects: (1). quality of aligned protein representation for the predictive tasks; (2) evaluation on the unconditional sequence generation and protein design with textual annotations."}, {"title": "Construction of ProtAnnotation Dataset", "content": "To enable multi-level alignment, we build the ProtAnnotation dataset with annotation-sequence pair set for protein design task. Specifically, we select all proteins with \"Domain\" entry in UniPro-tKB Consortium (2022) to build ProtAnnotation dataset, resulting 129, 727 proteins. The domain annotations are extracted by these \u201cDomain\" entries including the domain description and start & end index of this domain. Additionally, we select four properties as the property annotations, that is, \"protein_name\u201d, \u201corganism_name\u201d, \u201clength\u201d and \u201cSIMLARITY\u201d. The textual description is assembled by the template function G, which can accept any subset of annotation as input. More details, including the data examples are deferred in Appendix A.2."}, {"title": "Quality of Aligned Representation", "content": "We first conduct multiple experiments on predictive tasks. to evaluate the quality of protein represen-tation produced by PAAG.\nSettings To make a fair comparison with ProtST, we use the same proteins in dataset ProtDescribe Xu et al. (2023) to first pretrain PAAG, followed by full-model fine-tuning on various downstream tasks. For the full-model fine-tuning, we add a task head for each task and fine-tune the model for 100 epochs. We use the validation set to select the model and report the results on random seed 0, adhering to the same settings as in ProtST Xu et al. (2023). More details are deferred in Appendix A.6.\nBenchmark Tasks: We adopt 7 downstream tasks within two task types as the benchmark task.\nProtein Localization Prediction aims to forecast the subcellular locations of proteins. Derived from DeepLoc Almagro Armenteros et al. (2017), we focus on two similar tasks. The subcellular localization prediction (Abbr. as, Sub) encompasses 10 location categories and binary localization prediction (Abbr. as, Bin) that includes 2 location categories, soluble and membrane-bound. The splits of data follow the original split in DeepLoc.\nFitness Landscape Prediction is primarily focused on the prediction of the effects of residue mutations on the fitness of proteins. We evaluate our models on B-lactamase (Abbr., B-lac) landscape from PEER Xu et al. (2022), the AAV and Thermostability (Abbr., Thermo) landscapes from FLIP Dallago et al. (2021), and the Fluorescence (Abbr., Flu) and Stability (Abbr., Sta) landscapes from TAPE Rao et al. (2019). The splits of data follow the splitting setting of ProtST Xu et al. (2023)\nBaselines: We adopt two types of baseline. The first type is the models train from scratch, including CNN Shanehsazzadeh et al. (2020), ResNet Rao et al. (2019), LSTM Rao et al. (2019), Trans-former Rao et al. (2019). The second type is the pretrained models with full model tuning, including OntoProtein Zhang et al. (2022), ProtBert Elnaggar et al. (2021) and ESM2 Lin et al. (2023). For the ProtST and PAAG, we train two variants with different initialization weights from ProtBert and ESM2 to verify the enhancement of text knowledge on protein representations from different protein encoders. We utilize distinct subscripts to denote the initialized parameters, such as PAAGProtBert and PAAGESM2."}, {"title": "Unconditional Protein Generation", "content": "To verify the learning effect of the decoder, we compare the ability of different models in unconditional generation task. A good decoder is able to generate protein sequences that conform to the distribution of the training set while simultaneously exhibiting adequate novelty.\nSetting: In unconditional generation task, we only specify the length of generated proteins and conditions. We sample the same length from natural proteins, to ensue a fair comparisons across different models.\nBaselines: We compare PAAG with two representative protein design models, i.e., ProGen Madani et al. (2023) and Chroma Ingraham et al. (2023). Furthermore, we introduce two naive baselines: RandomUniform and RandomEmpirical. RandomUniform generates protein sequence by randomly selecting amino acids based on a uniform distribution, while RandomEmpirical adheres to the empirical amino acid distribution observed in the training dataset. Additionally, we also report the results of the sequence set sampled from natural proteins, denoted as Natural, to serve as a reference. The details of baselines are deferred to Appendix A.5.\nEvaluation metrics: To evaluate the quality of generated protein sequences, we employ three metrics: Distinct-n, Diversity and Novelty. Suppose S is the protein sequence set.\nDistinct-n Li et al. (2016) is a classical metric in natural language processing that measures textual diversity of generated text by counting distinct n-grams. We use this metric to assesses the fraction of repetitive sequence motifs in sequences from S, which exhibits the biological importance Andrade et al. (2000). A higher Distinct-n suggests fewer repetitive amino acid segments. We set n = 2 here."}, {"title": "Protein Design with Domain Annotations", "content": "In this section, we evaluate the performance of PAAG in generating proteins under the given domain annotations.\nSettings: We utilize two biologically significant domains, zinc-finger domain Klug and Rhodes (1987) and immunoglobulin domain Brummendorf (1995), as the target domain annotation to generate the proteins respectively. For each case, we generate N = 300 protein sequences given the"}, {"title": "Protein Design with Property Annotations", "content": "In this section, we explore the potential of PAAG to generate proteins with certain properties guided by property annotations.\nSettings: We employ the subcellular location of proteins as an example property. An additional dataset, termed ProtLocation, is generated by extracting subcellular location labels, encompassing both Bin (2-class) and Sub (10-class) tasks as delineated in Section 4.2, from Deeploc Almagro Ar-menteros et al. (2017). These labels are then incorporated into annotation-sequence pairs derived from Uniprot. ProtLocation includes 10100 proteins for training, while a separate set of 2434 test proteins is reserved for constructing the annotation set for generation. More details can be found in Appendix\nEvaluation protocol: For the generated sequences, we employ the official server provided by Deeploc Almagro Armenteros et al. (2017) and Deeploc2 Thumuluri et al. (2022) as the pre-defined oracle to construct the Global-property metric MT() for predicting binary location label and 10-class label respectively. A generation is deemed successful if the predicted label aligns with the input annotation label."}, {"title": "Ablation Study", "content": "To ascertain the contribution of each component towards the generation of the functional proteins, the ablation study reports the SR\u2081 of zinc-finger and immunoglobulin domains in the absence of each alignment loss, as presented in Table 4. We observe that $L_{ADC}$ is the key to the high SuccessRate of PAAG. The SuccessRate decreases to 0% without $L_{ADC}$, underscoring the importance of incorporating domain range into the learning framework. Furthermore, $L_{APC}$ and $L_{APM}$ also enhance the SuccessRate of generating high-quality immunoglobulin domains by 4.33% and 3.33%, respectively. Moreover, $L_{APC}$ and $L_{APM}$ are more important to zinc-finger domain, by improving the SR1 by 12% and 9.67%."}, {"title": "Case study: Joint Generation with Domain and Property Annotations", "content": "Due to the flexibility of combining annotations, PAAG can jointly generate the proteins with both specific domain and property. Specifically, we generate the zinc-finger proteins in membrane-bound by the model in Section 4.5. We showcase two examples of the generated results in Figure 6. As shown in Figure 6, PAAG can successfully generate proteins under the conditions derived from both domain and property annotations, demonstrating its potential in complex protein design tasks."}, {"title": "Related Work", "content": "Moltimodal Representation Learning. By harnessing the potential of extensive image-text pair data, the Contrastive Language-Image Pretraining (CLIP) model, as proposed by Radford et al. Radford et al. (2021), employs contrastive learning to align the representations between image and text modalities.Following by CLIP, many image-text pertaining model are proposed, such as BILP Li et al. (2022), BLIP-2 Li et al. (2023) and ClipCap Mokady et al. (2021). Beyond the image-text pretraining, several studies introduce the more modalities, such as videos Xu et al. (2021), audios Tang et al. (2023) and even molecules Liu et al. (2022) into a unified representation. Specifically, for multimodal learning on protein sequences, OntoProtein Zhang et al. (2022) first learns protein representations by combining them with textual descriptions in a knowledge graph. ProtST Xu et al. (2023) constructs a large-scale dataset containing aligned pairs of protein sequences and property descriptions, and pretrain a protein-biotext model to improve performance on downstream predictive task and enables zero-shot retrieval.\nProtein Generation Model. With huge success of language models Devlin et al. (2019), several studies Elnaggar et al. (2021); Shin et al. (2021); Ferruz et al. (2022); Rives et al. (2019) treat protein sequences consisting chains of amino acids as a type of \u201clanguages\u201d and pretrain models on millions of protein sequences. Upon the pretrained model, they generate the protein sequences in an autoregressive manner. In addition to the autoregressive model, Evodiff Alamdari et al. (2023b) extracts the evolutionary information from protein sequences and proposes an evolution-guided diffusion model to generate protein sequences. Given the significance of structural information for protein function, a group of methods Hsu et al. (2022); Ingraham et al. (2019); Tan et al. (2022); Jing et al. (2020), known as inverse folding, utilize the structure as a conditional input, allowing for the generation of amino acid sequences. Some studies Shi et al. (2022); Watson et al. (2023) integrate both sequential and structural information and propose a co-design model that accepts sequences and structures as conditions. Recently, ProteinDT Liu et al. (2023b) first proposes a text-sequence alignment framework, enabling its capabilities for text-guided protein generation and editing. Chroma Ingraham et al. (2023) trains a protein caption model ProCap and utilize it as a classifier guidance to generate proteins via a diffusion model."}, {"title": "Conclusion", "content": "In this paper, we present PAAG, a multi-modality framework that first incorporates the rich annotation information derived from protein database, achieving the superior performance in various applications, such as representation learning and annotation-guided protein design. Crucially, we demonstrate that it is possible to use the flexible combinations of various kinds of textual annotations to guide the protein design process. We hope that PAAG will expand the possibilities of protein design"}, {"title": "General settings", "content": "Backbone Models of PAAG: Unless otherwise specified, we utilize the weights from ProtBert-BFD Elnaggar et al. (2021) as the initialization of our protein encoder. For the text encoder, we choose SciBert Beltagy et al. (2019) as the initial backbone. The scarcity of training data does not allow us to train a large decoder, we utilize a lighter decoder, initialized by DistilProtBert Geffen et al. (2022). A decoder that contains more parameters is also our future direction.\nTraining Configurations: We train PAAG on ProtAnnotation, using the AdamW optimizer (with a learning rate of 3e-5 and zero weight decay) for 100 epochs. Our generation experiments are conducted on 16 NVIDIA Tesla A100-SX4-40GB GPUs."}, {"title": "ProtAnnotation and the template function G(.)", "content": "Figure 5 depicts additional statistics of ProtAnnotation. We demonstrate several example data samples in ProtAnnotation as well as the corresponding textual description generated by the template function G(.).\nThe explanation of four property annotations:\nprotein_name: The protein name is a naming method used to describe the function, characteristics, or origin of a protein. These names usually contain information about the protein's structure, function, substrate specificity, and biological process.\norganism_name: Organism names are used to identify and classify different species of living organisms, including bacteria, fungi, plants, and animals. These names usually consist of the genus and species of the organism, and sometimes include additional information such as strain or cultivar.\nlength: The number of amino acids in the protein sequence.\nSIMILARITY: In the context of biology and protein classification, \u201csimilarity\" refers to the shared characteristics or features among different proteins. This can include similar structures, functions, or evolutionary origins. Proteins with high similarity are often grouped into the same family or subfamily."}, {"title": "Biological background of zinc-finger and immunoglobulin domain", "content": "We provide some introductions of zinc-finger and immunoglobulin domain from biological perspec-tive, elucidating their significance and functions within the context of molecular biology.\nZinc-finger: Zinc finger Klug and Rhodes (1987) domains are compact protein structural motif with multiple protrusions that interact with target molecules. Initially identified in the transcription factor TFIIIA from Xenopus laevis as a DNA-binding motif, these domains are now recognized for their ability to bind DNA, RNA, proteins, and lipids. Their binding characteristics depend on the amino acid sequence, the linker structure, the higher-order structures, and the number of fingers. Often found in clusters, each finger within the Zinc finger domain can have unique binding specificities. Despite being present in several unrelated protein superfamilies, these domains maintain stable structures and play a crucial role in diverse cellular processes such as gene transcription, translation, mRNA trafficking, and more. Zinc-binding motifs, part of the Zinc finger domains, rarely undergo significant conformational changes upon target binding.\nImmunoglobulin domain: Immunoglobulin (Ig) domains Brummendorf (1995), also referred to as immunoglobulin-like domains or Ig-like domains, are protein domains that of importance in the immune system and are found in a wide range of proteins, both within and beyond the immune system. These domains are characterized by their highly conserved structure, which typically consists of 70 to 110 amino acid residues that form a compact, globular fold. The Ig domains are ubiquitous in nature and can be found in a diverse array of proteins, including antibodies, T-cell receptors, major histocompatibility complex (MHC) molecules, cell adhesion molecules, and various receptor proteins. The primary function of Ig domains is to facilitate protein-protein interactions, particularly in the context of the immune system, where they mediate the recognition, binding, and neutralization of foreign antigens."}, {"title": "The evaluation metric for the unconditional generation", "content": "In the computation of the Distinct-n metric, we assign a value of n to 2. For the Novelty metric, the default parameters of Mmseq2 are employed with an exception for the e-value threshold, which"}, {"title": "More details of baselines", "content": "ProGen Madani et al. (2023) first forms each property of protein as keyword tags. In the training stage, ProGen prepends the keyword tags to the corresponding amino acid sequence, and try to reconstruct the amino acid sequence in the auto-regressive manner. ProGen consequently has the ability to generate the protein sequence given the keyword tags. During controllable generation, ProGen first translates desired properties into keyword tags, then can controllably generate protein sequence.\nChroma Ingraham et al. (2023) is also a protein design model that but focus more on generating the protein backbone structure. Specifically, Chroma is a diffusion model that can generate protein backbone with desired properties and functions by accepting the classifier guidance. Therefore, Chroma introduces ProCap, a protein-to-text model, as the classifier guidance to enable text-guided protein design. With an additional sequence sampling model, Chroma is capable of sampling both structure and sequence based on the conditions.\nIn unconditional generation, PAAG only accept the length I of the protein sequence. We also define this is the k-th protein we generated Consequently, we set the template function as \u201cprotein number k, contains 1 amino acids.\u201d\nWhile generate proteins with functional domains, we randomly sample the organism name, length and similarity from natural proteins that have corresponding functional domain. We specify the generative hyper-parameters in A.8. When given property annotations. Deeploc Almagro Armenteros et al. (2017) contains training data and test data in its original split. We utilize template function G() to form the textual descriptions of test data. Then PAAG can generate protein with properties given these descriptions."}, {"title": "Training configurations", "content": null}, {"title": "Predictive Task", "content": "In downstream prediction tasks, PAAG utilizes the embedding in the aligned space. Specifically, we not only use the protein encoder to extract the protein representations. We also keep the projector head to extract the more informative aligned features. Furthermore, we also incorporate dropout rate and weight decay to improve the generalization ability."}, {"title": "Protein Design with Domain Annotations", "content": "While training PAAG with ProtAnnotation, we introduce momentum contrast He et al. (2020), and we set the queue size as 16384, and momentum as 0.995. In the contrastive learning, we initialize the learnable temperature as 0.07. Moreover, we align the our latent space in 256 dimension with one linear layer. Also, following the setting of Li et al. (2022), to enable soft labeling, we initialize the learnable alpha as 0.4. When fine-tuning on the dataset only contains zinc-finger and immunoglobulin"}, {"title": "Protein Design with Property Annotations", "content": "Due to the scarcity of training data, we conduct fine-tuning process based on the models trained in Section 4.4 for 100 epochs. We mostly keep hyper-parameters as training on ProtAnnotation but decrease the learning rate to le-5 without warming up.\nFor binary localization properties, soluble and membrane-bound, we set template function G() as \"is soluble\u201d or \u201cis membrane-bound\u201d. For 10-class subcellular localization, G() is \u201clocates in {location}\u201d."}, {"title": "Additional experimental results", "content": null}, {"title": "Results of protein generation with 10-class subcellular location annotation", "content": "We represent the confusion matrix of 10-class subcellular location annotation result in Figure 8 for clearer illustration."}, {"title": "Hyper-parameters", "content": null}, {"title": "Hyper-parameters of generative task", "content": "In our generative task, we utilize nucleus sampling in our decoder. Specifically, we will sample the amino acids based on their probability instead of always choosing the amino acids with the highest probability. We set the Top-p hyperparameter as 0.9 in nucleus sampling, which means the decoder will only consider the most likely amino acids that make up 90% of the probability mass. Besides, we set the repetition penalty of decoder as 1.2."}, {"title": "Hyper-parameters of the predictive task", "content": "The hyperparameters for the predictive task concerning PAAG-ProtBert and PAAG-ESM-2 can be found in Table 7 and Table 8, respectively."}]}