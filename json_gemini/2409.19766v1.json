{"title": "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "authors": ["Son Quoc Tran", "Matt Kretchmar"], "abstract": "This paper proposes a novel training method\nto improve the robustness of Extractive Ques-\ntion Answering (EQA) models. Previous re-\nsearch has shown that existing models, when\ntrained on EQA datasets that include unanswer-\nable questions, demonstrate a significant lack\nof robustness against distribution shifts and ad-\nversarial attacks. Despite this, the inclusion\nof unanswerable questions in EQA training\ndatasets is essential for ensuring real-world reli-\nability. Our proposed training method includes\na novel loss function for the EQA problem and\nchallenges an implicit assumption present in\nnumerous EQA datasets. Models trained with\nour method maintain in-domain performance\nwhile achieving a notable improvement on out-\nof-domain datasets. This results in an overall\nF1 score improvement of 5.7 across all testing\nsets. Furthermore, our models exhibit signifi-\ncantly enhanced robustness against two types\nof adversarial attacks, with a performance de-\ncrease of only about a third compared to the\ndefault models.", "sections": [{"title": "1 Introduction", "content": "Unanswerable questions are a valuable part in the\ntraining datasets of Extractive Question Answering\n(EQA) models. By learning from these questions,\nmodels can develop the ability to avoid extracting\nmisleading responses, ultimately improving their\nreliability in real-world applications.\nCurrently, there are two lines of research on\nunanswerable questions in EQA. Firstly, Rajpurkar\net al. (2018) introduced the SQUAD 2.0 dataset\nby adding adversarial unanswerable questions\ninto SQUAD 1.1 (Rajpurkar et al., 2016). This\nwork later inspired similar benchmarks in other\nlanguages such as French (Heinrich et al., 2021)\nand Vietnamese (Nguyen et al., 2022). In the\ncrowdsourcing process for adversarial unanswer-\nable questions, human annotators are typically pre-\nsented with a triple of context, an answerable ques-\ntion, and its corresponding answer(s). They are\nthen asked to write unanswerable questions that\nexhibit an adversarial similarity to the presented\nanswerable ones.\nIn addition to the adversarially-written unanswer-\nable questions, Natural Question (Kwiatkowski\net al., 2019), Tydi QA (Clark et al., 2020b), and\nSQUAD AGent (Tran et al., 2023b) propose more\nnaturally constructed unanswerable questions. This\ncategory of unanswerable questions is also known\nas information-seeking unanswerable questions,\nemerging within the realm of information retrieval.\nThese questions are initially independent of any\ncontext. The contexts are then paired with the ques-\ntions as a result of the attempt to locate answers for\nthe given questions within a large database contain-\ning multiple contexts.\nThe distinct characteristics of these two types of\nunanswerable questions pose a challenge for mod-\nels. Models trained with one type of unanswerable\nquestions often struggle when encountering the\nother type (Sulem et al., 2021; Tran et al., 2023a),\ndefined in Machine Learning as a lack of robust-\nness under distribution shift in the inputs. Addi-\ntionally, models trained on unanswerable questions\nalso demonstrate a lack of robustness against adver-\nsarial attack (Tran et al., 2023b). Notably, models\ntrained on adversarial unanswerable questions in\nSQUAD 2.0 tend to output an \u201cempty\" response\nupon detecting any sign of contradiction between\nthe attack sentence and the given question.\nWe hypothesize that the observed lack of robust-\nness in EQA models can be attributed to two pri-\nmary factors. First, the current EQA training loss\nobjective (Devlin et al., 2019) inaccurately treats\nunanswerable questions as if they have an answer\nspan. This span is designated to start and end at the"}, {"title": "2 Related Work", "content": "There are two key research areas on improving the\nrobustness of natural language processing (NLP)\nmodels: robustness against adversarial attacks and\nagainst distribution shift (Wang et al., 2022). Ad-\nversarial attacks involve editing a test sample to\ncreate a more challenging example for trained mod-\nels without causing additional difficulty for humans.\nThese attacks can be classified based on whether\nthe attack process has access to the models' pa-\nrameters (white-box attacks, (Blohm et al., 2018;\nNeekhara et al., 2019; Alzantot et al., 2018; Wal-\nlace et al., 2019; Ebrahimi et al., 2018)) or not\n(black-box attacks, (Jia and Liang, 2017; Ribeiro\net al., 2018; Wang and Bansal, 2018; Blohm et al.,\n2018; Iyyer et al., 2018)). On the other hand, ro-\nbustness against distribution shift is measured us-\ning test samples that exhibit linguistic differences\nfrom the samples encountered by models during\nthe training phase (Miller et al., 2020).\nFindings of limited robustness in NLP models\nhave spurred significant efforts to improve their\nresilience. From a data-driven perspective, adver-\nsarial attacks can be employed during the training\nphase to enhance model robustness. Augmented\ntraining data can be created by heuristically edit-\ning (Wang and Bansal, 2018) or through neural-\nbased generation (Iyyer et al., 2018; Khashabi et al.,\n2020a; Bartolo et al., 2021; Fu et al., 2023). Ad-\nditionally, increasing the diversity of training data\nhas proven to be an effective strategy for improv-\ning model robustness (Fisch et al., 2019; Khashabi\net al., 2020b).\nIn addition to data-driven approaches, model-\nbased approaches are also effective in improving\nmodel robustness. Following the success of BERT,\nvarious studies have shown that the pretraining pro-\ncess, which involves a self-supervised objective\nand the use of large amounts of diverse pretraining\ndata, significantly enhances the generalization of\nlanguage models in downstream tasks (Hendrycks\net al., 2020; Tu et al., 2020).\nAnother research direction involves using a bi-\nased model during the training phase to force the\ntarget model to discard some spurious patterns in\nthe training set. These biased models can be de-\nsigned with a specific targeted type of bias (Clark\net al., 2019; Schuster et al., 2019; He et al., 2019;\nUtama et al., 2020a; Karimi Mahabadi et al., 2020),\nor without prior knowledge about the biases present\nin the training dataset (Clark et al., 2020a; Utama\net al., 2020b; Ghaddar et al., 2021; Sanh et al.,\n2021).\nOur work distinguishes itself by combining both\ndata-driven and model-driven approaches. From\ndata-driven side, we challenge the implicit assump-\ntion of single answers in multiple current EQA\ndatasets by augmenting \u201csynthetic\u201d answers to a\nnumber of training samples. We hope that our\nexperimental results with synthetic answers will\ninspire the development of EQA datasets that in-"}, {"title": "3 Models and Tasks", "content": "In Extractive Question Answering (EQA), models\nare trained to identify the answer (a text span in\nthe context) to the given question. The dataset\nmay include unanswerable questions, for which a\nvalid prediction is an \u201cempty\" answer. A common\nmetric to evaluate MRC systems is F1-score. It\nmeasures the average overlap between the words\nin the predicted answer and the human-annotated\ngold answer."}, {"title": "3.1 Models", "content": "In this work, we evaluate our newly proposed\ntraining method using the base version of three\npre-trained models BERT (Devlin et al., 2019),\nROBERTa (Liu et al., 2019), and SpanBERT (Joshi\net al., 2020)."}, {"title": "3.2 Extractive Question Answering", "content": "An EQA problem is given by a test set D of triplets\n(q, c, a) where q is a question posed to models, c\nis the corresponding context (usually a short para-\ngraph of text), and a is the expected answer (or set\nof \"gold\" answers). The performance of the EQA\nmodel f is measured by\n$Per(f, D) = \\frac{1}{|D|} \\sum_{(c,q,a) \\in D} m(a, f(c,q))$\nwhere m, in this paper, is the F1-score metric.\nIn our experiments, we evaluate models on both\nanswerable and unanswerable questions from dif-\nferent domains as outlined in the next section. To\ncompare the performance of models across all\ntested domains, we assume that (1) the number\nof answerable questions is equal to the number of\nunanswerable questions, and that (2) the impor-\ntance of different domains is the same.\n$Per(f) = \\frac{Per_{has-ans}(f) + Per_{no-ans}(f)}{2}$,\nwhere $Per_{has-ans}(f)$ and $Per_{no-ans}(f)$ are\nthe average performance of model f on all do-\nmain of answerable and unanswerable ques-\ntions, respectively. Specifically, we can calculate\n$Per_{has-ans}(f)$ as follows:\n$Per_{has-ans}(f) = \\frac{1}{|S_{has-ans}|} \\sum_{D \\in S_{has-ans}} Per(f, D)$\nwhere $S_{has-ans}$ is the set of all testing set with\nanswerable questions."}, {"title": "3.3 Datasets", "content": "In our experiments, we fine-tune our EQA models\nby conducting additional training on SQUAD 2.0\n(Rajpurkar et al., 2018) (for Sections 6 or 7) and\nSQUAD AGent (Tran et al., 2023a) (for Section\n7). While both datasets share the same answer-\nable questions, SQUAD 2.0 includes adversarially\nwritten unanswerable questions, whereas SQUAD\nAGent utilizes information-seeking unanswerable\nquestions.\nWe test the performance of our models on\n\u2022 SQUAD 2.0: We test our models on both an-\nswerable (has-ans) and unanswerable (no-\nans) questions of this dataset. The unanswer-\nable questions in SQUAD 2.0 are adversarially\nwritten.\n\u2022 SQUAD AGent: We only test models on unan-\nswerable questions (AGent) of this dataset be-\ncause the answerable questions in this dataset\nare the same as ones in SQUAD 2.0. The\nunanswerable questions from this dataset are\ninformation-seeking.\n\u2022 ACE-whQA (Sulem et al., 2021): We test\nmodels on answerable (has-ans) questions\nand two types of unanswerable questions:\ncompetitive (no-ans competitive), where the\npassage contains an entity of the same type\nas the expected answer, and non-competitive\n(no-ans non-com), where the passage does not\ncontain any entity of the same type as the ex-\npected answer.\nThe diversity of testing domains enables us to mea-\nsure the robustness of models against distribution\nshifts, which occur when encountering testing data\nthat differs from the training data."}, {"title": "4 Adversarial Attacks", "content": "In addition to evaluating models' robustness\nagainst distribution shift, we also measure the ro-\nbustness against adversarial attacks."}, {"title": "4.1 Robustness Evaluation", "content": "An attack algorithm A transforms triplets (q, c, \u0430)\nin D into adversarial test samples (q', c', a') in the\nadversarial test set $D^{attacked}$ where c', q', and a'\nare the modified (attacked) versions of c, q, and a.\nThe robustness of a model is then computed as the\ndifference between the performance of the model\non the original test set vs attacked test set:\n$\\Delta^{A} = Per(f, D) - Per(f, D_{attacked})$\nWhen there are more than one attack algorithm,\nwe measure the overall robustness by\n$\\Delta = \\frac{1}{|T|} \\sum_{A \\in T} \\Delta^{A}$\nwhere T is the set of all tested types of adversarial\nattacks."}, {"title": "4.2 Algorithms for Attack Construction", "content": "In this paper, we test the experimented models on\ntwo types of adversarial attacks."}, {"title": "4.2.1 AddOneSent Attacks", "content": "Table 1 gives an example of AddOneSent (AOS)\nattack (Jia and Liang, 2017). The AddOneSent\nattack strategy creates the attack sentence from\na modified question and a fake answer. To con-\nstruct the modified question, nouns and adjectives\nin the original question are substituted with their\nantonyms sourced from WordNet (Fellbaum, 1998).\nMeanwhile, the fake answer is nearest word to the\noriginal gold answer in the vector space of GloVe\n(Pennington et al., 2014)."}, {"title": "4.2.2 Negation Attacks", "content": "The Negation Attack, shown in Table 1, is designed\nto mislead models into giving incorrect \"empty\"\npredictions. This method involves the crafting of\nan attack statement that has significant lexical over-\nlap with the original question yet is easy to identify\nas contradictory by simply inserting \u201cnot\u201d in front\nof the first adjective within the question. The fake\nanswer is created similarly to the AddOneSent at-\ntack.\nThe questions and answers are unchanged in\nboth types of attacks (q' = q and a' = a)."}, {"title": "5 Extractive Question Answering Loss Functions", "content": "EQA models are typically fed a question q and a\ncontext c as input. State-of-the-art EQA models,\noften employing BERT-style language models at\ntheir core, process q and c together as a sequence\ninput < [CLS]q[SEP]c>, with [CLS] and [SEP]\nas special tokens of pre-trained tokenizer accompa-nying the pre-trained model.\nGiven an input sequence (pair of question-context) with n tokens seq = ($t_1, t_2, ..., t_n$), we\nhave\nM(seq) = ($v_1, v_2, ..., v_n$)\nwhere M is a pre-trained language model that takes\nsequence seq as the input and output n contextu-alized vectors ($v_1, v_2, ..., v_n$), each corresponds to"}, {"title": "5.1 Default Loss Function", "content": "Devlin et al. (2019) use the Cross Entropy loss\nfunction for training BERT on SQUAD 2.0.\n$L_{Default} = -\\sum_{k=1}^n y_k^s log \\frac{exp(s_k)}{\\sum_{i=1}^n exp(s_i)} - \\sum_{k=1}^n y_k^e log \\frac{exp(e_k)}{\\sum_{i=1}^n exp(e_i)}$\nwhere $y_k^s$ and $y_k^e$ are the labels of whether kth token\nin the input sequence is the start or end of a gold\nanswer identified by human annotators. Unanswer-\nable questions are treated as having an answer span\nwith start and end at the [CLS] token, which means\n$y_k^s$ and $y_k^e$ are 1s.\nAs of the time of writing this paper, the training\nmethodology utilizing this particular loss function\nremains widely adopted in most EQA models. We\nterm this training methodology the \"default\" ap-\nproach."}, {"title": "5.2 Our Loss Function", "content": "This component ($L_{QA}$) of the newly proposed loss\nfunction is similar to the Cross Entropy loss func-\ntion used in work by Devlin et al. (2019). However,\na key difference lies in how we handle unanswer-\nable sequences. In our approach, since we treat all\ntokens in these sequences as equally unlikely to be\nthe start or end of an answer, all tokens within an\nunanswerable sequence are assigned the same label\nuniformly, represented as $y_k^s = y_k^e = \\frac{1}{n}$, where n\ndenotes the sequence length.\nNote that setting these all labels to 0 would re-\nsult in no backpropagation signal for unanswerable\nsequences. By using a ground truth of $\\frac{1}{n}$ for n to-\nkens, the sum of these values equals 1, which is\nan appropriate scale for the output of the softmax\nfunction of the Cross Entropy loss."}, {"title": "Sequence Tagging Loss", "content": "We enable our models to naturally signal \"unan-\nswerable\" predictions by using an inference\npipeline that outputs an \u201cempty\u201d prediction if the\nmaximum span score of $s_i + e_j$ is negative. To en-\nable models to output negative $s_i + e_j$ scores for all\nspans in unanswerable sequences, we incorporate\nsequence tagging loss alongside the standard QA\nloss:\n$L_{Tag} = -\\sum_{k=1}^n (y_k^s log \\sigma(s_k) + (1 - y_k^s) log(1 - \\sigma(s_k)) - \\sum_{k=1}^n (y_k^e log \\sigma(e_k) + (1 - y_k^e) log(1 - \\sigma(e_k))$\nwhere $\\sigma(x) = \\frac{1}{1+exp(-x)}$, the labels for the gold\nstart tokens are assigned $y = 1$, and labels for all\nother tokens are set to $y = 0$. This logic extends\nto the labels for end tokens. Consequently, all $y_k^s$\nand $y_k^e$ in unanswerable sequences are zeros."}, {"title": "Overall Loss", "content": "$L_{Ours} = \\lambda_{QA} \\cdot L_{QA} + \\lambda_{Tag} \\cdot L_{Tag}$\nwhere $\\lambda_{QA}$ and $\\lambda_{Tag}$ denote weights for their cor-responding losses. In this paper, we set $\\lambda_{QA} = 2$\nand $\\lambda_{Tag} = 1$. Appendix A discusses the selection\nof these weights in more detail."}, {"title": "5.3 Inference Pipeline", "content": "In both model types, the score for a candidate span\nranging from position i to position j is given by\n$s_i + e_j$, The span with the highest score, where\nj > i, is selected for prediction.\nModels trained with the default training loss\nfunction indicate an unanswerable question by out-\nputting an \"empty\" string when the highest scoring\nspan is (0,0), which corresponds to the [CLS] to-\nken.\nConversely, models trained with our method in-\ndicate an \"empty\" string response when the maxi-\nmum span score of $s_i + e_j$ is negative."}, {"title": "6 Experiments", "content": "In the experiments in this section, we train our\nmodels using the SQUAD 2.0 dataset. For models\ntrained with the default loss function, the origi-\nnal SQUAD 2.0 dataset is used without modifica-\ntions. However, for models trained using our pro-\nposed method in this section, we introduce mod-\nifications to the SQUAD 2.0 dataset to eliminate\nthe single-answer assumption during the training"}, {"title": "6.1 Experiment Design", "content": "phase. We augment approximately 20% of the an-\nswerable questions in the original dataset with an\nadditional \"synthetic\" answer, resulting in these\nquestions having two answers. In Appendix B, we\nprovide a detailed information on how we generate\n\"synthetic\" answers, along with our experiments\non the risks of hallucinations when training EQA\nmodels using these synthetic answers."}, {"title": "6.2 Results", "content": "Table 2 shows performances of models trained on\ndefault and our training methods. Firstly, models\ntrained with our method (new loss function and\nadditional synthetic answers) achieve almost the\nsame performance as those trained using default\napproach on SQuAD 2.0, the in-domain testing\nset. Specifically, models trained with the default\nloss function achieve an average F1 score of 79.7\n(across both answerable and unanswerable ques-tions 83.3+76.1) on SQuAD 2.0, while our models\nachieve an average F1 score of 79.8.\nOn the other hand, our models consistently out-perform default model on out-of-domain unan-swerable questions, including those from SQUAD\nAGent and both competitive and noncompetitive\nunanswerable questions from ACE-whQA. On\ninformation-seeking unanswerable questions from\nSQUAD AGent, our models outperform default\nmodels by a large margin of 18.4 F1 score on av-erage. Furthermore, on the unanswerable ques-tions in ACE-whQA, our models outperform de-fault ones by 17.5 F1 for noncompetitive unanswer-able questions and 14.2 F1 for competitive ones.\nThis enhanced robustness against distribution shifts\nenables our models to attain a higher overall per-formance of 71.2, compared to the 65.5 achieved\nby default models across all evaluated answerable\nand unanswerable questions.\nWe then analyze the performance gap of each\nmodel on unanswerable questions between SQUAD\n2.0 and SQUAD AGent over three training epochs.\nFigure 1 presents the dynamics of this performance\ngap for ROBERTa models trained with the default\nmethod and our proposed method on SQUAD 2.0.\nNotably, models using the default loss function\nexhibit an increasing performance gap through-out the training process. This indicates that as\nmodels better perform on adversarial unanswerable\nquestions within SQuAD 2.0, their performance\non information-seeking unanswerable questions in\nSQUAD AGent decreases significantly. Conversely,models trained with our proposed loss function\ndemonstrate a stable robustness against such shifts\nacross three training epochs."}, {"title": "7 Further Analysis", "content": "To evaluate the effectiveness of our proposed train-\ning method under different scenarios, we design\ntwo experiments."}, {"title": "7.1 Experiment Design", "content": "1. We train models on SQuAD 2.0 using our\nproposed loss function without introducing\n\"synthetic\" answers. We then compare these\nmodels (referred to as \u201cno synthetic\") with\nthose trained using the default loss function,\nalso trained on SQUAD 2.0. This experiment\nis designed to study the independent contri-butions of the newly proposed loss function\nand the augmented \u201csynthetic\" answers to the\nrobustness of our models.\n2. We train models on the information-seeking,\nunanswerable question dataset SQUAD AGent\nusing our proposed training method (includ-\ning new loss function and \"synthetic\" an-swers). We then compare these models with\nthose trained using the default method, also\ntrained on SQUAD AGent. This experiment\ninvestigates the effectiveness of our proposed\nmethod on datasets with information-seeking\nunanswerable questions."}, {"title": "7.2 Robustness against Distribution Shift", "content": "We now evaluate the performance of models\ntrained on SQuAD 2.0 using our proposed loss\nfunction, while excluding synthetic answers. The\nexperimental results, in Table 4, highlight that even\nin the absence of synthetic answers, our models\nbetter generalize to information-seeking unanswer-able questions. The \u201cNo synthetic\" outperform de-fault models by a large margin of 18.4 on F1 when\ntested on AGent unanswerable questions. This find-"}, {"title": "7.3 Robustness against Adversarial Attacks", "content": "While models trained with our method on\nSQUAD AGent do not exhibit improved robust-ness against distribution shifts to SQUAD 2.0, they\ndemonstrate significant improveme nts when en-countering adversarial attacks.\nThe experimental results in Table 6 show that\nwhen using SQUAD AGent as the training set, mod-els trained with default approach exhibit a signif-icant reduction in performance of 30.0 F1 points.\nConversely, models trained with our method (new\nloss function and the synthetic answers) experi-ence a much smaller performance drop of 16.2 F1\npoints. Our findings conclusively demonstrate that\nour training method notably enhances the robust-ness of models trained on both SQUAD 2.0 and\nSQUAD AGent against adversarial attacks.\nWith this significant improvement established,\nwe then shift our focus to identifying the primary\nfactor behind this increased robustness. We hy-pothesize that our models' robustness against ad-versarial attacks might be mainly thanks to the aug-mented \"synthetic\" answers, which eliminate the\nsingle-answer assumption in the SQUAD dataset.\nTherefore, we examine the robustness against ad-versarial attacks of \u201cno synthetic\" models trained\non SQUAD 2.0 using our proposed loss function,\nwhile omitting synthetic answers. The experimen-tal results, in Table 7, indicate that without the\nsynthetic answers, our models are no longer robust\nagainst adversarial attacks. The performance gap\n$\\Delta$ of our models without synthetic answers is even\nhigher than that of default models (41.3 compared\nto 40.7). This finding strongly supports our hypoth-esis that the inclusion of \u201csynthetic\" answers in our\ntraining method is a key factor in the improved ro-bustness against adversarial attacks of our models.\nIn Appendix B, we further validate this hypothe-sis by training models on SQuAD 1.1 (Rajpurkar"}, {"title": "8 Conclusion", "content": "In this paper, we introduce a novel training method-ology for EQA models aimed at enhancing their ro-bustness against distribution shifts and adversarial\nattacks. Our new training method is characterized\nby a novel training loss for the EQA problem, as\nwell as challenging the single-answer assumption\nby creating a new \u201csynthetic\u201d answer span in a\nnumber of answerable questions. Our experimen-tal findings demonstrate that models trained using\nour approach exhibit significant improvement on\nout-of-domain testing datasets. Furthermore, the\nrobustness of these models against two tested types\nadversarial attacks is also significantly better than\nthat of the default models.\nIn Section 7, we study the independent con-tributions of our new loss function and the aug-mented \u201csynthetic\u201d answers to the robustness of\nour models. Our analysis reveals that the new\nloss function specifically benefits the performance\non information-seeking unanswerable questions.\nThis improved performance of information-seeking\nunanswerable questions contribute to the robust-ness against distribution shifts of models trained\non SQUAD 2.0 with our method.\nOn the other hand, our training method chal-lenges the single-answer assumption of many exist-ing EQA datasets by creating \u201csynthetic\u201d answers\nfor a number of answerable questions. Our ex-periments indicate that these \u201csynthetic\u201d answers\nsignificantly contribute to the robustness of models\ntrained with our method on both SQUAD 2.0 and\nSQUAD AGent against adversarial attacks. This\nfinding strongly corroborates our initial hypothe-sis, suggesting that the longstanding single-answer\nassumption of many EQA training datasets is a\nlearning shortcut for models that can significantly\ncompromise their robustness. We believe this work\nhighlights the importance of future Question An-swering datasets that incorporate the possibility of\nmultiple, non-contiguous answer spans, similar to\nthe MultiSpanQA dataset (Li et al., 2022)."}, {"title": "Limitations", "content": "We acknowledge certain limitations in our work.Our study primarily focuses on evaluating the pro-posed training methodology using multiple pre-trained transformers-based models in English. Thisdoes not guarantee that our method will maintainits effectiveness when applied to other languages."}, {"title": "A Derivation on Unanswerable Sequence", "content": "Let us consider the $k^{th}$ token in an unanswerable\nsequence. Our objective is to ensure that the logit\n$s_k$ generally decreases if $s_k \\geq 0$ after each training\nbatch. To achieve this, we need the partial deriva-\ntive of $L_{Ours}$ with respect to the start score $s_k$ of\nthe $k^{th}$ token, i.e. $\\frac{\\partial L_{Tag}}{\\partial s_k} + \\lambda \\frac{\\partial L_{QA}}{\\partial s_k}$, remains\npositive whenever $s_k \\geq 0$.\nIt is established that the partial derivative of\nthe tagging loss $L_{Tag}$ with respect to the score\n$s_k, \\frac{\\partial L_{Tag}}{\\partial s_k}$, is positive. Nonetheless, there is no\nassurance that the partial derivative of the question-answering loss $L_{QA}$ with respect to $s_k, \\frac{\\partial L_{QA}}{\\partial s_k}$ will\nalso be positive.\nFirstly, we assume that both Tagging weight\n$\\lambda_{Tag}$ and Question Answering weight $\\lambda_{QA}$ are positive. We then have that\n$\\frac{\\partial L_{Tag}}{\\partial s_k}$\n$\\frac{\\partial L_{QA}}{\\partial s_k} = \\frac{\\lambda_{Tag} \\frac{d}{ds_k} log[\\frac{1}{1 + exp(-s_k)}]}{- \\lambda_{Tag}} = \\frac{ -\\frac{d}{ds_k} [ \\frac{1}{1 + exp(-s_k)} ]}{\\frac{1}{1 + exp(-s_k)} = -\\lambda_{Tag} \\frac{ - exp(-s_k)}{(1 + exp(-s_k))^2}} = \\lambda_{Tag} \\frac{ exp(-s_k)}{1 + exp(-s_k)} \\frac{1}{1 + exp(-s_k)} = \\lambda_{Tag} \\frac{ exp(s_k)}{(1 + exp(-s_k))^2(1 + exp(-s_k))^2} =  \\lambda_{QA} \\frac{  \\frac{exp(s_k)}{1 + exp(-s_k)} }{ - \\lambda_{Tag}}$\nTherefore, we can derive that\n$\\frac{\\frac{\\partial L_{Tag}}{\\partial s_k}}{\\lambda_{Tag}} > \\frac{\\frac{\\partial L_{QA}}{\\partial s_k}}{\\lambda_{QA}} \\Rightarrow \\lambda_{Tag} \\frac{ exp(s_k)}{(1 + exp(-s_k))^2} > \\lambda_{QA} (\\frac{1}{n} \\frac{\\frac{exp(s_k)}{1}}{\\sum_i^n exp(s_i)}) \\Leftrightarrow \\frac{\\lambda_{Tag}}{\\lambda_{QA}} > (\\frac{2}{n})$\nConsequently, the partial derivative of the overall\nloss ($L_{Ours}$) with respect to the score $s_k, \\frac{\\partial L_{Ours}}{\\partial s_k}$\nwill be positive whenever $s_k \\geq 0$ if the ratio of\n$\\frac{\\lambda_{Tag}}{\\lambda_{QA}} > (\\frac{2}{n})$. In our experiments, the number of\ntokens in a question-context sequence is set to n =\n384. We set $\\lambda_{Tag} = 1$ and $\\lambda_{QA} = 2$. Therefore,\n$\\frac{\\lambda_{QA}}{\\lambda_{Tag}} < \\frac{384}{2}$"}, {"title": "B Synthetic Answers", "content": "Table 8 illustrates the incorporation of Synthetic\nanswers into the context of 20% of the answerable\nquestions within the training set, serving as an ex-\nample of our augmentation approach.\nIncorporating \u201csynthetic\u201d answers into contexts\nof answerable questions involves three steps:\n1. Creating fake answers that differ from the\nground truth answers annotated by human\ncrowdsource workers.\n(a) We re-match each answerable question\nwith 10 new contexts.\n(b) We train 10 models on SQUAD 2.0\nand obtain their predictions on the re-matched question-context pairs.\n(c) For each answerable question, we extract\nthe answer span that is most frequently\npredicted by the models.\nIn this step, we ensure that the extracted spans\nare different from the corresponding ground\ntruth answers, with F1 score lower than 0.2.\nThrough this method, we can extract relevant\nand plausible answers that can serve as \u201csyn-thetic\" answers for the corresponding ques-tions.\n2. Given the fake answer and the original ques-tion, we use ChatGPT-turbo3.5 to convert\nthem into a natural statement. We use the"}, {"title": "B.1 Generate Synthetic Answers", "content": "prompt:\nGiven the question and its answer", "statement": "nExample:\n<example1>\n<example2>\nQuestion: <question>\nAnswer: <answer>\nStatement: ...\n3. We then insert the newly created statement\ninto the original context at a random posi-\ntion between existing sentences. We utilize"}]}