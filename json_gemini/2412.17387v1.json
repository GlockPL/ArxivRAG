{"title": "Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement", "authors": ["Hyeonjin Kim", "Jaejun Yoo"], "abstract": "While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs.", "sections": [{"title": "Introduction", "content": "Generative models like Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) and Diffusion models (Ho, Jain, and Abbeel 2020) have achieved remarkable performance across various computer vision tasks such as image generation (Karras et al. 2020; Ho, Jain, and Abbeel 2020; Wang et al. 2022), image editing (Baykal et al. 2023; Pehlivan, Dalva, and Dundar 2023; Kawar et al. 2023; Zhang et al. 2023), even video generation (Skorokhodov, Tulyakov, and Elhoseiny 2022; Ho et al. 2022; Blattmann et al. 2023) and 3D generation (Chan et al. 2022; Karnewar et al. 2023). The impressive performance of these generative models, however, often comes at the cost of high memory and computational demands, limiting their real-world applicability.\nTo address these issues, several model compression technique for generative models have been proposed (Liu et al. 2021; Xu et al. 2022; Chung et al. 2024; Fang, Ma, and Wang 2023).\nModel compression typically involves two steps: 1) pruning to reduce model size while retaining essential pre-trained knowledge, and 2) fine-tuning to restore performance. Among these, effective pruning has received significant attention for its ability to enhance following fine-tuning step. The retained pre-trained knowledge from pruning can enhance fine-tuning, leading to improved performance and faster convergence without additional training costs. While previous pruning methods (Chung et al. 2024; Fang, Ma, and Wang 2023) effectively maintain model performance, they often focus solely on preserving crucial connections in the pre-trained model, overlooking the impact of pruned weights on subsequent process, leading to inefficient fine-tuning or distillation. This issue becomes more severe as the model's capacity decreases, and in some cases, the pruned weights results in worse performance compared to random initialization; i.e. slow convergence speed and lower performance. Therefore, addressing these factors is essential for achieving more efficient generative models.\nOur key observation is that pruned weights often exhibit dominant singular vectors, which results in a large disparity between the largest and smallest singular values. The presence of such dominant singular vectors significantly impacts the model's forward propagation, overshadowing the contributions of minor singular vectors, and the overall fine-tuning process is dominated by these few dominant singular vectors. We find that this could limit the exploration of diverse weight space. Therefore, ensuring a balanced contribution among the singular vectors within pruned weights at initialization may offer a potential solution to the inefficiencies observed in the fine-tuning process.\nIn this paper, we introduce a simple yet effective refinement technique called Singular Value Scaling (SVS) to enhance the efficiency of fine-tuning pruned weights. The dominant singular values is scaled down to reduce the disparity between the largest singular values and smallest singular values while preserving the relative order of the singular values at initialization. In this way, the refined pruned weights makes the fine-tuning process easier compared to directly using the pruned weights by ensuring all singular vectors contribute more evenly at the beginning of fine-tuning. Note that since our method focuses on the knowledge within each weight and is independent of specific architectures, it can be applied to both GANs and diffusion models.\nWe conduct extensive experiments with representative generative model architectures across various datasets, including StyleGAN2, StyleGAN3, and Denoising Diffusion Probabilistic Model (DDPM) on CIFAR10, CelebA-HQ, FFHQ, and LSUN-Church. The results demonstrate that our method enhances the fine-tuning process, leading to both faster convergence and improved solutions for the compressed model without additional training cost. Our contributions can be summarized as follows:\n\u2022 We propose a simple yet effective method that enhances the efficacy of fine-tuning process of pruned generative models without additional training cost.\n\u2022 By simply scaling down the singular values of pruned weights at initialization, our method, Singular Value Scaling (SVS) enables the compressed model to converge faster and achieve superior performance than using the existing baseline methods.\n\u2022 We are the first to provide a general method for generative model compression that can be applied to both model types of GANs and Diffusion models."}, {"title": "Related Works", "content": "StyleGAN Compression\nRecently, several studies (Liu et al. 2021; Xu et al. 2022; Chung et al. 2024) have been proposed for compressing unconditional GANs, particularly the StyleGAN family (Karras et al. 2020, 2021), which are the state-of-the-art models in the area. CAGAN (Liu et al. 2021) introduced the first framework for pruning pre-trained StyleGAN models and fine-tuning them via pixel-level and feature-level knowledge distillation. Following this, StyleKD (Xu et al. 2022) further enhanced the fine-tuning process by incorporating a specialized relation loss tailored for StyleGAN. In particular, CAGAN introduced a content-aware pruning technique that preserves connections in the pre-trained model crucial for the semantic part of generated images, while StyleKD tackled output discrepancies problem by inheriting only the pre-trained model's mapping network and randomly initializing the synthesis network with smaller size. More recently, DCP-GAN (Chung et al. 2024) proposed a diversity-aware channel pruning technique, which preserves connections in the synthesis network that contribute to sample diversity, significantly enhancing both the diversity of generated images and the training speed.\nDiffusion Model Compression\nDiffusion models have garnered significant attention for their stable training and impressive generative capabilities. However, their performance incurs high computational costs due to iterative sampling. To enhance efficiency, various sampling techniques have been developed to reduce the number of required iterations (Song, Meng, and Ermon 2021; Lu et al. 2022; Zheng et al. 2023). Orthogonal to the these sampling techniques, Diff-Prune (Fang, Ma, and Wang 2023) introduced a seminal work for reducing computational costs by compressing Diffusion models. This demonstrated that by primarily pruning connections involved in less important diffusion steps, a reduced-size Diffusion model with minimal performance loss can be achieved in significantly fewer training iterations compared to the original-sized model."}, {"title": "Weight Initialization and Trainability", "content": "Weight initialization is essential for efficient training in deep learning. Glorot (Glorot and Bengio 2010) and He initialization (He et al. 2015) are widely used to maintain activation variance and ensure stable optimization, particularly in feed-forward networks with ReLU activations. Orthogonal initialization (Saxe, McClelland, and Ganguli 2014), which ensures that all singular values of weights are 1, achieves dynamical isometry, the ideal state for trainability. In classifier pruning, TPP (Wang and Fu 2023) recently highlighted the importance of preserving trainability in pruned networks to support effective performance recovery."}, {"title": "Analysis", "content": "suboptimal Results with Pruned Weights\nPrevious works in the field of generative model compression (Chung et al. 2024; Fang, Ma, and Wang 2023) have shown that well-designed pruning techniques can lead compressed models to better solutions compared to their randomly initialized counterparts, while reaching the same performance more quickly (\u00d72.5 for StyleGAN2, \u00d78.0 for DDPM). This is because pruned weights provide the compressed model with an initialization state that preserves the pre-trained model's performance, starting with better performance compared to the random counterpart. This initial gain continues throughout the fine-tuning process, ultimately resulting in a better-performing compressed model. However, we observe that as fine-tuning progresses, models initialized with pruned weights converge more slowly compared to those initialized randomly (see Figure 2: orange line (DCP-GAN) vs. blue line (StyleKD)). This issue worsens as the model size decreases, resulting in suboptimal solutions, which indicates that the pruned weights themselves contain factors that hinder fine-tuning efficiency.\nAnalyzing Pruned Weights with SVD\nWe analyze the learned prior of pruned weights inherited from the pre-trained model by employing Singular Value Decomposition (SVD), which is widely adopted for low-rank approximation by identifying important basis from the weights (Denton et al. 2014; Zhang et al. 2015; Girshick 2015; Yoo et al. 2019). For a weight matrix  W \u2208 \\mathbb{R}^{m \\times n} (m < n) , we can decompose it using SVD:\nW = U \\Sigma V^T = \\sum_{i=1}^{m} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\nwhere  U = [\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_m] \u2208 \\mathbb{R}^{m \\times m} and  V = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n] \u2208 \\mathbb{R}^{n \\times n} are orthogonal matrices and  \\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_m) \u2208 \\mathbb{R}^{m \\times n} is a diagonal matrix with singular values on the diagonal. Here, for a fully connected layer, W \u2208 \\mathbb{R}^{C_{out} \\times C_{in}} . For a convolutional layer, W \u2208 \\mathbb{R}^{C_{out} \\times (C_{in} \\times k \\times k)} , where k is the kernel size. For a convolutional layer, we consider the weight is flattened. Each singular value represents the influence of its corresponding singular vector within the weight. The most notable observation is the large gap between the largest and smallest singular values (\u223c \u00d7100). This implies that the forward and backward propagations of the weights are heavily influenced by these dominant singular vectors. This can potentially bias the compressed model towards these singular vectors during training, severely limiting diverse exploration in the weight space (Saxe, McClelland, and Ganguli 2014; Wang and Fu 2023)."}, {"title": "Method", "content": "Scaling Singular Values of Pruned Weights\nBased on our observation, we propose \"Singular Value Scaling (SVS)\", to refine pruned weights to enhance fine-tuning efficiency. Our primary goal is to reduce the gap between the singular values of pruned weights. In the pruned weights, dominant singular vectors tend to have significantly larger singular values compared to smaller ones, and this gap increases as the values grow. Since all bases in pruned weights contain important knowledge from the pre-trained model, we prevent any single basis from dominating to let these bases contribute equally at the beginning of training. To achieve this, we simply scale the singular values using the \"square root function\",\nW_{\\text{scaled}} = U \\Sigma_{\\text{scaled}} V^T = \\sum_{i=1}^{m} \\sqrt{\\sigma_i} \\mathbf{u}_i \\mathbf{v}_i^T.\nHere, U and V remain unchanged, and  \\Sigma_{\\text{scaled}} = \\text{diag}(\\sqrt{\\sigma_1}, \\sqrt{\\sigma_2}, \\dots, \\sqrt{\\sigma_m}) \u2208 \\mathbb{R}^{m \\times n} . Square root function has several good properties: 1) Since singular values are non-negative, it maps non-negative values to non-negative values, 2) For  \\sigma_i < 1 , it increases the value, while for  \\sigma_i > 1 , it decreases the value more as the value grows. 3) In the positive domain, it is monotonically increasing, thereby maintaining the relative order of singular values. By scaling the singular values of pruned weights in this manner, we can preserve the original bases while balancing their relative contributions. This balanced contribution helps the compressed model fully leverage the pre-trained model's knowledge, facilitating a more effective path to the optimal solution. Alternative functions, such as  \\log(x + 1) or  |\\log(x)| , could also achieve similar effects. We provide ablation study on these functions in the Experiments section.\nScaling Bias with respect to the Scaled Weights\nSince biases and weights are learned simultaneously, we need to consider the impact of scaling the singular values of the weights on the corresponding biases. Let us consider the following linear equation, y = Wx + b , where  W \u2208 \\mathbb{R}^{m \\times n} is the weights,  x \u2208 \\mathbb{R}^n is the input and  b \u2208 \\mathbb{R}^m is the bias. We can factor the equation with respect to W as follows:\ny = W (x + W^\\dagger b) = W (x + V \\Sigma^{-1} U^T b)\nwhere W^\\dagger \u2208 \\mathbb{R}^{n \\times m} is the Moore-Penrose inverse (Petersen 2012) and  \\Sigma^{-1} = \\text{diag}(\\sigma_1^\\dagger, \\sigma_2^\\dagger, \\dots, \\sigma_m^\\dagger) \u2208 \\mathbb{R}^{n \\times m} with\n\\sigma_i^\\dagger = \\begin{cases}\n\\frac{1}{\\sigma_i}, & \\text{if } \\sigma_i \\neq 0 \\\\\n0, & \\text{if } \\sigma_i = 0\n\\end{cases}\nBy representing the bias b as b = |b| \\beta , where |\u00b7| is the vector norm and  \\beta is the normalized vector of b, it becomes:\ny = W (x + V \\Sigma^{-1} U^T b) = W (x + V (|b| \\Sigma^{-1}) U^T \\beta)\nwhere  |b| \\Sigma^{-1} = \\text{diag}(|b| \\sigma_1^\\dagger, |b| \\sigma_2^\\dagger, \\dots, |b| \\sigma_m^\\dagger) . Thus, for  \\sigma_i > 0 , when scaling the biases, instead of only scaling  \\sigma_i , we must also scale b: b_{\\text{scaled}} = \\frac{b}{\\sqrt{|b|}} , which leads to:\n\\frac{|b_{\\text{scaled}}|}{\\sigma_{i,\\text{scaled}}} = \\sqrt{|b|} / \\sqrt{\\sigma_i}."}, {"title": "Experiments", "content": "Experimental Setup\nBaselines. To evaluate our method, we conducted experiments on StyleGAN2, which is the most extensively studied model in the field of generative model compression. Additionally, to validate our method on generative models with different architectures, we conducted experiments on StyleGAN3 and the Denoising Diffusion Probabilistic Model (DDPM), which, along with StyleGAN2, are representative models in the field of generative modeling.\nImplementation details. For the StyleGAN2 compression, we re-implement previous StyleGAN2 compression methods, which were implemented on the unofficial StyleGAN2 implementation, on the official StyleGAN2 implementation because the official implementation provides more optimized StyleGAN2 training settings and are easy to use. we apply our method to the weights pruned by DCP-GAN (Chung et al. 2024), which is the state-of-the-art pruning method in StyleGAN2 compression. We mainly compare our method against StyleKD (Xu et al. 2022) and DCP-GAN because they are the methods that demonstrate the best performance in StyleGAN2 compression. We use two StyleGAN2 architectures: the optimized architecture provided by the official implementation (denoted as \"StyleGAN2 (small)\") and the original structure used in previous works (denoted as \"StyleGAN2 (base)\"). Experiments are conducted on the FFHQ (Karras, Laine, and Aila 2019) and LSUN Church (Yu et al. 2015) datasets. Following DCP-GAN, we use a facial mask as the content mask for the FFHQ dataset and a uniform mask (all pixel values are assigned values 1) for LSUN Church dataset. For StyleGAN3 compression, we implement our method based on the DCP-GAN implementation. Similar to StyleGAN2 compression, we apply our method to the pruned weights using the DCP-GAN method for our experiments. We follow the experimental setup of DCP-GAN and train StyleGAN3 on the FFHQ dataset until the discriminator see 10 million images for both DCP-GAN and our method. For DDPM compression, we build upon Diff-Prune (Fang, Ma, and Wang 2023), a foundational work in Diffusion model pruning. We train pruned DDPMs using the CIFAR10 (Alex 2009) and CelebA-HQ (Liu et al. 2015) datasets, following the experimental setup of Diff-Prune. Unlike Diff-Prune, which tested only on a 30% channel sparsity, we explore more extreme levels of sparsity in our experiments, including 50% and 70% channel sparsity. We provide more detailed implementation in the supplementary material.\nEvaluation metrics. For StyleGAN compression, we evaluate our method using the Fr\u00e9chet Inception Distance (FID) (Heusel et al. 2017) and Precision and Recall (P&R) (Kynk\u00e4\u00e4nniemi et al. 2019). We also measure Density and Coverage (D&C) (Naeem et al. 2020), which are more robust to outliers than Precision and Recall. Here, Precision and Density evaluate the fidelity of generated samples, while Recall and Coverage assess their diversity. To calculate FID, we use all real samples from each dataset and 50K fake samples. For P&R and D&C calculation, we use 50K fake samples and all real samples from FFHQ, and 50K real samples from LSUN Church due to computational costs. For DDPM compression, in addition to FID, we employ Structural Similarity (SSIM) (Wang et al. 2004) to evaluate consistency with the pre-trained model, following the Diff-Prune. The SSIM score is measured between images generated by the pre-trained model and the compressed model, given the identical noise inputs. We use a 100-step DDIM sampler (Song, Meng, and Ermon 2021) for sampling.\nAnalysis on Convergence Speed\nFigure 2 visualizes the FID convergence graph for two different StyleGAN2 architectures (StyleGAN2 (base) and StyleGAN2 (small)) in the FFHQ dataset. As shown in Figure 2 (a) (StyleGAN2 (base)), although DCP-GAN converges to better performance than StyleKD, the FID convergence speed of DCP-GAN noticeably slows down in the later stages of training. This issue becomes even more severe as the model's capacity decreases. In Figure 2 (b), this slowdown allows StyleKD to surpass DCP-GAN early in fine-tuning, leading DCP-GAN to a suboptimal point. These results indicate that fine-tuning a model naively initialized with the pruned weights can lead to significantly poor outcomes, depending on the model's capacity. In contrast, when pruned weights are refined using SVS, the model is much easier to fine-tune. It not only fits faster across most intervals but also converges to a significantly better solution compared to previous methods."}, {"title": "Implementation Details", "content": "StyleGAN2 We first prune the pre-trained model using the DCP-GAN (Chung et al. 2024) method with 70% channel sparsity. We then refine the pruned weights with our method. Basically, we follow the training scheme of the DCP-GAN, which shares same training scheme with StyleKD (Xu et al. 2022). In detail, we employ non-saturating loss, LGAN (Goodfellow et al. 2014) with R1 regularization loss (Mescheder, Geiger, and Nowozin 2018; Karras, Laine, and Aila 2019), optimizing the model with Adam optimizer (Kingma and Ba 2015) with \\beta_{1} = 0.0 and \\beta_{2} = 0.99. The the batch size and learning rate are set to 32 and 2e-3 for the StyleGAN2 (base) architecture, while set to 64 and 2.5e-3 for the StyeGAN2 (small) following official Style-GAN2 implementation. For all experiments, we consistently employ 4 GPUs to train the models. For the knowledge distillation, we adopt L_{rgb} (pixel loss), L_{lpips} (LPIPS loss (Zhang et al. 2018)), and L_{LD} (latent-direction-based distillation loss (Xu et al. 2022)), with the hyperparameters, \\lambda_{{AGAN}} = 1, \\lambda_{{rgb}} = 3, \\lambda_{{lpips}} = 3, \\lambda_{{LD}} = 30 respectively. To match the training iterations with previous studies, we train the models until the discriminator sees 15000K images for StyleGAN2 (base), which corresponds to approximately 470,000 training iterations. For StyleGAN2 (small), we train the models until the discriminator sees 25000K images, which corresponds to approximately 390,000 training iterations.\nStyleGAN3 For StyleGAN3 compression, we implement our method based on the StyleGAN3 compression implementation from DCP-GAN (Chung et al. 2024). Similar to StyleGAN2 compression scheme, we first prune the pre-trained model using DCP-GAN method with 70% channel sparsity and refine the pruned weights with our method. The same training losses and hyperparameters used for StyleGAN2 compression are employed here as well. The model is optimized with Adam optimizer with \\beta_{1} = 0.0 and \\beta_{2} = 0.99. The batch size is set to 16 following DCP-GAN, and the learning rate is set to 2.5e-3 for the generator and set to 2e-3 for the discriminator. We change the layer outputs used in L_{LD} from {layer-1, layer_2} (used in the DCP-GAN implementation) to {layer_2, layer_4, layer_6, layer_9} to match the number of layers used in L_{LD} with StyleGAN2 compression.\nDDPM For Diffusion model compression, we first prune the pre-trained model using Diff-Prune (Fang, Ma, and Wang 2023) method with varying pruning ratios and then refine the pruned weights with our method. And then, the pruned models are trained with noise prediction loss, proposed in DDPM (Ho, Jain, and Abbeel 2020). The batch size and learning rate are set to 128, 2e-4 for CIFAR10, 96, 2e-4 for CelebA and 64, optimizing the models with Adam optimizer (Kingma and Ba 2015) with \\beta_{1} = 0.9 and \\beta_{2} = 0.999, following the official implementation of Diff-Prune."}, {"title": "Reproducibility and Benchmarking", "content": "Reproducibility Naively adopting the implementation of StyleKD on the official StyleGAN2 implementation will fail to reproduce the results. This problem is caused by the use of mixed precision operations in the official StyleGAN2 implementations, where the intermediate layer output type is float16. If we directly use the intermediate layer output features for L_{LD} (latent-direction-based distillation loss (Xu et al. 2022)), the back-propagated gradients become inaccurate, leading to divergence in training. Therefore, we need to convert these intermediate features to float32 before using them in the loss function to ensure stable model training.\nBenchmarking In this paper, we conduct a comprehensive benchmarking of existing StyleGAN2 compression methods using the official StyleGAN2 implementation. In concurrent work, Nickel & DiME (Yeo, Jang, and Yoo 2024) have similarly reproduced previous StyleGAN2 compression methods using the official implementation. However, they failed to reproduce the state-of-the-art compression methods, such as StyleKD and DCP-GAN. With our implementation, the performance of the DCP-GAN (Chung et al. 2024), can be faithfully reproduced (FID=6.35 (reported) \u2192 FID=6.51 (reproduced) in the FFHQ dataset). One notable thing is that in our implementation, StyleKD (Xu et al. 2022) demonstrates significantly improved results (FID=7.25 (reported) \u2192 FID=6.70 (reproduced) in the FFHQ dataset.). We speculate that this performance improvement is due to the optimized GAN training in the official implementation. We hope that our implementation and benchmarks will provide a useful starting point for future research."}, {"title": "Further Analysis", "content": "Effect of Weight initialization technique Unlike other CNN-based models where weights are typically initialized with He initialization (He et al. 2015) (e.g., torch.nn.init.kaiming_uniform in torch.nn.Conv2d), the weights in StyleGAN families (Karras et al. 2020, 2021) are initialized using a unit normal distribution (e.g., torch.nn.randn). We observe that applying He initialization to randomly initialized weights improves further improves fine-tuning process, leading to better performance (see Table 2). This even outperforms pruned weights initialization in the FFHQ dataset. However, in the LSUN Church dataset, pruned weights initialization still outperforms random weight initialization. Similarly, in the case of Diffusion model compression (Fang, Ma, and Wang 2023), random weight initialization yields significantly worse results compared to pruned weights initialization. Therefore, we consider this a special case limited to StyleGAN2 compression in the FFHQ dataset."}]}