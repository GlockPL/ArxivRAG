{"title": "Rectified Lagrangian for Out-of-Distribution Detection in Modern Hopfield Networks", "authors": ["Ryo Moriai", "Nakamasa Inoue", "Masayuki Tanaka", "Rei Kawakami", "Satoshi Ikehata", "Ikuro Sato"], "abstract": "Modern Hopfield networks (MHNs) have recently gained significant attention in the field of artificial intelligence because they can store and retrieve a large set of patterns with an exponentially large memory capacity. A MHN is generally a dynamical system defined with Lagrangians of memory and feature neurons, where memories associated with in-distribution (ID) samples are represented by attractors in the feature space. One major problem in existing MHNs lies in managing out-of-distribution (OOD) samples because it was originally assumed that all samples are ID samples. To address this, we propose the rectified Lagrangian (RegLag), a new Lagrangian for memory neurons that explicitly incorporates an attractor for OOD samples in the dynamical system of MHNs. RecLag creates a trivial point attractor for any interaction matrix, enabling OOD detection by identifying samples that fall into this attractor as OOD. The interaction matrix is optimized so that the probability densities can be estimated to identify ID/OOD. We demonstrate the effectiveness of RecLag-based MHNs compared to energy-based OOD detection methods, including those using state-of-the-art Hopfield energies, across nine image datasets.", "sections": [{"title": "Introduction", "content": "Associative memory models have been proposed to model memory retrieval in the brain through fixed-point search in an artificial neural network. Hopfield networks are classic examples, based on the idea of using recurrently connected neurons to store and retrieve memory patterns. Although these models are theoretically sound, they suffer limited memory capacity, as the number of distinct memory patterns is at most proportional to the dimension of the feature space. Recently, numerous studies have explored models with significantly increased memory capacity, the so-called modern Hopfield networks (MHNs). Some of them are known to have an exponentially large memory capacity with respect to the feature dimension\nFrom a theoretical perspective, Krotov and Hopfield introduced a dynamical system that represents asso-\nWhile these studies have expanded the potential of MHNs both theoretically and practically, one of the primary limitations of existing MHNs lies in managing out-of-distribution (OOD) samples. The dynamical system does find a fixed point for any test input; i.e., an OOD sample is inevitably associated with one of the memorized in-distribution (ID) samples. Zhang et al. proposed an OOD-sample detection method based on the Hopfield energy function. However, they lack theoretical foundation explaining the relationship between the energy and the probability of the input/transient states. We are thus motivated to develop new MHNs equipping probability-aware OOD rejection functionality within the fixed-point search mechanism.\nIn this paper, we propose the rectified Lagrangian (RegLag), a new Lagrangian for memory neurons that creates an attractor for OOD samples in the dynamical system of MHNs, as shown in Fig. 1. RegLag introduces a rectified linear unit (ReLU) with a constant indicating the ID memory strength to the Lagrangian function of memory neurons. We theoretically show that 1) RecLag creates a trivial point attractor for any interaction matrix and 2) RecLag-based MHNs are reduced to vanilla MHNs when the ID memory strength is infinitely large, indicating our approach is a natural extension of existing MHNs. We further devise a training method for RecLag-based MHNs via probabilistic interaction, along with a probability density estimated for ID samples by optimizing the interaction matrix. Our contributions are summarized below:\n1. We propose RecLag, a new Lagrangian for memory neurons. RecLag is designed to create a trivial point attractor for any interaction matrix, enabling OOD detection by identifying samples that fall into the attractor as OOD.\n2. We propose a training method for RecLag-based MHNs having a probabilistic interaction between memory and feature neurons. We prove that samples with low probability fall into the special attractor created by RecLag.\n3. We demonstrated the effectiveness of our approach in comparison with energy-based OOD detection methods, including those using state-of-the-art Hopfield energy functions on nine image datasets."}, {"title": "Related Work", "content": "Hopfield Networks. Hopfield networks are a type of artificial neural network with recurrent structures that model associative memory. Their development laid the foundation for later models such as Boltzmann machines and long short-term memory in the latter part of the 20th century.\nIn recent years, MHNs, also known as dense associative memory have been attracting attention because they can have an exponentially large memory capacity. Numerous studies have demonstrated the effectiveness of MHNs on various tasks including image classification, immune repertoire classification, tabular data classification, reaction template prediction, predictive coding and reinforcement learning.\nMHNs are formulated as dynamical systems described by analytical differential equations. Specifically, Ramsauer et al. generalized the energy function from discrete states to continuous states, and then Krotov and Hopfield formulated the dynamical system of MHNs with two-body differential equations. Follow-up studies, such as work on universal Hopfield networks, have further generalized the dynamical system.\nOOD Detection. OOD detection aims to identify data samples that deviate from the distribution of training data samples. This paper focuses on post hoc approaches, where the detection mechanism is applied after the model has been trained. One of the most well-known approaches is maximum softmax probability (MSP) scoring, which uses the highest softmax output score to identify OOD samples, under the assumption that ID samples yield higher MSP scores compared to OOD samples. To more precisely estimate the distribution of OOD samples, various enhancements and alternative post hoc methods have been proposed Among them, energy-based OOD detection approaches are related to this study in the sense that MHNs have a scalar-valued function associated with the network states, the so-called the Hopfield energy.\nMost recently, several pioneering studies have demonstrated the effectiveness of Hopfield energy for OOD detection. Their methods identify data samples with high Hopfield energy as OOD samples and achieve superior performance among energy-based OOD detection methods. However, from a theoretical perspective, every test sample, including an OOD sample, falls into one of the attractors representing a memory pattern associated with an ID data sample as the dynamical system of MHNs evolves over time. To address this problem, this paper explores MHNs that explicitly have an attractor for OOD samples."}, {"title": "Modern Hopfield Networks", "content": "This paper discusses MHNs with the Lagrangian-based dynamical system proposed by Krotov and Hopfield. We denote the feature neurons as \\(v(t) \\in \\mathbb{R}^{N_V}\\) and the memory neurons as \\(h(t) \\in \\mathbb{R}^{N_H}\\), both at continuous time \\(t \\in \\mathbb{R}_{>0}\\), where \\(N_V, N_H \\in \\mathbb{N}\\) are the numbers of neurons. The dynamical system is described by the following differential equations:\n\\( \\tau_V \\frac{dv_i(t)}{dt} = \\sum_{\\mu=1}^{N_H} \\xi_{i \\mu} f_{\\mu}(h(t)) - v_i(t), \\) (1)\n\\( \\tau_H \\frac{dh_{\\mu}(t)}{dt} = \\sum_{i=1}^{N_V} \\xi_{\\mu i} g_i(v(t)) - h_{\\mu}(t), \\) (2)\nwhere \\(\\xi \\in \\mathbb{R}^{N_H \\times N_V}\\) is an interaction matrix representing the strength of synapses, \\(f : \\mathbb{R}^{N_H} \\rightarrow \\mathbb{R}^{N_H}\\) and \\(g : \\mathbb{R}^{N_V} \\rightarrow \\mathbb{R}^{N_V}\\) are activation functions, and \\(\\tau_V, \\tau_H \\in \\mathbb{R}\\) are constants that determine the dynamics of neurons. The activation functions are determined by the Lagrangians \\(L_H : \\mathbb{R}^{N_H} \\rightarrow \\mathbb{R}\\) and \\(L_V : \\mathbb{R}^{N_V} \\rightarrow \\mathbb{R}\\) such that\n\\( f(h) = \\frac{\\partial L_H(h)}{\\partial h}, \\quad g(v) = \\frac{\\partial L_V(v)}{\\partial v}, \\) (3)\nwhere \\(h \\in \\mathbb{R}^{N_H}\\) and \\(v \\in \\mathbb{R}^{N_V}\\). The energy function is then given by\n\\( E(v, h) = \\sum_{i=1}^{N_V} v_i g_i(v) - L_V(v) + \\sum_{\\mu=1}^{N_H} h_{\\mu} f_{\\mu}(h) - L_H(h) - \\sum_{\\mu, i} f_{\\mu}(h) \\xi_{\\mu i} g_i(v). \\) (4)\nNote that this energy monotonically decreases; that is, we have \\(dE(v(t), h(t))/dt \\leq 0\\) along the trajectory of the dynamical system when the Hessian matrices of the Lagrangians are positive semi-definite.\nLagrangians. If we suppose a fixed interaction matrix \\(\\xi\\), then the model dynamics are defined by the choice of the Lagrangians. For example, when the Lagrangian functions are given by the additive functions\n\\( L_H(h) = \\sum_{\\mu=1}^{N_H} \\sigma(h_{\\mu}), \\quad L_V(v) = \\sum_{i=1}^{N_V} v_i, \\) (5)\nwhere \\(\\sigma : \\mathbb{R} \\rightarrow \\mathbb{R}\\) is a nonlinear function, the energy function reduces to\n\\( E(v) = - \\sum_{\\mu=1}^{N_H} \\sigma \\Biggl( \\sum_{i=1}^{N_V} \\xi_{\\mu i} \\mathrm{isgn}(v_i) \\Biggr). \\) (6)\nunder the adiabatic limit \\(\\tau_V \\gg \\tau_H\\) when \\(\\xi\\) is a symmetric matrix. This energy function is identical to that of dense associative memory. Further, when \\(\\sigma(x) = x^2\\), it reduces to the energy function of the classical Hopfield network.\nRecently, Krotov and Hopfield introduced the following Lagrangians:\n\\( L_H(h) = \\frac{1}{\\beta} \\sum_{\\mu=1}^{N_H} \\log \\Biggl( \\exp(\\beta h_{\\mu}) \\Biggr), \\quad L_V(v) = \\frac{1}{N_V} \\sum_{i=1}^{N_V} v_i^2, \\) (7)\nwhere \\(\\beta \\in \\mathbb{R}_{>0}\\) is a constant. Under the adiabatic limit and when \\(\\beta = 1\\), the energy function reduces to\n\\( E(v) = - \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp \\Biggl( \\sum_{i=1}^{N_V} \\xi_{\\mu i} v_i \\Biggr) \\Biggr) + \\frac{1}{N_V} \\sum_{i=1}^{N_V} v_i^2. \\) (8)\nThis energy function is identical to that of the MHNs proposed by Ramsauer et al."}, {"title": "Energy-Based OOD Detection", "content": "Let us consider classification problems and denote the number of ID classes for training as C. The goal of OOD detection is to identify data samples that do not belong to any of the C classes. Zhang et al. proposed using the energy function of MHNs for OOD detection. Specifically, they introduced two energy functions: modern Hopfield energy (MHE) and simplified Hopfield energy (SHE). MHE is obtained by replacing the interaction matrix \\(\\xi\\) in Eq. (8) with a class-specific pattern matrix \\(S^c \\in \\mathbb{R}^{d \\times N}\\) and by omitting the second term as follows:\n\\( \\mathrm{MHE}(\\tilde{v}) = - \\log \\Biggl( \\sum_{\\mu=1}^{N} \\exp \\Biggl( \\sum_{i=1}^{d} S_{i\\mu}^c \\tilde{v}_i \\Biggr) \\Biggr), \\) (9)\nwhere \\(\\tilde{v} \\in \\mathbb{R}^d\\) is a test pattern, \\(c \\in \\{1, 2, \\dots, C\\}\\) is the classification result of \\(\\tilde{v}\\) obtained from a pre-trained classification model, \\(d\\) is the hidden dimension, and \\(N\\) is the number of stored patterns. SHE is a Taylor approximation of MHE, but is more effective than MHE at detecting OOD. It is defined as\n\\( \\mathrm{SHE}(\\tilde{v}) = \\sum_{\\mu=1}^{N} \\sum_{i=1}^{d} S_{i\\mu}^c \\tilde{v}_i. \\) (10)\nOOD samples can be detected by applying a threshold to these energy functions. However, as the dynamical system of MHNs evolves over time, every test sample falls into an attractor associated with an ID data sample, indicating a lack of theoretical consistency."}, {"title": "Rectified Lagrangian", "content": "This section introduces RecLag, a Lagrangian function that creates a point attractor for OOD samples in the dynamical system of HMNs. As shown in Figure 2, RecLag creates a point attractor in the feature space. This attractor is designed to exist for any interaction matrix \\(\\xi\\), enabling OOD detection by identifying data samples that fall into it as OOD."}, {"title": "Definition", "content": "To incorporate a point attractor for OOD samples in the dynamical system, we propose a minimal yet effective modification to the Lagrangian function of memory neurons. Specifically, we introduce an inverse memory strength constant \\(\\gamma\\), which determines the strength of ID samples stored in memory, with a max function to screen out negative values, which is applied in the same way as ReLU. The proposed RecLag is defined as follows.\nDefinition 1. We define RecLag as\n\\( L_H(h) = \\max \\Biggl( \\frac{1}{\\beta} \\sum_{\\mu=1}^{N_H} \\log \\Biggl( \\exp(\\beta h_{\\mu}) \\Biggr) - \\frac{1}{\\gamma}, 0 \\Biggr), \\) (11)\nwhere \\(\\beta, \\gamma \\in \\mathbb{R}_{>0}\\) are constants."}, {"title": "Existence of a Trivial Point Attractor", "content": "With the dynamical system using RecLag \\(L_H\\) in Eq. (11) for memory neurons and the Lagrangian \\(L_V\\) in Eq. (7) for feature neurons, Theorem 1 shows that there exists a trivial point attractor at the origin of the feature space for any interaction matrix.\nTheorem 1. Suppose that activation functions \\(f\\) and \\(g\\) in the dynamical system of Eqs. (1,2) are given by the derivatives of RecLag \\(L_H\\) in Eq. (11) and the Lagrangian \\(L_V\\) in Eq. (7), respectively. For any interaction matrix \\(\\xi \\in \\mathbb{R}^{N_H \\times N_V}\\), a trivial point attracting set \\(A = \\{0\\}\\) exists at the origin \\(0 \\in \\mathbb{R}^{N_V}\\) in the feature space when \\(\\gamma > N_H\\) under the adiabatic limit \\(\\tau_V = dt\\).\nSketch of proof. With RecLag, writing the differential equations of the dynamical system in finite differences with \\(\\frac{dv_i}{dt} \\approx \\frac{v_i^{(k+1)} - v_i^{(k)}}{\\Delta t}\\) and \\(\\tau_V = \\Delta t\\) gives the following update rule for feature neurons:\n\\( v_i^{(k+1)} = \\chi \\Biggl( G(v^{(k)}) \\Biggr) \\sum_{\\mu=1}^{N_H} \\xi_{i\\mu} \\mathrm{softmax} \\Biggl( \\beta \\sum_{j=1}^{N_V} \\xi_{\\mu j} v_j^{(k)} \\Biggr), \\) (12)\nwhere \\(k \\in \\mathbb{N}\\) is a discrete time step, and\n\\( G(v) = \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp \\Biggl( \\beta \\sum_{j=1}^{N_V} \\xi_{\\mu j} v_j \\Biggr) \\Biggr) - \\frac{1}{\\gamma}, \\) (13)\n\\( \\chi(x) = \\begin{cases} 1 & (x > 0) \\\\ 0 & (x \\leq 0) \\end{cases} \\) (14)\nWhen \\(v^{(k)} = 0\\), we have \\(\\chi(G(v^{(k)})) = 0\\), and thus we have \\(v^{(k+1)} = 0\\). This shows that \\(0 \\in \\mathbb{R}^{N_V}\\) is a fixed point of the dynamical system in the feature space. Further, with the epsilon neighborhood of the origin \\(U_\\epsilon = \\{u : ||u||_2 < \\epsilon\\}\\), we have \\(\\chi(G(u)) = 0\\) for every \\(u \\in U\\) if \\(\\epsilon\\) is small enough. This shows that \\(A = \\{0\\}\\) is an attracting set for every fixed interaction matrix \\(\\xi\\). A full proof is given in Appendix A."}, {"title": "Reduction to Vanilla MHNS", "content": "Along with the existence of the trivial point attractor, it is also worth noting the limit where it disappears. Theorem 2 shows that RecLag-based MHNs reduce to vanilla MHNs when the memory strength of ID samples is infinitely large, that is, when the inverse memory strength constant \\(\\gamma \\rightarrow 0\\). This theoretical result indicates that our approach is a natural extension of MHNs. A proof is given in Appendix B.\nTheorem 2. Let \\(v_A\\) and \\(v_B\\) be feature neurons of a vanilla MHN and a RecLag-based MHN, respectively. Suppose \\(v_A^{(0)} = v_B^{(0)}\\). For every \\(\\epsilon > 0\\), there exists a small \\(\\gamma > 0\\) such that \\(\\sup_k ||v_A^{(k)} - v_B^{(k)}||_2 < \\epsilon\\)."}, {"title": "Visualization and Discussion", "content": "Visualization. Figure 2 compares the energy distributions of a vanilla MHN and a RecLag-based MHN, where each red point indicates a fixed point \\(\\xi_\\mu \\in \\mathbb{R}^{N_V}\\) at a local minimum of the energy function. As shown, RecLag creates an attractor at the origin of the feature space. This attractor is associated with OOD samples as described in the next section. The 3D visualization of these energy functions is shown in Figure 1 with trajectories of a test sample (white diamond-shaped point) over time. As shown, with the vanilla MHN, the test sample falls into one of the attractors even if it is an OOD sample. In contrast, with the RecLag-based MHN, the same test sample falls into the created attractor, indicating that none of the memory patterns are associated with it. This shows that the RecLag-based MHN can explicitly manage OOD samples in the dynamical system.\nMemory Strength. The size of the created attractor increases as the inverse memory strength constant \\(\\gamma\\) increases. Consequently, the number of samples identified as OOD samples also increases with \\(\\gamma\\). This indicates that \\(\\gamma\\) can serve as a threshold parameter that adjusts the sensitivity of RecLag-based MHNs to OOD samples. In practice, to draw a receiver operating characteristic (ROC) curve, one could vary \\(\\gamma\\) to generate different true positive rates (TPRs) and false positive rates (FPRs) for OOD detection."}, {"title": "Training via Probabilistic Interaction", "content": "This section discuss the basin of the attractor created by RecLag, and proposes a method for training the interaction matrix with ID samples. Because the basin obviously involves \\(B_0 = \\{v : G(v) < 0\\}\\), as shown in the sketch of the proof for Theorem 1, we introduce a method to train the interaction matrix via probabilistic interaction, by which data samples with low probability density values fall into \\(B_0\\)."}, {"title": "Probabilistic Interaction", "content": "The probabilistic interaction explicitly chooses a single memory neuron for each input feature during training in a probabilistic manner. This creates a cycle of interaction between feature neurons and memory neurons in the following two steps. First, given an input feature \\(x \\in \\mathbb{R}^{N_V}\\), a memory neuron is sampled as \\(\\mu \\sim p_H(\\mu | x)\\), where \\(\\mu \\in \\{1, 2, ..., N_H\\}\\) is an index of memory neurons and \\(p_H(\\mu | x)\\) is a pre-defined conditional probability mass distribution. Second, given an index \\(\\mu\\), an output feature \\(y \\in \\mathbb{R}^{N_V}\\) is sampled as \\(y \\sim p_V(y | \\mu)\\), where \\(p_V(y | \\mu)\\) is a pre-defined conditional probability density distribution.\nBecause this interaction can be understood as the stochastic feedforward neural network (SFNN) proposed by Tang and Salakhutdinov which samples an index of neurons in a hidden layer, we train the interaction matrix using the training method for SFNN. Specifically, given a set of ID data samples \\(D \\subset \\mathbb{R}^{N_V}\\), the interaction matrix \\(\\xi\\) is trained to maximize the sum of probability products:\n\\( P = \\sum_{x \\in D} p_V(x | \\mu) p_H(\\mu | x). \\) (15)\nHere, the distribution \\(p_H(\\mu | x)\\) is computed through the joint probability distribution described in the next subsection. The distribution \\(p_V(x | \\mu)\\) is used only for training, and thus we use a Gaussian distribution following Tang and Salakhutdinov:\n\\( p_V(x | \\mu) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp \\Biggl( -\\frac{1}{2} (x - \\xi_\\mu)^T \\Sigma^{-1} (x - \\xi_\\mu) \\Biggr), \\) (16)\nwhere \\(\\Sigma\\) is a learnable covariance matrix."}, {"title": "Attracting Probability", "content": "Interestingly, there exists a joint probability distribution \\(p_H(x, \\mu)\\) that relates the SFNN and the basin \\(B_0\\). Specifically, Definition 2 provides the joint probability distribution, by which the conditional probability for the SFNN is computed as \\(p_H(\\mu | x) = p_H(x, \\mu) / \\sum_\\mu p_H(x, \\mu)\\), and data samples with low probability density values fall into the basin.\nDefinition 2. Let \\(X\\) be a continuous random variable of feature neurons over \\(\\mathbb{R}^{N_V}\\), and let \\(M\\) be a discrete random variable of the index of hidden neurons over \\(\\{1, 2, ..., N_H\\}\\). We define the joint probability distribution function as\n\\( p_H(X = x, M = \\mu) = \\frac{1}{Z} \\exp \\Biggl( \\beta \\sum_{j=1}^{N_V} \\xi_{\\mu j} x_j \\Biggr). \\) (17)\nHere, \\(Z\\) is a normalization constant given by\n\\( Z = \\sum_{\\mu=1}^{N_H} \\int_S \\exp \\Biggl( \\beta \\sum_{j=1}^{N_V} \\xi_{\\mu j} x_j \\Biggr) dx, \\) (18)\nwhere \\(S \\in \\mathbb{R}^{N_V}\\) is a sufficiently large hypersphere to cover all data samples."}, {"title": "OOD detection", "content": "Finally, Theorem 3 shows that the probability density distribution \\(p_H(x) = \\sum_\\mu p_H(x, \\mu)\\) explicitly models the distribution of ID samples and that all data samples with a probability density lower than \\(\\delta\\) fall into the attractor created by RecLag. Therefore, OOD samples can be detected by evaluating \\(p_H(\\tilde{v})\\) given a test sample \\(\\tilde{v} \\in \\mathbb{R}^{N_V}\\).\nTheorem 3. The basin \\(B_0 = \\{v : G(v) < 0\\}\\) is identical to the set of points that have low probability density values. In other words, a threshold \\(\\delta\\) exists such that\n\\( B_0 = \\{x: p_H(X = x) < \\delta\\}. \\) (19)\nVisualization. Figure 2(d) shows the probability density function, where the basin boundary is drawn in white."}, {"title": "Experiments", "content": "We focus on evaluating OOD detection performance of our proposed method along with strong baselines in this work."}, {"title": "Experimental Settings", "content": "Datasets. Eleven image datasets were used to conduct OOD detection experiments: CIFAR-10, CIFAR-100, SVHN, LSUN-C, LSUN-R, iSUN, Places365, DTD, TinyImageNet (TIN), SUN, and iNaturalist. The CIFAR-10 or CIFAR-100 dataset was used as the ID dataset, and the other nine datasets were used as OOD datasets.\nEvaluation Measure. We used FPR95 as the primary evaluation measure, which is the FPR of OOD samples when the TPR for ID samples is 95.0%. ROC curves and the area under the curve (AUC) are also reported.\nBaselines. We chose five baseline methods: MSP scoring, energy-based detection (Energy), rectified activations applied to energy (ReAct), MHE, and SHE. Note that the last four methods are energy-based OOD detection methods, with MHE and SHE being state-of-the-art using MHNs. Also note that these methods, including ours, process representations from a frozen encoder. For a fair comparison, we use the same encoder in each experiment. Another type of ODD detection methods (such as Zhang et al.) that jointly optimize encoder and OOD module in a specific fashion is excluded.\nNeural networks. Three image classification networks were used: ResNet18, ResNet34, and WideResNet40-2 (WRN40-2). They were trained on an ID dataset using cross-entropy loss. The OOD benchmark was conducted with no dynamics simulations (no extra computational costs compared to the baselines)."}, {"title": "Experimental Results", "content": "Comparison With Energy-Based Methods. Table 1 shows the OOD detection performance on the nine OOD datasets with the three neural networks trained on the CIFAR-10 dataset. As shown, our RecLag-based MHN (RecLag) achieved the best average performance across all neural networks. This demonstrates the effectiveness of our approach, which incorporates an attractor for OOD samples in post-hoc OOD detection scenarios.\nROC Curves. Figure 3 reports the ROC curves with the AUC values. As shown, RecLag exhibited the best AUC value in 23 out of 27 comparisons (highlighted with the yellow background), indicating its consistent superiority in OOD detection performance.\nIn-Distribution Data. To investigate how OOD detection performance is affected when a neural network is trained on a more complex task, Table 2 shows the results for WRN40-2 trained on CIFAR-100. As shown, the OOD performance decreases for all methods compared to those in Table 1. This is because the variance of features in ID samples increased, making OOD detection more challenging. However, even in this case, our RecLag-based MHN outperformed the other methods. This result indicates that the relative effectiveness of our approach is robust against differences in ID samples.\nTime Evolution. Figure 4 analyzes how the detection scores change as the dynamical system of MHNs evolves over time. With MHE, OOD samples have higher energy scores than ID samples at time \\(t = 0\\); however, the scores decrease over time, making it almost impossible to distinguish between ID samples and OOD samples at a discrete time step of 4. In contrast, RecLag-based MHN can distinguish between ID samples and OOD samples even after the score converges, thereby maintaining OOD detection performance over time as shown in Figure 5. This demonstrates that our approach successfully managed OOD samples within the dynamical system of MHNS.\nVisual Analysis. Figure 6 analyzes the OOD samples incorrectly identified as ID samples. As shown, images of animals, people, and foods were difficult to detect as OOD. This study focused on post-hoc OOD detection, but in the future, it would be interesting to simultaneously train MHNs and classification networks to further improve the performance."}, {"title": "Conclusion", "content": "We proposed the RecLag function, a specially designed Lagrangian to equip MHNs with OOD rejection functionality. In our method, the interaction matrix is optimized so as to compute probability densities, which are used to determine ID/OOD. Theoretically, RecLag-based MHNs reduces to vanilla MHNs when the ID memory strength is infinitely large; therefore, the proposed method is a natural extension of existing MHNs. Experiments on nine image datasets demonstrated the effectiveness of our approach, surpassing energy-based OOD detection methods.\nLimitation. While this work introduced a new Lagrangian for memory neurons, the Lagrangian for feature neurons remains underexplored. Similar to previous works, we used the activation function \\(g\\) that takes the simplest form in Euclidean space, \\(g_i(v) = v_i\\), because recent deep learning efforts often assume that the feature space is Euclidean. Investigating new Lagrangians in other non-linear feature spaces, such as spherical or hyperbolic space, might be promising.\nFuture Work. In future work, we will focus on generalizing RecLag for structured memory patterns such as hierarchical memory patterns. Applications to regression tasks are also intriguing. We believe this work has opened up new avenues for exploring the potential of MHNs."}, {"title": "Appendix A. Proof of Theorem 1", "content": "Theorem 1 shows that a point attractor exists at the origin of the feature space. We first define the attracting set \\(A\\) and its basin \\(B\\)", "cdot)": "X \\rightarrow X\\) are continuous functions. We say that a closed set \\(A \\subset X\\) is an attracting set if there exists a neighborhood \\(U\\) of \\(A\\) that satisfies the following two conditions.\n(a) There exists \\(T\\) such that \\(\\bigcap_{t>T"}, {"x)": "x \\in U\\"}, "A\\).\n(b) There exists \\(T\\) such that, for every neighborhood \\(V\\) of \\(A\\), \\(t > T \\Rightarrow \\{\\phi(t, x) : x \\in U\\} \\subset V\\).\nWe define the basin \\(B\\) of attraction of \\(A\\) as \\(B = \\bigcup_{t>0} \\{x : \\phi(t, x) \\in U\\}\\).\nTheorem 1. Suppose that activation functions \\(f\\) and \\(g\\) in the dynamical system of Eqs. (1, 2) are given by the derivatives of RecLag \\(L_H\\) in Eq. (11) and the Lagrangian \\(L_V\\) in Eq. (7), respectively. For any interaction matrix \\(\\xi \\in \\mathbb{R}^{N_H \\times N_V}\\), a trivial point attracting set \\(A = \\{0\\}\\) exists at the origin \\(0 \\in \\mathbb{R}^{N_V}\\) in the feature space when \\(\\gamma > N_H\\) under the adiabatic limit \\(\\tau_V = dt\\).\nProof. With RecLag, the activation function \\(f_\\mu\\) is given by\n\\( f_\\mu(h) = \\frac{\\partial}{\\partial h_\\mu} \\max \\Biggl( \\frac{1}{\\beta} \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu) \\Biggr) - \\frac{1}{\\gamma}, 0 \\Biggr) \\)\n\\(= \\frac{\\partial}{\\partial h_\\mu} \\chi \\Biggl( \\frac{1}{\\beta} \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu) \\Biggr) \\Biggr) \\)\n\\(= \\chi \\Biggl( \\frac{1}{\\beta} \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu) \\Biggr) \\Biggr) \\frac{\\partial}{\\partial h_\\mu} \\Biggl( \\frac{1}{\\beta} \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu) \\Biggr) \\Biggr) \\)\n\\(= \\chi \\Biggl( \\frac{1}{\\beta} \\log \\Biggl( \\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu) \\Biggr) \\Biggr) \\Biggl[ \\frac{1}{\\sum_{\\mu=1}^{N_H} \\exp(\\beta h_\\mu)} \\exp(\\beta h_\\mu) \\Biggr"]}