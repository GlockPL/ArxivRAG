{"title": "AutoTest: Evolutionary Code Solution Selection with Test Cases", "authors": ["Zhihua Duan", "Jialin Wang"], "abstract": "Abstract-With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen.\nAutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.", "sections": [{"title": "I. INTRODUCTION", "content": "Code generation technology has made significant progress in recent years, but selecting the correct solution from multiple candidate solutions remains a challenge [1], [2]. This paper proposes the AutoTest method, which combines the steps of automatically generating test cases and executing code solutions to select the correct solution using an evolutionary genetic algorithm.\nAs shown in Figure 1, the first step involves reusing large models such as codegen-16B [3], code-davinci-002 [4], and incoder-6B [5] to generate code solutions and test cases for each programming problem. Next, each solution is executed on the generated test cases, and a consensus set is formed by finding commonalities among multiple solutions and test cases. Then, by adjusting the alpha and beta parameters and applying the selection, mutation, and transformation mecha- nisms of evolutionary genetics, the code solutions are finely ranked, and the highest-ranked code solution is selected as the best solution. The AutoTest method is simple and effi- cient. Experimental results demonstrate that AutoTest achieves significant improvements on the HumanEval benchmark test. By combining the automatic generation of test cases with the execution of code solutions, the AutoTest method is able to select the correct code solution from multiple candidate solutions, thereby improving the pass@1 score."}, {"title": "II. RELATED WORK", "content": "Large language models leverage abundant open-source code resources and undergo training with billions of parameters, showcasing remarkable performance in code generation tasks. For instance, models such as AlphaCode [6], Codex [7], CodeT5 [8], CodeGen [3], InCoder [5], CodeGeeX [9] and StarCoder [10] have achieved significant advancements in code generation applications.\nBy utilizing automated test case generation techniques, it is possible to greatly enhance work efficiency. Pretrained language models such as BART, T5, Codex, and other large-scale models can be employed in a zero-shot setting to sample and generate test cases using prompt statements.\nDespite the excellent performance of large-scale models in code generation tasks, they still require iterative sampling to determine the correct output. Researchers such as [11], and others have proposed solutions to address this issue. In contrast to their work, this paper leverages the code solutions and test cases provided by large-scale models and employs an evolutionary genetic algorithm to select the best code solution."}, {"title": "III. METHODOLOGY", "content": "The goal of code generation tasks is to generate code solutions based on contextual prompts, which include natural language problem descriptions and code snippets. Using large language models, a set of code solutions is generated based on the contextual prompts, and the best code solution is selected from the generated set. The AutoTest method utilizes pre-trained language models to provide code solutions and test cases related to code programming problems.\nCode Solutions: Powerful large-scale language models such as codegen-16B, code-davinci-002, and incoder- 6B can be employed to generate code solutions. These models possess extensive programming knowledge and capabilities, allowing them to generate high-quality code based on the provided requirements and contextual infor- mation.\nTest Cases: Test cases are generated to evaluate the cor- rectness of the code solutions. Using the same pre-trained language model used for generating code solutions, input and expected output for defining functions are specified within the context to generate test cases. It is ensured that the generated test cases exhibit diversity and difficulty.\nIn this paper, an evolutionary genetic algorithm is employed to select the best code solution. The fundamental idea behind the evolutionary genetic algorithm is to execute the generated code solutions and compare their execution results on the generated test cases to determine their correctness. Through iterative execution and comparison, consistent combinations of code solutions and test cases are identified. The evolutionary genetic algorithm is then utilized to select the highest-ranked solution based on the included test cases and solutions as the best code solution.\nAs shown in the figure 2, AutoTest proposes a method for selecting the best code solution based on the provided set of code solutions and test cases. By executing the code solutions and comparing them with the expected outputs of the test cases, it can be determined whether the code solutions pass the tests.\nThe AutoTest method follows the following steps for im- plementation:\n1. Randomly select a pair of code solutions and test cases and attempt to execute the code solution on the test cases. If the code solution passes the test cases, it is labeled as an inlier; otherwise, it is labeled as an outlier. For each inlier, collect other code solutions and test cases that are consistent with it to form a consensus set.\nFor example, both \"return a^2\" and \"return axa\" pass the test cases \"assert num_square(1) == 1,\" \"assert num_square(2) == 4,\" and \"assert num_square(0) == 0,\u201d forming a collection of code solutions and test cases with 2 code solutions and 3 test cases.\n2. Based on the consensus set of code solution and test case pairs, Assuming alpha is set to 0.5 and beta is set to 1.1,use an evolutionary genetic algorithm with selection, mutation, and crossover mechanisms to rank the code solutions. Individuals with higher fitness are selected as parents, and mutation and crossover operations introduce new solutions and generate offspring.\n3. Select the code solution with the highest score from the consensus set as the best solution. the best code solution obtained through the evolutionary genetic algorithm is either \"return a^2\" or \"return axa.\"\nAutoTest proposes an automated testing method based on code generation solutions and test cases from large language models. By utilizing code solutions and test cases generated by the large language model, the method employs an evolutionary genetic algorithm to select the best code solution.\nThe alpha parameter controls the weight assigned to code generation, and the beta parameter controls the weight as- signed to test case generation. By adjusting the values of alpha and beta, AutoTest can adapt to different coding problems and datasets. The choice of optimal values for alpha and beta depends on the characteristics of the problem domain, the complexity of the code solutions, and the nature of the test cases.\nAutoTest algorithm allows for exploration and evaluation of diverse code solutions and test cases, reducing the dependence on any single model and mitigating the impact of biases and overfitting."}, {"title": "IV. EXPERIMENT", "content": "The HumanEval [7] dataset is a collection of 164 pro- gramming problems designed for evaluating the correctness of functions. Each problem consists of a function signature, a documentation string, a function body, and multiple unit tests, with an average of 7.7 tests per problem. As the large language models are trained on code from GitHub, which contains solutions from various sources, the programming tasks in the HumanEval dataset are manually crafted to assess language comprehension, reasoning abilities, algorithms, and basic mathematical skills. The dataset serves as a measure of the problem-solving capabilities of the large language models.\nThis study utilized experimental data provided by Microsoft [2], which tested the codegen-16B, code-davinci-002, and incoder-6B large-scale language models. The code-davinci- 002 model, developed by OpenAI, is a large language model optimized specifically for code generation tasks. It can be applied in various scenarios such as code autocompletion and code inference. The incoder-6B model, released by Meta,"}, {"title": "C. Metrics", "content": "The pass@k (n samples) metric is used for performance evaluation, and test cases are employed to verify the correct- ness of the code solutions. For each problem, n code solutions are randomly selected from the samples, and k solutions are chosen for evaluation. If any of these selected solutions pass all the test cases, it is considered that the problem has been successfully solved. The pass@k metric represents the percentage of problems solved. An unbiased definition of pass@k, as a benchmark, involves randomly selecting k solutions from n samples. In this study, an evolutionary genetic next-generation algorithm protocol is applied. It selects k solutions from n samples and dynamically adjusts alpha and beta values. Methods such as selection, alteration, and mutation are used to group the solutions based on test outputs and sort them according to their scores."}, {"title": "D. Experimental Results", "content": "The evaluation of the HumanEval benchmark test set was conducted based on the Codex, InCoder and CodeGen pre-trained models. The experimental results on the HumanEval benchmark test for the codegen-16B, code-davinci-002, and incoder-6B large models are summarized in Table I. In the baseline methods, the pass@100 scores of the large models were significantly higher than the pass@1 scores, indicating a significant advantage in selecting the best code solution from 100 samples. Additionally, in the AlphaCode and AutoTest methods, pass@2 and pass@10 scores were also evaluated in this study. Comparing the AutoTest scores with the baseline scores for pass@1, it was found that the AutoTest models achieved an improvement of approximately 10% compared to the baseline pass@1. For pass@2, AutoTest showed an improvement over the AlphaCode scores. For pass@10, the AutoTest score for code-davinci-002 was 85.0, which is a 0.6% improvement over the AlphaCode score of 84.4. These results demonstrate that AutoTest can enhance the performance of various pretrained language models."}, {"title": "E. Discuss", "content": "This paper uses an evolutionary genetic algorithm for op- timizing solution approaches. Further research is needed to evaluate its effectiveness in scoring and solving code-related problems. Nonetheless, the study offers promising directions for future research.\nImproved scoring mechanism: Revisiting the scoring mechanism, the original scoring method may have certain limitations. To address this issue, new scoring metrics can be introduced to better measure the performance of solutions.\nOptimization of algorithm parameters: Optimizing and adjusting the alpha and beta parameters of the evolutionary genetic algorithm."}, {"title": "V. CONCLUSION", "content": "This paper proposes a large language model based approach called AutoTest, which utilizes a large model to generate code solutions and test cases simultaneously. The generated test cases are then executed to evaluate the code solutions, and an evolutionary genetic algorithm is employed to select the best solution. Future work can focus on further improving the AutoTest method to address more complex programming problems, support multiple programming languages, handle more sophisticated algorithms, and enhance the diversity and coverage of test case generation."}]}