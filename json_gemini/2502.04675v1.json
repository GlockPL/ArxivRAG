{"title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing", "authors": ["Xueru Wen", "Jie Lou", "Xinyu Lu", "Junjie Yang", "Yanjiang Liu", "Yaojie Lu", "Debing Zhang", "Xing Yu"], "abstract": "As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) critique of critique can be easier than critique itself, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) this difficulty relationship is recursively held, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. To examine these hypotheses, we perform Human-Human, Human-AI, and AI-AI experiments across multiple tasks. Our results demonstrate encouraging evidence supporting these hypotheses and suggest that recursive self-critiquing is a promising direction for scalable oversight.", "sections": [{"title": "1. Introduction", "content": "The provision of supervision signals is fundamental to AI alignment (Bowman et al., 2022). From the supervision signal acquisition perspective, tasks can be categorized as: (1) tasks with well-defined criteria, where ground truth can be deterministically obtained with low computational overhead, e.g., Go games and mathematical problems (Silver et al.,\nThe main idea for this work came from a late-night, insightful discussion between Jie Lou and Xing Yu, which was part of some truly wonderful days.\nThis work was conducted during Xueru Wen and Xingyu Lu's internship at Xiaohongshu."}, {"title": "2. Recursive Self-Critiquing", "content": "In this section, we introduce the interaction protocols that define how recursive self-critiquing progresses through multiple levels, from initial response to higher-order critiques. We then present two baselines, majority voting and naive voting, for fair comparison of the recursive critique's effectiveness."}, {"title": "2.1. Protocols", "content": "As shown in the Figure 1, the hierarchical criticism architecture progresses through multiple levels: from initial response, through first-order critique, to second-order critique of critique (C2) and higher-order critiques.\nResponse Response represents the initial attempt to answer the question, serving as the foundation of the criticism chain. Each response comprises a complete solution process and its corresponding answer, which is formally defined as:\n\\(R(Q) \\rightarrow (T^0, A^0),\\)\n(1)\nwhere Q denotes the input question, T represents the solution process, and A is the final answer. The inclusion of the full solution process enables critiques to evaluate the complete reasoning path rather than merely the result.\nCritique The critique evaluates pairs of candidate responses for a given input question, conducting comparative analysis and providing reasoning judgment as follows:\n\\(C^1(Q, R_1, R_2) \\rightarrow (T^1, A^1),\\)\n(2)\nwhere R1 and R2 denote candidate responses, T\u00b9 represents the critique rationale, and A\u00b9 is the answer to the question based on the responses and critique rationale.\nCritique of critique The second-order critique evaluates pairs of critiques, extending the evaluation to a higher level of abstraction, which is defined as:\n\\(C^2(Q, R_1, R_2, C_1, C_2) \\rightarrow (T^2, A^2),\\)\n(3)\nwhere C1 and C2 are two critiques of the original responses. T\u00b2 represents the second-order critique's analysis of the critiques' reasoning, and A\u00b2 denotes the final judgment.\nHigher-order critiques The n-th order critique extends this evaluation pattern to higher levels of abstraction, analyzing the assessments from all previous levels, and it is defined as:\n\\(C^n(Q, R_1, R_2, ..., C^{n-1}, C^{n-1}) \\rightarrow (T^n, A^n),\\)\n(4)\nwhere C^{n-1} and C^{n-1} are evaluations from the (n \u2212 1)-th order critiques, T^n represents the n-th order critic's analysis of the previous critiques, and A^n denotes the final judgment."}, {"title": "2.2. Baselines", "content": "We introduce two baseline strategies for the rigorous comparison for the recursive critique: majority voting that ensures fair comparison under equivalent computational effort, and naive voting that simply aggregates all available judgments to verify whether recursive critique generates meaningful new insights beyond naive consensus."}, {"title": "3. Is Recursive Critique Increasingly Easier?", "content": "In this section, we validate the hypothesis that critique of critique is easier than direct critique and examine whether\nMajority voting Since higher-order critiques is based on all previous evaluation results, we need to consider computational cost for fair comparison. To validate that improvements stem from the recursive structure rather than increased computation, we compare higher-order critiques with majority voting of lower-order evaluations under similar effort. Specifically, let \u20ac(\u00b7) denote the computational overhead for each single evaluation. As presented in Figure 1, the total efforts E(\u00b7) for different order recursive critique C1, C2 and C\u00b3 are defined as:\n\\(E(C^1) = \\epsilon(R_1) + \\epsilon(R_2) + \\epsilon(C^1) \\approx 3\\epsilon(R),\\)\n\\(E(C^2) = \\epsilon(R_1) + \\epsilon(R_2) + \\epsilon(C_1) + \\epsilon(C_2) + \\epsilon(C^2) \\approx 5\\epsilon(R),\\)\n(5)\n\\(E(C^3) = \\epsilon(R_1) + \\epsilon(R_2) + \\epsilon(C_1) + \\epsilon(C_2) + \\epsilon(C^2) + \\epsilon(C^2) + \\epsilon(C^3) \\approx 7\\epsilon(R).\\)\nThen, we define majority voting at each level. For level l, given a set of n evaluations, the majority voting result is:\n\\(Major_l(A) = \\underset{a}{argmax} \\sum_{i=1}^n \\mathbb{1}(A_i = a),\\)\n(6)\nwhere A represents the judgment from the i-th evaluation at level l, and 1(\u00b7) is the indicator function. Intuitively, this formula counts the occurrences of each possible answer among the n evaluations and selects the most frequent one as the final result. In case of ties where multiple answers have the same highest frequency, one is randomly selected. To ensure effort equivalence when comparing with recursive critique at level l, we calculate Major where k < l and n = E(C^l)/E(C^k). As an example, for a fair comparison, C\u00b3 should be compared with Major (majority voting among three C\u00b2 critiques) and Major (majority voting among five C\u00b9 critiques).\nNaive voting baseline A natural strategy for higher-order critique is to simply aggregate all judgments from previous stages through voting, adding no new analysis but merely following the consensus. The naive voting is defined:\n\\(C_{naive}(R_1, R_2) \\rightarrow Major({A_1^1, A_2^1}),\\)\n\\(C_{naive}^2(C_1^1, C_2^1) \\rightarrow Major({A_1^1, A_2^1, A_1^2, A_2^2}),\\)\n\\(C_{naive}^3(C_1^2, C_2^2) \\rightarrow Major({A_1^1, A_2^1, A_1^2, A_2^2, A_3^2, A_3^3}).\\)\n(7)\nWe introduce this as a baseline to verify that proposed recursive critique outputs new insights rather than just follow simple vote aggregation results."}, {"title": "3.1. Tasks", "content": "We select five representative tasks, which calls for diverse cognitive capabilities, and remain in moderate difficulty. All tasks include 64 multiple-choice questions.\nCET-6 College English Test Band 6 (CET-6) is a standardized English proficiency assessment for Chinese university students. We select one question per passage from its Careful Reading section; each passage is 400-450 words and includes multiple-choice questions testing main idea comprehension, vocabulary understanding, or inference abilities. Given that few annotators have passed CET-6, these questions present a substantial challenge for them.\nGAOKAO Chinese The chinese reading comprehension questions are drawn from China's National College Entrance Examination (Gaokao). These questions evaluate comprehensive reading abilities through textual analysis, logical inference, and meaning interpretation. As our annotators are college graduates who have taken Gaokao before, these questions present moderate difficulty for them.\nGAOKAO Math The mathematics questions are sourced from standardized high school tests in (Zhang et al., 2023). We select first ten multiple-choice problems with moderate difficulty\u00b9, as our annotators left campus for a few years and some have non-science backgrounds.\nKAOGONG The questions are from China's National Civil Service Examination (KAOGONG), the annual government recruitment assessment. These questions include logical reasoning, verbal comprehension, and quantitative analysis. We exclude general knowledge questions to focus on cognitive processing rather than factual recall.\nFigure Reasoning The questions are from the Civil Service Examination as well. These visual task assesses logical abilities through non-verbal reasoning without requiring extra domain knowledge or cultural context.\n1 Question difficulty increases with problem number."}, {"title": "3.2. Setup", "content": "Participants We recruit 32 participants with bachelor's degrees, including 22 with STEM backgrounds and 10 with liberal arts backgrounds. Most participants passed CET-4 level English and achieved approximately 100 scores (out of\n4"}, {"title": "3.3. Critique of Critique can be Easier than Critique", "content": "We validate the hypothesis that critique of critique is easier than critique across five tasks. The results in Table 1 present consistent improvements from response to critique to C2 stages. Take GAOKAO Math as an example, the average accuracy improves from 66.29% (response) to 82.50% (critique) and further to 90.62% (C\u00b2), while completion time remains stable or slightly decreases (e.g., from 18.36 to 15.82 minutes for CET 6). Under comparable effort, majority voting shows similar trends. For example, accuracy improves from 81.81% (response) through 86.61% (critique) to 90.62% (C2) in GAOKAO Math, demonstrating the advantage of higher-order criticism. Compared to naive voting, average accuracy consistently performs better. Take GAOKAO Math as an example, naive voting achieves only 66.41% at critique stage and 81.25% at C\u00b2, significantly lower than average accuracy 90.62%. These results validate that recursive critique generates new insights rather than merely aggregating previous judgments. Moreover, annotator confidence shows steady improvement across stages, suggesting that higher-order criticism becomes more tractable."}, {"title": "3.4. Recursive Critique Remains Consistently Easier", "content": "We extend the recursive criticism to the third-order critique (C3) on two representative tasks. As shown in Table 2, accuracy improves continuously at the C\u00b3 level in both tasks, with CET-6 increasing from 60.94% at C2 to 67.19%, and GAOKAO Math from 90.62% to 93.75%. More im-"}, {"title": "4. Can Recursive Self-Critiquing Enable Human Oversight of AI?", "content": "In this section, we examine whether recursive critique enables effective human oversight when capabilities exceed human performance by asking human to evaluate AI-generated outputs and AI self-criticisms."}, {"title": "4.1. Tasks", "content": "We select tasks that AI demonstrates superior performance than our human annotators' performance. The GAOKAO Math comprises the last two multiple-choice questions from high school mathematics examination (Zhang et al., 2023), while the TEM4 (Test for English Majors Grade Four) include reading comprehension questions. Mathematics questions demand advanced problem-solving skills, and TEM-4 questions require professional-level English proficiency. Both tasks are beyond most annotators' abilities."}, {"title": "4.2. Setup", "content": "We employ the same annotators and evaluation metrics (accuracy, completion time, and confidence scores) as in Human-Human experiments. To generate AI responses, we utilize both Qwen-7B and Qwen-72B models (Qwen et al., 2025) to examine recursive criticism across different AI capability levels. We filter out questions on which models"}, {"title": "4.3. Experimental Results", "content": "Recursive critique enables effective human oversight of AI. Table 3 indicates that humans' response accuracy is lower than that of AI, showing that AI surpasses human in these tasks. However, for the following critique, humans consistently achieve higher accuracy than AI's previous outputs. For example, with Qwen2.5-7B on GAOKAO Math, human accuracy reach 53.12% at the critique stage (7.03% higher than AI's initial 46.09%), and further increases to 56.25% at C\u00b2 (8.59% above AI's critique stage). This finding suggests that recursive criticism enables human supervision even when AI outperforms human for generation.\nRecursive critique improves evaluation efficiency and confidence. Despite processing more information at higher levels, completion time generally decreases or remains stable. For TEM4 with Qwen-72B, time decreases from 22.44 minutes at response level to 15.47 minutes at C2. Meanwhile, annotator confidence shows consistent improvement across levels and model scales, particularly in the mathematics task with Qwen-72B where confidence"}, {"title": "5. Can AI perform Recursive Self-Critiquing?", "content": "To investigate the effectiveness of the Recursive Self-Critiquing protocol on existing AI models, we conduct experiments on a wide range of LLMs."}, {"title": "5.1. Setup", "content": "We utilize multiple reasoning, knowledge, and alignment-related datasets, including MATH (Hendrycks et al., 2021a), GPQA (Rein et al., 2023), TruthfulQA (Lin et al., 2022a), MMLU-Pro (Wang et al., 2024b), and BoolQ (Clark et al., 2019), as detailed in Appendix B.1. We adopt consistent metrics (Accuracy) and baselines (Majority Voting and Naive Voting) as in human experiments. Each score in the experiments is averaged over 10 different runs. Main results is listed in this section and additional results are included in Appendix B.3."}, {"title": "5.2. Experimental Results", "content": "Current AI Models struggle to perform recursive critique tasks. We plot the average imporvement of critique and recursive cirtics over response stage in Figure 2, where we find that the performance of critiques and higher-order critiques remains constrained in most scenarios. In detail, critique, critique of critique (C2) and third-order critiques (C\u00b3) struggle to surpass the accuracy levels achieved at the"}, {"title": "6. Related Works", "content": "Many works have been dedicated to developing protocols that enable weak annotators to supervise strong AI systems.\nDebate This protocol (Irving et al., 2018a) involves two models/humans debating opposing answers. Arguments can be generated synchronously or asynchronously. In certain variants like iteractivate debate, the Judge can interact with the Debaters (Khan et al., 2024). Several studies have analyzed debate as a scalable oversight approach in practical settings. For example, Khan et al. (2024) and Michael et al. (2023) conducted machine and human experiments"}, {"title": "7. Discusssion", "content": "Limitations in Current AI Alignment Methods. RLHF has emerged as the dominant approach in AI alignment, building upon the fundamental principle that \u201cverification is easier than generation\" (Irving et al., 2018b). As illustrated in Figure 3, the optimal RLHF setup requires direct human preferences (Golden RM) for policy optimization. However, the acquisition of real-time human feedback during training presents significant operational challenges. This limitation necessitates the deployment of static reward models as proxies for human judgment. Such reliance on static proxies introduces a fundamental challenge: reward hacking (Gao et al., 2022; Karwowski et al., 2023), optimizing against proxy reward models rather than ideal human preferences can lead to policies that diverge from intended objectives due to Goodhart's Law (Manheim & Garrabrant, 2019; Karwowski et al., 2023; Wen et al., 2024). While approaches such as iterative annotation and tool augmentation (Li et al., 2024a; Gou et al., 2024) provide intermediate solutions, they ultimately encounter the supervision capability limitations. Addressing reward hacking requires reward model capabilities to scale alongside policy model improvements. The recursive criticism framework, while not eliminating reward hacking entirely, offers a promising approach by enabling sustained human oversight even as direct evaluation becomes intractable.\nThe Possible Mechanism of Recursive Self-Critiquing. The effectiveness of recursive self-critiquing in Human-Human and Human-AI experiments likely stems from several key mechanisms. First, higher-order criticism progressively shifts human's attanetion from specific details to more abstract evaluation principles, making the task more tractable even as context length increases. Second, each critique level provides structured context that helps frame subsequent evaluation, allowing critiques to build on previous analyses rather than starting fresh. Third, the recursive structure transforms absolute evaluation tasks into a series of pairwise judgments, leveraging humans' cognitive advantage in relative assessment over absolute evaluation. This aligns with cognitive science findings (Jones & Inglis, 2015; Kelly et al., 2022) that humans often perform better at comparative judgment than absolute evaluation, particularly when provided with structured information.\nImproving AI Critique Capabilities. From AI-AI experiments, our findings suggest several promising directions for training AI systems with recursive critique capabilities. Firstly, models could be explicitly trained to more accurately identify critical errors (Xi et al., 2024), rather than merely aggregating lower-level judgments. Additionally, future work can explore ways to maintain sufficient diversity in AI-generated critiques, perhaps through ensemble methods or explicit diversity-oriented training objectives, to better mirror the effectiveness seen in human recursive self-critiquing experiments. Finally, research into the optimal number of recursive levels and the diminishing returns of additional critique iterations could help establish practical guidelines for implementing recursive critique systems effectively."}, {"title": "8. Conclusion", "content": "This work investigates how to obtain reliable supervision signals when AI capabilities surpass human abilities. Through comprehensive experiments in human-human, human-AI, and AI-AI contexts, we examine the hypotheses that critique of critique is easier than critique and demonstrate that this difficulty relation holds recursively. These insights of recursive self-critiquing mechanisms could be crucial for maintaining effective oversight in scenarios where direct human evaluation becomes infeasible, and suggest a promising pathway for scalable oversight."}, {"title": "Impact Statement", "content": "This recursive self-critiquing framework aims to address challenges in scalable AI oversight, enabling more effective human supervision as AI systems become increasingly capable. Beyond technical contributions, this research promotes responsible AI development by involving more people from different backgrounds in AI supervision. While acknowledging potential limitations, we believe this work helps promote social welfare via contributing scalable oversight mechanisms."}, {"title": "A. Human Experiments Guidelines", "content": "This section details the guidelines and quality assurance of involved in the Human-Human and Human-AI experiments. We establish consistent and comprehensive guidelines for annotation tasks at different stages across different tasks.\nOur guidelines emphasize the quality of reasoning process over accuracy rates, requiring annotators to clearly articulate their thinking process without accessing external references. While accuracy is encouraged, the primary focus is on providing clear, well-reasoned justifications for their decisions. Annotators are instructed to invest their time primarily in analytical thinking, expressing their reasoning in clear, concise, and logically coherent natural language. The guidelines provide suggested formats but maintain flexibility, prioritizing the clear documentation of thought processes over rigid adherence to specific forms2. We provide detailed instruction at each stage in following sections."}, {"title": "A.1. Response Stage", "content": "In the response stage, annotators are presented with a source text, a question, and multiple choice options. The primary task is to select the correct answer and provide comprehensive reasoning for their choice.\nRecommmanded Annotation Template The response should clearly indicate the selected answer and provide a complete reasoning process. This process should include specific citations from the source text as evidence, logical analysis that connects the evidence to the conclusion, and step-by-step reasoning where applicable. For example, responses can follow two primary patterns:\n\u2022 Option B is correct because [evidence + reasoning].\n\u2022 Options A/C/D are incorrect because [evidence + reasoning], therefore B is selected.\nOther patterns are also acceptable as long as they maintain clear reasoning and sufficient evidence support. The examples of high-quality and low-quality responses are provided in Table 7 for illustration.\nQuality Requirements Response annotations must satisfy four fundamental criteria:\n\u2022 Relevance: Direct connection to the question and source text\n\u2022 Organization: Clear logical structure and information flow\n\u2022 Clarity: Concise expression without unnecessary complexity\n\u2022 Coherence: Smooth transitions between reasoning steps"}, {"title": "A.2. Critique Stage Annotation", "content": "In the critique stage, annotators evaluate two responses from the previous stage based on the source text and question. The evaluation should focus on the correctness of responses, examining their logical coherence and evidence support.\nRecommended Annotation Template The critiques should clearly present the final judgment and supporting rationale with referenced evidence cited in the responses or the question. For example, common annotation patterns include:\n\u2022 Agreement with Response 1 with specific justification, noting uncertainties or disagreements with Response 2.\n\u2022 Agreement with Response 1 with justification, identifying specific errors in Response 2.\n\u2022 Agreement with both responses, providing supporting evidence for the shared conclusion.\n\u2022 Disagreement with both responses, detailing specific errors and providing justification for an alternative answer.\nCritiques should prioritize identifying key errors that affect the final judgment, while minor issues that do not impact the conclusion are optional. The high quality and low quality examples is presented in Table 8 and Table 9."}, {"title": "A.3. Higher-Order Critique Stage", "content": "In the higher-order critique stage, annotators evaluate two critique annotations based on the source text, question, and responses. The evaluation should focus on assessing the critiques' reasoning process, examining the validity of their evidence analysis, and identifying any logical gaps or oversights.\nRecommended Annotation Template The higher-order critiques should clearly present their evaluation of both critiques' analyses and provide a final judgment with supporting rationale. For example, common annotation patterns include:\n\u2022\n\u2022 Agreement with Critic 1 with specific justification, noting uncertainties or disagreements with Critic 2.\n\u2022 Agreement with Critic 1 with justification, identifying specific errors in Critic 2's analysis.\n\u2022 Agreement with both critics, acknowledging their shared valid points while noting potential weaknesses.\nDisagreement with both critics, detailing specific logical flaws and providing independent justification.\nCritics should prioritize identifying key errors in the critics' reasoning while noting potential improvements even when agreeing with their conclusions.\nQuality Requirements Higher-order critique annotations must satisfy six fundamental criteria:\n\u2022 Relevance: Direct connection to the question and critics' analyses.\n\u2022 Organization: Clear logical structure and information flow.\n\u2022 Clarity: Concise expression without unnecessary complexity.\n\u2022 Coherence: Smooth transitions between reasoning steps.\n\u2022 Objectivity: Fair analysis of critics' strengths and weaknesses.\n\u2022 Improvement: Identification of gaps or potential enhancements in critics' reasoning."}, {"title": "B. AI-AI Experiment Details", "content": "This section introduces the specific setup of our AI-AI experiments. We employ a unified sampling strategy to evaluate multiple models across different tasks. Detailed explanations are provided in the following sections."}, {"title": "B.1. Tasks Info", "content": "Our experiments are conducted across five types of tasks. Below is a detailed introduction to each task category.\n\u2022 MATH(Hendrycks et al., 2021b) is a mathematical problem-solving dataset consisting of 12,500 challenging competition-level math problems, designed to assess machine learning models' mathematical reasoning abilities. Each problem is accompanied by a fully worked-out step-by-step solution, enabling models to learn how to generate answer derivations and explanations.\n\u2022 GPQA(Rein et al., 2023) is a highly challenging multiple-choice question dataset consisting of 448 questions crafted by domain experts in biology, physics, and chemistry. The dataset is designed to assess the reasoning capabilities of both human experts and state-of-the-art AI models on complex scientific topics. To ensure its difficulty and quality, questions were validated by experts with PhD-level knowledge, achieving an accuracy of only 65% (or 74% after correcting clear retrospective mistakes). In contrast, highly skilled non-expert validators, even with unrestricted web access for over 30 minutes per question, achieved only 34% accuracy.\n\u2022 TruthfulQA(Lin et al., 2022b) evaluates the truthfulness of language models in answering questions, comprising 817 questions across 38 categories, including health, law, finance, and politics. The questions were carefully designed to reflect common human misconceptions or false beliefs, making them particularly challenging. To perform well, models must avoid generating false answers learned from imitating human-written text, which often contains misinformation.\n\u2022 BoolQ(Clark et al., 2019) is a reading comprehension dataset designed to study naturally occurring yes/no questions, meaning questions that arise spontaneously in unprompted and unconstrained settings. The dataset presents unexpected challenges, as its questions often involve complex, non-factoid information and require entailment-like inference rather than simple fact retrieval.\n\u2022 MMLU-Pro(Wang et al., 2024a) is an enhanced version of MMLU designed to go beyond MMLU's primarily knowledge-driven evaluation. MMLU-Pro incorporates more challenging reasoning-focused questions, expands the answer choice set from 4 to 10 options, and removes trivial and noisy questions from MMLU. Experimental results show that MMLU-Pro significantly increases difficulty, leading to an accuracy drop of 16% to 33% compared to MMLU."}, {"title": "B.2. Sampling Strategy", "content": "In the AI-to-AI experiment, to ensure fairness across different stages of effort, we followed the sampling strategy illustrated in Figure 4 to sample the responses of the model to a question and critics of various orders. Each sampling begins by obtaining 7 responses to the same question. From the first two responses, we further derive 5 critics. Similarly, we generate 3 critics of critics and 1 critics of critics of critics. To ensure the reliability of the results, we repeat the entire process 10 times for the same question and report the average outcomes of these ten iterations. To enhance the diversity of the sampling process, we set the sampling temperature to 1 and top-p to 0.95. The prompts used at different sampling stages are listed in the Appendix B.4."}, {"title": "B.3. Additional Results", "content": "In this section, we provide additional results from AI-AI experiments. Tables 12 and 13 include the results from Gemma2 9B Instruct and Qwen-2.5 14B Instruct on MMLU-Pro and BoolQ."}, {"title": "B.4. Generation Prompts", "content": "In this study, we explore whether AI can engage in Self Recursive Critiquing in an inference-only scenario. We employ the structured prompt illustrated in Figure 5, 6, 7, 8, 9 to obtain consistent forms of response, critique, and higher-order critique across different models and datasets. Given the variations in how different models adhere to and comprehend instructions, the prompt structure is slightly adjusted for each model. These adjustments primarily focus on constraints related to output length and the format of the decision-making answers."}]}