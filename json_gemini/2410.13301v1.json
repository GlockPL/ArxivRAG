{"title": "Automating IETF Insights generation with Al", "authors": ["Jaime Jim\u00e9nez"], "abstract": "This paper presents the IETF Insights project, an auto-mated system that streamlines the generation of com-prehensive reports on the activities of the Internet Engineering Task Force (IETF) Working Groups. The systemcollects, consolidates, and analyzes data from variousIETF sources, including meeting minutes, participantlists, drafts and agendas. The core components of thesystem include data preprocessing code and a reportgeneration module that produces high-quality docu-ments in LaTeX or Markdown. By integrating largeLanguage Models (LLMs) for summaries based on thedata as ground truth, the IETF Insights project enhancesthe accessibility and utility of IETF records, providinga valuable overview of the IETF's activities and contri-butions to the community.", "sections": [{"title": "INTRODUCTION", "content": "The Internet Engineering Task Force (IETF), establishedin 1986, is the foremost standards body for the Internet,dedicated to producing influential technical documents.It operates on principles of openness, technical com-petence, and rough consensus, inviting participationfrom all interested parties. Working Groups (WGs) arecentral to the IETF's mission, serving as the primarymeans for developing Internet standards. These groups,guided by charters, focus on specific technical areasand make decisions through rough consensus ratherthan formal voting.\nThe IETF's primary output is a series of technical docu-ments called Request for Comments (RFCs). These RFCs,often developed within WGs, describe the Internet'stechnical foundations, including addressing, routing,transport or security technologies. Example output ofthis process are protocol specifications like TLS [16],QUIC [9], or WebRTC [1]. This structure ensures thatthe IETF's standards reflect the collective expertise andreal-world experience of the global Internet community.\nDuring this process, the IETF produces extensive docu-mentation, such as minutes, participant lists, drafts, andagendas, making it challenging to maintain an overviewof activities. This paper introduces the IETF Insightsproject, an automated system that streamlines reportgeneration on IETF WG activities. By utilizing advanceddata processing and large language models (LLMs), thesystem consolidates and analyzes data, enhancing theaccessibility and utility of IETF records. This projectaims to provide a comprehensive overview of IETF ac-tivities, supporting transparency and informed decision-making [10].\nThe rest of the paper is structured as follows: the In-troduction presents the IETF's mission and the IETFInsights project. The Background explores IETF public"}, {"title": "BACKGROUND", "content": "IETF Public Records\nThe IETF maintains an Open Records policy that makesa wide range of data publicly available. This data in-cludes document-based information such as email mes-sages and meeting minutes, structured data about Work-ing Groups, and raw data like meeting attendance sta-tistics [6].\nThe IETF Mail Archive Website [5] provides access toan extensive archive of all IETF mailing lists, whichcontain discussions, decisions, and historical context ofvarious working groups. This archive offers a search-able interface for users."}, {"title": "Large Language Models (LLMs)", "content": "Large Language Models (LLMs) have revolutionizedNatural Language Processing (NLP) by enabling ma-chines to understand and generate human-like text.These models employ mechanisms to capture long-range dependencies and contextual relationships in text.Pre-trained on vast corpora, LLMs can be fine-tuned forspecific tasks, demonstrating versatility and efficiency.LLMs can be categorized based on their deploymentmethod: those running locally and those accessed viaAPIs. LLMs can be further divided into large and smallmodels based on their parameter size and computa-tional requirements.\nLocal Models provide greater control and privacy, butoften requires quantization to reduce size and compu-tational demands. This quantization can lead to somelimitations in performance and accuracy. Using Ollama,a platform designed for efficient deployment and man-agement of machine learning models [14], we havetested various locally-run LLMs.\nCommercial Models are accessed through APIs andare typically proprietary, meaning they are developedand maintained by specific organizations and are notfully open-source. For example, popular commercialmodels, such as GPT-4o and Claude 3.5 Sonnet, offer ad-vanced capabilities and are optimized for performanceand efficiency."}, {"title": "COMPONENTS", "content": "System Architecture\nThe code has two main components, each composed ofmultiple Python scripts for preprocessing and reportgeneration. These components can be broadly dividedinto data preparation and report generation.\nData Preprocessing This component includes scriptsfor retrieving, consolidating, and preparing IETF data.It extracts data from various sources, cleans it, andorganizes it for further processing. The main task is toconsolidate data from different sources. It uses rsync tosynchronize data, it processes meeting attendance datausing the pandas library for data manipulation and usesfuzzy matching with the fuzzywuzzy library to matchparticipant names, maintaining data consistency evenwith slight variations in affiliation or names. The scriptuses vectorized operations to speed up data processing.\nReport Generation This component is dedicated togenerating detailed reports about the IETF workinggroups during a specific session. It takes the prepareddata and produces comprehensive reports. The reportsare compiled using LaTeX, allowing for quality docu-ment formatting. This component leverages LLMs forgenerating summaries from a vectorized database of"}, {"title": "Workflow", "content": "The typical workflow for generating an IETF compiledreport involves:\n(1) Updating the datastores and indexing with the latestIETF meeting data.\n(2) Running the report generation code with appropri-ate parameters (meeting, output format, workinggroups, etc).\n(3) Generating individual reports for each specifiedworking group.\n(4) Compiling the individual reports into a single LaTeXdocument if LaTeX output is selected.\n(5) Producing the final comprehensive LaTeX document,which can be further processed into PDF format.\nThis implementation allows for flexible, automated gen-eration of detailed IETF working group reports, leverag-ing both structured data analysis and advanced natural language processing capabilities."}, {"title": "DISCUSSION", "content": "We now address some of the objectives posed in the in-troduction based on the analysis conducted. Due to con-straints in time and resources, we have not conducteda statistically significant evaluation for each LLM. In-stead, our conclusions are drawn from our practicalexperiences as developers and users. The experimentsand development were conducted on a MacBok with 64GB RAM and an Apple M3 Max processor.\nDO1: How accurately can LLMs generate struc-tured data and summaries from IETF meeting min-utes, participant lists, and agendas?\nOur findings indicate that smaller LLMs often strug-gle with context size limitations and do not producehigh-quality outputs. These models, while being freelyavailable, lack the capacity to handle complex and ex-tensive input data effectively. We tried modifying thedefault context size in Ollama but still there were quitea lot of hallucinations, chunking the text also meantthat the context provided was not kept in between AIA\nPI interactions."}, {"title": "What are the performance implications ofusing different sizes and types of LLMs for pro-cessing IETF data?", "content": "The performance implications of using different LLMsare significant. Lightweight models like Phi3 [13] offercomparable processing times to API calls to larger mod-els and require fewer computational resources, how-ever they are simply not useful for processing largecontexts. In contrast, larger models like Llama3 [11]provide more accurate and nuanced outputs but at thecost of increased processing time and resource con-sumption, while still not being on par with the largercommercial models.\nOne future approach could be that of delegating somesimple subtasks to local LLMs and more complex ones tothe API models, but this would require a more complexorchestration and data management, which we havenot implemented.\nWhat are the challenges and limitations ofintegrating LLMs into the IETF Insights project,and how can they be mitigated?"}, {"title": "CONCLUSION", "content": "This project aims at showing the potential of leveragingdata processing techniques and LLMs to automate thegeneration of comprehensive reports on IETF WorkingGroup activities. By consolidating and analyzing vastamounts of data from various IETF sources, the systemenhances the accessibility and utility of IETF records,providing valuable insights into the organization's ac-tivities and contributions.\nOur findings indicate that smaller LLMs struggle withcontext limitations and are not reliable enough whilelarger models like GPT-4o and Claude 3.5 Sonnet of-fer sufficient performance in generating accurate andstructured summaries. The primary challenges lied inpreprocessing inconsistent data and managing the inte-gration of multiple data sources."}]}