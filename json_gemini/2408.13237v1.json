{"title": "JacNet: Learning Functions with Structured Jacobians", "authors": ["Jonathan Lorraine", "Safwan Hossain"], "abstract": "Neural networks are trained to learn an approximate mapping from an input domain to a target domain. Incorporating prior knowledge about true mappings is critical to learning a useful approximation. With current architectures, it is challenging to enforce structure on the derivatives of the input-output mapping. We propose to use a neural network to directly learn the Jacobian of the input-output function, which allows easy control of the derivative. We focus on structuring the derivative to allow invertibility and also demonstrate that other useful priors, such as k-Lipschitz, can be enforced. Using this approach, we can learn approximations to simple functions that are guaranteed to be invertible and easily compute the inverse. We also show similar results for 1-Lipschitz functions.", "sections": [{"title": "1. Introduction", "content": "Neural networks (NNs) are the main workhorses of modern machine learning, used to approximate functions in a wide range of domains. Two traits that drive NN's success are (1) they are sufficiently flexible to approximate arbitrary functions, and (2) we can easily structure the output to incorporate certain prior knowledge about the range (e.g., softmax output activation for classification).\nNN flexibility is formalized by showing they are universal function approximators. This means that given a continuous function y on a bounded interval I, a NN approximation \u0177e can satisfy: \u2200x \u2208 I, |y(x) \u2212\u0177o(x)| < 6. Hornik et al. (1989) show that NNs with one hidden layer and non-constant, bounded activation functions are universal approximators. While NNs can achieve point-wise approximations with arbitrary precision, less can be said about their derivatives w.r.t. the input. For example, NNs with step-function activations are flat almost everywhere and can not approximate arbitrary input derivatives yet are still universal approximators."}, {"title": "1.1. Contributions", "content": "\u2022 We propose a method for learning functions by learning their Jacobian, and provide empirical results.\n\u2022 We show how to make our learned function satisfy regularity conditions - invertible, or Lipschitz - by making the Jacobian satisfy regularity conditions\n\u2022 We show how the learned Jacobian can satisfy regularity conditions via appropriate output activations."}, {"title": "2. Background", "content": "This section sets up the standard notation used in \u00a7 4. Our goal is to learn a C\u00b9 function y(x) : X \u2192 Y. Denote $d_x$, $d_y$ as the dimension of X, Y respectively. Here, we assume that x ~ p(x) and y is deterministic. We will learn the function through a NN - \u0177(x) parameterized by weights \u03b8\u2208 \u0398. Also, assume we have a bounded loss function L(y(x),\u0177o(x)), which attains its minimum when y(x) = \u0177o(x). Our population risk is $R(\\theta) = E_{p(x)}[L(y(x), \\hat{y}_{\\theta}(x))]$, and we wish to find $\u03b8^* = argmin_\\theta R(\\theta)$. In practice, we have a finite number of samples from the input distribution $D = \\{(x_i, y_i)\\}_{i = 1 ... n}$, and we minimize the empirical risk:\n$\u03b8^* = \\underset{\\theta}{\\operatorname{argmin}} R(\\theta) = \\underset{\\theta}{\\operatorname{argmin}} 1/n \\sum L(y(x_i), \\hat{y}_{\\theta}(x_i))$"}, {"title": "3. Related Work", "content": "Enforcing derivative conditions: There is existing work on strictly enforcing derivative conditions on learned functions. For example, if we know the function to be k-Lipschitz, one method is weight clipping (Arjovsky et al., 2017). Anil et al. (2018) recently proposed an architecture which learns functions that are guaranteed to be 1-Lipschitz and theoretically capable of learning all such functions. Amos et al. (2017) explore learning scalar functions that are guaranteed to be convex (i.e., the Hessian is positive semi-definite) in their inputs. While these methods guarantee derivative conditions, they can be non-trivial in generalizing to new conditions, limiting expressiveness, or involving expensive projections. Czarnecki et al. (2017) propose a training regime that penalizes the function when it violates higher-order constraints. This does not guarantee the regularity conditions and requires knowing the exact derivative at each sample - however, it is easy to use.\nDifferentiating through integration: Training our proposed model requires back-propagating through a numerical integration procedure. We leverage differentiable numerical integrators provided by Chen et al. (2018) who use it to model the dynamics of different layers of an NN as ordinary differential equations. FFOJRD (Grathwohl et al., 2018) uses this for training as it models layers of a reversible network as an ODE. Our approach differs in that we explicitly learn the Jacobian of our input-output mapping and integrate along arbitrary paths in the input or output domain.\nInvertible Networks: In Behrmann et al. (2018), an invertible residual network is learned with contractive layers, and a numerical fixed point procedure is used to evaluate the inverse. In contrast, we need a non-zero Jacobian determinant and use numerical integration to evaluate the inverse."}, {"title": "4. Theory", "content": "We are motivated by the idea that a function can be learned by combining initial conditions with an approximate Jacobian. Consider a deterministic C\u00b9 function y : X \u2192 Y that we wish to learn. Let $J_X : X \\rightarrow \\mathbb{R}^{d_x \\times d_y}$ be the Jacobian of y w.r.t. x. We can evaluate the target function by evaluating the following line integral with some initial condition (xo, y = y(x)):\n$y(x) = y_0 + \\int_{c(x_0,x)} J_X(x) ds$\nIn practice, this integral is evaluated by parameterizing a path between x\uff61 and x and numerically approximating the integral. Note that the choice of path and initial condition do not affect the result by the fundamental theorem of line integrals. We can write this as an explicit line integral for some path c(t,x,x) from x\uff61 to x parameterized by t\u2208 [0,1] satisfying c(0, x, x) = x, and c(1,x,x) = x with $d/dt(c(t, x_0, x)) = c'(t, x_0, x)$:\n$y(x) = y_0 + \\int_{t=0}^{t=1} J_X(c(t, x_0, x)) c'(t, x_0, x) dt$\nA simple choice of path is c(t,x,x) = (1 - t)x + tx, which has c' (t, xo, x) = x \u2212 xo. Thus, to approximate y(x), we can combine initial conditions with an approximate J. We propose to learn an approximate, Jacobian Je(x) : X \u2192 $\\mathbb{R}^{d_x \\times d_y}$, with a NN parameterized by \u03b8 \u2208 \u0398. For training, consider the following prediction function:\n$y(x) = y_0 + \\int_{t=0}^{t=1} J_\\theta(c(t, x_0, x)) c'(t, x_0, x) dt$\nWe can compute the empirical risk, R, with this prediction function by choosing some initial conditions (x, y) \u2208 D, a path c, and using numerical integration. To backpropagate errors to update our network parameters 0, we must backpropagate through the numerical integration."}, {"title": "4.1. Derivative Conditions for Invertibility", "content": "The inverse function theorem (Spivak, 1965) states that a function is locally invertible if the Jacobian at that point is invertible. Additionally, we can compute the Jacobian of f-1 by computing the inverse of the Jacobian of f. Many non-invertible functions are locally invertible almost everywhere (e.g., y = x\u00b2).\nThe Hadamard global inverse function theorem (Hadamard, 1906), is an example of a global invertibility criterion. It states that a function f : X \u2192 Y is globally invertible if the Jacobian determinant is everywhere non-zero and f is proper. A function is proper if whenever y is compact, f-1(y) is compact. This provides an approach to guarantee global invertibility of a learned function."}, {"title": "4.2. Structuring the Jacobian", "content": "By guaranteeing it has non-zero eigenvalues, we could guarantee that our Jacobian for an $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ mapping is invertible. For example, with a small positive e we could use an output activation of:\n$J_\\theta(x) = J_\\theta(x)J_\\theta(x)^T + \\epsilon I$\nHere, $J_\\theta(x)J_\\theta(x)^T$ is a flexible PSD matrix, while adding el makes it positive definite. A positive definite matrix has strictly positive eigenvalues, which implies invertibility. However, this output activation restricts the set of invertible functions we can learn, as the Jacobian can only have positive eigenvalues. In future work, we wish to explore less restrictive activations while still guaranteeing invertibility.\nMany other regularity conditions on a function can be specified in terms of the derivatives. For example, a function is Lipschitz if the function's derivatives are bounded, which can be done with a k-scaled tanh activation function as our output activation. Alternatively we could learn a complex differentiable function by satisfying $\\frac{\\partial u}{\\partial a} = \\frac{\\partial v}{\\partial b}$, $\\frac{\\partial u}{\\partial b} = -\\frac{\\partial v}{\\partial a}$, where u, v are output components and a, b are input components. We focus on invertibility and Lipschitz because they are common in machine learning."}, {"title": "4.3. Computing the Inverse", "content": "Once the Jacobian is learned, it allows easy computation of f-1 by integrating the inverse Jacobian along a path c(t, yo, y) in the output space, given some initial conditions (xo, y = y(x)):\n$x(y, 0) = x_0 + \\int_{t=0}^{t=1} (J_\\theta(c(t, y_0, y)))^{-1}c'(t, y_0, y) dt$\nIf the computational bottleneck is inverting Je, we propose to learn a matrix which is easily invertible (e.g., Kronecker factors of Je)."}, {"title": "4.4. Backpropagation", "content": "Training the model requires back-propagating through numerical integration. To address this, we consider the recent work of Chen et al. (2018), which provides tools to efficiently back-propagate across numerical integrators. We combine their differentiable integrators on intervals with autograd for our rectification term c'. This provides a differentiable numerical line integrator, which only requires a user to specify a differentiable path and the Jacobian.\nThe integrators allow a user to specify solution tolerance. We propose annealing the tolerance tighter whenever the evaluated loss is lower than our tolerance. In practice, this provides significant computational savings. Additionally, suppose the computational bottleneck is numerical integration. In that case, we may be able to adaptively select initial conditions (x, y) that are near our target, reducing the number of evaluation steps in our integrator."}, {"title": "4.5. Conservation", "content": "When our input domain dimensionality $d_x > 1$, we run into complexities. For simplicity, assume $d_y = 1$, and we are attempting to learn a vector field that is the gradient. Vector fields that are the gradient of a function are known as conservative. Our learned function Je is a vector field but is not necessarily conservative. As such, Je may not be the gradient of any scalar potential function, and the value of our line integral depends on the path choice. Investigating potential problems and solutions to these problems is relegated to future work."}, {"title": "5. Experiments", "content": "In our experiments, we explore learning invertible, and Lipschitz functions with the following setup: Our input and output domains are X = Y = $\\mathbb{R}$. We select L(y1,\u04232) = ||y1 y2||. Our training set consists of 5 points sampled uniformly from [-1,1], while our test set has 100 points sampled uniformly from [-2,2]. The NN architecture is fully connected with a single layer with 64 hidden units, and output activation on our network depends on the gradient regularity condition we want. We used Adam (Kingma & Ba, 2014) to optimize our network with a learning rate of 0.01 and all other parameters at defaults. We use full batch gradient estimates for 50 iterations.\nTo evaluate the function at a point, we use an initial value, parameterize a linear path between the initial and terminal point, and use the numerical integrator from Chen et al. (2018). The path is trivially the interval between x\uff61 and x because $d_x = 1$, and our choice of the initial condition is x = 0. We adaptively alter the integrator's tolerances, starting with loose tolerances and decreasing them by half when the training loss is less than the tolerance, which provides significant gains in training speed.\nWe learn the exponential function y(x) = exp(x) for the invertible function experiment. We use an output activation of $J_\\theta(x) = J_\\theta(x)J_\\theta(x)^T + \\epsilon I$ for \u0454 = 0.0001, which guarantees a non-zero Jacobian determinant. Once trained, we take the learned derivative J\u00f3(x) and compute its inverse, which by the inverse function theorem gives the derivative of the inverse. We compute the inverse of the prediction function by integrating the inverse of the learned derivative. Figure 1 qualitatively explores the learned function, and the top of Figure 3 quantitatively explores the training procedure.\nFor the Lipschitz experiment, we learn the absolute value function y(x) = |x|, a canonical 1-Lipschitz example in Anil et al. (2018). We use an output activation on our NN of $J_\\theta(x) = \\text{tanh}(J_\\theta(x)) \\in [-1,1]$, which guarantees our prediction function is 1-Lipschitz. Figure 2 qualitatively explores the learned function, and the bottom of Figure 3 quantitatively explores the training procedure."}, {"title": "6. Conclusion", "content": "We present a technique to approximate a function by learning its Jacobian and integrating it. This method is useful when guaranteeing the function's Jacobian properties. A few examples of this include learning invertible, Lipschitz, or complex differentiable functions. Small-scale experiments are presented, motivating further exploration to scale up the results. We hope that this work will facilitate domain experts' easy incorporation of a wide variety of Jacobian regularity conditions into their models in the future."}]}