{"title": "DFDT: DYNAMIC FAST DECISION TREE FOR IOT DATA STREAM MINING ON EDGE DEVICES", "authors": ["Afonso Louren\u00e7o", "Jo\u00e3o Rodrigo", "Jo\u00e3o Gama", "Goreti Marreiros"], "abstract": "The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. Ensemble-based solutions improve predictive performance, but incur higher resource consumption, latency, and memory demands. This paper presents DFDT: Dynamic Fast Decision Tree, a novel algorithm designed for energy-efficient memory-constrained data stream mining. DFDT improves hoeffding tree growth efficiency by dynamically adjusting grace periods, tie thresholds, and split evaluations based on incoming data. It incorporates stricter evaluation rules (based on entropy, information gain, and leaf instance count), adaptive expansion modes, and a leaf deactivation mechanism to manage memory, allowing more computation on frequently visited nodes while conserving energy on others. Experiments show that the proposed framework can achieve increased predictive performance (0.43 vs 0.29 ranking) with constrained memory and a fraction of the runtime of VFDT or SVFDT.", "sections": [{"title": "1 Introduction", "content": "The Internet of Things (IoT) connects a vast network of physical devices, generating massive, high-speed data streams. To extract valuable insights from this ever-growing stream of raw sensor data, the edge computing paradigm has emerged as a key enabler for low-latency IoT applications and 5G networks, bridging the gap between cloud services and end-users. However, benefiting from edge solutions not only implies bringing machine learning models for real-time inference, but also continuous updates to adapt to the evolving nature of unbounded streams. Unlike batch learning, where all training data is available upfront, data streams arrive incrementally. Thus, making models susceptible to becoming outdated, especially due to concept drifts, i.e., changes in data distribution over time [6].\nTo circumvent these challenges, many stream mining algorithms have been proposed, with tree-based methods being the state-of-the-art for tabular data sources. Their success can be attributed to approximation-based splitting [4], with incrementally updated statistical summaries of entropy-based metrics, e.g. information gain, to determine whether the observed utility of a split is statistically significant when the data distribution is unknown, e.g. via the Hoeffding bound (HB). To find the split value, for categorical attributes, one can simply do a split for each category, and group them by similarity criteria [4], while for continuous attributes, one can compute the Gaussian for each combination with the target classes and select the highest posterior probability [5].\nMoreover, combining these into ensemble strategies is another effective approach. Upon a concept drift, the diversity among trees can be exploited as some local minima are less affected than others. Affected components can be dynamically deleted, added, and combined with a voting scheme, weighting different weak learners based on their past performance [10, 11]. However, with data stream researchers shifting their focus to ensemble-based solutions prioritizing increased predictive performance, these come at the cost of higher resource consumption, increased latency, and greater memory requirements. This trade-off is particularly problematic in memory-scarce edge environments, making it essential to revisit strategies that allow to control the tree growth efficiency, by delaying the expansion of less confident branches.\nTo address this challenge, a novel algorithm tailored for memory-constrained data stream mining is introduced, coined as DFDT: Dynamic Fast Decision Tree, which consolidates several key contributions from prior research. The algorithm employs dynamic, adaptive strategies for controlling decision tree growth, specifically by adjusting the grace period and tie thresholds during the tree building process, according to the incoming data samples. Moreover, the algorithm incorporates a stricter set of rules based on entropy, information gain, and the number of instances observed at the leaf. To further manage memory constraints, a deactivation mechanism is introduced for low activity leaves, so that the algorithm can grow more organically depending on the distribution and number of instances observed at each leaf of the tree. Thus allowing to spend more computation on the most visited nodes while saving energy on others.\nExperiments conducted on various benchmark datasets, evaluating the accuracy, memory usage, and runtime of DFDT, along with statistical tests to assess significant differences, demonstrate that the proposed adaptive strategies achieve enhanced efficiency (ranking 0.74 vs. 0.65). An ablation study of the algorithm's key components further highlights that omitting adaptive expansion modes with leaf deactivation seems to lead to even superior overall efficiency, by increasing accuracy and reducing computation time, at the cost of memory. The paper is structured as follows: Section 2 reviews contributions from prior research that underpin the proposed algorithm. Section 3 introduces DFDT, detailing its core concepts and pseudocode. Section 4 presents a comprehensive ablation study of the algorithm components, and compares the performance of DFDT with VFDT, svfdt_I, and svfdt_II. Finally, Section 5 outlines conclusions and future research directions."}, {"title": "2 Related work", "content": "The core component for incrementally constructing decision trees (DTs) on edge devices, while maintaining a fixed time complexity per sample, is approximation-based splitting [4]. As new instances arrive, they are processed from the root to a leaf node, updating statistics at each node. These updates are used to periodically adjust heuristic values for each attribute, such as information gain (IG) and Gini index (GI) [4]. The tree attempts to perform splits based on a statistical bound that quantifies the confidence interval for the heuristic function, given a minimum amount of data. Typically, DTs compare the Hoeffding bound (HB) to the difference in evaluation between the best and second-best attribute splits [4]. When this difference exceeds the bound, the leaf node is split into child nodes.\nRules. While traditional incremental DTs exclusively focus on evaluating the top two attributes, one can introduce extra splitting rules to promote even more valuable splits. For example, one can incorporate the fluctuation of the HB, tracking its mean, minimum, and maximum values, along with an accuracy metric, as a pre-pruning condition for split decisions [17]. Alternatively, constraints can be applied that compare current metrics against historical data and cross-leaf information, while introducing a skipping mechanism to bypass these constraints when significant changes in the data are detected [3]. Furthermore, a constraint can be added to assess whether the best-ranked feature, chosen for splitting at a leaf node, provides substantial gains compared to the gains observed in previous splits within the same branch [1].\nTie break. A critical aspect of incremental DTs is the tie-breaking procedure. While using the statistical difference in IG between two attributes helps control tree growth, competition between two equally favored split candidates can hinder progress, especially when either option would be equally suitable. To mitigate this, if the difference in heuristic values exceeds a predefined tie threshold, denoted as T, the split is performed. This threshold effectively controls the minimum rate of tree growth, with the attributes' ability to separate before reaching the threshold influencing the speed at which the tree expands. However, a fixed 7 value may cause ties to be broken prematurely, before a meaningful decision can be made, due to a lack of suitable candidates rather than a true tie situation. To address this, alternative approaches can be employed, such as comparing the difference between the best and second-best candidates with the difference between the second-best and worst candidates [12], or designing an adaptive tie threshold that is dynamically calculated from the mean of HB, which has been shown to be proportionally related to the input stream samples [16].\nGrace period. To prevent premature splits with small sample sizes that undermine the validity of the HB, a grace period $n_{min}$ specifies the minimum number of instances a leaf must observe before considering a split. However, without leveraging any information from the processed data, this can still result in computationally expensive split attempts or unnecessary delays in predictions. An alternative approach involves detecting frequent tie-breaking situations and applying steps to reduce the frequency of ties. For example, the default tie-breaking wait period can be adjusted by increasing the wait period for the child nodes after each tie is broken. This effect is cumulative until a valid split is found, after which the wait period is reset to the default value [12]. Additionally, local statistics can be used to predict the optimal split time, minimizing delays and unnecessary split attempts. For instance, class distributions from previous split attempts can be used to estimate the minimum number of examples required before the HB is met [13]. Alternatively, $n_{min}$ can be adapted after an unsuccessful split attempt, adjusting based on the reasons for failure to ensure a split in the next iteration [8]. For example, if the best attributes are not too similar, but their GI difference is"}, {"title": "3 Methodology", "content": "The proposed DFDT integrates previous research advancements to optimize the trade-off between accuracy and resource efficiency of the original VFDT. These modifications are here described in detail, while referring to the pseudo-code in Algorithm 1. The initialization phase creates the root node structure of the decision tree (Alg. 1, Step 2), sets up estimators (Alg. 1, Step 3), and instance counters (Alg. 1, Step 4). This initialization involves only constant-time operations, resulting in a computational complexity of O(1), independent of the number of instances or features. The main loop iterates over each instance (X, y) in the data stream S, running N times in total. For each instance, several steps are performed. First, the instance is routed through the tree to the corresponding leaf node, where the prediction \u0177 is obtained (Alg. 1, Step 6). Assuming a balanced tree, the depth of traversal is O(logB |LH|), where |LH| is the number of leaf nodes and B is the branching factor. The prediction at the leaf node is a constant-time operation, O(1). Following this, the instance count and feature estimators are updated at the leaf node (Alg. 1, Steps 7, 8). This update takes O(F) time.\nWhile the HB ensures that splits are made with statistical confidence, it treats all nodes equally in terms of expansion, despite the fact that not all nodes contribute equally to the decision-making process. To improve VFDT's efficiency in allocating computational resources, a notion of relative importance can be introduced to dynamically adjust the rate of expansion at each node. To assess the activity level at each node, a fraction parameter is calculated as [9]:\n$fraction = \\frac{(n_{l} \u2013 nleaf_{l}) \u00d7 LH|}{n-n_{Ntree}}$\nwhere (\u03b7\u03b9 \u2013 nleaf\u2081) is the number of instances observed at a particular leaf l since its creation, n \u2013 nNtree the total instances observed by the tree since the creation of leaf l, and LH the current total number of leaves. If the fraction is below 0.2, the leaf node is deactivated, halting further splits to save on resources (Alg. 1, Steps 10), which is a constant-time operation, O(1). If the metric falls above 2, a boolean saves the intention to apply a more aggressive expansion strategy, accelerating the growth of important nodes (Alg. 1, Step 12).\nTo avoid costly evaluations, the algorithm only checks for potential splits once a leaf node accumulates $n_{min}$ instances since the last splitting opportunity (Alg. 1, Step 14). Then a split attempt is performed (Alg. 1, Step 16), with its pseudocode detailed in Algorithm 2. This check involves computing G(\u00b7) values, requiring O(F) time, and sorting the"}, {"title": "4 Experiments", "content": "This section evaluates the performance of the Dynamic Fast Decision Tree (DFDT) against the VFDT, svfdt_I, and svfdt_II, while performing an ablation study on the proposed algorithm. The following notation is used to described the isolated contributions of each individual component of the algorithm: additional splitting conditions (DFDT), adaptive expansion nodes (VFDTE), adaptive tie threshold (VFDTT), and adaptive grace period (VFDTG). With VFDTGT, DFDTGE, DFDTGT, DFDTTE, and DFDTGTE being the respective combinations. All models and algorithm components were implemented using a custom experimental environment built with the pystream library\u00b9, a Python-based tool for data stream mining, inspired by the Massive Online Analysis (MOA) framework. It supports both Python and Cython implementations, with the latter significantly reducing runtime overhead by compiling Python code into C.\nFor evaluation, a diverse set of real-world datasets obtained from the USP Data Stream Repository were used [15]. These are shown in Table 1, encompassing both binary and multiclass classification tasks. The experiments were conducted using a prequential (test-then-train) evaluation strategy [7], monitoring the interrelationships among accuracy, memory usage, and runtime. To provide a more intuitive and holistic assessment, an efficiency metric was computed as the equal-weighted sum of these three components. Each component was normalized on a per-dataset basis to ensure consistency and comparability across datasets of varying scales. To maintain a consistent interpretation, the normalized memory and runtime values were inverted, such that lower values correspond to better performance in these dimensions.\nOverall, DFDTGTE emerges as a well-rounded top performer, excelling in accuracy, memory usage, and runtime,\nwith the exception of the Chess and Luxembourg datasets. This performance discrepancy is likely due to the limited number of instances, which hinders the full effectiveness of the adaptive mechanisms. Indeed, across all experiments conducted on the four smaller datasets, each containing fewer than 2,500 instances, no consistent performance trends were observed across the algorithms. Consequently, the subsequent analysis focuses on the remaining eight larger real-world datasets. To provide a more nuanced perspective on the trade-offs between accuracy and memory usage across algorithms, Figure 1 illustrates the memory usage and accuracy metrics for each dataset.\nAs shown, DFDTGTE maintains consistent high performance, outperforming other algorithms in nearly all datasets, except ForestCoverType. Although it does not achieve the highest accuracy in this dataset, the difference in accuracy is minimal, and DFDTGTE demonstrates significantly better memory efficiency. Across the other datasets, DFDTGTE generally occupies more memory than competing algorithms, reflecting a trade-off for its enhanced accuracy. This balance between memory usage and accuracy suggests that DFDTGTE is optimized for scenarios where accuracy is prioritized, yet it remains reasonably efficient in memory-constrained environments. To further study this claim, an ablation study is performed on DFDTGTE's components, with the obtained accuracy across the largest real-world datasets, is presented in Table 3.\nLooking at Table 3, we see however that despite DFDTGTE having better accuracy than VFDT, svfdt_I and svfdt_II, it seems that exclusively adding adaptive tie threshold VFDTT or also adaptive grace period VFDTGT yields better results. This hints that most of the predictive performance improvement comes from these mechanisms, while the additional splitting constraints and adaptive expansion modes favor memory usage. Inspecting the accuracy evolution of the largest dataset PokerHand provides further light on how accuracy and memory usage evolve as more instances are processed. Figure 2 is organized into three sections: an ablation of the three adaptive mechanisms while using the additional splitting constraints, an ablation of the two adaptive parameters while using the original splitting condition, and a direct comparison between svfdt_I, svfdt_II, and DFDT.\nThe first pair of graphs demonstrates the impact of incorporating the adaptive tie threshold DFDTT. This is particularly evident as more data instances are processed. However, this improvement comes at the cost of increased memory usage growing rapidly. To mitigate this issue, introducing the adaptive grace period DFDTGT helps control tree growth, effectively reducing memory usage while only slightly affecting accuracy. Further enhancements, such as the adaptive expansion modes with leaf deactivation DFDTGTE, yield even more substantial memory savings, however substantially hindering accuracy. The second pair of graphs shows that adaptive tie threshold has the same positive impact on accuracy and memory when using the original splitting condition without the additional constraints. Finally, the third pair of graphs highlights the advancements brought by the proposed algorithm. While svfdt_I and svfdt_II have a more stable performance, they lack the predictive and resource efficiency of DFDT with the four combined mechanisms: additional splitting conditions, adaptive expansion nodes, adaptive tie threshold, and adaptive grace period. For a big picture of this trade-off between accuracy and memory, their Nemeyi critical difference diagrams are shown in Figures 3 and 4, respectively.\nWhile VFDT implementations with adaptive parameters consistently achieve high ranks, above 4, in terms of accuracy,\nthis improved performance comes with a trade-off of increased tree growth, adversely impacting memory usage. Conversely, algorithms using the adaptive tie threshold along with the additional splitting constraints, adaptive expansion modes and/or adaptive grade period have significantly better memory usage. To further understand how the different combinations of components affect this trade-off, Table 4 revisits the aggregate efficiency ranking across real-world datasets."}, {"title": "5 Conclusions", "content": "In this paper, the Dynamic Fast Decision Tree (DFDT) was introduced for memory-constrained data stream mining. DFDT consolidates different adaptive strategies from prior research to control decision tree growth effectively. These features make DFDT particularly suitable for resource-constrained environments, such as IoT devices, edge computing applications, and real-time monitoring systems. Experiments show the proposed consolidated adaptive strategies achieve improved predictive performance (0.43 vs. 0.29 ranking) compared to VFDT and SVFDT, with significantly reduced runtime under 12 diverse datasets.\nDespite its strengths, DFDT has limitations, particularly in handling non-stationary data streams. While it inherits VFDT's robust response to concept drifts through resplitting nodes and approximating information gains [14], it could be further improved by incorporating adaptive strategies, such as memory-conservative alternate subtree growth when a concept drift is detected at a node, using loss-based detection mechanisms like ADWIN [2]. Addressing such limitations could enhance DFDT's adaptability to diverse, rapidly changing environments. Additionally, testing DFDT on a broader range of datasets, including noisy imbalanced high-dimensional data streams, could provide further insights into its robustness."}]}