{"title": "01 Replication Journey: A Strategic Progress Report \u2013 Part 1", "authors": ["Yiwei Qin", "Xuefeng Li", "Haoyang Zou", "Yixiu Liu", "Shijie Xia", "Zhen Huang", "Yixin Ye", "Weizhe Yuan", "Hector Liu", "Yuanzhi Li", "Pengfei Liu"], "abstract": "This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the \u201cjourney learning\u201d paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. We share valuable resources including technical hypotheses and insights, cognitive exploration maps, custom-developed tools, etc at https://github.com/GAIR-NLP/01-Journey.", "sections": [{"title": "1 Chronological Overview of the O1 Exploration Journey", "content": ""}, {"title": "2 Introduction", "content": "The landscape of artificial intelligence research has been dramatically altered by the announcement of OpenAI's O1 model, a purportedly groundbreaking language model capable of complex reasoning tasks. Despite the excitement generated by this announcement, the AI community finds itself in a peculiar position: we know of Ol's existence and its claimed capabilities, but the details of its implementation, training data, and even its complete outputs remain shrouded in mystery. This lack of transparency not only hampers technological progress but also raises important questions about the open nature of scientific advancement in the AI field. It is within this context that our team embarked on the O1 Replication Journey. Our primary goal is not to achieve performance parity with OpenAI's 01 a task we acknowledge as extremely challenging given the limited information and resources available. Instead, our mission is to transparently document and share our exploration process, focusing on the fundamental questions we encounter, uncovering new scientific questions, and sharing our trial-and-error experiences with the broader AI community. By doing so, we aim to reduce the total collective cost of trial-and-error in the world and identify the key factors contributing to Ol's reported success.\nThis report's structure marks a significant departure from traditional scientific publications, addressing key challenges in modern AI research. In an era of prolonged, team-based AI projects, we aim to combat information isolation and researcher burnout through enhanced transparency and real-time feedback. Additionally, this report represents a bold reimagining of AI research methodology. It aims not only to provide a valuable reference for current Ol replication efforts but also to establish a new paradigm for future AI research and broader scientific exploration. Through this innovative approach, we strive not only to achieve technological breakthroughs but also to actively shape a more open, collaborative, and responsible scientific culture. Simultaneously, we are accumulating invaluable learning materials for future AI systems capable of scientific discovery, thus laying the groundwork for the next generation of artificial intelligence in scientific research.\nOur initial explorations have already yielded intriguing insights into the potential mechanisms behind Ol's reported capabilities. A key breakthrough in our research at the current stage is the proposed \u201cjourney learning\" paradigm, which represents a fundamental shift in how we approach model training. This innovative method encourages models to learn not just shortcuts to solutions, but the complete exploration process, including trial and error, reflection, and backtracking (see Figure 3). The power of this approach is evident in its performance: with only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded so far."}, {"title": "3 Why We Created Progress Report?", "content": "In the rapidly evolving landscape of artificial intelligence research, traditional methodologies and reporting practices are increasingly proving inadequate to address the complexities and scale of modern AI projects. This report represents a pioneering effort to reimagine the process of conducting and communicating AI research. By providing a comprehensive, real-time account of our journey to replicate the groundbreaking O1 model, we aim to address critical challenges in contemporary AI research, foster open science, redefine scientific communication, lay the groundwork for AI-driven scientific discovery, and promote responsible AI development. What follows is not merely a documentation of our findings, but a bold proposition for a new paradigm in scientific exploration and collaboration in the AI era.\n1. Addressing the Challenges of Modern AI Research: The rapid evolution of artificial intelligence technologies has ushered in a new era of research paradigms, characterized by prolonged, team-based endeavors that often span six months or more. This shift, while conducive to breakthrough innovations, has inadvertently introduced novel challenges to the scientific process. The inherent insularity of extended team collaborations frequently results in a diminished flow of information to the broader scientific community. Moreover, the protracted nature of these projects often leads to delayed gratification for researchers, potentially fostering anxiety and diminished motivation throughout the research journey. Additionally, the complexity of large-scale team projects complicates the recognition of individual contributions, potentially eroding the traditional academic incentive structures. Our progress report methodology aims to address these emergent challenges by enhancing transparency, facilitating real-time feedback and recognition, and encouraging sustained commitment to long-term research initiatives.\n2. Fostering Open Science and Collective Advancement: In the spirit of open science and collective advancement, the primary impetus behind this report is to disseminate the invaluable insights, resources, and lessons gleaned from our endeavor to replicate the O1 model. This approach transcends the mere sharing of a trained model; it encompasses a comprehensive documentation of the tools, datasets, and methodologies employed throughout our exploratory process. By candidly sharing our setbacks and unsuccessful attempts, we aim to provide educational value that often surpasses that of mere success stories. This transparency is intended to assist other researchers in navigating potential pitfalls, thereby accelerating progress across the"}, {"title": "4 Journey Learning: A New Paradigm Shift from \u201cShortcut Learning\"", "content": "We claim that most existing approaches to machine learning or large language model training (e.g., supervised fine-tuning) can be characterized as \u201cshortcut learning.\" This traditional paradigm, while potentially effective in specific, well-defined tasks, shows significant limitations when faced with complex, dynamic, and open-ended problems. Shortcut learning is defined by several key characteristics: (I) Quick results orientation: It emphasizes achieving specific performance metrics or completing particular tasks in a short time frame. (2) Heavy data dependency: Performance improvements often rely on increasing the volume of training data rather than enhancing the learning algorithms themselves. (3) Limited generalization: Performance can deteriorate dramatically in scenarios outside the distribution of the training data. (4) Lack of self-correction: These systems typically lack the ability to identify and correct their own errors. While shortcut learning has driven many advances in AI, it struggles to produce truly intelligent and reliable AI systems capable of handling the complexities of real-world challenges. As we pursue more advanced forms of artificial intelligence or even superintelligence, the limitations of"}, {"title": "5 Background", "content": "Process-level Reward Model Process reward models (PRMs) are used to provide fine-grained evaluations of responses from LLMs, especially in the area of mathematical reasoning. By accurately assessing the correctness of each step, PRMs can enhance post-training quality and improve accuracy during inference through various search methods. Implementing PRMs can involve using proprietary models with advanced prompting techniques or training with step-level supervision data . The latter approach is challenging because it requires high-quality annotated data. This has led to interest in using reinforcement learning principles, which model the multi-step reasoning process as a Markov Decision Process (MDP) and use techniques like Monte Carlo Tree Search to estimate the value of each step, either online or offline .\nCOT Theory Chain-of-thought (CoT) prompting has significantly advanced the reasoning capabilities of LLMs. Foundational studies demonstrate that providing intermediate reasoning steps enhances performance on complex tasks, such as arithmetic and commonsense reasoning . Additionally, theoretical investigations reveal that CoT empowers decoder-only transformers by enabling inherently serial computation, which is otherwise lacking, particularly in low-depth transformers. Recent studies also reveal CoT prompting enhances LLMs by showing that even constant-sized autoregressive Transformers can solve complex tasks like arithmetic and decision-making through CoT derivations, using circuit complexity theory. Recent work emphasizes the integration of \"error-correction\u201d data into the pretraining stage to enhance reasoning accuracy, showing that such data can lead to higher accuracy without the need for multi-round prompting. Overall, these findings underscore the pivotal role of CoT prompting in enhancing LLM performance and accessibility in complex reasoning tasks.\nInternal Thought The exploration of internal thought in AI models has evolved as researchers emphasize the need for models to reflect on their reasoning and refine their outputs. Early work like STaR proposed bootstrapping reasoning by having models generate rationales that explain their decisions, allowing them to improve their performance on complex tasks through iterative refinement. Building on this, Quiet-STaR generalizes the approach by training language models to generate rationales after each token, helping"}, {"title": "6 Exploration Journey", "content": "This section represents the core of our O1 replication endeavor. This section systematically unfolds our exploration process through a series of pivotal questions, mirroring the complex pathway illustrated in our research timeline diagram. From the initial evaluation of Ol using OlympicArena datasets to the intricate \"Long Thought Construction\" phase, our journey has been marked by multiple attempts, continuous iterations, and deep dives into the essence of Ol's capabilities.\nThe questions we address in this chapter not only reflect the progression of our research but also embody our profound inquiry into the nature of Ol's cognitive processes. We begin by examining the structure of Ol's thoughts, then delve into the mechanics and construction of long thoughts - a concept central to our \"Long Thought Construction\" phase depicted in the diagram. Our exploration extends to the development of reward models, the construction of on-policy reasoning trees, and the integration of these elements into cohesive long thoughts, mirroring the complex web of interconnected processes in our research timeline. Our methodology, as visualized in the diagram, involves multiple iterations and parallel streams of investigation. This approach is reflected in our discussion of evaluation methods and training strategies, showcasing how we validate hypotheses and refine our techniques through cycles of quantitative and qualitative assessments, including human checks and specialized analysis tools.\nBy structuring this chapter around these key questions, we not only provide a clear narrative of our technical journey but also demonstrate a systematic approach to exploring unknown AI technologies. This question-driven format aligns with our \"journey learning\u201d paradigm, emphasizing the importance of the entire learning and exploration process, not just the final outcomes. As we progress through each question, readers will gain insights into our decision-making process, the challenges we faced, and the innovative solutions we developed. This transparent sharing of our thought processes, attempts, and even failures, as illustrated in our research timeline, aims to contribute valuable insights to the AI community and foster collective advancement in the field.\nThrough this section, we invite readers to traverse our exploration journey, understanding not just what we discovered about 01, but how we approached the daunting task of replicating a groundbreaking AI model with limited information. Our journey, marked by curiosity, persistence, and innovation, serves as a testament to the power of open, collaborative AI research in pushing the boundaries of what's possible in artificial intelligence."}, {"title": "6.1 Q1: What does Ol's Thought Look Like?", "content": "The Table 3 is created based on a detailed analysis of Ol's thought examples provided by OpenAI, which includes eight instances of reasoning steps, or \"thoughts,\" for solving complex tasks. Each example in this is meticulously examined to extract relevant features such as the number of tokens, lines, and keyword. These examples are categorized into different problem types, each associated with a difficulty level ranging from simple English reading comprehension to complex multi-step math reasoning tasks. Our analysis demonstrates a trend: as the difficulty increases, the response length (both tokens and lines) tends to grow proportionally. This suggests that higher difficulty problems involve more reasoning steps.\nIn addition to token and line counts, we conducted a keyword frequency analysis to identify recurring terms that may characterize the reasoning process. In addition to commonly observed connective words like \u201cand\u201d and \"so\", our analysis highlights several less frequently occurring but highly significant keywords. Keywords such as \"consider\", \"if\" and \"possible\" appear frequently, often signaling branching in the reasoning process where multiple paths are considered. The frequency of these keywords was notably higher in problems with higher complexity, indicating the model's exploration of different solution paths in these scenarios. Keywords like \"wait\" and \"Alternatively\u201d are crucial indicators of the model's ability to engage in reflection and self-correction. This suggests a deeper understanding and a more nuanced approach to reasoning, as the model is not just following a linear path but is capable of reconsidering and refining its approach based on reflection.\nTo understand the thought process of OpenAI's O1, we consult two PhD candidates from the mathematics department to carefully review the reasoning process employed by OpenAI's O1 in solving mathematical problems. Through their detailed examination, they extracted the underlying thought chain that reflects how O1 approaches and reasons through complex equations. This structured thought graph is illustrated in Figure 5. After these explorations, we determined that the long thought data we need to construct should have the following characteristics:\n\u2022 Iterative Problem-Solving: The model starts by defining functions and gradually explores related expressions, breaking down complex equations into simpler components, reflecting a structured and methodical approach.\n\u2022 Key Thought Indicators: The use of terms like \u201cTherefore\u201d for conclusions, \u201cAlternatively\" for exploring different paths, \u201cWait\u201d for reflection, and \"Let me compute\" for transitioning into calculations highlights the model's reasoning stages."}, {"title": "6.2 Q2: How does Long Thought Work?", "content": "\u2022 Recursive and Reflective Approach: The model frequently reassesses and validates intermediate results, using a recursive structure to ensure consistency, which is typical in rigorous mathematical reasoning.\n\u2022 Exploration of Hypotheses: The model tests different hypotheses, adjusting its approach as it gathers more information, demonstrating flexibility in its reasoning process.\n\u2022 Conclusion and Verification: Finally, the model solves the equations and verifies the results, emphasizing the importance of validating conclusions before finishing.\nThis is a question we consider important. However, at our current stage of progress, we are merely putting forward our hypotheses. We don't believe we have sufficient empirical evidence to verify their accuracy. The remarkable success of Ol's long-thought approach can be attributed to journey learning, which we have introduced in \u00a74. Unlike traditional shortcut learning, journey learning allows the model to explore the entire decision trajectory, mimicking human problem-solving processes. This comprehensive exploration enables Ol to consider multiple solution paths, learn from errors, and understand the complete problem-solving process. By experiencing both correct and incorrect paths, the model develops robust error-handling and self-correction capabilities, enhancing its adaptability to new challenges. This approach fosters a deeper understanding of the problem domain, going beyond merely knowing the correct answer to comprehending why and how to arrive at it. The journey learning process closely simulates human cognitive processes, incorporating trial-and-error, reflection, and adjustment. This results in enhanced explainability, as O1 can provide detailed solution steps and explain its reasoning, including how it recovers from mistakes. Consequently, Ol's long thought process, grounded in journey learning, is not simply about extended computation time but represents a thorough, human-like reasoning exploration. This methodology equips Ol to handle more complex problems, offer more reliable and interpretable answers, and demonstrate greater adaptability when faced with novel challenges, thus explaining its exceptional performance across various tasks."}, {"title": "6.3 Q3: How to Construct Long Thoughts?", "content": "Constructing long thoughts with actions such as reflection and backtracking is the core part of journey learning. To achieve this, we undertook a series of attempts.\nAttempt 1: Tree Search with LLM and Reward Based on our observations of long thought in \u00a76.1, its most prominent feature is the attempt to reflect and backtrack when reasoning leads to an incorrect or unhelpful node. This resembles searching on a reasoning tree for a problem, backtracking at erroneous nodes, until the correct solution path is found. To achieve this, we need to construct a reasoning tree where the root node represents the problem, and each other node represents a reasoning step. The path from the root to any node signifies the reasoning process from the problem to that conclusion. Moreover, backtracking and reflection must be based on incorrect reasoning steps, necessitating a more fine-grained reward model (i.e., process-level) to indicate the correctness of each node in the tree. By executing a search algorithm on a reasoning tree with process-level rewards, we can integrate erroneous steps into a chain of thought, thereby constructing long thought that encompasses actions like backtracking and reflection.\nAttempt 2: Propose-Critique Loop Attempt 1 constructs long thought by executing searches on the tree based on predefined rules, but this limits the freedom of actions like backtracking and reflection. Therefore, we allow the model to choose its current actions. We constructed a Propose-Critique Loop, where we pre-define some possible actions for the model (i.e., continue, backtracking, reflection, terminate) and let the model select actions to build the reasoning tree. If the tree does not reach the final answer, the model can be informed of this negative signal, guiding it to reflect and correct its approach.\nAttempt 3: Multi-Agent Approach Building long thought on the foundation of a reasoning tree presents several challenges, including the presence of numerous ineffective nodes that do not contribute to constructing Long Thought, as well as issues of logical inconsistency caused by reasoning steps that do not depend on the reflection behavior. To address this, we designed an algorithm utilizing multi-agent debate, where one agent acts as the policy model, continuously reasoning, while another agent serves as the critique model, indicating whether the policy model should continue with the current reasoning or perform actions like backtracking. The two agents engage in ongoing dialogue, naturally constructing a long thought dataset when the correct answer is found.\nAttempt 4: Complete Human Thought Process Annotation When humans tackle reasoning problems, they typically do not engage in constant forward reasoning until they either solve the problem or fail; instead, they reflect, backtrack, and rewrite reasoning when they can no longer proceed. This behavior closely aligns with the characteristics of long thought. Thus, we can faithfully and comprehensively document the process by which humans solve reasoning tasks, resulting in high-quality long thought."}, {"title": "6.4 Q4: How to Construct Reward Models?", "content": "The first step in utilizing the reward model is to define the granularity. Instead of focusing solely on the final results, we aim to enhance the capabilities of LLMs specifically in reflection, backtracking, and related cognitive processes. Therefore, we define the evaluation granularity at the step level. Specifically, we use the fine-tuning data from Chern et al. (2023) to make the solutions distinct by line numbers. The process of implementing the reward model can involve using either open-source reward models or proprietary models. We compare the performance of"}, {"title": "6.5 Q5: How to Construct an On-policy Reasoning Tree?", "content": "The construction of a reasoning tree requires a policy model \ud835\udf0b that can perform single-step reasoning. Given a problem q and its corresponding final answer \u03b1, \ud835\udf0b starts from the problem as the root node and continuously adds new nodes to the tree. It first generates \ud835\udc64 possible first-step reasoning steps as child nodes of the root node. Then, it iteratively performs forward reasoning, generating \ud835\udc64 possible subsequent reasoning steps for each current node (e.g., the first-step reasoning) as child nodes of that node. This process is repeated until a preset maximum depth \ud835\udc37 is reached or all leave nodes reach the final answer.\nPolicy Model and Step Segmentation Constructing the reasoning tree requires a clear definition of reasoning steps. To this end, we adopt the data format proposed in Abel , transforming mathematical problem solutions into a form with clear steps, dividing answers into multiple lines, each beginning with a line number and including reasoning within the line. Thus, we fine-tuned DeepSeekMath-7B-Base  using the dataset from Abel to obtain Abel-DSMath, serving as the policy model \ud835\udf0b. The model fine-tuned on this specific format data can conveniently control the generation of individual reasoning steps.\nReward Model and Pruning The tree generation algorithm proposed above is computationally expensive. When setting \ud835\udc64 to 3 and \ud835\udc37 to 10, the last iteration requires generating 3\u00b9\u2070 reasoning steps. Therefore, we use a reward model to prune erroneous reasoning steps, improving operational efficiency. Specifically, we employ beam search, selecting only a small number of candidates for retention in each iteration for the next round. Depending on the reward model used, the details of pruning implementation vary. We attempted two reward models: math-shepherd and 01-mini. Math-shepherd provides a real number between 0 and 1 for each step, representing the probability of correctness for the current step. In each iteration of tree generation, we score all reasoning steps and select the top \ud835\udc3e with the highest scores for the next iteration. This reduces the total generation count from \ud835\udc64 to \ud835\udc5b\ud835\udc3e\ud835\udc37. However, math-shepherd struggles to effectively evaluate reasoning steps for difficult problems, necessitating a more robust reward model that offers high accuracy in correctness indications for each step. Thus, finally, we use o1-mini to provide rewards for each step, directly indicating whether each reasoning step is correct or incorrect. At this point, in each iteration of tree generation, we utilize the rewards from 01-mini and select at most \ud835\udc3e correct reasoning steps for the next iteration."}, {"title": "6.6 Q6: How to Derive a Long Thought from a Reasoning Tree?", "content": "Once the reasoning tree is constructed, our goal is to derive a long thought from the tree that incorporates trial and error. This approach contrasts with traditional methods that focus solely on a shortcut to the correct answer and valid intermediate steps. In our framework, each node of the reasoning tree is annotated with a rating from a reward model that indicates whether the step is correct or incorrect, along with reasoning that justifies this judgment.\nConstructing the ShortCut from a Reasoning Tree We first construct the shortCut from the reasoning tree, which includes only the correct answer and valid intermediate steps. Starting from the root node, which represents a question, we identify a path that leads to a correct answer leaf node. If there are multiple correct answer nodes, multiple correct paths will be established.\nTraversal Path from a Reasoning Tree To derive a long thought, we employ a Depth First Search (DFS) traversal of the tree. This traversal constructs a path in DFS order, documenting each step from the root question node to a correct answer leaf node while including reasoning for any node marked as incorrect. The challenge with DFS lies in its exploration of a vast search space, resulting in numerous trial-and-error paths that may not yield a correct solution. To simplify this initial exploration, we introduce specific constraints to manage the complexity."}, {"title": "6.7 Q7: How to Evaluate our Trials?", "content": "Initially, we mark all nodes in the tree based on whether they lie on the correct path (i.e., the shortCut). The traversal adheres to the following rules: (i). Nodes on the correct path: We allow exploration of child nodes that are not on the correct path. This means that when DFS encounters a node on the correct path, it may explore a child node that leads to an incorrect outcome. Once this node reaches a leaf node and is determined to be incorrect, the algorithm backtracks to continue traversing along the correct path. (ii). Nodes not on the correct path: The traversal randomly selects one child node to explore without branching into trial and error. To further streamline the process, we apply an additional constraint: each node on the correct path is permitted a maximum of \ud835\udc3e trials-one trial on an incorrect path and one on the correct path.\nThese constraints ensure that the DFS traversal focuses on a manageable subset of the search space, allowing for meaningful trial-and-error exploration while avoiding excessive exploration of incorrect paths. In future experiments, we plan to remove or adjust these constraints to investigate the relationship between the length of trial paths and the performance of the final model.\nLong Thought from a Traverse Path With the traversal path generated and reasoning attached to the wrong nodes, we construct a draft long thought by concatenating all steps in the path. This draft incorporates the reasoning for each incorrect step. However, initial experiments using this raw draft to train models have demonstrated suboptimal performance. To address this, we employ GPT-40 to modify the draft. GPT-40 enhances the coherence and smoothness of the thought process while preserving all reasoning steps, including incorrect steps, reflections, and corrections. This approach ensures that the final long thought is not only accurate but also flows naturally, simulating the human problem-solving process with both correct and incorrect steps."}, {"title": "6.8 Q8: How to Train our Models?", "content": "Our experiments utilize the pre-trained language model deepseek-math-7b-base. The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO).\nPhase 1: Supervised Fine-Tuning (SFT) The SFT process consists of two stages: 1. ShortCut Learning: In this initial stage, we focus on fine-tuning the model using responses that include only the correct intermediate steps and the final correct answer. We fine-tune Deepseek-math-7b-base on the Abel dataset, which comprises 120k examples, and the PRM800K dataset . For each question in PRM800K, we utilize a single correct step-by-step solution, discarding responses that do not lead to the final answer. This results in a total of 6,998 examples for fine-tuning. During this stage, we conduct fine-tuning for one epoch on each dataset, primarily aiming to familiarize the model with the desired response format. 2. Journey Learning: In this second stage, we further fine-tune the initial stage SFT model using the long thoughts we constructed, which comprise 327 examples. This phase is designed to enhance the model's ability to detect errors, incorporate reflections, execute corrections, and perform backtracking. By training on long thoughts that include not only the correct reasoning paths but also erroneous trials, we aim to equip the model with a deeper understanding of the complexities involved in longer reasoning chains. As a comparison, we also fine-tune the model on the corresponding shortCut generated from the same reasoning tree, which also consists of 327 examples. Both the long thought SFT and shortCut SFT settings are trained for 3 epochs on these 327 examples.\nPhase 2: Direct Preference Learning (DPO) In this phase, we generate 20 responses per question from the MATH Train dataset, a re-divided dataset from PRM800k that includes 12,000 examples, using nucleus sampling with top_p = 0.95 and temperature T = 0.7. These 20 responses are categorized into positive and negative responses based on the correctness of the final answer. From these, we randomly select 5 positive responses and 5 negative responses to create 5 preference pairs. We then train the model using these preference pairs with DPO loss, allowing it to learn from the comparison of correct and incorrect answers."}, {"title": "6.9 Q9: What Would be an Effective Annotation Strategy for Human-AI Collaboration?", "content": "The results of our experiments are shown in Table 6. All results are tested on the MATH test set, using a re-divided subset from PRM800K, which includes 500 examples. The results show that Journey Learning led to significant improvements compared to Shortcut Learning, with gains of +8.4 and +8.0 on the deepseek-sft-abel and deepseek-sft-prm800k models, respectively, demonstrating the effectiveness of our proposed Journey Learning method. However, the improvement from DPO was more modest, and we acknowledge that this is an initial exploratory result. In future experiments, we plan to further explore preference learning and Reinforcement Learning (RL) techniques. This will include, but not be limited to, iterative self-improvement, incorporating process-level reward models, and transitioning from outcome-level DPO to process-level DPO/RL approaches.\nWe have developed a human-AI pipeline designed to generate high-quality, long-form reasoning data for problems derived from the MATH dataset. This pipeline enables the expansion of a human-annotated solution of several lines into thousands of tokens, which follows our \u201cjourney learning\" paradigm. During the pipeline's construction, we identified key techniques for efficient annotation, including:\nComplete Thought Process It is not essential for annotators to record every word that comes to mind in detail, but it is crucial to document each trial, reflection, association, and correction. These diverging cognitive pathways may not always be explicitly expressed or consciously recognized in everyday thinking. Nevertheless, capturing shifts in thought, along with the reasons behind these shifts, is critical. This ability to navigate and understand cognitive transitions is a core skill that large language models must learn from our data.\nAdditional Explanation for Common Sense Humans often omit information that can be inferred from context, such as references to previously mentioned formulas or the application of well-known theories. However, this can lead to hallucination when large language models attempt to interpret human annotations. Therefore, high-quality data must include explicit explanations of common-sense knowledge to prevent misinterpretation by LLMs.\nWith the essential components outlined previously, the concise yet precise annotated data is fully generated by human effort. The next stage involves AI-driven processes. By designing sophisticated prompts, we implement data augmentation by LLMs in aspects below:\n1. Enhancement of Data Granularity The prompt emphasizes breaking down the problem-solving process into finer, smaller steps. By splitting the process into fine-grained, easily digestible chunks, it becomes easier for LLMs to grasp and internalize each concept before moving on to the next. This ensures deeper comprehension at every stage.\n2. Gradual Reasoning LLMs are required to frequent pause, reflect on known information or to clarify the next step should be added to help guide reasoning. Taking pauses in reasoning mimics how students would naturally think about the problem, helping them stay engaged and connected to the reasoning process rather than passively following instructions.\n3. Student-Explorer Perspective Instead of presenting the solution as if the answer is already known, LLMs are encouraged using a tone of discovery, where the they solving the problem is thinking through it for the first time. This fosters curiosity and encourages students to think critically, making them feel like they are part of the learning process rather than simply receiving information."}, {"title": "7 Detailed Event Explanation of Our Research Exploration", "content": ""}, {"title": "8 Future Plan", "content": "As our O1 Replication Journey continues to evolve, our future plans are shaped by the insights gained and challenges encountered thus far. Drawing from our research timeline and the progress we've made, we've identified several key areas for future exploration and development:\n1. Scaling Up Long Thought Integration: Building on our successful iterations of long thought integration, we plan to conduct a third round of integration, as indicated in our research diagram. This will involve scaling up our processes to handle more complex and diverse thought patterns, potentially uncovering new dimensions of Ol's capabilities.\n2. Experiments on Long Thought Scaling Laws: Our diagram highlights planned experiments on long thought scaling laws. This research stream aims to understand how the performance and capabilities of our model scale with increases in data, model size, and computational resources. These insights will be crucial for optimizing our approach and potentially discovering fundamental principles underlying advanced AI systems.\n3. Fine-Grained, Thought-Centric Evaluation: We plan to develop and implement more sophisticated evaluation methodologies, focusing on fine-grained, thought-centric assessment. This approach, highlighted in our research timeline, will allow us to more accurately measure the quality and coherence of the generated long thoughts, providing deeper insights into our model's reasoning capabilities.\n4. Human-AI Collaboration for Quality Thought: A key component of our future plan, as shown in the diagram, is to explore and enhance human-AI collaboration for producing high-quality thoughts. This involves developing interfaces and methodologies that leverage the strengths of both human intelligence and AI capabilities, potentially leading to breakthroughs in hybrid intelligence systems.\n5. Continued Improvement of Reward and Critique Models: Building on our process-level reward model and critique model setup, we aim to refine these systems further. This ongoing process will involve iterative improvements to better capture the nuances of human-like reasoning and problem-solving strategies.\n6. Advanced Integration of Reasoning Trees: We plan to explore more sophisticated methods of deriving and integrating long thoughts from our reasoning trees. This will involve developing advanced algorithms for traversing and synthesizing information from these complex structures.\n7. Expansion of Training Methodologies: Our future plans include further experimentation with and refinement of our training pipeline. This encompasses enhancements to our pre-training, iterative training, reinforcement learning, preference learning, and DPO (Direct Preference Optimization) stages, as outlined in our research diagram.\n8. Continued Transparency and Resource Sharing: In line with our commitment to open science, we will continue to share resources, insights, and tools developed throughout our journey. This ongoing practice, represented by the resource-sharing icons in our diagram, aims to foster collaboration and accelerate progress in the wider AI research community.\n9. Exploration of Multi-Agent Approaches: Building on our initial attempts with multi-agent systems, we plan to delve deeper into this area, potentially uncovering new ways to model complex reasoning and decision-making processes.\n10. Refinement of Analysis Tools: We aim to further develop and enhance our analysis tools, as indicated in our research timeline. These tools will be crucial for interpreting model outputs, tracking progress, and guiding future research directions.\nBy pursuing these avenues, we aim to not only advance our understanding and replication of Ol's capabilities but also to push the boundaries of AI research methodologies. Our future plans reflect our commitment to the journey learning paradigm, emphasizing continuous improvement, transparent exploration, and collaborative advancement in the field of artificial intelligence. As we move forward, we remain adaptable to new discoveries and challenges, ready to adjust our plans as our understanding of O1 and advanced AI systems continues to evolve. Through this ongoing journey, we hope to contribute significantly to the development of more capable, interpretable, and ethically aligned AI systems."}]}