{"title": "Time-Reversal Provides\nUnsupervised Feedback to LLMs", "authors": ["Varun Yerram", "Rahul Madhavan", "Sravanti Addepalli", "Arun Suggala", "Karthikeyan Shanmugam", "Prateek Jain"], "abstract": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback. Moti-\nvated by this, we explore the question of whether LLMs can be empowered to think\n(predict and score) backwards to provide unsupervised feedback that complements\nforward LLMs. Towards this, we introduce Time Reversed Language Models\n(TRLMS), which can score and generate queries when conditioned on responses,\neffectively functioning in the reverse direction of time. Further, to effectively infer\nin the response to query direction, we pre-train and fine-tune a language model\n(TRLM-Ba) in the reverse token order from scratch. We show empirically (and\ntheoretically in a stylized setting) that time-reversed models can indeed comple-\nment forward model predictions when used to score the query given response for\nre-ranking multiple forward generations. We obtain up to 5% improvement on the\nwidely used AlpacaEval Leaderboard over the competent baseline of best-of-N\nre-ranking using self log-perplexity scores. We further show that TRLM scoring\noutperforms conventional forward scoring of response given query, resulting in\nsignificant gains in applications such as citation generation and passage retrieval.\nWe next leverage the generative ability of TRLM to augment or provide unsupervised\nfeedback to input safety filters of LLMs, demonstrating a drastic reduction in false\nnegative rate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) trained on a large corpora of text are able to accomplish a wide\nvariety of downstream tasks such as summarization, open-ended/ context-based question answering,\ndocument retrieval, and citation generation [Brown et al., 2020, Zhao et al., 2023a]. While the gener-\nations from pre-trained and instruction-tuned models already show significant promise, alignment\ntechniques such as Reinforcement Learning via Human Feedback (RLHF) [Anil et al., 2023a, Ouyang\net al., 2022] are widely used to improve the quality of their generations further. However, these\nmethods rely heavily on additional supervision to construct preference data, which can be expensive\nto acquire, or noisy for training. This brings up a natural question \u2013 Can we generate useful feedback\non LLM generations without additional supervised data?"}, {"title": "2 Related Work", "content": "Reverse Direction in Language Modeling: Classical work [Serdyuk et al., 2017] showed how\nsequence to sequence models can regularize the current word token embedding based on the ability\nof the future tokens to be able to predict the current token. Such bi-directional (forward and reverse)"}, {"title": "3 TRLM - Time Reversed Language Models", "content": "We introduce our primary contribution - TRLM (Time Reversed Language Models), a class of language\nmodels that operate in the response \u2192 query direction during scoring and generation. This is\nachieved by either (a) [TRLM-Ba ] reversing the token order and effectively utilizing previous token\nprediction instead of next token prediction during pre-training, scoring, and generation, or (b)"}, {"title": "4 Scoring in Reverse", "content": "In this section, we provide formal results on TRLM and the benefit of using pre-training in the reverse\ndirection. Let us denote by $P_{FW}(A|Q)$ the conditional distribution of a forward LLM. Similarly,\ndenote $P_{TRLM}(Q|A)$ to be the conditional distribution of the Time Reversed Language Model. For\nsimplicity, we merge the instruction and question together."}, {"title": "4.1 Formal Results on Reverse LLM based Alignment", "content": "In this subsection, we focus on the distribution shift encountered while using a reverse model based\nscorer on forward generations.\nSpecifically, we conclude that while reranking using Forward Baseline is equivalent to temperature\nscaling [Yang et al., 2024b], reranking using TRLM induces a distribution shift that is not equivalent to\ntemperature scaling.\nConsider the alignment problem of learning a new forward LLM - $P_{Fw}(Answer | Question)$. A very\npopular framework is the KL constrained optimization objective with respect to a reward oracle\n$R(Question, Answer)$, for some threshold \u0394:\n$\\max_{P_{FW}} E_{\\substack{Question\\sim\\Omega, \\text{Answer} \\sim P_{FW} (Answer | Question)}} [R(Question, Answer)] \\text{ s.t. } D_{KL}(P_{FW}||P_{FW}) \\le \\triangle$                                                                                                                             (1)\nLog-perplexity of the forward model used as reward: In general, for long form question answering\nwhere an explicit reward model is not available, a typical method is to use log-perplexity of the\nforward model i.e. log $P_{Fw}$ as a reward. Then, we have the following corollary of Lemma 1 in Yang\net al. [2024b],\nLemma 1 (Corollary of Lemma 1 in Yang et al. [2024b]). The new LLM policy $P_{Fw}$ that optimizes (1)\nis given by: $P_{Fw}^{*}(Answer|Question) \\propto P_{FW}^{1+\\alpha}(Answer|Question)$ where \u03b1 is chosen appropriately\ndepending on the threshold \u25b3 when reward $R(\u00b7)$ is set to log perplexity of the forward model $P_{FW}$.\nA policy obtained post the constrained KL-alignment procedure is akin to temperature re-scaled\nforward model, since $p^{1+\u03b1}$ is equivalent to temperature rescaling exp$(1+\u03b1)$ log p.\nLog-perplexity of the TRLM-Ba.score used as reward: Suppose $R(\u00b7)$ is set to output of\nTRLM-Ba.score computed on the the question given the answer, then we have:\nLemma 2 (Corollary of Lemma 1 in Yang et al. [2024b]). The new LLM policy $P_{Fw}$ that optimizes\n(1) is given by: $P_{Fw}(Answer|Question) \\propto P^{1+\\alpha}_{FW}(Answer|Question)P^{\\alpha}_{TRLM-Ba}(Question|Answer)$\nwhere \u03b1 is chosen appropriately depending on A when reward $R(\u00b7)$ is set to log perplexity of the\nreverse model $P_{TRLM}$.\nOptimal distribution after alignment using TRLM scores results in a non-trivial distribution that is not\nsimply temperature re-scaling. While we have not used TRLM for alignment using KL constraints in\nour experiments, the distribution shift that is induced by reverse token training is indeed non-trivial\neven with Best-of-N-re-ranking, which we adopt in our experiments."}, {"title": "5 Experimental Results", "content": "In this section, we explore the effectiveness of time reversed language models on different downstream\ntasks, by utilizing unsupervised feedback to improve upon existing forward model generations.\nBroadly, these applications fall into two categories - first, where we utilize the scoring capacity of\nTRLM (three use cases), and second where we utilize the generative capacity of TRLM for generating\nqueries given a response."}, {"title": "5.1 Best-of-N reranking", "content": "The best-of-N reranking task involves outputting the best response out of N model responses to a\nuser query.\nSpecifically, given N LLM outputs to a user query, a reranking algorithm finds the best response\nbased on scalar scores assigned to each response. Prior works [Rafailov et al., 2023, Mudgal et al.,\n2023a] aim to improve LLM performance on this task by using feedback-based RLHF algorithms and\ntraining on KL-regularized alignment objectives. Yang et al. [2024a] show that best-of-N reranking is\nthe most effective way to approximate these RL objectives, and further, it is empirically observed to\noutperform them.\nIn this work, we consider several best-of-N reranking based algorithms based on TRLM.Score, for\nevaluating a base model response. The methods considered rely on nothing more than the pre-training\n(or instruction-tuning) corpus to achieve alignment of response to the user query. We further note that\nsuch scores from TRLM may be used within RL objectives as well, but we leave the exploration of\nsuch rewards to future work."}, {"title": "5.1.1 Alpaca Leaderboard Evaluation", "content": "Benchmark and Evaluation: The AlpacaEval leaderboard [Dubois et al., 2024] is a widely used\nbenchmark to evaluate the capability of language models. In this benchmark, there are 805 questions\nfrom the AlpacaFarm evaluation set \u2013 consisting of questions ranging from general writing, chat\nability, and reasoning to general knowledge. The goal is to output a response that is better than a base\nmodel's response, as judged by an annotator model. Both base model and annotator model are set as\nGPT4-1106-Preview on the AlpacaEval leaderboard as on May 10, 2024, and hence we use the\nsame for our evaluations. The evaluation benchmark computes various metrics including winrates,\ndiscrete winrates and length-controlled winrates [Dubois et al., 2024]. The length-controlled winrates\nare calculated using a debiasing algorithm that removes the length bias that is otherwise preferred by\nGPT4-1106-Preview.\nFormally, we define the task for TRLM as follows Given a query Q from the dataset and N model re-\nsponses A = {$A_{1}... A_{N}$} from a generator model, we wish to use TRLM. score to output the highest\nscoring response ai \u2208 A, which is further evaluated against an answer from GPT4-1106-Preview.\nIn our experiment, we consider outputs from a generator model that is Gemini-Pro-1.0 [Anil et al.,\n2023a]. We generate 16 responses using a temperature r = 0.8 to ensure diversity of answers. We\nthen rerank the responses using different variants of TRLM from the PALM2-Otter family of models\n(TRLM training details in the supplement). We further consider two baselines, Self scoring and\nForward Baselines, as described in Table 1. Scoring prompts and Conditioning prompts used\nwith various TRLM variants for this task are described in the Table 7 of Appendix C.1.\nDiscussion of Results: In Table 2, we see that TRLM-Ba scores the highest length controlled win\nrate which is 5% over the self scoring baseline of Gemini-Pro-1.0 with 16 generations against\nthe GPT4-1106-Preview judge. Further, it registers an 8% increase over the reported number for\nsingle generations in the benchmark leaderboard. We note that scoring Response->Query seems to\nbring out some improvements as TRLM-Fo improves over Forward Baseline. Further, TRLM-Ba\noutperforms TRLM-Fo indicating the impact of reverse token pre-training. This demonstrates that time\nreversed scoring provides an intrinsic unsupervised feedback that could help improve the performance\nof even larger capacity models. We note that pre-training in both forward and reverse directions\n(TRLM-FoBa models) and scoring in the reverse direction is better than TRLM-Fo variant.\nWe present further results where the generations of a Mixtral model [Jiang et al., 2024b] are\nreranked and compared against GPT4-1106-Preview, and the generations of a smaller Mixtral\nmodel are reranked and compared against a larger Mixtral model. These results are presented\nin the Appendix C.2. We note a 4% improvement over Forward Baseline with the proposed\nTRLM-Ba.Score method of reranking.\nKey Takeaway: Through empirical justifications, we show that TRLM variant models can be\nused as effective re-rankers of generations from multiple classes of models (Gemini-Pro-1.0,\nMixtral8x22B, Mixtral8x7B), and improve the instruction following capability of the model as a\nwhole. This is consistent with Theorem 1 considering the fact that we outperform generation model's"}, {"title": "5.2 Citation Attribution", "content": "In this section, we describe applications of reverse scoring to the task of producing citations to\noriginal passages that can corroborate the sentences in an already produced summary. Summaries\nare created from long form articles, and one often wants to know which part of the article a given\nsummary sentence is derived from (Cohen-Wang et al. [2024]).\nDataset and Evaluation: For this task, we take the CNN Daily Mail Dataset [CNN] which consists\nof pairs of news articles and their respective highlights. Our goal is to identify which sentence (or\ngroups of sentences) within a given news article provides the most direct corroboration for a specific\narticle highlight given as a query. We evaluate the attributed citations using various relevancy metrics.\nWe use cosine similarity on the embeddings of the Gecko model [Lee et al., 2024], cosine similarity\non TF-IDF features, BLEU score and ROUGE score to compute metrics. We score and choose the best\npairing using all the models from the TRLM PALM2-Otter family trained in the forward, reverse and\nforward-reverse directions as outlined in Section 5.1.1.\nAlgorithms: Different search algorithms, Linear Search, Binary Search and Exclusion\nSearch are coupled with using TRLM.score to find the attribution. We outline these in Algo-\nrithms 7, 8 and 9 along with details in the supplement. The number of inference calls is O(log N)\nwhere N is the number of article sentences for Binary Search, and this method produces multiple\nsentences as a citation. The other methods require O(N) calls to produce the citation for a sentence.\nOur results shown in Table 3, demonstrate the efficacy of TRLM for the attribution task. Specifically,\nwe show 44% gains over the baseline in the linear search method, 39% gains in the binary search\nmethod and 34% gains in the exclusion search method as measured through gecko cosine similarity.\nKey Takeaway: Through our results on CNN-Daily Summarization dataset we present multiple\nmethods of citation attribution and demonstrate significant gains with TRLM model variants. We note\nthat a direction of low information to high information (summary \u2013> article) is harder to reason\nupon and select among a given set of texts. Further, we highlight the importance of binary selection\nbased approach over log-perplexity based exclusion based search. We show 9% improvement using\nTRLM-Ba on Gecko embedding-based metric using only O(log N) inference calls to the main model."}, {"title": "5.3 Document Retrieval", "content": "In this section, we study the performance of TRLM in retrieving relevant passages from a corpus to\nanswer a specific question. Our goal is to show the efficacy of TRLM based reverse scoring over doing\nit in the forward direction. The task is as follows: Given a question, the goal is to retrieve relevant\ndocuments from the given corpus. We retrieve k documents from the corpus and compute various\ninformation-retrieval metrics to calculate performance w.r.t. the golden set of documents.\nWe experiment with two retrieval-based datasets from MTEB benchmark [Muennighoff et al., 2023]\nas shown in Table 4. Metrics are precision, recall and normalized discounted cumulative gain\n(NDCG) (details in Appendix E.1). We show our results in Table 5. TRLM reverse scoring algorithms"}, {"title": "5.4 Defending against Jailbreak attacks", "content": "We next aim to leverage the generative ability of TRLM to augment toxicity filters that are used to\nimprove the safety of LLMs. Prior works show that LLMs (and their input filters) can be jailbroken\nusing crafted adversarial attacks [Zou et al., 2023], while output filters tend to have a high false\nnegative rate due to the sensitivity to the presence of toxic words, despite being in a neutral context\n(See Table-10). We propose to combine the benefits of input and output filters by projecting the"}, {"title": "6 Conclusions", "content": "In this work, we explore the capabilities of TRLM for scoring and generation of queries, when\nconditioned on responses. Our study points to the importance of the response \u2192 query direction\nin LLMs. When deploying TRLM models for reverse scoring, we show improvements on AlpacaEval\nleaderboard, Citation attribution and retrieval tasks. We further show that generations from TRLM can\naugment safety filters effectively."}, {"title": "7 Limitations", "content": "We note that the assumptions made for our theoretical results in Section 4 are stylized, and may\nnot hold true in practice, as the space of all answers to questions may not be adequately captured\nby assumptions in that section. Given this assumption, one may wish to explore other models for\nhallucination that are more general and provide results about reverse scoring. We leave such a\ntheoretical exploration to future work.\nFurther, TRLM benefits have thus far been explored on tasks related to short form queries that have\nlong answers. One may wish to understand and demonstrate the effects of reverse scoring on other\ntasks. For instance, one might pose the question \u2013 does TRLM provide possible benefits for a broader\nset of tasks that language models are used for. We leave the exploration of such settings in which the\nreverse scoring direction of response \u2192 query is better than the forward scoring direction, along\nwith obtaining an understanding on the reason behind such an advantage, as part of future work."}, {"title": "A Results on a Bipartite Graph Model for Questions and Answers", "content": "In this section, we outline a simple toy model involving a universe of questions and answers with\nrelations between them where we show how TRLM-Ba perplexity based alignment distribution helps\nin picking the right answer when the forward model \"hallucinates\". For simplicity of exposition, we\nwill only focus on the distribution $P_{TRLM-Ba} (Q|A)$ for the TRLM class of models.\nUniverse of Questions and Answers: We consider a universe of questions and answers in the form\nof a bi-partite graph which are deemed to constitute the ground truth. Let Q \u2286 $V_{K}$ and A \u2286 $V_{K}$\nwhere $V$ is the vocabulary, be the universe of questions and answers respectively. For a given question\nQ, let $N(Q) \u2208 A$ denote the set of ground truth answers of Q. Let G(Q, A, E) be a bipartite graph\nsuch that E = {(Q, A)}$_{Q\u2208Q,A\u2208N(Q)}$ is the edge set of all valid answers. In other words, Ideally,\none may like a forward model to approximate the distribution, P(A|Q) = 1/|$N(Q)$|, A\u2208 N(Q)\nand 0 otherwise, closely.\nHallucination Model (Hamming distance version): We would like to model an imperfect forward\nmodel that does not fully adhere with the ideal ground truth forward model. For a given question Q, the\nimperfect model produces answers $N(Q')$ to the neighbouring questions Q' which are at a hamming\ndistance of 1 from Q. Concretely, let H(,) denote the hamming distance function. The support of\nthe answer distribution is then S = $\\bigcup_{Q':H(Q,Q')\u22641} N(Q')$. It follows immediately that $P_{Fw}(A|Q)$ =$\n$\\frac{1}{\\sum_{Q':H(Q,Q')\u22641}} 1_{A\u2208N(Q')}/|S|$. Analogously, for a given answer A, let S' =$\\bigcup_{\u0391':H(A,A')\u22641} N(A')$. Then\nfor TRLM-Ba we have $P_{TRLM-Ba} (Q|A) = \\sum_{\u0391':H(A,A')\u22641} \\frac{1}{N(\u0391')/|S'|}$.\nTheorem 1. Let us assume the hallucination model above. Assume that for two questions Q, Q' :\nH(Q, Q') \u2265 1, $ \\underset{(A,A')\u2208N(Q)\u00d7N(Q')}{\\text{min}} H(A, A') > 1$, then the optimal alignment distribution when\n$P_{TRLM-Ba}(\u00b7)$ is used a scoring model (i.e. distribution in Lemma 2) has the support N(Q) for Q.\nTheorem 1. From Lemma 2, we have that\n$P_{FW}(A|Q) \\propto P^{1+\\alpha}_{FW}(A|Q)P^{\\alpha}_{TRLM}(Q|A)$\n(2)\nfor some \u03b1 > 0. For a fixed question Q, left hand side is potentially non-zero only for A \u2208 N(Q') :\nH(Q, Q') \u2264 1. since the first term in the right hand side is non-zero only for those by definition of the\nhallucination model. Consider an A such that \u2203Q\u2032 : A \u2208 N (Q'), H(Q, Q') = 1. We will argue that\nthe second term is zero for such an answer A. Suppose it is non-zero, according to the hallucination\nmodel for the reverse direction, it means that \u2203A\u2032 : H(A, A') = 1, A' \u2208 N(Q). However Q and Q'\nare hamming distance one away. From the assumptions, their neighborhood are far apart by more\nthan 1, therefore contradicting the implication that H(A, A') = 1.\n\u03a0\nKey Takeaway: Therefore under the above simplistic hallucination model, although the forward\nmodel has a wider support |S| in the answer space, due to alignment with TRLM-Ba 's perplexity, the\nnew distribution has a support of at most N(Q) provably. While assumptions in the theorem are\nnot reflective of true complexities of the universe of questions and answers in a domain, this simple\nmodel shows that alignment using TRLM's scoring metric can give rise to better re-ranking whenever\nnearby questions produce far away answers and generating forward models tends to confuse between\nnearby questions (a form of hallucination)."}, {"title": "B TRLM Subroutines - Score, Generate and Pretrain", "content": "In this section, we provide the subroutines of our TRLM models as described in Section 3."}, {"title": "C Details on the Experimental Section", "content": "We describe details about our experiments in the following figure 1:"}, {"title": "D Details on the Citation Task", "content": null}, {"title": "E Details on the Retrieval Tasks", "content": null}, {"title": "F Details on our Defence Task: Defending against Jailbreak attacks", "content": null}, {"title": "F.1 Datasets used in the Defence Task", "content": "JBB Dataset: We form the union of all question-answer pairs that have been generated by various\nattack strategies in JailbreakBench. There are a total of 1037 question and answer pairs of which"}, {"title": "F.2 GPT4 prompt used as a toxicity classifier", "content": "We prompt GPT4-1106-Preview with the following prompt to verify the toxicity of the question\nand answer."}, {"title": "F.3 Algorithm for Question Generation for Defense", "content": null}, {"title": "F.4 Additional Tables relating Jailbreak Defense", "content": null}, {"title": "G Compute Requirements:", "content": "To pre-train TRLM models we use two TPUv5e pods [Cloud] for two weeks in the setup described by Anil et al.\n[2023b]. Further details on pre-training are provided in Appendix B. We run fine-tuning on FLAN-dataset using\na TPUv5e pod [Cloud] for 1 day."}]}