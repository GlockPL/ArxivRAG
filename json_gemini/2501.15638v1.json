{"title": "A Comprehensive Survey on Self-Interpretable Neural Networks", "authors": ["Yang Ji", "Ying Sun", "Yuting Zhang", "Zhigaoyuan Wang", "Yuanxin Zhuang", "Zheng Gong", "Dazhong Shen", "Chuan Qin", "Hengshu Zhu", "Hui Xiong"], "abstract": "Neural networks have achieved remarkable success across various fields. However, the lack of interpretability limits their practical use, particularly in critical decision-making scenarios. Post-hoc interpretability, which provides explanations for pre-trained models, is often at risk of robustness and fidelity. This has inspired a rising interest in self-interpretable neural networks, which inherently reveal the prediction rationale through the model structures. Although there exist surveys on post-hoc interpretability, a comprehensive and systematic survey of self-interpretable neural networks is still missing. To address this gap, we first collect and review existing works on self-interpretable neural networks and provide a structured summary of their methodologies from five key perspectives: attribution-based, function-based, concept-based, prototype-based, and rule-based self-interpretation. We also present concrete, visualized examples of model explanations and discuss their applicability across diverse scenarios, including image, text, graph data, and deep reinforcement learning. Additionally, we summarize existing evaluation metrics for self-interpretability and identify open challenges in this field, offering insights for future research. To support ongoing developments, we present a publicly accessible resource to track advancements in this domain: https://github.com/yangji721/Awesome-Self-Interpretable-Neural-Network.", "sections": [{"title": "I. INTRODUCTION", "content": "OVER the past decade, neural networks have achieved remarkable success in solving complex problems across various fields. This success is largely due to their vast hypothesis space, facilitated by deep signal propagation through hidden units. Despite their impressive predictive power, the lack of interpretability poses significant challenges. Without a clear understanding of how the model arrives at its decisions, it becomes difficult to build user trust in the model's predictions, particularly in critical decision-making scenarios.\nExplainable AI (XAI) has emerged as a popular research area to address the interpretability challenges of neural networks. Post-hoc interpretability methods, which provide explanations for pre-trained models, have been widely adopted to enhance model transparency. These methods aim to generate human-understandable explanations for pre-trained model predictions. Despite the flexibility, post-hoc methods often fall short of offering a transparent view of the model's internal workings [1], [2]. Moreover, they are often computationally expensive and struggle to faithfully capture the true behavior of pre-trained models [3]\u2013[6]. As highlighted in recent studies [7], [8], the inherent opaqueness of these models hinders the understanding of the decision-making process. This growing awareness of the limitations of post-hoc explanations has driven the demand for neural network architectures that can intrinsically reveal the reasoning behind their predictions.\nTraditional machine learning models, such as decision trees and linear models, are naturally interpretable. However, their optimization and learning paradigms differ fundamentally from those of neural networks, limiting their integration in the current AI landscape, where neural networks dominate. For instance, while decision trees are effective for classification tasks when the features are pre-defined, they cannot operate within the gradient-based framework of neural networks, which prevents end-to-end learning. This incompatibility highlights the need to explore self-interpretable neural network approaches that retain the powerful learning capabilities of deep models while offering intrinsic interpretability.\nAlthough numerous reviews have discussed interpretability techniques in neural networks, they either focus on specific domains or post-hoc explanations [9]\u2013[13]. Model interpretation itself is a subjective and complex concept, which often varies significantly across domains and thus makes systematic summarization challenging. To the best of our knowledge, there still lacks a comprehensive and systematic review to summarize the current advances of self-interpretable neural networks (SINNs). Furthermore, the field lacks a consolidated summary of evaluation metrics for assessing self-interpretability, and future research directions in this area remain under-explored.\nIn this paper, we provide a comprehensive review of state-of-the-art SINNs. We propose a taxonomy that categorizes existing work into five key dimensions. This taxonomy design is grounded in the various explanation forms embedded within the architectures of these networks.\n\u2022 Attribution-based methods identify which input elements most strongly influence the model's prediction."}, {"title": "II. PRELIMINARIES", "content": "In this section, we begin by describing SINNs and distinguishing them from post-hoc interpretability methods to clarify the scope of our survey. We then provide an overview of foundational surveys in interpretable machine learning, emphasizing the unique focus of our survey."}, {"title": "A. Survey Scope", "content": "Building on the definition of interpretable machine learning by [28], [29], SINNs are neural architectures where interpretability is built-in architecturally and enforced through domain-specific constraints. The neural components (neurons, layers, or modules) could represent human-understandable units, allowing the network to explain its decisions based directly on its internal representations, without relying on external parsers or explainers. These constraints can vary significantly depending on the application domain. SINNs have two key properties: self-interpretability, where the model provides explanations simultaneously with its predictions, and end-to-end differentiability, allowing for efficient neural network training. As shown in Figure 1, SINNs differ from post-hoc methods in how model explanations are generated. Post-hoc methods work on pre-trained models, interpreting their predictions through a separate explainer. In contrast, SINNs produce both predictions and explanations simultaneously during the inference process. This paper specifically focuses on the interpretability arised directly from the neural structures, excluding non-neural network-based methods, such as algorithmic interpretability [30], dataflow analysis [31], and parts of neuro-symbolic methods which reply on external program parsers for structured module organization [32].\nWe focus on collecting and reviewing SINN-related papers published in leading AI-related conferences and journals, including but not limited to NeurIPS, ICML, ICLR, AAAI, CVPR, ACL, KDD, WWW, IJCAI, TAPMI, JMLR, TKDE, TOIS, and Nature-related journals, as well as influential works from the broader research community. Our goal is to provide a systematic and accessible overview of the state-of-the-art design principles, applications, and evaluation metrics of SINNs. Additionally, we aim to identify commonalities and"}, {"title": "B. Related Surveys", "content": "Recent advancements in XAI have been extensively documented through various surveys. This subsection reviews foundational XAI surveys and elucidates the distinct focus of our survey on self-interpretable neural networks.\nSeveral important surveys [33]\u2013[37] take an interdisciplinary approach, using ideas from philosophy, psychology, and cognitive science. For example, Miller [33] combines philosophical and social science views to build a theory of explanations in XAI. Langer et al. [34] discuss what different users, like developers and regulators, expect from model explanations. Vilone and Longo [37] review definitions and methods for interpretability, connecting theory with practice. These surveys have shaped both technical and interdisciplinary research in XAI.\nAs summarized in Table I, numerous studies [9]\u2013[14], [38], [39] delve into specific aspects of self-interpretability. However, many of these primarily focus on post-hoc interpretability methods, only briefly addressing self-interpretable approaches without thoroughly exploring their unique designs and contributions. Some surveys [12], [13], [38], [39] do review self-interpretable techniques but embed these discussions within the broader XAI context, lacking a systematic and dedicated exploration of SINNs as a standalone topic. This gap underscores the need for a focused survey on SINNs, given their diverse implementations across various domains and different innovative approaches.\nFurthermore, surveys targeting specific data types or application domains, such as NLP [15], [18]\u2013[20], [40], [41], graph data [21]\u2013[23], and RL [24]\u2013[27]. They provide unique taxonomies of interpretability methods tailored to those fields. However, many SINN architectures across diverse domains share common foundational ideas but lack unified understanding and categorization. Existing surveys do not fully capture the breadth and depth of SINNs, which span multiple fields and employ various innovative approaches. By integrating these shared concepts and examining domain-specific adaptations, our survey provides a cohesive framework for understanding self-interpretable network designs, thereby addressing a significant gap in previous surveys."}, {"title": "III. SELF-INTERPRETATION NEURAL NETWORKS", "content": "In this section, we introduce the general principles of self-interpretation, focusing on ideas that are broadly applicable across various fields while omitting specific application details. For example, in computer vision, we will not discuss image feature extraction backbones and leave such topics in the next section. Notably, we highlight the common structures used in SINNs and classify them based on their interpretability techniques. We categorize SINNs into groups including attribution, function, concept, prototype, and rule-based approaches."}, {"title": "A. Attribution-based Self-Interpretation", "content": "Feature attribution is a key class of interpretability techniques that aims to identify the features most responsible for a model's predictions [42], [43]. As highlighted in previous studies [1], [44], [45], many interpretation algorithms can be unified under the framework of additive attribution, which has provided a cohesive lens for understanding post-hoc methods [1], [46]. Extending this viewpoint, self-attributing models pursue the same interpretive goals by providing decomposed and meaningful explanations. However, they differ in that post-hoc methods generate interpretations after training, whereas self-attributing models integrate interpretability directly into their network design.\nAlthough different tasks may involve varying forms of input, the key idea is that feature attribution ultimately focuses on lower-dimensional representations. For instance, while many studies evaluate interpretability techniques on image datasets, the actual attributions often occur on feature maps extracted from raw pixels rather than solely on the pixel values themselves. In this work, we concentrate on the attribution component and leave the details of feature extraction (e.g., convolutional layers for images or embedding layers for textual data) abstracted away. Nevertheless, certain domain-specific constraints or properties (discussed further in Section IV) may shape how attributions are computed or interpreted in practice.\nTo formalize this, let r denote the original raw sample, and let x = h(r) represent the features extracted by the backbone function h(\u00b7). In a general setting, let each feature xi be in Rd, where d is the dimension of a given \"feature field.\" Common examples include: (1) Image data: r is the raw image, N is the number of extracted feature maps, and d is the spatial size of each map; (2) Tabular data: r = x is directly the feature vector, often with d = 1 if each feature is scalar; and (3) Recommender systems: one-hot vectors are extracted so that d corresponds to the embedding dimension or size of the data field. Without loss of generality, one can assume a consistent dimensionality d across all features, since any discrepancies in shape can be managed within a neural network via projection or reshaping. We thus write {x1,...,xN} to denote the set of N components subject to attribution in the model.\nHowever, merely training a neural network with a decomposition structure does not guarantee that attributions will be meaningful or domain-grounded. In the absence of explicit constraints, the attribution terms \u03b1i need not correspond to the true contribution of each feature i. For example, \u03b1i might encode spurious correlations or depend on higher-order feature interactions, leading to interpretations that are neither stable nor consistent across samples. To address these concerns, a variety of self-attributing models have been proposed that impose constraints, including regularizers and structural inductive biases, on the attribution terms. The subsequent sections categorize these self-attribution neural networks according to the attribution forms and constraints they employ. We will detail the core methods they use to ensure that attributions are meaningful and trustworthy.\n1) Generalized Coefficient Attribution: The concept of using coefficients to interpret models is a long-standing tradition in statistics, with its roots in linear regression (LR) and generalized linear model (GLM). LR, expressed as f(x) =  \u2211_{i=1}^{N} a_i x_i + a_0, is one of the most interpretable modeling techniques, where the coefficients ai quantify the sensitivity of the output to each input feature. GLM extends LR by introducing link functions and probabilistic frameworks, enabling broader applications while retaining the interpretable structure. Moreover, coefficient interpretation can be generalized to neural networks by applying generalized coefficients to the feature layer. These coefficients act as proxies for feature re-weighting or selection and are learned by the model to quantify the importance of each feature in the prediction, as expressed:\nf(x; \u03b8, \u03c6_0) = \u2211_{i=1}^{N} \u03b1_i(x; \u03b8)x_i + \u03c6_0,  x = h(r), s.t. \u03b1 \u2208 C.        (1)\nIn this case, xi \u2208 Rd operates as the input features and is directly aggregated in the high dimensional space. The coefficients \u03b1i(x; \u03b8) can be regarded as the attribution value of the i-th feature. Various constraints C can be imposed on the coefficients to ensure meaningful attributions. These constraints should be sparse and interpretable, reflecting the actual sensitivity of the output to the input features. It is worth noting that feature attribution can be flexibly augmented with feature extractors upstream, transformations downstream, or even stacked in multiple layers. We will not discuss the profusion of variations, but rather focus on their general principles and implications.\nGradient-based Constraints. SENN [45] is a pioneer- ing study to formulate self-attribution neural networks with generalized coefficients. SENN proposes to ensure that the coefficients \u03b1(x) mirror the sensitivity of the output f(x) to the input x by introducing the constraint set: CSENN = {\u03b1(x) : \u03b1(x) \u2248 \u2207_x f(x)}. Using the chain rule, SENN generalizes this gradient alignment to the Jacobian matrix of the feature transformation function h(\u00b7), denoted as CSENN = {\u03b1(r) : \u2207_r f(r) \u2248 \u03b1(r) J_h(r)}. Essentially, SENN utilizes a composition of local linear models to represent a complex nonlinearity, with each coefficient indicating the importance of a feature by its sensitivity to local disturbances.\nArchitectural Constraints. The difficulty of feature attribution lies in the highly nonlinearity of neural networks."}, {"title": "B. Function-based Self-Interpretation", "content": "Transparent function-based SINNs aim to render neural computations interpretable through structured mathematical forms. By mapping the transformations between inputs, hidden representations, and outputs, these models provide a clear mapping from raw data to final predictions. In this subsection, we discuss two key paradigms for interpretable neural networks: (1) structured compositions of univariate functions (functional decomposition) and (2) multivariate symbolic equations (equation learning), which aim to discover explicit mathematical formulas."}, {"title": "C. Functional Decomposition.", "content": "Functional decomposition provides a principled way to break down complex neural networks into interpretable sub-functions, each focusing on specific relationships among input features and hidden concepts. Formally, a neural network can be expressed as\nf(x) := (\u03a6_L \u25e6 \u03a6_{L\u22121} \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03a6_2 \u25e6 \u03a6_1)x,     (7)\nwhere \u03a6l represents the transformation (e.g., layer) at stage l. Unlike standard MLPs with dense, fully connected layers, functional-decomposition approaches enforce structural constraints that reveal how features evolve into increasingly abstract representations.\nVOTEN [134] implements functional decomposition via a hierarchical \"voting\" strategy. Each layer constructs higher-level concepts by applying univariate nonlinear transformations to the current variables, then aggregating them with weights that sum to one. Formally, the j-th concept in layer l + 1 is computed as:\nx_{k,j}^{(l+1)} = \u2211_{k=1}^{n_l} \u03b1_{k,j}^{(l)}\u03c6_{k,j}^{(l)}(x_k^{(l)}), s.t. \u2211_{k=1}^{n_l} \u03b1_{k,j}^{(l)} = 1,       (8)\nwhere x_{k,j}^{(l+1)} is the j-th concept in layer l + 1, \u03b1_{k,j}^{(l)} are the voting weights, and nl denotes the number of concepts in the l-th layer. By enforcing normalized weights and single-valued transformations, VOTEN's design reveals how each feature evolves into more abstract concepts at successive layers.\nInspired by the Kolmogorov-Arnold representation theorem, KAN [135] generalizes functional decomposition by replacing standard neural connections with learnable univariate spline functions. Formally, the activation value of the j-th neuron in layer l + 1 is simply summed over all neurons in layer l:\nx_{k,j}^{(l+1)} = \u2211_{k=1}^{n_l} \u03c6_{k,j}^{(l)}(x_k^{(l)}),      (9)\nwhere \u03c6_{k,j}^{(l)}(x_k^{(l)}) represents the univariate spline function connecting the k-th neuron in layer l to the j-th neuron in layer l + 1. Each \u03c6_{k,j}^{(l)}(x_k^{(l)}) is parameterized as:\n\u03c6_{k,j}^{(l)}(x) = \u03c9_b b(x) + \u03c9_s \u2211_i c_i B_i(x),      (10)\nwhere b(x) is a base function (e.g., silu activation, silu(x) = x/(1+e^\u2212x)), Bi(x) are B-spline basis functions, and \u03c9b, \u03c9s, ci are learnable parameters. This formulation generalizes the Kolmogorov-Arnold representation theorem while maintaining interpretability through explicit univariate transformations at each edge. The spline parameterization allows for flexible function learning while preserving the ability to visualize and understand each transformation's behavior.\nDespite different implementations, both VOTEN and KAN share the core objective of disentangling a high-dimensional function into interpretable sub-functions. Their architectures, however, can be complex\u2014choosing an optimal network structure remains challenging. To tackle this, both works propose pruning strategies to remove redundant components and reveal the most salient features. Looking ahead, automating network design and leveraging progressive growing techniques represent promising directions for simplifying functional decomposition models while preserving their interpretability."}, {"title": "D. Equation Learning.", "content": "Compared to the functional decomposition paradigm, equation learning focuses on directly learning concise, generalizable, and interpretable mathematical equations from data. It provides a more straightforward and global view on model interpretability, as the learned equations can be directly inspected and analyzed by domain experts. Equation learning has shown particular success in discovering physical laws and scientific relationships. Typically, equation-learning optimization target is formulated as:\nf*(x) = arg min_{f\u2208F} {1/D} \u2211_{i=1}^{D} loss(f(x_i), y_i),       (11)\nwhere F denotes the space of interpretable functions (such as polynomials, trigonometric functions, and their combinations), D is the training dataset consisting of input-output pairs, and loss(\u00b7) is typically mean squared error or similar regression losses. In essence, this is a constrained optimization task aimed at identifying the most interpretable function f*(\u00b7) that fits the data well while maintaining mathematical simplicity and physical plausibility.\nArchitectural-based methods [136], [137], [144] typically rely on multi-layer feed-forward networks that incorporate specialized units reflecting basic algebraic operators. These units often include unary operations (e.g., identity mapping f(x) = x, trigonometric functions sin(x) and cos(x), exponential functions ex), binary operations (e.g., multiplication x_1 * x_2, division x_1/x_2), and linear transformations implemented through weight matrices. For example, EQL [144] successfully learns physics equations like Coulomb's law F = k*(q1*q2)/r^2 and GINN-LP [137] can discover complex multivariate Laurent polynomials of the form P = \u2211_{i=1}^{N} C_i \u220f_{j=1}^{V} x_j^{p_i(j)} , where ci \u2208 R and pi(j) \u2208 Z represent coefficients and powers respectively. A key limitation of such designs is their dependence on pre-defined network structures, which can lead to unnecessarily complex equations and reduced flexibility in modeling intricate data relationships. To mitigate these issues, architectural-based models often employ sparsity-inducing constraints [136] (like l0, l1 regularization term), systematic architecture selection through validation performance, or adaptive network growth strategies to achieve both conciseness and predictive effectiveness while avoiding overfitting. For instance, GINN-LP uses an iterative growth strategy where the optimization objective combines prediction error and model complexity:\nL = 1/D \u2211_{i=1}^{D} (f(x_i) \u2212 yi)\u00b2 + \u03bb_1 \u2211_{i=1}^{D} ||w||_0 + \u03bb_2 \u2211_{i=1}^{D} ||w||_1, where the last two terms promote sparsity and prevent overfitting respectively.\nIn contrast, generative-based methods [138]\u2013[140] cast equation learning as a set-to-sequence modeling task, wherein a set of numerical data points is transformed into a sequence of symbolic tokens. Two predominant workflows are employed in this domain: (1) skeleton prediction followed by constant fitting [140]\u2013[142] and (2) simultaneous prediction of model structure and constants [138], [139], [143]. Furthermore, recent studies have shown that these generative models benefit from pre-training on large corpora of symbolic equations, thereby improving their generalization capabilities [140]\u2013[142]."}, {"title": "E. Concept-based Self-Interpretation", "content": "In this section, we review concept-based methods, which enhance model interpretability by structuring learned representations around human-understandable concepts. These models not only predict outcomes but also articulate how specific concept information within the input contributes to decisions, providing intuitive understanding of the prediction.\nDespite different architectural designs, concept-based models could be broadly categorized into two stages: (1) mapping raw inputs to interpretable concepts, and (2) leveraging these concepts for final predictions. The core idea is to design a specialized \"bottleneck\" layer that encodes human-interpretable concepts, such as specific image features, into an intermediate representation. These concepts could be predefined, learned from data, or a combination of both. This strategy enforces explicit concept learning before prediction, enabling practitioners to analyze both the presence of specific concepts and their influence on the model's prediction.\nFormally, concept models predict a target y \u2208 R from input x \u2208 Rd through intermediate concepts c \u2208 Rk using a composite function f(g(x)) with two components:\n1) g(\u00b7) : Rd \u2192 Rk, mapping raw inputs x into the representation restricted to a concept space c.\n2) f(\u00b7) : Rk \u2192 R, mapping the concept-based representation c to the target y, typically through a linear or logic layer.\nThe model minimizes loss functions Lc for concepts and Ly for the target, supporting three training schemes: independent, where f(\u00b7) and g(\u00b7) are learned separately; sequential, where g(\u00b7) is trained first followed by f(\u00b7); and joint training, optimizing both functions simultaneously with a balancing hyperparameter \u03bb. Most concept models [145], [148] also allows users to directly manipulate the predicted concepts at test time and observe how these changes impact the final prediction. This facilitates counterfactual reasoning (\u201cWhat if the model didn't think this feature was present?\u201d), and allows users to correct potential model errors.\nRecent theoretical analysis by Luyten et al. [165] reveals that concept model performance depends on two critical factors: concept expressiveness (the capacity to capture salient information) and model-aware inductive bias (coherence between concept set and model architecture). This is also evidenced by the model's reliance on concept annotations, which can be costly and incomplete. Therefore, recent works have focused on enhancing the performance and scalability by designing a more expressive and model-aware concept layer. We categorize these into two key directions: concept Representation, concept organization and concept supervision. Table IV summarizes representative concept-based self-interpretation methods, highlighting their concept layer designs and human intervention strategies.\n1) Concept Representation: In concept-based models, the design of concept representation is crucial for regularizing the model's latent space, thereby shaping both interpretability and performance. The Concept Bottleneck Model (CBM) [145] employs direct scalar activation, mapping inputs to scalar concept scores via an activation function s(\u00b7) : R \u2192 [0, 1]. This mapping can be hard, applying thresholding (s(x) = 1_{x>0.5}), or soft, using a sigmoid activation function (s(x) = 1/(1 + e^{\u2212x})). ENN [147] refines concept mapping with differentia neurons that distinguish sub-concept pairs and sub-concept neurons that aggregate these distinctions. Although these methods offer intuitive interpretability, they struggle to capture nuanced concept semantics.\nTo enhance expressiveness, the Concept Embedding Model (CEM) [148] and IntCEM [159] extend representations into high-dimensional embeddings, where each concept ci comprises active and inactive states (c+, c\u2212 \u2208 Rm), later mapped by a sigmoid to determine concept presence. Concept Whitening [146] aligns predefined concepts with latent axes via whitening and orthogonal transformations. Meanwhile, approaches such as PCBM [149], SITE [102], and Concept Transformer [120] adopt a geometric perspective, using similarity scores to project input embeddings onto concept subspaces. Recognizing the inherent ambiguity of real-world concepts, recent studies [151], [152], [160], [161] propose a distributional lens, modeling concepts as prior distributions, for example, Gaussian distributions p(zc | x) ~ N(\u03bcc, diag(\u03c3c)). This probabilistic framework naturally captures uncertainty and partial presence.\n2) Concept Organization: Concept organization encompasses various approaches to modeling relationships between concepts, each offering distinct perspectives on capturing concept dependencies. While vanilla CBM use a flat structure with independent concepts, this simplification fails to capture the rich interdependencies in human conceptual understanding. As shown by Raman et al. [166], such independence assumptions can lead to instability and robustness issues in concept models.\nOne perspective on concept organization focuses on complementary information channels [152], [153], [158]. Typically, these methods introduce additional channels to encode complementary information to enhance the model's understanding of complex concepts. For example, Side-channel CBM [152] implements this through an auto-regressive architecture, where concept predictions incorporate both input features and previously predicted concepts.\nHierarchical organization offers a valuable perspective in structuring concepts. For example, CF-CBM [157], MICA [164], and ENN [147], arrange concepts across multiple levels, where high-level concepts guide the interpretation of lower-level features. This hierarchical approach is well-suited to domains with natural hierarchical relationships, including medical diagnosis and scene understanding."}, {"title": "F. Prototype-based Self-Interpretation", "content": "Prototype-based models are inspired by case-based reasoning, a human-like problem-solving paradigm that leverages past experiences to address new challenges. This approach treats previously encountered situations as \"representative examples\" [169], guiding decision-making for novel cases. As end-to-end trainable neural networks, prototype-based models aim to learn these representative and global examples from data. They automatically identify prototypes within the input feature space and utilize them to make predictions. The prediction process involves computing the similarity between the input data and the learned prototypes and aggregating these similarities through a transparent function. This methodology not only facilitates accurate and robust predictions but also clearly indicates the decision-making pathway. Formally, this process can be abstracted as:\nz = f_{enc}(G(x)), sim(z, p_j) = log((\u03b4 * ||z \u2212 p_j||^2 + 1)/(||z \u2212 p_j||^2 + \u03f5)), N_p = \u2211_j W_j * \u2211_i sim(z_i, p_j)       (12)\nHere, G(\u00b7) represents an input processing function (e.g., substructure extraction), and f_{enc}(\u00b7) is an encoder network that maps the processed input to a latent representation z. Each prototype embedding pj acts as an anchor reference point in the latent space shared with z. Np denotes the total number of prototypes. Typically, the similarity function sim(z, pj) uses a log-activation form, which decreases monotonically with the squared L2 distance between z and pj. A small constant \u03f5 is included to avoid numerical instability. It could be replaced by other similarity metrics, such as cosine similarity [170] or Mahalanobis distance [171]. The prediction \u0177 is obtained by aggregating similarities through a linear weight matrix W. For classification tasks, the output layer often applies a Softmax function to produce a probability distribution over the classes.\nA early work PrototypeDNN [172] introduced an L2 distance-based model for image classification, learning prototypes directly in the input feature space through dual clustering regularization. ProtoPNet [173] refined the framework by introducing patch-based processing, class-aware separation costs, and a more direct prototype alignment with training samples. Typically, prototype-based models follow an alternating optimization method [173], which includes three stages: (1) the initial training phase, which aims to learn diverse and representative prototype embeddings in the hidden state, (2) the prototype refinement phase, which maps learned embeddings into the nearest training data samples, and then prunes or merges duplicated prototypes for better human interpretability, and (3) the model fine-tuning phase, which fixed all the parameters of the encoder and prototype embeddings, and learns a sparse weight matrix for higher accuracy. All these stages aim to enhance the model's interpretability by learning meaningful prototype embeddings or aligning them with human-understandable case examples. Following this fundamental framework, recent research has focused on enhancing the interpretability and performance of prototype-based models by enhancing prototype representations and alignment strategies.\n1) Prototype Representations.: To enhance the prototype interpretability and performance, recent studies have pursued three distinct but complementary approaches. The first direction optimizes input representation through decomposition strategies. With the inclusion of clustering regularization, the interpretability of prototypes largely depends on the quality of the original input data. ProtoPNet proposes to partition the input images into patches for capturing local fine-grained features. This patch-based methodology has gained widespread adoption in computer vision [174]\u2013[177]. Domain-specific adaptations can be merged for different scenarios. For instance, ProtoryNet [178] implements syntactic decomposition for text. PGIB [78] develops information-theoretic subgraph extraction for graphs. The second direction focuses on prototype embedding methodologies through diverse mathematical frameworks. TesNet [179] establishes Grassmann manifold-based construction to ensure prototype orthogonality and representativeness. Several VAE-based approaches [171], [180], [181] conceptualize prototypes as the center of a Gaussian distribution in the latent space, which forms a probabilistic anchor point for representations. ProtoConcepts [182] extends each single prototype to a set of prototypes by introducing a ball-based prototype geometry. The third direction focuses on building organizational frameworks. ProtoTree [183] implements hierarchical tree-based decision paths. ProtoPool [184] establishes dynamic prototype allocation. ST-ProtoPNet [185] incorporates SVM theory into the prototype learning process, which learns two distinct sets of prototypes by differentiating boundary-distant and boundary-proximate prototypes.\n2) Prototype Alignment.: Prototype alignment with human-interpretable concepts has emerged as a crucial aspect of these models' development. While traditional approaches rely on iterative search to project prototypes onto nearest training samples [173], newer methods have introduced more efficient alternatives, especially for those data types with inherent combinatorial nature. ProtGNN [186] and PGIB [78] employ Monte Carlo Tree Search (MCTS) to navigate this complex search space efficiently. For sequence data, ProSeNet [187] employs greedy beam search algorithm. For set-structured data, SeSRDQN [170] implements a hybrid methodology combining neural decoders with progressive MCTS to balance computational efficiency with alignment accuracy. Human integration has also become increasingly important, with systems like PIP-Net [176] and ProtoPDebug [188] incorporating direct human feedback for prototype refinement. PW-Net [189] takes"}, {"title": "G. Rule-based Self-Interpretation", "content": "Rule-based methods aim to enhance the interpretability of neural networks by incorporating explicit rules into their architectures. These rules can be either predefined or learned during training", "Neurons": "The opaqueness of standard neural networks stems from the non-linear functions modeled by neurons", "203": [204]}, {"203": "defines continuous fuzzy logic operations using product", "1": ".", "205": "and FINRule [206", "207": ".", "208": "and DR-Net [209", "x(1)": "while disjunction layers compute s(l+1) = \u2228w(l+1) x(l+1)", "210": [211], "212": "instead introduces a hyper-network to learn a distribution of network weights"}, {"205": ".", "204": "designs networks where each neuron takes 2 inputs and learns a single logic gate via softmax during training: flogic(x) = \u2211 g\u2208G p(g) g(x)", "213": "and continuous input feature values [214", "Constraints": "Unlike traditional approaches that directly use logical operators as neurons", "216": [218], "1": "k"}, {"1": "r", "f(\u00b7)": "C \u2192"}]}