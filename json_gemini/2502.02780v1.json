{"title": "Classroom Simulacra: Building Contextual Student Generative Agents in Online Education for Learning Behavioral Simulation", "authors": ["Songlin Xu", "Hao-Ning Wen", "Hongyi Pan", "Dallas Dominguez", "Dongyin Hu", "Xinyu Zhang"], "abstract": "Student simulation supports educators to improve teaching by interacting with virtual students. However, most existing approaches ignore the modulation effects of course materials because of two challenges: the lack of datasets with granularly annotated course materials, and the limitation of existing simulation models in processing extremely long textual data. To solve the challenges, we first run a 6-week education workshop from N = 60 students to collect fine-grained data using a custom built online education system, which logs students' learning behaviors as they interact with lecture materials over time. Second, we propose a transferable iterative reflection (TIR) module that augments both prompting-based and finetuning-based large language models (LLMs) for simulating learning behaviors. Our comprehensive experiments show that TIR enables the LLMs to perform more accurate student simulation than classical deep learning models, even with limited demonstration data. Our TIR approach better captures the granular dynamism of learning performance and inter-student correlations in classrooms, paving the way towards a \"digital twin\" for online education.", "sections": [{"title": "1 Introduction", "content": "Accurate simulation of students' learning behaviors in online education settings can help building a \"digital twin\u201d classroom, which can serve as a high-fidelity sandbox for the instructors to explore diverse pedagogies. This can in turn help improve students' learning performance. With the rapid advancement of generative AI, using large language models (LLMs) for student simulation is becoming a promising approach. For example, GPTeach [53] used GPT-based virtual students for interactive TA training and MATHVC [92] used LLMs-based virtual classroom for mathematics education. However, these approaches did not systematically evaluate the realism of the virtual students. On the other hand, LLMs-based knowledge tracing models [15, 31, 36, 37, 42] demonstrated high accuracy, but they focus on students' performance prediction rather than multi-faceted behavioral simulation. We argue that an accurate digital twin should encompass contextual simulation of students' behaviors, capturing the dynamic modulation effect of course materials on both individual students' learning performance and correlations among students. Such dynamism can be reflected in various contextual factors, such as lecture content, individual background, questions, skills, and so on.\nHowever, two main challenges hinder the integration of course materials' effects into student simulation. The first lies in the lack of fine-grained datasets that annotate course materials along with students' real-time performance. Most existing datasets (such as Ednet [7], Junyi [61] and Assistments 2009-2010 benchmark \u00b9) only contain the test question without course materials. The EduAgent dataset [86] indeed contains the course content, but the lecture length is too short (5 minutes) to reveal the student learning process across a whole lecture. Moreover, students may get tired easily during the course and the resulting data quality may not be guaranteed [76], which unfortunately is not considered by most existing data collection efforts [7, 61].\nA second challenge is that existing language models can only deal with limited contextual data when learning from example demonstrations. LLM-based simulation typically adopts either finetuning-based or prompting-based approaches. The former [36] enable pretrained models to learn from new training data directly through model finetuning. But they require a significant amount of computational resources. Therefore, researchers usually resort to smaller language models such as BERT for student simulation [36, 37, 42]. However, such smaller language models have very low token limits (e.g., 512 tokens), which can only deal with short textual input but fail to capture complex contextual information such as course materials. Advanced LLMs such as GPT4 could support longer contextual text input, but their performance also drops under a long context [41] and they need significantly more computational resources for fine-tuning. On the other hand, prompting-based methods [38] do not need model training since they directly learn from contextual prompts and example demonstrations. However, the model's in-context learning ability drops significantly in the presence of long demonstrations [41].\nTo tackle the first challenge, we run a new user study (N = 60 students and N = 8 instructors for real-time teaching) in the form of a 6-week online education workshop including 12 lectures (1 hour per lecture). We collected student learning performance and fine-grained annotations of course materials and mapped them to specific post-test questions. To guarantee the data quality and improve students' engagement during learning [76], we developed a new online education system that integrated multi-modality sensing techniques to monitor students' cognitive states and prompt instructors to take recommended actions to increase students' learning engagement in real-time.\nFurthermore, to deal with the second challenge, inspired by the self-reflection ability of LLMs [52] to distill knowledge [20, 87], we propose a new LLM-based student simulation framework by introducing a transferable iterative reflection (TIR) module, which guides the LLMs to perform reflections on specific course materials and compress the learned knowledge to augment the LLM simulation. Different from a straightforward self-reflection [52], our TIR architecture incorporates iterations between a novice agent and reflective agent to ensure that the reflections could be generalized into new domains to enable transferable reflections (Section 3). As a result, this TIR module can augment and solve the bottlenecks of both finetuning-based models and prompting-based models. For finetuning-based models, TIR overcomes the token limit issue by focusing its reflections on specific course materials so as to compress the learned knowledge. For prompting-based models, TIR provides an efficient way to enable the LLMs to learn from example demonstrations more effectively, where the learned knowledge can be transferred to a new simulation without example demonstrations. This ensures that LLMs learn general knowledge instead of locally optimal knowledge from example demonstrations.\nWe have evaluated the student simulation performance in both the EduAgent public dataset [86] and our newly collected dataset. The results show that our TIR modules enhance the LLM-based student simulation, making it even more powerful than deep learning methods that are trained and fit to given datasets. Specifically, the evaluation examines whether our model can better capture the dynamics of student behaviors. Existing research generally defines human behavior as a collection of observable actions and reactions in response to internal (genetic factors) and external (environmental factors) stimuli [48]. Therefore, to demonstrate that our simulator replicates student learning behaviors, we model student responses to these stimuli, represented by post-test accuracy after engaging with course content. Course knowledge, delivered via lecture slides, constitutes external stimuli, while internal stimuli arise from individual differences such as prior knowledge. Accordingly, we assess the model's ability to capture variations in post-test accuracy at multiple levels: individual (per student), lecture (per session), question (per post-test item), and skill (per knowledge concept). Additionally, we evaluate inter-student correlations to determine whether the simulation model accurately reflects the response patterns between student pairs. Overall, the results show that our approach better captures the granular dynamism of learning performance and inter-student correlations in classroom, pointing towards a potential \"digital twin\" for online classrooms.\nTo summarize, the main contributions in this paper include:\n\u2022 We run a new 6-week online education experiment with N = 60 students and N = 8 instructors to collect student learning performance with fine-grained annotations of course materials. This is powered by our newly developed online education system that integrates sensing techniques and feedback recommendations to increase student learning engagement during real-time online instruction. The online education system"}, {"title": "2 Related Work", "content": "Our work draws inspirations from and advances the knowledge in the following three categories of research."}, {"title": "2.1 Generative Agents in HCI", "content": "The rapid advancement of LLMs has inspired a wide range of HCI applications, including social behaviors (Generative Agents [58]), virtual reality [72] with tour guidance (VirtuWander [79]), Human-AI collaboration (AI Chains [83], [77]), creative tasks (CharacterMeet [62], Luminate [69], C2Ideas [24], ABScribe [64], AngleKindling [59], PopBlends [74]), healthcare (MindfulDiary [33], ChaCha [67], Narrating Fitness [68], [63]) with health intervention (MindShift [82], [5, 29, 30, 51]), web interaction [9] with UI design (ReactGenie [89], [12], [73]), coding (CollabCoder [16], [44]), behavioral change (CatAlyst [3],[4]) with human augmentation (Memoro [98], [26]), business (Marco [14]), and so on.\nIn educational context, LLMs-powered agents have been utilized to serve as teachable agents (Mathemyths [93], [28], [43]) to provide instructions [71], recommend learning concepts [40], and give feedback [54]. For example, DevCoach [75] supports students' learning in software development at scale with LLMs-powered generative agents. ReadingQizMaker [49] proposes a Human-NLP collaborative system which supports instructors to design high-quality reading quiz. In programming education, CodeTailor [23] uses LLM-powered personalized parsons puzzles to support engagement in programming. PaTAT [17] presents a human-AI collaborative qualitative coding system using LLMs for explainable interactive rule synthesis.\nSuch existing work either uses generative agents as the instructors to directly teach students [93] or serves as student agents [53] to augment intelligent tutoring systems. Our work focuses on the second aspect. In what follows, we specifically discuss related work in student simulation using either machine learning or generative agents."}, {"title": "2.2 Student Simulation", "content": "Student simulation aims to predict student learning behaviors in education, thus providing insights for supporting intelligent tutoring systems [19]. A majority of existing research formulates student simulation as a knowledge tracing problem, i.e. predicting students' future learning performance based on their past records [1]. This learning performance is usually represented by the question answering accuracy in the course to measure students' skill levels for specific course concepts. Early work in this domain employed classical Bayesian models [91]. In recent years, deep learning models [60] have been the predominant approach for knowledge tracing, combining graph models [55], cognitive theories [97], memory-augmented components [94], and so on."}, {"title": "2.3 LLM-based Student Simulation", "content": "Recent work has explored the feasibility of using LLMs directly for predicting students' learning performance [85, 95] or for knowledge tracing [90] in open-ended questions [45]. These methods have better explainability than deep learning models, owing to LLMs' capability to reveal the reasoning process[38]. They can also augment deep learning-based knowledge tracing [15, 31]. In HCI, researchers have developed multi-agent collaboration environment to simulate the whole classroom interaction [6, 96] and used LLM-simulated student profiles to support question item evaluation [50]. These approaches can enable adaptive and personalized exercise generation to augment student learning performance [8]. For example, Sarshartehrani et al. [65] further leveraged embodied AI tutors for personalized educational adaptation. GPTeach [53] also demonstrated the feasibility of using GPT-based virtual students for interactive TA training. In addition, MATHVC [92] explored the effectiveness of LLM-simulated multi-character virtual classroom for mathematics education. Moreover, LLMs can help students engage in post-lesson self-reflection [34] and also support language learning and growth mindset cultivation [32].\nHowever, there is also evidence [56] showing the limitation of LLMs in student performance prediction compared with deep learning (DL) models. We argue that this is mainly because of the lack of contextual course materials. Specifically, existing LLM-based approaches [15, 31, 36, 37, 42] simply treat student simulation as a sequence prediction problem, which predicts future test performance based on past records, ignoring the modulation effect of course materials. In this case, DL models are very likely to work better than LLMs owing to their capacity to learn from historical data [35, 70]. In contrast, LLMs are better at few-shot contextual learning based on their large pretrained knowledge base [11]. Therefore, incorporating contextual course materials could better unleash the power of LLMs to capture the potential effects of course materials on learning performance even with limited data, thus enabling more accurate student simulation.\nExisting work in this respect is quite limited, due to both the model limitations and the lack of dataset containing course materials (Section 1). EduAgent [86] incorporated course materials to simulate students' cognitive states and post-test performance, but the lectures' duration was too short (5 minutes) to represent the learning process across a typical lecture. By contrast, our work conducts longer-term experiments (6-week, 12 lectures, 1 hour per lecture) to collect high quality learning behavioral data. Our study employs a self-developed online education system integrating sensing techniques and action recommendations to help instructors increase students' learning engagement. Moreover, our proposed transferable iterative reflection module further augments the student simulation performance for both finetuning-based and prompting-based models, which departs from existing approaches in language models [36, 37, 42] and deep learning models [60, 94]."}, {"title": "3 Simulation Methodology", "content": "Our classroom simulacra framework aims to build LLM-based generative student agents that could mimic real students' learning behaviors based on their learning histories. The agents can then simulate the students' future learning performance, which is represented by the question answering correctness in the post-lecture tests."}, {"title": "3.1 Problem Formulation", "content": "In our online education scenario, students first listen to the lecture and then finish a post-course test, which evaluates their learning performance based on the accuracy of their answers. As depicted in Fig. 3, the input of our simulation includes past learning history ($l_{past}$) and future learning information ($l_{future}$). Here $l_{past}$ includes past questions' contents ($q_{past}$), past answers' correctness (i.e. labels in past questions, denoted as $y_{past}$), and course materials related to specific questions ($c_{past}$) in the learning history. The future learning information $l_{future}$ includes future question contents ($q_{future}$) and corresponding course materials ($c_{future}$). The output of the model is the sequence of future answers' correctness ($\\hat{y}_{future}$), with the corresponding ground truth denoted as $y_{future}$.\nNote that the course material inputs are represented by text only to match the LLM's requirement. Although there might be images in the actual lecture slides, we have converted the images into textual descriptions during our dataset annotation process. The course materials include the titles/bullet points on slides, or human-annotated descriptions of images on slides. The lecturers' didactics typically align with the slides but are not delivered as a word-for-word readout. Therefore, we do not use the lecturer's transcripts as course materials nor model input. Examples of model input can be found in Fig. 4."}, {"title": "3.2 Model Training and Evaluation", "content": "We totally have three kinds of simulation models: prompting-based LLM simulation, finetuning-based LLM simulation, and deep learning-based simulation (baseline). In order to evaluate the models in a comparable manner, all kinds of models use the same training set to train the model and the same testing set to evaluate the simulation performance. However, some prompting-based models only need part of the training set, which will be specified later. For each dataset, we first split it into training and testing set by following an individual-wise manner with a specific ratio R% (detailed ratio is depicted in each experiment). Specifically, all test performance of R% students was used as the training set and all test performance of another (1-R%) students was testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with the same R% ratio. The reason why we use the individual-wise dataset splitting is that our classroom simulacra instantiates each digital student based on the corresponding real student's past learning history and simulates that student's future learning process. We set the first five questions as past questions ($q_{past}$) of the student history and other questions as future questions ($q_{future}$) for prediction."}, {"title": "3.3 Transferable Iterative Reflection (TIR)", "content": "The objective of transferable iterative reflection (TIR) is to improve the LLM-based student simulation by learning from the students' past learning history more effectively. The main difference between the TIR and the existing (multi-round) reflection-based methods [25, 27, 34, 39, 52, 78, 88] lies in the transferable feature in the model design. Traditional reflection methods simply ask LLMs to reflect based on the difference between their predictions and labels. In contrast, the TIR module iteratively prompts LLMs to reflect on its previous simulations by comparing with labels in the example demonstrations (i.e. past learning history) so that LLMs could generate general reflections results that could be easily transferred to novice LLMs which do not have the example demonstrations. This ensures the generalization of such reflected results to be applied into new future learning information. As a result, the reflected results can not only directly improve the simple prompting-based simulation by increasing the data utilization efficiency, but also compress the information while avoiding missing important information to improve the finetuning-based simulation. At a higher level, the TIR module consists of four phases: initial prediction, reflection, testing, and iteration (Fig. 2).\n(1) Initial prediction: Ask the LLM (reflective agent) to predict future question correctness (i.e., $\\hat{y}_{future}$) based on $l_{past}$ and $l_{future}$, and obtain the initial prediction accuracy ($acc_0$) by comparing $\\hat{y}_{future}$ with the ground truth ($y_{future}$), as depicted in Section 3.1 and Fig. 3.\n(2) Reflection: Provide the reflective agent with the ground truth of future question correctness (i.e. label: $y_{future}$), and ask it to reflect on why it fails to predict some future answers' correctness. The reflection at iteration k is denoted as $r_k$.\n(3) Testing: Use $r_k$ together with $l_{past}$ and $l_{future}$ to ask another novice agent which has not experienced the ground truth to make a new prediction. Denote the predictions from the novice agent in this iteration k as $\\hat{y}_{novice,k}$.\n(4) Iteration: Obtain the prediction accuracy ($acc_k$) by comparing $\\hat{y}_{novice,k}$ with the ground truth ($y_{future}$). If $acc_k$ is lower than the initial prediction accuracy ($acc_0$), we ask the reflective agent to reflect in a different direction in the next iteration. Otherwise, we inform the reflective agent that the accuracy has indeed improved and it could reflect towards the similar direction. The iteration ends when the novice agent achieves 100% accuracy with the help of $r_k$ or we reach the maximum number of iterations.\nFinally, we select the reflection that yields the highest prediction accuracy by the novice agent as the best reflection ($r_{best} = r_{\\operatorname{argmax}(acc_k)}$) and log it into the reflection database. This database will be used to augment existing prompting-based LLMs by giving example demonstrations or augment finetuning-based models to provide reflections.\nThe iterative reflection in TIR improves the simulation performance by adjusting the reasoning process of LLMs during reflection. Due to the diversity of individuals and course contents, responses to course stimuli varies a lot across students. With a simple reflection, LLMs' results can be biased due to its pre-training corpus [22], and cannot be well-adapted to the specific student and course stimuli. The iterative reflection helps LLMs to overcome such bias from its pre-training corpus and iteratively adjust the reasoning process during reflection regarding the causality of how course stimuli modulate students' behaviors while also respecting the students' past history. One example is depicted in Fig. 4. When the LLM had a straightforward reflection in $r_1$, it overestimated the student's comprehension and thought the student could answer Question 12 correctly. However, After iterative reflection in $r_3$, the LLM found a potential misunderstanding or oversight that led to the wrong prediction. Such an experience was stored in the successful reflection database. Once retrieved, it could give inspiration for the future new reflection agent to consider such oversight in the testing set.\nFor prompt-based models, TIR can directly improve the LLMs' performance without fine-tuning. For finetuning-based models, TIR can effectively compress the overflowed information in $l_{past}$ and $l_{future}$ to deal with the token limit problem. More details are described in the following sections."}, {"title": "3.4 TIR Augments Standard Prompts", "content": "Here we describe how to apply the TIR module to augment the standard prompting-based simulation models.\nA standard simulation model simply uses LLMs to take all information as prompt input ($l_{past}$ and $l_{future}$) and predict future question correctness sequence ($\\hat{y}_{future}$), as depicted in Fig. 18. Directly inputting all of students' data from the training set as example demonstrations poses an obvious challenge. The data including course materials often exceed the token limits of LLMs, hampering their capability to extract useful information.\nTo this end, we apply the TIR module to enable the LLM to effectively learn from the training data set. Specifically, in the training stage, we first run the TIR module for each student in the training set, following the procedure in Section 3.3. The output reflections are stored into a successful reflection database. In the testing stage, as depicted in Fig. 2, we do not run the TIR module. Instead, we use a new reflective agent powered by LLMs to retrieve reflections from the successful reflection database. For each simulated student in the testing set, we retrieve the reflections of M students from the reflection database. The M students from the training set are randomly selected but we make sure they are in the same course as the simulated student in the testing set. This is the only criteria of retrieving reflections, which ensures contextual consistency during reflection. Random selection ensures that the example demonstrations are not manually biased. However, we use a random seed to also ensure that we can replicate such random selection to enhance the replicability of our results. Our pilot experiment shows that M = 4 is enough to achieve reasonable simulation performance. The retrieved reflection from M students serves as the example demonstrations in the same course so that the new reflection agent in the testing set can leverage the experience from the retrieved reflections to perform a transferable reflection. Based on the retrieved reflections and past learning history ($l_{past}$) and future learning information ($l_{future}$), the new reflective agent conducts simulation for the specific student in the testing set. To prevent label leakage, this new reflective agent does not experience any other training data. So it is different from the reflective agent in the training set.\nIt is worth noting that the iterative reflection process only happens in the training set. Moreover, we do not limit the LLMs' reflection to be either content-specific or metacognitive, in order to give LLMs free enough space to do reflection. But we make it generalizable by evaluating whether the reflections can be transferred to a novice agent for prediction. In addition, the reflection has to be both content-specific and metacognitive, because the course modulation effect is usually different across different lectures. So content-specific reflection is necessary to adapt to specific course context. However, LLMs also have metacognitive reflections because the example demonstration students in the training set are different from the simulated students in the testing set. In our current setting, we only need to run the iterative reflection once per lecture to generate the successful reflection database for that speicific lecture. There is no need to run it for each question/knowledge concept/student. Running reflection offline for each lecture is reasonable, because the lecture materials are usually prepared in advance and available well before the class in real-world teaching scenarios. We have not tested if TIR can generalize across different lectures by using one single lecture's reflection database, but this can be one promising future exploration. The different reflection direction means that LLMs are instructed to reflect in another reasoning about why a wrong prediction is made. But we do not limit the specific direction content to give LLMs enough space to explore. Examples about such different directions are in Fig. 4 and Appendix Fig. 18."}, {"title": "3.5 TIR Augments CoT-based Models", "content": "Existing work has shown that using the chain of thought (CoT) prompting strategy can improve the capability of LLMs [80]. The idea of CoT is to use prompts to guide the LLMs to reason step by step like a human, instead of solving the problems all at once. Our TIR module can be integrated into CoT to further improve the simulation accuracy of prompting-based models. The integration works similarly to that in standard prompting-based model in Section 3.4. One example workflow is depicted in Appendix Fig. 19. The only difference is that we provide step-by-step guidance to the LLMs whenever predicting future questions' correctness, as depicted below.\n(1) Analyze the student's past performance:\n\u2022 Identify course concepts in past questions the student has performed well in and those they have struggled with.\n\u2022 Consider the complexity of the questions and the related course materials.\n(2) Review the course concepts and related lecture materials of the future questions:\n\u2022 Determine the difficulty level of the future questions based on the related course concepts and course materials.\n\u2022 Identify if the future questions are related to the concepts of past questions that the student has previously struggled with or excelled in.\n(3) Predict the student's performance in future questions:\n\u2022 Based on the analysis from steps (1) and (2), predict whether the student will answer each future question correctly or not."}, {"title": "3.6 TIR Augments Finetuning-based Models", "content": "In addition to prompting-based methods, TIR can also improve finetuning-based language models by compressing the input tokens to avoid token overflow. We fine-tune BERT (Bidirectional Encoder Representations from Transformers), a language representation model that has pre-trained weights [10]. The input of BERT is a sentence and the output could be anything from question answering to semantic classification. However, BERT has very low token limits (512 tokens), which apparently can not directly handle all past/future question inputs or related course materials. The TIR module solves this problem by distilling useful reflections from such data so that there is no need to input the extremely long course materials into BERT. As depicted in Fig. 3, the input of the TIR augmented BERT is composed of three parts: future question contents $q_{future}$, initial LLMs-based future question correctness prediction results, and reflections from the TIR module. The model output is a binary value to decide whether one student answers one future question correctly or not. This is achieved by finetuning the BERT-based classifier from HuggingFace \u00b2. In the training stage, we have the labels for the training set, so the TIR module directly runs on the training set to generate successful reflections, as depicted in Fig. 3(a) and Section 3.3. However, in the testing stage, the labels can not be used for TIR to avoid label leakage. Therefore, we instead use a new reflective agent to generate new reflections based on the retrieved reflections as example demonstrations from the successful reflection database.\nTo show the effectiveness of our TIR module in augmenting the BERT model, we prepare another baseline BERT model without TIR, which directly takes all information as input (past questions $q_{past}$ with related course materials $c_{past}$ and real past question correctness labels $y_{past}$, future questions $q_{future}$ with related course materials $c_{future}$) and predict the correctness of future questions. We denote the BERT model without TIR as BertKT, in contrast to that with TIR (BertKT+TIR). One example of the workflow is depicted in Appendix Fig. 20.\nFor a fair comparison, the fine-tuned data is the same as the training set of deep learning models. Therefore, we only fine-tune the BertKT once in our data. However, for future potential applications to extend the fine-tuned models in external datasets, it is necessary to fine-tune models again in such new datasets, which is similar to deep learning models that use training data to update model weights."}, {"title": "3.7 Deep Learning Models", "content": "We have also implemented five deep learning models with pyKT [47] for student simulation as baseline models, which come from recent state-of-the-art knowledge tracing models. These five models are from four categories: attention-based models (AKT [18], SimpleKT [46]), adversarial-based models (ATKT [21]), deep sequential models (DKT [60]), and memory-augmented models (DKVMN [94]). These models are widely used knowledge tracing models in the computational education domain to model student learning [18, 21, 46, 60, 94]. For example, DKT is the first architecture that applies deep learning to model student learning behaviors [60], which has become the standard baseline model for benchmarking in computational education domain. AKT [18] is the first model that applies the monotonic attention mechanism into student modeling. The Appendix Section A.1 includes more details about each model.\nThe training and testing scheme in deep learning models are the same as other models for fair comparison, as depicted in Section 3.2. The model input is the same as BertKT, i.e. past questions $q_{past}$ with related course materials $c_{past}$ and real past question correctness labels $y_{past}$, future questions $q_{future}$ with related course materials $c_{future}$. The model output is the prediction of future question correctness $\\hat{y}_{future}$. The difference is that deep learning models can not directly take textual data as input. Therefore, we use BERT again to extract embeddings from the textual data (such as question contents and course materials) as deep learning model input, which is a common practice for knowledge tracing models to predict student performance [46]. Each model is initialized using the default configurations in pyKT. The models are trained with Binary Cross Entropy Loss and the AdamW optimizer with a learning rate of le-5. Our pilot experiments show that the model validation accuracy stablizes after about 15 epochs. Therefore, we train each model for 30 epochs and select the best model in validation for testing."}, {"title": "4 Simulation Study", "content": "We first explored the feasibility of our framework compared with baseline models in the public dataset named EduAgent[86]."}, {"title": "4.1 The EduAgent Dataset", "content": "The EduAgent dataset was collected from N = 301 students, who were asked to watch 5-min online course videos. After that, students were prompted to finish a post test which comprises 10-12 questions. The dataset contained students' correctness on each post test question, as well as corresponding question contents and course materials which were specifically related to each question. More details about this dataset could be obtained from [86]."}, {"title": "4.2 Experiment Settings", "content": "We split the dataset into training and testing set by following a individual-wise manner with 0.8 ratio. Specifically, all post test performance of 80% students were used as the training set and all post test performance of another 20% students were testing set. The training set was further divided into model training and model validation set following the same individual-wise manner with 0.8 ratio as well. We set the first five questions as past questions of the student history and other questions as future questions for prediction. As depicted in Fig. 2, the simulation model input included the correctness of past questions of real students, as well as corresponding past questions contents and course materials, which were specifically related to each corresponding past question. Moreover, the model input also included future question contents and course materials which are specifically related to each future question. The model output was the correctness of each future question for predictions."}, {"title": "4.3 Results and Analysis", "content": "Results were depicted in Table. 1. We found that the integration of the TIR module improved the simulation performance so that both the simulation accuracy and f1 score were better than all deep learning baseline models. Specifically, the best deep learning model was SimpleKT with 0.6772 accuracy and 0.6698 F1 score. Without the TIR module, the best LLMs-based model was CoT-based prompting with 0.6222 accuracy and 0.5610 F1 score. However, after integrating the TIR module, the best LLMs-based model was finetuning-based BertKT model with 0.7012 accuracy and 0.6880 F1 score, which was superior than the best deep learning model.\nMoreover, we found that the integration of the TIR module could improve all LLMs-based models including standard prompting, CoT prompting, and BertKT, as supported by Table. 1. Although the accuracy in CoT slightly decreased, its F1 score was however obviously improved.\nThese results demonstrate the feasibility and effectiveness of our TIR module to enhance existing LLMs-based approaches for more realistic student simulation, which were even better than deep learning models."}, {"title": "5 Online Education Workshop and Dataset", "content": "Although our simulation experiment on the EduAgent dataset demonstrated the effectiveness of our framework compared with baseline models, the EduAgent dataset itself only contains 5-min lectures. Such short duration may not capture the fine-grained effect of course stimuli on student learning performance. Therefore, it is necessary to examine the student simulation in lectures with longer duration to reveal further insights."}, {"title": "5.1 Workshop Design", "content": "To this end, we conducted a 6-week online education workshop to deliver 12 lectures, where each lecture lasted 1 hour. The long-duration lectures could not only verify the simulation results, but also reveal new insights about how the simulation models can capture students' learning performance variations across the whole lecture (depicted in Section 6.8). The workshop syllabus is depicted in Appendix Table. 2."}, {"title": "5.1.1 Participants", "content": "We recruited 30 elementary school students, 30 high school students, and 8 instructors from high schools and universities in the local area using emails and social media. We removed the demographic information (such as age and gender) for privacy concerns. Our data collection was approved by the Institutional Review Board (IRB). All participated students and instructors were informed of the experiment form and then signed consent forms. For participants under 18 years old, we obtained the written consent form from both participants and their parents."}, {"title": "5.1.2 Task and Procedure", "content": "We first prompted the students and instructors to watch an introduction video about how to use our online education system to facilitate learning and teaching, as well as the detailed procedures of our data collection (Fig. 7). After that, students were required to first go through a gaze calibration process (depicted in Section. 5.2.2) for accurate gaze collection. Then students were prompted to perform facial expressions (including confused and neutral expressions) in order to train a model for cognitive information detection (more details in Section 5.2.2). After that, both students and instructors were in the same online video conference system (Section 5.2) and instructors presented the course materials to the students. The lecture materials were slides prepared by our research team. During the lectures, our online education system provided visual feedback to the instructors about the students' learning status, and the instructors could adapt their teaching strategies accordingly (depicted in Section. 5.2.3). After the lecture, students were required to finish a post-test composed of 10-12 questions related to each specific lecture to measure their learning outcome."}, {"title": "5.1.3 Experiment Design", "content": "Our six-week workshop was composed of a series of 12 lectures about the basics of artificial intelligence, covering different topics such as basic concepts in machine learning, computer vision, natural language processing, reinforcement learning, etc. Each lecture lasted one hour. The difficulty of the lectures was tailored to match the knowledge level of elementary and high school students, respectively. For each week"}]}