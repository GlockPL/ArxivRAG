{"title": "KNOWLEDGE GRAPH BASED AGENT For CoMPLEX, KNOWLEDGE-INTENSIVE QA IN MEDICINE", "authors": ["Xiaorui Su", "Yibo Wang", "Shanghua Gao", "Xiaolong Liut", "Valentina Giunchiglia", "Djork-Arn\u00e9 Cleverts", "Marinka Zitnik"], "abstract": "Biomedical knowledge is uniquely complex and structured, requiring distinct reasoning strategies compared to other scientific disciplines like physics or chemistry. Biomedical scientists do not rely on a single approach to reasoning; instead, they use various strategies, including rule-based, prototype-based, and case-based reasoning. This diversity calls for flexible approaches that accommodate multiple reasoning strategies while leveraging in-domain knowledge. We introduce KGAREVION, a knowledge graph (KG) based agent designed to address the complexity of knowledge-intensive medical queries. Upon receiving a query, KGAREVION generates relevant triplets by using the knowledge base of the LLM. These triplets are then verified against a grounded KG to filter out erroneous information and ensure that only accurate, relevant data contribute to the final answer. Unlike RAG-based models, this multi-step process ensures robustness in reasoning while adapting to different models of medical reasoning. Evaluations on four gold-standard medical QA datasets show that KGAREVION improves accuracy by over 5.2%, outperforming 15 models in handling complex medical questions. To test its capabilities, we curated three new medical QA datasets with varying levels of semantic complexity, where KGAREVION achieved a 10.4% improvement in accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Medical reasoning involves making diagnostic and therapeutic decisions while also understanding the pathology of diseases (Patel et al., 2005). Unlike many other scientific domains, medical reasoning often relies on vertical reasoning, using analogy more heavily (Patel et al., 2005). For instance, in biomedical research, an organism such as Drosophila is used as an exemplar to model a disease mechanism, which is then applied by analogy to other organisms, including humans. In clinical practice, the patient serves as an exemplar, with generalizations drawn from many overlapping disease models and similar patient populations (Charles et al., 1997; Menche et al., 2015). In contrast, fields like physics and chemistry tend to be horizontally organized, where general principles are applied to specific cases (Blois, 1988). This distinction highlights the unique challenges that medical reasoning poses for question-answering (QA) models.\nWhile large language models (LLMs) (OpenAI, 2024; Dubey et al., 2024; Gao et al., 2024) have demonstrated strong general capabilities, their responses to medical questions often suffer from incorrect retrieval, missing key information, and misalignment with current scientific and medical knowledge. Additionally, they can struggle to provide contextually relevant answers that account for specific local contexts, such as patient demographics or geography, as well as specific areas of biology (Harris, 2023). A major issue lies in these models' inability to systematically integrate different types of evidence. Specifically, they have difficulty combining scientific factual (structured, codified) knowledge derived from formal, rigorous research with tacit (noncodified) knowledge-expertise and lessons learned-which is crucial for contextualizing and interpreting scientific evidence in relation to the specific modifying factors of a given medical question (Harris, 2023).\nLLM-powered QA models often lack such multi-source and grounded knowledge necessary for medical reasoning, which requires understanding the nuanced and specialized nature of medical concepts. Additionally, LLMs trained on general knowledge may struggle to solve medical problems"}, {"title": "2 RELATED WORK", "content": "LLM-based reasoning. General-purpose LLMs (GPT (OpenAI, 2024), LLaMA family (Dubey et al., 2024; Touvron et al., 2023), Mistral (Jiang et al., 2023)), and LLMs fine-tuned on biomedical data (BioMedLM (Venigalla et al., 2022), Codex (Li\u00e9vin et al., 2024), MedAlpaca (Han et al., 2023), Med-PaLM (Singhal et al., 2023), PMC-LLaMA (Wu et al., 2024a)) are used for medical reasoning by leveraging their vast embedded knowledge. Other models utilize the open-ended reasoning capabilities of LLMs to break down queries into sub-tasks, arriving at the final answer step by step, such as Chain-of-Though (CoT) (Wei et al., 2024), CODEX COT (Gramopadhye et al., 2024). However, these methods often struggle with knowledge-intensive medical queries requiring multi-sources an specific knowledge.\nRAG-based models. Self-RAG (Asai et al., 2024) is a pioneering framework that enhances LLM performance through retrieval and self-reflection. LLM-AMT (Wang et al., 2023b) improves medical question answering by integrating authoritative medical textbooks into large language models with specialized knowledge retrieval and self-refinement techniques. Adaptive-RAG (Jeong et al., 2024) introduces a dynamic RAG framework that adapts retrieval strategies based on question complexity. However, its accuracy is constrained by the quality of retrieved knowledge (Zhang et al., 2024).\nKG-based models. Before the rise of LLMs, several models, such as QAGNN (Yasunaga et al., 2021), JointLK (Sun et al., 2022), and Dragon (Yasunaga et al., 2022), were developed to tackle medical queries solely using KGs in an end-to-end manner. However, these methods cannot be easily applied to questions involving unseen nodes or incomplete knowledge within the graphs. In addition, KGs, with their structured and reliable information, have driven research toward RAG models based on graph data, motivating models like GraphRAG (Edge et al., 2024), KG-RAG (Soman et al., 2023), and MedGraphRAG (Wu et al., 2024b). To improve retrieval accuracy, KG-Rank (Yang et al., 2024) is introduced to rank retrieved triplets and filter out irrelevant knowledge. Additionally, Gen-Ground (Shi et al., 2024) uses a Generate-then-Ground pipeline that grounds answers by prompting LLMs to validate retrieved knowledge. However, all these approaches rely heavily on semantic dependencies, overlooking the rich structural information within KGs."}, {"title": "3 APPROACH", "content": "Given is a set of medical questions Q, each question comprising the question stem q, and a set of candidate answers C. For example, the sample question in Fig. 2b has a stem q = \u201cIs there an interaction between the Heat Shock Protein 70 family that acts as a molecular chaperone and the gene or protein implicated in Retinitis Pigmentosa 59 due to DHDDS mutation?\" along with a set of semantically related candidate answers C = {HSPA, HSPA, HSPA1B, HSPA1A}. The goal is to identify the correct answer a \u2208 Cusing an LLM (denoted as P) and a KG (denoted as G). Here, a KG is given as a set of triplets G = {(h,r,t)}, where each triplet consists of a head entity, a relationship, and a tail entity. Full notation is listed in Table D.2. Note that in addition to this multi-choice setting, we consider open-ended reasoning as well (see Results).\nTo address this problem, we propose developing an LLM-powered agent framework (Wu et al., 2023; Li et al., 2023) that leverages various actions (Schick et al., 2023; Shen et al., 2023; Nakano et al., 2021) to collaboratively perform complex tasks (Tang et al., 2023; Bran et al., 2023; Boiko et al., 2023). Fig. 2 shows an overview of KGAREVION, which comprises four key actions, including Generate (\u00a73.1), Review (\u00a73.2), Revise (\u00a73.3), and Answer (\u00a73.3) actions. The Generate action is responsible for generating triplets related to the input question. The Review action then assesses the correctness of each generated triplet, while the Revise action corrects any triplet identified as being incorrect. Finally, the Answer action outputs the final answer based on the triplets identified as correct by the Review action."}, {"title": "3.1 GENERATE ACTION", "content": "The Generate action aims to gather comprehensive structured knowledge from input questions. Specifically, this action first identifies all medical concepts involved in the input question stem q and then generates a set of triplets T related to the question based on the extracted medical concepts."}, {"title": "3.2 REVIEW ACTION", "content": "To enable LLMs to accurately judge the correctness of generated triplets, beyond relying solely on semantic dependencies inferred by LLMs (Shinn et al., 2023), the Review action also leverages the relationships among various medical concepts contained in KGs. This is achieved by fine-tuning the LLM on a KG completion task, explicitly integrating entity structural embeddings learned from KGs into the LLM. Once fine-tuned, the Review action utilizes the model to assess the correctness of generated triplets, as shown in Fig. 2."}, {"title": "3.3 REVISE AND ANSWER ACTIONS", "content": "If F has triplets, KGAREVION calls the Revise action to adjust the triplets in F to include more knowledge that helps with the answering of the input question. The head and tail entities of the revised triplets are then reviewed by the Review action to make sure that they are correct and related to the input question. If the Review action outputs \"True\", then the revised triplets are added to the set of True triplets V. Otherwise, KGAREVION continues to call the Revise action until the max round k (k \u2265 1) is achieved.\nAfter obtaining the set of True triplets from the Review or Revise actions, KGAREVION finally calls the Answer action to prompt the LLM to select the most suitable answer y from the set of answer candidates C of input question q based on the triplets in V, where y = P(q, V, C) and y \u2208 C."}, {"title": "4 RESULTS", "content": "Datasets. We first start with four multi-choice medical QA benchmarks (Xiong et al., 2024a) (Table 1). In addition, we introduce a new benchmark for multi-choice complex medical QA focused on differential diagnosis (DDx), named MedDDx. We begin by collecting questions and corresponding answers from STaRK-Prime (Wu et al., 2024c). For each question, we then select the top three entities with the highest semantic similarity to serve as additional answer candidates. MedDDx comprises a total of 1,769 multi-choice QA samples. Based on the standard deviation of semantic similarity between answer candidates and the correct answer, we categorize the dataset into three difficulty levels: MedDDx-Basic, MedDDx-Intermediate, and MedDDx-Expert (The samples in each dataset are shown in Fig. 2b, and details are available in Appendix 5).\nBaselines. We consider 8 LLM-based reasoning models, 4 RAG-based models, and 3 KG-based models. The LLM-based reasoning models include LLaMA (2-7B/13B, 3-8B, 3.1-8B) (Touvron et al., 2023; Dubey et al., 2024), Mistral (Jiang et al., 2023), MedAlpaca (7B) (Han et al., 2023), PMC-LLaMA (7B) (Wu et al., 2024a), LLaMA3-OpenBioLLM-8B (Ankit Pal, 2024), and MED-ITRON (Chen et al., 2023). The RAG-based models include Self-RAG (Asai et al., 2024), MedRAG (Xiong et al., 2024b), KG-RAG (Soman et al., 2023), and KG-Rank (Yang et al., 2024). The KG-based models include QAGNN (Yasunaga et al., 2021), JointLK (Sun et al., 2022), and Dragon (Yasunaga et al., 2022).\nEvaluation setup. We consider two evaluation settings. Multi-choice reasoning: This setting evaluates the model's performances on all collected multi-choice QA datasets. The model is tasked to select the correct answer to a user input question from a set of candidate answers. Open-ended reasoning: All candidate answers are masked, meaning that the model has to generate a response to the input question independently without being presented with a set of candidate answers. The model produces an answer solely on its own generated response. Additionally, we design two new evaluation scenarios for each setting to test model abilities in solving complex medical questions by considering the number of medical concepts and the semantic similarity among answer candidates. Query complexity scenario (QSS): This is a hard evaluation scenario to test how the model performs with the increase of the number of medical concepts present in a question since the question often becomes more intricate, requiring more nuanced inferences between concepts to achieve the correct answer, with the increase of medical concepts. Semantic complexity scenario (CSS): This is a harder evaluation scenario that tests the model's ability to identify the correct answer among semantically similar and closely medically related candidate answers."}, {"title": "4.1 BENCHMARKING KGAREVION UNDER MULTI-CHOICE REASONING SETTING", "content": "Table 2 shows the accuracy and variance of KGAREVION and all baselines on all datasets. Evaluation on four gold standard medical QA datasets shows that KGAREVION improves the average accuracy by over 4.8%, outperforming all baselines in handling medical queries."}, {"title": "4.2 BENCHMARKING KGAREVION UNDER OPEN-ENDED REASONING SETTING", "content": "We transform multiple-choice questions into descriptive, open-ended ones to better simulate real-world medical scenarios, where such inquiries are more common (see details in Appendix A.3). This adjustment requires our model to generate responses without predefined choices, encouraging holistic reasoning and the integration of diverse knowledge sources. By removing answer choices, we can more effectively assess the reasoning ability of KGAREVION in complex medical situations, resulting in a more realistic evaluation of its capabilities. Table 3 shows the accuracy and variance"}, {"title": "4.3 ABLATION ANALYSES", "content": "Effect of the 'Review' action. As shown in Table 2, 3, and Fig. 4 (KGAREVION (w/o Review) vs. KGAREVION (w/o Revise))), the Review action plays an important role in answering medical questions under two settings, which improves the average accuracy across all datasets by 3.3% and 3%, respectively. Fig. 4 shows that the Review action has a more pronounced effect on the MedDDx dataset than the four gold-standard datasets under two settings, suggesting that its integration enhances the model's ability to tackle complex medical questions. Furthermore, the Review action leads to greater accuracy improvements in the four gold-standard datasets under the open-ended reasoning setting compared to the multiple-choice reasoning setting. This highlights the significance of verifying generated answers, particularly in an open-ended reasoning setup.\nNumber of refinement rounds in the 'Revise' action. The Revise action is designed to enhance accuracy by correcting erroneous triplets until they are verified as true by the Review action. Tables 2 and 3, and Fig. 4 demonstrate its positive impact on KGAREVION across both settings. Specifically, Figure 4 indicates that the Review action significantly improves performance on MedDDx dataset, yielding average enhancements of 9% and 4% in accuracy for both settings compared to questions in the gold-standard datasets. Additionally, we investigate the impact of the number of revision rounds across all datasets in both settings, as shown in Table 2 and 3. The results indicate that KGAREVION can achieve optimal performance with k = 1 on most of datasets in the multi-choice reasoning setting. However, it benefits from additional iterations when addressing complex questions, such as those in the MedDDx-Expert dataset. In the open-ended reasoning setting, KGAREVION typically requires more iterations to arrive at the correct answer."}, {"title": "4.4 VERSATILITY OF KGAREVION", "content": "KGAREVION can be used with different LLMs. KGAREVION is a versatile agent that can be implemented with a variety of LLMs. We implement KGAREVION using three distinct models: LLaMA3-8B, LLaMA3.1-8B, and GPT-40. The averaged results across all datasets, as shown in Fig. 5a, demonstrate the effectiveness of KGAREVION's architecture, consistently improving the performance of the backbone LLMs by 6%, 7%, and 2%, respectively.\nKGAREVION can be used with different medical KGs. The Review action in KGAREVION grounds the generated triplets using KGs. To evaluate the impact of different KGs (see details in Appendix B.5), we implement KGAREVION with two comprehensive KGs and assess its performance across all datasets, as shown in Fig. 5b. The results show that KGAREVION is not sensitive to the choice of knowledge bases, highlighting its robustness and generalizability, despite PrimeKG (Chandak et al., 2023) being much larger than OGB-biokg (Hu et al., 2020). This robustness arises because KGs are used only in the Review action to verify generated triplets rather than as a source for retrieving knowledge. This also explains why KGAREVION outperforms KG-based RAG models, which heavily rely on the chosen KGs, whereas KGAREVION uses comprehensive KGs simply to ensure that the generated triplets are aligned with medical knowledge."}, {"title": "4.5 SENSITIVITY ANALYSES", "content": "Recent studies have revealed that LLMs can be surprisingly sensitive to both how candidate answers are ordered and indexed in multi-choice setups (Zheng et al., 2023; Pezeshkpour & Hruschka, 2023). These studies found that LLMs are not robust multiple-choice selectors and exhibit order sensitivity, favoring answers at the first position (Li et al., 2024). To investigate this issue, we examine how the order and indexing of answers affect model performance. We evaluate KGAREVION using LLaMA3-8B and LLaMA3.1-8B as the backbone and compare its performance with their LLM-only counterparts across all datasets (details are provided in Appendix B.4.1). Fig. 6 illustrates the changes in accuracy when the order or labels of the candidate answers are altered.\nOrdering of candidate answers in multi-choice setups. Fig. 6a shows that pure LLMs are sensitive to answer order, with an average accuracy shift of 8.4% for LLaMA3-8B and 16.0% for LLaMA3.1-8B. In contrast, KGAREVION demonstrates significantly greater robustness to answer order. This robustness is primarily due to KGAREVION's ability to fairly evaluate each answer using the Generate action, effectively mitigating the impact of answer order on model performance.\nIndexing of candidate answers in multi-choice setups. The accuracy of pure LLMs shows a substantial shift when relabeling answers from ABCD to EFGH, as illustrated in Fig. 6b. Specifically, the average accuracy shift is 8.1% for LLaMA3-8B and 12.9% for LLaMA3.1-8B. In contrast, our agent KGAREVION significantly improves the stability of these LLMs, reducing the accuracy loss to 2.59% and 3.86%, respectively. This finding further highlights the robustness of KGAREVION."}, {"title": "4.6 CASE STUDIES", "content": "Fig. 7 illustrates the reasoning process of KGAREVION in both settings, using the same input question. In both cases, KGAREVION arrives at the correct answer, but the reasoning processes differ. In the open reasoning setting, KGAREVION requires more iterations to revise triplets and guide the reasoning compared to the multiple-choice setting. Additionally, the verified correct triplets provide a reasoning path that helps explain the final answer, such as 'partial deletion of the long arm of chromosome 19 \u2192 associated with \u2192 19q13.11 deletion syndrome and partial chromosome 19 deletions \u2192 associated with \u2192 disease'."}, {"title": "5 CONCLUSION", "content": "Medical reasoning presents unique challenges that require integrating multi-source, grounded, and specialized domain knowledge. In this work, we introduced KGAREVION, a KG-based LLM agent that addresses these challenges by combining the non-codified knowledge of LLMs with the structured, codified knowledge of medical concepts stored in KGs. Through its adaptive reasoning and mechanisms for generating, verifying, and revising knowledge, KGAREVION can handle complex medical QA. Experiments across multiple-choice and open-ended tasks, using a variety of datasets including challenging new benchmarks-demonstrate KGAREVION's ability to systematically improve accuracy. By grounding LLM-generated knowledge in KGs, KGAREVION ensures contextual relevance and reliability, making it a valuable tool for knowledge-intensive medical QA."}, {"title": "3.2 REVIEW ACTION", "content": "To enable the LLM to capture the structural information embedded in KGs, we employ TransE (Bordes et al., 2013) to learn structural embeddings for both entities and relations in G. These obtained embeddings are fixed when fine-tuning the LLM on knowledge completion tasks.\nFine-tuning stage. Given a triplet (h,r,t) \u2208 G and its pre-trained embedding eh \u2208 Rd, er \u2208 Rd, and et \u2208 Rd, where d is the dimension of the embedding, the aim of this stage is to learn a function f(.) that makes the LLM capable of determining if a given triplet is True or False by considering its structural embeddings in the KG, as follows:\n\n$b = f(P(\u00b7), (h, r,t), e_h, e_r, e_t), b \\in \\{True, False\\}$\n\nwhere b is a bool value (True or False) and P(\u00b7) denotes the LLM.\nHowever, it is complex for LLMs to directly understand structural embeddings given that the embeddings in LLMs are obtained based on their vocabularies (Radford et al., 2019). To make sure that LLMs can understand structural embeddings, we generate a description for each triplet based on the relationship between the two entities. Then, we tokenize the generated descriptions (Touvron et al., 2023) and obtain the corresponding token embeddings. The token embeddings are aligned with the pre-trained structural embeddings of the triplets to produce new embeddings for each input triplet.\nIn detail, given the input triplet (h, r, t), we denote its description as D(r), where D represents the description dictionary with the key as relation and the value as a pre-defined description template, as shown in Appendix 8. Assuming the token embedding of D(r) obtained from LLM P(\u00b7) is X \u2208 R|l|\u00d7dp, where |l| denotes the number of max tokens and dp denotes the dimension of the embeddings in P(\u00b7), we first adopt a linear layer (g(\u00b7) : Rd \u2192 Rdr) to map the dimension of pre-trained embeddings of entities and relations to the same dimension of that of token embeddings. Then, we concatenate them to create the triplet embedding matrix V = [g(eh); g(er); g(et)] \u2208 R3\u00d7dp. Afterwards, we adopt an attention block (Vaswani, 2017), followed by a two-layer feedforward neural network (denoted as FFN in Fig. 2b) to obtain the aligned triplet embedding matrix Z \u2208 R3\u00d7dp, as follows:\n\n$\\hat{V} = V + \\sigma(VX)X$\n\n$Z = V + (\\left(((V)\\textrm{W}_1\\right))\\textrm{W}_2$\n\nwhere \u03c3(\u00b7) denotes the Softmax function, \u03c6(\u00b7) denotes the layer normalization function,W1 \u2208 Rdp\u00d7dh and W2 \u2208 Rdh\u00d7dp denotes the trainable parameters in the two-layer FFN, and dh denotes the dimension of hidden layer in the FFN.\nThe obtained aligned triplet embedding matrix is input into the LLM in the form of three prefix tokens together with an instruction s to fine-tune the LLM to execute the knowledge graph completion task. Then, the Eq. 2 could be rewritten by:\n\n$b = f(P(\u00b7), D(r), g(e_h), g(e_r), g(e_t)) = f(P(\u00b7), X, V) = f(P(Z, s))$\n\nDuring the fine-tuning stage, the pre-trained entity and relation embeddings are frozen, and LORA (Hu et al., 2022) is adopted to fine-tune the LLM. The trainable parameters are optimized using the next token prediction loss (Radford, 2018).\nInference stage. After fine-tuning, the model is used to evaluate the triplets in T derived from the Generate action (3.1). In detail, we first adopt UMLS code (Bodenreider, 2004) to map entities in each triplet (h, r, t) \u2208 T to KGs to get the corresponding pre-trained entity and relation embeddings, denoted as eh, er, et. Then, the triplets are input into the fine-tuned LLM to determine if they are correct or not by Eq. 5.\nHowever, not all entities in generated triplet (h, r, t) \u2208 T can be mapped to those in KGs. To address this, the Review action applies a soft constraint rule to distinguish whether the generated triplet is factually incorrect or the result of incomplete knowledge in KGs, as follows:\n\u2022 Factually Wrong: if we can map h and t to entities in KGs and b = False, then the triplet (h, r,t) is factually wrong and is removed from T.\n\u2022 Incomplete Knowledge: if we cannot map either h or t to entities in KGs, then the triplet (h, r, t) is considered incomplete knowledge and is kept.\nIn this way, the triplet in T can be grouped into two categories, i.e., the True triplet set V and False triplet set F, where T = V \u222a F and V \u2229 F = \u00d8."}]}