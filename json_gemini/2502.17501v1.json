{"title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game", "authors": ["Qiheng Sun", "Hongwei Zhang", "Haocheng Xia", "Jiayao Zhang", "Jinfei Liu", "Kui Ren"], "abstract": "Large language models (LLMs) have achieved remarkable success on various aspects of human life. However, one of the major challenges in deploying these models is the substantial memory consumption required to store key-value pairs (KV), which imposes significant resource demands. Recent research has focused on KV cache budget allocation, with several approaches proposing head-level budget distribution by evaluating the importance of individual attention heads. These methods, however, assess the importance of heads independently, overlooking their cooperative contributions within the model, which may result in a deviation from their true impact on model performance. In light of this limitation, we propose CoKV, a novel method that models the cooperation between heads in model inference as a cooperative game. By evaluating the contribution of each head within the cooperative game, CoKV can allocate the cache budget more effectively. Extensive experiments show that CoKV achieves state-of-the-art performance on the LongBench benchmark using LLama-3-8B-Instruct and Mistral-7B models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are widely applied across various domains, including content generation, automated services, and decision support systems. To enhance the application capabilities of large language models, it is essential for them to handle long texts. For example, GPT-4 and Llama-3 support a context size of 128k tokens, while the context size of Claude 3 is up to 200k tokens. LLMs consist of multiple transformer blocks that store key and value states (KV) during inference. KV cache allows efficient decoding in token generation without recomputing key and value states by using previously cached KV pairs. However, the KV cache grows excessively large when dealing with long texts, inevitably straining GPU memory and increasing decoding latency.\nEviction of less important key and value states in the cache has garnered significant attention. Many studies have explored methods for ranking the importance of tokens within a single attention head, retaining only the top k most significant ones. For example, H2O evaluates token importance using the sum of attention weights. StreamingLLM directly removes KV from the middle segment of the cache to reduce the cache size as they incorporate less information. SnapKV calculates token scores by pooling the attention weights between tokens in the local window and those in the cache. Recently, some studies have recognized that the importance of each attention head varies, enabling methods like AdaKV and HeadKV. AdaKV improves budget utilization by adaptively allocating the overall budget across different attention heads based on their varied concentration degrees. Heads with sparse concentrations require a small cache proportion, whereas more dispersed heads demand larger allocations. HeadKV evaluates the retrieval-reasoning scores of different heads and allocates a larger cache size to those with higher scores.\nMotivated by evidence that attention heads vary in importance, we propose a novel approach to better evaluate and utilize this variability. We identify two key insights. First, existing methods evaluate attention head importance independently. For example, AdaKV evaluates the concentration degrees of heads while HeadKV assesses the retrieval-reasoning capability of each head in isolation as a measure of importance. However, these approaches treat heads as isolated units, overlooking the fact that their true importance emerges from their cooperation rather than individual capabilities. As a result, independently assessing head importance may lead to suboptimal allocation. Second, existing methods evaluate head importance in a task-agnostic manner. However, heads that play a critical role in query answering may not hold the same level of significance in code generation. Consequently, applying the same importance scores to heads across all tasks within a model may fail to reflect the practical need of each specific task accurately. Based on these insights, we propose CoKV (Cooperation-based Key-Value), a method that evaluates the contribution of all attention heads in their cooperation within the model and dynamically allocates cache budgets based on their contribution to the specific task.\nCoKV is inspired by the Shapley value from cooperative game theory. The Shapley value of a player pi measures the expected marginal contribution that pi provides to a coalition of players. Similarly, we can use the Shapley value to assess the importance of each attention head by viewing each head as a player. Marginal contribution is defined as U(S\u222a {pi}) \u2013 U(S) where S is a coalition of players excluding i and U is the utility function. A simple intuition for computing the Shapley value of each head in the model is to define U as the model performance metric. However, calculating the Shapley value is #P-hard, as there are an exponential number of coalitions and corresponding marginal contributions. As a result, evaluating the Shapley value for each head in LLMs requires an enormous number of model inferences. Although many studies have explored approximating the Shapley value to reduce computational costs, the process remains costly.\nThe computational bottleneck in calculating the Shapley value arises from the fact that each sample of the marginal contribution only can be applied to a single player. Fortunately, Shapley value can be expressed as the expectation of the weighted complementary contribution, defined as U(S) \u2013 U(N \\ S), where N represents the set of all players. Complementary contribution has an advantage over marginal contribution is that U(S) \u2013 U(N \\ S) can be used to update the Shapley values for all players i \u2208 S. By expressing the Shapley value in terms of complementary contributions, we can interpret it as an expectation over these contributions computed at different coalition sizes |S|. However, in the LLM setting, the cost of computing the complementary contributions in all coalition sizes is still prohibitively high. We observe that the average complementary contribution at each coalition size exhibits a strong correlation with the Shapley value of the players in Appendix Section B.3. This insight allows us to approximate attention head importance by computing complementary contributions at only a few selected coalition sizes, rather than evaluating all possible sizes (i.e., from 1 to |N|). By focusing on a few representative coalition sizes, we can significantly reduce the computational cost of estimating the contributions of heads. Additionally, we provide a theoretical analysis of this approach and demonstrate its efficiency.\nCoKV is a simple-yet-effective method and can integrate well with other inference optimization techniques. We integrate CoKV with widely used methods including FlashAttention and grouped-query attention (GQA). CoKV achieves state-of-the-art performance in LongBench using Llama-3-8B-Instruct and Mistral-7B models. Results from the Llama-3-8B-Instruct model show that when each KV cache retains an average of 128 KV pairs (1.6% of the full cache), it achieves 97.29% of the performance of the full KV cache. Furthermore, when each cache retains just 512 tokens on average, CoKV outperforms the full KV cache in terms of average accuracy. This demonstrates that CoKV not only reduces computational costs but also improves inference performance by identifying which heads benefit from cache retention and which may have a detrimental effect. Additionally, we evaluate all methods within the token range of 1k to 31k in the Needle-in-a-Haystack test, where CoKV also demonstrated the best retrieval capability."}, {"title": "2 Preliminaries", "content": "2.1 Key-Value Caching and Compression\nIn Multi-Head Attention (MHA), for each attention head hi in one layer, the embedded input X = {X1,X2,...,xm} \u2208 \\mathbb{R}^{m \\times d_{model}} of m tokens is mapped into different subspaces using query $W^Q_i$, key $W^K_i$, and value $W^V_i \\in \\mathbb{R}^{d_{model} \\times d_h}$ matrices:\nQi = XW\u00ae, K\u2081 = XW,V\u2081 = XW \u2208 R^{m \\times d_h}"}, {"title": "3 Method", "content": "Our method consists of two phases. First, we pre-compute the importance scores for each attention head. Second, these scores are utilized for KV cache compression during inference. The overview of our approach is illustrated in Figure 1.\n3.1 Head Importance Evaluation\nAlthough the complementary contribution helps in increasing efficiency when approximating the Shapley value, it is still computationally costly, especially in the LLM setting. Given a set of players N = {p1,...,pn}, a coalition of j players (1 \u2264 j \u2264 n) is called a j-coalition. Moreover, for a player pi (1 \u2264 i \u2264 n), a j-coalition that contains pi is called a (i, j)-coalition. Denote by Si,j = {S\u222a{pi}|S \u2286 N \\{pi}, |S| = j \u2212 1} the set of (i, j)-coalitions, and by SVi,j the expected complementary contributions of (i, j)-coalitions. That is,\nSVi,j ="}, {"title": "3.2 KV Cache Compression", "content": "Existing KV cache compression methods have partially addressed the importance of layers, yet this consideration remains insufficient during cache allocation. While AdaKV attempts to preserve tokens with larger attention weights across all heads when allocating cache size, it overlooks the varying importance of different attention heads. Conversely,"}, {"title": "4 Experiments", "content": "4.1 Experiment Settings\nDatasets. LongBench is a multitask benchmark for long context understanding and exhibits a wide range of average input lengths, spanning from 1,235 to 18,409 tokens.\nBaselines and Settings. We compare CoKV with four strong KV cache compression methods. All methods keep the same total cache size for fair comparison. Besides, we implement all methods with GQA and FlashAttention for efficient computation.\n\u2022 SnapKV uses the last several tokens as local windows to guide KV cache selection. Attention scores from these windows to the remaining tokens are pooled to cluster and guide the selection process.\n\u2022 PyramidKV allocates more KV cache to lower layers to retain key information while reducing the budget for higher layers where information is already aggregated.\n\u2022 Ada-KV dynamically allocates budgets to heads within each layer based on their concentration degrees, and can be combined with SnapKV or PyramidKV. Ada-SnapKV is used as the baseline due to its superior performance over Ada-PyramidKV.\n\u2022 HeadKV-R2 allocate budgets to heads based on their retrieval-reasoning score, and it uses SnapKV to rank the importance of KV pairs in each head.\nIn CoKV, we allocate the KV cache size for each head based on the normalized Sliced Shapley value of H = {32, 64, 96, 128}. Following HeadKV-R2, we set the local window size to 8, and randomly split each dataset into a validation dataset and a test dataset, with proportions of 15% and 85%, respectively. The hyperparameter a is selected from the set {1, 5, 10, 15, 20, 30, 40}. The validation dataset is used to compute Sliced Shapley value and determine the optimal a for each task. We evaluate CoKV on the Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 models. Due to the page limit, the Mistral-7B-Instruct-v0.2 results are provided in Appendix. For test data that exceeds the maximum input length of Llama-3-8B-Instruct, we adopt the approach of HeadKV by utilizing the first 4k tokens and the last 4k tokens. Following standard practices in prior studies, we perform cache eviction after the prefilling phase of each layer for consistent comparison. In GQA, a group of 4 heads shares the same KV cache. We treat each cache within a group as a player in a cooperative game, evaluating their Sliced Shapley value to determine their importance scores. For HeadKV-R2, we calculate the importance score of each group by averaging the retrieval-reasoning scores of the 4 heads within the group. This adaptation ensures compatibility with GQA, as HeadKV is implemented with MHA in the original paper. For the efficiency and computation cost analysis of Sliced Shapley value, please refer to Appendix Section B.1. For the test in Needle-in-a-Haystack, please refer to Appendix Section B.5."}, {"title": "4.2 Main Results", "content": "Benchmark Results. The complete benchmark results are presented in Tables 4 and 5 in the appendix. We include a simplified table (Table 1), showing the performance of Llama-3-8B-Instruct when keeping 64-128 KV pairs on average. The results demonstrate that CoKV consistently outperforms all baseline methods. The average accuracy of the two models on 16 datasets are presented in Figure 2. Notably, in Llama-3-8B-Instruct, with an average of 128 tokens cached per group KV cache, CoKV retains 97.29% of the model performance. Furthermore, CoKV significantly surpasses FullKV when it maintains an average of over 512 KV pairs per group cache. When retains an average of 1024 KV, the average results of both models outperform FullKV. This demonstrates that CoKV achieves near-lossless performance under resource-constrained settings. The superior performance of CoKV arises from its ability to effectively evaluate the importance of each cache within a group while considering the cooperation among all groups. It is not only capable of identifying which groups are important but also able to recognize those groups that do not contribute or even have a negative contribution. By optimizing the cache size to enhance overall cooperation, CoKV ensures efficient and high-quality inference.\nHyperparameter Free Results. Since both HeadKV-R2 and CoKV provide importance scores for each group, we conduct an experiment to compare their effectiveness without introducing any additional hyperparameters. In this experiment, we mask the caches of groups based on the importance scores assigned by each algorithm. Specifically, we mask the caches of both the highest-ranked (top) and lowest-ranked groups (low). The complete results are shown in Tables 6 and 7 in the appendix. We include a simplified table for the results of masking 16,128 groups of Llama-3-8B-Instruct model in Table 2. The results show that when masking the top-ranked groups identified by each method, the performance of CoKV degrades more significantly than that of HeadKV-R2. Conversely, when masking the unimportant groups (low), the performance of CoKV declines more gradually than HeadKV-R2. This suggests that CoKV is more effective at ranking group importance, as it better distinguishes between critical and non-critical caches. The results of masking 16 groups in both models outperformed the FullKV approach as shown in Figure 3. This further demonstrates that CoKV can identify groups that have a negative impact on the model. By removing the KV pairs from these groups, the model inference not only optimizes storage and decoding speed but also enhances overall performance."}, {"title": "4.3 Decoding Latency and Memory Usage", "content": "We conduct experiments using the Mistral-7B-Instruct-v0.2 model, which supports a maximum context window of 32k tokens, with FlashAttention enabled as the default setting, on an A100 GPU with 40GB of memory. We design two key experiments with the average KV cache size set to 128 tokens(comparative experiments showed less than 2% variation across 64/256/512/1024 tokens).\nDecoding Latency With a fixed input context length of 28k tokens, we measure decoding latency (including both the pre-filling time and the decoding time) across different generation lengths (1/512/1024/2048/4096 tokens). As shown in the Decoding Latency of Figure 4, CoKV achieves less than 50% of the total latency compared to the FullKV baseline, with negligible differences observed between the other baselines.\nPeak Memory Usage Under fixed generation length (1 token), we measure the peak GPU memory usage (including model parameters and runtime states) across varying input contexts (1k/2k/4k/8k/16k/32k tokens). As shown in the Peak Memory Usage of Figure 4, CoKV reduces memory usage by 64% compared to FullKV baseline at 32k input length."}, {"title": "5 Conclusion", "content": "Large language models (LLMs) face significant challenges in handling long texts due to the excessive memory and latency overhead caused by the growing size of the KV cache. To this end, we introduce the Sliced Shapley value (SSV) to evaluate the collaborative importance of attention heads and a novel method called CoKV to dynamically allocate cache sizes based on SSV. Our experimental results demonstrate that CoKV achieves state-of-the-art performance across 16 LongBench datasets, outperforming the full KV cache in 9 datasets while reducing memory and latency overhead. CoKV provides a scalable and practical solution for enhancing the efficiency of LLMs in real-world applications."}, {"title": "Limitations", "content": "Our work has two main limitations that suggest future research directions:\nTask-specific constraint: CoKV requires calculating head importance scores for different tasks. While experiments in Appendix Section B.4 demonstrate strong generalizability across datasets within the same task category. Despite this constraint, CoKV is highly practical for LLM providers serving diverse users. Users can simply select their task type, and the model will apply the corresponding head importance scores for KV cache compression. Importantly, the underlying inference process remains consistent across all tasks; only the cache budget allocation varies based on the task-specific importance scores. This ensures both flexibility and efficiency, enabling the model to adapt to various user needs without requiring significant changes to its core architecture.\nPrecomputation cost: The computation of importance based on cooperative game theory for attention heads is computationally intensive. Although we propose the Sliced Shapley Value (SSV), which significantly reduces the computational cost, our precomputation overhead remains higher than that of baseline methods. However, our experiments in Appendix Section B.1 demonstrate that this precomputation is still entirely acceptable. We plan to address optimizing computational complexity as one of our future research directions by developing efficient approximation algorithms and parallel computing strategies."}, {"title": "Appendix", "content": "A Related Works\nKV Cache Compression The memory overhead of storing key-value (KV) pairs for LLM has motivated extensive research on KV cache compression. StreamingLLM preserves the initial and recent tokens, which empirically exhibit higher informativeness during generation. Similarly, Scissorhands proposes the persistence of importance to identify and retain pivotal tokens. H2O employs a heavy-hitter oracle to drop tokens with low attention scores. SnapKV uses the attention scores of the recent tokens to retain critical tokens. While these methods reduce memory usage and accelerate inference, they implicitly assume uniform importance across attention heads, limiting their applicability. Recent works address head diversity through layer-wise and head-wise optimizations. PyramidKV implements a hierarchical allocation strategy, assigning larger cache budgets to lower layers based on the observed attention patterns across layers. Fast-Gen is an adaptive KV cache compression method that reduces LLMs' memory usage by profiling attention modules and constructing caches adaptively. RazorAttention and Duoattention divide attention heads into retrieval heads(critical for long-context processing) and non-retrieval heads, apply full KV cache to retrieval heads and compressed KV cache to non-retrieval heads. ArkVale proposes a page-based KV cache manager that asynchronously copies filled pages into external memory (e.g., CPU memory) as a backup and supports the recall of important tokens that were previously evicted. AdaKV dynamically adjusts cache budgets across heads based on their concentration degrees and HeadKV calculates head importance scores to allocate cache budget before inference. However, these methods assess heads in isolation, neglecting their collaborative interactions. For example, the standalone score of a head may not reflect its true contribution when working synergistically with others. Additionally, these approaches overlook the task-dependent variations in head importance. Our approach tackles these limitations by modeling head interactions as a cooperative game, dynamically allocating cache resources based on the varying complementary contributions of heads across different tasks.\nIn addition to KV cache eviction methods, KV cache quantization is also one of the mainstream approaches for KV cache compression. However, while eviction methods can be used to retain less than 1% of the cache, KV cache compression cannot be applied to such an extent because it must preserve at least 1 bit. Nevertheless, the combination of these two methods is an interesting direction for future research.\nModel Architecture and Computation Optimization Modern LLMs employ architectural optimizations to balance efficiency and performance. Multi Query Attention (MQA) shares a single key-value pair across all attention heads, drastically reducing memory usage but potentially sacrificing performance. Group Query Attention (GQA) strikes a balance by grouping heads to share key-value pairs, preserving performance while maintaining memory efficiency, which is widely adopted in recent LLMs like Llama and Mistral. Concurrently, Flash Attention optimizes hardware utilization by minimizing memory reads/writes during attention computation, significantly accelerating inference. Notably, our approach is fully compatible with GQA and Flash Attention, ensuring seamless integration with current LLMs.\nCooperative Game Theory Cooperative game theory offers a principled framework for understanding how multiple players can jointly contribute to overall system performance. Shapley value, a classic solution in cooperative game theory, provides a method for fairly allocating joint benefits based on the marginal contribution of each player. However, traditional Shapley value computation methods allow each sample to be used to calculate the marginal contribution of only a single player. Recent works address this limitation through complementary contributions that enable simultaneous estimation of multiple players' contributions. In the context of LLMs, these methods still encounter scalability issues, as the cost of computing complementary contributions across all coalition sizes remains prohibitively high. We propose the Sliced Shapley value, which samples only a subset of coalition sizes. This approach not only accelerates the computation but also accurately re"}, {"title": "B Supplementary experiments", "content": "We introduce the detailed information of Long-Bench in Table 3, including the task types, evaluation metrics, average length, languages, and the number of samples for each task..\nB.1 Computation Efficiency\nWe conduct experiments to demonstrate the efficiency of approximating the Sliced Shapley value using the qasper dataset with the Llama-3-8B-Instruct model. We randomly select 15% of the qasper dataset as the validation set to compute the Sliced Shapley value. The experiments are performed on a server equipped with 8 RTX 3090 GPUs. We compute the Sliced Shapley value for coalition sizes of {32, 64, 96, 128}. GPUs 0-3 are assigned to compute the complementary contributions for coalitions of sizes {32, 64, 96, 128}, respectively, while GPUs 4-7 compute another independent Sliced Shapley value. Table 8 shows the computation time for each GPU from 50 to 500 samples of complementary contributions, as well as the mean absolute error (MAE) between the two independently computed Sliced Shapley values. The MAE is calculated as:\nMAE = \\frac{\\sum_{i=1}^{n} |SSV_i^H - SSV_i^{H'}|}{n},\nwhere SSV and SSV represent the Sliced Shapley values from the two independent computations. The experimental results show that when the number of samples reaches 250 for each coalition size, the MAE is 3.8e \u2013 3 < 1/256 with 20.93 hours. In GQA inference, the Llama-3-8B-Instruct model has a total of 32 \u00d7 8 = 256 groups. Since the model accuracy lies in the range [0, 1], when the MAE between two sampling runs is less than 1/256, the sum of absolute errors across all groups is less than 1. At this point, the Sliced Shapley value can reliably reflect the contributions of the groups.\nWe recommend performing two independent sampling runs when computing the Sliced Shapley value for a task. The sampling results are considered stable when the mean absolute error between the two runs is less than 1/n, where n represents the number of players in the cooperative game. At this point, the results from the two sampling runs can be averaged and used as the importance scores of the heads in the model.\nB.2 Distribution of Sliced Shapley Value\nFigures 5 and 6 illustrate the distribution of the Sliced Shapley values computed for selected coalition sizes H = {32, 64, 96, 128} in our experiment. We observe that the distributions of Sliced Shapley values exhibit significant differences across datasets of different task categories, while showing relatively smaller variations within datasets of the same domain type.\nB.3 Distribution of j-coalition Complementary Contribution\nIn Figures 7 and 8, we present the distributions of the expected complementary contributions of heads in Llama-3-8B-Instruct model on the hotpotqa dataset (multi-document question answering) and the lcc dataset (code generation), with coalition sizes of {32, 64, 96, 128, 160, 192, 224}. We observe strong correlations in the distributions across all coalition sizes. Additionally, the distributions of the expected complementary contributions for coalition sizes S and n-|S| are nearly identical, exhibiting symmetry around the size of 128. To optimize computational efficiency, we restrict the calculation of complementary contributions to coalitions with sizes below 128. These observations provide a justification for our approach of computing complementary contributions using only a small subset of coalition sizes, as it effectively captures the contributions of the heads.\nB.4 Generalization\nTo validate the generalization capability of our method, we conduct cross-dataset evaluations on two task categories: 1. Multi-Document QA including 2WikiMQA and Musique datasets. 2. Code Processing including Lcc and RB-P datasets.\nFollowing Section 4.2, we mask top and low-ranked attention heads but cross-apply head importance scores between datasets within the same task (e.g., mask 2WikiMQA using Musique-derived scores). As shown in Table 9 and Table 10, our method maintains superior accuracy over baselines across both models, confirming that learned importance scores can generalize across datasets within shared task domains.\nB.5 Needle-in-a-Haystack Test\nTo evaluate the performance of different KV cache compression methods in long-context retrieval tasks, we conduct a Needle-in-a-Haystack benchmark test using the Mistral-7B-v0.2 model. With the average KV cache size 128, we systematically insert target texts (needles) at ten equidistant positions (11%, 22%, ..., 100%) across varying context lengths ranging from 1,000 to 31,000 tokens (in 1,000-token increments). Experimental results demonstrate that CoKV outperforms other baseline methods, achieving an average score of 95.89% - the closest performance to the uncompressed FullIKV benchmark."}, {"title": "C Proof", "content": "In this section, we give the proof of Theorem 1. Denote H the selected coalition sizes. The approximation of SVi,j(1 \u2264 i,j \u2264 n) is unbiased, which can be proven following Corollary 1 in . So it is evident that SSVi, being the weighted average of SVi,j, serves as an unbiased estimator of SSV\u2081. Hence, we have\nP(|SSVH \u2013 SSV \u2265 \u20ac)\n<P( \u2211=|SVi,j \u2013 SVi,j| \u2265 \u20ac)\nJEH\n\u2264P(\u03a3|SVij - SV H \u2265 \u20ac)\n\u22642|H|exp( )."}]}