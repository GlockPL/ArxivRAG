{"title": "Semantic Retrieval at Walmart", "authors": ["Alessandro Magnani", "Feng Liu", "Suthee Chaidaroon", "Sachin Yadav", "Praveen Reddy Suram", "Ajit Puthenputhussery", "Sijie Chen", "Min Xie", "Anirudh Kashi", "Tony Lee", "Ciya Liao"], "abstract": "In product search, the retrieval of candidate products before re-ranking is more critical and challenging than other search like web search, especially for tail queries, which have a complex and specific search intent. In this paper, we present a hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries. Our system significantly improved the relevance of the search engine, measured by both offline and online evaluations. The improvements were achieved through a combination of different approaches. We present a new technique to train the neural model at scale. and describe how the system was deployed in production with little impact on response time. We highlight multiple learnings and practical tricks that were used in the deployment of this system.", "sections": [{"title": "1 INTRODUCTION", "content": "Search is one of the most important channels for customers to discover products on an e-commerce website such as Walmart.com. Given our huge catalog which contains millions of products, helping users find relevant products for their queries is a very challenging problem [32]. Existing literature on information retrieval focuses mostly on web search [25]. While product search shares many common challenges with web search, there are many unique aspects of product search. Like other search, product search usually involves two steps: the first step is to retrieve all relevant products from the catalog that form the recall set; these candidate products then go through a re-rank step to identify which products are the best to return to the customer.\nOne major difference with web search is that the retrieval step in product search is a more critical and challenging problem [3, 5, 18]. This is because product titles (the main search-able text) are generally much shorter than web documents. Also, while many web documents may contain the same information, a specific product from a seller rarely has a duplicate. Retrieving a specific product while matching on shorter text is a challenging problem.\nTraditionally, retrieval is based on text match between queries and documents, utilizing a heuristic score function like Okapi BM25 [25] and an inverted index [1] like Apache Lucene\u00b9. Text"}, {"title": "2 RELATED WORK", "content": "Neural information retrieval (NIR) has been a popular topic in the search community recently. It leverages a set of sub-topics such as unsupervised learning of text embeddings like word2vec [26], deep Siamese [4] models based on query logs like CLSM [33] and DSSM [14], and query document interaction-based models like kernel pooling [39]. A good summary of the field can be obtained in [27]. In [23] the authors consider the tradeoff between sparse and dense retrieval and propose a multi embedding approach.\nMultiple companies have described their production systems leveraging semantic retrieval. In [42], a two-tower neural model is trained using a mixed negative sampling in addition to batch random negatives. Baidu [44] described a production system that leverages multiple model pre-training strategies. In [13, 22] Facebook presented a system that combines an inverted index with semantic retrieval; the presented architecture includes multiple product features like images and titles. Taobao [43] proposed a way to better learn relevance from user engagement data. Sears used embeddings to represent products for a recommendation system [2]. In [29], Amazon presented a retrieval system based on a bag-of-words model; similar to our system, semantic retrieval was used in combination with a standard inverted index system. In [10] a residual-based learning framework was used to learn embeddings that compensate for shortcomings of the inverted index. In our approach the two systems are created separately.\nNegative item selection. Multiple papers have investigated the problem of selecting negative samples to be used during training. Several works considered caching embeddings for the entire dataset [11, 41]. In [41], an iterative approach was suggested to find negative samples. We follow a similar approach, and extend it to improve the results in the common situation where not all relative items are known for a query. In [12], a streaming negative cache was used, but it cannot work for dual encoder training.\nMultiple embeddings. There are also approaches where queries and items are represented by multiple embeddings for retrieval and ranking [8, 15, 20, 23, 24]. SPLADE [7] produces a sparse representation at the token level that improves storage requirements, and COIL [9] produces a token-level representation only for matching between query and document.\nTraining strategies. In [19], they showed for a question-answering task how a simple training strategy can effectively beat a state-of-the-art system. In [31], they used a teacher cross-interaction model, to help the training and selection of true negatives."}, {"title": "3 ARCHITECTURE", "content": "We propose a hybrid architecture that leverages the advantages of both traditional inverted index and neural retrieval. A traditional inverted index is still state of the art for retrieving documents with rare tokens [23] such as product ids and model numbers. Moreover, our production inverted index has capabilities like facet navigation and category filtering, which are hard to replicate using semantic retrieval alone. On the other hand, semantic retrieval helps bridge the vocabulary gap especially for tail queries; it helps with synonyms, misspellings, and other query variants that users type. Semantic retrieval also helps to better understand the semantics of longer queries that might contain a nuanced intent from the user.\nThe overall architecture is shown in Figure 1. When a user types a query, it is directed to the Query Planner to generate a query plan for inverted-index retrieval as well as a query embedding vector sent to the ANN Fetcher. The query embedding is then sent to an approximate nearest neighbor (ANN) index to retrieve the items with the closest embeddings. The ANN index contains the embeddings of all the products currently available in our catalog. When new products become available, a dedicated pipeline feeds the product information to the product embedding model to generate the embeddings. The embeddings are then stored in the updated ANN index.\nBoth the inverted index and ANN index retrieve a set of products which are merged to become the recall set. Finally, the retrieved items are ranked by our re-ranking system to produce the final list of products to be shown to the customers."}, {"title": "4 SEMANTIC MODEL", "content": "The semantic model architecture is a two tower structure as shown in Figure 2. Each tower produces an embedding for query and product respectively. The score of a query and product pair is the cosine similarity of the embeddings. We experimented with the inner product of the embedding as well (see Section 9).\nThe product information consists of a title, description, and a number of attribute values. Attribute values are, for example, color, brand, material. Attributes are not always available for all products. We experimented with different number of attributes in Section 8.\nThere are two main classes of model used. The first one is a traditional bag of words model [16, 29] (BoW) and the second is based on a BERT [35] architecture. The BERT based model is far superior for this application and it will be our main focus. We report the performance of the BoW model as a baseline."}, {"title": "4.1 Models", "content": "We have experimented with different transformer architecture by leveraging HuggingFace pre-trained models repository \u00b2. Specifically we have used the BaseBERT with 12 layers and 1024 embedding size and DistillBERT with 6 layers and 768 embedding size. We report below on the performance of different model architectures. We use the pre-trained tokenizer, and all training is done by starting with the pre-trained model. Our experiments use identical towers (Siamese network). For most experiments, we use the embedding vector corresponding to the special token '[CLS]'.\nIn our experiments in Section 8, titles provide most of the signal for retrieval. We use a baseline model with only title as input. We then added more attributes to the input. Each attribute is concatenated to the title by using a prefix which is an unused token selected specifically for the attribute. For example, when adding the color red to a product, the input looks like '[title tokens] [color token] red'. This technique allows the model to determine which attributes have been concatenated. We experiment with four common attributes (product category, brand, color and gender) and with a longer list of 26 attributes including the basic ones."}, {"title": "4.2 Loss function", "content": "We used a sampled softmax loss where for each query we have both relevant and irrelevant products with a corresponding score. As described in section 5.2, selecting negative items is essential for good performance. We also notice that allowing multiple relevant products during training helps. Since there are, in general, many relevant products for a given query, we sample a few relevant products for each epoch.\nEquation 1 shows the loss contribution of query i, where N is the number of products under consideration, Sij is the score of product j for query i, and qi, pj are the embedding for query i and product j respectively. o is a temperature factor that is trained together with all the model parameters. Other loss functions have been evaluated including pointwise and pairwise losses, but the softmax loss outperforms them and is very robust during training. The number of products considered for each query is N = 20. There is a trade-off between N and the batch size. Increasing N reduces the batch size and therefore also the in-batch negatives.\nWe will discuss in Section 5 how the scores Sij are selected and how the products are selected in more detail."}, {"title": "5 DATA", "content": "The training of the model is performed on engagement data collected at Walmart.com over a one year period. The data contains the top 2 million queries based on number of impressions. For each query, we consider the products that were shown to the customers, and construct labels based on the corresponding number of purchases, clicks, and impressions. Note that we do not account for presentation bias, since in a retrieval model, the order in which items are returned is not relevant. In our experience correcting for presentation bias adds more complexity for a negligible performance improvement."}, {"title": "5.1 Labeling", "content": "For each query and product pair we assign a score Sij between 0 and 10. Since the loss is insensitive up to a multiplicative factor, the range of score can be selected arbitrarily. Query-product pairs with purchases are assigned the highest scores between 10 and 8. Products that only received clicks are assigned scores between 7 and 5, and if products only received impressions scores between 4 and 2. We assigned scores of 0 to negative items as described in section 5.2.\nOrdered products are assigned a score based on a smoothed estimation of their order rate $rate = \\frac{orders+a}{impressions+a}$ where a is the smoothing factor. The product with the highest order rate receives a score of 10, while the product with the lowest order rate receives a score of 8 following equation $S = (10-8) \\frac{rate-min(rate)}{maxrate-min rate}+ 8$. A similar approach is used for products that received only clicks.\nAlthough this labeling strategy is arbitrary, it has been shown to be effective in practice. In particular differentiating the scores of items that have been purchased from items that have only been clicked is in our experience essential to create effective retrieval models. This is consistent with [29] even though we use a different loss."}, {"title": "5.2 Negative item selection", "content": "The selection of negative items is necessary to help the model discern a relevant title among millions of products. We used two sources of negative products: random products from within a batch"}, {"title": "5.2.1 In batch negatives", "content": "Selecting negative items from a batch is a common technique that reduces the computation because the embeddings are already computed for all the products in the batch. Since for memory constraints, it is not possible to select all products in a batch as negative, we experimented with two ways of sampling them. The first approach is a random selection. The second approach is to perform a hard in batch selection [22]. This approach focuses on only the hardest samples in a training batch. For a given query in a batch the negatives are generated by using all the products from other queries in the batch as a pool and selecting only the ones which receive the highest cosine similarity."}, {"title": "5.2.2 Hard negative search", "content": "Out of the several negative construction approaches available, we used ANCE [41] to which we added novel strategies that cause the model to retrieve more relevant results, thereby boosting the recall metric. Unlike in the original paper [41] where the relevant items are known, in the e-commerce setting, there can be many relevant products for a given query. For example, a query like \"red shoes\" can have hundreds of relevant products. Since it is impractical to editorially evaluate all of them, we introduce a set of different heuristics to overcome this limitation.\nThe procedure to generate hard negatives is as follows: 1) Generate top-k results for each query in training corpus using a partially trained model 2) Select negatives from top-k results based on a selection strategy (see below) 3) inject the hard negatives into the training corpus and resume training 4) repeat the steps 1-3.\nWe explored three strategies to select negatives out of the top-k results.\n\u2022 PT match: For each item in the top-k results of a given query, if the item is not in the training data and matches the product type (PT) attribute of the top-m items in the training data, it is removed from the set of negative candidates.\n\u2022 Token match: The negative candidates generated using this strategy are more strict. We take PT match candidates as an input to this and for each candidate item find its overlap with query tokens. If the overlap score is below threshold t, we keep the item; otherwise discard it. We have experimented with different thresholds and found that t=0.5 gave best results.\n\u2022 Student-Teacher: We trained a separate model (Teacher) wherein the query tokens can directly attend to product tokens. The teacher model is a MonoBERT-based [38] single encoder network that concatenates query and the product information together as input. The top-k items are generated via this model, and the above PT match filter is applied to get the final negative candidates that are then injected into the embedding model (Student)."}, {"title": "5.3 Reducing the model size", "content": "The initial embedding size was 768 for which the best recall was obtained in our offline evaluations. This embedding size creates a fairly large storage footprint for the ANN index (Section 7.1) and"}, {"title": "5.4 Multi Embeddings", "content": "E-commerce queries often have multiple interpretations. For example, the query \"apple\" could refer to the fruit or to the electronics brand. For this reason, we explore the possibility of having multiple embeddings to represent a query or product. For a query, this means to generate m embeddings, and make m calls to the ANN service. Denoting $q^{i}_{f}$ as the dth embedding of the ith query, the product score is $max_{p_{j}} (cos(q^{i}_{f}, p_{j}))$ for $d \\in 1...m$. Following [23], we let the m embeddings correspond to the first m tokens. This is a natural extension of using the embedding corresponding to the first token '[CLS]'. For multiple embeddings on the product side, like in [23], there are n embeddings, and we therefore store in the ANN service n times the number of products."}, {"title": "5.5 Freezing token embeddings", "content": "We also experimented with a different setup where the token embeddings are kept frozen to preserve the learning of the pretrained model. The hypothesis is that the pretrained model which has been trained on a much larger dataset, can better preserve its learning if the embeddings are not changed."}, {"title": "5.6 Different pooling", "content": "The last experiments are regarding the pooling mechanism on top of the BERT model. We tried three different options. The first one uses the output of the [CLS] token. The second uses the average of the embeddings coming from all tokens. The third uses the maximum per component across all tokens."}, {"title": "6 RE-RANK STAGE", "content": "In this section, we describe how the two recall sets from the inverted index and semantic retrieval are merged and ranked. A key aspect of e-commerce search is the presence of useful ranking features that capture the different query and product attributes. The features used in the ranking model can be organized into the following groups:\n\u2022 Query Features: These features capture the different attributes and properties learnt from the query. For example, query attributes like product type, brand, query length.\n\u2022 Item Features: These features capture the different product attributes as well as engagement features computed at the product level. For example, title attributes, title length, user ratings, user reviews, product sales, product department, etc.\n\u2022 Query-Item Features: These features capture the relations related to the query-item pair. For example, BM25 text match score, query-item engagement, query-item attribute match score.\nFigure 1 also shows the re-rank architecture with the query-product BERT embedding feature. The query is encoded to the query embedding at runtime. The item embedding for all the items in the catalog are pre-generated and saved to the item embedding datastore. For all the items from both the inverted index and ANN, the corresponding item embeddings are fetched from the item embedding datastore.\nAt the re-rank layer, the cosine similarity between query and item embeddings is included as a query-item feature. The re-rank model, which is a Gradient Boosted Decision Tree (GBDT) model, ranks all items in the merged list."}, {"title": "7 IMPLEMENTATION", "content": "In this section we discuss some of the engineering considerations and challenges in implementing the hybrid retrieval system in production."}, {"title": "7.1 ANN service", "content": "One major challenge for online neural product retrieval is the trade-off among accuracy, speed, and memory. Brute force algorithms that retrieve the exact k closest vectors of a query vector from millions of item vectors with respect to a pre-defined distance cannot be used in the production setup due to their high time complexity. Some amelioration can be obtained by first compressing the data size so that it may be easier for the vectors to be fed into memory. Such techniques include but are not limited to locality sensitive hashing (LSH), quantization, and product quantization (PQ). For faster retrieval, not all indices will be scanned when a query is executed. Candidate vectors are usually split into multiple clusters, and only vectors in the closest few clusters will be scanned - if not further reduced by other pruning methods.\nTools and services that support approximate nearest neighbor (ANN) search have emerged in the past few years. One popular tool among developer communities and has the potential to be productionized is FAISS\u00b3. However, to reduce the cost of maintenance and to minimize the system level risk, as the real traffic might surge during certain periods of time, we use a managed ANN service available commercially.\nAs with all other ANN algorithms, hyperparameter tuning is necessary to achieve the desired recall quality within an acceptable level of latency, storage, and computation cost. Our experiments show that with normalized vectors of dimension 256, the ANN services can yield 99% for recall@20, evaluated against the full nearest neighborhood search, with an average latency around 13 ms;"}, {"title": "7.2 Runtime Implementation", "content": "The in-house search engine accepts queries from customers. If a query is eligible for neural retrieval (i.e. a tail query), the query planner sends its embedding vector, which is the output of the query encoder, to the ANN index. The results of the ANN index are cached with a preset time-to-alive (TTL) to reduce latency and cost.\nThe recall federation framework retrieves products from both the ANN index and the inverted index, and then de-duplicates and merges the product sets before sending them to the rerank phase."}, {"title": "7.3 Query modeling latency and consideration", "content": "In real production scenarios, a large portion of queries submitted by users are not predictable and hence cannot be vectorized offline beforehand to reduce the overall runtime latency. For simple models such as the Bag-of-Words (BoW) model, the computation is fast and usually does not raise any concern. However, the power of such simple models is also limited for this application. In contrast, BERT-based models describe in section 4 and their variations have drawn a lot of attentions to their capacity in semantic understanding, as well as to its application in solving search ranking problems, and have shown in our experiments there much higher performance along multiple metrics. Unfortunately, BERT-based models also require more computation resources and may increase response time of the runtime system.\nWe integrate the BERT-based query encoder as part of our query understanding module. We found that with the same capacity of computing clusters with CPUs, the 6-layer Distilled BERT model almost doubled the P99 latency of the query understanding module. Our original plan was to reduce the number of layers from 6 to 2. However, we found that the latency was not reduced linearly with the number of layers and the contribution of the token embedding lookup was substantial.\nOur model is exported from Torch checkpoints into ONNX\u2074 format and is served in a Java codebase. The embedding lookup operation implementation in the ONNX backend seems to be highly inefficient. Therefore, in our experiments, we tried to move this embedding lookup to Java hashmap and to feed the model with"}, {"title": "8 EXPERIMENTS AND RESULTS", "content": "All modeling effort has been performed using a dataset of 2 million queries collected over a period of one year from Walmart.com logs. The dataset was divided between a training and validation dataset of size 90% and 10% respectively. The test dataset contains 140 thousand queries disjoint from both training and validation for which a set of relevant items has been identified using user engagement and editorial feedback.\nThe training was performed using PyTorch [30] and Hugging-Face. Adam [21] was used to train with a learning rate of 10-5. The batch size was 40, and the number of products during training for each query was set to 20."}, {"title": "8.1 Offline Metrics", "content": "We evaluate the models using a recall metric which measures the percentage of relevant products retrieved for a golden dataset. The golden dataset contains a set of relevant products for 140 thousand queries out of a set of around 7 million products. Since scoring all products for a given query is not possible, only a subset of the relevant products is known.\nIt has been clear that the recall metric alone does not fully capture the performance of the semantic search. We noticed that the model could have a relatively high recall while at the same time retrieving a set of completely unrelated products. This is in part due to the small user engagement available for some products as well as the presence in the catalog of products with noisy titles. For this reason, we define a new metric that tries to measure the approximate number of irrelevant products. Since in e-commerce, each product has a product category associated with it (e.g. dining chairs, toothpaste),"}, {"title": "8.2 Offline model results", "content": "In this section, we report the results of our modeling effort based on the two metrics described in Section 8.1. All the results are reported with respect to a baseline model specified for each set of experiments. We also report as baseline the performance of a simple inverted index implementation where only the title of the product is indexed. In Table 3, we report all the main findings with respect to number of layers and embedding size. We observed that the BoW model had lower offline numbers compared to inverted index lookup. After switching to BERT based model, we were able to beat the baseline by 8.25% in Recall@40 and switching to DistilBERT model gave an extra lift of 4%. We experimented with including product attributes, and got a further 6% boost after including attributes like product category, brand, color and gender to the input for a product. But, when we added even more product attributes (like description etc.), we did not see any further improvement in model performance. Notice how Category Recall@40 is dramatically lower without the use of negatives.\nIn Table 4, the performance of different negative selection techniques are shown. The first row corresponds to the best model in Table 3. When we added hard negatives to the training data, described in section 5.2, we observed 0.87% lift in Recall@40 and 18% lift Category Recall@40 when using only product category match. When combining that with token match, the improvement on recall is 2.8%. On the other hand, our student-teacher negative selection"}, {"title": "8.3 Live Experiments", "content": "8.3.1 Manual Evaluation Results. We evaluated the performance of our proposed architecture by using human assessors to evaluate the top-10 ranking results of the proposed architecture and the current production at Walmart. The candidate model uses an embedding size of 256 and uses the \"[CLS]\" pooling. For a query, the human assessors are shown the product image, title and price of the product along with the product link in Walmart website. They rate the relevance of the product on a 3-point grading scale as not relevant, relevant with missing attributes, and perfect match. The queries are selected via random sample of the search traffic from the tail segment. As shown in Table 9, the proposed architecture significantly improves the relevance of tail queries. Note that BERT embedding with dimension 256 performed similarly or even better than dimension 768.\n8.3.2 Interleaving Results. We assessed the user engagement performance of our proposed architecture compared with the current production at Walmart using interleaving [17]. We evaluate two different models that have the same DistillBERT architecture but in one case they have a linear projection layer to an embedding size of 256. Interleaving is an online evaluation approach where each user is presented with a combination of ranking results from both the control and variation. We observe the add-to-carts (ATC) between the control and variation ranking. The metric measured is ATC@40 which is the count of add-to-carts in the top 40 position between control and variation ranking models. The results shown in Table 10 demonstrate the effectiveness of the proposed architecture in improving the user engagement performance. We notice a similar"}, {"title": "9 LESSONS LEARNED", "content": "Among the many things that we learned while creating this system, we would like to highlight a few of them.\nInner product vs. cosine similarity. During the model training, we experimented with inner product. The inner product is more stable during training and does not require the temperature factor \u03c3 shown in Eq. 1. This removes the need to select o and in general produces better results. Unfortunately, inner product was much harder to optimize when creating the ANN index, compared to cosine similarity. For this reason, we eventually focused on cosine similarity only.\nBlending features. Many text fields are generally available for each product. These include different descriptions and many attributes. There was a common belief that the description of the product would help improve the recall performance. In all our experiments, we could not extract any boost in performance. This is probably because descriptions can contain a lot of irrelevant text that simply adds noise.\nModel complexity. As seen in the Section 8, for this application, a very deep model or very large embedding size is not necessary to achieve top performance. This is probably because queries and product titles are not very complex from a semantic perspective."}]}