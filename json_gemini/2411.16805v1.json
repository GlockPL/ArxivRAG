{"title": "Human Motion Instruction Tuning", "authors": ["Lei Li", "Sen Jia", "Wang Jianhao", "Zhongyu Jiang", "Feng Zhou", "Ju Dai", "Tianfang Zhang", "Wu Zongkai", "Jenq-Neng Hwang"], "abstract": "This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the model's ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction.", "sections": [{"title": "1. Introduction", "content": "Understanding human motion is a central challenge in multimodal AI, impacting numerous fields such as digital human, human-computer interaction, sports analytics, healthcare, and virtual human modeling [10, 24, 26, 36]. Motion data, which captures skeletal movements and body dynamics, provides a structured, appearance-invariant representation of human actions, concentrating on the essential movement patterns while excluding irrelevant visual details. This data has emerged as a privacy-conscious alternative to visual inputs, offering finer control over action recognition and facilitating analysis in applications ranging from digital human avatars to behavioral monitoring in healthcare [9, 20]. As digital human representations and motion-centric applications continue to expand, developing models that can leverage the unique characteristics of motion data becomes increasingly crucial for achieving robust, context-aware behavior analysis.\nRecently, large language models (LLMs) have shown promise in motion analysis, leveraging their capacity to model temporal dependencies across multimodal data [2, 5]. State-of-the-art models, such as MotionGPT [12] and MotionLLM [2], treat motion data as a multimodal extension, encoding it as discrete tokens or translating it into language-like representations for processing by LLMs. These models have yielded notable results in motion understanding and recognition tasks, illustrating the adaptability of language models to non-linguistic data. However, their reliance on tokenization or textual conversion can limit the richness of the motion representation, abstracting away critical spatial-temporal details essential for high-fidelity human activity recognition.\nDespite recent advancements, notable challenges remain in current approaches to motion analysis using LLMs. First, encoding motion data into language tokens, as implemented in models such as MotionGPT, relies on a quantization process that may obscure critical fine-grained motion information, potentially limiting the model's capacity to accurately capture detailed spatial-temporal dynamics [7, 28]. The common approach of translating motion data into text tokens further contributes to this issue, often resulting in the loss of essential motion-specific nuances that are critical for in-depth behavior interpretation [5, 15]. This translation process poses considerable challenges in understanding the intricate details of human movement, impeding the model's ability to achieve spatial and physical comprehension of human actions. Moreover, motion data inherently contains 3D structural information and insights into connected human behaviors, which are often difficult to fully capture when relying solely on language tokens, as this representation can overlook the underlying spatial dependencies crucial to modeling human motion accurately.\nIn addition, existing methods frequently treat video and motion data as isolated modalities, overlooking the potential benefits of a complementary, integrated approach. Processing these data types independently neglects the rich environmental context and interaction cues provided by video inputs, which are crucial for accurately interpreting complex human behaviors [10, 20]. This separation limits the capacity for a holistic understanding of human motion, particularly in scenarios where context, spatial relations, and dynamic interactions play essential roles in behavior analysis. Furthermore, video-based approaches, while potentially more informative, are computationally intensive, posing significant constraints for large-scale or real-time applications, which limits their feasibility in practical motion analysis systems.\nIn response to these challenges, we propose LLaMo, a framework designed to integrate motion data as a distinct modality within LLMs without translating it into textual intermediaries. LLaMo incorporates a Motion Estimator and an Enhancer to facilitate the direct input of motion or/and video data, thereby offering a unified, human-centric analysis pipeline. The framework also introduces a Cross Talker module, which dynamically aligns the motion and text features, allowing for text-aware, fine-grained motion representations. This approach retains the original spatial-temporal characteristics of the motion data, providing a more precise understanding of human actions and enabling high-resolution behavior analysis in motion-intensive contexts [1, 23].\nThrough extensive evaluations on benchmarks such as MoVid-Bench [2] and BABEL-QA [4], we demonstrate that LLaMo achieves state-of-the-art performance in motion-centric human activity recognition [5, 15]. These results underscore LLaMo's potential to advance multimodal AI, providing a robust foundation for human behavior understanding. Our contributions can be summarized as follows:\n\u2022 We introduce LLaMo, which treats motion data as an independent modality within LLMs, preserving essential motion-specific details for robust analysis.\n\u2022 We propose Cross Talker, a text-guided mechanism that dynamically focuses on and aggregates key motion frames features, optimizing computational efficiency and enabling the model to pick relevant motion features.\n\u2022 LLaMo's architecture supports the direct input of both raw motion and video data, providing a generalized framework for human-centric analysis across diverse applications, such as sports analytics, healthcare, and behavioral monitoring."}, {"title": "2. Related Work", "content": "Multimodal representation learning is integral to human-centric analysis, especially for applications requiring spatial-temporal reasoning to decode complex behaviors [14, 17, 21]. Recent advances, such as Video-LLaVA, have made significant strides by embedding visual data from images and videos into a shared linguistic feature space, enabling sophisticated visual reasoning for behavioral analysis tasks [17]. Despite these achievements, many current models are optimized primarily for static images or discrete video frames, limiting their effectiveness in sequential, dynamic scenarios where understanding progression and continuity in motion is crucial [9, 19, 21]. Privacy concerns further challenge the adoption of video-based methods in sensitive domains; thus, researchers are exploring motion data as a privacy-conscious alternative, enabling analysis focused on human actions without revealing identifiable visual information [31, 34]. By combining visual and motion data, emerging multimodal frameworks hold promise for more comprehensive, privacy-aware human behavior analysis, leveraging the strengths of both modalities for adaptability and depth across applications."}, {"title": "2.2. Human Motion Understanding", "content": "Human motion understanding has traditionally relied on skeletal data represented as sequences of joint keypoints to capture movement while preserving user privacy [25, 28, 33]. Early models like 2s-AGCN [27] and more recent transformer-based methods, such as MotionCLIP [3], have achieved success in tasks like activity recognition, captioning, and behavior analysis by mapping motion data to language tokens, effectively capturing structural aspects of movement. However, these methods often lack environmental context, which is crucial for nuanced interpretations in real-world applications, as similar movements can convey different meanings across varied scenarios [19, 30]. To address this limitation, recent frameworks integrate motion and visual data, allowing models to generalize across diverse, dynamic environments [8, 18]. This fusion of skeletal data with video-provided context has shown promise in applications like sports and professional activity analysis, where contextual cues are essential. Additionally, skeletal-based data remains valuable in privacy-sensitive applications, such as healthcare and security, where it allows for robust analysis without compromising personal identity."}, {"title": "3. Methods", "content": "The LLaMo framework is composed of three primary modules to effectively process and integrate video, motion data and text. An overview of this pipeline is provided in Figure 2. The architecture comprises the following components:\n1. Multimodal Feature Extraction Module: This module independently encodes the video or motion inputs, which makes LLaMo is a general human motion assistant with video input or motion input. Then the Motion features are enhanced by distilling valuable representations in video.\n2. Cross Talker: Here, the enhanced motion features are aggregated in a text-guided manner and aligned with text within a shared semantic space, allowing the natural feature fusion between motion and text, which provides fine-grained inputs for LLM.\n3. Behavior Generation Module: Using the aggregated and text-aligned motion features, along with the text representations, this module generates a contextually aware textual description of the observed human behavior."}, {"title": "3.1. Enhanced motion Feature Extraction Module", "content": "The enhanced motion feature extraction module consists of four primary components: the motion estimator, followed by the motion and video encoders, and the feature enhancement module. The motion estimator addresses scenarios where motion data may be unavailable by estimating motion information directly from video frames. The motion and video encoders then independently process the motion and video data, encoding them into their respective feature spaces. This approach allows the system to leverage complementary information from both modalities, creating a richer representation of human behavior.\nGiven a sequence of video frames $V = \\{v_1, v_2, ..., v_T\\}$, the video encoder $f_v(\\cdot)$ extracts a set of visual features:\n$F_v = f_v(V) = \\{f_v(V_1), f_v(V_2), ..., f_v(V_T)\\}$.\nSimultaneously, the motion encoder $f_m(\\cdot)$ processes the motion data $M = \\{m_1, m_2,..., m_T\\}$ to generate motion-specific features:\n$F_M = f_m(M) = \\{f_m(m_1), f_m(m_2),..., f_m(m_T)\\}$.\nHere, $F_v \\in \\mathbb{R}^{T \\times H}$ and $F_M \\in \\mathbb{R}^{T \\times H}$ represent the extracted feature sets for the video and motion data, respectively, where $T$ is the sequence length and $H$ denotes the feature dimension.\nThe feature enhancement module then aligns and fuses these modality-specific features. By using the motion features $F_M$ as queries, this module selectively extracts semantically relevant information from the video features $F_v$. This alignment and fusion process allows the motion features to be enriched with contextual information from the video, producing refined, contextually enriched representations that enhance the model's understanding of nuanced human behaviors. For the detailed design refer to the Appendix.\nFirst, the video features and motion features will conduct self-attention operations respectively to get their augmented features $F_V$ and $F_M$. This allows the video and motion to initially capture the dependencies on their respective time steps. Then, in order to enrich the motion data features with semantic information from the video, we subtly design a cross-attention block where augmented motion features as the query to extract valid semantic information in the video features. Finally, the selected video semantic representation and motion features are connected by residuals followed by a feed-forward neural network to obtain the enhanced motion features $F_M$"}, {"title": "3.2. Cross Talker Module", "content": "Given the enhanced motion features $F_M$ from the feature enhancement module, directly feeding the entire sequence of motion frames into the Large Language Model (LLM) incurs significant computational costs and may lead to suboptimal performance. This limitation arises from the self-attention mechanism in transformers, which has a computational complexity of $O(L^2)$, where $L$ denotes the sequence length. When incorporating both motion and text, the sequence length becomes $L = L_T + T$, where $L_T$ is the number of text tokens and $T$ is the number of motion frames. As $T$ increases, the quadratic growth in computational cost becomes prohibitive, leading to complexity proportional to $(L_T + T)^2$.\nFurthermore, computing attention over long sequences can dilute attention weights, making it challenging for the model to focus on essential parts of the sequence and effectively capture long-range dependencies. Furthermore, the motion features are not well aligned with text either. To address these issues, we propose the Cross Talker Module, which selectively distills key motion frames-referred to as viewpoint frames-based on their relevance to the text context, as well as aligning the motion features with the text. An overview of the Cross Talker Module is shown in Figure 3.\nLanguage-Guided Frame Selection Given the enhanced motion features $F_M \\in \\mathbb{R}^{T \\times H}$ and text embeddings $F_{TE} \\in \\mathbb{R}^{L_T \\times H}$, we aim to identify $K$ viewpoint frames that are most relevant to the textual context. This selection is achieved through a cross-attention operation, where text features act as queries and motion features as keys and values:\n$A = \\text{Softmax} \\left(\\frac{(F_{TE}W_Q)(F_MW_K)^T}{\\sqrt{d}}\\right),$ (1)\nwhere $W_Q, W_K \\in \\mathbb{R}^{H \\times d}$ are learnable projection matrices, and $d$ is the dimensionality of the queries and keys. The resulting attention matrix $A \\in \\mathbb{R}^{L_T \\times T}$ contains attention weights, with each element $A_{i,j}$ indicating the relevance between the $i$-th text token and the $j$-th motion frame.\nTo determine the importance of each motion frame, we aggregate attention weights across all text tokens using max pooling:\n$S_j = \\max_{i=1,..., L_T} A_{i,j}, j = 1,..., T.$ (2)\nThe resulting scores $s_j$ represent the significance of each frame with respect to the text context. We then select the top K frames with the highest relevance scores as viewpoint frames, reducing the effective sequence length from $T$ to $K$. This reduction in sequence length lowers the self-attention complexity from $O((L_T + T)^2)$ to $O((L_T + K)^2)$, which is computationally efficient when $K \\ll T$.\nAdaptive Contextual Feature Aggregation For each selected viewpoint frame, we enrich its representation by aggregating both local and global contexts. An adaptive receptive field size $r_k$ is predicted for each viewpoint frame k using a receptive field regression module. The module performs a cross-attention operation where each viewpoint frame serves as a query, and the unselected motion features serve as keys and values, followed by a sigmoid activation to estimate $r_k$. Based on $r_k$, we define a local window $W_k$ around frame k:\n$W_k = \\{j | |j - k| \\leq r_k \\times T\\}.$\nWithin this window, we apply local attention to capture fine-grained details:\n$F_{\\text{local}}(k) = F_M(k) + \\text{Attention}(F_M(k), F_M(W_k), F_M(W_k)),$\nwhere $F_M(W_k)$ denotes the motion features within the local window around frame k.\nTo incorporate global context, we partition the full motion sequence into N segments and compute segment-level features via average pooling:\n$F_M^{\\text{seg}} = \\{F_M(n)\\}_{n=1}^N,$\nwhere the segment size is controlled by the hyperparameter $S_n$. We then apply global attention:\n$F_{\\text{global}}(k) = F_{\\text{local}}(k) + \\text{Attention}(F_{\\text{local}}(k), F_M^{\\text{seg}}, F_M^{\\text{seg}}).$\nThe final representation for each viewpoint frame is obtained by concatenating local and global features:\n$F_M(k) = [F_{\\text{local}}(k); F_{\\text{global}}(k)] \\in \\mathbb{R}^{2H},$\nallowing the model to capture both fine-grained motion details and broader contextual information.\nBidirectional Cross-Modal Fusion To integrate the enhanced motion features with the text embeddings, we employ a bidirectional cross-attention mechanism. First, we update the motion features by performing a cross-attention operation, where the motion features act as queries and the text features serve as keys and values, and vice versa for the text features. The updated features are then passed through feed-forward networks, and we concatenate the motion and text representations to form the input for the next stage:\n$F_{\\text{fusion}} = [F_T; \\{F_M(k)\\}_{k\\in K}].$\nThis bidirectional fusion enhances the interaction between motion and text, enabling a coherent, contextually grounded understanding of human behavior. By selectively focusing on key motion frames and integrating enriched motion and text features, the Cross Talker Module reduces computational costs and improves the model's ability to capture complex human actions."}, {"title": "3.3. Behavior Generation Module", "content": "The Behavior Generation Module leverages the fused features $F_{\\text{fusion}}$ to generate a textual description $Y$ that encapsulates the observed human behavior. This module utilizes a language model $h(\\cdot)$ to transform $F_{\\text{fusion}}$ into a semantically coherent and contextually relevant output sequence:\n$Y = h(F_{\\text{fusion}}) = \\{Y_1, Y_2,\\ldots, Y_L\\},$ (9)\nwhere $y_t$ denotes the token generated at the $t$-th time step, and $L$ is the length of the generated description. The probability of generating each token $y_t$ is given by:\n$P(Y_t | Y_{1:t-1}, F_{\\text{fusion}}) = \\text{Softmax}(W_o h_t),$ (10)\nwhere $h_t$ represents the hidden state of the language model at time $t$, and $W_o$ is an output projection matrix. The hidden state $h_t$ is derived from the model's internal mechanisms, which integrate attention over the input features and previously generated tokens. This autoregressive approach enables LLaMo to produce detailed and contextually aligned descriptions that incorporate nuanced information from both motion and text modalities.\nBy iteratively generating tokens based on the fused features and preceding context, the Behavior Generation Module constructs a meaningful and comprehensive narrative of human activity, grounded in the multimodal input data."}, {"title": "3.4. Training Objective", "content": "To train LLaMo, we finetune a pretrained language model using supervised learning, guiding it to generate accurate and contextually relevant descriptions based on the input motion, video, and text features. Both the motion and video encoders remain frozen during training, focusing the optimization on the language model's alignment with multimodal inputs. The training objective minimizes the discrepancy between the generated descriptions and the ground-truth annotations in the dataset.\nGiven a training set of N samples, each with video frames $V^{(i)}$, motion data $M^{(i)}$, and corresponding ground-truth textual descriptions $\\hat{Y}^{(i)} = \\{\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_{Y_L}^{(i)}\\}$, the overall training objective is to minimize the negative log-likelihood of the ground-truth tokens across all samples:\n$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^{Y_L} \\log p(\\hat{y}_t^{(i)} | \\hat{Y}_{1:t-1}^{(i)}, F_{\\text{fusion}}^{(i)}),$ (11)\nwhere $F_{\\text{fusion}}^{(i)}$ represents the fused input features for the $i$-th sample. The term $p(\\hat{y}_t^{(i)} | \\hat{Y}_{1:t-1}^{(i)}, F_{\\text{fusion}}^{(i)})$ denotes the probability of the ground-truth token $\\hat{y}_t^{(i)}$ given the previous tokens $\\hat{Y}_{1:t-1}^{(i)}$ and the fused features.\nThis objective ensures that the generated descriptions are syntactically and semantically aligned with the target outputs, capturing the detailed aspects of human behavior present in the video and motion inputs."}, {"title": "4. Experiments", "content": "To evaluate LLaMo, we performed experiments across datasets representing complex human behaviors and tasks, including professional sports and cyclic action repetition counts."}, {"title": "4.1. Implementation Details", "content": "Training Datasets: LLaMo was trained using a combination of bimodal video-motion datasets and unimodal video or motion datasets to enable a robust understanding of both modalities. For video-motion data, we utilized MoVid from MotionLLM [2] and our custom Swing dataset containing 20,000 annotated videos of golf swings with corresponding motion capture data and expert Q&A instructions. Motion-only datasets included HumanML3D [6] and KIT-ML [24], while Mo-RepCount, a filtered subset of RepCount [11], provided high-quality repetition count videos. This multimodal data configuration ensures that LLaMo effectively learns from both single and combined modalities.\nEvaluation Datasets: LLaMo was evaluated on MoVid-Bench [2] to assess both video and motion understanding capabilities, with additional tests on BABEL-QA [4] to evaluate motion-based question answering. For specific tasks, we used Mo-RepCount to gauge motion details and Swing to evaluate professional sports guidance.\nEvaluation Metrics: For MoVid-Bench, we follow previous LLM evaluation metrics [16] [14] [13] on accuracy and scores, which contains body-part awareness, sequentially, direction analysis, reasoning ability, and hallucination, respectively. BABEL-QA used the metric followed by [4], Mo-RepCount applied metrics such as OBO, MAE, OBZ, and RMSE, and Swing performance was assessed on reasonableness, coherence, pertinence, and adaptability using GPT-4 evaluations, scoring from 0 to 5."}, {"title": "4.2. Results", "content": "Evaluation on Motion Understanding in MoVid-Bench. We evaluated LLaMo's motion understanding capabilities on the MoVid-Bench, focusing on five critical aspects. Both accuracy and score metrics were used to comprehensively assess LLaMo's performance. Table 1 provides a comparison with several baselines, including GPT-3.5 [22], MotionGPT [3], and MotionLLM [2]. The GPT-3.5 baseline, limited to textual data processing, struggles significantly with motion interpretation, yielding low performance. MotionGPT, while capable of incorporating motion data, is primarily code motion into a codebook, which limits its reasoning ability on coiled motion data and leaves it vulnerable to hallucinations. Although MotionLLM provides improvements in understanding motion sequences, it still conducts translation on motion data, resulting in low accuracy on seq. and Hall.\nLLaMo, in contrast, achieves superior results across nearly all metrics on MoVid-Bench-Motion, attaining the highest accuracy in most categories. This performance is primarily attributed to its preservation of critical motion-specific nuances, as LLaMo processes motion data as a distinct modality without requiring any conversion. LLaMo effectively captures the intricate details of 3D sequences associated with human behaviors. Its Feature Enhancer, combined with the Cross Talker module, enables simultaneous input of video and motion data, allowing LLaMo to selectively focus on motion frames most relevant to the text context. This design enhances the model's ability to capture complex patterns of human motion and improves its reasoning on motion-intensive tasks. Overall, LLaMo's results on MoVid-Bench demonstrate its effectiveness in advancing human behavior analysis, highlighting its potential for broader applications in general motion understanding.\nEvaluation on Video Understanding in MoVid-Bench. We evaluated LLaMo on the video component of the MoVid-Bench dataset to assess its capability for comprehensive video understanding. As presented in Table 1, LLaMo achieves strong performance across most metrics, demonstrating LLaMo's extraordinary comprehension ability on videos even without motion input.\nCompared to other models like GPT-3.5 [22], Video-LLAVA [16], and MotionLLM [2], while LLaMo slightly trails behind MotionLLM in body-part awareness, LLaMo shows significant improvements in reasoning ability and overall scores. This indicates that our model excels in understanding complex video content and making accurate inferences about human behaviors.\nThe rewarding results can be attributed to our model's powerful ability to estimate motion data accurately in video and align it seamlessly with video inputs. This approach retains the motion capture of motion data and the auxiliary information provided by video, enriching the understanding of dynamic scenes.\nEvaluation on BABEL-QA. We evaluated LLaMo on the BABEL-QA dataset to assess its capability to handle complex motion-based queries. As shown in Table 2, LLaMo achieves the highest overall score of 0.458, outperforming all baselines. It excels particularly in the Action category with a score of 0.525, demonstrating superior action recognition capabilities. Additionally, it achieves top scores in temporal reasoning tasks (Before and After), indicating a strong understanding of temporal relationships in motion sequences. This capability benefits from the direct integration of motion data as an independent modality and the effective text-aware motion frames extract mechanism in LLaMo, preserving crucial motion-specific nuances, as well as leading to more accurate and contextually rich interpretations.\nProfessional Sports Analysis. We further evaluate LLaMo on the challenging swing-dataset, termed as golf-swing, details refer to Appendix, where the ground truth (GT) consists of answers provided by professional coaches, serving as expert benchmarks. The evaluation focuses on four key indicators: Reasonableness: Logical soundness and plausibility of the responses. Coherence: Consistency and logical flow within the responses. Pertinence: Relevance of the answers to the specific questions. Adaptability: Ability to adjust responses based on the stage on the athlete.\nAs presented in Table 3, LLaMo significantly outperforms both GPT-3.5 [22] and MotionLLM [2] across all metrics. While MotionLLM shows improvements over GPT-3.5, with higher accuracy and scores, it still lags behind LLaMo. Specifically, LLaMo achieves an overall accuracy of 24.80.\nThe superior performance of LLaMo can be attributed to the design of the Cross Talker module, which enables language-guided frame selection to capture complex motion relationships inherent in golf swings. This allows LLaMo to effectively model intricate motion patterns and generate responses that are more reasonable, coherent, pertinent, and adaptable, closely aligning with the expert-level insights provided by professional coaches.\nIn contrast, while MotionLLM demonstrates certain strengths over MotionGPT\u2014such as better pertinence and coherence. It lacks the advanced motion understanding capabilities of LLaMo, lagging behind LLaMo distinctly.\nEvaluation on Mo-RepCount. We evaluated LLaMo on the Mo-RepCount dataset to assess its ability to capture fine-grained motion details and complex temporal features required for accurate repetition counting. For a fair comparison, we trained all state-of-the-art models, including RepNet [32], TransRAC [11], PoseRAC [35] and ES-counts [29], on the Mo-RepCount dataset without using any additional data. As presented in Table 4, LLaMo outperforms the RepNet and TransRAC across almost all metrics. Since both of them are models built for loop counting tasks, the effect of our model on this task is convincing, which shows that LLaMo is also competent in capturing the details of the motion and videos.\nWhile LLaMo did not outperform EScounts and poseRAC, which are specialized for counting tasks, on many metrics, LLaMo achieved amazing results on the OBZ metric, demonstrating that LLaMo has an unmatched ability to accurately capture motion details and spatio-temporal relationships. These results show that our model, LLaMo, has an outstanding ability to get rewarding results on motion and video fine-grained tasks. This demonstrates the potential of LLaMo's applications on motion and video detail tasks.\nWe demonstrated LLaMo's capabilities across a range of tasks, from generalized motion analysis to specialized motion tasks and fine-grained motion recognition. Furthermore, our evaluation highlighted LLaMo's ability to accurately capture essential features in both video and motion modalities."}, {"title": "5. Conclusion", "content": "We present LLaMo, a novel framework that directly integrates motion data into large language models, enhancing multimodal understanding by unifying text, video, and motion data within an architecture fine-tuned for human motion instruction. LLaMo demonstrates significant advances in tasks such as behavior comprehension, motion-focused captioning, and interactive Q&A, capturing fine-grained motion details essential for delivering precise, context-aware insights. Experimental results, particularly in professional sports analysis, underscore LLaMo's capability to interpret complex human motion accurately. Future work will extend LLaMo's framework to various fields where detailed motion analysis and multimodal understanding are essential. Key challenges involve refining cross-modal integration techniques and improving efficiency to support broader, real-time applications, expanding LLaMo's potential in human-centered AI. We hope our work will inspire future research on building more advanced human-centric multimodal models."}]}