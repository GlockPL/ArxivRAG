{"title": "Diffusion Model with Representation Alignment for Protein Inverse Folding", "authors": ["Chenglin Wang", "Yucheng Zhou", "Zijie Zhai", "Jianbing Shen", "Kai Zhang"], "abstract": "Protein inverse folding is a fundamental problem in bioinformatics, aiming to recover the amino acid sequences from a given protein backbone structure. Despite the success of existing methods, they struggle to fully capture the intricate inter-residue relationships critical for accurate sequence prediction. We propose a novel method that leverages diffusion models with representation alignment (DMRA), which enhances diffusion-based inverse folding by (1) proposing a shared center that aggregates contextual information from the entire protein structure and selectively distributes it to each residue; and (2) aligning noisy hidden representations with clean semantic representations during the denoising process. This is achieved by predefined semantic representations for amino acid types and a representation alignment method that utilizes type embeddings as semantic feedback to normalize each residue. In experiments, we conduct extensive evaluations on the CATH4.2 dataset to demonstrate that DMRA outperforms leading methods, achieving state-of-the-art performance and exhibiting strong generalization capabilities on the TS50 and TS500 datasets.", "sections": [{"title": "1. Introduction", "content": "Protein inverse folding, a crucial task in bioinformatics and computational biology, aims to reversely explore possible amino acid (AA) sequences from a given protein 3D structure [17, 26, 48]. These predicted sequences can autonomously fold into functional proteins, enabling the design of novel proteins with desired structural and functional properties. Moreover, some of these designed proteins, which may not occur naturally, have significant applications in biological research, including drug design and antibody engineering [5, 20, 43].\nRecent studies have revealed the effective application of neural networks in analyzing protein 3D structures [3, 29, 38]. Predicting AA sequences based on protein backbone structures is a 3D structure-to-sequence mapping problem. Numerous studies have utilized GNNs [32, 35] to extract protein structural features (i.e., residue features and their connections) [5, 17, 18], followed by the Transformer architecture to generate protein sequences in an autoregressive manner [6, 41].\nRecently, diffusion models have been extensively applied for generating meaningful contents in both vision and language [2, 11, 13, 28, 31, 44, 49], due to their ability to produce highly diverse yet faithful data from the desired distribution. Moreover, diffusion models have shown promise in analyzing and interpreting protein structures. For instance, DPLM [42] adopted a discrete diffusion framework to train protein sequences, exhibiting the potential of the diffusion model for protein representation learning. Similarly, Grade-IF [46] proposed a graph diffusion model for protein inverse folding, effectively learning latent protein representations by capturing inter-residue interactions, which encapsulate various reasonable sequences for a given backbone structure.\nFrom a biological standpoint, protein neighbor inter-residue interactions occur among spatially close amino acids and contribute to salient substructures, which is bene-ficial for protein representation learning [1, 30, 40]. In addition, the state of a protein chain is intrinsically linked to the collective contributions of its residues [30, 33, 34]. Viewing a protein chain as a steady-state system, each residue is vital for maintaining overall stability. Therefore, effective communication among residues is essential for protein representation learning. Recent studies improve diffusion models by aligning external representations, i.e., facilitating the alignment of noisy hidden representations with external ones during the denoising process [4, 45, 47]. In AA sequences, AA types are not merely discrete tokens but also could serve as alignment guidance for residue semantics. However, existing methods mainly utilize these sequences for model supervision, neglecting their potential to provide semantic feedback for hidden representations.\nIn this work, we propose a novel method that leverages diffusion models with representation alignment (DMRA) for protein inverse folding. As shown in Figure 1, to facilitate effective communication across residues on the protein chain, we propose a shared center that aggregates contextual information from the entire protein structure, distributing it selectively to each residue. Our empirical analysis shows that accessing contextual information significantly improves the non-linear transformation capabilities of each layer, thereby improving residue representation. Moreover, to align the noisy hidden representation with the clean semantic during the denoising process, we predefine the semantic representations for AA types and employ a representation alignment method that uses type embeddings as semantic feedback to normalize each residue. By integrating type-specific semantic embeddings as supplementary prior knowledge, the hidden representations of residues are better aligned with type semantics during denoising, enhancing the recovery of the AA sequence. Finally, unlike previous methods that concatenate node and edge features for sequence prediction, we propose a cell module that more effectively integrates these features by estimating the relevance between adjacent nodes and edges.\nExtensive experiments demonstrate that our method achieves state-of-the-art performance on the CATH4.2 dataset [17]. To assess the generalization capacity of our approach, we also evaluate it on the TS50 and TS500 datasets [22] and observe its significant improvements against baseline methods. Finally, we provide detailed visualization and analysis in case studies to illustrate the effectiveness of our approach.\nThe contributions of this work are summarized below:\n\u2022 We introduce a novel diffusion model with representation alignment for protein inverse folding.\n\u2022 We design a shared center to integrate and distribute information among residues, thereby enhancing residue representation learning.\n\u2022 We propose a representation alignment method for AA type embeddings and noisy hidden representations during the denoising process.\n\u2022 Our method surpasses state-of-the-art approaches on the CATH4.2 dataset and demonstrates strong generalization on the TS50 and TS500 datasets."}, {"title": "2. Related Work", "content": "Protein inverse folding can be formulated as a structure-based conditional generation, where 3D structure can be encoded to a knn-graph. residues and their relationships can be compressed into node features and edge features. Graph-Trans [17] extracted protein backbone features as structural representation, including angles and distances, etc., and then used the decoder to obtain the amino acid sequences in an autoregressive manner. Recent works enhanced graph features to better compress structural information. GVP-GNN [18] introduced geometric vector perceptrons to replace MLP, simultaneously leveraging the geometric and relational features. ProteinMPNN [5] introduced distances between N, Ca, C, O, and a virtual C\u03b2 as additional input features, resulting in a significant performance. PiFold [9] introduced a novel residue featurizer that used virtual atoms to capture hidden information, and designed PiGNN layers to generate protein sequences. VFN [24] established a set of virtual atoms for each residue and utilized a vector field operator to extract geometric features to augment the features of vector field network layers. Besides, to fully consider sequence information in the mapping process, ESM-IF [15] augmented training data and used this additional data to train, resulting in significant improvements. LM-Design [51] and KWDesign [10] employed pre-trained language models to refine amino acid sequences iteratively. These methods have achieved significant success in sequence recovery, but they may encounter challenges in encapsulating the diversity of feasible protein sequences.\nIn recent years, generative models have garnered significant attention [12, 21, 37].DDPM [13] utilized the Diffusion paradigm by progressively introducing Gaussian noise into images and learning its reverse process, which has achieved remarkable success in image generation. Furthermore, Latent Diffusion [31] and ControlNet [49] enhanced controllability by incorporating text as a condition for image generation. In Addition, D3PM [2] has extended the multinomial diffusion model by Hoogeboom et al. [14] to handle discrete data. In the realm of protein, DPLM [42] employed the Diffusion framework to model protein sequences in an unconditional generation manner, leading to a better understanding of proteins. Zhang et al. [50] proposed the Diff-PreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling and enhance DiffPreT by SiamDiff, a method to capture the correlation between different conformers of a protein. Hu et al. [16] proposed CPDiffusion-SS, a latent graph diffusion model that generates protein sequences based on coarse-grained secondary structural information, enhancing the reliability and diversity of the generated proteins. GRADE-IF [46] proposed an innovative graph denoising diffusion model for structure-based protein sequence design, demonstrating significant potential in generating diverse protein sequences."}, {"title": "3. Method", "content": "In this section, we introduce our novel method, DMRA, to use the diffusion model with representation alignment for protein inverse folding. As shown in Figure 2, Our approach starts with protein structure compression and diffusion modeling. We then delve into our denoising network, comprising message passing with cell module, shared center, and representation alignment method."}, {"title": "3.1. Protein Structure Compression", "content": "To compress protein 3D structure into a compact representation, we parse the backbone structure and construct a K-nearest neighbor graph G(X, E) based on the coordinates of Ca atoms, where K is 30 at default. The G(X, E) comprises node features $X \\in \\mathbb{R}^{N,d_n}$ and edge features $E \\in \\mathbb{R}^{M,d_e}$, where these features are used to represent residues and their relationships, and N and M denote the numbers of nodes and edges, respectively. Following Yi et al. [46], node and edge features are defined as follows:\n$X = \\text{Encoder}_{\\text{Node}} (X_b; X_{sasa}; X_a; X_s)$ (1)\n$E = \\text{Encoder}_{\\text{Edge}} (E_k; E_{sp}; E_{se})$ (2)\nwhere B-Factor $X_b \\in \\mathbb{R}^{N,1}$ and solvent-accessible surface area (SASA) $X_{sasa} \\in \\mathbb{R}^{N,1}$ are derived from the scalar values of Ca atoms. B-Factor reflects the static stability of the protein, while SASA provides insights into protein folding and hydrophobicity. Angle features $X_a \\in \\mathbb{R}^{N,4}$ contain the sine and cosine of backbone dihedral angles \u03c6 and \u03c8, i.e., local geometry of residues. Surface-aware features $X_s \\in \\mathbb{R}^{N,5}$ are encoded as vectors according to a set of hyperparameters \u03bb, representing the normalized distances between the central amino acid and its one-hop neighbors [7]. For edge features, kernel-based distances $E_k \\in \\mathbb{R}^{M,15}$ are described using Gaussian radial basis functions (RBF) with varying bandwidths to capture distance information between connected residues at different scales, totaling 15 different distance features. $E_{sp} \\in \\mathbb{R}^{M,12}$ are derived from the heavy atom positions of the corresponding residues, totaling 12 relative position features [46]. The relative sequence distances $E_{se} \\in \\mathbb{R}^{M,66}$ use 65-dimensional one-hot vectors as bins to encode the relative sequence distance of two residues in the protein chain, along with a binary feature indicating whether the Euclidean distance between two connected residues is less than a specified threshold."}, {"title": "3.2. Diffusion Modeling", "content": "Our method is based on a diffusion modeling framework [2] for protein inverse folding, which includes both diffusion and denoising processes.\nDiffusion Process In the diffusion process, noise is introduced to the node's AA type. Specifically, at timestep t, each node's AA type $S_0$ in the sequence transforms to other amino acid types using a probability transfer matrix [2, 14] $Q_t = a_tI + (1 \u2212 a_t)\\frac{[1]}{d}, Q_t \\in \\mathbb{R}^{20\u00d720}$ with I being the transpose of the identity matrix and d being the number of AA types and [1] being the one vector of dimension d, i.e.,\n$p(S_t|S_{t-1}) = S_{t-1}.Q_t$ (3)\nwhere $S_t$ and $S_{t-1}$ represent node's noise AA type in step t and t \u2212 1, respectively. Similar to DDPM [13], we can compute node's noise AA type in step t from initial step, denoted as follows:\n$p (S_t| S_0) = S_0. Q_t$ (4)\n$Q_t$ denotes transition probability from initial step to t directly, and $S_0$ represents node's original AA type.\nDenoising Process In the denoising process, each node's noise AA type is sampled from the uniformly prior distribution and iterated back to the initial distribution. The transformation between distributions is sketched as follows:\n$P_\u03b8 (S_{t-1}|S_t, G) = \\sum_{\u015c_0} q(S_{t-1}|\u015c_0, S_t, G)p_\u03b8(\u015c_0|S_t, G)$ (5)\nwhere $\u015c_0$ is predicted AA type and $q(S_{t\u22121}|\u015c_0, S_t, G)$ represent posterior that can be computed as follows:\n$q(S_{t-1}|\u015c_0, S_t, G)=\\text{DIST}(S_{t-1}| \\frac{S_t Q_t S_0 Q_t^{-1}}{\u015c_0 Q_t S_t})$ (6)\nwhere DIST is a categorical distribution over 20 AA types with probabilities computed by the posterior distribution."}, {"title": "3.3. DMRA Denoising Network", "content": "As shown in Figure 2, we propose a DMRA denoising network $\u03f5_\u03b8(S_t, t, G)$ to predict $p_\u03b8(\u015c_0|S_t, G)$. The network includes three key components: the Message Passing module, the Shared Center module, and the Representation Alignment module. Initially, we concatenate the corresponding $S_t$ and $X$ to form the initial node representation, $h = {h_1, ...h_i, ...h_N}$."}, {"title": "Message Passing.", "content": "The Message Passing module updates node representations using information from neighboring nodes and their relationships. Firstly, given a node $h_i$ as an example, a gating mechanism within the Cell in Eq.(7-10), dynamically adjusts both node and edge features, producing the message $m_{ij}$. Specifically, the node $h_i$ is concatenated with its neighboring node $h_j$ as the message $m_{ij}$, which is then merged with the edge features $e_{ij}$ as gates, i.e.,\n$g_{ij}^{(1)} = \u03c3(\\text{Linear}([e_{ij}; m_{ij}]))$ (7)\n$g_{ij}^{(2)} = \u03c3(\\text{Linear}([e_{ij}; m_{ij}]))$ (8)\nwhere \u03c3 is the sigmoid function. $g_{ij}^{(1)}$ and $g_{ij}^{(2)}$ are two gates, which are used to update message $m_{ij}$, i.e,\n$n_{ij} = \\text{Act}(\\text{Linear}(e_{ij}) + g_{ij}^{(1)} \\text{Linear}(m_{ij}))$ (9)\n$m_{ij} = g_{ij}^{(2)} m_{ij} + (1 - g_{ij}^{(2)}) n_{ij}$ (10)\nwhere Act is activation function. Subsequently, messages from all neighboring nodes are aggregated to update the central node's representation, i.e.,\n$h'_i = \\text{MLP}(h_i, \\sum_{j \\in N_i} m_{ij})$, (11)\nwhere $h'_i$ is the updated feature of node i, and $N_i$ represents the set of neighbors of node i."}, {"title": "Shared Center.", "content": "To effectively enhance representations for residues, we propose the Shared Center module, as shown in Figure 3, which integrates the contextual information of the entire protein chain as shared content and selectively distributes it to each residue for access. Specifically, a virtual shared node $h_s \\in \\mathbb{R}^{1,d}$ is initialized at first. Subsequently, it is transformed to the query space, while the node representations aggregated from the Message Passing are transformed to the key and value spaces, i.e,\n$Q_s = W_q \\cdot h_s; K_h = W_k \\cdot h'_i; V_h = W_v \\cdot h'$ (12)\nwhere $W_q \\in \\mathbb{R}^{d,d}, W_k \\in \\mathbb{R}^{d,d}, W_v \\in \\mathbb{R}^{d,d}$ are the projection matrices. $h_s$ is randomly initialized, and $h' = {h_1, h_2,..., h_N}$ represents all nodes in the protein. We compute the attention score between them and use the score to adaptively aggregate information from the entire protein chain, i.e,\n$h_g = \\text{Softmax}(\\frac{Q_s K_h^T}{\\sqrt{d}}) V_h$ (13)\nwhere the output, $h_g$, encapsulates the contextual information of the entire protein structure, which is then provided to each residue for access. We employ the Cell module with a gating mechanism shown in Eq.(14-17), which selectively receives the quantity of information based on the current node to enhance residue representation, i.e.,\n$g_i^{(1)} = \u03c3(\\text{Linear}([h'_i; h_g])$ (14)\n$g_i^{(2)} = \u03c3(\\text{Linear}([h'_i; h_g]))$ (15)\nwhere \u03c3 is the sigmoid function. $g_i^{(1)}$ and $g_i^{(2)}$ are two gates, which are used to receive information from the central contextual feature $h_g$ according to node $h_i$, i.e,\n$c_i = \\text{Act}(\\text{Linear}(h'_i) + g_i^{(1)} \\text{Linear}(h_g))$ (16)\n$h_i = g_i^{(2)} h_g + (1 - g_i^{(2)}) c_i$ (17)\nwhere $h_i$ is the updated feature of node i."}, {"title": "Representation Alignment.", "content": "To normalize residue representations during the denoising process, we introduce the representation alignment module, in Figure 4. This module adaptively injects semantic embeddings of AA type into the noisy hidden representations. Firstly, all types are initialized as $h_c \\in \\mathbb{R}^{20,d}$. They are then mapped to the key and value spaces served as a reference, while the node representation $h_i$ is mapped to the query space served as a request, i.e.,\n$Q_h^{(i)} = W_q h_i; K_c = W_k \\cdot h_c; V_c = W_v h_c$ (18)\nwhere $W_q \\in \\mathbb{R}^{d,d}, W_k \\in \\mathbb{R}^{d,d}$, and $W_v \\in \\mathbb{R}^{d,d}$ are the projection matrices. We calculate the correlation between the node and the 20 AA types using scaled dot-product attention, i.e.,\n$p(h^{(i)}) = \\text{Softmax} (\\frac{Q_h^{(i)} K_c^T}{\\sqrt{d}})$ (19)\nwhere $p(h^{(i)}) \\in \\mathbb{R}^{1,20}$ represents the correlation between the i-th node and the 20 AA types. The AA-type semantic embeddings are then weighted and normalize the node, i.e.,\n$h''_i = p(h^{(i)}) \\cdot V$ (20)\nTo ensure each node aligns with the corresponding AA type, we apply cross-entropy loss to constrain the correlation matrix, i.e.,\n$L_{attn} = - \\frac{1}{N} \\sum_{i=1}^{N} p(h_{true}^{(i)})\\text{log}(p(h^{(i)}))$ (21)\nwhere N is the number of nodes, and $p(h_{true}^{(i)})$ is ground truth AA types of node i. Finally, obtained from the share center and representation alignment modules, the node representations are enhanced in each layer.\nFollowing the diffusion framework [28], The time step t is mapped to \u03b3 and \u03b2 to dynamically adjust the scale of features after computing the representation at each layer:\n$h_i^* = h_i' * (\u03b3 + 1) + \u03b2$ (22)\n\u03b3 and \u03b2 denote scale and shift respectively. The dimensions of them are consistent with the node representation $h_i'$. The final node representation in layer m is mapped to the 20 AA types, which are associated with the secondary structure embedding ss [46]:\n$p_i = \\text{MLP}(h_i^* + \\text{Linear}(ss))$ (23)\nwhere $p_i \\in \\mathbb{R}^{20}$ represents the predicted amino acid type of the i-th node."}, {"title": "3.4. Training Objective", "content": "For the model training, we employed cross-entropy loss to optimize the model's final predictions for each node type, i.e.,\n$L_{pred} = - \\frac{1}{N} \\sum_{i=1}^{N} p(h_{true}^{(i)}) \\text{log}(p_i)$ (24)\n$L = \u03b1 \\cdot L_{pred} + \u03bb \u00b7 L_{attn}$ (25)\nwhere \u03b1 and \u03bb are weight coefficients, and $p(h_{true}^{(i)})$ represents the true type of the i-th residue, $p_i$ represents the predicted type of the i-th residue and $L$ represents the final loss, which includes the prediction cross-entropy loss $L_{pred}$ and the constraint loss $L_{attn}$ in representation alignment module."}, {"title": "4. Experiments", "content": "In our experiments, we compare our method against other approaches on the CATH4.2 dataset, a widely-used benchmark categorized based on the CATH topology classification [27]. Following the data-splitting in previous works, e.g., GraphTrans [17], PiFold [9], and GRADE-IF [46], we divide the dataset into 18,024 proteins for training, 608 proteins for validation, and 1,120 proteins for testing. In addition, we extend our evaluation to the TS50 and TS500 datasets."}, {"title": "4.4. Generalization Capability Analysis", "content": "To verify the generalization capability of our model, we directly evaluated the trained model on the TS50 and TS500 datasets. The TS50 and TS500 datasets consist of 50 and 500 test proteins, respectively. As shown in Table 2, our model achieves state-of-the-art performance on both datasets, significantly outperforming existing methods. Specifically, our model achieves a perplexity (PPL) of 2.67 on TS50 and 2.31 on TS500, substantially outperforming existing approaches such as GRADE-IF and VFN-IF. For recovery rate (Rec), our model achieves 67.03% on TS50 and 71.61% on TS500. Notably, it is the first model, to our knowledge, that exceeds a recovery rate of 70% on the TS500 and over 65% on the TS50 without leveraging external knowledge in training. These results underscore the robustness and generalization capability of our approach. While models incorporating external knowledge in training, such as LM-Design [51] and KW-Design [10], also achieve competitive results, our model demonstrates that better performance can be reached purely through representation alignment in denoising network, thus reducing the reliance on external domain-specific information."}, {"title": "4.5. Ablation Study", "content": "To evaluate the impact of each module within the denoising network, we conduct an ablation study, and the results are shown in Table 3. The performance metrics are evaluated across three datasets: CATH, TS50, and TS500. Firstly, removing the representation alignment module (\"w/o RA\") disrupts the model's understanding of various residue types, leading to a decline in performance across the CATH, TS50, and TS500 datasets. It demonstrates the necessity of the model's understanding of various residue types by aligning their representation during the denoising process. Similarly, excluding the shared center module (\u201cw/o SC\") also leads to a marked decline in performance. Without this module, the model is restricted to relying purely on graph neural network (GNN)-based neighbor inter-residue interactions, without leveraging holistic information from the entire protein chain. This limitation hinders the model's ability to contextualize residue interactions, as evidenced by decreased recall and increased perplexity across all datasets. These results confirm the effectiveness of integrating global chain-level information to enrich residue representations and improve predictive accuracy. When both the representation alignment and shared center modules are simultaneously removed (\"w/o RA & SC\"), the model suffers further performance degradation. This reinforces the complementary contributions of these two components, highlighting that both are indispensable for capturing complex residue dependencies within the denoising network. Lastly, we explore the role of the Cell module within the Message Passing part, which integrates node and edge representations from neighboring nodes. When the Cell module is replaced with a MLP, model performance declines, indicating the crucial role of the Cell module in effectively integrating neighboring node and edge representations."}, {"title": "4.6. Nonlinear Analysis in Shared Center", "content": "To better understand the effectiveness of the Shared Center (SC) module, we examine its influence on the nonlinear characteristics of layer outputs. In Figure 5, we compare two scenarios: one where the SC module is used and another where it is not. The y-axis represents the logarithm of the KL divergence, which quantifies the changes in feature distributions between adjacent layers. A higher value indicates a greater divergence, suggesting more pronounced nonlinear transformations. The results reveal that incorporating the SC module significantly increases the KL divergence across layers, especially in the earlier stages. This indicates that SC enhances the model's ability to capture and propagate complex nonlinear patterns, thereby improving its representation of inter-residue relationships. In contrast, without the SC module, the divergence remains consistently lower, suggesting limited capacity for nonlinear feature extraction. Thus, the SC module's impact is particularly beneficial for tasks that require nuanced representation of protein structures."}, {"title": "4.7. Visualization of Representation Alignment", "content": "To investigate representation alignment, we present visualization results of model predictions. Specifically, we analyze the AA types to which individual residues attend across different layers of the denoising network, as shown in Figure 6. We also show the AA types attended to by multiple residues in the final layer, in Figure 7. In Figure 6, the vertical axis (L1 to L6) represents the network layers, while the horizontal axis represents the 20 AA types. The values indicate the attention weights computed by the representation alignment module for the specific residue and each of the 20 AA types. For the 5tvo.B protein, the true type of the randomly chosen residue is glutamic acid (E), and for the 1aaz.A protein, it is lysine (K). The results demonstrate that as the number of layers increases, the attention weight for the node corresponding to the true AA type of each residue gradually rises. By the final layer, the residue aligns with its true AA type, suggesting that the model effectively aligns residues with their true AA types and incorporates this information into the node representations. In addition, Figure 7 visualizes the attention alignment of multiple residues in the model's final layer. These residues are randomly selected from those with correctly predicted types. The horizontal axis represents the 20 AA types, while the vertical axis represents the true amino acid types of the correctly predicted residues. The visualization shows that the correctly predicted residues have the highest attention weights for their true AA types, which indicates that injecting true AA-type information into the residue representations is beneficial for prediction."}, {"title": "4.8. Folding Ability", "content": "We further explore the folding ability of the generated amino acid sequences to verify its rationality. Specifically, we randomly select test proteins 3fkf and 4u13 from the CATH4.2 test set and utilize the protein structure prediction method ColabFold [25], which offers user-friendly access to AlphaFold2 [19] for predicting the 3D structures of the generated amino acid sequences. These predicted structures are then aligned with the corresponding PDB structures. As shown in Figure 8, the recovery rate of the generated sequences 3fkf is 0.679, and secondary structure elements such as a-helices and B-sheets are effectively formed. The average pLDDT score is 90.39, and the RMSD is 0.917, where the average pLDDT score assesses confidence in the predicted structure, and the RMSD measures the deviation between the predicted and fixed structures. These results demonstrate the validity and rationality of our model in generating new sequences based on fixed backbone structures."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel approach DMRA that leverages diffusion models with representation alignment for protein inverse folding. Firstly, our method integrates contextual information from the entire 3D structure as shared center and assigns it selectively to each residue to maintain inter-residue communication, enhancing the protein representation learning. Moreover, we normalize residue representations and align noisy hidden representations with clean type-specific feature embeddings in the denoising process. In addition, our cell module effectively decouples and computes the relevance of adjacent node and edge information. In experiments, our method surpasses existing leading approaches on the CATH4.2, TS50 and TS500 datasets."}]}