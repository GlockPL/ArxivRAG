{"title": "Review and Recommendations for using Artificial Intelligence in\nIntracoronary Optical Coherence Tomography Analysis", "authors": ["Xu Chen", "Yuan Huang", "Benn Jessney", "Jason Sangha", "Sophie Gu", "Carola-Bibiane Sch\u00f6nlieb", "Martin Bennett", "Michael Roberts"], "abstract": "Artificial intelligence (AI) methodologies hold great promise for the rapid and accurate diagnosis of coronary artery disease\n(CAD) from intravascular optical coherent tomography (IVOCT) images. Numerous papers have been published describing\nAl-based models for different diagnostic tasks, yet it remains unclear which models have potential clinical utility and have\nbeen properly validated. This systematic review considered published literature between January 2015 and February 2023\ndescribing Al-based diagnosis of CAD using IVOCT. Our search identified 5,576 studies, with 513 included after initial\nscreening and 35 studies included in the final systematic review after quality screening. Our findings indicate that most of\nthe identified models are not currently suitable for clinical use, primarily due to methodological flaws and underlying biases.\nTo address these issues, we provide recommendations to improve model quality and research practices to enhance the\ndevelopment of clinically useful Al products.", "sections": [{"title": "1 Introduction", "content": "Accurate diagnosis of coronary artery disease (CAD) is essential for clinical assessment and the accurate identification of\nplaque composition and features could aid in identifying patients at risk of future coronary events. Intravascular Optical\nCoherence Tomography (IVOCT) offers detailed visualisation of coronary arterial structures, 1,2 with significantly higher\nresolution than other invasive imaging modalities (such as intravascular ultrasound (IVUS) and angiography)\u00b3 or non-\ninvasive methods (such as magnetic resonance imaging (MRI) and computed tomography (CT)). IVOCT can accurately\nidentify plaque tissue types, (e.g., fibrous tissue, calcium, lipid) and different plaque types (e.g., pathological intimal\nthickening, fibrocalcific, thin and thick cap fibroatheroma etc.). 4,5 However, OCT datasets are large with structures that are\noften unclear and variable in appearance, and manual interpretation of hundreds of images per patient is not only time-\nconsuming but also prone to errors. There is a growing demand for automated methods of IVOCT image analysis to\naddress these challenges.\nThe application of Artificial Intelligence (AI) techniques, particularly Machine Learning (ML), enables rapid image\ninterpretation by automating classification and segmentation processes, thereby potentially accelerating IVOCT analysis\n(Figure 1). However, there is increasing 7-9 concern about the datasets used to train ML models, their validation,\nreproducibility and generalisability, resulting in a lack of standardisation. We found 172 papers using AI/ML methods have\nbeen applied to IVOCT, but there was inconsistent standardisation such as data collection/selection procedures,\nperformance validation and evaluation metrics. While previous reviews offered a broad analysis of Al models for IVOCT\nanalysis, 10,11 we specifically focus on the systematic methodological pitfalls in the current literature. We assess the risk of\nbias in the literature, incorporating a quality screening stage to ensure that only papers with sufficiently documented\nmethodologies are reviewed. We also provide detailed recommendations across four domains: (1) considerations for\ncollating IVOCT imaging datasets intended for public release; (2) methodological considerations for AI/ML researchers and\nspecific issues regarding validation of results; (3) specific issues regarding the reproducibility of results; and (4)\nconsiderations for reviewers conducting peer reviews of manuscripts."}, {"title": "2 Methods", "content": "Review strategy and selection criteria.\nInitial screening. To obtain the screening population, we searched SCOPUS for all whose titles or abstracts contained either\n\"OCT\" or \"optical coherence tomography\" along with either \"deep learning\", \"artificial intelligence\", \"AI\", \"machine learning\",\n\"neural networks\u201d, \u201cauto\u201d, \u201cML\u201d or \u201cnet\u201d). This search was not case sensitive and returned 5,576 papers whose titles and\nabstracts were pre-screened to exclude those papers focussed on retinal IVOCT imaging and retain those focussed on\ncoronary arteries (detailed search criteria are detailed in the Supplementary Information).\nTitle and abstract screening. This was performed by three independent reviewers with conflicts resolved via consensus.\nFull-text screening. Two reviewers performed the full-text screening with conflicts resolved by consensus with a third reviewer.\nQuality screening. This stage aimed to exclude those deep learning papers which had poor quality documentation of critical\ndetails necessary for reproducibility of the method described. We followed the approach in Roberts et al. comparing\nmanuscripts with the Checklist for Artificial Intelligence in Medical Imaging (CLAIM) 12 and excluding those papers which failed\nany of eight mandatory criteria (detailed in the Supplementary Information). The full CLAIM checklist is reported for each\npaper which passes the quality screen. Traditional machine learning papers are assessed using the radiomic quality score\n(RQS)13 guidelines but no papers were excluded on the basis of their score.\nAssessing the risk of bias in studies. In order to assess bias in the datasets, predictors, outcomes and model analysis in each\npaper, we use the PROBAST of Wolff et al. 14 Papers that passed the quality screening stage were split among two reviewers\nto complete the PROBAST review, with conflicts resolved by a third reviewer.\nData extraction. Four reviewers extracted data from the manuscripts, with two reviewers considering each manuscript and\nresolving conflicts. The full dataset is in the Supplementary Data and forms the basis of this review."}, {"title": "3 Results", "content": "3.1 Study selection\nWe identified 5,576 papers that satisfied our search criteria and 172 had abstracts or titles relevant to this review, i.e.\ndeveloping ML methods using IVOCT imaging for diagnostic modelling of CAD. After full-text screening, 71 papers remained\nand quality screening retained 35/71 papers for consideration in this review (Table 1). Of these, 30/35 were pure deep\nlearning (DL) papers and 3/35 we refer to as traditional ML papers (i.e. non-deep learning ML papers). Two papers developed\na hybrid of both approaches 15,16 (Figure 2).\nPaper quality screen\nOf the 68 deep learning papers screened using the CLAIM checklist12, over half (36/68) were excluded for failing mandatory\nCLAIM checklist criteria (detailed in the Supplementary Information). 19/68 failed just one, 14/68 failed two and 3/68 failed\nthree or more. The two most common reasons for failing the quality screen were insufficient documentation of the data\nsource (28/68) and description of the training approach (13/68). Of 68 papers, 35 passed the initial quality screening, with\n30 being DL papers, 3 were traditional ML papers and 2 were a hybrid of these.\nIn the next stage, DL papers were evaluated using the full CLAIM checklist, while traditional ML papers were assessed with\nthe RQS13 checklist to establish how adherent the manuscripts were to established guidelines for manuscript completeness.\nDL paper quality screen\nOnly 4/32 papers failed more than 10 items on the 42-point CLAIM checklist and 27/32 failed more than five. The five most\ncommon items not satisfied were missing: robustness/sensitivity analysis of models (31/32), details for manual annotation\ntools (27/32), external model validation (25/32), patient demographics (25/32), and clear details on inclusion/exclusion to\nobtain patient populations (25/32).\nTraditional ML paper quality screen\nThe three papers assessed using RQS received scores of 4,17 5,18 and 8.19 No papers reported calibration statistics, statistical\nsignificance for discrimination statistics, the potential clinical utility, cost-effectiveness nor shared code, data or models. Only\none paper performed feature reduction when selecting features 19.\n3.2 Datasets\nSources. To train generalisable and reproducible ML models, it is best practice to train using data from multiple sources,\nrepresentative of the patient populations under study, and ideally available for other practitioners to access and\ntrain/validate against. However, the majority of papers (31/35) used entirely private data sources (one used both a public"}, {"title": "3.3 Model Development", "content": "Outcomes of interest. The power of Al to identify and measure tissue types on IVOCT, and thus classify plaques, depends\nupon accurate localisation of tissues with a range of different appearances, using dense segmentations, bounding boxes or\nA-line- or patch-based classification (Figure 1). 17/35 papers described segmentation models for calcium (12/17), lipid (5/17),\nor fibrous (4/17) plaques, and 10/35 papers described methods which identified a bounding box around tissues including\ncalcified (1/10) or mixed plaques (7/10) and thin fibrous caps (2/10). Classification models were developed in 8/35 papers,\nfocussed on classifying entire frames as showing lipid and calcium plaques (6/8), rupture22 (1/8) and thin cap fibroatheromas\n(1/8)20. Three of these papers describe classification models operating on the A-line raw IVOCT data and aimed to distinguish\nlipid vs calcium. 19,23,42 (Figure 4).\nDataset partitioning. Only 14/35 papers partitioned their datasets at the patient-level for their model development and\nevaluation, whereas 1/35 partitioned 24 at pullback-level, 11/35 at the lesion-level and 6/35 at the frame-level. One paper43\ntrained both a classifier and segmentation models, using a patient-level split and lesion-level split respectively. One paper20\ntrained a model independently for two datasets, with a lesion-level split for one and an unclear approach for the other. Finally,\none paper was unclear in how the data had been split 44. Failing to split data at the patient-level risks data from the same\npatient, pullback or lesions being used for both training and model evaluation and the likelihood of optimistic performance\nreporting. Most papers (19/35) used cross-validation to develop their models, with 8/35 using a fixed internal validation set\nand 6/35 relying only on internal holdout data. One paper32 was unclear in how the model was evaluated and another 20\ndeveloped two models, one using cross-validation and another using a fixed internal validation cohort.\nData pre-processing. Despite raw IVOCT imaging data being acquired in polar coordinates, most papers (18/35) performed\ntheir analysis on Cartesian transformed images, which are more familiar to clinicians and one used both polar and Cartesian\nimages 20 (Figure 1). Real-world IVOCT pullbacks contain multiple artefacts 45, a significant proportion of which affect\ninterpretation of the underlying vessel wall. However, only 12/35 papers applied denoising techniques to suppress these\nartefacts (e.g., using Gaussian and Gabor filters). Segmentation quality and measurements can be greatly affected by image\nresolution and size, both of which are commonly standardised during pre-processing. The resampled image size is mentioned\nin only 21/35 papers with only two papers considering an image resolution above 512 x 512 pixels 23,24.\nModel inputs. IVOCT images are acquired using a spinning catheter sampling radially as it is pulled along the artery, these\nradial samples are stacked to form the 2D frames of the final image. The 2D frames themselves are stacked to form a 3D\nvolume. A majority of papers (26/35) developed their models using 2D frames as inputs with 6/35 considering the pullback\nas 3D volumetric data15,21,24,30,37,46 and 3/3519,23,42 focussing on the 1D A-line data. None of the papers consider the spiral\nnature of IVOCT data acquisition in their modelling approach.\nModel architecture. There is a rapidly increasing selection of available Al applications and model architectures reflected in\nthe published literature. Of the 17 papers describing segmentation models, 14/17 built upon existing DL architectures\nincluding: U-Net-like (5/14), 16,24,37,40,47 SegNet (4/14),32,33,48,49 Deeplabv3+ (4/14), 29,38,39,48 and Transformer-based models\n(1/14),25 while the other DL-based paper trained a custom architectures. The 2/23 papers which focussed on traditional ML\nalgorithms evaluated a random forest17, XGBoost 17 and support vector machines 18 for pixel-level classification. For the 10/35\npapers describing object detection applications, all used DL with 2/10 papers using a Faster R-CNN26,31 and ResNet, 28,30 while\nthe rest employed a highly diverse range of architectures including a vision transformer, 21 DenseNet, 41 Inception-V3,34"}, {"title": "3.4 Risk of bias assessment", "content": "Al models can be very prone to bias, most commonly arising from the participants, predictors, outcome and analysis (see\nSupplementary Data), The risk of bias (ROB) and concerns of applicability were assessed for all papers using the prediction\nmodel risk of bias assessment tool (PROBAST) guidance. We found a high ROB in at least one of the four domains in 20/35\npapers (see Supplementary Data).\nParticipants. The ROB was rated as low for the participants domain in only 5/35 papers, 21,22,24,34,41 with a high ROB found in\n17/35 papers (see Table 1). This was primarily due to the selection of samples at region-, segment- or frame-level, with a risk\nof validating the model using patients that the model was trained upon. Risks of bias were also increased due to: selecting a\nsubset of pullback frames without clear inclusion/exclusion criteria; using only well-defined samples and dropping those with\nequivocal findings; only sampling from selected plaque pathologies; using a data subset where the demographics varied\nsignificantly from the full cohort; and only using pullbacks from patient groups where particular plaque compositions were\nsignificantly enriched. The ROB was rated as unclear for the Participants domain in 13/35 papers in which insufficient\ndemographics and recruitment information were given, and was found for both private and public datasets.\nPredictors. Almost all papers (32/35) developed DL models in which the predictors are abstract and unknown imaging\nfeatures. Therefore, the ROB was rated as unclear for these papers. The remaining three papers 17-19 were found to have a low\nROB, relying on pre-defined hand-engineered features derived from the images.\nOutcome. All papers included in this review described models for classifying plaque types or localising their components with\nthese outcomes defined using consensus recommendations. Therefore, all papers were rated low ROB in this domain.\nAnalysis. Most papers (23/35) were found to have a low ROB for their analysis (see Table 1). High ROB was found in 9/35\npapers due to small sample sizes 17,23,27,32,33,39,40,47 or inappropriate performance evaluation. 31 Two papers had an unclear\nROB30,49 as they did not report the proportion of positive samples in their dataset."}, {"title": "3.5 Code and model availability", "content": "Al model performance can vary greatly between centres and datasets, and external reproduction of results is required to be\nconfident of their generalisability. However, no papers provided detailed instructions or open-source code to allow for the\nexternal reproduction of results. Most papers also did not share any data (28/35), with the remainder stating that data was\navailable on reasonable request. Many papers did not mention the software in which their models were implemented (13/35)\nand of those which did, most used MATLAB (12/35) or Python (10/35). One paper used a commercial tool OctPlus24."}, {"title": "4 Discussion", "content": "IVOCT is the highest-resolution widely available modality for the imaging of the coronary arteries and the only one able to\nidentify and measure high-risk thin fibrous caps. In 2024, European guidelines were updated to strongly recommend the use\nof IVOCT to guide stenting of complex lesions. However, accurate interpretation of IVOCT imaging requires significant training\nand is a barrier to the scaling of IVOCT adoption. Al models hold significant promise for far faster and more scalable analysis\ncompared with a human reader. Therefore, with increasing availability of IVOCT data and the ability to analyse them with\ndeep learning tools and hardware, it is only natural that we see increasing appetite from researchers to develop Al models\nfor interpreting IVOCT images. However, we found that although published studies show considerable promise and potential\nin this field, many are burdened with methodological and reporting deficiencies, with most of the reviewed literature not\nready for clinical application. We have identified issues around dataset documentation, methodologies, reproducibility and\nbiases in study design, which we now summarise and suggest recommendations to improve the evidence base to allow for\nwider adoption of Al tools for automated IVOCT interpretation.\nIn general, there is a strong preference in the literature towards training of deep learning models rather than more traditional\nradiomics approaches, with only three focussing on the latter (only one since 2020). Additionally, very few papers developed\nmodels to give a frame-level classification for disease status, with all others focussed on localising the disease through\nsegmentation and object detection within the image. All the papers which disclosed their catheter manufacturer reported\nusing images acquired by Abbott catheters for model training, likely due to data availability with Abbott distributing the\nmajority of IVOCT catheters globally. Consequently, the majority of models are likely applicable only to Abbott acquired\nimaging.\nImage acquisition and datasets.\nSpiral acquisitions. IVOCT images are acquired by a spinning catheter pulled down an artery, with image data acquired as 1D\nprofiles (A-lines) in a spiral. These 1D grayscale acquisitions are often stacked into (artificially RGB coloured) 2D frames, which\nthemselves are then stacked into 3D images. No papers in this review considered the spiral nature of the acquisition in their\nmodelling, rather focussing the IVOCT images as 2D/3D acquisitions. It is likely that incorporating the acquisition technique\ninto the methodology will improve model quality by reducing known artefacts (such as seam artefacts) that occur due to\nstacking into 2D frames.\nDataset sizes. We found that many papers used relatively small IVOCT datasets for model development with a median of 55\npatients and only six studies using datasets with more than 100 patients. Developing models using small-scale datasets risks\nintroducing potential biases into the model, limiting its generalisability, and findings should be approached cautiously. For\ndeep learning models, it is common to require many thousands of training samples due to their over parametrisation, which\nis more achievable at the 2D frame level but often hard to achieve (and of unclear necessity) at the 3D pullback level.\nDataset diversity. Given the real-world diversity of IVOCT imaging data collected from patients of different backgrounds, it is\nunfortunate that only six studies utilised data from multiple countries with others training primarily with data from the United\nStates and China. This bias in geographic scope will likely limit model applicability, and it is of primary importance to ensure\na diversity of patients and of disease profiles. to avoid bias in the model and enhance its applicability.\nGround truth. Clinicians are trained to interpret the RGB cartesian IVOCT images, derived from the raw grayscale polar IVOCT\ndata. Surprisingly, however, in around half of papers it is the raw image data that is used to train models and it is generally\nunclear how the ground truth segmentation labels were generated when these data are never observed in clinical practice.\nInclusion/exclusion criteria. Reporting inclusion and exclusion criteria is crucial for understanding a model's training\npopulation and likely limitations of applicability. Their absence was particularly striking in our review, with only fifteen papers\nproviding detailed inclusion criteria and exclusion criteria reported in only twelve papers. The inclusion and exclusion criteria\nvaried widely between studies (depending on area of focus), highlighting the heterogeneity in patient populations used for\ntraining and underscores the need for transparent reporting to allow for fair study comparisons and improve reproducibility.\nMethodologies for model building\nDataset partitioning. Most papers did not partition their datasets at the patient level and used only an undisclosed subset of\nframes from the pullback for training and evaluation. This leads to a high-risk of data leakage in the literature whereby frames\nfrom the same patient are being allocated to train/validation/holdout cohorts, and similar plaque features from adjacent\nframes may inadvertently influence model performance and lead to optimistic performance.\nValidation issues. For model development, twice the number of papers employed cross-validation against those using a single\nfixed validation cohort to evaluate their model performance. Most models developed using a single private dataset and\nexternal validation is absent in most papers, both of which raise important concerns around the generalisability of the models\nto new populations. Furthermore, with the widespread lack of disclosure of inclusion/exclusion criteria and demographic\ninformation on the training population, our review of bias assessment gives a high concern in the ability of the models to\ngeneralise to new populations for a majority of papers.\nReproducibility of the existing literature\nManuscript documentation. In general, the literature is poorly documented for reproducibility, as shown by the quality\nscreening, which removed over half of all papers considered. This was primarily due to insufficient documentation of data\nsources and model training descriptions. However, as discussed previously, the provenance of data, the inclusion/exclusion\ncriteria and the demographics of the training cohort are critical for accurately understanding the cohort that the results\nmay reproduce. In addition, for those models developed and assessed against selected frames from pullbacks, rather than\nfull images themselves, it is hard to reproduce this selection without a sufficient description of the process. Similarly,\nwithout sufficient descriptions of the model training procedure, it is impossible to reproduce the results of any experiments.\nOpen science. The only public dataset that is considered in the literature, CCCV-IVOCT from 2017, appears no longer to be\navailable or accessible. This is very unfortunate as it prevents the transparent benchmarking of model performance and is\nin contrast with many application areas of Al. For example, there are several large (curated) datasets publicly available for\nAl model development using Chest X-Rays51,52, and the lack of this resource may be due to IVOCT being perceived as a\nniche modality. Similarly, for Al methods in other clinical domains it is common for academic papers to openly share code\nand trained models to allow for an easier assessment of both whether the proposed model outperforms prior models and\nhow well models generalise to new data. Without such convenience, each author must develop and validate their tools in\nisolation without performance benchmarks to compare against. However, we find that no papers share their code nor\nwere trained models shared."}, {"title": "Recommendations", "content": "Validation strategies. Before developing models, developers must have a clear validation strategy with external datasets,\nheld out during model development for assessing the generalisability of the models.\nMore open science. The IVOCT research community should aim to publicly release (and permanently archive) more\nIVOCT datasets from different demographics, to allow for better (transparent) benchmarking of model performances.\nProgress by the research community would be accelerated by a more transparent and open sharing of code and models\nto allow for easier assessment of how well models generalise to new data. Platforms such as Hugging Face and Code\nOcean allow for rapid deployment of models reproducibly to a wide audience of model developers.\nEncourage full pullbacks use. Models should be trained and evaluated against full IVOCT pullbacks. If only selected\nframes are used in model training, the applicability will likely be limited in the real world and model performance will\nlikely be optimistic. If full pullbacks are not used, it should be clearly stated as a limitation of the study.\nDataset partitioning. Dataset sizes should be primarily reported at the patient level and partitioning into training,\ninternal validation and holdout cohorts should be performed at that level.\nDataset sourcing. Standardised reporting guidelines should be adopted to ensure transparency regarding dataset origins\nand patient demographics. Authors should adhere to established guidelines for reporting inclusion and exclusion criteria\nto ensure transparency and consistency in reporting. Model developers should avoid building models using collections of\n2D images which are of unknown provenance, as the inherent structural relationships between images from the same\npullback give rise to risks of reporting overly optimistic performance.\nGround truth. Manuscripts should clearly disclose how the images were assigned their ground truth labels. For\nsegmentation models this should include information on whether the false-colour RGB images were those labelled and\nthe software used. The known performance differences between experienced and junior IVOCT readers 53 means that it\nis important to disclose the annotator's experience level. For classification models, if disease labels (e.g. PIT, AIT and\nTCFA) are assigned based on the IVOCT images themselves this risks incorporation bias (where labels are not\nindependent of the predictors), therefore authors should also compare model performance to labels assigned from\nother sources e.g. histopathology slices.\nImproving documentation. During manuscript drafting, we recommend that authors assess their papers against\nestablished standards such as CLAIM12, RQS13, PROBAST14, REFORMS (Reporting Standards for Machine Learning Based\nScience) 54, TRIPOD (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis) 55\nand QUADAS (Quality Assessment of Diagnostic Accuracy Studies) 56. For manuscript reviewers and journal editors, we\nrecommend using these checklists to identify weaknesses in methodology reporting when giving feedback on\nmanuscripts."}, {"title": "5 Conclusions", "content": "In this systematic review, we have examined the published literature on AI/ML methodologies applied to the diagnosis of\nCAD using IVOCT, focusing on the quality, reproducibility and potential clinical utility of these methodologies. In their current\nreported forms, few of the Al-based models reviewed are immediately suitable candidates for clinical translation for the\ndiagnosis of CAD. To enhance the likelihood of these models being integrated into future clinical trials, we recommend (1)\nusing datasets with precise and transparent descriptions of data collection, pre-processing, and any transformations applied;\n(2) providing thoroughly documented manuscripts with detailed methodologies to ensure that studies can be accurately\nreplicated; (3) Conducting comprehensive external validation with independent datasets to ensure the model's performance\nis reliable and generalizable across different populations and clinical settings."}]}