{"title": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding", "authors": ["Yixiong Fang", "Ziran Yang", "Zhaorun Chen", "Zhuokai Zhao", "Jiawei Zhou"], "abstract": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in multimodal tasks but are prone to misinterpreting visual inputs, often resulting in hallucinations and unreliable outputs. To address these challenges, we propose DROPOUT DECODING, a novel inference-time approach that quantifies the uncertainty of visual tokens and selectively masks uncertain tokens to improve decoding. Our method measures the uncertainty of each visual token by projecting it onto the text space and decomposing it into aleatoric and epistemic components. Specifically, we focus on epistemic uncertainty, which captures perception-related errors more effectively. Inspired by dropout regularization, we introduce uncertainty-guided token dropout, which applies the dropout principle to input visual tokens instead of model parameters, and during inference rather than training. By aggregating predictions from an ensemble of masked decoding contexts, DROPOUT DECODING robustly mitigates errors arising from visual token misinterpretations. Evaluations on benchmarks including CHAIR, THRONE, and MMBench demonstrate that DROPOUT DECODING significantly reduces object hallucinations (OH) and enhances both reliability and quality of LVLM outputs across diverse visual contexts. Code is released at https://github.com/kigb/DropoutDecoding.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capabilities [11, 15, 17, 54, 59, 62, 66], in tasks such as image caption-ing [58], visual question answering (VQA) [1, 22, 29, 61], multimodal reasoning [6, 37, 39, 40, 63] and so on. However, LVLMs still face challenges in accurately perceiving and interpreting visual inputs, leading to inaccurate outputs and hallucinations [35]. These issues often stem from LVLMs misrepresenting key image elements or overlooking critical details, compromising the reliability of their outputs in tasks demanding precise visual understanding [4, 5, 16, 57].\nIn practice, LVLMs typically process visual inputs token by token, which we refer to as visual tokens. This can fall short in effectively focusing on the most informative parts of the visual context. While attention mechanisms are designed to prioritize relevant information, they are not always perfect [46, 52], especially when the inputs are complex or ambiguous for the model, or in other words, of high uncertainty. Existing methods to address these challenges in the training stage often involve fine-tuning on specific tasks [33, 34, 49, 56], or using additional supervision signals especially at lower level to guide the model [7, 55]. However, these approaches are resource-intensive and not easily extensible to new tasks. Alternative inference-time strategies, such as attention-based or logits-based mechanisms on decoding correction [4, 21, 45, 51, 60], attempt to identify important regions in the input without additional training, but they typically rely on heuristic design choices and largely increase inference cost. Therefore, enhancing the trustworthiness of LVLMs and reducing hallucinations require more principled methods that can more effectively emphasize the most informative parts of the visual input.\nTo address this challenge, we propose a novel approach that quantifies uncertainty in visual token contexts and removes uncertain tokens, both directly at inference time to improve the reliability of LVLM outputs. Inspired by traditional dropout [47] techniques-typically applied to model parameters but difficult to implement directly in pretrained LVLMs [13, 24]-we introduce token dropout, which applies the dropout principle to input context tokens instead of model parameters. Furthermore, it is applied to regular-ize the inference process instead of training, by introducing randomness in decoding contexts to reduce overfitting to noisy visual tokens."}, {"title": "2. Related Work", "content": "Reliable generation. Reliable generation in LLMs is often challenged by hallucinations, where the model generates irrelevant or factually incorrect information [18, 48, 64]. These hallucinations stem from issues in data, training, and inference stages [53], with attention mechanisms exacerbating the problem as sequence lengths grow [8]. To mitigate these, methods like factual-nucleus sampling have been proposed to balance output diversity and accuracy [26]. Besides, while Arias et al. [2] leverage quantified uncertainty to guide the decoding process for LLM, our method differs significantly. We quantify uncertainty at the level of visual input context rather than of model ensemble which is heavy.\nOH in LVLMs. OH is a common issue in LVLMs, where models generate descriptions containing objects, attributes, or relationships not present in the actual image. The CHAIR metric [41] is widely used to evaluate OH, measuring the hallucination rate on the MSCOCO dataset [31]. Another benchmark, POPE [30], treats object hallucination as a binary classification task. More recently, THRONE [23] takes a more holistic approach, using open-ended, object-based image descriptions for evaluation. In our work, we use CHAIR and THRONE to assess OH.\nOH reduction. Recent methods addressing OH in LVLMs include internal signal guidance, contrastive decoding, and selective information focusing, all of which are inference-time strategies. OPERA [19] uses internal signals like attention patterns to refine outputs and improve alignment with visual content. Contrastive decoding methods, like VCD [27], enhance coherence by comparing image-specific outputs. Selective information focusing approaches, such as HALC [4], prioritize key image regions, while CDG [10] uses CLIP embeddings to align generation with visual input. In contrast, DROPOUT DECODING works with any LVLM by 1) selecting visual information from visual tokens during generation, unlike HALC which selects regions initially; 2) using uncertainty to guide visual information selection, requiring no external models, unlike HALC and CDG; 3) introducing a token-level majority voting strategy."}, {"title": "3. Preliminaries", "content": "Widely adopted LVLM architectures [28, 32, 33] typically include a vision encoder, a vision-text interface module, and a Transformer-based LLM decoder. As we mostly focus on the decoder side inference optimization, we assume the LLM decoder is with parameter \u03b8.\nThe visual input, such as an image, is segmented into patches and processed by the vision encoder, followed by the vision-text interface module, to produce a sequence of visual tokens x\\u00b0 = (x1, x2,...,xN). Each token x_i^v is a contextualized embedding of an image patch, serving as the direct input to the text decoder. The text input such as a query or instruction is x^t = (x_1^t,x_2^t,...,x_M^t). The input to the text decoder is denoted as x = [x\\u00b0, x^t], which is the concatenation of visual and text tokens. At this point, the visual and text tokens are aligned and serve as a sequential input to the LLM decoder.\nDuring autoregressive decoding, the decoder generates output text tokens y = (y1, y2,...) as continuation from prompt x, following the conditional probability distribution\n$P_{\\theta}(y_j | x^v, x^t, y_{<j}) = \\text{softmax}(W_v h_j)$ (1)\nwhere y<j = (y1,..., yj\u22121) is the sequence of previously generated tokens, f\u03b8 denotes the LLM forward pass to produce hidden states hj \u2208 Rd on top of the Transformer layers, Wv \u2208 R|V|\u00d7d is the output projection matrix onto the text vocabulary V, and yj \u2208 V the output token at j-th step."}, {"title": "3.2. Uncertainty Quantification", "content": "Our approach quantifies the information uncertainty of visual tokens used for decoding by adapting the concept of epistemic uncertainty for measurement, as detailed in \u00a75, and drawing inspiration from classical uncertainty decomposition [20, 43, 44]. To provide the necessary background, we first introduce the concept of uncertainty decomposition.\nUncertainty decomposition separates the total uncertainty of a model's prediction into two components: aleatoric uncertainty, which is inherent to the data, and epistemic uncertainty, which relates to the model's lack of knowledge. The Bayesian framework offers a principled way to quantify uncertainty about some candidate model with weights w, through the posterior estimation over the hypothesis space for a given dataset D. The Bayesian model average (BMA) predictive distribution is defined as\n$p(y | x, D) = \\int_{W} p(y | x, w) p(w | D) dw.$ (2)\nThe total information uncertainty is measured by the entropy of BMA: H[p(y | x,D)], which equals the posterior expectation of the cross-entropy between the predictive distribution of the candidate model and the BMA distribution:\nH[p(y | x, D)] = E_{p(w|D)} [CE[p(y | x, w), p(y | x, D)]]\nThe epistemic uncertainty, expressed as the KL divergence between candidate models' predictive distributions and the BMA, has proven effective in various applications [3, 13, 38, 65]. Our approach, adopts a similar formulation for uncertainty quantification, calculating the KL divergence between candidate prediction distributions on individual visual tokens and an aggregated average distribution."}, {"title": "4. Textual Interpretation of Visual Tokens", "content": "As discussed in \u00a71, identifying the visual tokens that carry significant information and quantifying their uncertainty is critical for improving the reliability of LVLMs. To address this, we propose a supervision-free, scalable approach that maps visual tokens to the text token space, effectively translating visual content into an interpretable text-based representation. This mapping acts as a heuristic for understanding visual tokens, leveraging the LVLM's inherent ability to align visual and textual contexts.\nText-space projection of visual tokens. While LVLMs are trained to generate text only after processing all visual tokens x_v and text instruction tokens x_t, the hidden representations h on top of the text decoder layers inherently capture textual semantics. This is due to their proximity to the text vocabulary projection, even at visual token positions where the model is not explicitly trained to generate text.\nBuilding on this intuition, we adopt a heuristic approach to interpret visual tokens by projecting them onto the text vocabulary at the top Transformer layers. In particular, for each visual token x_i^v at position i, we obtain its textual projected distribution over the vocabulary V from the last layer of the LLM decoder in the LVLM as:\nh_i = f_{\\theta}(x_{<i}^v)\nq_{proj}^v = P_{\\theta}( \\cdot | x_i^v) = \\text{softmax}(W h_i) (3)\nwhere h_i is the LLM decoder top-layer hidden representation aligned at the i-th visual token positions, x_i^v denotes the visual tokens up until index i.\nHere, q_{proj}^v, which we refer to as visual-textual distribution, represents the projection of the visual input onto the text space. It encapsulates the model's interpretation of the i-th visual token. This projection offers a text-based summarization, akin to an unordered caption or a \"bag-of-words\" representation of the visual content. As we will demonstrate in \u00a76, this heuristic method serves as an effective proxy for uncertainty estimation."}, {"title": "5. Method", "content": "We propose Dropout DECODING, which leverages visual uncertainty to selectively drop out visual tokens and guide decoding. As shown in Fig. 2 and Algorithm 1, our approach comprises two stages: uncertainty quantification (\u00a75.1) before decoding and uncertainty-guided token generation (\u00a75.2) for decoding."}, {"title": "5.1. Uncertainty Quantification Before Decoding", "content": "Average visual-textual distribution. We begin by defining the averaged distribution qproj, which represents the overall projection of the entire visual input (e.g. an image) into the text space. Using the projected distribution defined in Eq. (3), we define the average projection distribution over all visual tokens as:\n$q_{proj} = E_i [q_{proj}^v] = \\frac{1}{N} \\sum_{i=1}^N q_{proj}^v$ (4)\nwhere qrproj represents the text-space projection of the i-th visual token, and N is the total number of visual tokens. Note that the subscript i indicates different distributions rather than elements within a single distribution. This provides us with a \"baseline\" representation of the visual input, against which we can quantify the surprisal of a specific visual token. This idea is grounded in classical uncertainty decomposition where a Bayesian average distribution is needed to quantify epistemic uncertainty [20, 43].\nUncertainty measurement for visual tokens. We aim to quantify the uncertainty associated with each visual token at inference time. To distinguish from those uncertainty terms in classical settings as introduced in \u00a73.2, we refer to ours as perception uncertainty. We start by quantifying the perception total uncertainty of the visual input as the entropy of the average visual-textual distribution H [qproj]. Then, to attribute this total uncertainty to individual visual tokens, we decompose it (more details in Appendix A) as follows:\n$U_{total} = H [q_{proj}] = E_i [CE (q_{proj}^v, q_{proj})]$ (5)\nFurther decomposing the cross-entropy (CE), the perception total uncertainty can be expressed as:\n$U_{total} = E_i [H [q_{proj}^v] + D_{KL} (q_{proj}^v || q_{proj})] = E_i [U_{ale} (i) + U_{epi}(i)]$"}, {"title": "5.2. Uncertainty-Guided Decoding", "content": "During the text decoding process, we leverage the computed uncertainty measures to guide the generation of each token. Our method involves two main steps for each generated text token: (1) identifying relevant visual tokens (optional), and (2) performing token dropout with uncertainty-guided masking. The first step is optional, designed to enhance decoding by retaining more relevant visual tokens.\nIdentifying relevant visual tokens (optional). We selectively retain only the most relevant visual tokens from the context, which are excluded for dropout. To do this, when generating each output text token, yj, we first perform a preliminary forward pass to generate an initial prediction token yinit.\n$y_j^{init} \\sim p_{\\theta}( \\cdot | x^v, x^t, y_{<j})$ (7)\nNext, we determine the set of visual tokens that are relevant to this initial prediction. Specifically, a visual token $x_i^v$ is considered relevant if the initial prediction $y_j^{init}$ appears among the top-k tokens of its visual-textual projection $q_{proj}^v$. Formally, the set of relevant visual tokens for the j-th generation is:\n$S_j = \\{ x_i^v | y_j^{init} \\in \\text{TopK}(q_{proj}^v) \\}$ (8)\nwhere TopK(.) denotes the function returning the top-k entries of a given distribution.\nTo illustrate the intuition behind this step, consider an image depicting a cat. Suppose the model correctly predicts the token \"cat\" during the preliminary forward pass. In that case we retain the visual tokens associated with \"cat\" and drop out among the remaining visual content. Conversely, if the model incorrectly predicts \"dog\" or unrelated tokens irrelevant to an object, these predictions will not align with the top text projections of any $q_{proj}^v$ if the visual interpretation is accurate. In such cases, no visual tokens are retained due to a lack of clear relevance, and dropout is applied across the entire visual context as the best alternative.\nIt is worth noting that this step is optional. Omitting it can improve efficiency by reducing the computational overhead of the preliminary forward pass. As shown by the ablation studies in \u00a77, while skipping this step may lower performance on certain benchmarks like THRONE [23], it still achieves comparable results on others such as CHAIR [41].\nVisual token dropout with uncertainty guidance. Using the epistemic uncertainty measurements $U_{epi}(i)$ from Eq. (6), we introduce dropout masks over visual tokens. As illustrated in Fig. 1, the projected visual-textual distributions sometimes misalign with the image content, and regions of high information can lead to substantial errors, resulting in hallucinations. Building on this intuition, we selectively target visual tokens with high epistemic uncertainties for dropout.\nSpecifically, we formulate a controllable series of sample distributions for visual token dropout based on $U_{epi}(i)$, for each visual position i:\n$p_{dropout}^{(k)}(x_i^v \\cdot x_i^v) = \\gamma^{(k)} \\frac{U_{epi}(i) - U_{min}^{epi}}{U_{max}^{epi} - U_{min}^{epi}} + \\delta^{(k)}$ (9)\nwhere $U_{min}^{epi}, U_{max}^{epi}$ are the minimum and maximum epistemic uncertainty values across all visual tokens, and $\\gamma^{(k)}$ and $\\delta^{(k)}$ are hyperparameters controlling the probability range of the dropout. By adjusting the values of $\\gamma^{(k)}$ and $\\delta^{(k)}$, we can modulate the intensity of visual token dropout.\nWith the dropout distributions, we can sample dropout masks for each visual token independently. Denote the binary mask as $M^{(k)} \\in \\{0,1\\}^N$, consisting of a binary indicator $M_i^{(k)}$ for each visual token $x_i^v$, where the corresponding visual token is retained if $M_i^{(k)} = 1$, and dropped if $M_i^{(k)} = 0$. The dropout mask sampling follows $P(M_i^{(k)} = 0) = p_{dropout}^{(k)}(x_i^v \\cdot x_i^v)$, and the sampling is done for each visual token position independently. A higher value of $P_{dropout}^{(k)}(x_i^v)$ indicates that $x_i^v$ is more likely to be dropped out. If we performed the optional preliminary forward pass to identify relevant visual token set Sj, these visual tokens are never dropped, i.e., \u2200$x_i^v$ \u2208 Sj, set $M_i^{(k)} = 1$ directly.\nEnsemble-based reliable generation. Our inference-time context dropout introduces stochasticity, so we employ an ensemble decoding approach by independently sampling K distinct dropout masks, {$M^{(k)}$}$_{k=1}^{K-1}$, to enhance generation quality. Since the masks are independent, the text generative distribution from K masks can be efficiently computed in a parallel forward pass\n$y_j^{(k) Decoding} \\sim p_{\\theta}( \\cdot | x^v/M^{(k)}, x^t, y_{<j})$ (10)\nwhere $x^v/M^{(k)}$ denotes the visual tokens after applying dropout mask M(k), and $y_j^{(k) Decoding}$ denotes invariance to the decoding algorithm used (e.g., greedy search in our implementation, though others are applicable).\nEach $y_j^{(k) Decoding}$ serves as a candidate prediction for the next text token, with the final token yj selected via majority voting among the K masked inputs. In case of a tie, we choose the prediction from the forward pass with the fewest dropped tokens, as it retains the most information and is deemed more reliable. By forming an ensemble of predictions derived from various subsets of the visual input, enabled through token dropout, we diversify the model's perspective on the visual content. This diversity mitigates the impact of any single misinterpretation, ultimately leading to more reliable and robust generation, which is also observed in other ensemble-based methods [4, 12, 14, 25, 42]."}, {"title": "6. Experiments", "content": "We evaluate the proposed DROPOUT DECODING from two aspects: OH reduction and overall generation quality. For OH, we use the CHAIR [41] and THRONE [23] metrics to assess the performance of different decoding methods on the MSCOCO dataset. Additionally, we employ MMBench [36] to evaluate the overall generation quality and general ability of these methods."}, {"title": "6.1. Experimental Setup", "content": "Base LVLMs. We evaluate all methods on three representative LVLMs: LLaVA-1.5 [32], InstructBLIP [9] and LLaVA-NEXT [34]. LLaVA-1.5 employs linear projection layers to align image and text features, generating 576 visual tokens for detailed visual representation, while LLaVA-NEXT extends this approach by utilizing thousands of visual tokens. In contrast, InstructBLIP uses a Q-former with only 32 visual tokens to bridge the modalities. This diversity highlights the flexibility of our approach, validating its effectiveness across models with both high and low token counts, and confirming its robustness and adaptability.\nHallucination reduction baselines. In addition to the original LVLM outputs, we compare our method with beam search as well as two state-of-the-art decoding methods: VCD [27], which contrasts original and distorted visuals to reduce hallucinations, and OPERA [19], which applies penalties and token adjustments for better grounding."}, {"title": "7. Analysis and Ablation Studies", "content": "As described in \u00a75.2, we generate K candidate predictions by applying token dropout with different dropout masks M(k). In this section, we investigate how varying K from 1 to 4 affects generation quality.\nWe fix \u03b4(k) = 0.1 and adjust \u03b3(k) based on a predefined order: \u03b3(1) = 0.3, \u03b3(2) = 0.5, and \u03b3(3) = 0.7. However, setting \u03b4(k) to 0.9 leads to excessive dropout of visual tokens and degrades InstructBLIP's performance, so we set \u03b3(4) = 0.1. Moreover, our majority voting algorithm favors candidates with fewer dropped tokens in the event of a tie, meaning that when comparing only two candidates, both will yield identical outputs. To address this, we remove Candidate 1 in the second round, leaving only Candidate 2.\nOverall, we find that selecting three candidates strikes the optimal balance between increased certainty from additional votes and the controlled uncertainty introduced by candidate dropout probability, allowing DROPOUT DECODING to achieve more trustworthy and stable generation results."}, {"title": "7.2. Initial Identification of Relevant Visual Tokens", "content": "As discussed in \u00a75.2, DROPOUT DECODING may employ a preliminary forward pass to retain most relevant objects during generation, which helps reduce hallucinated objects while maintaining high-quality outputs."}, {"title": "8. Conclusion", "content": "We introduce DROPOUT DECODING, a novel uncertainty-guided context selective decoding approach aimed at enhancing the reliability of LVLMs. After quantifying the uncertainty in visual inputs, DROPOUT DECODING accordingly drops out visual tokens to regularize effect of information uncertainty, and employs an ensemble-based decoding approach to stabilize generation. Extensive experiments on benchmarks including CHAIR, THRONE, and MMBench validate the effectiveness, demonstrating consistent performance improvements over existing methods in both hallucination reduction and general multimodal capability tasks."}, {"title": "A. Details of Uncertainty Decomposition", "content": "A detailed derivation of Eq. (5):\n$U_{total} = H [q_{proj}] = -\\sum_{y \\in V} q_{proj} (y) log q_{proj} (y) = -\\sum_{y \\in V} (E_i [q_{proj}^v (y)]) log q_{proj} (y) = -\\sum_{y \\in V} E_i [q_{proj}^v log q_{proj} (y)] = -E_i \\sum_{y \\in V} q_{proj}^v (y) log q_{proj} (y) = E_i [CE (q_{proj}^v, q_{proj})] = E_i [H [q_{proj}^v] + D_{KL} (q_{proj}^v || q_{proj})] = E_i [U_{ale} (i) + U_{epi}(i)]$ (11)"}, {"title": "B. Implementation Details", "content": "The experimental setup of DROPOUT DECODING is shown in Table 3. We set the maximum new tokens to 512 to ensure the complete generation of models, therefore achieving more reliable results from CHAIR and THRONE. In MMBench, as all questions are single-choice questions, we set the maximum new tokens to 1 for a more precise evaluation. We set other parameters in generation to greedy for more stable and repeatable results.\nIn addition to general generation settings, DROPOUT DECODING includes hyperparameters specified in \u00a75.2. The details of these hyperparameter settings are provided below:\nTop-k in identifying relevant visual tokens. Before the decoding process, we first obtain qproj, which is then used in the decoding process for generating the relevant visual tokens. The higher the top-k is, the more visual tokens are expected to be kept during the decoding process. In LLaVA-1.5, we set k = 5, and in InstructBLIP, we set k = 10. The difference of k between LLaVA-1.5 and InstructBLIP derives from the informative level of each visual token, where in LLaVA-1.5, each visual token carries less information than in InstructBLIP, which only contains 32 visual tokens.\nNumber of mask K. K refers to the number of predictions that will join the majority vote progress. We set K = 3 in our experiment settings.\n$\\gamma^{(k)}$ and $\\delta^{(k)}$ in uncertainty-guided masking We set $\\delta^{(k)}$= 0.1, $\\gamma^{(k)}$ = 0.2 * k + 1; k = 1,2, ..., K; K = 3 in our experiment settings.\nMoreover, we provide the hyperparameter settings of our baselines. OPERA's hyperparameters can be referred to Table 4; VCD's hyperparameters can be referred to Table 5."}, {"title": "C. Further Discussion on Ablation Studies", "content": "To further validate our uncertainty guidance' effectiveness, we select random masking strategy as an additional baseline to compare with DROPOUT DECODING's uncertainty-guided masking. The experimental setup remains identical, except that tokens are masked randomly, that is, candidate k masks each vision token at y(k) instead of using uncertainty guidance. The generated results using the random masking strategy often suffer from issues, with models producing repeated tokens until reaching the maximum token limit. For instance, the model might repeatedly generate \u201cskiers\u201d hundreds of times (generation: \"The image shows shows a a snowy snowy slope with a skiers skiers skiers skiers ...\"); this occurred in approximately 20\u201325 out of 500 cases, an issue nearly never encountered with our method. This behavior likely stems from random masking disrupting essential context information within LVLMs. In contrast, our uncertainty-guided masking applies a lower masking rate to tokens that the LVLMs are less \u201csurprised\" by and a higher rate to tokens that elicit greater surprise. This allows the model to generate content in a rather \"expected\" manner, even though many informative vision tokens are masked. By preserving base context information, our approach effectively maintains the LVLMs' consistency and coherence."}]}