{"title": "A Dynamic Systems Approach to Modelling\nHuman-Machine Rhythm Interaction", "authors": ["Zhongju Yuan", "Wannes Van Ransbeeck", "Geraint Wiggins", "Dick Botteldooren"], "abstract": "In exploring the simulation of human rhythmic\nperception and synchronization capabilities, this study introduces\na computational model inspired by the physical and biological\nprocesses underlying rhythm processing. Utilizing a reservoir\ncomputing framework that simulates the function of cerebellum,\nthe model features a dual-neuron classification and incorporates\nparameters to modulate information transfer, reflecting biological\nneural network characteristics. Our findings demonstrate\nthe model's ability to accurately perceive and adapt to rhythmic\npatterns within the human perceptible range, exhibiting behavior\nclosely aligned with human rhythm interaction. By incorporating\nfone-tuning mechanisms and delay-feedback, the model enables\ncontinuous learning and precise rhythm prediction. The intro-\nduction of customized settings further enhances its capacity to\nstimulate diverse human rhythmic behaviors, underscoring the\npotential of this architecture in temporal cognitive task modeling\nand the study of rhythm synchronization and prediction in\nartificial and biological systems. Therefore, our model is capable\nof transparently modelling cognitive theories that elucidate the\ndynamic processes by which the brain generates rhythm-related\nbehavior.", "sections": [{"title": "I. INTRODUCTION", "content": "THERE are a lot of works are working on the mod-\nelling cognitive tasks [1-3], specifically on the rhythmic\nperception task [4]. Rhythm is a time series that exhibits\nperiodic patterns or cycles [5] over a certain time interval.\nThe sounds that humans use for communication are temporally\nstructured sequences of events, such as musical notes. Here,\nrhythm means the pattern of timing and stress in the amplitude\nenvelope of an acoustic sequence [6], which has a basic\nbeat, the tactus, in a specific frequency range. The meter\nis a perceived temporal structure that includes the tactus\nfrequency [7, 8].\nIn humans and embodied systems, the act of striking a\nrhythm has to begin prior to the real beats to account for delays\nin the system. Hence, for synchronization with others [9],\nprecise prediction is crucial [10, 11]. Humans are capable of\nrhythm and rhythm synchronization within a particular range\nof beat and meter frequencies, a characteristic that we want\nthe model architecture to simulate. In accordance with the\nprocess of biological rhythm processing in music, we designed\na reservoir containing a pool of resonators to fulfill a role\nsimilar to the cerebellum [12, 13]. Reservoir computing is\ncommonly used for time series prediction tasks [14]. In the\nwork by Rigotti et al. [15], the authors present a biological\nmodel of cognition where single-neuron activity in the human\nbrain is tuned to combinations of multiple task-related aspects,\nresembling the nature of the reservoir readout layer.\nWe propose a physical inspired reservoir to model a predic-\ntive coding task [1, 15]. In the proposed reservoir structure,\nneurons are classified into two types: primary and interme-\ndiate. The primary neurons correspond to pressure in the\ninspirational physical system, while the intermediate neurons\nmap to particle velocities. To modulate information transfer,\nwe introduce two types of weights, c and k. In the equivalent\nphysical system, they represent propagation speed and decay\nrate, respectively. This physical inspiration imposes certain\nconstraints on the reservoir weight matrix, yet it also enables\neasy adjustments to the overall reservoir speed and activation\nlevel. Furthermore, distinct regions within the reservoir can\nbe designated distinct roles (e.g., oscillation frequencies),\nallowing for targeted design of their dynamics.\nThe biological plausibility of the proposed architecture\ncould also be examined. Traveling waves and oscillations\nplay a crucial part in dynamically coordinating neural con-\nnectivity, allowing for the flexible organization of the tim-\ning and directionality of network interactions throughout the\ncortex, which is essential for supporting cognition and be-\nhavior [16]. The overall topology-preserving structure can\nalso be observed in the auditory cortex, where specific areas\nexhibit greater responsiveness to sound modulated at various\nfrequencies [17, 18]. At the level of individual biological\nneurons, the intermediate artificial neurons can be considered\nas (groups of) synapses with a temporal storage capacity.\nThis temporary information storage capability enables the\nintermediate neurons to model delays, a crucial aspect of\nbiological neural networks [19]. The response of the primary\nneurons is further modulated by a global parameter that may\nbe analogous to neurotransmitter control. This combination\nof structures enables the generation of slow rhythms without\ncompromising the biological constraints of individual biolog-\nical neurons. Additionally, the double-parameter architecture\ncan discriminate between low and high spontaneous activity\nneurons, which exhibit varying sensitivities [20].\nFor the basic task of rhythm perception, specifically the\npredictive coding task related to meter, this study conducted"}, {"title": "II. METHODS", "content": "The main objective of the current model is to predict the\noccurrence of rhythmic beats during interactions with human-\nlike (im)precision in a biologically plausible way.\nThe proposed model uses reservoir computing to capture\nthe temporal aspects of rhythms. Reservoirs typically exhibit\nmultiple damped resonances. Because slow temporal behavior\ncan be obtained based on interactions between neurons that are\nfast, this can be considered a biologically plausible backbone.\nOur innovation contains two steps. Firstly, we introduce a\ntopology-preserving reservoir structure (Section II-B). During\ntraining it is combined with classical output weight training.\nSecondly, the new structure allows tuning the reservoir during\nthe application phase by means of parameters with physically\npredictable impact that are biologically interpretable. Based on\nour specifically designed synchronization loss function and the\nDynamical Selection (DS) mechanism, adjustments are made\nto the reservoir's connection matrix to fine-tune predictions."}, {"title": "A. A Framework for motor-auditory interaction", "content": "A typical task for our model is illustrated in Fig 1: beat\na rhythm based on another aurally-presented rhythm with the\nsame underlying meter. To instruct the model what rhythm\nit is required to beat, a priming phase is foreseen. As this\nwill be done via visual stimulation in the human experiment\nwe refer to it as the visual input in Fig 1, yet as far as the\nmodel is concerned it consists of a second periodic signal\nenvelope. Computer models are inherently very fast and could\nsimply follow, but for the system to be human-like and even\nimplementable in a robot it needs to be predictive. Here we\nselected a 200 milliseconds forward prediction.\nTo create this predictive behavior, an oscillating system\nis needed. A reservoir of connected neurons is chosen as a\ncomplex resonating system containing thousands of degrees\nof freedom. To select the desired rhythm, the internal states\nof this system are combined using trainable output weights.\nTraining is done during the priming phase and could represent\nthe biological process of selecting the appropriate envelope\nfollowing component from the disentangling of the rhythm by\nthe auditory system.\nThe outcome of this training process is a generative model\ndesigned to simulate sequential human behavior in rhythmic\ncognitive tasks. Central to the generative model is a latent\ndynamical system characterized by state variables ht. The\nevolution of the latent state is governed by the following\ndynamics:\n$h_{t+1} = f_0(h_t, x_t, b_t, \\xi_t),$ (1)\nwhere xt represents the inputs (primer, auditory reference,\nfeedback) at time t, bt is the bias at time t, $\\xi_t$ is the noise\nterm at time t, and fo denotes the dynamics function. Here, fo\nis modelled as a highly expressive physical-inspired reservoir\nstructure (Section II-B ).\nThe representation of the input employs a smooth pulse\nto accentuate the occurrence or lack of a beat at any given\nmoment. The injection of zero-mean Gaussian noise $\\xi_t$ at\neach time step is essential. From a physical perspective, the\nnoise will trigger the oscillating system to exhibit \"natural\"\nmetronomes. Additive noise is biologically plausible due to\nspontaneous emission of the (auditory) neurons.\nModel outputs, denoted by yt, are produced through a\nlinear combination acting as a representation of mixed selec-\ntivity [15], derived from the activity of neurons within the\nreservoir. The motor behavior is not explicitly modelled yet\nit is assumed that this introduces a delay of At. Therefore,\nthe model is trained to predict upcoming beats At ahead of\ntheir occurrence. For training the output layer weights, we\nemploy the Mean Squared Error (MSE) metric to ascertain the\nproximity of the model's predictions to the target behavior."}, {"title": "B. Reservoir weights", "content": "The proposed model is based on a reservoir computing (RC)\nmethod: the Echo State Network (ESN) [23, 24], a specific\nrecurrent neural network [25-27]. Its hidden states change\naccording to the current input and the hidden states from the\nprevious time step, which follows the equations:\n$h_{t+1} = (1 - \\alpha)h_t + \\alpha f(W_{in}x_t + Wh_t + \\xi_t),$ (2)\n$\\hat{y}_t = W_{out}h_t,$\nwhere W is a sparse matrix defining the connectivity of the\nnetwork, Win is the input weight, and Wout is the output\nweight matrix, and \u03b1 is the leakage rate of the model. f(\u00b7) is\na non-linear function, for which tanh(\u00b7) is used in this paper.\nxt is the input signal at time step t, and ht is the hidden state\nat time step t, and yt is the output of the model at time step t,\nwhich is the prediction. The expected prediction should satisfy\n$\\hat{y}_t = x_{t+n}$ where n is the number of time steps predicted\nahead: $\\Delta t = n\\delta t$ with $\\delta t$ the simulation time step.\nConventionally, the weight matrix of the reservoir W, the\ninput weight matrix Win, and the bias matrix bt are randomly\ngenerated at each time step. In this paper, Wbias includes the\nGaussion noise $\\xi_t$, regenerated every time step. The weight\nmatrix W is usually adapted to keep all of its eigenvalues\ninside the unit circle [28] in the z-domain, thereby assuring\nstability of this dynamic system in the linear, low amplitude,\nregime.\nFor the problem at hand, predicting the occurrence of\nrhythmic beats in a human-like way, the poles in the z-\ndomain of the W matrix and thus the resonances in the\nrandom reservoir are not optimally placed: (1) they span\na frequency range that does not match human capabilities;\n(2) many of them are too much damped. To overcome this\nproblem, we propose a novel reservoir structure designed\nfollowing physical principles. To simplify the tuning of W,\nwe design it based on a 2D Finite-Difference Time-Domain\n(2D-FDTD) computational approximation of the linearized\nEuler equations [29] for wave propagation in a medium with\nrandomly generated properties. Because this system results in\nlocal connections, it has a clear topology which allows crafting\nconnections from input or outputs to areas showing specific\ndynamics. Starting from the wave equations Eq 3 where c is\nthe wave speed and k is a damping (amplification if negative)\nfactor, and p and o are proportional to pressure and velocity.\n$\\frac{\\partial p}{\\partial t} + c^2\\nabla o = 0$ (3)\n$\\frac{\\partial o}{\\partial t} - ko + p = 0,$\nThe simplest FDTD model, a staggered grid, central differ-\nences, and an explicit time stepping approximation of these\nequations leads to their discretised form (Eq 4):\n$p_{i,j}(t + \\delta t) = p_{i,j}(t) + c^2\\delta t * (o_{x,i+1,j} - o_{x,i,j})/\\delta x + c^2\\delta t * (o_{y,i,j+1} - o_{y,i,j})/\\delta y$\n$o_{x,i,j}(t + \\delta t/2) = \\frac{1 - k_{i,j}\\delta t/2}{1 + k_{i,j}\\delta t/2} o_{x,i,j}(t - \\delta t/2) + \\frac{\\delta t}{\\delta x(1 + k_{i,j}\\delta t/2)}(p_{i,j} - p_{i-1,j}),$ (4)\nwhere the indices, i and j refer to spatial locations and the\ntime dependence has been omitted on the right hand side of\nthe equation. A similar equation holds for oy. Stability is\nguaranteed by keeping the Courant number, which relates the\ndt to dx and dy, smaller than 1.\nThe two groups of unknowns could be interpreted as two\ntypes of artificial neurons in a reservoir as in Fig 1: one\nis the primary neuron denoted as pi,j, and the other is the\nintermediate neuron, labeled as ox,i,j or oy,i,j. These can be\ngrouped in a hidden state matrix x like in Eq 2. As p, o are\ncoupled locally and sparsely the coupling matrix A derived\nfrom Eq 4 will also be sparse.\nThe weight matrix W of the reservoir is computed by:\nW = (A \u2013 (1 \u2212 \u03b1) \u00b7 I)/\u03b1, (5)\nwhere I is the identity matrix. In this way the update equations\nof the reservoir Eq 1 become very similar to the FDTD\nupdate equations. It implies very strong symmetry constraints\non the W matrix. The local value of c determines how\nstrongly the p-neuron responds to inputs from surrounding o-\nneurons and together with the coupling to its neighbors this\ncan result in local resonances, where the physics equivalent\nlearns that small c correspond to low-frequency resonances."}, {"title": "C. Tuning for synchronization", "content": "In this study, we employ stochastic gradient descent (SGD)\nto minimize the Mean Squared Error (MSE) between the\nprediction \u0177 and target y signals during training that is based\non a large number of rhythmic beats that could theoretically\nbe encountered in music. Following training, the output layer\ncan thus identify a suitable combination of correct oscillators,\nthereby providing an initial estimate of the target beat period-\nicity and timing.\nTo synchronize the prediction with the target beats more\naccurately, an adaptation phase is introduced. A prediction of\nan upcoming beat can fail in to ways: 'too early' or 'too late',\nhence the error is split in two parts. In both cases, there is\nusually an overlap between the sound envelopes corresponding\nto a beat in the prediction and target. If the data consists of\ndiscrete moments in time, the peak is artificially extended.\nThus, the slope of the peaks is employed to calculate the error\nIearly and Ilate of the prediction \u0177 and target y signals.\nIf the prediction is descending while the target is ascending,\nwe consider the prediction to be too early. Otherwise, if the\nprediction is ascending while the target is descending, the\nprediction is too late. Both values, Iearly and Ilate until an\nupdate_step is reached and the reservoir weights are changed,\nand reinitialize to 0 when the interval ends, as shown in\nAlgorithm 1. To ensure proximity in amplitude between the\ntarget and prediction within the same time window, a moving\naverage and a softmax normalisation are first applied to both\nthe target and prediction values:\n$y_{norm}(t) = \\frac{y_t - y_{mean}}{y_{softmax}(t)},$ (6)\n$\\hat{y}_{norm}(t) = \\frac{\\hat{y}_t - \\hat{y}_{mean}}{\\hat{y}_{softmax}(t)},$ (7)\nwhere\n$y_{softmax}(t) = \\ln(\\int e^{yt} dt'),$ (8)\n$\\hat{y}_{softmax}(t) = \\ln(\\int e^{\\hat{y}t} dt'),$ (9)\nwhere is an exponential averaging time constant spanning\nmultiple interbeat intervals.\nBy weighted comparison of Iearly and Ilate, the decision\nis made to increase or decrease the speed-up factor de by a\nfixed amount as shown in Algorithm 1. If the prediction is\ntoo early, we thus decrease all elements of c proportionally to\ntheir value; if it is too late, we increase c it. In this way, the\nwhole reservoir slows down or speeds up.\nSecondly, a proposed method: Dynamical Selection (DS)\nmechanism, is used to control the damping of the oscillations\nin the reservoir. The poles of the W matrix are modified by"}, {"title": "D. Fine-tuning and Continuation", "content": "Having completed the model training, a first approximation\nto the output Wout is obtained. However, predictions for task\nillustrated in Fig 1 involving two inputs, lack accuracy. In this\nstudy, distinct combinations of beat multiples under the same"}, {"title": "E. Customization during human behavior simulation", "content": "When two humans interact musically, they can adopt dif-\nferent behaviors. In broad terms they can act as a follower or\nas a leader in the interaction. When comparing the proposed\nmodel with interaction experiments (Fig 3(b)), this is included\nby customizing the update learning rate. To this end, we utilize\nthe Wasserstein distance W(\u00b7,\u00b7) between the inter-beat interval\ngenerated by Participant 1 (P1) and the primer interval, and\nbetween the inter-beat interval generated by P1 and participant\n2 (P2).\nThe Wasserstein distance [31] measures the amount of\n'work' required to move one set of timings to another set,\neffectively identifying the inter-beat interval distribution sim-\nilarity. The equation is as follows:\n$\\Upsilon_{EF}(P_{pred}, P_{target}) = min_{y\\in \\digamma(P_{pred}, P_{target})} \\int c(d_{pred}, d_{target}) dy,$ (12)\nwhere Ppred and Ptarget are the inter-beat interval distribution\nof human prediction and visual reference respectively, dtarget\nrepresents the set of inter-beat intervals from the participant,\ndpred denotes the set of inter-beat intervals from the refer-\nence or another subject, miny\u2208F(Ppred,Ptarget) signifies the\nminimum over all joint distributions y with marginals Ppred,\nPtarget, c(dpred, dtarget) denotes the time difference between\npoint dpred and point dtarget, and dy(dpred, dtarget) represents\nthe joint distribution.\nThe update learning rates are defined as:\n$\\beta * \\frac{1}{W(P_{pred}, P_{target})} * exp(-\\frac{1}{T}),$ (13)\nwhere \u03b2 = 0.001 is the learning rate, ds = 6ms is the\ndownsample factor, and l is the sequence length."}, {"title": "III. RESULTS", "content": "A. The proposed model shows human-like rhythm analysis\nIn human perceptive beat, lower-level dynamics, such as the\ngrouping of two or three beats, are processed by sensory tis-\nsues such as the cerebellum, which establishes the fundamental\nrhythm in interaction with the environment. Meanwhile, higher\ncognitive processes are responsible for selecting and refining\ncomplex tasks [12]. This allows humans to generate rhythms\nwithin a certain frequency range, and upon mastery, anticipate\nbeats ahead of the actual timing. Slower rhythms will be\nsubdivided into faster meters.\nTo assess the model's frequency perception range, the model\nis pre-trained on large dataset of artificially created plausible\nrhythms. Then, test are conducted using rhythms spanning\ninter-beat intervals from 400 to 3000 milliseconds with an\nintervals of 50 milliseconds. The time-step it was set to 6\nmilliseconds. The model was tested without fine-tuning and\nwith and without tuning c and k for synchronisation.\nAs depicted in Fig 2, when the inter-beat interval is less than\n2000 milliseconds, the model demonstrates accurate rhythm\nprediction, with minimal variance observed between predicted\nbeats. However, as shown in Fig 2(a) left side, when the\nintervals increase, the model's predictions begin to incorporate\nsubdivisions, notably aligning well with intervals of one half\nand one third of the target intervals. Such behavior would also\nbe expected in humans as this inter-beat interval falls well\nbeyond human capabilities for rhythmic prediction.\nFollowing the tuning of c and k for synchronisation (Fig 2(a)\nright side) the model becomes more precise but also generates\nmore subdivisions. A common observation is that humans also\ntend to perform internal counts between consecutive beats.\nAt six specific inter-beat intervals, the time offset ratio\nbetween the model's outputs before and after adjustments\nis investigated in more detail."}, {"title": "IV. CONCLUSION", "content": "In this study, we developed a biologically inspired dynamic\nsystem to simulate human rhythmic cognition by integrating\npredictive coding, aligning it with human behavior. Unlike tra-\nditional data-intensive machine learning models, our approach\nemploys a recurrent neural network (RNN) trained on basic\nrhythmic patterns, demonstrating the capability to learn new\nrhythmic combinations and interact similarly to humans.\nOur model, designed with a weight matrix inspired by\nwave equation discretization, can emulate human-like rhyth-\nmic behaviors. To improve learning capacity, we introduced\nfone tuning on the output weights, enhancing the selection\nof neuron activity and enabling rapid adaptation to new\nrhythmic combinations. Our model maintains learned rhythms\nby selectively outputting and delaying feedback as input.\nInteraction tests with and without customization revealed\nthat our model generalizes well to most basic human rhythmic\nbehaviors. Broadly, our framework can extend to other time-\nvarying signals, offering a potential method for generating\nmodels that simulate the brain's dynamic functions and enable\nthe emergence of rhythmic behavior."}]}