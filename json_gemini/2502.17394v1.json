{"title": "FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event Detection", "authors": ["Tanmay Parekh", "Yuxuan Dong", "Lucas Bandarkar", "Artin Kim", "I-Hung Hsu", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Event Detection (ED) is the task of identifying typed event mentions of interest from natural language text, which benefits domain-specific reasoning in biomedical, legal, and epidemiological domains. However, procuring supervised data for thousands of events for various domains is a laborious and expensive task. To this end, existing works have explored synthetic data generation via forward (generating labels for unlabeled sentences) and inverse (generating sentences from generated labels) generations. However, forward generation often produces noisy labels, while inverse generation struggles with domain drift and incomplete event annotations. To address these challenges, we introduce FIG, a hybrid approach that leverages inverse generation for high-quality data synthesis while anchoring it to domain-specific cues extracted via forward generation on unlabeled target data. FIG further enhances its synthetic data by adding missing annotations through forward generation-based refinement. Experimentation on three ED datasets from diverse domains reveals that FIG outperforms the best baseline, achieving average gains of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings, respectively. Analyzing the generated trigger hit rate and human evaluation substantiates FIG's superior domain alignment and data quality compared to existing baselines. We will release our code at https://github.com/PlusLabNLP/FIG.", "sections": [{"title": "1 Introduction", "content": "Event Detection (ED) involves identifying and categorizing significant events from natural language text based on a pre-defined ontology. It has applications in widespread domains like biomedicine, epidemiology, law, etc. However, procuring annotations for each domain-specific ontology to train models is expensive and impractical."}, {"title": "2 Problem Definition", "content": "We focus on the task of Event Detection (ED) for this work. The task of ED aims to extract mentions of any events of interest from natural language text. Following ACE 2005, we define an event as something that happens or describes a change of state and is labeled by a specific event type. The word/phrase that most distinctly highlights the occurrence of the event is defined as the event trigger, and the trigger-event type pair is referred to as the event mention. Event Detection, in particular, requires extracting the event triggers from the sentence and classifying it into one of the pre-defined event types."}, {"title": "3 Methodology", "content": "In this work, we focus on Large Language Model (LLM)-powered synthetic data generation to alleviate the need for domain-specific annotated training data, as zero-shot LLM-based approaches usually struggle in ED in specialized domains. By generating a large amount of data instances Ds = {(X, Y)}, we can train downstream supervised ED models. Here, we first briefly review prior works on forward generation and backward generation. Next, we introduce our proposed data generation method FIG."}, {"title": "3.1 Background", "content": "We focus on two major paradigms for data generation in our work: forward generation and inverse generation. We describe them briefly below and provide illustrations in Figure 2.\nForward Generation: This is a more straightforward manner of data generation, wherein LLMs are utilized to generate labels Y' on unlabeled data X \u2208 D' (i.e. X \u2192 Y'). This generation is analogous to weak/distant supervision (Weak Sup) where noisy/weak labels are assigned"}, {"title": "3.2 FIG", "content": "LLM reasoning has been poor for ED, which makes forward generation vulnerable to noisy label quality. Since LLMs are pre-trained to generate natural sentences, their generation capabilities are stronger, which supports inverse generation. However, owing to the lack of any domain-specific information, inverse generation can synthesize highly diverse sentences, leading to a domain drift. Merely specifying the domain in the prompt is not sufficient. Furthermore, inverse generation sentences can mention additional events which remain unannotated introducing noise in the data.\nTo this end, we propose Forward-Inverse Generation (FIG), a hybrid approach that infuses forward generation reasoning into the base pipeline of inverse generation to ensure closer alignment to the target domain while improving the quality of the annotated labels. To procure domain-specific cues, we assume access to an unlabeled target domain data D' (similar to forward generation). We provide our architectural diagram in Figure 2 and explain each component of our pipeline below.\nTarget Structure Extraction: Similar to inverse generation, we first create potential triggers (i.e. labels) for each event type. However, generation from scratch can lead to very divergent set of triggers. For example, an \u201cAttack\" event can refer to a war in the news, cyberattacks in the cybersecurity domain, diseases in epidemiology, or criticism in economics. Naturally, each scenario would assume different triggers, which inverse generation struggles to generate even when provided with clear event definitions instead, we generate potential triggers per event type by utilizing an LLM to extract triggers using a forward generation over the unlabeled data D'T. To ensure highly precise extraction of triggers, we develop a two-stage prompt setup. The first stage is tasked to identify and filter possible event types mentioned in the text based on the task instructions and event definitions. The second stage aims to find the most appropriate trigger word from the unlabeled sentence for each filtered event type. After extraction, we aggregate and sort the triggers for each event type at the corpus level and filter out the top t triggers for each event type as the set of clean triggers. This filtering helps remove many noisy triggers introduced in the forward generation. Finally, we create target structures Y by sampling 1-2 event types per instance and sampling triggers"}, {"title": "4 Experimental Setup", "content": "Datasets: We consider three ED datasets from diverse domains for our experiments: (1) ACE in the news domain, (2) SPEED in the social media domain, and (3) GENIA in the biomedical domain. We simplify GENIA by converting the original document level annotations to sentence-level annotations. For the few-shot setting, we compile k data instances from the training data as the few-shot examplers.\nFor our unlabeled data, we consider two sources: (1) Train - annotation-free training splits (i.e. only the text) of each dataset and (2) External unlabeled data from other external sources. For ACE, we utilize News Category Dataset comprising Huffpost news articles from 2012-2022 as the external data source. We filter articles corresponding to political, financial, and business articles. For SPEED, we utilize COVIDKB mining tweets from the Twitter COVID-19 Endpoint released in April 2020 as the external data source. Finally, we utilize GENIA2013 dataset as the external data for GENIA. We provide statistics about these datasets in Table 8.\nBaseline methods: We consider three LLM-based techniques for low-resource ED as the baselines for our work. (1) Inference: LLMs are used to directly infer on the target test data using their reasoning capability. (2) STAR: This model is the state-of-the-art inverse generation model for ED. It utilizes trigger generation, passage generation, and data refinement steps without using any unlabeled data, (3) Weak Supervision (Weak Sup): This model acts as the forward generation baseline. We utilize the Inference model to provide labels for the unlabeled data. For an upper bound reference, we also include a human generation baseline (Human) wherein we sample N data instances from the gold training data of each dataset to train the downstream ED model.\nBase models: For our base LLMs, we consider three instruction-tuned LLMs of varying sizes, namely Llama3-8B-Instruct (8B model), Llama3-"}, {"title": "5 Results", "content": "We present the results and insights for our zero-shot and few-shot settings below."}, {"title": "5.1 Zero-shot Results", "content": "We present the main results comparing the various methods across different LLMs and datasets in Table 1. For FIG, we show results using both the train data as well as the external data as the external data source. We highlight some of our main findings below.\nForward generation > Inverse generation: Although STAR performs decently for ACE, its performances for SPEED and GENIA are quite poor (even below the Inference baseline). Since SPEED (social media) and GENIA (biomedical) are more domain-specific than ACE (news), we attribute this poor performance to STAR's inability to generate domain-specific data instances (further validated in \u00a7 6.3). On the other hand, Weak Sup outperforms STAR, providing average gains of 13% Tri-C F1, thus establishing the superiority of forward generation-based synthetic data.\nFIG performs the best: Our hybrid approach combines the benefits from both forward and inverse generation and provides the best performance. On average, FIG beats STAR by 17.3% Eve-I F1 and 16.3% Tri-C F1 points - proving how domain-"}, {"title": "5.2 Few-shot Results", "content": "We also study the various methods in the presence of small annotated data as part of our few-shot experiments. Specifically, we study the k = 2 and k = 5 few-shot settings, where k annotated examples per event type are utilized. Majorly, we utilize the k shots as in-context examples in the LLM prompts where applicable and append these few-shot examples to the synthesized training data as well. Additionally, we consider another baseline (Supervised) of downstream models trained only on the k shot examples. We present the Tri-C results for all the datasets for the Llama3-8B model in Figure 4 and summarize our major findings below.\nFIG consistently performs the best: Similar to zero-shot results, we observe that FIG consistently beats all other baseline models. On an average, FIG outperforms STAR and Weak Sup by 5.4% Tri-C F1 and 7% Tri-C F1 respectively.\nGeneration abilities improve significantly: Contrary to our findings of small improvements when scaling up LLM model size in \u00a7 5, we observe that inverse generation-centric methods (STAR, FIG) consistently improve as we increase the number of shots. Contrastingly, reasoning-centric models stagnate and don't improve as much. This highlights how inverse generation becomes particularly effective in the presence of few exemplars."}, {"title": "6 Analysis", "content": "In this section, we provide various analyses to study FIG's superior performance. Unless specified, we utilize Llama3-8B-Instruct as the base LLM for the analyses."}, {"title": "6.1 Ablation study", "content": "Table 2 shows the ablation study for each of our forward generation components. We observe how forward generation is critical in both trigger generation and data refinement stages with average performance reductions of 3.8% F1 and 6% F1 on removing the respective components from FIG."}, {"title": "6.2 Comparison with Data mixing", "content": "Data-mixing is a widely used technique for leveraging complementary information across datasets to promote robust downstream model training. We mix forward (Weak Sup) and inverse generation (STAR) as a hybrid baseline to compare with FIG. To keep comparisons fair, we consider N/2 data instances per event type from each dataset. Results from Table 3 demonstrate how FIG beats the data-mixing based hybrid model by 5-6% F1, highlighting the need for explicit model design to combine the benefits of forward and inverse generation."}, {"title": "6.3 Analyzing trigger quality and drift", "content": "ED models have a strong tendency to over-rely on lexical relations between triggers and events. Thus, we compare the synthetic data triggers with the gold test triggers as a raw study on the quality and drift of triggers in the synthesized data. Specifically, we extract triggers per event type in both datasets and measure the hit rate of the synthesized triggers on the gold set, as reported in Table 4. As observed, the poor hit rate shows the poor overlap of STAR's triggers with the gold triggers, which is a primary reason for its domain drift. Furthermore, the consistently stronger precision of FIG relative to Weak Sup demonstrate FIG's better trigger annotation quality.\nTo further study the label quality and relevance, we conduct a human evaluation. Specifically, a human expert ED annotator is tasked to score the generations (between 1-5) on the naturalness of the sentence for the domain, the correct relevance of the event mentioned in the generated sentence, and the annotation quality. We provide the averaged scores across the three datasets for 90 samples in Table 5. Weak Sup has high sentence quality but poor label annotations; while STAR suffers from poor event relevance indicating domain drift. Overall, FIG performs the best with high annotation quality and event relevance."}, {"title": "6.4 Domain-adapted LLM Fine-tuning", "content": "We fine-tune the base LLM used for passage generation on the unlabeled target-domain data D' to better align the generated passages to the target domain. Naturally, this can be applied only for smaller LLMs owing to fine-tuning costs. We present the results of fine-tuning Llama3-8B-Instruct on the unlabeled train data in Table 7. On average, we observe how target data fine-tuning improves FIG by a slight 0.5-2% F1, suggesting that target-domain passage generation may help, but it is not a highly influencing factor to improve downstream model performance."}, {"title": "6.5 Qualitative analysis of generated data", "content": "We provide qualitative evidence for FIG's domain-aligned triggers compared to STAR in Table 6 (more examples in Table 16). Owing to lack of domain grounding in STAR, the resulting triggers often appear misaligned (e.g. asphyxiation for death"}, {"title": "7 Related Works", "content": "Low-resource Event Detection Event Detection (ED) has been studied extensively, leading to diverse datasets in news, Wikipedia, and general domains, as well as niche areas like biomedical, multimedia, cybersecurity, epidemiology, and pharmacovigilance. To address the growing need for event detection across expanding domains, better low-resource domain-specific techniques are essential. Prior works have explored transfer learning via Abstract Meaning Representation, Semantic Role Labeling, and Question Answering. Reformulating ED as a conditional generation task has also aided low-"}, {"title": "8 Conclusion and Future Work", "content": "We introduce FIG, a hybrid forward-inverse generation approach for better domain-specific synthetic data in low-resource ED. Experiments on three diverse datasets reveal that forward generation suffers from noisy labels due to poor LLM reasoning, while inverse generation faces domain drift. FIG mitigates these issues, achieving superior domain alignment and cleaner data, leading to the best downstream ED performance in zero and few-shot settings. Future works can explore enhancing stronger domain alignment and extending FIG for multilingual generation."}, {"title": "Limitations", "content": "We consider only Event Detection (ED) as the main task for data generation, but our method can be extended to other structured prediction tasks as well. We leave this exploration for future works. We consider three specialized domains of news, social media, and biomedical to provide a proof-of-concept of our work. There are other specialized domains for ED as well which can be explored as part of future work. Finally, our proposed method FIG makes a practical assumption of access to unlabeled data to procure target domain cues to guide the data generation. However, for specific super-specialized domains or if data has privacy concerns, this may not be possible and our method may not be applicable here. We assume such cases to be super rare and beyond the scope of our work."}, {"title": "Ethical Considerations", "content": "The theme of our work is to generate high-quality domain-specific data using Large Language Models (LLMs) using forward-inverse generations. The inherent LLMs can have certain biases, which can lead to potentially harmful or biased generations. Furthermore, the LLM can introduce potential hallucinations in the annotations which can hurt the model performance. We do not check or consider any bias/hallucination detection method as part of our work, as it is beyond the scope. So future users should take due consideration of this vulnerability.\nOur proposed method FIG utilizes unlabeled data as a basis to procure domain-specific cues. If there are any biases in this data, it can propogate to the downstream model as well. We provide a proof-of-concept about our method in this work but do not detect or rectify such biases.\nSince the inverse generation paradigm causes the LLM to generate sentences/passages, which can potentially be copied from the pre-training data the LLM has been trained on. This can potentially lead to copyright infringements and we do not consider any of such violations under consideration for our method. Users should consider this vulnerability before usage in commercial applications.\nWe would also like to mention and acknowledge that we have utilized AI chatbots to help with the writing of the work."}, {"title": "A Additional details about FIG", "content": "We provide illustrations about the various prompts used in FIG. For target structure extraction forward generation, we consider a two-prompt approach. The first prompt aims to identify if any events of interest are mentioned in the text as illustrated in Figure 6. It comprises the task definition, full event ontology with definitions, the task instructions, and the query sentence. The second prompt identifies the trigger corresponding to the event of interest, as illustrated in Figure 7. Here, we specify the task definition, the event ontology details, and the query text with the task instructions. To aid inverse generation for passage generation, we provide the task definition, the event ontology with event definitions, and the query comprising the sampled target structure. We illustrate this prompt in Figure 8. Finally, we provide the simplified one-prompt setup for forward generation utilized for data refinement in Figure 9."}, {"title": "B Additional Experimental Setup Details", "content": "We discuss details about our dataset in \u00a7 4. Our test target domain data includes the test data splits of (1) ACE in the news domain, (2) SPEED in the social media domain, and (3) GENIA in the biomedical domain For unlabeled data, we utilize the training data of each dataset as one data source. For the other data source, we utilize data from external sources, specifically: (1) News Category Dataset comprising Huffpost news articles from 2012-2022 for ACE. We filter articles corresponding to political, financial, and business articles, (2) COVIDKB mining tweets from the Twitter COVID-19 Endpoint released in April 2020 as the external data source, (3) GENIA2013 dataset as the external data for GENIA. We provide statistics about this data in Table 8."}, {"title": "C Implementation Details", "content": "Here, we provide detailed implementation details for each component and models used in our work. We run most of our experiments on NVIDIA RTX A6000/A100 machines with support for 8 GPUs, while for GPT3.5, we make API calls through OpenAI."}, {"title": "C.1 LLM-based Generation", "content": "We provide details about the various hyperparameters for using LLMs in all the components of STAR and FIG. For Llama3-8B-Instruct and Llama3-70B-Instruct, we present the hyperparameters in Table 9; while Table 10 presents the hyperparameters for GPT3.5."}, {"title": "C.2 Few-shot Implementation Details", "content": "For the few-shot setting, we can access additional k datapoints per event type to aid better performance. For LLM-based prompting, we simply add these examples in the prompt as in-context examples to help the model do better reasoning/generation. For inverse generation methods (STAR, FIG), we do not add the k triggers to the extracted/generated trigger list, as it leads to a drop in model performance. This can be attributed to the presence of duplicate information as the trigger generation/extraction al-"}, {"title": "C.3 Downstream Model Training", "content": "We choose DEGREE as our downstream model for evaluation, a generation-based prompting model that utilizes natural language templates. We implemented the DEGREE model under TextEE framework Table 11 presents the primary hyperparameters for this model."}, {"title": "C.4 LLM Fine-tuning", "content": "We discuss domain-adapted passage generation through LLM fine-tuning in \u00a7 6.4. Specifically, we conduct a low-rank finetuning (LoRA) to reduce computational overhead to fine-tune Llama3-8B-Instruct. We implement LoRA using the peft and trl packages. We choose the task of causal language modeling (i.e. continual pre-training) to perform domain adaptation on unlabeled in-domain sentences. We utilize cross-entropy loss on the dev split of the unlabeled data to select the best model. We provide additional details about the hyperparameters for this fine-tuning for each dataset in Table 12 below."}, {"title": "D Additional analyses", "content": "In this section, we provide additional analyses to support our main experiments."}, {"title": "D.1 STAR with domain-specific prompt", "content": "A simple way to infuse domain specific information in inverse generation pipelines like STAR would be to add domain-related information in the prompts to the LLM. We experiment with two"}, {"title": "D.2 Impact of different number of training samples", "content": "We conduct a small analysis to study the impact of changing the number of generated samples on the downstream model performance for FIG. We present the results for Llama3-8B-Instruct in Figure 5. As observed, the performance continues to increase as we increase the data from N = 10 to N = 100 datapoints per event type. This promises"}, {"title": "D.3 Human Evaluation Details", "content": "We conduct a small human evaluation to judge the quality of the synthetic data in \u00a7 6.3. Here, we provide additional details about the human study and evaluation. Since the evaluation is conducted on three diverse and niche domains, we only utilize a single human annotator who is an ED expert and has previously worked on all three datasets as the primary annotator.\nWe majorly evaluate on three dimensions: (1) Sentence naturalness (SN): this metric judges"}, {"title": "D.4 Additional Qualitative Examples", "content": "In \u00a7 6.5, we discussed how FIG improves domain drift qualitatively relative to STARand provided some examples. Here, we provide more examples to further support that study in Table 16. This table further demonstrates how STAR can have a domain drift without a lack of domain-specific cues, while FIG is better here."}]}