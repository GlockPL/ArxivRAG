{"title": "Breaking the Manual Annotation Bottleneck: Creating a Comprehensive Legal Case Criticality Dataset through Semi-Automated Labeling", "authors": ["Ronja Stern", "Ken Kawamura", "Matthias St\u00fcrmer", "Ilias Chalkidis", "Joel Niklaus"], "abstract": "Predicting case criticality helps legal professionals in the court system manage large volumes of case law. This paper introduces the Criticality Prediction dataset, a new resource for evaluating the potential influence of Swiss Federal Supreme Court decisions on future jurisprudence. Unlike existing approaches that rely on resource-intensive manual annotations, we semi-automatically derive labels leading to a much larger dataset than otherwise possible. Our dataset features a two-tier labeling system: (1) the LD-Label, which identifies cases published as Leading Decisions (LD), and (2) the Citation-Label, which ranks cases by their citation frequency and recency. This allows for a more nuanced evaluation of case importance. We evaluate several multilingual models, including fine-tuned variants and large language models, and find that fine-tuned models consistently outperform zero-shot baselines, demonstrating the need for task-specific adaptation. Our contributions include the introduction of this task and the release of a multilingual dataset to the research community.", "sections": [{"title": "1 Introduction", "content": "Predicting the impact of legal cases is a critical task in the legal domain, as it aids professionals in the judicial system in navigating large volumes of case law. Despite its significance, the task of predicting the case criticality remains relatively under-explored. Existing approaches to evaluating the importance of legal cases are primarily manual, very resource-intensive and subject to the judgments of individual annotators. This paper introduces a novel dataset, licensed under CC BY 4.0, and a more challenging evaluation framework\u2014Criticality Prediction\u2014designed to predict the potential influence of Swiss Federal Court cases on future jurisprudence.\nWhile prior work such as the Importance Prediction task proposed by Chalkidis et al. (2019) for European Court of Human Rights (ECtHR) cases focused on predicting importance on a defined scale using human-assigned labels, our approach employs algorithmically derived labels to evaluate case criticality. Our dataset introduces a two-tier labeling system: the LD-Label, a binary indicator of whether a case is published as a Leading Decision (LD), and the Citation-Label, a more nuanced categorization based on the frequency and recency-weighted citation counts of these decisions across subsequent cases. This automated and distinct formulation of \"criticality\" not only distinguishes critical cases from non-critical ones but also ranks them by their relative importance over time. The complexity of this dataset challenges even recent large language models (LLMs) such as GPT-3.5 (Brown et al., 2020).\nOur contributions are threefold: (1) We propose a novel Criticality Prediction task that provides a more comprehensive and challenging evaluation of case law importance. (2) We release the datasets to the community, providing valuable resources for further research in legal NLP. (3) We evaluate several multilingual models of various sizes, including fine-tuned variants to set baselines."}, {"title": "2 Related Work", "content": "One of the most common text classification tasks in the legal domain is Legal Judgment Prediction (LJP), which involves predicting the outcome based on its facts. Researchers have leveraged diverse datasets with unique characteristics and annotations to analyze and predict case outcomes across various languages, jurisdictions, and input types (Feng et al., 2022; Aletras et al., 2016; \u015eulea et al., 2017; Medvedeva et al., 2018; Chalkidis et al., 2019; Niklaus et al., 2021, 2022; Semo et al., 2022).\nWhile LJP is focused on the outcome of individual cases, Importance Prediction shifts the focus toward assessing the broader significance of a case.\nChalkidis et al. (2019) introduced the Importance Prediction task using cases from the European Court of Human Rights (ECtHR). In this task, ECtHR provided scores that denote each case's 'importance' to the common law. These scores, ranging from 1 (key case) to 4 (unimportant), were designed to help legal practitioners identify cases that play a crucial role in shaping jurisprudence. The labels reflect the long-term impact of a case on future rulings and the evolution of legal precedent. While the task is invaluable for identifying landmark cases, it relies on legal experts to assign the labels, making the process resource-intensive and potentially subject to subjective interpretations. Additionally, the ECtHR dataset is monolingual in English, whereas our Swiss dataset is multilingual. To our knowledge, no other study addresses a similar task to Criticality Prediction."}, {"title": "3 Task and Dataset", "content": ""}, {"title": "3.1 Criticality", "content": "Understanding the legal framework of the Swiss Federal Supreme Court (SFSC) is key to defining criticality in Swiss case law. The SFSC shapes the legal landscape through its rulings, with a subset known as Leading Decisions (LDs) published separately due to their influence on future decisions. We quantify case criticality using two labels: the binary LD-Label and the more granular Citation-Label.\nThe LD-Label is binary, categorizing cases as critical or non-critical. SFSC cases are labeled critical if also published as LD, reflecting their recognized importance within the Swiss legal system. We used regular expressions to extract SFSC cases from LD headers if present. SFSC cases not published as LD are labeled non-critical.\nWe developed the Citation-Label to provide a more granular measure. It counts how often each LD case is cited in SFSC cases, with older cases weighted less to prioritize recency. The score is calculated as: $score = count \\times \\frac{year - 2002 + 1}{2023 - 2002 + 1}$. The $count$ is the citation frequency, and the weighting reduces older cases' influence. More details on the constants are in Appendix A. This score ranks LD cases, and we categorized them into four criticality levels\u2014critical-1 (least critical) to critical-4-based on the 25th, 50th, and 75th percentiles.\nUnlike prior approaches such as Chalkidis et al. (2019), we explicitly incorporate temporal weighting to account for both the influence of a case and how its criticality shifts over time. Furthermore, our framework allows for the dynamic recalculation of scores and re-labeling of criticality as case law evolves. This adaptability ensures that our dataset reflects the ongoing changes in the legal system, whereas prior studies would require manual re-annotation as the law develops."}, {"title": "3.2 Criticality Prediction Task and Dataset", "content": "The Criticality Prediction (CP) task leverages two primary inputs from the Swiss Federal Supreme Court (SFCS) cases: facts and considerations. Facts describe a factual account of the events of each case and form the basis for the considerations of the court. Considerations reflect the formal legal reasoning, citing laws and other influential rulings, and forming the basis for the final ruling.\nWe see two distinct applications for these inputs in the Criticality Prediction task, illustrated in Figure 1. In the Case Prioritization task, only the facts are used as input. This produces a score indicating how critical or important a case is. The goal is to help prioritize cases, which could assist in determining which cases should be heard sooner or assigned to more experienced judges."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Models", "content": "We evaluated the following models, various size variants: MiniLM (Wang et al., 2020), DistilmBERT (Sanh, 2019), mDeBERTa-v3 (He et al., 2021), XLM-R (Base and Large) (Conneau and Lample, 2019), X-MOD (Base) (Pfeiffer et al., 2022), SwissBERT (Vamvas et al., 2023), mT5 (Small and Base) (Xue et al., 2021), BLOOM (560M) (Scao et al., 2022), Legal-Swiss-RoBERTa and Legal Swiss Longformer (Base) (Rasiah et al., 2023), GPT-3.5 (Brown et al., 2020), and LLaMA-2 (Touvron et al., 2023).\nWe fine-tuned all models per task, using early stopping on the validation dataset. Due to resource constraints, further fine-tuning of GPT-3.5 and LLaMA-2 is reserved for future work, with their current performance serving as baseline results. SwissBERT and Legal-Swiss models were chosen for their Swiss-specific pretraining, while the other models were chosen for their multilingual capabilities, essential for our multilingual dataset.\nWe evaluated GPT-3.5 and LLaMA-2 0-shot following Chalkidis (2023), using one instruction and example as input. Samples were randomly selected from the validation set to prevent test set leakage for future evaluations especially for a closed model (GPT-3.5). To manage costs, we limited the validation set to 1000 samples. Our experiments focused solely on zero-shot classification due to the long input lengths. We show the prompts used in Appendix Figure 3 and Figure 4. We used the Chat-Completion API for GPT-3.5 (as of June 7, 2023), and ran LLaMA-2 locally with 4-bit quantization."}, {"title": "4.2 Metrics", "content": "We adopt the LEXTREME benchmark setup (Niklaus et al., 2023), and use hierarchical aggregation of macro-averaged F1 scores with the harmonic mean to emphasizes lower scores, promoting fairness across languages and input types. Scores are averaged over random seeds, languages (de, fr, it), and input types (facts or considerations), penalizing models with outlier low scores, encouraging consistent performance across all configurations."}, {"title": "5 Results", "content": "We present results in Table 3, with standard deviations in Appendix Table 5 and scores on the validation dataset in Appendix Table 6. The best performance was achieved by XLM-RLarge, with an aggregate (Agg.) score of 37.1. SwissBERT also demonstrated competitive results, with an Agg. score of 34.8. Interestingly, larger models did not always outperform their smaller counterparts. For example, mT5Base and mT5small both underperformed compared DistilmBERT in all configurations.\nThe Legal-Swiss models performed well in LD labels, particularly the Legal-Swiss-LFBase, which achieved the highest scores in LD-F and LD-C. However, their weak performance on Citation labels highlights the dataset's complexity, even for domain-specific models.\nLLMs such as GPT-3.5 and LLaMA-2 underperform fine-tuned models, underlining the need for specialized models for these tasks. The difference is largest in the LD labels where the small fine-tuned models always outperformed LLMs.\nTable 2 shows more detailed results on the language specific scores. SwissBERT pretrained with a focus on German achieved the highest aggregate score in German, but interestingly with scores in Italian being the highest by far in C-F. Models pretrained on CC100 (Conneau et al., 2020) (MiniLM, mDeBERTa, XLM-R and X-MOD) exhibited mixed results in French and Italian, with all models performing best in German. MiniLM, mDeBERTa, and X-MOD showed underperformance in Italian but stronger results in French. In contrast, XLM-R, particularly the large variant, demonstrated robust performance in Italian. mT5 models performed well in French, and the base variant additionally also performed well on Italian. BLOOM was much better in French than in other languages, not surprising given it did not have German and Italian in the pretraining data.\nOverall, there only seems to be a weak trend connecting higher percentage of a given language in the pretraining corpus leading to better downstream results in that language."}, {"title": "6 Conclusions and Future Work", "content": "This work introduced a novel Criticality Prediction task to assess the potential influence of Swiss Federal Court cases on future jurisprudence. Our approach utilizes algorithmically derived labels for a more comprehensive and challenging multilingual evaluation of case law importance compared to existing methods. We also released its multilingual dataset to advance research in legal NLP.\nWe conducted a comprehensive evaluation of our proposed Criticality Prediction task, comparing a diverse range of models, from smaller multilingual models to large language models like GPT-3.5. Our findings demonstrate that fine-tuned models consistently outperform their zero-shot LLMs, achieving superior macro-F1 scores. This underscores the importance of task-specific adaptation for optimizing performance in legal NLP applications.\nFuture studies could explore application of the Criticality Prediction task in other legal contexts by incorporating sources from different jurisdictions and languages. This would broaden the impact of this research, providing valuable insights into cross-jurisdictional case influences and enhancing the model's adaptability to various legal systems."}, {"title": "Limitations", "content": "It is very difficult to estimate the importance of a case. By relying on proxies such as whether the case was converted to a leading decision (LD-label) and how often this leading decision was cited (Citation-label), we were able to create labels semi-automatically. While we discussed this with lawyers at length and implemented the solution we agreed on finally, this task remains somewhat artificial."}, {"title": "Ethics Statement", "content": "While automating case prioritization and identifying leading decisions can greatly benefit legal professionals, there are potential risks associated with deploying such classifiers. One concern is the risk of perpetuating biases present in historical legal trends. For instance, case prioritization decisions should not be influenced by factors such as gender, race, or other protected characteristics. We acknowledge these concerns and will pursue measures to mitigate such biases in future work.\nAdditionally, there are challenges with reproducibility when using closed models like ChatGPT. Since the internal workings of these models are not fully transparent, results may be difficult to replicate. To promote open science, we have provided comprehensive evaluations of open source multilingual models, aiming to make our findings more accessible and reproducible."}, {"title": "A Weighting Formula for Citation-Label", "content": "The weighting formula used for the Citation-Label is designed to balance the impact of older cases with more recent rulings, ensuring that the ranking reflects both citation frequency and recency. The formula is as follows:\n$score = count \\times \\frac{year - 2002 + 1}{2023 - 2002 + 1}$\nWhere:\n\u2022 count refers to the number of times a particular case is cited in Swiss Federal Supreme Court (SFSC) decisions.\n\u2022 The year 2002 is the starting year of our dataset.\n\u2022 The year 2023 is the end point of the our dataset.\nWe have +1 adjustment in weighting factor $\\frac{year - 2002 + 1}{2023 - 2002 + 1}$. This ensures that cases from the year 2002 are still included in the weighted calculation and do not receive a weight of zero."}, {"title": "B Dataset Example", "content": ""}, {"title": "C General Dataset Metadata", "content": ""}, {"title": "D Dataset Licensing", "content": "The original case data is available from the Swiss Federal Supreme Court\u00b2 and the Entscheidsuche portal\u00b3 was used to download HTML files for each case.\nIn compliance with the Swiss Federal Supreme Court's licensing policy4, we are releasing the dataset under a CC-BY-4.0 license. The link to the dataset will be made available upon acceptance.\nPersonally identifying information has already been anonymized by the Swiss Federal Supreme Court in accordance with its anonymization rules5."}, {"title": "E Zero-shot Prompts", "content": ""}, {"title": "F Hyperparameters and Package Settings", "content": "We used a fixed learning rate of le-5 without tuning, running each experiment with three random seeds (1-3) and excluding seeds with high evaluation losses. Gradient accumulation was applied when GPU memory was insufficient to maintain a final batch size of 64. Training employed early stopping with a patience of 5 epochs, based on validation loss. To reduce costs, AMP mixed precision was used where it didn't cause overflows (e.g., mDeBERTa-v3). The max-sequence length was set at 2048 for Facts and 4096 for Considerations.\nFor the analysis of consideration and fact lengths, we used SpaCy's en_core_web_sm for tokenization."}, {"title": "G Resource", "content": "The experiments were run on NVIDIA GPUs, including the 24GB RTX3090, 32GB V100, 48GB A6000, and 80GB A100, using approximately 50 GPU days in total."}, {"title": "H Additional Results", "content": ""}, {"title": "I Validation Set Result", "content": ""}, {"title": "J Use of AI Assistants", "content": "We used ChatGPT and Claude to enhance grammatical correctness and style, and utilized Google Colab's Generate AI feature for some of the dataset analysis."}]}