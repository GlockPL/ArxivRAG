{"title": "Multi-modal clothing recommendation model based on large model and VAE enhancement", "authors": ["BINGJIE HUANG", "QINGYU LU", "SHUAISHUAI HUANG", "XUE-SHE WANG", "HAOWEI YANG"], "abstract": "Accurately recommending products has long been a subject requiring in-depth research. This study proposes a multimodal paradigm for clothing recommendations. Specifically, it designs a multimodal analysis method that integrates clothing description texts and images, utilizing a pre-trained large language model to deeply explore the hidden meanings of users and products. Additionally, a variational encoder is employed to learn the relationship between user information and products to address the cold start problem in recommendation systems. This study also validates the significant performance advantages of this method over various recommendation system methods through extensive ablation experiments, providing crucial practical guidance for the comprehensive optimization of recommendation systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The fashion industry's rapid development has resulted in an overwhelming amount of data and information for users when selecting clothing. The abundance of clothing product information makes it difficult for consumers to choose among different clothing options when shopping online. In the era of information explosion, accurately retrieving clothing products that meet users' needs has become an urgent research problem to be solved. Therefore, it is necessary to adopt an automatic and effective method that can enable consumers to purchase suitable clothes more efficiently. The intelligent clothing recommendation system can analyze a series of user characteristics and rank the most suitable products according to certain rules, giving more importance or attention to clothing products that users may be interested in or most suitable for, thereby achieving effective information filtering to achieve the effect of recommendation [1].\nTypical recommender systems use two types of data: rating or purchase behavior reflecting attribution information of products and users, and keywords or textual information that record user-item interactions. Generally, recommender systems rely on various types of input. Explicit feedback data is usually structured and can be directly modeled without special processing. This data primarily records key textual information and details of user-item interactions. For instance, a common example is users expressing their preferences for products through likes or star ratings [2]. In the absence of explicit feedback, recommender systems often infer user information from implicit feedback, such as purchase history or watchlists for clothing items. Most recommender systems can be categorized into three types: content-based recommender systems, collaborative filtering recommender systems, and hybrid recommender systems. One of the earliest and most widely used techniques in recommender systems is collaborative filtering. It estimates and generates recommendations based on the commonalities and similarities of users with similar purchase behaviors and information [3]. In addition to collaborative filtering models, content-based filtering is another significant type of recommender system. Content-based recommender systems recommend other items similar to a user's preferences by considering the contributions of a particular item, focusing mainly on a single user's ratings rather than multiple users' ratings. Moreover, content-based recommender systems rely more heavily on the higher-level features of users and items for predictions compared to collaborative filtering [4].\nBoth collaborative filtering and content-based recommender systems have their limitations. For instance, collaborative filtering systems tend to focus more on structured data, while content-based recommender systems may overlook the similarity of preferences among individuals, impacting the accuracy of predictions [5]. Hybrid recommender systems integrate the advantages of both types of recommender systems to improve the comprehensiveness of algorithms. However, these systems face issues such as cold start, sparsity, and outdated information, which limit their predictive performance [6]. The problem solved in this paper is as follow:\nQ1:how to set up a multimodal model to achieve good performance during cold start (lack of data)?\nThis study develops an integrated recommendation model combining a large-scale model and variational autoencoder to address performance degradation issues in cold start or low data volume scenarios. The contributions of this model are threefold:\n1. LLMs is introduced to encode and fuse different modalities to improve the accuracy of information expression\n2. Neural collaborative filtering is used to replace traditional filtering methods to improve information interaction"}, {"title": "2 LITERATURE REVIEW", "content": "3.  VAE is used to obtain pseudo-label enhanced data to alleviate the cold start problem\nThis method effectively addresses the current cold start and accuracy issues in recommendation systems, significantly contributing to the improvement of recommendation systems, and introduces a new paradigm that integrates multiple modalities to enhance recommendation accuracy.\nWith the emergence of deep networks, the application of artificial intelligence in education [7], medical care [8], business [9], credit modeling [10], chip design [11] and other fields has made great progress. In traditional collaborative filtering algorithms, matrix factorization commonly uses the inner product to represent interactions. In theory, neural networks can fit any function, thus allowing them to learn from data to replace the inner product for modeling latent features of users and items. This approach is referred to as Neural Collaborative Filtering [12]. In addition to the improvement of the matrix decomposition algorithm, in order to overcome the above-mentioned obstacles of traditional models and achieve more accurate recommendations, researchers are considering the possibility of using deep learning for recommender systems. With deep learning, hidden information can be obtained from including contextual, textual, and visual inputs have detected nonlinear relationships between users and recommended items [13]. For example, Bhasker et al. proposed a novel deep learning hybrid recommendation algorithm that uses specific embeddings and deep neural networks to characterize users and items in order to learn nonlinear latent factors [14] Lin et al. developed a clothing recommendation system based on a neural network model, which firstly uses a convolutional neural network for gender recognition, and then modules such as Dlib and GoogleNet for clothing attribute recognition [15]. With the rise of training large models, the accuracy of this recommendation system has been further improved, such as directly recommending products through large models [16] or training large models to provide embedded vectors for prediction [17]. However, although the semantics of large models greatly enhance the effectiveness of recommendation systems, there are still challenges in cold start and fusion methods [18]. Large models usually require a lot of data to generate effective vector representations, but it may be difficult to have enough data to support subsequent training and fine-tuning. At the same time, large models are usually difficult to update frequently, and retraining or fine-tuning a large model also requires a lot of resources. This may reduce the timeliness of the recommendation system in a rapidly changing recommendation environment.\nModal fusion methods typically include early fusion and late fusion. Early fusion refers to the merging of data from multiple modalities prior to model input, allowing the model to process interactions between different modalities from the outset. Late fusion, on the other hand, involves the combination of independent judgments from different modalities at the final decision stage of the model [19]. While early fusion is simple and efficient, it can lead to incomplete utilization of semantic information due to the compression of features with different scales at the initial stage of fusion. In contrast, late fusion allows for the independent training of each modality, applying the most appropriate techniques to handle different types of modalities. However, integrating outputs after separate training can present challenges in capturing the deep interactions between different modalities. Additionally, there exists an intermediate fusion approach, which serves as a compromise by combining multimodal features at certain intermediate layers to capitalize on the advantages of both early and late fusion.\nTechniques employed in intermediate fusion include concatenation, weighted summation, and neural networks.\nThe cold-start problem in multimodal fusion generally refers to the initial stage of the model, due to a lack of sufficient multimodal data or user interaction history, the model struggles to effectively utilize information from each modality for accurate predictions or recommendations [20]. When This issue may arise from various challenges, including data sparsity, modality inconsistency, and insufficient model training [21]. To address the cold-start problem in recommendation systems, some studies have employed enhanced collaborative filtering [22], transfer learning [23], and pseudo-label generation techniques [24]. We adopt a pseudo-labeling method for enhancement."}, {"title": "3 MODELING", "content": "In general, our overall model meets the requirements of cross-modal tasks and neural collaborative filtering algorithms. Our model process is shown in Figure 1, which mainly includes data organization, cross-modal embedding representation, model training and evaluation. We collect text and image information of users and products and send them to the cross-modal communication block. The data generated in the communication block will then be sent to the neural collaborative network for comparative prediction."}, {"title": "4 EVALUATES MATREIX", "content": "The paper evaluates the performance of models using three common methods: Mean Squared Error (MSE), Average Accuracy, and Precision@K. MSE, represented in Formula 2, assesses the regression performance by calculating the average squared difference between the predicted and actual values. n is the total number of sample points, is the predicted result of the i-th observation predicted by the model, and ri is the actual value of the i-th observation.\nMSE =  $\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{r_{i}} - r_{i})^{2}$ \nPrecision@K evaluates recommendation systems by measuring the ratio of relevant items in the top K recommendations to the total number of recommendations, as shown in Formula 3. U represents users, Ru is standing for the interaction between the user and the item, and  Run Ruk is the results of recommended items the user put the relatively high ranking."}, {"title": "5 EVALUATES MATREIX", "content": "In order to adapt our proposed model, we refer to the Fashion-mnist dataset [28] and obtain data from Taobao, China's largest online shopping website. We created a multimodal dataset that includes user avatars and introductions, clothing pictures and introductions. The introduction has been screenshotted to ensure that all text information in the picture only contains pure picture information. We still prepared a more detailed feature for subsequent ablation experiments, which includes the number of clicks on the product page by the user, the age and gender of the user's Taobao account opening, the user's comments on the product, clothing category, color, style and other features. This dataset has 11 features and a total of 82 data.\nIn all experiments, all MLP models were configured as three-layer structures, with the first layer comprising 128 neurons and the second layer comprising 64 neurons. All neural networks were trained for 10 epochs, the number of recommended items in the recommendation system K was set to 5, and cross-validation was employed for training. To assess the efficacy of the proposed model, we conducted a series of ablation experiments and compared the results with those of the following models. We use the following mainstream models for comparison:\n1. Matrix factorization model (MF) [29]\n2.Neural matrix factorization model (NMF) [30]\n3.multimodal recommendation mode CNN(CNN) [31]\n4. multimodal recommendation model of DNN (DNN) [32]\n5. CLIP [33]\n6. VIIBERT [34]\nMF and NMF will directly use MLP to process text and image data. Image data is processed using Scale-Invariant Feature Transform, a non-deep learning method, while text is processed using Term Frequency-Inverse Document Frequency to obtain vector embedding. It should be noted that all the benchmark models only involve modal fusion and only contain text and image content. The fusion of non-structural content will be performed separately in the ablation experiment. Non-structural content will be additionally entered into the splicing after multi-modal merging and merged through MLP."}, {"title": "6 EXPERIMENTS", "content": "6 EXPERIMENTS"}, {"title": "7 DISCUSSION", "content": "In total, VIIBERT and CLIP demonstrate outstanding performance across all metrics, especially in MSE and NDCG. Muti (ours) and its variants show balanced and commendable performance across all evaluated metrics, making them reliable choices. In contrast, traditional MF and NMF models are relatively less competitive in modern applications. In addition, we conducted another round of ablation experiments to verify whether mid-term fusion is the best. In our ablation experiment, we added early fusion and late fusion. We directly used simple concatenation for early fusion. Then we put the VAE module at the output and added a fully connected network. Finally, we experimented with late fusion.\nThrough a series of experiments, we have reached three preliminary insights: First, multimodal models based on large language models perform significantly better than traditional methods and deep neural network-based methods. This advantage is primarily attributed to the richness of the vector embeddings. Large language models can generate embeddings with richer semantic information, thereby performing better in multimodal tasks. Second, although our model also uses large language models for vector embeddings, its performance is slightly inferior to mainstream models like CLIP and Vilbert. This is likely because our model lacks the crucial step of explicit feature alignment. Although we performed image-text alignment through MLPs, the absence of an explicit self-supervised learning process may have led to performance loss. Third, fortunately, our ablation experiments demonstrate that the introduction of the VAE structure can significantly reduce the dependency on data.\nExperiments show that incorporating VAE significantly enhances the model's predictive capabilities. However, we remain uncertain about the profound impact of VAE on the model, as VAE can distort images during generation, such as sharpening colors. These data augmentation behaviors may not necessarily provide guidance in real-world recommendation systems. In addition, the extent to which the VAE model is added to the model should also be further considered, which is related to the overall consideration of the entire multimodal construction [35]. Additionally, we conducted ablation experiments to assess the impact of unstructured data on the model. We integrated unstructured data into the VAE-generated embeddings and performed dimensionality reduction to revert to the original dimensions. Experiments show that adding extra information does positively affect the model's performance, but the effect is still inferior to models like CLIP that only use explicit alignment methods. This further confirms the importance of explicit alignment in multimodal recommendation systems. However, as with VAE, how this explicit modality alignment should be reasonably built into the model remains an issue that needs further discussion [36].\nIn the field of data augmentation, a commonly used approach is to establish relationships using graph-based [37] methods or to map with Apriori rule algorithms [38]. These methods rely on predefined rules, making the data augmentation process interpretable and the results highly predictable. However, unlike rule-"}, {"title": "8 CONCLUSION", "content": "This paper proposes an integrated recommendation model that combines large-scale models and variational autoencoders to solve the problem of performance degradation of recommendation systems in cold start or scenarios with small amounts of data. Experimental results show that multi-modal models based on large language models are better than traditional methods and deep neural network methods in terms of effect, benefiting from the richness of vector embeddings. Although the performance is slightly inferior to mainstream models such as CLIP and Vilbert, the introduction of VAE significantly improves prediction capabilities and reduces data dependence. Ablation experiments further confirm the importance of explicit alignment in multi-modal recommender systems."}]}