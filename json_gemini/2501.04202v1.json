{"title": "GENERATIVE DATASET DISTILLATION BASED ON SELF-KNOWLEDGE DISTILLATION", "authors": ["Longzhen Li", "Guang Li", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "abstract": "Dataset distillation is an effective technique for reducing the cost and complexity of model training while maintaining performance by compressing large datasets into smaller, more efficient versions. In this paper, we present a novel generative dataset distillation method that can improve the accuracy of aligning prediction logits. Our approach integrates self-knowledge distillation to achieve more precise distribution matching between the synthetic and original data, thereby capturing the overall structure and relationships within the data. To further improve the accuracy of alignment, we introduce a standardization step on the logits before performing distribution matching, ensuring consistency in the range of logits. Through extensive experiments, we demonstrate that our method outperforms existing state-of-the-art methods, resulting in superior distillation performance.", "sections": [{"title": "1. INTRODUCTION", "content": "The rapid advancement of deep learning has driven the creation of increasingly large models that require vast amounts of data to achieve optimal performance [1]. However, this growth in data volume brings with it several significant challenges. First, storing and maintaining large datasets incurs high costs, both in terms of storage and the computational resources needed for processing [2]. Second, as datasets grow, the time required for model training becomes increasingly difficult to manage, with efficiency concerns becoming more prominent [3]. Additionally, large-scale datasets raise important issues related to data privacy and security, particularly when handling sensitive information [4].\nTo address these issues, dataset distillation has emerged as a promising solution [5]. This technique condenses the information from a large dataset into a smaller, more efficient version, enabling models trained on the synthetic data to achieve performance similar to those trained on the full dataset [6]. Furthermore, dataset distillation can help alleviate privacy and sharing concerns [7-9]. As a result, dataset distillation has been applied in many downstream tasks such as continual learning [10], federated learning [11], and neural architecture search [12]. In recent years, various types of dataset distillation algorithms have been proposed, including performance matching [5], gradient matching [13], trajectory matching [14, 15], and so on. Among these approaches, generative dataset distillation [16] has garnered attention due to its potential to enhance flexibility and adaptability across different architectures, offering a more efficient solution for real-world applications.\nGenerative dataset distillation aims to condense the information from large-scale datasets into a generative model rather than a static dataset [16, 17]. Unlike traditional dataset distillation methods, which produce a smaller fixed dataset, generative dataset distillation trains a model capable of generating effective synthetic data on the fly [18]. This approach has been shown to offer better cross-architecture performance compared to traditional methods, while also providing greater flexibility in the data it generates. The generative dataset distillation process typically consists of two steps. First, a generative network is trained to generate a synthetic dataset that captures the essential characteristics of the original dataset. Next, the prediction logits of synthetic and original datasets are compared. The generative network is continuously optimized based on this comparison, improving its ability to generate effective synthetic data. The accuracy of the logits matching directly influences the overall performance of the generator. However, current generative dataset distillation methods rely on relatively simple logits matching, which can constrain their effectiveness. To address this limitation, a new approach is required that enhances the precision of logits matching, enabling the generator to capture more essential information from the original dataset and improving the distillation performance.\nIn this paper, we propose a novel generative dataset distillation method that leverages self-knowledge distillation to enhance the generator's performance. A key innovation of our approach is the integration of self-knowledge distillation into the optimization process, guiding the generator to more effectively align the distribution of the synthetic dataset with the original dataset. Unlike simple logits matching, distribution matching captures the overall structure and relationships in the data, leading to more robust and accurate alignment. To further improve the accuracy of alignment, we introduce a standardization step on the logits before performing distribution matching between the original and generated data. This standardization ensures consistency in the range of logits, reducing variability and improving the precision of the matching process. As a result, our method enables the generator to produce more accurate and representative synthetic data, resulting in higher-quality distillation. Extensive experiments on several benchmark datasets demonstrate the effectiveness of the proposed method compared to existing state-of-the-art methods.\nOur contributions are summarized as follows.\n\u2022 We propose a novel generative dataset distillation method that incorporates self-knowledge distillation and improves the matching process by introducing logits standardization before performing distribution matching, which enhances the generator's ability to produce high-quality synthetic datasets.\n\u2022 We validate the proposed method through extensive experiments, demonstrating its effectiveness and superior performance compared to existing state-of-the-art methods."}, {"title": "2. GENERATIVE DATASET DISTILLATION BASED ON SELF-KNOWLEDGE DISTILLATION", "content": "The proposed method consists of two main steps. First, we train a generative adversarial network (GAN) to generate a synthetic dataset S. Next, a model is randomly selected from a model pool to align the synthetic dataset S with the original dataset O. To improve this alignment, we incorporate self-knowledge distillation, where the distributions of the original and synthetic data are matched with the selected model. Additionally, we standardize the output logits to ensure consistency in their range and apply distribution matching to enhance the accuracy of the alignment between the synthetic and original datasets. The distillation process is illustrated in Fig. 1."}, {"title": "2.1. GAN Generator Training", "content": "First, we train a conditional GAN generator to generate the synthetic dataset. The GAN framework consists of two components: a generator and a discriminator [19]. During training, the generator and the discriminator engage in a competitive process, with the generator aiming to generate increasingly realistic images, while the discriminator works to distinguish between real and generated images. This competition drives the generator to improve continuously. The training procedure for the conditional GAN is described as follows:\n$\\mathcal{L}_{CGAN} = \\min_{G} \\max_{D} V(D, G)$ \n$= \\mathbb{E}_{r \\sim p(r)}[\\log D(r|y)]$ \n$+ \\mathbb{E}_{z \\sim p(n)}[\\log(1 - D(G(n|y)))]$,\n(1)\nwhere $\\mathcal{L}_{CGAN}$ represents the conditional GAN training loss, G is the generator, D is the discriminator. r means real data and n means random noise. y is the additional information that is used as input in conditional GAN. In our method, labels are utilized as this additional input to enhance the generator's ability to generate a more relevant and realistic synthetic dataset. Once the GAN generator is trained, it can synthesize dataset S as follows:\n$S = G ([n+ y] ; W)$,\n(2)\nwhere \u2295 represents the concatenation operation, n means random noise, y is the label information, and W denotes the parameter of generator G.\nUnlike traditional GAN networks, which aim to generate visually realistic images, the objective of our method is to generate a synthetic dataset that effectively condenses core information from the original data. Instead of prioritizing visual fidelity, our approach focuses on preserving the essential features of the original dataset. Through iterative optimization, the generator refines the synthetic data, evolving it from random noise into a dataset that increasingly captures the core information of the original.\nAt this stage, the dataset generated by the generator still lacks sufficient information from the original dataset. To solve this, subsequent logits matching between the synthetic and original datasets is necessary. This matching process enables continuous optimization of the generator, allowing it to progressively capture more detailed information from the original dataset in the synthetic dataset."}, {"title": "2.2. Dataset Distillation via Self-knowledge Distillation", "content": "Before the matching process, we randomly select a model from a pool of models. Traditional dataset distillation methods typically rely on a single model for matching, which limits their ability to generalize across different architectures. In our approach, both the original and synthetic datasets are passed through the selected model to obtain prediction logits, representing the activation values just before the final output layer. By continuously matching the distribution of logits between the original and synthetic datasets, we progressively improve the GAN generator's ability to produce synthetic data that more accurately captures the key features of the original dataset. This strategy ensures improved performance across various model architectures.\nDirectly aligning non-standardized prediction logits can introduce some biases. For instance, variations in the logits range between the synthetic and original data can lead to instability in the matching process, potentially favoring incorrect predictions due to similar logits ranges, even if the predictions are inaccurate. To mitigate this, our method standardizes the logits before performing distribution matching. By ensuring the output logits fall within a consistent range, we achieve more accurate distribution matching, leading to improved prediction results. The detailed standardization process is shown as follows:\n$Z(x; \\tau) = \\frac{x - mean(x)}{std(x) \\times \\tau}$ \n(3)\nwhere x represents the input logits vector, mean(x) and std(x) denote the mean and standard deviation of the logits respectively. The parameter \u03c4 is a reference temperature used to scale the logits during standardization. $Z(x; \\tau)$ represents the standardized logits.\nAfter performing standardization, the standardized logits are passed through the softmax function to compute the probability distribution. This allows us to calculate the probability distributions for both the original and synthetic data independently as follows:\n$d(x_0) = softmax(Z(x_0; \\tau))$,\n(4)\n$d(x_s) = softmax(Z(x_s; \\tau))$.\n(5)\nHere, d(xo) represents the probability distribution of the original data, obtained through the softmax calculation, while d(xs) refers to the probability distribution of the synthetic data, also calculated using the softmax function. The distributions will be used for further comparison and optimization during the distillation process.\nThe self-knowledge distillation loss LSKD can be calculated as follows:\n$\\mathcal{L}_{SKD} = \\sum_{k=1}^{K} d(x_0)^{(k)} log (\\frac{d(x_0)^{(k)}}{d(x_s)^{(k)}})$ \n(6)\nwhere K is the number of categories, the terms $d(x_0)^{(k)}$ and $d(x_s)^{(k)}$ denote the probability distributions of the original data and the synthetic data for the kth category, respectively. Unlike the previous logits matching approach, which relied on calculating the mean squared error between the logits of the synthetic and original datasets, our method introduces a more effective solution through self-knowledge distillation. Specifically, we utilize distribution matching, which captures the overall structure and relationships within the data, resulting in a more robust and accurate alignment.\nTo further refine this process, we incorporate a standardization step to ensure the logits remain within a consistent range, reducing variability and improving the precision of the matching process.\nThe total loss composed of the conditional GAN loss $\\mathcal{L}_{CGAN}$ and the self-knowledge distillation loss $\\mathcal{L}_{SKD}$ is calculated as follows:\n$\\mathcal{L}_{total} = \\mathcal{L}_{CGAN} + \\lambda_{SKD} \\tau^2 \\mathcal{L}_{SKD}$ .\n(7)\nwhere $\\lambda_{SKD}$ and \u03c4 denote the weight parameter and the temperature parameter, respectively. By aligning the output logits based on their distribution and standardizing them beforehand, we achieve a more precise alignment between the generated and original datasets. This approach enhances prediction accuracy and enables more effective optimization of the generator. The objective of our method is to continuously minimize the total loss, which directly optimizes the parameters W of the generator G. As the loss is reduced, the generator gradually enhances its ability to produce synthetic datasets that closely resemble the original. By minimizing the total loss, the synthetic data capture finer details and better retain the essential information from the original dataset."}, {"title": "3. EXPERIMENTS", "content": "We conducted extensive experiments to verify the effectiveness of the proposed method. First, we designed benchmark experiments using three datasets: MNIST [29], FashionMNIST [30], and CIFAR-10 [31]. Each dataset was categorized into 10 classes. To validate the performance of our approach, we compared it with several dataset distillation methods. The comparison includes baseline methods such as CGAN [19] and DiM [16], as well as four data selection methods: random selection (Random) [13], herding method (Herding) [20], K-Center [21], and example forgetting (For-"}, {"title": "3.2. Benchmark Results", "content": "In this section, we evaluate the effectiveness of the proposed method by comparing it with other SOTA methods on three benchmark datasets. Additionally, to highlight the significance of the standardization step, we performed two sets of experiments: one using standardized prediction results (Ours) and another without standardization (Ours (No Stand.)). Within each group, we conducted three experiments by varying the IPC (Images Per Class) and batch-size parameters. The IPC values were set to 1, 10, and 50, while batch sizes were 32, 64, and 128, respectively. For all experiments, we utilized a model pool consisting of three neural networks, randomly selecting from ConvNet3 [32], ResNet10 [33], and ResNet18 [33]. The weight parameter $\\lambda_{SKD}$ was set to 0.01 for FashionMNIST and CIFAR-10, and 0.001 for MNIST. The temperature parameter \u03c4 was set to 2 in all experiments. To ensure reliability, we conducted five experiments for each IPC and batch-size configuration and averaged the results to determine the results. All experiments were conducted on an NVIDIA RTX A6000 GPU and the PyTorch framework.\nAs shown in Table 1, the proposed method demonstrates superior distillation performance across most experimental settings. In particular, for CIFAR-10, our method significantly outperforms other state-of-the-art dataset distillation methods. Furthermore, standardizing the logits leads to a notable improvement in model accuracy compared to results obtained without standardization, highlighting the effectiveness of the standardization step. The visualization results provided in Fig. 2 further demonstrate that our method generates more accurate and representative synthetic data."}, {"title": "3.3. Cross-architecture Results", "content": "In this section, we conducted the cross-architecture experiment to demonstrate that the proposed method exhibits strong generalization performance across different network architectures. Cross-architecture performance refers to the ability to test a model trained on one architecture using a different architecture. In this experiment, we used the CIFAR-10 dataset with an IPC of 10. We evaluated the cross-architecture performance using ConvNet3 [32], ResNet18 [33], AlexNet [34], and VGG11 [35] as the same as previous studies [16].\nAs shown in Table 2, the proposed method outperforms previous dataset distillation approaches in terms of cross-architecture performance, even without standardization. After applying standardization, the results improved further. In addition to enhancing distillation performance, our method also increased the stability of the results, ensuring more robust results across different neural network architectures."}, {"title": "4. CONCLUSION", "content": "In this paper, we have proposed a novel generative dataset distillation method that incorporates self-knowledge distillation to improve the overall distillation process. A key innovation of our approach is the redesign of the logits matching process, where we employ distribution matching to better align the prediction logits between the original and synthetic datasets. To further enhance the accuracy of alignment, we apply a standardization step on logits before the matching process. This ensures a more consistent and reliable comparison between the original and synthetic datasets. Extensive experimental results show the effectiveness of the proposed method, demonstrating its superior performance in comparison to other SOTA dataset distillation methods."}]}