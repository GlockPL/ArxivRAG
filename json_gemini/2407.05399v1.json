{"title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning", "authors": ["Abhinav Joshi", "Shounak Paul", "Akshat Sharma", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "abstract": "Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/) where the research community can upload and compare legal text understanding systems.", "sections": [{"title": "Introduction", "content": "Besides several other purposes, legal systems have been established in various countries to ensure, at the very minimum, order and fairness in society and to safeguard fundamental human rights. However, legal systems worldwide struggle with exponentially growing legal cases in various courts. It is even more pronounced in populous countries; e.g., in India, there are about 50 million pending cases in multiple courts at various levels (district, state, federal) (National Judicial Data Grid, 2023). Such a massive backlog of cases goes against the fundamental human right of fair access to justice. Documents in different natural languages are the backbone of various legal processes. Natural Language Processing (NLP) based techniques could be helpful in various legal processes involving fundamental tasks related to information extraction, document understanding, and prediction. This paper introduces IL-TUR, a benchmark for Indian Legal Text Understanding and Reasoning. The purpose of IL-TUR is twofold. First, it aims to foster research in the Legal-NLP (L-NLP) domain and plans to address the pain points associated with processing legal texts (see below); second, it provides a platform for comparing different models and further advancing the L-NLP domain.\nWhy a separate benchmark for the legal domain? The legal text involves natural language but differs from the regular text used to train NLP models. 1) Many of the terms used in legal documents are domain-specific. For example, some words used in everyday language have specialized meanings in legal parlance. The presence of a different lexicon posits a need for specialized NLP tools to handle legal texts. 2) Legal documents are typically very long compared to regular texts. For example, the average length of a legal document from the Supreme Court of India (SCI) is 4000 words (Malik et al., 2021). It poses a challenge for existing NLP models (e.g., LLMs) as the information is spread throughout the document and must be linked together for reasoning. Moreover, many of the existing language models (e.g., BERT (Devlin et al., 2019)) have limitations on the length (512 tokens) of the input. It requires developing specialized models for processing and handling long legal documents. 3) Legal documents are highly unstructured and sometimes noisy (for example, in the Indian setting, most documents are typed manually in the courts and prone to grammatical mistakes and typos). The absence of structure in the documents makes extracting semantically relevant information from large chunks of text difficult. 4) The legal domain is further subdivided into specialized subdomains; for example, criminal law differs from civil law, and both differ from banking and insurance law. Even though some fundamental legal principles are shared across various laws, models trained on a particular law (e.g., civil law) may not work on another (e.g., banking and insurance law). Hence, domain adaptation is a challenge. 5) Lastly, many existing state-of-the-art (SOTA) NLP models are black boxes; however, explainability is not a second-class citizen for the legal domain. For models to be widely usable by legal practitioners, these need to be explainable. Due to the above reasons, a separate set of models/systems is required to process and understand legal documents. Given the huge backlog of cases, NLP-based technologies could come to our rescue and help streamline the legal workflow. Even a small technical intervention can have a considerable impact. Hence, a benchmark is needed to promote the development of models in this area. In a nutshell, we make the following contributions:\n\u2022 We introduce IL-TUR: a benchmark for Indian Legal Text Understanding and Reasoning. The benchmark has eight tasks (in English and 9 Indian languages) requiring different types of legal knowledge and skills to solve. Moreover, the list of tasks is not exhaustive, and we plan to keep adding more tasks to IL-TUR. Currently, there are various L-NLP-specific tasks; however, these occur in isolation, making it difficult to keep track of progress made in the field. Similar to existing NLP benchmarks (e.g., GLUE (Wang et al., 2018a)), we consolidate and harmonize some of the existing L-NLP tasks and create new tasks resulting in a unified benchmark.\n\u2022 We report baseline model results on each of the tasks. We also experiment with various LLMs (\u00a74), and results show that LLMs are far from solving the tasks and hence point towards the need to develop better models.\n\u2022 We release the dataset and baseline models associated with each task. Further, we create a leaderboard where anyone can upload their model and test against the baselines and other proposed systems (e.g., Fig. 1). The datasets, models, and the leaderboard are available via the following website: https://exploration-lab.github.io/IL-TUR/."}, {"title": "Related Work", "content": "Over the past few years, L-NLP has been a fertile area for research. Researchers have explored different aspects of the legal domain via various tasks such as Prior Case Retrieval (Joshi et al., 2023; Jackson et al., 2003a), Case Prediction (Malik et al., 2021; Chalkidis et al., 2019; Strickson and De La Iglesia, 2020; Kapoor et al., 2022), Summarization (Moens et al., 1999), Semantic Segmentation of Legal Documents (Malik et al., 2022; Kalamkar et al., 2022b; Bhattacharya et al., 2019), and Information Extraction and Retrieval (Tran et al., 2019; Lagos et al., 2010). On the modeling side, various techniques have been proposed, ranging from classical ML-based methods such as SVM (Al-Kofahi et al., 2001; Jackson et al., 2003b) to recent transformer-based models (Chalkidis et al., 2019; Malik et al., 2021). Researchers have also proposed legal domain-specific language models such as LegalBERT (Chalkidis et al., 2020), CaseLawBERT (Zheng et al., 2021) and InLegalBERT and InCaseLawBERT (Paul et al., 2023). However, legal LLMs have shown limited success and have not demonstrated generalization and transfer learning capabilities (Chalkidis, 2023; Malik et al., 2021; Joshi et al., 2023).\nComparison with Existing Benchmarks: Benchmarks have played a crucial role in the development of better techniques and models in almost every domain, such as computer vision (Deng et al., 2009; Guo et al., 2014; Wu et al., 2013) and reinforcement learning (Laskin et al., 2021; Cobbe et al., 2020; Zhang et al., 2018). Similarly, in the NLP domain, various benchmarks have been proposed, for example, GLUE (Wang et al., 2018a), Super-GLUE (Wang et al., 2019a), XTREME (Hu et al., 2020), CLUE (Xu et al., 2020), GLGE (Liu et al., 2020), and IndicNLPSuite (Kakwani et al., 2020). However, these benchmarks focus on the general NLP domain, and models developed for the generic domains do not perform well for the legal domain (Malik et al., 2022; Joshi et al., 2023). Similar attempts have thus been made for the legal domain; for example, Chalkidis et al. (2022a) developed LexGLUE, a specialized English language benchmark (restricted to EU and US legal systems) for evaluating legal NLP models, by consolidating existing datasets for various tasks. LexGLUE introduces six main (all classification-based) tasks: violated article identification, case issue classification, concept identification, contract topic prediction, unfair contractual terms identification, and case holding identification. Niklaus et al. (2023) have proposed LEXTREME, a multi-lingual (24 EU languages) legal NLP benchmark (all tasks classification-based) restricted to EU and Brazilian jurisdictions. Chalkidis et al. (2022b) have introduced FAIRLEX, a multi-lingual benchmark consisting of cases from 5 languages and 4 jurisdictions, to test the fairness of different models on legal judgment and topic prediction. Hwang et al. (2022) have introduced LBOX benchmark for the Korean legal system. The benchmark targets tasks related to classification and summarization; the documents are in Korean. Recently, Guha et al. (2023) released LegalBench, a large, collaborative legal benchmark (restricted to US legal system) consisting of 162 tasks (in English) to test the reasoning abilities of LLMs. The tasks belong to six different categories of legal reasoning and address various stages in the pipeline of the litigation process. LegalBench is primarily focused on testing the ability of LLMs to handle legal processes at various stages of litigation; consequently, the tasks involve shorter texts (avg. length ~ 200 words). To benchmark LLMs for Chinese law, Fei et al. (2023) released LawBENCH, a benchmark consisting of 20 tasks (in Chinese) to evaluate the capability of LLMs to memorize and understand legal knowledge. Most of these tasks consist of longer texts than LegalBench (avg. length ~ 300 words).\nIL-TUR differs from the existing benchmarks (see Table 1). First, IL-TUR focuses on multiple tasks that are not restricted to classification but also involve information retrieval, generation, and explanation. Second, via IL-TUR, we introduce tasks that are grounded in the actual legal workflow and, consequently, are more complex and involve actual long legal documents (average length 4000"}, {"title": "IL-TUR: Legal-NLP Benchmark", "content": "Table 2 summarizes various tasks proposed in IL-TUR. The tasks cover multiple aspects of the legal domain and require specialized skills and knowledge to solve them."}, {"title": "Design Philosophy", "content": "We want to develop technology that enables automated semantic and legal understanding of legal documents and processes. We created IL-TUR with the following principles in mind.\n1) Legal Understanding and World Knowledge: The tasks should cater exclusively to the legal domain. Solving a task should require in-depth knowledge and understanding of the law and its associated areas. Further, the tasks should not be restricted to only classification but should also involve retrieval, generation, and explanation. The proposed tasks address the pain points of processing legal texts (\u00a71). Moreover, solving legal tasks should require knowledge about the law as well as commonsense knowledge and societal norms about the world (e.g., facts in conjunction with socioeconomic conditions in a particular case). 2) Difficulty Level: The difficulty level should be such that these are not solvable by a layperson (having minimal knowledge and expertise in legal matters). It ensures that general language learners cannot easily solve the tasks, and the tasks would be sufficiently challenging for the current state-of-the-art models (e.g., LLMs). 3) Language: Since India is a multi-lingual society, the tasks should cater to the most frequent languages used in the courts. We cover tasks in English and 9 other Indian languages. 4) Evaluation: The tasks should be automatically evaluable, and the metrics used should align with human judgments. 5) Public Availability: The data used for the tasks should be publicly available so anyone can use it for research purposes without licensing or copyright restrictions. Further, a leaderboard should be available to compare different systems and models. We release the data via a Creative Common Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license and create a public leaderboard."}, {"title": "IL-TUR Tasks", "content": "Based on the design philosophy, in this version of IL-TUR, we selected eight different tasks. Table 2 provides a summary of the tasks. We briefly describe the tasks here; details about the dataset and evaluation metrics are provided in App. A.\n\u2022 Legal Named Entity Recognition (L-NER): This is a newly created task in IL-TUR. Formally, given a legal document, the task of Legal Named Entity Recognition is to identify entities (set of 12 entity types), namely, Appellant, Respondent, Judge, Appellant Counsel, Respondent Counsel, Court, Authority, Witness, Statute, Precedent, Date, and Case Number. L-NER is different from the standard NER task; if one were to run a standard NER system on a legal document, the judge, petitioner, and respondent would all be labeled with a \u201cPERSON\" tag. Hence, a separate task is needed to identify the legal named entities in the documents. The standard NER (identifying person/organization/location names) can be done by any non-legal professional/person, but identifying the roles of entities involved in a legal case (L-NER) requires an in-depth understanding of the legal terminologies and the law. Hence, we develop a gold-standard dataset for L-NER with the help of law students (details in A.1). Moreover, the set of legal entities and corresponding definitions are formulated with the help of legal academicians (experts).\n\u2022 Rhetorical Role Prediction (RR): As pointed out earlier, legal documents are typically long (avg. length 4000 words) and highly unstructured, with the legal information spread throughout the document. Segmenting the long documents into topically coherent units (such as facts, arguments, precedent, statute, etc.) helps highlight the relevant information and reduces human effort. These topically coherent units are termed as Rhetorical Roles (RR). Given a legal document, the task of RR prediction involves assigning RR label(s) to each sentence. We focus on 13 RR labels: Fact, Issue, Arguments (Respondent), Argument (Petitioner), Statute, Dissent, Precedent Relied Upon, Precedent Not Relied Upon, Precedent Overruled, Ruling By Lower Court, Ratio Of The Decision, Ruling By Present Court, None. Details about RR labels, definitions, and the dataset are provided in the App. A.2.\n\u2022 Court Judgment Prediction with Explanation (CJPE): Formally, the task of Court Judgment Prediction with Explanation (CJPE) involves predicting the final judgment (appeal accepted or denied, i.e., the binary outcome of 0 or 1) for a given judgment document (having facts and other details) and providing the explanation for the decision. In this case, the explanations are in the form of the salient sentences that lead to the decision. Note that the idea behind this task is not to replace human judges but to augment them in decision-making. Furthermore, the task requires the system to explain its decision so that it is interpretable for a human (details in App. A.3).\n\u2022 Bail Prediction (BAIL): A large fraction of the pending cases in India are from the district-level courts and have to do with bail applications (https://en.wikipedia.org/wiki/Bail) (Kapoor et al., 2022). Many of the district courts in India use Hindi as their official language (also refer to the Limitations section). Given a legal document in the Hindi language (having the facts of the case), the task of Bail Prediction involves predicting if the accused should be granted bail or not (i.e., a binary decision of 0/1) (details in App. A.4).\n\u2022 Legal Statue Identification (LSI): The task of Legal Statute Identification (LSI) is formally defined to automatically identify the relevant statutes given the facts of a case. One of the first steps in the judicial process is finding the applicable statutes/laws based on the facts of the current situation. Manually rummaging through multiple legislation and laws to find out the relevant statutes can be time-consuming, making the LSI task important for reducing the workload and improving efficiency (more details in App. A.5).\n\u2022 Prior Case Retrieval (PCR): When framing a legal document, legal experts (judges and lawyers) use their expertise to cite previous cases to support their arguments/reasoning. Legal experts have relied on their expertise to cite previous cases; however, with an exponentially growing number of cases, it becomes practically impossible to recall all possible cases. Given a query document (without citations), the task of Prior Case Retrieval (PCR) is to retrieve the legal documents from the candidate pool that are relevant (and hence can be cited) in the given query document (details in App. A.6).\n\u2022 Summarization (SUMM): Summarization is a standard task in NLP; however, as mentioned in \u00a71, summarizing legal documents requires legal language understanding and reasoning. The task of summarization involves generating a gist (of a legal document) that captures the critical aspects of the case. We focus on abstractive summarization (more details in App. A.7).\n\u2022 Legal Machine Translation (L-MT): In the Indian legal setting, when a case is transferred (due to re-appeal) from a district court to a High court, the corresponding document (typically in a regional language) needs to be translated to English. Additionally, since a large majority of the Indian population is not proficient in English, High Court / Supreme Court documents often need to be translated from English to Indian languages. In both scenarios, such translations, if done by humans, become a primary reason for delay in administering justice. Machine translation (MT) can augment human translators who could post-edit the translated document rather than translating from scratch. India is a diverse country with multiple languages across different states; the task of Legal Machine Translation (L-MT) attempts to close the language barrier by encouraging the development of systems for translating legal documents from English to Indian languages and vice-versa. Given that many Indian languages are low-resource, MT becomes even more challenging, requiring specialized models for translating legal documents in low-resource Indian languages. We focus on 9 Indian languages, namely, Bengali (BN), Hindi (HI), Gujarati (GU), Malayalam (ML), Marathi (MR), Telugu (TE), Tamil (TA), Punjabi (PA), and Oriya (OR) (details in App. A.8).\nThe tasks in IL-TUR require quite varied skills to solve the problem (Table 2). The skills include a deep understanding of language, the ability to generate legal language, foundational knowledge of law and statutes, application of law to social settings (e.g., decision-making in CJPE and BAIL), and the ability to reason using legal principles. The requirement of such a rich set of skills makes IL-TUR quite challenging; a single model struggles to solve all these tasks, as we observed in our experiments with BERT, LegalBERT, InLegalBERT, GPT3.5 and GPT-4 models (\u00a74).\nHarmonization of Tasks: This resource paper introduces a new benchmark for promoting research and development in the Indian legal system. Since it is a benchmark paper, the aim is to bring domain-specific tasks and datasets under one umbrella so that researchers can compare their models across tasks and with respect to each other. Earlier, no such effort was made for the Indian legal NLP domain. Some of the tasks included in the benchmark already exist; however, there is a lack of standardization across these, e.g., each task and dataset follows its file format, evaluation metric, etc. We have collated all these datasets and converted them to a uniform, JSON-based format so that the community can easily understand and use them. We have also collated all the training scripts for these different tasks together and devised a standard evaluation setup for all these tasks. Further, we have created a website (https://exploration-lab.github.io/IL-TUR/) and a public leaderboard that brings all relevant tasks together. The public leaderboard will further promote transparent and fair comparisons of techniques for each task. Moreover, the leaderboard will lead to the development of more sophisticated models (e.g., GLUE (Wang et al., 2018a) and SuperGLUE (Wang et al., 2019a) benchmarks promoted further research in NLP). Furthermore, to harmonize these tasks, we also conducted experiments with GPT-3.5 and GPT-4 (see \u00a74) on all the tasks (except PCR), which involved converting the data to the desired format for GPT and formulating the prompts and verbalizers. Also, we plan to grow IL-TUR by introducing more new tasks in the future. We would also like to point out that many existing popular NLP benchmarks such as GLUE (Wang et al., 2018a), SuperGLUE (Wang et al., 2019a), as well as legal benchmarks like LEXGLUE (Chalkidis et al., 2022a) mostly comprised of datasets released by prior works. GLUE and LEXGLUE introduced only one new dataset each, whereas SuperGLUE did not have any new datasets.\nAnonymization of datasets: In order to address ethical concerns (also see Ethical Considerations section) and to prevent the model from developing any bias, we anonymized named entities in the dataset of the relevant tasks, namely RR, CJPE, BAIL, LSI and PCR (details in App. A.10).\nRelevance of Tasks to Litigation Process: In general, considering the pipeline of a litigation process for a case, all the tasks in the IL-TUR benchmark help formulate various ways in which automatic legal language processing can augment legal practitioners. Among the tasks, LSI is con-"}, {"title": "Models, Experiments and Results", "content": "We extensively experimented with various models for each proposed task, including transformer-based language models. Table 3 summarizes baseline models and results for all tasks. Due to space limitations, we provide only the top-performing models here; details of experiments (e.g., hyperparameters) and other models are in App. B. In general, results indicate that the tasks are far from being solved, and more research is required. In particular, we experimented with both generic BERT model (Devlin et al., 2019) and legal domain-specific BERT models: LegalBERT (Chalkidis et al., 2020) (BERT pre-trained on EU legal documents), CaseLawBERT (Zheng et al., 2021) (BERT pre-trained on US legal documents), and InLegalBERT (Paul et al., 2023) (BERT pre-trained on Indian legal documents). For L-NER, InLegalBERT (with CRF on top) shows the best performance, possibly because of in-domain data pre-training. For the RR task, vanilla BERT (or other transformers) and Legal-BERT do not work well; hence, RR prediction is posed as a sequence prediction problem (at the sentence level), and the Multi-Task Learning (MTL) model based on BERT developed by Malik et al. (2022) shows the best performance. Since legal documents are long, and BERT has a limitation of 512 tokens in the input, for the CJPE task, hierarchical InLegalBERT (InLegalBERT and BiLSTM on top of that) (Paul et al., 2023) works best. For explanations, we use the occlusion method for finding the sentences leading to the final decision (Malik et al., 2021). But these fall short of expert-annotated important sentences in terms of ROUGE-L and BLEU scores. For BAIL prediction, since the documents are in Hindi, IndicBERT (Kakwani et al., 2020), a BERT model trained on Indian languages, was used. A pre-filtering of salient sentences, followed by IndicBERT, works best (Kapoor et al., 2022). For the LSI task, we conduct experiments with hierarchical LegalBERT and InLegalBERT, along with LeSICIN, a graph-based method proposed by Paul et al. (2022). We observe that LeSICIN outper-"}, {"title": "Conclusion and Future Directions", "content": "This paper presented IL-TUR, a benchmark for Indian Legal Text Understanding and Reasoning. The benchmark has eight tasks requiring different types of legal skills to solve. Results indicate that the tasks are far from solved using state-of-the-art transformer-based models and LLMs. The list of tasks in IL-TUR is not exhaustive, and we plan to expand the list of tasks in the future; for example, we are working on developing foundational tasks like Legal Coreference Resolution (L-Coref) that are required for various applications such as information extraction and knowledge graph creation. Although such tasks have been addressed well in general NLP, our initial experiments show that using SOTA transformer models (which have become part of standard NLP toolkits) do not perform well on legal texts. Due to the usage of specialized terms, new models are needed for the legal domain. On the modeling side, in the future, we plan to develop one model that generalizes and works across all the tasks (e.g., mT5 (Xue et al., 2020) and Multi-task Adapters (Pfeiffer et al., 2020)). Overall, we hope that IL-TUR (along with its leaderboard) and its successive versions would create excitement in the Legal-NLP community and lead to the development of new technologies that could benefit society immensely and facilitate fair access to justice, a fundamental human right."}, {"title": "Limitations", "content": "IL-TUR is a first step towards creating a benchmark for the Indian legal domain, which desperately needs technological solutions. The benchmark is not perfect and has certain limitations. Given the dynamic nature of the legal domain, new cases and precedents keep getting added. Hence, we plan to keep updating IL-TUR in the future. The legal domain is vast and covers various areas such as criminal law, civil law, banking, insurance, etc. In IL-TUR, we could not cover each of the sub-domains in each task as it is a time-consuming and expensive affair to annotate many documents. One of our goals for IL-TUR is to test the cross-area generalization abilities of models; nevertheless, we would expand the datasets of each task in the future. IL-TUR is multi-lingual only concerning the L-MT task. Additionally, the BAIL task is in Hindi. All the High Courts and the Supreme Court in India use English as the official language. Hindi is the prominent language used in the district courts in most north Indian states. Nevertheless, India is a multi-lingual society, and legal models for other languages should also be developed for more tasks in the legal domain. We plan to extend the benchmark in the future and include some more tasks in Indian languages. The main challenge in doing so is a scarcity of legal data in regional languages in digitized formats from lower courts. Datasets of some of the tasks (e.g., LSI) use ML-based models (that may not be perfect) in the dataset creation process (e.g., fact extraction in the case of LSI). Extracting facts manually at a large scale is an expensive and time-consuming effort; in the future, we plan to employ legal professionals and create a more refined dataset. Regarding explainability, at present, we mainly address model explainability in the context of the CJPE task. For discussion regarding other tasks, please refer to App. A.9. Regarding LLM experiments, some of the tasks, such as BAIL and CJPE, require the entire document to be a part of the model's input. Obtaining LLM pre-dictions overall test set samples is challenging in terms of expense and computation. Hence, we evaluated over a small subset, assuming that it is a good proxy of LLM performance. Lastly, the benchmark has only eight tasks. Creating legal tasks is time-consuming and expensive since it requires the help of legal experts. Nevertheless, as explained earlier, IL-TUR is a work in progress, and we will keep growing by adding more tasks. In this work, we presented different models for various tasks; although many of the models (e.g., BERT, GPT) are common across all tasks, in the future, we plan to develop a single model that could solve all the tasks (e.g., mT5) with reasonable accuracy."}, {"title": "Ethical Considerations", "content": "We use publicly available and open-source datasets for the tasks; no copyright is infringed. To the best of our knowledge, five of the proposed tasks (L-NER, RR, LSI, PCR, and Summ) do not have any direct ethical consequences since the proposed tasks are mainly related to information retrieval and summarization. Moreover, the tasks are meant to encourage the development of systems that would lead to streamlining the legal workflow and will not directly affect the life of any personnel.\nFor the LSI task, to prevent any bias in the model, named entities in the dataset were anonymized (details in App. A.10). Similarly, the named entities were anonymized in the RR and PCR datasets. App. A.10 provides more details about various measures and potential risks associated with failure to anonymize legal data. The documents are selected randomly for all tasks to avoid bias towards any entity, organization, or law.\nTwo tasks (CJPE and BAIL) have ethical considerations. Given a large quantum of pending cases in Indian courts, these tasks aim to develop systems that augment judges and not replace them; consequently, the systems are meant to provide recommendations, and a human judge takes the final decision. We follow all the steps as done by Malik et al. (2021); Kapoor et al. (2022) to avoid any bias in the data for these two tasks. For example, we removed cases (documents) related to sensitive issues like rape and sexual violence, and named entities were anonymized.\nNote that we do not endorse the use of the benchmark data for non-research (commercial and real-life) applications, and the primary motivation for creating the IL-TUR benchmark is to consolidate all the research happening in parallel for the Indian Legal domain. Hence, we will release the benchmark and datasets under the Creative Common Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license. Moreover, we believe providing a platform by maintaining a common leaderboard for multiple tasks will advance the field with more transparency and reproducibility."}]}