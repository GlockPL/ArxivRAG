{"title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning", "authors": ["Abhinav Joshi", "Shounak Paul", "Akshat Sharma", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "abstract": "Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/ IL-TUR/) where the research community can upload and compare legal text understanding systems.", "sections": [{"title": "1 Introduction", "content": "Besides several other purposes, legal systems have been established in various countries to ensure, at the very minimum, order and fairness in society and to safeguard fundamental human rights. However, legal systems worldwide struggle with exponentially growing legal cases in various courts. It is even more pronounced in populous countries; e.g., in India, there are about 50 million pending cases in multiple courts at various levels (district, state, federal) (National Judicial Data Grid, 2023). Such a massive backlog of cases goes against the fundamental human right of fair access to justice. Documents in different natural languages are the backbone of various legal processes. Natural Language Processing (NLP) based techniques could be helpful in various legal processes involving fundamental tasks related to information extraction, document understanding, and prediction. This paper introduces IL-TUR, a benchmark for Indian Legal Text Understanding and Reasoning. The purpose of IL-TUR is twofold. First, it aims to foster research in the Legal-NLP (L-NLP) domain and plans to address the pain points associated with processing legal texts (see below); second, it provides a platform for comparing different models and further advancing the L-NLP domain.\nWhy a separate benchmark for the legal domain? The legal text involves natural language but differs from the regular text used to train NLP models. 1) Many of the terms used in legal documents are domain-specific. For example, some words used in everyday language have specialized meanings in legal parlance. The presence of a different lexicon posits a need for specialized NLP tools to handle legal texts. 2) Legal documents are typically very long compared to regular texts. For example, the average length of a legal document from the Supreme Court of India (SCI) is 4000"}, {"title": "2 Related Work", "content": "Over the past few years, L-NLP has been a fertile area for research. Researchers have explored different aspects of the legal domain via various tasks such as Prior Case Retrieval (Joshi et al., 2023; Jackson et al., 2003a), Case Prediction (Malik et al., 2021; Chalkidis et al., 2019; Strickson and De La Iglesia, 2020; Kapoor et al., 2022), Summarization (Moens et al., 1999), Semantic Segmentation of Legal Documents (Malik et al., 2022; Kalamkar et al., 2022b; Bhattacharya et al., 2019), and Information Extraction and Retrieval (Tran et al., 2019; Lagos et al., 2010). On the modeling side, various techniques have been proposed, ranging from classical ML-based methods such as SVM (Al-Kofahi et al., 2001; Jackson et al., 2003b) to recent transformer-based models (Chalkidis et al., 2019; Malik et al., 2021). Researchers have also proposed legal domain-specific language models such as LegalBERT (Chalkidis et al., 2020), CaseLawBERT (Zheng et al., 2021) and InLegalBERT and InCaseLawBERT (Paul et al., 2023). However, legal LLMs have shown limited success and have not demonstrated generalization and transfer learning capabilities (Chalkidis, 2023; Malik et al., 2021; Joshi et al., 2023).\nComparison with Existing Benchmarks: Benchmarks have played a crucial role in the development of better techniques and models in almost every domain, such as computer vision (Deng et al., 2009; Guo et al., 2014; Wu et al., 2013) and reinforcement learning (Laskin et al., 2021; Cobbe et al., 2020; Zhang et al., 2018). Similarly, in the NLP domain, various benchmarks have been proposed, for example, GLUE (Wang et al., 2018a), Super-GLUE (Wang et al., 2019a), XTREME (Hu et al., 2020), CLUE (Xu et al., 2020), GLGE (Liu"}, {"title": "3 IL-TUR: Legal-NLP Benchmark", "content": "Table 2 summarizes various tasks proposed in IL-TUR. The tasks cover multiple aspects of the legal domain and require specialized skills and knowledge to solve them."}, {"title": "3.1 Design Philosophy", "content": "We want to develop technology that enables automated semantic and legal understanding of legal documents and processes. We created IL-TUR with the following principles in mind.\n1) Legal Understanding and World Knowledge: The tasks should cater exclusively to the legal domain. Solving a task should require in-depth knowledge and understanding of the law and its associated areas. Further, the tasks should not be restricted to only classification but should also involve retrieval, generation, and explanation. The proposed tasks address the pain points of processing legal texts (\u00a71). Moreover, solving legal tasks should require knowledge about the law as well as commonsense knowledge and societal norms about the world (e.g., facts in conjunction with socio-economic conditions in a particular case). 2) Difficulty Level: The difficulty level should be such that these are not solvable by a layperson (having minimal knowledge and expertise in legal matters). It ensures that general language learners cannot easily solve the tasks, and the tasks would be sufficiently challenging for the current state-of-the-art models (e.g., LLMs). 3) Language: Since India is a multi-lingual society, the tasks should cater to the most frequent languages used in the courts. We cover tasks in English and 9 other Indian languages. 4) Evaluation: The tasks should be automatically evaluable, and the metrics used should align with human judgments. 5) Public Availability: The data used for the tasks should be publicly available so anyone can use it for research purposes without licensing or copyright restrictions. Further, a leaderboard should be available to compare different systems and models. We release the data via a Creative Common Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license and create a public leaderboard."}, {"title": "3.2 IL-TUR Tasks", "content": "Based on the design philosophy, in this version of IL-TUR, we selected eight different tasks. Table 2 provides a summary of the tasks. We briefly describe the tasks here; details about the dataset and evaluation metrics are provided in App. A.\n\u2022 Legal Named Entity Recognition (L-NER): This is a newly created task in IL-TUR. Formally, given a legal document, the task of Legal Named Entity Recognition is to identify entities (set of 12 entity types), namely, Appellant, Respondent, Judge, Appellant Counsel, Respondent Counsel, Court, Authority, Witness, Statute, Precedent, Date, and Case Number. L-NER is different from the standard NER task; if one were to run a standard NER system on a legal document, the judge, petitioner, and respondent would all be labeled with a \u201cPERSON\" tag. Hence, a separate task is needed to identify the legal named entities in the documents. The standard NER (identifying person/organization/location names) can be done by any non-legal professional/person, but identifying the roles of entities involved in a legal case (L-NER) requires an in-depth understanding of the legal terminologies and the law. Hence, we develop a gold-standard dataset for L-NER with the help of law students (details in A.1). Moreover, the set of legal entities and corresponding definitions are formulated with the help of legal academicians (experts).\n\u2022 Rhetorical Role Prediction (RR): As pointed out earlier, legal documents are typically long (avg. length 4000 words) and highly unstructured, with the legal information spread throughout the document. Segmenting the long documents into topically coherent units (such as facts, arguments, precedent, statute, etc.) helps highlight the relevant information and reduces human effort. These topically coherent units are termed as Rhetorical Roles (RR). Given a legal document, the task of RR prediction involves assigning RR label(s) to each sentence. We focus on 13 RR labels: Fact, Issue, Arguments (Respondent), Argument (Petitioner), Statute, Dissent, Precedent Relied Upon, Precedent Not Relied Upon, Precedent Overruled, Ruling By Lower Court, Ratio Of The Decision, Ruling By Present Court, None. Details about RR labels, definitions, and the dataset are provided in the App. A.2.\n\u2022 Court Judgment Prediction with Explanation (CJPE): Formally, the task of Court Judgment Prediction with Explanation (CJPE) involves predicting the final judgment (appeal"}, {"title": "4 Models, Experiments and Results", "content": "We extensively experimented with various models for each proposed task, including transformer-based language models. Table 3 summarizes baseline models and results for all tasks. Due to space limitations, we provide only the top-performing models here; details of experiments (e.g., hyperparameters) and other models are in App. B. In general, results indicate that the tasks are far from being solved, and more research is required. In par-"}, {"title": "5 Conclusion and Future Directions", "content": "This paper presented IL-TUR, a benchmark for Indian Legal Text Understanding and Reasoning. The benchmark has eight tasks requiring different types of legal skills to solve. Results indicate that the tasks are far from solved using state-of-the-art transformer-based models and LLMs. The list of tasks in IL-TUR is not exhaustive, and we plan to expand the list of tasks in the future; for example, we are working on developing foundational tasks like Legal Coreference Resolution (L-Coref) that are required for various applications such as information extraction and knowledge graph creation. Although such tasks have been addressed well in general NLP, our initial experiments show that using SOTA transformer models (which have become part of standard NLP toolkits) do not perform well on legal texts. Due to the usage of specialized terms, new models are needed for the legal domain. On the modeling side, in the future, we plan to develop one model that generalizes and works across all the tasks (e.g., mT5 (Xue et al., 2020) and Multi-task Adapters (Pfeiffer et al., 2020)). Overall, we hope that IL-TUR (along with its leaderboard) and its successive versions would create excitement in the Legal-NLP community and lead to the development of new technologies that could benefit society immensely and facilitate fair access to justice, a fundamental human right."}, {"title": "Limitations", "content": "IL-TUR is a first step towards creating a benchmark for the Indian legal domain, which desperately needs technological solutions. The benchmark is not perfect and has certain limitations. Given the dynamic nature of the legal domain, new cases and precedents keep getting added. Hence, we plan to keep updating IL-TUR in the future. The legal domain is vast and covers various areas such as criminal law, civil law, banking, insurance, etc. In IL-TUR, we could not cover each of the sub-domains in each task as it is a time-consuming and expensive affair to annotate many documents. One of our goals for IL-TUR is to test the cross-area generalization abilities of models; nevertheless, we would expand the datasets of each task in the future. IL-TUR is multi-lingual only concerning the L-MT task. Additionally, the BAIL task is in Hindi. All the High Courts and the Supreme Court in India use English as the official language. Hindi is the prominent language used in the district courts in most north Indian states. Nevertheless, India is a multi-lingual society, and legal models for other languages should also be developed for more tasks in the legal domain. We plan to extend the benchmark in the future and include some more tasks in Indian languages. The main challenge in doing so is a scarcity of legal data in regional languages in digitized formats from lower courts. Datasets of some of the tasks (e.g., LSI) use ML-based models (that may not be perfect) in the dataset creation process (e.g., fact extraction in the case of LSI). Extracting facts manually at a large scale is an expensive and time-consuming effort; in the future, we plan to employ legal professionals and create a more refined dataset. Regarding explainability, at present, we mainly address model explainability in the context of the CJPE task. For discussion regarding other tasks, please refer to App. A.9. Regarding LLM experiments, some of the tasks, such as BAIL and CJPE, require the entire document to be a part of the model's input. Obtaining LLM predictions overall test set samples is challenging in terms of expense and computation. Hence, we evaluated over a small subset, assuming that it is a good proxy of LLM performance. Lastly, the benchmark has only eight tasks. Creating legal tasks is time-consuming and expensive since it requires the help of legal experts. Nevertheless, as explained earlier, IL-TUR is a work in progress, and we will keep growing by adding more tasks. In this work, we presented different models for various tasks; although many of the models (e.g., BERT, GPT) are common across all tasks, in the future, we plan to develop a single model that could solve all the tasks (e.g., mT5) with reasonable accuracy."}, {"title": "Ethical Considerations", "content": "We use publicly available and open-source datasets for the tasks; no copyright is infringed. To the best of our knowledge, five of the proposed tasks (L-NER, RR, LSI, PCR, and Summ) do not have any direct ethical consequences since the proposed tasks are mainly related to information retrieval and summarization. Moreover, the tasks are meant to encourage the development of systems that would lead to streamlining the legal workflow and will not directly affect the life of any personnel.\nFor the LSI task, to prevent any bias in the model, named entities in the dataset were anonymized (details in App. A.10). Similarly, the named entities were anonymized in the RR and PCR datasets. App. A.10 provides more details about various measures and potential risks associated with failure to anonymize legal data. The documents are selected randomly for all tasks to avoid bias towards any entity, organization, or law.\nTwo tasks (CJPE and BAIL) have ethical considerations. Given a large quantum of pending cases in Indian courts, these tasks aim to develop systems that augment judges and not replace them; consequently, the systems are meant to provide recommendations, and a human judge takes the final decision. We follow all the steps as done by Malik et al. (2021); Kapoor et al. (2022) to avoid any bias in the data for these two tasks. For example, we removed cases (documents) related to sensitive issues like rape and sexual violence, and named entities were anonymized.\nNote that we do not endorse the use of the benchmark data for non-research (commercial and real-life) applications, and the primary motivation for creating the IL-TUR benchmark is to consolidate all the research happening in parallel for the Indian Legal domain. Hence, we will release the benchmark and datasets under the Creative Common Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license. Moreover, we believe providing a platform by maintaining a common leaderboard for multiple tasks will advance the field with more transparency and reproducibility."}]}