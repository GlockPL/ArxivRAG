{"title": "SPformer: A Transformer Based DRL Decision Making Method for Connected Automated Vehicles", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Xingyu Hu", "Yixia Lu"], "abstract": "In mixed autonomy traffic environment, every decision made by an autonomous-driving car may have a great impact on the transportation system. Because of the complex interaction between vehicles, it is challenging to make decisions that can ensure both high traffic efficiency and safety now and futher. Connected automated vehicles (CAVs) have great potential to improve the quality of decision-making in this continuous, highly dynamic and interactive environment because of their stronger sensing and communicating ability. For multi-vehicle collaborative decision-making algorithms based on deep reinforcement learning (DRL), we need to represent the interactions between vehicles to obtain interactive features. The representation in this aspect directly affects the learning efficiency and the quality of the learned policy. To this end, we propose a CAV decision-making architecture based on transformer and reinforcement learning algorithms. A learnable policy token is used as the learning medium of the multi-vehicle joint policy, the states of all vehicles in the area of interest can be adaptively noticed in order to extract interactive features among agents. We also design an intuitive physical positional encodings, the redundant location information of which optimizes the performance of the network. Simulations show that our model can make good use of all the state information of vehicles in traffic scenario, so as to obtain high-quality driving decisions that meet efficiency and safety objectives. The comparison shows that our method significantly improves existing DRL-based multi-vehicle cooperative decision-making algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomus-driving vehicles are playing an increasingly important role in modern transportation systems. At present and for a long time to come, autonomous and human driving vehicles (HDVs) will coexisit both in urban and highway traffic environment. Multi-vehicle collaborative decision-making will play a crucial role in mixed autonomy traffics. It has advantages that single-vehicle autonomous driving cannot match in terms of safety, traffic efficiency, driving experience, energy conservation, and environmental protection [1]. However, due to the dynamic state information and complex interactions of traffic participants, high-quality collaborative driving decision-making is very challenging. Therefore, to develop a good collaborative decision-making algorithm, we should effectively represent the interaction between agents and make full use of it in decision making process.\nDeep reinforcement learning is an effective method to solve multi-agent decision-making problems. Deep neural networks helps modeling and understanding complex environments and interaction of agent. Reinforcement learning algorithms allow agents to gain experience through interaction with the surrounding environment and other traffic participants, and continuously improve their decision-making ability. Connected automated vehicles usually need to weigh between competition and collaboration to meet their own driving purposes and overall traffic efficiency requirements. By reasonably setting reward functions and formulating exploration and exploitation schemes, DRL algorithms can help agents learn high-performance strategies.\nThis paper introduces SPformer, a multi-vehicle collaborative decision-making method based on DRL and transformer architecture. The framework adopts transformer encoders as part of the DRL algorithm. The input of the network is the State sequence of all vehicles in the traffic scenario and the output is the multi-vehicle joint driving Policy. The main contribution of this paper can be summarized as follows:\n1) An effective multi-vehicle collaborative decision-making framework based on deep reinforcement learning is proposed. It can effectively solve the lateral-longitudinal joint decision making of CAVs from the perspective of mesoscopic traffic.\n2) A learnable policy token are introduced as the learning medium of policy and an intuitive physical positional encoding is designed to improve the algorithm's per-formance. SPformer can well extract the interactive information between agents, thereby speeding up the learning process of DRL algorithm and improving the quality of learned policy. We verified the algorithm in on-ramp tasks and compared it with the state of the art multi-vehicle decision making algorithms. The results show that our methods have better performance than other deep reinforcement learning algorithms in terms of safety and efficiency."}, {"title": "II. RELATED WORKS", "content": "Multi-vehicle decision making: Multi-vehicle decision-making aims to provide safer and more efficient driving strategies for autonomous driving systems. Early multi-vehicle cooperative decision-making researches can be traced back to the study of longitudinal platooning such as ACC and CACC [2]. These studies use limited on-board sensors, and the objective is mainly concerned with the string stability in one dimention. Optimization-based planning methods such as mixed integer optimization and dynamic priority allocation can also solve collaborative decision-making problems to some extent [3]\u2013[5], but it is difficult to guarantee the speed and quality of the solution at the same time in large-scale collaborative driving tasks.\nWith the development of artificial intelligence, V2X com-munication, and edge computing technologies, CAVs can make more reasonable decisions in a wider spatial dimension and a longer time range [6]\u2013[9]. The application of deep learning in autonomous driving impels researchers to solve multi-vehicle decision-making problems with DL methods. A. J. M. Muzahid et al. [10] systematically summarized the multi-vehicle cooperative collision avoidance technol-ogy of CAVs, and proposed a multi-vehicle cooperative perception-communication-decision framework based on deep reinforcement learning. Y. Zheng et al. [11] modeled the multi-vehicle decision-making of urban multi-intersections as a predator-pray problem, and used deep reinforcement learning to establish a multi-agent decision-making method where the agents show collaborative behavior patterns far beyond humans. The DL based multi-vehicle decision making algorithms can effectively deal with complex traffic situations, but refined modeling for collaborative interaction is needed for better performance.\nIn addition, game theory, Monte Carlo Tree Search algo-rithm(MCTS), etc. are also used or combined with deep learning methods to solve multi-vehicle decision-making problems recently [12]\u2013[15]. These methods have shown great potential in solving problems in complex multi-agent systems.\nSequencial interaction modeling: Natural language pro-cessing (NLP), recommendation systems, time series analysis, etc. all need to properly handle the interaction between sequencial inputs [16]\u2013[18]. RNN and RNN-based LSTM are often used to construct complex sequence interaction models of time series [19]. In terms of spatial sequences, A. Alexandre et al. [20] proposed social LSTM to predict pedestrian trajectory, and designed a convolutional social pooling to connect the spatial close LSTMs so that infor-mation can be shared with each other, which represents the space interaction of agent in complex scenes. Graph neural networks (GNN) introduce graphs to represent the structural relationships between sequences, which are used by many researchers to model the interactions between vehicles in autonomous driving studies. S. Chen et al. [21] proposed a DRL model combined with GNN to make efficient and safe multi-vehicle cooperative lane change. D. Xu et al. [22] established a multi-vehicle GRL algorithm to realize the cooperative control of vehicles in highway mixed traffic, the graph attention mechanism significantly improves the decision efficiency. In GNN, however, the propagation of information is usually carried out through the adjacency relationship on the graph, which makes long-distance information dissemination difficult.\nTransformer is a deep learning architecture with multi-head attention mechanism [23]. It has achieved great success in the field of NLP and has been applied to trajectory prediction [24] and decision making [25], [26] problems. H. Liu et al. [27] implemented two Transformer blocks for scene"}, {"title": "III. PROBLEM STATEMENT", "content": "This paper aims to solve the collaborative decision-making problem of connected automated vehicles through DRL algorithms. The scenario is mixed traffic where CAVs and HDVs coexist. It is assumed that the CAVs has the ability of global traffic state perception and information sharing in the area of interest. In fact, it is not difficult to realize with the help of roadside facilities and V2X technology. We model the cooperative driving problem from the perspective of mesoscopic traffic flow, considering the lane change and logitudinal acceleration of vehicles, but do not discuss about how these behaviors are realized in terms of vehicle dynamic. This work focuses on the development and verification of interactive collaborative decision-making algorithms between vehicles, and currently does not consider factors such as communication delay and sensing information uncertainty."}, {"title": "IV. APPROACH", "content": "State representation: The vehicle's state consists of its individual dynamic characteristics, traffic environment, driving intention, and the relative states of surrounding vehicles. Specifically, vehicle' individual dynamic charac-teristics include the current lateral and longitudinal position, speed, and acceleration, The traffic environment includes road information, key road element characteristics (locations of intersections, ramps, etc.), The driving intention is the destination of the vehicle in the concerned area. The relative states of surrounding vehicles takes into account all of the other vehicle's relative information to the current vehicle. In this study, we use a multi-modal coupled matrix to represent the state of the vehicle which consists of the informations mentioned above.\nThe backbone of the state matrix is the rasterized road area. Take the off-ramp scene shown in Fig.2 as an example. $n_{lanes}$ is the number of main road lanes and $l_{main}$ is the length. State matrix of the i-th vehicle $S_{i} \\in R^{(n_{lanes} +1)\\times l_{main}}$, and,\n$S_{i} = S_{position,i} + S_{speed,i} + S_{intention,i} + S_{-i}\n= S_{self,i} + S_{-i}$ (1)\nwhere $S_{position, i}, S_{speed,i}, S_{intention,i}, S_{i}$ is the ego vehicle's position matrix, velocity field matrix, intention matrix, and relative vehicle information matrix respectively. The sum of the first three is denoted as $S_{self,i}$.\nThe position matrix $S_{position, i} = I_{ego} \\cdot M_{position, i}$, where $M_{position, i}$ is one hot matrix, where occupied by the ego vehicle is 1, and $I_{ego}$ is the position state factor.\nA two-dimensional Gaussian potential field is used for velocity representation. In the above scenario, the speed state matrix of the red vehicle\n$S_{speed, i} (r, c) = I_{potential}V_{ie}^{-\\frac{(r-x_{i})^{2}}{2\\sigma_{x}^{2}}-\\frac{(c-y_{i})^{2}}{2\\sigma_{y}^{2}}}$ (2)\nwhere r and c are the row and column index of the velocity state matrix, $I_{potential}$ is the speed state factor, $\\sigma_{x}, \\sigma_{y}$ are the longitudinal and lateral speed state decay factors of the vehicle respectively.\nA single row matrix $S_{intention,i}$ is added to represent the location of the target ramp. $S_{intention,i}$ is initialized as an all-0 matrix and make $x_{int}(r, c) = I_{intention}$, if $(x_{int} - int_{range}) < r < x_{int}$ and c = 3. where $I_{intention}$ is the intention state factor, $int_{range}$ is the range of the vehicle's intention area.\nFor vehicle i, $S_{-i}$ is the weighed sum of the state matrices of all vehicles except itself, i.e.,\n$S_{-i} = \\sum_{j\\neq i} wS_{self, j}$ (3)\nSo far, the final expression of $S_{i}$ is obtained. Fig.3 shows an example state representation for a single vehicle."}, {"title": "Action space", "content": "We consider both the lateral and longitudinal behaviors of the vehicle. Longitudinal actions include accel-erating, speed keeping and decelerating, and lateral actions consist of left lane changing, lane keeping and right lane changing. Considering that longitudinal and lateral actions can be performed at the same time, there are 9 elements in the joint action space. $A = \\{(a_{lon}, a_{lat}) | a_{lon} \\in A_{lon}, a_{lat} \\in A_{lat}\\}$, where $A_{lon} = \\{AC, SK, DC\\}$, and $A_{lat} = \\{LC, LK, RC\\}$.\nReward function: Our work aims at the driving efficiency and safety of the global traffic. The reward function is designed as shown in Equation 4, the implementation details can be found in paper [29].\n$R = w_{1}R_{speed} + w_{2}R_{intention} + w_{3}P_{collision} + w_{4}P_{LC}\n= \\frac{1}{N}(\\sum_{i=1}^{N}\\frac{V_{i}}{U_{max}} + w_{2} \\cdot N_{onramp} + w_{3} \\cdot N_{collision} + w_{4} \\cdot N_{LC})$ (4)\nwhere N is the number of vehicles in the scene (including HDVs and CAVs), $N_{onramp}$ is the vehicle passing through intention area at the previous time step and aiming for the ramp, $N_{collision}$ is the number of collisions, and $N_{LC}$ is the number of frequently lane-changing vehicles.\nThere are two main differences between our work and the reference work in the calculation of rewards :\n1) For speed reward, we take into account all vehicles' speed while most previous work only considered the average speed of CAVs. It has been proved that the be-havior of autonomous vehicles can affect other vehicles in the traffic environment.\n2) To encourage vehicles to explore more diverse driving strategies, we only set intention rewards in a small area close to the ramp as shown in Fig.2, and no punishment related to intention is set when the vehicles are in other area."}, {"title": "B. Interactive feature extraction method based on Trans- former", "content": "In this paper, the transformer encoder is used to extract the interaction features of vehicles. We introduce policy-token as the learning medium of multi-agent joint strategy. The multi-head self-attention mechanism of the transformer helps to extract the interaction information between vehicles. In ad-dition, we integrated physical position encoding into the basic transformer, which makes the network more sensitive to the vehicles' location and effectively improves the performance of the algorithm.\nTransformer encoder with policy-token: Inspired by the research in NLP and CV [30]\u2013[32], we introduce a learnable policy token as the policy learning medium. Policy token enables the network to have perception of global traffic state, it has the same dimension as the vehicle's state feature. $x \\in R^{H\\times W}$ is the input state matrix of each vehicle, it is reshaped to 1 \u00d7 HW and then embedded to 1 \u00d7 D. $X_{policy} \\in R^{1\\times D}$ is the policy token.\nWe design the transformer encoder based on ViT [31]. The encoder consists of multi-head attention layer (MHA) and multi-layer perceptron(MLP) layer alternately. Layer-norm(LN) is added before each block, and residual connection is performed after each MHA and MLP. The architecture is shown in the right side of Fig.1 and can be summarized as follows:\n$z_{0} = [x_{policy}; x_{1}; x_{2};\u00b7\u00b7\u00b7 ;x_{N}] + E_{pos}$\n$z'_{l} = MHA (LN (z_{l-1})) + z_{l-1}, l = 1 . . . L$\n$z_{l} = MLP (LN (z'_{l})) + z'_{l}, l = 1...L$\n$y = LN (z_{L})$ (5)\nwhere $E \\in R^{(H\\cdot W)\\times D}, E_{pos} \\in R^{(N+1)\\times D}$.\nWe use the standard qkv self-attention to caculate the attention matrix. For the input sequence z \u2208 $R^{N\\times D}$,\n$[q, k, v] = z \\cdot U_{qkv}$\n$Att(z) = softmax (\\frac{q\\cdot k^{T}}{\\sqrt{D_{head}}}) v$ (6)\nwhere $U_{qkv} \\in R^{D\\times 3D_{head}}$, and $Att(z) \\in R^{N\\times N}$.\nThe multi-head self-attention is the expansion of self-attention, and k self-attention matrices are calculated simul-taneously. The results are concatenated into a multi-head attention matrix.\n$MHA(z) = [Att_{1}(z); Att_{2}(z);\u00b7\u00b7\u00b7 ; Att_{k}(z)] W_{o}$ (7)\nwhere $W_{o} \\in R^{k\\cdot D_{head}\\times D}$.\nIt can be seen that the policy token and the embedded states are fed into the transformer block together, and the final output result is used to derive the policy. The transformer encoder processed a total of N + 1 tokens of dimension D, and only the output of the policy token is used to derive the policy. This architecture forces the exchange of information between vehicles' state feature and policy token.\nPhysical positional encoding: In NLP studies, word order is of great importantance. Vaswani et al. give the classic position encoding method of sine-cosine alternation. Researchers have improved the PE method according to different tasks, and the performance of transformer has been significantly improved [33]\u2013[36]. For driving tasks, location information is of natural importance. Although location feature is contained in the vehicle states input, it will be diluted by other information such as speed, intention, and maps. To our knowledge, after steps of feature extraction, the superposition of physical location encoding can strengthen location features and improve the network performance.\nIn this paper, we simply refer to the original transformer, and generate PPE in the form of sines and cosines combi-nation. The map in the area of interest are discretized into $N_{pos}$ physical positions. For the physical position $pos_{ph} \\geq 1$, the PPE is calculated according to Equation 8.\n$PPE_{2k}(i, pos_{ph}) = sin(pos_{ph}/(2N_{pos}))^{2k/D})$\n$PPE_{2k+1}(i, pos_{ph}) = cos(pos_{ph}/(2N_{pos}))^{2k/D})$ (8)\nwhere 2k and 2k + 1 are index of PE vector, D is the model dimention."}, {"title": "V. EXPERIMENT", "content": "We carried out the verification of the algorithm on the Flow platform [37]. Using DQN as the basic reinforcement learning algorithm, the performance of different deep learning networks is compared."}, {"title": "A. Simulation environment and experiment settings", "content": "We use Flow to build simulation scenarios and verify the algorithm. Flow is a computational framework for deep RL and control experiments for traffic microsimulation. It provides a set of basic traffic control scenarios and tools for designing custom traffic scenarios. In the simulation, the built-in EIDM model of the framework is implemented as HDVs [38]. In order to maximize the ability of the algorithm, all active safety detection of the vehicle controlled by the reinforcement learning algorithm are removed during the training process.\nThe simulation scenario is the on-ramp scenario shown in Fig.2. Considering a one-way three-lane main road with length of 250m, the exit ramp is 200m away from the start. The agents in the case include 2 CAVs and 4 HDVs. 2 CAVS are trageted to enter the ramp, and HDVs are set to drive along the main road until the simulation ends. At the beginning of the simulation, all vehicles are generated at the initial position with the initial speed. Episode ends when the first vehicle in the environment reached the end of main road. Specified parameters of the agent are shown in Table I. The initial position and lane of CAVs are marked by *."}, {"title": "B. RL agent implementation details", "content": "We use the classical deep reinforcement learning algorithm DQN to verify the performance of the proposed method. DQN is a value-based reinforcement learning algorithm. The Q-Learing algorithm maintains a Q-table, and uses the table to store the return obtained by taking action a under each state s, that is, the state-value function Q(s, a). But in many cases, the state space faced by reinforcement learning tasks is continuous, and there are infinite states. In this case, the value function can no longer be stored in the form of tables. To solve this problem, we can use a function $Q(s, a; \\theta)$ to approximate the action-value Q(s, a), which is called Value Function Approximation. We use neural networks to generate this function $Q(s, a; \\theta)$, called Deep Q-network, $\\theta$ is a parameter for neural network training. DQN introduces the neural network in deep learning, and uses the neural network to fit the Q table in Q-learning, which solves the problem of dimension disaster.\nFor single-agent DQN, we update the neural network weights $\\theta$ by minimizing the loss function:\n$L(s,a | \\theta) = (r + \\gamma max_{a'} Q (s', a' |\\theta) \u2013 Q (s,a | \\theta))^{2}.$ (9)\nIn our work, we use a single neural network to simultane-ously predict the Q values of multiple agents. The MADQN architecture is discussed in [39]. Since the reward function is designed to be the mean value of the current state values of all agents, the Q value in this condition should be the discounted sum of the state values of all agents. To this end, we design the following loss function:\n$L(s,a | \\theta) = (r + \\frac{1}{N_{CAV}}\\sum_{i=1}^{N_{CAV}} \\gamma max_{a'} Q_{i} (s', a' |\\theta) \u2013 \\frac{1}{N_{CAV}}\\sum_{i=1}^{N_{CAV}} Q_{i} (s, a |\\theta))^{2}.$ (10)\nSPformer is applied to reinforcement learning agents. The overall structure of the network is described by Formula 5, where each MLP contains two fully connected layers with Gaussian error linear unit(GELU) between, and the input is state vector of size 6 \u00d7 1000. The specific parameters of SPformer are shown in Table II. The implementation details of DQN are shown in Table.III."}, {"title": "C. Compared Methods", "content": "We compare the performance of convolutional neural network, graph neural network and SPformer in DQN algorithm. The convolutional neural network has a kernel with size of 4 \u00d7 4, followed by a two-layer fully connected network, and rectified linear unit(ReLU) is added after each layer. The implementation details of the graph neural network are shown in paper [40]."}, {"title": "Average velocity (Velo.)", "content": "The mean value of average velocity(in m/s) of all vehicles per episode."}, {"title": "E. Results and Comparasion", "content": "During the training process, the curves of the average traffic state value and the number of collisions are shown in Fig.5 and Fig.6. We use the rule-based approach as a baseline for comparison, which can represent the general level of human drivers.\nIn the early stage of training, because the agent has a high exploration rate and does not have safety-related experience, the number of collisions is large. This leads to the overall performance of the DRL algorithm agent worse than the rule-based method."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In summary, this research proposed SPformer, a DRL-based multi-vehicle collaborative decision-making method, which provides an effective solution to multi-vehicle collabo-rative lateral and longitudinal joint decision-making problem. SPformer uses policy-token as a learning medium for multi-vehicle driving strategies and integrates an intuitive physical positional encoding. Policy token can prompt the network to obtain a global perception of the traffic state, and physical positional encoding enhances the vehicle location information that is crucial to the quality of decision-making. Therefore, SPformer can effectively improve multi-vehicle cooperative driving strategy learned by DRL algorithms. We tested the performance of SPformer in the on-ramp scenario. Compared with CNN and GNN networks, SPformer have obvious advantages in strategy learning speed and quality.\nThe future work will focus on improving the performance of cooperative driving algorithms in large-scale scenarios. Although the current algorithm has good interactive decision-making performance, it is difficult to achieve excellent per-formance in cases with large number of vehicles and random traffic flow, where more diverse cooperative behavior patterns can be found. In addition, physical positional embedding with higher dimension, new architectures combined with game theory and MCTS, and more efficient collaborative state representation methods are to be verified in more complex CAV decision making problems."}]}