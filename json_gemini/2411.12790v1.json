{"title": "Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models", "authors": ["Zhen Zeng", "Leijiang Gu", "Xun Yang", "Zhangling Duan", "Zenglin Shi", "Meng Wang"], "abstract": "Knowledge editing aims to efficiently and cost-effectively correct inaccuracies and update outdated information. Recently, there has been growing interest in extending knowledge editing from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs), which integrate both textual and visual information, introducing additional editing complexities. Existing multimodal knowledge editing works primarily focus on text-oriented, coarse-grained scenarios, failing to address the unique challenges posed by multimodal contexts. In this paper, we propose a visual-oriented, fine-grained multimodal knowledge editing task that targets precise editing in images with multiple interacting entities. We introduce the Fine-Grained Visual Knowledge Editing (FGVEdit) benchmark to evaluate this task. Moreover, we propose a Multimodal Scope Classifier-based Knowledge Editor (MSCKE) framework. MSCKE leverages a multimodal scope classifier that integrates both visual and textual information to accurately identify and update knowledge related to specific entities within images. This approach ensures precise editing while preserving irrelevant information, overcoming the limitations of traditional text-only editing methods. Extensive experiments on the FGVEdit benchmark demonstrate that MSCKE outperforms existing methods, showcasing its effectiveness in solving the complex challenges of multimodal knowledge editing.", "sections": [{"title": "1. Introduction", "content": "Traditional knowledge editing primarily strives to update and correct the knowledge in Large Language Models (LLMs) to ensure their accuracy and reliability. Many works, e.g., , have proposed various knowledge editing methods for LLMs, which have shown strong performance in refining model knowledge. Recently, there has been a growing interest in extending knowledge editing to Multimodal Large Language Models (MLLMs) . MLLMs enhance the capabilities of traditional LLMs by incorporating multiple modalities, such as text and images, allowing them to process and generate content that integrates both textual and visual information . In MLLMs, errors can arise not only from the language module but also from the visual components or the interactions between modalities. As a result, knowledge editing in MLLMs presents unique challenges that go beyond the complexities found in LLMs.\nTo advance the field, some pioneered works have introduced multiple multimodal knowledge editing benchmarks. However, these benchmarks still primarily focus on text-oriented knowledge editing involving coarse-grained visual understanding, only considering single entities within images. In such settings, editing methods do not require access to the visual module; simply editing the language model suffices (e.g., by replacing the word \"bird\" with \"kite\"). As a result, knowledge editing methods for LLMs tend to perform well within these settings. In con-"}, {"title": "3. Fine-Grained Visual Knowledge Editing", "content": "Building upon SERAC , we propose a Multimodal Scope Classifier-based Knowledge Editor (MSCKE) framework, as illustrated in Fig. 2. Structurally, our MSCKE framework comprises the following four major components: the Multimodal Edit Memory, the Multimodal Scope Classifier, the Base Multimodal Model (fbase), and the Counterfactual Multimodal Model (fcfr).\nDuring the editing process, MSCKE does not modify the base model parameters. Instead, it stores the editing examples in the MultiModal Edit Memory. The MultiModal Edit Memory is a model-agnostic component designed to accept and store the knowledge that needs to be updated, represented as edit examples. These examples specify the precise modifications or corrections to the model's knowledge. By maintaining these edit examples separately, MSCKE can reference them when necessary without altering the base model directly.\nWhen receiving a new input, the MultiModal Scope Classifier evaluates the relevance of the input to the stored editing examples based on visual and textual information. It then decides whether to invoke the edited information or rely on the base model's response. This ensures that edits are applied appropriately to relevant inputs, while unrelated knowledge remains stable. The MultiModal Scope Classifier is defined as $f_{cls}(i_{test}, t_{test}, i_e, t_e, Y_e)$.\nThe index k* of the most similar editing example and its corresponding similarity \\rho are determined as follows:\n$k^* = arg\\min_k f_{cls}(i_{test}, t_{test}, i_k, t_k^e, y_k^e)$, (1)\n$\\rho = f_{cls}(i_{test}, t_{test}, i_k, t_k^e, y_k^e)$, (2)\nwhere $\\rho \\in [0,1]$. Inputs with similarity less than 0.5 are considered out-of-scope samples, while those with similarity equal to or greater than 0.5 are regarded as in-scope samples. The final output of the model is:\n$Y_{test} = \\begin{cases} f_{base}(i_{test}, t_{test}), & \\rho < 0.5, \\\\ f_{cfr}(i_e, t_e, Y_e, i_{test}, t_{test}), & \\rho \\ge 0.5, \\end{cases}$ (3)"}, {"title": "3.1. Problem Formulation", "content": "Fine-grained visual knowledge editing (FGVEdit) task focuses on updating specific and detailed knowledge within MLLMs that process both visual and textual information. Unlike other multimodal knowledge editing tasks, which treat an image as an entity and perform coarse-grained edits that may overlook complex visual details, FGVEdit aims to modify knowledge related to specific entities and relationships involving multiple interactive elements within complex images.\nFormally, let $f_\\theta$ denote a pre-trained MLLM with parameters $\\theta$ that can generate outputs based on combined image-text inputs. An example of the model's editing is represented as $(i_e, t_e, Y_e)$, where $i_e$ denotes an image containing multiple entities, $t_e$ is the textual prompt, and $Y_e$ represents the expected output for the input $(i_e, t_e)$. The model's original output is given by $Y_o = f_\\theta(i_e, t_e)$ and $Y_o \\neq Y_e$.\nThe objective of the FGVEdit task is to update the model's knowledge so that it accurately reflects $Y_e$ when queried for specific fine-grained details, without affecting its performance on unrelated content. We define a knowledge editing operation $\\mathcal{E}$ to update the model parameters as $\\theta_e = \\mathcal{E}(\\theta, i_e, t_e, Y_e)$. After applying $\\mathcal{E}$, the updated model $f_{\\theta_e}$ should be capable of generating the corrected output for the specific input as $Y_e = f_{\\theta_e}(i_e, t_e)$.\nFine-grained visual editing scope. For the fine-grained knowledge defined by the editing example $(i_e, t_e, Y_e)$, there exists a fine-grained editing scope $S(i_e, t_e, Y_e)$. This editing scope is determined by both visual and textual information. For in-scope inputs on the same image, represented as $(i_e, t_{in}) \\in S(i_e, t_e, Y_e)$, the original model's output is $Y_{in} = f_\\theta(i_e, t_{in})$, and the edited model is required to produce a correction as $Y_{in} = f_{\\theta_e}(i_e, t_{in})$. For out-of-scope inputs on the same image, represented as $(i_e, t_{out}) \\notin S(i_e, t_e, Y_e)$, the original model's output is $Y_{out} = f_\\theta(i_e, t_{out})$, and the edited model maintains the original output as $Y_{out} = f_{\\theta_e}(i_e, t_{out})$."}, {"title": "3.3. Multimodal Scope Classifier", "content": "Unlike the text editing scope classifier in SERAC, we have developed a new multimodal scope classifier within the MSCKE framework, designed to integrate both visual and textual modalities. This allows the classifier to capture fine-grained similarities between input and editing examples. Such integration is essential, as relying solely on textual semantics may fail to capture true similarity when $t_e$ and $t_{test}$ refer to different entities related to $i_e$ and $i_{test}$. By combining image and text information, our multimodal scope classifier enables a more comprehensive evaluation of the relevance between the query and editing examples. For both images and texts, we first map them into a unified feature space:\n$h_i^e = A_i(E_i(i_e)), \\quad h_i^{test} = A_i(E_i(i_{test})),$ (4)\n$h_t^e = A_t(E_t(t_e, Y_e)), \\quad h_t^{test} = A_t(E_t(t_{test})),$ (5)\nin which $E_i(\\cdot)$ and $E_t(\\cdot)$ are encoders for images and texts, respectively, which extract high-level features from the inputs. The Alignment modules $A_i(\\cdot)$ and $A_t(\\cdot)$ project the encoded features into a shared multimodal space, ensuring that the image and text representations are compatible for fusion. To simplify the process, we utilize a pre-trained CLIP model for fine-tuning, which is capable of extracting aligned features from both images and text. We then compute the image-text fused features:\n$Z_e = Fusion(h_i^e, h_t^e),$ (6)\n$Z_{test} = Fusion(h_i^{test}, h_t^{test}),$ (7)\nin which the $Fusion(\\cdot, \\cdot)$ function combines the visual and textual features to capture the cross-modal interactions. To simplify the training process, we employ the dot-product attention mechanism to extract the portions of the image that are relevant to the text. Finally, the similarity is calculated from the fused features:\n$\\rho = Sim(Z_e, Z_{test}),$ (8)\nwhere $Sim(\\cdot, \\cdot)$ is a similarity function, such as cosine similarity or a learned distance metric, which quantifies the relevance between the query and the editing example in the fused feature space. We would highlight that our multi-"}, {"title": "4. FGVEdit Benchmark", "content": "Specificity. We introduce a new metric, Specificity, specifically designed to evaluate fine-grained visual editing performance. Previous multimodal knowledge editing metrics treat an image as single entity and fail to account for the multiple entities present in the same image. These metrics primarily assess the editing method's impact on the language model of MLLMs, rather than the nuanced interactions within the image itself. In contrast, Specificity, denoted as $M_{specificity}$, is capable of evaluating multiple entities and their interactions within the same image. To capture the different relationships between entities in an image, $M_{specificity}$ is computed by two components: $M_{in}$ and $M_{out}$. $M_{in}$ evaluates the model's ability to respond correctly to questions within the visual scope of the editing entity, while $M_{out}$ assesses the model's performance on questions outside the visual scope of the editing entity. These components are defined as follows:\n$M_{specificity} = \\frac{1}{2} [M_{in} + M_{out}],$ (12)\n$M_{in} = \\mathbb{E}_{(i_e, t_{in}, Y_{in}) \\sim D_{in}} [f_{\\theta_e}(i_e, t_{in}) = Y_{in}],$ (13)\n$M_{out} = \\mathbb{E}_{(i_e, t_{out}, Y_{out}) \\sim D_{out}} [f_{\\theta_e}(i_e, t_{out}) = f_{\\theta}(i_e, t_{out})],$ (14)\nwhere $D_{in}$ refers to dataset of in-visual-scope data, $D_{out}$ represents dataset of out-of- visual-scope data.\nReliability. The reliability of the model refers to its ability to change its prediction from the original output $Y_o$ to the desired output $Y_e$ when provided with an image and text input. This is a key objective in knowledge editing. We measure reliability as follows:\n$M_{rel} = \\mathbb{E}_{(i_e, t_e, Y_e) \\sim D_{edit}} [[f_{\\theta_e}(i_e, t_e) = Y_e]]$ (15)\nwhere $D_{edit}$ denotes to the dataset of Reliability.\nLocality. Locality measures the model's stability, ensuring that it produces consistent outputs for data outside the editing scope before and after knowledge editing. We measure locality as follows:\n$M_{loc} = \\mathbb{E}_{(t_l, y_l) \\sim D_{loc}} [[f_{\\theta_e}(t_l) = f_{\\theta}(t_l)]]$ (16)\nwhere $t_l$ represents question out of editing scope, $y_l$ is the corresponding label, $D_{loc}$ refers to dataset of Locality.\nGenerality. In addition to accurately changing the prediction from the original $Y_o$ to the desired $Y_e$, the edited multimodal model should also adapt its predictions within the editing scope, such as for rephrased questions. We define generality as follows:\n$M_{gen} = \\mathbb{E}_{(t_r) \\sim D_{gen}} [[f_{\\theta_e}(i_e, t_r) = f_{\\theta}(i_e, t_e)]]$ (17)\nwhere $t_r$ represents the rephrased question. $D_{gen}$ refers to dataset of Generality."}, {"title": "4.2. Dataset Construction", "content": "We select Visual Question Answering (VQA) as the evaluation task and use VQAv2 as the foundation for constructing our FGVEdit dataset. VQAv2 was originally designed to assess a model's ability to answer open-ended questions about images. It contains over 1 million question-answer pairs, each associated with an image, spanning a broad array of topics and requiring diverse reasoning capabilities.\nSpecificity dataset construction. We select the first question of each image as the classification criterion. Additionally, it also serves as the editing sample for the calculation of reliability. To categorize the remaining questions, we manually construct two prompts and apply them twice. First, using the classification criterion and the corresponding image, we request GPT-40-mini with a prompt \"Using the provided image, analyze whether classification criterion logically entails this question, specifically, whether a change in Answer 1 would impact Answer 2. Please make a simple judgment (Yes, No, Maybe), and your response should not contain any other characters.\", to classify the remaining questions, assessing whether the criterion logically entails the other questions. The results of this first classification yield two lists: In-scope samples, representing questions logically entailed by the first question, and Out-of-scope samples, representing questions that are not logically entailed by it. Next, based solely on the classification criterion, we request GPT-40-mini again with a prompt \"analyze whether classification criterion logically entails this question, specifically, whether a change in Answer 1 would impact Answer 2. Please make a simple judgment (Yes, No, Maybe), and your response should not contain any other characters.\", to classify the questions in In-scope samples and Out-of-scope samples, generating the final results: \"hard in-visual-scope\" for questions within the visual context of the image and \"hard out-of-visual-scope\" for questions outside the visual context.\nLocality and Generality dataset construction. For the Locality dataset, we follow the approach in MMEdit and use the NQ dataset , a benchmark for model stability, which includes question-answer pairs that fall outside the editing scope. For the Generality dataset, we request GPT-40-mini with a prompt like \"Please rewrite the following question differently. Do not include the original question\" and assess whether the model can accurately respond to the generated questions within the editing scope."}, {"title": "4.3. Data Statistics", "content": "For each sample, we generate instances of specificity, locality, and generality data to ensure dataset completeness. Due to the limited availability of specificity data that includes both hard in-visual-scope and hard out-of-visual-scope data, we retain specificity data that contains either one of these categories. This approach maintains the task's challenge while ensuring sufficient data coverage. Non-hard data is used as a substitute for the other category when necessary. The final dataset consists of 11,112 samples, which are then split into training and testing sets in a 3:1 ratio."}, {"title": "5. Experiments and Results", "content": "Base MLLMs. We utilize BLIP-2 OPT and MiniGPT-4 as the base MLLMs for Editing. BLIP-2 OPT is a vision-language model that combines a pre-trained Vision Transformer (ViT) with a lightweight Querying Transformer (Q-Former). It uses a two-stage pre-training strategy: the first stage focuses on learning vision-language representations, while the second emphasizes generative learning, allowing for efficient zero-shot image-to-text generation. MiniGPT-4 integrates a pre-trained ViT with a Q-Former to effectively align visual features with the Vicuna LLM. This architecture enables it to process and generate text based on visual inputs, enhancing its performance on complex vision-language tasks.\nBaselines. Since there are currently no methods specifically designed for FGVEdit, we apply the LLM editing methods to the LLM within MLLMs as baselines for evaluation. Fine-tuning adjusts a pre-trained model's parameters to incorporate new information. While effective, it is computationally intensive and may unintentionally degrade unrelated behaviors. IKE is a paradigm that utilizes contextual learning to modify specific factual knowledge in LLMs without altering their parameters. By providing carefully crafted demonstrations as input prompts, IKE guides the model to update its responses to reflect new or corrected information, avoiding the computational costs of fine-tuning. SERAC stores new facts in an addi-"}, {"title": "5.1. Experimental Setup", "content": "61.60, and for MiniGPT-4, from 37.85 to 57.20. While MEND, which updates both visual and language modules, performs better than other baselines, MSCKE-MEND further enhances performance, improving Specificity from 65.85 to 68.38 for BLIP-2 OPT and from 67.39 to 71.98 for MiniGPT-4. These results underline the effectiveness of our approach in tackling complex multimodal editing tasks."}, {"title": "5.3. Effect of Multimodal Scope Classifier", "content": "Next, we analyze the performance of our multimodal scope classifier compared to the SERAC scope classifier. Our classifier can process both text-only and text-image inputs, whereas the SERAC classifier is limited to text-only input. We evaluate their editing scope classification performance on two tasks: a text editing scope classification and a text-image editing scope classification task, both using our FGVEdit dataset.\nFor text editing scope classification, both classifiers receive text-only inputs and achieve high accuracy, demonstrating the robustness of our multimodal scope classifier in maintaining strong classification performance for text-based tasks. However, in the text-image editing scope classification, the SERAC scope classifier shows limited effectiveness, largely classifying most samples as in-scope. This limitation arises from the fine-grained nature of our FGVEdit dataset, where both in-scope and out-of-scope samples are closely related to the target editing text. In contrast, our multimodal scope classifier excels by leveraging both textual and visual information, significantly improving classification accuracy."}, {"title": "5.4. Implementation Analysis of Multimodal Scope Classifier", "content": "The multimodal scope classifier consists of two key components: a feature extraction module and a feature fusion module. We investigate how different implementations of these modules impact the classifier's performance. The feature extraction module can be implemented using either CLIP-ViT-B/32 or CLIP-ViT-L/14, while the feature fusion module can be realized through feature concatenation, cross-attention, or dot-product attention. The results on FGVEdit dataset are shown in Table 2.\nFor feature extraction, CLIP-ViT-B/32 performs similarly to CLIP-ViT-L/14, suggesting that CLIP-ViT-B/32 is sufficient for extracting comprehensive features. This finding highlights that our classifier does not require the more computationally expensive CLIP-ViT-L/14, making the approach more efficient and suitable for practical applications with limited computational resources. Regarding feature fusion, dot-product attention delivers the best performance with minimal computational overhead. Its superior efficiency in capturing essential interactions between text and image features, without the complexity of methods like cross-attention, makes it an ideal choice for feature fusion in our multimodal classifier."}, {"title": "5.5. Transferability of Multimodal Scope Classifier", "content": "To further highlight the advantages of our multimodal scope classifier, we investigate its transferability across different editing methods and base models. Specifically, we transfer the multimodal scope classifier, initially trained with FGVEdit dataset on BLIP-2 OPT, to various settings, such as using MEND as the counterfactual model and MiniGPT-4 as the base model. The results are presented in Table 3. Our multimodal scope classifier demonstrates exceptional transferability. In all experiments, the transferred classifier performs nearly identically to a classifier retrained for each specific setting. This decoupled design and robust transferability enable our method to be quickly adapted for deployment across new MLLMs."}, {"title": "5.6. Computational Cost Analysis", "content": "The MSCKE framework comprises a multimodal scope classifier, a base model, and a counterfactual model. We"}, {"title": "6. Conclusion", "content": "This paper presents a significant advancement in the field of multimodal knowledge editing through the introduction of the Fine-Grained Visual Knowledge Editing (FGVEdit) benchmark and the Multimodal Scope Classifier-based Knowledge Editor (MSCKE) framework. By addressing the unique challenges posed by multimodal contexts, our approach enables precise editing while retaining the integrity of unrelated content. The experimental results demonstrate that MSCKE outperforms existing text editing approaches, showcasing its potential to enhance the accuracy and efficiency of knowledge updates in MLLMs."}]}