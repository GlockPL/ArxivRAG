{"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "authors": ["Jinghui Lu", "Haiyang Yu", "Yanjie Wang", "Yongjie Ye", "Jingqun Tang", "Ziwei Yang", "Binghong Wu", "Qi Liu", "Hao Feng", "Han Wang", "Hao Liu", "Can Huang"], "abstract": "Recently, many studies have demonstrated that exclusively incorporating OCR- derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that in- tegrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM) for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal signifi- cant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.", "sections": [{"title": "1 Introduction", "content": "Recent research has increasingly focused on applying Large Language Models (LLMs) [1\u201323] to document-oriented Visual Question Answering (VQA) and Key Information Extraction (KIE) scenar- ios. Efforts to build a text-sensitive MultiModal Large Language Models (MLLMs) based on existing LLMs, particularly aimed at enhancing Visually Rich Document Understanding (VRDU), have made significant progress [6, 12, 24]. Although existing MLLMs show promising results in document understanding, they often encounter challenges related to image resolution. When the input image is of low resolution, it is too blurry to extract visual features effectively. Conversely, high-resolution images require additional computational resources to capture detailed textual information [12].\nConcurrently, another line of research employs off-the-shelf OCR tools to extract text and spatial layouts, which are then combined with LLMs to address VRDU tasks. These approaches assume that most valuable information for document comprehension can be derived from the text and its spatial layouts, viewing spatial layouts as \u201clightweight visual information\u201d [25]. Following this premise,"}, {"title": "2 Related Work", "content": "2.1 OCR-based LLMs for Document Understanding\nEarly document understanding methods [32\u201336] tend to solve the task in a two-stage manner, i.e., first reading texts from input document images using off-the-shelf OCR engines and then understanding the extracted texts. Considering the advantages of LLMs (e.g., high generalizability), some recent methods endeavor to combine LLMs with OCR-derived results to solve document understanding. For example, inspired by the \u201ccoordinate-as-tokens\" scheme [26], He et al. [29] propose to use \u201c[x_min, y_min, x_max, y_max]\" to introduce the layout information, which can fuse the layout information and texts into a unified text sequence and fully exploit the autoregressive merit of LLMs. To reinforce the layout information while avoiding increasing the number of tokens, DocLLM [25] designs a disentangled spatial attention mechanism to capture cross-alignment between text and layout modalities. Recently, LayoutLLM [27] utilizes the pre-trained layout-aware model [37], to insert the visual information, layout information and text information. However, the aforementioned methods neither suffer from the computational overhead leading by the increasing tokens or hardly take advantage of autoregressive characteristics of LLMs. Thus, it is an urgent problem to address how to better incorporate layout information without significantly increasing the number of tokens.\n2.2 OCR-free MLLMs for Document Understanding\nAnother approach to solve document understanding tasks is the OCR-free method. Benefiting from the end-to-end training framework, it involves processing the text content of documents directly, without relying on OCR engines. Donut [38] first presents an OCR-free method through mapping a text-rich document image into the desired answers. Pix2Struct [39] is trained to parse masked screenshots of web pages into simplified HTML, where variable resolution inputs are supported. While these approaches eliminate the need for OCR tools, they still necessitate task-specific fine- tuning. With the increasing popularity of LLMs/MLLMs [10\u201317], various methods are proposed to solve the document understanding task through explicitly training models on visual text understanding datasets and fine-tuning them with instructions to perform a zero-shot prediction. LLaVAR [40] and UniDoc [10] are notable examples that expand upon the document-oriented VQA capabilities of LLaVA [41] by incorporating document-based tasks. These models pioneer the use of MLLMs for predicting texts and coordinates from document images, enabling the development of OCR- free document understanding methods. Additionally, DocPedia [9] operates document images in the frequency domain, allowing for higher input resolution without increasing the input sequence length. Recent advancements in this field, including mPLUG-DocOwl [24], Qwen-VL [6], and TextMonkey [12], leverage publicly available document-related VQA datasets to further enhance the document understanding capability. Although these OCR-free methods have exhibited their advantages, they still struggle with the high-resolution input to reserve more text-related details."}, {"title": "3 Method", "content": "In this section, we present our LayTextLLM. First, we introduce a innovative Spatial Layout Projector (Sec. 3.1.1) converts four-dimensional layout coordinates into a single-token embedding. To reduce parameter overhead, we apply Partial Low-Rank Adaptation (Sec. 3.1.2). We also introduce two specific training tasks: Layout-aware Next Token Prediction (Sec. 3.2.1) to align layouts with text during pre-training, and Shuffled-OCR Supervised Fine-tuning (Sec. 3.2.2) to enhance the generalizability of the model. An illustration of our approach is shown in Fig. 2.\n3.1 Model Architecture\n3.1.1 Spatial Layout Projector (SLP)\nA key innovation in LayTextLLM is the Spatial Layout Pro- jector (SLP), which transforms a spatial layout into a singular bounding box token. This enhancement enables the model to process both spatial layouts and textual inputs simultane- ously. To be specifically, each OCR-derived spatial layout is represented by a bounding box defined by four-dimensional coordinates [x1, Y1, X2, Y2], these coordinates represent the normalized minimum and maximum horizontal and vertical extents of the box, respectively. The SLP maps these coordinates into a high-dimensional space that the language model can process as a single token. The process can be computed as z = W \u00b7 c + b, where c\u2208 R4 is the vector of the bounding box coordinates. W \u2208 Rd\u00d74 is a weight matrix with d represents the dimension of the embedding, b \u2208 Rd\u00d71 is a bias vector, z is the resulting bounding box token represented as an d-dimensional embedding. As illustrated in Fig. 2, the resulting bounding box token z will be interleaved with corresponding textual embeddings to put into LLMs. Note that the SLP is shared by all bounding box tokens so very limited number of parameters are introduced.\nCompared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token. This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence. This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs.\n3.1.2 Layout Partial Low-Rank Adaptation\nAfter using the SLP to generate bounding box tokens and a tokenizer to produce text tokens, these two modalities are then communicated using a Layout Partial Low-Rank Adaptation (P-LORA) module in LLMs. P-LORA, introduced in InternLM-XComposer2 [15], is originally used to adapt LLMs to visual modality. It applies plug-in low-rank modules specified to the visual tokens, which adds minimal parameters while preserving the LLMs inherent knowledge."}, {"title": "3.2 Training Procedure", "content": "LayTextLLM is trained with innovative layout-aware tuning procedure, which consists of two stages: Layout-aware Next Token Prediction pre-training and Shuffled-OCR Supervised Fine-tuning.\n3.2.1 Layout-aware Next Token Prediction\nInspired by the next token prediction commonly used in current LLM pre-training [1-7], we propose the Layout-aware Next Token Prediction (LNTP). Fig. 4 presents the contrast of the proposed Layout- aware Next Token Prediction and the conventional next token prediction task. The traditional next token prediction (Fig. 4(a)) relies solely on the textual content, predicting each subsequent token based on the prior sequence of tokens without considering their spatial layouts. Layout-aware next token prediction (Fig. 4(b)), however, interleaves the spatial information encoded by SLP (i.e., b\u00b2) with the text tokens (i.e., t\u00b2). This integration considers both the content and its layout within the document, leading to a richer, more precise understanding of both the structure and the content.\nSimilarly, primary objective of LNTP is to maximize the likelihood of its predictions for the next token. Thus the loss function is defined as\nL = \u2212 \u2211log P (t' | t1, t2, ..., ti\u22121) \nwhere P (t' | t1, t2, ..., ti\u22121) represents the probability of ith token ti given the sequence of preceding tokens t1, t2,..., ti-1, as predicted by the model. Note that we compute the loss only for text tokens, excluding bounding box tokens. During pre-training, our goal is to enhance the alignment between spatial layouts and textual modality, while preserving the LLM's inherent knowledge as much as possible. Thus, we freeze the LLMs and only update the parameters of SLP and P-LORA.\nIt is important to note that the proposed Layout-aware Next Token Prediction is a completely self- supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4 [27]. Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost.\n3.2.2 Shuffled-OCR Supervised Fine-tuning\nOCR engines typically process text from top to bottom and left to right. This order is also adopted as the input sequence for current OCR-based LLMs [25, 27]. However, modern LLMs often exhibit a strong inductive bias toward the positions of input tokens, influenced by designs such as Rotary Position Embeddings (RoPE) [44]. Specifically, tokens that are close together in the input sequence are likely to receive higher attention scores, which is advantageous for processing standard text sequences. Such inductive bias brings cons and pros.\nConsider the example illustrated in Fig. 5, where the OCR input text reads: \" Change, 1.30, GST%, Amt(RM), GST(RM), Total(RM), SR, 6, 17.64, 1.06, 18.70 ...\". If the question posed is \"What is the value of the field Change?\" (highlighted in a blue box), the model easily identifies \"1.30\" as it is closely positioned to the word \"Change\" in the sequence. However, for a more challenging query like \"What is the value of the field Total(RM)?\" (highlighted in a red box), the model struggles to determine the correct answer due to the presence of multiple subsequent numbers closed to \"Total(RM)\u201d. LayTextLLM integrates spatial layouts with textual data, reducing reliance on input sequence order. Thus, we posit that shuffling the OCR input order could enhance the resilience of LayTextLLM in discerning relevant information irrespective of token proximity in the sequence.\nSpecifically, we propose Shuffled-OCR Supervised Fine-tuning (SSFT) that randomly shuffles the order of OCR-derived text in a certain proportion of examples. The range of exploration for the shuffling ratio can be found in Tab. 7 and 20% shuffled ratio is applied. The training objective is equivalent to predicting the next tokens, but in this scenario, only the tokens of the response are used to compute loss. During SSFT, we unfreeze all parameters including those of LLMs. Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order."}, {"title": "4 Experiments", "content": "4.1 Datasets\nPre-training data In our training process, we exclusively use open-source data to facilitate replica- tion. We collect data from two datasets for pre-training: (1) IIT-CDIP Test Collection 1.0 [45] and (2) DocBank [46]. The IIT-CDIP Test Collection 1.0 comprises an extensive repository of more than 16 million document pages. DocBank consists of 500K documents, each presenting distinct layouts with a single page per document. For training efficiency, we choose to utilize the entire DocBank dataset and only subsample 5 million pages from the IIT-CDIP collection 1.0.\nSFT data For document-oriented VQA, we select Document Dense Description (DDD) and Layout-aware SFT data used in Luo et al. [27], which are two synthetic datasets generated by GPT-4. Besides, DocVQA [47], InfoVQA [48], ChartQA [49], VisualMRC [50] is included following [12]. For KIE task, we select SROIE [51], CORD [52], FUNSD [53], POIE [54] datasets following [12, 25, 27].\n4.2 Implementation Detail\nThe LLM component of LayTextLLM is initialized from the Llama2-7B-base [42], which is a widely- used backbone. Other parameters including SLP and P-LoRA are randomly initialized. During pre-training, the LLM is frozen, and the parameters of SLP and P-LoRA modules are updated. During SFT, all parameters are fine-tuned. Other detailed setup can be found in Appendix B.\nWe have configured the model with three versions of LayTextLLM for a side-by-side comparison under different settings. Aligned with Luo et al. [27], the first version, LayTextLLMzero, is trained exclusively with DDD and Layout-aware SFT data. Building upon this, and in alignment with the setting of Liu et al. [12], we introduce the DocVQA, InfoVQA, and ChartQA training"}, {"title": "5 Limitation", "content": "Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications. There are some instances, particularly in chart analysis, where reasoning must be based solely on visual cues (e.g. size, color)\u2014a challenge that remains unmet. Questions such as \"What is the difference between the highest and the lowest green bar?\" illustrate this gap. The ChartQA results, detailed in Appendix E, also underscore these limitations. Addressing these challenges highlights the urgent need for future enhancements that integrate visual cue within the capabilities of LayTextLLM."}, {"title": "6 Conclusion", "content": "We propose LayTextLLM for various VRDU tasks, in which spatial layouts and textual data are seamlessly interleaved to make more accurate prediction by introducing a innovative Spatial Layout Projector. Two tailored training tasks - Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning - are designed to improve the comprehension of document layouts. Extensive experiments confirm the effectiveness of LayTextLLM."}, {"title": "Appendix", "content": "A Qualitative Examples\nQualitative examples of document-oriented VQA (upper row) and KIE (bottom row) are shown in Fig. 6. The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples. For example, in the upper right figure, many numeric texts in the receipt act as noise for the baseline method. In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM.\nB Implementation Detail\nAll training and inference procedures are conducted on eight NVIDIA A100 GPUs.\nTraining LayTextLLM is initialized with Llama2-7B-Base model, the pre-training, SFT, and other model hyper-parameters can be seen in Tab. 6. Please note that all variants of LayTextLLM, including those utilized in ablation studies, are trained in accordance with the SFT settings. All baseline results are sourced from their respective original papers, with the exception of the Llama2-7B series and the Llama2-7Bcoor series. These were re-implemented and can be referenced in [27, 29].\nInference For the document-oriented VQA test set, we use the original question-answer pairs as the prompt and ground truth, respectively. For Key Information Extraction (KIE) tasks, we reformat the key-value pairs into a question-answer format, as described in [12, 25, 27]. Additionally, for the FUNSD dataset, we focus our testing on the entity linking annotations as described in [27].\nTo eliminate the impact of randomness on evaluation, no sampling methods are employed during testing for any of the models. Instead, beam search with a beam size of 1 is used for generation across"}]}