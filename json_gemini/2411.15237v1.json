{"title": "Stain-invariant representation for\ntissue classification in histology\nimages", "authors": ["Manahil Raza", "Saad Bashir", "Talha Qaiser", "Nasir Rajpoot"], "abstract": "The process of digitizing histology slides involves multiple factors that can\naffect a whole slide image's (WSI) final appearance, including the staining\nprotocol, scanner, and tissue type. This variability constitutes a domain shift\nand results in significant problems when training and testing deep learning\n(DL) algorithms in multi-cohort settings. As such, developing robust and\ngeneralizable DL models in computational pathology (CPath) remains an\nopen challenge. In this regard, we propose a framework that generates stain-\naugmented versions of the training images using stain matrix perturbation.\nThereafter, we employed a stain regularization loss to enforce consistency\nbetween the feature representations of the source and augmented images.\nDoing so encourages the model to learn stain-invariant and consequently,\ndomain-invariant feature representations. We evaluated the performance of\nthe proposed model on cross-domain multi-class tissue type classification\nof colorectal cancer images and have achieved improved performance\ncompared to other state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "The advent of Deep Learning (DL) has revolutionised the field of\nComputational Pathology (CPath) and has enabled the automated and\nquantitative analysis of histology images [1,2]. Despite the success of DL\nmethods, they are vulnerable to domain-specific variations [3]. Some\nmajor sources of variations include staining and scanning processes, where\ndistinct institutions may employ different staining protocols and use various\nscanners, resulting in differences in the visual appearance of whole slide\nimages (WSIs). This variability poses a significant challenge known as domain\nshift for training robust DL models and encumbers their ability to generalise\nwell across diverse histology datasets. Addressing the domain shift problem\nhas been a focal point for CPath researchers, leading to several efforts in\nstain normalisation [4-6], augmentation and adaptation. Stain augmentation\n(SA) aims to mitigate the effects of domain shift by generating augmented\nvariations of the source images to mimic the stain variations present in the\ntarget domain for improving the model's generalisability on unseen data\n[7-9]. Tellez et al. [24] has stressed upon the importance of using stain\naugmentations for histopathology images for a more robust classification\nperformance. Abbet et al. [11] proposed a novel domain adaptation (DA)\nmethod, Self-Rule to Multi-Adapt (SRMA), for single-source and muti-source\ntissue classification with multiple datasets by using in-domain and cross-\ndomain losses. Unlike in DA, domain generalisation (DG) methods cannot\nleverage unlabelled data from the target domain [10]. To this effect, Vuong\net al. [12] adopted a self-supervised contrastive learning approach using a\ncombination of encoders and momentum encoders for colorectal cancer\nclassification using patch shuffling augmentations. Our proposed method,\ninspired by [14] uses stain augmentations to help extract domain-invariant\nfeature representations for colorectal cancer tissue images, thus ensuring\nthat the class labels assigned to an image remain consistent in the face of\nstaining variability."}, {"title": "Methodology", "content": "The proposed framework comprises of two modules, one for classification\nand the other for stain augmentation, as shown in Fig. 1. Each image x\nwith label y is passed through ResNet-18 based feature extractor $f_e$ for\nextracting the feature representation as $f_e(x) = r$. This feature representation\nis then passed onto an MLP-based classifier $f_c$ for a classification decision,\n\u0177 as $f_c(f_e(x)) = \\hat{y}$During the training process, we also employ the stain\naugmentation network, which generates stain-altered version(s) of the\nsource image as $x' = {x'_1..x'_N}$. For this purpose, we use the Vahadane\n[18] method for extracting the stain matrix. The stain concentrations are\nperturbed to create the stain-altered images using [15]. These images are\nthen passed onto the same feature extractor $f_e(x') = r' = {r'_1..r'_n}$ to extract\nfeature representations of the stain-altered images. Two loss functions are\nemployed in the proposed workflow. We use the cross-entropy loss as the\nprimary classification loss $L_c$ between the predicted label $\\hat{y}$ and the ground-\ntruth label y. Additionally, we employed a mean squared error (MSE) loss\nas a stain regularisation loss, $L_s = ||r - r'||_2$, which measures the distance\nbetween the extracted feature representations of the source and augmented\nimages. The MSE loss acts as a strong penalisation factor, enforcing"}, {"title": "Results and Discussions", "content": "We have employed two datasets to validate the proposed framework 1).\nKather-19 (K19) [16], which contains 100,000 images (224x224 pixels) from\n9 tissue classes and 2). Kather-16 (K16) [17], which consists of 5,000 images\n(150\u00d7150 pixels) from 8 classes. Since there are discrepancies between the\nclass labels of the two datasets, we followed the strategy for relabelling\n[11] and grouped the data into seven classes, namely adipose, background,\ndebris, lymphocytes, normal colon mucosa, stroma and colorectal\nadenocarcinoma epithelium. Sample images from K16 are shown in Fig.1.\nThe results of the experiments are reported in Table 1, where ImageNet\nUpper Bound denotes an experiment where both the training and testing are\nperformed with the same dataset (K16). The degradation in performance in\nImageNet Lower bound is due to the presence of a domain shift when the\nmodel is trained and tested on different source (K19) and target domains\n(K16). We observe that the proposed method which generated 6 augmented\nimages for each input image, outperforms the ImageNet baseline by 22% in\nterms of accuracy. Whereas it performed 20% and 12% better as compared\nto MocoV2 [22] and InfoMin [23]. IMPaSh [12] is a combination of InfoMin [23]\nand PatchShuffling augmentations, and our methods still outperform it by\n1% while being less computationally expensive. To summarise, the proposed\nworkflow leveraged stain augmentations to encourage the DL model to learn\nstain and domain-invariant feature representations and outperformed other\nstate-of-the-art methods which begs the question \u201cIs Stain Augmentation\nreally all you need for Domain Generalisation?\u201d Our future work will include\nautomating the selection of the optimal number of augmentations.\nconsistency in the face of stain augmentations. The overall loss function\ncombines the two loss functions, $L = L_c + L_s$."}]}