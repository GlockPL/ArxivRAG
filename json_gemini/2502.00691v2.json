{"title": "Learning Autonomous Code Integration for Math Language Models", "authors": ["Haozhe Wang", "Long Li", "Chao Qu", "Fengming Zhu", "Weidi Xu", "Wei Chu", "Fangzhen Lin"], "abstract": "Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness-the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training.\nWhile reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT. Code, models and data are released via an anonymous repository.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains (Kaddour et al., 2023; Achiam et al., 2023; Dubey et al., 2024; Team et al., 2023; Yang et al., 2024a). Yet, solving complex mathematical problems still remains challenging, as the task requires hybrid skills in abstract reasoning, symbolic manipulation, and precise numerical computation (Gao et al., 2023; Yue et al., 2023; Gou et al., 2023; Li, 2024). Current approaches adopt two complementary paradigms: (1) chain-of-thought (CoT) reasoning, which decomposes problems into intermediate reasoning steps (Wei et al., 2022; Yu et al., 2023), and (2) external tool integration, where models generate code snippets to offload computations to interpreters or symbolic solvers (Toshniwal et al., 2024; Yue et al., 2023). While CoT reasoning excels at semantic parsing and stepwise logic, its reliance on token-level autoregressive generation often propagates numerical errors. Conversely, tool-based approaches ensure computational precision but suffer from a semantic-to-symbolic translation gap, where even minor syntactic errors or contextual misinterpretations disrupt execution (Li, 2024).\nRecent hybrid frameworks like Mammoth (Yue et al., 2023), Deepseek-Math (Gou et al., 2023; Shao et al., 2024), and Qwen-2.5-Math (Yang et al., 2024b) attempt to combine these paradigms through interleaved CoT-code reasoning. However, as our empirical analysis reveals (Fig. 1), current methods exhibit a critical rigidity: they either default to CoT reasoning unless explicitly prompted for code generation or adhere to static templates for tool invocation. We trace this limitation to prevailing supervised fine-tuning (SFT) paradigms that condition models to (1) passively follow user instructions (e.g., \"Let's write a Python program\" (Yue et al., 2023)), (2) replicate fixed code-integration patterns from curated datasets (Yang et al., 2024b), or (3) imitate teacher-forced tool-use trajectories (Gou et al., 2023; Shao et al., 2024). Consequently, LLMs lack metacognitive awareness - the capacity to dynamically evaluate their intrinsic capabilities against problem contexts and autonomously determine when and how to integrate tools. This deficiency motivates our central research question:\nHow can mathematical LLMs learn autonomous code integration (AutoCode) that optimally complements their inherent reasoning capabilities?\nReinforcement learning (RL) offers a promising pathway by optimizing policies through self-generated trajectories, as evidenced by recent successes like DeepSeek R1 (Guo et al., 2025). However, we empirically observe that standard RL methods is inefficient in learning autonomous code integration (AutoCode) strategies (see Sec. 4.2). This stems from RL's tendency to exploit local policy neighborhoods, thereby insufficiently exploring the vast combinatorial space of potential CoT-code interleaving patterns. Such myopic exploration constrains the discovery of high-reward reasoning paths that judiciously blend both modalities, particularly as the model's reasoning capabilities evolve during training.\nTo address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes guided exploration with policy optimization. Our key innovation lies in formulating code-integration decisions as latent variables within an EM paradigm, creating a self-reinforcing cycle: the E-step identifies high-potential code-integration decisions through guided exploration, while the M-step optimizes policy parameters for joint metacognitive tool-usage and reasoning.\nThis dual mechanism enables models to adapt tool-use strategies as their capabilities evolve during training. Practically, we achieve efficiency through two design choices: (1) an offline data curation step (E-step) that prioritizes high-return code invocation decisions through guided exploration, and (2) an off-policy RL optimization step (M-step) that jointly improves tool-usage and reasoning. This approach offers enhanced control and efficiency compared to standard RL, which is particularly beneficial for resource-constrained companies or researchers.\nExtensive experiments demonstrate that our method (a) preserves higher training efficiency while achieving better performance, and (b) learns intelligent code integration strategies that achieves higher accuracy than either CoT or code prompted in isolation. Notably, our show consistent improvements across different benchmarks, raising MATH500 from 60.4% to 71.4%.\nOur contribution is summarized as follows: (1) We diagnose a critical gap in mathematical LLM \u2013 the inability to autonomously integrate tools based on metacognitive awareness - and demonstrate standard RL's inefficiency in addressing it. (2) We propose a novel EM-based framework that jointly adapts the tool-usage strategies with evolving reasoning abilities, with a simple yet efficient implementation. (3) We demonstrate superior results in both training efficiency and accuracy on challenging benchmarks."}, {"title": "Background", "content": "Problem Statement. Modern tool-augmented language models address mathematical problems $x_q \\in X_Q$ by generating step-by-step solutions that interleave natural language reasoning with executable Python code (Fig. 2). Formally, given a problem $x_q$, a model $M_\\theta$ iteratively constructs a solution $Y_a = {y_1, \\dots, y_T}$ by sampling components $y_t \\sim p(y_t|Y_{<t}, x_q)$, where $Y_{<t}$ encompasses both prior reasoning steps, code snippets and execution results $e_t$ from a Python interpreter. The process terminates upon generating an end token, and the solution is evaluated via a binary reward $r(Y_a,x_q) = \\mathbb{I}(Y_a = y^*)$ indicating equivalence to the ground truth $y^*$. The learning objective is formulated as:\n$\\max_\\theta \\mathbb{E}_{x_q\\sim X_Q} [r(y_a, x_q)]$\nChallenge and Motivation. Developing autonomous code integration (AutoCode) strategies poses unique challenges, as optimal tool-usage behaviors must dynamically adapt to a model's intrinsic capabilities and problem-solving contexts. While traditional supervised fine-tuning (SFT) relies on imitation learning from expert demonstrations, this paradigm fundamentally limits the emergence of self-directed tool-usage strategies. Unfortunately, current math LLMs predominantly employ SFT to orchestrate tool integration (Yue et al., 2023; Gou et al., 2023; Shao et al., 2024; Li, 2024), their rigid adherence to predefined reasoning templates therefore struggles with the dynamic interplay between a model's evolving problem-solving competencies and the adaptive tool-usage strategies required for diverse mathematical contexts.\nReinforcement learning (RL) offers a promising alternative by enabling trial-and-error discovery of autonomous behaviors. Recent work like DeepSeek-R1 (Guo et al., 2025) demonstrates RL's potential to enhance reasoning without expert demonstrations. However, we observe that standard RL methods (e.g., PPO (Schulman et al., 2017)) suffer from a critical inefficiency (see Sec. 4.2): Their tendency to exploit local policy neighborhoods leads to insufficient exploration of the vast combinatorial space of code-integrated reasoning paths, especially when only given a terminal reward in mathematical problem-solving.\nTo bridge this gap, we draw inspiration from human metacognition \u2013 the iterative process where learners refine tool-use strategies through deliberate exploration, outcome analysis, and belief updates. A novice might initially attempt manual root-finding via algebraic methods, observe computational bottlenecks or inaccuracies, and therefore prompting the usage of calculators. Through systematic reflection on these experiences, they internalize the contextual efficacy of external tools, gradually forming stable heuristics that balance reasoning with judicious tool invocation.\nTo this end, our focus diverges from standard agentic tool-use frameworks (Yuan et al., 2025), which merely prioritize successful tool execution. Instead, we aim to instill human-like metacognition in LLMs, enabling them to (1) determine tool-usage based on their own capability boundaries (see the analysis in Sec. 4.2), and (2) dynamically adapt tool-usage strategies as their reasoning abilities evolve (via our EM framework)."}, {"title": "Methodology", "content": "Inspired by human metacognitive processes, we introduce an Expectation-Maximization (EM) framework that trains LLMs for autonomous code integration (AutoCode) through alternations (Fig. 3):\n1. Guided Exploration (E-step): Identifies high-potential code-integrated solutions by systematically probing the model's inherent capabilities.\n2. Self-Refinement (M-step): Optimizes the model's tool-usage strategy and chain-of-thought reasoning using curated trajectories from the E-step.\n3.1 The EM Framework for AutoCode\nA central challenge in AutoCode lies in the code triggering decisions, represented by the binary decision $c\\in {0,1}$. While supervised fine-tuning (SFT) suffers from missing ground truth for these decisions, standard reinforcement learning (RL) struggles with the combinatorial explosion of code-integrated reasoning paths. Our innovation bridges these approaches through systematic exploration of both code-enabled ($c = 1$) and non-code ($c = 0$) solution paths, constructing reference decisions for policy optimization.\nWe formalize this idea within a maximum likelihood estimation (MLE) framework. Let $P(r = 1|x_q; \\theta$ denote the probability of generating a correct response to query $x_q$ under model $M_\\theta$. Our objective becomes:\n$J_{MLE}(\\theta) = \\log P(r = 1|x_q; \\theta) \t(1)$\nThis likelihood depends on two latent factors: (1) the code triggering decision $\\pi_\\theta(c|x_q)$ and (2) the solution generation process $\\pi_\\rho(Y_a|x_q, c)$. Here, for notation-wise clarity, we consider code-triggering decision at a solution's beginning ($c$ following $x_q$ immediately). We show generalization to mid-reasoning code integration in Sec. 3.2."}, {"title": "Practical Implementation", "content": "The EM framework provides a principled way to optimize this MLE objective in the presence of latent variables (Bishop and Nasrabadi, 2006). We derive the evidence lower bound (ELBO):\n$J_{ELBO} (s, \\theta) =\\mathbb{E}_{s(c|x_q)} \\log \\frac{\\pi_\\rho(c|x_q) \\cdot P(r = 1|c, x_q; \\theta)}{s(c|x_q)} \\t(2)$\nwhere $s(c|x_q)$ serves as a surrogate distribution approximating optimal code triggering strategies. It is also considered as the reference decisions for code integration.\nE-step: Guided Exploration computes the reference strategy $s(c|x_q)$ by maximizing the ELBO, equivalent to minimizing the KL-divergence:\n$\\max_s J_{ELBO} (s, \\theta) =\\\\-D_{KL} (s(c|x_q)||P(r = 1, c|x_q; \\theta)) \\t(3)$\nThe reference strategy $s(c|x_q)$ thus approximates the posterior distribution over code-triggering decisions $c$ that maximize correctness, i.e., $P(r = 1,c|x_q;\\theta)$. Intuitively, it guides exploration by prioritizing decisions with high potential: if decision $c$ is more likely to lead to correct solutions, the reference strategy assigns higher probability mass to it, providing guidance for the subsequent RL procedure.\nM-step: Self-Refinement updates the model parameters $\\theta$ through a composite objective:\n$\\max_\\theta J_{ELBO}(s, \\theta) = \\mathbb{E}_{c \\sim s(c|x_q)} [r(x_q, Y_a)]\\\\-CE(s(c|x_q) || \\pi_\\theta (c|x_q)) \\t(4)$\nThe first term implements reward-maximizing policy gradient updates for solution generation, while while the second aligns native code triggering with reference strategies through cross-entropy minimization (see Fig. 3 for an illustration of the optimization). This dual optimization jointly enhances both tool-usage policies and reasoning capabilities."}, {"title": "Practical Implementation", "content": "In the above EM framework, we alternate between finding a reference strategy $s$ for code-triggering decisions in the E-step, and perform reinforcement learning under the guidance from $s$ in the M-step. We implement this framework through an iterative process of offline data curation and off-policy RL.\nOffline Data Curation. We implement the E-step through Monte Carlo rollouts and subsampling. For each problem $x_q$, we estimate the reference strategy as an energy distribution:\n$s^*(c|x_q) = \\frac{\\exp (\\alpha \\cdot \\pi_\\theta(c|x_q)Q(x_q, c; \\theta))}{Z(x_q)} \\t(5)$\nwhere $Q(x_q, c; \\theta)$ estimates the expected value through $K$ rollouts per decision, $\\pi_\\theta(c|x_q)$ represents the model's current prior and the $Z(x_q)$ is the partition function to ensure normalization. Intuitively, the strategy will assign higher probability mass to the decision $c$ that has higher expected value $Q(x_q, c; \\theta)$ meanwhile balancing its intrinsic preference $\\pi_\\theta(c|x_q)$.\nOur curation pipeline proceeds through:\n*   Generate $K$ rollouts for $c = 0$ (pure reasoning) and $c = 1$ (code integration), creating candidate dataset $D$."}, {"title": "Experiments", "content": "Our implementation uses $K = 8$ rollouts per query (temperature=1.0, top-p=0.9). Training completes in about 10 hours on 8\u00d7 A100 (80GB) GPUs across three epochs of 7K queries. We release code, models and data via an anonymous repository.\n4.1 Main Results\nNotably, we observe a minimum performance gain of 11% on the MATH500 benchmark, escalating to an impressive 9.4% absolute improvement on the highly challenging AIME benchmark. Across in-domain benchmarks, our method yields an average improvement of 8.9%, and for out-of-domain benchmarks, we achieve a substantial average gain of 6.98%. These results validate the effectiveness of our approach across model families and problem difficulty levels."}, {"title": "Ablation Study", "content": "We conduct three primary analyses: (a) comparison with standard RL and SFT baselines to validate our method's effectiveness in facilitating exploration, (b) visualization of exploration patterns to reveal limitations in the standard RL paradigam, and (c) behavioral analysis of code integration strategies. These analyses collectively demonstrate our method's benefits in facilitating guided exploration and explains how it improves performance.\nTraining Efficiency. We evaluated the learning dynamics of our approach in direct comparison to three established training paradigms:\n*   Base+RL: On-policy Reinforcement Learning (RL) initialized from a base model without Supervised Fine-Tuning (SFT). This follows the methodology of DeepSeek R1, designed to isolate and assess the pure effects of RL training.\n*   SFT: Supervised Fine-Tuning, the prevailing training paradigm widely adopted in current tool-integrated math Language Models (LMs).\n*   SFT+RL: Standard RL applied after SFT, serving as a conventional baseline for evaluating our EM-based RL method.\nFrom the figure, we make the following key observations:\n*   While Reinforcement Learning directly from the base model (Base+RL) exhibits consistent performance improvement, its training efficiency is lower than training paradigms incorporating SFT. In addition, the model rarely explores code-integrated solutions, with the code invocation rate below 5%. This strongly suggest that reinforcement learning tool-usage behavior from scratch is inherently inefficient.\n*   SFT effectively provides a strong initialization point, but SFT alone exhibits limited asymptotic performance. This suggests that SFT lacks the capacity to adapt and optimize beyond the scope of the expert demonstrations, thereby limiting further improvement.\n*   Standard RL applied after SFT shows initial further improvement but subsequently plateaus, even after an extended training stage. This suggests the exploration-exploitation dilemma when applying RL for LLM post-training: standard RL with vanilla rollout exploration tends to exploit local optima and insufficiently explores the combinatorial code-integrated trajectories.\nTo further substantiate the exploration limitations inherent in the conventional SFT+RL paradigm, we present a visualization of the exploration patterns. We partitioned the model-generated responses during self-exploration into three distinct training phases and analyzed the statistical distribution of code invocation rates across queries as the model's policy evolved throughout training. As depicted in Figure 5, the distribution of code invocation progressively concentrates towards the extremes \u2013 either minimal or maximal code use - indicating the model's growing tendency to exploit its local policy neighborhood. This exploitation manifests as a focus on refining established code-triggering decisions, rather than engaging in broader exploration of alternative approaches.\nThese empirical observations lend strong support to our assertion that standard RL methods are susceptible to premature exploitation of the local policy space when learning AutoCode strategies. In sharp contrast, our proposed EM method facilitates a more guided exploration by sub-sampling trajectories according to the reference strategy (Sec. 3.2). This enables continuous performance (evidenced in Sec. 4.1) and mitigating the risk of converging to suboptimal local optima (Fig. 4)."}, {"title": "Analysis on Code Integration Behaviors", "content": "We investigated the properties of the learned code integration strategies to gain deeper insights into the mechanisms behind our method's performance gains. Our central hypothesis posits that optimal code integration unlocks synergistic performance benefits by effectively combining the strengths of CoT and code executions. This synergy presents a \"free lunch\" scenario: a well-learned metacognitive tool-usage strategy can elevate overall performance, provided the model demonstrates competence in solving distinct subsets of queries using either CoT or code execution.\nTo empirically validate this \"free lunch\" principle and demonstrate the superiority of our approach in realizing it, we benchmarked our model against baselines that inherently support both code execution and Chain-of-Thought (CoT) reasoning: GPT-4, Mammoth-70B, and DeepseekMath-Instruct-7B. Our analysis evaluated the model's autonomous decision to invoke code when not explicitly instructed on which strategy to employ. We compared this \"AutoCode\" performance against scenarios where models were explicitly prompted to utilize either code or CoT reasoning. We also considered the theoretical \"free lunch\" upper bound \u2013 the accuracy achieved by combining the successful predictions from either strategy (i.e., taking the union of queries solved by CoT or code).\nAs visually presented in Figure 6, existing baseline models exhibit inferior performance in AutoCode mode compared to scenarios where code invocation is explicitly prompted, e.g., DeepseekMath-Instruct-7B shows a degradation of 11.54% in AutoCode mode. This suggests that their AutoCode strategies are often suboptimal, performing closer to random selection between CoT and code (selection accuracy near 50%), resulting in AutoCode falling between the performance of explicitly triggered CoT and code. In contrast, our models learn more effective code integration strategies. AutoCode4Math-Qwen2.5, for example, improves upon explicitly code-triggered performance by 7%, indicating a true synergistic integration of reasoning and code execution.\nTo quantify the effectiveness of these learned \"AutoCode\" strategies, we calculated the CoT/code selection accuracy. We used the outcome of explicit instruction (i.e., performance when explicitly prompted for CoT or code) as a proxy for the ground-truth optimal method selection. Our model achieves a selection accuracy of 89.53%, showcasing the high efficacy of the learned code integration strategy."}, {"title": "Related Work and Discussion", "content": "Tool-Integrated Math LLMs. Math language models adopted two major paradigms: Chain-of-Thought (CoT) reasoning and the use of external tools, such as Python programs (Yu et al., 2023; Yue et al., 2023; Toshniwal et al., 2024). Each paradigm offers unique benefits, and recent hybrid frameworks (Yue et al., 2023; Gou et al., 2023; Li, 2024; Shao et al., 2024; Yang et al., 2024b) increasingly seek to combine them for synergy. However, current models exhibit critical rigidity, motivating our work to realize the true metacognitive capacity that enjoys synergistic benefits of CoT and code.\nEM for RL. Expectation-Maximization (EM) has proven effective for maximum likelihood problems involving hidden variables, such as Expert Iteration (Anthony et al., 2017), Iterative Maximum Likelihood (Wu, 2016; Agarwal et al., 2019), Meta-Reinforcement Learning (Zintgraf et al., 2019; Wang et al., 2020), and Adversarial Games (Wang et al., 2023). In the context of math LLMs, the most relevant works are (Singh et al., 2023) and (Ni et al., 2022), which apply EM-style iterative self-training to math problem-solving. Unlike these approaches, we leverage the EM framework for guided exploration during reinforcement learning of language models.\nConclusion. Existing tool-integrated math language models lack the metacognitive capacity to effectively determine code integration, hindering their ability to fully realize the synergistic benefits of tool integration and CoT. To address this critical gap, we propose a novel EM-based framework that combines guided exploration with policy optimization. Our experiments demonstrate the limitations of standard SFT and RL in efficiently exploring the combinatorial space of code-integrated trajectories and highlight the superior training efficiency and performance of our approach."}, {"title": "Limitations", "content": "The scope of our work is primarily focused on mathematical problem-solving. While we observe promising results on challenging benchmarks like MATH500, the generalizability of our approach to other domains requiring the metacognitive capacity of tool integration and CoT, such as scientific reasoning or code generation for general-purpose tasks, remains to be explored. Future work should investigate the effectiveness of our framework across a wider range of tasks and domains."}]}