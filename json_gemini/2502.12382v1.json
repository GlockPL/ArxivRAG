{"title": "Hybrid Machine Learning Models for Intrusion Detection in IoT:\nLeveraging a Real-World IoT Dataset", "authors": ["Md Ahnaf Akif", "Ismail Butun", "Andre Williams", "Imadeldin Mahgoub"], "abstract": "The rapid growth of the Internet of Things (IoT) has\nrevolutionized industries, enabling unprecedented connectivity\nand functionality. However, this expansion also increases vul-\nnerabilities, exposing IoT networks to increasingly sophisticated\ncyberattacks. Intrusion Detection Systems (IDS) are crucial for\nmitigating these threats, and recent advancements in Machine\nLearning (ML) offer promising avenues for improvement. This\nresearch explores a hybrid approach, combining several stan-\ndalone ML models such as Random Forest (RF), XGBoost, K-\nNearest Neighbors (KNN), and AdaBoost, in a voting-based hy-\nbrid classifier for effective IoT intrusion detection. This ensemble\nmethod leverages the strengths of individual algorithms to en-\nhance accuracy and address challenges related to data complexity\nand scalability. Using the widely-cited IoT-23 dataset, a prominent\nbenchmark in IoT cybersecurity research, we evaluate our hybrid\nclassifiers for both binary and multi-class intrusion detection\nproblems, ensuring a fair comparison with existing literature.\nResults demonstrate that our proposed hybrid models, designed\nfor robustness and scalability, outperform standalone approaches\nin IoT environments. This work contributes to the development\nof advanced, intelligent IDS frameworks capable of addressing\nevolving cyber threats.", "sections": [{"title": "I. INTRODUCTION", "content": "THE Internet of Things (IoT) has brought about new\ndimensions in the modern world, facilitating connection\nbetween devices through networks in fields such as healthcare,\ntransportation, and smart cities. Among all these advantages\nIoT technology brings, cyber-crime attackers readily attack\ncritical vulnerabilities. This is because IoT networks are\nextensive in scale and, therefore, complex, making them hot\ntargets for attacks related to data breaches, malware infections,\nand Distributed Denial-of-Service (DDoS) incidents. Although\nthese threats are becoming more sophisticated, there is a need\nfor effective and adaptive intrusion detection mechanisms to\ncope with the ever-expanding complexity of cyberattacks.\nThis development has opened new avenues to improve\nIoT security through computational advancements. Learning-\nbased methods are generally effective in detecting malicious\nactivities. In contrast to static and rule-based systems, they\ncan analyze large volumes of data to reveal hidden patterns\nand detect abnormalities in real-time. Their advantage is their\nadaptability, as the system can always stay one step ahead\nof the bad guys. These approaches equip IoT networks with\ndynamic defense against cyber risks by drawing on insights\ngained from various data sources.\nIntrusion Detection Systems (IDS) play a critical role in\ncybersecurity by identifying malicious activities in a network\nor system. In the recent past, researchers often employed\nmachine learning (ML) and deep learning (DL) methods, either\nas standalone models or in hybrid configurations, to enhance\nIDS capabilities, especially to detect anomalies (abnormal\nbehaviors or activities within their systems or networks). These\nmethods cater to binary classification (e.g., distinguishing\nbetween malicious and benign activities) and multi-class clas-\nsification (e.g., identifying specific types of attacks).\nIn this study, we utilize IoT-23 Dataset, a benchmark IoT\ndataset for intrusion detection research, developed by the\nStratosphere Laboratory [12]. The main focus of this dataset\nis on malicious and benign traffic. The dataset contains 23\nlabeled scenarios and a wide range of attacks like DDoS,\ndata exfiltration, malware activities, and normal traffic from\nIoT devices such as smart cameras, motion sensors, and smart\nhubs. It is captured in packet capture format, hence suitable for\nsignature-based and ML based analysis. This dataset is of great\nvalue in evaluating IDS due to its representation of real-world\nIoT scenarios, the balance between attack and benign data,\nand the coverage of different attack vectors in IoT networks.\nEarly steps of our study were presented in [3], especially\ninvolving binary classification with standalone ML methods.\nIn this study, we explore a hybrid approach that combines\nthe strengths of multiple computational models to improve\nintrusion detection in IoT systems. This approach addresses\ncommon challenges, such as balancing detection accuracy with\nscalability and minimizing false alarms. By uniting comple-\nmentary techniques, the framework aims to create a more\nresilient solution capable of countering previously unseen\nthreats. Through this work, we hope to offer practical strategies\nfor building secure IoT environments tailored to the unique\ndemands of this rapidly growing field.\nThe paper's organization is as follows: Section II provides\nthe related work (literature) in the field. Section III includes\nthe prominent phase of this study, which is the \u201cData Pre-\nProcessing\u201d. Section IV provides the foundations and method-\nology related to ML algorithms used in this study. Section V\nplots all the necessary experimental results for evaluating the"}, {"title": "II. RELATED WORK", "content": "Given their growing and linked character, IoT\nnetworks-which are increasingly vulnerable to cyber-\nattacks rely on IDS to maintain their security. Using both\nbinary and multi-class intrusion detection models, several\nmachine ML and DL methods have been employed to improve\nIDS. This review of the literature investigates several current\nworks addressing intrusion detection problems using various\ntechniques, both as stand-alone models and in hybrid setups.\nThe authors of [7] presented a new AI framework that\ndetects malware in IoT devices to mitigate cyber-attacks. The\nauthors focus on enhancing security in various use cases\nfor smart environments through an all-inclusive AI-enabled\napproach in this paper. Emulation of a smart environment\nemploying the Raspberry Pi and NVIDIA Jetson as gateways\nin capturing data from IoT devices connected via the MQTT\nprotocol, therefore enabling monitoring of real-time malware\nattacks for their prediction. In this work, many models of AI\nhave been evaluated, among which the DNN model demon-\nstrated superior accuracy and classification capability with an\nF1-score of 92% and detection accuracy of 93% on Edge-\nIIoTset and IoT-23. Concerns about the impact on system\nresources by specifying metrics are drawn to traffic and CPU\nusage on both devices, while challenges include the lack of\nground-truth data in most cyberattacks. Future research shall\nbe on few-shot learning, lightweight model implementation,\nDL cutting-edge methodologies, penetration testing, and the\nuse of additional sensor and actuator data to enhance the\nanomaly detection system.\nUsing the IoT 2023 dataset as a thorough benchmark, the\nstudy of [23] tackles the issue of feature extraction from IoT\ndata. The goal is to gain a better understanding of the dataset's\nproperties and possible uses by evaluating both classic statis-\ntical approaches and ML-based methods. Feature extraction\ntakes on more significance in the context of the IoT because\nof the \"curse of dimensionality,\" the well-known fact that data\nprocessing and analysis get more complicated as the number\nof dimensions grows. Various techniques have been surveyed\nto place them in the context of their strengths in capturing\nrelevant information, reducing dimensionality, and improving\nperformance in IoT analytics. Some key findings in this respect\ninclude the Hughes phenomenon: classifier performance may\nget better with more features up to some optimal point before\ndeteriorating. This paper guides the choice of suitable feature\nextraction methods to be deployed for various IoT applications\nvia ample experiments and performance analysis. This will,\ntherefore, help in the practical development of IoT solutions\nin 2023 and beyond. Besides, according to the authors, little\neffect of reducing features on the model performance is up\nto an accuracy of 93.04% using Decision Trees and 93.05%\nusing Random Forest (RF) models.\nUsing ML approaches, Prazeres et al. (2022) evaluates AI-\nbased malware detection in IoT network traffic. The study\nmakes use of the IoT-23 dataset using real IoT network\ntraffic of both benign and malicious, including numerous\nforms of malware including botnets and DDoS attacks. Key\nstrategies to categorize network traffic are feature selection,\ndata normalization, and the application of several ML models\n(Logistic Regression, RF, ANN, and Na\u00efve Bayes) [21].\nEspecially in multi-class issues, RF has repeatedly shown\nto be a useful classifier in identifying intrusions. For example,\nwith high accuracy rates in multi-class classification, a RF\napplication to the IoT-23 dataset revealed better performance\nthan other ML models [18].\nLikewise, XGBoost, known for its boosting power, has\ndemonstrated extraordinary intrusion detection capability. As\nnetwork IDS built for IoT networks show, XGBoost low-\ners mistakes and raises classification accuracy by iteratively\nstrengthening weak models [6]. Research also shows how\nwell it can handle prevalent network dataset class imbalance\nproblems [13].\nUsing multiclassification models within the PySpark archi-\ntecture, a 2024 study by Alrefaei et al. [4] offers an IoT\nnetwork real-time intrusion detection system (IDS). One-Vs-\nRest (OVR) method ML approaches include RF, Decision\nTrees, Logistic Regression, and XGBoost help to enhance\ndetection accuracy and minimize prediction latency. Class\nimbalance is solved via data cleansing, scaling, and SMote\nusing the IoT-23 dataset. RF displayed the fastest prediction\ntime at 0.0311 seconds, but XGBoost achieved the highest\naccuracy at 98.89%, underlining the system's value in real-time IoT threat detection and so reducing security concerns.\nCombining the advantages of several classifiers, ensemble\napproaches have shown notable gains in IDS performance.\nIn particular, hybrid models, which combine classifiers in\nvoting systems to improve overall accuracy and detection rate,\nhave encouraging results. Upadhyay et al. (2021) presented a\nmajority voting ensemble combining RF, XGBoost, and KNN\nwith additional classifiers for SCADA-based power grids.\nTheir model demonstrated gains in binary and multi-class\nclassification by selecting features using Recursive Feature\nElimination and then utilizing majority voting to increase\nprecision and recall [26].\nA hybrid model that included RF, XGBoost, and KNN\nand was improved using feature selection approaches was\nalso investigated by Liu et al. (2023). When tested on many\ndatasets, their approach showed an increase in overall detection\naccuracy and a decrease in false positives. In this study, the\nvoting system of a hybrid classifier, which combines other\nindividual classifiers, is achieving better performance than\nstand-alone classifiers [17].\nResearch shows that a voting ensemble of classifiers includ-\ning RF, XGBoost, AdaBoost, KNN, and SVC performs better\nthan any classifier taken on alone. Leevy et al. (2021) evaluated\ndifferent ensemble models, including XGBoost and RF, to find\nassaults in the framework of the IoT. Their studies show that\nin terms of adaptation and accuracy, ensemble models usually\noutperform individual classifiers [15].\nWhen testing IDS, the gold standard is the IoT-23 dataset,\nwhich simulates real-world IoT network traffic. It provides\nseveral attack scenarios that researchers can use to test the\nmulti-class classification capabilities of DL and ML models."}, {"title": "III. DATA PRE-PROCESSING", "content": "Data pre-processing is essential in preparing datasets for\nanalysis in training ML models. This stage ensures that the\ndata is well organized, cleaned, and structured in such a way\nas to guarantee meaningful insights in the subsequent pro-\ncessing. Data pre-processing includes identifying the dataset\ntype, understanding class distributions, uncovering patterns,\nand extracting other useful information that will drive how\nthe data should be analyzed. This leads to improved model\nperformance and reliability [2].\nThe following sections will discuss the steps to pre-process\nour data."}, {"title": "A. Data Analysis", "content": "Data analysis is a crucial stage in the pipeline of data pre-\nprocessing. It is used to study the dataset for its structure,\ndistribution of classes, and any inherent patterns or anomalies.\nThese are important for formulating a strategy on how the data\nshould be transformed, normalized, or cleaned to optimize the\nresults in further analysis.\nClass Distribution: We used 100,000 data observations for\nbinary classification, and for multi-class classification, we used\n69,398 data observations. Each class has 50,000 observations\nfor binary classification and for multi-class classification, each\nclass has around 10,000 observations.\nFor binary classification there are only two classes: Benign\nand Malicious and for the multi-class classification, there\nare seven classes: Benign, C&C- HeartBeat, DDoS, Okiru,\nPartOfHorizontalPortscan, C&C, and Attack."}, {"title": "B. Missing data Handling", "content": "Missing data handling refers to missing values so they don't\nnegatively impact target prediction. To address the missing\nvalues in the IoT-23 dataset, we substituted the empty values\nof numerical features with the \"mean\" value of that feature.\nWe have chosen to fill the empty values with mean because\nif we keep them, it will compromise the data integrity and\ndistort the proper data distribution. Additionally, there was a\ncategorical feature called service that had been filled with the\nvalue \"unknown\" and also had empty values.\nWe didn't use the \"mean\" values for the categorical features\nbecause this feature will be converted into 0 and 1 later. If we\nfill the empty values of this feature with the mean, then it\ncannot indicate the classes of that feature because the mean\nwill be the average of that feature, which is a float value.\nEffective handling of missing data achieves the integrity of\nmodel training and minimizes the risk of biased or incorrect\npredictions [22]."}, {"title": "C. Categorical Feature Conversion", "content": "We need to convert the categorical features into numeric\nvalues to train the ML models because the models cannot\nunderstand a word as an input value. To do that, we applied\nthe One-hot encoding method, which converts features such\nas if a specific class is present in an observation. It denotes\nthe value of that observation for that class as one otherwise 0."}, {"title": "D. Feature Engineering", "content": "The goal of feature engineering in ML is to improve a\nmodel's prediction power by enhancing it with additional\nfeatures or tweaking its current features. It converts raw data\ninto a format better suited for model training so the computer\ncan more effectively find significant trends, correlations, and\npatterns.\nPractical feature engineering uses domain expertise; for\ninstance, dimensionality needs to be decreased to increase\ncomputational efficiency, and data quality needs to be en-\nhanced by building meaningful features. This technique makes\nmodel accuracy and interpretability and tailoring ML solutions\nto particular application contexts possible through this tech-\nnique [14]."}, {"title": "E. Data Splitting", "content": "In ML, data splitting refers to partitioning a dataset into\ndistinct subsets to facilitate model training, validation, and\nevaluation. This step is critical for ensuring that an ML model\ndoes not simply memorize the training data, a problem known\nas overfitting, but instead learns to generalize effectively to\nunseen, real-world data. Proper data splitting helps evaluate\nthe model's performance on independent data, objectively\nassessing its predictive capabilities. The following sections\ndiscuss the specific data-splitting ratios used in our models\nand their significance for achieving robust generalization."}, {"title": "1) Splitting Ratio of the models", "content": "We had taken 80% of the\ntotal data for training and 20% of the total data for testing.\nAs we use cross-validation, which we will present later, it will\nrandomly split the training dataset into validation and training\ndata."}, {"title": "2) K-Fold Cross Validation", "content": "K-fold cross-validation is a\nresampling method for assessing ML models that involves\nsplitting the data into k equal-sized folds. Each iteration's\nvalidation set is one-fold, while the training set consists of\nthe remaining K-1 folds. A more accurate evaluation of the\nmodel's efficacy can be obtained by averaging the results of\nthis k-times method.\nIt aids in avoiding overfitting. Moreover, it offers a more rea-\nsonable assessment of a model's generalization ability. Picking\nK (usually 5 or 10) strikes a compromise between computing\nexpense and the stability of the performance estimate. With K-\nfold cross-validation, you can get strong accuracy metrics from"}, {"title": "F. Feature Scaling", "content": "In ML, feature scaling is a preprocessing method used to\nchange the range or distribution of data values, guaranteeing\nconsistency across features. For KNN, this stage is essential\nsince it standardizes or normalizes the data to a uniform\nscale, lowering the variability among features. Eliminating\nbias resulting from features of different magnitudes, which\ncould skew model predictions, depends on feature scaling.\nBy lowering variations in feature range, techniques such as\nnormalization and standardization help ML models to operate\nbetter [19]. For the feature scaling of the IoT-23 dataset, we\nhave used Min-Max Scaler, as provided below.\nMin-Max Scaling: Min-Max in ML scaling is a normalizing\ntechnique used to rescale traits within a specific range, usually\n[0, 1]. This approach guarantees that no feature dominates\nthe others and contributes equally [5]. It is also beneficial\nin the cases of variable scales or units for the features. The\nconversion is accomplished with the following formula:\n$X' = \\frac{X - X_{min}}{X_{max} - X_{min}}$"}, {"title": "G. Challenges", "content": "During the pre-processing of the data, we faced three crucial\nchallenges, which were data volume, class imbalance, and\ninformation leakage:"}, {"title": "1) Data Volume", "content": "Data volume plays a vital role in ML.\nOur IoT-23 dataset had 21 GB of data with around 60 million\nobservations, making the pre-processing tasks like feature\nscaling, one hot encoding, and label encoding very slow.\nMoreover, it made the training process very slow, too, and\nthat's why, to mitigate these problems, we took a portion\nof data for binary and multi-class classification, which will\nnot affect the model performance negatively. Also, after doing\nthat, the pre-processing tasks became very fast along with the\ntraining process."}, {"title": "2) Class Imbalance", "content": "In ML, the class imbalance results\nfrom an unbalanced distribution of classes in a dataset whereby\none class (or a small number of classes) has significantly more\ninstances than others. This usually happens in classification\nproblems when some events or results are rare in relation to\nothers. There was a class imbalance in our dataset consisting\nof eleven assault kinds or classes; as a result, we removed the\nclasses with the fewest observations and kept seven that had\na sufficient amount of data."}, {"title": "3) Information Leakage", "content": "In ML, information leakage re-\nsults from a model inadvertently accessing unrestricted mate-\nrial from the training set, producing poor generalization and\nunrealistic performance measures and possibly undermining\nthe predicting capability of the model. In our case, after the\ndata pre-processing, when the training of the models just\nstarted, we have observed that from the very first epoch, the\naccuracy is too high, like over 90%, which is not sophisticated\nbecause in the very first epoch, no model can be perfect or\nclose to perfect so after getting this kind of output, we had\nsuspected that there is an information leakage which was\nduring the data scaling. Before we knew the problem, we\nscaled the whole dataset. At that time, the scaler we had\nused gained access to all the data, which was not supposed\nto have happened, and we scaled the data according to that.\nAfter finding the problem, we split the data. Then, we fit our\nscaler on the training data and used that scaler to transform our\ntraining, testing, and validation dataset. In this way, the scaler\nwas not gaining access to the validation and testing data."}, {"title": "IV. METHODOLOGIES", "content": ""}, {"title": "A. Binary vs. Multi-class Classification", "content": "Binary classification represents predicting if an IoT device\nis being attacked or not which means it has two outputs one\nis benign and another one is malicious if the device is being\nattacked it will output as malicious and if it is not, it will output\nas benign. We had done research work on Binary Classification\n[3] where we had used several ML and DL techniques and\namong those our XGBoost has performed the best with 98.9%\nAccuracy, 98.5% of Precision, 98.7% of Recall and 98.9% of\nF1-Score.\nMulti-class classification represents predicting the attack\ntype like Okiru, DDoS, etc., and if it is not being attacked\nthen representing it as benign.\nWe proposed two different hybrid models that use voting for\nclassification. In these two models, we combined XGBoost,\nRF, and KNN for binary classification and combined RF,\nXGBoost, and AdaBoost for multi-class classification. In the\nnext sections, we will present these methods."}, {"title": "B. Random Forest", "content": "Random Forest (RF) is one of several ensemble methods of\nML that offer a versatile approach suitable for regression and\nclassification. The mode of the classes used for classification\ntasks can be generated by training with a large number of\ndecision trees. This method makes RF quite successful for\nclassification and regression projects and helps lower overfit-ting [24]."}, {"title": "C. Extreme Gradient Boosting (XGBoost)", "content": "Considered a subset of ensemble learning and a ML method\nincluded in the gradient boosting framework is extreme gradi-\nent boost boosting (XGBoost). Beginning with simple learn-\ners, decision tree regularization methods help increase model\ngeneralization. XGBoost is computationally efficient; it offers\nsound processing, perceptive feature significance analysis, and\nsmooth management of missing data. In a study by Chen et\nal. [8], the following statement is mentioned: \"Especially in\nproblems with complicated data linkages, XGBoost can reach\nstate-of-the-art performance using this sequential boosting\nwith regularization approach.\""}, {"title": "D. K-Nearest Neighbors (KNN)", "content": "Using a distance metric, typically Euclidean distance, the\nKNN algorithm classifies data points according to the majority\nlabel of their nearest neighbors; it finds extensive use in\nnon-parametric ML tasks like regression and classification.\nAlthough KNN is computationally heavy for big datasets since\nit must compute distances for every query point, it is popular\nfor its ease of use and effectiveness in uses, including pattern\ndetection and recommendation systems [27]."}, {"title": "E. AdaBoost", "content": "Combining several weak classifiers, the AdaBoost (adaptive\nboost) algorithm is an ensemble learning method that produces\na strong classifier with better accuracy. AdaBoost strengthens\nthe general prediction performance of the model by iteratively\nchanging the weights of training data, hence focusing on the\nsamples that past classifiers misclassified. This method espe-\ncially helps to raise classification accuracy in many different\nfields [16]."}, {"title": "F. Hybrid Voting Classifier", "content": "In ML, a hybrid model combines several methods or models\nto improve predicted accuracy and robustness based on voting.\nIn these hybrid systems, voting functions as a consensus\nprocess whereby every model votes on the forecast result,\nand the final choice is determined depending on a weighted\nor majority voting system. Particularly useful in classification\nproblems, voting-based hybrid algorithms use the different\ncapabilities of several models to manage heterogeneous and\ncomplex data, hence improving prediction accuracy [10].\nBelow (Figures 3 and 4) are the flow chart representation\nof the decision process in our Binary and Multi-class classifi-cation models:"}, {"title": "V. EVALUATION, RESULTS AND DISCUSSION", "content": "We have evaluated our models in terms of 4 different scales;\nF1-Score, Accuracy, Precision, and Recall. Below are the\nformulas for those metrics:\n$Accuracy = \\frac{Number \\;of \\;Correct \\;Predictions \\;(TP \\;+\\;TN)}{Total \\;Number\\; of\\; Predictions\\; (TP+TN+FP+FN)}$\n$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP + FN}$\n$F1-Score = 2 \u00d7 \\frac{Precision \u00d7 Recall}{Precision + Recall}$\nHere; TP stands for True Positive, TN stands for True\nNegative, FP stands for False Positive and FN stands for False\nNegative.\nWe had listed down the evaluation metrics values from the\nmodels of our related works in a tabular format where Table I\nis for Binary Classification and Table II is for Multi-Class\nClassification.\nAfter analyzing the comparison tables (Tables I- II), we\ncan conclude that our hybrid model for binary classification\noutperforms all other relevant proposed schemes and mod-\nels in the literature, in all metrics, Accuracy (A), Precision\n(P), Recall (R), F1-Score (F1), respectively. For multi-class\nclassification, our hybrid model outperforms all other relevant\nproposed schemes and models in the literature in all metrics\n(A, P, R, F1); except RF of Paper [21] (slightly worse in\nP and F1), Filtered Classifier of [11] (slightly worse in A).\nHowever, these models are not tree-based, therefore they are\nnot expected to be as scalable and faster as our proposed\nHybrid model, which is based on tree-based ML algorithms.\nNow, the confusion matrix of the hybrid models of binary\nand multi-class classification will be described below:"}, {"title": "VI. CONCLUSION", "content": "The rapid expansion of the Internet of Things (IoT) has\nincreased its vulnerability to sophisticated cyber threats, neces-\nsitating robust and scalable Intrusion Detection Systems (IDS).\nThis study introduces a hybrid approach to mitigate these\nthreats, leveraging the IoT-23 dataset. Our hybrid classifier\nmodels integrate tree-based algorithms-Random Forest (RF),\nXGBoost, and AdaBoost for multi-class classification, and RF,\nXGBoost, and K-Nearest Neighbors (KNN) for binary clas-\nsification. These models achieved exceptional performance,\nreaching 99.99% accuracy, precision, recall, and F1-score\nacross all metrics for binary classification, and 99% for\neach metric in multi-class classification. This performance\nsurpasses existing binary classification results and improves\nupon some multi-class metrics (as detailed in Tables I and II).\nThe scalability of our approach stems from the use of tree-based machine learning models in conjunction with a large\ndataset, enabling it to effectively handle the complexities of\nIoT environments. The superior performance of our hybrid\nmodels compared to those in Table I and Table II demonstrates\ntheir suitability for IoT IDS.\nFuture research will focus on real-time implementation,\nfurther feature selection optimization, adaptability to emerging\ncyber threats, and the integration of explainable AI to enhance\ntransparency and trust in cybersecurity decision-making."}]}