{"title": "Exploring Gaze Pattern in Autistic Children: Clustering, Visualization, and Prediction", "authors": ["Weiyan Shi", "Haihong Zhang", "Jin Yang", "Ruiqing Ding", "YongWei Zhu", "Kenny Tsu Wei Choo"], "abstract": "Autism Spectrum Disorder (ASD) significantly affects the social and communication abilities of children, and eye-tracking is commonly used as a diagnostic tool by identifying associated atypical gaze patterns. Traditional methods demand manual identification of Areas of Interest in gaze patterns, lowering the performance of gaze behavior analysis in ASD subjects. To tackle this limitation, we propose a novel method to automatically analyze gaze behaviors in ASD children with superior accuracy. To be specific, we first apply and optimize seven clustering algorithms to automatically group gaze points to compare ASD subjects with typically developing peers. Subsequently, we extract 63 significant features to fully describe the patterns. These features can describe correlations between ASD diagnosis and gaze patterns. Lastly, using these features as prior knowledge, we train multiple predictive machine learning models to predict and diagnose ASD based on their gaze behaviors. To evaluate our method, we apply our method to three ASD datasets. The experimental and visualization results demonstrate the improvements of clustering algorithms in the analysis of unique gaze patterns in ASD children. Additionally, these predictive machine learning models achieved state-of-the-art prediction performance (81% AUC) in the field of automatically constructed gaze point features for ASD diagnosis. Our code is available at https://github.com/username/projectname.", "sections": [{"title": "I. INTRODUCTION", "content": "Autism Spectrum Disorder (ASD) is a developmental disorder marked by challenges in social interaction, communication, and repetitive behaviors, and is increasingly prevalent worldwide [1], [2]. Thus, diagnosis and intervention tools are highly demanded. Currently, diagnostic tools focus on eye-tracking as it can capture atypical gaze patterns, which are considered a key indicator of ASD [3]. To be specific, extensive gaze signal data are collected in eye-tracking experiments. These gaze points can reveal differences in how individuals with ASD focus on social cues compared to typically developing (TD) children [4]\u2013[6]. Areas of Interest (AOIs) have been used to analyze differences in gaze patterns between ASD and TD individuals by calculating the time spent viewing specific regions in images or videos [7], [8]. Traditional methods demand heavy manual labeling for AOIs, limiting their efficiency. Additionally, it may not fully capture the complexity of gaze behaviors, especially when AOIs are ambiguous, thus lowering analysis performance.\nUnsupervised clustering can be used to group gaze points based on spatial and temporal characteristics automatically, potentially revealing more nuanced patterns without the need for predefined AOI. However, only a few methods apply clustering to automatically analyze gaze patterns from gaze points. These methods cannot process complex stimuli, leading to insufficient exploration of complex patterns. For example, KMeans [9] clustering techniques are employed to gaze points and then evaluated by silhouette coefficients (SC) [10]. These gaze points are generated from children's viewing of simplistic image stimuli composed of only two parts [11]. Additionally, few studies have been conducted to explore whether clustering techniques can improve visualization to efficiently extract distinctive gaze.\nMoreover, the correlations between the features extracted from clustering algorithms and ASD have not been investigated. Thus, few works employ predictive models to predict ASD based on gaze signal data.\nTo tackle these limitations, we propose a novel method for automatic ASD gaze analysis. This method consists of four main steps: unsupervised clustering on gaze points from complex visual stimuli, visualization enhancement of ASD gaze points, significant feature extraction for ASD prediction, and comprehensive benchmarking of predictive models. Specifically, we first perform unsupervised clustering on gaze data using seven different clustering algorithms, which are then optimized based on the SC. To evaluate the effectiveness of these clustering algorithms qualitatively, we visualize gaze data by highlighting clustered gaze points with convex hulls overlaid on the original image, aiding in the analysis of spatial distribution. Subsequently, we utilize nine indices to evaluate unsupervised clustering quality. These indices are then used to extract multiple significant features to describe gaze patterns. Finally, these features can be utilized to train an ensemble of multiple predictive machine-learning models for AD predictions. Our contributions can be summarized as\n1) We conducted the first comprehensive significance test of clustering algorithms combined with clustering in-"}, {"title": "II. METHODOLOGY", "content": "A. Unsupervised Clustering on Gaze Points from Complex Visual Stimuli\nDuring preprocessing, we prepare data for clustering algorithms by removing invalid gaze points and extracting data from individual children and specific experiment segments. For a thoughtful analysis, we employed four different types of clustering algorithms and selected the most superior ones from each type.\n\u2022 Partitioning Methods: K-Means [9] and K-Medoids [12]. These two methods employ different mechanisms to cluster data points, so more feature patterns can be captured.\n\u2022 Hierarchical Methods: Agglomerative Clustering (AC) [13] and BIRCH [14]. AC and BIRCH cluster data into two different structures, a hierarchy, and a tree, thus exploring the most suitable structure to present data.\n\u2022 Density-Based Methods: DBSCAN [15] and OPTICS [16]. They identify clusters based on the density of points, allowing them to handle clusters of arbitrary shapes and filter out noise.\n\u2022 Model-Based Methods: GMM [17]. GMM clusters data to a mixture of separate Gaussian distributions, thus improving their diversity.\nTo ensure a fair comparison, all clustering algorithms have been optimized to their optimal models by performing the grid search and evaluating SC.\nB. Visualization Enhancement of ASD Gaze Points\nTo further understand the mechanisms of these clustering algorithms, we visualize the clustering results of ASD and TD gaze points. To be specific, we compute the convex hulls of the clustered points, overlay these hulls onto the original image, and highlight each cluster with distinct colors, making it easy to see the spatial distribution of gaze points in the context of the original visual stimuli. Additionally, we calculated the proportion of each cluster after gaze point clustering to illustrate the differences in gaze focus between the two groups of children.\nC. Significant Feature Extraction for ASD Prediction\nTo validate the effectiveness of our unsupervised clustering results, we extracted nine widely used validity indices. While some of these indices have been used in previous studies [11], but they were not applied comprehensively. These indices can be summarized as follows: (i) Compactness and Separation indices include Calinski-Harabasz (CH) [18], Davies-Bouldin's index (DB) [19], DaviesBouldin* index (DB*) [20], Dunn's index (DI) [21], and Chou-SuLai (CSL) [22], which evaluate clustering quality by balancing intra-cluster compactness with inter-cluster separation. (ii) Similarity and Separation indices include SC [10], which assesses clustering by comparing intra-cluster similarity to inter-cluster separation. (iii) Generalized Measures include the Generalized Dunn index (GD33) [23], which considers complex cluster shapes with generalized distance metrics. (iv) Comprehensive Indices such as the PBM index (PBM) [24] and STR index (STR) [25] combine multiple features like compactness, separation, and stability for a holistic clustering evaluation. Subsequently, we conducted a Mann-Whitney U test [26] on these observation values to explore statistically significant differences between ASD and TD children groups.\nD. Comprehensive Benchmarking of Predictive Models\nBased on the first step using seven unsupervised clustering methods and the second step extracting nine different indices from each algorithm, we extracted a total of 7 \u00d7 9 features. We employed several machine learning models to utilize these 63-dimensional features to predict ASD and TD children.\nFor a comprehensive benchmarking of predictive models, we selected the most suitable models from four different types of machine learning models:\n\u2022 Linear Models: Logistic Regression (LR) [27] is simple, interpretable, and effective when features are linearly separable. Support Vector Machine (SVM) [28] works well in high-dimensional spaces and can handle non-linear data using kernel functions.\n\u2022 Instance-based Models: K-Nearest Neighbors (KNN) [29] is simple and effective for small datasets, requiring no training phase and classifying data based on the nearest neighbors.\n\u2022 Tree-based Models: Decision Tree provides an intuitive model for splitting data. Random Forest [30] uses bagging to reduce overfitting, improve accuracy, and handle large datasets. XGBoost [31] employs boosting, which is effective with imbalanced datasets and reduces overfitting by focusing on correcting errors from previous trees.\n\u2022 Neural Networks: Multilayer Perceptron (MLP) [32], with a structure of 63 \u00d7 128 \u00d7 32 \u00d7 1, captures complex, non-linear relationships and is effective for large, high-dimensional data.\nThe performance of these predictive models is evaluated by accuracy, precision, recall, F1-score, and AUC."}, {"title": "III. EXPERIMENT", "content": "A. Dataset\nWe utilize three datasets of eye-tracking in ASD children to exploit three types of visual stimuli, including structured, semi-structured, and non-structured categories, respectively. (i) The structured category is provided in Qiao's study [33], which involved 74 children (50 ASD, 24 TD) aged around 5. They used a Tobii X120 eye tracker to collect gaze point data, where the child viewed four objects placed at the top, bottom, left, and right, with cartoon faces, fingers, or arrows as directional cues. (ii) The semi-structured category is from Cilia's study [34], which analyzed gaze patterns of 59 children (30 TD, 29 ASD) aged 3-12. They used an SMI Red-M eye tracker to collect eye-tracking data as subjects viewed photos and videos with content ranging from colorful balloons and cartoons to human presenters guiding attention. (iii) The non-structured category is represented by the Saliency4ASD dataset [35], where 28 children (14 ASD, 14 TD) aged 5-12 used a Tobii T120 eye tracker to view 500 images selected from three public eye-tracking datasets, featuring diverse semantic content such as animals, buildings, and people. Qiao and Cilia only provided gaze points, while Saliency4ASD provided visual stimuli along with gaze points. For the experimental dataset, each entry represents one complete instance of a child viewing a single visual stimulus. The Qiao dataset contains 8,790 entries, Cilia's dataset has 2,612 entries, and Saliency4ASD includes 602 entries.\nB. Experimental Results\nTable I shows the experimental results. KMedoids, BIRCH, and OPTICS achieved strong performance, with more than 80% of their indices showing significance. KMedoids performed the best with 89%. In contrast, the other algorithms were less effective at distinguishing between ASD and TD children, with significance ratios below 70%. The Qiao dataset performed the best overall, with all algorithms showing strong results across all indices. This may be due to the fact that the gaze points in Qiao were generated from highly structured visual stimuli. In contrast, the less structured visual stimuli in the Cilia and Saliency4ASD datasets resulted in significantly lower proportions of significant indices, at 57.1% and 68.3%, respectively.\nSince previous baselines are all based on AOI-constructed gaze point features, our method achieves an average AUC of 81%, which is currently state-of-the-art in the field of automatically constructed gaze point features for ASD diagnosis."}, {"title": "C. Visualization Results", "content": "Figure 1 shows the visualization results. Density-based clustering is more effective in capturing unique gaze patterns in ASD children, while other methods provide less clear visualizations. In the face-focused scene, DBSCAN shows ASD children focusing on the yellow region (32.89%) and green region (23.49%) with 43.62% noise, while TD children focus 59.81% on the yellow region and 40.19% noise. OPTICS presents similar trends. ASD children also paid attention to the guitar, while TD children focused mostly on faces. In the daily room scene, DBSCAN shows ASD children focusing on the yellow region (23.49%) and green region (32.89%) with 43.62% noise, while TD children focus 59.81% on the yellow region and 40.19% noise. OPTICS shows ASD children focusing on tables and windows, while TD children concentrated on table items and posters.\nOur study aligns with previous research [36] [37], showing that ASD children tend to focus on non-face areas in face-focused scenes and prefer non-social elements in object-focused scenes."}, {"title": "D. Prediction Results", "content": "We conducted a benchmark experiment to explore the effectiveness of the clustering methods by employing multiple predictive models on three datasets. This experiment was conducted on the CPU with 16GB DDR4 RAM and Ubuntu 20.04 LTS system. The deep learning models were implemented using Python 3.9 with PyTorch 2.0, and traditional machine learning models were implemented using scikit-learn 1.4.0 and SciPy 1.12.0. We employed 5-fold cross-validation to ensure robust evaluation results.\nBased on the average metrics across the three datasets, Random Forest performs the best overall, achieving the highest scores in Accuracy (0.734), Precision (0.734), Recall (0.736), F1-score (0.719), and AUC (0.813). To be specific, in the Qiao dataset, MLP outperformed all models, achieving the highest scores in Accuracy (0.744), Precision (0.743), Recall (0.742), F1-score (0.741), and AUC (0.804). In the Cilia dataset, Random Forest showed the best overall performance with the top scores in most metrics, including AUC (0.813), while XGBoost had a slightly higher Accuracy (0.735). For the Saliency4ASD dataset, Random Forest dominated across all metrics, with an AUC of 0.834, demonstrating the best performance.\nThese well-performed models, including MLP, Random Forest, and XGBoost, also presented high prediction efficiency. These models can all complete inference within 0.1 seconds."}, {"title": "IV. CONCLUSION", "content": "We propose a novel method to automatically analyze gaze behaviors in ASD children, addressing the inefficiency of manually segmenting AOIs. Our method employs unsupervised clustering on gaze points from complex visual stimuli and extracts significant features for ASD prediction from clustering results. We utilize predictive models to learn gaze patterns from these features and make predictions on future ASD behaviors. We believe that our method, combined with traditional features like gaze speed and duration, as well as visual stimuli features, can lead to better insights into gaze behaviors in ASD children, providing a new direction for automating the exploration of ASD children's gaze patterns."}]}