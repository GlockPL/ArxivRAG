{"title": "DeepVoting: Learning Voting Rules with Tailored Embeddings", "authors": ["Leonardo Matone", "Ben Abramowitz", "Nicholas Mattei", "Avinash Balakrishnan"], "abstract": "Aggregating the preferences of multiple agents into a collective decision is a common step in many important problems across areas of computer science including information retrieval, reinforcement learning, and recommender systems. As Social Choice Theory has shown, the problem of designing algorithms for aggregation rules with specific properties (axioms) can be difficult, or provably impossible in some cases. Instead of designing algorithms by hand, one can learn aggregation rules, particularly voting rules, from data. However, the prior work in this area has required extremely large models, or been limited by the choice of preference representation, i.e., embedding. We recast the problem of designing a good voting rule into one of learning probabilistic versions of voting rules that output distributions over a set of candidates. Specifically, we use neural networks to learn probabilistic social choice functions from the literature. We show that embeddings of preference profiles derived from the social choice literature allows us to learn existing voting rules more efficiently and scale to larger populations of voters more easily than other work if the embedding is tailored to the learning objective. Moreover, we show that rules learned using embeddings can be tweaked to create novel voting rules with improved axiomatic properties. Namely, we show that existing voting rules require only minor modification to combat a probabilistic version of the No Show Paradox.", "sections": [{"title": "Introduction", "content": "Research in Computational Social Choice (COMSOC) and Algorithmic Game Theory (AGT) focuses heavily on the design and analysis of mechanisms for collective decision making. Canonically, agents arrive with individual preferences over a set of alternatives or outcomes, and a centralized mechanism must aggregate these preferences into a shared choice (voting and selection) or allocation of items (matching and auctions) Shoham and Leyton-Brown [2008]. The goal is to design mechanisms with certain desirable properties, characterized by axioms; i.e. optimizing a particular objective or satisfying certain constraints.\nOne of the central results in social choice is Arrow's General Impossibility Theorem Arrow et al. [1963], which identifies a set axioms that no collective mechanism can satisfy at once. Following Arrow, decades of research has produced myriad theorems showing which axioms are satisfied by which mechanisms and which sets of axioms lead to an impossibility result Sen [2018]. On the algorithmic and computational side this includes properties of optimality and computational complexity Brandt et al. [2016].\nFinding rules that satisfy a given set of axioms can be difficult, especially when it is unknown if such a rule exists. Hence, recent work has turned to machine learning techniques to design novel mechanisms. This idea has been applied in areas from auctions to voting rules to matchings Xia [2013], Sandholm [2003], Curry et al. [2022], Ravindranath et al. [2021]. Previous work on learning voting rules has been hampered by technical challenges, including extremely large/sophisticated neural nets Anil and Bao [2021], limited data Burka et al. [2022], or failure to account for the full consequences of the design choices Firebanks-Quevedo [2020].\nOur approach to improving the learning of existing and novel voting rules is to use hand-crafted embeddings derived from the social choice literature. As we show, these embeddings enable fast learning with fewer parameters and the ability to scale to larger populations of voters. Our embeddings reduce the input size of our neural net, greatly reducing the number of model parameters. Anil and Bao [2021] found that using a multi-layer perceptron (MLP), i.e., a deep neural network, to learn voting rules was limited by the network's fixed input size, and so they introduced more sophisticated architectures to permit scaling and pad smaller profiles. In their tests to see which architectures scaled best with the number of voters, their MLP architecture was necessarily excluded. By contrast, our embeddings enable the size of the input layer of the MLP to be independent of the number of voters, thereby enabling greater generalization with scale. Similarly, when we train or test our model with preference profiles containing fewer voters, our input does not require any form of padding. As we show, when using embeddings to learn particular rules or axioms, the choice of embedding must correspond to the learning objective.\nSen [2018] observed that mechanisms can be examined in terms of what information they use and ignore from preference profiles. A key contribution of our work is a more complete understanding of the relationship between the choice of embedding and the resulting learnability and efficiency of the mechanisms. We observe that, like any compression algorithm, embeddings can be lossy and impose a bound on the learnability of rules and axioms. Indeed, we show that certain embeddings lose the required information needed to learn particular rules. Our experiments have also lead us to pose new theoretical questions about the information preservation of embeddings.\nWhen most people think of voting they think of classical deterministic rules that take in preferences over a small set of candidates and return a single winner Zwicker"}, {"title": "Related Work", "content": "[2016], Taylor [2005]. However, the full space of social choice mechanisms is much richer with mechanisms varying by the data types of their inputs and outputs. For example, voters may give approval ballots, rankings, or weightings to different candidates; the outcome of the mechanism may be a single winning candidate, collection of winners, or ordering of the candidates, among other options.\nWe study probabilistic social choice functions (PSCFs) which take a profile over candidates and return a lottery (probability distribution) over the candidates Brandt [2017]. PSCFs offer several advantages when learning voting rules with neural networks as compared to single-winner voting rules. First, since we are outputting a lottery over the alternatives, we are not as concerned with tie-breaking. Tie-breaking introduces complications into the design and analysis of voting rules Aziz et al. [2013b], and random tie-breaking is not learnable. Second, PSCFs provide a natural connection between the discrete formulations of rules and axioms and the construction of continuous loss functions for training based on divergences between distributions, e.g. L1.\nAfter first showing that existing rules can be efficiently learned using tailored embeddings, we address the challenge of taking existing rules and tweaking them to improve their axiomatic properties. In particular, we focus on the No Show Paradox in which a voter can induce an outcome they prefer by abstaining instead of voting Moulin [1988]. Certain single-winner rules, like Plurality, Borda, and Simpson-Kramer are known to satisfy the Participation axiom, so no voter can be made better off by abstaining and the paradox does not arise. However, most other common voting rules exhibit this paradox. In our experiments, we take models trained to learn existing PSCFs and retrain them using a loss function that adds in a continuous relaxation of the Participation axiom and show which rules can be adjusted to be more resistant to the paradox without sacrificing accuracy.\nWe choose the Participation axiom in particular because it is an inter-profile axiom, which requires reasoning about counterfactuals on what the preference profile could have otherwise been had the voters behaved differently. Inter-profile axioms are particularly challenging for learning from data because they increase the computational complexity of computing loss functions. It also requires that the model be able to take profiles of different sizes (differing by one voter) as input, which our embeddings enable us to do. Since abstention can be a strategic behavior by voters, learning the Participation axiom is closely related to Automated Mechanism Design, which focuses on creating desirable economic mechanisms for strategic agents and \"shifts the burden of design from man to machine\" Sandholm [2003].\nXia [2013] and Procaccia et al. [2009] proposed incorporating voting axioms into a machine-learning framework as a means of evaluating learned social choice mechanisms. In the space of auction design and matching there has been work on using neural nets for better mechanisms D\u00fctting et al. [2019], Pavlov [2011], Malakhov and Vohra [2008] including learning new types of auction mechanisms Curry et al. [2022] as well as complex preference structures Peri et al. [2021]. More recently, the work of Ravindranath et al. [2021] has looked at how to learn new allocation mechanisms"}, {"title": "Preliminaries", "content": "that bridge the gap between stability (as compared to the deferred acceptance algorithm Gale and Shapley [1962]) and strategyproofness (as compared to random serial dictatorship (RSD) Aziz et al. [2013a]). While the work of Ravindranath et al. [2021], Firebanks-Quevedo [2020], and most recently Anil and Bao [2021] has shown promise for learning mechanisms, these efforts do not closely consider the role of embeddings.\nWhile formal proposals to learn voting rules date back over a decade Xia [2013], considerable attention to learning voting rules has increased in recent years. Kujawska et al. [2020] and Burka et al. [2022] used several common machine learning methods to mimic existing voting rules. However, both of these works overlooked the importance of the choice of embedding in the role of learning, finding that certain rules were \"easier\" to learn. Subsequently, Anil and Bao [2021] showed that PIN architectures have the advantage of better generalization to larger numbers of voters. We build on this work by showing that we can achieve high accuracy efficiently with small MLPs by using hand-crafted embeddings.\nProcaccia et al. [2009] showed that positional scoring rules are efficiently PAC learnable, but learning pairwise comparison-based voting rules requires an exponential number of samples. While we do not escape the asymptotic limits, we examine two embeddings based on tournament graphs that are design to facilitate more efficient learning of pairwise-comparison based rules. Firebanks-Quevedo [2020] use one measure of optimality (Condorcet consistency) and strategyproofness for learning. However, the manipulations of the chosen embedding cannot learn strategyproofness, leading to poor results. Finally, Wilson [2019] focus on the problem of learning a voting rule given a set of pair-wise relations and properties that must hold for the optimization criteria. However, this work is focused on the possibility of learning these functions and does not employ any machine learning techniques.\nThe loss function chosen by Armstrong and Larson [2019] was a function of the profile and outcome, and thus could learn a rule but not inter-profile axioms like Participation. Recently, Mohsin et al. [2022] focused on the problem of designing and/or learning fair and private rules, proving that under measures of differential privacy there is an upper bound on the trade-off between group fairness and efficiency.\nLearning voting rules bears some similarity to the well studied area of learning to rank (L2R) from the machine learning literature Cao et al. [2007]. In L2R one typically is concerned only with accurate recovery of the population preference and not the axioms or properties of the aggregation method itself (e.g., fairness). Indeed, one can think of our work enforcing inter-profile axioms on the learned aggregation procedures as an important step.\nAgents and Preference Profiles Let V be a set of n voters and C a set of m candidates. Each voter $i \\in V$ reports a strict order $x_i$ over all the candidates in C as their ballot. We will denote that i strictly prefers a over b by $a >_i b$ for $a, b \\in C$. There are $m!$ possible ballots, or ways to strictly order (permute) the candidates in C. A list of n ballots, one for each voter, constitutes a profile $X = (x_i)_{i\\in v}$. Voter i ranks candidate a at position $x_i(a) \\in [m]$, using $[k] = \\{1,...,k\\}$. Let X be the set of all possible"}, {"title": "PSCF Preservation Under Embedding", "content": "profiles.\nProbabilistic Social Choice Functions A probabilistic social choice function (PSCF) is a function $f : X \\rightarrow \\Delta(C)$ that takes a profile $X \\in X$ as input and returns a lottery, or probability distribution $f(X) \\in \\Delta(C)$ over the set of candidates in the profile, where $\\Delta(C)$ is the set of all lotteries over C. Let F be the set of all such PSCFs.\nAny PSCF can be used to construct a non-deterministic voting rule by sampling a winner from the lottery. A PSCF that places all probability mass on a single candidate for all profiles is deterministic. If the candidate that receives all of the probability mass is the same candidate for all profiles, then it is a dictatorial voting rule. Many PSCFs we consider return for all profiles a lottery that is a uniform distribution over a non-empty subset of the candidates, i.e., there are multiple potential winners that we would have to choose from for a single-winner voting rule. Therefore, let U (Y) denote the uniform distribution over any finite set Y. When referring to lotteries over candidates, we let U(Y) denote the distribution that is uniform over $Y \\subseteq C$ and zero on $C\\backslash Y$.\nEmbedding\nSimple feed-forward neural networks require a fixed-size input for learning and inference, corresponding to the size of their input layer. If we were to learn voting rules using neural networks that take the entire profile as input, then not only does the input layer need to be large (m \u00d7 n), but it also prevents scaling up as the number of voters grows. Similarly, if the number of voters shrinks, then the profile would have to be padded carefully in a way that doesn't negatively impact the model. If we want to learn rules that are agnostic to the number of voters, we need to construct embeddings of fixed-size that retain the relevant information for profiles with any number of voters. Naturally, different embeddings preserve different information from the original profile, leading them to different efficacy when learning different rules and axioms. Note that most rules and axioms in the literature are defined for any positive number of voters, so we would like our learned mechanisms to be similarly agnostic.\nAn embedding T is a function $T : X \\rightarrow X'$ mapping profiles to some codomain X'. The embeddings we are concerned with are many-to-one mappings. This means multiple different profiles may have the same embedding, i.e. $T(X) = T(X)$ for some $X, X \\in X$ where $X \\neq X$. In other words, T will not be reversible, and T(X) will not preserve all information about X. We denote by F' the set of all probabilistic functions of the form $f' : X' \\rightarrow \\Delta(C)$. Note that while we designate X to always contain strict orders over candidates, the structure of X' will be different for different embeddings. Our three embeddings are drawn from the voting literature, but are not commonly recognized as embeddings in the machine learning literature.\nDefinition 1 (Tournament Embedding). Given a profile, the tournament embedding $T_T$ produces a $m \\times m$ matrix M where $M[j,k] = 1$ if a majority of voters prefer $j >_i k$, $M[j, k] = 0$ if a majority prefer $k >_i j$, and $M[j, k] = \\frac{1}{2}$ if an equal number of voters prefer each candidate (when n is even), for candidate pairs $j, k \\in C$.\nWe are concerned with what information is preserved by embeddings, and whether this information is sufficient to implement PSCFs, i.e. to learn them perfectly.\nDefinition 9 (PSCF Preservation). A PSCF $f : X \\rightarrow \\Delta(C)$ is preserved by embedding $T : X \\rightarrow X'$ if $\\exists f' : X' \\rightarrow \\Delta(C)$ such that $f'(T(X)) = f(X)$ for all profiles $X \\in X$.\nProposition 1 says that for an embedding T to preserve a PSCF, there cannot be two profiles with the same embedding under T for which the PSCF returns different lotteries.\nProposition 1. Embedding T preserves PSCF f if and only if for all pairs of profiles X, Y \u2208 X we have $T(X) = T(X) \\Rightarrow f(X) = f(X)$."}, {"title": "Learning Lotteries from PSCFS", "content": "Some embeddings preserve strictly greater information than others. For example, lexicographically sorting the preference orders in a profile preserves all information necessary to compute $T_{WT}$, and $T_{WT}$ preserves all information necessary to compute $T_T$ from a profile. This implies that if $T_T$ preserves a function f, then $T_{WT}$ must preserve f as well.\nProposition 2. Suppose that for $T : X \\rightarrow X'$ there exist $T_1 : X \\rightarrow X$ and $T_2 : X \\rightarrow X'$ such that $T(X) = T_2(T_1(X))$ for all $X \\in X$. Then for all $f \\in F$, T preserves f only if $T_1$ preserves f.\nSince we can compute $T_T$ from $T_{WT}$, $T_T$ can only preserve a PSCF if $T_{WT}$ does as well. However, the reverse does not hold. $T_{WT}$ may preserve PSCFs that are not preserved by $T_T$. If an embedding preserves a PSCF, then the PSCF is perfectly learnable from the embedding.\nIn our first set of experiments, we show that with proper embeddings we can learn PSCFs that generalize common voting rules using network architectures with few parameters. We train each of our 21 rule-embedding pairs separately to compare their performance, and show how the choice of embedding must correspond to the choice of rule."}, {"title": "Resisting the No Show Paradox", "content": "PSCFs based on voting rules can be vulnerable to the No Show Paradox, where a voter prefers the outcome yielded by a rule when they do not vote. The voter therefore has an incentive to abstain rather than report their true preference. A rule for which this cannot occur is said to satisfy the Participation axiom. We now employ transfer learning, taking our already trained models from Section 5 and retraining them with a loss function that adds a term for Participation loss, which is our relaxation of the binary participation axiom to a continuous loss function.\nFor all definitions below, let $P_X$ be a probability distribution derived from profile X by some PSCF f (implicit), and let $P_X(c)$ be the probability assigned to candidate c. Where the specific profile is not relevant, we will denote simply by $P(c)$ the probability assigned to candidate c by a lottery P. We now provide a loss function that measures\nhow well a PSCF satisfies participation or how close a PSCF comes to satisfying participation. We use stochastic dominance to model a voter's preference between two lotteries based on their preference order over candidates.\nDefinition 10 (Stochastic Dominance). Let \u03c3 be an ordering (or permutation) over the set of candidates C, and let \u03c3[k] be the $k^{th}$ element of \u03c3 for $k \\in [m]$. Given two lotteries P and Q over C, P stochastically dominates Q with respect to o if for all $k\\in [m], \\sum_{l<k} P(\\sigma[l]) \\geq \\sum_{l<k} Q(\\sigma[l])$.\nWe say that a voter's abstention leads to an outcome (P) they prefer if the new outcome stochastically dominates the outcome (Q) that would derive from the true profile, with respect to the voter's ordering of the candidates \u03c3 = $x_i$. We want our PSCF to be strategyproof with respect to the limited class of strategic abstentions.\nDefinition 11 (Participation). A PSCF f obeys participation if, for all profiles, every voter prefers the outcome under f when they vote their true preference to the outcome under f when they abstain (i.e. removed). We say that a voter prefers the outcome Q from voting truthfully over the lottery P from abstaining if Q stochastically dominates P.\nSince Participation is a binary condition for a PSCF, to learn PSCFs that resist the No Show Paradox, we define a non-binary loss function for Participation based on stochastic dominance.\nDefinition 12 (Stochastic Dominance Loss). Given ordering \u03c3 over C, a lottery P, and a reference lottery Q, we say that the stochastic dominance loss is zero if P stochastically dominates Q. If P does not stochastically dominate the reference lottery Q, then the loss is equal to $L(P|o, Q) = max_{k\\in[m]} (\\sum_{l<k} Q(\\sigma[l]) \u2013 \\sum_{l<k} P(\\sigma[l]))$, i.e. the largest difference between the sums of prefixes of the lotteries over all prefixes when the distributions' supports are ordered by \u03c3.\nDefinition 13 (Participation Loss). Given a profile X, Let $P_{ix}$ be the lottery under f when voter i abstains and all others vote truthfully, and let $Q_x$ be the lottery under f when voting truthfully. $L(f, X) = max_{i\\in V} L(P_{ix} \\mid x_i, Q_x)$."}, {"title": "Conclusions and Future Work", "content": "In this paper we have shown that not only can we efficiently learn known PSCFs from preference data, but also that we can modify these rules in order to improve them in ways that, to date, have not be possible through traditional algorithmic design methods. We have highlighted the importance of the choice of embedding on the efficiency and quality of the learned rules, finding some surprising results including that rules like IRV, Borda, and Black's Rule, which are not majoritarian rules, can be learned well from $T_{WT}$. It remains to be seen whether other embeddings can be designed, of size m \u00d7 m or smaller, that outperform the embeddings we took from the social choice literature. Different embeddings may be beneficial in particular for rules whose outcomes are NP-Hard to compute or other common axioms."}, {"title": "Additional Voting Rules", "content": "In this section we give full definitions of other voting rules we study.\nInstant Runoff Voting is not a scoring rule, but is defined by iteratively using plurality scores.\nDefinition 14 (Instant Runoff Voting (IRV)). IRV is a deterministic, iterative voting rule that, in each of m \u2212 1 rounds, eliminates the candidate with the lowest plurality score and removes them from the preference orders of all voters before the next round. When candidates are tied for lowest plurality score we break ties in lexicographically. The rule returns the lottery that assigns all probability to the single candidate that was never eliminated; $U(W(X))$ where $|W(X)| = 1$.\nBlack's rule is an example of a rule that is not a scoring rule, tournament rule, or weighted-tournament rule, but is still Condorcet-consistent.\nDefinition 15 (Black's Rule). If the profile X admits a Condorcet winner c, then let $W(X) = c$. Otherwise, if there is no Condorcet winner, let W(X) be the subset of candidates with maximum Borda score B(c). The probabilistic Black's rule returns the lottery $U(W(X))$ ."}, {"title": "Rule Preservation", "content": "Plurality and Borda are scoring rules, which are necessarily computable from a rank frequency embedding. However neither rule is preserved by the tournament embedding. Plurality is known not to be preserved by the weighted tournament either, but for Borda this remains an open question.\nPlurality\nPlurality is a scoring rule, and therefore necessarily computable from a rank frequency embedding. It requires only one column of information from the rank frequency matrix, representing how often each candidate is ranked first by a voter. By contrast, Plurality is not preserved by the weighted tournament embedding, and therefore not by the tournament embedding either.\nTheorem 1. The weighted tournament embedding does not preserve Plurality.\nProof. $X_1$ = (a > b > c), (b > a > c), (c > a > b), $X_2$ = (a > b > c), (a > b > c), (c > b > a).\nCorollary 1. The tournament embedding does not preserve Plurality.\nBorda\nTheorem 2. The tournament embedding does not preserve Borda."}, {"title": null, "content": "Proof. $X_1$ = (a > b > c), (b > a > c), (b > c > a), $X_2$ = (a > b > c), (a > b > c), (a > b > c).\nChallenge. Does the weighted tournament embedding preserve Borda?\nCopeland\nCopeland is the only probabilistic social choice function we consider that is preserved by the tournament embedding, and hence by the weighted tournament as well. However, Copeland is not preserved by the rank frequency embedding.\nTheorem 3. The rank frequency embedding does not preserve Copeland.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c > d), (b > a > d > c), (d > c > b > a).\nSchulze and Simpson-Kramer are weighted-tournament rules that are not preserved by the tournament or rank frequency embedding.\nSchulze\nTheorem 4. The rank frequency embedding does not preserve Schulze.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c > d), (b > a > d > c), (d > c > b > a).\nTheorem 5. The tournament embedding does not preserve Schulze.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c > d), (b> c > d > a), (d > a > b > c).\nSimpson-Kramer (Maximin)\nTheorem 6. The rank frequency embedding does not preserve Simpson-Kramer.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c > d), (b > a > d > c), (d > c > b > a).\nTheorem 7. The tournament embedding does not preserve Simpson-Kramer.\nProof. $X_1$ = (a > b > c > d), (b > c > a > d), (d > c > a > b), $X_2$ = (a > b > c > d), (b > c > a > d), (c > a > b > d)."}, {"title": null, "content": "IRV\nTheorem 8. The rank frequency embedding does not preserve IRV.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c>d), (b > a > d > c), (d > c > b > a).\nTheorem 9. The tournament embedding does not preserve IRV.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > c > b), $X_2$ = (a > b > c>d), (b > c > d > a), (d > a > b > c).\nChallenge. Does the weighted tournament embedding preserve IRV?\nBlack's Rule\nTheorem 10. The rank frequency embedding does not preserve Black's Rule.\nProof. $X_1$ = (a > b > c > d), (b > c > d > a), (d > a > b > c), $X_2$ = (a > b > c>d), (b > a > d > c), (d > c > b > a).\nTheorem 11. The tournament embedding does not preserve Black's Rule.\nProof. $X_1$ = (a > b > c > d), (b > c > a > d), (d > c > a > b), $X_2$ = (a > b > c>d), (b>c> a > d), (c > a > b > d).\nChallenge. Does the weighted tournament embedding preserve Black's Rule?"}]}