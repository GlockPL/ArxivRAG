{"title": "How to Measure Human-Al Prediction Accuracy in Explainable Al Systems", "authors": ["SUJAY KOUJALGI", "ANDREW ANDERSON", "IYADUNNI ADENUGA", "SHIKHA SONEJI", "RUPIKA DIKKALA", "TERESITA GUZMAN NADER", "LEO SOCCIO", "SOURAV PANDA", "RUPAK KUMAR DAS", "MARGARET BURNETT", "JONATHAN DODGE"], "abstract": "Assessing an Al system's behavior-particularly in Explainable AI Systems-is sometimes done empirically, by measuring people's abilities to predict the agent's next move-but how to perform such measurements? In empirical studies with humans, an obvious approach is to frame the task as binary (i.e., prediction is either right or wrong), but this does not scale. As output spaces increase, so do floor effects, because the ratio of right answers to wrong answers quickly becomes very small. The crux of the problem is that the binary framing is failing to capture the nuances of the different degrees of \"wrongness\". To address this, we begin by proposing three mathematical bases upon which to measure \"partial wrongness\". We then uses these bases to perform two analyses on sequential decision-making domains: the first is an in-lab study with 86 participants and a size-36 action space; the second is an analysis of a size-4 action space. Other researchers adopting our operationalization of the prediction task and analysis methodology will improve the rigor of user studies conducted with that task, which is particularly important when the domain features a large output space.", "sections": [{"title": "1 INTRODUCTION", "content": "When AI began to make its way into mainstream software development, many researchers began calling for software engineering methods suitable for Al systems (SE4AI) (e.g., [8, 34, 59, 66]). In response, the software engineering community has begun to create with SE4AI methods and tools, recently surveyed by Martinez-Fernandez et al. and by Giray [27, 51].\nStill, gaps remain, especially in SE4AI methods for attributes that are unique to AI-powered systems [51]. One such attribute is transparency. 1 In some software, transparency is considered to be so critical, it is a legal requirement. For example, U.S. Executive Order 13960 requires \"certain federal agencies\" to abide by nine principles except in cases of national security, two of which are understandability and transparency [26]. Likewise, the European Union's 2023 AI Act introduces specific transparency obligations [?].\nThe subarea of eXplainable AI (XAI) exists to fulfill such requirements of transparency and understandability. However, how to assess whether an XAI feature actually achieves of this property is not yet well established.\nThis paper aims to shed light on how to assess an XAI system's transparency using several mathematical measures in empirical studies with humans. Drawing from Hoffman et al.'s [35] suggested of eleven empirical tasks researchers could rely upon to evaluate their explanations (see their Tables 4 and 5), we consider several ways how well human participants can predict an AI system's next decision. Using human predictions [55] is an important way to assess human understanding that has seen widespread use (e.g., [3, 20, 57, 74]).\nMeasuring human performance on predicting AI behaviors is difficult, in part because the obvious binary measure (correct/wrong) produces very little useful information. Consider the following example: a self-driving car approaches a construction zone with cones defining the lane instead of painted lines. One prediction question we might ask is \"Will it hit any cones? Yes or No.\" This binary framing is fairly common in XAI research (e.g., [3, 20, 57, 74]), but it is very susceptible to the floor and ceiling effects these authors report. In particular, some decisions are very easy and most participants get them correct; while other decisions are more difficult and no one gets them correct. The second situation is particularly dangerous in XAI because as the output space grows, the probability of any particular decision being correct shrinks quite rapidly.\nThe main contribution of this paper is a method for assessing XAI's transparency property using the empirical Prediction task. As such, our work improves our ability to empirically study a non-functional requirement: that an agent's behavior is predictable. The current method of binary prediction framing does not account for partially correct predictions. As a result, binary prediction measures are susceptible to Type 2 errors, which occur \"when we declare no differences or associations between study groups when, in fact, there was\" [71]. Such Type 2 errors become even more likely as output space size increases. One way to counteract this trend could be to incorporate domain-specific knowledge, but that would not generalize well. Some actions appear similar, some actions produce similar outcomes, but knowing when similarities are merely superficial requires domain knowledge. Instead, our approach is domain-agnostic: it requests values (and rank-order preferences) from the agent to measure \"partial credit,\" providing an alternative to binary prediction analysis. Note that our studies focused on controlling the size of the action space, while many other games have action spaces that are far larger; in an extreme case DeepMind's StarCraft parameterization \u201chas an average of approximately 1026 legal actions at every time-step\u201d [78]. Clearly a binary prediction framing is ill-suited to such an action space.\nOur core goal is to rigorously define measures for the meaning of \"partially correct\" when predicting an AI output. Some recent work by Bondi et al. [10] moves past a binary framing by having participants enter their prediction on a 5-point Likert scale that ranges from definitely not present to definitely present. Unfortunately, the higher granularity in"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Human-Al Relationships in Al Systems\nWhen people are assessing an Al or engaged in some other AI-based task, what are their relationships with the AI? Papachristos et al. [60] found four categories of how humans perceive and adopt their role in human-AI interactions; the two most pertinent to assessing an AI are the \"Guide\" case and the \"Assistant\" case. In their Guide case, the human takes a back-seat, intervening only when Al confidence scores slipped. In their Assistant case, the human adopts a strictly dominant role to the AI, which assists with classification only when human confidence in their own abilities wane. As an example of a domain where people preferred the dominant role (Assistant), consider DuetDraw [58], a human-Al interactive drawing application providing the two described roles. Those authors report that the preference for Assistant role was stronger when the application provided \"detailed\" explanations.\nAnother human role between the extremes of Assistant and Guide is that of teammate, in which the human and AI system collaborate with each entity playing to their own strengths in (hopefully) complementary ways (e.g., [5, 15, 44]).\nWhen faced with the prototype of an AutoAI system, data scientists suggested a collaborative relationship with the Al system instead of the fully automated design [81]. This role attempts to mimic collaboration in human teams. Molenaar [52] introduces an example of this role in their \u201csix levels of automation model\u201d for personalized learning. In this role, the teacher \"monitors\" and the technology oversees specific tasks, but the technology can still defer to the teacher and cede control. The customer service domain has also utilized this type of human-AI relationship, or \"hybrid intelligence,\" so that AI and human customer service agents augment each other [84]. According to Wang et al. [80], designing AI systems as tools like other complex technologies, instead of encouraging collaborative relationships between human and AI systems, could be more beneficial. A possible reason is that, in such collaborative environments, people judge their abilities and feel less competent than they actually are (i.e., their perceived effectiveness is less than the actual) [36]. In a similar case, data scientists had more confidence in the model they created by themselves in notebooks than the more accurate model they generated with AutoDS, an Al system [79]. Jacobsen et al. [36] posits that this mismatch between perception and reality would persist if people are not able to get instant assessment of the collaborative decision.\nTo foster better collaborative human-AI relationships, AI systems designs should communicate clearly, provide explanations, and offer control of their actions to users [1, 4, 69]. Copying existing human behavior and relationships is inadequate [70], it is important to leverage unique human and computer features and allow for different control levels based on the specific tasks [69]. For example, the scheduling system designed by Cranshaw et al. [18] uses a 3-tiered technique for assigning tasks, assigning to the system tasks like creation of meetings; while the human takes care of updating meeting information or rescheduling events. Researchers have found that providing information about the system's data source and decision process allows for easier adoption, productive use, and may help calibrate expectations-whether the stakes are high (e.g., detecting prostate cancer [13]) or low (e.g., scheduling events [39])."}, {"title": "2.2 Al Explanations", "content": "Examples of AI models that are particularly difficult to explain because of their complexity include neural networks (NN), ensemble models, and support vector machines (SVM). These AI models feature in a lot of critical systems, so there are earnest efforts to make them more explainable while also maintaining high accuracy.\nExplanations can provide different types of information, which Lim et al. [46] formalized into a set of \"intelligibility types.\" Intelligibility types have proven helpful as XAI researchers design and evaluate their explanations' content. For example, Lim et al. found that people preferred \"Why\u201d information for unexpected behavior, but in other contexts people have prioritized \"What\" information such as in strategy games [46, 61] and smart homes [14]. The Why and Why Not intelligibility types have attracted the most attention from XAI researchers, examining explanations targeted at these types in a variety of domains, including pervasive computing [77], email classification [42], database queries [9, 31], news feeds [17], and robotics [30, 49, 63].\nExplanations for how an Al model generates output can be presented in different formats (e.g., text [33, 48, 86], saliency-maps [11, 67, 72], graphs [40, 41], and timelines [16, 76], etc.). Based on the \"focus\" of the explanation in a single system, they can either be local (providing information about a particular decision) or global (providing information about how the whole system works) [53, 54]. Also, there are three types of techniques used for explanations: opaque box, transparent box, and hybrid.\n2.2.1 Opaque Box Explanations. Opaque box explanation techniques treat the whole AI model as opaque and operate at the input/output level of various decision instances. Earlier research efforts such as LIME [62], SHAP [50], and LORE [29]\n2.2.2 Transparent Box Explanations. Transparent box explanation techniques reveal the internal structures and op-erations of the Al model. An example of this technique that works for image classification is the deconvolutional network (deconvnet) method introduced by Zeiler and Fergus [89]. The deconvnet model \"maps the feature activity in intermediate layers to the input pixel space\" [89]. When an image input is initially passed to a CNN during training, each layer generates features to pass as an input to the deconvnet model in that layer. This method outputs a visualization that shows the parts of the input image that are informative for classification. Similarly, in Deep Q networks, the learned features \"map\" to different regions [88]. Upon examination of these regions, applicable rules and policies are identifiable, forming an explanation of the AI model. A different approach for CNNs, called network dissection [6, 7], also helps measure the degree of its \"interpretability\u201d by: (1) collecting a list of human-categorized concepts (2) computing the features activated in each hidden unit for each concept (3) aligning concept-activation pairs.\nAside from the feature activity in each layer, one could visualize granular level information such as each neuron's \"multiple facets\" [56]. Weidele et al. [83] modified this type of visualization system with interaction components such that users could explore \"What If?\u201d scenarios.\nThe main advantage of transparent box explanations is their ability to assess and debug the model. The main drawbacks relate mostly to scale, leading Sarkar [65] to ask \"Is explainable Al a race against model complexity?\"\n2.2.3 Hybrid Explanations. Hybrid explanation techniques combine and/or blend characteristics of opaque and trans-parent box approaches. The hybrid explanation mechanisms possess a combination of characteristics (to a degree) from the opaque box and transparent box explanation techniques. They do not necessarily operate exclusively on the AI model's inputs/outputs and may not be concerned about the AI model's granular internal structures, like neurons. An"}, {"title": "2.3 How XAI Researchers Evaluate Al Explanations", "content": "Researchers have evaluated explanations both analytically and empirically. For the context of XAI, Hoffman et al. [35] terms these two evaluation approaches as evaluating \"goodness\" and \"satisfaction,\" respectively. In the analyti-cal/goodness approach, XAI researchers are the main actors, comparing an explanation's content/structure/presentation against accepted criteria and established guidelines. For example, an XAI researcher could consider where their explanation system follows Amershi et al.'s [1] human-AI interaction guidelines and where it does not.\nIn contrast, explanation consumers are the main actors performing the empirical/satisfaction approach, by seeing and using various instances of some type(s) of explanations as they complete a task [53]. Many XAI evaluations are empirical. One example is Dodge et al.'s lab study, in which participants assessed an Al system's effectiveness using instances of several different types of explanations [23]. Those authors used the results of the participants' assessments to gain insights into each explanation type's strengths and weaknesses. Another empirical example is Bondi et al.'s investigation into the effects of communication contents and style on the people in human-AI collaboration when performing an image classification prediction task [10]. Their results showed that participants performed better and provided more accurate results when the communication content was strictly that the AI model \"deferred\u201d to them. Kulesza et al. [41] investigated people working with explanations that instantiated several explanation principles, and used the results to evaluate the principles as well as the explanations. Khanna et al. [38] investigated people assessing an Al with the help of explanations with/without a scaffolding approach called After-Action Review for AI (AAR/AI), and found that the AAR/AI-scaffolded explanations were significantly more effective than the same explanations without the scaffolding. Other examples of the empirical approach abound; Lai et al. [43] survey over 100 more.\nDesigning an approach for evaluating a particular AI explanation depends on what the explanation is trying to help a user do. ....[?] Explanations have different purposes, but most have some connection to helping users build mental models of how the AI works. A mental model is an in-the-head representation that an individual generates based on their prior experiences [37]. Users \"execute\" (in their heads) these models to understand and explain system behaviors, and predict the system's future behaviors, even for inputs/situations they have not yet seen [53]. Thus, evaluating how"}, {"title": "3 DATA COLLECTION METHODS", "content": "3.1 Study 1 - MNK Games Domain\nWe recruited 86 participants at Oregon State University to complete IRB-approved tasks in-lab via a flier distributed over email and posted on campus. The only inclusion criteria were that participants be 18 or older and not study Computer Science, since we were interested in the perceptions of AI non-experts. Once eligible participants gave informed consent, we scheduled them for a two-hour session, then randomly assigned them to one of eight treatments.\nThe treatments are based on combinations of the three explanations from Dodge et al. [23] (shown in Figure 1). Treatments could have no explanations (NONE, our control group), one explanation (STT, OTB, BTW), two explanations (STT+OTB, OTB+BTW, STT+BTW), or all three explanations (ALL). This study employed multiple tasks\u00b2, but this paper will focus on the Prediction Task, as described by Muramatsu et al. [55].\n3.1.1 Domain. Our domain was MNK games, which are a generalization of the well-known board game Tic-Tac-Toe (3-3-3). In MNK games, each player alternates placing their piece (X or O) in an effort to arrange their pieces in a sequence of length K on a board with a size of M X N. In our study, we used 9-4-4, in which players attempt to create a 4 length sequence on a 9 \u00d7 4 board. Thus, the action space in this study was 9x4x4=144 possible actions-that is, each time a participant predicted the agent's next move, they had to choose 1 out of 144 possibilities.\nThe MNK domain offers the following advantages for us in human lab studies: 1) the move tree has a limited depth because the board will ultimately fill, 2) it offers robust empirical controls, and 3) despite having a comparable representation to domains like Go, its rules and strategy are significantly easier to understand, which makes tasks both accessible to laypersons and short enough for lab studies.\nWe implemented the game using an adaptation of Dodge et al. [23]'s source code, which offers a simulator for the straightforward transition model of MNK games. This simulator encodes each board using three states per square-controlled by opponent, controlled by the agent, and empty-in just two bits.\n3.1.2 Agent. This section, which essentially follows Dodge et al.'s [23] methodology, provides a brief overview of those authors' three interactive explanations (Figure 1). The agent has-a convolutional neural network (CNN) to construct these explanations by predicting outcome tuples O = (Win%, Loss%, Draw%) for each square, given the M \u00d7 N board. The network features a two-channel input layer with the opponent's pieces always in channel two and the agent's pieces always in channel one, resulting in a tensor with dimension MXN \u00d7 2 (never visible in the interface). The agent is CNN-based, with 8 total layers, producing an output of shape (M \u00d7 N \u00d7 O) [23]. To select actions, the agent begins by performing a forward pass on the network, then flattening the outcome dimension via a generalized value function (proposed by Sutton et al. [75], though still used, e.g., [47]).\n3.1.3 Explanation 1: Scores Through-Time (STT). The STT explanation focuses on the temporal aspect of the data in an effort to address the question, \"How did the agent score each square at each decision?\u201d. Time is the X-axis in this"}, {"title": "3.1.4 Explanation 2: Scores On-the-Board (OTB).", "content": "The OTB explanation emphasizes a connection between the temporal and spatial dimensions of the data, attempting to answer: \"How effective is this square at various times?\". As Figure 1(mid-dle) shows, each square of the OTB explanation is itself a mini STT, showing the scores for that square through time. When the user hovers a square on the gameboard, a single OTB chart with the same coordinates is highlighted; similarly, hovering a chart on the OTB highlights a single square on the gameboard. Any time the explaining agent makes a decision, each of the 36 charts receives one new data point with the score for that square."}, {"title": "3.1.5 Explanation 3: Scores Best-to-Worst (BTW).", "content": "The BTW explanation places emphasis on the data's value dimension, in an effort to address the question, \"How did the agent score its choices at each timestep?\". Storing all the values for all squares at all times results in a tensor with both space and time dimensions. This explanation cuts along the time dimension, sorting all 36 squares by descending value. Figure 1(bottom) shows how each decision results in a single data series, with only the most recent being colored in the agent's color (Green) and interactive (hovering a score highlights a square and vice versa). The grey colors given to the old data series get increasingly light to illustrate how the score distribution shifts with time."}, {"title": "3.1.6 Procedure.", "content": "During the study's three tasks, participants observed a total of 12 agents, created via mutant agent generation [23]. Every task contained 4 games with controlled randomization so each participant saw the same moves, and participants had 5 minutes for the first 3 games and 9.5 minutes for the last game. To keep all the participants working at the same pace, we used lock dialogues\u00b3. Once the allotted time expired, a researcher provided a 2-letter password to everyone in the room, allowing participants to proceed to the next game/task. After each game, participants would fill out a paper form containing questions from AAR/AI [24]. Specifically, the form would collect: \u201cWhat happened in this game (write any good, bad, or interesting things you've observed in these past moves and/or games)? (a few sentences)", "Is there anything in the explanation that helps you understand Why the Al you're assessing did the things it did? Please specify which explanations you are referring to. (~2 sentences)": "We did not include the rest of the questions from AAR/AI to reduce participant fatigue. Participants kept these forms for reference until the end of a task, at which point researchers collected the old forms and gave fresh ones for the next task.\nParticipants first went through a researcher-led tutorial to better understand the MNK game and our tasks. In the tutorial, participants played a game against the Green agent. Then, they would observe Green playing Purple, and the researcher described each of the explanations and tasks. Before starting the tasks, we told participants they would be observing games and using the information available on the interface to assess the agents.\nAlthough participants performed three tasks, as mentioned previously, the scope of this paper covers only the Prediction task. During this task, participants first watched the Sky agent play three games against three yet-unseen opponents, using the \"Step\" button to proceed through the games and the \"Rewind Slider\" to revisit previous states as"}, {"title": "3.2 Study 2 - Analysis in Four Towers Domain", "content": "Our second analysis used data produced by Anderson et al.'s earlier study [2, 3] in a size 4 action space-each time a participant made a prediction, they needed to chose 1 out of 4 possibilities. These data allowed us to compare results from a very small action space against the larger 144-action space in Study 1. This section summarizes Anderson et al.'s methodology [2, 3].\n3.2.1 Domain. The player controls a tank with a kite-like design that is positioned in the middle of a map with four quadrants. The player's objective is to damage and destroy adversaries in order to score points. However, if it attacks allies, it loses points. The player has four actions available, attacking one of four quadrants; whether it contains an adversary (black) or ally (white). The object's quadrants can contain: big/small forts (forts can return fire), cities/towns (the agent always loses points when attacking non-combatants), and enemy tanks.\n3.2.2 Agent. The agent playing the game used reinforcement learning (RL) to learn a Q-function that calculates the anticipated total reward of executing a given action in a given state, then following the policy. The policy was based on a decomposed Q-function [47, 75] to divide the reward into various components based on the reward type (e.g., damaging an enemy vs taking damage), with the goal of revealing more relevant information about the agent's preferences. In"}, {"title": "3.2.3 Explanation 1: Reward Bars.", "content": "Anderson et al. [2, 3]'s first explanation presented the reward decomposition as a bar chart. For a specific action, the chart contained a bar cluster where each bar shows the Q-value contribution of a particular reward component."}, {"title": "3.2.4 Explanation 2: Saliency Maps.", "content": "Anderson et al. [2, 3]'s second explanation used saliency maps to show input features the agent concentrates on most when making decisions. These maps showed the relative weight of several game state components using a heatmap overlay. Those authors adopted a perturbation-based saliency maps created by altering game objects, such as enemy tanks, then evaluated how much the output Q-values were impacted, and finally visualized the result as a heatmap."}, {"title": "3.2.5 Procedure.", "content": "Anderson et al. [2, 3]'s study contained four treatments: saliency maps, reward decomposition bars, both, or no explanations (control). Each of the 124 participants took part in a 2-hour lab experiment, all of whom were not computer science majors. Before each session, the researchers provided a tutorial describing the game mechanics, explanations, and interface. After that, participants watched as the trained agent made 14 decisions. Along the way, participants predicted which quadrant the agent would attack next and explained their reasoning in an open-ended manner at each step. After submitting their prediction, participants viewed the agent's actual decision and the accompanying explanation."}, {"title": "3.3 Statistical Analysis", "content": "In this paper we test hypotheses via: ANOVA (when we have both equivariance and normality); and Kruskal-Wallis (when we have equivariance but not normality). We verify the equivariance assumption with Levene's Test and the normality assumption with the Shapiro-Wilk Test."}, {"title": "4 PROPOSED ANALYSIS METHODS FOR PREDICTION TASK DATA", "content": "This section will give our mechanistic answer to RQ1 - How can we operationalize analyzing prediction task data in a way that has some degree of independence from domain and model?. After we see the results of applying these approaches, we will return to RQ1 in Section 7 to discuss the efficacy of our proposed mechanisms. All our strategies are based on the notation presented in Table 1, and we divide them into two categories; those yielding a distribution and those yielding a single score. We have also provided a notional illustration of our measurement constructs in Figure 3."}, {"title": "4.1 Strategies Yielding a Distribution", "content": "The main advantage of a distribution is support for computing comparative statistics.\n4.1.1 Loss in Value (LV). This strategy leverages the idea that two actions are similar if the agent assigns them similar values, meaning the agent perceives them to have similar outcomes. In our case, values are interpretable: in Study 1, values represent \u201cadvantage,\u201d measured by Win% \u2013 Loss%; in Study 2, values represent the number of points the agent expects to obtain.\n$LV(\u00e2, ai) = V(\u00e2) \u2013 V(ai)$"}, {"title": "4.1.2 Loss in Rank (LR).", "content": "However, values in general need not possess any intuitive semantic equivalent; in such instances, it could be better to utilize the agent's values' rank ordering, since ranks are a simple and widely applicable concept. This strategy leverages the idea that two actions are similar if the agent assigns them values which are similar in the rank ordering of all values. However, ranks have a disadvantage since they destroy exact relationships between the predicted values.\n$LR(\u00e2, ai) = R(\u00e2) \u2013 R(ai)$"}, {"title": "4.1.3 Discretized Loss in Rank (DLR).", "content": "This strategy is based on Loss in Rank, but discards some detail by applying a discretization function, which can be helpful for interpretation and visualization. Discretizing uses a customizable number of bins, so in our case to analyze Study 1, we defined D(\u00b7) with six bins based on deciles 4 in a common grading system found in U.S. education (e.g., 90%+ earned an A, 80-89% a B, 70-79% a C, 60-69% a D, and < 60% an F). Note that\n$DLR(\u00e2, ai) = D(LR(\u00e2, ai)) = D(R(\u00e2) \u2013 R(ai))$"}, {"title": "4.2 Strategies Yielding a Single Score", "content": "The main advantage of a single score is that it is easy to understand and present. Clearly, the distributions of LV(), LR(), and DLR() will have measures of central tendency (e.g., Tables 2 and 5 show the mean). However, we have an additional single-score producing metric to propose, the mRBO.\n4.2.1 Modified Rank-Biased Overlap between agent's preferences and participants' group-wise preferences (mRBO). This strategy infers two preference rankings and compares them by computing a metric similar to Rank-Biased Overlap (RBO) [82]. The first rank list comes directly from the agent, defined by computing the R() function Va \u2208 A. The second comes from a group of participants, using the counts of each prediction within the group to impose an order on the actions. Our approach to constructing the second ranking is essentially a voting schema, meaning it is likely that many squares will be tied at 0 votes, which we truncate from the ranking.\nRBO produces an output in the range [0, 1], where 0 means the rank lists are disjoint and 1 meaning they are identical. RBO is defined on: lists S and T; in addition to hyperparameters p (which controls weighting for high ranking values), and k (the evaluation depth) as follows (Equation 23 from [82]):\n$RBOEXT(S, T, p, k) = \\frac{|S:k \u2229 T:k|}{k} . p^k + \\frac{1-p}{k} \\sum_{d=1}^k p^d \\frac{S:d \\subseteq T:d}{d}$ \nThe reason that we modified the RBO metric is because, after we applied it off-the-shelf, we observed it behaving in counter-intuitive ways, which we will detail later in Section 5. Our modifications are fairly simple, shown in red in the next equation: 1) adjusting the denominator inside the sum to stop increasing when one of the lists runs out of elements; and 2) changing the denominator of the first term in the same way. Without loss of generality, assume that S is the shorter of the two lists. Thus, our version is as follows, where k = |T|:\n$MRBOEXT (S, T, p, k) = \\frac{|S:k \u2229 T:k|}{|S|} . p^k + \\frac{1-P}{k} \\sum_{d=1}^{min(|S|, d)} \\frac{p^d}{d}$"}, {"title": "5 RESULTS RQ2 - HOW WELL DID PARTICIPANTS PREDICT IN MNK GAMES?", "content": "Overall, our results suggest that binary measurement of participants' prediction performance is too coarse-grained to answer this RQ convincingly. However, the metrics proposed in this paper reveal a bit more, as we will see."}, {"title": "5.1 Adopting the binary prediction framing", "content": "It is troublesome but unsurprising that both floor effects and ceiling effects are prevalent in the data we collected. For example, Figure 4 shows that of the 86 participants in the experiment: 10 participants correctly made the first prediction"}, {"title": "5.2 Adopting the proposed framing, LV() and LR()", "content": "However, when we switch over to using LR() (Equation 5), we observe some significant results in the data. Applying this strategy to our data adds shades of gray to the black and white images shown in Figure 4 summarizing all participants' grades across the four predictions that they made.\nAs an example, in Figure 5, we saw that participants in the OTB+BTW treatment were the only ones in the first prediction where nobody guessed the correct square. However, it is difficult to infer much from that since participants in the other treatment groups did not do much better, with around one person providing the correct prediction. Now, consider that same OTB+BTW treatment's bar in Figure 6, which shows a lot of participants receiving F scores."}, {"title": "5.3 Adopting the proposed framing, mRBO", "content": "As described in Section 4.2.1, mRBO relies on having two lists to compare. The agent's preference list is visible in Figure 6. To create the other list, we select a group of participants to combine predictions in a voting schema. One result of this grouping is visible in Figure 8), the rest are available in our supplemtal documents. From the figure, it is clear that there are marked differences between the vote distributions of the treatment groups."}, {"title": "6 RESULTS RQ3 - HOW WELL DID PARTICIPANTS PREDICT IN THE FOUR TOWERS DOMAIN?", "content": "In the course of our reanalysis of the data Anderson et al. [2, 3], we performed a series of statistical tests to evaluate the underlying distributional properties and homogeneity of variances across four distinct treatments. Table 3 shows the \"right answer\" for each decision, as well as predicted values and vote counts.\n6.1 Adopting the binary prediction framing\nAnderson et al. [2, 3] already provided an analysis using the binary prediction framing, reporting no significant results based on the prediction data. Figure 9 provides a high level look at that data by combining all treatments and focusing on the decisions (Anderson et al. [3]'s Figure 10 provides the data split by treatment, so we will omit that data for brevity). Suffice to say, floor and ceiling effects abound, once again.\n6.2 Adopting the proposed framing, LV() and LR()\nFor value space, we worked on the Q-values associated with each action taken at every decision point, which in this domain represent a prediction of the number of points the agent thinks it will obtain if it selects a particular action and then follows the policy. Since we have 4 different quadrants that the AI can decide amongst, for the rank approach we did not do any discretization (DLR(), Equation 6). Over all 14 decisions, we found that the Q values ranged from -366 to 53. We also noticed that for many DPs, the gap between two consecutively ranked Q-values was large. For example, the Q-values for DP1 were (in rank order) 31, -28, -284, -313. In this case, the difference between the top choice and second best choice is 59, whereas the difference between second and third-ranked actions is 256-a gap five times greater."}, {"title": "6.3 Adopting the proposed framing, mRBO", "content": "We then turn to our other proposed framing, the mRBO, whose results are visible in Table 5. Unfortunately, this approach yields a lot of ties in columns, which makes it much harder to compare treatments. The table shows participants in the NONE treatment provided the best (or tied for best) votes 8 times, Saliency claimed the same number, Reward bars only 7, and Both at 10. Notably, Anderson et al. [3]'s original analysis found that participants in the Both treatment tended to outperform the other groups as well.\nIn summary, our analysis for the Four Towers domain consistently suggests a lack of discernible differences in predictive performance across various treatment levels, even after incorporating the indicator variables into the model."}, {"title": "7 DISCUSSION", "content": "To analyze the theory we propose in this paper, we will employ the framework by S"}]}