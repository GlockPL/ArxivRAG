{"title": "Positive-Unlabelled Learning for Improving\nImage-based Recommender System Explainability", "authors": ["\u00c1lvaro Fern\u00e1ndez-Campa-Gonz\u00e1lez", "Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdi\u00f1as"], "abstract": "Abstract-Among the existing approaches for visual-\nbased Recommender System (RS) explainability, uti-\nlizing user-uploaded item images as efficient, trustable\nexplanations is a promising option. However, current\nmodels following this paradigm assume that, for any\nuser, all images uploaded by other users can be consid-\nered negative training examples (i.e. bad explanatory\nimages), an inadvertedly naive labelling assumption that\ncontradicts the rationale of the approach. This work\nproposes a new explainer training pipeline by leveraging\nPositive-Unlabelled (PU) Learning techniques to train\nimage-based explainer with refined subsets of reliable\nnegative examples for each user selected through a\nnovel user-personalized, two-step, similarity-based PU\nLearning algorithm. Computational experiments show\nthis PU-based approach outperforms the state-of-the-\nart non-PU method in six popular real-world datasets,\nproving that an improvement of visual-based RS ex-\nplainability can be achieved by maximizing training\ndata quality rather than increasing model complexity.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, society has undergone a remarkable trans-\nformation thanks to the incorporation of new technologies,\nwith the growing importance of Artificial Intelligence (AI)\nstanding out [1]. Within AI, Recommender Systems (RS)\nperform the critical task of associating users and items\nin a personalized manner [2], based on algorithms and\ntechniques that operate on implicit or explicit information\nsuch as user-item ratings, product descriptions, or user\nprofiles. The significant growth in the amount and com-\nplexity of existing data has amplified the importance of\nRS, making them essential in the retrieval of relevant and\npersonalized content for users, and providing benefits for\ncitizens, industry and institutions alike [3].\nCommonly having a \"black-box\" nature, an important\naspect of Recommender Systems is explainability. Explain-\nable Recommender Systems are those that, in addition to\nproviding their suggestions, explain why a particular item\nhas been recommended to this particular user. Through\nthese explainable systems, not only do they achieve a higher\nlevel of transparency, providing greater credibility and\ntrust from the users' perspective, but they also enable an\nunderstanding of how the system operates [4] [5].\nOne of the most popular approaches for achieving ex-\nplainability in RS has been the use of visual information [6],\ndue to its critical importance in human cognition. Among\nthe manifold visual-based explainability approximations,\nthis work focuses on the use of user-uploaded item images\nas a means of visual explainability. Compared to other\nvisual-based approaches (such as word clouds or graphs [7],\ntext-to-image explanation generation [8], or user-oriented\nRS interfaces [9]), leveraging existing user-uploaded item\nimages as explanations has multiple advantages: 1) it does\nnot require the use of generative AI technologies, 2) it\navoids the potentially intrusive use of auxiliary, extra\ninformation of users or items and 3) makes the explanation\nself-contained, as explanations are always representative\nof the item's real features; overall, this allows to integrate\ntransparency into the RS in an efficient, sustainable,\nreliable and privacy-preserving manner.\nIn recent years, this approach has been studied by the\nresearch community. Most notably, D\u00edez et al. [10] intro-\nduced ELVis, a model that leverages user-uploaded images\nby modelling implicit feedback with pre-trained image\nembeddings, learnt user embeddings, and an MLP-based\narchitecture; however, the existing work in this field carries\na limitation likely to hinder explanation quality and the\noptimality of the training process. When defining training\ndata from implicit feedback, state-of-the-art approaches use\nauthorship task modelling (i.e. predicting if a user would\nupload an image of an item is equivalent to predicting if\nthe image would be a good explanation of the item for that\nuser) and, since no negative examples (i.e. user-image pairs\nwhere the user would not upload that image) are known\nin the data, a strong assumption is made: for any given\nuser, all images not uploaded by the user are considered\nto be negative examples for the user, i.e. bad explanations\nfor said user. However, reasonably, this is not necessarily\ntrue: images uploaded by other users may capture the same\ntraits of items as the images taken by the user.\nIn reality, the RS explainability task based on user-\nuploaded images deals with Positive-Unlabelled Data, that\nis, data where only a portion of positive examples are\nknown (the images uploaded by users represent good\nexplanations of item recommendations for them), while\nthe rest are \"unlabelled\", comprising both positive and"}, {"title": "A. Problem Formulation", "content": "When defining the RS explainability approach of using\nuser-uploaded item images, D\u00edez et al. [10] proposed\nmodelling it as an authorship task: if a user u is the author\nof an image p that captures an item i, it can be assumed\nthat p captures the characteristics that explain u's choice\nof the item i. To obtain the best explanations for any\nuser-item pair, the system must learn how to select the\nitem's image that best represents the user's preferences\nand, by the authorship-preference equivalence, this equates\nto determining which image of the item would be most\nlikely to be uploaded by said user.\nTo decide which image is this among all the item's images,\nthe authorship probability of each individual image p of\nitem i must be calculated for user u (i.e. $R_{up}$) and ranked\nby their probabilities in descending order; as such, the task\ncan be modelled as a Learning to Rank (LtR) task. To learn\nthe user's preferences, a model has access to training data\nin the form of user-item-image tryads (u, i, p), indicating\nthat the user u uploaded p in their review of item i, where\ntrivially $R_{up}$ = 1; these are used in the State of the Art as\nthe positive examples (good explanatory images for user\nu), as they directly reflect user preferences [10]; on the\nother hand, the definition of negative examples (i.e. bad\nexplanatory images for user u), forming tryads (u, i, $P_{neg}$)\nis more complex and must be designed by the researcher,\nas they are not directly present in the data."}, {"title": "B. The present context: Data and Model", "content": "1) Datasets: In this work, we apply our proposal to the\nspecific use-case of explaining restaurant recommendations\nwith restaurant images uploaded by users; these datasets\nwere introduced by D\u00edez et al. [10] and contain images of\nrestaurants uploaded in users' reviews on TripAdvisor in six\ndifferent cities between 2018-19: Gij\u00f3n, Barcelona, Madrid,\nParis, New York, and London. Each example in the datasets is a triad user-restaurant-\nphoto indicating a user uploaded said photograph in their\nreview of a given restaurant.\nConcerning the dataset partitioning, it is well known that\nthe chosen strategy has a notable impact on the measured\nperformance of tasks that deal with interaction data [13],"}, {"title": "2) State of the Art: ELVis and its Limitations:", "content": "At\npresent, and for this particular image-based explainability\napproach, ELVis [10] is the state-of-the-art method for\nselecting the user-uploaded photograph of an item that\nis most adequate to explain new recommendations of the\nitem to any given user.\nTo achieve this, ELVis follows the Learning to Rank,\nauthorship-based modelling of the explainability task,\nranking the possible image explanations by the probability\nthat the user would upload them, and correlating this\nauthorship to a match of the image's features with the\nuser's explanatory preferences. Topologically, as seen in\nFigure 1, these features and preferences are represented\nin ELVis by two latent vectors generated based on the\nuser's ID and the projection of the Inception ResNet-V2\n[14] image embedding. These vectors are then concatenated\nand the result is fed into a multi-layer perceptron (MLP) to\nundergo a series of non-linear transformations and a final\nsigmoid activation. The output represents the predicted\nauthorship (explanatory preference) score of the image\np for u, i, i.e. how good of an explanation is p for the\nrecommendation of its represented item i for user u.\nTo model the Learning-to-Rank task, ELVis surrogates\nthis to a binary classification task. As such, for training, a\nbalance between positive and negative examples is required.\nAs we mentioned in Section II-B1, the positive examples for\na given user are trivially those images that were uploaded\nby user u, as they are assumed to be representative of their\nexplanatory preferences.\nIn order to define negative examples (u, i, $P_{neg}$) for each\nuser u, ELVis' authors choose to sample random images up-\nloaded by other users and representing the same or different\nitems from the ones reviewed by u. In spite of its simplicity,\nthe primary issue of this approach is that it assumes all\nimages from other users represent item characteristics that\nare different from the ones represented by u's images (i.e.\nthat explanatory preferences of different users are mutually\nexclusive). User tastes are rarely fully unique [15], so\nimages uploaded by other users but similar to the user's\nown will be incorrectly labelled as negative; Figure 1 shows\nan example of this phenomenon in a restaurant review\nscenario. Consequently, the negative examples generated by\nELVis's labelling can be unreliable; this creates a significant\nproblem during training, as the model will be utilizing a\ntraining dataset with a potentially high degree of label noise\ninside the negative class, hindering the model's ability to\nserve optimal explanations to the RS' users."}, {"title": "C. PU Learning", "content": "In Section I, we anticipated that, under the authorship-\nbased modelling, predicting the best user-uploaded image\nto explain a user-item recommendation must deal in\nreality with Positive-Unlabelled (PU) data. In this context,\npositive examples are all triads in the original, raw data (i.e.\nall the images uploaded by a user are good explanations for\nthat user), and all other possible user-photograph pairs can\nbe considered unlabelled (i.e. we don't know whether the\nuser would upload images like those of other users). While\nall these pairs with unknown label can be naively considered\nnegative for training, it breaks the basic assumption that\nsome of the other users' photographs can be used as good\nexplanations for any given user, as discussed in II-B2. As a\ngeneral solution, Positive-Unlabelled Learning is a Machine\nLearning paradigm intended for data scenarios with only\npositive and unlabelled examples, where no known negative\ntraining examples are available [11].\nA basic assumption of PU Learning, which matches\nthe definition of the explainability task discussed in this\nwork, is that the subset of unlabelled examples comprises\nboth positive and negative ones; in our particular case, this\ntranslates to the existence of good explanatory images for a\nuser within the images uploaded by other users. The major\nchallenge of this paradigm lies in separating the unlabelled\nsamples; existing approaches are broadly categorized by\nBekker and Davis [11] into three main approximations:\n1) biased learning, which handle the unlabelled set as a\nnegative set with labelling noise [16], [17], 2) inclusion\nof the class prior into the learning process, where the\nprobability of an unlabelled sample being positive is known\nor can be estimated [18], [19], and 3) two-step approaches,\nwhere the model is trained using the positive examples\nand a subset of reliable negative examples inferred from\nthe unlabelled set [20], [21]. The two-step techniques are\nthe most widespread in the literature [11], and as the\nsparsity and user-personalized nature of interaction data\nmake biased learning or class prior estimation unreasonable,\nwe focus on the use of two-step PU Learning techniques\nfor our work, as discussed in the following Section."}, {"title": "III. PROPOSAL (PU-ELVIS)", "content": "In Section II-B2, we showed how existing state-of-the-\nart methods perform an overly naive sampling of negative\nexamples for the LtR task of selecting user-uploaded\nitem images as personalized explanations. Instead of a\nrandom selection, which can mistakenly label as negative\n(bad explanations) images by other users that share good\nexplanatory features with the target user's, this work\nproposes a novel labeling method through a distance-based\ntwo-step Positive-Unlabelled (PU) Learning [11] approach.\nFor each user, our method identifies images from other\nusers that significantly differ from the user's, and uses\nthem as reliable negative examples (bad explanations for\nthe user) without introducing label noise during training.\nThis PU Learning approach was designed under the\nSCAR assumption [22] as labelling mechanism, which posits\nthat the set of labelled positive examples forms a i.i.d\nsample of the positive distribution. In our particular use-\ncase, this implies assuming user-wise 1) uniformity and 2)\nseparability so, for any given user, within the unlabelled\nset (i.e. images uploaded by other users): 1) images that\nare good explanations for the user have similar features to\nthe user's, and 2) images that are not good explanations\nfor the user have dissimilar features to the user's.\nFor the first step of the two-step paradigm, which involves\nthe selection of these reliable negatives, we designed a novel\napproach for the task based on user-wise personalization\nof these negatives using a Rocchio [16] classification, where\ninstead of defining a global criterion for the selection\nof reliable negatives, this criterion is particular for each\nuser based on their explanatory preferences. This process,\ndepicted in Figure 2, works as follows for each user u:\n1) A prototype of u's known positive examples (i.e.\nthose images originally uploaded by u) is found by\ncalculating the centroid $C_u$ of the images (using the\nInception [14] latent embeddings of size d = 1536):\n$C_u = \\frac{1}{|P_u|} \\sum_{p \\in P_u} V_p$\nwhere $P_u$ is the set of u's images, and $V_p$ is an image's\nlatent embedding. $C_u$ forms a global representation\nof u's images, i.e. a prototype of what is a good\nexplanation when recommending items to u.\n2) A similarity criterion is defined to act as a selection\nthreshold for the user's reliable negatives. This\nthreshold was defined as the 10th percentile ($P_{10}$)\nof similarities of user's images to their centroid $C_u$,\ndue to its good outlier filtering properties in existing\nPU Learning works using Rocchio classifiers [16],\n[23]. The cosine similarity (Eq. 2) was used as the\nsimilarity metric given its demonstrated performance\nin image feature similarity tasks in the literature [24]:\n$S_C(V_p, V_{p'}) = \\frac{\\sum_{i=1}^{1536} V_p[i] * V_{p'}[i]}{\\sqrt{\\sum_{i=1}^{1536}(V_p[i])^2} \\sqrt{\\sum_{i=1}^{1536}(V_{p'}[i])^2}} \\in [-1,1]$\nwhere (p, p') is any image pair. $S_C(V_p, V_{p'})$ = 1 means\nimages with maximum similarity, and vice-versa.\n3) Before training starts, negative examples (images\nthat are not useful as explanations for the user) are\nselected for each user. To ensure a fair comparison,\nthe overall skeleton of the selection follows that of\nthe state-of-the-art method ELVis [10]: for each of\nthe user's images (i.e. a positive example (u,i,p)),\n20 images are selected, where 10 are images from\nthe same restaurant i (forming negative examples\n(u, i, $p_{neg}$)), and the remaining 10 are from different\nrestaurants (forming negative examples (u, i', $P_{neg}$)).\nOur PU Learning approach adds the additional strict\nselection criterion for negative example reliability: all\nselected \"negative\" images $p_{neg}$ must comply that\n$S_C(C_u, V_{pneg}) >= P_{10_u}$\nthus verifying that they are dissimilar from the user's\nown, avoiding the label noise caused by the random\nnegative selection used in the State of the Art.\nWhen the selection of reliable negative examples is\ncompleted, a full training set of positive examples (images\noriginally uploaded by their users, i.e. good recommenda-\ntion explanations for them) and reliable negative examples\n(images unlikely to be uploaded by the users they have\nbeen paired with, i.e. reliably bad explanations for them)\ncan be used to train the final explainer model; we have\nmaintained ELVis'architecture and training policy for\nfairness of comparison, but our PU learning approach is\nmodel agnostic and could be used with any other explained\ndesigned for the task under the authorship modelling."}, {"title": "A. Experimental Setup", "content": "We designed the evaluation experiments maximizing the\nfairness of the comparison of our PU-based approach with\nthe non-PU alternative from Diez et al's ELVis [10]. A\nRandom image selection method RND is also evaluated as\na baseline equivalent to not personalizing explanations.\nWith respect to hyperparameter optimization, a ran-\ndom search of 25 hyperparameter combinations against\nBarcelona's validation partition was performed on the\nsearch space lr = {1e-3,5e-4, 1e-4,5e-5, 1e-5} (learning\nrate) and d = {64, 128, 256, 512, 2014} (no. of latent\nfeatures). The best found hyperparameter combination\nwas used to train and test the model for all datasets.\nRegarding evaluation, we measured the quality of the\nexplanation rankings for each test case with classic implicit\nfeedback metrics Recall@k and NDCG@k, with k = 10 [25].\nD\u00edez et al. [10] observed that most users have very few\nimages, leading to cold start evaluation issues; consequently,\nwe followed their evaluation criterion of only considering\ntest cases that test the performance of users with \u2265 10\nimages in the train partition. All training, inference and\nevaluation code is made available on a public repository\u00b9."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "This section shows the results and performance compari-\nson of the proposed PU Learning approach for image-based\nRS explainability in accordance with the experimental\nsetup defined in the previous section.\nFigure 3 shows the Recall@k (upper) and NDCG@k\n(lower) curves (higher values are better) with k=1 to k=10\ncurves in each dataset for a Random baseline and the state-\nof-the-art method ELVis using either the naive non-PU\ntraining policy and our proposed PU Learning approach\nfor the selection of training negative examples."}, {"title": "V. CONCLUSIONS", "content": "Recommender Systems' increasing significance has made\nexplainability a crucial requirement for future user ac\u0441\u0435\u0440-\ntance, trust, fairness and compliance with international AI\nregulations. This work explores the particular visual-based\napproach to RS explainability consisting of personalizing\nrecommendations with user-uploaded item images. The top\nstate-of-the-art approaches [10] model this as an authorship\nprediction task, but have one crucial limitation: to generate\nnegative training examples they assume that, for any given\nuser, any image uploaded by other users can be considered\na bad explanation for the user; this is exceedingly naive and\nhinders the explainer model's performance, as images from\nother users can share the same good explanatory features\nas the user's own.\nIn this work we propose a novel method that, leveraging\nPositive-Unlabelled (PU) Learning techniques to handle the\nnegative example labelling problem, allows us to surpass the\nState of the Art in this task. A user-personalized two-step\nPU Learning is designed based on latent image similarity\nand Rocchio classification, generating for each user a set\nof reliable negative training examples (images that are\nnot good explanations for the user) which are confidently\nnot similar to the user's own; any explainer model can\nthen be trained on this refined set of training data with\nminimal label noise. In computational experiments, the\ndesigned PU-based approach consistently outperformed\nD\u00edez et al.'s state-of-the-art, naive non-PU method [10]\nin six real-world datasets of image-based explanation of\nrestaurant recommendation, reaching relative performance\nimprovements of ~ 20% in large datasets.\nOverall, the contribution of this novel PU-based labelling\nof negative examples in the task has relevant two-fold\nimplications in the field of user-personalized explainability\nof Recommender Systems: not only does it improve the\nquality of the explanations, therefore increasing user trust\nand usability of the RS, but it does it without an increase in\nthe explainer's complexity and computational cost, instead\nfocusing on refining the quality of the training data.\nRegarding future work, and concerning the PU Learning\napproach, our design utilizes the underlying SCAR as-\nsumption for labelling mechanism, the most common and\nreasonable in the field. However, there may exist a minority\nof polarized users with clustered, very differing tastes (e.g.\na user that uploads photos of either wine bottles or the\noutside view of the restaurant), which would invalidate this\nassumption. In spite of the good obtained performance,\nthe extension of the proposed algorithm to respect weaker\nassumptions may provide an edge in explanation quality."}]}