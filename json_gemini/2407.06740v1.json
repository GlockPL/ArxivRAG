{"title": "Positive-Unlabelled Learning for Improving Image-based Recommender System Explainability", "authors": ["\u00c1lvaro Fern\u00e1ndez-Campa-Gonz\u00e1lez", "Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdi\u00f1as"], "abstract": "Among the existing approaches for visual-based Recommender System (RS) explainability, utilizing user-uploaded item images as efficient, trustable explanations is a promising option. However, current models following this paradigm assume that, for any user, all images uploaded by other users can be considered negative training examples (i.e. bad explanatory images), an inadvertedly naive labelling assumption that contradicts the rationale of the approach. This work proposes a new explainer training pipeline by leveraging Positive-Unlabelled (PU) Learning techniques to train image-based explainer with refined subsets of reliable negative examples for each user selected through a novel user-personalized, two-step, similarity-based PU Learning algorithm. Computational experiments show this PU-based approach outperforms the state-of-the-art non-PU method in six popular real-world datasets, proving that an improvement of visual-based RS explainability can be achieved by maximizing training data quality rather than increasing model complexity.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, society has undergone a remarkable trans-formation thanks to the incorporation of new technologies, with the growing importance of Artificial Intelligence (AI) standing out [1]. Within AI, Recommender Systems (RS) perform the critical task of associating users and items in a personalized manner [2], based on algorithms and techniques that operate on implicit or explicit information such as user-item ratings, product descriptions, or user profiles. The significant growth in the amount and com-plexity of existing data has amplified the importance of RS, making them essential in the retrieval of relevant and personalized content for users, and providing benefits for citizens, industry and institutions alike [3].\nCommonly having a \"black-box\" nature, an important aspect of Recommender Systems is explainability. Explain-able Recommender Systems are those that, in addition to providing their suggestions, explain why a particular item has been recommended to this particular user. Through these explainable systems, not only do they achieve a higher level of transparency, providing greater credibility and trust from the users' perspective, but they also enable an understanding of how the system operates [4] [5].\nOne of the most popular approaches for achieving ex-plainability in RS has been the use of visual information [6], due to its critical importance in human cognition. Among the manifold visual-based explainability approximations, this work focuses on the use of user-uploaded item images as a means of visual explainability. Compared to other visual-based approaches (such as word clouds or graphs [7], text-to-image explanation generation [8], or user-oriented RS interfaces [9]), leveraging existing user-uploaded item images as explanations has multiple advantages: 1) it does not require the use of generative AI technologies, 2) it avoids the potentially intrusive use of auxiliary, extra information of users or items and 3) makes the explanation self-contained, as explanations are always representative of the item's real features; overall, this allows to integrate transparency into the RS in an efficient, sustainable, reliable and privacy-preserving manner.\nIn recent years, this approach has been studied by the research community. Most notably, D\u00edez et al. [10] intro-duced ELVis, a model that leverages user-uploaded images by modelling implicit feedback with pre-trained image embeddings, learnt user embeddings, and an MLP-based architecture; however, the existing work in this field carries a limitation likely to hinder explanation quality and the optimality of the training process. When defining training data from implicit feedback, state-of-the-art approaches use authorship task modelling (i.e. predicting if a user would upload an image of an item is equivalent to predicting if the image would be a good explanation of the item for that user) and, since no negative examples (i.e. user-image pairs where the user would not upload that image) are known in the data, a strong assumption is made: for any given user, all images not uploaded by the user are considered to be negative examples for the user, i.e. bad explanations for said user. However, reasonably, this is not necessarily true: images uploaded by other users may capture the same traits of items as the images taken by the user.\nIn reality, the RS explainability task based on user-uploaded images deals with Positive-Unlabelled Data, that is, data where only a portion of positive examples are known (the images uploaded by users represent good explanations of item recommendations for them), while the rest are \"unlabelled\", comprising both positive and"}, {"title": "A. Problem Formulation", "content": "When defining the RS explainability approach of using user-uploaded item images, D\u00edez et al. [10] proposed modelling it as an authorship task: if a user u is the author of an image p that captures an item i, it can be assumed that p captures the characteristics that explain u's choice of the item i. To obtain the best explanations for any user-item pair, the system must learn how to select the item's image that best represents the user's preferences and, by the authorship-preference equivalence, this equates to determining which image of the item would be most likely to be uploaded by said user.\nTo decide which image is this among all the item's images, the authorship probability of each individual image p of item i must be calculated for user u (i.e. Rup) and ranked by their probabilities in descending order; as such, the task can be modelled as a Learning to Rank (LtR) task. To learn the user's preferences, a model has access to training data in the form of user-item-image tryads (u, i, p), indicating that the user u uploaded p in their review of item i, where trivially Rup = 1; these are used in the State of the Art as the positive examples (good explanatory images for user u), as they directly reflect user preferences [10]; on the other hand, the definition of negative examples (i.e. bad explanatory images for user u), forming tryads (u, i, Pneg) is more complex and must be designed by the researcher, as they are not directly present in the data."}, {"title": "B. The present context: Data and Model", "content": "1) Datasets: In this work, we apply our proposal to the specific use-case of explaining restaurant recommendations with restaurant images uploaded by users; these datasets were introduced by D\u00edez et al. [10] and contain images of restaurants uploaded in users' reviews on TripAdvisor in six different cities between 2018-19: Gij\u00f3n, Barcelona, Madrid, Paris, New York, and London. Each example in the datasets is a triad user-restaurant-photo indicating a user uploaded said photograph in their review of a given restaurant.\nConcerning the dataset partitioning, it is well known that the chosen strategy has a notable impact on the measured performance of tasks that deal with interaction data [13],"}, {"title": "2) State of the Art: ELVis and its Limitations", "content": "At present, and for this particular image-based explainability approach, ELVis [10] is the state-of-the-art method for selecting the user-uploaded photograph of an item that is most adequate to explain new recommendations of the item to any given user.\nTo achieve this, ELVis follows the Learning to Rank, authorship-based modelling of the explainability task, ranking the possible image explanations by the probability that the user would upload them, and correlating this authorship to a match of the image's features with the user's explanatory preferences. Topologically, as seen in Figure 1, these features and preferences are represented in ELVis by two latent vectors generated based on the user's ID and the projection of the Inception ResNet-V2 [14] image embedding. These vectors are then concatenated and the result is fed into a multi-layer perceptron (MLP) to undergo a series of non-linear transformations and a final sigmoid activation. The output represents the predicted authorship (explanatory preference) score of the image p for u, i, i.e. how good of an explanation is p for the recommendation of its represented item i for user u.\nTo model the Learning-to-Rank task, ELVis surrogates this to a binary classification task. As such, for training, a balance between positive and negative examples is required. As we mentioned in Section II-B1, the positive examples for a given user are trivially those images that were uploaded by user u, as they are assumed to be representative of their explanatory preferences.\nIn order to define negative examples (u, i, Pneg) for each user u, ELVis' authors choose to sample random images up-loaded by other users and representing the same or different items from the ones reviewed by u. In spite of its simplicity, the primary issue of this approach is that it assumes all images from other users represent item characteristics that are different from the ones represented by u's images (i.e. that explanatory preferences of different users are mutually exclusively). User tastes are rarely fully unique [15], so images uploaded by other users but similar to the user's own will be incorrectly labelled as negative; Figure 1 shows an example of this phenomenon in a restaurant review scenario. Consequently, the negative examples generated by ELVis's labelling can be unreliable; this creates a significant problem during training, as the model will be utilizing a training dataset with a potentially high degree of label noise inside the negative class, hindering the model's ability to serve optimal explanations to the RS' users."}, {"title": "C. PU Learning", "content": "In Section I, we anticipated that, under the authorship-based modelling, predicting the best user-uploaded image to explain a user-item recommendation must deal in reality with Positive-Unlabelled (PU) data. In this context, positive examples are all triads in the original, raw data (i.e. all the images uploaded by a user are good explanations for that user), and all other possible user-photograph pairs can be considered unlabelled (i.e. we don't know whether the user would upload images like those of other users). While all these pairs with unknown label can be naively considered negative for training, it breaks the basic assumption that some of the other users' photographs can be used as good explanations for any given user, as discussed in II-B2. As a general solution, Positive-Unlabelled Learning is a Machine Learning paradigm intended for data scenarios with only positive and unlabelled examples, where no known negative training examples are available [11].\nA basic assumption of PU Learning, which matches the definition of the explainability task discussed in this work, is that the subset of unlabelled examples comprises both positive and negative ones; in our particular case, this translates to the existence of good explanatory images for a user within the images uploaded by other users. The major challenge of this paradigm lies in separating the unlabelled samples; existing approaches are broadly categorized by Bekker and Davis [11] into three main approximations: 1) biased learning, which handle the unlabelled set as a negative set with labelling noise [16], [17], 2) inclusion of the class prior into the learning process, where the probability of an unlabelled sample being positive is known or can be estimated [18], [19], and 3) two-step approaches, where the model is trained using the positive examples and a subset of reliable negative examples inferred from the unlabelled set [20], [21]. The two-step techniques are the most widespread in the literature [11], and as the sparsity and user-personalized nature of interaction data make biased learning or class prior estimation unreasonable, we focus on the use of two-step PU Learning techniques for our work, as discussed in the following Section."}, {"title": "III. PROPOSAL (PU-ELVIS)", "content": "In Section II-B2, we showed how existing state-of-the-art methods perform an overly naive sampling of negative examples for the LtR task of selecting user-uploaded item images as personalized explanations. Instead of a random selection, which can mistakenly label as negative (bad explanations) images by other users that share good explanatory features with the target user's, this work proposes a novel labeling method through a distance-based two-step Positive-Unlabelled (PU) Learning [11] approach. For each user, our method identifies images from other users that significantly differ from the user's, and uses them as reliable negative examples (bad explanations for the user) without introducing label noise during training.\nThis PU Learning approach was designed under the SCAR assumption [22] as labelling mechanism, which posits that the set of labelled positive examples forms a i.i.d sample of the positive distribution. In our particular use-case, this implies assuming user-wise 1) uniformity and 2) separability so, for any given user, within the unlabelled set (i.e. images uploaded by other users): 1) images that are good explanations for the user have similar features to the user's, and 2) images that are not good explanations for the user have dissimilar features to the user's.\nFor the first step of the two-step paradigm, which involves the selection of these reliable negatives, we designed a novel approach for the task based on user-wise personalization of these negatives using a Rocchio [16] classification, where instead of defining a global criterion for the selection of reliable negatives, this criterion is particular for each user based on their explanatory preferences. This process, depicted in Figure 2, works as follows for each user u:\n1) A prototype of u's known positive examples (i.e. those images originally uploaded by u) is found by calculating the centroid Cu of the images (using the Inception [14] latent embeddings of size d = 1536):\nCu = \\frac{1}{|P_u|} \\sum_{p \\in P_u} V_p\n(1)\nwhere Pu is the set of u's images, and V is an image's latent embedding. Cu forms a global representation of u's images, i.e. a prototype of what is a good explanation when recommending items to u.\n2) A similarity criterion is defined to act as a selection threshold for the user's reliable negatives. This threshold was defined as the 10th percentile (P10) of similarities of user's images to their centroid Cu, due to its good outlier filtering properties in existing PU Learning works using Rocchio classifiers [16], [23]. The cosine similarity (Eq. 2) was used as the similarity metric given its demonstrated performance in image feature similarity tasks in the literature [24]:\nSc(V_p, V_{p'}) = \\frac{\\sum_{i=1}^{1536} V_p[i] * V_{p'}[i]}{\\sqrt{\\sum_{i=1}^{1536} (V_p[i])^2} \\sqrt{\\sum_{i=1}^{1536} (V_{p'}[i])^2}} \\in [-1,1]\n(2)\nwhere (p, p') is any image pair. Sc(Vp, Vp') = 1 means images with maximum similarity, and vice-versa.\n3) Before training starts, negative examples (images that are not useful as explanations for the user) are selected for each user. To ensure a fair comparison, the overall skeleton of the selection follows that of the state-of-the-art method ELVis [10]: for each of the user's images (i.e. a positive example (u,i,p)), 20 images are selected, where 10 are images from the same restaurant i (forming negative examples (u, i, pneg)), and the remaining 10 are from different restaurants (forming negative examples (u, i', Pneg)). Our PU Learning approach adds the additional strict selection criterion for negative example reliability: all selected \"negative\" images pneg must comply that\nSC(Cu, V_{p_{neg}}) >= P_{10u}\n(3)\nthus verifying that they are dissimilar from the user's own, avoiding the label noise caused by the random negative selection used in the State of the Art.\nWhen the selection of reliable negative examples is completed, a full training set of positive examples (images originally uploaded by their users, i.e. good recommenda-tion explanations for them) and reliable negative examples (images unlikely to be uploaded by the users they have been paired with, i.e. reliably bad explanations for them) can be used to train the final explainer model; we have maintained ELVis'architecture and training policy for fairness of comparison, but our PU learning approach is model agnostic and could be used with any other explained designed for the task under the authorship modelling."}, {"title": "A. Experimental Setup", "content": "We designed the evaluation experiments maximizing the fairness of the comparison of our PU-based approach with the non-PU alternative from Diez et al's ELVis [10]. A Random image selection method RND is also evaluated as a baseline equivalent to not personalizing explanations.\nWith respect to hyperparameter optimization, a ran-dom search of 25 hyperparameter combinations against Barcelona's validation partition was performed on the search space lr = {1e-3,5e-4, 1e-4,5e-5, 1e-5} (learning rate) and d = {64, 128, 256, 512, 2014} (no. of latent features). The best found hyperparameter combination was used to train and test the model for all datasets.\nRegarding evaluation, we measured the quality of the explanation rankings for each test case with classic implicit feedback metrics Recall@k and NDCG@k, with k = 10 [25]. D\u00edez et al. [10] observed that most users have very few images, leading to cold start evaluation issues; consequently, we followed their evaluation criterion of only considering test cases that test the performance of users with \u2265 10 images in the train partition. All training, inference and evaluation code is made available on a public repository\u00b9."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "This section shows the results and performance compari-son of the proposed PU Learning approach for image-based RS explainability in accordance with the experimental setup defined in the previous section.\nFigure 3 shows the Recall@k (upper) and NDCG@k (lower) curves (higher values are better) with k=1 to k=10 curves in each dataset for a Random baseline and the state-of-the-art method ELVis using either the naive non-PU training policy and our proposed PU Learning approach for the selection of training negative examples."}, {"title": "V. CONCLUSIONS", "content": "Recommender Systems' increasing significance has made explainability a crucial requirement for future user ac\u0441\u0435\u0440-tance, trust, fairness and compliance with international AI regulations. This work explores the particular visual-based approach to RS explainability consisting of personalizing recommendations with user-uploaded item images. The top state-of-the-art approaches [10] model this as an authorship prediction task, but have one crucial limitation: to generate negative training examples they assume that, for any given user, any image uploaded by other users can be considered a bad explanation for the user; this is exceedingly naive and hinders the explainer model's performance, as images from other users can share the same good explanatory features as the user's own.\nIn this work we propose a novel method that, leveraging Positive-Unlabelled (PU) Learning techniques to handle the negative example labelling problem, allows us to surpass the State of the Art in this task. A user-personalized two-step PU Learning is designed based on latent image similarity and Rocchio classification, generating for each user a set of reliable negative training examples (images that are not good explanations for the user) which are confidently not similar to the user's own; any explainer model can then be trained on this refined set of training data with minimal label noise. In computational experiments, the designed PU-based approach consistently outperformed D\u00edez et al.'s state-of-the-art, naive non-PU method [10] in six real-world datasets of image-based explanation of restaurant recommendation, reaching relative performance improvements of ~ 20% in large datasets.\nOverall, the contribution of this novel PU-based labelling of negative examples in the task has relevant two-fold implications in the field of user-personalized explainability of Recommender Systems: not only does it improve the quality of the explanations, therefore increasing user trust and usability of the RS, but it does it without an increase in the explainer's complexity and computational cost, instead focusing on refining the quality of the training data.\nRegarding future work, and concerning the PU Learning approach, our design utilizes the underlying SCAR as-sumption for labelling mechanism, the most common and reasonable in the field. However, there may exist a minority of polarized users with clustered, very differing tastes (e.g. a user that uploads photos of either wine bottles or the outside view of the restaurant), which would invalidate this assumption. In spite of the good obtained performance, the extension of the proposed algorithm to respect weaker assumptions may provide an edge in explanation quality."}]}