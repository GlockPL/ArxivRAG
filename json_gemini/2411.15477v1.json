{"title": "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "authors": ["Abhinav Joshi", "Shaswati Saha", "Divyaksh Shukla", "Sriram Vema", "Harsh Jhamtani", "Manas Gaur", "Ashutosh Modi"], "abstract": "Large Language Models (LLMs) have shown to be a great success in a wide range of applications ranging from regular NLP-based use cases to AI agents. LLMs have been trained on a vast corpus of texts from various sources; despite the best efforts during the data pre-processing stage while training the LLMs, they may pick some undesirable information such as personally identifiable information (PII). Consequently, in recent times research in the area of Machine Unlearning (MUL) has become active, the main idea is to force LLMs to forget (unlearn) certain information (e.g., PII) without suffering from performance loss on regular tasks. In this work, we examine the robustness of the existing MUL techniques for their ability to enable leakage-proof forgetting in LLMs. In particular, we examine the effect of data transformation on forgetting, i.e., is an unlearned LLM able to recall forgotten information if there is a change in the format of the input? Our findings on the TOFU dataset highlight the necessity of using diverse data formats to quantify unlearning in LLMs more reliably.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable performance on a variety of tasks (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and a broad range of applications going beyond regular NLP tasks (Xi et al., 2023; Wei et al., 2024). However, LLMs have been trained using vast sources of texts, which may include personal information of an individual as well. It has encouraged researchers to develop methods for forcing LLMs to forget undesirable information without degrading the performance on regular tasks, giving rise to the area of Machine Unlearning (MUL) (Liu et al., 2024; Si et al., 2023; Yao et al., 2024; Blanco-Justicia et al., 2024; Maini et al., 2024). Moreover, recently, user privacy in terms of unintended use of personal data has gained some interest, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, which empower users with the \"Right to be Forgotten\" (RTBF), i.e., an organization must remove/delete all the information if a user wants to revoke access to their information, with a minimal delay. Researchers in the MUL community have proposed various methods (Ilharco et al., 2023; Chen and Yang, 2023; Dong et al., 2024) and text-based benchmarks (Maini et al., 2024; Li et al., 2024). For example, to evaluate forgetting in LLMs Maini et al. (2024) have created the TOFU benchmark built using a dataset having facts about various fictitious entities. The TOFU dataset uses a particular format (e.g., Q&A (Questions and Answers)); however, the same information can be expressed in multiple ways in natural language. In this work, we investigate if unlearning algorithms are sensitive to data formats, i.e., we experiment with a setting where the learning/unlearning happens in one default format and study how the unlearning performance varies when the same information is presented in a different format. In a nutshell, we make the following contributions:\n\u2022 We propose a new evaluation scheme to enhance the quality checks in the unlearning benchmarks. By creating a dataset built over TOFU (fictitious authors dataset), we present 5 new formats in which the same information can be represented. The formats include multiple-choice, odd-one-out, analogies, cloze tests, and comprehension.\n\u2022 We present different evaluation metrics to validate the performance over the created dataset formats and perform analysis of some representative unlearning algorithms.\n\u2022 We observe different performance gaps between target and unlearned models on different formats, highlighting the need to consider multiple formats for a more reliable/robust evaluation of unlearning algorithms. We re-"}, {"title": "2 Related Work", "content": "LLMs, despite their significant advancements (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), are susceptible to inadvertently disclosing sensitive information or personal details as billions of trainable parameters are utilized during training. Recent studies have adopted different approaches using machine unlearning (Cao and Yang, 2015) to alleviate this issue and achieve trustworthiness (Lu et al., 2022) and fairness (Yu et al., 2023) by removing sensitive information (Hendrycks et al., 2023; Barrett et al., 2023). The primary objective of machine unlearning is to modify the weights of a pre-trained model, allowing it to unlearn the knowledge acquired from a specific subset of data intended to be erased while maintaining performance on the retained set. Recently, the notion of exact unlearning has garnered significant attention. This method involves re-training the model from scratch after removing specific training data points, which are considered the gold standard for unlearning. Nevertheless, this method entails substantial computation cost and demands access to the whole training set (Thudi et al., 2022). To overcome these challenges, recent research efforts have shifted focus towards developing scalable and effective approximate unlearning (Chen et al., 2023; Becker and Liebig, 2022; Warnecke et al., 2021; Golatkar et al., 2020; Thudi et al., 2022; Jia et al., 2023) methods. One of the concurrent works by Liu et al. (2024), emphasizes on usage of data transformation techniques to evaluate unlearning effectiveness in LLMs. In this work, we provide a medium to achieve this by creating an extended version of the TOFU benchmark."}, {"title": "3 Problem Definition and Methodology", "content": "Problem Setup: A broader applicability of LLMs considers using an open-weight model Me with parameters \u03b8 as a base to enhance them with new proprietary information Dp. A general machine learning/unlearning pipeline follows training/finetuning the base model over new information Dp by constructing a training set Dtrain = {(Xi, Yi)}i=1 derived from information in Dtrain ~ fi(Dp), where fi denotes the transformation of the information into a format, such as Q&A. The model M\u0473 is trained/finetuned over the created Dtrain to obtain a Finetuned-LLM M\u00f4 where ) represents the updated model parameters. Since the new proprietary information is user-specific, user(s) may ask to remove/erase their data, leading to a forget set split from the Dtrain = Dretain \u222a Dforget. The goal of an unlearning algorithm is to update the fine-tuned LLM M\u00f4 to obtain an unlearned version M\u014d (here \u03b8 represents model parameters after unlearning) that shows behavior similar to Me over the held-"}, {"title": "Measuring Effectiveness of Unlearning via Data Transformation", "content": "out forget-set D forget.\nBenchmarking of the unlearning algorithms usually relies on a single format (fi). However, the same information Dp can be represented in M different format f1, f2,... fm \u2208 F where F is the set of all possible dataset formats. When unlearning, it becomes imperative to ensure the information in the forget set is removed from model parameters \u0113 and does not depend on the transformation style fi, i.e., the model performance on Dforget should be similar for all the formats in which the dataset can be represented. Fig. 1 explains the entire process with an example.\nMeasuring Effectiveness of Unlearning via Data Transformation: In our study, we make use of a recent machine unlearning benchmark TOFU (Maini et al., 2024) that considers a setup of unlearning via new information simulated as details about 200 fictitious authors. The TOFU dataset uses 20 Q&A queries about each of the fictitious authors to represent all the information in a Q&A format. The total dataset consists of 4k Q&A pairs. To study the effect of data format, we choose a set of 3 new formats to cover different aspects of knowledge retrieval about the same information, including MCQA (Multiple Choice Question Answering), Cloze, and Analogy (See Fig. 1 for examples), to ask similar questions in a different style. Additionally, we propose using two additional formats, Odd-one-out and Comprehension, to enhance the evaluation quality. We briefly describe each of the transformations in here (details in App. A).\n1) MCQA (Multiple Choice Question Answering): For each of the queries present in the default Q&A format, we rephrase the same question by providing multiple options for the answers.\n2) Cloze test: One could also form a Cloze test setting where the queries are provided with a passage that has certain words missing from it to mask out an information specific to an author. We mask entities only towards the end of the sentence for easier validation over autoregressive LMs.\n3) Analogy: Another way in which the information can be retrieved is if the network is able to make relations between the entities (e.g., author name \u2192 birth year :: author name \u2192 country) by providing some examples in the context (ICL) and asking about another author as a query. In other words, we assume the information pool contains details about 5 authors A1, A2,..., A5 and the Fintuned-LLM is trained over all the details about these authors. During unlearning, if we remove the information about two of the 5 authors (A2 and A5), the goal of the analogy test is to check if the Unlearned LLM is able to retrieve the information about A2 and A5, given the relationship from retained authors. For example, given A\u2081 <name> : A1 <place-of-birth> :: A2 <name> : ?, the analogy test validates if the Unlearned-LLM can still retrieve A2 <place-of-birth> .\n4) Odd-one-out: In this format, a query is given to choose the odd one out from a given set of options where one option is coming from retain/forget and another set of wrong options is coming from forget/retain set. Ideally, the Finetuned-LLM is expected to perform badly over these queries (having no distinction between forget and retain sets), and as the unlearning progresses, the Unlearned-LLM should show an increased performance since it contains information only about the retain set.\n5) Comprehension: Another interesting way to enhance the validity of unlearning would be to provide all the information in the context and ask the same questions in different styles such as Q&A, MCQA, etc. Since all the information is present in the context, ideally, the Unlearned-LLM should perform equally as the pretrained LLM, i.e., the unlearning algorithms should show no gap between the retain and the forget set. A gap in retain and forget set for this task would mean the unlearned LLM suppressing generation of the forget set answers to perform well on the objective. For this task, we draw our inspiration from SQUAD 2.0 (Rajpurkar et al., 2018), which tests the model's ability to extract information from a prompt and answer questions accurately."}, {"title": "4 Experiments, Results and Analysis", "content": "4.1 Unlearning Algorithms\nWe briefly discuss the key unlearning algorithms studied in this paper.\n1) Gradient Ascent (Maini et al., 2024): This method decreases the probability of generating these memorized tokens by maximizing the log-likelihood loss on the memorized data, a reversal of the next token (xt) prediction loss: $L_{UL} = \\sum_{t=1}^{T}log(M_{\\theta}(x_t | X_{<t}))$\n2) Gradient Difference (Liu et al., 2022): We"}, {"title": "4.2 Results", "content": "If unlearning went perfectly, we would expect the unlearned model to perform the same as a pretrained model on the forget set, and both to be lower than the finetuned model. Fig. 2 and Fig. 3 show the results. As can be seen in Fig. 2, we observe deviations from this expectation. More importantly, the behavior is different across various formats. For instance, the unlearned model gets a higher score than the pretrained one in Q&A format on the forget set but much lower than a finetuned model, suggesting that the unlearning algorithm did well. However, under an alternative format (Cloze), the unlearned model gets a much higher score than the pretrained one, and its gap with fine-tuned is also relatively less, suggesting that the unlearning algorithm did not perform as well as perceived only on the basis of the original Q&A format. We observe similar patterns when evaluating across multiple data formats, demonstrating that unlearning methods do not perform as well as perceived only on the basis of the original data format. The observations hold true across all three unlearning methods when using llama-2 (App. Table 3) as well as the Phi model (App. Table 4) as the underlying base model. Similarly, Fig. 3 shows the performance over the retain set, we observe a varying performance with different dataset formats. More specifically, we find that over the Comprehension-Q&A format, where all the information is available in the context, the performance of the model should be maintained across the three models, however, we observe a decline with the unlearning algorithm, hurting the comprehension ability of the LLMs. Similar trends are observed for the Phi model (App. Fig. 19 and Fig. 18)\nQualitative Analysis: In the App. E, we provide a few qualitative examples where the same information is present in different proposed formats. We find that when evaluating these, the genera-"}, {"title": "5 Discussion", "content": "tion/performance quality of the Unlearned-LLMs varies by a significant margin. For a few cases, the Unlearned-LLM predicted the correct choice in the MCQA format and failed to generate the expected text in another format (Fig.9). In Fig.10, Q&A (the default format) and the MCQA provided the correct predictions. In Fig.11, we observe a different query for the same author present in Fig.10, and the predictions over Q&A format are almost correct, whereas the other two formats gave wrong predictions. Similarly, Fig.12 shows a varied prediction over different formats, and some examples show a wrong prediction in all the formats (Fig.13).\nIn general, predictions across formats vary, making it essential for unlearning benchmarks to validate performance on different formats to ensure the quality of unlearning algorithms."}, {"title": "6 Conclusion", "content": "In this work, we study the role of dataset transformation in unlearning. We enhance an existing dataset with multiple new formats, validating the effectiveness of unlearning algorithms. We further experiment with open-weight models over the created evaluation settings, highlighting the impact of data transformation. With quantitative and qualitative analysis, our empirical findings point towards reaching a better validation criterion for unlearning algorithms. We find that evaluation over a single format may lead to unreliable improvements, and unlearning benchmarks should consider evaluation over multiple formats. We hope the curated dataset transformation in 5 different formats will be a useful resource for future benchmarking of unlearning algorithms."}, {"title": "Limitations", "content": "One of the primary limitations of our work is a limited set of formats to highlight the effect of changes in dataset. We only considered five common task formats; in the future, it would be good to add more variety to improve the quality of unlearning evaluation.\nIn all our experiments, we consider using the default format provided by the ToFU benchmark (Maini et al., 2024), and the learning and unlearning take place in the default format. In the future, it would be interesting to perform the same evaluation using different combinations, i.e., learning and unlearning on different sets of dataset formats.\nAnother limitation of our work is the limited set of unlearning methods used for reporting the evaluation findings. In the current version, we specifically chose the widely used methods that were benchmarked by the ToFU benchmark. In the future, a more detailed study can be done to evaluate more unlearning methods.\nIn summary, the primary focus of this work was to enhance the evaluation scheme used by the unlearning benchmarks and point towards the varied performance under dataset format transformation. We hope this research will facilitate the evaluation of the ToFU benchmark and help frame better evaluation schemes for future unlearning benchmarks."}, {"title": "Ethical Aspects", "content": "To the best of our knowledge, our work does not have any direct negative ethical consequences. The entire dataset was built upon a fictitious author dataset (ToFU, Maini et al. (2024)), and all the facts present in the TOFU dataset were manually verified after each dataset format conversion."}, {"title": "Appendix", "content": "A Data Transformations Details\nIn this section, we provide additional details for each of the created data transformations.\n1) MCQA (Multiple Choice Question Answering): For each of the queries present in the default Q&A format, we rephrase the same question by providing multiple options for the answers. We use GPT-3.5-turbo to convert the answers into a shorter option form and also generate three other plausible but false answer options. After the conversion, we manually inspect if the generated set of MCQA queries reflects the correct choice as an answer label by comparing it with the Q&A format.\n2) Cloze test: To get the information about an author present in the Q&A format, we frame a Cloze test setting where the queries are provided with a passage that has certain words missing from it to mask out an information specific to an author. We mask entities only towards the end of the sentence for easier validation over autoregressive LMs.\n3) Analogy: For creating the Analogy format of the dataset, we used GPT-3.5-turbo to extract (subject, relation, fact) for all the authors and manually inspect them to verify they contain the same factual information. Further, we choose the context relationships from the retain set, and query relations come from both retain and forget sets to assess the quality of both. Table 2 presents the relation types we used to generate prompts for the analogy evaluation format.\n4) Odd-one-out: In this format, as explained in the main paper, a query is given to choose the odd one out from a given set of options where one option is coming from retain/forget and another set of wrong options is coming from forget/retain set. Ideally, the Finetuned-LLM is expected to perform badly over these queries (having no distinction between forget and retain sets), and as the unlearning progresses, the Unlearned LLM should show an increased performance since it contains information only about the retain set. To create this format, we consider answers from the default Q&A format as facts.\n5) Comprehension: For creating this format, we take inspiration from SQUAD 2.0 (Rajpurkar et al., 2018), which tests the model's ability to extract information from a prompt and answer questions accurately. For creating this format, we combine each author in the ToFU dataset's related answers into a single paragraph and rewrite them with ChatGPT-4 to create a more comprehensive reading prompt. We then match these prompts with the multiple choice and question-answer pairs related to that author to evaluate the model's comprehensive ability. Keeping in line with the size of the TOFU dataset Maini et al. (2024), we generate same number of samples for our evaluation formats as mentioned in Table 1. We also maintain the same size splits for Forget01/Retain99, Forget05/Retain95, and Forget10/Retain90 in our evaluation formats.\nWe provide the evaluation prompt templates used for all the formats in App. C. Fig. 4, Fig. 5, Fig. 6, Fig. 7, and Fig. 8 highlight the MCQA, Cloze test, Analogy, Odd-one-out, and Comprehension, respectively."}, {"title": "B Evaluation in different Formats", "content": "For each of the different proposed formats, we make use of a few standard evaluation metrics.\nQ&A: For reporting the performance over Q&A format, we follow Maini et al. (2024) and consider using ROUGE score (Lin, 2004) as the performance metric over the expected answer text as reference and the text predicted by the Language Models.\nMCQA: We frame the prompt as a multi-choice question-answering (MCQA) objective (Robinson and Wingate, 2023). The prompt is intentionally structured so that the LLM is intended to predict a single-choice token (Such as \u201cA\u201d, \u201c B\u201d, etc.). Further, The next-token prediction probabilities of the option IDs are used as the observed prediction distribution, and the success rate is computed by comparing the predicted option IDs with the true label. The success rate corresponds to the percentage of queries where the LLM predicts the desired choice.\nCloze Test: For evaluating the Cloze test format, recognizing that probabilities of answer sequence might be skewed by especially common or uncommon tokens or sequences of varying length, we follow Brown et al. (2020) and report the metric where the sequence's probability is normalized for length by taking the nth root.\n$P(x_1,x_2,...,x_n) = \\sqrt[n]{\\Pi_{i=1}^{n} P(X_i)}$\nIn general, all the MCQA-based evaluations (including MCQA, Analogy-MCQA, Odd-one-out, comprehension-MCQA dataset formats) are done"}]}