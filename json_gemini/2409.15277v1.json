{"title": "A PRELIMINARY STUDY OF 01 IN MEDICINE: ARE WE CLOSER TO AN AI DOCTOR?", "authors": ["Yunfei Xie", "Juncheng Wu", "Haoqin Tu", "Siwei Yang", "Bingchen Zhao", "Yongshuo Zong", "Qiao Jin", "Cihang Xie", "Yuyin Zhou"], "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's 01, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a preliminary exploration of 01 on different medical scenarios, comprehensively examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of 01 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, 01 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we also identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/ol_medicine/ for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Intelligence, a complex and elusive concept, has puzzled psychologists, philosophers, and computer scientists for years (Bubeck et al., 2023). While there is no single agreed-upon definition of intelligence, it is widely accepted that it spans a broad range of cognitive skills, rather than being confined to a specific task (McCarthy et al., 1955). Creating artificial systems with such general intelligence has been a long-standing and ambitious goal of AI research. The most exciting progresses in AI are achieved by language models in these years, from the initial start of ChatGPT to its evolution and other open-source projects (Touvron et al., 2023a;b; Jiang et al., 2023; Bai et al., 2023; Peng et al., 2024).\nEarly LLM pioneers set out goals to understand and interact with human by exploring generalizable reasoning mechanisms and building knowledge bases with vast amounts of commonsense information. With parameters and data volume in place, the question of how to effectively prompt the model from the user end and train it from the developer end has become a trending topic of exploration (Wei et al., 2022; Ouyang et al., 2022). On the user side, varying prompting techniques can significantly impact model performance. Chain-of-thought (CoT) prompting (Wei et al., 2022; Dong et al., 2022; Saunders et al., 2022), one of the most popular strategies, leverages the model's internal reasoning patterns to enhance its ability to solve complex tasks. OpenAI capitalized on this by embedding the CoT process into model training, integrating reinforcement learning, and finally introduced the 01 model (OpenAI, 2024). While the 01 model demonstrates strong performance in general domains, its effectiveness in specialized fields like medicine\u2014where domain-specific training may be lacking\u2014remains uncertain. Moreover, current benchmarks for LLMs in the medical domain often evaluate models only on a limited set of factors, often focusing on isolated aspects such as knowledge and reasoning (Nori et al., 2023b; Li\u00e9vin et al., 2024), safety (Han et al., 2024), or multilinguality (Wang et al., 2024). These factors make a comprehensive assessment of LLMs' capabilities-especially for advanced models like 01-in medical challenging tasks (Figure 1).\nThis paper aims to provide an initiative to close this gap, focusing on 01. We identify three fundamental aspects of LLMs in medicine: understanding, reasoning, and multilinguality. To evaluate these capabilities, we assembled 35 existing medical datasets and developed two novel, challenging QA datasets that include instructions and expected outputs, ensuring comprehensive assessment. With evaluation on this extensive suite, our key findings include:\n\u2022 01 demonstrates improved transfer of clinical understanding and reasoning abilities, validating its competence in real-world diagnostic scenarios compared with both close- and open-source models as presented in Figure 1 and Figure 2;\n\u2022 No single model excels across all tasks on our medical leaderboard, though 01 comes close to dominating most evaluations;\n\u2022 01 still suffers from the long-standing issue of hallucination and complex multilingual medical cases;\n\u2022 Inconsistencies in metrics for medical NLP can significantly affect models' standings, which calls for a re-evaluation of reliable metrics for future LLMs;\n\u2022 CoT prompting can further enhance 01 in medicine, despite its training having already integrated CoT data.\nIn addition to these findings, we also elevate the discussion section as an initial attempt to address the issues identified during our benchmarking in Section 5. Particularly, we highlight the potential negative effects of 01, emphasize the urgent need for consistent and unified evaluation metrics for future LLMs, and advocate for improved instruction templates that can be applied to models with embedded prompting strategies."}, {"title": "2 RELATED WORKS", "content": "Large Language Models with Enhanced Reasoning Ability. Large Language models (LLMs) based on next token prediction pre-training (Touvron et al., 2023a;b; Achiam et al., 2023) have demonstrated promising capabilities on various language undersanding tasks. Instruction fine-tuning further improved the abilites of these LLMs for following user instructions. However, recent studies suggest that LLMs struggle with complex tasks involving logical reasoning. To address this issue, some researches propose to instruct LLMs to mimic human thinking processes by producing a chain-of-thought (CoT) (Feng et al., 2024; Wei et al., 2022) before generating a final answer. Reinforcement learning from human feedback (Ouyang et al., 2022) has also been employed to enhance reasoning while make sure the models align with human values (Tu et al., 2023b;a). Recently, OpenAI introduced 01, which was trained on a vast amount of CoT data, further enhancing the capability of LLMs in solving scientific problems. In this paper, we aim to investigate whether enhanced abilities of 01 effectively transfer to the clinical medical domain.\nMedical Large Language Models. Benefiting from the generalization capabilities of LLMs, general-purpose models such as GPT-4 have demonstrated impressive performance on challenging medical problems (Nori et al., 2023a; Wu et al., 2024b). Some researchers have attempted to further equip LLMs with biomedical knowledge by fine-tuning them using domain-specific corpora (Chen et al., 2023; Wang et al., 2023; Wu et al., 2024a; Li et al., 2023). However, for clinical applications, LLMs are not only required to understand medical domain-specific knowledge but also to produce reliable responses by performing logical reasoning. In this paper, we aim to explore the potential of 01 as a clinical viable model. Our experimental findings reveal that with enhanced understanding, reasoning, and multilinguality medical capabilities, 01 makes a step closer to reliable clinical AI-system."}, {"title": "3 EVALUATION PIPELINE", "content": "3.1 OVERALL TAXONOMY OF EVALUATIONS\nFirst, we present the taxonomy of our evaluation, along with an overview of the evaluation pipeline as shown in Figure 3. Firstly, we specify three aspects of the model capabilities, namely understanding, reasoning, and multilinguality, that correspond to the real-world needs of clinical physicians. To ensure a comprehensive evaluation, we collect a diverse range of medical tasks and datasets that fall under these three aspects. Moreover, we explore three prompting strategies in our pipeline, including (1) direct prompting, which instructs LLMs to solve specific problems directly, (2) chain-of-thought, which requires models to think step-by-step before generating the final answer, (3) few-shot"}, {"title": "3.2 ASPECTS AND TASKS", "content": "In Table 1, our evaluation efforts are structured into three main parts: aspect, task, and dataset. Specifically, a dataset refers to the data itself along with the metrics used in the current context. We utilize 35 existing datasets and create 2 additional challenging datasets for evaluation. A task is a collection of multiple datasets that share a common goal or evaluate similar capabilities within the model. We categorize all 37 datasets into 6 tasks for clearer evaluation and analysis. An aspect describes a specific capability or property to understand how well the model performs in a particular area. In our evaluation pipeline, we focus on three key aspects.\nFormally, we illustrate these three evaluation aspects with their corresponding tasks as follows:\n\u2022 Understanding refers to the model's ability to utilize its internal medical knowledge to comprehend medical concepts. For example, in concept recognition task, the model is required to extract or elaborate medical concepts from article (Savery et al., 2020; Pafilis et al., 2013; Nye et al., 2018) or diagnosis report (Zhao et al., 2023). And in text summarization, the model need to understand concepts in complex texts to generate a concise summary (Lee et al., 2021; Wallace et al., 2021; Johnson et al., 2019; 2023).\n\u2022 Reasoning is the ability to conduct multiple steps of logical thinking to arrive at the conclusion. In question answering tasks, the model is prompted to select correct option from multi-choices based on reasoning derived from the medical information provided in the question. In addition to common question-answering datasets (Jin et al., 2019; Pal et al., 2022; Jin et al., 2021), we collect real-world clinical questions from The Lancet, the New England Journal of Medicine (NEJM), and Medbullets (Chen et al., 2024) to better assess the clinical utility of LLMs. In the clinical suggestion task, the model is required to provide treatment suggestions (Dubey et al., 2023; Li et al., 2023) or diagnostic decisions (Xie et al., 2022; Fansi Tchango et al., 2022) based on patients' information. In the AI Hospital (Fan et al., 2024) and AgentClinic (Schmidgall et al., 2024) datasets, we task the model with serving as a medical agent. Furthermore, in the MedCalc-Bench (Khandekar et al., 2024) dataset, the model is required to perform mathematical reasoning and calculate answers.\n\u2022 Multilinguality is the ability to complete a task when the languages of input instruction and/or output answers are changed to different languages. For example, XMedBench (Wang et al., 2024) dataset requires LLMs to answer medical questions in six languages, including Chinese, Arabic, Hindi, Spanish, Chinese and English. In AI Hospital dataset (Fan et al., 2024), the model is required to serve as an agent using Chinese."}, {"title": "3.3 METRICS", "content": "In this section, we elaborate on metrics employed in our evaluation pipeline.\n\u2022 Accuracy is used to directly measure the percentage of models' generated answer which exactly match with the ground-truth. We use accuracy for multi-choice question datasets, MedCalc-Bench (Khandekar et al., 2024) dataset, and portions of clinical suggestion and concept recognition datasets where the ground-truth answer is a single word or phrase.\n\u2022 F1-score (Pedregosa et al., 2011) is the harmonic mean of precision and recall. It is employed in datasets where the model is required to select multiple correct answers.\n\u2022 BLEU (Papineni et al., 2002) and ROUGE (Lin & Hovy, 2002) are NLP metrics measuring the similarity between the generated respond and the ground-truth. Specifically, we utilize BLEU-1 and ROUGE-1 for all free-form generation tasks in our evaluation.\n\u2022 AlignScore (Zha et al., 2023) is a metric to measure the factual consistency of generated text. In this paper, we use AlignScore for all free-form generation tasks to evaluate the extent of model's hallucination.\n\u2022 Mauve (Pillutla et al., 2021) is a measure of gap between distribution of generated and human-written text. It is employed for all free-form generation tasks."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT DETAILS\nPrompting strategies. For most datasets, we employ the same prompting strategy as described in previous literature (Wu et al., 2024b; Nori et al., 2023b;a): For knowledge QA tasks, agent tasks, medical calculation tasks, and multilingual-related tasks, we use the direct prompting evaluation method, which is consistent with the settings of these benchmarks. For other tasks derived from MedS-Bench (Wu et al., 2024b), we follow their benchmark settings, leveraging a few-shot (3-shot) prompt strategy with its template shown in Section A.1. As officially suggested by OpenAI, common prompting techniques such as Chain-of-Thought (CoT) (Wei et al., 2022) and in-context examples may not boost 01's performance as it has implicit CoT built in. To further validate this claim, we also investigate the effect of several advanced promptings in our evaluation (e.g., CoT, Self-Consistency (Wang et al., 2022), and Reflex (Shinn et al., 2024)), the detailed input instruction formats are in Section A.1\nModels for evaluation. We choose the following models to evaluate: GPT-3.5 (gpt-3.5-turbo-0125)3, an advanced language model by OpenAI known for its enhanced contextual understanding; GPT-4 (gpt-4-0125-preview) (Achiam et al., 2023), the successor to GPT-3.5 with significant improvements in reasoning and language comprehension; 01 (01-preview-2024-09-12) (OpenAI, 2024), the lastest LLM model that is capable of performing highly complex reasoning by employing chain-of-thought reasoning. Apart from these close-source models, we have also incorporated two open-source ones in our experiments: MEDITRON-70B (Chen et al., 2023), an LLM trained with medical-centric data and Llama3-8B (Meta, 2024), the latest and strongest open LLM right now."}, {"title": "4.2 MAIN RESULT: Yes! WE ARE ONE STEP CLOSER TO AN AI DOCTOR", "content": "Enhanced ability of 01 transfers to its clinical understanding. Given the established results from 01, which underscore its remarkable effectiveness in knowledge and reasoning abilities such as mathematical problem-solving and code generation (OpenAI, 2024), we observe that this superior capability can also be transferred to the specific clinical knowledge understanding. Results presented in Table 2 demonstrate that 01 outperforms other models on the understanding aspect in most clinical tasks. We also present these statistics in Figure 1, where we observe that 01 has a larger cover radius across various medical datasets. For instance, on 5 concept recognition datasets that use F1 as the metric, 01 outperforms both GPT-4 and GPT-3.5 by an average of 7.6% and 26.6%, respectively (i.e., 72.6% vs. 65.0% vs. 46.0%), with a notable 24.5% average improvement on the widely used BC4Chem dataset.\nAdditionally, on the summarization task in Table 3, 01 achieves a 2.4% and 3.7% increase in ROUGE-1 score over GPT-4 and GPT-3.5 (i.e., 31.4% vs. 29.0% vs. 27.7%), demonstrating its enhanced capacity for real-world clinical understanding. This improved performance confirms that advancements in general NLP capabilities for LLMs can effectively translate to enhanced model understanding in the medical domain.\nThe 01 model demonstrates strong reasoning in clinical diagnosis scenarios. On the reasoning aspect, 01 takes a significant step forward in demonstrating its advantages in real-world diagnostic situations. In our newly constructed challenging QA tasks, NEJMQA and LacentQA, 01 showcases an average accuracy improvement of 8.9% and 27.1% over the performance of GPT-4 (79.6%) and GPT-3.5 (61.5%) on the respective datasets (Table 2). Another noteworthy improvement in 01 is its capacity for mathematical reasoning, elevating the baseline of MedCalc-Bench to 34.9%, which surpasses GPT-4 by a significant 9.4%. In more complex reasoning scenarios that involve multi-turn conversations and environmental simulations, 01 outperforms both GPT-4 and GPT-3.5 on the AgentClinic benchmark, achieving accuracy gains of at least 15.5% and 10% with scores of 45.5% and 20.0% on its MedQA and NEJM subsets, respectively. These observations serve as compelling evidence of 01's competence in complex real-world diagnosis and clinical utility scenarios."}, {"title": "4.3 FURTHER ANALYSIS", "content": "No model excels across all tasks in the medical domain. Table 2 and Table 3 indicate that, for now, there are always trade-offs (even under the same metric) to be made when selecting a model to use in the medical domain. One example is the clinical decision support task in Table 2, 01 outperforms both GPT-4 and GPT-3.5 on most datasets, but lags far behind GPT-4 on the MIMIC4ED-Critical Triage dataset by 5% in accuracy. Interestingly, we also found the recent released open LLM\u2014Llama3 takes a lead in PMC-Patient and PICO-Intervention datasets with an unexpected 19.6% accuracy gap between 01 and Llama3 on PMC-Patient (76.4% vs. 96.0%). Nevertheless, 01 comes close to being the best in most situations, it boasts a leading position across datasets in clinical decision support, knowledge QA, and medical calculation. This claim is supported by the average result over 19 dataset accuracy in Table 2 and Figure 2: 01 (74.3%) > GPT-4 (68.1%) > GPT-3.5 (53.2%)\nAdvanced prompting can partially help models trained with CoT data. 01 was released using chain-of-thought (CoT) data embedding in the training process; however, we found that applying the CoT prompting still enhances 01's performance on knowledge QA tasks in medicine, as shown in Table 6. The table reveals an average boost of 3.18% over the original 83.6% accuracy of 01. While this improvement is not as significant as with GPT-4, CoT proves to be a promising way for guiding 01 in medical tasks. However, when it comes to other fancy promptings, such as self-consistency (SC) (Wang et al., 2022) and reflex (Shinn et al., 2024), this conclusion may not stand still. We witness an average performance decline of 12.8% using these two strategies compared to only CoT on LancetQA (Table 7).\nHallucination remains a significant challenge. We use AlignScore (Zha et al., 2023) to evaluate hallucination in LLMs. In Table 4, the 01 model demonstrates a 1.3% decrease in AlignScore compared to GPT-4 across five text summarization datasets. Moreover, the overall improvements of 01 across three tasks (Table 4) in AlignScore significantly lag behind those of other evaluation metrics-averaging 0.7 in AlignScore compared to 9.9 in Mauve relative to GPT-4. This indicates that 01 is still susceptible to language hallucination, highlighting that such problem remains a persistent challenge in LLMs.\n01 struggles in reasoning over complex multilingual tasks. Advanced LLMs are expected to demonstrate equivalent reasoning abilities to languages other than English. However, as 01 consistently outperforms other models in multilingual QA tasks: 01 (85.2%) > GPT-4 (75.7%) > GPT-3.5 (54.1%) on average (Table 8), it falls short in a much more complex Chinese agent benchmark in Table 5-showing a 1.6% accuracy drop in the medical examinations scenario over GPT-4 (43.4% vs. 45.0%), leaving its multilingual reasoning in complex situations to be desired. This interesting outcome might be attributed to the lack of multilingual CoT data during 01's training, as learning complex reasoning routes generally requires more efforts than plain instructions in the few-shot paradigm (Kim et al., 2023; Singh et al., 2024). We present a failure example of 01 on AI Hospital in Figure 5. We identified instances of mixed language output in the generation from the doctor, which contribute to the suboptimal performance of 01 in this context.\nLLMs are facing biased judgement using different metrics. Choosing different metrics can lead to varied results of LLM evaluation (Liang et al., 2022), in our experiments, we observe a similar unaligned trend even leveraging traditional NLP metrics such as BLEU-1, ROUGE-1, and Mauve. In most cases from Table 3, 01 surpasses GPT-4 in both two traditional reference-based measurements (i.e., BLEU-1, ROUGE-1) on average. One exception arises in the BLEU-1 comparison for clinical suggestion tasks. While 01 significantly triumph over GPT-4 in ROUGE-L (24.4% vs. 17.2%), it surprisingly underperforms in BLEU-1: 01 (15.3) < GPT-4 (16.2). When considering Mauve scores, although 01 consistently surpasses GPT-4 in both averaged BLEU-1 and ROUGE-1 for text summarization tasks, it still falls short by 2.9 points in Mauve, even when evaluated on the same output texts. A similar anomaly can also be observed in the comparison between accuracy and F1 score. While Llama3 significantly outperforms 01 in accuracy on two concept recognition datasets, it consistently falls behind 01 in F1 on the same cases. These findings underscore the urgent need to identify or devise more reliable metrics for modern LLMs."}, {"title": "5 DISCUSSION", "content": "What adverse impacts does 01 bring? The model 01 has made significant strides in both general NLP and the medical domain\u2014as demonstrated in this paper. But what adverse impacts does 01 have on users compared to the previous generations of LLMs? While embedding the Chain of Thought (CoT) process during generation by default requires more time (OpenAI, 2024), what exactly distinguishes 01 from other OpenAI models? In Table 10, we see that 01 has more than 2\u00d7 and 9\u00d7 longer decoding time cost on four medical tasks compared to GPT-4 and GPT-3.5, respectively (13.18s vs. 6.89s vs. 1.41s). This increased decoding time can lead to significant waiting periods when handling complex tasks.\nAdditionally, 01 does not always outperform other models, with inconsistent performance across different tasks. For instance, in the concept recognition task detailed in Table 2, 01 underperforms compared to other LLMs on half of the datasets. This discrepancy may relate to recent findings suggesting that CoT data is most advantageous in more complex reasoning tasks (Sprague et al., 2024). However, in tasks that do not require complex reasoning, such as concept recognition, 01 does not have significant advantages over them.\nRethinking evaluation metrics for stronger LLMs. Traditional evaluation metrics like BLEU and ROUGE, which rely on n-gram overlap, have long been criticized for their limitations in capturing the quality of generated text, particularly for LLMs. As a result, using models like GPT-4 as evaluators, i.e., \"LLM-as-a-judge\", has gained popularity for assessing the outputs of other models. However, this approach may not be valid when applied to the most advanced models such as 01, as GPT-4 is even less capable and thus may produce less reliable evaluation. This is especially true for specialized domain like medicine. Therefore, there is a growing need to develop more robust and nuanced evaluation metrics that can better assess the performance of state-of-the-art LLMs in complex scenarios.\nCall for reliable prompting techniques for future LLMs. As noted in Section 4.3, not all advanced prompting techniques positively impact 01's performance. As future LLMs like 01 may continue to evolve with internal prompts for efficient user instruction, new prompting methods should consider their adaptability to existing strategies. One potential exploration could be the integration of two prompting strategies (Wang et al., 2022; Zheng et al., 2024)."}, {"title": "Limitations.", "content": "While we conduct comprehensive evaluations in the medical domain on understanding, reasoning, and multilingual capabilities, there are many other dimensions to consider such as safety (Han et al., 2024) and we leave them for future work. Additionally, we leave more advanced prompting techniques such as retrieval augmented generation (RAG) (Lewis et al., 2020) for future work, which may enhance the factuality and mitigate hallucination. It is worth noting that current GPT-like models may still underperform BERT-based specialists in classification tasks (Nori et al., 2023b). However, we focus on GPT-like generalists in this paper due to their greater flexibility as zero-shot learners."}, {"title": "6 CONCLUSION", "content": "This preliminary study assesses 3 important aspects across 35 existing and 2 novel medical datasets using the latest 01 model. It marks the first step towards a holistic evaluation of 01 in medicine, and we present our initial results, analysis, and discussion over the benchmark. The findings provide convincing evidence that 01 is narrowing the gap between AI and human doctors, shaping the vision of an ideal AI doctor closer to reality."}, {"title": "A.1 PROMPTING STRATEGIES", "content": "Base Prompt for MCQ.\nQuestion:\n{question}\nOptions:\nA)\nB)\n{Format Constraint}\nFormat Constraint Examples for MCQ.\nDefault:\nAnswer only with the option index such as A/B/C/D in plain text.\nTrue/False Statement Questions:\nAnswer only with Yes/No in plain text.\nFew-Shot Prompt.\nCase1:\nCase2:\nCase3:\n{Manually Written Definitions}\nPlease learn from the few-shot cases to see what content you have to output.\n{Input Case}\nCOT Format Constraint.\nReason step-by-step before answering. {Base Format Instruction}. Your final output should strictly follow this format:\n(Reason) {your step-by-step reasoning}(/Reason)(Answer){your answer}(/Answer)\nSelf Consistency.\nGiven the following question and the {n_sample} answers, please select the most consistent response with other answers and the question. {Base Format Constraint} in strictly this format: (Answer) {your final answer} (/Answer).\n#Question: {Base Prompt with CoT}\n# Answer 1:\n{Model Answer 1}\n# Answer 2:\n{Model Answer 2}\n# Answer 3:\n{Model Answer 3}"}, {"title": "Prompt for Critic Generation for Reflex.", "content": "{Base Prompt with CoT Format Constraint}\n# Response:\n{Model Response}\nPlease review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True'."}, {"title": "Prompt for Reflected Answer Generation for Reflex.", "content": "{Base Prompt with CoT Format Constraint}\n# Original Answer:\n{Model Answer}\n# Critic:\n{Model Critic}\nGiven previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better."}, {"title": "Prompt for Final Answer Generation for Reflex.", "content": "{Base Prompt with CoT Format Constraint}\n# Answer 1:\n{Reflected Answer 1}\n# Answer 2:\n{Reflected Answer 2}\n# Answer 3:\n{Reflected Answer 3}\nPlease summarize the previous attempts and feedback and provide a final answer. {Base Format Constraint} in strictly this format: (Answer) {your final answer}(/Answer)."}, {"title": "A.2 DETAILS ABOUT DATASETS", "content": "In this paper, we present a summary of 36 medical-related datasets spanning 6 distinct tasks, as outlined in Table 1. Notably, the inclusion of commercial models, particularly 01, leads to significant costs and response latency. To address this, for some tasks we randomly sampled a subset of test cases, which are detailed below.\nConcept Recognition\n\u2022 BC4Chem (Savery et al., 2020) is a dataset comprising 10,000 PubMed abstracts with 84,355 chemical entity mentions, manually annotated by expert chemistry literature curators. The task is to extract chemical names from the given abstracts. For evaluation, we randomly sample 300 instances from the test set.\n\u2022 BC5Chem and BC5Disease are from BC5CDR (Li et al., 2016), a widely-used resource in biomedical natural language processing, annotated for chemical and disease entities and their relationships. Following MedS-Bench (Wu et al., 2024b), BC5CDR is split into 2 datasets: chemical name extraction and disease name extraction. For evaluation, we randomly sample 300 instances from each task's test set.\n\u2022 Species800 (Pafilis et al., 2013) comprises 800 PubMed abstracts with annotated organism mentions. The task is to extract organism names from the given abstracts. For evaluation, we randomly sample 300 instances from the test set."}, {"title": "Clinical Decision Support", "content": "\u2022 DDXPlus (Fansi Tchango et al., 2022) is a dataset for Automatic Symptom Detection and Automatic Diagnosis systems, featuring synthesized patient data. The task is to make diagnostic decisions based on dialogues. For evaluation, we randomly sample 300 instances.\n\u2022 SEER (Dubey et al., 2023) is a treatment planning dataset based on the Surveillance, Epidemiology, and End Results breast cancer databases. The task is to recommend treatment plans from five types. For evaluation, we randomly sample 300 instances.\n\u2022 MIMIC4ED-Hospitalization, MIMIC4ED-72h ED Revisit, and MIMIC4ED-Critical Triage are datasets from the MIMIC4ED Benchmark (Xie et al., 2022) for predicting clinical outcomes in emergency medicine. For each dataset, we randomly sample 300 instances for evaluation.\n\u2022 MedNLI-Dis. (Discriminative) and MedNLI-Gen. (Generative) are derived from MedNLI (Romanov & Shivade, 2018), a natural language inference dataset for the clinical domain. The dataset involve discriminative and generative entailment based on clinical premises. For each task, we randomly sample 300 instances for evaluation.\n\u2022 EBMS (Moll\u00e1 & Santiago-Martinez, 2011) is a justification verification dataset. We use the entire test set of 304 instances for evaluation.\n\u2022 PUBHEALTH Exp. (Explanation) (Kotonya & Toni, 2020) requires models to provide explanations for specified claims using supporting material from given paragraphs. For evaluation, we randomly sample 300 instances.\n\u2022 PUBHEALTH Ver. (Verification) (Kotonya & Toni, 2020) is a fact verification task where models determine if a claim contradicts evidence in a given paragraph. For evaluation, we randomly sample 300 instances.\n\u2022 Chatdoctor (Li et al., 2023) is based on 100K patient-physician conversations from an online medical consultation website. The task involves engaging in medical consultations based on this data."}, {"title": "Agent", "content": "\u2022 AI Hospital (Fan et al., 2024) is a multi-agent framework simulating medical interactions in Chinese. It includes Patient, Examiner, Chief Physician, and Doctor agents, with 506 cases from diverse departments. The task involves simulating clinical scenarios through dialogue. Evaluation uses Chief Physician's 1-4 scale scoring across five dimensions: symptoms, examinations, diagnostic results, rationales, and treatment plan. 200 cases are sampled for evaluation.\n\u2022 AgentClinic (Schmidgall et al., 2024) is a clinical environment benchmark with 107 patient agents from MedQA and 15 multimodal agents from NEJM challenges. The task is patient diagnosis through dialogue and data collection. Evaluation considers diagnostic accuracy and patient perception metrics in biased scenarios."}, {"title": "Medical Calculation", "content": "\u2022 MedCalc-Bench (Khandekar et al., 2024) evaluates LLMs' medical calculation abilities using 1,047 instances across 55 tasks. It requires computing medical values from patient notes and questions. Evaluation compares LLM outputs to ground truth, with exact matches for rule-based and 5% tolerance for equation-based calculators."}, {"title": "Multilinguality", "content": "\u2022 XMedBench (Wang et al., 2024) is a multilingual medical benchmark in six languages: English, Chinese, Hindi, Spanish, French, and Arabic. It uses multiple-choice questions from various sources, including translated versions for Arabic and Hindi. The task evaluates LLMs' medical knowledge across languages, using accuracy as the primary metric.\n\u2022 AI Hospital (Fan et al., 2024) is a multi-agent framework simulating medical interactions in Chinese. We also include this dataset into the multilinguality aspect because it is in Chinese."}, {"title": "A.3 MODEL-BASED EVALUATION", "content": "As discussed in Section 5, Rethinking evaluation metrics for stronger LLMs, we also explore using techniques such as \"LLM-as-a-judge\" to assess the quality of generated outputs. Table 9 shows that 01 achieves nearly the same score as GPT-4 and outperforms GPT-3.5 (i.e., 3.3% vs. 3.3% vs. 3.0%), which contrasts with the traditional evaluation metrics in Table 3. This indicates that the \"LLM-as-a-judge\" method may be unreliable when applied to advanced models like 01, as GPT-4, being less capable, may provide less accurate evaluations. This limitation is particularly evident in specialized domains such as medicine. The prompt used for \"LLM-as-a-judge\" is shown in Figure A.3."}, {"title": "A.4 DECODING TIME", "content": "We evaluated the model's time cost"}]}