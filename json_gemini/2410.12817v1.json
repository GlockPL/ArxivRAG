{"title": "Interactive Explainable Anomaly Detection for Industrial Settings", "authors": ["Daniel Gramelt", "Timon H\u00f6fer", "Ute Schmid"], "abstract": "Being able to recognise defects in industrial objects is a key element of quality assurance in production lines. Our research focuses on visual anomaly detection in RGB images. Although Convolutional Neural Networks (CNNs) achieve high accuracies in this task, end users in industrial environments receive the model's decisions without additional explanations. Therefore, it is of interest to enrich the model's outputs with further explanations to increase confidence in the model and speed up anomaly detection. In our work, we focus on (1) CNN- based classification models and (2) the further development of a model-agnostic explanation algorithm for black-box classifiers. Additionally, (3) we demonstrate how we can establish an interactive interface that allows users to further correct the model's output. We present our NearCAIPI Interaction Framework, which improves AI through user interaction, and show how this approach increases the system's trustworthiness. We also illustrate how NearCAIPI can integrate human feedback into an interactive process chain. With this work, we plan to provide a new industry dataset for anomaly detection.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection, also known as outlier detection, plays a crucial role in our soci- ety. With the advancements in computer vision technology, numerous researchers are delving into the challenges of 2D anomaly detection, which encompasses detecting ir- regularities in both images [3, 23, 26, 28] and videos [2, 5, 16, 17, 38]. The goal of 2D anomaly detection is to pinpoint odd or unusual occurrences within visual data, such as photos and videos. This technique finds its use in multiple areas, including surveil- lance, security, healthcare imaging, and the inspection of industrial processes. Given the specific nature of the data, traditional statistical methods like isolation forest and k-NN, which are effective for structured data, are not directly applicable to visual data. Hence, methods based on deep learning are widely adopted for identifying anomalies in 2D data.\nHowever, while it's been demonstrated that deep learning models are capable of spotting anomalies in visual content, the transparency behind how these models arrive at their conclusions is often lacking. Explainability refers to the model's ability to make its processes and decisions understandable to humans. In the context of 2D anomaly de- tection, an explainable model is expected to provide clear and reliable justifications for its judgments. Indeed, the issue of explainability is a significant barrier to the broader acceptance of data-driven approaches in the industrial sector [11, 13, 14]. A typical use- case for anomaly detection in the industrial sector would be quality assurance in the production line. To speed up the finding process of the issue it is helpful to give end- users a reasoning for the classification decision, which can be done in the case of images by highlighting the pixels that are responsible for the anomaly detection. This also re- sults in a higher trust in the model itself. Moreover, making AI systems explainable is not just an ethical imperative but also a legal one, particularly in sectors where human safety is at stake. Therefore, developing explainable models for 2D anomaly detection is crucial for supporting a wide range of human endeavours. Common explainability methods for classification networks, such as RISE (Randomized Input Sampling for Explanation) [24] focus on common object classification datasets such as COCO [15] or Pascal VOC [4] consisting of multiple different object categories. Anomaly detection in industrial settings only focuses on two classes: (1) the object is OK, or (2) the object is NOK because it has a scratch or some other anomaly. This introduces new problems, e.g. while blackening the pixel area of a dog in the image would result in not classifying the image as a dog anymore, in our scenario, by blackening the area of the scratches on a welding seam, we would still expect the model to classify the image as an anomaly. This also affects the usability of explainable methods such as RISE that were defined on the standard classification task.\nUltimately, the process should be designed to incorporate the human expert actively within the optimization process [10], enabling them to make interactive adjustments to both the explanations and the decisions. This approach aligns with interactive machine learning (ML) methods, which bear a resemblance to active learning [31]. In active learning, the selection of instances and labels is facilitated through a collaborative ef- fort between the algorithm and the user. Fulfilling these criteria ensures that the user retains comprehensive control over the entirety of the ML process by integrating the human expert into the workflow interactively, embodying the human-in-the-loop con- cept. In this work, one of our objectives is to analyze whether human feedback through explanation corrections enhances the model's performance. Our primary contribution is aimed at enhancing the usability and performance of the CAIPI [36] algorithm, specifi- cally within the field of industrial quality assurance. The CAIPI algorithm allows for the optimization of models by actively incorporating user feedback through the use of gen- erated refutations that challenge predictions and explanations. We modified the CAIPI algorithm by using RISE [24] instead of LIME [27], introducing additional user feed- back in case of correct prediction but wrong explanation [34], and incorporating near hits and misses [8] into the CAIPI algorithm.\nOur contributions are as follows:\n1. We introduce an industrial dataset consisting of welding seams for an anomaly classification task.\n2. We introduce InvRISE, i.e. inverted RISE, a model agnostic explanation method specifically designed for anomaly classification.\n3. We install an extension of CAIPI [36] for human expert feedback by incorporating the idea of near hits and misses [8] which we name NearCAIPI.\nWe will start by reviewing related work in section 2, introduce our method in section 3 followed by the evaluation in section 4."}, {"title": "2 Related Work", "content": "With the success of deep learning models based on convolutional neural networks (CNNs) for tasks such as image classification and object recognition, several classic backbone architectures such as VGG [33], ResNet [6] and ResNeXt [39] have become popular. However, they tend to be considered black box models because of the lack of transparency in the decision process.\nIn many scenarios, machine learning (ML) models need to present decisions in a transparent way, a requirement known as Explanatory AI (XAI). XAI methods can gen- erally be divided into model-agnostic [24, 27] and model-specific approaches [8, 37]. Model-specific methods can produce impressive results but are limited to specific mod- els. In contrast, model-agnostic methods offer broader applicability across different models. For example, LIME [27] (Local Interpretable Model-Agnostic Explanations) provides explanations for individual predictions. This distinction between local (indi- vidual predictions) [18, 24] and global (overall model behaviour) [1, 8] explanations is crucial to understanding and selecting appropriate XAI techniques. RISE [24] (Ran- domized Input Sampling for Explanation), a model agnostic XAI method for estimating pixel saliency, has shown to outperform Lime in the image classification task and hence will be in focus for our work.\nThe idea of active learning [20, 21, 25] is that a machine learning algorithm can reach higher levels of accuracy using fewer labeled training examples if it can select the data from which it learns. This approach is particularly well-suited for machine learn- ing challenges where there is a plentiful supply of unlabeled data, but utilizing all data for training is unfeasible. In some cases, the reduced training set even improves perfor- mance [29]. Incorporating human feedback into the model development loop is another approach to enhance trust. In the work [12], they describe the principles an interactive correction framework should follow. Namely: Be Actionable, which makes the benefit for the user clear; Be Reversible, because feedback can make a system worse than im- proving it; Always Honor User Feedback for nudging the user to keep giving feedback; Lastly, Incremental Changes Matter as showing the user the results and changes of their feedback will result in motivation and a better mental model of the user. Further con- siderations in regards to interactive learning can be found in [9, 22, 35]. CAIPI [36], for instance, allows users to adjust faulty explanations, feeding corrected versions back into the dataset to improve accuracy. Hence, it falls under the category of interactive learn- ing. To mitigate the learning of confound variables, [30] utilize the CAIPI algorithm to overcome the \"clever hans\" behaviour. Furthermore, there are existing extensions to account for ethical correct behaviour ([7]).\nWe are going to see how explainable methods have to be adapted for the task of anomaly classification by modifying the RISE explanations [24]. Afterwards, we will utilize the concept of active learning in terms of a modified CAIPI [36] algorithm that builds upon the idea of near hits and misses [8]."}, {"title": "3 Methodology", "content": "In the following, we will focus on a setup where RGB images were taken of objects that were either okay or had defects, such as scratches. An initial overview of our approach can be found in Fig. 3."}, {"title": "3.1 Dataset", "content": "While we also conduct experiments on the publicly available dataset [32], we created a new dataset for anomaly classification of welding seams.\nWe introduce a dataset of self-made Metal Inert Gas (MIG) welding seams on alu- minium plates. Here, a human expert produced 413 weldings where irregularities were placed on purpose. A welding seam is classified as correct if it is present, with a regular fish-scale, is fully bonded, has no cracks or pores and no interruption.\nThe final dataset consists of 413 images of 413 welding seams, where 139 are cor- rect, 110 are welding plates without a welding seam and 164 are welding seams with irregularities. We classify welding seams as irregular if it has an irregular fish scale, if the welding seam contains black areas, if it has a binding error, if it has cracks or pores, or if it has an unfilled end crater.\nAll images were taken in the same setup. The camera was placed at a fixed distance with an angle of 90\u00b0. The resolution of the images is 1600 \u00d7 1200. Each image is labeled by a human in one of the two categories: Welding regular (OK) and welding irregular (NOK). Due to strict limitations for a welding seam to be classified as OK we consider it as a challenging dataset for anomaly classification."}, {"title": "3.2 Classification", "content": "Anomaly detection can be treated in multiple ways. Segmentation methods would be suited for detection of the anomaly area. The problem with segmentation models would be that they are restricted to finding the welding seams with information limited to the size. Irregularities, such as scratches, irregular fish scales would not be detected through this approach. Hence, we select the most general variant by treating the anomaly detec- tion problem as a simple classification problem with the two classes: OK and NOK. The advantage of this approach is that the models we use can be lightweight which speeds inference and reduces the amount of compute recourses needed and are not limited to a specific kind of anomaly.\nAll ResNet [6] models require an input size of 224 \u00d7 224. Therefore, all images were resized to a resolution of 224 \u00d7 224 before processing."}, {"title": "3.3 Explainability: InvRISE", "content": "A well known method for generating local explanations of black box models is RISE [24]. For the image classification task, RISE applies random masks to the image in order to find the relevant parts for the classification. Pixels within these masks are set to zero. Then the effect on the confidence for the predicted class is measured. If the confidence score goes down rapidly for a mask, then it means that the respective pixels were important for the class prediction. In the case of predicting anomalies on objects, such as scratches, holes or any deformations, this approach could produce issues, e.g. as a blacked out area could be treated as a found anomaly. With this finding, we want to introduce a modified explanation method, which we call inverted RISE (InvRISE):\nThe first step is to sample $k$ (we use $k = 1000$, higher values can also be considered) masks in the following way. We start with a quadratic matrix $L$ of dimension $l\\in \\mathbb{N}$ (we use $l = 8$), where we sample the matrix entries $l_{x,y}$ from a Bernoulli distribution with $p = 0.5$\n$$l_{x,y} \\sim Ber(0.5), \\forall (x, y) \\in \\{1, ..., l\\}^2.$$"}, {"title": "Human Interaction Pipeline", "content": "Inspired by CAIPI [36] we want to introduce an interactive algorithm that brings a human into the loop. After training is complete, the human is provided with an interface where he is given examples of the model's output and is granted the option to correct predictions. The refinements from the human are then incorporated into the dataset to enable an iterative training procedure. Here, we assume to be given further data that is used only for the interactive component, which we call interactive dataset in the following."}, {"title": "Algorithm 1 Near CAIPI", "content": "1: Select instance x with the highest potential information gain from the unlabeled dataset U\n2: pred \u2190 AI(x)\n3: exp \u2190 InvRISE(x)\n4: if pred and exp are correct then\n5:  Cr \u2190 Refutations(exp)\n6: else\n7:  Cr \u2190 Refutations(Correction)\n8:  if pred is wrong then\n9:   Xhit, Xmiss \u2190 HitsAndMisses(U, x)\n10:   predhit \u2190 AI(Xhit)\n11:   predmiss \u2190 AI(Xmiss)\n12:   exphit \u2190 InvRISE(Xhit)\n13:   expmiss \u2190 InvRISE(Xmiss)\n14:   if predhit and exphit are correct then\n15:   Chit \u2190 Refutations(exphit)\n16:   else\n17:   Chit \u2190 Refutations(Correctionhit)\n18:   end if\n19:   if predmiss and expmiss are correct then\n20:    Cmiss \u2190 Refutations(expmiss)\n21:   else\n22:   Cmiss \u2190 Refutations(Correctionmiss)\n23:   end if\n24:   T \u2190 T + [Xhit, Xmiss, Chit, Cmiss]\n25:  end if\n26: end if\n27: T \u2190 T + [x, Cx]\n28: if N repetitions completed then\n29:  Retrain AI with the extended training dataset T\n30: end if"}, {"title": "4 Experiments", "content": "For our experiments we conduct the evaluation on our own dataset consisting of 413 welding seams and a dataset of casting manufacturing product [32] consisting of 1,300 images. In addition, we use a deep metallic surface defect detection dataset [19] as background for the refutations."}, {"title": "4.1 Experiments on Explainability", "content": "Explanations are visualized via highlighted pixels, as seen in Fig. 2. The explainability experiments were performed by training AlexNet, VGG-16, ResNet-18, and ResNeXt- 50 using the welding and casting datasets. We had an expert annotate the irregular parts of the NOK images and compared the explanations with these annotations. In order to compare the generated saliency maps from RISE and InvRISE with a binary annotation, we also convert the saliency maps into a binary mask. To do this, we select the top 10% of the pixels that are highlighted by InvRISE and mark them as important. Other pixels are therefore considered unimportant. With this method, we can compare the binary importance masks with the expert's binary annotated masks by using the Dice coeffi- cient and the Jaccard metric. The Dice coefficient measures the similarity between two sets based on the ratio of the intersection to the total number of elements. In contrast,"}, {"title": "Interactive Explainable Anomaly Detection for Industrial Settings", "content": "The Jaccard metric quantifies the similarity based on the ratio of the intersection to the union. In addition, we use a metric called hit accuracy, which tells us how often the most important pixel in the importance map was actually located in the region that the expert marked as important. Conducted experiments can be found in Tab. 1. Overall, we can see that in terms of accuracy and explanation, the welding seam dataset is more challenging. Older backbone architectures like AlexNet do not show a sufficient per- formance. We can see slight improvements from InvRISE over RISE in terms of the hit Accuracy. The Dice and Jaccard metrics show comparable results for both approaches. Overall, we see a slight improvement when using InvRISE and hence decided to use it as our explanation method for the interactive component."}, {"title": "4.2 Experiments on Human Interaction", "content": "The experiments were performed using ResNet-18 with the pre-trained IMAGENET1K_V2 weights. We used early stopping with a patience of 10 epochs. As optimizer, we used SGD with a learning rate of 0.001 and momentum of 0.9.\nThe human interaction experiments were designed by splitting the entire dataset into four sub-datasets: training, validation, testing and interactive. All results presented were analyzed using the test dataset. The interactive dataset is the dataset from which images are transferred to the training dataset through user interactions. All images in the interactive data set were labelled and annotated in advance for automatic evaluation. It is, therefore, possible to simulate the user's decisions. However, since we cannot per- fectly simulate whether a user accepts a given explanation, we always let the simulated user correct the images by the ground truth, guaranteeing good refutations.\nWe compare four methods to add new data from the interactive dataset to the train- ing dataset. The first method is random addition, where random samples are selected and added to the training dataset. This corresponds to the normal procedure when a user labels new data. In the active learning (AL) approach, the AI predicts the entire unknown dataset and the instance with the lowest confidence is added to the training dataset. In the near active learning (near AL) approach, we add the nearest hit and miss of the wrong predicted image from the unknown dataset to the training dataset. CAIPI generates refutations and adds them to the training data set. We defined the refutations by zooming in or out on the anomaly and generating additional augmented versions of the images, as can be seen in Fig. 1. Although no additional user interaction is required for the refutations, the user must evaluate the explanation and annotate the image if the prediction or explanation is wrong. This interaction is more time-consuming and intensive than active learning and random addition but takes the human into the loop. Finally, our proposed Near CAIPI approach additionally incorporates the idea of near hits and misses and generates refutations for them as well."}, {"title": "Interactive Explainable Anomaly Detection for Industrial Settings", "content": "and thus describes the balance between true positives, false positives, and false nega- tives.\nResults can be seen in Tab. 2 for the Casting dataset, where we used 211 images as base training dataset and had 42 interactions per iteration. Here, we can see that all approaches are able to outperform the random addition baseline. For the first few iter- ations, the active learning approaches show the best results. With increasing iterations, the CAIPI algorithm outperforms the active learning, especially with the integration of near hits and misses. For both approaches, we find that introducing near hits and misses increases the performance.\nFor the Welding dataset, we used 135 initial training images of the welding dataset and having 27 interactions per iteration and the results are displayed in 3. Here, we can see that random addition and active learning are performing worse than the other methods. Again, adding near hits and misses increases the performance for both, active learning and CAIPI. Also CAIPI outperforms active learning with and without near hits and misses."}, {"title": "5 Conclusion", "content": "In this paper, we investigated the task of anomaly classification on industrial datasets consisting of objects that are defective, e.g. disks containing scratches or irregular weld- ing seams. In particular, we explored the required vision modules for this problem, i.e. a CNN backbone for the classification extended with an additional explanation module, InvRISE. On top of that, we explored how humans can be incorporated via an inter- active framework by extending CAIPI with the idea of near hits and misses. Both, the explanation module and the interactive framework increase trustworthiness for the hu- man user and increase capabilities of the model. Experimental results show that the proposed framework is promising for industrial quality assurance."}]}