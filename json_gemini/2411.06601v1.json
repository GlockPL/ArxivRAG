{"title": "OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control", "authors": ["Rohit Bokade", "Xiaoning Jin"], "abstract": "Efficient traffic signal control (TSC) is essential for modern urban mobility, but traditional systems often struggle to adapt to the complex and dynamic nature of city traffic. While Multi-Agent Reinforcement Learning (MARL) offers promising adaptive solutions, online MARL methods require a significant amount of interactions with the environment, which can be expensive and time consuming. Offline MARL addresses these concerns by leveraging historical traffic data for training, but it faces challenges due to the heterogeneity of behavior policies in real-world datasets a mix of different controllers makes learning difficult. We introduce OffLight, a novel offline MARL framework specifically designed to handle heterogeneous behavior policies within TSC datasets. OffLight employs Gaussian Mixture Model Variational Graph Autoencoder (GMM-VGAEs) to model the complex distribution of behavior policies, enabling effective learning from diverse data sources. To enhance coordination between agents, we integrate Graph Attention Networks (GATs), allowing agents to make informed decisions based on aggregated information from neighboring intersections. Furthermore, OffLight incorporates Importance Sampling (IS) to correct for differences between the behavior and target policies and utilizes Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences, thereby improving sample efficiency. Extensive experiments across three real-world urban traffic scenarios-Jinan (12 intersections), Hangzhou (16 intersections), and Manhattan (196 intersections)-demonstrate that OffLight significantly outperforms existing offline RL methods. Notably, OffLight achieves up to a 7.8% reduction in average travel time and an 11.2% decrease in queue length compared to baseline algorithms, particularly in datasets with mixed-quality data. Our ablation studies confirm the effectiveness of OffLight's components in handling data heterogeneity and enhancing learning performance. These results highlight OffLight's ability to accurately model heterogeneous behavior policies, mitigate the impact of suboptimal data, and scale to large urban networks. By addressing the critical challenges of offline MARL in TSC, OffLight offers a practical and impactful solution for improving urban traffic management without the risks associated with online learning.", "sections": [{"title": "1 Introduction", "content": "Efficient traffic signal control (TSC) is important for modern urban mobility, directly affecting congestion levels, travel times, and overall city livability. As urban populations grow and vehicular usage intensifies, traditional traffic management systems struggle to adapt to dynamic and complex traffic flows. Recent advancements in Multi-Agent Reinforcement Learning (MARL) offer promising solutions by enabling decentralized, adaptive, and intelligent control of traffic signals [1,2].\nWhile online MARL requires agents to interact with the environment in real-time\u2014which can be impractical and unsafe in real-world traffic systems-offline MARL leverages historically collected traffic data to train agents without live experimentation [3]. This approach offers several advantages:\n\u2022 Extensive Data Utilization: Utilizes pre-collected traffic data, allowing agents to learn from diverse scenarios and rare events difficult to replicate online [4].\n\u2022 Safety and Risk Mitigation: Eliminates risks associated with deploying untested policies in live traffic, ensuring only well-trained agents are implemented [5].\n\u2022 Cost Efficiency and Scalability: Reduces the need for expensive simulations or field tests, facilitating deployment across various scales and environments [3].\n\u2022 Accelerated Policy Development: Enables rapid iteration and refinement of control policies, expediting research and development [6]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Multi-Agent Reinforcement Learning", "content": "Multi-Agent Reinforcement Learning (MARL) involves multiple agents interacting within a shared environment. Each agent learns to maximize the cumulative reward while adapting to the behaviors of other agents. MARL introduces additional challenges such as non-stationarity (due to concurrent learning), the need for coordination, and partial observability, all of which make policy learning significantly harder than in single-agent settings [14, 15]."}, {"title": "2.1.1 Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs)", "content": "Multi-agent problems are commonly modeled as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) [16]. A Dec-POMDP is formally defined by a tuple $(N, S, \\{A^i\\}_{i=1}^N, \\{O^i\\}_{i=1}^N, P, R, H, \\gamma)$, where:\n\u2022 N is the number of agents.\n\u2022 S is the set of environment states.\n\u2022 $A^i$ is the action space of agent i.\n\u2022 $O^i$ is the observation space of agent i.\n\u2022 $P : S \\times A^1 \\times ... \\times A^N \\times S \\rightarrow [0,1]$ is the state transition probability function.\n\u2022 $R: S \\times A^1 x ... x A^N \\rightarrow R$ is the shared reward function.\n\u2022 H is the finite horizon.\n\u2022 $\\gamma \\in [0, 1)$ is the discount factor.\nEach agent i selects actions based on its action-observation history $h^i_t = \\{o^i_0, a^i_0, ..., o^i_{t-1}, a^i_{t-1}, o^i_t\\}$, where $o^i_t \\in O^i$ is the observation at time t, and $a^i_t \\in A^i$ is the action."}, {"title": "2.2 Application to Traffic Signal Control (TSC)", "content": "In the context of Traffic Signal Control (TSC), each traffic signal controller can be modeled as an agent within the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework [16]. Each agent operates independently and makes decisions based on local observations, such as vehicle counts, vehicle speeds, queue lengths, and current signal phases.\nObservation Space: Each traffic signal agent has a limited range of observation. This range reflects the realistic coverage of common sensors such as inductive loop detectors or cameras. For each incoming lane, the agent observes:\n\u2022 The number of vehicles $\\{n_l^i\\}_{l=1}^{L_i}$, where $L_i$ is the number of lanes for traffic signal i.\n\u2022 The average speed of vehicles $\\{v_l^i\\}_{l=1}^{L_i}$, normalized by the speed limit.\n\u2022 The number of halted vehicles, representing the queue length $\\{q_l^i\\}_{l=1}^{L_i}$.\n\u2022 The current phase ID, indicating the traffic signal's current phase.\nThese local observations provide the sensory input required for each agent to make real-time decisions about the traffic signal phases.\nAction Space: Each traffic signal agent controls the signal phases at its intersection. The available actions correspond to selecting one of the predefined green phases from the list of possible phases for that intersection. An agent may select a green phase and it must then transition to the yellow phase, which is enforced by the environment. The action selection interval is fixed at 5 simulation seconds.\nReward Function: A commonly used reward metric is queue length. Minimizing the queue length across all lanes in the network directly reflects the reduction of vehicle stoppage. The global reward at each time step $r_t$ is defined as:\n$r_t = -\\sum_{l \\in L} q_l(t)$,\nwhere $q_l(t)$ represents the queue length on lane l at time t, and L is the set of all lanes in the network. This reward"}, {"title": "2.3 Offline Multi-Agent Reinforcement Learning", "content": "Offline Multi-Agent Reinforcement Learning involves learning effective policies from a fixed dataset of interactions without further environmental exploration during training [3, 4]. This setting is crucial in scenarios where data collection is expensive, risky, or impractical, such as traffic signal control. Offline MARL introduces unique challenges:\n\u2022 Distributional Shift: The static dataset may not adequately represent the state-action distribution encountered when executing the learned policies, leading to overestimation of out-of-distribution actions and degraded performance [22].\n\u2022 Extrapolation Error: Without any interaction with the environment, agents must generalize based solely on the dataset provided. This limitation risks extrapolation errors, where agents might attempt actions unsupported by the data, leading to poor or unsafe decisions when deployed.\n\u2022 Limited Coverage of Optimal Strategies: Offline datasets may lack the comprehensive coverage needed for optimal joint policies. This is particularly challenging in MARL, as optimal coordination patterns often require data from multiple interacting policies, which may be missing in historical data.\nRecent offline RL algorithms, such as Conservative Q-Learning (CQL) [12] and TD3+BC [23], have been developed to address these challenges by regularizing learned policies and penalizing overestimation of Q-values for unseen actions. While originally designed for single-agent settings, these methods can be extended to multi-agent systems using frameworks like Centralized Training with Decentralized Execution (CTDE) [24]."}, {"title": "2.3.1 Conservative Q-Learning (CQL)", "content": "Conservative Q-Learning (CQL) [12] is an offline RL algorithm designed to address the overestimation of out-of-distribution (OOD) actions by learning conservative Q-value estimates. CQL modifies the standard Bellman error minimization by adding a regularization term that penalizes Q-values for actions not present in the dataset. The CQL objective is:\n$\\min_Q (E_{s \\sim D, a \\sim \\pi(a|s)} [Q(s, a)] - E_{(s,a) \\sim D}[Q(s,a)]) + L_{Bellman}(Q)$,\nwhere D is the offline dataset, $\\pi(a|s)$ is a policy (e.g., the uniform policy), $\\alpha$ is a regularization coefficient, and $L_{Bellman}(Q)$ is the standard Bellman error. In multi-agent settings, the joint action space grows exponentially with the number of agents, making it infeasible to represent the full joint Q-function. To address this, CQL can be extended using factorization methods or the CTDE paradigm [24]. CTDE allows agents to be trained centrally with access to global information, while at execution time, they act based on local observations, enabling scalability and coordination among agents even with limited data."}, {"title": "2.3.2 TD3+BC", "content": "TD3+BC [23] enhances the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm [25] by integrating a behavior cloning (BC) term into the policy update. This addition encourages the learned policy to stay close to the behavior policy present in the offline dataset, mitigating issues arising from distributional shift. The modified policy objective in TD3+BC is:\n$\\min_{\\pi_{\\theta}} -E_{s \\sim D}[Q_{\\phi}(s, \\pi_{\\theta}(s))] + \\lambda E_{(s,a) \\sim D}[||\\pi_{\\theta}(s) \u2013 a||^2]$,\nwhere $\\pi_{\\theta}$ is the policy network parameterized by $\\theta$, $Q_{\\phi}$ is the critic network parameterized by $\\phi$, and $\\lambda$ balances the importance of the behavior cloning term. In multi-agent systems, TD3+BC can be adapted by allowing each agent to learn its own policy while considering the interactions with other agents. Using the CTDE framework [26], agents are trained with access to additional global information"}, {"title": "2.4 Challenge of Heterogeneous Behavior Policies in Offline MARL", "content": "A unique challenge in offline MARL, particularly in domains like Traffic Signal Control (TSC), is managing heterogeneous behavior policies present in the offline dataset. In TSC, data often come from various sources, times, or operational contexts, leading to a mix of behavior policies that agents must learn from. This heterogeneity adds complexity to offline MARL in the following ways:\n\u2022 Increased Risk of Distributional Shift: When policies in the offline data vary significantly, the likelihood of a distributional mismatch between the training data and the learned policy increases. The agents are more prone to encountering out-of-distribution actions, resulting in unreliable Q-value estimates and degraded policy performance, as the diversity in behavior data can make distributional correction methods less effective [22].\n\u2022 Difficulty in Behavior Policy Estimation: For methods relying on importance sampling or policy constraints, accurately estimating a single behavior policy becomes challenging when the data reflects multiple, potentially conflicting policies. Modeling this mixture accurately is essential to effective learning but is significantly more complex in offline MARL with heterogeneous datasets [13]."}, {"title": "2.5 Motivation for the OffLight Framework", "content": "To tackle the specific challenge of heterogeneous behavior policies in offline MARL, we propose the OffLight framework. OffLight is designed to:\n\u2022 Model Heterogeneous Behavior Policies: Utilize advanced modeling techniques, such as Variational Graph Auto-Encoders (GMM-VGAEs), to accurately estimate and represent the diverse behavior policies present in the offline dataset.\n\u2022 Mitigate Distributional Shift: Employ importance sampling with the estimated behavior policy to correct for discrepancies between the behavior policies and the target policy, reducing overestimation and improving policy evaluation.\n\u2022 Improve Sample Efficiency: In addition to IS and GMM-VGAE, OffLight integrates Return-Based Prioritized Sampling (RBPS) based on episodic returns. RBPS prioritizes high-reward episodes, ensuring that the learning process focuses on the most informative and successful experiences. This prioritization accelerates convergence and enhances the robustness of the learned policies.\nBy focusing on accurately modeling and leveraging the heterogeneous behavior policies in the offline data, OffLight addresses a critical challenge in offline MARL for TSC."}, {"title": "3 OffLight Framework", "content": "OffLight is designed to tackle the challenges of offline multi-agent reinforcement learning (MARL) in the domain of traffic signal control (TSC). It combines a self-supervised learning approach with importance sampling (IS) to account for heterogeneous policies and improve learning from static, pre-collected datasets. IS and RBPS weights are collected before the offline RL training and stored. During the offline RL training phase, these weights are used to improve learning. Below, we outline the architecture and key components of OffLight, highlighting how it handles mixed-policy data to enhance traffic management performance."}, {"title": "3.1 Dataset Structure", "content": "We consider a dataset D collected from a network of N intersections over M episodic trajectories. Each trajectory $T_k$ comprises a sequence of interactions between the agents (traffic signals) and the environment. Formally, the dataset is represented as:\n$D = \\{T_k\\}_{k=1}^M$, where $T_k = \\{(O_t, A_t, O'_t, R_t)\\}_{t=t_0}^{t_0+T_k}$,\nwhere:"}, {"title": "3.2 Self-Supervised Learning of Behavior Policies", "content": "To effectively model diverse behavior policies in offline traffic signal control (TSC) data, OffLight uses a Gaussian Mixture Model-based Variational Graph Autoencoder (GMM-VGAE), designed to capture both spatial and temporal dependencies while addressing policy heterogeneity across intersections and time.\n\u2022 GMM for Policy Heterogeneity: The GMM structure in the VGAE models the latent space as a mixture of Gaussian distributions, allowing OffLight to represent multiple underlying policies. This disentanglement of varied traffic control behaviors within the latent space enables OffLight to more accurately capture and leverage diverse policies from real-world datasets.\n\u2022 Graph Attention Networks (GATs) and LSTM for Spatial-Temporal Dynamics: GATs capture spatial dependencies by allowing each agent to attend to relevant neighboring intersections. This attention mechanism is structured to combine local information effectively, creating a global latent representation of behavior policies across the network. Additionally, GATs enable efficient parameter sharing across agents, improving scalability for TSC in a manner similar to MARL by processing local data in a structured and computationally manageable way. LSTM layers capture the temporal evolution of traffic patterns by processing sequences of observations over time, preserving the dynamic dependencies needed for effective policy modeling.\nTogether, these components enable GMM-VGAE to accurately model both local and global interactions in the traffic network, ensuring OffLight's robustness to the diverse and dynamic nature of traffic conditions. This architecture makes OffLight highly scalable and effective in learning from heterogeneous TSC datasets.\nBy constructing a latent space with Gaussian mixtures and capturing spatial-temporal dependencies through"}, {"title": "3.3 Importance sampling Integration", "content": "To address the challenge of distributional shift between the behavior policy and the target policy, OffLight integrates importance sampling (IS). This mechanism adjusts the influence of each transition based on its alignment with the target policy, ensuring that the learning algorithm emphasizes relevant and high-quality data.\n$w_{IS,t}^k = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\pi_{\\theta}(a_t^n | h_t^n)}{\\pi_\\phi(a_t^n | h_t, z_t)}$,\nwhere:"}, {"title": "3.4 Return-Based Prioritized Sampling", "content": "To enhance sample efficiency and accelerate the learning process, OffLight employs Return-Based Prioritized Sampling (RBPS), similar to [27]. This strategy prioritizes episodes based on their cumulative rewards, ensuring that the learning algorithm focuses on more successful traffic control experiences.\n$w_{RBPS}^k = C \\frac{G_k - G_{min}}{G_{max} - G_{min}} + P_{base}$ (6)\nwhere:\n\u2022 $w_{RBPS}^k$ is the Return-Based Prioritized Sampling weight for episode k.\n\u2022 $G_k = \\sum_{t=t_0}^{t_0+T_k} r_t$ is the total reward for episode k.\n\u2022 $G_{min}$ and $G_{max}$ are the minimum and maximum total rewards across all episodes in the dataset, respectively.\n\u2022 $P_{base}$ is a small positive constant added to ensure that all episodes have a non-zero probability of being sampled.\n\u2022 C is a normalization constant to maintain numerical stability or ensure the weights sum to a desired value.\nRBPS ensures that episodes with higher returns, indicative of more effective traffic management, are sampled more frequently. This focus accelerates learning by emphasizing experiences that contribute significantly to improved traffic flow and reduced congestion. This especially proves beneficial when the distribution of the data is skewed or multimodal as can be seen in Figure 3."}, {"title": "3.5 Combining importance sampling and Return-Based Prioritized Sampling", "content": "OffLight synergistically combines importance sampling (IS) and Return-Based Prioritized Sampling (RBPS) to address both distributional shift and sample efficiency in offline MARL for TSC. This combined weighting scheme ensures that the learning algorithm emphasizes transitions that are both aligned with the target policy and derived from high-reward episodes, thereby enhancing the effectiveness and robustness of policy learning.\n$w_{combined,t}^k = w_{IS,t}^k \\times w_{RBPS}^k$ (7)\nimportance sampling adjusts the weights based on policy alignment, while RBPS prioritizes transitions from successful episodes, ensuring that the model learns from both relevant and high-quality data. 3\nNormalization: After combining, the weights are normalized within each minibatch to maintain their probabilistic interpretation and ensure numerical stability.\n$\\tilde{w}_{combined,t}^k = \\frac{w_{combined,t}^k}{\\sum_i w_{combined,t}^k}$ (8)\nClipping: To prevent extremely large weights from destabilizing the training process, the combined weights are clamped to a maximum value.\n$w_{combined,t} = clamp (\\tilde{w}_{combined,t}, clip\\_max)$ (9)\nIntegration into Loss Functions: The normalized and clamped combined weights are applied to the Temporal Difference (TD) error and the Conservative Q-Learning"}, {"title": "3.6 Integration with Offline RL Algorithms", "content": "OffLight can easily be integrated with existing offline RL algorithms. CQL and TD3+BC are chosen as baseline offline RL algorithms. The combined weighting scheme comprising importance sampling (IS) weights and Return-Based Prioritized Sampling (RBPS) weights is incorporated into both the loss functions and the sampling mechanisms of these algorithms to ensure effective bias correction and sample prioritization.\nLoss Function Adjustment: The combined weights $W_{combined,t}$ are used to scale the loss components, ensuring that transitions from high-reward episodes aligned with the target policy have a greater impact on policy and value updates. This scaling is formalized in the loss functions of CQL and TD3+BC as follows:\n$L = E_{(O,A,O',R) \\sim D} [W_{combined,t} \\cdot l(O, A, O', R)]$, (10)\nwhere $l(O, A, O', R)$ represents the specific loss term associated with the chosen RL algorithm (e.g., Bellman error for CQL or policy loss for TD3+BC).\nSampling Mechanism: Episodes are sampled based on their total rewards, prioritizing those with higher cumulative returns. This is implemented by adjusting the sampling probabilities $p_i$ according to $W_{RBPS}$, ensuring that more successful episodes are sampled more frequently. This prioritization enhances the efficiency of the learning process by focusing on the most informative and effective experiences.\nBy integrating these techniques, OffLight provides a robust framework for learning effective traffic control poli-"}, {"title": "4 Experimental Setup", "content": "We conduct experiments to evaluate OffLight's performance across three real-world urban traffic scenarios: Jinan (12 intersections), Hangzhou (16 intersections), and Manhattan (196 intersections). These scenarios vary in complexity and traffic demand, allowing for a comprehensive evaluation of OffLight's scalability and robustness. The experiments are designed to address the following key questions:\n1. How effectively does OffLight model heterogeneous behavior policies in offline datasets?\n2. What is the impact of incorporating importance sampling on learning from mixed-policy datasets?\n3. How does the proportion of suboptimal policy data affect overall performance?"}, {"title": "4.1 Traffic Scenarios", "content": ""}, {"title": "5 Results and Discussion", "content": "This section presents the evaluation of OffLight across various traffic scenarios. We measure performance in terms of Average Travel Time (ATT) and Queue Length (QL), highlighting the effectiveness of OffLight in learning from mixed-policy datasets."}, {"title": "5.1 Performance on Mixed-Quality Data", "content": "We assess OffLight's performance on datasets with varying proportions of expert and random policy data. The results are shown for three traffic networks: Jinan, Hangzhou, and Manhattan, under low, medium, and high traffic demand."}, {"title": "5.1.1 Average Travel Time (ATT)", "content": "Figure 5 compares the performance of OffLight with baseline offline RL methods across three traffic scenarios-Jinan, Hangzhou, and Manhattan at low, medium,"}, {"title": "5.1.2 Queue Length (QL)", "content": ""}, {"title": "5.1.3 Behavior Representation", "content": ""}, {"title": "5.2 Ablation Studies", "content": ""}, {"title": "5.2.1 Performance Comparison with Different Levels of Mixing", "content": ""}, {"title": "5.2.2 Effectiveness of Improved Sampling Strategies", "content": ""}, {"title": "5.3 Discussion", "content": "In this study, we proposed OffLight, an offline multi-agent reinforcement learning (MARL) framework tailored for traffic signal control (TSC) using heterogeneous, pre-collected datasets. The experiments illustrate that OffLight not only outperforms existing baseline methods but also remains robust across different traffic scenarios and dataset compositions. Key insights from the results include:\nPerformance with Mixed-Quality Data: OffLight consistently demonstrates superior performance in scenarios involving heterogeneous behavior policies. This is critical for real-world deployments where datasets often comprise both expert and suboptimal policy data. The Graph-GMVAE was crucial in capturing the latent behavior distributions, enabling more accurate policy learning.\nEffectiveness of Sampling Strategies: Both importance sampling (IS) and return-based prioritized sampling (RBPS) played pivotal roles in enhancing learning efficiency. IS proved more impactful overall, particularly in high traffic demand scenarios, by correcting for distributional shifts and focusing on transitions more aligned with the target policy. In contrast, RBPS provided more consistent, albeit smaller, gains by prioritizing high-reward episodes. Together, these strategies enabled OffLight to maximize performance, especially in complex environments like Manhattan.\nScalability and Generalization: OffLight scaled well across different network sizes and traffic conditions, from small networks like Jinan to highly complex ones like Manhattan. This demonstrates the framework's generalizability, making it suitable for deployment in diverse urban settings."}, {"title": "5.3.1 Limitations:", "content": "Training Overhead for Graph-GMVAE: Training the Graph-GMVAE, which models the behavior policies, can be computationally expensive, especially in large-scale networks with numerous intersections. This adds to the overall training time and resource requirements, which could limit its real-time deployment capabilities in certain urban environments.\nDependence on Dataset Quality: OffLight's performance is highly dependent on the quality and diversity of the offline dataset. If the dataset lacks sufficient expert data or is overly biased toward suboptimal policies, OffLight's learning process may be negatively impacted."}, {"title": "6 Conclusion", "content": "OffLight demonstrates the potential to significantly enhance traffic signal control (TSC) in urban networks through offline multi-agent reinforcement learning (MARL). By addressing the key challenges of heterogeneous behavior policies and distributional shifts, OffLight offers a robust solution capable of learning effective policies from mixed-quality datasets. The framework's incorporation of Graph-GMVAE for behavior policy modeling, alongside importance sampling (IS) and return-based prioritized sampling (RBPS), leads to substantial improvements in traffic efficiency. These advancements reflect the ability of OffLight to scale effectively across different network sizes and traffic conditions, making it a practical tool for offline reinforcement learning for real-world traffic management."}, {"title": "A OffLight Implementation Details", "content": ""}, {"title": "A.1 Rationale for Averaging Importance Sampling Weights", "content": "In multi-agent settings, computing the Importance Sampling (IS) weight as a product across agents can lead to exponential scaling, causing numerical instability and disproportionately amplifying certain transitions. Specifically, the original formulation is given by:\n$w_{IS,t} = \\frac{\\pi_{\\theta}(A_t | S_t)}{\\pi_\\phi(A_t | S_t)} = \\frac{\\prod_{i=1}^N \\pi_{\\theta}(a_t^i | h_t^i)}{\\prod_{i=1}^N \\pi_\\phi(a_t^i | h_t^i)} \\approx \\frac{\\sum_{i=1}^N \\pi_{\\theta}(a_t^i | h_t^i)}{\\sum_{i=1}^N \\pi_\\phi(a_t^i | h_t, z_t)}$"}, {"title": "A.2 Pseudocode for Training Behavior Policies", "content": "Algorithm 1 summarizes the training procedure of OffLight, highlighting the key steps where Importance Sampling and Return-Based Prioritized Sampling are applied."}, {"title": "A.3 GMM-VGAE Architecture", "content": ""}, {"title": "A.3.1 Encoder: Capturing Spatial and Temporal Dependencies", "content": "The encoder processes trajectory data by embedding local observations and actions through linear layers. GATs"}, {"title": "A.3.2 Latent Space: Modeling Behavior Policies", "content": "The latent space is structured to represent the distribution of underlying behavior policies using a Gaussian Mixture Model (GMM). By modeling the latent variables z as a mixture of K Gaussian components, OffLight captures the multimodal nature of behavior policies present in the heterogeneous dataset. This mixture model facilitates the disentanglement of different policy behaviors, enabling accurate estimation and differentiation of effective and suboptimal policies within the data.\nThe GMM is defined as:\n$p(z) = \\sum_{k=1}^K \\pi_k N(z | \\mu_k, \\Sigma_k)$,\n$q(z | T_i) = \\sum_{k=1}^K \\gamma_{ik} N(z | \\mu_{ik}, \\Sigma_{ik})$"}, {"title": "A.3.3 Decoder: Reconstructing Policy Distributions", "content": "The decoder reconstructs the action distributions from the latent variables by concatenating the sampled latent vectors with the encoded observations and passing them through additional GAT layers. This process effectively decodes the policy distributions corresponding to different latent behaviors. The reconstruction ensures that the latent representations accurately capture the decision-making processes of the behavior policies, enabling reliable policy reconstruction and subsequent importance sampling.\nThe decoding process is formalized as:\n$A_t = Decoder(z_t, O, A_{t-\u221e}, A)$,\n$p(A_t | z_t, O_t) = \\prod_{i=1}^N p(a_t^i | z, O_t)$"}, {"title": "A.4 Offline RL Framework Details", "content": ""}, {"title": "A.5 Hyperparameters", "content": "We used PyTSC library [28] and the CityFlow simulator [29] to generate traffic scenarios for our experiments, while the OffLight algorithm was implemented using the OffPyMARL framework [30]. Each offline reinforcement"}, {"title": "B Theoretical Analysis", "content": "We analyze the properties of the importance sampling integration. This analysis underscores the robustness and effectiveness of OffLight in offline multi-agent reinforcement learning (MARL) settings, particularly for traffic signal control (TSC) applications."}, {"title": "B.1 Importance Sampling Weight Consistency", "content": "In OffLight, importance sampling is integrated to correct for the distributional shift between the behavior policy $\\pi_i$ and the target policy $\\pi_\\theta$. We aim to show that the importance sampling weights computed using the estimated behavior policy $\\pi_b$ provide consistent and unbiased estimates of expectations under the target policy. For any function f(\u03c4) of a trajectory \u03c4, the expectation under $\\pi_\\theta$ can be estimated using samples from $\\pi_i$ as:\n$E_{\\tau \\sim \\pi_{\\theta}} [f(\\tau)] = E_{\\tau \\sim \\pi_i} [w(\\tau)f(\\tau)]$,\nwhere the importance weight w(\u03c4) is given by:\n$w(\\tau) = \\prod_{t=0}^{T_N} \\prod_{n=1}^N \\frac{\\pi_{\\theta}(a | s_i)}{\\pi_b(a | s_i)}$"}, {"title": "B.2 Variance Reduction via Accurate Behavior Policy Estimation", "content": "While importance sampling provides unbiased estimates, the variance of the estimator can be high, particularly when the target and behavior policies differ significantly. By accurately estimating $\\pi_b$ using the GMM-VGAE, OffLight reduces the variance of the importance weights.\nVariance Analysis: The variance of the importance sampling estimator is given by:\n$Var_{\\pi_i} [w(\\tau) f(\\tau)] = E_{\\pi_i} [w^2(\\tau) f^2(\\tau)] - (E_{\\pi_i} [f(\\tau)])^2$."}, {"title": "B.3 Computational Complexity and Scalability", "content": "OffLight leverages Graph Neural Networks (GNNs) to capture spatial dependencies among agents. The computational complexity per forward pass is:\n$O(L \\cdot (N + E) \\cdot d)$,\nwhere: L is the number of GNN layers. N is the number of nodes (agents). E is the number of edges (inter-agent connections). - d is the dimensionality of the embeddings. In sparse traffic networks, E = O(N), so the complexity scales linearly with N. This demonstrates that OffLight is scalable to large networks, such as the Manhattan scenario with 196 intersections."}]}