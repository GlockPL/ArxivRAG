{"title": "Automated Extraction and Creation of FBS Design Reasoning Knowledge Graphs from Structured Data in Product Catalogues Lacking Contextual Information", "authors": ["Vijayalaxmi Sahadevan", "Sushil Mario", "Yash Jaiswal", "Divyanshu Bajpai", "Vishal Singh", "Hiralal Agarwal", "Suhas Suresh", "Manjunath Maigur"], "abstract": "Ontology-based knowledge graphs (KG) are desirable for effective knowledge management and reuse in various decision-making scenarios, including design. Creating and populating extensive KG based on specific ontological models can be highly labour and time-intensive unless automated processes are developed for knowledge extraction and graph creation. Most research and development on automated extraction and creation of KG is based on extensive unstructured data sets that provide contextual information. However, some of the most useful information about the products and services of a company has traditionally been recorded as structured data. Such structured data sets rarely follow a standard ontology, do not capture explicit mapping of relationships between the entities, and provide no contextual information. Therefore, this research reports a method and digital workflow developed to address this gap. The developed method and workflow employ rule-based techniques to extract and create a Function-Behaviour-Structure (FBS) ontology-based KG from legacy structured data, especially specification sheets and product catalogues. The solution approach consists of two main components: a process for deriving context and context-based classification rules for FBS ontology concepts and a workflow for populating and retrieving the FBS ontology-based KG. KG and Natural Language Processing (NLP) are used to automate knowledge extraction, representation, and retrieval. The workflow's effectiveness is demonstrated via pilot implementation in an industrial context. Insights", "sections": [{"title": "Introduction", "content": "There is growing interest among enterprises and their knowledge engineers in updating and migrating their legacy knowledge base to knowledge-graph-based systems. Further, mapping to scalable, high-level ontology, such as the Function-Behaviour-Structure (FBS) ontology (Gero 1990), can support functional reasoning. Most research and development on automated knowledge extraction focuses on using extensive unstructured data sets, as they provide valuable contextual information. In an ideal scenario, enterprises and knowledge engineers would leverage these large unstructured data sets to create knowledge graphs (KG) that represent and organize their enterprise's product information. However, in practice, some of the most useful information is available in a structured data format, such as specification sheets and product catalogues. Such structured data sets do not follow a standard ontology, do not provide an explicit mapping of relationships between the entities, and provide no contextual information. Therefore, there is a need for techniques and methods for automated extraction and creation of ontology-based KG from structured legacy data that lack contextual information and ontological congruence. To address this gap, the following key challenges must be resolved:\n\u2022\n\u2022\nExtracting knowledge in terms of the target ontology-specific concepts and terminologies, which differ from the concepts and terminologies used in the source legacy structured data lacking contextual information. That is, extracting knowledge when there is no ontological congruence between the target KG and the source data set. For instance, the target KG in this research, the FBS ontology, requires descriptions and mappings in terms of the Function (F), Behaviour (B) and Structure (S) concepts and terminologies. Whereas, the source structured data set, such as the product categories, is described in terms of parts, specifications, and terminologies that are appropriate and known to vendors and customers but have no explicit mapping to the FBS concepts.\nThe challenge in extracting targeted, ontology-specific KG from legacy structured data is compounded because legacy structured data does not explicitly map relationships between the"}, {"title": "Background", "content": "The literature emphasizes the necessity of advanced methods for intelligent retrieval of design knowledge in the context of machine part design (Ning et al. 2024). Automated knowledge extraction and reuse have been used in different design and manufacturing activities. For instance, Helgoson and Kalhori (2012) employed CAD and CAM documentation for knowledge extraction. Shen et al. (2020)"}, {"title": "Research steps for developing the FBS classification guidelines", "content": "The F, B, and S classification guidelines were developed following the steps summarised in Figure 1.\nTo develop a set of guidelines for classifying an artefact into F, B, and S, multiple sessions were conducted with the problem domain experts to understand the components and to create a shared understanding of the FBS ontology. Multiple team brainstorming sessions and focus group discussions were conducted to discuss and refine the definitions of F, B, and S using examples. As the definitions started to be stable, the focus of the brainstorming and discussions shifted to determining the rules that could be used to extract the variables from the dataset.\nFollowing the engagement with the problem domain experts, experiments with three experts in FBS ontology were conducted to review and assess the ease of applicability and accuracy of the developed rules. Each FBS expert was provided with all the extraction rules, examples of variables, and the datasheet for one of the components- contacts. In addition, an FBS mapping table and a Feedback form were provided. The rule sets were iteratively refined based on the feedback obtained from each expert.\nThe accuracy percentage for the first, second, and third respondents were 54%, 93%, and 87%,\nrespectively.\nFollowing the sessions with the FBS ontology experts, a large-language model-based agent, the ChatGPT, was further used to test the rules. ChatGPT was passed a brief context about the project, followed by the rules. The agent was then provided multiple key-value pairs from the data sheet and was tasked to classify them based on the rules. The results showed about 78% accuracy of classification. The rules were further refined and retested with ChatGPT based on the above. The output exhibited 100% accuracy."}, {"title": "Rule-based FBS-KG Framework", "content": "The developed Rule-based FBS-KG Framework is shown in Figure 2. The core of the framework is the workflow for population and retrieval of the FBS-based KG. The inputs to the architecture are the specification sheets of the components, the FBS-based KG structure, and the FBS classification rules."}, {"title": "Workflow Development", "content": "The system architecture for the workflow is shown in Figure 4. The developed workflow aims to achieve two purposes:\n\u2022\n\u2022\nTo extract information from specification sheets and populate the FBS based KG.\nTo retrieve configurations based on user-provided queries."}, {"title": "Knowledge Graph Retrieval System Design and Development", "content": "The process of creation and utilisation of a KG typically follows a sequence of three steps: extraction of usable data and its relationships, representation of information into knowledge, and derivation of actionable insights. The system for retrieving the identifying information of the target component models in the current work comprises two constituent pipelines:\n\u2022\n\u2022\nKG population pipeline\nKG querying pipeline\nThe first pipeline is concerned with populating the KG with all models of every component in the problem domain. The second handles the generation and execution of cypher queries that query the KG hosted on the Neo4j instance to retrieve the identifying information of all target models. Figure 5 shows the mapping between the sequence of steps and the pipelines associated with the KG."}, {"title": "Datasheet (As PDF)", "content": "The input to the pipeline is a set of product data sheets, represented in PDF form, that detail the specifications of a particular component model. Each datasheet consists of tables under several headings, each denoting a type of specification relating to the material properties, technical characteristics, identification, conformity to standards, etc. The tables are composed of two columns, indicating the name and value of each property that falls under the respective heading.\nEach row of the table represents a single property. Notably, each property may have several associated values, such as a list of specifications under the conformity to standards section. Every value occupies its own sub-row within each property row, an important aspect for the next step of the pipeline."}, {"title": "PDF Parser", "content": "The input to this step is a product datasheet in PDF form consisting of model specifications. A Camelot-py library is used to parse the PDF and render its contents in a pandas DataFrame called specs_df with two columns - property name and property value. The library uses image processing techniques to read each line of text in the PDF, splitting them into columns wherever the outline of a boundary is detected.\nThere are cases in which a property may have multiple values associated with it, each occupying its sub-row, which in most cases had no associated property name. As a result, these values were each considered part of separate, non-existent properties during parsing. To resolve this issue, an algorithm was developed to associate such 'orphan' property values with their property names as part of the same property.\nFollowing this, irrelevant text read by the parser, including pagination footers, is detected using a regular expression and removed from the dictionary result. Finally, a list of key-value pairs is constructed from the result, which is then converted back into a pandas DataFrame and assigned to specs_df. The output of this step is the pandas DataFrame specs_df which contains all property names and associated property values of the model."}, {"title": "FBS Automator \u2013 Operation", "content": "The pandas DataFrame specs_df is input for this processing step. Each property in specs_df is classified into one of four categories: F, B, S, or Unknown."}, {"title": "Graph Population Engine", "content": "This is the primary step in the KG population pipeline. The logic used in this step is shown in the flowchart in Figure 9. The corresponding algorithm is shown in Figure 10.\nThe logic outlines the mechanism by which:\n\u2022\n\u2022\nA new graph is created from scratch if it does not already exist, and\nAn existing graph is updated with information about a new model of the same component if the graph for that component already exists, taking care not to duplicate information already present in the property name and value nodes of the previous model(s)."}, {"title": "Updated KG", "content": "The final output of the KG population pipeline is a new or updated KG, hosted on a Neo4j AuraDB instance in the cloud."}, {"title": "Knowledge Graph Retrieval Pipeline", "content": "Figure shows steps by a natural language query that transforms it into a Cypher query format that can be run on the Neo4j instance to retrieve the part number(s) of the target product(s) from the KG. The details of each of the steps are described below.\nNatural Language Query: The input to the pipeline is a natural language query in plain English, following a standard format. The example query used to showcase the operation of the system is as follows:- \"Give me a crimp contact that conforms to these specifications :-\nMaterial (contacts) equal to Copper alloy, Surface (contacts) equal to Silver plated, Conductor cross-section-2 between 22 and 26, Gender = Female, Manufacturing process = Turned contacts, Operating Current lesser than or equal to 10, Contact resistance \u22643, Mating cycles greater than or equal to 500, Stripping length = 8\". Thus, the query specifies the target model type and the set of identifying properties (names and values) to be determined from the graph.\nNatural Language Parser: This processing step takes the natural language query as its input. Firstly, a regular expression is employed to detect and isolate the model_type. Secondly, another regular expression is used to extract groups of property_name, relational operators, and property_value from the text. Each group contains all the information necessary to match a single property in the target product(s). A Python dictionary named model_spec is created, whose keys hold property names and values, which are sub-dictionaries containing values to match and the concomitant relational operator to be used for comparison.\nThe sub-dictionaries are classified into three types depending on the nature of the matched values: single-valued, single-limit ranged, and dual-limit ranged. The relational operator is used to select the"}, {"title": "Model Spec Dictionary", "content": "The Python dictionary model_spec will be parsed to yield a Neo4j cypher query in the following steps. It contains all property names, value pairs that should be present in the target product(s)."}, {"title": "Graph Querying Engine", "content": "The Python dictionary model_spec is input to this operation and consists of two sub-operations: Generating cypher query and executing cypher query. The algorithm for generating Cypher Query is shown in Figure 12.\nAlgorithm 2 lays out the steps followed in populating the cypher query with three sets of conditions to be met. The first constitutes a set of match conditions, one for each property, considering the different types of property values (single-valued, single-limit, and dual-limit ranges). The second part uses the model_name attribute of the relationship HAS between a property name and its associated value to check if all matched properties belong to the same model. The third part checks if the matching model_name contains the model_type as a substring. Finally, a statement to return the values of part_number for all products that match all the conditions is included.\nExecute cypher query: The query thus generated is then run on the Neo4j AuraDB instance hosted on the cloud. A list of part_number values of products that match all conditions specified in the query is returned as the output of the graph querying engine."}, {"title": "Part Number(s) Final Output", "content": "A list of part_number values of all matched models of Component"}, {"title": "Workflow Testing", "content": "The data sheet of a component called the 'male contact' was used to test the workflow. This section only provides the instances of the outputs from the workflow. Figure 13 shows the specification sheet for the component.\nKG population output is shown in Figure 14. The KG classification, population, and retrieval snapshots are provided in Figures 15 to 17."}, {"title": "Workflow Evaluation", "content": "The framework was evaluated against two criteria, 1.\n\u2022\n\u2022\nThe performance of the classification model for obtaining F, B, and S using confusion matrices.\nThe perceived usefulness of the workflow to the end user, the industry partner. The evaluation of the model was carried out iteratively throughout the development of the workflow. This section summarizes the results obtained from the final evaluation.\nEvaluation of the classification model's performance: The classification model's performance was assessed using the Accuracy, Precision, Recall, and F1 score indices.\nBased on the observed accuracy and the averages, it can be inferred that the classifier model provides reasonable accuracy in classification for the partner company. The model performed better in classifying Functions than the other two variables. Behaviour classification had the lowest performance.\nEvaluation of the workflow's usefulness: The workflow's usefulness was evaluated at each step of the workflow development. A smaller R&D team was involved in biweekly meetings and evaluations in the initial stages. The final assessment was conducted by a larger team involving senior management and their knowledge management experts. The team found the workflow useful and applicable in their knowledge management practice."}, {"title": "Discussion", "content": "A review of the existing ontological frameworks revealed the FBS ontology's advantages in ease of application, generalizability, and capturing the overall design-reasoning steps. To overcome the limitations of the FBS ontology in terms of ambiguity in its definitions, guidelines for FBS classification needed to be developed. While the rules proved reasonably effective in the current case of electrical configurator components, applying the rules to a different case might necessitate adjustments.\nThe focus of this work was to develop a method and workflow for automated extraction and creation of the targeted FBS-ontology-based KG from a legacy structured dataset. The evaluation of the framework gave indications regarding the effectiveness of the developed framework. The confusion matrix showed reasonable overall accuracy in classification but revealed differences in performances across F, B, and S variables. The model exhibited relatively accurate identification of the F variables with variations in the accuracy of classification of B and S. The results indicate the need for further refining the rules, particularly in classifying the B and S variables. A necessary investigation towards the comprehensiveness of the rules to accurately classify data of various formats needs to be conducted.\nThe investigation highlighted two crucial aspects in the discourse on applying FBS ontology. While the functional decomposition of the product into the three variables was achieved, further decomposition was not pursued in the current work. Additionally, it was observed that the structure of the KG contained only vertical linkages. Developing an approach to identify cross-linkages between behaviours is essential for enabling knowledge synthesis and reasoning-related tasks. This could enable a more accurate KG representation of a product from its documentation.\nA crucial necessity within an ontological framework is the seamless applicability for users. The present work focuses on this aspect and suggests an approach that involves furnishing users with rules for the framework's application. With robust guidelines, it is anticipated that the FBS can provide a shared language for representing all forms of design knowledge, including design rationale, constraints, environment, and scientific principles. Further, it can be applied to other types of industrial processes to drive process innovations.\nThe results revealed that the Rule-based FBS-KG framework effectively automates the process of populating and retrieving information using FBS-based ontology from legacy data. This suggests that"}, {"title": "Conclusion", "content": "The current work presents a method towards the automated FBS-based knowledge representation and retrieval framework from structured legacy dataset. Towards this end, a set of FBS guidelines, an FBS-based KG structure, and a workflow for populating and retrieving FBS-based KG were designed, developed, and evaluated. The framework was developed in collaboration with an industry partner and implemented in the case of an electrical configurator. The implementation results revealed that the framework showed reasonable effectiveness in using structured data legacy data for populating and retrieving information from FBS-based KG.\nThis framework provides the foundation for automated design generation using automated FBS reasoning. Future work will focus on enhancing the system's ability to generate design solutions. This would require further investigation through the development of the FBS reasoning-based automation system."}, {"title": "Appendix-I", "content": "Behaviour - based on property name\nA property is classified as type of Behaviour if it can be either\na. Measure or\nb. Classification\nMeasure - based on property name\nA sequence of conditions undertakes to determine if the property constitutes a measurable quantity or not. In parallel, checks are performed to discern whether the value of the property, should it prove to be a Measure, is single-valued or a range of values, with or without associated units of measurement, to enable different representations in property value nodes in the graph while populating them in the next step downstream. Regular expressions can be used to detect such patterns, but their presence is not conclusive proof of the property being a Measure. Consider, for instance, the following two properties, with 'Assembly instructions' and 'Size' names. The former has a value of 'At voltages of < 5 V and currents < 5 mA, gold-plated contacts are recommended', and the latter, \u20181 A'.\nIf analysis were to be restricted to just the mentioned property values, the presence of a range of permitted values and an ostensible unit of measurement would merit a classification of Measure. This\nis, however, incorrect as the first represents a specific recommendation and the second a model number, not a measurement. To account for this, it was determined that analysis of the property name would yield valuable context regarding the property's status as being a measurable quantity or not.\nTo this end, an NER model trained on a small dataset of relevant property names was used to classify the property name attribute as either a Measurable Quantity,Immeasurable Property or Model Number. The first would capture the intended properties to be classified as a Measure, while the second and third capture values like the first and second examples previously introduced.\nIf the property is determined to be a Measurable Quantity, it is classified as a Measure. Finally, regular expressions are employed to discern whether the property value constitutes a a single value or a range of permissible values, for the purpose of downstream use.\nClassification - based on property name\nA set of associated keywords is assembled, and a cosine similarity measure is arrived by pairwise comparison of the property name with each keyword, considering both local and contextual similarity. The maximum pairwise similarity is then compared to a threshold (0 . 7). If it exceeds the threshold, the property is of type Classification.\nIf the property is of either type of Measure or Classification, it is classified under Behaviour.\nStructure - based on property name\nFor a property to be classified under Structure, a check is performed to see if it is associated with the physical, material composition of its associated Component. The same custom NER model used in the classification of Measure has been trained to classify whether the property name is Material or not. If it is, the property is classified under Structure.\n**Function - based on property name**"}]}