{"title": "Are KAN Effective for Identifying and Tracking Concept Drift in Time Series?", "authors": ["Kunpeng Xu", "Lifei Chen", "Shengrui Wang"], "abstract": "Dynamic concepts in time series are crucial for understanding complex systems such as financial markets, healthcare, and online activity logs. These concepts help reveal structures and behaviors in sequential data for better decision-making and forecasting. Existing models struggle with detecting and tracking concept drift due to limitations in interpretability and adaptability. This paper introduces Kolmogorov-Arnold Networks (KAN) into time series and proposes WormKAN, a KAN-based auto-encoder to address concept drift in co-evolving time series. WormKAN integrates the KAN-SR module, in which the encoder, decoder, and self-representation layer are built on KAN, along with a temporal constraint to capture concept transitions. These transitions, akin to passing through a \"wormhole\", are identified by abrupt changes in the latent space. Experiments show that KAN and KAN-based models (WormKAN) effectively segment time series into meaningful concepts, enhancing the identification and tracking of concept drifts.", "sections": [{"title": "1 Introduction", "content": "Time series analysis plays a crucial role in various fields such as finance, healthcare, and meteorology. Recently, deep learning models have made significant strides in forecasting tasks. Notable advance- ments include CARD [34], which aligns channels within transformers for accuracy, and TimeMixer [33], which employs a multiscale mixing approach. MSGNet [4] leverages inter-series correlations for multivariate forecasting, while Crossformer [45] exploits cross-dimensional dependencies. DeepTime [36] improves forecasting by integrating time indices into its structure. Additionally, large language models (LLMs) like Time-LLM [18] and UniTime [23] have been integrated into time series, opening new avenues for zero-shot and cross-domain forecasting.\nDespite these advancements, a critical challenge remains \u2013 the ability to detect and track concept drift, particularly in co-evolving time series where multiple series exhibit interdependent behavior over time. Concept drift \u2013 changes in a series' statistical properties \u2013 can significantly degrade model performance. This is crucial, particularly in fields like finance, where shifts in market regimes and nonlinear relationships are just as important as prediction accuracy for decision-makers. While recent methods like CluStream [1] and DenStream [5] improve scalability, they often fail to capture temporal dependencies and dynamic transitions. Deep learning models such as OneNet [35] and FSNet [29] tackle concept drift but prioritize predictive accuracy over understanding the underlying concepts.\nKolmogorov-Arnold Networks (KAN) [25] offer a promising solution to the challenges of concept drift in time series analysis. Inspired by the Kolmogorov-Arnold representation theorem [21, 20], KAN replaces linear weights with spline-parametrized univariate functions, allowing the model to learn more complex relationships while improving both accuracy and interpretability. A notable advantage of KAN is its ability to refine spline grids, offering deeper insights into how inputs influence outputs, making the network's decision-making process more transparent. However, while KAN has"}, {"title": "2 WormKAN", "content": "We introduce WormKAN, a framework for concept-aware deep representation learning in co-evolving time sequences. The model uses Kolmogorov-Arnold Networks (KAN) in both the encoder and decoder, with a self-representation layer implemented as a 2-layer KAN. Combined with a temporal smoothness constraint, this architecture captures dynamic concepts and their transitions. To process co-evolving sequences, we segment the multivariate time series S using a sliding window, resulting in segments W = W1,W2, ..., wn, where each w\u2081 contains multiple time steps and channels. The framework is illustrated in Figure 1."}, {"title": "2.1 Kolmogorov-Arnold Networks", "content": "Kolmogorov-Arnold Networks (KAN) leverage the Kolmogorov-Arnold representation theorem [21], which states that any multivariate continuous function can be represented using univariate functions. Specifically, a function f(x1,x2,...,xn) can be expressed as f(x1,x2,...,xn) = $\\sum_{q=1}^{2n+1} \\Phi_q (\\sum_{p=1}^n \\varphi_{q,p}(x_p))$, where $\\varphi_{q,p}$ and $\\Phi_q$ are univariate functions. KAN replaces linear weights with learnable univariate functions, often parametrized using splines, allowing complex nonlinear relationships to be modeled with fewer parameters and greater interpretability. Inputs Xp are transformed by $\\varphi_{q,p}(x_p)$, aggregated, and passed through $\\Phi_q$. Stacking multiple KAN layers allows the network to capture intricate patterns while maintaining interpretability, with the deeper architecture described as KAN(x) = (\u03a6L\u221210\uff65\uff65\uff65 \u25cb \u03a6\u03bf)(x), where L is the total number of layers."}, {"title": "2.2 KAN-Based Deep Representation Learning", "content": "WormKAN utilizes KAN to learn robust representations of co-evolving sequences through the KAN-SR module, which comprises an Encoder, a Self-Representation Layer, and a Decoder.\nEncoder: The encoder employs a KAN to map input segments W into a latent representation space. Specifically, the encoder performs a nonlinear transformation Zee = KAN\u0473 (W), where \u039a\u0391\u039d\u0398 is the encoding function implemented using KAN, and Zee represents the latent representations.\nSelf-Representation Layer: Implemented as a 2-layer KAN (only input and output layers), this layer captures intrinsic relationships among the latent representations and enforces that each latent representation can be expressed as a combination of others: Zee = Zee Os, where Os \u2208 Rn\u00d7n is the self-representation coefficient matrix learned by the KAN. Each column Os,i of Os represents the weights used to reconstruct the i-th latent representation from all latent representations. To promote sparsity in Os and highlight the most significant relationships, we introduce an l\u2081 norm regularization: Lself(s) = $||\\Theta_s||_1$.\nDecoder: The decoder reconstructs the input segments from the refined latent representations using another KAN network: W\u0473 = KAN\u0473\u300f(\u017b\u04e9\uff61), where KAN\u0189\u300f is the decoding function implemented using KAN, and W\u0189 represents the reconstructed time series segments.\nTemporal Smoothness Constraint: To ensure that the latent representations vary smoothly over time, we incorporate a temporal smoothness constraint on Os. We define a difference matrix"}, {"title": "2.3 Concept Transition Detection", "content": "Once the self-representation coefficient matrix Os is ob- tained, the next step is to segment the matrix to identify concept transitions. Unlike most methods, we do not re- quire the number of concepts to be known beforehand.\nConcept transitions can be viewed as passing through \"wormholes,\" where boundaries between concepts indicate rapid shifts in behavior. These boundaries are detected by analyzing the self-representation matrix OR. Significant deviations in OR signal concept changes, with large shifts indicating transitions between disconnected concept spaces. Within a segment, columns of OR should be near zero, while deviations suggest boundaries between segments, akin to transitioning through a wormhole to a new concept space (Figure 2). To pinpoint these transi- tions, we compute the absolute value matrix B = |OR| and calculate the vector of the column-wise means. A peak-finding algorithm is then applied to the vector, where peaks correspond to concept transition points, effectively detecting dynamic changes in the time series structure (see Appendix C for details)."}, {"title": "3 Experiments", "content": "In this section, we first evaluate the original KAN model's effectiveness in detecting concept drift, followed by experiments validating the performance of WormKAN for concept-aware deep represen- tation learning in co-evolving time series."}, {"title": "3.1 Original KAN Model Evaluation", "content": "As illustrated in Figure 3, we use a sliding window to traverse the synthetic time series, creating input-output pairs for training the KAN model. For simplicity, two historical time steps predict the next step. Different KAN structures and activation functions represent different concepts, and observing variations in KAN can reveal concept drift. For example, the learnable activation functions in KAN1, KAN4, and KAN5 behave consistently, indicating they are within the same concept. Details on the synthetic data used, the results on real-world datasets, as well as the KAN model's parameter settings, can be found in the Appendix D.1."}, {"title": "3.2 WormKAN Performance and Baseline Comparison", "content": "We evaluated WormKAN using three co-evolving time series datasets: human motion, financial markets, and online activity logs (details in the Appendix D.2). WormKAN was compared against StreamScope [19], TICC [15], and AutoPlait [27], which are methods for discovering patterns in co-evolving series. The results in Table 1 show that WormKAN outperforms all baselines.\nWe also visualized the concept transitions detected by WormKAN on the Motion Capture Data. Figure 4 illustrates time series segments with marked transitions between different motion types, such as walking and dragging. These visualizations highlight WormKAN's ability to accurately detect boundaries between different types of activity, reinforcing its effectiveness in identifying dynamic changes in co-evolving sequences."}, {"title": "4 Conclusion", "content": "This work demonstrates the effectiveness of Kolmogorov-Arnold Networks (KANs) in detecting concept drift in time series. We introduced WormKAN, which outperforms baseline models in identifying concept transitions across various datasets. Our results highlight KANs' potential for robust, adaptive modeling in dynamic time series environments."}, {"title": "A Extended Related Work", "content": "Kolmogorov-Arnold Networks (KAN) represent a recent innovation proposed by the MIT team [25]. KAN is a novel neural network architecture designed as a potential alternative to MLPs, inspired by the Kolmogorov-Arnold representation theorem [21, 20, 3]. Unlike MLPs, KAN applies activation functions on the connections between nodes, with these functions being capable of learning and adapting during training. By replacing linear weights with spline-parametrized univariate functions along the network edges, KAN enhances both the accuracy and interpretability of the network. A significant advantage of KAN is that a spline can be made arbitrarily accurate to a target function by refining the grid. This design not only improves network performance but also enables them to achieve comparable or superior results with smaller network sizes across various tasks [25, 32, 13].\nTheoretical Foundation. The Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be decomposed into a finite sum of compositions of univariate functions [21]. Formally, the theorem is expressed as:\nf(x1,...,xn) = $\\sum_{q=1}^{2n+1} \\Phi_q (\\sum_{p=1}^n \\varphi_{q,p}(x_p))$\nwhere q,p are univariate functions that map each input variable xp, and Iq are continuous functions. This allows KAN to model complex interactions in high-dimensional data through compositions of simpler univariate functions.\nNetwork Architecture. KAN leverages the Kolmogorov-Arnold representation theorem by replacing traditional linear weights in neural networks with spline-parametrized univariate functions. Unlike conventional Multi-Layer Perceptrons (MLPs), which use fixed activation functions at the nodes, KAN applies adaptive, learnable activation functions on the edges between nodes. These functions are parametrized as B-spline curves, which adjust dynamically during training to better capture the underlying data patterns. This unique structure enables KAN to effectively capture complex nonlinear relationships within the data. Formally, a KAN layer can be defined as \u03a6 = {$6q,p$}, p =\nq = 1,2, . . ., Nout,, where (q,p are parametrized functions with learnable parameters. This structure allows KAN to capture complex nonlinear relationships within the data more effectively than traditional Multi-Layer Perceptrons (MLPs).\nTo extend the capabilities of KAN, deeper network architectures have been developed. A deeper KAN is essentially a composition of multiple KAN layers [25], enhancing its ability to model more complex functions. The architecture of a deeper KAN can be described as:\nKAN(x) = (\u03a6L\u22121 \u25cb \u03a6L\u22122 \u00b0 . . . \u03bf \u03a6\u03bf)(x),"}, {"title": "A.2 Concept Drift in Time Series", "content": "Concept drift has been a significant challenge in time series analysis, particularly in streaming data environments where the underlying data distributions may change over time [40, 39]. Traditional models such as Hidden Markov Models (HMM) and Autoregression (AR) have been widely used but often lack adaptability in the presence of continuous data streams. Recent advancements, such as OrbitMap [26] and TKAN [43], have improved scalability but still face challenges in capturing temporal dependencies and dynamic transitions. Additionally, models like Cogra [28] and Dish- TS [11] have introduced techniques to address concept drift by incorporating stochastic gradient descent and distribution shift alleviation, respectively."}, {"title": "Co-Evolving Sequences and Dynamic Concept Identification", "content": "The identification of dynamic concepts in co-evolving sequences is crucial for understanding complex temporal patterns. Various methods have been proposed to segment time series data into meaningful patterns, including the use of hierarchical HMM-based models like AutoPlait [27], Kernel-based models [41, 39, 42] and Clustering-based methods [7, 8, 15]."}, {"title": "Deep Representation Learning for Time Series", "content": "Deep learning techniques have gained traction in temporal data analysis, offering powerful tools for representation learning. OneNet [35] and FSNet [29] are notable examples that enhance time series forecasting by adapting to concept drift. However, these models primarily aim to improve predictive accuracy rather than offering insights into the concept identification process. Informer [46], TIMESNET [37], Triformer [9], and Non-stationary Transformers [24] further extend the capabilities of deep learning models in handling long sequence time-series forecasting and non-stationary behaviors in time series."}, {"title": "Concept-Aware Models", "content": "The idea of concept-aware models, which can detect transitions between different behaviors or patterns, has been explored in various domains. StreamScope [19] and the Generative Learning model [16] for financial time series have contributed to this area by automat- ically discovering patterns in co-evolving data streams. There are also advancements in evolving standardization techniques for continual domain generalization [38]. Other approaches such as online boosting adaptive learning [44], temporal domain generalization via concept drift simulation [6], and drift-aware dynamic neural networks [2] have also been proposed to handle concept drift in temporal data. However, these models do not explicitly address the interpretability of the learned represen- tations, a gap that Wormhole seeks to fill by providing clear demarcations of concept transitions, enhancing the understanding of dynamic temporal patterns."}, {"title": "B Expanded Explanation of Temporal Smoothness Constraint", "content": "In the main text, we introduced the temporal smoothness constraint, which ensures that the latent representations vary smoothly over time. This constraint is critical in capturing gradual transitions and dynamic concept changes in time series data. Here, we provide a more detailed explanation of the underlying components and their formulation.\nDefinition of the Difference Matrix:\nThe key element of the temporal smoothness constraint is the difference matrix R. This matrix captures the differences between consecutive columns of the self-representation matrix Os, ensuring smooth transitions across time steps. Formally, R \u2208 Rn\u00d7(n-1) is defined as:\nR =\n\u300c-1 0 0 ... 0\n1 -1 0 ... 0\n0 1 -1 ... 0\n:\n:\n:\n:\n0 0 0 ... 1 -1\nEach row of R captures the difference between two consecutive columns in Os. By multiplying Os with R, we compute the differences between consecutive time steps as follows:\nOR = [0,2 \u2013 0,1, 0,3 - 0,2,..., 0s,n - 0s,n-1].\nThis constraint measures how the latent representations change over time and is used to enforce smoothness."}, {"title": "C Concept Identification", "content": "Once a solution to s has been found, the next step is to segment the coefficient matrix Os to find the concept. We discuss methods for segmentation depending on amount and type of prior knowledge about the original data. Unlike most methods, we do not require the number of concepts to be known beforehand.\n\u2022 If we assume that concept is drawn from a set of disconnected subspaces, i.e., Os is block diagonal, we can use the information encoded by OR to identify the boundaries between"}, {"title": "D Additional Experiments Details", "content": "In this subsection, we provide additional details on the evaluation of the original KAN model using both synthetic data and real-world datasets.\nDetail of Datasets. We utilized a financial time series dataset for our experiments due to its complex and often unpredictable nature, characterized by high volatility and a lack of consistent periodicities. This dataset comprises daily OHCLV (open, high, close, low, volume) data spanning from January 4, 2012, to June 22, 20222. Our primary objective was to predict the implied volatility of each stock. Since true volatility cannot be directly observed, we approximated it using an estimator based on realized volatility. The conventional volatility estimator is defined as: V\u2081 = $\\sqrt{\\sum_{t=1}^{T}(rt)^2}$, where rt = ln(ct/Ct-1) and ct represents the closing price at time t. We also crafted a Synthetic SyD dataset comprising 500 simulated time series, each generated by a combination of following five nonlinear functions. The synthetic dataset allows the controllability of the structures/numbers of concepts and the availability of ground truth. To make a 780-steps long time series, we randomly choose one of the five functions ten times; every time, this function produces 78 sequential values \u2013 which are considered as a concept.\nExperimental setup. For the Original KAN Model, we created two variants, T-KAN and MT-KAN, designed for univariate and multivariate time series, respectively. Both T-KAN and MT-KAN are implemented using a spline-based parameterization for the univariate functions. Both T-KAN and MT-KAN are implemented using a spline-based parameterization for the univariate functions. For the stock datasets, we use a 21-step window, with four such windows (totaling 84 steps) as inputs to predict the volatility for the next 21 days. We adopt the network structure from the original KAN paper, utilizing a simple two-layer (the input layer is not accounted as a layer per se) architecture with only 5 hidden neurons, i.e., [84, 5, 21]. For MT-KAN, we group 5 variables together for input,"}, {"title": "D.2 Detailed Evaluation of WormKAN", "content": "Detail of Datasets. We evaluated WormKAN using three datasets representing different domains of co-evolving time series. The Motion Capture Streaming Data from the CMU database\u00b3 captures various motions such as walking and dragging, making it ideal for analyzing transitions between different types of human activities. The Stock Market Data includes historical prices and financial indicators from 503 companies\u2074, providing a large-scale, high-dimensional dataset to test the model's performance in detecting concept changes in financial markets. Lastly, the Online Activity Logs from GoogleTrend event streams include 20 time series of Google queries for a music player from 2004 to 2022, used to evaluate the model's ability to detect behavioral shifts in user interactions.\nComparison with Baseline Models. To thoroughly evaluate the effectiveness of our WormKAN model, we compared it with several baseline models. These included StreamScope[19], a scalable streaming algorithm for automatic pattern discovery in co-evolving data streams; TICC[15], which segments time series into interpretable clusters based on temporal dynamics; and AutoPlait[27], a hierarchical HMM-based model for automatic time series segmentation that identifies high-level patterns."}]}