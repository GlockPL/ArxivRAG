{"title": "Enhancing Equitable Access to AI in Housing and Homelessness System of Care through Federated Learning", "authors": ["Musa Taib", "Jiajun Wu", "Steve Drew", "Geoffrey G. Messier"], "abstract": "The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing. An HHSC typically consists of many agencies serving the same population. Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another. Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not. To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data. We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC. This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent. Our experimental results using real-world HHSC data from Calgary, Alberta, demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies.", "sections": [{"title": "Introduction", "content": "Emergency housing shelters/agencies are an important part of the housing and homelessness system of care in most North American cities. While originally intended to provide low-barrier access to shelter for short periods, many people now make use of shelters over long periods either continually (chronic shelter use) or sporadically (episodic shelter use). Most long-term shelter users face multiple physical and mental health challenges (Hopper, Bassuk, and Oliver 2009) that prevent them from making a quick exit from the shelter system without first connecting them with housing and the wrap-around supports necessary to help them address their unique set of challenges. Helping these people exit the shelter quickly is important. Shelters are difficult environments, and the Housing First philosophy recognizes that a person having trouble exiting a shelter should be connected to support as soon as possible (Goering et al. 2011).\nProperly matching shelter users to supportive housing is a difficult problem. In most cities, the number of supportive housing spaces is much less (Coleman 2023) than the number of new people entering homelessness. The vast majority (approximately 85%) of first-time shelter users will exit the shelter quickly and without support (transitional shelter use) (Aubry et al. 2013a; Culhane et al. 2007). Therefore, it is the longer-term chronic and episodic shelter users who should be prioritized for the limited supply of supportive housing. However, chronic and episodic shelter use patterns can take months or even years to become evident. As a result, most government definitions of chronic homelessness require a history of homelessness that is one year or longer (Byrne and Culhane 2015). Requiring a person to live in a shelter for this length of time before they can demonstrate a historical record of chronic or episodic homelessness is unacceptable. Living conditions in shelters are difficult and pose further risks to a person's physical and mental health (Hopper, Bassuk, and Oliver 2009).\nMachine learning (ML) has great potential for rapidly identifying which first-time shelter users are at risk of becoming chronic or episodic shelter users in the long term (Toros and Flaming 2018; Purao et al. 2019). Rather than requiring a person to accumulate a record of shelter use stretching over many months or years, ML can identify at-risk people using only the first few months of a person's shelter record (VanBerlo et al. 2021).\nIt must be stressed that the purpose of using ML within an HHSC is not to automate the decision of who should be assigned to supportive housing. The established best practice within an HHSC is to support people with a variety of programs, not a single program or automated instrument (Shinn and Richard 2022). Rather than replacing front-line human staff, our tool would be used by that staff to identify possible \"under the radar\" shelter users who have been missed by other programs. Being identified by our machine learning approach would not automatically qualify someone for housing. Instead, it would initiate a conversation with a support worker to more fully understand that person's needs.\nWhile ML has great potential, the issue of equitable access to high-quality ML tools within an HHSC is challenging and nuanced. An HHSC typically consists of many not-for-profit agencies of varying sizes. Smaller agencies will usually have fewer IT resources in terms of both expertise and infrastructure. However, IT platform and talent barriers can be overcome by the pooling of resources and/or collab-"}, {"title": "Data Preprocessing", "content": "The dataset utilized for this study has a series of timestamped records indicating when a person slept in an emergency shelter. This data is pre-processed as follows:\n1.  The sleep data records for each client are truncated after $T_o$ days. This is the observation window used to predict whether the client will become chronic/episodic.\n2.  The $T_o$ day observation window is then subdivided into $T_o$ equal segments known as time bins, where the number of sleeps in each bin is summed. For example, a client with a $T$ = 120-day observation window divided into $T_o$ = 10 time bins would have each bin containing the sum of their shelter stays over a period of 12 days. Each of these time bin values is a different input data feature for the machine learning algorithm.\n3.  As mentioned before our data is a series of timestamps however that does not provide much useful information to ML models. Therefore, we engineered two features to reflect the number of sleep and episodes during the observation window to increase the total number of features. We counted the number of days that a client slept at a shelter over a specified period, and an episode is defined as a period of 30 or more days between consecutive instances of sleeping.\nNote that the HHSC dataset was pre-screened by the CHF, so it was unnecessary to add preprocessing to deal with invalid data entries. By default, the data from the CHF is linked across all HHSC agencies and corresponds to the Centralized dataset scenario. To evaluate the Federated and Isolated scenarios, data is unlinked so that a person is assigned a new client identification (ID) number each time they access a new agency."}, {"title": "Labeling", "content": "The labeling process begins by using the historical shelter access records of an individual. A person's pattern of shelter access is labeled as transitional, chronic, or episodic by first creating the 2-tuple $(N_s, N_E)$ for that person where $N_s$ is that person's total number of shelter stays, and $N_E$ is the person's total number of shelter episodes. $N_s$ and $N_E$ are calculated using the person's first $T_p$ days of HHSC interaction.\nIn a machine learning context, $T_p$ corresponds to the prediction window since this is the window over which a person's specific pattern of shelter access will manifest. In previous studies (Culhane et al. 2007; Aubry et al. 2013a; Kneebone et al. 2015), $T_p$ has been equal to the total number of days of data available for each person. However, in the CHF data described in the Data Section, this can be several years, which is an overly ambitious prediction window given that we would like to restrict the observation window $T_o$ to only a few months. We will demonstrate in the Hyperparameter Selection Section that reducing $T_p$ to be on the order of 1-2 years improves performance while still providing a very impressive prediction horizon.\nOnce the 2-tuples are determined for each person in the data, the k-means algorithm with 3 clusters is used to asso-"}, {"title": "Algorithm 1: DecentralizedLabeling", "content": "Input: $K$ is the list of agencies; $k \\in K$ is an agency; $|D_k|$ is the size of the dataset for agency k. $\u03bc_c$, $\u03bc_e$, $\u03bc_t$ are chronic, episodic, and transitional cluster centroids on agency k; $\u03bc_c$, $\u03bc_e$, $\u03bc_t$ are global chronic, episodic, and transitional cluster centroids used for labeling.\nOutput:  $\u03bc_c$, $\u03bc_e$, $\u03bc_t$.\n1: for each agency $k \\in K$ do\n2:   Initialize $\u03bc_c^k$, $\u03bc_e^k$, $\u03bc_t^k$ from $D_k$\n3:   Apply k-means clustering for $D_k$\n4:   Update $\u03bc_c^k$, $\u03bc_e^k$, $\u03bc_t^k$\n5: end for\n6: $\u03bc_c$, $\u03bc_e$, $\u03bc_t \u2190 \u2211_{k=1}^K \\frac{|D_k|}{|K|} \u03bc_c^k,  \u2211_{k=1}^K \\frac{|D_k|}{|K|} \u03bc_e^k, \u2211_{k=1}^K \\frac{|D_k|}{|K|}\u03bc_t^k $.\n7: return $\u03bc_c$, $\u03bc_e$, $\u03bc_t$\nciate each person with a centroid that corresponds to transitional, chronic, and episodic shelter use as shown in Fig. 1. For the Centralized scenario, the 2-tuples are created for everyone in the data using the fully merged CHF dataset. For the Isolated case, a person's interaction with multiple agencies is unlinked and treated as different individuals as described in the Data Preprocessing Section. Each agency performs its own k-means labeling process for the unlinked individuals in their own datasets.\nFor the Federated scenario, we propose a new labeling approach, Decentralized k-means, as shown in Algorithm 1, that allows agencies to benefit from pooling their data without the technical complexity and privacy concerns of fully linking their datasets. Each agency performs k-means clustering on its own datasets for the Isolated case. The clusters are then forwarded to a central location that performs a weighted average. These averaged clusters are then shared back with the agencies who can use them to re-classify their 2-tuples."}, {"title": "Methodology", "content": "We formulate the allocation of the homelessness support problem as a multi-class prediction task. Consider a dataset $D$, which is a $m \\times n$ matrix where m is the number of clients and n is the number of features. The sleep pattern of an individual client is split into $T_o$ time bins in addition to two engineered features: total number of sleeps and total number of episodes. Therefore, each client would have an input feature vector f with length $n = T_o + 2$. Our task is to train a classifier $C(\u00b7)$, which produces the prediction results for a given f; $\\hat{y} = C(f)$, where $\\hat{y} \u2208 [chronic, episodic, transitional]$ refers to the output labels."}, {"title": "Training Details", "content": "Fig. 2 shows the training scenarios of the three models. Training for Centralized and Isolated scenarios involves the preprocessing steps described in the Data Preprocessing Section, dataset labeling as described in the Labeling Section, and updating the machine learning model as shown in"}, {"title": "Algorithm 2: AgencyTraining. The Local Prediction Model Training for each agency.", "content": "Input: $w$ is the machine learning model; $\u03b7$ is the learning rate; $E$ is the number of training epochs; $B$ is the training batch size; $L$ is the loss function of training.\nOutput: The trained agency model.\n1: B \u2190 split $D_k$ into batches of size $B$\n2: for each local epoch $i$ from 1 to $E$ do\n3:   for each batch $b \u2208 B$ do\n4:    $w \u2190 w \u2212 \u03b7\u2207L(w; b)$\n5:   end for\n6: end for\n7: return $w$"}, {"title": "Algorithm 3: Federated Prediction Model Training", "content": "Input: $K$ is the list of agencies; $D$ is the list of dataset for all agencies; $|D_k|$ is the size of the dataset for agency $k$; $T$ is the number of communication rounds; $w_t$ is the global model for agency k at round t; B is the batch size; $\\O$ is the clusters.\nOutput: The trained global model parameters.\n1:  DecentralizedLabeling(K, D) \\triangleright Alg. 1\n2:  Server initialize $w_0$;\n3:  Training Starts:\n4:  for each round t from 1 to T do\n5:    for each agency k \u2208 K in parallel do\n6:     $w_k \u2190 AgencyTraining(k, D_k, w_{t\u22121})$ \\triangleright Alg. 2\n7:    end for\n8:    $w_{t+1} \u2190 \u2211_{k=1}^K \\frac{|D_k|}{|K|}w_k^t$\n9:  end for"}, {"title": "Experimental Setup", "content": "The experiments were conducted in a simulated setup on an Apple M1 Pro MacBook with 16GB of RAM 1. The neural networks trained at each simulated agency were isolated to ensure privacy for isolated and federated models. There was no direct inter-agency communication except for controlled weight sharing between training rounds in federated cases. There was no communication for isolated cases."}, {"title": "Train/Test Splits", "content": "We apply a normalization process to the dataset using z-scoring (Pedregosa et al. 2011). This results in a standardized score for each data point, reflecting how many standard deviations it is from the column average.\nFollowing the normalization, the dataset is divided into stratified training and testing sets, adhering to an 80/20 split. This strategy guarantees that both sets maintain proportional representations of clients across different agencies and la-"}, {"title": "Machine Learning Models", "content": "Multi-layer Perceptrons (MLPs) are a class of neural networks characterized by their stacked arrangement of fully connected layers, which makes them particularly adept at extracting and learning complex patterns from the input data (Goodfellow, Bengio, and Courville 2016). The MLP model used for this paper features three ReLU-activated hidden layers (Glorot, Bordes, and Bengio 2011) of sizes 512, 128, and 16, forming a structure capable of identifying and processing complex patterns within the dataset. This configuration, with its hierarchical design, is adept at capturing and learning the intricate relationships present in the input data. To facilitate model generalization and curtail overfitting, dropout layers with rates ranging from 0.1 to 0.4 are judiciously applied across the dense layers (Srivastava et al. 2014). In the final output layer, a sigmoid function (Goodfellow, Bengio, and Courville 2016) is employed to handle multi-class classification.\nThe training for each model is performed as described in Algorithm 2. The models employed the Adam optimizer (Kingma and Ba 2014) with a learning rate \u03b7 = 0.02. For loss calculation, categorical cross-entropy is selected, aligning with the multi-class nature of the output. Optimization is performed via mini-batch gradient descent with a batch size B = 500 and the models are trained for a total of 200 epochs (E).\nThe performance of the model can vary as a result of random weight initializations. Therefore, each MLP model was trained 10 times on different initializations for all three cases (federated, isolated, and centralized). The results for all 10 rounds were then averaged to get a more generalized picture of the results.\nThe aforementioned details apply uniformly across all three models, with the federated model incorporating additional parameters to tailor the training process. According to Algorithm 3, the federated model undergoes T = 75 communication rounds. With K = 8 each representing a unique agency contributing to the model. Furthermore, each client undergoes E = 15 training epochs on their local dataset for every round."}, {"title": "Hyperparameter Selection", "content": "The Data Preprocessing and Labeling Sections present some important hyperparameters that have a significant impact on model performance. Increasing the size of the observation window, $T_o$, will generally improve model performance. However, a long observation window is at odds with our objective of identifying at-risk HHSC users as soon as possible. Therefore, the smallest possible $T_o$ that yields acceptable performance must be selected.\nIncreasing the number of time bins, $T_b$, improves the model's ability to capture temporal trends and provide a better fit to the data. In general, a small value of $T_b$ will underfit and a large value of $T_b$ will overfit the data (Taib and Messier 2023)."}, {"title": "Discussion", "content": "Before discussing the results in the Evaluation Section, it is important to provide context regarding how a machine learning tool should be utilized within an HHSC. As noted in the Introduction Section, machine learning should never be the only method for connecting people to housing. Instead, it is used as part of a range of programs for engaging with HHSC users (Shinn and Richard 2022), including \u201cdiversion\u201d programs specifically designed for making contact with new HHSC entrants.\nWhile improving the absolute performance of the models presented in the Evaluation Section is always desirable, these performance levels are acceptable if the models are used in the context of supporting a robust diversion program with human front-line staff. Performance could also be further improved if the methods in this paper are applied in an HHSC where other data features, such as demographic information, are collected.\nCentralized vs. Federated The results from Table 2 show that the centralized model outperforms the other two models. The Centralized scenario is ideal and may be achievable in some HHSCs that have a centralized agency capable of collecting data, as is the case in Calgary. However, most HHSCS are better represented by the Isolated scenario where agen-"}, {"title": "Achieving Equity with Federated Learning", "content": "One significant challenge within the HHSC network is the disparity in data analytics capabilities between large and small agencies. Smaller agencies often struggle with insufficient datasets. This limitation hampers their ability to identify at-risk clients accurately and tailor their services effectively. Looking at the results in Figure 3, it is quite clear that our federated model has a substantial gain in performance over the isolated case. A breakdown of the per agency performance keeping in mind the number of clients per agency in table 4 shows exactly where the federated model gains its performance. Notably, smaller shelters, those with fewer than 10,000 clients, which struggle to adequately train their models due to insufficient data, benefit markedly from the aggregated insights available through larger shelters. Conversely, larger shelters observed minimal performance gains, as their pre-existing data volumes were already sufficient to effectively train their models.\nThe federated model presents a solution to this problem by enabling smaller agencies to leverage larger, aggregated models trained across multiple data sources without directly sharing sensitive information. FL allows for the collective improvement of model performance, benefiting all participants in the network. When using machine learning for social good, equity is important. This means that all people accessing an HHSC should benefit from the same high-quality tools regardless of whether they choose to interact with a large or small agency. The fact that our proposed FL approach benefits smaller agencies, in particular, shows it is an important tool for achieving this equity goal."}, {"title": "Federated models reduce the need for data linkage", "content": "The conventional approach to enhancing machine learning models across different agencies involves the linkage of data to create a centralized dataset. This process not only raises significant privacy concerns but also involves logistical complexities and substantial resources dedicated to ensuring the secure and accurate merging of sensitive information. The federated model offers a compelling advantage by eliminating the necessity for direct data linkage. By training models on decentralized data and aggregating the learned models, FL circumvents the challenges associated with data consolidation. This approach significantly reduces the complexity and cost associated with data linkage, reallocating those resources toward improving service delivery and operational efficiencies. Moreover, FL upholds the privacy and confidentiality of individual records, addressing one of the critical barriers to data sharing among agencies."}, {"title": "Conclusion", "content": "In this work, a critical gap in current homelessness management strategies is addressed by using a machine learning approach that respects the privacy and autonomy of individual agencies while leveraging data from across the HHSC. In jurisdictions where raw data sharing across agencies is challenged by privacy concerns or logistical challenges, our proposed FL model demonstrated virtually the same performance as that of a centralized data approach. Compared to the status quo in the domain, our federated approach greatly enhances the ability of smaller shelters to make predictions and also promotes equity across the HHSC. By demonstrating the feasibility and effectiveness of federated learning in this context, this study lays the groundwork for future research and development of ML applications within the social services sector."}]}