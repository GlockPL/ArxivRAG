{"title": "Enhancing Equitable Access to AI in Housing and Homelessness System of Care through Federated Learning", "authors": ["Musa Taib", "Jiajun Wu", "Steve Drew", "Geoffrey G. Messier"], "abstract": "The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing. An HHSC typically consists of many agencies serving the same population. Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another. Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not. To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data. We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC. This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent. Our experimental results using real-world HHSC data from Calgary, Alberta, demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies.", "sections": [{"title": "Introduction", "content": "Emergency housing shelters/agencies are an important part of the housing and homelessness system of care in most North American cities. While originally intended to provide low-barrier access to shelter for short periods, many people now make use of shelters over long periods either continually (chronic shelter use) or sporadically (episodic shelter use). Most long-term shelter users face multiple physical and mental health challenges that prevent them from making a quick exit from the shelter system without first connecting them with housing and the wrap-around supports necessary to help them address their unique set of challenges. Helping these people exit the shelter quickly is important. Shelters are difficult environments, and the Housing First philosophy recognizes that a person having trouble exiting a shelter should be connected to support as soon as possible. Properly matching shelter users to supportive housing is a difficult problem. In most cities, the number of supportive housing spaces is much less than the number of new people entering homelessness. The vast majority (approximately 85%) of first-time shelter users will exit the shelter quickly and without support (transitional shelter use). Therefore, it is the longer-term chronic and episodic shelter users who should be prioritized for the limited supply of supportive housing. However, chronic and episodic shelter use patterns can take months or even years to become evident. As a result, most government definitions of chronic homelessness require a history of homelessness that is one year or longer. Requiring a person to live in a shelter for this length of time before they can demonstrate a historical record of chronic or episodic homelessness is unacceptable. Living conditions in shelters are difficult and pose further risks to a person's physical and mental health. Machine learning (ML) has great potential for rapidly identifying which first-time shelter users are at risk of becoming chronic or episodic shelter users in the long term. Rather than requiring a person to accumulate a record of shelter use stretching over many months or years, ML can identify at-risk people using only the first few months of a person's shelter record. It must be stressed that the purpose of using ML within an HHSC is not to automate the decision of who should be assigned to supportive housing. The established best practice within an HHSC is to support people with a variety of programs, not a single program or automated instrument. Rather than replacing front-line human staff, our tool would be used by that staff to identify possible \"under the radar\" shelter users who have been missed by other programs. Being identified by our machine learning approach would not automatically qualify someone for housing. Instead, it would initiate a conversation with a support worker to more fully understand that person's needs. While ML has great potential, the issue of equitable access to high-quality ML tools within an HHSC is challenging and nuanced. An HHSC typically consists of many not-for-profit agencies of varying sizes. Smaller agencies will usually have fewer IT resources in terms of both expertise and infrastructure. However, IT platform and talent barriers can be overcome by the pooling of resources and/or collaboration between agency IT teams. The more difficult problem is related to equitable access to ML due to the nature of how data exists within an HHSC. Most HHSCs are made up of a large number of government and not-for-profit community-based services. The same person may interact with multiple agencies, and each agency will record those interactions in its administrative database. Predicting the risk of episodic/chronic homelessness is most accurate when training on data that captures a person's interaction with all the agencies within an HHSC. However, maintaining a single, merged dataset for an entire HHSC is difficult. In the spirit of being a low barrier to entry, many shelters do not require a person to sign data-sharing consent forms. Agency information technology (IT) systems are often incompatible and many municipalities also lack a single system coordinator agency with the expertise and infrastructure to merge data. Finally, people are not typically assigned HHSC-wide unique identity numbers which means that merging records between agencies must be done using names and birth dates. Sharing this identifying information across agency boundaries without consent violated the privacy of the people in the data. The fragmentation of data across an HHSC means that agencies typically only have access to the data they collect by themselves. As a result, smaller agencies that see fewer clients will have correspondingly small data sets that make it difficult to effectively train and test an ML model. Equitable access to ML means overcoming this barrier and ensuring that all agencies have access to the same high-quality ML tools, regardless of their data set size. At the same time, the privacy of the people in the data must be respected and identifying information must not be shared between agency boundaries without consent. Our solution is to utilize a Federated Learning (FL) approach that collectively utilizes the disconnected datasets within the HHSC while respecting the privacy of the people in that data. Our scenario is an example of horizontal partitioning where datasets collected by each individual agency define the partitions. The model will be trained using only shelter stay data (i.e., records of when a person slept in a shelter). While many other data features (i.e., medical history, demographics, etc.) could potentially be useful predictors, these features are not uniformly collected since not everyone utilizing an HHSC consents to providing information about themselves. There is also a risk of bias if those who do consent to providing additional information become more readily identifiable by the machine learning model. Basing our methods on patterns of shelter use only ensures that our approach can be generalized across the widest possible number of HHSCs and is as equitable as possible. A FL perspective is also required to label the examples in the historical shelter use dataset used to train and test our model. While many government definitions exist for chronic homelessness, there are few established definitions for episodic homelessness. Instead, the accepted methodology for labeling a shelter use pattern as chronic, episodic, or transitional is based on the k-means cluster analysis of historical data records . The k-means analysis assumes a person's entire record of homelessness is available. This requires a merged dataset of all HHSC records, which is difficult to obtain, as noted above. The labeling process begins by using the historical shelter access records of a person to determine their number of stays and episodes. An episode is defined as a series of shelter stays with gaps of less than 30 days . As shown in Fig. 1, the k-means algorithm is instructed to create 3 clusters on a scatter plot of the two above values of everyone in the training dataset. The transitional label is assigned to people associated with the low stay/low episode cluster. The episodic labels are assigned to the low stay/high episode and chronic labels are assigned to high stay/low episode clusters, respectively. Our paper will demonstrate how to create Fig. 1 using a federated approach that preserves privacy and does not require merging records. We consider three scenarios in this paper that achieve different compromises between machine learning model performance and expense:\n1. Centralized: All agency data has been merged by a central service. The k-means algorithm for assigning the chronic, episodic, and transitional labels to the data is run on this centralized dataset. The machine learning model is trained centrally and then deployed to the individual agencies for their use.\n2. Federated: Agencies train a single model using a federated approach that avoids the need to centrally merge data. Labels are assigned using a federated k-means algorithm and the model is also trained using a federated approach. The resulting federated model is used by all the individual agencies.\n3. Isolated: The agencies work in isolation. Each agency runs a k-means labeling algorithm and trains its own machine learning model using only its dataset.\nUsing HHSC data from Calgary, Alberta, we demonstrate that our FL approach performs almost as well as the much less practical centralized approach. Using per-agency performance metrics, we show that smaller shelters benefit the"}, {"title": "Related Work", "content": "Homelessness Prediction\nThe Housing First philosophy of rapidly connecting shelter users to housing and the prioritization of people experiencing chronic/episodic homelessness is well established. While some alternative methods have been recently proposed , the most established method for labeling shelter access patterns as chronic, episodic or transitional is using a k-means clustering analysis of historical data records. To date, much of the machine learning work related to homelessness has been concentrated on predicting a first entrance or re-entrance to homelessness. The specific prediction of chronic homelessness has also been explored . However, all of these studies assume a single dataset. None have explored how to overcome the practical issues that arise when working with data that has been fragmented across an HHSC.\nFederated Learning\nFederated Learning (FL) is a decentralized and privacy-preserving machine learning approach that involves training local models in each agency/device using local data, then aggregating the local models using only the model weights. First introduced by McMahan et al., FL enables collaborative machine learning without the need for raw data to be shared with a central authority. In recent years, FL has seen a surge in the healthcare sector where patient data privacy is a paramount concern.\nFL for Risk Prediction Healthcare, a field highly analogous to the services provided by an HHSC, has adopted FL widely for building collaborative prediction models with administrative data , in particular, Electronic Health Records (EHRs). For instance, FL has been used to train prediction models for cardiovascular conditions , clinical outcomes of COVID-19 patients, as well as multi-party diabetes mellitus risk prediction. Despite FL's capability to securely handle sensitive data, no existing studies are using FL with shelter data for risk prediction in the field of Homelessness. This study aims to close this gap by introducing a practical FL training framework for shelters to collaboratively predict the risk of episodic and chronic homelessness without sharing the data."}, {"title": "Dataset", "content": "This study utilizes anonymized shelter data provided by the Calgary Homeless Foundation (CHF). The protocol governing the anonymization, secure storage, and analysis of this secondary dataset was approved by the University of Calgary Conjoint Ethics Review Board (REB-19-0095). The CHF is an organization that provides and supports a data record system utilized by most of the housing and homelessness service agencies in Calgary, Canada. This allows the CHF to aggregate and merge records of service use for the same group of people accessing services across the system of care using privacy-preserving methods. As a result, the anonymized dataset used for this paper represents the Centralized scenario described in the Introduction Section. The dataset stretches from January 1, 2009 to December 31, 2019 and contains timestamped records of when a person using the HHSC slept at an emergency shelter. There are 6,840,069 sleep records for 50,455 individuals accessing 8 different shelters.\nRestricting the machine learning model to predict episodic, chronic, and transitional shelter access patterns using sleep event data only is challenging. There are many other data features sometimes recorded by HHSC services that may be useful in a predictive model (i.e., demographic information, medical history, employment status, family relationships, etc.). However, it is important to stress that this information is not always collected. Demographic information may only be recorded when accessing certain programs. Low barrier-to-entry emergency shelters may not require clients to disclose any information to access sleep services on a cold night. In contrast, a record of sleeping in a shelter is consistently and uniformly collected for all HHSC users. Focusing on this data feature ensures all people are represented equitably in the data. While specific data recording practices may vary between HHSCs, shelter sleep data is recorded by most, which means the methods presented in this paper can be easily adopted in many jurisdictions."}, {"title": "Data Preprocessing", "content": "The dataset utilized for this study has a series of timestamped records indicating when a person slept in an emergency shelter. This data is pre-processed as follows:\n1. The sleep data records for each client are truncated after $T_0$ days. This is the observation window used to predict whether the client will become chronic/episodic.\n2. The $T_0$ day observation window is then subdivided into $T_0$ equal segments known as time bins, where the number of sleeps in each bin is summed. For example, a client with a $T = 120$-day observation window divided into $T_0 = 10$ time bins would have each bin containing the sum of their shelter stays over a period of 12 days. Each of these time bin values is a different input data feature for the machine learning algorithm.\n3. As mentioned before our data is a series of timestamps however that does not provide much useful information to ML models. Therefore, we engineered two features to reflect the number of sleep and episodes during the observation window to increase the total number of features. We counted the number of days that a client slept at a shelter over a specified period, and an episode is defined as a period of 30 or more days between consecutive instances of sleeping.\nNote that the HHSC dataset was pre-screened by the CHF, so it was unnecessary to add preprocessing to deal with invalid data entries. By default, the data from the CHF is linked across all HHSC agencies and corresponds to the Centralized dataset scenario. To evaluate the Federated and Isolated scenarios, data is unlinked so that a person is assigned a new client identification (ID) number each time they access a new agency."}, {"title": "Labeling", "content": "The labeling process begins by using the historical shelter access records of an individual. A person's pattern of shelter access is labeled as transitional, chronic, or episodic by first creating the 2-tuple $(N_S, N_E)$ for that person where $N_S$ is that person's total number of shelter stays, and $N_E$ is the person's total number of shelter episodes. $N_S$ and $N_E$ are calculated using the person's first $T_P$ days of HHSC interaction.\nIn a machine learning context, $T_P$ corresponds to the prediction window since this is the window over which a person's specific pattern of shelter access will manifest. In previous studies, $T_P$ has been equal to the total number of days of data available for each person. However, in the CHF data described in the Data Section, this can be several years, which is an overly ambitious prediction window given that we would like to restrict the observation window $T_0$ to only a few months. We will demonstrate in the Hyperparameter Selection Section that reducing $T_P$ to be on the order of 1-2 years improves performance while still providing a very impressive prediction horizon.\nOnce the 2-tuples are determined for each person in the data, the k-means algorithm with 3 clusters is used to associate each person with a centroid that corresponds to transitional, chronic, and episodic shelter use as shown in Fig. 1. For the Centralized scenario, the 2-tuples are created for everyone in the data using the fully merged CHF dataset. For the Isolated case, a person's interaction with multiple agencies is unlinked and treated as different individuals as described in the Data Preprocessing Section. Each agency performs its own k-means labeling process for the unlinked individuals in their own datasets.\nFor the Federated scenario, we propose a new labeling approach, Decentralized k-means, as shown in Algorithm 1, that allows agencies to benefit from pooling their data without the technical complexity and privacy concerns of fully linking their datasets. Each agency performs k-means clustering on its own datasets for the Isolated case. The clusters are then forwarded to a central location that performs a weighted average. These averaged clusters are then shared back with the agencies who can use them to re-classify their 2-tuples."}, {"title": "Methodology", "content": "Problem Formulation\nWe formulate the allocation of the homelessness support problem as a multi-class prediction task. Consider a dataset $D$, which is a $m \\times n$ matrix where $m$ is the number of clients and $n$ is the number of features. The sleep pattern of an individual client is split into $T_0$ time bins in addition to two engineered features: total number of sleeps and total number of episodes. Therefore, each client would have an input feature vector $f$ with length $n = T_0 + 2$. Our task is to train a classifier $C(.)$, which produces the prediction results for a given $f$; $\\hat{y} = C(f)$, where $\\hat{y} \\in [chronic, episodic, transitional]$ refers to the output labels.\nTraining Details\nFig. 2 shows the training scenarios of the three models. Training for Centralized and Isolated scenarios involves the preprocessing steps described in the Data Preprocessing Section, dataset labeling as described in the Labeling Section, and updating the machine learning model as shown in Algorithm 2. The only difference is that Centralized and Isolated scenarios utilize linked and unlinked data described in the Data Preprocessing Section.\nOur proposed FL framework is based on the FedAvg algorithm, following the steps below:\n\u2022 Step 1: Apply Decentralized k-means from Algorithm 1 to generate the labels for the training data.\n\u2022 Step 2: The central server broadcasts the global model to $K$ available local agencies.\n\u2022 Step 3: Each agency updates and uploads its local model to the server after local training.\n\u2022 Step 4: The server aggregates all uploaded local models into the new global model for the next round. Repeat Steps 2-4 until the global model converges.\nThe pseudocode for the FL scenario is shown in Algorithm 3."}, {"title": "Evaluation", "content": "We explain the experimental setup of our 3 proposed methods: Centralized, Isolated, and Federated.\nExperimental Setup\nThe experiments were conducted in a simulated setup on an Apple M1 Pro MacBook with 16GB of RAM 1. The neural networks trained at each simulated agency were isolated to ensure privacy for isolated and federated models. There was no direct inter-agency communication except for controlled weight sharing between training rounds in federated cases. There was no communication for isolated cases.\nTrain/Test Splits\nWe apply a normalization process to the dataset using z-scoring . This results in a standardized score for each data point, reflecting how many standard deviations it is from the column average.\nFollowing the normalization, the dataset is divided into stratified training and testing sets, adhering to an 80/20 split. This strategy guarantees that both sets maintain proportional representations of clients across different agencies and labels.\nMachine Learning Models\nMulti-layer Perceptrons (MLPs) are a class of neural networks characterized by their stacked arrangement of fully connected layers, which makes them particularly adept at extracting and learning complex patterns from the input data. The MLP model used for this paper features three ReLU-activated hidden layers of sizes 512, 128, and 16, forming a structure capable of identifying and processing complex patterns within the dataset. This configuration, with its hierarchical design, is adept at capturing and learning the intricate relationships present in the input data. To facilitate model generalization and curtail overfitting, dropout layers with rates ranging from 0.1 to 0.4 are judiciously applied across the dense layers . In the final output layer, a sigmoid function is employed to handle multi-class classification.\nThe training for each model is performed as described in Algorithm 2. The models employed the Adam optimizer with a learning rate $\\eta = 0.02$. For loss calculation, categorical cross-entropy is selected, aligning with the multi-class nature of the output. Optimization is performed via mini-batch gradient descent with a batch size $B = 500$ and the models are trained for a total of 200 epochs ($\\varepsilon$).\nThe performance of the model can vary as a result of random weight initializations. Therefore, each MLP model was trained 10 times on different initializations for all three cases (federated, isolated, and centralized). The results for all 10 rounds were then averaged to get a more generalized picture of the results.\nThe aforementioned details apply uniformly across all three models, with the federated model incorporating additional parameters to tailor the training process. According to Algorithm 3, the federated model undergoes $T = 75$ communication rounds. With $K = 8$ each representing a unique agency contributing to the model. Furthermore, each client undergoes $\\mathcal{E} = 15$ training epochs on their local dataset for every round.\nHyperparameter Selection\nThe Data Preprocessing and Labeling Sections present some important hyperparameters that have a significant impact on model performance. Increasing the size of the observation window, $T_0$, will generally improve model performance. However, a long observation window is at odds with our objective of identifying at-risk HHSC users as soon as possible. Therefore, the smallest possible $T_0$ that yields acceptable performance must be selected.\nIncreasing the number of time bins, $T_b$, improves the model's ability to capture temporal trends and provide a better fit to the data. In general, a small value of $T_b$ will underfit and a large value of $T_b$ will overfit the data .\nComparing Centralized, Federated and Isolated Prediction\nUsing the hyper-parameters from the Hyperparameter Selection Section, Table 2 shows precision and recall for all three data scenarios averaged across 10 randomized initializations.\nAs expected, the Centralized scenario achieves the best performance in Table 2 due to the model being able to utilize a fully linked dataset. However, the Federated model achieves performance that comes within 4% of the Centralized model. This is excellent performance, especially since the federated methods avoid the privacy and technical barriers of linking data.\nLooking at Table 3 we can also see another advantage of using FL over the current local shelter implementation. The per class precision and recall for the FL case are in general much better than for local classes and fall in line with previous trends. This is quite important for this case because of the highly imbalanced nature of the dataset. As overall higher results shown before could have simply been attributed to a single class's performance improvement. However, it is quite obvious that clients in all three classes are seeing the benefits of this improvement making the model even more fairer to use compared to the current local implementations.\nPerformance achieved by the Isolated scenario is far below Federated and Centralized, especially in terms of recall. This shortfall can be explained by the per-agency Federated and Isolated scenario performance results presented in Table 4. Each row in this table includes the agency ID number, the number of individuals in that agency's dataset, and the precision and recall achieved by the Federated and Isolated scenarios.\nTable 4 demonstrates that it is the smaller shelters (shelters with capacity below 12,000) that perform particularly poorly in the Isolated case due to the very small size of their datasets. An exception to this trend is noted in the case of agency 213. Given its limited size (293 clients) and the consequently small test sample (approximately 70 clients), the results for this agency are deemed insufficient for a reliable evaluation of model performance and are considered an outlier.\nDiscussion\nBefore discussing the results in the Evaluation Section, it is important to provide context regarding how a machine learning tool should be utilized within an HHSC. As noted in the Introduction Section, machine learning should never be the only method for connecting people to housing. Instead, it is used as part of a range of programs for engaging with HHSC users , including \u201cdiversion\u201d programs specifically designed for making contact with new HHSC entrants.\nWhile improving the absolute performance of the models presented in the Evaluation Section is always desirable, these performance levels are acceptable if the models are used in the context of supporting a robust diversion program with human front-line staff. Performance could also be further improved if the methods in this paper are applied in an HHSC where other data features, such as demographic information, are collected.\nCentralized vs. Federated The results from Table 2 show that the centralized model outperforms the other two models. The Centralized scenario is ideal and may be achievable in some HHSCs that have a centralized agency capable of collecting data, as is the case in Calgary. However, most HHSCS are better represented by the Isolated scenario where agencies would need to operate independently without collaboration. The Federated approach, as proposed in this paper, is posited as a viable compromise, offering a balance between prediction performance and minimizing the amount of centralized IT infrastructure required. It can also be implemented without requiring the people using the HHSC to provide consent to share data across agency boundaries.\nAchieving Equity with Federated Learning One significant challenge within the HHSC network is the disparity in data analytics capabilities between large and small agencies. Smaller agencies often struggle with insufficient datasets. This limitation hampers their ability to identify at-risk clients accurately and tailor their services effectively. Looking at the results in Figure 3, it is quite clear that our federated model has a substantial gain in performance over the isolated case. A breakdown of the per agency performance keeping in mind the number of clients per agency in table 4 shows exactly where the federated model gains its performance. Notably, smaller shelters, those with fewer than 10,000 clients, which struggle to adequately train their models due to insufficient data, benefit markedly from the aggregated insights available through larger shelters. Conversely, larger shelters observed minimal performance gains, as their pre-existing data volumes were already sufficient to effectively train their models.\nThe federated model presents a solution to this problem by enabling smaller agencies to leverage larger, aggregated models trained across multiple data sources without directly sharing sensitive information. FL allows for the collective improvement of model performance, benefiting all participants in the network. When using machine learning for social good, equity is important. This means that all people accessing an HHSC should benefit from the same high-quality tools regardless of whether they choose to interact with a large or small agency. The fact that our proposed FL approach benefits smaller agencies, in particular, shows it is an important tool for achieving this equity goal.\nFederated models reduce the need for data linkage The conventional approach to enhancing machine learning models across different agencies involves the linkage of data to create a centralized dataset. This process not only raises significant privacy concerns but also involves logistical complexities and substantial resources dedicated to ensuring the secure and accurate merging of sensitive information. The federated model offers a compelling advantage by eliminating the necessity for direct data linkage. By training models on decentralized data and aggregating the learned models, FL circumvents the challenges associated with data consolidation. This approach significantly reduces the complexity and cost associated with data linkage, reallocating those resources toward improving service delivery and operational efficiencies. Moreover, FL upholds the privacy and confidentiality of individual records, addressing one of the critical barriers to data sharing among agencies."}, {"title": "Conclusion", "content": "In this work, a critical gap in current homelessness management strategies is addressed by using a machine learning approach that respects the privacy and autonomy of individual agencies while leveraging data from across the HHSC. In jurisdictions where raw data sharing across agencies is challenged by privacy concerns or logistical challenges, our proposed FL model demonstrated virtually the same performance as that of a centralized data approach. Compared to the status quo in the domain, our federated approach greatly enhances the ability of smaller shelters to make predictions and also promotes equity across the HHSC. By demonstrating the feasibility and effectiveness of federated learning in this context, this study lays the groundwork for future research and development of ML applications within the social services sector."}]}