{"title": "A CASE FOR AI CONSCIOUSNESS: LANGUAGE AGENTS\nAND GLOBAL WORKSPACE THEORY", "authors": ["Simon Goldstein", "Cameron Domenico Kirk-Giannini"], "abstract": "It is generally assumed that existing artificial systems are not phenomenally\nconscious, and that the construction of phenomenally conscious artificial systems\nwould require significant technological progress if it is possible at all. We\nchallenge this assumption by arguing that if Global Workspace Theory (GWT)\na leading scientific theory of phenomenal consciousness \u2014 is correct, then\ninstances of one widely implemented AI architecture, the artificial language\nagent, might easily be made phenomenally conscious if they are not already.\nAlong the way, we articulate an explicit methodology for thinking about how\nto apply scientific theories of consciousness to artificial systems and employ\nthis methodology to arrive at a set of necessary and sufficient conditions for\nphenomenal consciousness according to GWT.", "sections": [{"title": "1 Consciousness", "content": "The ordinary concept of a conscious state or being is imprecise, failing to differentiate among a\ndiverse set of properties. For this reason, it is customary to distinguish between two important\nsenses in which a state or being can be conscious: access consciousness and phenomenal con-\nsciousness. A state is access conscious to the extent that it is available for use in reasoning and\nguiding speech and action, while it is phenomenally conscious just in case it has experiential\nproperties (Block 2002). Correspondingly, a being is conscious in the access sense to the extent\nthat it tokens access-conscious states and conscious in the phenomenal sense just in case it tokens\nphenomenally conscious states.\nWhile there are interesting scientific questions to ask about the kinds of biological and artificial\nfunctional architectures which give rise to access-conscious states, it is phenomenal conscious-\nness which is generally regarded as the greater puzzle both scientifically and philosophically.\nSubstantial literatures exist in philosophy, psychology, and cognitive neuroscience on the nature"}, {"title": "2 Global Workspace Theory", "content": "Global Workspace Theory originated with the work of Bernard Baars (Baars 1988, 1997b) and\nhas since grown to be one of the leading theories of consciousness in biological systems. GWT\nposits the existence of a global workspace: a component of cognitive architecture that receives\ninformation from a range of cognitive modules, processes this information, and then broadcasts\nthe processed information back to the cognitive modules to which it is connected. According\nto GWT, a representation is conscious when it is present in the global workspace.5 Though\nproponents of GWT are not always clear on the sort of consciousness they have in mind, we\ninterpret GWT as endorsing the claim that information in the global workspace is conscious in\nboth the access and phenomenal senses, and that being in the global workspace is what renders a\npiece of information phenomenally conscious.\nIt is helpful to distinguish the various claims associated with GWT into structural claims about\ncognitive architecture and functional claims about how the global workspace receives information,\nprocesses it, and transmits it to other parts of the cognitive system. The core structural claim of\nGWT is that human cognitive architecture consists of a number of relatively autonomous modules\nwhich process information specific to particular tasks (e.g. particular sense modalities, motor\ncontrol, and so on) together with a single global workspace with which all of these modules\ninterface. The cognitive architecture posited by GWT thus contrasts with a pairwise architecture,\nwhere modules communicate with one another directly rather than pooling their information into\na shared space (Goyal et al. 2021)."}, {"title": "2.1 Uptake", "content": "At any given time, parallel processing in the modules is generating a large amount of information\nwhich could potentially enter the global workspace. Baars (1997b) emphasizes three types of\ninformation: outer senses, inner senses (including visual imagery, inner speech, and dreams), and\nideas. Because the global workspace has limited capacity, however, only some of this information\ncan be taken up from the modules at any given time. There is therefore a kind of competition for\ninformation to achieve uptake into the global workspace and to remain there once it does.\nOne important example of competition for uptake is binocular rivalry, one instance of a more\ngeneral class of \u2018two-channel experiments'. In binocular rivalry, a subject is shown a different\nimage in each of their eyes. The subject's conscious experience flips back and forth between\nthe two images (Moreno-Bote et al. 2011). Crucially, the subject cannot have simultaneous\nconscious experiences of inconsistent visual information. This suggests that there is a bottleneck\non conscious experience. Consciousness selects among competing visual inputs to generate a\ncoherent narrative about the world.\nBaars (1988) suggests a number of ways in which this competition might be implemented in the\nmind. First, informational signals coming from different modules might have different levels of\nactivation, and activation might be increased when different modules work together to boost a\nparticular signal (Baars (1988); p. 95). However, Baars holds that activation alone is not sufficient\nto get information stably into the global workspace: in addition to high levels of activation,\ninformation in the global workspace must receive positive feedback from the modules to which it\nis broadcast (Baars (1988); p. 205)."}, {"title": "2.2 Broadcast", "content": "The most important function of the global workspace is to serve as a central repository of\ninformation available to the cognitive architecture's various parallel processors. In addition to\ntaking up information from these processors, then, the global workspace must also transmit\ninformation back to them. We refer to this function of the global workspace as broadcast.\nAs we have seen, Baars (1988) holds that there is an intimate connection between uptake and\nbroadcast: it is only through broadcasting to a coalition of processors and receiving positive\nactivation signals from them that a representation can remain stably in the global workspace.\nIn addition to broadcast from the global workspace to processors that handle functions like\nperception and memory, broadcast from the global workspace also drives intentional action.\nBuilding on James (1890)'s ideomotor theory of action, Baars (1997b) (p. 138) suggests that\nwhen a goal is conscious, it will automatically begin to recruit unconscious \u201ceffectors\u201d to promote\nthe goal and cause action, unless inhibited by the conscious representation of a conflicting goal.\nThe process of broadcast from the global workspace is thus the means by which serial processing\nof perceptual and other inputs leads to coordinated agency."}, {"title": "2.3 Processing", "content": "According to GWT, the global workspace is not simply a passive repository of information\navailable to various cognitive modules; it plays an active role in processing information. Pro-\ncessing in the global workspace is notable for its generality. As Dehaene et al. put it, \u201conce we\nare conscious of an item, we can readily perform a large variety of operations on it, including\nevaluation, memorization, action guidance, and verbal report\" (Dehaene et al. (1998); 14529).\nIndeed, there are some kinds of processing that can only occur in the global workspace. For\nexample, Baars (1997a) (p. 17) suggests that conscious processing in the global workspace is\nrequired to construct concepts out of two-word compounds like potato soup. This is illustrated\nexperimentally by priming effects. Unconsciously seeing the word dog makes it easier to identify"}, {"title": "3 What is Essential to Consciousness? Existing Work", "content": "So far, we've surveyed a range of claims that GWT makes about consciousness in humans. But we\nhaven't said precisely which properties of conscious human systems are necessary and sufficient\nfor consciousness. This is a difficult question. Several authors have noted that it is tricky to\nsay exactly how similar a system needs to be to a human global workspace in order to count as\nconscious (Carruthers 2019; Birch 2022; Butlin et al. 2023). More permissive conceptions of\nconsciousness, according to which conscious systems might be less like the human brain, will\nmake it easier for AI systems to be conscious, while less permissive conceptions, according to\nwhich conscious systems must be more like the human brain, will make it harder.\nAs we noted above, we think about these questions through a computational and functionalist\nperspective on consciousness. The idea here is that there is some functional role associated with\nconsciousness, so that all and only systems instantiating this functional role are conscious. More\ncarefully, we can also think about a necessary and sufficient functional role for a particular mental\nstate to be conscious, and then say that a system is conscious when one of its mental states is\nconscious.\nIt is consistent with this methodology that the relevant functional role is not metaphysically\nnecessary and sufficient for consciousness. Rather, for us what is relevant is that the functional\nrole in question be nomically necessary and sufficient for consciousness. This distinction allows\nus to sidestep tricky questions about phenomenal zombies hypothetical beings that are physi-\ncally like conscious systems but lack conscious experience. Perhaps phenomenal zombies are\nmetaphysically possible, and so for any functional role, it is metaphysically possible to satisfy\nit without being phenomenally conscious. Nonetheless, there could still be functional roles that"}, {"title": "4 What is Essential to Consciousness? Theoretical Choice Points", "content": "The existing proposals described in the previous section raise a number of broader questions\nabout how to think about consciousness in AI systems in the context of GWT. We highlight some\nof these questions in this section before arguing for our considered view in the next.\nA first question is how to functionally capture GWT's notion of attention. Butlin et al. (2023)\nsuggest that consciousness requires \u201ctop-down\u201d attention, where the information in the workspace\ncan control what further information enters the workspace, in addition to \u201cbottom-up\u201d attention,\nwhere the strength of the signals from various modules determines which information enters the\nworkspace. In contrast, though VanRullen and Kanai mention this distinction between top-down\nand bottom-up attention, they do not build it into their conditions on phenomenal consciousness.\nA second question concerns the strength of the broadcast condition. Whereas (J1) simply requires\nthat the workspace interacts with some modules, (B3) and (VK4) require that information in the\nglobal workspace is broadcast to every single module. Here, one especially relevant sub-question\nis whether information in the workspace must be sent back to the perception modules, which\nwould enable top-down perceptual processing.\nA third question concerns the status of information processing in the global workspace. VanRullen\nand Kanai do not require the global workspace to engage in processing at all, whereas Juliani et\nal. do but remain unspecific about the nature of this processing. Butlin et al. do not clarify how\nmuch processing is required for state-dependent attention, so it is not clear where they come down\non this issue. One might require that the workspace engage in very specific types of processing\nfamiliar from human working memory; for example, the presence of a visuospatial sketchpad and"}, {"title": "5 What is Essential to Consciousness? A Theory", "content": "How do we assess the competing proposals of section 3 and answer the thorny questions of section\n4? In this section, we consider two strategies: first, appealing to the ways in which consciousness\nis advantageous to systems that possess it; second, employing thought experiments about which"}, {"title": "5.1 The Value of Consciousness", "content": "A first way of approaching the problem of distilling a set of necessary and sufficient conditions\nfor consciousness from GWT is to consider what the theory says about the usefulness of con-\nsciousness. What purpose does consciousness have in systems that possess it? We might find that\nconsciousness is steered towards solving particular kinds of problems, but that only some and\nnot all of the candidate functional roles sketched in the previous two sections are necessary in\norder to solve those problems. In general, this approach assumes that phenomenal consciousness\nis causally efficacious, connecting in lawlike ways to the cognitive and even physical behavior of\nthe organisms that possess it. The methodology is to hold that a property is part of the functional\nrole of consciousness according to GWT just in case that property plays a suitably central role in\nexplaining how having a global workspace helps systems solve certain canonical problems.\nWhat problems does consciousness help conscious systems solve? Baars (1997b) describes a\nnumber of information processing tasks that consciousness facilitates on the GWT picture; here,\nwe distill them into four. First, consciousness helps to classify inputs to central cognition by\nprioritizing more important information over less important information. Second, consciousness\nenforces the coherence of the information being processed in central cognition. Third, conscious-\nness helps to coordinate cognition by recruiting a range of unconscious tools to solve a single\nproblem. Fourth, consciousness allows for correction of errors in reasoning. We suggest that\nwhen we think about consciousness in terms of serving these purposes, we should favor more\npermissive approaches to its functional role.\n1. Classification. Any cognitively sophisticated agent faces the problem of deciding which\ninformation is important and relevant to its current situation and which is not. Solving\nthis problem is crucial for reducing the computational complexity of reasoning about\nhow to act. As Dehaene et al. put it:\n\u201cThe organization of the brain into computationally specialized subsystems is\nefficient, but this architecture also raises a specific computational problem: the\norganism as a whole cannot stick to a diversity of probabilistic interpretations;\nit must act and therefore cut through the multiple possibilities and decide in\nfavor of a single course of action. Integrating all of the available evidence\nto converge toward a single decision is a computational requirement that, we\ncontend, must be faced by any animal or autonomous AI system\u201d (2021; 3).\nBy its nature, the information bottleneck induced by the global workspace requires a\nconscious system to classify some information as more important than other information\nit is only the information that makes it from the modules into the global workspace\nthat is broadcast to the entire system and available for further processing.\n2. Coherence. A related function of the global workspace is to create a coherent narrative\nabout the world on the basis of potentially inconsistent representations. In the process of\nselecting information for uptake using attention, the global workspace works to produce\nconsistent interpretations. We saw this demand for coherence in cases of binocular\nrivalry and other two-channel experiences, as well as in the conscious experience of\nperceptual illusions like the impossible trident. On the present picture, coherent conscious\nexperience is a tool for facilitating effective agency. If our action-guiding representations\nof the world change dramatically from moment to moment, we will struggle to form and\nexecute effective plans over time."}, {"title": "5.2 Thought Experiments", "content": "A second way of approaching the question of what it would take for an artificial system to be\nconscious according to GWT is to consider thought experiments where a candidate condition is\nremoved from a conscious system and we judge whether that system is still conscious.\nFirst, consider the fact that working memory in human beings is limited to just a few items (seven\nor less). It would be easy to design AI systems with much larger working memories. But the very\nsmall size of human working memory is not plausibly essential to consciousness. Imagine that a\nperson's working memory expanded from seven to ten thousand items. Intuitively, they would not\nthereby go from being conscious to being unconscious. Instead, they would be superconscious,\nconscious of vastly more."}, {"title": "6 Language Agents", "content": "In the rest of the paper, we'll apply (1)-(4) to a particular type of AI system: the language\nagent. Language agents are created by embedding an LLM into a functional architecture with the\nstructure of an agent that acts predictably according to the laws of folk psychology. The LLM\nperforms all of the relevant information processing of the system, while the functional architecture\nensures that this information processing produces coherent agentic behavior. Below, we'll focus\non the language agents developed by Park et al. (2023) as a case study. But there are numerous\nother examples of language agents, including AutoGPT13, BabyAGI14, Voyager15, SPRING16,\nand others17.\nLanguage agents record and store their beliefs, desires, and plans, as well as their perceptual\nobservations, in natural language. The functional architecture with which a language agent is\nprogrammed specifies how these sentences recording beliefs, desires, plans, and observations are\nfed into the LLM as it considers how the agent will act. Indeed, it is the roles assigned to different\nstored sentences by the architecture of the language agent which make it the case that they count\nas the agent's beliefs, desires, and so forth. In this respect, the structure of a language agent\nmirrors the structure of the human mind according to cognitive theories which posit a distinction\nbetween the content of a representation and its functional role. What makes a representation with\nthe content I am eating a desire rather than a belief in the human mind, according to such theories,\nis the way in which it enters into an agent's broader cognitive economy, and especially action\nplanning. 18\nCognition requires not only moving and storing information, but also processing it. The\ninformation-processing role in a language agent is played by its LLM. Different cognitive tasks\nwithin the language agent may rely on the LLM in different ways. For example, the route from\nperception to memory might require the LLM to summarize a text description of the agent's\nobservations to highlight those aspects worth recording, while planning action might require the\nLLM to reason about what course of action would be rational given the agent's beliefs and desires.\nWhile the language agents in which we are most interested rely more or less exclusively on a\nsingle LLM for cognitive processing, for our purposes it would not matter if they relied on some\ncombination of different LLMs, or even if some of their cognitive capacities were realized by\nhand-coded algorithms.\nFor concreteness, let us return to the language agents developed by Park et al. (2023). Park et\nal.'s agents live in a text-based simulation called \"Smallville\". They observe and interact with\ntheir simulated environment and each other via text descriptions of what they see and how they\nchoose to act. Each agent's perceptual observations are stored in a text file called the memory"}, {"title": "7 Language Agents Could Easily Have Conscious Experiences", "content": "We have seen that, while Park et al.'s language agents contain a centralized cognitive module\nwhich stores and manipulates information in the manner of a global workspace, it is less clear\nthat they contain a series of input modules to this workspace whose representations must compete\nfor entry into it. In this section, we describe how the architecture of Park et al.'s language agents\ncould be changed to more closely mirror the structure of a conscious system according to GWT.\nNone of the changes we consider will affect how the LLM underlying a language agent is trained\nor structured. Rather, all of the changes pertain to how that LLM is scaffolded to produce an\nagent. In this way, once we have an LLM that can process information, consciousness will depend\non how that information processing is exploited in systematic, lawlike ways by hard-coded rules.\nTo begin, note that Park et al.'s language agents in fact have representations of several kinds\n(beliefs, desires, plans) and engage in a range of forms of cognitive processing including assigning\nimportance scores to memory items, reflection, assigning relevance scores to memory items during\nretrieval, plan formation and revision, and practical reasoning leading to action. Park et al. make\nthe architectural choice to store all these kinds of representations in the same cognitive workspace,\nthe memory stream, and to have all cognitive processing take as input a series of items from the\nmemory stream. But language agents could also be built with many of the same features arranged\ninto a different architecture.\nImagine, for example, that we modify Park et al.'s architecture in the following ways. First,\ninstead of a single memory stream containing perceptual inputs, other beliefs, desires, and plans,\nwe have a central workspace connected to three further cognitive input modules: a perception\nmodule, a belief module, and a desire-and-plan module. Each of these three modules stores\nrepresentations of the appropriate type. The central workspace can store representations of any\ntype.\nSecond, imagine that the three input modules perform the following information processing tasks\nin parallel. The perception module receives observations from the environment and assigns them\nsalience ratings corresponding to how relevant it judges them to be to the agent's likely future\ncognition. The belief module assigns each belief an importance score and engages in reflection-\ndriven inference in the same way as Park et al.'s agents. The desire-and-plan module assigns each\ndesire an importance score and engages in a desire-based analog of reflection: generalizing new\ndesires from the agent's existing list of desires."}, {"title": "8 Potential Modifications", "content": "So far, we have argued for a particular view of the necessary and sufficient conditions GWT\nimposes on phenomenally conscious cognitive systems and described the architecture of a type of\nlanguage agent which satisfies these conditions. This completes our core case for the near-term\npossibility of phenomenally conscious artificial systems if GWT is true.\nWe are aware, however, that not everyone will agree with us on our characterization of the\nconditions GWT imposes on phenomenally conscious artificial systems. In this section, we\nintroduce and respond to some further conditions which might be thought relevant in this context.\nFirst, according to both GWT and the global neuronal workspace hypothesis, information in\nthe global workspace does not stay there indefinitely. Instead, it must compete with incoming\ninformation from the modules, and, if it loses this competition, it is replaced by that incoming\ninformation. To put it slightly differently, one might worry that information in the global\nworkspace needs to be actively maintained there. This feature of GWT is not represented in the\narchitecture we described in the previous section.19\nIt is not difficult to modify the architecture described above to address this concern. Imagine that\neach time the competition function is called, the set of representations it generates serves as input\nto a second function, which we might call the refresh function. The refresh function takes as\ninput the current contents of the central workspace and the output of the competition function.\nIt works by assigning each of the current contents of the central workspace a score based on its\nimportance, relevance to the agent's current situation, and recency (that is, a score of the same\ntype as those assigned by the competition function) and then discarding any current workspace\ncontents whose score falls below some threshold (for example, items which score lower than the\nmedian of the outputs of the competition function)."}, {"title": "9 Objections", "content": "Before concluding, we'll consider two related objections to our approach. First, there is the small\nmodel objection. This claims that the functional roles we have considered must not be sufficient\nfor consciousness because they can be realized by extremely simple systems. Second, there\nis the within/between objection, which claims that we have conflated two different questions:\nwhat distinguishes consciousness from unconsciousness within a single organism, and what\ndistinguishes consciousness from unconsciousness between organisms.\nAccording to the small model objection, the functional role we have considered cannot be\nsufficient for consciousness because the distinction between global and local processing can be\nmade in simple neural nets that only have on the order of magnitude of ten neurons (Herzog\net al. 2007). In particular, we could have two sensory neurons that initially fire in response\nto distinct inputs. Each such neuron could be a \u201cperceptual module\". These could feed into\na \"global workspace\" neuron system, which could consist of one input neuron that controls\n\u201cignition\" and another that handles \u201cprocessing\u201d. This processing neuron could then \u201cbroadcast\u201d\ninto an \"action module\" neuron that moves a robotic limb, and it could also send information\nback to the two perceptual modules. Such a five-neuron system might come close to satisfying\nour functional conditions on consciousness. And we might be able to completely satisfy our\nfunctional conditions by adding just a few more neurons to the system to allow for slightly more"}, {"title": "10 Conclusion", "content": "This paper has approached AI consciousness from an architectural perspective. We have identified\na series of functional conditions that may be necessary and sufficient for consciousness. Then\nwe have considered the designed architecture of a few AI systems and considered whether this\narchitecture exhibits the relevant functional conditions.\nIn closing, we'd like briefly to mention another approach to AI consciousness that is still broadly\nin the spirit of GWT. On this second approach, we test for AI consciousness by focusing directly\non the behavioral evidence that cognitive scientists have identified as best explained by a global\nworkspace architecture. For example, we might search for analogues of binocular rivalry, the\nattentional blink, priming effects, and so on. These phenomena motivate core features of GWT,\nsuch as the information bottleneck, modularity, and central processing. With this data in hand, we\ncould say that an AI system is likely to be conscious when it exhibits proper analogues of the\nbehavioral features that have motivated GWT in humans. We hope that future work will critically\nexplore both methodologies, in particular determining whether they make different predictions\nabout any Al systems."}]}