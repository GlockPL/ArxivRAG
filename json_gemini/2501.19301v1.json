{"title": "Beyond checkmate: exploring the creative chokepoints in AI text", "authors": ["Nafis Irtiza Tripto", "Saranya Venkatraman", "Mahjabin Nahar", "Dongwon Lee"], "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities. This rapid advancement has spurred research into various aspects of LLMs, their text generation & reasoning capability, and potential misuse, fueling the necessity for robust detection methods. While numerous prior research has focused on detecting LLM-generated text (AI text) and thus checkmating them, our study investigates a relatively unexplored territory: portraying the nuanced distinctions between human and AI texts across text segments. Whether LLMs struggle with or excel at incorporating linguistic ingenuity across different text segments carries substantial implications for determining their potential as effective creative assistants to humans. Through an analogy with the structure of chess games comprising opening, middle, and end games-we analyze text segments (introduction, body, and conclusion) to determine where the most significant distinctions between human and AI texts exist. While AI texts can approximate the body segment better due to its increased length, a closer examination reveals a pronounced disparity, highlighting the importance of this segment in AI text detection. Additionally, human texts exhibit higher cross-segment differences compared to AI texts. Overall, our research can shed light on the intricacies of human-AI text distinctions, offering novel insights for text detection and understanding.", "sections": [{"title": "1 Results", "content": "Motivated by the chess middlegame analogy, we analyze the differences between human and AI text across the introduction, body, and conclusion segments (segment comparison) and cross-segment differences between human and AI text (source comparison). Our study focuses on three domains: news articles (Reuters), emails (Enron), and essays (Persuade). The Methodology section details our dataset construction, text segmentation, feature extraction, and statistical analysis procedures for segment and source comparisons. Our code and datasets are available at https://github.com/tripto03/chess_inspired_human_ai_text_distinction.\nLike chokepoints in chess, we examine LLM text generation, reflected through stylometric and psycholinguistic features, to determine if patterns in human text are mirrored in AI text. Our source and segment comparisons reveal whether feature differences are consistently replicated across sources and segments. For instance, both human and AI texts exhibit higher vocabulary richness and readability in the body segment compared to the introduction and conclusion. However, the human-AI difference in vocabulary richness is more pronounced in the body segment (segment comparison). Additionally, the cross-segment difference (introduction-body and other pairs as well) in vocabulary richness is higher in human text (source comparison). We analyze individual features across all possible combinations to address source and segment comparisons, with key results summarized in Table 1.\nIn our original experimental setting (E), segment comparisons reveal that the body segment shows less difference between human and AI text than the introduction and conclusion. However, the body segment's greater length can dilute the impact of syntax features like POS-tag or named entity tag distributions, which may vary significantly in shorter segments due to a few unusual word choices. Similarly, opinion-based features such as sentiment or formality scores tend to average out over the longer body segment, reducing observable human-AI differences. The extended length also makes it easier for AI text to align with the content of human text in the body segment. However, length-independent features like vocabulary richness and perplexity indicated higher differences in the body.\nThis observation led us to conduct length-controlled studies (C1 and C2), which support our initial chess analogy. In these settings, stylometric features (LIWC, Writeprints) and others (e.g., vocabulary richness, readability scores, sentiment) show higher differences in the body/middle segment. Also, when segment lengths are similar, certain features show no significant differences across segments. The body segment often contains core arguments, detailed elaboration, or creative storytelling in writing [26]. Therefore, our findings suggest that, while its length allows LLMs to average out distributional language patterns, the creative and adaptive demands of this segment ultimately reveal detectable inconsistencies when length is accounted for, as validated through human vs. AI text detection in the next subsection.\nFor source comparison, our findings show that human text exhibits higher cross-segment variation than AI text, offering an innovative lens to differentiate between the two. While these findings complement observations from existing studies [19, 35],"}, {"title": "1.2 Checkmating AI text: which segment reveals its origins?", "content": "A crucial focus in current LLM research is the detection of LLM-generated text, resulting in an ongoing \"arms race\" between the development of LLMs and AI text detectors. In this context, our study aims to explore how different text segments contribute to AI text detection. Since creating a new AI text detector is not the objective of our work, we rely on existing, well-established detectors for analysis. We employ prominent zero-shot detectors such as GPT-Zero [14], ZipPy [36], RoBERTa base OpenAI Detector [37], MAGE: Machine-generated Text Detection in the Wild [38], GPT-Who [39], and a fine-tuned language model (BERT) used as a classifier (details of each detector are provided in the Appendix). Our primary interest lies in understanding the significance of various text segments-introduction, body, and conclusion-in distinguishing human text from AI text. Therefore, we apply these detectors across the entire text, as well as on individual segments, the combination of introduction & conclusion (since these segments are often shorter than the body in the original experimental setup). Additionally, we evaluate the performance of a voting mechanism from each segment's results. Table 4a summarizes the results for our datasets under these different conditions.\nOverall, our findings indicate that using the entire text yields better performance in most scenarios, except for the email dataset. This is intuitive, as emails typically feature greetings in the introduction and closing remarks in the conclusion, leaving the body as the most content-rich segment. Thus, focusing solely on the body segment enhances detection performance for email. Additionally, when analyzing individual segments, the body consistently plays a more significant role in AI text detection compared to the introduction and conclusion combined. Interestingly, applying a voting mechanism across the individual segments did not lead to substantial performance improvements. It could be due to the inherent redundancy in the information across segments or the dominance of features from the body segment, overshadowing the contribution of the introduction and conclusion, resulting in limited additional insights gained through voting. Another key observation is that fine-tuned classifiers consistently benefit from analyzing the complete text, as they leverage more data during training. The Roberta base OpenAI detector does not perform well, likely due to its initial fine-tuning on GPT-2 outputs, while LLMs have significantly advanced since then. Similarly, ZipPy's performance is limited by its reliance on text compression as an indirect measure of perplexity rather than direct perplexity calculation. While we observe comparative performance differences between detectors, this is not the primary focus of the study, and thus we refrain from a detailed discussion on this aspect.\nWhile our results suggest that AI text detection performance is higher in the body segment, indicating that the distinction between human and AI text is more pronounced in this segment to a detector, it is essential to consider that the body segment"}, {"title": "1.3 Segmental analysis of human and AI chess moves", "content": "As our study was inspired by the chess middlegame analogy, a concept extensively discussed in chess literature [3, 4], we investigate whether differences between human players and AI chess engines are more pronounced in the middlegame from a computational perspective. To this end, we conduct a concise computational analysis of chess games to explore how human-AI differences manifest across game phases. While not the central focus of our research, this exploration bridges the conceptual parallels between text segmentation and game phases, offering deeper insights into the nuanced dynamics of human and AI behavior.\nChess opening, middlegame, and endgame moves are widely recognized as distinct phases, each with its own strategic patterns [42, 43]. Specific opening patterns, such as the Italian Game, Queen's Gambit, King's Indian Attack, as well as endgame strategies like the Centralized King Strategy or Rooks Battery, exemplify the structured nature of these phases. Given this inherent variation, we examine the differences in moves between human and AI chess players across various game phases. For our analysis, we utilize approximately 166K games between human and AI players from the FICS Games Database, segmenting them into three phases and extracting features from the moves (detailed in the methods section).\nTo quantify the divergence between human and AI chess players across different game phases, we compute the Jensen-Shannon Divergence (JSD) between the feature sets of human and AI moves in the opening, middle game, and endgame phases. As shown in Fig 2a, the middlegame exhibits a statistically significant (\u03b1 = 0.05) increase in JSD, indicating a greater disparity between human and AI moves in this phase compared to the opening and endgame. Additionally, the middlegame shows a broader range of JSD values, reflecting greater variability in the differences between human and AI players during this phase. Furthermore, we calculate the Jaccard similarity between the unique moves (ECO codes) made by human and AI players across these three game phases. Notably, the middlegame exhibits lower similarity compared to the opening and endgame. Therefore, our results suggest that human and AI chess strategies diverge most substantially during the middlegame while converging more", "latex": "\\alpha"}, {"title": "2 Discussion", "content": "As LLMs have become ubiquitous in text generation and modification, the need for robust AI text detection and an understanding of their differences from human-authored text has received substantial attention from the research community. Recent studies have explored various techniques for detecting AI-generated text [12-16] and examined linguistic distinctions between human and AI text [18-20, 22]. However, our research offers a novel perspective by identifying subtle differences between human and AI texts across specific text segments, an area that remains largely unexplored. Understanding LLM performance in incorporating linguistic creativity within these segments has important implications for assessing their potential as creative tools and contributing valuable insights to the ongoing development of AI text detection methods.\nDrawing parallels from chess game phases, we conduct a thorough evaluation of linguistic features analogous to chess \"chokepoints\" and explore how they vary in each segment between AI and human text. We select three domains-news articles, emails, and essays-that commonly exhibit the introduction-body-conclusion structure. Recognizing the inherent subjectivity of text segmentation, we employ an LLM for this task and demonstrate strong agreement with human judgment and other computational segmentation methods. We conduct both segment and source comparisons to perform the statistical significance tests. Our segment comparison identifies the specific textual portion where human and AI writing diverges significantly, providing a crucial focal point for detection efforts. Similarly, our source comparison introduces a new dimension to this distinction by examining cross-segment feature variation.\nWhile we observe a higher resemblance between AI and human text in the body segment, this similarity is primarily attributable to its greater length [28]. However, length-independent features and those that assess the continuous properties of text, such as vocabulary richness and perplexity, exhibit higher divergence in the body segment. To account for this, we conduct two additional experimental setups for segment comparison: dividing texts into three equal segments and sampling a continuous"}, {"title": "3 Methods", "content": "We compile datasets from three domains, each containing human-authored texts paired with corresponding LLM-generated versions. Our study includes four LLMs: Chat- GPT (gpt-3.5-turbo) from OpenAI, PaLM (text-bison-001) from Google, LLaMA2 (llama2-chat-76) from Meta, and Mistral_76 from Mistral AI. This selection includes open-source and proprietary models from diverse organizations to ensure a comprehensive evaluation. For the essay domain, we use the Persuade corpus [44], consisting of argumentative essays written by US students (grades 6-12) across different prompts. Our dataset includes approximately 1700 human samples and corresponding LLM generations from the Kaggle competition [45]. For news, we employ the Ghostbuster dataset [46], which contains Reuters articles and existing LLM generations (we generate missing samples using identical prompts). For emails, we use a subset of the Enron dataset [47], selecting users with at least 10 emails and excluding excessively short or long emails, as well as those with attachments. We then prompt the LLMs to generate responses based on the original email's header, sender/recipient information, and a concise summary of its content. The corresponding prompts are detailed in the Appendix. We conduct thorough sanity checks to ensure dataset integrity.\nSegmenting text into introduction, body, and conclusion is inherently subjective [48, 49], as these sections often lack clear boundaries and vary significantly across writing styles, domains, and contexts. Manual annotation of our large dataset would thus be prohibitively expensive and time-consuming. However, recent advances in LLMs have shown their remarkable performance in natural language understanding tasks, such as data annotation and judgment, often achieving human-level performance [50-52]. Therefore, we employe gemini-1.5-flash (an LLM not included in our authorship analysis, mitigating potential bias) to segment texts in our original setting (E). Since the body segment is naturally longer in most texts [25, 53], we also explore length-controlled segmentation. In setting C1, we divide the text into three approximately equal segments, while in setting C2, we extract a continuous portion of the body closest in length to the average introduction and conclusion of each text. In all scenarios, we ensured that segment boundaries fell between complete sentences, as splitting sentences disrupts semantic coherence and comprehension [54, 55].\nGiven the subjective nature of text segmentation, we demonstrate that our LLM-based approach is robust and aligns well with alternative methods. We use the Segmentation Similarity Score [56] (ranging from 0 to 1, where 1 indicates identical segmentation) to evaluate the division of text into three segments based on sentence counts. To validate our approach, we consider a subset of 300 samples (across all domains). Two human annotators (authors of this paper) segment this subset to assess alignment with human perception. Additionally, we segment the same subset using another LLM (GPT-4) to evaluate consistency between LLMs."}, {"title": "3.3 Feature extractions", "content": "We extract various features from each text segment, including traditional stylometric feature sets such as LIWC (Linguistic Inquiry and Word Count), which provides psycholinguistic characteristics [23], and Writeprint features, which capture an author's distinctive stylometric patterns in communication [24]. Both feature sets are extensively used in authorship analysis, digital forensics, and stylometry [17, 62]. Additionally, we examine how specific features vary across different segments and sources. Therefore, we include several individual lexical, syntactic, fluency, contextual, and text perplexity-related features, offering a comprehensive analysis of the text's stylistic and structural attributes.\nWe employ several feature extraction methods to capture distinct linguistic properties. For vocabulary richness, we consider the Brun\u00e9t Index [63], as it is less sensitive to text length than the type-token ratio (TTR), making it more suitable for segments of varying lengths. Syntactic features include part-of-speech (POS) tags, named entity recognition (NER), and stopword distributions, extracted using SpaCy [64]. To assess fluency and opinion across segments, we calculate readability scores (Flesch Reading Ease [65]), average sentiment scores, subjectivity score and formality scores [66]. For content analysis, we use OpenAI text embeddings (text-embedding-ada-002) to capture the content within segments and measure the variation in embeddings between consecutive sentences. Finally, to evaluate text predictability, we examine perplexity (and its token-level list), which indicates how well a language model predicts the next word, and burstiness, which measures variation in sentence structure and word choice, features shown to be effective in recent AI text detection research [12-14].\nFor the next step, to conduct statistical significance tests for each feature, we first define a difference measure, denoted as \u0394, between two feature values. These features can be categorized as either scalar values (e.g., vocabulary richness, readability score, avg. sentiment score) or distributions (e.g., POS-tag distributions, stopword distributions, LIWC features). For scalar features, we use the absolute difference as the measure of difference. For distributional features, we employ Jensen-Shannon Divergence (JSD) [67] as the distance metric due to its symmetry, bounded range (0-1, where 0 indicates identical distributions and 1 represents maximum difference), and suitability for comparing discrete probability distributions [68]. For features represented as vectors but not summing to 1 (e.g., perplexity scores and contextual embeddings), we utilize correlation distance and cosine distance, respectively. Correlation distance", "latex": "\\Delta"}, {"title": "3.4 Statistical significance test", "content": "As we are interested in understanding how linguistic features differ between human and AI texts across textual segments (Introduction, Body, Conclusion), we design statistical significance tests with feature values extracted from each segment. Specifically, we conduct separate statistical tests for each linguistic feature (Table 1). Given two text sources (Sources, H: Human, A: AI) and three segments from each text (Segments, I: Introduction, B: Body, C: Conclusion), we define Zx as an individual feature from segment x for source Z.\nFor source comparison tests, we consider pairwise segments, x,y \u2208 {I,B,C'}, compute their differences for human and AI texts, \u0394(Hx, Hy) and (Ax, Ay), respectively. Then, we address the key question, whether \u2206(Hx, Hy) differs significantly from \u0394(Ax, Ay) for any segment pair. We conduct a two-way ANOVA test (\u03b1 = 0.05) [72] focusing on the interaction effect of source (H vs. A) and cross-segment differences. If the interaction effect is significant, we proceed with post-hoc pairwise comparisons using the Wilcoxon signed-rank test. We opted for Wilcoxon signed-rank tests instead of t-tests due to the robustness to non-normal distributions [73]. These pairwise tests reveal whether human cross-segment differences (Hx, Hy) are statistically greater than (>), less than (<), or comparable (~) to AI cross-segment differences \u0394(Ax, Ay), for specific segment pairs. If no significant interaction effect is found in the ANOVA test, we infer that cross-segment differences between human and AI texts are not statistically meaningful.\nSimilarly, for segment comparison, we compute the difference between human and AI texts for all three segments, \u0394(\u03971, \u0391\u2081), \u2206(\u0397\u0392, \u0410\u0432), and \u2206(Hc, Ac). Then, we conduct a one-way ANOVA test (\u03b1 = 0.05) with the three measures. If the result is statistically significant, we perform post-hoc pairwise comparisons between \u0394(\u0397, \u0391x) and (Hy, Ay) for all segment pairs x, y \u2208 {I,B,C'}. The post-hoc tests determine whether the human-AI feature difference is more pronounced in a specific segment or whether the differences are statistically indistinguishable across segments. If the ANOVA test shows no significant effects, we conclude that the differences between human and AI texts for the analyzed feature do not vary meaningfully across segments.", "latex": ["x,y \u2208 {I,B,C'}", "\u0394(Hx, Hy)", "(Ax, Ay)", "\u03b1", "\u0394(Ax, Ay)", "\u0394(Ax, Ay)", "\u0394(\u03971, \u0391\u2081)", "\u2206(\u0397\u0392, \u0410\u0432)", "\u2206(Hc, Ac)", "\u03b1", "\u0394(\u0397, \u0391x)", "(Hy, Ay)", "x, y \u2208 {I,B,C'}"]}, {"title": "3.5 Chess dataset creation", "content": "Since our study was motivated by the middlegame analogy between human and AI chess players, we conduct a concise yet systematic analysis of chess games to computationally explore whether these differences vary by phases from a computational perspective. Using games from the Free Internet Chess Server (FICS) database 1, we compile a dataset of ranked human vs. computer (AI) games played between 2018 and 2020. This period was selected due to the invention of AlphaZero [74] in 2017 and the subsequent increase of various open-sourced AI chess bots based on this [75]. To ensure sufficient game length for segmental analysis, we include only games lasting between 30 and 100 moves, excluding short games (due to early blunders or resignations) and"}, {"title": "A.1 Prompt engineering", "content": "While we primarily use existing human and AI text from various domains, we also employ LLMs for missing data generation and text/chess game segmentation. For text generation, we utilize GPT-3.5 (OpenAI), text-bison-001 (Google), LLaMA 2-Chat-7B (Meta), and Mistral-7B (Mistral AI). For instance, since Reuters news articles from Google were unavailable in the original datasets [46], we generated them using text-bison-001 using identical prompts from the original paper [46]. Similarly, for the email dataset, we generate AI text from all four LLMs, as only human-written emails are available in the Enron corpus [47]. For segmentation, we use Gemini-1.5-Flash (Google) and GPT-4 (OpenAI), which are distinct from the models used for text generation in our study. Proprietary models from Google and OpenAI are accessed via their official APIs, while open-source models from Meta and Mistral are sourced from their stable weights on Hugging Face. Across all settings, we use top_p = 0.95 and temperature = 0.9. However, it is important to note that even with identical prompts and hyperparameters, LLM outputs are not entirely deterministic."}, {"title": "A.2 AI text detection methods", "content": "We briefly discuss the details of our AI text detection methods.\nTo determine whether a text is LLM-generated, GPTZero [14] uses perplexity to measure the text's complexity and burstiness to evaluate sentence variants for providing the final output. We utilize the official API of GPT-Zero in our experiments.\nIt was initially developed as the GPT-2 output detector model, which was achieved by adjusting a RoBERTa big model using the 1.5B-parameter GPT-2 model's outputs. We utilize it directly from the HuggingFace repository2.\nMAGE (Machine-generated Text Detection in the Wild) is a Longformer model [38], finetuned on the entire Deepfakedetect [16] dataset (comprising 447,674 human-written and AI texts). By effectively managing more than 512 tokens, Longformer [82], a modified Transformer architecture, gets around the drawbacks of conventional transformer models. Longer documents can be processed more easily because of their attention pattern, which scales linearly with sequence length. We also access the model from the HuggingFace repository\u00b3.\nGPT-who [13] is a domain-agnostic statistical AI text detector that uses UID-based characteristics to capture unique statistical signatures. UID features are created via GPT2 inference and trained with a logistic regression model.\nWe fine-tuned BERT (bert-base-cased) on each dataset training set and evaluated it on the test set, as fine-tuned language models have been state-of-the-art in text classification."}]}