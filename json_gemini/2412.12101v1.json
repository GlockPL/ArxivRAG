{"title": "INTERPLM: DISCOVERING INTERPRETABLE FEATURES IN PROTEIN LANGUAGE MODELS VIA SPARSE AUTOENCODERS", "authors": ["Elana Simon", "James Zou"], "abstract": "Protein language models (PLMs) have demonstrated remarkable success in protein modeling and design, yet their internal mechanisms for predicting structure and function remain poorly understood. Here we present a systematic approach to extract and analyze interpretable features from PLMs using sparse autoencoders (SAEs). By training SAEs on embeddings from the PLM ESM-2, we identify up to 2,548 human-interpretable latent features per layer that strongly correlate with up to 143 known biological concepts such as binding sites, structural motifs, and functional domains. In contrast, examining individual neurons in ESM-2 reveals up to 46 neurons per layer with clear conceptual alignment across 15 known concepts, suggesting that PLMs represent most concepts in superposition. Beyond capturing known annotations, we show that ESM-2 learns coherent concepts that do not map onto existing annotations and propose a pipeline using language models to automatically interpret novel latent features learned by the SAEs. As practical applications, we demonstrate how these latent features can fill in missing annotations in protein databases and enable targeted steering of protein sequence generation. Our results demonstrate that PLMs encode rich, interpretable representations of protein biology and we propose a systematic framework to extract and analyze these latent features. In the process, we recover both known biology and potentially new protein motifs. As community resources, we introduce InterPLM (interPLM.ai), an interactive visualization platform for exploring and analyzing learned PLM features, and release code for training and analysis at github.com/ElanaPearl/interPLM.", "sections": [{"title": "1 Introduction", "content": "Language models of protein sequences have revolutionized our ability to predict protein structure and function [1, 2]. These models achieve their impressive performance by learning rich representations from the vast diversity of naturally evolved proteins. Their success raises a fundamental question: what exactly do protein language models (PLMs) learn? Advances in interpretability research methods now enable us to investigate this question and understand at least some of the representations they have learned in service of this challenging task.\nGaining insights into the internal mechanisms of PLMs is crucial for model development and biological discovery [3]. Understanding how these models process information enables more informed decisions about model design, beyond merely optimizing performance metrics. By analyzing the features driving predictions, we can identify spurious correlations or biases, assess the generalizability of learned representations, and potentially uncover novel biological insights. For instance, models might learn to predict certain protein properties based on subtle patterns or principles that have eluded human analysis, opening up new avenues for experimental investigation. Conversely, detecting biologically implausible features can guide improvements to the models' inductive biases and learning algorithms. This systematic analysis of prediction mechanisms offers opportunities to enhance model performance and reliability while extracting biological hypotheses from learned representations. Furthermore, PLM interpretation can illuminate how much these models learn genuine physical and chemical principles governing protein structure, as opposed to simply memorizing structural motifs. As PLMs continue to advance, systematic interpretation frameworks may uncover increasingly sophisticated biological insights."}, {"title": "2 Related Works & Background", "content": null}, {"title": "2.1 PLM Interpretability", "content": "PLMs are deep learning models (typically transformers) that perform self-supervised training on protein sequences, treating amino acids as tokens in a biological language to learn their underlying relationships and patterns [5]. These models consist of multiple transformer layers, each containing attention mechanisms and neural networks that progressively build up intermediate representations of the protein sequence. Prior work on interpreting PLMs has focused on analyzing these internal representations - both by probing the hidden states at different layers and by examining the patterns of attention between amino acids. Studies have demonstrated that attention maps can reveal protein contacts [6] and binding pockets, while the hidden state representations from different layers can be probed to predict structural properties like secondary structure states [7]. Recent evidence suggests that rather than learning fundamental protein physics, PLMs primarily learn and store coevolutionary patterns - coupled sequence patterns preserved through evolution [3]. This finding aligns with traditional approaches that explicitly modeled coevolutionary statistics [8], and with modern deep learning methods that leverage evolutionary relationships in training [9].\nHowever, even if PLMs are memorizing evolutionarily conserved patterns in motifs, key questions remain about their internal mechanisms: How do they identify conserved motifs from individual sequences? What percentage of learned features actually focus on these conserved motif patterns? How do they leverage these memorized patterns for accurate sequence predictions? What additional computational strategies support these predictions? Answering these questions could both reveal valuable biological insights and guide future model development."}, {"title": "2.2 Sparse Autoencoders (SAES)", "content": "In attempts to reverse-engineer neural networks, researchers often analyze individual neurons the basic computational units that each output a single activation value in response to input. However, work in mechanistic interpretability has shown that these neurons don't map cleanly to individual concepts, but instead exhibit superposition - where multiple unrelated concepts are encoded by the same neurons [10, 11]. Sparse Autoencoders (SAEs) are a dictionary learning approach that addresses this by transforming each neuron's activation into a larger but sparse hidden layer [12, 13, 14].\nAt their core, SAEs learn a \"dictionary\" of sparsely activated features that can reconstruct the original neuron activations. Each feature i is characterized by two components: its dictionary vector ($d_i$) in the original embedding space (stored as rows in the decoder matrix) and its activation value ($f_i$) that determines its contribution. The reconstruction of an input activation vector x can be expressed as:\n$x = b + \\sum_{i=1}^{d_{dict}} f_i(x)d_i$\nwhere b is a bias term and $d_{dict}$ is the size of the learned dictionary. This decomposition allows us to represent complex neuron activations as combinations of more interpretable features (see Appendix Figure 8 for more details).\nSAE analysis has advanced our understanding of how language and vision models process information [13, 15]. Neural network behavior can be understood through computational circuits - interconnected neurons that collectively perform specific functions. While traditional circuit analysis uncovers these functional components (like edge detectors [11] or word-copying mechanisms [16]), using SAE features instead of raw neurons has improved the identification of circuits responsible for complex behaviors [17].\nResearchers can characterize these features through multiple approaches: visual analysis [18], manual inspection [14], and large language model assistance [19]. Feature functionality can be verified through intervention studies, where"}, {"title": "3 Results", "content": null}, {"title": "3.1 Sparse Autoencoders Find Interpretable Concepts in Protein Language Models", "content": "To understand the internal representations of protein language models (PLMs), we trained sparse autoencoders (SAEs) on embeddings from ESM-2-8M. We expanded each of ESM-2's six layers from 320 neurons to 10,420 latent features. The quantity and domain-specificity of these features necessitated developing both qualitative visualization methods and quantitative evaluation approaches to enable interpretation. See Methods for training and analysis details.\nAnalysis of feature activation patterns revealed three distinct modes of amino acid recognition: structural patterns (coordinated activation between spatially or sequentially proximal residues), protein-wide patterns (broad activation across single proteins), and functional patterns (consistent activation within annotated domains, motifs, and binding sites)."}, {"title": "3.2 InterPLM: An Interactive Dashboard for Feature Exploration", "content": "Features from SAEs of every ESM-2 layer can be explored through InterPLM. ai, which provides multiple complemen- tary views of feature behavior: (1) sequential vs structural activation patterns, revealing how features capture both local sequence motifs and long-range structural relationships, (2) protein coverage analysis, distinguishing between features that identify specific local properties versus those capturing broader domain-level patterns, (3) feature similarity through UMAP [22] visualization, and (4) alignment with known Swiss-Prot [23] concepts. These feature's characteristic activation modes often persist across diverse proteins, suggesting that many features capture consistent biochemical and structural properties (Figure 2). By interactively examining the full distribution of features through multiple lenses their protein coverage, activation patterns, and spatial properties - we uncover a spectrum of behaviors, ranging from precise detectors that activate on single amino acids across many proteins to broader features that respond to complete sequences of specific protein families. As shown in Figure 1, examining the spatial distribution of activation sites uncovers features sensitive to various forms of proximity \u2013 whether sequential, structural, or both \u2013 providing insights into how the model encodes different aspects of protein organization."}, {"title": "3.3 Sparse Autoencoder Features Capture More Biological Annotations than Neurons", "content": "When comparing binarized feature activation values to biological concept annotations extracted from Swiss-Prot (dataset and methodological details in Methods), we observed that our learned features are much more specific than many concept annotations. The 433 Swiss-Prot concepts we evaluated span structural patterns, biophysical properties, binding sites, and sequential motifs (concept categories enumerated in Appendix C.2.1). While some concepts are specific to individual amino acids, others can cover broad domains that include many different physical regions, resulting in standard classification metrics overly penalizing features that activate on individual subsets of larger domains. For example, f/7653 activates on two conserved positions in tyrosine recombinase domains. While this feature has perfect"}, {"title": "3.4 Features Form Clusters Based on Shared Functional and Structural Roles", "content": "Clustering features by their dictionary vectors exposes groups with coherent biological functions and distinct specializa- tions. Within a kinase-associated feature cluster, three features exhibited activation on binding site regions with subtle variations in their spatial preferences (Figure 4). One feature activated specifically on the conserved alpha helix and beta sheet ($\\alpha$e, VI) preceding the catalytic loop, while others concentrated on distinct regions near the catalytic loop, as evidenced by their maximally activated examples in both structural and sequence representations (4b,c). Though their peak activation positions varied, all features maintained high activation levels (> 0.8) on their respective maximum examples across the cluster, suggesting they identify similar kinase subtypes.\nAnalysis of a beta barrel-associated cluster revealed a distinct generalization pattern. While all three features in the cluster were labeled as TonB-dependent receptor (TBDR) beta barrels, only one feature exhibited specificity for TBDR beta barrels, whereas the other two identified beta barrel structures more broadly, including TBDRs, as demonstrated by their maximally activating examples (4e). All three features exhibit F1 associations with TBDR beta barrels, yet they differ markedly in their specificity. f/1503 shows exceptional specificity (F1=.998), functioning as a true TBDR detector. The other features, though capable of identifying TBDR structures (high recall), also recognize various other beta barrel proteins (lower precision), resulting in lower but still significant F1 scores (.793, .611). This clustering demonstrates how the model's embedding space captures the natural hierarchy of beta barrel structures, where specialized TBDR detectors and general beta barrel features maintain similar representations despite operating at different levels of structural specificity."}, {"title": "3.5 Large Language Models Can Generate Meaningful Feature Descriptions", "content": "To extend beyond Swiss-Prot concepts, which label less than 20% of features across layers (Appendix Figure 9a), we developed an automated pipeline using Claude to generate feature descriptions. By providing Claude-3.5 Sonnet (new) with the Swiss-Prot concept information including text information not applicable for classification, along with examples of 40 proteins with varying levels of maximum feature activation, we generate descriptions of what protein and amino acid characteristics activate the feature at different levels. As validation, the model generated descriptions and Swiss-Prot metadata are used to predict feature activation levels on separate proteins and showed high correlation with actual feature activation (median Pearson r correlation = 0.72) across diverse proteins. As shown in Figure 5, the descriptions accurately match specific highlighted protein features, with density plots revealing distinct clusters of correctly predicted high and low feature activations. The two examples with higher correlations identify specific conserved motifs directly tied to consistent protein structure and function, while the third example (f/8386) detects a structural motif (hexapeptide beta helix) that does not have Swiss-Prot annotations and appears across multiple functionally diverse proteins, potentially explaining the increased difficultly and lower performance at protein-level activation prediction. However, we note that across the distribution of tested features, there is only a weak dependence between a feature's Swiss-Prot concept prediction performance and its LLM description accuracy (Pearson r = 0.11), suggesting that LLM-generated descriptions can effectively characterize many protein features regardless of whether they are easily categorized by annotations (Appendix Figure 10)."}, {"title": "3.6 Feature Activations Identify Missing and Novel Protein Annotations", "content": "Analysis of features with strong concept associations revealed cases where \"false positive\" activations actually indicated missing database annotations. For example, f/939 highlights a single conserved amino acid in a Nudix box motif. We found one example that has high activation of 939 but no Swiss-Prot label, while every other highly activated protein does have the annotation. As seen in Figure 6, the region surrounding this activated position greatly resembles the labeled Nudix motifs. This feature is likely identifying a missing label, and if we examine the InterPro [24] entry for this"}, {"title": "3.7 Protein Sequence Generation Can be Steered by Activating Interpretable Features", "content": "To validate that features capture causally meaningful patterns, we performed targeted interventions in the model's predictions. Unlike in language models where features can be meaningfully steered by clamping their values across entire sequences, protein features rarely maintain consistent activation across domains. This makes it crucial to test whether localized feature manipulation can still influence model behavior at a distance. While many features capture complex biological concepts, quantitatively demonstrating specific interventions' effects is challenging for patterns lacking clear sequence-based validation. We therefore focused on a simple, measurable example: showing how activating specific features can steer glycine predictions in periodic patterns.\nWe tested three features that activate on periodic glycine repeats (GXXGXX) within collagen-like domains. Given a sequence with a glycine followed by a mask three positions later, increasing these features' activation values on the first glycine position increased the probability of glycine at both that position and the masked position - consistent with the expected periodic pattern (Figure 7a). Importantly, these features were originally identified by their characteristic activation pattern occurring every third amino acid in natural sequences. The fact that activating them on just a single glycine position - rather than their standard periodic distribution - still produced interpretable effects demonstrates the robustness of their learned patterns. As expected, highly glycine-specific features (F1 scores: .995, .990, .86) only influenced the directly modified position. In contrast, the periodic glycine features demonstrated a more sophisticated capability: they successfully steered predictions for unmodified positions, even propagating this effect for multiple subsequent periodic repeats with diminishing intensity (Figure 13), revealing their capture of higher-order sequence patterns.\nThese results demonstrate that features activating on interpretable biological patterns can be capable of causally influence model behavior in predictable ways, even at positions beyond direct manipulation. However, further research"}, {"title": "4 Discussion", "content": "Our work demonstrates that sparse autoencoders can extract thousands of interpretable features from protein language model representations. Through quantitative evaluation against Swiss-Prot annotations, we showed that these features capture meaningful biological patterns that exist in superposition within the model's neurons. Our analysis framework combines automated interpretation through large language models with biological database annotations, enabling scalable feature characterization. Beyond identifying features that activate on annotated patterns, we showed that feature activation patterns can identify missing database annotations and enable targeted control of model predictions through interpretable steering."}, {"title": "4.1 Implications for PLM Interpretability", "content": "The dramatic difference between neuron and SAE feature interpretability (46 vs 2,548 features per layer interpreted with Swiss-Prot concepts) provides strong evidence for information storage in superposition within PLMs. Interestingly, we found that SAEs trained on randomized PLMs still extract features specific to individual amino acids but fail to capture any complex biological concepts we tested for, suggesting that meaningful feature extraction requires learned model weights. This phenomena of identifying amino acids from randomized embeddings aligns with recent observations on randomized language models [20] showing that SAEs capture both properties of the model and the underlying data distribution.\nBy identifying feature-concept associations through domain-level F1 scores, our biological concept evaluation frame- work provides a new quantitative approach to assessing interpretability methods, that handles concepts which are annotated more coarsely than the learned features. Furthermore, our LLM-generated feature descriptions achieve strong predictive power at identifying highly activated proteins, even when there are no Swiss-Prot annotations, offering a complementary approach to Swiss-Prot annotations for understanding protein features. While these metrics are only approximations of the true biological interpretability of a model, these metrics clearly distinguish between different approaches and could enable systematic comparison of interpretability techniques. The discovery that \"false positive\" predictions often indicate missing annotations demonstrates how interpretability tools can provide immediate practical value.\nAnalysis of feature activation patterns and dictionary vectors reveals both capabilities and open questions about model representations. While we identify many known conserved motifs and 3D patterns, the proportion of features dedicated to motif memorization versus other computational strategies remains unclear. Our framework provides a potential method for quantifying this balance, extending recent work showing PLMs primarily learn coevolutionary statistics [3]. Additionally, our steering experiments demonstrate that features can influence both local and neighboring amino acid predictions, though the precise mechanisms by which features contribute to masked token prediction require further investigation."}, {"title": "4.2 Applications of Interpretable PLM Features", "content": "Our SAE framework offers valuable applications across model development, biological discovery, and protein engi- neering. For model development, these features enable systematic comparison of learning patterns across different PLMs, revealing how architectural choices influence biological concept capture. Feature tracking during training and fine-tuning could provide insights into knowledge acquisition, while analysis of feature activation during failure modes could highlight systematic biases and guide improvements.\nBeyond confirming known patterns, these interpretable features may reveal novel biological insights. Features that don't align with current annotations but show consistent activation patterns across protein families could point to unrecognized biological motifs or relationships that have escaped traditional analysis methods. Additionally, we have demonstrated that feature-based steering can be used to influence sequence generation in targeted ways. While periodic glycine steering may not revolutionize protein engineering itself, it demonstrates a promising new direction for controlling sequence generation during protein design tasks."}, {"title": "4.3 Limitations and Future Directions", "content": "Scaling this approach to structure-prediction models like ESMFold or AlphaFold represents a crucial next step. Understanding how learned features evolve with increased model size and structural capabilities could enable fine-"}, {"title": "5 Methods", "content": null}, {"title": "5.1 Sparse Autoencoder Training", "content": null}, {"title": "5.1.1 Dataset Preparation", "content": "We selected 5M random protein sequences from UniRef50, part of the training dataset for ESM-2. For each protein, we extracted hidden representations from ESM-2-8M-UR50D after transformer block layers 1 through 6, excluding <cls> and <eos> tokens. The dataset was sharded into groups of 1,000 proteins each, with tokens shuffled within these groups to ensure random sampling during training."}, {"title": "5.1.2 Architecture and Training Parameters", "content": "We trained sparse autoencoders following the architecture described in [14], using an expansion factor of 32x, creating feature dictionaries of size 10,240 from ESM2-8M embedding vectors of size 320. For each layer, we trained 20 SAES for 500,000 steps using a batch size of 2,048. Learning rates were sampled in increments of 10x from 1e-4 to 1e-8, with L1 penalties ranging from 0.07 to 0.2. Both the L1 penalty and learning rate were linearly increased from 0 to their final values during the initial training phase, with the learning rate reaching its maximum within the first 5% of training steps."}, {"title": "5.1.3 Feature Normalization", "content": "To standardize feature comparisons, we normalized activation values using a scan across 50,000 proteins from Swiss- Prot. For each feature, we identified the maximum activation value across this dataset and used it to scale the encoder weights and decoder weights reciprocally, ensuring all features were scaled between 0 and 1 while preserving the final reconstruction values."}, {"title": "5.2 Swiss-Prot Concept Evaluation Pipeline", "content": null}, {"title": "5.2.1 Dataset Construction", "content": "From the reviewed subset of UniprotKB (Swiss-Prot), we randomly sampled 50,000 proteins with lengths under 1,024 amino acids. We converted all binary and categorical protein-level annotations into binary amino acid-level annotations, maintaining domain-level relationships for multi-amino-acid annotations. The dataset was split equally into validation and test sets of 25,000 proteins each. We retained only concepts present in either more than 10 unique domains or more than 1,500 amino acids within the validation set."}, {"title": "5.2.2 Feature-Concept Association Analysis", "content": "For each normalized feature, we created binary feature-on/feature-off labels using activation thresholds of 0, 0.15, 0.5, 0.6, and 0.8. We evaluated feature-concept associations using modified precision and recall metrics:\nprecision = TruePositives / (TruePositives + FalsePositives)\nrecall = DomainsWithTruePositive / TotalDomains\nF1 = 2 * precision * recall / (precision + recall)\nFor each feature-concept pair, we selected the threshold yielding the highest F1 score for final evaluation."}, {"title": "5.2.3 Model Selection and Evaluation", "content": "We conducted initial evaluations on 20% of the validation set to compare hyperparameter configurations (only including the 135 concepts that had > 10 domains or 1,500 amino acids in this subset alone). For each concept, we identified the feature with the highest F1 score and used the average of these top scores to select the best model per layer. These six models (one per ESM2-8M layer) were used for subsequent analyses and the InterPLM dashboard.\nTo calculate the test metrics we (1) identify the feature with highest F1 score per-concept on the full validation set, then for each concept, calculate the F1 score of the selected feature on the test set and report these values then (2) identify all feature-concept pairs with F1 > 0.5 in the validation set, calculate their F1 scores on the test set, and report the number of these that have F1 > 0.5 in the test set."}, {"title": "5.2.4 Baselines", "content": "To train the randomized baseline models, we shuffled the values within each weight and bias of ESM2-8M, calculated embeddings on the same datasets, and repeated the same training (using 6 hyperparameter choices per layer), concept- based model selection, and metric calculation processes. To compare with neurons, we scaled all neuron values between 0 and 1 (based on the minimum and maximum values found in our Swiss-Prot subset), then input these into an SAE with expansion factor of 1x that has an identity matrix for the encoder and decoder, such that all other analysis can be performed identically."}, {"title": "5.3 LLM Feature Annotation Pipeline", "content": null}, {"title": "5.3.1 Example Selection", "content": "Analysis was performed on a random selection of 1,200 (10%) features. For each feature, representative proteins were selected by scanning 50,000 Swiss-Prot proteins to find those with maximum activation levels in distinct ranges. Activation levels were quantized into bins of 0.1 (0-0.1, 0.1-0.2, ..., 0.9-1.0), with two proteins selected per bin that achieved their peak activation in that range, except for the highest bin (0.9-1.0) which received 10 proteins. Additionally, 10 random proteins with zero activation were included to provide negative examples. For features where fewer than 20 proteins reached peak activation in the highest range (0.9-1.0), additional examples were sampled from the second- highest range (0.8-0.9) to achieve a total of 24 proteins between these two bins, split evenly between training and evaluation sets. Features were excluded if fewer than 20 proteins could be found reaching peak activation across the top three activation ranges combined."}, {"title": "5.3.2 Description Generation and Validation", "content": "For each feature, we compiled a table containing protein metadata, quantized maximum activation values, indices of activated amino acids, and amino acid identities at these positions. Using this data, we prompted Claude-3.5 Sonnet (new) to generate both a detailed description of the feature and a one-sentence summary that could guide activation level prediction for new proteins.\nTo validate these descriptions, we provided Claude with an independent set of proteins (matched for size and activation distribution) along with their metadata, but without activation information. Claude's predicted activation levels were compared to measured values using Pearson correlation."}, {"title": "5.4 Feature Analysis and Visualization", "content": null}, {"title": "5.4.1 UMAP Embedding and Clustering", "content": "We performed dimensionality reduction on the normalized SAE decoder weights using UMAP (parameters: met- ric=\"euclidean\", neighbors=15, min dist=0.1). Clustering was performed using HDBSCAN [25] (min cluster size=5, min samples=3) for visualization in the InterPLM interface."}, {"title": "5.4.2 Sequential and Structural Feature Analysis", "content": "We characterized features' sequential and structural properties using the following procedure:\n1. Identified high-activation regions (>0.6) in proteins with available AlphaFold structures\n2. For each protein's highest-activation residue, calculated:\n\u2022 Sequential clustering: Mean activation within \u00b12 positions in sequence\n\u2022 Structural clustering: Mean activation of residues within 6\u00c5 in 3D space\n3. Generated null distributions through averaging 5 random permutations per protein\n4. Generated null distributions through averaging 5 random permutations per protein\n5. Assessed clustering significance using paired t-tests and Cohen's d effect sizes across 100 proteins per feature\nThe 100 proteins per feature were randomly sampled from our Swiss-Prot dataset. Features with < 25 examples meeting this criteria were excluded from this analysis. For structural feature identification in InterPLM, we considered only proteins with Bonferroni-corrected structural p-values < 0.05, with features colored based on the ratio of structural to sequential effect sizes."}, {"title": "5.5 Steering Experiments", "content": "Following the approach described in [20] we decomposed ESM embeddings into SAE reconstruction predictions and error terms. For sequence steering, we (1) Extracted embeddings at the specified layer (2) Calculated SAE reconstructions and error terms (3) Modified the reconstruction by clamping specified features to desired values (4) Combined modified reconstructions with error terms (5) Allowed normal model processing to continue (6) Extracted logits and calculated softmax probabilities for comparison across steering conditions. Steering experiments all conducted using NNsight [26]."}, {"title": "A Additional background on Protein Language Models", "content": "Protein language models (PLMs) adapt techniques from natural language processing to learn representations of protein sequences that capture both structural and functional properties. These models typically use transformer architectures, treating amino acids as discrete tokens. While natural language modeling typically uses an autoregressive training method, many modern PLMs employ masked language modeling objectives similar to BERT, as protein sequences fundamentally exhibit bidirectional dependencies - the folding and function of any given amino acid depends on both N-terminal and C-terminal context. These models learn impressively rich protein representations through self- supervised training on large sequence databases without requiring structural annotations. The learned embeddings capture hierarchical information ranging from local physicochemical properties to global architectural features. Notably, these representations have proven crucial for protein structure prediction - they serve as the input embeddings for dedicated folding models like ESMFold, and even AlphaFold, while not explicitly a language model, dedicates most of the computational resources in a given prediction to learning protein representations from multiple sequence alignments in a conceptually similar manner. Many efforts have demonstrated that the representations learned through masked language modeling alone contain remarkable structural and functional information, enabling state-of-the-art performance on tasks ranging from structure prediction to protein engineering. This success appears to stem from the models' ability to capture the underlying patterns in evolutionary sequence data that reflect physical and biological constraints."}, {"title": "B Extended SAE Details and Analysis", "content": null}, {"title": "B.1 Additional background", "content": "SAEs transform the latent vector for a single amino into a new vector with increased size and sparsity. When a specific position in this vector has a non-zero value, that corresponds to the presence of a specific pattern in the amino acid's neuron embedding. Ideally these model patterns correspond to human interpretable features that we can understand based on patterns by which the feature activates and the impact it has on the model when activated. Specifically, when we perform analysis on feature activation levels, these are using the $f_i(x)_i$ values, while dictionary value analysis uses the learned weights in $W_{d_i}$ as visualized in 8."}, {"title": "B.1.1 SAE metrics", "content": "While the goal of SAE training is to learn features that are maximally interpretable and accurate, these qualities are challenging to explicitly optimize for so during training we optimize for sparsity, hoping that more sparse features are more interpretable, and reconstruction quality.\nSpecifically, during training we calculate our loss as a weighted sum of an L1 norm calculating the absolute sum of all feature values, and mean squared error calculating how close the reconstructed x' is to the original x. Then, when training SAEs or comparing different implementations, people evaluate LO, the average number of nonzero elements in the latent representation per token and Percent Loss Recovered, the percent of the original cross entropy of the model that is achieved when the model's embeddings are replaced by reconstructions. This last metric measures the amount 'recovered' by comparing the cross entropy of the model using reconstructed embeddings ($CE_{Reconstruction}$) to the original cross entropy ($CE_{Original}$) and the cross entropy when the specified embedding layer is instead replaced with all zeros($CE_{Zero}$) but all later layers remain identical.\nMetrics described are calculated per these equations:\n$L_1(f(x)) = \\sum_{i=1}^{d_{dictionary}}|f_i(x)|$\n$MSE(x,x') = \\frac{1}{d_{model}} \\sum_{i=1}^{d_{model}} (x_i - x'_i)^2$\n$L_0(f(x)) = \\sum_{i=1}^{d_{dictionary}} \\mathbb{1}(f_i(x) > 0)$\n% Loss Recovered = $\\frac{(CE_{Zero} - CE_{Reconstruction})}{(CE_{Original} - CE_{Zero})} * 100$\nWhile we do our final model selection based on Swiss-Prot concept associations as this more closely targets biological interpretability than pure sparsity, initial experiments optimized final hyperparameter ranges with these metrics. Final hyperparameters along with Lo and % Loss Recovered for our 6 SAEs are:"}, {"title": "C Swiss-Prot Concept Associations", "content": null}, {"title": "C.1 Swiss-Prot Metadata Categories", "content": "These tables contain all of the metadata used for quantitative feature-concept assoications and LLM feature descriptions and validation, organized into groups with similar themes. The last two columns specify whether each field was used in quantitative concept analysis, LLM descriptions, or both."}, {"title": "C.2 Swiss-Prot Metadata Categories", "content": "These tables contain all of the metadata used for quantitative feature-concept associations and LLM feature descriptions and validation, organized into groups with similar themes. The last two columns specify whether each field was used in quantitative concept analysis, LLM descriptions, or both."}, {"title": "C.2.1 Basic Identification Fields", "content": null}, {"title": "C.2.2 Structural Features", "content": null}, {"title": "C.2.3 Modifications and Chemical Features", "content": null}, {"title": "C.2.4 Targeting and Localization", "content": null}, {"title": "C.2.5 Functional Domains and Regions", "content": null}, {"title": "C.2.6 Functional Annotations", "content": null}, {"title": "D LLM-based Autointerpretability", "content": null}, {"title": "D.1 Prompts", "content": "Generate description and summary\nAnalyze this protein dataset to determine what predicts the 'Maximum activation value' and 'Amino acids of highest activated indices in protein' columns. This description should be as concise as possible but sufficient to predict these two columns on held-out data given only the description and the rest of the protein metadata provided. The feature could be specific to a protein family, a structural motif, a sequence motif, a functional role, etc. These WILL be used to predict how much unseen proteins are activated by the feature so only highlight relevant factors for this.\nFocus on:\n\u2022 Properties of proteins from the metadata that are associated with high vs medium vs low activation.\n\u2022 Where in the protein sequence activation occurs (in relation to the protein sequence, length, structure, or other properties)\n\u2022 What functional annotations (binding sites, domains, etc.) and amino acids are present at or near the activated positions\n\u2022 This description that will be used to help predict missing activation values should start with \"The activation patterns are characterized by:\"\nThen, in 1 sentence, summarize what biological feature or pattern this neural network activation is detecting. This concise summary should start with \"The feature activates on\"\nProtein record: Insert table with Swiss-Prot metadata and activation levels\nPredict activation levels\nGiven this protein metadata record, feature description, and empty table with query proteins, fill out the query table indicating the maximum feature activation value within in each protein (0.0-1.0).\nBase activation value on how well the protein matches the described patterns. There could be 0, 1 or multiple separate instances of activation in a protein and each activation could span 1 or many amino acids.\nOutput only these values in the provided table starting with \"Entry,Maximum activation value\". Respond with nothing but this table.\nProtein record: Insert table with Swiss-Prot metadata\nTable to fill out with query proteins: Insert empty table of IDs to fill out with predictions\nThe activation patterns are characterized by: Insert LLM description"}, {"title": "D.2 Example Descriptions and Summaries", "content": "Full Description Layer 4 Feature 9047\nThe activation patterns are characterized by:\n\u2022 Highest activations (0.9-1.0) occur in glycosyltransferase proteins, particularly glycogen synthases (GlgA) and similar enzymes that transfer sugar molecules\n\u2022 Activated positions consistently occur around amino acid positions 280-450 in these proteins, specifi- cally involving glycine (G), alanine (A), or proline (P) residues\n\u2022 The activated sites frequently overlap with substrate binding regions, particularly nucleotide-sugar binding sites (e.g., ADP-glucose, UDP-glucose, GDP-mannose)\n\u2022 Medium activations (0.3-0.8) are seen in other transferases and synthetases with similar substrate binding patterns\n\u2022 Proteins without sugar/nucleotide binding functions show no activation (0"}]}