{"title": "Integrating Hierarchical Semantic into Iterative Generation Model for Entailment Tree Explanation", "authors": ["Qin Wang", "Jianzhou Feng", "Yiming Xu"], "abstract": "Manifestly and logically displaying the line of reasoning from evidence to answer is significant to explainable question answering (QA). The entailment tree exhibits the lines structurally, which is different from the self-explanation principle in large-scale language models. Existing methods rarely consider the semantic association of sentences between and within hierarchies within the tree structure, which is prone to apparent mistakes in combinations. In this work, we propose an architecture of integrating the Hierarchical Semantics of sentences under the framework of Controller-Generator (HiSCG) to explain answers. The HiSCG designs a hierarchical mapping between hypotheses and facts, discriminates the facts involved in tree constructions, and optimizes single-step entailments. To the best of our knowledge, We are the first to notice hierarchical semantics of sentences between the same layer and adjacent layers to yield improvements. The proposed method achieves comparable performance on all three settings of the EntailmentBank dataset. The generalization results on two out-of-domain datasets also demonstrate the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Plausible explanations are fundamental to artificial intelligence, which underpin the results predicted by models and make an important contribution to human understanding. The reasoning capabilities of Large-scale Language Models (LLMs) elicited by Chain of Thought (CoT) prompts help boost the quality of the explanations in natural language [2022, 2022, 2023]. But they still suffer from hallucinations on many tasks [2023, 2023]. Credible explanation generation has undoubtedly become a principal aspect of the implementation of artificial intelligence.\nWe follow [2021] to adopt the form of entailment trees to generate credible explanations, which employ verified facts and standardized interpretation steps, to present the decision-making process transparently and controllably. As illustrated in Fig. 1, for a given hypothesis, the tree will explain why the hypothesis is correct. Facts in the corpus will become the leaf nodes, and the hypothesis is obtained by multi-entailment steps. The popular approach employs sequence-to-sequence models to output linearized entailment trees in a time [2021,2022]. However, this approach is not conducive to error analysis. Another existing approach is based on the controller-generator framework, which iteratively generates single entailment steps through two modules that operate independently [2022,2022]. The controller selects facts in each step, while the generator generates the conclusion after combining those facts (also known as the premises). In comparison, the controller-generator framework-based approaches have a rigorous derivation principle.\nNevertheless, most previous methods use semantic similarity to filter premises. As shown in Fig. 1, the distractors (i.e., s5,s6) have higher semantic similarity to the hypothesis compared to s1, but the hypothesis cannot be derived from it. Intuitively, the hierarchy serves as the basis of trees, and there should be hierarchical semantic relationships between sentences in the tree. If we can also hierarchically organize the flat facts in the corpus, it will be beneficial to subsequent operations. Therefore, we try to adjust the sentence representation, integrate hierarchical features into its original semantics, and build a representation space with hierarchical semantics of sentences. We design a novel representation of the hierarchical semantics of sentences. Inspired by translation models such as TransE to embed entities and relationships of multi-relational data in low-dimensional vector spaces [2013], we obtain more efficient entailment steps based on integrating hierarchical semantics of sentences.\nIn this work, we propose an architecture of integrating the Hierarchical Semantics of sentences under the framework of Controller-Generator called HiSCG. We decompose it into three parts: hierarchical semantic encoder, selection controller, and intermediate generation. Firstly, as our main innovative module, the hierarchical semantic encoder adjusts the embedding space to establish hierarchical connections between facts. In an entailment tree, hierarchical connections can be propagated as levels increase. Therefore, all relevant facts form a cluster with the hypothesis as the core, which establishes a hierarchical mapping and increases the retrieval performance of relevant facts accompanied by the gain on step selection. Secondly, the selection controller module selects two facts from the fact set with hierarchical representation for compositions. We retain the previous training methods and add a loss function that can better utilize hierarchical representation. Thirdly, the intermediate generation module based on sequence-to-sequence language models generates conclusions from the two facts (premises), which will be added to the fact set for subsequent selection and generation. In summary, we have three contributions described below.\n\u2022 To the best of our knowledge, we are the first to integrate hierarchical features into the original semantics of sentences, and build a hierarchical representation space to construct entailment trees.\n\u2022 Experiments on three settings on the EntailmentBank benchmark demonstrate that our method achieves comparable results compared to other existing baselines.\n\u2022 Further experiments on two cross datasets indicate that our model has better generalization abilities."}, {"title": "2 Related Works", "content": "Interpretability of QA Systems Understanding the chain of reasoning from facts to hypotheses can help to establish an explainable QA system. The approaches based on rationale emphasize paragraph retrieval, including sparse retrieval represented by BM25 [2009] and dense retrieval represented by DPR [2020], which are mostly under the retriever-reader framework [2021,2022]. Instead of simple textual explanations, there have been other attempts to produce a sequence of reasoning steps by iterative retrieval, which can be divided into two subcategories. Use entity links/hyperlinks to extract and match keywords, adding hyperlinks and paragraphs with hyperlinks to previous paragraphs [2021]. Reformulate the question by adding to, modifying, or re-weighting the question texts [2020,2021,2021].\nWith the introduction of CoT, the self-explanation of language models become a appealing way to gain more insight into predictions. Wei et al. [2022], Kojima et al. [2022] and Lampinen et al. [2022] leverage a few training examples as prompts without updating any parameters of the LLMs to generate explanations. However, ChatGPT [2022], the comprehensive model with the best performance among them, can produce the hallucination of human-like fluency and confidence without faithfulness to the truth [2023, 2023]. In addition, the applied methodologies are still poorly informed by theories, making them still lack credibility and scalability for real-world applications [2023, 2023]. This enables the retrieval-based reasoning to still have priority.\nEntailment Tree Generation [2021] proposes the EntailmentBank dataset, where the trees are composed of multi-premise textual entailment steps. [2022] proposes an iterative retrieval generative reasoner (IRGR) architecture, which enables the model to use intermediate conclusions to generate step-by-step from textual premises to the hypothesis systematically. [2022] proposes a module-based framework, in which the controller selects the candidate steps and the individual generator supplies conclusions. [2022] introduces reinforcement learning to generate entailment trees with elaborately designed aligned reward function that is consistent with the evaluation."}, {"title": "3 Task Definition", "content": "As shown in Fig. 1, we display the reasoning lines in the form of entailment trees. Given a hypothesis and a corpus of facts $C = s_1, s_2,..., s_n$ (simple textual sentences) containing relevant facts $s^+$ and irrelevant facts $s^-$. The aim is to determine all relevant facts and combine these facts to generate a valid entailment tree. That is, through reliable compositions and reasonable intermediate conclusions generation, a tree with a root node that is semantically close to the hypothesis will be obtained. Each entailment tree represents as $T = (S, I, P, h)$, where $S$ is the set of leaf nodes ($S \\in C$). $I$ is the set of generated intermediate nodes ($I \\& C'$). $P$ is all of the reasoning steps (e.g. $s_1 \\land s_2 \\Rightarrow i, s_1, s_2 \\in S, i \\in I$). $h$ is the root node of the entailment tree."}, {"title": "4 Architecture", "content": "Following the setting of [2021], the composition only occurs between two facts. The overall framework of HiSCG contains three modules as shown in Fig. 2. We use the hierarchical semantic encoder to obtain hierarchical semantic embeddings of sentences. In each entailment step, the selection controller selects appropriate steps in the fact set. The intermediate generator generates intermediate conclusions for selected steps. We introduce the three modules in detail in Sections 4.1, 4.2, and 4.3, respectively. Section 4.4 describes the reasoning strategy of our framework."}, {"title": "4.1 Hierarchical Semantic Encoder", "content": "Existing methods directly obtain the embeddings of all facts in C. In this way, when the entailment tree is deep, the relevant fact $s^+$ and irrelevant fact $s^-$ have insignificant differences since the facts have low syntactic and semantic similarity with hypothesis h. This will not be conducive to improving the generalization performance of subsequent combination operations. We intuitively think that the problem needs to be solved in the hierarchical semantic space. We propose a novel method to augment the sentence representation, establishing the relationship between the conclusion $i$ and the premises $s_b, s_e$. To this end, large quantities of single steps are required. Following [2022], we use synthetic data extracted from Wikipedia based on rules as simple samples.\nInspired by TransE of embedding entities and relations, we replace those entity and relation nodes with each fact in C. We establish connections between premises and conclusions from the perspective of augmenting hierarchical semantics. For the premises $s_b, s_e$ and the conclusion $i$, the embedding of $i$ should be close to the embedding of $s_b$ plus the embedding of $s_e$. As shown in Fig. 3, the premises $s_b, s_e$ and the conclusion $i$ will be input into the Pre-trained Language Model (PLM) to compute their sentence embedding. The training objectives are making the golden conclusion and the adduct of $s_e$ and $s_b$ as close as possible, and connecting relative premises simultaneously.\nIn terms of implementation, we hope that $s_b$ and $s_e$ are getting closer, and the embedding of $i$ should be close to the embedding of $s_b$ plus the embedding of $s_e$ in the hierarchical representation space $\\varepsilon$. According to the vector addition rule, it ultimately displays that $s_b$ and $s_e$ are constantly moving closer to $i$, and $i$ has a geometric diagonal property between them.\nThis phenomenon is also reflected in the transmission of correlation with the increase of depths in the entailment tree. Specifically, in a single step $s_{(m,b)} \\land s_{(m,e)} \\Rightarrow i_m$ at m-level in the tree, $i_m$ as an intermediate node will become the premise at m-1-level (assuming $i_m = s_{(m-1,b)}$). Due to the establishment of the geometric relationship (i.e.,$s_{(m-1,b)} \\land s_{(m-1,e)} \\rightarrow i_{(m-1)}$), $s_{(m,b)}$ and $s_{(m,e)}$ are correlated with $i_{(m-1)}$, and the correlations are continuously transmitted up to the root node. The loss function for this stage is shown in Equation 3.\n$L_{con} = \\sum_{(s_b, s_e, i) \\in S} [d(s_b + s_e, i) - d(s'_b + s_e, i) + \\gamma_1]_+$ (1)\n$L_{mut} = \\sum_{(s_b, s_e, i) \\in S} [d(s_b, s_e) - d(s_b, s'_e) + \\gamma_2]_+$ (2)\n$L_{cor} = L_{con} + L_{mut}$ (3)\nWhere $[x]_+$ denotes the positive part of x, $\\gamma_i > 0$ is a margin hyperparameter. $d(s_b+s_e, i)$ is for measuring distance, which we take to be the L2-norm, and $S' = \\{(s', s_e, i) \\cup (s_b, s'_e, i)\\}$, $s'$ is the negative sample of $s$. We select other samples trained in the same batch as negative samples, namely $D_{batch} = \\{(s_1, s_2), (s'_1, s'_2), ..., (s^M_1, s^M_2)\\}$ ($M$ donates the batch size), $s_1^{(j \\ne i)}$ is the negative sample of $s_b$, and $s_2^{(j \\ne i)}$ is the negative sample of $s_e$."}, {"title": "4.2 Selection Controller", "content": "There are $C^2$ step selections in the given $C$. We use the selection controller to obtain reasonable steps. A selection controller should serve two purposes. On the one hand, it identifies relevant facts $s^+$ due to distractors contained in $C$. On the other hand, it determines single entailment steps required at a certain state. We implement both deductive and abductive modules (e.g., $i-s_b \\Rightarrow s_e$) to perform forward and backward reasoning respectively [2022]. We concatenate the hypothesis and all facts of the tree as :\n$T = Enc_{cor}([CLS] h \\{[SEP] s_l\\}^n [SEP], l \\in [1,n])$ (4)"}, {"title": "4.3 Intermediate Generation", "content": "The selected steps are utilized as the input of the intermediate generation module, and the conclusions derived are the outputs. This generation method can determine the precision of each entailment step, because the intermediate conclusion is only related to the two inputted premises. In contrast, ensemble generations are subjected to reasoning rules [2021, 2022], causing significant hallucinations. The premises $s_b, s_e$ are inputted in the sequence-to-sequence model in the format of $[CLS][Suffix] s_b [SEP] s_e [SEP]$ where the $[Suffix]$ is \"connection:\", and the output is the corresponding conclusion $i$. As for abduction, the input format is $[CLS][Suffix] s_b(s_e) [SEP] i [SEP]$ and the output is the $s_e (s_b)$. The synthetic data mentioned in Section 4.1 will also participate in training as silver samples. We use a typical auto-regressive language modeling loss to train the module."}, {"title": "4.4 Reasoning Strategy", "content": "In the given fact set C, the selection controller obtains $score_{fact}$ of each fact in C. The fact will be discarded if $score_{fact} < \\delta$, where $\\delta$ is a threshold. The sub-set of retained facts is called $\\phi$. Then the selection controller obtains $score_{step}$ of all potential steps. The step with the highest $score_{step}$ will be selected, and the intermediate generation module will generate the intermediate conclusion with the selected step. Note that the intermediate conclusion will add to $\\phi$, and the facts implicated in steps will withdraw from $\\phi$. This process iterates until the reasoning trace to the hypothesis is found."}, {"title": "5 Experimental Settings", "content": "We evaluate our method on EntailmentBank [2021], a benchmark that employs an entailment tree to represent QA explanation processes in the scientific domain. The detailed composition is shown in Table 1. Three increasingly complex explanation tasks are included in EntailmentBank. Sufficient valid information without distractors is supplied in Task1. 25 facts with distractors are provided in Task2. We need to distinguish the relevant facts and complete the construction. There is no scope limitation in Task3. We need to build the entailment tree from a corpus."}, {"title": "5.2 Evaluation Metrics", "content": "We follow [2021] to evaluate the prediction tree from four dimensions of leaves, steps, and intermediates.\nLeaves(F1, AllCorrect): We compute an F1 score by comparing predicted leaves and golden leaves. AllCorrect =1 only if all predicted leaves conform to golden leaves, and equals 0 otherwise.\nSteps(F1, AllCorrect): To evaluate whether the entailment step is correct. AllCorrect=1 if all steps in the prediction tree precisely match the golden tree (i.e., FI=1).\nIntermediates(F1, AllCorrect): We use BLEURT-Large-512 [2020] to measure the synthesized intermediate nodes. The predicted conclusion is correct when the bleurt score of the aligned pair exceeds the threshold $\\tau$=0.28. AllCorrect=1 if all predicted conclusions match (i.e., Fl=1).\nOverall (AllCorrect): The Overall AllCorrect is set to 1 only if the AllCorrect of leaves, steps, and intermediate all are 1. Overall is the comprehensive evaluation metric. All-Correct=1 means that the predicted and golden tree are exactly matched, which is the most optimal result."}, {"title": "5.3 Baselines", "content": "EntailmentWriter (2021) [2021] generates linearized entailment trees via sequence-to-sequence models with T5-11B and T5-Large versions.\nMETGEN (2022) [2022] is a module-based generative framework that divides the reasoning process into premises retrieval and conclusions generation.\nIRGR (2022) [2022] generates iterative steps, allowing the model to leverage intermediate conclusions to improve retrieval performance.\nRLET (2022) [2022] is a reinforcement learning framework based on entailment tree generation, which is trained utilizing the cumulative signals across the whole tree."}, {"title": "5.4 Implementation Details", "content": "We experimented on four A40 GPUs with 48GB of memory. The hierarchical semantic encoder utilizes albert-xxlarge-v2 [2019] as the backbone, and is trained for 20 epochs. The selection controller module reuses the hierarchical semantic encoder and is fine-tuned on downstream tasks. We iteratively generate the entailment tree with all provided facts in Task1. As for Task2, the selection controller filters out the distractors, and the output tree is the best match to the hypothesis in all partial trees. In Task3, we leverage IRGR-retriever to select 50 candidate facts for each hypothesis from the corpus, and then use the hierarchical semantic encoder to refine the fact set within 25 facts (similar to Task2). An important property in Task3 is that hypotheses may entail diverse reasoning lines due to massive facts, so the beam search will be used to expand the tree set. The intermediate generation module is implemented with T5-large."}, {"title": "5.5 Hyperparameters", "content": "In Task1, we set the learning rate to le-5, batch size to 16, A to 0, and train for 1000 epochs. In Task2, we set the learning rate to le-5, A to 1, batch size to 8, and train for 1500 epochs. $\\gamma_i$ is 0.1. The weight constants $\\alpha$, $\\beta$ are both 1.0. The threshold $\\delta$ is 0.001 when inferencing on the Task2 test set. In Task3, we reuse the model of Task2 for inferencing. Besides, we employ beam search to increase the probability of correct reasoning in Task3. We use $top_p$ and $top_{abdp}$ to select candidate steps for deductive and abductive reasoning, respectively. In the pilot experiment, setting $top_p$ to 0.4, $top_{abdp}$ to 0.1, $\\delta$ to 0.1 and beam size to 3 can achieve the best results. $\\delta$ is the most influential factor among them."}, {"title": "6 Result & Analysis", "content": "As shown in Table 2, HisCG exposes competitive performance on the Overall AllCorrect in Task1 and Task2. Specifically in Task1, the leaves F1/AllCorrect is 100% since no distractors, which is not guaranteed by the linearized generation method. The performance of leaves underpins the advancements of other indicators in Task2. The leaves AllCorrect of HisCG outperform other non-generative models. Furthermore, our method is better on all AllCorrect metrics when compared to EntailmentWriter with T5-large. While using T5-11B as its backbone, we also achieve noteworthy results on AllCorrect with fewer parameters. Regarding the superior performance of EntailmentWriter with T5-11B, we guess the reason is the impressive natural-language understanding capabilities brought by the parameter scale, which makes substantial metric improvements after fine-tuning. However, the sharp decrease in Task3 also indicates that this approach heavily relies on shortcut learning, while without adaptability in open-setting reasoning.\nWe also exceed all baselines on the strictest Overall All-Correct in Task3. We also achieved the best results on the leave F1 and step AllCorrct metrics, which demonstrates the effectiveness of the proposed method via hierarchical semantic augmented. In the hierarchical embedding space, more relevant facts are retrieved, and the performance of step selection is improved as a by-product due to the established relationship between premises and conclusions. The intermediate F1/AllCorrect is greater than that of the baselines, which we believe benefited from the step AllCorrect and the data augmentation."}, {"title": "6.2 Ablation Result", "content": "Effectiveness of hierarchical semantic encoder With the desire to verify whether the hierarchical semantic encoder works, we performed ablation studies on three tasks. As shown in Table 3, without the hierarchical semantic encoder in Task2, the decline of leaves F1/AllCorrect affects the sharp decrease of other metrics, which reveals our effective retrieval strategy. The leaves/steps/intermediates/overall All-Correct also relatively fall by 29%/28%/6%/33% in Task3. This demonstrates that our strategy has established profound connections between sentences, enabling step selection more robust. Additionally, we use t-SNE to visualize the representation of facts and hypotheses in Fig. 4. As the picture shows, the hierarchical semantic encoder can gather relevant facts near the hypothesis and distance irrelevant facts. And overall, it brings sentences with combinatorial relationships closer together. This demonstrates that HisCG can capture the hierarchical semantics required for sentences in the entailment tree construction, which provides good prerequisites for the controller to improve model performance.\nEfficiency of Intermediate Generation We further investigate whether redundant training data has a prominent influence on the intermediate generation module. We conduct ablation studies on Task1&Task2&Task3. The results in Table 4 show that extra training data has brought certain advancements, but it is not the primary driver of Overall All-Correct."}, {"title": "6.3 Results Breakdown", "content": "We analyze Task1 in terms of the number of leaves to better assess the contribution of the hierarchical semantic module. As shown in Fig. 5, HisCG can predict all correct steps due to the uniqueness (accounting for 22.9%) in the setting of fact-1/fact-2. For the case of fact-3 (accounting for 19.7%), HisCG achieves significant improvements. When the number of leaves is greater than 4, HisCG comes with relatively poor performance. On the one hand, the increase of relevant facts introduces multiplied exponentially difficulty. On the other hand, some reasoning steps are imprecise in the case of increasing relevant facts, which brings noisy interference to the model."}, {"title": "6.4 Entailment Step Error Analysis", "content": "We analyze the errors manually to further understand the strengths and weaknesses of HisCG in the test set of Task2. Fig.7 illustrates the error categories comprehensively. The step errors account for 52%, and the intermediate errors account for 46% (existing cross-cases). Moreover, there are cases of underestimation (10%) in our manual evaluation (match errors). One example in the Task2 test set is depicted in Fig.6. Steps error includes missing necessary leaves and selecting invalid steps under complete necessary premises. Intermediates error includes repeated premises (outputs consistent with the inputs), ignored important details (i.e., critical connection details) and generated hallucinatory conclusions caused by the probability distribution of words."}, {"title": "6.5 Cross-dataset Setting", "content": "Annotated reasoning traces are difficult to expand, making generalization ability even more imperative. We follow [2022] to conduct generalization experiments on the eOBQA dataset and the eQASC dataset respectively, which can be regarded as a single-step entailment tree, that is, $s_1$ and $s_2$ from the candidate sentences can form as $s_1 \\land s_2 \\Rightarrow h$. We select applicable cases and apply them to the Task2 model. P@1 and NDCG will be exerted to evaluate the model [2020]. The results in Table 5 show nearly 6% and 5% improvements on the P@1, which demonstrates that our proposed model has better generalization ability in the single-step entailments."}, {"title": "7 Conclusion", "content": "We propose a method for integrating the hierarchical semantics of sentences to generate explanatory entailment trees iteratively, which consists of the hierarchical semantic encoder, selection controller, and intermediate generator. In light of the representation of entities and relation nodes in translate models, we innovatively designed the hierarchical semantic encoder with the connection between the premise and the conclusion as the intention. Experimental results compared with other baselines demonstrate that our architecture has comparable advantages. From the above phenomena, it can be seen that retrieval and reasoning performance is still an essential bottleneck. We will investigate efficient paradigms based on semantic features more thoroughly in the future."}]}