{"title": "Boosting K-means for Big Data by Fusing Data Streaming with Global Optimization", "authors": ["Ravil Mussabayeva", "Rustam Mussabayev"], "abstract": "K-means clustering is a cornerstone of data mining, but its efficiency deteriorates when confronted with massive datasets. To address this limitation, we propose a novel heuristic algorithm that leverages the Variable Neighborhood Search (VNS) metaheuristic to optimize K-means clustering for big data. Our approach is basen on the sequential optimization of the partial objective function landscapes obtained by restricting the Minimum Sum-of-Squares Clustering (MSSC) formulation to random samples from the original big dataset. Within each landscape, systematically expanding neighborhoods of the currently best (incumbent) solution are explored by reinitializing all degenerate and a varying number of additional centroids. Extensive and rigorous experimentation on a large number of real-world datasets reveals that by transforming the traditional local search into a global one, our algorithm significantly enhances the accuracy and efficiency of K-means clustering in big data environments, becoming the new state of the art in the field.", "sections": [{"title": "1. Introduction", "content": "The process of clustering represents a foundational function, consisting in detecting clusters of objects that share similarities within a specified object collection. This task is complicated by the swift expansion of digital information and is utilized across a multitude of applied fields (e.g., image analysis, customer segmentation, etc.). Cluster analysis is a multifaceted field with various models, one of which is the Minimum Sum-of-Squares Clustering (MSSC) [1]. At its core, the MSSC is about finding the best way to group a set of m data points within a multi-dimensional space. The goal is to locate k central points, known as centroids, in such a way that the total squared distances from each data point to the nearest centroid are minimized."}, {"title": "", "content": "Mathematically, this is expressed as:\nmin f (C, X) = \u03a3 min ||xi - cj||2\nC i=1 j=1,...,k\nHere, the challenge is to find the optimal cluster centers C that minimize the distance equation, where || || denotes the Euclidean norm. The equation above defines the main task of the MSSC, called the sum-of-squared distances. When solved, each solution creates a specific partitioning of the data points into clusters.\nIt is worth noting that the MSSC problem is far from simple. In fact, MSSC is classified as an NP-hard problem [1]. This complexity makes it a global optimization problem with the overarching objective of dividing a dataset into distinct groups or clusters.\nOne of the standout features of MSSC is how it operates. By minimizing the expression in Equation (1), it automatically reduces the similarity within the same cluster and increases the difference between separate clusters. This dual-action approach makes MSSC highly effective and provides an essential measure of clustering accuracy. It not only helps in forming cohesive groups but also ensures that these groups are as distinct from each other as possible, which is often the fundamental objective in clustering algorithms.\nStudies have demonstrated that global minimizers offer a more precise representation of the underlying clustering patterns within a dataset [2]. However, finding these global minimizers within the context of MSSC is no easy task due to the complex, non-convex and non-smooth nature of the objective function.\nTo tackle this difficulty, various strategies have been devised. These include gradient-based optimization, stochastic optimization algorithms, and metaheuristic search strategies, among others. Each of these approaches has its particular focus:\n\u2022 Gradient-based techniques tend to reach local minimizers quickly but can become stuck in suboptimal solutions, owing to the non-convex form of the objective function;\n\u2022 Stochastic optimization algorithms, on the other hand, use randomness to break free from local minima, thus exploring a more extensive part of the solution space;\n\u2022 Metaheuristic search strategies attempt to find a middle ground between exploring new possibilities and exploiting known solutions;\n\u2022 Hybrid methods combining the above techniques in various ways are often employed to capitalize on the strengths of each. This enables them to uncover unique benefits over individual approaches.\nDespite the array of optimization methods available, no single approach has emerged as the definitive solution for this highly non-convex challenge. Each has its unique strengths and shortcomings. This state of affairs emphasizes the ongoing need for further research in this area. Developing more efficient and effective methods for finding global minimizers is not just a theoretical concern; it is a practical necessity in the expanding field of pattern recognition for"}, {"title": "", "content": "big data. By recognizing the complexity of this issue and acknowledging that there is no universal solution, we open the door for innovation and advancement in clustering techniques.\nIn this work, we present an algorithm, BigVNSClust, whose core concept is to conduct a simultaneous search across two modalities: (1) exploring partial solution landscapes derived from random samples of the original dataset, and (2) cycling through increasingly expansive neighborhoods within these landscapes to refine the incumbent solution. For the latter, a special neighborhood structure can be defined, which defines two solutions to be neighboring if they differ only in a certain number of centroids. Exploring this structure according to some suitable metaheuristic may give enough guidance to achieve a more educated traversal of solutions.\nExploiting both of these modalities allows the algorithm to effectively shake the incumbent solution, and thereby escape unfavorable local minima. By gaining control over the amount of input data we feed into the problem, the algorithm efficiently reduces the time complexity and adaptively scales to big data. Moreover, integrating an advanced metaheuristic technique, Variable Neighborhood Search (VNS) [3], into the search within each consecutive solution landscape enables a more detailed exploration of its local characteristics and peculiarities, including such pitfalls as degenerate clusters or fragmentation of a single true cluster by several centroids.\nMore specifically, one of the critical drawbacks of the existing initialization-sensitive iterative approaches, like K-means++, lies in their inability to handle the situation when two or more initial centroids fall inside a single cluster that is well-separated from the other data points by a large margin. This occurs due to difficulties for centroids in traveling across the empty space of the margin by shifting centroids to the means, even if the original input dataset is sparsified well enough by drawing a small random sample. We empirically demonstrate this issue in Section (...) of the article.\nThus, in each iteration of the algorithm, the degenerate clusters and p random centroids of the incumbent solution C are reinitialized using K-means++ on a new random sample S from X. This initial step amounts to shaking the incumbent solution. Variable p is called the shaking power. It is cyclically incremented across iterations, being bounded by the hyperparameter Pmax. Then, the local search (K-means) is commenced on S from the incumbent solution, seeking a possibly better one in the current neighborhood. The iterations proceed until a predefined time limit T.\nAll in all, our study investigates the potential benefits of combining big data clustering with a cutting-edge optimization metaheuristic, exploring the possibilities of hybridization to enhance clustering outcomes. We introduce a novel clustering heuristic named BigVNSClust, which seeks to amplify the global optimization capabilities of standard approaches. By integrating a powerful metaheuristic framework, Variable Neighborhood Search (VNS) [3], we develop a refined approach to clustering. Through a comprehensive experimental analysis performed on a large number of real-world datasets, BigVNSClust demonstrates state-of-the-art performance within the family of MSSC clustering algorithms. Notably, it surpasses its closest competitor, Big-means [4], solidifying its position as a robust and innovative advancement in the field.\nThis paper has the following outline. Section 2 provides an elementary introduction to the VNS metaheuristic."}, {"title": "2. Variable Neighborhood Search", "content": "In the realm of computer science, artificial intelligence, and mathematical optimization, heuristics represent essential tools designed to expedite problem solving. They are used when conventional methods prove too slow or when the quest for an exact solution becomes intractable. It is crucial to acknowledge, however, that heuristics do not provide an absolute guarantee of uncovering the optimal solution, often falling under the category of approximate algorithms. Typically, these algorithms excel at swiftly and efficiently generating solutions that closely approximate the optimal one. On occasion, they may even attain highest accuracy in identifying the absolute best solution. Nevertheless, they retain their heuristic classification until the solutions produced by them are proved to be optimal [5]. In a broader context, metaheuristics serve as versatile frameworks for crafting heuristics to tackle a diverse array of combinatorial and global optimization challenges [6].\nAn optimization problem can be generally formulated as\nmin{f(x) | x \u2208 S C S},\nwhere:\n\u2022 S represents the ambient solution space;\n\u2022 S denotes the feasible set within S;\n\u2022 x is a feasible solution in S;\n\u2022 f is a real-valued objective function.\nHenceforth, the symbol S is used to represent two distinct concepts: firstly, a feasible solution set S within the ambient solution space S; and secondly, a specific sample S extracted from the entire dataset X. The particular interpretation of S in any given instance should be readily discernible to the reader based on the surrounding context since the feasible solution space is S = S = R\" in the context of MSSC (1).\nVariable Neighborhood Search (VNS), introduced by Mladenovic in 1997 [3], represents a contemporary meta-heuristic approach offering a versatile framework for effectively addressing combinatorial and continuous non-linear global optimization problems. VNS systematically exploits the idea of neighborhood change, both in descent to local minima and in escape from the valleys containing them [3, 7, 6]. It explores distant neighborhoods of the current solution and moves to a new one if and only if this movement leads to an improvement in the objective function."}, {"title": "2.2. Shaking procedure", "content": "A shaking procedure is a necessary step that allows to escape local optima traps.\nThe most straightforward shaking procedure involves selecting a random solution from the neighborhood Nk(x), with k being predetermined. For certain problems, a completely random jump within the k-th neighborhood may result in an overly aggressive perturbation. As a consequence, an \"intensified shaking\u201d approach is sometimes favored. This method considers the sensitivity of the objective function to minor variations in the variable x. Nevertheless, for the purposes of this chapter, our definition of the shaking procedure will align with the simpler, random-selection approach. The corresponding pseudocode for this shaking procedure is provided in Algorithm 1."}, {"title": "2.3. Neighborhood change step", "content": "The neighborhood change procedure aims to steer the direction of the variable neighborhood search heuristic throughout the solution space. Specifically, it dictates which neighborhood will be probed next and determines whether a given solution should be adopted as the new incumbent. While various versions of the neighborhood change step can be found in the literature [8], the sequential and cyclic methods are the most prevalent.\n1. Sequential neighborhood change. The steps of this procedure are listed in Algorithm 2. If an improvement of the incumbent solution in some neighborhood occurs, then the search is resumed in the first neighborhood (according to the predefined order in N) of the neighborhood structure at the new incumbent solution; otherwise, the search is continued in the subsequent neighborhood of the incumbent solution."}, {"title": "2.4. Improvement procedures", "content": "There are two main types of improvement procedures that can be used inside a VNS heuristic: local search and variable neighborhood descent (VND) [8].\nA local search heuristic offers a fundamental approach to solution improvement. At each iteration, it delves into the neighborhood structure N(x) of the incumbent solution x. The process starts with this incumbent solution. If a superior"}, {"title": "2.5. Basic Variable Neighborhood Search", "content": "Unlike many other metaheuristics, Variable Neighborhood Search (VNS) and its various extensions are characterized by their simplicity, often requiring minimal or even zero tuning of parameters. Consequently, besides delivering high-quality solutions, often in more straightforward fashion compared to alternative approaches, VNS also offers insights into the underlying reasons for such effectiveness. These insights can, in turn, pave the way for the development of more efficient and refined implementations [6].\nThe pseudocode of the basic VNS is presented in Algorithm 6, while Figure 1 schematically shows the process of VNS optimization applied to some non-convex function f.\nWithin the basic VNS framework, the following key components can be identified:\n\u2022 f(x): Real-valued objective function;\n\u2022 k: Shaking intensity;"}, {"title": "3. Proposed algorithm", "content": "The pseudocode of the proposed BigVNSClust algorithm is shown in Algorithm 7. In the context of MSSC, where k already denotes the number of clusters, we use p and Pmax to represent the current and maximum shaking powers, respectively."}, {"title": "3.2. Analysis of the algorithm", "content": "The proposed algorithm iteratively builds and optimizes partial solution landscapes of the MSSC problem restricted to relatively small input data samples S randomly drawn from original dataset X. The K-means algorithm plays the role of a local search for identifying best solutions in cyclically expanding neighborhoods of incumbent solution C.\nAlso, BigVNSClust employs an explicit procedure for shaking the incumbent solution. After the local search phase, exactly p random centroids, including any degenerate ones, are reinitialized in the incumbent solution C. Clearly, as p increases, the perturbation applied to C becomes more intense. In the edge case when Pmax k and p reaches Pmax, the whole incumbent solution is reinitialized according to the K-means++ logic. This amounts to a total restart. However, it is most practical to set Pmax much smaller than k. This limits the strength of applied perturbations, and therefore helps to avoid a complete or near-complete restart.\nThe shaking power p is increased in every iteration. This allows to incrementally, but in a limited manner, expand the search space for the local search procedure in each new partial solution landscape. By employing the K-means++ logic to sample new centroids for every shaken incumbent solution, we ensure that the new centroids are distributed"}, {"title": "3.3. Time complexity", "content": "In Algorithm 7, the K-means++ reinitialization of p and all degenerate clusters (as seen in line 9) shares the same time complexity, O(s \u00b7 n\u00b7k), with a single iteration of the K-means local search. This equivalence arises since all operations are executed on a sample S of size s. The additional segment in lines 19 to 22 of Algorithm 7, which deals with the incrementation of shaking power p, operates at a complexity of O(1). Consequently, the overall time complexity of a single iteration of the BigVNSClust method remains O(s \u00b7 n\u00b7k), consistent with the preceding Big-means algorithm."}, {"title": "4. Experimental evaluation", "content": "We conducted our experiments on a system running Ubuntu 22.04 64-bit, powered by an AMD EPYC 7663 processor with 8 active cores. The system had 1.46 TB of RAM. Our software setup included Python 3.10.11, NumPy 1.24.3, and Numba 0.57.0. We used Numba [12] to speed up Python code execution and enable parallel processing. Numba's ability to compile Python code into machine code and run it on multiple processors made it a valuable tool for our experiments."}, {"title": "4.2. Competitive algorithms", "content": "A detailed experimental analysis is performed to quantify the enhancements brought by BigVNSClust in comparison with the state-of-the-art Big-means algorithm [4]. To ensure a fair and focused comparison, we evaluate both algorithms under the same conditions, employing the inner parallelization technique, which was proposed in [13]. Investigating the optimal parallelization approach for the BigVNSClust algorithm might be a subject for future research."}, {"title": "4.3. Datasets", "content": "We conducted experiments on 23 datasets, comprising 19 publicly available and 4 normalized datasets. These datasets are identical to those used in [4], where additional information and URLs can be accessed. Our datasets are numerical, with no missing values, and exhibit significant diversity in terms of size (ranging from 7,797 to 10,500,000 instances) and attribute count (varying from 2 to 5,000). This diversity allows us to evaluate BigVNSClust's adaptability across various data scales. Furthermore, we followed the methodology of Karmitsa et al. [14] to facilitate comparative analysis."}, {"title": "4.4. Experimental design and evaluation metrics", "content": "We performed clustering on each dataset nexec times, using different numbers of clusters (k): 2, 3, 5, 10, 15, 20, and 25. Each clustering run was considered a separate experiment. We assessed each experiment by measuring the relative error (e) and CPU time (t). The relative error metric compared the algorithm's performance to the historical best results (f*), using the formula \u025b = 100. (f - f*)/f*. If the relative error is negative, it means the algorithm performed even better than expected.\nOur experimental results are organized in a custom table format. Each algorithm and the combination of a dataset with a cluster number (X, k) generates a set of nexec experiments. We computed the minimum, median, and maximum relative accuracy & and time t values for each set, averaging across multiple runs. The tables show the average values of these metrics for each dataset, aggregated across various k values.\nFor example, consider a table entry for a specific algorithm: ISOLET #Succ = 6/7; Min = 0.01; Median = 0.24; Max = 0.59. This entry means that for each of the 7 different cluster numbers (k = 2, 3, 5, 10, 15, 20, and 25), we ran multiple experiments. For each combination of dataset and cluster number, we performed 15 independent runs of each algorithm. In total, we have 7 sets of runs for each algorithm, with each set containing 15 results. The \"#Succ\" value of 6/7 indicates that, for 6 out of 7 sets, the median performance of this algorithm was better than the median performance of all other algorithms.\nThe last rows of the tables summarize the overall performance of each algorithm across all datasets. We bolded the top-performing results for each metric and dataset pair to highlight the best algorithms. An algorithm is deemed successful when its median performance for a given cluster number (k) matches or surpasses the best result among all algorithms for that cluster number."}, {"title": "4.5. Hyperparameter selection", "content": "We set a CPU time limit (T) and stopped the K-means clustering process if it took too long (over 300 iterations) or made very little progress (less than 0.0001 improvement between steps). For K-means++, we chose three candidate points for each new centroid. We fine-tuned sample sizes based on initial tests to ensure optimal performance. The exact values for T and nexec are listed in the supplementary material's detailed tables."}, {"title": "4.6. Experimental results", "content": "The total number of conducted experiments reached 7,366.\nIn our experimentation, we did not detect significant correlation between the obtained results and the choice of a neighborhood change procedure. However, the cyclic neighborhood change procedure produced slightly better final accuracy & than the sequential one.\nAlso, we explored two distinct methodologies for centroid shaking within our algorithm: a uniformly random approach and a reinitialization based on the K-means++ strategy. It was observed that, while the uniformly random"}, {"title": "", "content": "approach can be executed more rapidly than its K-means++ counterpart, it resulted in a marked decrease in final accuracy, measured by \u025b. Specifically, the accuracy deteriorated by up to fivefold in comparison to the K-means++ reinitialization. This significant discrepancy in performance can be attributed to the nature of the uniformly random method, which introduces an excessive level of perturbation to the existing solution. Such a drastic alteration necessitates a considerably higher number of iterations in the subsequent local search phase to converge to an optimal solution.\nFor every algorithm, dataset X, and number of clusters k, the minimum, median, and maximum values of relative accuracy & and CPU time t were calculated with respect to nexec runs. To determine the optimal value of the maximum shaking power Pmax, all the experiments were restarted with different values of pmax. The resulting performance of the proposed BigVNSClust algorithm across various values of pmax is shown in Table 1."}, {"title": "4.7. Synthetic experiment", "content": "In every BigVNSClust iteration, the sampling process sparsifies the given dataset to some extent. We anticipate that this sparsification enables the initial centroids to shift more freely between the clusters, provided the clusters overlap. In essence, sampling with a carefully selected sample size enhances the flexibility of centroids within the current solution while still maintaining a reasonable approximation of the original dataset. Consequently, it is crucial to strike the appropriate balance between these two objectives when determining the sample size.\nTo examine this hypothesis further, we have conducted additional experiments wherein we assess the performance of BigVNSClust using carefully generated synthetic data.\nFirst, consider a mixture of 3 isotropic Gaussian distributions {N1, N2, N3} with standard deviations 0.15, 0.08, 0.1, respectively. Let \u03bc\u2081 = [0.2,0.5]7,\u03bc2 = [0.7,0.8]7, \u03bc3 = [0.5, 1.0]7 be the coordinates of the means of these distributions. Let X\u2081 be the dataset obtained by sampling 3000 points from N\u2081, 1500 points from N2, and 1500 points from N3. Dataset X\u2081 is shown in Figure 2a. If we assign the initial centroids C = {C1, C2, C3} as c\u2081 = [0.1, 0.2]7, c2 = [0.1, 0.15]7, c3 = [0.5, 1.0]7, then the result of running K-means++ on X\u2081 with C as initial centroids is shown in Figure 2b. The value of the objective function f({\u03bc1, \u03bc2, \u03bc3}, \u03a7\u2081) for the ground truth is 171.5. We see that K-means++ was trapped in a local minimum with the objective value of 191.52. Choosing the sample size s = 70 and T = 1.5, it is possible to obtain a result of BigVNSClust as shown in Figure 2c, with the objective value equal to 172.08. Hence, BigVNSClust managed to avoid being trapped in a bad local minimum."}, {"title": "4.8. Analysis of obtained results", "content": "The summarized experimental results reveal that incorporating the VNS metaheuristic into the Big-means scheme leads to a significant boost of the clustering accuracy e. Concretely, the accuracy increased by more than threefold in comparison to the original Big-means algorithm.\nWe note that the integration of a new advanced parallelization method could enhance BigVNSClust's performance even further. Exploring such a parallelization method may be an exiting direction for future research.\nIn spite of the fact that BigVNSClust exhibited slightly worse results than Big-means with respect to the processing time t, its time results are still much better than those of all other advanced parallel HPClust versions from [13]. One must also acknowledge the fact that the current average time result of 3.0 seconds achieved by BigVNSClust is practically acceptable in most cases. Thus, BigVNSClust can be readily recommended for immediate use by practitioners as a powerful tool for solving industry-level big data clustering problems.\nThe higher resulting accuracy of BigVNSClust is attributed to the novel shaking properties introduced by the VNS metaheuristic. This iterative shaking of the incumbent solution addresses a key issue with Big-means, discussed in Subsection 4.7. Specifically, BigVNSClust inherently possesses a natural shaking property due to its iterative sampling approach. The addition of VNS further enhances this by iteratively reassigning a portion of the incumbent centroids. This process not only shakes the solution landscape more intensely but also allows centroids from distinct, non-overlapping clusters to transition between each other's clusters, facilitating better solution exploration."}, {"title": "5. Conclusion and future research", "content": "In this work, we proposed BigVNSClust, a novel VNS-based and big-data-efficient MSSC algorithm. Also, the basics of VNS were reviewed, while the most efficient choice of VNS ingredients has been empirically established to ensure superior algorithm performance. The conducted experimental analysis confirmed that BigVNSClust provides a significant increase in the obtained accuracy over the state-of-the-art Big-means approach, being on par in this performance metric with other advanced HPClust parallel schemes (competitive, cooperative, and hybrid). Also, BigVNSClust showed a much better time efficiency compared to the advanced parallel HPClust versions, while being only slightly inferior to HPClust-inner in that regard.\nThese results convince us that the VNS metaheuristic favorably combines with the natural shaking properties of the problem decomposition approach, producing a novel algorithm with highly desirable practical qualities. This result was achieved at the cost of minimal algorithmic complexity. Exploring more advanced ways to parallelize BigVNSClust is a promising future research direction that can make the algorithm even more effective and efficient.\nMore possible future research directions include: the development of a tool that would allow to automatically choose the appropriate parameter triple (pmax, s, T) for each individual dataset, extension of the algorithm to the clustering paradigms other than MSSC, and exploring combinations of other well-known metaheuristics that can lead to an even more significant improvement of Big-means."}]}