{"title": "BOUNDLESS SOCRATIC LEARNING\nWITH LANGUAGE GAMES", "authors": ["Tom Schaul"], "abstract": "An agent trained within a closed system can master any desired capability, as long\nas the following three conditions hold: (a) it receives sufficiently informative and\naligned feedback, (b) its coverage of experience/data is broad enough, and (c) it\nhas sufficient capacity and resource. In this position paper, we justify these condi-\ntions, and consider what limitations arise from (a) and (b) in closed systems, when\nassuming that (c) is not a bottleneck. Considering the special case of agents with\nmatching input and output spaces (namely, language), we argue that such pure\nrecursive self-improvement, dubbed 'Socratic learning, can boost performance\nvastly beyond what is present in its initial data or knowledge, and is only limited\nby time, as well as gradual misalignment concerns. Furthermore, we propose a\nconstructive framework to implement it, based on the notion of language games.", "sections": [{"title": "INTRODUCTION", "content": "On the path between now and artificial superhuman intelligence (ASI; Morris et al., 2023; Grace\net al., 2024) lies a tipping point, namely when the bulk of a system's improvement in capabilities\nis driven by itself instead of human sources of data, labels, or preferences (which can only scale\nso far). As yet, few systems exhibit such recursive self-improvement, so now is a prudent time to\ndiscuss and characterize what it is, and what it entails.\nWe focus on one end of the spectrum, the clearest but not the most practical one, namely pure\nself-contained settings of 'Socratic' learning, closed systems without the option to collect new in-\nformation from the external world. We articulate conditions, pitfalls and upper limits, as well as a\nconcrete path towards building such systems, based on the notion of language games.\nThe central aim of this position paper is to clarify terminology and frame the discussion, with an\nemphasis on the long run. It is not to propose new algorithms, nor survey past literature; we pay no\nheed to near-term feasibility or constraints. We start with a flexible and general framing, and refine\nand instantiate these definitions over the course of the paper."}, {"title": "DEFINITIONS", "content": "Consider a closed system (no inputs, no outputs) that evolves over time (see Figure 1 for an illus-\ntration). Within the system is an entity with inputs and outputs, called agent, that also changes over\ntime. External to the system is an observer whose purpose is to assess the performance of the\nagent. If performance keeps increasing, we call this system-observer pair an improvement process.\nThe dynamics of this process are driven by both the agent and its surrounding system, but setting\nclear agent boundaries is required to make evaluation well-defined: in fact an agent is what can be\nunambiguously evaluated. Similarly, for separation of concerns, the observer is deliberately located\noutside of the system: As the system is closed, the observer's assessment cannot feed back into\nthe system. Hence, the agent's learning feedback must come from system-internal proxies such as\nlosses, reward functions, preference data, or critics.\nThe simplest type of performance metric is a scalar score that can be measured in finite time, that\nis, on (an aggregation of) episodic tasks. Mechanistically, the observer can measure performance"}, {"title": "THREE NECESSARY CONDITIONS FOR SELF-IMPROVEMENT", "content": "Self-improvement is an improvement process as defined above, but with the additional criterion\nthat the agent's own outputs (actions) influence its future learning. In other words, systems in which\nagents shape (some of) their own experience stream, potentially enabling unbounded improvement\nin a closed system. This setting may look familiar to readers from the reinforcement learning com-\nmunity (RL; Sutton, 2018): RL agents' behaviour changes the data distribution it learns on, which\nin turn affects its behaviour policy, and so on. Another prototypical instance of a self-improvement\nprocess is self-play, where the system (often a symmetric game) slots the agent into the roles of both\nplayer and opponent, to generate an unlimited experience stream annotated with feedback (who\nwon?) that provides direction for ever-increasing skill-learning.\nFrom its connection to RL, we can derive necessary conditions for self-improvement to work, and\nhelp clarify some assumptions about the system. The first two conditions, feedback and coverage,\nare about feasibility in principle, the third (scale) is about practice."}, {"title": "FEEDBACK", "content": "Feedback is what gives direction to learning; without it, the process is merely one of self-\nmodification. In a closed system where the true purpose resides in the external observer, but can\nnot be accessed directly, feedback can only come from a proxy. This creates the fundamental chal-\nlenge for system-internal feedback is be aligned with the observer, and remain aligned throughout\nthe process. It places a significant burden on the system at set-up time, with the most common pitfall\nbeing a poorly designed critic or reward function that becomes exploitable over time, resulting in"}, {"title": "COVERAGE", "content": "By definition, a self-improving agent determines the distribution of data it learns from. To prevent\nissues like collapse, drift, exploitation or overfitting, it needs to preserve\u00b2 sufficient coverage of the\ndata distribution everywhere the observer cares about. In most interesting cases, where performance\nincludes a notion of generalisation, that target distribution is not given (the test tasks are withheld),\nso the system needs to be set up to intrinsically seek coverage, a sub-process classically called\nexploration (Ladosz et al., 2022). Note that aligned feedback is not enough for this on its own:\neven if a preferred behaviour is never ranked lower than a dispreferred one, that is not tantamount to\ngaranteeing that the agent will find the preferred behaviour."}, {"title": "SCALE", "content": "The research field of RL has produced a lot of detailed knowledge about how to train agents, which\nalgorithms work in which circumstances, an abundance of neat tricks that address practical concerns,\nas well as theoretical results that characterize convergence, learning dynamics, rates of progress,\netc. It would be futile to try and summarize such a broad body of work here, but see Patterson\net al. (2023) for a primer. However, one general observation that matters for our argument is that\n'RL works at scale': in other words, when scaling up experience and compute sufficiently, even\nrelatively straightforward RL algorithms can solve problems previously thought out of reach (high-\nprofile examples include: Tesauro et al., 1995; Mnih et al., 2015; Silver et al., 2016; 2018; Vinyals\net al., 2019; AlphaProof & AlphaGeometry, 2024). For any specific, well-defined practical problem,\nthe details matter (and differ), and greatly impact the efficiency of the learning dynamics; but the\nasymptotic outcome seems a foregone conclusion. The \u2018bitter lesson' of Sutton (2019) argues a\nrelated point: betting on scaling up computation (as opposed to building in human knowledge)\nhas consistently paid off in the history of AI. Hence, with an availability of compute that keeps\nexpanding, the resource constraints of agents (memory and compute) may be a transient concern;\nnot all inefficiencies need to be fixed fully."}, {"title": "SOCRATIC LEARNING", "content": "The specific type of self-improvement process we consider here is recursive self-improvement,\nwhere the agent's inputs and outputs are compatible (i.e., live in the same space), and outputs become\nfuture inputs. This is more restrictive but less mediated than the general case where outputs merely\ninfluence the input distribution, most commonly instantiated by a (complex) environment that maps\nagent outputs into inputs. This type of recursion is an attribute of many open-ended processes, and\nopen-ended improvement is arguably a central feature of ASI (see Hughes et al., 2024). On the other\nhand, compatibility is less restrictive than homoiconic self-modification, see Section 6.\nAn excellent example of such a compatible space of inputs and outputs is language. A vast range\nof human behaviours are mediated by, and well-expressed, in language, especially in cognitive\ndomains (which are definitionally part of ASI). As argued by Chalmers (2024) and a few centuries\nof rationalists before him (Cottingham, 1988), language may well be sufficient for thinking and"}, {"title": "EXAMPLE", "content": "To help make these ideas more concrete, we describe a hypothetical but not a priori im-\nplausible system (cf. Poesia et al., 2024). Consider the domain of mathematical statements\n(a subset of language). The observer's performance metric is binary: has a proof for the\nRiemann hypothesis been found? The agent reads and writes mathematical statements and\nproofs (which are compatible input/output spaces). The system is closed, and contains the\nagent plus:\n\u2022 a proof verifier (e.g., Lean)\n\u2022 a collection C of theorems or conjectures.\n\u2022 a proxy reward for the agent: +1 for each verified new proof of a statement in C.\n\u2022 a second collection L of lemmas (or subgoals), initially empty.\nThe system allows the agent to produce proofs, verify them, formulate new statements, and\nadd those to L. Over time, the agent may learn to simplify and decompose existing theorems,\naccumulate lemmas in L, learn to formulate lemmas that are more and more reusable, and\nincrease the fraction of theorems in C for which it can produce valid proofs. It self-improves.\nAt some point, the expanding frontier of verified mathematical knowledge reaches a proof of\nthe Riemann hypothesis, and the observer, satisfied, stops the system."}, {"title": "THE FUNDAMENTAL LIMITS OF SOCRATIC LEARNING", "content": "Among the three necessary conditions for self-improvement, two of them, coverage and feedback\napply to Socratic learning in principle, and remain irreducible. To make their implications as clear\nas possible, we ignore the third (the scale, practicality and efficiency concerns, see Section 2.3) in\nthis section. We motivate this simplification by taking the long view: if compute and memory keep\ngrowing exponentially, scale constraints are but a temporary obstacle. If not, considering a resource-\nconstrained scenario for Socratic learning (akin to studying bounded rationality) may still produce\nvalid high-level insights."}, {"title": "LANGUAGE GAMES ARE ALL YOU NEED ...", "content": "Fortunately, language, learning and grounding are well-studied topics. A particularly useful concept\nfor us to draw on is Wittgenstein's notion of language games. For him, it is not the words that\ncapture meaning, but only the interactive nature of language can do so. To be concrete here, define\na language game as an interaction protocol (a set of rules, expressible in code) that specifies the\ninteraction of one or more agents ('players') that have language inputs and language outputs, plus a\nscalar scoring function for each player at the end of the game.8\nLanguage games, thus defined, address the two primary needs of Socratic learning; namely, they\nprovide a scalable mechanism for unbounded interactive data generation and self-play, while au-\ntomatically providing an accompanying feedback signal (the score). In fact, they are the logical\nconsequence of the coverage and feedback conditions, almost tautologically so: there is no form of\ninteractive data generation with tractable feedback that is not a language game. As a bonus, seeing\nthe process as one of game-play immediately lets us import the potential of rich strategic diversity\narising from multi-agent dynamics (as spelled out in depth in Leibo et al., 2019; Du\u00e9\u00f1ez-Guzm\u00e1n\net al., 2023), which is likely to address at least part of the coverage condition. It also aligns with\nour intuition that dynamic, social co-construction (e.g., the circle of philosophers) has an edge over\nthe self-talk of a single person that lives for millennia. Pragmatically too, games are a great way\nto get started, given the vast human track record of creating and honing a vast range of games and\nplayer skills (Berne, 1968); with Nguyen (2020) framing this richness as a demonstration of the flu-\nidity of human agency and (local) motivations. Derrida might even argue that under the right lens,\ndiscourse is already structured as a game.10 Colas et al. (2022) discuss a related set of ideas under\nthe terminology of Vygotskian autotelic agents; while they do not assume a closed system, many\nof their 'internalised social interactions' could be cast as language games. A number of common\nLLM interaction paradigms are also well represented as language games, for example debate (Irving"}, {"title": "... IF YOU HAVE ENOUGH OF THEM...", "content": "Returning to our circle of deliberating philosophers: is there any one language game we could imag-\nine them playing for millennia? Instead, maybe, they are more likely to escape a narrow outcome\nwhen playing many language games. It turns out that Wittgenstein (him again) proposed this same\nidea: he adamantly argued against language having a singular essence or function.11\nUsing many narrow but well-defined language games instead of a single universal one resolves a\nkey dilemma: For each narrow game, a reliable score function (or critic) can be designed, whereas\ngetting the single universal one right is more elusive (even if possible in principle, as argued by\nSilver et al., 2021).12 From that lens, the full process of Socratic learning is then a meta-game,\nwhich schedules the language games that the agent plays and learns from (which is an 'infinite'\ngame as per Carse (2011)). We posit that in principle, this idea is sufficient to address the issue of\ncoverage (Section 2.2). Concretely, if a proxy of the observer's distribution of interest is available\n(e.g., a validation set of tasks), that can be used to drive exploration in the meta-game."}, {"title": "AND YOU PLAY THE RIGHT ONES", "content": "Socrates was famously sentenced to death and executed for 'corrupting the youth.' We can take this\nas a hint that a Socratic process is not guaranteed to remain aligned with external observers' intent.\nLanguage games as a mechanism do not side-step this either, but they arguably reduce the precision\nneeded: instead of a critic that is aligned at the fine granularity of individual inputs and outputs, all\nthat is needed is a 'meta-critic' that can judge which games should be played: maybe no individ-\nual language game is perfectly aligned, but what is doable is to filter the many games according to\nwhether they make an overall net-positive contribution (when played and learned about). Further-\nmore, the usefulness of a game does not need to be assessed a priori, but can be judged post-hoc,\nafter playing it for a while. Relatedly, a beneficial asymmetry is that it may be much easier to detect\ndeviant emergent behaviour post-hoc than to design games that prevent it. All of these properties\nare forms of structural leniency that give the language games framework a vast potential to scale.\nStepping out of our assumption of the closed system for a moment: when we actually build ASI, we\nwill almost surely want to not optimistically trust that alignment is preserved, but instead continually\ncheck the process as carefully as possible, and probably intervene and adjust the system throughout\ntraining. In that case, explicitly exposing the distribution of games (accompanied by interpretable\ngame descriptions and per-game learning curves) as knobs to the designer may be a useful level of\nabstraction."}, {"title": "HIGHER-LEVEL RECURSIONS", "content": "So far, we discussed the minimal necessary form of recursion, a form of circularity that feeds (some\nof) the agent's outputs back to it. Within the framework of language games, two further types of\nrecursion come to mind. The first idea is to tell the agent which game it is playing, and give it the\nchoice to switch games, which game to switch to, and when to switch (Pislar et al., 2021). This is\nrelated to hierarchical or goal-conditioned RL, providing the agent with more autonomy and a more\nabstract action space. While shifting more responsibility into the agent, this setup could dramatically\nimprove outcomes, as compared to a hardwired game-selection process outside of the agent-but of\ncourse this extra freedom could introduce additional risks of collapse or misalignment.\nSecond, as games are interaction protocols that can be fully represented as code, they can live in\na language agent's output space. Consequently, the agent could learn to generate games for itself\nto play. 13 Initially, it could simply produce local variations of exiting games, which adapt the dif-\nficulty level of theme, later on crafting recombinations of games, and ultimately ending up with de\nnovo generation (Todd et al., 2024). This leads to second-order coverage concerns, in the space of\nlanguage games instead of the space of language, to be addressed with filtering, prioritization, or\ncurricula (Jaderberg et al., 2019; Parker-Holder et al., 2022).\nThe combination of both of these recursive extensions is an empowered agent that plays the full\nmeta-game of how to improve itself via game generation and play. While appealingly elegant, this\nmeta-game lacks the well-defined feedback mechanism of the inner language games, and it is an\nopen research question whether established proxy metrics like learning progress would be sufficient\nto preserve both the coverage and alignment properties over time.\nSELF-REFERENTIAL SYSTEMS\nThe next and final step of recursion is recursive self-modification, that is, agents whose actions\nchange their own internals, not merely influencing their input stream. These methods live on a\nspectrum characterized by the scope of what can be modified in such a way (and which elements\nremain fixed), and what amount of introspection, or access to its own workings, is available to the\nagent (Schaul & Schmidhuber, 2010). At the extreme end, a fully self-referential agent can observe\nand modify any14 aspect of itself, without indirection. In principle, this type of agent has the highest\ncapability ceiling; as asymptotic performance is capped by its fixed structure, unfreezing some of\nit and making it modifiable can only increase that upper bound-in particular, it is always possible\nto set the newly-flexible parameters to how they were while frozen, to recover the performance of\nthe less-flexible agent (modulo learning dynamics that could get into the way). Past proposals for\nhow to design self-referential systems were not (intended to be) practical (e.g., Schmidhuber, 1993;\n2003; Schmidhuber et al., 1997; Kirsch & Schmidhuber, 2022), but modern LLMs' competence in\ncode comprehension and generation is changing the playing field and may soon move these ideas\nfrom esoteric to critical."}, {"title": "CONCLUSION: OPEN-ENDED SOCRATIC LEARNING IS POSSIBLE", "content": "We set out to investigate how far recursive self-improvement in a closed system can take us on\nthe path to AGI, and are now ready to conclude on an optimistic note. In principle, the potential\nof Socratic learning is high, and the challenges we identified (feedback and coverage) are well\nknown. The framework of language games provides a constructive starting point that addresses\nboth, and helps clarify how a practical research agenda could look like. We leave the fleshing out\nof that roadmap to future work, but the overall direction is becoming apparent. In particular, an\nunderstudied dimension is the breadth and richness of the many such language games. We think a\ngreat place to start is with processes capable of open-ended game generation. And not without seeing\nthe irony, we propose all these ideas to scrutiny within an academic setting instead of resorting to\nself-talk in a closed system."}]}