{"title": "DIFFUSION-BASED PLANNING FOR AUTONOMOUS DRIVING WITH FLEXIBLE GUIDANCE", "authors": ["Yinan Zheng", "Ruiming Liang", "Kexin Zheng", "Jinliang Zheng", "Liyuan Mao", "Jianxiong Li", "Weihao Gu", "Rui Ai", "Shengbo Eben Li", "Xianyuan Zhan", "Jingjing Liu"], "abstract": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance, due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles. Project website: https://zhengyinan-air.github.io/Diffusion-Planner/.", "sections": [{"title": "1 INTRODUCTION", "content": "Autonomous driving as a cornerstone technology, is poised to usher transportation into a safer and more efficient era of mobility (Tampuu et al., 2020). The key challenge is achieving human-like driving behaviors in complex open-world environment, while ensuring safety, efficiency, and comfort (Muhammad et al., 2020). Rule-based planning methods have demonstrated initial success in industrial applications (Fan et al., 2018), by defining driving behaviors and establishing boundaries derived from human knowledge. However, their reliance on predefined rules limits adaptability to new traffic situations (Hawke et al., 2020), and modifying rules demands extensive engineering effort. In contrast, learning-based planning methods acquire driving skills by cloning human driving behaviors from collected datasets (Caesar et al., 2021), a process made simpler through straightforward imitation learning losses. Additionally, the capabilities of these models can potentially be enhanced by scaling up training resources (Chen et al., 2023).\nThough promising, current learning-based planning methods still face several limitations. Firstly, human drivers often exhibit multi-modal behaviors in planning scenarios (Nayakanti et al., 2023). Existing methods that rely on behavior cloning lack a guarantee of fitting such complex data distributions, even when utilizing large transformer-based model architecture or sampling multiple trajectories (Cheng et al., 2023). Secondly, when encountering out-of-distribution (OOD) scenarios, directly using model output may result in low-quality planning outcomes, forcing many methods to"}, {"title": "2 RELATED WORK", "content": "Rule-based Planner. Rule-based methods rely on predefined rules to dictate the driving behavior of autonomous vehicles, offering a highly controllable and interpretable decision-making process (Treiber et al., 2000a; Fan et al., 2018; Dauner et al., 2023a). While they have been widely validated in real-world scenarios (Leonard et al., 2008; Urmson et al., 2008), these frameworks are limited in their ability to handle novel complex situations that fall beyond the predefined rules.\nLearning-based Planner. Learning-based planning focuses on leveraging methods such as behavior cloning in imitation learning to directly model human driving behaviors, which has emerged as a popular solution in autonomous driving, particularly in recent end-to-end training pipelines (Hu"}, {"title": "3 PRELIMINARIES", "content": "3.1 AUTONOMOUS DRIVING AND CLOSED-LOOP PLANNING\nThe primary objective of autonomous driving is to allow vehicles to navigate complex environments with minimal human intervention, where a critical challenge is closed-loop planning (Caesar et al., 2021). Unlike open-loop planning (Caesar et al., 2019) or motion prediction (Ngiam et al., 2021; Zhou et al., 2023), which only involves decision making that adapts to static conditions, closed-loop planning requires a seamless integration of real-time perception, prediction, and control. Vehicles must continuously assess their surroundings, predict the behavior of other neighboring vehicles, and implement precise maneuvers. The dynamic nature of real-world driving scenarios, combined with uncertainty in sensor data and environmental factors, makes closed-loop planning a formidable task.\n3.2 DIFFUSION MODEL AND GUIDANCE SCHEMES\nDiffusion Model. Diffusion Probabilistic Models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a class of generative models that generate outputs by reversing a Markov chain process known as the forward diffusion process. The transition distribution of the forward process satisfies:\n$q_{t|0}(x^{(t)}|x^{(0)}) = \\mathcal{N}(x^{(t)} | \\alpha_t x^{(0)}, \\sigma_t^2 I), t \\in [0, 1],$\nwhich gradually adds Gaussian noise to generate a series of noised data from \u00e6(0) to x(t) with t\u2208 [0,1]. \u03c3\u03c4 > 0 is a variance term that controls the introduced noise and at > 0 is typically defined as at = \u221a1 \u2013 07, ensuring \u00e6(t) \u2192 N(0, I), as t \u2192 1. The reversed denoising process of Eq. (1) can be equivalently expressed as a diffusion ODE (Song et al., 2021):\n(Diffusion ODE) $dx^{(t)}=[f(t)x^{(t)} - \\frac{1}{2}g^2(t)\\nabla_{x^{(t)}}\\text{log}q_t(x^{(t)})]dt,$"}, {"title": "4 METHODOLOGY", "content": "In this section, we redefine the planning task as a future trajectory generation task, which jointly generates the ego vehicle's planning and the prediction of neighboring vehicles. We then introduce the Diffusion Planner, a novel approach that leverages the expressive and flexible diffusion model for enhanced autonomous planning. Lastly, we demonstrate how the guidance mechanism in diffusion models can be utilized to align planning behavior with safe or human-preferred driving styles.\n4.1 TASK REDEFINITION\nAutonomous driving requires considering the close interaction between the ego and neighboring vehicles, resulting in a cooperative relationship between planning and motion prediction tasks (Ngiam et al., 2021). Supervising the future trajectories of neighboring vehicles has been shown to be helpful to enhance the ability of closed-loop planning models to handle complex interaction scenarios (Hu et al., 2023). For real-world deployment, motion prediction can also enhance safety by providing more controllable measures, facilitating the implementation of the system (Fan et al., 2018). Consequently, the trajectories of neighboring vehicles have become crucial privileged information for model training. However, the common approaches that use a dedicated sub module (Huang et al., 2023) or additional loss design (Cheng et al., 2023; Huang et al., 2023) to capture privileged information limit their modeling power during training and also lead to a more complex framework.\nIn this work, we address this issue by collectively considering the status of key participants in the driving scenario and jointly modeling the motion prediction and closed-loop planning tasks as a"}, {"title": "4.2 DIFFUSION PLANNER", "content": "Diffusion Planner is a model based on the DiT architecture (Peebles & Xie, 2023), with a core design focusing on the fusion mechanism between noised future vehicle trajectories x and conditional information C. Figure 1 provides an overview of the complete architecture. A detailed description of these interaction and fusion modules is provided as follows.\nVehicle Information Integration. In the first step, the future vehicle trajectory x is concatenated with the current state of each vehicle, represented as x\u00ba = [xego, neighbor 1,..., neighbor]T. This concatenation acts as a constraint to guide the model, simplifying the planning task by providing a clear starting point. Notably, velocity and acceleration information for the ego vehicle is excluded, which has been shown to enhance closed-loop performance, as highlighted in previous works (Cheng et al., 2023; Li et al., 2024). Integration of the information from different vehicles during model execution is achieved through multi-head self-attention mechanisms.\nHistorical Status and Lane Information Fusion. The historical status of neighboring vehicles and lane information is represented using vectors (Gao et al., 2020). Specifically, each neighboring vehicle is represented as Sneighbor \u2208 RLX Dneighbor, and lanes as Slane \u2208 RP \u00d7 Diane, where L refers to the number of past timestamps, and P indicates the number of points per polyline. Dneighbor contains data such as vehicle coordinates, heading, velocity, size, and category, while Dlane provides lane details such as coordinates, traffic light status, and speed limits. Since these vectors are information-sparse, directly fusing them would make training challenging. To address this, we use MLP-Mixer network (Tolstikhin et al., 2021) to extract information-dense representations. Compared to existing work (Huang et al., 2023; Cheng et al., 2023) that uses complex structural designs, we offer a more unified and simplified solution. This is achieved by iteratively passing the vectors through the MLP mixing layers, which operate on both the vector and feature dimensions. The forward process of each mixing layer can be formulated as follows:\n$S = S + MLP(S^T), \\bar{S} = \\bar{S} + MLP(\\bar{S})$\nWe use two separate MLP-Mixer networks for neighboring vehicles and lanes. Here, S represents the features for each neighboring vehicle or lane. After passing through multiple mixing layers, we apply pooling on the final output along the vector dimension. We also consider the static objects"}, {"title": "4.3 PLANNING BEHAVIOR ALIGNMENT VIA CLASSIFIER GUIDANCE", "content": "Enforcing versatile and controllable driving behavior is crucial for real-world autonomous driving. For example, vehicles must ensure safety and comfort while adjusting speeds to align with user preferences. Thanks to its close relationship to Energy-Based Models (Lu et al., 2023), diffusion model can conveniently inject such preferences via classifier guidance. It can steer the model outputs via gradient surgery during inference, offering significant potential for customized adaptation.\nSpecifically, given the original driving behavior q%(\u00e6(0)), we aim to encode additional guidance to reinforce some preferred behavior upon the existing behavior qo. This operation can be formulated as generating a target behavior: Po(x(0)) x qo (x(0))e-E(x(0)), where E(x(0)) can be some form of energy function that encodes safety or preferred behavior. As mentioned in Section 3.2, the gradient of the intermediate energy (Lu et al., 2023) is employed to adjust the original probability score, promoting the generation of trajectories within the target distribution. This process often necessitates an additional trained classifier to provide an accurate approximation. However, diffusion posterior sampling (Chung et al., 2022; Xu et al., 2025) offers a training free method that only uses the trained diffusion model \u03bc\u03b5 in Eq. (5) to approximate the guidance energy, bypassing the classifier training, which incurs additional computational overhead:\n$ \\nabla_{x^{(t)}} \\text{log} p_t (x^{(t)}) \\approx \\nabla_{x^{(t)}} \\text{log} q_t (x^{(t)}) - \\sqrt{\\gamma(t)} E_{q_{t|0}(x^{(0)}|x^{(t)})} [\\nabla_{x^{(0)}} E(x^{(0)})]$\n$= \\nabla_{x^{(t)}} \\text{log} q_t (x^{(t)}) - \\nabla_{x^{(t)}} E (\\mu_{\\theta} (x^{(t)},t, C)) .$\nOne restriction of this method is that Eq. (8) needs to use a pre-defined differentiable energy function E() to calculate the guidance energy. Fortunately, in autonomous driving scenarios, many trajectory evaluation protocols can be defined using differentiable functions. Next, we briefly describe some applicable energy functions that can be used to customize the planning behavior of the model, more details are shown in Appendix C.3.\n\u2022 Target speed maintenance: The speed difference is used as the energy, calculated by comparing the planned average speed with the set target speed.\n\u2022 Comfort: The energy function is calculated by measuring the amount by which the vehicle's state exceeds the predefined limits.\n\u2022 Collision avoidance: The signed distance between the ego vehicle and neighboring vehicles is computed at each timestamp.\n\u2022 Staying within drivable area: The distance the ego vehicle deviates outside the lane at each time step is calculated."}, {"title": "4.4 PRACTICAL IMPLEMENTATION FOR CLOSED-LOOP PLANNING", "content": "Data augmentation can help alleviate the out-of-distribution issue and is widely used in planning. Before training, we add random perturbations to the current state (Cheng et al., 2023). Then, interpolation is applied to create a physically feasible transition, enabling the model to resist perturbations and regress to the ground-truth trajectory (Bansal et al., 2018). After that, we transform the data from the global coordinate system into an ego-centric formulation through coordinate transformation. Considering the significant difference between the longitudinal and lateral distances traveled by the vehicle, z-score normalization is used to ensure the mean of the data distribution is close to zero, thereby further stabilizing the training process. During inference, DPM-Solver (Lu et al., 2022) is employed to achieve faster sampling, while low-temperature sampling (Ajay et al., 2022) enhances determinism in the planning process. We can complete trajectory planning for the next 8 seconds at 10 Hz, along with predictions for neighboring vehicles, with an inference frequency of approximately 20 Hz. Please see Appendix C for implementation details."}, {"title": "5 EXPERIMENTS", "content": "Evaluation Setups. We conduct extensive evaluations on the large-scale real-world autonomous planning benchmark, nuPlan (Caesar et al., 2021), to compare Diffusion Planner with other state-of-the-art planning methods. The Val14 (Dauner et al., 2023b), Test14, and Test14-hard benchmarks (Cheng et al., 2023) are utilized, with all experimental results tested in both closed-loop non-reactive and reactive modes. The final score is calculated as the average across all scenarios, ranging from 0 to 100, where a higher score indicates better algorithm performance. To further validate the algorithm's performance across diverse driving scenarios and with vehicles exhibiting different driving behaviors, we collected 200 hours of real-world data using a delivery vehicle from Haomo.AI. Unlike nuPlan, the delivery vehicle demonstrates more conservative planning behavior and operates in bike lanes, which involve dense human-vehicle interactions and unique traffic regulations. The collected data were integrated into the nuPlan framework, and the same evaluation metrics were applied in closed-loop simulations, as detailed in Appendix D.\nBaselines. The baselines are categorized into three groups (Dauner et al., 2023b): Rule-based, Learning-based, and Hybrid, which incorporate additional refinement to the outputs of the learning-based model. To enable a more comprehensive comparison, we utilize an existing refinement module (Sun et al., 2024), which applies offsets to the model outputs and scores all trajectories (Dauner et al., 2023b). Without any parameter tuning, we integrate this module as post-processing for the Diffusion Planner (Diffusion Planner w/ refine.). We compare the Diffusion Planner against the following baselines, with more implementation details provided in Appendix C.4.\n\u2022 IDM (Treiber et al., 2000b): A classic rule-based method implemented by nuPlan.\n\u2022 PDM (Dauner et al., 2023b): The first-place winner of the nuPlan challenge offers a rule-based version that follows the centerline (PDM-Closed), a learning-based version conditioned on the reference line (PDM-Open), and a hybrid approach that combines both (PDM-Hybrid).\n\u2022 UrbanDriver (Scheel et al., 2021): A learning-based method using policy gradient optimization and implemented by nuPlan.\n\u2022 GameFormer (Huang et al., 2023): Modeling ego and neighboring vehicle interactions using game theory (Game Former w/o refine.), followed by rule-based refinement.\n\u2022 PlanTF (Cheng et al., 2023): A state-of-the-art learning-based method built on a transformer architecture, exploring various designs suitable for closed-loop planning."}, {"title": "5.1 EMPIRICAL STUDIES OF DIFFUSION PLANNER PROPERTIES", "content": "Multi-modal Planning Behavior. We selected an intersection scenario and performed multiple inferences without low temperature sampling from the same initial position to obtain different possible outputs, in order to evaluate the model's ability to fit multi-modal driving behaviors. As shown in Figure 5, without navigation information, the vehicle can exhibit three distinct driving behaviors-left turn, right turn, and straight ahead with clear differentiation. When navigation information is provided, the model accurately follows it to make a left turn, demonstrating the diffusion model's ability to fit driving behaviors with varying distributions and its capacity for switching between them.\nFlexible guidance mechanism. Based on the trained Diffusion Planner model, different types of classifier guidance, as described in Section 4.3, are added during inference time without requiring additional training. We present two cases to demonstrate the effectiveness of guidance and its flexible composability, as shown in Figure 4. 1)"}, {"title": "5.2 ABLATION STUDIES", "content": "Design Choices for training. We demonstrate the effectiveness of key components of our method: data processing, the handling approach of ego current state, and the number of predicted vehicles. 1) We ablate the model's performance without using z-score normalization (w/o z-score norm), as well as without data augmentation (w/o augmentation), or by only perturbing the current state without applying interpolation to future trajectories (w/o interpolation). The results are summarized in Table 3. For the w/o z-score norm variant, even with ego-centric transformation, the data range remains large, making it difficult for the model to fit the distribution. The w/o augmentation variant faces out-of-distribution issues, leading to poor performance. Results also show that future trajectory interpolation is essential compared to perturbing only the current state. 2) We analyze the impact of the ego vehicle's current state on the model. Retaining velocity, acceleration, and yaw rate (w/ ego state) may lead to learning shortcuts, resulting in decreased planning capability. While a state dropout encoder (Cheng et al., 2023) (w/ SDE) mitigates this, directly discarding the information is more effective. Additionally, the w/o current state shows that adding current state information to the decoder improves planning capability. 3) We also ablate the choice of the number of M. Figure 6 shows that including too many neighboring vehicles in the decoder introduces noise, affecting the performance of the ego vehicle. However, most choices still outperform PlanTF.\nDesign Choices for Inference. We sweep two hyperparameters: the number of denoise steps and the magnitude of low-temperature sampling, as shown in Figure 7. Low temperature helps improve the stability of the output trajectories. Additionally, the model leverages DPM-Solver to achieve efficient denoising and remains robust across different step counts. We report the detailed parameter selection in Table 5."}, {"title": "6 CONCLUSION", "content": "We propose Diffusion Planner, a learning-based approach that fully exploits the expressive power and flexible guidance mechanism of diffusion models for high-quality autonomous planning. A transformer-based architecture is introduced to jointly model the multi-modal data distribution in motion prediction and planning tasks through a diffusion objective. Classifier guidance is employed to align planning behavior with safe or user preferred driving styles. Diffusion Planner achieves state-of-the-art closed-loop performance without relying on any rule-based refinement on the nuPlan benchmark and a newly collected 200-hour delivery-vehicle driving dataset, demonstrating strong adaptability across diverse driving styles. Due to space limit, more discussion on limitations and future direction can be found in Appendix E."}, {"title": "A VISUALIZATION OF CLOSED-LOOP PLANNING RESULTS", "content": ""}, {"title": "B ADDITIONAL RESULTS", "content": "B.1 COMPARED TO DIFFUSION-BASED PLANNING METHODS\nTo further demonstrate the advantages of our model, we compared it with two recent works using diffusion models for motion planning. Diffusion-es (Yang et al., 2024) enhances a diffusion model by incorporating an LLM as a trajectory filter. STR-16M (Sun et al., 2023) uses a diffusion model as a decoder. STR2-CPKS-800M (Sun et al., 2024) builds on the former with 800M parameters and includes a PDM-like refinement module. We compared the model's performance in non-reactive mode and recorded the inference time, as shown in Table 4. We observe that current diffusion-based methods also experience significant performance degradation when detached from LLMs or rule-based refinement. Another important point is that these methods, due to their reliance on LLMs"}, {"title": "B.2 MORE CASE STUDIES FOR THE GUIDANCE MECHANISM.", "content": ""}, {"title": "C EXPERIMENTAL DETAILS", "content": "This section outlines the experimental details to reproduce the main results in our papers.\nC.1 TRAINING DETAILS\nDatasets. We use the training data from the nuPlan dataset and sample 1 million scenarios for our training set. The number of different scenarios is shown in Figure 11. For each scenario, we"}, {"title": "C.2 INFERENCE DETAILS", "content": "We utilize DPM-Solver++ as diffusion reverse process solver, adopting variance-preserving(VP) noise schedule where the noise is ot = (1-t)\u1e9emin+t\u1e9emax. Low-temperature sampling is employed to further enhance the stability of the denoising process. We found that directly using the model output with a higher temperature facilitates generating high-quality trajectories. Conversely, if a refinement module is applied after the model output, a lower temperature helps produce more stable trajectories, which supports more accurate judgments by the refinement module. In addition, the model achieves an inference frequency of 20 Hz on a single A6000 GPU. We also report the detailed setup in Table 5."}, {"title": "C.3 CLASSIFIER GUIDANCE DETAILS", "content": "We then specifically introduce the mathematical formulation of the different energy functions, as mentioned in Section 4.3.\nCollision Avoidance. Based on the ego vehicle's planning and the neighboring vehicles' predictions from the decoder at diffusion timestamp t, we calculate the signed distance D between the ego vehicle and each neighboring vehicle at each timestamp 7. When the bounding boxes of the vehicles overlap, we use the minimum separation distance, otherwise, we use the distance between the nearest points. The energy function for collision avoidance is then defined as:\n$E_{collision} = \\frac{1}{w_c} \\sum_{i=1}^{M, \\tau}  \\frac{1_{D_i>0} \\cdot \\Psi \\big(w_c \\cdot \\text{max} \\big(1 - \\frac{D_i}{\\delta}, 0 \\big) \\big)}{\\sum_{i=1}^{M, \\tau} 1_{D_i>0} + eps}$ + $\\frac{1}{w_c} \\sum_{i=1}^{M, \\tau}  \\frac{1_{D_i<0} \\cdot \\Psi \\big(w_d \\cdot  \\text{max} \\big(1 - \\frac{\\delta}{D_i}, 0 \\big) \\big)}{\\sum_{i=1}^{M, \\tau} 1_{D_i<0} + eps}$"}, {"title": "Target Speed Maintenance.", "content": "We calculate the energy function based on the difference between the average speed of the generated trajectory and the target speed range:\n$E_{target\\_speed} =  \\text{max} \\big(\\frac{d x_{ego}}{d \\tau} - v_{low}, 0 \\big)^2 + \\text{max} \\big(\\frac{d x_{ego}}{d \\tau} - v_{high}, 0 \\big)^2$\nWhere Vlow is the setting lower bound of speed, Vlow is the setting higher bound of speed."}, {"title": "Comfort.", "content": "Taking longitudinal jerk as an example, the difference between each point and the comfort threshold is calculated, ignoring cases where the comfort requirements are met:\n$E_{comfort} = E \\sum_{\\tau} [\\text{max} \\big(j_{ego} - \\frac{d^3 x}{\\tau^3} \\Delta \\tau^3, 0 \\big)]$\nWhere jmax is the maximum longitude jerk limit."}, {"title": "Staying within Drivable Area.", "content": "We construct the differentiable cost map M by using Euclidean Signed Distance Field with parallel computation (Cheng et al., 2024), which can compute the distance the ego vehicle goes beyond the lane at each timestamp. Then the energy is defined as:\n$E_{drivable} = \\frac{1}{T}  \\sum_{\\tau} \\frac{\\Psi \\big(w_d \\cdot M (x_{ego}) \\big)}{1_{M(x)>0} + eps}$\nGiven the diverse options for energy function design, our choices were made primarily to validate whether the model could support various types of guidance and may not be optimal. However, through extensive empirical experiments, we can share some of our insights and experiences regarding energy function selection to assist future work in exploring more effective options:"}, {"title": "C.4 BASELINES SETUP", "content": "nuPlan Datasets Evaluation. For IDM and UrbanDriver, we use the official nuPlan code, with the UrbanDriver checkpoint sourced from the PDM codebase, which also provides the checkpoints for PDM-Hybrid and PDM-Open. For PlanTF and PLUTO, we use the checkpoints from their respective official codebases. In the case of PLUTO w/o refine, we skip the post-processing code and rerun the simulation without retraining. Following the guidelines from the official codebase, we train GameFormer and skip the refinement step to obtain GameFormer w/o refine.\nDelivery-vehicle Datasets Evaluation. We adopt the same metrics and models as those used on nuPlan, but by modifying various vehicle-related parameters to adapt the baselines to the delivery-vehicle training. Based on this, we retrain and test the models following the official training code."}, {"title": "D DETAILS ON DELIVERY VEHICLE EXPERIMENTS", "content": "We collected approximately 200 hours of real-world data using an autonomous logistics delivery vehicle from Haomo.AI. The task of the delivery vehicle is similar to that of a robotaxi in nuPlan, as it autonomously navigates a designated route. During operation, the vehicle must comply with traffic regulations, ensure safety, and complete the delivery as efficiently as possible. Compared to the vehicles in the nuPlan dataset, the delivery vehicle is smaller, as shown in Table 6, and operates at lower speeds. As a result, it is able to travel on both main roads and bike lanes. During deliveries, it frequently interacts with pedestrians and cyclists, and the driving rules differ from those for motor vehicles, as shown in 10. This dataset serves as a supplement to nuPlan, allowing for the evaluation of algorithm performance under diverse driving scenarios."}, {"title": "E LIMITATIONS & DISCUSSIONS & FUTURE WORK", "content": "Here, we discuss our limitations, potential solutions and interesting future works.\n\u2022 Scenario Inputs. Our method relies on vectorized map information and detection results of neighboring vehicles. Compared to mainstream end-to-end pipelines, this approach involves some information loss and requires a data processing module. However, unlike end-to-end methods, our focus is more on the planning stage, particularly on the ability for closed-loop planning."}, {"title": "Lateral Flexibility.", "content": "We find that learning-based methods struggle with flexibility, particularly when significant lateral movement is required. In contrast, rule-based methods perform better in this aspect due to the provision of a reference trajectory. Being consistent with findings from previous work (Li et al., 2024), we find this is mostly because that the dataset mainly consists of straight-driving scenarios, with few instances of lane changes or avoidance maneuvers. This makes it challenging for learning-based methods to generalize and acquire these skills. Additionally, since the model only outputs the planned trajectories instead of the controlling signal such as brake and throttle, there is a gap between the planned trajectory and the results from the downstream controller (Cheng et al., 2023). This discrepancy also leads to potential poor performance, or even out-of-distribution behavior, in scenarios that require more flexible actions.\nSolution and future work: We find that data augmentation can somewhat alleviate the issue of the vehicle being reluctant to make lateral movements, but it still performs poorly in cases requiring significant lane changes. This could be improved by incorporating more data involving large lateral progress, leveraging reinforcement learning with a reward mechanism, or designing a more effective diffusion guidance mechanism to help the model learn lane-changing behaviors. We believe this is an interesting observation and leave this direction for future work."}, {"title": "Sample Efficiency.", "content": "The high performance of Diffusion comes at the cost of requiring multiple model inferences, leading to reduced sample efficiency.\nSolution and future work: We addressed this issue to a large extent by using a high-order ODE solver, enabling trajectory planning for 8 seconds at 10 Hz in 0.05 seconds. Considering real-world application requirements, techniques such as consistency models (Song et al., 2023) or distillation-based sampling methods (Meng et al., 2023) could be employed for further acceleration.\nOverall, although some design choices may appear simple and certain limitations exist, we have thoroughly demonstrated the capabilities of diffusion models for closed-loop planning in autonomous driving through extensive experiments. Moreover, we demonstrate the potential of the diffusion model to align with safety or human-preferred driving behaviors. It provides a high-performance, highly adaptable planner for autonomous driving systems."}]}