{"title": "Measuring Diversity in Synthetic Datasets", "authors": ["Yuchang Zhu", "Huizhe Zhang", "Bingzhe Wu", "Jintang Li", "Zibin Zheng", "Peilin Zhao", "Liang Chen", "Yatao Bian"], "abstract": "Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets - an aspect crucial for robust model performance - remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have shown exceptional performance across a range of fields, such as chatbots (Achiam et al., 2023), computer programming (Gu, 2023), and reasoning (Yuan et al., 2024). Inspired by their remarkable capacities, some research (Ye et al., 2022; Abdullin et al., 2024; Ding et al., 2024) employs LLMs as dataset generators to mitigate the shortage of training data. Although generated data facilitates model optimization, recent studies (Yu et al., 2024; Lee et al., 2023) suggest that a lack of diversity within the dataset-measured by the variation between samples (Long et al., 2024)\u2014may lead to performance degradation in some scenarios. Although previous studies (Yu et al., 2024; Wang et al., 2022) leverage well-designed generation strategies to create highly diverse synthetic datasets, a crucial factor for measuring the diversity of these datasets has been overlooked. A principled diversity evaluation metric serves not only to guide LLM generators in creating more diverse data but also extends its utility to data selection (Cao et al., 2023), quantifying augmentation performance (Yang et al., 2024a), and assessing mode collapse (Dan Friedman & Dieng, 2023). Thus, a diversity evaluation method for synthetic datasets is becoming increasingly important.\nSince the diversity evaluation of synthetic datasets remains under-explored, a natural solution is to directly employ diversity evaluation methods from the fields of natural language processing (NLP) (Khurana et al., 2023) and machine learning (ML) (Jordan & Mitchell, 2015). Specifically, efforts to measure diversity within these domains can be summarized into three categories: N-gram-based method (Zhu et al., 2018; Mishra et al., 2020), Reference-based method (Heusel et al., 2017; C\u00edfka et al., 2018), and Transformation-based method (Du & Black, 2019; Zhang et al., 2024). The n-gram-based method assesses diversity through n-gram statistics, e.g., the distinct n-grams metric (Li et al., 2015) calculates the proportion of unique n-grams out of the total number of n-grams. This approach primarily focuses on the form differences of evaluated texts, often overlooking semantic aspects and offering limited flexibility for evaluators. To align the diversity criteria with human judgment, the reference-based method has emerged as a promising alternative. This approach employs a reference distribution or data as an approximation of human judgment and calculates the similarity between the evaluated data and the reference data (Holtzman et al., 2019). However, collecting reference data can be both time-consuming and may introduce potential biases.\nDrawing inspiration from deep representation learning (Butepage et al., 2017; Zhang et al., 2021), the transformation-based method evaluates diversity by first"}, {"title": "2. Related Work", "content": "We give a brief literature review of diversity evaluation methods. Moreover, limited by the space, further related works on LLM dataset generators can be found in Appendix A."}, {"title": "2.1. Diversity Evaluation Methods", "content": "N-gram-based Methods. With the development of LLMs as dataset generators, the diversity evaluation of synthetic datasets has become a challenging task and remains under-explored in recent evaluation studies (Liang et al., 2022; tatsu lab, 2023). The most comparable diversity evaluation research can be traced back to studies in NLP and ML, which can be summarized into the n-gram-based method (Mishra et al., 2020), reference-based method (Heusel et al., 2017), and transformation-based method (Lai et al., 2020). The n-gram-based method is the most popular lexical diversity evaluation method, leveraging n-grams to capture differences in sentence form (Yu et al., 2024). Commonly used n-gram-based diversity metrics include distinct n-grams (distinct-n) (Song et al., 2024), self-BLEU (Shu et al., 2019), and ROUGE-L (Wang et al., 2022; Padmakumar & He, 2023). However, this method has limitations, as it overlooks differences in other aspects such as semantics and style.\nReference-based Methods. Diversity evaluation is inherently subjective, leading to a reliance on human judgment. Consequently, the reference-based method evaluates diversity by comparing the distribution of the evaluated data to that of a reference dataset (Heusel et al., 2017). MAUVE (Pillutla et al., 2021) exemplifies this idea by employing a divergence-based metric to capture correlations with human judgment. Regarding the natural language inference (NLI) training set as the reference dataset, (Stasaski & Hearst, 2022) first trains an NLI model to infer the relationship between pairs of generated texts and then calculates diversity based on these inference results. Due to the challenges in collecting reference datasets, a recent study (Le Bronnec et al., 2024) proposes evaluating diversity through precision and recall. Despite these advancements, the reference-based method remains significantly constrained because of the need for reference datasets.\nTransformation-based Methods. The transformation-"}, {"title": "3. Prelinimaries", "content": ""}, {"title": "3.1. LLM as a Dataset Generator", "content": "Since the exceptional performance of LLMs, previous works (Dai et al., 2023; Yoo et al., 2021) employ LLMs as a dataset generator or for data augmentation purposes. LLMs significantly reduce the cost of label annotation and data collection (Tan et al., 2024), and in several tasks, even outperform human annotators (Gilardi et al., 2023). While some studies attempt to use LLMs to generate datasets from scratch, it is a challenging task for LLMs. In most cases, a pre-trained LLM, denoted as M, takes the data Dsup to be augmented and the generation task T as input, and outputs the augmented dataset D. Formally, this process can be formulated as follows:\n$D \\leftarrow M(T, D_{sup})$\nwhere T can be texts describing the generation task, such as annotation. Dsup, which comprises a small number of seed samples or unlabeled inputs, serves as supplementary materials to facilitate data augmentation. For example, we want LLMs to perform an annotation task for sentiment labeling, such as determining whether the sentiment is positive or negative. If we assume Dsup to be \u201cIt's a boring movie.", "The sentiment of the movie review is\". $D = \\{T_i\\}_{i=1}^n = \\{(x_i, y_i)\\}_{i=1}^n$ is the generated dataset with n samples, where $T_i = (x_i, y_i)$, $x_i$, and $y_i$ are the input-output sample, the input text, and the output text, respectively. Let TD denote the downstream task of D, when TD is the question-answering task, xi and Yi represent the question and answer, respectively.\"\n    },\n    {\n      \"title\"": "3.2. Problem Formulation"}, {"content": "The diversity evaluation of the dataset is a sample richness evaluation problem. Based on the generation scenarios presented in Table 1, we find that in certain downstream tasks, the diversity of some components in the synthetic dataset does not influence the performance of the trained models. Conversely, the diversity of other components significantly impacts model performance. We refer to these components, whose diversity influences performance, as diversity-sensitive components, denoted as Ti. For example, the input text xi in sentiment classification tasks is the diversity-sensitive component. Conversely, the output text Yi, which represents the sentiment label of the sample and is typically a numerical label (e.g., 0 or 1), does not influence model performance in terms of diversity. Thus, the output text cannot be considered as the diversity-sensitive component. It should be underscored that diversity-sensitive components vary across downstream tasks. The diversity evaluation of synthetic datasets can be transformed into the diversity evaluation of diversity-sensitive components.\nGiven a synthetic dataset $D = \\{T_i\\}_{i=1}^n$, we define ${\\tilde{T}_i\\}_{i=1}^n$ as a collection of diversity-sensitive components. The problem of diversity evaluation of D can be defined as follows:\n$DiversityScore \\leftarrow Eval({\\tilde{T}_i\\}_{i=1}^n)$\nwhere Eval is the diversity evaluation function, which takes ${\\tilde{T}_i\\}_{i=1}^n$ as input and outputs the diversity score DiversityScore of D."}, {"title": "4. Present Work", "content": "In this section, we first introduce our proposed method DCScore from a classification perspective. Then, we present the properties of DCScore followed by theoretical proofs. Finally, we provide a detailed complexity analysis of DCScore and the transformation-based counterpart."}, {"title": "4.1. DCScore: Measuring Diversity from a Classification Perspective", "content": "Due to the intrinsic nature of measuring sample differences in diversity evaluation, it is natural to evaluate diversity as a classification task. Consequently, we propose DCScore, which formulates diversity evaluation as a sample classification task. Specifically, the difference between samples can be measured through a n-classification task, where evaluating n sample datasets involves n n-classification tasks, with each sample corresponding to a distinct category. As shown in Figure 1, DCScore consists of three stages: text representation, pairwise similarity, and diversity summarization. According to the problem formulation in section 3.2, DCScore outputs the diversity of synthetic datasets by evaluating diversity-sensitive components.\nLet $D = \\{T_i\\}_{i=1}^n = \\{(x_i, y_i)\\}_{i=1}^n$ denote a synthetic dataset comprising n input-output samples, and ${\\tilde{T}_i\\}_{i=1}^n$ represents the diversity-sensitive components. DCScore adheres to the paradigm of the transformation-based method to evaluate the diversity of D. Specifically, given $\\tilde{T}_i$, DCScore first applies an embedding function \u03a6 to extract the sample representation $h_i = \\Phi(\\tilde{T}_i)$. For all samples in D, we obtain the sample representation matrix $H \\in R^{n \\times d}$ across all samples, where d denotes the dimension of sample representations. Subsequently, DCScore utilizes a kernel function Kernel to calculate a kernel matrix K, where $K\\in R^{n \\times n}$ and entry K[i, j] represents similarity between $\\tilde{T}_i$ and $\\tilde{T}_j$. From a classification perspective, K[i, j] can be considered as the logit of $\\tilde{T}_i$ being classified into category cj. where cj corresponds to $\\tilde{T}_j$. Formally, the aforementioned process can be formulated as follows:\n$H = \\Phi(\\{\\tilde{T}_i\\}_{i=1}^n), K = Kernel(H)$"}, {"title": "4.2. Properties of DCScore", "content": "We provide theoretical proof that DCScore satisfies several axioms (Leinster & Cobbold, 2012) defined for a principled diversity metric. Specifically, DCScore meets four axioms: effective number, identical samples, symmetry, and monotonicity axioms. These guarantees ensure a reasonable and robust diversity evaluation. The matched axioms of our proposed method are outlined below, while their proofs are detailed in Appendix B due to space constraints."}, {"title": "4.3. Complexity Analysis", "content": "To better understand DCScore, we provide a brief analysis of its time complexity in Table 2. Since VendiScore is quite similar to DCScore, we compare the computational complexities between these two methods. Our analysis reveals that DCScore exhibits lower computation complexity under non-linear kernels compared to VendiScore.\nDenoting Okernel as the complexity associated with general kernels (i.e., kernels other than linear kernels), we analyze"}, {"title": "5. Experiments", "content": "We conduct experiments to verify the effectiveness of DCScore by examining correlation, computational cost, hyperparameter sensitivity, and further probing. Additionally, we provide additional experiments in Appendix E."}, {"title": "5.1. Experimental Settings", "content": "To verify the effectiveness of DCScore, we conduct a series of correlation experiments following the setup in (Tevet & Berant, 2020). As shown in Figure 2, the core idea of our experimental evaluation is to correlate the diversity measurement results of DCScore with diversity pseudo-truths, such as the softmax temperature Tg of dataset generation, human judgment, and LLMs evaluation. Specifically, we evaluate l generated datasets to obtain l diversity scores and then calculate the correlation with diversity pseudo-truths. To calculate the correlation between measured diversity scores and diversity pseudo-truths, we employ Spearman's \u03c1 (Spearman, 1961), a measure of rank correlation ranging from -1 to 1, with higher absolute values indicating stronger correlations. Due to space limitations, we present detailed experimental settings in Appendix D.\nDatasets. We utilize two categories of datasets in our experiments: self-generated datasets and publicly available generated datasets. Self-generated datasets are generated through two data generation strategies (Li et al., 2023): zero-shot and few-shot settings. Additionally, we generate datasets for two natural language generation tasks: text classification and story completion. We utilize three publicly available existing datasets, including SST2 (Socher et al., 2013), Yelp (Zhang et al., 2015), and AG News (Zhang et al., 2015), and their AttrPrompt-augmented version (Yu et al., 2024). Detailed information about these datasets can be found in Appendix D.1.\nGeneration Models. To generate datasets through zero-shot and few-shot settings, we employ two commonly used LLMs as our dataset generators, including Llama2-13B (13B) and Llama2-70B (70B) (Touvron et al., 2023).\nBaseline Methods. We compare DCScore with three baseline methods detailed in Appendix F, i.e., Distinct-n (Li et al., 2015), K-means inertia (Du & Black, 2019), and VendiScore (Dan Friedman & Dieng, 2023)."}, {"title": "5.2. Correlation Evaluation", "content": "We investigate the correlation between the diversity evaluation of DCScore and diversity pseudo-truth, such as Tg, human judgment, and LLMs evaluation. We compare DCScore with all baselines on self-generated datasets."}, {"title": "5.2.1. CORRELATION WITH GENERATION TEMPERATURE Tg", "content": "Evaluation on self-generated datasets. Previous works (Caccia et al., 2018; Tevet & Berant, 2020; Chung et al., 2023) have demonstrated a positive correlation between Tg and the diversity of generated texts, making Tg as a reasonable diversity pseudo-truth. LLMs with lower Tg generate less diverse content, whereas higher T\u2084 values yield more diverse content. Thus, we evaluate the performance of DCScore on self-generated datasets with varying Tg, ranging from 0.2 to 1.2 at 0.05 intervals. We present more information about self-generated datasets in Appendix D.1.1. All methods accurately capture the true diversity of generated datasets, as demonstrated by high Spearman's \u03c1 values. DCScore performs on par with VendiScore while providing better scalability for larger synthetic datasets. K-means Inertia exhibits the weakest correlation on the text classification dataset generated by the 13B model under the zero-shot setting, potentially due to its sensitivity to the number of cluster centroids. Overall, DCScore outperforms all baselines in most cases, and its evaluation results exhibit a strong correlation with the diversity pseudo-truth according to (Akoglu, 2018)."}, {"title": "5.2.2. CORRELATION WITH HUMAN JUDGMENT", "content": "Diversity evaluation is a subjective task, and an ideal method should align well with human judgment. Therefore, we investigate the correlation between DCScore and human judgment. Based on this observation, DCScore performs better in two settings: Story-Few (story completion task generation under the few-shot setting) and Text-Zero (text classification task generation under the zero-shot setting). This is confirmed by higher human-DCScore correlation in these two settings. This is consistent with previous studies (Akoglu, 2018)."}, {"title": "5.2.3. CORRELATION WITH LLM EVALUATOR", "content": "To further verify the effectiveness of DCScore, we investigate the evaluation correlation between DCScore and LLMs. This result can still be considered a strong correlation according to (Akoglu, 2018).\nIn summary, DCScore exhibits a strong correlation (Spearman's p \u2265 0.6), with three diversity pseudo-truths thereby verifying the effectiveness of DCScore."}, {"title": "5.3. Computational Cost", "content": "The computational cost is crucial in diversity evaluation methods, especially with the increasing sample sizes of synthetic datasets. Overall, compared to VendiScore, DCScore maintains lower complexity in most cases. As shown in Chapter 4 of (Seeger, 2004), it is essential to employ different kernel functions to accommodate a wider range of scenarios."}, {"title": "5.4. Hyperparameters Sensitivity", "content": "According to, the temperature (\u03c4) in the Softmax function is a critical hyperparameter that affects classification resolution."}, {"title": "5.5. Further Probe: Downstream Task Training", "content": "To investigate the correlation between DCScore and downstream task training"}, {"title": "6. Conclusion", "content": "In this work, we investigate the diversity evaluation of synthetic datasets a topic. Furthermore"}, {"title": "A. Additional Related Work", "content": "Limited by the space, we provide a literature review of the LLM dataset generator and application of diversity evaluation methods as follows."}, {"title": "A.1. LLM Dataset Generator", "content": "Prompt-guided and Dataset-guided Strategies. Generally, efforts to employ LLMs as dataset generators can be categorized into three strategies . The prompt-guided strategy, a prominent data augmentation approach using LLMs, involves designing task-specific prompts to guide LLMs to augment data in a few-shot manner.\nInstruct-guided Strategy. Generally speaking, the instruct-guided strategy leverages LLMs to generate instructions that guide another LLM in dataset generation.\nIn a nutshell, LLMs are employed to generate or augment datasets through prompt engineering and multi-step strategies, which encompass various application scenarios and downstream tasks."}, {"title": "A.2. Application of Diversity Evaluation Methods", "content": "Quantifying Augmentation Performance. As data augmentation becomes an essential component in the training of deep neural networks, researchers gradually explore a better quantification of the quality of data augmentation. Inspired by this observation, a series of studies have introduced diversity evaluation metrics into the performance assessment of data augmentation strategies.\nEvaluating Mode Collapse. Generative adversarial networks suffer from a well-known phenomenon called mode collapse, which can result in a lack of diversity in the generated samples. Consequently, existing studies assess mode collapse by evaluating the diversity of the generated samples.\nOther Applications. In addition to the aforementioned applications, diversity evaluation metrics have valuable applications in various areas."}, {"title": "B. Proof of Properties of DCScore", "content": "We theoretically confirm that DCScore satisfies several intuitive axioms pointed out by previous studies, thereby demonstrating its superiority.\n\u2022 Effective number (Restated): Diversity should be defined as the effective number of samples in a dataset, ranging from 1 to n. DCScore meets this axiom, as evidenced by its behavior: DCScore equals 1 when all samples in D are identical and equals n when all samples are distinct.\nProof. For DCScore, if all samples in a dataset are the same, the probability of any given sample being classified into all categories is the same, i.e., for all i, j = {1, 2, ..., n}, P[i, i] = P[i, j] = 1. Then, we have DCScore = 1. If all samples in the dataset are distinct, for all i, j = {1, 2, ..., n}, P[i, i] = 1. In other words, the classification function confidently predicts that Ti belongs to the i-th category. Then, we have DCScore tending to n.\n\u2022 Identical samples (Restated): Given two identical datasets D\u2081 and D2, the diversity of the synthetic dataset D' generated by merging these two datasets remains unchanged. The values of DCScore are the same across D1, D2, and D', i.e.,\n$DCScore(D_1) = DCScore(D_2) = DCScore(D')$\nProof. Assuming that D\u2081 and D2 are completely identical, and the samples within each dataset are entirely different, i.e., DCScore(D1) = DCScore(D2) = n. Let P = [P1, ..., Pn, ..., P2n] denote the probability matrix of the merged dataset D' = D\u2081 \u222a D2 = {Ti}21. For 1 \u2264 i \u2264 n, Ti = T2i, where Ti \u2208 D1, T2i \u2208 D2. Consequently, for each diversity-sensitive component \u00ce\u00bf in D', P[i, i] = P[i, 2i] = Finally, DCScore(D') \nHowever, the assumption that all samples in the dataset are completely different may be too stringent. We further provide a proof with a more relaxed assumption.\n\u2022 Symmetry (Restated): Diversity remains constant regardless of the order of the samples, exhibiting permutation invariance. Let \u03c0 denote the permutation function for the sample order, DCScore remains unchanged for any sample permutation of D, i.e.,\n$DCScore(D) = DCScore(\u03c0(D))$\nProof. According to (4), the order of samples does not affect the classification task. Thus, the diagonal elements of P remain unchanged, indicating the symmetry property of DCScore.\n\u2022 Monotonicity (Restated): The diversity of a dataset decreases as the similarity between its samples increases. Given two datasets D\u2081 and D2, and a new sample Tn+1\n$DCScore(D_1) > DCScore(D_2)$.\nProof. If $S(T_i^1, T_{n+1}) < S(T_i^2, T_{n+1})$"}, {"title": "C. Algorithm and Other Implementations for a Classification Perspective", "content": "C.1. Other Implementations. When n > d, the classification probability modeling for any sample within the evaluated dataset is primarily determined by samples that are relatively similar to it, while the"}, {"title": "D. Experimental Settings", "content": "D.1. Datasets Two types of generated datasets, including self-generated datasets and publicly available generated datasets, are employed in our experiments. We provide detailed information on these datasets below.\nD.1.1. SELF-GENERATED DATASETS We utilize two different self-generated datasets in three subsections: Section 5.2.1, Section 5.2.2, and Section 5.4.\nGeneration Settings. We use different generation settings for generated datasets used in Section 5.2.1, Section 5.2.2, and Section 5.4.\n\u2022 Datasets on Section 5.2.1, Section 5.4, We generate 21 sub-datasets\n\u2022 Datasets on Section 5.2.2 We employ the 70B model to generate 6 sub-datasets\n\u2022 Datasets on Appendix 5.5.\nD.1.2. PUBLICLY AVAILABLE GENERATED DATASETS We use three datasets.\n\u2022 Datasets on Section 5.3. We remove samples with text token lengths less than 50."}, {"title": "D.2. Implementation details", "content": "D.2.1. HYPERPARAMETER SETTINGS OF DIVERSITY EVALUATION\nD.2.2. HYPERPARAMETER SETTINGS OF DOWNSTREAM TASK TRAINING\nD.2.3. EVALUATION PROTOCOL\nD.2.4. PROMPT SETTINGS OF LLM EVALUATION"}, {"title": "E. Additional Experiments", "content": "E.1. Computational Cost for larger datasets\nE.2. Correlation with Downstream Task Training"}, {"title": "F. Detailed Modeling of Existing Methods", "content": "We present the detailed modeling of existing methods into DCScore as follows:"}]}