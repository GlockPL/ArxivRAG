{"title": "MARVEL: ACCELERATING SAFE ONLINE REINFORCEMENT LEARNING WITH FINETUNED OFFLINE POLICY", "authors": ["Keru Chen", "Honghao Wei", "Zhigang Deng", "Sen Lin"], "abstract": "The high costs and risks involved in extensive environment interactions hinder the practical application of current online safe reinforcement learning (RL) methods. While offline safe RL addresses this by learning policies from static datasets, the performance therein is usually limited due to reliance on data quality and challenges with out-of-distribution (OOD) actions. Inspired by recent successes in offline-to-online (O2O) RL, it is crucial to explore whether offline safe RL can be leveraged to facilitate faster and safer online policy learning, a direction that has yet to be fully investigated. To fill this gap, we first demonstrate that naively applying existing O2O algorithms from standard RL would not work well in the safe RL setting due to two unique challenges: erroneous Q-estimations, resulted from offline-online objective mismatch and offline cost sparsity, and Lagrangian mismatch, resulted from difficulties in aligning Lagrange multipliers between offline and online policies. To address these challenges, we introduce Marvel, a novel framework for O2O safe RL, comprising two key components that work in concert: Value Pre-Alignment to align the Q-functions with the underlying truth before online learning, and Adaptive PID Control to effectively adjust the Lagrange multipliers during online finetuning. Extensive experiments demonstrate that Marvel significantly outperforms existing baselines in both reward maximization and safety constraint satisfaction. By introducing the first policy-finetuning based framework for O2O safe RL, which is compatible with many offline and online safe RL methods, our work has the great potential to advance the field towards more efficient and practical safe RL solutions.", "sections": [{"title": "1 Introduction", "content": "Safe reinforcement learning (safe RL) [1, 2], prioritizes not only the maximization of rewards but also the adherence to specific safety constraints, enhancing its applicability in real-world scenarios. For example, an autonomous vehicle must reach its destination without exceeding a preset fuel limit. However, solving safe online RL from scratch in fields such as robotics [3, 4], and healthcare [5, 6] is often prohibitive, due to substantial risks and costs caused by the need for extensive interactions with the environment. To address this, offline safe RL [7, 8, 9] has been introduced, enabling the derivation of safe policies from a static dataset [10] without the need for real-time environmental interaction. Nonetheless, offline safe RL faces its own set of limitations: it typically shows limited performance [11], heavily relies on the quality of the offline dataset, and suffers from the impact of out-of-distribution (OOD) actions, restricting its effectiveness across varying scenarios.\nThe pretraining-and-finetuning paradigm is a well-established strategy in the fields of computer vision and natural language processing, for enabling fast and sample-efficient online learning based on offline pretrained models, par-"}, {"title": "2 Preliminaries", "content": "We consider a standard constrained Markov Decision Process (CMDP) [22, 23], defined by a tuple (S, A, T, R, C, \u03b3, \u03b7, Cth). Here S \u2286 Rn represents the state space, A\u2286 Rm denotes the action space, T : S \u00d7 A \u00d7 S \u2192 [0,1] is the transition probability function, R : S \u00d7 A \u2192 [0, Rmax] is the reward function, and C:S\u00d7A \u2192 [0, Cmax] is the cost function. \u03b3\u2208 [0,1] is the discount factor, \u03b7 represents the initial state distribution, and Cth is the cost threshold that sets the limit on cumulative costs for the policy. A policy \u03c0 : S \u2192 P(A) is a mapping from states to a probability distribution over actions, where \u03c0(a|s) denotes the probability of selecting action a in state s. In this work, we consider parameterized policies \u03c0\u03b8, where \u03b8 denotes the parameters of the policy, typically represented by neural networks in deep RL. Given a policy \u03c0, its cumulative reward under policy \u03c0 is defined as R(\u03c0) = E\u03c4\u223c\u03c0 [\u2211t=0 \u03b3tr(st, at)], where \u03c4 = (s0,a0,s1,a1,...) is a trajectory induced by policy \u03c0, and the expectation is taken over the distribution of trajectories. Similarly, its cumulative cost is defined as C'(\u03c0) = E\u03c4\u223c\u03c0[\u2211t=0 \u03b3tc(st, at)]. The Q-function, for a given policy \u03c0, is defined as the expected cumulative reward starting from a state-action pair (s, a) and thereafter following policy \u03c0: Q\u03c0(s,a) = E\u03c4\u223c\u03c0[\u2211t=0 \u03b3tr(st, at) | s0 = s, a0 = a]. Similarly, the cost Q-function Q\u03c0c(s, a) is defined as the expected cumulative cost starting from the same state-action pair (s, a) and thereafter following policy \u03c0:\nQ\u03c0c(s,a) = E\u03c4\u223c\u03c0 [\u2211t=0 \u03b3tc(st, at) | s0 = s, a0 = a]. In the context of CMDP, the goal is to find an optimal policy \u03c0* that maximizes the cumulative reward R(\u03c0), subject to the constraint that the cumulative cost C'(\u03c0) does not exceed a predefined threshold Cth. This can be formulated as the following constrained optimization problem:\nmax R(\u03c0), s.t. C(\u03c0) \u2264 Cth.        (1)\nTo solve this, a common approach is to apply the Lagrangian relaxation method [9], where a Lagrange multiplier \u03bb is introduced to enforce the cost constraint. This leads to the following primal-dual optimization formulation:\nmin\u03bb>0 max\u03c0 [R(\u03c0) \u2013 \u03bb(C(\u03c0) \u2013 Cth)]       (2)\nwhich can be solved by iteratively updating the policy \u03c0 and the Lagrange multiplier \u03bb. Specifically, \u03bb is updated by:\n\u03bbt+1 = \u03bbt + \u03b1\u03bb(C(\u03c0t) \u2013 Cth)            (3)\nwhere \u03b1\u03bb is the learning rate.\nPrimal-dual based algorithms have shown great effectiveness and superior performance in the literature for online safe RL, which can combine a wide range of online unconstrained RL algorithms with the Lagrange multiplier method to create online safe RL algorithms. Without loss of generality, we consider SAC-lag [9] as the online algorithm, a primal-dual based algorithm that integrates the widely used SAC algorithm [24] with the Lagrange multiplier method. More specifically, SAC minimizes the following objectives for the actor (policy) and the critic (Q-function), respectively:\nLSAC(\u03b8) = Es\u223cdEa\u223c\u03c0\u03b8(\u00b7|s) [\u2212log \u03c0\u03b8(a|s) \u2013 Q(s, a; \u03bc)]  (4)\nLSAC(\u03bc) = E(s,a,s')\u223cd[(Q(s, a; \u03bc) \u2013 y(r, s'))2]   (5)\nwhere y(r, s') = r + \u03b3Ea'\u223c\u03c0\u03b8(\u00b7|s') [Q(s, a'; \u03bc') \u2013 \u03b1 log \u03c0(a'|s')], Q(s, a; \u03bc) is parameterized by \u03bc, Q(s, a; \u03bc') is the target reward Q-function parameterized by \u03bc', d represents the data distribution in the replay buffer, and \u03b1 > 0 is"}, {"title": "3 Warm-Start Safe RL with Value Pre-Alignment", "content": "As demonstrated in Fig. 1, naively finetuning the offline policy for safe RL would not work well and the finetuned policy shows clear \u201cinertia\" in improving its performance: within a long period after online finetuning starts, its cost stays far below the limit, but its reward is quite low and not improving at all. This implies that such a strategy automatically \"inherits\" the conservatism from offline safe RL and is reluctant to actively explore in order to fully utilize the safe gap below the cost limit. In this section, we delve into the failure of naive finetuning, which points to two unique challenges for policy finetuning in O2O safe RL, i.e., erroneous offline Q-estimations and Lagrange multiplier mismatch. To address these problems, we propose a framework for O2O safe RL, namely warM-stArt safe Reinforcement learning with Value prE-aLignment (Marvel)."}, {"title": "3.1 Pre-Finetune Phase", "content": "By learning from a fixed dataset without online environment interactions, offline safe RL typically suffers from large extrapolation errors for OOD actions beyond the support of the dataset. A general principle to handle this is to penalize the reward/cost estimations for the OOD actions in such a way that risky explorations outside the dataset are discouraged. Particularly, the optimization of Q-functions in offline safe RL can be captured as follows:\nOffline (Q): min\u03bc Es,a,r,s'\u223cD [(Q(s,a) \u2013 (r + \u03b3 maxa'E [Q(s', a')]))2] + \u03c8 \u00b7 P(s,aOOD),  \nOffline (Qc): min\u03bcc Es,a,c,s'\u223cD [(Qc(s,a) \u2013 (c + \u03b3 maxa'E [Qc(s', a')]))2] \u2013 \u03c8c \u00b7 Pc(s, aOOD).\nSolution: Value Pre-Alignment. To address the first challenge, a naive approach is to reevaluate the offline policy in online environments using Monte Carlo simulations, which however introduces additional interaction costs. Motivated by the recent advances in Off-Policy Evaluation (OPE) [28], we borrow the idea from Fitted Q Evaluation [29] to align the offline Q-functions with the online learning objectives for the offline policy, by using the offline dataset before online policy finetuning. In particular, we seek to minimize the following objectives for the reward and cost Q-functions by starting from the pretrained Q-functions from offline learning, respectively:\nLVPA(\u03bc) = ED [L2 (Q(s, a; \u03bc) \u2013 (r + \u03b3 Ea\u2032\u223c\u03c0\u03b8(\u00b7|s\u2032) [Q(s\u2032, a\u2032; \u03bc\u2032) \u2013 \u03b1PA log \u03c0\u03b8(a'|s')]))]  (8)\nLVPA(\u03bcc) = ED [L2 (Qc(s, a; \u03bcc) \u2013 (c + \u03b3 Ea\u2032\u223c\u03c0\u03b8(\u00b7|s\u2032) [Qc(s\u2032, a\u2032; \u03bc\u2032c) \u2013 \u03b1PA log \u03c0\u03b8(a'|s')]))], (9)"}, {"title": "3.2 Finetune Phase", "content": "However, achieving this through experimental tuning is challenging, and currently, there is no theory to accurately predict these values. To address this problem, instead of directly finding the best initial value, we take an alternative path by quickly adapting the Lagrange multipliers in an effective manner. Motivated by the recent success of leveraging PID control for updating the multiplier in online safe RL [20, 30, 31], we introduce an adaptive PID control approach specifically tailored for O2O safe RL.\n\u03bbt+1 = \u03bbt + Kpe(t) + Ki\u222b0te(\u03c4)d\u03c4 + Kdde(t)dt (10)"}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to verify the effectiveness of our approach, aiming to answer the following questions: 1) RQ1: How does our method compare with naive finetuning and other SOTA baselines in both reward and cost? 2) RQ2: How do different components in Marvel affect the performance? Due to the space limit, we delegate the experimental details and some additional results to Appendix D and Appendix F."}, {"title": "4.1 Evaluation Setup", "content": "We consider the DSRL benchmark [10] and select ten environments from the Bullet Safety Gym [17] and Safety Gymnasium [33]: BallRun, BallCircle, CarRun, CarCircle, HalfCheetah, AntCircle, AntRun, DroneCircle, Hopper, and Swimmer. The cost threshold is set to be 20 in these environments. As mentioned earlier in Section 2, we choose CPQ and SAC-lag as base algorithms in our proposed framework Marvel for offline training and online finetuning, respectively, due to the effectiveness and representativeness of them. Each experiment was conducted using five random seeds, and the results were averaged to generate the final learning curves. We use a dataset that includes data provided by DSRL [34] and random data generated by a random policy to control the quality of the offline dataset."}, {"title": "4.2 Main Results", "content": "As shown in Fig. 3, Marvel demonstrates better or comparable performance compared to all baselines consistently across all environments, i.e., achieving the higher return while keeping the cost below the threshold. In stark contrast, the naive warm start method proves largely ineffective, often causing performance drop or stagnation during training. Without aligning the Q-estimations, both JSRL and PEX struggles a lot to improve during online learning and fails to control the cost. Besides, PEX also suffers from poor training stability and high variance across different settings."}, {"title": "4.3 Ablation Studies", "content": "To answer RQ2, we conduct experiments in various setups. As shown in Fig. 5, the performance is best when both VPA and aPID are used. In contrast, if only VPA is used with traditional dual ascent during online finetuning, it significantly slows down safe online learning and takes a much longer time to reduce the cost. If only aPID is applied without VPA, the learning performance is very similar to naive policy finetuning, which struggles to improve due to the erroneous Q-estimations. We also evaluate the effectiveness of adaptive control in aPID, by comparing the performance between Marvel (VPA+aPID) and Marvel with aPID replaced by PID (VPA+PID). It can be seen from Fig. 5 that the training curve for cost exhibits significant fluctuations without using aPID. More critically, when the cost is close to the limit, PID cannot reduce its control strength. As a result, even if on average the cost of VPA+PID is close to the threshold, it is very frequent that the real-time cost exceeds the limit substantially, which is in fact not safe. Moreover, the inability to adjust the Lagrange multiplier promptly and appropriately affects the weight of reward and cost in policy updates, thereby influencing reward performance, as shown in the plots for CarCircle and HalfCheetah."}, {"title": "5 Conclusion", "content": "O2O safe RL has great potentials to put safe RL on the ground in real-world applications, by leveraging offline learning to facilitate fast online safe learning. In this paper, we proposed the first policy-finetuning based framework, namely Marvel, for O2O safe RL. In particular, by showing that naive finetuning would not work well, we identified two unique challenges in O2O safe RL, i.e., the erroneous Q-estimations and Lagrangian mismatch. To address these challenges, Marvel consisted of two key designs: 1) value pre-alignment to correct the Q-estimations before online finetuning, and 2) adaptive PID control to dynamically change the control parameters so as to rapidly and appropriately control the cost. Extensive experiments demonstrate the superiority of Marvel over multiple baselines. More importantly, Marvel is compatible to a variety of offline and online safe RL approaches, making it very practically appealing. For future work, it is interesting to take a closer look at the offline dataset, to identify states that are more worth exploring during online finetuning given the environmental information and cost threshold. Ultimately, we hope our work will bridge the gap between offline and online algorithms in safe RL, distinct from unconstrained RL, and enhance the efficiency of online safe RL, laying the foundation for the usage of safe RL in practical applications."}, {"title": "A Overview of Algorithm", "content": "Offline (Q): min\u03bc Es,a,r,s'\u223cD [(Q(s,a) \u2013 (r + \u03b3 maxa'E [Q(s', a')]))2] + \u03c8 \u00b7 P(s,aOOD),  \nOffline (Qc): min\u03bcc Es,a,c,s'\u223cD [(Qc(s,a) \u2013 (c + \u03b3 maxa'E [Qc(s', a')]))2] \u2013 \u03c8c \u00b7 Pc(s, aOOD)."}, {"title": "B Related Work", "content": "Online Safe RL. Online safe RL approaches can be generally divided into several categories. The first category includes primal-dual based methods, such as PDO [21], which combines PPO [38] with the Lagrange multiplier method to obtain a policy that satisfies safety constraints. CPPO-PID [20] combines PID control with Lagrangian methods to dampen cost oscillations. Similar Lagrangian-based methods are applied in conjunction with other unconstrained safe RL algorithms, such as TRPO-lag, PPO-lag, and SAC-lag. CPO [7] inherits from TRPO [39], optimizing with the Lagrange multiplier method within the trust region. CUP [40] extends CPO by incorporating the generalized advantage estimator. In comparison, RCPO [41] uses different update rates for the primal and dual variables. Two-stage iterative methods have also been developed for online safe RL, e.g., PCPO [42] and FOCOPS [43]. Besides the primal-dual based methods, primal methods, which are also known as Lyapunov methods, have been leveraged in some studies for online safe RL. For instance, IPO [44] uses logarithmic barrier functions. P3O [45] employs an exact penalty function to derive an equivalent unconstrained objective and restrict policy updates within the trust region. [46] leverages Lyapunov functions to handle constraints, which contains two parts, safe policy iteration and safe value iteration. Additionally, some studies [47, 48] borrow techniques from the control theory, such as HJ reachability [49, 50] and control barrier functions [51], to ensure state-wise zero costs.\nOffline Safe RL. Offline safe RL seeks to learn a safe policy from static datasets without online environmental interactions. Similar to online safe RL, Lagrangian methods can still be applied here, by adapting offline unconstrained RL algorithms like BCQ [52] and BEAR [37] to the safe RL setting. CPQ [19] uses a VAE to detect OOD [53] actions and penalizes them in terms of cost. COptiDICE [54] extends OptiDICE [55] by adding safety constraints and derives a safe policy through the stationary distribution of the optimal policy. FISOR [8] decouples the process of satisfying safety constraints from maximizing rewards and employs a diffusion model as the policy. VOCE [18] estimates Q-"}, {"title": "C Details on Baselines", "content": "Considering the characteristics of safe RL, which requires keeping the cost below a certain threshold, not all O2O unconstrained RL algorithms are suitable for O2O safe RL. For instance, AWAC [12], which maximizes the advantage function, has not yet been applied in the safe RL context. We compare Marvel with the following baselines:\nBy analyzing Q-value estimation in offline to online transitions, the SO2 algorithm achieves more accurate Q-value estimation through Perturbed Value Update and by increasing the frequency of Q-value updates.\nJSRL employs an offline pretrained policy as the exploration policy and a policy under training during the online phase as the target policy. Initially, the exploration policy is used, followed by the target policy during online interaction to facilitate curriculum learning. To adapt to the safe RL setting, we update the Lagrange multipliers using the aPID method when updating the target policy.\nSimilarly to JSRL, PEX uses an offline pretrained policy and a policy under training during the online phase for online interaction. However, PEX selects one of the actions based on the Q-networks's value estimation of actions chosen by the two policies. To meet the safe RL requirements concerning cost, like the modifications to JSRL, we use the aPID method to update the Lagrange multipliers.\nWe directly utilize the policy, Q-network, and Qc-network networks obtained from offline safe RL without any modifications (no VPA and aPID), and apply online safe RL algorithms for finetuning.\nWe selected SO2, JSRL, and PEX as baselines because they represent prominent methods in O2O RL, and adapting them to the safe RL context provides a meaningful comparison. Including these baselines allows us to demonstrate the effectiveness of Marvel in a fair and relevant context."}, {"title": "D More Experimental Results", "content": "In Fig. 6, we additionally provide experimental results in more environments, including DroneCircle, AntRun, Hopper, and Swimmer. The results indicate that our proposed Marvel algorithm achieves competitive performance across these settings.\nAdditionally, we present the performance of the baseline algorithms and Marvel, along with the offline pretrained policy as the starting point, as shown in Fig. 3 and Fig. 6, and summarized in Table 2."}, {"title": "E More Analysis of Marvel", "content": "Similar to the analysis presented in [65], this section introduces an alternative way to evaluate safe RL performance beyond training curves, as shown in Fig. 16. The cumulative cost represents the total cost accumulated from all"}, {"title": "F Experimental Details", "content": "We aim to explore the effect of VPA on the distribution of the Q and Qc networks. Specifically, for different stateaction pairs, we need to analyze the true Q and Qc values versus the predicted values from the Q and Qc networks. To achieve this, we choose to use Spearman's rank correlation coefficient, which allows us to quantify the ranking accuracy of the Q and Qc values over a sequence of state-action pairs.\nSpearman's correlation coefficient ranges from -1 to 1, where \u03c1=1 indicates a perfect positive rank correlation (i.e., the predicted Q and Qc values perfectly match the rank of the true values) \u03c1=\u22121 indicates a perfect negative rank correlation, \u03c1=0 indicates no correlation between the ranks of the predicted and true values."}, {"title": "G Limitation", "content": "While Marvel performs well in most environments, it does not exhibit the same effectiveness in certain scenarios, such as in the AntRun environment. As depicted in Fig. 6, during finetuning, Marvel does not significantly improve cost and reward metrics. Consequently, aPID evidently does not function optimally in these settings. This suggests that further enhancements are needed for VPA to increase the agent's exploratory behavior during online finetuning. Combined with aPID's efficient control over costs, this approach could achieve optimal performance with minimal interaction with the environment, thus minimizing the time required."}]}