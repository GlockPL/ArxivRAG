{"title": "TOWARDS NEXT-GENERATION MEDICAL AGENT: HOW 01 IS RESHAPING DECISION-MAKING IN MEDICAL SCENARIOS", "authors": ["Shaochen Xu", "Yifan Zhou", "Zhengliang Liu", "Zihao Wu", "Tianyang Zhong", "Huaqin Zhao", "Yiwei Li", "Hanqi Jiang", "Yi Pan", "Junhao Chen", "Jin Lu", "Wei Zhang", "Tuo Zhang", "Lu Zhang", "Dajiang Zhu", "Xiang Li", "Wei Liu", "Quanzheng Li", "Andrea Sikora", "Xiaoming Zhai", "Zhen Xiang", "Tianming Liu"], "abstract": "Artificial Intelligence (AI) has become essential in modern healthcare, with large language mod-\nels (LLMs) offering promising advances in clinical decision-making. Traditional model-based\napproaches, including those leveraging in-context demonstrations and those with specialized medical\nfine-tuning, have demonstrated strong performance in medical language processing but struggle with\nreal-time adaptability, multi-step reasoning, and handling complex medical tasks. Agent-based AI\nsystems address these limitations by incorporating reasoning traces, tool selection based on context,\nknowledge retrieval, and both short- and long-term memory. These additional features enable the\nmedical AI agent to handle complex medical scenarios where decision-making should be built on\nreal-time interaction with the environment. Therefore, unlike conventional model-based approaches\nthat treat medical queries as isolated questions, medical AI agents approach them as complex tasks\nand behave more like human doctors. In this paper, we study the choice of the backbone LLM for\nmedical AI agents, which is the foundation for the agent's overall reasoning and action generation.\nIn particular, we consider the emergent ol model and examine its impact on agents' reasoning,\ntool-use adaptability, and real-time information retrieval across diverse clinical scenarios, including\nhigh-stakes settings such as intensive care units (ICUs). Our findings demonstrate ol's ability to\nenhance diagnostic accuracy and consistency, paving the way for smarter, more responsive AI tools\nthat support better patient outcomes and decision-making efficacy in clinical practice.", "sections": [{"title": "1 Introduction", "content": "The medical environment requires dynamic artificial intelligence (AI) tools. Foundational large language models (LLMs)\nlike GPT-3.5 and GPT-4 have transformed how the medical field interacts with information technology (IT). Beyond\nserving as powerful tools for knowledge synthesis [1, 2], general-purpose LLMs like GPT-4 can achieve excellent\naccuracy handling domain-specific queries when provided with in-context demonstrations of medical reasoning \u2013 they\nserve as reasoning engines to support complex decision-making. [3, 4, 5]. Moreover, specialized medical models like"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLMs in Medical Domains", "content": "In recent years, significant progress has been made in the application of LLMs within the medical field [8, 32, 2].\nThese advancements have seen LLMs being applied in various medical domains such as computer-aided diagnosis,\ngenotyping and phenotyping, drug discovery, medical evidence summarization, and treatment planning in radiation\noncology [33, 34, 16, 35, 36, 37, 38]. However, despite their potential, LLMs still face challenges when confronted\nwith clinical tasks that require complex reasoning and specialized medical domain knowledge [1, 39], both of which are\nessential for enhancing LLMs' effectiveness across clinical scenarios.\nCurrently, two major research directions exist in this area: tool-augmented methods [40] and instruction-tuning methods\n[41]. In the tool-augmented approach, LLMs are equipped with external tools to gather additional medical information\nfor better reasoning. For example, GeneGPT [42] integrates Web APIs from the National Center for Biotechnology\nInformation (NCBI) to meet biomedical information needs. Similarly, Almanac [43] integrates a RAG framework with\nGPT-4 designed for accessing medical guidelines and treatment recommendations. Another notable method, KARD\n[44], enhances domain-specific knowledge by fine-tuning smaller language models with rationales from LLMs and\naugmenting them with external knowledge from non-parametric memory sources.\nOn the other hand, instruction-tuning research leverages external clinical knowledge bases and synthetic datasets to\ncreate specialized instruction datasets. These datasets are used to fine-tune LLMs for specific medical applications.\nSome models are trained on a vast array of medical and biomedical literature to gain broad medical insights [8, 45],\nwhile others focus on specific fields such as radiology report generation for task-oriented design [34, 38]. In contrast to\nthese methods, our work aims to leverage the latent medical knowledge within LLMs and improve clinical reasoning in\na training-free setting.\nIn addition to the existing literature, recent studies like the one by Xie et al. [28] have provided a broad evaluation of\nadvanced LLMs, such as o1, specifically examining their diagnostic and reasoning capabilities across diverse medical\nscenarios. While these studies highlight the potential of LLMs in medicine by focusing on understanding, reasoning, and\nhandling clinical data, our study emphasizes the performance of LLMs within the context of agent systems. Here, each\nagent must not only apply medical knowledge but also engage dynamically in knowledge retrieval and reasoning-based\ntool usage. Thus, while standalone LLMs demonstrate impressive capabilities, integrating them into agent-based\nframeworks requires models adept at collaborative decision-making."}, {"title": "2.2 LLM Agents and Their Applications in the Medical Domain", "content": "Recent advancements in LLM agents have enabled these systems to act autonomously in complex environments by\nleveraging advanced reasoning capabilities and memory-based reflection. These agents can dynamically respond to new\ninformation, process complex inputs, and make decisions based on their understanding of context [46, 47]. Moreover,\nby utilizing memory, they can maintain continuity across multi-turn interactions, adapting their responses in line with\nprevious exchanges\u2014much like a human professional would in an ongoing consultation [48].\nA critical feature of effective LLM agents is their ability to reason through complex tasks using multi-step processes.\nApproaches like Chain-of-Thought (CoT) prompting have been developed to enhance the reasoning depth of LLMs,\nallowing them to break down problems into sequential steps that mirror human logical reasoning [49]. This method has\nproven especially valuable in the medical field, where diagnostic reasoning requires careful consideration of patient\nsymptoms, history, and test results. By generating intermediate steps, CoT prompting enables LLM agents to better\nnavigate the complexities inherent in medical decision-making."}, {"title": "3 Agent", "content": ""}, {"title": "3.1 CoD Agent", "content": "Overview: The Chain of Diagnosis (CoD) [29] framework is a medical AI agent designed for automated diagnosis. It\nis characterized by a pipeline that simulates the cognitive workflows of healthcare professionals. When a patient is\nseen in a healthcare setting, the healthcare professional goes through a systematic process summarized by the SOAP\nformat (subjective, objective, assessment, plan) [57], which includes collecting subjective symptoms (e.g., sore throat)\nwith objective symptoms (e.g., temperature, congestion) and compares these DiagnosisGPT applies CoD to standardize\nthis assessment process and compares these implicit and explicit symptoms against a disease database to develop a\ndiagnostic reasoning process, a differential diagnosis with confidence distributions, and a final diagnosis. Similarly,\nCoD enables a stepwise diagnostic approach with knowledge retrieval and inference based on confidence distribution\nover the candidate diseases. The framework thus ensures that the diagnostic inference of CoD is grounded in a provided\nknowledge base, highlighting its potential flexibility in handling emerging medical scenarios.\nAgent Design: From the high level, CoD takes a structured Symptom Abstraction synthesized from the patient's explicit\nsymptoms as its inputs, and outputs a diagnostic inference with an optional confidence score. The workflow of CoD\ncontains three stages:\n1) Target Disease Retrieval: In this stage, a list of candidate diseases that are most relevant to the input Symptom\nAbstraction is retrieved. These candidate diseases are ranked based on their similarity with the Symptom Abstraction\nin the embedding space. This step significantly narrows down the diagnostic scope, focusing computational resources\non the most likely conditions.\n2) Diagnostic Reasoning: Once identified, the candidate diseases are assessed against the input symptoms. Specifi-\ncally, a prompt including the input symptom, the candidate diseases, and the description of each candidate disease\nis used to query the backbone LLM for a confidence score estimated for each candidate disease. The descriptions\nfor the candidate diseases are stored in an internal knowledge base and can be easily retrieved based on the disease\nname. In the original CoD paper, the backbone LLM in this stage is fine-tuned on a specially created dataset to\nlearn the mapping from the input symptoms and the candidate diseases to the confidence scores. In our experiments,\nwhen evaluating CoD with foundation LLMs (without the special fine-tuning of CoD), we skip this step by directly\ninferring the most likely disease without generating confidence scores.\n3) Confidence Assessment: In this final stage, CoD establishes a disease confidence distribution that reflects the\nmodel's diagnostic confidence across candidates. This distribution not only guides decision-making but also provides\nan interpretation of the inference results. Again, we simplify the pipeline of CoD in our experiments by directly\nproducing the inference results on the candidate diseases and the input symptoms."}, {"title": "3.1.1 Experiment Design", "content": "Agent Settings In this experiment, we adjust the diagnostic framework of CoD to make it more suitable for foundation\nLLMs. In Step 2, we deviate from the original method of CoD that utilizes embedding similarity to select the top-k\nrelevant diseases. Instead, we retrieve all diseases in the candidate pool provided in the dataset and the relevant\ndescriptions and clinical summaries for each disease. These descriptions are then inputted into the LLM, which\nperforms a ranking operation across all candidate diseases. By evaluating each disease's relevance to the extracted\nsymptoms, the model ranks the candidates and identifies the top-ranked disease. This ranked selection substitutes\nfor the confidence level estimation traditionally used, providing a more interpretable and model-driven method for\ncandidate prioritization.\nDatasets and Metrics The datasets in use here are Dxbench, Dxy, and Muzhi. Muzhi and Dxy are built based on\nreal doctor-patient consultation. DxBench is built based on the MedDialog dataset, which contains real doctor-patient\ndialogues.\n1) Dxy Dataset evaluated diagnostic models in medical AI, using real doctor-patient dialogue data. It focuses on\nassessing models' ability to predict diseases based on symptoms, allowing for both multiple-choice answers and\ntargeted symptom inquiries. For our evaluation of the Dxy dataset, we conduct tests on a subset of 120 cases.\nThe results are divided into three folds. For each group, we calculate the mean diagnostic accuracy. The overall\nperformance metric is derived by averaging these three group means and calculating the standard deviation across\nthem, providing a robust measure of model performance variability across different subsets.\n2) DxBench Dataset was designed to rigorously test diagnostic models in medical AI, using real doctor-patient\ndialogues covering a wide range of 461 diseases across 15 medical specialties. It challenges models to accurately"}, {"title": "3.2 MedAgents", "content": "Overview: MedAgents is a medical collaborative agent based on role-play. It simulates the real clinical environment\nwhere a group of experts with diverse expertise discuss a patient's description of symptoms and reach a consensus on\ndiagnosis and treatments. The pipeline of MedAgents is composed of five major stages: gathering a group of experts,\nsymptom analysis by each expert, summarization of individual expert's analysis, consultation for mutual agreements,\nand final decision-making [30]. The simulation of expert discussions with zero-shot reasoning of MedAgents enables it\nto outperform simple single-agent design on medical datasets and effectively harness latent medical knowledge.\nAgent Design: From a high level, MedAgents takes clinical questions and patient data as structured inputs, synthesizing\ninsights from various domain experts. The output is a consensus diagnostic inference after possibly multiple rounds of\n\"discussion\u201d between the experts. The MedAgents workflow consists of five stages:\n1) Expert Gathering: MedAgents initiates by selecting virtual experts from various medical domains relevant to\nthe clinical question. This is achieved by querying the backbone LLM with the input description of the patient's"}, {"title": "3.2.1 Experiment Design", "content": "Agent Setting In each step, we replace the MedAgents backbone model to study the influence of the backbone model\non task performance. By flexibly utilizing various backbones during expert gathering, analysis, report summarization,"}, {"title": "3.3 AgentClinic", "content": "Overview: AgentClinic [31] is a multi-agent system simulating real clinical environments. Different from MEDAgents\nwhich uses a role-play-based approach, AgentClinic involves multiple individual agents interacting directly with\neach other. The AgentClinic system includes four agents: a patient agent, a doctor agent, a measurement agent,\nand a moderator agent. The patient agent provides symptoms like a real human patient, while the doctor agent asks\nquestions and consults the measurement agent for test results to refine its diagnosis. The moderator agent then validates\nthe doctor's conclusion, simulating the collaborative, multi-step process typical of real healthcare scenarios. Such\na simulation of the real-world clinical environments aims to evaluate how well AI-driven agents manage patient\ninteractions, respond to evolving cases, and make decisions that mirror the flow of actual clinical practice. In a simulated\nscenario conducted purely through text-based interactions, the patient agent presents a set of symptoms without access"}, {"title": "3.3.1 Experiment Design", "content": "Agent Settings: In our study, we evaluate and compare the performance of two different LLM backbones, GPT-4 and\n01-preview, as the core models for language agents within the AgentClinic framework. This setup involves four key\nagents-doctor, patient, measurement, and moderator each responsible for specific tasks in the diagnostic process.\nWe perform two sets of experiments: one where all agents are powered by GPT-4 and another with o1-preview as the\nbackbone. By consistently replacing the backbone model across all agents, we aim to observe how each model impacts\noverall diagnostic accuracy, reasoning consistency, and adaptability within simulated medical decision-making tasks.\nDatasets and Metrics: We utilize two primary datasets to benchmark the diagnostic reasoning and specialized\nknowledge capabilities of each model in clinical contexts: MedQA and the NEJM case challenge dataset.\n\u2022 1) MedQA: Derived from the U.S. Medical Licensing Examination (USMLE), the MedQA dataset contains 215\nmultiple-choice questions (MCQs) with four answer options. For efficiency and computational feasibility, we\nrandomly sampled 120 entries from this dataset, ensuring a broad representation of medical topics while minimizing\nprocessing requirements.\n\u2022 2) NEJM: This dataset consists of 120 clinical cases that require complex diagnostic reasoning, often using\nmultimodal data (text and images). Given that o1-preview does not support image processing, we conducted all"}, {"title": "4 Conclusion", "content": "This study investigated the integration of the o1 model into various LLM-based agents within the medical domain,\nreplacing traditional backbone models like GPT-4. Our experiments demonstrated that the ol model significantly\nenhances performance across multiple medical reasoning tasks. In the CoD Agent framework, o1-preview not only\nmatched but often exceeded GPT-4 in accuracy, showing more consistent diagnostic results with lower standard\ndeviations. In the MedAgent framework, 01-preview performs better on the MedQA and MedMCQA datasets than\nGPT-4 and demonstrates greater stability. Similarly, in the AgentClinic simulations, o1-preview achieved higher mean\naccuracies on both the MedQA and NEJM datasets compared to GPT-4, indicating superior diagnostic capabilities even\nin complex, multimodal scenarios.\nThe future of healthcare envisions a sophisticated, multi-agent LLM framework that enables seamless collabora-\ntion across departments and specialties. Leveraging the o1 model's advanced Chain-of-Thought (CoT) reasoning\nand Retrieval-Augmented Generation (RAG) techniques, LLM agents can enhance diagnostic workflows and cross-\ndisciplinary consultations. By embedding CoT reasoning directly into its training process and incorporating reinforce-\nment learning from human feedback, o1-preview demonstrates enhanced multi-step reasoning abilities, effectively\nmanaging complex diagnostic queries, simulating clinical workflows, and adapting to dynamic environments like\nICUs. Through specialized agents for diagnosis, monitoring, and treatment coordination, future hospital systems can\ndynamically adapt to real-time patient data, enhance diagnostic precision, and optimize treatment pathways. This\ncollaborative, multi-agent approach promises to transform medical decision-making into a comprehensive, efficient\nprocess, aligning interdisciplinary insights to ultimately improve patient outcomes.\nHowever, the experiments also highlighted some limitations. The current version of ol-preview lacks support for\nmultimodal data processing, affecting its performance on datasets that include imaging data, such as the NEJM dataset.\nIn the medical field, the ability to process multimodal inputs is essential, as complex clinical tasks often require the\nintegration of various data types, such as radiology images/videos, clinical notes, and speech data, to provide accurate\nand comprehensive insights. Leveraging the advanced reasoning capabilities of the o1-preview model as the central\ncomponent of a multimodal agent framework could effectively address the complexities involved in interpreting diverse\nclinical factors across various scenarios. Additionally, the increased computational runtime with o1-preview indicates a\ntrade-off between accuracy and efficiency, which could be a concern in time-sensitive clinical settings.\nFuture research should focus on addressing these limitations by integrating the o1 model into a multimodal multi-agent\nframework. This integration would enhance diagnostic capabilities by positioning the o1 model as a task planner that\nutilizes advanced reasoning to coordinate and assign subtasks across different modalities for various expert models.\nFurthermore, optimizing computational efficiency without compromising performance is also crucial for practical\napplications in real-world clinical environments.\nIn conclusion, replacing traditional backbone models with the o1 model in medical AI agents offers significant practical\nbenefits. The ol model's superior reasoning, adaptability, and consistent performance across diverse clinical tasks\nposition it as a promising tool for advancing healthcare delivery. By continuing to refine models like 01 and integrating\nthem into clinical practice, we move closer to developing AI agents that can meet the complex demands of modern\nmedicine, ultimately enhancing patient care and outcomes."}]}