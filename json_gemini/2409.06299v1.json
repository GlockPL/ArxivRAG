{"title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory", "authors": ["Dingxin Cheng", "Mingda Li", "Jingyu Liu", "Yongxin Guo", "Bin Jiang", "Qingbin Liu", "Xi Chen", "Bo Zhao"], "abstract": "Recently, integrating visual foundation models into large language models (LLMs) to form video understanding systems has attracted widespread attention. Most of the existing models compress diverse semantic information within the whole video and feed it into LLMs for content comprehension. While this method excels in short video understanding, it may result in a blend of multiple event information in long videos due to coarse compression, which causes information redundancy. Consequently, the semantics of key events might be obscured within the vast information that hinders the model's understanding capabilities. To address this issue, we propose a Hierarchical Event-based Memory-enhanced LLM (HEM-LLM) for better understanding of long videos. Firstly, we design a novel adaptive sequence segmentation scheme to divide multiple events within long videos. In this way, we can perform individual memory modeling for each event to establish intra-event contextual connections, thereby reducing information redundancy. Secondly, while modeling current event, we compress and inject the information of the previous event to enhance the long-term inter-event dependencies in videos. Finally, we perform extensive experiments on various video understanding tasks and the results show that our model achieves state-of-the-art performances.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) (Touvron et al. 2023; Chiang et al. 2023) has brought about revolutionary changes in the field of NLP, with their exceptional understanding and reasoning abilities enabling the generation of high-quality language texts across various domains. Nonetheless, to genuinely realize the universality of the model, it needs to be capable of integrating and understanding data stemming from multiple modalities, including images, videos, and audio. In response to this requirement, some researchers aim to harness the potent capabilities of LLMs to concurrently integrate information from multiple modalities, thereby addressing a variety of multimodal tasks (Xu et al. 2023; Ye et al. 2024). For instance, video understanding (Islam and Bertasius 2022; Islam et al. 2023) serves as a prime example of those multimodal tasks.\nTo address video understanding tasks, most existing models primarily employ visual foundation models (Fang et al. 2023) to extract visual tokens, which are then directly fed into LLMs to generate inferential outcomes. However, this can be problematic in that some methods (Lin et al. 2023; Li et al. 2023b) only input a smaller number of frames into LLMs for video comprehension due to the limitations of the LLMs context input length and computational resources. While this practice yields satisfactory results for short videos, it may lead to information loss for longer videos, proving detrimental to the temporal modeling of the video content. To cope with this, some methods (He et al. 2024; Song et al. 2024a,b) employ token compression to simultaneously process a larger number of frames in order to compensate for information loss. However, in longer videos, such coarse compression may lead to the amalgamation of various event information, resulting in information redundancy. Consequently, the semantics of key events may be obscured by the overwhelming amount of information, thereby affecting the model's comprehension capabilities.\nTo address the aforementioned challenges, we propose a Hierarchical Event-based Memory-enhanced LLM (HEM-LLM) for better understanding of long videos. Considering the diverse event information contained in long videos, it is crucial that the model processes each event individually to prevent information redundancy. Specifically, we first devise a novel adaptive sequence segmentation scheme to partition multiple events in long videos. In this way, our model can treat each event individually, thereby reducing information clutter. Secondly, we introduce event-based local memory to model individual events, storing information from historical frames within an event to establish intra-event contextual connections. Thirdly, while modeling the current event, we employ global video memory to compress and inject information from the previous event, so enhancing the long-term inter-event dependencies in long videos. Finally, we conduct extensive experiments on various video understanding tasks, such as video question answering, video captioning, and long video activity classification. The results on nine benchmark datasets demonstrate the effectiveness of our model."}, {"title": "Related Work", "content": "Large language models (LLMs) (Chiang et al. 2023; Brown 2020; Achiam et al. 2023) have exhibited remarkable ca-\nMulti-modal Large Language Models"}, {"title": "Proposed methodology", "content": "We propose a Hierarchical Event-based Memory-enhanced\nLLM for better understanding of long videos. We no longer\ncompress the entire long video directly. Instead, we devise\na novel adaptive sequence segmentation scheme to parti-\ntion multiple events within the long video. Subsequently,\nlocal memory is utilized to establish intra-event contex-\ntual connections for each event separately. Then, to enhance\nthe long-term inter-event dependencies, we introduce global\nmemory to compress and inject historical event information,\npromoting the memory modeling of the current event. Fi-\nnally, the well-modeled video tokens are inputted into the\nLLMs for video comprehension. The overall framework of\nthe model is shown in Figure 1."}, {"title": "Adaptive Sequence Segmentation", "content": "Essentially, video sequences consist of consecutive frames.\nTherefore, our model processes the videos in a sequen-\ntial manner to facilitate temporal modeling of videos more\nconveniently. Meanwhile, compared to short videos, long\nvideos encompass a more diverse array of scenes and event\ntransitions. Capturing these transitions enables the accurate\nsegmentation of different scenes and events within the video.\nThe transition points between various scenes and events in a\nvideo are usually accompanied by significant changes in ele-\nments such as characters, backgrounds, activities, and shots.\nAs a result, our model is designed to accurately and effi-\nciently capture these significant changes.\nSpecifically, for a given video sequence $V = \\{v_i\\}_{i=1}^T \\in \\mathbb{R}^{3 \\times T \\times H \\times W}$ of T frames, we employ token-level cosine\nsimilarity to capture significant changes between events and\ndetermine segmentation points. We first perform average\npooling on the spatial dimensions of the T original frames\ncontaining RGB channels to integrate the global informa-\ntion of each frame. Next, we calculate the token-level cosine\nsimilarity Score between pairwise pooled frames to deter-\nmine the magnitude of change between them. Finally, we\napply TopK filtering to the similarity sequence, selecting\nthe top K-1 frames as the segmentation points $s_j$ between\nevents in the video. Consequently, the video sequence V is\nadaptively partitioned into an event sequence $E = \\{e_i\\}_{i=1}^K$,\nwhere each event consists of a series of frames. The compu-\ntation process is described below:\n$V = \\text{AdaptiveAvgPool}(V) \\in \\mathbb{R}^{3 \\times T}$  (1)\nwhere $\\text{AdaptiveAvgPool}$ represents AdaptiveAvgPool2d\nto perform average pooling of spatial dimensional informa-"}, {"title": "Intra-event Local Memory Modeling", "content": "For the event sequence $E = \\{e_i\\}_{i=1}^K$ obtained from the\nabove computation, we perform independent local memory\nmodeling for each event. We utilize a pre-trained visual en-\ncoder to perform token extraction for each frame in the event\n$e_k$, yielding $e_k = \\{f_i\\}_{i=1}^n \\in \\mathbb{R}^{d \\times n \\times p}$, p is the number of\nvisual tokens per frame. To emphasize the temporal infor-\nmation between frames within an event, we assign frame-\nlevel positional encoding to each event respectively, thereby\nincorporating timestamp information into each frame. The\nmain process can be written as:\n$e_k = \\{f_i\\}_{i=1}^n \\in \\mathbb{R}^{d \\times n \\times p}, f_i = VE(v_i)$ (4)\nwhere $e_k$ means the k-th event, $VE$ represents the pre-\ntrained visual encoder and p is the number of visual tokens\nper frame.\nWe perform positional encoding of $f_i$ to add timestamp\ninformation as:\n$f_i = f_i + PE(t_i)$ (5)\nwhere $PE$ denotes the frame-level positional encoding and\n$t_i$ indicates the timestamp information of each frame $f_i$.\nSimultaneously, in order to effectively and efficiently\nbridge the semantic gap between video and text, we incor-\nporate local memory into the Q-Former architecture pro-\nposed by BLIP-2, which uses a finite set of query tokens\n$Q = \\{q_i\\}_{i=1}^{32} \\in \\mathbb{R}^{d \\times 32}$ to facilitate multimodal representa-\ntion learning. In this, we efficiently store the historical frame\ninformation in local memory in a concatenated manner to\nform a memory sequence $LM = \\text{Concat} [f_1, f_2, ..., f_n] \\in\n\\mathbb{R}^{d \\times np}$, which enables query tokens to fully learn the con-\ntextual information of the event. The Q-Former comprises\ntwo components: the self-attention among query tokens and\nthe cross-attention between visual tokens and query tokens.\nThe calculation process for the self-attention component\namong query tokens is as follows:\n$O = \\text{SelfAttn}(Q_q, Q_k, Q_v)$ (6)\nwhere $O \\in \\mathbb{R}^{d \\times 32}$ represents the output of the self-attention\ncomputation between query tokens. Inspired by (He et al.\n2024), we follow the LM operation to collect the query to-\nkens $Q = \\text{Concat} [Q_1, Q_2, \u2026, Q_n] \\in \\mathbb{R}^{d \\times 32n}$ at each time"}, {"title": "Inter-event Global Memory Modeling", "content": "Since long videos contain more than one event and there\nare temporal relationships between consecutive events, it is\nessential to capture the inter-event long-term dependencies\nfor a comprehensive understanding of video semantics. In\nview of this, after completing the local memory modeling\nwithin each event, we incorporate the events into the global\nmemory to better learn the global information of the video.\nThis process is shown below:\n$GM = \\text{Concat} [Q_1, Q_2, ..., Q_N]$ (8)\nwhere GM represents the global memory sequence, N de-\nnotes the number of events processed by the model up to\nthe current event. Considering the limitations of computa-\ntional resources, we employ token compression from (He\net al. 2024) to control the number of tokens in GM.\nAt the same time, to explore the progressive relationship\nbetween events, we inject information from the previous\nevent while modeling the local memory of the current event.\nSpecifically, we adopt GM to replace Q in Equation (6) to\nincorporate historical events into the modeling of the current\nevent. Thus, Equation (6) can be reformulated as:\n$O = \\text{SelfAttn}(Q_q, GM_k, GM_v)$ (9)\nwhere $O \\in \\mathbb{R}^{d \\times 32}$ denotes the output of the self-attention\namong query tokens."}, {"title": "Text Tokens Generation", "content": "Based on the calculated visual tokens $O_c$ for each event, we\nconcatenate them together as the visual tokens $Z_v$ for\nthe entire long video and integrate them into the LLMs through\na projection layer. Subsequently, the LLMs perform auto-\nregressive reasoning on the basis of instruction prompts,\nultimately generating high-quality text tokens $Z_t$. Conse-\nquently, our model can achieve efficient video understand-\ning with a smaller number of tokens, without being con-\nstrained by the LLM's context input and computational re-\nsource limitations. Furthermore, we apply cross entropy loss\nto the reasoning-generated tokens for Self-Supervised Fine-\nTuning (SFT). The supervision process is as follows:\n$Z_v = \\text{Concat} [O_{c1}, O_{c2}, ..., O_{cK}] \\in \\mathbb{R}^{d \\times 32K}$ (10)\nwhere K indicates the number of events in the video.\n$Z_t = LLM(\\Delta(Z_v, \\text{Prompts}))$ (11)\nwhere $\\Delta$ represents the projection layer.\n$\\mathcal{L} = \\text{CrossEntropy}(Z_t, \\text{Target})$ (12)\nwhere CrossEntropy denotes the cross entropy loss func-\ntion and Target means the target tokens."}, {"title": "Experiments", "content": "To comprehensively validate the effectiveness of our pro-\nposed model, we conduct numerous experiments on video\nunderstanding downstream tasks such as video question an-\nswering, video captioning, and long video activity classifica-\ntion. Additionally, we employ multiple datasets with varying\nvideo lengths to verify the universality and generalization\ncapabilities of our model."}, {"title": "Experimental Results", "content": "To conduct a comprehensive\ncomparison with existing models, we perform extensive ex-\nperiments on four open-ended datasets employing two eval-"}, {"title": "Ablation Studies", "content": "In this section, we conduct a comprehensive ablation anal-\nysis of the various components of HEM-LLM to ensure the\ncompleteness of the experiments. We perform extensive ex-\nperiments on the Breakfast and MSVD-QA datasets and re-\nport Top-1 and Top-5 accuracy as research metrics.\nAnalysis of the validity of individual components. We\nperform an ablation analysis of the individual components\nproposed in this paper. The experimental results are shown\nin Table 5, where Local represents Intra-event Local Mem-\nory Modeling, Global denotes Inter-event Global Memory\nModeling, and ASS means Adaptive Sequence Segmenta-\ntion. The results demonstrate that as different components\nare successively incorporated, the Top-1 accuracy metric\ncontinuously improves, showing a positive correlation with\nthe addition of different components. This validates the"}, {"title": "Visualization and Analysis", "content": "To more intuitively validate the accuracy of our proposed\nadaptive sequence segmentation, we perform the case visu-\nalization analysis on the Breakfast. The visualization results\nare shown in Figure 3. In (a), we determined three segmen-\ntation points based on the cosine similarity calculations be-\ntween adjacent frame pairs. According to these points, the"}, {"title": "Conclusion", "content": "In this paper, we propose a Hierarchical Event-based\nMemory-enhanced Large Language Model (HEM-LLM) for\nbetter understanding of long videos. To learn the seman-\ntics of individual events in long videos more precisely, we\nno longer directly compress the visual tokens of the entire\nlong video. Instead, we design a novel adaptive sequence\nsegmentation scheme to segment multiple events in long\nvideos. This allows our model to handle each event respec-\ntively, thus reducing information confusion. Subsequently,\nwe conduct multi-granular memory modeling for intra-event\nand inter-event scenarios to establish long-term dependen-\ncies in long video sequences, thereby enhancing the video\nunderstanding capabilities of the model. Finally, we carry\nout extensive experiments on various benchmarks of multi-\nple video understanding tasks, demonstrating the effective-\nness and universality of our proposed model."}, {"title": "Appendix", "content": "Investigation of the number of event segments.\nstudy the impact of different event segmentation quantities\non experimental performance, we also perform experiments\non the COIN, and the results are shown in Figure 5. We also\nconduct a quantitative study with the number of events rang-\ning from 2 to 6, with a step size of 1. We report three perfor-\nmance metrics, namely Top-1, Top-5, and their average. The\nresults show that the performance is optimal when the num-\nber of event segments is 3. Therefore, we set the number of\nevent segments on the COIN dataset to 3. Furthermore, for\nthe VQA, we set the number of event segments to 2 across\nall datasets. For the video captioning, we set the number of\nevent segments to 3 across all datasets.\nStudy of different sizes of LLM.\nWe conduct experiments on YouCook2 using Vicuna with\nvarious parameters, as shown in Table 7. The results demon-\nstrate that our method outperforms existing models across\ndifferent scales of LLM, which further validates the effec-\ntiveness and versatility of our proposed method. Remark-\nably, our method outperforms the previous methods which\nutilize 7B LLM when using a parameter count of only 3.7B\nLLM, further demonstrating the superiority of our method."}]}