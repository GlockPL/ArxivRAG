{"title": "Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines", "authors": ["Chongyu Qu", "Ritchie Zhao", "Ye Yu", "Bin Liu", "Tianyuan Yao", "Junchao Zhu", "Bennett A. Landman", "Yucheng Tang", "Yuankai Huo"], "abstract": "Quantizing deep neural networks,reducing the precision (bit-width) of their computations, can remarkably decrease memory usage and accelerate processing, making these models more suitable for large-scale medical imaging applications with limited computational resources. However, many existing methods studied \u201cfake quantization\", which simulates lower precision operations during inference, but does not actually reduce model size or improve real-world inference speed. Moreover, the potential of deploying real 3D low-bit quantization on modern GPUs is still unexplored. In this study, we introduce a real post-training quantization (PTQ) framework that successfully implements true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet, ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use TensorRT to perform fake quantization for both weights and activations with unlabeled calibration dataset. Second, we convert this fake quantization into real quantization via TensorRT engine on real GPUs, resulting in real-world reductions in model size and inference latency. Extensive experiments demonstrate that our framework effectively performs 8-bit quantization on GPUs without sacrificing model performance. This advancement enables the deployment of efficient deep learning models in medical imaging applications where computational resources are constrained. The code and models have been released, including U-Net, TransUNet pretrained on the BTCV dataset for abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and VISTA3D pretrained on TotalSegmentator V2 for full body (104-label) segmentation.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks have become indispensable in medical imaging tasks, remarkably enhancing diagnostic accuracy and efficiency in tasks such as image classification [15, 20, 50], segmentation [2, 11, 12, 33, 54], and anomaly detection [3, 40, 45]. Despite their effectiveness, deploying these models in large-scale medical imaging applications poses challenges due to their substantial computational requirements, especially in environments with limited hardware capabilities.\nA promising approach to mitigate these challenges is model quantization [4, 34, 38, 53], which reduces the precision (bit width) of computations within a neural network. By converting high-precision representations (e.g. 32-bit floating point numbers) to lower-precision formats (e.g. 8-bit integers) for both weights and activations, quantization can remarkably decrease memory usage and accelerate processing speeds. This transformation not only makes models more suitable for deployment on devices with constrained resources but also enables faster inference times essential for real-time medical applications.\nQuantization methods are broadly categorized into two types, Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). Quantization-Aware Training (QAT) trains a model by simulating low-precision calculations during both forward and backward passes, allowing the model to adjust to these constraints. As a result, the model can retain its accuracy after being quantized. For example, DeepSeek V3 [25] introduces an FP8 mixed precision training framework and, for the first time, validates its effectiveness on an extremely large-scale model. By leveraging FP8 computation and storage, DeepSeek V3 achieves both accelerated training and reduced GPU memory usage. While QAT can achieve high accuracy at lower precisions, it is time-consuming and requires access to the entire labeled training dataset, which is a notable drawback given the massive size and sensitivity of medical imaging data. PTQ quantizes a pre-trained model without the need for retraining, using only a small set of unlabeled samples to"}, {"title": "2. Related Work", "content": "Model quantization has become a critical technique for deploying deep neural networks on resource-constrained hardware by reducing model size and computational demands. Quantization methods are generally categorized into two main approaches: QAT and PTQ.\nQAT integrates quantization into the training process, allowing the model to learn and adapt to quantization effects"}, {"title": "3. Methodology", "content": "Overview. In this section, we present our proposed PTQ framework designed to implement real INT8 quantization on SOTA medical segmentation models on modern GPUs, as shown in Figure 3. Our method aims to reduce model size and inference latency without retraining, thus making advanced models more accessible in resource-constrained environments. The framework consists of two main components: first, we leverage NVIDIA TensorRT to perform fake quantization by inserting QuantizeLinear and DequantizeLinear nodes into the Open Neural Network Exchange (ONNX) model with unlabeled calibration dataset (\u00a73.1), simulating the quantization process while still relies on FP32 resources. Second, we convert this fake quantized model into a real INT8 quantized TensorRT engine using for efficient deployment on modern GPUs (\u00a73.2). During this conversion, TensorRT detects the QuantizeLinear and DequantizeLinear nodes to perform real INT8 quantization. Our framework addresses the limitations of previous PTQ methods that rely on fake quantization, which simulates lower-precision computations without yielding actual reductions in model size or improvements in inference speed. By converting fake quantization into real quantization, we ensure that the quantized models are not only theoretically efficient but also practically deployable with remarkably resource savings."}, {"title": "3.1. Perform Fake Quantization on ONNX", "content": "In the first component of our framework, we perform fake quantization by quantizing both the weights and activations of the pre-trained network with unlabeled calibration dataset. To facilitate this process, we begin by converting the pre-trained PyTorch model into the ONNX format:\nONNX is an open standard for representing deep learning models, enabling interoperability between different frameworks and tools. This conversion is essential because it allows us to leverage optimization tools that may not be directly compatible with PyTorch models.\nSimulation of INT8 Quantization with ModelOpt. After converting the model to ONNX, we use the NVIDIA TensorRT Model Optimizer (ModelOpt) to simulate the INT8 quantization process:\nThe ModelOpt analyzes the computational graph of the model and inserts QuantizeLinear and DequantizeLinear nodes where appropriate. The QuantizeLinear nodes simulate the conversion of FP32 weights and activations into INT8 values by applying scale factors and zero-points determined during calibration, while the DequantizeLinear nodes convert these simulated INT8 values back into FP32 for subsequent computations. By inserting these nodes, we prepare the model for real quantization in the next step, where NVIDIA TensorRT can detect these nodes and convert the operations into real INT8 computations on modern GPUs. The calibration process involves mapping floating-point values to discrete integer levels that can be represented with lower bit-width precision. For a given floating-point value x the quantized value $x_q$ can be defined as:\n$x_q = \\text{clamp} (\\left[\\frac{x}{s}\\right] + z, 0, 2^k - 1),$  (1)\nwhere the scale factor s and zero-point z are quantized parameters determined during calibration by\n$s = \\frac{X_{\\text{max}} - X_{\\text{min}}}{2^k-1}$, (2)\n$z = -\\frac{X_{\\text{min}}}{s}$, (3)\nwhere k is the number of quantization bits, [.] denotes rounding to the nearest integer, and clamp(\u00b7, a, b) restrict the value within the range [a, b]. Here, $X_{\\text{min}}$ and $X_{\\text{max}}$ represent the minimum and maximum values of x observed in the calibration dataset."}, {"title": "3.2. Converting to Real Quantization Using NVIDIA TensorRT", "content": "In the second step of our framework, we transform the fake quantized ONNX model into a real INT8 quantized engine optimized for efficient deployment using NVIDIA TensorRT. This involves converting the QuantizeLinear and DequantizeLinear nodes inserted during the simulation phase into actual INT8 operations, enabling true real computations on modern GPUs."}, {"title": "4. Experiments & Results", "content": "4.1. Dataset.\nBTCV [17] consists of 70 CT volumes with 13 labeled anatomies. They are randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. Of these, 50 CT volumes, which are publicly available through the MICCAI 2015 Multi-Atlas Labeling Challenge, are used to pre-train our U-Net and TransUnet models. The remaining 20 CT volumes are used for evaluation.\nTotalSegmentator V2 [43] includes 1,228 full-body CT volumes with 117 labeled anatomies, created by the Department of Research and Analysis at University Hospital Basel. We use 200 of these CT volumes to evaluate nnU-Net, SegResNet, SwinUNETR, and VISTA3D across 104 labels. The remaining 1,028 volumes are used to pre-train these models.\nWhole Brain Segmentation Dataset [10] combines 4,859 T1-weighted (T1w) MRI volumes collected from eight different sites, with segmentation labels generated by a multi-atlas segmentation pipeline. Among these volumes, 50 come from the Open Access Series on Imaging Studies (OASIS) dataset [29] and have been manually traced to 133 labels based on the BrainCOLOR protocol [16] by Neuromorphometrics Inc. We use these 50 manually-labeled scans to evaluate our UNesT models, while the remaining 4,809 scans are used for pre-training."}, {"title": "4.2. Implementation Details.", "content": "U-Net and TransUNet are pre-trained and evaluated on the BTCV dataset, UNesT is pre-trained and evaluated on the Whole Brain Segmentation dataset, nnU-Net, SegResNet, SwinUNETR, and VISTA3D are pre-trained and evaluated on TotalSegmentator V2. Except for nnU-Net, which follows its default training plan and original learning rate, these models share the same data augmentation and preprocessing steps, and are trained on a single NVIDIA RTX 4090 GPU with an input volume size of 96 \u00d7 96 \u00d7 96. They employ the Adam optimizer starting at 1e-4 and a weight decay of 1e-5,, with the learning rate dynamically adjusted based on the combined Dice and Cross Entropy (DiceCE) Loss. After training, all models are quantized into INT8 engines using NVIDIA TensorRT, retaining FP32 for input and output, and both quantization and evaluation perform on a single NVIDIA RTX 4090 GPU. Segmentation accuracy is measured via the mean Dice Similarity Coefficient (mDSC), and to compare model efficiency between INT8 and FP32 versions, we monitor model size, GPU memory usage during inference, and inference latency."}, {"title": "4.3. Quantization Results of Segmentation Models", "content": "We evaluate our PTQ framework using seven SOTA medical segmentation models, i.e. U-Net, TransUNet, UNesT, nnU-Net, SwinUNETR, SegResNet and VISTA3D, across three dataset with different number of samples (N) and number of classes (C) i.e., BTCV (N = 20, C = 13), Whole Brain Segmentation (N = 50, C =133) and TotalSegmentator V2 (N = 200, C = 104). To eliminate inconsistencies between libraries (PyTorch vs. TensorRT), all models are converted to TensorRT engines for both FP32 and INT8. As shown in Table 1, the INT8 quantized models achieve 2.42\u00d7 (42.42/17.48) to 3.85\u00d7 (61.98/16.09) reductions in model size and 2.05\u00d7 (5.59/2.72) to 2.66\u00d7 (9.58/3.59) reductions in inference latency, while maintaining the same mDSC performance as their FP32 counterparts. These results demonstrate that our PTQ framework delivers real-world gains in efficiency, especially beneficial when working with large-scale datasets that typically require a large amount of inference time.\nWe also measure GPU memory usage for U-Net and TransUNet on BTCV, as shown in Table 2. Our PTQ framework uses NVIDIA TensorRT, which automatically optimizes models to conserve computational resources during inference. Compared with FP32 models on PyTorch, the FP32 TensorRT engines reduce GPU memory usage by 3.37\u00d7 (6391.25/1893.28) and 2.83\u00d7 (6331.25/2235.28) for U-Net and TransUNet, respectively. After quantizing to INT8 with our PTQ framework, these reductions increase to 3.57\u00d7 (6391.25/1787.28) and 3.37\u00d7 (6331.25/1873.28). These results show that our PTQ framework leverages TensorRT's optimizations to achieve real-world computational savings and improve model efficiency."}, {"title": "4.4. Scaling Analysis", "content": "To evaluate the impact of scaling effectiveness on TotalSegmentator V2 (N = 200, C = 104), we compare the performance and efficiency of STU-Net-S (14M parameters) and STU-Net-H (1.4B parameters) [9]. As shown in Table 3, by scaling the model size, segmentation accuracy (measured by mDSC) increases from 0.837 to 0.869, demonstrating the benefits of scaling for capturing complex anatomical structures.\nDespite the performance gains, scaling introduces higher computational demands. On TotalSegmentator V2, inference latency for STU-Net increases from 2.59 ms to 98.45 ms, making it prohibitively slow when processing large-scale datasets. After applying our PTQ framework, the INT8 quantized STU-Net-H model size decreases by 3.65\u00d7 (1457.33/398.41), and its inference latency drops by 3.26\u00d7 (98.45/30.15), while maintaining comparable mDSC scores to the FP32 counterpart. Compared with smaller STU-Net-S that already achieve a latency of under 10 ms, our PTQ approach yields even more pronounced gains for large-scale models like STU-Net-H, cutting latency by an additional 68.3 ms. Furthermore, the INT8 quantized STU-Net-H achieves a higher compression ratio in terms of model size (3.65x) than STU-Net-S (2.71\u00d7), because smaller models are more impacted by overheads and non-quantizable layers, whereas larger models have a greater proportion of parameters that benefit from quantization. Consequently, our PTQ framework proves particularly beneficial for large-scale models, making it a powerful solution for deploying resource-efficient deep learning systems.\nThese results demonstrate that our PTQ framework effectively addresses the computational challenges of scaling, enabling the deployment of large-scale models like STU-Net in resource-constrained environments. This highlights the synergy between scaling laws and quantization for advancing medical image segmentation."}, {"title": "5. Discussion", "content": "The results of our experiments demonstrate that our proposed PTQ framework effectively reduces model size, computational demands and inference latency without compromising model performance. Notably, the quantized INT8 models maintain segmentation accuracy comparable to their FP32 counterparts, as indicated by the nearly unchanged mDSC. This finding is noteworthy because it challenges the common concern that quantization, particularly at low bit-widths like INT8, inherently leads to a degradation in model performance.\nRobustness Across Models and Datasets. The effectiveness of our framework across SOTA medical segmentation models, i.e., U-Net, TransUNet, UNesT, nnU-Net, SwinUNETR, SegResNet and VISTA3D, and datasets, i.e., BTCV, Whole Brain Segmentation dataset and TotalSegmentator V2, underscores its generalizability. These models vary in architecture and complexity, and the datasets cover a range of anatomical structures and imaging modalities. The consistent performance across these variations indicates that our PTQ framework is robust and adaptable to various AI models used in medical imaging domain.\nClinical Applications. Diagnostic accuracy is critically important in the medical domain, as it directly affects patient care and treatment outcomes. Maintaining model performance after quantization is essential to ensure that efficiency gains do not compromise clinical effectiveness. Our successful application of INT8 quantization to complex segmentation tasks involving high-resolution medical images demonstrates that our framework preserves diagnostic accuracy and can be safely integrated into clinical workflows. This integration has the potential to improve patient outcomes through faster and more efficient diagnostics.\nPotential Limitations and Future Work. Despite the promising results, there are certain limitations to our PTQ framework that need to be addressed. One notable limitation stems from the use of NVIDIA TensorRT for converting fake quantization into real INT8 computations. TensorRT may not fully support models with dynamic blocks or layers that require runtime flexibility, such as those involving variable input sizes or conditional operations. These dynamic architectures can pose compatibility issues with TensorRT's optimization and quantization processes, potentially limiting the applicability of our framework to a subset of AI models. Addressing this limitation would involve enhancing the compatibility of TensorRT with dynamic model components or exploring alternative optimization tools that can handle such architectures effectively.\nFor future work, a promising direction is to focus on quantization to even lower bit-widths, such as 4-bit integer (INT4). Exploring INT4 quantization has the potential to further reduce model size and computational requirements, offering additional benefits for deployment in extremely resource-constrained environments. However, achieving INT4 quantization without compromising model accuracy presents substantial challenges. The reduced precision can introduce considerable quantization errors, leading to performance degradation. Developing advanced quantization techniques, such as adaptive quantization strategies or error compensation methods, will be crucial to maintain model performance at these lower precisions. Additionally, researching hardware accelerators optimized for INT4 computations could enhance the practical feasibility of deploying such quantized models in real-world medical imaging applications."}, {"title": "6. Conclusion", "content": "We have introduced a PTQ framework that achieves real INT8 quantization for SOTA 3D AI models in medical imaging applications. Our framework effectively reduces real-world model size, computational requirements and inference latency without compromising segmentation accuracy on modern GPU, as evidenced by the maintained mDSC comparable to full-precision models. The framework's robustness across diverse set of AI architectures, ranging from CNN based to transformer based models, and a wide variety of medical imaging datasets. These datasets are collected from multiple hospitals with distinct imaging protocols, cover different body regions (such as brain, abdomen, or full body), and include multiple imaging modalities (CT and MRI). collectively, these factors highlight our PTQ framework's strong generalizability and adaptability for a broad spectrum of medical imaging tasks.\nBy preserving diagnostic accuracy while enhancing computational efficiency, our PTQ framework holds considerable potential for clinical integration. It enables the deployment of advanced AI models in resource-constrained environments, facilitating faster and more efficient diagnostics without compromising patient care."}]}