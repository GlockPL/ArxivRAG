{"title": "Large Language Models: An Applied Econometric Framework", "authors": ["Jens Ludwig", "Sendhil Mullainathan", "Ashesh Rambachan"], "abstract": "Large language models (LLMs) are being used in economics research to form predictions, label text, simulate human responses, generate hypotheses, and even produce data for times and places where such data don't exist. While these uses are creative, are they valid? When can we abstract away from the inner workings of an LLM and simply rely on their outputs? We develop an econometric framework to answer this question. Our framework distinguishes between two types of empirical tasks. Using LLM outputs for prediction problems (including hypothesis generation) is valid under one condition: no \u201cleakage\u201d between the LLM's training dataset and the researcher's sample. Using LLM outputs for estimation problems to automate the measurement of some economic concept (expressed by some text or from human subjects) requires an additional assumption: LLM outputs must be as good as the gold standard measurements they replace. Otherwise estimates can be biased, even if LLM outputs are highly accurate but not perfectly so. We document the extent to which these conditions are violated and the implications for research findings in illustrative applications to finance and political economy. We also provide guidance to empirical researchers. The only way to ensure no training leakage is to use open-source LLMs with documented training data and published weights. The only way to deal with LLM measurement error is to collect validation data and model the error structure. A corollary is that if such conditions can't be met for a candidate LLM application, our strong advice is: don't.", "sections": [{"title": "Introduction", "content": "How should large language models (LLMs) be used in economics research?\u00b9 Since they are easy-to-use, general-purpose tools, researchers are already using LLM outputs to form predictions, label text, answer surveys, simulate humans in experiments, generate hypotheses, and even produce data for time periods and places for which such data don't currently exist.\u00b2 These uses are no doubt creative, but are they valid? How can we assess whether empirical research using LLM outputs is leading to correct inferences?\nConsider how we normally answer this question for a more familiar econometric procedure like ordinary least squares (OLS). At its core, OLS is nothing more than an algorithm that, when applied to a dataset, returns the coefficients that minimize the sum of squared residuals. Econometrics clarifies what assumptions the researcher must believe about the underlying data generating process (DGP) to justify possible interpretations of the algorithm's output. If we want to interpret  $\\beta$ as the best linear unbiased estimator of the conditional expectation function, the Gauss-Markov theorem tells us what we need to be willing to assume (and defend) about the DGP; if we want to interpret  $\\beta$ causally, we need to be willing to defend the conditional independence assumption. In this sense, econometrics provides empirical researchers with the terms of a contract: to endow an estimate with a particular interpretation, these are the assumptions you must defend.\nAppropriately using LLM outputs requires the same sort of contract: what assumptions about LLMs must be defended in order to draw some particular inference? Yet at present, no such contracts exist. In fact, producing such a contract is challenging given how econometrics typically models data procedures. Our contracts for OLS are based on knowing the exact operations of OLS on a data matrix. In contrast, LLMs are extraordinarily complex machine learning models involving many layers of interactivity and billions of parameters. Their training datasets and architecture (among many other details) are proprietary that is, intentionally hidden from the user. They are also a diverse, dynamic set of commercial products. Different families of LLMs make different choices along each of these dimensions. The state-of-the-art is ever-evolving: new LLMs are released constantly. Deriving guarantees from the exact operations of these models, like we do for OLS, is simply not feasible."}, {"title": "Evaluating General-Purpose Technologies: the LLM Conundrum", "content": "Why has our research community been willing to create a VIP lane for LLMs straight into our studies? For two reasons. The first relates to the impressive performance of LLMs on benchmark evaluations, combined with memorable examples of their capabilities. The second is our tendency to draw inferences and generalize from how LLMs performed on some tasks to how they are likely to perform on other tasks. After all, if we saw a human ace the math GRE, we would naturally assume this person would perform well on all manner of other math problems as well. We tend to make the same assumption about LLMs.\nYet both reasons turn out to be problematic. The domains on which LLMs perform well turn out to be quite selective, especially when compared to the grand ambition to have LLMs serve as general-purpose technologies. And the way that the capabilities of LLMs generalize across tasks turns out to be quite different than what we intuitively assume. In what follows we give a sense for how truly odd the behavior of LLMs can be, and consequently how far they are from being able to perfectly substitute for human intelligence, much less serve as a sort of \u201csuper intelligence.\u201d"}, {"title": "Benchmark Evaluations and Anthropomorphic Generalization", "content": "The central difference between LLMs and supervised learning algorithms is that LLMs aspire to serve as useful tools not just for a single task, but all tasks to be a general-purpose technology (GPT). The scope of that ambition is both the most remarkable feature of LLMs and also their Achilles' heel.\nBecause supervised learning algorithms (e.g., convolutional networks) aim to tackle a single task (e.g., image classification), it is straightforward to evaluate any algorithm's prediction to know how well an algorithm works and which alternative design works better. This was operationalized by the machine learning community through the creation of open public prediction tasks that competing algorithms could be deployed on to assess performance, the so-called common"}, {"title": "Impressive Feats, But Also Puzzling Examples", "content": "This anthropomorphic generalization of the LLM's capacity is common, understandable (given their seemingly human interface), and deeply problematic. Recent work in computer science shows the capabilities of LLMs are brittle: For every example of impressive performance, there is a counterexample that leaves users scratching their heads.\nThe type of brittleness that has attracted the most public attention is the periodic tendency of LLMs to hallucinate to report back plausible-sounding \u201cfacts\u201d that are not actually facts. For example, an LLM asked to provide proof that dinosaurs created a civilization will reply that there is fossil evidence of dinosaur tools, and even credit dinosaurs with the invention of primitive art forms like stone engravings (Szempruch, 2023). This tendency to hallucinate is not limited to trivial application domains, as was discovered by the lawyer who used an LLM to prepare a legal brief, only to discover it cited a number of \u201ccases\u201d that are not real cases (Bohannon, 2023). These hallucinations are not rare events; one study found that LLMs hallucinate in 58% of legal applications (Dahl et al., 2024). While some may hope that there exists some future technological solution to hallucination, recent work provides statistical and computational arguments suggesting this may be inevitable an intrinsic and unavailable feature of the language generation problem (e.g., Kalai and Vempala, 2024; Xu, Jain and Kankanhalli, 2024).\nWhile hallucinations are well-known, economists and other users who are impressed by some LLM's performance on the math SAT or math Olympiad might be surprised to see examples of how poorly these same LLMs can perform on other math problems. For example, the same LLM that can reliably solve (9/5) x + 32 cannot solve (7/5) x + 31; an LLM that can reliably implement common ciphers such as shift by 13 cannot implement other variations (McCoy et al., 2024).\nThe performance of LLMs turns out to be remarkably sensitive to seemingly minor details. An LLM that can correctly answer a given multiple choice question will often answer the same question incorrectly after the order of the answers has been permuted (Zong et al., 2024). LLMs struggle on \"counterfactual\" versions of tasks for example, being able to program a list sort in 0-based indexing but unable to perform the same task in 1-based indexing (Wu et al., 2024). Similar sensitivity has been documented on logical reasoning tasks (Lewis and Mitchell, 2024). Their behavior can be substantially influenced by appending an adversarial string to an existing question (Zou et al., 2023). The same GPT-4 model that demonstrates impressive spatial awareness in"}, {"title": "An Empirical Framework for LLMs", "content": "A key feature of modern LLMs is that we typically interact with them as black boxes: even though key choices in their design and the exact contents of their training datasets are unknown to us, we nonetheless use them to generate responses given prompts. We are willing to use these LLMs as a generic tool for processing almost any text\u2014a true general-purpose technology given their impressive performance on benchmark evaluations. But does this heuristic make any sense?\nIn this section, we develop a framework that helps clarify what properties an LLM must satisfy in order for us to safely incorporate it into empirical research. While social scientists in practice tend to treat LLMs as an all-purpose black box, in reality the model's performance will hinge critically\u2014as our framework demonstrates- on what text the algorithm is trained on, to what text we seek to apply it, and for what purpose."}, {"title": "Setting and the Researcher's Dataset", "content": "Let \u2211* denote the collection of strings (up to some finite length) in an alphabet with elements \u03c3\u0395\u03a3*, and a training dataset is any collection of strings. We summarize a training dataset by the vector t, whose elements to are sampling indicators for whether a particular string is collected in the training dataset.\nOf course, for any given empirical question, not all strings are relevant; we denote those that are as R\u2286\u03a3* with elements r\u2208 R that we refer to as text pieces. The researcher's dataset is also summarized by the vector d, whose elements do are similarly sampling indicators for whether the researcher collected a particular string. The researcher only collects economically relevant text pieces, and so d =0 for all strings \u03c3\u2208\u03a3*\\R.\nEach text piece r is (or can be) linked to observable economic variables (Yr,Wr), which can be thought of as economic outcomes that might be influenced by the text (Y) or candidate economic determinants that might influence the text (W). Altogether the researcher observes (r,Yr,Wr) for each collected text piece with dr=1. To make this more concrete, consider two illustrative empirical applications that we return to throughout the paper."}, {"title": "Example: Congressional legislation", "content": "Consider descriptions of bills introduced in the United States Congress. Each text piece r\u2208 R refers to a bill's brief description such as \u201cA bill to revise the boundary of Crater Lake National Park in the State of Oregon.\u201d The economically relevant outcome Y, might be whether the associated bill passed its originating chamber of Congress. The candidate economic determinant W, of the bill's text might be the party affiliation or roll-call voting score of the bill's sponsor."}, {"title": "Example: Financial news headlines", "content": "Consider financial news headlines about publicly traded companies. Each text piece r\u2208R refers to a particular financial news headline such as \u201cBank of New York Mellon Q1 EPS $0.94 Misses $0.97 Estimate, Sales $3.9B Misses $4.01B Estimate.\u201d The economic outcome Y, might be the company's realized return in some event window after the headline's publication date, while the candidate determinant W of the news headline itself could be the company's past fundamentals.\nImportantly, each text piece r also expresses some economic concept Vr for which it is costly in terms of either time or money to obtain measurements. A key assumption (often left implicit) in much empirical research is that there is some procedure that could in principle be applied to each text piece to collect what the researcher would be willing to consider a \u201cgold standard\u201d measurement of the economic concept; that is, there exists some mapping such that  Vr = f*(r) for all text pieces r\u2208R.\nFor example, in some applications a researcher is willing to argue that a sufficiently reliable measure of the economic concept could be derived by having a single expert spend the time and effort needed to read each text piece carefully. In other applications, the researcher, motivated by findings in psychology (e.g., Biemer et al., 2013; Kahneman, Sibony and Sunstein, 2021), will worry that human annotations of some text are noisy proxies for the economic concept, but is"}, {"title": "Large Language Models", "content": "To capture how we typically interact with LLMs as black boxes to generate responses given prompts without knowing or worrying much about their design or the contents of the training datasets we define a large language model as any mapping from possible training datasets t to mappings between strings, where m(\u00b7;t) : \u03a3* \u2192 \u03a3* is its text generator when it is trained on dataset t and m(\u03c3;t) is the LLM's response when prompted by string \u03c3.\nWhile this definition is quite general, notice it has a specific implication: The LLM \u201calgorithm\u201d is actually two algorithms: a training algorithm that takes in any training dataset t and learns the mapping between strings; and what we call the text generator, m(\u00b7;t), which is the output of the training algorithm and is what the user interacts with to obtain responses to any given prompt. (We return below to the importance of this implication).\nThe other important feature of our definition is that our analysis will not depend on how exactly an LLM's training algorithm and text generator accomplish their functionalities. This is"}, {"title": "Interpreting the Text Generator and the Training Algorithm", "content": "Our framework, even as general as it is, nonetheless captures all of the key design choices underlying LLMs. Some of those choices are made by the researcher themselves, while others are made by the algorithm builder many of which may even be invisible to the researcher.\nThe researcher interacts entirely with the text generator m(\u00b7;t), which winds up capturing all choices that influence how an LLM generates responses from any prompt once it has been trained. For example, research in computer science has found that alternative prompt engineering strategies, such as personas, chain-of-thought reasoning, or few-shot prompting, materially affect the quality of responses given by an LLM to a fixed task or question (Liu et al., 2023; Wei et al., 2024; White et al., 2023; Chen et al., 2024). In our framework, any particular prompt engineering strategy can be cast as an alternative choice of the text generator m(.;t).\nThe text generator additionally captures alternative choices of other hyper-parameters that govern how the LLM randomly generates text in response to a given prompt, such as its \u201ctemperature,\u201d top-p sampling or top-k sampling parameters. An LLM models the probability distribution over tokens that are likely to come next given an existing string; these hyperparameters all govern the extent of randomness in how the text generator samples from its probability distribution over next tokens. Alternative LLMs may have different default choices of these parameters; various APIs give researchers access to choose these parameters themselves. By defining the text generator as a deterministic mapping from prompts to responses, our framework can be interpreted as focusing on the case in which these parameters are such that the LLM greedily only generates its most likely token. Our results extend naturally to stochastic text generators as well, but at the expense of more cumbersome notation.\nWe briefly note that the text generator m(\u00b7;t) also captures other design choices that are typically hidden to the user. Since it is computationally costly to generate text from the transformer architecture, there exist alternative \u201cinference algorithms\u201d that may be used, such as greedy"}, {"title": "Prediction with LLMs", "content": "With this framework in hand, we can turn to identifying the conditions under which the LLM can be incorporated into an economics research pipeline. We start with what we defined above as a \"prediction problem.\u201d A researcher would like to assess the predictability of the linked economic variable Y, based on the text pieces r. The economic variable could be, say, a stock price or an indicator for whether some piece of legislation passed Congress, but (as we discuss further below) could also be something more abstract like a new scientific hypothesis.\nWhile the researcher could in principle build their own predictor based on text from scratch, doing so would require vast amounts of training data to first learn the sort of lower-dimensional representation of language needed to successfully predict from text. Since LLMs have already learned some lower-dimensional representation, having been trained on enormous datasets, we may hope to use them as a foundation upon which to tackle our prediction problem.\nWe use our framework to establish the key condition under which the use of an LLM for this type of research question is valid: there must be no leakage between the LLM's training dataset and the researcher's context. Training leakage is a thorny problem for benchmark evaluations of LLMs in computer science, and it places meaningful limits on the types of prediction problems that researchers can tackle using LLMs. Nonetheless, we discuss how training leakage can be managed through careful choice of the researcher's context and LLM. As a result, LLMs can be valuable tools for solving prediction problems, provided we take the threat of training leakage seriously far more seriously than has typically been the case in social science research to date."}, {"title": "The Researcher's Prediction Problem", "content": "Suppose the researcher prompts an LLM, trained on some unknown training dataset t, to form predictions of the linked variable based on the text pieces,  \\hat{Y}_r = m(r;t). For some non-negative loss function l(y,\u1ef9), the researcher then calculates the sample average loss of the LLM's predictions on their collected dataset:\n\\frac{1}{N} \\sum_{r \\in R} D_r l(Y_r, m(r;t)) \\qquad (1)\nwhere N = \\sum_{r \\in R} D_r is the number of text pieces collected by the researcher. We would like to draw conclusions about the predictability of the linked economic variable Y from the text piece r based on the LLM's sample average loss (Equation 1). What conditions must the LLM satisfy in order for this to be valid?\nTo answer this question, we define the researcher's inferential goal in the prediction problem. We associate the researcher with a context Q(\u00b7) \u2208Q that summarizes their own sampling distribution over economically relevant text pieces and beliefs about the LLM's sampling distribution over training datasets; more precisely, Q(\u00b7) is a joint distribution over the sampling indicators (D,T). The researcher's context Q(\u00b7) therefore summarizes two distinct features: first, it defines the collection of text pieces over which the researcher would like to assess the predictability of Y this is chosen and known to the researcher; second, since its exact contents are unknown, it also captures the researcher's uncertainty over what strings entered into the LLM's training dataset.\nWe assume that the collection of research contexts Q satisfies the following assumptions.\nAssumption 1. Letting t = (t_{o,1},...,t_{o,|\\Sigma^*|}) denote the large language model's realized training dataset, all research contexts Q(\u00b7) \u2208Q satisfy the following assumptions:\n(i) For all values d,  Q(D=d,T=t)= \\Pi_{r \\in R} Q(D_o=d_o,T=t_o).\n(ii) E_Q[\\sum_{r\\in R}D_r]=E_Q[\\sum_{r\\in R}D_r | T=t].\nAssumption 1(i) states that the researcher's and the LLM's sampling process over strings is independent but not identically distributed, capturing the idea that not all strings may be sampled with equal probability. Assumption 1(ii) states that irrespective of the LLM's corpus, the researcher always samples the same number of text pieces on average. As notation, q_r^{TD}(t_o)=Q(T_o=t_o | D_o=1) is the conditional probability the string is sampled by the LLM's training dataset given that it is sampled by the researcher. We let q(t_o)=Q(T_o=t_o) and q^D =Q(D=1)."}, {"title": "Training Leakage as a Threat to Prediction", "content": "We next clarify what guarantee M is necessary and sufficient for an LLM to generalize in a research context, and therefore whether it is a general-purpose technology for prediction.\nLemma 1. Under Assumption 1, for any research context Q(\u00b7) \u2208Q and text generator m(\u00b7),\nE_Q \\Big[\\sum_{r \\in R} D_r l(Y_r, \\hat{m}(r)) \\Big] - E_Q \\Big[\\sum_{r \\in R} D_r l(Y_r, \\hat{m}(r)) | T=t \\Big] = E_Q \\Big[\\sum_{r \\in R} D_r  \\Big(\\frac{q^{TD}_r(t_r)}{q^D_r(t_r)} -1\\Big)  l(Y_r, \\hat{m}(r)) \\Big]\nProposition 1. The large language model m(\u00b7;t) generalizes for research context Q(\u00b7) \u2208Q if and only if it satisfies the guarantee M(Q) for\nM(Q)= \\Big{ m(\u00b7) : -E_Q \\Big[\\sum_{r \\in R}  \\Big(\\frac{q^{TD}_r(t_r)}{q^D_r(t_r)} -1\\Big) D_r  l(Y_r, \\hat{m}(r)) \\Big] = 0  \\Big} \\qquad (3)\nConsequently, the LLM m(\u00b7;t) is a general-purpose technology for prediction if and only if it satisfies the guarantee  \\bigcap_{Q \\in Q} M(Q).\nLemma 1 and Proposition 1 together establish that the given LLM generalizes if and only if there is no training leakage (Equation 3).\nWhat is the intuition behind this result? The term (\\frac{q^{TD}_r(t_r)}{q^D_r(t_r)} -1) captures any dependence between the researcher's sampling distribution over text pieces and their beliefs over the LLM's sampling distribution over strings. When this term is non-zero for a text piece, knowing that the researcher has sampled a text piece affects our beliefs about the likelihood the text piece has entered the LLM's training dataset. In this sense, Equation (3) captures the extent to which there is \"overlap\" between the researcher's dataset and the LLM's training dataset in the research context Q(.). If further the LLM predicts well on text pieces that are likely included in the training dataset, then Equation (3) will tend to be positive and so the sample average loss of the LLM will tend to overstate its performance. Our uncertainty over what exact strings entered into the LLM's training dataset acts like an omitted variables bias in the prediction problem."}, {"title": "Evidence on Training Leakage", "content": "Most existing evidence on training leakage sits in computer science and natural language processing and unfortunately indicates that training leakage is a pernicious problem in evaluating the capabilities of LLMs. There is ample evidence, for instance, that the training datasets of LLMs contain examples from common standardized-test benchmarks that have captured the public's imagination, such as the SAT, GRE and AP exams, as well as popular benchmark datasets in"}, {"title": "Assessing Training Leakage in Congressional Legislation", "content": "We first assess training leakage in an empirical setting relevant for economists studying politics and political economy: congressional legislation. We use data from the Congressional Bills Project (Wilkerson et al., 2023; Adler and Wilkerson, 2020), which contains the text description r for more than 400,000 bills proposed in the U.S. Congress. For each piece of legislation, we also observe whether it passed either the House or the Senate Y. For our empirical exercise, on a random sample of 10,000 Congressional bills introduced from 1973 to 2016, we explore whether the bill passed the House or the Senate can be predicted from the text of its description alone. A substantial literature in economics and political science studies the complicated strategic dynamics involved in congressional voting patterns; these dynamics make predicting legislative outcomes a challenging activity - especially using only a bill's short text description. Indeed, among these 10,000 randomly sampled bills, only 7.4% pass the House and 6.0% pass the Senate.\nFor our empirical analysis we generate predictions \\hat{Y}_r = m(r;t) based on each bill's text description r by prompting GPT-40 (see Appendix Figure A10 for the specific prompt). Impressively, we find that GPT-40 correctly predicts the bill's outcome 91.2% of the time in the House and 92.5% of the time in the Senate (left panel of Table 1). Taking these results at face value, it would be tempting to conclude that GPT-40 is able to detect subtle undercurrents in the bill's writing that point the way the political winds are blowing.\nSo what drives GPT-40's ability to accurately predict whether a Congressional bill will pass the House or the Senate based only on its text description? The answer is actually much simpler: the text of congressional legislation is likely included in GPT-40's training dataset, producing training leakage in this prediction problem.\nTo evaluate training leakage, we prompt GPT-40 to complete each bill's text description"}, {"title": "Assessing Training Leakage in Financial News Headlines", "content": "We next consider another domain relevant for economists: financial markets. Existing work such as Glasserman and Lin (2023) and Lopez-Lira and Tang (2024) found that LLMs appear to predict company-level stock returns accurately using the text of pertinent financial news headlines. We complement this work by assessing the scope for training leakage."}, {"title": "Recommendations for Empirical Practice", "content": "We have shown that naive uses of large language models in economic prediction problems can suffer from training leakage, which cannot be fully addressed through simple prompt engineering strategies. Fortunately our econometric framework provides some guidance for how (and how not) to use LLMs productively in prediction problems.\nFirst and foremost, researchers interested in using LLMs for prediction tasks in economics applications must use open-source LLMs that either clearly document their underlying training data and/or provide a clear time-stamp beyond which the LLM has not been re-trained. As"}, {"title": "Plug-In Estimation with LLMs", "content": "In this section, we turn to the conditions under which such models could potentially serve as a general-purpose technology for estimation problems. We define those as ones in which the researcher would like to measure some economic concept Vr expressed by each collected text piece r in order to estimate some downstream parameter of interest. The researcher is interested in having an LLM automate the collection of gold standard measurements, so that the model's outputted labels of the economic concept of interest could be plugged into the economist's estimation problem.\nWe argue that this practice requires that there is no measurement error in the large language model's labels of the economic concept. In other words, the LLM must be as good as the gold standard everywhere for every dataset and for every task. This assumption, stated so directly, is as a conceptual matter unlikely to be true with vanishingly small odds. Yet it is often implicit in many current research uses of LLMs within economics.\nAs an empirical matter, the assumption of no measurement error contradicts the results of benchmark evaluations of LLM performance as reported by either tech companies or computer science researchers. We extend that evidence here by empirically illustrating the severity of the implications of LLM measurement error for downstream economic parameter estimation. Specifically, in both of our empirical applications (financial headlines and Congressional legislation), we show that seemingly minor implementation choices such as which LLM to use or how to prompt the model affect not only the labels the LLM assigns to a given piece of text, but also substantially affect the size, significance and even sign of economic parameters estimated using those LLM labels.\nThe good news is that there is a fix: collect a sub-sample of gold-standard validation data to model the LLM's measurement error. The corollary of this fix is that economists should not use the LLM's output for plug-in estimation without this type of validation data and measurement error correction, which winds up ruling out applications in which validation data cannot be collected."}, {"title": "The Researcher's Estimation Problem", "content": "To illustrate what guarantee is needed for an economist to use an LLM to solve an estimation problem, let us further formalize what we mean by an estimation problem.\nThe researcher specifies some parameter of interest \u03b8\u2208\u0472 and a moment condition g(\u00b7) that, in principle, identifies this parameter. Consequently, if the researcher observed the economic concept Vr associated with each collected text piece r, she would be satisfied to calculate the parameter estimate\n\\hat{\\theta}^* = \\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} \\frac{1}{N} \\sum_{r \\in R} D_r g(V_r, W_r; \\theta). \\qquad (4)\nFor example, letting  g(Vr,Wr;\u03b8) = (Vr \u2212Wr\u03b8)2 , then the researcher would like to understand how the economic concept Vr relates to the linked variables Wr, and  \\hat{\\theta}^* corresponds to the sample regression coefficient. Due to the text processing problem, the researcher does not observe Vr and so  \\hat{\\theta}^* cannot be calculated directly.\nTo solve the text processing problem, suppose the researcher prompted an LLM to measure the economic concept on each text piece,  \\hat{V}_r= m(r;t), and plugged the LLM's labels into the moment condition\n\\hat{\\theta} = \\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} \\frac{1}{N} \\sum_{r \\in R} D_r g(\\hat{m}(r;t), W_r; \\theta). \\qquad (5)\nWe would like to draw conclusions about the target estimate  \\hat{\\theta}^* defined using the economic concept Vr based on the feasible estimate  \\hat{\\theta} defined using the LLM's labels  \\hat{m}(r;t). What conditions must the LLM satisfy in order for this workflow to be valid?\nTo answer this question, we must (as with prediction problems) define the researcher's inferential goal in the estimation problem. We associate the researcher with a research context Q(\u00b7) \u2208Q, where the collection Q satisfies Assumption 1. We further assume that the researcher could study any moment condition g(\u00b7) \u2208G satisfying the following assumption.\nAssumption 2. For all g(\u00b7) \u2208 G, g(\u00b7) is differentiable and there exists some G > 0 such that  |\\frac{\\delta g(V_r, W_r; \\theta)}{\\delta \\theta}| <= G  for all r\u2208R and \u03b8\u2208\u0398.\nConditional on the LLM's realized training dataset, under mild regularity conditions as the number of economically relevant text pieces grow large (see Appendix C.1), the researcher's sample moment condition converges to\n\\frac{1}{N} \\sum_{r \\in R} D_r g(\\hat{m}(r;t), W_r; \\theta) - \\frac{1}{E[\\sum_{r \\in R} D_r]} E  \\Big[ \\sum_{r \\in R} D_r g(\\hat{m}(r;t), W_r; \\theta) | T=t \\Big] \\qquad (6)\nat any parameter value \u03b8\u0395\u0398.\nWith this in hand, we introduce the following definition. Given an LLM with guarantee M, the researcher would like to recover the moment condition defined using the economic concept.\nDefinition 2. The LLM m(\u00b7;t) with guarantee M is a valid measure of the economic concept V for the moment condition g(\u00b7) in research context Q(\u00b7) if, for all models satisfying the guarantee"}, {"title": "Measurement Error as a Threat to Estimation", "content": "We next clarify what type of guarantee M is necessary and sufficient for an LLM to be a valid measure of the economic concept for a particular research context and moment condition, and therefore whether it is a general-purpose technology for estimation. We first decompose the difference between the plug-in moment condition and the target moment condition into two terms.\nLemma 2. Under Assumption 1, for any research context Q(\u00b7) \u2208Q, moment condition g(\u00b7)\u2208G and text generator m(\u00b7),  E_Q \\Big[ \\sum_{r \\in R} D_r g(\\hat{m}(r), W_r; \\theta) | T=t \\Big] - E_Q \\Big[ \\sum_{r \\in R} D_r g(V_r, W_r; \\theta) \\Big]  equals\nE_Q \\Big[ \\sum_{r \\in R} D_r \\Big[ g(\\hat{m}(r), W_r; \\theta) -  g(V_r, W_r; \\theta) \\Big] \\Big] + \\sum_{r \\in R} \\Big[ \\frac{q^{TD}}{q^D}(t_r) - 1 \\Big]  g(\\hat{m}(r), W_r; \\theta)  \\Big]  \\qquad (7)\nThe second term in Equation (7) is familiar from our analysis of prediction problems\u2014it represents training leakage between the LLM's training dataset and the researcher's context. As discussed in the previous section, training leakage can be controlled through an appropriate choice of open LLM and the researcher's context (e.g., Corollary 1). Consequently, we will maintain the assumption that the LLM satisfies the guarantee of no training leakage.\nWe will instead focus on what additional guarantee is needed to control the first term in Equation (7). Towards this, we introduce some additional notation. We define M(Q,\u03b4) to be the collection of text generators satisfying  ||\\hat{m}(\u00b7) - f^*(\u00b7)||_{\u03b4,Q} = \\max_{r \\in R: q_r^D>0} | \\hat{m}(r;t) - f^*(r) | \\leq \u03b4.\nWe further say that a moment condition g(\u00b7) is sensitive to the economic concept V, in research context Q(\u00b7) if  q_r^D >0 and there exists some G>0 such that |\\frac{\\partial g(v,W_r;\\theta)}{\\partial v}| > G for all v,\u03b8. Let R(g,Q) denote the collection of sensitive text pieces."}, {"title": "Evidence on Measurement Error", "content": "Plug-in estimation requires the strong guarantee that the LLM's labels are as-good-as gold standard measurements of the economic concept Vr. This guarantee is often implicitly invoked in many research uses of LLMs. It is also quite intuitively appealing from observing the LLM's impressive"}, {"title": "Linear Regression with LLMs", "content": "Consider a researcher that would like to relate the economic concept Vr and linked variables Wr by estimating a linear regression. Due to the text processing problem, this is infeasible; instead the researcher prompts an LLM and reports the associated plug-in regression. Of course, there are two types of regressions the researcher may run. First, she may regress the economic concept on the linked variables\n\\hat{V}_r=W_r\\beta^*+\\epsilon_r, and m(r;t)=W_r\\beta+\\hat{\\epsilon}_r. \\qquad (10)\nSecond, she may regress the linked variable on the economic concept\nW_r=V_r\\alpha^*+\\nu_r, and W_r=m(r;t)\\'\\alpha+\\bar{\\nu}_r. \\qquad (11)\nThe bias of the plug-in regression coefficients"}]}