{"title": "ASYNCHRONOUS LLM FUNCTION CALLING", "authors": ["In Gim", "Seung-seob Lee", "Lin Zhong"], "abstract": "Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6\u00d7-5.4\u00d7 compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.", "sections": [{"title": "1 INTRODUCTION", "content": "Function-calling capabilities enable large language models (LLMs) to access external data sources and tools, such as weather forecasts and calculators. Both commercial and open-source LLMs have integrated this feature (Schick et al., 2024; Patil et al., 2024), unlocking new possibilities for diverse applications, from autonomous AI agents operating in dynamic environments (Wang et al., 2024) to neurosymbolic systems combining symbolic reasoning with LLMs to solve complex problems (Trinh et al., 2024).\nLLM function calls are synchronous, with the LLM and the function call executor taking turns generating and executing calls. Although simple to implement, this approach is neither resource-efficient nor responsive. Each function call blocks LLM inference one of the most resource-intensive processes-until the function returns. From the executor's perspective, this limits concurrency since all function calls must finish in the order they are initiated by the LLM. These inefficiencies worsen as the number of functions increases with the complexity of the task (Zaharia et al., 2024).\nSeveral studies have tried to address these challenges, including using compilers to parallelize function calls (Kim et al., 2023), fusing sequential calls to reduce overhead (Singh et al., 2024a), designing concise call syntax (Chen et al., 2023), and optimizing LLM serving systems for function calling (Abhyankar et al., 2024; Gao et al., 2024; Xu et al., 2024). While these methods help reduce function execution time or the number of function calls (\u00a72), they are fundamentally limited by the synchronous nature of function call, e.g., the LLM waiting for the function call executor to finish.\nWe propose AsyncLM, a system that enables asynchronous interactions between LLMs and function call executors to overcome these limitations. In AsyncLM, the LLM and the executor operate independently without blocking each other, drawing inspiration from asynchronous programming paradigms where events, e.g., function call completions, occur independently of the main program flow, e.g., LLM token generation stream. Figure 1 illustrates the concept.\nThe key mechanism of AsyncLM is interruptible LLM decoding. In AsyncLM, function calls can be non-blocking; when a function call returns, the executor asynchronously notifies the LLM by injecting interrupt tokens into the"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "The concept of augmenting LLMs with code execution (Mialon et al., 2023) has been extensively explored, notably in contexts like retrieval-augmented generation (Khattab et al., 2022; Yao et al., 2023), autonomous agents (Huang et al., 2024; Wang et al., 2024), and neurosymbolic problem solving (Pan et al., 2023; Trinh et al., 2024).\nLearning to generate function calls. The most common method for LLM interaction with external systems is tool or function calling (Schick et al., 2024; Shen et al., 2024), where the model autonomously generates calls to external executors (e.g., API servers, code interpreters). This ability is refined either through fine-tuning on diverse task datasets (Patil et al., 2024) or using in-context instructions (Liang et al., 2024). Function descriptions, including arguments, are provided in a structured prompt format, often in JSON. Our approach builds on this paradigm by enabling asynchronous function calling, requiring LLMs to (1) consider execution time in call generation and (2) use interrupt semantics to decide subsequent calls.\nEfficient LLM function calling. A major challenge in function calling is optimizing efficiency to enhance resource utilization and reduce latency. Various studies have explored different optimization strategies to improve the function calling process. For instance, parallel function calling approaches (Kim et al., 2023; OpenAI, 2023) instruct the LLM to bundle calls that can be executed simultaneously, enabling external compilers to optimize these batches for parallel execution. Sequential function call optimizations include methods like function call fusion (Singh et al., 2024a), caching (Singh et al., 2024b), compact syntax for call representation (Chen et al., 2023), and partial execution of function calls (Xu et al., 2024), which allow overlapping of generation and execution of a single code block.\nLimitations of a synchronous interaction. Currently, LLMs perform synchronous function calls. This interleaved nature of generation and execution introduces extra overheads in LLM inference, due to the stateless nature of LLMs,"}, {"title": "3 REPRESENTING ASYNCHRONOUS\nINTERACTION WITH CML", "content": "We define a simple domain-specific language, called Context Markup Language (CML), to represent asynchronous function calls and interrupts. CML acts as the interface between the LLM and the executor, with its syntax ensuring that each component provides the necessary context when interacting through this interface. For example, the LLM can generate a function call in CML to notify the executor, while the executor signals completion by inserting interrupt tokens in CML, as demonstrated in Figure 3.\nCML uses a minimal set of specialized tokens: [CALL], [INTR], [TRAP], [END], and [HEAD]. The [CALL], [INTR], and [TRAP] tokens initiate a control block, which represents a function call (\u00a73.1), an interrupt (\u00a73.2), or a trap (a special interrupt), respectively. The [END] token marks the end of the control block, and [HEAD] separates optionally provided metadata, such as a function call identifier, and the body of function call. The rest of this section defines the semantics of CML."}, {"title": "3.1 Initiating Function Calls", "content": "In AsyncLM, the LLM initiates function calls using the following format: [CALL] function call [END]. The function call can be written in any valid executable language, such as Python code or a JSON abstract syntax tree (AST), supported by the executor. However, only one language should be used consistently within each function call block. Independent function calls should be placed in separate blocks to allow for parallel execution.\nThe generated function calls initiate execution without blocking the token stream, enabling implicit parallelism. For example, as shown in Figure 3, if the LLM generates two function calls (search_nearby and put), the executor processes each in a separate worker, allowing pipelined generation and execution of function calls. This overlap reduces latency, as the execution of search_nearby can occur simultaneously with the generation of writing text messages. Function calls are generated according to their dependency order, ensuring that dependent calls are executed in the correct sequence. For instance, the LLM can generate a function call, text, only after searching for florists and composing the message are done."}, {"title": "Assigning identifiers for interrupts.", "content": "If the LLM will need to refer to the function result for subsequent calls or reasoning, it can include an identifier in the function header (e.g., [CALL] job1 (HEAD) function call [END]), where job1 acts as a unique identifier. To avoid conflicts, the LLM generates identifiers following Python variable naming conventions. Identifiers must remain unique throughout the session. Although our prototype does not implement this, a uniqueness check could be included as part of syntax validation (\u00a75.1)."}, {"title": "3.2 Trigerring Interrupts", "content": "When the executor completes a function call with a registered identifier, it asynchronously notifies the LLM by inserting an interrupt block at the end of the token stream (i.e., the LLM context). Token generation resumes after the interrupt block is added. The format of an interrupt block is: [INTR] id (HEAD) value [END], where id matches the identifier from the corresponding [CALL] block (e.g., job1 from the earlier example). The value contains the executor's result, such as the function's output or an error message.\nCritical sections. Since interrupts modify the LLM's token generation flow, it is essential to ensure that the context following an interrupt conforms to CML syntax. For example, if an interrupt block is inserted during the generation of a function call, the resulting tokens could become logically incorrect, violating CML syntax. To prevent this, AsyncLM temporarily disables interrupts during specific token generation periods, inspired by how operating systems defer lower-priority interrupts while handling others. A flag called critical section, implemented within the interrupt manager (\u00a75.3), determines whether the executor can insert interrupts. This flag is set to false while a function call block is being generated and resets to true once the LLM exits the block. Any interrupts that occur during a critical section are queued and inserted when the flag is set to true.\nWaiting for interrupts. In synchronous function calling, the LLM stops generating tokens after making a function call, waiting for the call to complete. In contrast, asynchronous function calling lets the LLM continue generating tokens for other tasks without waiting. However, the LLM must sometimes pause to wait for function results when inter-task dependencies exist. The standard [EOS] token is not sufficient for this purpose because it cannot indicate whether token generation is complete and resources can be released, or if the LLM is temporarily pausing until the function result arrives.\nTo resolve this ambiguity, AsyncLM introduces self-initiated interrupts, called traps, which notify the LLM serving system (\u00a75.4) when to pause token generation. Each trap follows the simple structure [TRAP] [END]. Traps create explicit boundaries for asynchronous function calls, helping generate training samples to fine-tune LLMs for asynchronous function handling (\u00a74). They also enable optimization opportunities for the serving system (\u00a75)."}, {"title": "4 LEARNING TO HANDLE INTERRUPTS", "content": "We propose a fine-tuning scheme to train LLMs to (i) generate asynchronous function calls and traps using CML and (ii) handle interrupts that deliver the results of previous function calls. The core idea is to construct a dataset with simulated function calls and interrupts that model ideal interactions between the LLM and the executor. We extract task descriptions and function definitions from publicly available function-calling datasets, add estimated completion times, and present them to the LLM in JSON format as part of the prompt (OpenAI, 2023; Patil et al., 2024; Liang et al., 2024). For simplicity, we assume that each task can be completed with a finite number of function calls."}, {"title": "4.1 Training objectives", "content": "The primary objective of fine-tuning in AsyncLM is to train the LLM to minimize the total task completion time (i.e., the makespan) by effectively utilizing asynchronous function calling, while respecting task dependencies and considering estimated function execution times. Specifically, the LLM must make decisions in the following areas.\nDeciding the next function call. The LLM selects the next function to call based on the context. It identifies which functions are available to be called immediately-those without any pending dependencies\u2014and chooses the most suitable one. AsyncLM uses a Longest-Processing-Time-first (LPT) strategy, where the LLM prioritizes calling the function with the longest estimated execution time among those that are ready. This approach reduces idle time by maximizing the overlap between function call generation and execution, making it particularly effective when multiple independent functions can be called. LPT is optimal for scenarios with parallel function calling (\u00a76).\nHandling interrupts. When an interrupt appears in the context, the LLM must decide whether to continue generating tokens for the current task or shift to addressing the interrupted task. For example, the LLM might ignore the interrupt if the completed task has lower priority than the current task or if it was the final task in a sequence. AsyncLM handles interrupts using the same priority principles as for deciding the next function call. An interrupt may introduce new functions that are ready to be called; if a newly available function has the longest estimated processing time among the options, the LLM is trained to call it next. For instance, in the task scheduling scenario shown in Figure 2, when \"Read html\u201d and \u201cRead xls\" complete, the LLM chooses to do \"Summarize & save pdf\" first over \"Fetch contact,\" prioritizing the function with the longer"}, {"title": "4.2 Generating Training Samples", "content": "To create training samples, we simulate ideal interactions between the LLM and executor using existing function-calling traces from LLM benchmarks with multi-turn scenarios (Lu et al., 2024; Yan et al., 2024; Yao et al., 2024).\nDAG generation. We developed a Python program to extract directed acyclic graphs (DAGs) of function calls from each sample in the dataset. In sequential scenarios, the DAG is linear, with nodes representing function calls and edges indicating dependencies. In parallel scenarios, the DAG branches to represent independent function calls. Multi-turn scenarios consist of multiple DAGs.\nSimulating interrupts. We simulate interactions between the LLM and the executor on these DAGs using the LPT strategy. In each simulation run, we assign random estimated execution times to function calls, ranging from 1 ms to 1 s, to prevent the model from overfitting to specific function names. These estimates are provided to the LLM as part of the input prompt. To determine interrupt timing, we randomly set a time-per-output-token (TPOT) between 5 ms and 30 ms per simulation and track elapsed time by counting generated tokens. When a function call completes, we insert an interrupt block containing the execution result. If all functions in the DAG are waiting on dependencies, we insert a trap block and inject the next interrupt."}, {"title": "5 IMPLEMENTATION", "content": "We implement AsyncLM by intercepting the LLM's autoregressive token generation, as illustrated in Figure 4. The system is built using the Python transformers library (Wolf et al., 2020). AsyncLM consists of four main components: a token monitor, an executor, an interrupt manager, and a trap handler. In this section, we describe the implementation of each component and the modifications made to the LLM inference. While some aspects of these components can be implemented on cloud-based LLM APIs without modifying the serving system (\u00a75.5), such implementations are practical only in certain scenarios (\u00a76.2)."}, {"title": "5.1 Token Monitor", "content": "The token monitor audits and regulates the LLM's token generation process. Its primary functions are (i) to notify the executor or trap handler immediately when a function"}, {"title": "5.2 Executor", "content": "The executor manages the execution of function calls generated by the LLM. It receives each function call from the token monitor (formatted in Python syntax in our prototype) along with an optional identifier used to manage future interrupts. Each function call runs on a dedicated worker, allowing multiple calls to execute concurrently if resources permit. These workers interact directly with external systems, such as API servers or code interpreters. Once a function completes, the executor sends the result and identifier to the interrupt manager (\u00a75.3)."}, {"title": "5.3 Interrupt Manager", "content": "The interrupt manager has three main functions: (i) managing an interrupt queue, (ii) tracking when the token generation process is interruptible based on a critical section flag, and (iii) inserting CML-formatted interrupt blocks into the token stream. When the executor completes a function call, it adds the result to the interrupt queue. During each decoding step, the interrupt manager receives newly generated tokens and the critical section flag from the token monitor (\u00a75.1). If the process is interruptible, i.e., the critical section flag is not set, it formats all queued interrupts in CML, tokenizes them, and appends them to the generated tokens. These tokens will then be processed by the LLM in the next decoding step."}, {"title": "5.4 Trap Handler", "content": "The trap handler's goal is to minimize idle KV cache usage in GPU memory without adding latency to task completion. When the token monitor detects a trap-indicating that token generation needs to be paused-it notifies the trap handler with the current context. Based on this notification, the trap handler determines the best strategy for managing the KV cache during the pause by considering the number of tokens in the current context and the estimated time until the next interrupt completes. Prior work has shown that recomputing the KV cache scales quadratically with the number of tokens, while swapping it to host memory scales linearly (Abhyankar et al., 2024; Gim et al., 2024). Given the scaling characteristics of these costs, the trap handler keeps the KV cache in GPU memory if both recompute and swap times exceed the estimated wait time. Otherwise, it opts to recompute if recompute latency is lower, or to swap if swap latency is lower. We provide a real-world example of an ideal trap handling strategy in \u00a76.2."}, {"title": "5.5 Implementation on Chat Completion APIs", "content": "To demonstrate AsyncLM 's adaptability, we also implement it using OpenAI's (streaming) chat completion API without modifying the serving system. The executor implementation can be reused as is. For the token monitor, we implement only the CML parser without constrained decoding, as the API already streams sampled tokens. To emulate the interrupt insertion mechanism in the interrupt manager, we start a new request to the API server whenever an interrupt is triggered, discarding the previous session. The trap handler is unnecessary for cloud APIs, which are stateless and recompute the entire KV cache from scratch for each new session when an interrupt is inserted. We note that this implementation is practical only when the time-to-first-token (TTFT) latency is low, which is generally not the case for most cloud services (details in \u00a76.2)."}, {"title": "6 EVALUATION", "content": "Our evaluation answers two questions: (i) latency (\u00a76.1-\u00a76.4), i,e., how much does asynchronous function calling reduce task completion latency compared to synchronous methods, and (ii) correctness (\u00a76.5), i.e., how does the asynchronous mechanism affect the correctness of generated function calls.\nWorkloads. To evaluate task completion latency and function calling accuracy, we use the Berkeley function calling leaderboard (BFCL) (Yan et al., 2024), which captures real-world function calls across eight domains, such as vehicle control, travel booking, file system operations, and the Twitter API. BFCL includes 84 unique functions. We utilize three datasets from BFCL to cover different function calling scenarios: v1-parallel, v2-parallel-live, and v3-base-multi-turn. Specifically, v1-parallel and v2-parallel-live provide 400 parallel function calling scenarios, while v3-base-multi-turn offers 200 multi-step function calling scenarios. To simulate more complex multi-step parallel function calling, we created a new dataset v3-multi-step-parallel. This dataset consists of 200 scenarios formed by randomly combining three distinct multi-step samples from the first round in v3-base-multi-turn. From the total of 800 samples, we use 200 for fine-tuning and the remaining 600 for evaluation. The measured execution time of each function ranges from 30 ms to 500 ms, with an average of 110 ms.\nAsyncLM setup. We consider two LLM deployment settings: local and cloud. In the local deployment, LLM inference and function execution run on the same machine equipped with an NVIDIA RTX 4090 GPU. This deployment uses Llama-3.2 models (Dubey et al., 2024) with 3B and 1B parameters. We fine-tune the Llama models following default configurations in LlamaFactory (Zheng et al., 2024). Local models are served with Text-Generation-Interface (Hugging-Face, 2023). In the cloud deployment, only function execution is local. This deployment uses OpenAI's GPT-40 and GPT-40-mini. We adapt them using few-shot prompting; we select one example from each dataset and provide detailed instructions on their interpretation.\nBaselines. To compare against synchronous LLM function calling, we employ two synchronous baselines:\n\u2022 Sync: Sequential function calling where LLM code gener"}, {"title": "6.1 Parallel Function Calling", "content": "First we employ a simplistic setup where no function calls depend on each other and report results in Figure 5. Parallel function calling is common (OpenAI, 2023). For example, when a user asks \"Which city has a higher chance of rain tomorrow, Seattle or Vancouver?\", the LLM can generate two function calls: one for weather data in Seattle and another for Vancouver, which can be executed in parallel. We use v1-parallel and v2-parallel-live from BFCL for this evaluation.\nResults. Async improves latency by up to 2.1\u00d7 over Sync. We measure the end-to-end task completion latency as the time between the first and last token generation. Our results show that Async completes tasks faster than Sync by 1.6\u00d7 for the local deployment and 2.1\u00d7 for cloud. In comparison, Sync-Parallel is 1.3\u00d7 faster than Sync for local and 1.7\u00d7 faster for cloud. Although Async-Naive is slower than Async, it is still 1.2x faster than Sync-Parallel for local."}, {"title": "6.2 Multi-Step Parallel Function Calling", "content": "For evaluations on more complex setup under function call dependencies, we evaluate AsyncLM using the v3-multi-step-parallel dataset, which consists of three independent tasks requiring 1-5 sequential function calls each. The results are presented in Figure 6. In these function calling scenarios, AsyncLM must respect task order dependencies and manage interrupt identifiers for each function call. For example, the task \u201cmake pasta\" can involve two independent sequences: (i) boil_water() followed by put_pasta_noodles, and (ii) chop_vegetables() followed by stir-fry(), and culminating in mix_everything().\nResults. AsyncLM reduces latency by up to 5.4\u00d7 over Sync by parallelizing function calls in independent sequences; while Sync-Parallel reduces latency by 3.2\u00d7 compared to Sync. As illustrated in Figure 2, unlike in Sync-Parallel, where the LLM needs to wait for all bundled functions to complete, Async uses the LPT strategy to schedule individual function calls. This enables the LLM to optimize token generation cycles more effectively, similar to out-of-order execution in CPU instruction scheduling.\""}, {"title": "6.3 Latency Analysis", "content": "To understand how asynchronous function calling reduces latency, we conduct theoretical analyses under parallel function calling (\u00a76.1). All proofs are available in Appendix A.\nOverlapping generation and execution. Asynchronous function calling is at least as fast as synchronous function calling. The total latency for Sync in simple parallel function calling can be modeled as:\n\\(L_{Sync}(F) = \\sum_{f\\in F}G(f) + \\sum_{f\\in F}E(f)\\),\ngiven F as a set of functions that do not depend on each other for execution, where G(f) is the token generation latency for \\(f \\in F\\), and E(f) is the execution time. For Sync-Parallel, the total latency is:\n\\(L_{Sync-Parallel}(F) = \\sum_{f\\in F}G(f) + max_{f\\in F} E(f)\\),\nassuming negligible overhead for parallelizing them. For Async, using the LPT heuristic, the total latency is:\n\\(L_{Async}(F) = max_{f\\in F} (E(f) + \\sum_{g \\in pred(f, F)} G(g)),\\)\nwhere \\(pred(f, F) = \\{g \\in F \\vert E(f) \\leq E(g)\\}\\). Intuitively, this formation indicates that each function call f initiates after generating all function calls that has the same or higher priorities, i.e., sum of generation times in pred(f, F). We prove that Async is at least as fast as Sync-Parallel in the worst case, assuming the number of generated tokens is the same and there are no additional overheads in Sync-Parallel or Async beyond G(f)s and E(f)s.\nTheorem 6.1 For any set of independent functions F, \\(L_{Async}(F) \\leq L_{Sync-Parallel}(F) < L_{Sync}(F)\\).\nCharacterizing speedups with average function execution times. Assuming that the E(\u00b7) follows a normal distribution, we estimate the expected speedup of Async over Sync using E and G, where E represents the average execution time, and G denotes the average generation time of functions in F.\nTheorem 6.2 The ratio of speedup, \\(\\frac{L_{Sync}}{L_{Async}}\\) is approximately \\(1 + \\frac{E}{nG}\\) with an error of \\(O((\\frac{E}{nG})^2)\\) when |F| is large.\nThis result indicates that tasks involving long average execution times (e.g., expensive I/Os) leads to a better speedup, whereas tasks with short execution times (e.g., simple arithmetic operations) may not benefit as much from AsyncLM.\nRationale on using LPT heuristic. LPT is optimal for parallel function calling. We prove that the LPT heuristic minimizes total latency in asynchronous parallel function calling. Intuitively, starting the execution of longer functions earlier maximizes overlap with token generation, reducing the overall makespan. To put this argument formally,\nTheorem 6.3 Continuing from Theorem 6.1, Any deviation from the LPT order cannot result in a lower total latency."}, {"title": "6.4 System Overheads", "content": "Inference overhead of AsyncLM. AsyncLM introduces two potential sources of LLM inference overhead compared to Sync: (i) syntactic overhead from using CML and function call identifiers, and (ii) overhead from interrupts, caused"}, {"title": "6.5 Function Calling Accuracy", "content": "To assess how AsyncLM affects LLMs' ability to generate accurate function calls, i.e., correct signature, arguments, and calling order, we examine function call traces of each baseline using multi-step parallel dataset. We ensure all necessary function definitions are provided in the prompt. Function calling accuracy is measured by exact AST matching for each function call against the ground truth call, and their execution order.\nWe compare two LLM adaptation strategies: fine-tuning and few-shot prompting. Llama models are adapted using both methods, while GPT-40 is adapted with few-shot prompting only. To understand the effect of CML syntax, we also test a scenario where we format function calls in Sync in CML. Table 1 presents the results for GPT-40 and Llama models under these adaptation strategies.\nImpact of CML on accuracy. Formatting Sync responses using CML syntax has minimal effect on accuracy. In GPT-40, accuracy remains virtually unchanged, while Llama-3B shows a slight improvement. Fine-tuned models exhibit a modest increase in accuracy when using CML, likely because the fine-tuning samples were formatted with CML.\nImpact of Asynchronous Function Calling on Accuracy. Both GPT-40 (with few-shot prompting) and fine-tuned"}, {"title": "6.6 Discussion \u2013 Human-triggered Interrupts", "content": "The interrupt mechanism proposed in AsyncLM not only allows asynchronous function calling, but also enables new types of human-LLM and LLM-LLM interactions. We first discuss flexibility of this mechanism, then illustrate its potential with real-world examples.\nGenerality and flexibility of interrupt. AsyncLM extends interrupt functionality beyond executor notifications, allowing interrupts to signal external events, such as user inputs or system triggers. This capability enables real-time interactions, where users can interrupt an ongoing LLM inference to add or adjust tasks without waiting for the current task to finish. Although not detailed in this paper, we incorporated user-triggered interrupts into our fine-tuning dataset by randomly injecting new tasks from multi-turn conversation datasets. While this dataset may not perfectly represent real human-LLM interactions, we explored its effectiveness in the following scenarios.\nInterruptible AI assistants. LLM chatbots currently operate synchronously, requiring users to wait for complete responses. This approach increases perceived latency and limits real-time task handling. With AsyncLM, users can issue new requests immediately, even during ongoing LLM responses. For instance, users often make follow-up requests without waiting for the initial response. A user might first ask to \"find a hotel in Seattle\" and then quickly add \"near Space Needle.\" Similarly, a user can correct a previous request, such as saying \u201cActually, make it Thursday\" after initially scheduling a meeting for Wednesday.\nTo examine this scenario, we repeated the multi-step parallel function calling experiment, presenting tasks as a series of user-triggered interrupts rather than prompts. We inserted these interrupts every 200 ms and measured the end-to-end task completion latency of Async, as shown in Figure 8. For comparison, we ran Sync and Sync-Parallel with the same tasks, each processing them sequentially. Async reduced latency by 2.4\u00d7 compared to Sync, whereas Sync-Parallel achieved only a 1.1\u00d7 reduction. The limited improvement of Sync-Parallel is because it requires generating the entire function calls at once, which is unsuitable for this scenario.\nMulti-communicating LLM agents. AsyncLM also enables multiple autonomous LLM agents to communicate simultaneously. Typically, agent-to-agent communication uses synchronous message exchanges (Wu et al., 2023; Chan et al., 2024), where agents take turns. This limits their ability to multitask and confines communication to a round-robin style. With AsyncLM, one-to-many or many-to-many agent communication can be implemented using interrupt mechanisms. Each agent can send messages to others as interrupts, enabling more dynamic interactions, which could enable more realistic LLM-based social simulations (Zhou et al., 2024; Park et al., 2023). This new communication pattern may introduce more complexity and require LLMs to learn when to chime in just like human conversation."}, {"title": "7 CONCLUSION", "content": "AsyncLM is a system that enables asynchronous LLM function calling by allowing the LLM and the function executor to operate independently. Our core innovation is making LLM inference interruptible through (1) CML, an in-context interface that facilitates asynchronous interaction, (2) adaptation of LLMs to leverage asynchronous semantics for optimized task completion latency, and (3) efficient implementation of an interrupt mechanism on the LLM inference pipeline. Empirical evaluations on the BFCL benchmark demonstrate that AsyncLM reduces latency by 1.6x-5.4x compared to synchronous methods. AsyncLM opens new possibilities for improving the operational efficiency of LLMs that interact with external tools, data sources, humans, and other LLMs."}]}