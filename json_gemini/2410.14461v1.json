{"title": "The Propensity for Density in Feed-forward Models", "authors": ["Nandi Schoots", "Alex Jackson", "Ali Kholmovaia", "Peter McBurney", "Murray Shanahan"], "abstract": "Does the process of training a neural network to solve a task tend to use all of the available weights even when the task could be solved with fewer weights? To address this question we study the effects of pruning fully connected, convolutional and residual models while varying their widths. We find that the proportion of weights that can be pruned without degrading performance is largely invariant to model size. Increasing the width of a model has little effect on the density of the pruned model relative to the increase in absolute size of the pruned network. In particular, we find substantial prunability across a large range of model sizes, where our biggest model is 50 times as wide as our smallest model. We explore three hypotheses that could explain these findings.", "sections": [{"title": "Introduction", "content": "Large, dense neural networks are capable of impressive performance [28]. However, there is both a high computational cost associated with deploying such networks and a lack of transparency about their behaviour [19]. Accordingly, sparsity has become a target for a variety of researchers aiming to reduce cost and increase the interpretability of deep learning systems.\nAccording to Parkinson's law the time taken to complete a bureaucratic job expands to fill the time available to do it. In this paper, we ask whether an analogous law holds for neural network capacity. Does a trained network tend to use all of the parameters available to solve a task even if the task could be solved with fewer parameters?\nWe investigate how many weights of a trained network f(x; 0) can be set to zero before the model substantially loses performance. In particular, we prune away the lowest magnitude weights until the model's accuracy drops by 5% (without retraining the remaining subnetwork). The resulting subnetwork is what we consider the 'core' model with parameters w and we call the proportion of remaining weights the effective density of the model.\nGiven an architecture, training regime and task, our null hypothesis is that the 'core' model size |w| does not change with the network width. That is, the null hypothesis says, for an effective density $\\frac{|w|}{|\\theta|}$ the denominator |0| is the only quantity that changes.\nWe evaluate whether models tend towards density or sparsity by varying the width of a model (between 0.1x and 5x the default width) and assessing how the effective densities change as the model size increases. Note that the high performing subnetworks for a model architecture of size 0.1x are structurally also subnetworks of size 5x (because the 5x models are simply wider). In other words, the solutions for a model architecture of a given size are also valid solutions for wider models that use the same architecture.\nWe find that as we increase the width, the 'core' model size w vastly increases. This contradicts our null hypothesis, which says that the 'core' model size remains constant. Accordingly, we find that the effective densities for different model sizes are much more similar than they would be if the null hypothesis were true. Specifically, we find no 0.1x solutions when we train a 5x model, implying that certain solutions are favoured when training an architecture of a particular width.\nTo investigate this phenomenon, we explore three independent explanations: first, that the initialisation locks in a particular density; second, that the models implement substantially different functions; and third, that wider models use the abundance of parameters at their disposal to train more monosemantic units. We find that the first and last explanation may play a role for convolutional models trained with Adam, while the second may be relevant for fully connected models trained with SGD.\nOur contributions are the following:\n\u2022 In Section 4 we reject the null hypothesis that the 'core' model size does not change with the network width for the models we investigate, and report on the pruning trajectories and effective densities.\n\u2022 In Section 5 we discuss and evaluate three potential explanations for this finding, observing varying results across different architectures and optimizers."}, {"title": "Background and Related Work", "content": "Pruning and Distillation. The work most closely related to ours investigates the robustness of networks to random ablation of neurons by randomly pruning them, rather than performing magnitude-based weight pruning as we do. Casper et al. [2] vary the width of a network between 0.25x and 4x the default width, train the network, and apply a randomly generated pruning mask: they report what percentage of test labels do not change for different mask sizes. The results show the effective density decreases as the number of units increases. We expand on this line of work by: 1) investigating how the proportion of crucial parameters, as opposed to neurons, changes as the model increases in width; and 2) pruning parameters with a heuristic (magnitude pruning) as opposed to randomly ablating.\nIn their investigation of the effect of pruning on downstream tasks, Jaiswal et al. [13] tangentially show that, for different sizes of OPT models [31] (125m, 350m, 1.3B), the effective densities of the models are consistent with our findings. In contrast, we test a specific hypothesis relating density and width (as opposed to size); and also conjecture and test possible explanations for this relationship.\nOur aims sit in contrast to the typical focus of neural network pruning [1]: retaining model capabilities with a smaller compute budget. Specifically, instead of making networks sparser to, for instance, reduce the storage footprint of the network or the computational cost associated with inference [e.g. 12], we prune with the intention of uncovering the model's 'true' size or effective density.\nThe field of knowledge distillation [9] aims to compress a large teacher model into a small student model by training the student model to mimic the behavior of the teacher model. The method of compressing a teacher model into an ever smaller student model can be used to approximate the function complexity of the teacher model, but can not capture our focus in this paper: the 'size' of the implementation of the teacher model.\nImplicit Inductive Bias. Given a dataset and network architecture, there is a hypothesis space of models that can be considered by a learning algorithm when it is tasked with fitting this data. Any training regime inherently has implicit inductive biases that make it more likely one model is selected from the hypothesis space than another. We call a hypothesis that is selected by a training process a solution.\nThe parameter-mapping function [25] and initialization scheme [e.g. 10] are important forms of inductive bias in deep learning. Examples of recent insights about implicit regularization include the uncovering of grokking [26] and double descent [22]. Based on our findings we hypothesize that models have an implicit inductive bias towards using up a large portion of the available parameters.\nStrong Lottery Ticket Hypothesis. The Strong Lottery Ticket Hypothesis states that: 'Within a sufficiently overparameterized neural network with random weights (e.g. at initialization), there exists a subnetwork that achieves competitive accuracy' [27]. This conjecture has been proven under certain assumptions [20].\nRamanujan et al. [27] empirically find that subnetworks of randomly initialized Conv-2 networks can solve CIFAR-10 to a decent accuracy level. They introduce an algorithm they call 'edge-popup' to find well-performing subnetworks. The best subnetworks they find have a size of around 50% of the total network parameters, but they find well-performing subnetworks in the region of 30% to 70%. They hypothesize this is because there are more subnetworks of size 50% than of any other size, because of combinatorics. They use five random seeds and report on the mean and standard deviation. Note that their algorithm does not employ a brute-force method to search for subnetworks; rather, it utilizes a sophisticated approach for learning an effective mask, which may have its own inductive biases.\nSuppose that instead of training a model (which can involve an inductive bias) we randomly initialized a network and sampled subnetworks until we found a subnetwork with low loss. What would then be the distribution of 'winning' subnetwork sizes? The result in Ramanujan et al. [27] suggests that without any inductive bias (from a learning algorithm) and by just brute-force sampling, we may find that the winning networks sizes would be distributed around 50%.\nUnfortunately, it is not possible to conclude this since only the best subnetworks are reported on (existence) rather than the frequency by which they find good subnetworks of a given size (abundance).\nSummarizing, the above suggests that when we increase the width of a network, the 'core' model size may increase proportionally, which is opposite to our null hypothesis."}, {"title": "Experimental Setting", "content": "For the results presented in this work, we train a variety of small models for classification tasks [15], after which we prune them in a stepwise manner and record the pruned model's performance. Note that, in contrast to the literature [7], during magnitude pruning we do not train or finetune in between pruning steps. This is because we want to capture the 'size' of the implementation that was learned as opposed to e.g. finding the smallest possible network that can implement the same function."}, {"title": "Optimization Problem", "content": "We explore fully connected, convolutional and residual network architectures, and use three optimizers and two initialization schemes across our experiments. As discussed, we also vary the width of these architectures from their default values using a scaling factor.\nDatasets. We use the MNIST handwriting recognition dataset [4] and CIFAR-10 image dataset [16] as the tasks to be optimized. MNIST and CIFAR-10 respectively contain 70,000 and 60,000 labelled images that includes a test set of 10,000 samples each. For both datasets, we randomly partition a 5,000 image validation set from the remaining training set.\nDefault Architectures. We train fully-connected dense networks, as well as convolutional and residual networks. Below, we describe the default architectures we use. The scaling factor is applied to the widths mentioned in these default architectures.\n\u2022 The fully-connected dense networks are based on a LeNet-300-100 architecture [18] and consist of two dense layers with 300 and 100 units respectively (with ReLU activations). We refer to these networks as multi-layer perceptrons (MLPs).\n\u2022 The convolutional networks are based on a variation [7] of the VGG architecture [29] referred to as Conv-2. These networks consist of two convolutional layers with 64 filters, a max-pooling layer, followed by two dense layers with 256 units and ReLU activations.\n\u2022 The residual networks are based on the ResNet-18 architecture used in [7]. The first layer is a convolutional layer, followed by a batch norm layer and ReLU activation; there are then three residual blocks with two convolutional layers (again each followed by a batch norm layer and ReLU activations) and an average pool layer; finally, a dense layer projects the flattened output into 10 classes. By default, all convolutional layers have 16 filters.\nThe above default widths are taken from the literature but ultimately serve as an arbitrary reference point to compare the effect of scaling the layer widths."}, {"title": "Pruning Trajectory and Effective Density", "content": "After training, we use layer-wise magnitude pruning to obtain a pruning trajectory. This process is as follows:\n1. For each layer, prune the smallest 2% of weights by magnitude.\n2. Evaluate the pruned subnetwork with a validation set.\n3. Repeat until all weights have been pruned.\nRemark that this is not iterative magnitude pruning as we do not rewind the model to initialization, or train the model between pruning steps.\nGiven a threshold representing the acceptable decrease (or increase) in accuracy (or loss), we calculate the effective density of a model as the percentage of weights remaining at the first pruning cycle for which the model degrades beyond the acceptable threshold. In our visualizations we used a threshold of a 5% decrease in validation accuracy."}, {"title": "Experimental Results", "content": "In this section we will first show the pruning trajectories, effective densities and absolute prunability of different models. Based on the effective densities we reject the null hypothesis. We discuss the effect from different optimizers and architectures. Lastly, we investigate the effect of initalizing with Glorot versus He."}, {"title": "Pruning Trajectories and Effective Densities", "content": "In Figure 3, we plot the validation accuracy during the entire pruning trajectory for different model widths.\nThe effective density distribution is plotted in Figure 4 using the Kernel Density Estimate (KDE) function to estimate the probability distribution (frequency) of the effective densities. Apart from for the 0.1x models we find a fair amount of overlap between the effective densities for different model sizes. This suggests that training regimes may favour a certain amount of effective density.\nIn Figure 5 we show how the absolute subnetwork size grows as the model architecture grows. We find that in absolute terms the pruned models with the biggest architecture size are bigger than unpruned smaller architectures. Specifically, in absolute terms the pruned 5x MLP models are bigger than the unpruned smaller architectures. Similarly, the pruned 2x Conv-2 and Resnet models are bigger than the unpruned smaller models of those architectures.\nThe smallest networks we train have approximately 24,000 parameters, which is more than two times fewer than the number of data points. Even for these heavily underparametrized networks, we find that after training them to a loss of 0.2 we could prune around 30% of the weights without any substantial impact on performance.\nFor overparametrized networks, increasing the width of the layers did not result in substantially more sparsity. This means that regardless of the total number of initialized parameters, we could prune around 50% or more of the weights without causing any substantial harm to the model's performance."}, {"title": "Rejecting the Null Hypothesis", "content": "We calculate the mean effective density for a specific model width by taking the mean of the effective densities of all the models of that width that we trained. In Table 2 we show the mean and standard deviation of the observed effective densities for different model widths. For example, for MLP models, we find mean effective densities of 50.7% and 32.2% for model widths of 0.5x and 5x respectively.\nFor each model type (MLP, Conv with Adam, Conv with Adagrad and Resnet) we perform a one-way ANOVA test to verify whether the mean pruned model sizes of different network widths are statistically different. We find that for each of the four model types the p-value is less than 0.0001. On this basis we reject the null hypothesis that the 'core' model size does not change when we vary the network width.\nIn Table 2 we also calculate the mean and standard deviation of the effective densities that we would expect to see if the null hypothesis were true. According to the null hypothesis, for the same architecture, training regime and task, the 'core' model size should be the same when we decrease or increase the model width. Given an architecture, training regime and task, we match the data for every combination of initialization seed and data seed of a wider or more narrow model with the data for a 1x model with the same combination of hyper-parameters. We then calculate the effective density that we would have seen if the null hypothesis were true as follows. For a given narrow or wide model we look up what absolute number of weights were unpruned in its 1x counterpart. We then calculate what proportion that number of weights is of the narrow or wide architecture. Whenever the number of unpruned weights in the 1x counterpart is larger than the narrow architecture size, we say the proportion is 100% (even if the proportion is technically bigger than 100%).\nFor each row in Table 2 (other than the 1x rows) we do an unpaired t-test and for each row we find that the two-tailed p-value is less than 0.0001, which means that our findings are statistically significant."}, {"title": "Effect from Different Optimizers and Architectures", "content": "We find that although our MLPs and Resnets are both trained with SGD the pruning curves (Figure 3) come apart for the MLPs of different sizes, whereas the pruning curves for the Resnets are fairly overlapping. One thing to note is that multiplying the Resnet layer widths by a factor roughly leads to a parameter increase of that same factor, whereas the effect on parameter number for MLPs is larger.\nWe trained the same convolutional model architectures with either Adam or Adagrad. We trained the models until they reached a validation loss minimum. However, for Adagrad this resulted in all models having similar validation loss at the start of pruning, whereas for Adam, halting training when models reach minimum validation loss did not result in models with the same pre-pruning accuracy. This difference in training dynamic may help explain why the Adagrad pruning curves overlap more."}, {"title": "Glorot Leads to Lower Densities than He", "content": "Glorot (also known as Xavier) initialization involves sampling numbers from the uniform probability distribution over the interval [-a, a] where $a = \\sqrt{\\frac{6}{n_{in}+n_{out}}}$, with $n_{in}$ being the number of units in the previous layer (input units) and $n_{out}$ being the number of units in the next layer (output units). He (also known as Kaiming) initializes weights by drawing numbers from the uniform probability distribution over the interval [-b, b] where $b = \\sqrt{\\frac{6}{n_{in}}}$.\nFor the fully connected and convolutional models and every combination of hyper-parameters we trained one model using Glorot and one model using He. In Figure 6 we show the distribution of effective densities that we found for Glorot and He initialization schemes and different model widths.\nAs the width of the layers increases, the difference in effective density between Glorot and He initializations also increases. Networks initialized with the He scheme require fewer epochs to converge and have higher effective density.\nHowever, the difference in effective density is relatively small between the two initialization schemes, which suggests that the cause for propensity to density does not lie in initialization."}, {"title": "Investigating the Mechanism behind the Implicit Inductive Bias towards Density", "content": "In this section we explore three hypotheses for our finding that wide and narrow models have similar effective density:\n1. The weights that are small at initialization are the same weights that are small and unimportant after training. Since the proportion of small weights at initialization remains constant (regardless of model width) we see the same effective densities;\n2. The functions that wider models converge on are qualitatively different (and need more parameters to implement);\n3. Given a larger architecture the training regime makes use of this architecture by separating tasks out more."}, {"title": "Comparing Initialization and Trained Weight Magnitudes", "content": "We hypothesize that a similar effective density between wide and narrow models is due to wide and narrow models having the same proportion of small weights at initialization, and due to these weights remaining prunable after training.\nWe looked at the connections that had the lowest magnitude weights at initialization as well as the connections that had the lowest magnitude weights after training. More specifically, we looked at the overlap between the smallest 40% weights at initialization and the smallest 40% after training. By random chance we would expect the overlap between two random masks of 40% to be 16%. This is because under the assumption of independence, we can multiply the probabilities as follows 0.4 \u00b7 0.4 = 0.16.\nFor MLPs in Table 3 we find that the overlap between small initialized weights and small trained weights is larger than chance for model width 0.1x. Surprisingly, we find that the overlap is smaller than chance for the first two layers of the overparameterized models.\nIn contrast, for convolutional models trained with Adam in Table 4 we see more overlap than we would expect by chance for all layers. Based on these findings we provisionally conclude that for MLP models our main finding cannot be explained by the initialization. However, for convolutional models trained with Adam, initialization may play a role in the models' propensity for density."}, {"title": "Functional Similarity Between Models of Different Widths", "content": "If wider models were qualitatively different from narrow models, then this could explain why the implementations of those functions are larger, i.e. why in absolute terms wider models use so many unprunable parameters. In this section we investigate whether wider models are qualitatively different."}, {"title": "Activation Sparsity and Selectivity", "content": "Park [24] study the effects of layer width on neuron activations. In particular, they study the Hoyer sparsity [11] of activations as well as the class selectivity, also referred to as CCMAS (class-conditional mean activity selectivity) [21]. The Hoyer sparsity is calculated based on the ratio of the L1 norm (sum of absolute values) to the L2 norm (square root of the sum of squares) of a vector. The class selectivity of a unit is defined as the difference between the mean activation of the class with the largest mean activation, and the average mean of all other classes.\nThe authors find that activation sparsity and class selectivity for an MLP with a single hidden layer trained on MNIST, are highest when the model is trained with Adagrad, followed by Adam, and finally SGD. They find that increasing the width from roughly 300 units to 600 units (2x) increases Hoyer sparsity and class selectivity for both Adagrad and Adam and has a mixed effect on SGD.\nWe relate this to our findings by noticing that an increase in width leads to both more parameters being used in absolute terms (as we saw in Section 4) and generally leads to more activation sparsity and class selectivity (as shown by Park [24]). However, for SGD specifically, an increase in width does not necessarily lead to more activation sparsity and class selectivity.\nWe conclude that more work is needed to confirm or reject the hypothesis that the implicit inductive bias towards density that we find can be explained via a decrease in polysemanticity and superposition [23, 6] as a network increases in size. We propose two investigations for future work: 1) an investigation into overlap between circuits found via ACDC [3] for each class; and 2) an investigation into unit polysemanticity based on a more general interpretation of features than class."}, {"title": "Discussion", "content": "In summary, our main results demonstrate that for our feed-forward models and tasks, as the model architecture increases in size, the absolute number of unprunable parameters increases, but the proportion of unprunable parameters is relatively stable.\nAdditionally, we explored three hypotheses for why these results hold and provided preliminary evidence in favour and against them. Notably we found different results for MLP models trained with SGD and Conv models trained with Adam.\nLimitations and Future Work. Further investigations will be required to verify whether our results generalize to other architectures and tasks.\nOne limitation of our approach to calculating effective density is that we find a subnetwork by magnitude-based pruning. However, this method is unlikely to find the optimal subnetwork of a given size since some small weights may be crucial whereas some bigger weights may be unimportant. We propose to use the edge-popup algorithm [27] to find the optimal subnetwork of a trained network.\nAnother approach to calculating effective density might be to use a measure of effective dimensionality as used in singular learning theory such as the Real Log Canonical Threshold (RLCT) [17] to capture a model's 'true' size as the width is scaled."}]}