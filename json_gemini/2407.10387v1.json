{"title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "authors": ["Santiago Pascual", "Chunghsin Yeh", "Ioannis Tsiamas", "Joan Serr\u00e0"], "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models.", "sections": [{"title": "1 Introduction", "content": "Audio-visual cross-modal generation has gained a lot of traction in recent years, with the appearance of works for both audio-to-video (A2V) and video-to-audio (V2A) generation. V2A generation has some immediate and impactful applications for the media production industry. On the one hand, it promises to accelerate, improve, and/or simplify foley sound effect generation. On the other hand, tasks that feature both synchronization with respect to a visual input and also a textually guided conditioning, like automatic dubbing, can greatly benefit from a synchronized V2A generative model that features multi-modal conditioning.\nAutoregressive (AR) and mask-based deep generative models operate on discrete latent spaces. These generative strategies have been repeatedly applied to audio generation tasks recently, thanks to innovations coming"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Video to Audio Generation", "content": "Early neural V2A approaches proposed sound synthesis from videos as a way to study physical interactions of materials within a visual scene of limited diversity. Similarly, other early works started tackling V2A inside a cross-modality generative adversarial framework, where both V2A and A2V were tackled as a joint problem. A number of source-specific models (targeting specific video/sound classes) were also proposed.\nMotivated by the need to scale V2A as a source-agnostic problem, SpecVQGAN was proposed as a first multi-class visually-guided sound generator model. SpecVQGAN is built upon an autoregressive transformer that learns to generate sequences of codewords that represent mel spectrograms through a VQGAN lossy compression. Then, a neural vocoder is used to invert the mel spectrogram back into the audio waveform. Im2Wav is another Transformer-based audio language model conditioned on image representation to perform V2A. In this case, a pre-trained CLIP model is used to extract the sequence of visual features coming from the video frames. Then, their approach predicts the discrete tokens obtained from a VQ-VAE model. Similarly, CLIPSonicIQ leverages the CLIP features of individual visual frames to drive a sound generator. In this case, their generative approach follows a diffusion strategy that generates mel spectrograms. This skips the usage of a lossy compression, but requires a neural vocoder to produce audio waveforms, like many previous"}, {"title": "2.2 Audio-Visual Alignment Representations", "content": "A crucial aspect of V2A is the synchronization (temporal alignment) between an input video and the generated audio. This is often achieved with the help of an audio-visual alignment representation model. AVST (Audio-Visual Synchronisation with Transformers) detects audio-visual synchronisation in a self-supervised manner and predicts the class as either sync or off-sync. SparseSync considers that the audio-visual correspondence may only be available at sparse events. The proposed SparseSelector compresses the audio and visual input tokens into two small sets of learnable selectors. These selectors form an input to a transformer which predicts the temporal offset between the audio and visual streams. It formulates audio-visual synchronisation as a classification task onto a set of offsets (for example, 21 classes between -2/+2 sec.). As mentioned, Diff-Foley adopts CAVP to learn more temporally and semantically aligned features, then it trains a latent diffusion model (LDM) with CAVP-aligned visual features on spectrogram latent space. That is, it leverages CAVP for (1) generating audio that is temporally aligned with the visual events, and (2) deriving the Alignment Accuracy metric."}, {"title": "2.3 Autoregressive and Mask-based Audio Token Generation", "content": "Early works proved that waveform-based generative modeling was possible with explicit maximum-likelihood autoregressive (AR) strategies, as in WaveNet or SampleRNN. These proposals suffered from inefficiencies inherent to their AR nature, which was palliated by subsequent works like WaveRNN or parallel WaveNet. The advancement in neural audio codecs also facilitated the use of language modeling strategies for generative audio, and one of their strong advantages over previous models is the lower framerate featured in the codec spaces compared to the raw waveforms. Based on the SoundStream codec,"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Audio Tokenizer", "content": "In this work, we consider full-band single channel audio sequences. This means that our model has to process waveforms of audio sampled at 44.1 kHz or more. In order to decouple audio quality from the scalability of our generative strategy, we choose to operate in a latent space of low framerate, and since our strategy follows a discrete masked-token framework, our latent encoder must feature some discretized bottleneck at its core. To this end, we leverage a state of the art pre-trained neural codec for general audio, the Descript audio codec (DAC).\nDAC takes an audio waveform of T samples \\(x\u00ba \\in \\mathbb{R}^T\\) and returns a codegram, which is a tensor \\(C\u00ba = DAC(x\u00ba)\\), where \\(C\u00ba \\in \\mathbb{R}^{L \times K}\\). A strong convenience of DAC is the framerate reduction it features, converting the waveform at 44.1 kHz to K token sequences of 86.1 Hz. The number of parallel channels in \\(C^a\\) refers to the amount of RVQ levels, which hierarchically increase the codec bitrate while maintaining the same sequence length L. In our case, we stick to the pre-trained DAC with K = 9."}, {"title": "3.2 Masked Generative Video-to-Audio Transformer", "content": "Similarly to recent works , our generative strategy follows the formulation introduced in masked generative token modeling from computer vision"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We train both our models on the VGGSound dataset , which contains videos curated to maximize the audio-visual correspondence in the videos while remaining unconstrained in the nature of their content. Originally, the dataset contained 200 k video clips in their training partition, but since many videos are not available anymore and we further filter videos based on quality heuristics, we end up with a copy of approximately 155k video clips. The pre-processing heuristics involve removing videos with silent audio or whose audio length does not match a minimum of 10 seconds, as well as videos featuring less than 15 video frames per second (FPS). Each video ends up being 10s long, with the audio sampled at 44 kHz, and we only use the audio-visual contents of the dataset and require no labels for the development of our work. In order to build the validation split, we selected 535 video clips re-purposed from the original train split, which amount to approximately 1.5 hours of content."}, {"title": "5 Results", "content": "Tables 1 and 2 show the results evaluated with objective metrics for VGGSound and MUSIC test sets, respectively. Our proposed models beat all the baselines in FD terms across the full-band front-ends (FDD, FDM), exhibiting and advan- tage in natively modeling 44.1 kHz upon DAC. Nevertheless, when comparing the more prominent low-band content with the FAD metric, MaskVAT falls be- hind V2A-mapper. This may imply that the usage of a lossy codec is a quality upper bound (this is clear from Table 1, where the DAC reconstruction is already worse than V2A in FAD). Since SpecVQGAN generates 22.5kHz audio, it may"}, {"title": "6 Conclusion", "content": "In this work we proposed a masked generative video-to-audio Transformer, a model that generates audio based on an input silent video. MaskVAT makes special emphasis on tackling temporal alignment between the generated audio and the input video. Our solution connects a state of the art full-band gen- eral audio codec to ensure high quality outcomes, with a sequence-to-sequence masked-token generative approach, which is driven by pre-trained semantic and alignment features. Moreover, we also leverage a post-sampling selection strat- egy that minimizes the distance between the generated audio and the source input video. Our model outperforms existing solutions, exhibiting strong tem- poral alignment in the audio generations, which are fundamental in the overal"}]}