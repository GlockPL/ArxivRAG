{"title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "authors": ["Santiago Pascual", "Chunghsin Yeh", "Ioannis Tsiamas", "Joan Serr\u00e0"], "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models.", "sections": [{"title": "1 Introduction", "content": "Audio-visual cross-modal generation has gained a lot of traction in recent years, with the appearance of works for both audio-to-video (A2V) and video-to-audio (V2A) generation [9,10,14,20,26,35,49,56]. V2A generation has some immediate and impactful applications for the media production industry. On the one hand, it promises to accelerate, improve, and/or simplify foley sound effect generation. On the other hand, tasks that feature both synchronization with respect to a visual input and also a textually guided conditioning, like automatic dubbing, can greatly benefit from a synchronized V2A generative model that features multi-modal conditioning.\nAutoregressive (AR) and mask-based deep generative models operate on discrete latent spaces. These generative strategies have been repeatedly applied to audio generation tasks recently [1,3,4,12,30,37], thanks to innovations coming"}, {"title": "2 Related Work", "content": "Early neural V2A approaches proposed sound synthesis from videos as a way to study physical interactions of materials within a visual scene of limited diversity [40]. Similarly, other early works started tackling V2A inside a cross-modality generative adversarial framework, where both V2A and A2V were tackled as a joint problem [9,20]. A number of source-specific models (targeting specific video/sound classes) were also proposed [10,56].\nMotivated by the need to scale V2A as a source-agnostic problem, SpecVQGAN [24] was proposed as a first multi-class visually-guided sound generator model. SpecVQGAN is built upon an autoregressive transformer [48] that learns to generate sequences of codewords that represent mel spectrograms through a VQGAN lossy compression [15]. Then, a neural vocoder is used to invert the mel spectrogram back into the audio waveform. Im2Wav [43] is another Transformer-based audio language model conditioned on image representation to perform V2A. In this case, a pre-trained CLIP model is used to extract the sequence of visual features coming from the video frames. Then, their approach predicts the discrete tokens obtained from a VQ-VAE model [44]. Similarly, CLIPSonicIQ [14] leverages the CLIP features of individual visual frames to drive a sound generator. In this case, their generative approach follows a diffusion strategy that generates mel spectrograms. This skips the usage of a lossy compression, but requires a neural vocoder to produce audio waveforms, like many previous"}, {"title": "2.1 Video to Audio Generation", "content": "Early neural V2A approaches proposed sound synthesis from videos as a way to study physical interactions of materials within a visual scene of limited diversity [40]. Similarly, other early works started tackling V2A inside a cross-modality generative adversarial framework, where both V2A and A2V were tackled as a joint problem [9,20]. A number of source-specific models (targeting specific video/sound classes) were also proposed [10,56].\nMotivated by the need to scale V2A as a source-agnostic problem, SpecVQGAN [24] was proposed as a first multi-class visually-guided sound generator model. SpecVQGAN is built upon an autoregressive transformer [48] that learns to generate sequences of codewords that represent mel spectrograms through a VQGAN lossy compression [15]. Then, a neural vocoder is used to invert the mel spectrogram back into the audio waveform. Im2Wav [43] is another Transformer-based audio language model conditioned on image representation to perform V2A. In this case, a pre-trained CLIP model is used to extract the sequence of visual features coming from the video frames. Then, their approach predicts the discrete tokens obtained from a VQ-VAE model [44]. Similarly, CLIPSonicIQ [14] leverages the CLIP features of individual visual frames to drive a sound generator. In this case, their generative approach follows a diffusion strategy that generates mel spectrograms. This skips the usage of a lossy compression, but requires a neural vocoder to produce audio waveforms, like many previous"}, {"title": "2.2 Audio-Visual Alignment Representations", "content": "A crucial aspect of V2A is the synchronization (temporal alignment) between an input video and the generated audio. This is often achieved with the help of an audio-visual alignment representation model. AVST (Audio-Visual Synchronisation with Transformers) [7] detects audio-visual synchronisation in a self-supervised manner and predicts the class as either sync or off-sync. SparseSync [25] considers that the audio-visual correspondence may only be available at sparse events. The proposed SparseSelector compresses the audio and visual input tokens into two small sets of learnable selectors. These selectors form an input to a transformer which predicts the temporal offset between the audio and visual streams. It formulates audio-visual synchronisation as a classification task onto a set of offsets (for example, 21 classes between -2/+2 sec.). As mentioned, Diff-Foley [35] adopts CAVP to learn more temporally and semantically aligned features, then it trains a latent diffusion model (LDM) with CAVP-aligned visual features on spectrogram latent space. That is, it leverages CAVP for (1) generating audio that is temporally aligned with the visual events, and (2) deriving the Alignment Accuracy metric."}, {"title": "2.3 Autoregressive and Mask-based Audio Token Generation", "content": "Early works proved that waveform-based generative modeling was possible with explicit maximum-likelihood autoregressive (AR) strategies, as in WaveNet [39] or SampleRNN [36]. These proposals suffered from inefficiencies inherent to their AR nature, which was palliated by subsequent works like WaveRNN [27] or parallel WaveNet [38]. The advancement in neural audio codecs also facilitated the use of language modeling strategies for generative audio, and one of their strong advantages over previous models is the lower framerate featured in the codec spaces compared to the raw waveforms. Based on the SoundStream codec [53],"}, {"title": "3 Method", "content": "In this work, we consider full-band single channel audio sequences. This means that our model has to process waveforms of audio sampled at 44.1 kHz or more. In order to decouple audio quality from the scalability of our generative strategy, we choose to operate in a latent space of low framerate, and since our strategy follows a discrete masked-token framework, our latent encoder must feature some discretized bottleneck at its core. To this end, we leverage a state of the art pre-trained neural codec for general audio, the Descript audio codec\u00b3 (DAC) [31]. DAC takes an audio waveform of $T$ samples $x^{\\circ} \\in \\mathbb{R}^{T}$ and returns a codegram, which is a tensor $C^{\\circ} = DAC(x^{\\circ})$, where $C^{\\circ} \\in \\mathbb{R}^{L\\times K}$. A strong convenience of DAC is the framerate reduction it features, converting the waveform at 44.1 kHz to $K$ token sequences of 86.1 Hz. The number of parallel channels in $C^{a}$ refers to the amount of RVQ levels, which hierarchically increase the codec bitrate while maintaining the same sequence length $L$. In our case, we stick to the pre-trained DAC with $K = 9$."}, {"title": "3.1 Audio Tokenizer", "content": "In this work, we consider full-band single channel audio sequences. This means that our model has to process waveforms of audio sampled at 44.1 kHz or more. In order to decouple audio quality from the scalability of our generative strategy, we choose to operate in a latent space of low framerate, and since our strategy follows a discrete masked-token framework, our latent encoder must feature some discretized bottleneck at its core. To this end, we leverage a state of the art pre-trained neural codec for general audio, the Descript audio codec\u00b3 (DAC) [31]. DAC takes an audio waveform of $T$ samples $x^{\\circ} \\in \\mathbb{R}^{T}$ and returns a codegram, which is a tensor $C^{\\circ} = DAC(x^{\\circ})$, where $C^{\\circ} \\in \\mathbb{R}^{L\\times K}$. A strong convenience of DAC is the framerate reduction it features, converting the waveform at 44.1 kHz to $K$ token sequences of 86.1 Hz. The number of parallel channels in $C^{a}$ refers to the amount of RVQ levels, which hierarchically increase the codec bitrate while maintaining the same sequence length $L$. In our case, we stick to the pre-trained DAC with $K = 9$."}, {"title": "3.2 Masked Generative Video-to-Audio Transformer", "content": "Similarly to recent works [4, 17], our generative strategy follows the formulation introduced in masked generative token modeling from computer vision"}, {"title": "Visual Conditioning", "content": "The models we propose in Fig. 1 feature two possible conditioning front-ends, which extract video features from the RGB sequences $V_{RGB} \\in [\\mathbb{R}^{F\\times 3\\times h\\times w}$ to drive the V2A mapping, where $F$, $h$, and $w$ are the number of frames, their height, and their width respectively. First, a pre-trained CLIP image encoder is used to process each video frame $V^{RGB}$, projected then through a time-independent MLP into the shared dimensionality of the subsequent Transformer. The motivation to use CLIP as a video feature encoder in our case is twofold: (1) earlier works show its effectiveness in V2A already [14,45,49], and (2) its multi-modal nature expands the applicability of our proposal to text-driven video-editing applications.\nSecondly, we also consider a 3D convolutional video encoder named S3D [51]. We take the pre-trained version of S3D built in the SparseSync work, for detection of audio-visual temporal offsets, i.e. detecting temporal shifts between the two modalities [25]. The authors of SparseSync originally took S3D pre-trained on the Kinetics 400 dataset for video activity recognition [28], and fine-tuned S3D for the aforementioned offset detection task on AudioSet [18]. This video encoder yields a spatio-temporal tensor of features $V^{S3D} \\in \\mathbb{R}^{N_{S3D}\\times512\\times h_{S3D}\\times w_{S3D}}$ which we average-pool spatially to yield $V^{S3D} \\in \\mathbb{R}^{N_{S3D}\\times512}$. We consider these features"}, {"title": "Training Setup", "content": "In a masked token modeling scenario like this, we have a codegram representation $C^{a} \\in \\mathbb{R}^{L\\times K}$ (introduced in Sec. 3.1), and a subset of these $L\\times K$ tokens is masked with a special token [MASK], as shown in the training section of Fig. 2. The mask positions to be replaced by [MASK] in the codegram $M\\in {0,1}^{L\\times K}$ is determined by a masking scheduler function. For this work, we chose the cosine scheduler due to its proven effectiveness [6], so the probability of each position to be masked is computed as $p = cos(u)$, where $u \\sim U[0, 1]$, from which we obtain $M_{l,k} = Bernoulli(p)$. Let $C_{l}^{t}$ be the result of applying the mask $M$ to the codegram $C^{a}$ and $V$ be the collection of conditioning features in either format of the three proposed in Fig. 1. The training objective is to minimize the negative log-likelihood, particularly through a cross-entropy loss, for the masked positions [6]:\n$\\mathcal{L}_{mask} = \\mathbb{E}[\\sum_{ \\forall l\\in [1,L], \\forall k\\in [1,K], m_{l,k} = 1} log p(C_{l,k} | C_{M}, V)]$\nIn the sequence-to-sequence and hybrid setups of Fig 1-b and Fig. 1-c, we use a combination of a regression + contrastive loss between the visual embedding sequence after the Transformer encoder and its corresponding BEATs-projected audio embedding sequence. Regarding regression, we apply an MSE minimization between the pairs of sequences. On the contrastive side, we pre-pend a [CLS] token before injecting the sequence into the Transformer encoder, and select that position as the pooled embedding representative to contrast against the average projected BEATs embedding in a CLIP-like contrastive setup [43]. The total loss to train MaskVAT then becomes:\n$\\mathcal{L}_{maskvat-seq2seq} = \\mathcal{L}_{mask} + \\lambda_{reg} \\mathcal{L}_{MSE} + \\lambda_{cont} \\mathcal{L}_{contrastive}$,\nwhere $\\lambda_{reg}$ and $\\lambda_{cont}$ are hyper-parameters to control the loss magnitudes of the regression and contrastive regularizations respectively. Both default to $\\lambda_{reg} = \\lambda_{cont} = 1$ throughout the course of this work."}, {"title": "Sampling", "content": "Once we have trained the model to perform unmasking given the $C_{M}$ tensor, we need a sampling scheme in order to generate new audio codegrams from an initial fully masked instance, as depicted in the sampling section of Fig. 2. Following the sampling process of the original MaskGIT [6], we first determine a number of sampling steps $N_{steps}$ depending on the computational budget. Then, we begin estimating the probability distribution of each codegram position $(l, k)$ over the codewords of the $k$-th codebook at each step $n \\in [1, N_{steps}]$. While computing these probabilities, we also feature classifier-free guidance upon the logits, introducing the coefficient $\\gamma$ [5,22]. This technique is known to improve generation quality at the expense of sample diversity. Let $l = M(C_{M}^{n}, V)$ be the output logits of our MaskVAT $M$ in conditional form, and $l = M(C_{M,n})$ be the unconditional logits that only depend on the estimated and partially-masked codegram $\\hat{C}_{M,n}$, our guidance-weighted logits result in $l = (1 + \\gamma)l_{o} - \\gamma l$ [5], where $\\gamma \\geq 0$. When $\\gamma = 0$, this is equivalent to a regular conditional mode in our predictions. Then, for each masked position $(l,k)$ at step $n$, we sample from the multinomial distribution. With this we generate a candidate token $\\hat{c}_{l,k}^{n}$ per masked position at step $n$. Then, we compute the confidence of each of these sampled tokens based on the log-probability of each position $(l, k)$. Following previous works [2, 17], we introduce a diversity term $\\delta$, which is linearly annealed throughout the $N_{steps}$ as $\\delta_{n} = \\delta \\cdot (1 - \\frac{n + 1}{N_{steps}})$. This is used to add noise into to the confidence computation:\n$confidence(c_{l,k}^{n}) = log p(c_{l,k,n} | C_{M,n}, V) + \\delta_{n} \\cdot N$,\nwhere $\\hat{c}_{l,k,n}$ is a token estimate after applying guidance on its logits, at sampling step $n$, and $N$ is the i.i.d. noise sample drawn from Gumbel(0,1). This diversity technique has been proven to enhance the generation quality, especially when the number of $N_{steps}$ is increased [2, 17]. In what follows, we select the next $K$ number of tokens to mask at the next sampling iteration $n + 1$ (according to our selected mask scheduler), take the lowest $K$ confidence positions of our estimates, and build a new mask by placing the [MASK] values in these low confidence positions. The remaining ones are kept as successfully unmasked in the estimated codegram $\\hat{C}_{M,n+1}$ at the $n + 1$ sampling step . This whole block of operations is repeated until $n = N_{steps}$ (as shown in Fig. 2), and once we get our fully-unmasked estimated codegram $\\hat{C}^{n}$, we run it through the DAC decoder [31] in order to obtain our generated waveform."}, {"title": "Beam-based selection", "content": "The sampling process needs some tweaking of the diversity $\\delta$, $N_{steps}$, and guidance $\\gamma$ coefficients upon a validation set in order to produce good quality and diverse outcomes. Nonetheless, each sampling result can be very different, and some match better the input video in terms of semantic contents and alignment especially than others. In order to increase the semantic and time alignment matching with the input video, we first generate a beam-size $B$ amount of audio instances (exemplified with $B = 3$ in Fig. 2). Next, we train a sequential contrastive audio-visual (SCAV) encoder on the same"}, {"title": "4 Experiments", "content": "Datasets We train both our models on the VGGSound dataset [8], which contains videos curated to maximize the audio-visual correspondence in the videos while remaining unconstrained in the nature of their content. Originally, the dataset contained 200 k video clips in their training partition, but since many videos are not available anymore and we further filter videos based on quality heuristics, we end up with a copy of approximately 155k video clips. The pre-processing heuristics involve removing videos with silent audio or whose audio length does not match a minimum of 10 seconds, as well as videos featuring less than 15 video frames per second (FPS). Each video ends up being 10s long, with the audio sampled at 44 kHz, and we only use the audio-visual contents of the dataset and require no labels for the development of our work. In order to build the validation split, we selected 535 video clips re-purposed from the original train split, which amount to approximately 1.5 hours of content."}, {"title": "Objective Metrics", "content": "During the development of our V2A experiments we evaluated three axes of performance: (1) generated audio quality, (2) semantic matching between the generated audio and the original audio/video, and (3) temporal alignment between the generated audio and the original audio/video. For an objective measurement of quality, we rely on computing the Fr\u00e9chet distance (FD) upon different audio feature extractors, as done by previous audio synthesis works [14, 19, 29, 33, 41, 49]. FD is supposed to rate the trade-off between quality and diversity attained in the generated audio. Moreover, each audio feature extractor used to compute a different FD offers a different focus on aspects of the generated audio that fit those of the ground truth [19]. In this work, we leverage three types of embeddings to compute FDs. First, we use VGGish [21] to yield the more standardized Fr\u00e9chet audio distance (FAD [29]) for better comparability with the state of the art. This is a classifier working on magnitude filter-bank representations of the audio, with a receptive field of one second, that operates on 16 kHz signals. Secondly, we use an MFCC representation to obtain the FDM metric. This representation is frame-based, with each frame containing a window of 2048 samples and a shift of 512 samples. We extract 128 filter-banks and 64 MFCCs, so the embeddings to compute the FDM are 64-dimensional. Finally, we also leverage the DAC codec 8-dimensional embeddings across the $K$ RVQ levels after quantization, prior to the residual summation at the input of the decoder. This is the FDD metric, and the dimensionality of the embeddings are $8 \\times K$, which is $8 \\times 9 = 72$ in the default pre-trained DAC used in this work. This is also a frame-based representation, with a wider receptive field than the MFCC one. Importantly, both MFCC and DAC front-ends operate on 44.1 kHz signals, hence measuring the statistical distance in the full-band scenario, which is important for a general audio synthesis situation like ours [41].\nTo assess the semantic matching, we propose two metrics that measure the proximity of the signals in the highly semantic CLIP space, as other generative works proposed [52]. Nonetheless, since we generate audio to be evaluated instead of images or text/labels, we leverage the audio waveform front-end Wav2CLIP [50] to project our generated outcomes into CLIP space. Wav2CLIP was precisely trained to project 16 kHz audio waveforms of variable length into a fixed embedding in the CLIP space from audio-video data [43]. Then, we measure the cosine similarity between the two projected embeddings. We implement two ways of projecting through Wav2CLIP to measure the proximity: first, we project both the generated audio and the ground truth audio that came originally with the video. Then, we measure the cosine similarity of both embeddings L2"}, {"title": "Subjective Evaluation", "content": "We also set up a subjective test that features three sections explicitly asking 19 human subjects (with 11 audio processing experts) to rate: (1) audio quality and relevance (as semantic matching), (2) audio-video alignment, and (3) overall quality (mix of audio quality, semantic matching, and temporal alignment). For (1), we have selected samples from VGGSound-test containing both sparse events [25] (attack sounds with distinct onsets) and dense events (sustained sounds with temporal evolution). We take into account both 16 kHz and 44.1 kHz versions for all models and references for comparison. This was done by running the bandwidth-extension algorithm upon the baselines, or by resampling the references or MaskVAT generation down to 16 kHz. For (2), we have selected samples from VGGSound-Test-Sparse containing only sparse events. In order to focus on the audio-visual synchronicity, we use samples of 16 kHz only. For (3), we select samples from the MUSIC dataset, because the audio is highly correlated with the video and the music audio is of high quality, which is challenging to generate. Users are asked to rate (3) with all the criteria in mind (quality + semantic + alignment). Here we keep the original sample rate for all the samples such that the overall advantage of a model can be evaluated. More details about the subjective test setup can be found in the Supplementary Material."}, {"title": "5 Results", "content": "Tables 1 and 2 show the results evaluated with objective metrics for VGGSound and MUSIC test sets, respectively. Our proposed models beat all the baselines in FD terms across the full-band front-ends (FDD, FDM), exhibiting and advantage in natively modeling 44.1 kHz upon DAC. Nevertheless, when comparing the more prominent low-band content with the FAD metric, MaskVAT falls behind V2A-mapper. This may imply that the usage of a lossy codec is a quality upper bound (this is clear from Table 1, where the DAC reconstruction is already worse than V2A in FAD). Since SpecVQGAN generates 22.5kHz audio, it may"}, {"title": "6 Conclusion", "content": "In this work we proposed a masked generative video-to-audio Transformer, a model that generates audio based on an input silent video. MaskVAT makes special emphasis on tackling temporal alignment between the generated audio and the input video. Our solution connects a state of the art full-band general audio codec to ensure high quality outcomes, with a sequence-to-sequence masked-token generative approach, which is driven by pre-trained semantic and alignment features. Moreover, we also leverage a post-sampling selection strategy that minimizes the distance between the generated audio and the source input video. Our model outperforms existing solutions, exhibiting strong temporal alignment in the audio generations, which are fundamental in the overall"}]}