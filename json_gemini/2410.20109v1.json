{"title": "GiVE: Guiding Visual Encoder to Perceive Overlooked Information", "authors": ["Junjie Li", "Jianghong Ma", "Xiaofeng Zhang", "Yuhang Li", "Jianyang Shi"], "abstract": "Multimodal Large Language Models have advanced AI in applications like text-to-video generation and visual question answering. These models rely on visual encoders to convert non-text data into vectors, but current encoders either lack semantic alignment or overlook non-salient objects. We propose the Guiding Visual Encoder to Perceive Overlooked Information (GiVE) approach. GiVE enhances visual representation with an Attention-Guided Adapter (AG-Adapter) module and an Object-focused Visual Semantic Learning module. These incorporate three novel loss terms: Object-focused Image-Text Contrast (OITC) loss, Object-focused Image-Image Contrast (OIIC) loss, and Object-focused Image Discrimination (OID) loss, improving object consideration, retrieval accuracy, and comprehensiveness. Our contributions include dynamic visual focus adjustment, novel loss functions to enhance object retrieval, and the Multi-Object Instruction (MOInst) dataset. Experiments show our approach achieves state-of-the-art performance.", "sections": [{"title": "Introduction", "content": "The multimodal large language models (MLLMs) (Zhu et al. 2023; Liu et al. 2023b; Ye et al. 2023; Li et al. 2023) have significantly advanced general artificial intelligence by demonstrating exceptional capabilities in generation and inference across diverse real-world applications, such as text-to-video generation (Blattmann et al. 2023), visual question answering (Singh et al. 2019), and embodied robotics (Mu et al. 2023). To optimize performance while managing costs, a prevalent MLLM architecture integrates a visual encoder with the large language model (LLM). This architecture encoder embeds non-text data, transforming these embeddings into vectors comprehensible to the LLM through a mapping mechanism. Although prior research (Wang et al. 2023; Liu et al. 2023a) has underscored the efficacy of this architecture, the cross-modal perception abilities of MLLMs heavily relies on the visual encoder, with the quality of encoded image embeddings directly influencing the response quality of the MLLMs.\nAn image encoder is a specialized visual encoder designed to map high-dimensional image data to a lower-dimensional feature space. These encoders can be broadly categorized based on their pre-training tasks into two main types: reconstruction-based and cross-modal contrastive learning-based encoders.\n\u2022 Image encoding models trained with reconstruction tasks (van den Oord, Vinyals, and Kavukcuoglu 2017; Huang, Fu, and Bors 2023) are proficient in capturing comprehensive image details. However, these models lack semantic alignment with text during training, which complicates the LLM's ability to interpret image embeddings (Yin et al. 2023). Consequently, such encoders are infrequently utilized in MLLMs.\n\u2022 Vision transformer (ViT) models trained with image-text contrastive learning (Radford et al. 2021; Jia et al. 2021) generally align effectively with LLMs but face an implicit \"ignore\" problem, limiting their expressive capability. This limitation arises because different modalities convey distinct types of information. For instance, an image may feature multiple objects with unique attributes, such as texture, color, spatial location, and potential interactions. In contrast, abstract text typically highlights only the most salient objects and provides limited descriptions of other visual elements. ViTs trained for image-text matching tend to focus on the salient regions of the image that correspond to the text, thereby overlooking secondary elements like the background.\nIn summary, for effective integration with LLMs, MLLMs require an image encoder that is both (1) semantically aligned with text during training and (2) capable of flexible attention to prevent the omission of relevant visual features.\nTo address these challenges, we propose this Guiding Visual Encoder to Perceive Overlooked Information (GiVE) approach, which aims to guide the visual encoder in adaptively adjusting its attention to well capture overlooked information. In this approach, we introduce a novel Attention-Guided Adapter (AG-Adapter) module that enhances the representation ability of the visual encoder by aligning the visual representations with abstract semantics. This module also functions as a plug-in for generalizing abstract semantics, enabling it to more effectively address user queries.\nTo tackle the above limitations in detail, GiVE incorporates another innovative module: Object-focused Visual"}, {"title": "Related Work", "content": "Visual encoding models can be categorized based on their pre-training tasks into image reconstruction models and image-text contrast models. (i) Image reconstruction models, such as MAE (He et al. 2022), DeepMIM (Ren et al. 2023), and MIRL (Huang, Fu, and Bors 2023), use masked image modeling similar to language modeling. Other models, like VQ-VAE (van den Oord, Vinyals, and Kavukcuoglu 2017), VQGAN (Esser, Rombach, and Ommer 2021), and HQ-VAE (You et al. 2022), implement image reconstruction using autoencoder architectures. (ii) Models like CLIP (Radford et al. 2021), ALIGN (Jia et al. 2021), and EVA-CLIP (Sun et al. 2023) utilize cross-modal contrastive learning to align textual and visual information into a unified semantic space. Consequently, such pre-aligned encoders are more readily aligned with LLMs through alignment pre-training (Yin et al. 2023). In contrast, reconstruction-trained models lack this text correlation due to their independent training process. As detailed in Table 5 in the Appendix, 90% of the MLLMs we investigated use image-text contrast encoders.\nDiscussion. Our study preserves the semantic alignment capabilities of visual encoders while minimizing the omission of effective information by using semantic instructions."}, {"title": "Visual Perception of MLLMs", "content": "Despite some pioneering works mitigating the negative effects of visual ignorance, they still have limitations. For instance, InstructBLIP (Dai et al. 2023) injects textual instructions to guide the post-processing process with a lightweight Q-Former to translate the image embeddings. However, this downstream secondary coding only reinforces indication-related information in the image embeddings and does not recover useful features lost in the upstream image encoder. Similarly, LLaVA-NeXT (Liu et al. 2024) encodes the image multiple times after segmentation, allowing each subimage to have its own salient object, thus reducing the visual feature loss rate. However, this exponentially increases token occupancy and encoding time. Additionally, BLIVA (Hu et al. 2024) combines the mapper of BLIP-2 and LLaVA to enhance the LLM's understanding from more translation perspectives, but since both mappers share the same image encoder, this method does not fundamentally address the loss of useful information during visual encoding.\nDicussion. Overlooking the inherent imperfections of visual encoders and attempting to rectify them through alternative means is frequently inadequate. Our objective is to"}, {"title": "Methodology", "content": "The proposed approach takes a text encoder to encode both captions and instructions, and a visual encoder for visual content. An AG-Adapter module is inserted into each stacked visual encoder as the tunable parameters to align cross-modal data. The AG-Adapter is trained by the designed Object-focused Visual Semantic Learning module. Details of model architecture, AG-Adapter, and object-focused visual semantic learning module are presented in the following subsections. The overall architecture is depicted in Figure 2."}, {"title": "Model Architecture", "content": "T\nThe proposed model includes a visual encoder \u03a61(\u00b7,\u00b7) and a text encoder \u03a6T(\u00b7) to respectively encode visual and textual content. It accepts image-text-object triplets {xI,xT,xO} as input, where xI \u2208 I is an image, xT \u2208 T is a text, and xO \u2208 O is an indicative text denoting the target object, such as \"person\". The model then extracts conditional image and text features (yI, yT) using paired encoders. When extracting conditional image features, the instruction feature yO is fused with the visual data stream within the AG-Adapter module. Formally,\nyT = \u03a6T(xT), yO = \u03a6T(xO), yI\\O = \u03a61(xI,yO),\n(1)\nwhere yT \u2208 Rd is text feature, yO \u2208 Rd is instruction feature, and yI is conditional image feature, i.e. the output of the visual encoder integrated with the AG-Adapter. The AG-Adapter is trained using our designed Object-focused Visual Semantic Learning component containing three loss terms: OITC, OIIC and OID. Note that during the training phase, the loss is calculated based on the output embeddings of both encoders. During the inference phase, the text encoder is retained to serialize the textual instructions."}, {"title": "Attention-guided Adapter", "content": "The proposed AG-Adapter module, as highlighted in green rectangle in Figure 2, is a simple yet effective plug-in that interweaves semantic directives with visual cues, enabling visual model to perceive queried objects. The AG-Adapter is inspired by Latent Diffusion Model (LDM) (Rombach et al. 2022) which enhances the alignment of visual representations with abstract semantics by conditioning on text representations. In particular, the AG-Adapter is incorporated into the pre-training feature extraction layers of the \u03a61(\u00b7,\u00b7), with only the inserted layer undergoing the training process. Formally, in each layer of the visual encoder, the AG-Adapter module \u03c6(\u00b7) enhances fine-grained object features:\nfV\\O = \u03c6(yO, fV),\n(2)\nwhere yO is instruction feature derived from user queries and fV refers to the feature of the visual data flow. The fV \u2208 R(M+K)\u00d7d is composed of two types of tokens: image tokens fI \u2208 RM\u00d7d and other tokens fH \u2208 RK\u00d7d. The image tokens are derived directly from the input image, while the role of other tokens depends on the baseline model. In the case of GroupViT (Xu et al. 2022), other tokens are group tokens; in contrast, in CLIP (Radford et al. 2021), they are absent, and K = 0. Within the AG-Adapter, the dual-layer MLP serves to bridge the gap between text and image representations, namely,\nfO = MLP(yO),\n(3)"}, {"title": "Object-focused Visual Semantic Learning", "content": "During the abstract concept learning phase, the original parameters of the image and text encoders are frozen, and the parameters of the AG-Adapter are trained. In each training batch, we sample b image-text-object triples {xIk,xTk,xOk}k=1b and encode them to obtain image-text feature pairs {yTk, yIk}k=1b. Our goal is to train the AG-Adapter to extract the most relevant visual representations based on semantic instructions. As shown in Figure 3, to achieve this goal, we jointly optimize three training objectives that share model parameters.\nObject-focused Image-Text Contrast (OITC) objective is designed to align image and text representations centered around instructed objects, with the aim of maximizing their mutual information. This objective requires the visual encoder to generate distinct features for different instructions. The similarity between features is computed as follows:\nsijI\\O = (yIkI\\O)TyTj,\n(7)\nwhere sI\\Oij denotes the similarity of the i-th conditional image feature to the j-th text feature, and sijT represents the similarity between the i-th text feature and the j-th text feature. For the conditional image feature yIkI\\O in feature pair {yIk,yTk}, the corresponding text features yTjx\\Ox=xIk^xOx=xTjx\\Ox of the same object within the same image are positive, while other text features within the batch are negative. The loss of a batch can be represented by\nLI\\OT(yI, {yTj}j=1b) =\n\u22121b\u2211k log\n\u2211jexp(sTij)\n\u2211k,j exp(sTij|xO=xIk^xOx=xTkI\\O)\n,\n(8)\nLT\\OI(yT, {yIj}j=1b) =\n\u22121b\u2211k log\n\u2211jexp(sTij)\n\u2211k,j exp(sIij|xIk=xTkI\\O^xIk=xIkI\\O)\n,\n(9)\nLI\\OTITC =12(LI\\OT(yI, {yTj}j=1b) + LT\\OI(yT, {yIj}j=1b)),\n(10)"}, {"title": "Object-focused Image Discrimination (OID)", "content": "Object-focused Image-Image Contrast (OIIC) loss emphasizes the commonality of objects within the same class, requiring the encoder to generate similar features for these objects. The contrast is performed within the image. For a feature yIkI\\O, yIjkx\\Ox=xIkI\\O is its positive example, while other conditional image features in the batch serve as in-batch negatives. The similarity computation of conditional image features is expressed as\nsijI\\OI\\O = (yIkI\\O)TyIjx\\Ox=xIkI\\O\n(11)\nThe OIIC loss, denoted by LOIIC, can be represented as\nLOIIC = - 1b\u2211k log\n\u2211jexp(sI\\OI\\Oij)\n\u2211k,jexp(sI\\OI\\Oij|xIk=xIkI\\O^xIk=xIkI\\O)\n(12)\nObject-focused Image Discrimination (OID) is a binary classification task that requires the model to predict whether a given image and the indicated object match. For each batch of sample pairs {xIk,xOk}k=1b, we additionally construct b negative pairs {xIk,xNOk}k=1b, where xNOk indicating object not appear in xIk. These positive and negative samples {xIk,xOkUNO}k=12b are encoded to {yIkI\\OUNO}k=12b, which are then input into a binary linear classifier to obtain logits {zik}k=12b. The loss function is formalized as\npi =\n1\n1 + exp(\u2212zi)\n,\n(13)\nLoid = -2b\u2211i=12b[ti log(pi) + (1\u2212ti) log(1 \u2212 pi)],\n(14)"}, {"title": "Experimental Setup", "content": "Datasets We evaluate the effectiveness of GiVE using both the LVIS dataset (Gupta, Doll\u00e1r, and Girshick 2019) and MOInst dataset, each annotating multiple objects per image. The comprehensive LVIS dataset, referred to as LVIS, comprises 1,203 categories, from which, we derive a subset, denoted as LVISA, with 405 \u201cfrequent\u201d categories. In contrast, the MOInst dataset used for training contains 264 categories, with category label overlaps of 7.7% and 17.6% with the two LVIS datasets, respectively. We also conduct"}, {"title": "Performance Evaluation", "content": "Image Classification We conduct zero-shot image classification on the \"frequent\" subset and the full LVIS test set. Table 1 presents the results on five ViT baselines. Key observations from these experiments are as follows:\n\u2022 The evaluation value for the instruction text is nearly equivalent to random categorization, demonstrating that our metric successfully filters out textual interference. This outcome ensures that our work fairly compares the visual feature extraction capabilities of each model.\n\u2022 The GiVE demonstrates a notable improvement in all baselines across all metrics on both evaluation datasets."}, {"title": "Conclusion", "content": "This paper presents GiVE, a novel approach enhancing visual encoders' integration with LLMs by addressing semantic alignment and overlooked information. GiVE features the AG-Adapter and three innovative loss functions\u2014OITC, OIIC, and OID. The AG-Adapter aligns visual representations with abstract semantics, while the OITC loss ensures attention to both salient and non-salient objects, and the OIIC and OID losses enhance object retrieval accuracy and comprehensiveness. Experiments show GiVE's significant improvements over existing methods in multiple tasks."}]}