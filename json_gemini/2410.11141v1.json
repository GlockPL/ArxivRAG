{"title": "Can Structured Data Reduce Epistemic Uncertainty?", "authors": ["Shriram M S", "Sushmitha S", "Gayathri K S", "Shahina A"], "abstract": "In this work, we present a framework that utilizes ontology alignment to improve the learning process of deep learning models. With this approach we show that models fine-tuned using ontologies learn a downstream task at a higher rate with better performance on a sequential classification task compared to the native version of the model. Additionally, we extend our work to showcase how subsumption mappings retrieved during the process of ontology alignment can help enhance Retrieval-Augmented Generation in Large Language Models. The results show that the responses obtained by using subsumption mappings show an increase of 8.97% in contextual similarity and a 1% increase in factual accuracy. We also use these scores to define our Hallucination Index and show that this approach reduces hallucination in LLMs by 4.847%.", "sections": [{"title": "Introduction", "content": "In the current era of Large Language Models (LLMs), with an abundance of data, there is always a tricky question to be addressed: Is providing an abundance of data enough to solve complex tasks? The majority of modern-day models are fundamentally probabilistic, which though highly powerful in its way, gives the model only an uncertain output that cannot be reasoned out. This uncertainty is of 2 types, epistemic (EU) and aleatoric (AU), where the former is also called reducible uncertainty, caused due to the lack of knowledge of a model and the latter arises due to randomness in data Hora (1996). In this work we aim to show that ontology can be used as a means to reduce epistemic uncertainty in LLMs, wherein we show how additional knowledge acquired through the structural format of an ontology reduces EU. Consider the training data {(xi, Yi)}=1 \u20ac (X x Y) and our function F : X \u2192 Y. Our goal is to make the function F learn from such training data that is structured in nature.\nWe claim, to tackle these EU's, especially in Language Models where this effect is seen in the form of hallucinations Tonmoy et al. (2024), structured data could be used. In this work, we demonstrate why structured data is more important while attempting to improve the rate at which Language Models learn during the training process. Our experiments also prove how useful structured data is to tune prompts of Generative Language Models. We choose ontologies, which are essentially a collection of named concepts called classes with their properties and relationships. We first attempt to align ontologies by using a pre-trained transformer that is trained to perform sequential classification tasks. The ontologies are subsequently identified as source ontology S and target ontology T while aligning, where we can define ontology alignment as a process to identify the semantic correspondence between classes in S and T. During this process, we consider classes from S and T to retrieve equivalence and subsumption mappings, where equivalence implies a pair of same or similar concepts between two classes, one from S and the other from T, and subsumptions denote the relationship between pair of classes from S and T, where the class from S is a superclass of the corresponding class from T and vice versa.\nOur work uses an approach where we make use of subsumptions obtained through ontology mapping, to provide more contextual information to the prompt that is passed to the Language Model. One of the main issues with the current retrieval approaches using Retrieval-Augmented Generation is hallucination, where the model gives out irrelevant, incorrect, and unreal responses. By incorporating subsumptions in the prompt, we ensure hallucination is minimized and the response of the Language Model is more contextually and factually intact. Section 4 presents key insights from our experimentation with ontologies in the medical domain, demonstrating how our methodology could be used for quicker training and reducing hallucinations in LLMs."}, {"title": "Literature Survey", "content": "Ontologies and knowledge graph could play a vital role in enhancing the reasoning and contextual capabilities of deep learning models like a transformer, which already learns through an inherent attention mechanism. A lot of research is going on surrounding the topic of ontologies. Yuan He et al. in He et al. (2024) present a new framework Deep-Onto which is a python package developed specifically for ontology engineering. It offers various tools for ontology reasoning, verbalization, normalization, and projection. It also supports ontology alignment as well as the completion tasks using pre-trained LLMs. In Jaradeh and Kurdy (2023) the authors provide a novel mechanism of ontology alignment using BERTMap, where a fine-tuned BERT was used to predict mappings on a text semantics corpus extracted"}, {"title": "Methodology", "content": "We propose a comprehensive framework to help models learn from structured data and hence minimize EU. In this section, we demonstrate how ontologies can be used to reduce EU and help achieve quicker learning. We also explore a novel approach using ontologies to aid RAGs and overcome hallucinations that are often faced by LLMs. Figure 1 shows the overall framework of our work."}, {"title": "Using transformers to align ontologies", "content": "In our first step, we perform ontological alignment across two ontologies to obtain equivalence mappings. This is considered a synonyms classification task where we check if class c in ontology O and class c' in ontology O' are synonymous to each other. We let a pre-trained BERT model, which is essentially a model that is trained to perform sequential classification, predict if a particular class pair \u03a8 is synonymous or not.\n$\\Psi = (c, c')$\n$P(c = c') = \\{0,1\\}$"}, {"title": "Usage of the extremely fine-tuned model for sequential classification", "content": "Using the checkpoints obtained through the alignment of ontologies for equivalences mapping, we downstream the task to make the model perform sequential classification. Here we compare both the pre-trained model and the model trained with ontologies, herewith referred to as the extremely fine-tuned model. We train the models over the same dataset to check how well and quickly the models can capture dependencies during training with the same model parameters \u03b8. Our idea is to define a hypothesis to show how important structured data is for a model. We define our hypothesis as follows:\nHypothesis 1. Consider two models \u03b1 and \u03b2. Let model \u03b1 be an extremely fine-tuned model with additional ability acquired through structured data and model \u03b2 be the native version of model \u03b1. Both the models are bound to reach an optimal accuracy state S provided the models run for an arbitrary number of epochs \u03b5. The factor deciding the better model in such a case would be the rate \u03c1 at which a model reaches such an S.\n$\\rho\\varsigma(\\alpha) = X$\n$\\rho\\varsigma(\\beta) = X'$\nWe say that model \u03b1 converges to S at an epoch n, where n < \u03b5, giving a rate X which is quicker than X', which is the rate at which model \u03b2 learns at an epoch n', where n' > n."}, {"title": "Obtaining subsumption mappings from equivalences", "content": "Using the equivalence mapping obtained, we first attempt to construct a corpus of positive and negative subsumption candidates. We define a positive class pair $(c1, cs2) where c1 = c2 and cs2 \u2286 c2. The BERTSub pipeline (Chen et al. (2023)) also creates a negative class pair by replacing c$2 with an arbitary named class pair. We make use of a pretrained model to predict all the classes $(c1, cs2) in the chosen ontologies and claim cs2 \u2286 c1 when the score P(cs2 \ub4dc c1) \u2265 0.5. Obtaining such subsumptions through structured data provides us a method to relate concepts across different ontologies, enabling a richer knowledge of the chosen domain for the ontologies."}, {"title": "Prompt tuning using subsumptions to aid RAG", "content": "In this section, we dive into how subsumptions can be incorporated in Language Models ( chatbot, in our case) through prompts to give an enhanced answer. We train a language model which is provided a document, whose vector embeddings are saved in a vector database. Essentially, this is a RAG module, where the vector database has domain knowledge through the document provided. When a prompt is provided, say P, the model encodes the natural language prompt to give a vector embedding v(P). Similarly, the texts of the documents are stored as vectors, giving us v(Ti) where T\u1d62 \u2208 D. Traditionally, we compute the cosine similarity of these texts to retrieve the data for the prompt from the vector database, provided the vector embeddings are similar contextually. The change in our framework comes with what we retrieve from this database, we consider the prompt P and check if the tokens in P are the same as c\u2081 where c\u2081 is a class from the source ontology of a subsumption pair $(c1, cs2). If the token, Kp from the prompt P is similar to the class c1, we append the class that subsumes c1, that is cs2, to the list of tokens Kp, of P. Then we pass the new prompt P', which is the detokenized version of Kp to the model. This way, the model has new additional concepts to look up, which are contextually related to words in the prompt. This would essentially provide us with an enhanced version of the original prompt with more context being infiltrated into the prompt before it is passed to the model itself."}, {"title": "Experimentation", "content": "In this section, we first attempt to prove hypothesis 1 by making use of the Symptoms Ontology which contains 1019 classes surrounding symptoms from various systems and functions in the human body as the source ontology, and Clinical Signs and Symptoms Ontology is a collection of 303 classes surrounding various aspects of medical symptoms and signs of diseases as the target ontology. Ontology alignment is performed to obtain equivalence and subsumption mappings. These mappings are a result of the synonymous classification task which is done using a pre-trained version of a BERT-based model. These models are further used to perform sequential classification on the dataset."}, {"title": "Conclusion and Discussion", "content": "In this work, we prove our hypothesis where we show how a pre-trained model can learn features at a higher rate when fine-tuned on ontologies. Our experimentation assists our hypothesis and opens up a new method to train models. We furthermore display how subsumptions obtained could aid the current RAG framework by enriching the prompt with additional contextual information, hence decreasing the level of hallucinations, thus reducing the effects of epistemic uncertainty. This work could be extended by using a similar framework for multi-modal models, mainly Vision-Language Models, where we can use ontological data to obtain visual information and then utilize this information for further processing."}]}