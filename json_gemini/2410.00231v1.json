{"title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models", "authors": ["Qi Wu", "Zipeng Fu", "Xuxin Cheng", "Xiaolong Wang", "Chelsea Finn"], "abstract": "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end-effectors for manipulation, limited semantic under-standing using only simulation data, and low traversability and reachability in indoor environments. We present a system for quadrupedal mobile manipulation in indoor environments. It uses a front-mounted gripper for object manipulation, a low-level controller trained in simulation using egocentric depth for agile skills like climbing and whole-body tilting, and pre-trained vision-language models (VLMs) with a third-person fisheye and an egocentric RGB camera for semantic understanding and command generation. We evaluate our system in two unseen environments without any real-world data collection or training. Our system can zero-shot generalize to these environments and complete tasks, like following user's commands to fetch a randomly placed stuff toy after climbing over a queen-sized bed, with a 60% success rate.", "sections": [{"title": "I. INTRODUCTION", "content": "Quadrupedal robots powered by learning-based methods have made significant strides in locomotion capabilities in recent years, demonstrating impressive agility and robustness across diverse terrains [1], [2]. However, their potential for assisting humans in everyday indoor environments remains largely untapped, like the ability to understand and follow language instructions to fetch a bottle of water for you. Several key challenges have hindered progress in this direction. First, equipping quadrupeds with effective manipulation capabilities without compromising agility is difficult, as traditional robotic arms often add significant weight and complexity [3]. While recent advances have demonstrated impressive quadrupedal agility, navigating cluttered indoor spaces and reaching high surfaces like beds or sofas requires a level of body control and environmental reasoning that goes beyond existing approaches. Moreover, bridging the semantic gap between simulation and reality remains a significant hurdle. Learning-based controllers trained in simulation often struggle to generalize to the rich, context-dependent nature of real-world indoor scenes, due to mismatches between simulation rendering and real-world sensing, and complexity in specifying diverse real-world scenarios in simulation. This limits robots' ability to understand and interact with diverse household objects and environments.\nIn this paper, we present Helpful DoggyBot, a quadrupedal robot system that aims to overcome these limitations and enable helpful mobile manipulation skills that can understand human commands and generalize across different indoor envi-ronments. To empower quadrupeds with general manipulation capabilities while still maintaining their agility, we design a simple yet effective 1-DoF gripper that is mounted on the bottom front of the robot. Shown in Figure 1, The gripper, serving as the \u201cmouth\u201d of our robot, allows it to pick up and firmly hold everyday objects through \u201cbiting\u201d.\nTo increase the traversability and reachability of quadrupeds compared to prior work [4], [5], we use reinforcement learning and simulation to train a general-purpose low-level controller"}, {"title": "II. RELATED WORK", "content": "Legged Mobile Manipulation. Legged robots have long been of interest for their potential to traverse complex terrains while performing manipulation tasks. Much work in this area focused on bipedal humanoid platforms [6]- [11], demonstrating basic manipulation while maintaining balance [12]-[21]. More recently, quadrupedal robots have gained attention for their inherent stability and agility [1], [4], [5], [22]-[36]. Several approaches have been explored to enable manipulation capabilities on quadrupeds. One common method is to mount a robotic arm on the quadruped's back [3], [37]-[45]. While this provides significant dexterity, it also adds considerable weight and complexity to the system, hence reducing the agility of quadrupeds. An alternative approach is to utilize the quadruped's existing or modified limbs and torso for simple pushing tasks [46]\u2013[51]. Learning-based methods have shown promise in developing legged manipulation skills. For instance, [42] combines imitation learning for target end-effector trajectory generation and reinforcement learning for low-level control. [48] chains multiple polices to complete pushing tasks guided by fiducial markers. However, these approaches often struggle to generalize beyond the specific tasks and environments used during training. Our work builds upon these foundations by introducing a simple yet effective gripper design and a learning approach that enables generalization to unseen environments. Unlike previous work, we focus on enabling helpful indoor tasks that require both agile locomotion and object manipulation.\nRobot Learning using Large Pretrained Models. The advent of large pre-trained models, particularly in the domains of computer vision and natural language processing, has opened new avenues for robot learning [52]-[71]. These models, trained on vast amounts of visual data, offer rich semantic representations that can be leveraged for various robotic tasks. In the context of manipulation, [53], [55] use VLMs to generate cost functions for tasks specified in language instructions, while [57], [72]-[74] use VLMs to directly generate executable commands or intermediate presentations. These approaches, however, were primarily focused on static manipulation scenarios. For mobile robots, recent work has explored using large pretrained models for navigation and locomotion [50], [57], [75]\u2013[78]. For example, prior work demonstrates how VLMs can be used to generate navigation commands for wheeled robots [75] and legged robots [58]. However, the integration of these models with mobile manipulation remains relatively unexplored. Our work bridges this gap by leveraging pretrained vision-language models to enable semantic understanding and adaptive behavior generation for a quadrupedal robot performing mobile manipulation tasks. Unlike previous approaches, we demonstrate how large pretrained models can be effectively used in conjunction with learned low-level controllers to enable zero-shot generalization to mobile manipulation tasks."}, {"title": "III. HARDWARE", "content": "Shown in Figure 2, our robot hardware system consists of a 12-DoF Unitree Go2 quadruped robot and a 1-DoF gripper mounted on the bottom from of the robot. Both are powered by the onboard battery of Go2. We 3D-print and custom-build our Finray gripper which is actuated by a Dynamixel XM430-W350-T servo motor through a slider-crank mechanism for fast closing. We use the onboard Jetson to run our learned low-level controller that takes egocentric depth from a RealSense D435 and proprioception as input, and VLMs upon approaching objects that takes egocentric RGB as input. We send high-level commands, that are generated from VLMs running on a separate workstation that takes third-person top-down RGB stream as input, to the Jetsen through Wi-Fi."}, {"title": "IV. LEARNING A GENERAL WHOLE-BODY CONTROLLER", "content": "To enable effective mobile manipulation in diverse indoor environments, our robot requires both agile locomotion skills for traversing challenging obstacles and precise whole-body control for expanding its workspace. Previous works have typically addressed these challenges separately [32], [33], but integrating multiple objectives into a single learning frame-work introduces new complexities. These include increased exploration burden and the potential for sub-optimal behaviors when optimizing for multiple objectives simultaneously [43]. Shown in Figure 3, our approach leverages a two-phase training process focusing on the whole-body control and agility to overcome these challenges."}, {"title": "A. Phase 1: Training with Privileged Information", "content": "We develop our agile visual whole-body control policy through a two-phase training process: In the first phase, we train a policy using PPO [79] to optimize both whole-body control and agile locomotion objectives. During this phase, the policy uses privileged information in the form of scandots, capturing heights of terrain near the robot, as observations, allowing for efficient learning in simulation.\nWhole-body objective: This objective enables the robot to track a randomly sampled pitch command, expanding the workspace of the 1-DoF gripper. We define the reward as:\n$r_{wb} = exp(-3 \\cdot |p_{cmd} - p|)$    (1)\nwhere $p_{cmd}$ is the commanded pitch uniformly sampled from the range [-30\u00b0, 30\u00b0] and p is the actual pitch angle of the robot's body. We remove this objective only when the robot is encourtering obstacles to avoid conflicting objectives.\nAgile locomotion objective: This objective encourages the robot to traverse challenging obstacles such as high steps. To mitigate the exploration burden, we adopt a velocity tracking"}, {"title": "B. Phase 2: Policy Distillation using Egocentric Depth", "content": "To enable real-world deployment, we distill the learned policy during Phase 1 into a deployable policy that operates on depth images from a front-facing camera instead of privileged scandots information. We use Regularized Online Adaptation (ROA) [43] to train an online estimator to recover environmental information from the history of onboard observations. Our online estimator architecture consists of a convolutional neural network (CNN) followed by a gated recurrent unit (GRU) to process the temporal sequence of depth images. This design allows the policy to capture both spatial and temporal information from the visual input. The output of this estimator replaces the scandots input to the base policy learned in Phase 1. A key difference from previous work [32] is that we do not perform dual distillation of both the heading command and the exteroception simultaneously. Instead, we leverage a more powerful VLM to specify the robot's intended heading direction. This approach helps us avoid potential out-of-distribution problems that can arise in dual-distillation processes."}, {"title": "C. Simulation Environments and Training Curricula", "content": "To ensure robust performance across diverse scenarios, we train our policy in a variety of simulated environments featuring challenging obstacles such as stairs and uneven terrain. We randomly generate these environments for each training episode, varying parameters like stair height, number of stairs, and terrain friction to promote generalization.\nTo further improve learning efficiency and policy per-formance, we employ reward shaping techniques and a curriculum learning approach. We introduce auxiliary rewards for maintaining balance, minimizing energy consumption, and smooth transitions between different locomotion modes (e.g., walking, climbing, and tilting). The curriculum progressively increases the difficulty of the training environments, starting with simple flat terrains and gradually introducing more complex obstacles as the policy improves. By combining these techniques with our two-phase training process, we develop a general whole-body controller capable of agile"}, {"title": "V. ZERO-SHOT DEPLOYMENT USING VLMS", "content": "To enable zero-shot generalization to unseen environments and objects, we leverage pre-trained VLMs for semantic understanding and adaptive behavior generation. Our system integrates open-vocabulary object detection, efficient naviga-tion, and precise grasping, all without requiring task-specific training data or fine-tuning."}, {"title": "A. Open-Vocabulary Detection, Segmentation and Tracking", "content": "Our system employs a combination of state-of-the-art vision models to achieve robust open-vocabulary object detection, segmentation and tracking.\nInitial Detection: We utilize Florence-2 [80] to perform open-vocabulary object detection. This allows our system to identify and localize both the robot itself and the target objects based on natural language descriptions, enabling flexibility in task specification.\nSegmentation: Following initial detection, we apply SAM2 (Segment Anything Model 2) [81] to generate precise object masks. The integration of Florence-2 and SAM2 enables our system to handle a wide range of objects without prior training on specific categories.\nTracking: To maintain real-time performance, we employ SAM2 for object tracking at 10 Hz. This approach allows for continuous updating of the object's position in the environment, crucial for navigation and manipulation tasks."}, {"title": "B. Navigation", "content": "Our navigation system leverages a top-down fisheye camera mounted on the ceiling to provide a global view of the environment. This perspective enables simultaneous tracking of both the robot and target object positions, simplifying the planning process. We use the detected object position as a single waypoint for navigation, generating commands that guide the robot efficiently towards its goal. The system maintains a constant linear velocity of 0.8 m/s towards the waypoint, while angular velocity is computed using a proportional controller with $K_p = 0.5$ based on the difference between the robot's current heading and the vector pointing to the waypoint. During this phase, the pitch command is set to 0. To ensure smooth integration of locomotion and manipulation, the system transitions from navigation to grasping mode when the robot is approximately 1 meter away from the target object. We assume that our low-level controller can traverse most indoor obstacles like beds and sofas, thus alleviating the need for obstacle avoidance."}, {"title": "C. Grasping Objects", "content": "As the robot approaches the target object, it switches to a precise grasping strategy using its front-mounted gripper, transitioning from global to egocentric perception. The system now relies on egocentric depth and RGB cameras mounted on the robot for fine-grained control. Since SAM2 is compute-intensive and hence unsuitable for onboard inference, we employ an on-device multi-stage perception pipeline for accurate object localization, combining GroundingDINO [82] for object detection at 0.2 Hz, MobileSAM [83] for generating precise object masks on the RGBD input at 0.2 Hz, and Cutie [84] for high-frequency tracking at 10 Hz. This approach maintains accurate object position information between slower"}, {"title": "VI. EXPERIMENTS", "content": "Baselines in Simulation. We compare our controller with several baselines including Blind, No GRU, No Distill and No Waypoint. We also include Oracle (Phase 1), the policy trained in phase 1 using privileged information.\n\nBlind: a blind policy using only proprioception and no depth images as observations.\nNo GRU: a MLP policy baseline. Instead of using a GRU, it uses only the depth image and proprioception at the current time step without any memory to predict actions.\nNo Distill: an ablation training a deployable policy with GRU directly using PPO with our two-phase training process, so skipping the distillation stage.\nNo Waypoint: removing the agile locomotion objective guided by waypoints. Directly train the policy in Phase 1 with a reward encouraging tracking sampled linear and angular velocity commands.\nOracle (Phase 1): The policy from the first training phase, which has access to privileged information only available in simulation, such as terrain scandots.\nThese baselines allow us to assess the impact of various components in our approach, including the importance of visual input, temporal memory, the two-phase training process, and the use of waypoints in guiding robot forward. Additionally, comparing against the Oracle provides insight into the performance gap between our deployable policy and one with access to privileged environmental information.\nSimulation Results. Shown in Table I, the Blind and No GRU baselines exhibit poor performance, failing in most tasks except for the simple Walk task where they achieve 100% success. These baselines lack the necessary spatial awareness or memory mechanisms required for complex sequential navigation tasks involving climbing. Learning from vision directly increases the complexity of the training process, where the network can't learn from scratch properly. The No Waypoint baseline shows moderate success in Climb Up and Climb Down, but still struggles with the more challenging climbing tasks, highlighting the importance of on-the-fly velocity command generation for climbing. Without waypoints as guidance, the robot easily learns to walk pass the obstacle or turn around instead of trying to climb as a result of rewarding local velocity only. In contrast, our approach achieves consistently higher performance, with near-perfect scores in most tasks, especially Climb Up and Climb Down, and outperforms all baselines. We find only small degradation in performance from the oracle policy using priviledged information in Phase 1. Our approach shows that distilled policy can perform as well as the Oracle policy, suggesting the effectiveness of the two-phase training process. The overall results demonstrate the importance of integrating components such as depth information, memory, waypoint guidance and distillation."}, {"title": "B. Real-World Experiments", "content": "Baselines and Tasks in Real World. We compare our system deployed in the real world with several baselines. The baselines include Go2 Default, Teleop, and No Tracking:\nGo2 Default: the default controller built in with Go2. This controller does not use exteroception.\nTeleop: the commands are generated by an expert human operator through a remote controller, replacing VLMs.\nNo Tracking: the commands are generated open-loop using the initial pose detection of the robot itself and the object of interest.\nIllustrated in Figure 1, we select three objects and three environments that represent realistic real-world scenarios:\nBed + Toy: The robot needs to fetch a stuffed toy on a bed. The task requires the robot to climb up a queen-sized bed with 40cm in height, pick up the stuffed toy on the bed, and climb down the bed. The stuffed toy is placed uniformly randomly on a 1m by 1m region on the bed. The robot is initially randomly placed in the bedroom.\nSofa + Bottle: The robot needs to fetch an empty plastic water bottle on a sofa. The task requires the robot to climb up a sofa with a height of 44cm, pick up the bottle on the sofa, and climb down the sofa. The bottle is placed uniformly randomly on a 0.2m by 1m region on the sofa. The robot is initially randomly placed in the room.\nGround + Ball: The robot needs to fetch a ball on the ground. The ball is placed uniformly randomly on a 3m by 3m region on the ground.\nWe test our system and the three baselines on all four tasks. We measure the success rates and average time to completion across 10 trials per setting. Qualitative results can be found on the project website.\nReal-World Results. The real-world experiments, as summarized in Table II, demonstrate the effectiveness of our system compared to the three baselines. In the task involving navigating to a toy on a bed, our system achieved a 60% total first-attempt success rate, significantly outperforming the Go2 default controller and No Tracking baselines, both of which failed to complete the task. Go2 default controller fails to climb up high obstacles like beds and sofas, whereas No Tracking only generates an open-loop trajectory of commands and fails to compensate drifting in navigation and subsequent grasping. Our system's performance was close to that of teleoperation, with only a 20% gap in the first-attempt success rate. We find that though teleoperation can solve tasks perfectly given many attempts, the first-attempt success rates of teleoperation are only around 70-80% given an expert human operator. Similarly, in the task of fetching a bottle from a sofa with soft deformabile, our approach achieves a 60% success rate, close to teleportation. This tasks also demonstrates the robustness of our learned controller in walking on soft deformable surfaces. In the Ground + Ball task, which involves simpler navigation and grasping on flat terrain, our system achieving a 70% success rate, outperforming all baselines than teleoperation. In terms of average time to completion, our system consistently outperformed the baselines, completing tasks faster than both the Go2 Default and No Tracking methods. Notably, our system was also faster than teleoperation, particularly in the Ground + Ball task, where it completed the task in 23 seconds on average compared to teleoperation's 38 seconds. These results highlight the strength of our approach in achieving open-vocabulary object fetching in novel environments under a reasonable amount of time."}, {"title": "VII. CONCLUSION, LIMITATIONS & FUTURE DIRECTIONS", "content": "We presented Helpful DoggyBot, a quadrupedal robot system capable of zero-shot mobile manipulation in diverse indoor environments, integrating a 1-DoF gripper, learned whole-body control, and vision-language models. While our ap-proach demonstrates progress, limitations include the gripper's restricted dexterity, reliance on ceiling-mounted cameras for navigation, and potential occlusion to the perception system. In future work, we will focus on enhancing manipulation capa-bilities without compromising agility, developing navigation strategies using only onboard sensors, and future improving agility to achieve cheerful pet behaviors [85]. Additional places for improvement include integrating multiple tasks into complex sequences, improving robustness in dynamic envi-ronments, incorporating online learning and human feedback, and exploring societal implications of advanced quadrupedal robots in domestic settings. By addressing these challenges, this research direction has the potential to revolutionize human-robot interaction and assistance in daily life."}, {"title": "APPENDIX", "content": "A. Details of Whole-Body Controller\n1) Details of Simulation Environment\nTo ensure robust performance across diverse scenarios for the expert policy, we use Isaac Gym Preview 4 to train 6144 robots in 400 terrains. We introduce a curriculum learning approach which generates 10 different levels of stair heights in simulation. The criteria of updating the curriculum in training is the proportion of the terrain each episode the robot finishes. We randomly generate these environments for each training episode, varying parameters like stair height, number of stairs, and terrain friction as shown in Table III. We then distilled a policy with 384 robots in simulation with real-time depth image rendering. For Oracle policy, we trained 20k iterations in 10 hours on a single GeForce RTX 4090 GPU. For distilled policy, we trained 5k iterations in 6 hours."}, {"title": "B. Details of Zero-shot Deployment Using VLMs", "content": "As the robot approaches the target object, a transition to a precise grasping policy is triggered, allowing for more accurate command following. This transition is governed by the overhead camera and occurs when the robot is within 1 meter of the target object and oriented within 30 degrees of it. After successful grasping, the policy switches again once the robot is aligned within 30 degrees of the termination point."}]}