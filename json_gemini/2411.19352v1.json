{"title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational Recommendation", "authors": ["Se-eun Yoon", "Xiaokai Wei", "Yexi Jiang", "Rachit Pareek", "Frank Ong", "Kevin Gao", "Julian McAuley", "Michelle Gong"], "abstract": "In this paper, we present a systematic effort to design, evaluate, and implement a realistic conversational recommender system (CRS). The objective of our system is to allow users to input free-form text to request recommendations, and then receive a list of relevant and diverse items. While previous work on synthetic queries augments large language models (LLMs) with 1-3 tools, we argue that a more extensive toolbox is necessary to effectively handle real user requests. As such, we propose a novel approach that equips LLMs with over 10 tools, providing them access to the internal knowledge base and API calls used in production. We evaluate our model on a dataset of real users and show that it generates relevant, novel, and diverse recommendations compared to vanilla LLMs. Furthermore, we conduct ablation studies to demonstrate the effectiveness of using the full range of tools in our toolbox. We share our designs and lessons learned from deploying the system for internal alpha release. Our contribution is the addressing of all four key aspects of a practicable CRS: (1) real user requests, (2) augmenting LLMs with a wide variety of tools, (3) extensive evaluation, and (4) deployment insights.", "sections": [{"title": "1 Introduction", "content": "Imagine a user who wants to find new games but faces thousands to millions of options. Since trying out various games can be time-consuming, one may want to get recommendations simply by say-ing in natural language what they want to play. Examples of such user requests are depicted in Figure 1, where users express their unique needs through diverse expressions. A conversational rec-ommender system (CRS) that can take in such free-form requests and retrieve the most relevant items would greatly improve user experience in navigating through a vast choice of content.\nWhile there are many works on CRS [3, 18, 22, 38, 43, 55, 56], rarely do we see a system in practice. Even though large language models (LLMs) have been demonstrated to be effective in conver-sational movie recommendation [11, 33], LLMs alone cannot be directly applied to many industrial domains. One limitation of LLMs is their dependence on fixed parameters, which restricts their ability to handle a dynamic pool of items and integrate up-to-date world knowledge without the costly process of fine-tuning. Furthermore, LLMs exhibit high popularity bias, frequently recommending or addressing the most well-known items [11].\nThis work contributes to practicable CRS research through the following efforts. First, we collect a dataset of real user requests and recommendations. This distinguishes our work from papers that use synthetic queries generated from traditional user-item interac-tions [13, 16, 44]. Real user requests are more challenging to process than requests synthesized from templates due to their variety, un-structured nature, and subjective language [11, 51]. Second, in order to process such complex requests, we argue that a much larger num-ber of tools are required to augment LLMs for recommendations, compared to existing approaches that address synthetic queries with only 1-3 tools. For example, in real user requests, free-form ca-sual utterances (e.g., using 'ptfs' to refer to the game 'Pilot Training Flight Simulator') require specialized tools for processing, which is not necessary for synthetic requests that use clearly defined item names. Another example is handling complex conditions, such as a user who plays games on a PC and wants games to play with 7- and 10-year-old nephews who use tablets, and providing a list of liked and disliked games and reasons (see Figure 1). Using just a search API [4] or a lookup API [19] may be insufficient for handling such conditions; multiple tools are required to address factors such as games popular among age groups, device compatibility, and similar games search. While a large number of tools may initially seem daunting to implement, our tools are relatively generic (e.g., unlike tools that require reviews [16]) and can be easily constructed from databases and APIs available in many industry settings.\nThird, we propose OMULET (Orchestrating Multiple Tools), a framework for augmenting LLMs with diverse tools to meet com-plex requests. Our method translates a user's raw utterance into a formatted intent, applies a tool-execution policy, and then aug-ments the results to the LLM generating the recommendations (see Figure 3). This approach not only makes the system trans-parent and controllable, but it is more effective in performance than methods where an LLM generate its own tool execution pol-icy [13, 41]. Finally, we perform extensive evaluation on two LLMs (LLaMA-405B [1] and GPT-40 [29]) and 8 metrics covering factual-ity, relevance, novelty, and diversity. Our results show that using our framework is more effective than baseline LLMs, and multiple tools are necessary for the best performance. We implement our model for internal testing and share our insights for deployment. To the best of our knowledge, we are the first work to address all the following elements that are essential for a practicable CRS:\n\u2022 Real user requests. We use real user requests, which are more complex and diverse than queries synthesized from templates."}, {"title": "2 Problem Formulation", "content": "Given a user's recommendation request in free-form natural lan-guage, the agent should return a list of k items. The success of the task is measured by multiple criteria. First, the items should be relevant to the request; they should be what the user is asking for. An ideal approach to evaluate relevance is to get direct feedback from the user who made the request. However, in the early stages of model development, obtaining feedback for each iteration is imprac-tical. Thus, we construct an evaluation data as a proxy for relevance, which we discuss in Section 3.1. Another important criterion is that items should be novel, since the goal of recommendation closely tied to discovery [39]; we want to avoid recommending highly pop-ular items that often appear on the platform's front page. Finally, the collection of recommended items across all requests should have high coverage, ensuring a diverse range of recommendations. This breadth of visibility is especially crucial for the success of a platform that relies on millions of user-generated content."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Dataset", "content": "3.1.1 Requests. We identify a Reddit community /r/Roblox, where users discuss a wide range of topics about Roblox and its games. Here, we find that some posts are asking for Roblox game rec-ommendations. We sample the posts by using the Python Reddit API Wrapper (PRAW),\u00b9 using keyphrases such as \u2018recommend me games' and 'what games to play'. We further filter the posts by ask-ing GPT-3.5 [28] to judge whether the request is asking for game recommendations, and then removing the ones that are not. This process may still leave a handful of irrelevant posts, such as a game developer asking for recommendations on what game to make. As such, we manually remove the remaining irrelevant posts (73 out of 629 posts). We denote the resulting 556 posts as requests.\n3.1.2 Human Oracles. For each request, there are comments from other users in the community that recommend games that are relevant to the request. We regard these games as human oracles. When users mention game names, they may not precisely state the exact name, such as referring it with an acronym (e.g., 'MM2' instead of 'Murder Mystery 2') or dropping out parts of the name (e.g., 'Bloxburg' instead of 'Welcome to Bloxburg'). To handle this, we ask GPT-3.5 to extract any phrases that might be a game name (to ensure high recall), and link it to real game IDs using the Roblox search API2 (to ensure high precision).3 To ensure the quality of oracles, we measure community agreement through the net upvotes of comments. For each request, we keep games that have at least one net upvote and discard the rest. We obtain 553 requests with at least one oracle. There are 14.21(\u00b132.22) oracles per request and 2074 unique games in total.\n3.1.3 From Oracles to Ground-Truth Items. Human oracles may be noisy (i.e., some games are irrelevant) or insufficient (i.e., there"}, {"title": "3.2 Proposed Framework: OMULET", "content": "3.2.1 System Overview. Our system overview is depicted in Fig-ure 3. When a user submits a request, an LLM generates a dictionary summarizing the user's preference, denoted as the formatted in-tent. The formatted intent is given as input to the tool execution policy, which selects the tools and arguments to execute and re-turns an execution output in natural language. Note that the execution output is not the final recommendation; it contains the relevant information that would augment the LLM with external knowledge (e.g., item information) so that it generates better rec-ommendations. In the recommendation phase, both the raw request and execution output are provided to the LLM, which generates a list of game names. Each game is then linked to a real item in the Roblox database and displayed to the user.\n3.2.2 Formatted Intent Generation. While it is possible to make LLMs directly generate code policies for tool execution [23, 41], we later show that this approach is not effective for our task (Section 4). Furthermore, from an industry perspective, we want the system to be transparent (we can see how the system is operating), and controllable (we can easily control and fix how the system works). In this sense, we propose the following design: let the LLM first pro-cess the raw request into a formatted intent $D_{int}$, and execute a handcrafted policy P based on the formatted intent. This design has several practical benefits: (1) it allows us to view the intermedi-ate stage (formatted intent), helping us assess incoming requests and verify whether they are understood or parsed correctly; (2) instead of depending on LLMs for code generation-which can be a black box and have syntax errors-we rely on human experts for a better understanding and execution of tools; (3) it yields better performance than using LLM-generated policies. Specifically, we use the following prompt:\n\"Given a user's recommendation request, format the user's pref-erence into a JSON format. Fill in the following template of dict[str, dict[str, list]] with the relevant information accurately extracted from the user's request: <template> <demonstrations>\".\nThe  consists of preferences and user demographics, where each preference ('like' and 'dislike') contains four fields:\n\u2022 Genres: approximate game genres that do not need to match Roblox's official categories exactly\n\u2022 Game names: approximate game names that do not need to match Roblox's game names exactly\n\u2022 Properties: simple keyphrases describing the features or ele-ments of a game"}, {"title": "3.2.3 Toolbox", "content": "Our tools are Python functions, each performing a specific retrieval task that is potentially useful for recommendation. We present the entire list of tools in Table 1, along with each tool's input, output, and description. Tools are broadly classified into four categories: Lookup tools return simple game metadata from the Roblox database. Lookup tools can be used for informing LLMs with item knowledge (e.g., game descriptions), or filtering items based on attributes (e.g., compatible devices). Although some works propose to employ a single lookup tool by SQL query generation [44, 45], this method may not be suitable in many applications, including ours. For example, the database used in production may not be in a structure where LLMs can generate accurate and efficient SQL queries. Instead, we propose to have multiple simple tools for ac-cessing the database. Such design is also important to making the system transparent and controllable. Linking tools match game names and genres from user utterances to corresponding entities in the Roblox database. These tools are essential for handling real user requests where exact game IDs or genre categories are not used. Although implementing a drop-down list in the user inter-face [22] could bypass this issue, we believe it reduces engagement by requiring users to select from a list instead of typing naturally. Retrieval tools retrieve games that may be relevant to the user's request. For example, if a user references a game to express their preference, the similarity-search tools retrieve similar games using collaborative filtering (based on similar users) and game content (based on descriptions). While similar to candidate generators, rec-ommendations are not necessarily confined to the retrieved games. Instead, the purpose of retrieval tools is to make LLMs be 'aware' of the diverse items in the system instead of generating the most popular ones. Later we show that the absence of these tools results in much lesser diversity of recommended items. Formatting tools summarize the tool execution results into a natural language format, which would be provided in the prompt for the recommendation stage.\nNote that we do not use ranking tools. Instead of using a ranking tool to output the final recommendations [13], we let LLMs do the eventual recommendation by having them enumerate a list of items, as we later discuss in Section 3.2.5."}, {"title": "3.2.4 Tool Execution", "content": "We describe our tool execution policy P: $D_{int} \\rightarrow D_{aug}$ in Algorithm 1.7 The policy goes through each (key, value) in the formatted intent and runs the corresponding tools, adding information to $D_{aug}$ that would potentially be helpful to the recommendation stage. Then the policy goes through $D_{aug}$ again and filters items that can be sources of noise (e.g., games that are incompatible with the user's preferred devices). While we can skip the filtering and let the LLM disregard irrelevant items in the final recommendation stage, we find that simply filtering items in ad-vance improves recommendation performance. Finally, the policy uses the formatting tools to convert $D_{aug}$ into a readable format to be passed into the recommendation stage. For example,\n{'Users who played ido also played': [id1, id2,\u2026]}\nbecome\nUsers who played 'Da Amazing Bunker Simulator' also played:\n1. RetroStudio - Genre: Sandbox. This game allows players to create"}, {"title": "3.2.5 Recommendation", "content": "To generate high-quality recommenda-tions, a model needs to accurately understand complex and nuanced requests. LLMs excel in natural language understanding to such an extent that they surpass traditional, smaller models at conversa-tional recommendation [11]. As such, instead of having a separate tool (e.g., for ranking) to generate the final recommendations, we prompt an LLM with the raw request, tool execution output $D_{aug}$, and an instruction to generate a list of relevant items. This method"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Evaluation Metrics", "content": "We use multiple evaluation metrics for relevance, novelty, and cov-erage of recommended items. Additionally, since we are using LLMs as recommenders, we measure factuality to understand whether models are hallucinating."}, {"title": "4.1.1 Relevance", "content": "Hit@k evaluates whether a ground-truth item is included in the top-k recommendations. Precision@k is the proportion of ground-truth items in the recommendations. Simi-lar@k is the similarity of ground-truth items and recommended items by computing the cosine distance between the embedding centroids. In our work, we use SimCSE [5] embeddings obtained from item descriptions. We average each of the metrics across all requests."}, {"title": "4.1.2 Novelty", "content": "The concept of novelty in recommender systems can vary, but it is commonly linked to an item's popularity, such as the number of ratings it has received [14, 39]. In a similar vein, we use a metric that uses item popularity, where Pop50@k is the proportion of items in the top 50 most popular (or well-known) games, ranked by upvotes. Lower values are better since popular items are often listed on the Roblox front page, and our objective is to help users discover unfamiliar items. We also use RPop50@k, which computes the ratio of Pop50@k for the recommended items to that of the ground-truth items. Closer value to 1 indicates that the recommendations are as novel as the ground-truth items."}, {"title": "4.1.3 Coverage", "content": "Entropy@k measures the diversity of recom-mended items across all requests, formally computed by the follow-ing equation: Entropy@k = \u2013 \u03a3i pi log(pi), where pi is defined as the frequency of item i across the top-k recommendations. Higher entropy indicates a wider coverage of items [14, 30]. MaxFreq@k identifies the most frequently recommended item, and computes the proportion of requests that this item is recommended. For exam-ple, if 'Adopt Me!' appears in the top-10 list in 60% of the requests, then MaxFreq@10 is 0.60. A lower value is preferable, as it indicates that the system avoids recommending the same item repeatedly."}, {"title": "4.1.4 Factuality", "content": "Factual@k measures the proportion of real items in the top-k list. If the tool get_id_from_fuzzy_name returns noth-ing, we regard the game name as hallucinated. While factuality can be easily addressed by displaying only the actual items to the user, it remains an important metric for understanding model performance. We compute other metrics after filtering out hallucinated items."}, {"title": "4.2 Setup", "content": "4.2.1 LLMs. We use LLaMA-405B [1] and GPT-40 [29]. The temper-atures of LLMs are set to 0 for deterministic results. For simplicity, we use the same LLMs for formatting and recommendation. In practice, the two stages can be run by different LLMs.\n4.2.2 Baselines. Previous works (see Section 6) use zero-shot [11, 33, 52] or tool-augmented [13, 16, 19, 44, 47] LLMs for CRS. Since the tools in each paper are often domain-specific and difficult to apply in our work, we perform ablation tests to show the necessity of a large number of tools, which distinguishes us from existing methods. RAG-based approaches that retrieve similar queries from the training corpus [48] are also unsuitable for our setting due to a small volume of available queries (which we entirely use for evaluation). While some works fine-tune LLMs with traditional user-item data or synthetic queries [15, 54], we do not consider them as baselines since we want to incorporate external knowledge without the cost of fine-tuning. As such, our baselines are as follows:\n4.3 Results\nWe organize the results into multiple research questions. Results for Q1-3 are in Table 2, and the ablation study for Q4 is in Figure 4.\nQ1. Is OMULET more effective than base LLMs? OMULET outperforms base LLMs in all metrics for the human-annotated dataset (see Table 2). For the full dataset, OMULET outperforms base LLaMA-405B in all metrics and GPT-40 in all but Hit and Precision; this discrepancy could be attributed to the lack of accurate ground-truth items for the full dataset. Base LLMs have particularly poor novelty and coverage; LLaMA-405B recommends top-50 items \u00d73.19 more frequently than the ground-truths, and recommends the most frequent item ('Natural Disaster Survival') in 43% of requests. This is in contrast to OMULET, where LLaMA-405B recommends top-50 items only \u00d71.31 more than the ground-truths, and recommends the most frequent item in 10% of requests. While OMULET achieves near-perfect factuality (> 99%), base LLMs generate hallucinations among 21% (LLaMA-405B) and 11% (GPT-40) of top-10 recommendations.\nQ2. Is fixed P better than LLM-generated policies? We exper-iment to see if LLMs can generate their own policies, PLLM, per request using the same toolbox, to determine if they can create more effective, customized policies. We find that although LLMs generate reasonable policies, relevance metrics significantly drop"}, {"title": "Q3. Can we prompt LLMs to recommend more diverse items?", "content": "Base LLMs indeed generate more diverse items (higher novelty and coverage) when explicitly prompted to do so, but this leads to a"}, {"title": "Q4. Do we need all the tools?", "content": "Figure 4 shows the results of our ablation study, where we remove each tool to observe the im-pact on performance. We find that using all the tools generally improves relevance, with two unexpected results. One is that the performance of LLaMA-4050 significantly drops when any tool is omitted. A possible explanation is that augmenting with partial in-formation may mislead the model (e.g., by providing similar games but not age-relevant games). The model may also be sensitive to noise when the filtering tool is not used. Another interesting result is that dropping the search tool can slightly increase relevance (al-though at the notable cost of novelty and converage) for GPT-40. \u03a4\u03bf understand this, we examined the search tool's outputs. One issue is that the Roblox search API sometimes returns noisy results, such as retrieving low-quality games. But a more fundamental problem is that many user-described properties, such as 'sweet', 'not too horror', 'no progression', 'nice people', and 'unique premise', can be ambiguous or incompatible with search queries. OMULET is in-tended to handle such nuanced requests by letting LLMs understand the request holistically (e.g., 'sweet' as the game 'Oobja', or 'not too horror' as less intense than 'The Mimic') and use the provided game descriptions to match them with the request. However, their descriptions alone may not provide enough context to accurately match games with requests. One way to address this issue is to obtain descriptions of actual gameplay or user opinion, which we consider as future improvements. In terms of novelty and coverage, using all tools yields similar or better results than omitting any."}, {"title": "5 Deployment", "content": "To perform a feasibility study and identify the best implementation practices, we launch an internally hosted chatbot (see Figure 5). Our application is built on a full-stack server using Streamlit [37], which simplifies creating an interactive UI and managing backend operations. We deploy the application in an internal datacenter using HashiCorp's Nomad and Consul [9] for cluster orchestra-tion, deployment, and configuration. Several key areas are under evaluation to assess the feasibility of transitioning the chatbot to production. First is ensuring system safety by preventing irrelevant queries, policy violations, and jailbreak attempts (see Section 8). Second is latency and scalability. Our current chatbot takes several seconds per query to generate results. Further studies are necessary to understand user tolerance for latency and explore techniques to enhance inference efficiency at scale."}, {"title": "6 Related Work", "content": "Conversational Recommender Systems. There are two cate-gories of works based on the evaluation approach: interactive and dataset-based. In interactive evaluation, a user simulator replaces real users, and the problem is often framed into item or attribute selection for preference elicitation [3, 18, 38, 55]. In this approach, user simulators may fall short of reflecting real users [51]. Dataset-based evaluation recommends items given prior utterance. Most existing datasets are crowd-sourced [10, 22, 26], where workers role-play as seeker and recommender. Early works propose using two separate modules, language understanding and recommenda-tion, while more recent work suggests merging the two [43]. Most recently, zero-shot LLMs have shown to outperform all previous methods, especially for complex user utterances [11].\nLLMs for Recommendation. Since LLMs take language inputs, most recommendation task that uses an LLM inherently becomes 'conversational'. Often, queries are generated by inserting non-CRS datasets (e.g., user-item interactions) into templates [6, 8, 12, 15, 20, 42]. LLMs can be fine-tuned with such queries [27, 54], and further enhanced by incorporating collaborative filtering information dur-ing training [17, 49, 57, 59]. In contrast, our focus is on the user requests expressed in their own words, not bound in templates.\nTool-Augmented LLMs. Recent works explore using LLMs to create agents that can perform complex interactive tasks. Appli-cations include robotic control [2], scientific reasoning [24], and question answering [36, 50]. Solving such tasks often requires using tools [21, 31]. Tools are functions external to the LLM [45], and can help agents access external knowledge bases [7, 50], perform arithmetic operations [7, 34], use specialized models [25, 35], and interact with the world [40, 58]. Agents can even create simple tools and add them to the toolbox [46, 53]. Some works explore the possi-bility of having the agent generate a policy for using tools [23, 41], but we have shown in our experiments that using a fixed policy is more effective for our task."}, {"title": "Tool-Augmented LLMs for Recommendation", "content": "In recommen-dation, access to external knowledge is crucial because items are frequently added or removed, their information is updated (e.g., content updates or shifts in popularity), and external factors (e.g., seasonal demand) can influence user preference. As such, recent works propose tool-augmented LLMs for recommendation [13, 16, 19, 44, 47] to retrieve relevant information from an external knowl-edge base. From a practical perspective, we are faced with several limitations in directly applying this work: queries are often syn-thetic [13, 16, 44], which are different from real users; tools used in previous work are often unavailable in some use cases including ours, e.g., review-based item retrieval [16]. While demonstration papers [4, 16, 19] focus on implementing working systems, our work complements these efforts by providing extensive evaluation."}, {"title": "7 Conclusion", "content": "This work aims to advance practical conversational recommender systems by collecting a dataset of real user requests and propos-ing a novel approach to augmenting large language models with multiple tools. Our study includes comprehensive experiments and deployment insights. One limitation is we focus on game recommen-dations, which may not generalize to other domains. Additionally, the Reddit dataset may not fully represent all user types. As future work, we plan to develop models based on larger datasets."}, {"title": "8 Ethical Considerations", "content": "In developing our system, we prioritize ethical considerations, par-ticularly in the areas of fairness, diversity, and system integrity. To address fairness and diversity, the evaluation of our system is designed with native support for these principles, using carefully processed datasets and beyond-accuracy metrics to ensure equi-table recommendations across users and items. In terms of integrity, we implement dedicated modules to handle the following:\n\u2022 Jailbreak prevention: A mechanism to protect against exter-nal manipulation and unauthorized system exploitation.\n\u2022 Integrity verification: A mechanism to ensure the safety of recommendations and the words used by the conversational system, ensuring reliable outputs.\nWhile these measures significantly reduce the risks associated with fairness, diversity, and integrity, it is important to acknowledge that due to the inherent complexity of large language models, these issues cannot be entirely eliminated. The field is rapidly evolving, and ongoing research is essential to further refine and enhance these protections. In summary, our system incorporates robust solutions to address ethical concerns, though we recognize the need for continuous improvement as part of the broader research landscape."}]}