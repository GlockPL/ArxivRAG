{"title": "Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation", "authors": ["Jean-Pierre Sleiman", "Mayank Mittal", "Marco Hutter"], "abstract": "Reinforcement learning (RL) often necessitates a meticulous Markov Decision Process (MDP) design tailored to each task. This work aims to address this challenge by proposing a systematic approach to behavior synthesis and control for multi-contact loco-manipulation tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. We define a task-independent MDP to train RL policies using only a single demonstration per task generated from a model-based trajectory optimizer. Our approach incorporates an adaptive phase dynamics formulation to robustly track the demonstrations while accommodating dynamic uncertainties and external disturbances. We compare our method against prior motion imitation RL works and show that the learned policies achieve higher success rates across all considered tasks. These policies learn recovery maneuvers that are not present in the demonstration, such as re-grasping objects during execution or dealing with slippages. Finally, we successfully transfer the policies to a real robot, demonstrating the practical viability of our approach. For videos, please check: https://leggedrobotics.github.io/guided-rl-locoma/.", "sections": [{"title": "1 Introduction", "content": "Model-based optimal control techniques, such as trajectory optimization (TO) and model-predictive control (MPC), are valued for their inherent versatility. They can generate a range of easily interpretable behaviors, facilitating intuitive adjustments to the problem formulation [1, 2, 3]. However, these methods are sensitive to modeling mismatches and violation of assumptions. Conversely, reinforcement learning (RL) has demonstrated remarkable success in developing robust control policies for various contact-rich tasks, such as legged locomotion [4, 5], loco-manipulation [6, 7], and dexterous manipulation [8, 9]. Despite this, RL suffers from high sample inefficiency, the emergence of unnatural behaviors, and the need for labor-intensive reward design. In this work, we aim to leverage the complementary strengths of TO and RL to mitigate their weaknesses.\nWe build upon the versatile TO-based framework from Sleiman et al. [10], which demonstrates the discovery of long-horizon multi-modal behaviors for poly-articulated systems. This framework can safely and reliably solve complex loco-manipulation tasks, accounting for the multiple interaction regions in the object (handle, surfaces) and the robot (individual limbs). These behaviors are challenging to achieve with pure RL, even with an extensive task-specific design process. However, the successful execution of TO-based behaviors depends heavily on reasonably accurate environment models and minimal external disturbances. This reliance is due to the underlying MPC-based controller's strict adherence to tracking open-loop references without the ability to replan or respond to significant deviations, such as slippages. This limitation of the MPC controller motivates our work: using RL to obtain policies for the robust execution of contact-rich interaction plans.\nIn this paper, we propose a Markov Decision Process (MDP) to efficiently train robust loco-manipulation policies, particularly for articulated object interaction, with task-agnostic rewards and hyperparameter tuning. Our approach uses dynamically feasible demonstrations generated from a TO-based framework [10] to guide the RL agent in learning complex behaviors. By maintaining consistent MDP parameters and utilizing only one demonstration per task, we present an approach to train control policies that track the generated demonstrations while handling modeling uncertainties, external disturbances, and unforeseen events such as handle slippage. We benchmark our approach against prior motion imitation works on four loco-manipulation tasks: door pushing and pulling and dishwasher opening and closing. Finally, we deploy the trained policies on a quadrupedal mobile manipulator, showcasing their robustness to unknown object models and reactivity to slippages."}, {"title": "2 Related Work", "content": "Imitation learning has been successfully applied to various manipulation tasks through its algorithmic variants such as behavioral cloning (BC) [11], demo-augmented policy gradient [12, 13] (which bootstraps the RL policy through BC), and more recently, generative models [14, 15]. However, these approaches often struggle with distributional shifts and require substantial training data, posing challenges for high-dimensional robotic systems.\nAn alternate class of methods relies on state-only references to guide an RL agent toward desired behaviors. This approach commonly involves conditioning a low-level policy on references obtained either offline from a motion library or online by a separate high-level module. For instance, Bergamin et al. [16] incorporates a motion matcher that selects and blends the best-fitting motion clip from an extensive MoCap database based on handcrafted features. The works from Kang et al. [17] and Jenelten et al. [18] update the policy with on-demand \"optimal\" references that are computed in an MPC-like fashion. While MPC can provide precise guidance, its high computational demands can significantly slow down the RL training process.\nAnother common approach is inverse RL, where a reward function is defined through the demonstrations. Adversarial motion priors (AMP) [19, 20, 21] formulate a style reward that is maximized when the learned policy yields state transitions similar to those in the dataset. Subsequent variants [22, 23, 24] propose a hierarchical architecture to address the mode-collapse issue in adversarial learning setups. However, these methods still require task-specific goal-directing rewards and a large amount of data to learn the style reward. In contrast to AMP-style approaches, motion-imitation methods guide RL policies to robustly track target trajectories directly [25, 26, 27, 28]. They use reward terms that encourage adherence to the demonstrations while allowing exploration around the reference motions.\nOur work falls into the category of motion-imitation RL without any task-specific objectives. It closely relates to the work from Fuchioka et al. [26] and Bogdanovic et al. [27], as we aim to directly track and robustly stabilize offline trajectories generated using a fast trajectory optimizer. Differently from these works, we introduce domain-specific considerations relevant to multi-contact loco-manipulation tasks. Moreover, we propose a task-independent MDP formulation based on an"}, {"title": "3 Approach", "content": "Our proposed approach consists of two steps, as illustrated in Fig. 2. First, we employ the planner from Sleiman et al. [10] to generate whole-body multi-contact behaviors for various loco-manipulation tasks. Subsequently, we train a neural network using RL to reliably track these behaviors while leveraging only one pre-computed trajectory per task as an \u201cexpert demonstration\". We train this policy entirely in simulation with domain randomization to achieve a successful transfer to the real robot. At deployment, we assume access to the object's state. While this assumption limits the applicability in the wild, our primary focus is devising an MDP for learning robust behaviors across different loco-manipulation scenarios without requiring task-specific handcrafting."}, {"title": "3.1 Problem Formulation", "content": "We generate a single demonstration for each task using the multi-contact planner (MCP) from [10]. This planner takes the task specification and a set of user-defined object affordances (e.g. handle or surfaces) and the robot's end-effectors (e.g. the tool on the arm or the feet) for interaction as inputs. It then searches for possible robot-object interactions to provide a physically consistent demonstration based on a nominal robot and object model.\nThe demonstrations from the planner consist of the continuous robot and object state references $X^* = \\{x_t^*\\}_{t=1}^{T_{task}}$ and the manipulation schedule $M^* = \\{m_t^*\\}_{t=1}^{T_{task}}$ with $T_{task}$ being the demonstration's duration. In the case of articulated object interaction, the state reference $x_t^*$ includes the robot's base pose $(\\mathbb{I}_{r\\text{I}B}, \\Phi_{\\text{I}B})$ in a fixed inertial frame I, the robot's joint positions $q_j$, and the object's joint positions $q_o$. The contact mode $m_t^*$ specifies the interaction type for each end-effector (none, prehensile, or non-prehensile) and the object contacts involved at that timestep. Since the demonstrations for each task have varying lengths, we encode the notion of time into the task-phase $\\Phi \\in [0, 1]$, where $\\Phi = 0$ and $\\Phi = 1$ imply the start and end of the demonstration, respectively [25].\nThe objective of the learning agent is to track these references while staying robust against variations in the kinematic and dynamic properties of the object, external disturbances, and unforeseen events such as slippages. In the na\u00efvest way, one could increase the task phase linearly with time and track the corresponding references [25, 26]. However, as we show later in Sec. 5.1, this approach limits the success rate during object manipulation. For instance, if the door handle slips away and the door closes, strictly following the references will cause the robot to get stuck behind the door. Instead, we want to give the robot the time to recover and, thus, grow the task phase adaptively."}, {"title": "3.2 Adaptive Task Phase Dynamics", "content": "Linearly evolving the task phase with time (also referred to in this paper as the \u201cnominal\" formulation) can be written as: $\\Phi_{t+1} = \\Phi_t + \\frac{\\tau_k}{T_{task}}dt$, where $\\Phi$ denotes the nominal phase and $dt$ is the environment time-step. As motivated earlier, we seek an adaptive mechanism that adjusts the phase $\\Phi$ depending on the current robot and object states. To this end, we propose the phase dynamics: $\\Phi_{t+1} = \\Phi_t + \\nu dt$, where the task phase rate $\\nu = \\hat{\\nu} + \\sigma_1\\cdot \\delta \\nu_{,t}$ is determined by a state-dependent reference $\\hat{\\nu} := \\nu(x,x^*, m^*)$ and a learnable residual term $\\delta \\nu$ (scaled by $\\sigma_1 > 0$). We opt for a reward-dependent $\\hat{\\nu}$ that reflects the task-level tracking accuracy:\n$\\nu(x, x^*, m^*) = \\prod_{i=1}^{L} [ \\text{min} (k\\cdot\\text{exp}(-\\lambda) \\cdot r_{i,t}(x, x^*, m^*)),1]$, (1)\nwhere $k > 1$ and $\\lambda < 0$ are fixed constants, and $L$ is the number of task-level rewards $r_{i,t}$, with $r_{i,t} \\geq 0$ at all time-steps. For articulated object interaction, $L = 3$ with terms corresponding to the tracking rewards for the robot's base pose and the object's joint positions, as specified in Sec. 3.3.\nIntuition behind $\\hat{\\nu}$: The term $\\hat{\\nu}$ effectively pauses the phase evolution for large deviations from the current reference. As the tracking improves, it gradually approaches the nominal phase rate ($\\rightarrow 1$). Since adhering strictly to the reference may not always be the best strategy to complete the task, we introduce a clipping in Eq. 1 to encode perfect tracking only within a certain margin defined by $k$ and $\\lambda$. This operation is particularly useful when handling discrepancies between the nominal model used by the planner and the real model employed during training and deployment. The resulting task phase dynamics with $\\hat{\\nu}^\\phi$ induces a curriculum-like effect during training. At the start of the training, when the tracking is poor, it helps the agent learn to recover to a single reference state. Eventually, as the agent improves, it learns to follow more of the demonstration.\nIntuition behind $\\delta\\nu$: In some instances, unforeseen slippage or large disturbances could render the object uncontrollable due to a complete loss of contact, resulting in significant deviations from the reference pose. In these situations, the term $\\hat{\\nu} \\rightarrow 0$, and the robot cannot recover. To enable motion recovery in such scenarios, we introduce a residual phase $\\delta \\nu$ that allows potentially speeding up, slowing down, and even decreasing the phase whenever necessary. This residual phase is outputted from the policy, allowing it to adapt to the task phase dynamics via learnable parameters."}, {"title": "3.3 Rewards", "content": "To design task-agnostic reward functions such that loco-manipulation behaviors in the generated demonstrations can be learned through the same MDP formulation, we split the rewards into three parts: reference-tracking rewards, task progress rewards, and penalties for smooth motions.\nReference-tracking reward: These are simply defined through the tracking errors between the current state $x$ and the reference state $x^*$ from the demonstration:\n$r_{\\text{track}} = w_1\\cdot||\\mathbb{I}_{r\\text{I}b} - \\mathbb{I}_{r\\text{I}B}^*||^2 + w_2\\cdot ||\\Phi_{\\text{I}B} - \\Phi_{\\text{I}B}^*||^2 + w_3 \\cdot ||q_j - q_j^*||^2 + w_{4.1_{\\text{object}}} ||q_o - q_o^*||^2 + w_5 \\cdot 1_{\\text{prehensile}} \\cdot ||\\mathbb{I}_{T\\text{I}E} - \\mathbb{I}_{T\\text{I}H}||^2$, (2)\nwhere $w_i < 0$, H is a non-fixed handle frame that is initialized at I and moves with the object, and $1^* := 1_{m^*}(m^*)$ denotes indicator functions that depend on the reference manipulation mode $m^*$.\nThe indicator functions help avoid incurring irrelevant penalties that do not coincide with the behavior's mode schedule. The function $1_{\\text{object}}$ is true only when at least one end-effector is either already in contact or is establishing contact with the object. Essentially, it activates the object tracking error only when the object's state is controllable and essential for the task completion. Meanwhile, $1_{\\text{prehensile}}$ is true when a prehensile interaction is active. This choice helps improve the accuracy of prehensile contact and is particularly useful when we randomize the object models during training, as the location of the object frame H varies.\nTask progress reward: This reward term encourages the learning agent to progress in the task. It is defined as: $r_{\\Phi} = \\kappa_1\\cdot [\\text{exp}(-\\kappa_2\\cdot ||\\hat{\\Phi} - \\Phi||^2)]$, where $\\kappa_1, \\kappa_2 > 0$, and $\\hat{\\Phi}$ and $\\Phi$ are the reference task-phase rate and the nominal phase from Sec. 3.2. When the tracking is poor, $\\hat{\\nu} \\rightarrow 0$ and the agent gets less reward for the task progress, i.e., $r_{\\Phi} \\rightarrow 0$. Consequently, the agent gets encouraged to maximize the reference tracking reward and minimize the tracking deviation.\nPenalties: These terms penalize the robot's joint accelerations and torques, base velocity in unde-sired directions, and abrupt action changes. Since these are standard penalties used in RL, we skip detailing them here for space reasons and refer the reader to Appendix C.2 for further details."}, {"title": "3.4 Observation and Action Spaces", "content": "Observations: The observations comprise the tracking errors in the robot and object states, the po-sitions and velocities of all end-effector frames participating in prehensile interactions, the previous action, and the task-phase parameters. For a legged mobile manipulator with only one prehensile end-effector E on the arm, the observations can be denoted as $o_t = (O_e O_v O_p a_{t-1} O_{\\Phi})$, with:\n$O_e = \\begin{bmatrix} \\mathbb{I}_{TIB} - \\mathbb{I}_{rIB}^* \\\\ \\Phi_{IB} - \\Phi_{IB}^* \\\\ q_j - q_j^*\\\\q_o - q_o^* \\end{bmatrix}, O_v = \\begin{bmatrix} \\mathbb{B}_{UIB} \\\\ \\mathbb{B}_{WIB} \\\\ \\dot{q_j} \\\\  \\dot{q_o} \\end{bmatrix}, O_p = \\begin{bmatrix} \\mathbb{I}_{TIE} \\\\ \\mathbb{I}_{VIE} \\\\ \\Phi \\end{bmatrix}$  (3)\nThe choice of the observation terms $o_e$ and $o_v$ resemble that of a classical PD control law, where $o_e$ captures the position tracking error and $o_v$ is the velocity tracking error with zero velocity targets. Effectively, the learned policy is a more complex tracking controller around the demonstrations. Additionally, while including $o_p$ is not essential, we notice that having it improves the training as it eliminates the need to learn the mapping from the robot's state to operational space quantities.\nActions: The actions $a_t = (a_{q_j,t}, \\delta \\nu_{,t})$ are interpreted as the residuals over the robot's reference joint positions $q_j^*$ and the reference phase rate $\\hat{\\nu}$ from Sec. 3.2. The robot's actions are sent to its actuators as joint position commands: $q_j^{cmd} = q_{j,t}^* + \\sigma_2\\cdot a_{q_j,t}$, with $\\sigma_2 > 0$. While the planner [10] also provides reference joint velocities, we observed that including them in feed-forward control resulted in poorer performance. This finding aligns with that from Fuchioka et al. [26]."}, {"title": "3.5 Training Setup", "content": "Initial Distribution: At environment resets, we apply the reference state initialization (RSI) strategy from imitation-based RL setups [25, 26]. In RSI, we randomly sample the initial phase $\\Phi_{init} \\in [0,1]$ and spawn the robot and object at their corresponding reference states. However, differently from RSI, we randomize the robot configuration uniformly around the reference state when $\\Phi_{init} = 0$. This alleviates the need to always start the robot at the initial reference state at deployment and lets the agent learn the necessary recovery actions to stay near the demonstration.\nThe learning task is then to complete the remainder of the task from these varying initial states. This approach exposes the policy to regions of the state space that are relevant to the reference behavior and are hard to reach independently. Additionally, it removes the need for the episode length to be at least as long as the demonstration $T_{task}$.\nDomain Randomization (DR): Unlike [10], where the MPC-based tracking controller's success relies on a reasonable knowledge of the environment, we aim to make the learned policy robust to modeling uncertainties and disturbances through DR. This involves varying the properties of the robot (such as friction and base mass), the kinematic and dynamic models of the object (such as the door dimensions and spring loading parameters), and adding regular external disturbances to the system. For further details, please refer to Appendix C.4\nCurriculum: We introduce a curriculum that incrementally increases the external disturbances, observation noise, reward penalties, and initialization offsets. The curriculum level $l_{rand} \\in [0, 1]$ is updated according to the following rule:\n$l_{rand}^{k+1} = \\begin{cases} l_{rand}^k + 0.25 & \\text{if } p_\\Phi > 0.95 \\\\l_{rand}^k - 0.25 & \\text{if } p_\\Phi < 0.75\\end{cases}, \\text{ with } p_{\\Phi} = \\frac{\\Phi - \\Phi_{init}}{\\hat{\\Phi} - \\Phi_{init}}$ (4)\nThe term $p_\\Phi$ represents the progress of the task phase relative to the nominal one. As the agent becomes proficient in tracking the demonstrations, it progresses to a higher difficulty level with larger DR and penalties. Conversely, if its performance is poor, it moves to a lower level."}, {"title": "4 Experimental Setup", "content": "We validate our framework on a legged mobile manipulator consisting of the quadruped ANYmal-D [29] with a 6-DoF robotic arm [30]. We consider four tasks: Door Push/Pull: traversing a spring-loaded push or pull door, and Dishwasher Open/Close: opening or closing a heavy dishwasher."}, {"title": "5 Results and Analysis", "content": "Comparisons: We evaluate each learned policy in 4096 environments with randomized robot and object properties and random pushes. We report the success rates across the four tasks in Table 1. We observe that our proposed method performs significantly better than other motion-imitation RL setups, even without including the residual phase action $\\delta \\nu$. This result highlights the significance of integrating the reward-driven adaptive phase dynamics. The nominal phase update can reliably accomplish the relatively straightforward task of navigating through push doors. However, it struggles with more complex tasks, achieving notably lower success rates for door pulling. Meanwhile, the NNS strategy fails across all the tasks. While experimenting with various training setups to incorporate NNS, we consistently found the phase stuck at certain points along the reference path. We hypothesize this occurs because the policy exploits the NNS update mechanism to achieve a state where the phase remains unchanged, thereby maximizing the reference tracking reward.\nAblations: To evaluate the need for demonstrations as whole-body trajectories, we omit the robot's joint-level references, relying on partial task-level references that pertain solely to the floating base"}, {"title": "5.2 Real-World Deployment", "content": "We demonstrate the proposed approach on all four tasks on hardware. A motion capture system provides the handle location at the start and the joint angle for the door and the dishwasher. For the robot, we rely on its onboard state estimation. For each task, we perform six continuous runs, where we manually reset the object, place the robot randomly in front of the object, and execute the policy. As shown in Fig. 6, the deployed policies exhibit smooth motions and preserve the multi-contact behavior from the demonstration. They complete all the tasks six times in a row. Additionally, the policies yield different motions every time, which is particularly noticeable during traversing a pull door. They are able to handle random base placement, recover from slippages, and re-grasp the handle when it misses it. Please refer to the supplementary video to observe these behaviors."}, {"title": "5.3 Limitations", "content": "On hardware deployment, we notice that if the robot starts too far away or too close to the object, it aggressively tries to adjust its configuration. This, at times, led to the robot falling down. However, in practice, a navigation planner can appropriately place the robot in front of the object, so only local adjustments are needed, which the learned policies can handle. Additionally, while the policies can handle large intra-object category variations, they may still fail on certain object instances. For instance, if the door is too small, the policy fails to traverse through it as it collides with the door. In such cases, we need to use more demonstrations for the training. Lastly, a natural extension of this work is to devise perceptive loco-manipulation policies that rely on onboard sensing [5, 7]."}, {"title": "6 Conclusion", "content": "Our work integrates model-based TO and model-free RL to develop robust tracking policies for multi-contact loco-manipulation tasks. Central to our approach is a task-agnostic MDP formulation that utilizes loco-manipulation demonstrations generated by a precise trajectory planner. Unlike previous imitation-based RL methods that depend on time-based nominal phase updates, we showed that our state-dependent adaptive phase dynamics facilitate successful task execution despite model-ing inaccuracies and significant external disturbances. We validated our framework on a quadrupedal mobile manipulator performing four complex long-horizon tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. Additionally, we demonstrated that the learned policies transfer to hardware successfully and can effectively recover from slippages and missed handles, overcoming the limitations of the MPC-based tracking controller from [10]."}, {"title": "A Notations / Symbols", "content": null}, {"title": "B Demonstrations from the Loco-Manipulation Planner", "content": "The loco-manipulation planner from Sleiman et al. [10] efficiently generates physically con-sistent demonstrations for our proposed framework. The planner relies on a high-fidelity model that integrates the robot's full centroidal dynamics and first-order kinematics with the object's full dynamics [34]. This helps ensure that the discovered behaviors are dynami-cally feasible. The planner outputs the following the continuous states and system inputs: x := (xrx) = (hcom qb_qjqoq\uff61) and u = (we qj), where the robot state \u00e6r includes the centroidal momentum hcom, base pose qb, and joint positions qj, whereas the object state xconsists of its generalized coordinates qo and velocities qo. The control input u is composed of the robot's joint velocities q; and the contact wrenches we acting at the robot's end-effectors."}, {"title": "CMDP Formulation and PPO Training", "content": "This section summarizes the terms that formulate the proposed MDP and the learning algorithm."}, {"title": "C.1 Observation Terms", "content": "Table 4 specifies the observation terms and the noise added to them during training. The critic ob-tains the same observations as the actor but without any noise applied. Importantly, for the adaptive-phase dynamics formulation, the previous action at-1 comprises both the robot commands \u0101t\u22121 and the residual phase \u03b4\u03c5."}, {"title": "C.2 Reward Terms", "content": "To design a task-agnostic reward function, we split the reward function into generic reference-tracking terms that stabilize the open-loop trajectories and standard penalty terms that ensure smooth motions: total = rtrack + Irand \u00b7 rregularize, where lrand is the curriculum factor from Sec. 3.5. For the adaptive phase formulation, the reward also includes the task progress: rtotal = total + r\u00a2. The individual terms are:\nptrack = W1\u00b7||ITIB - Ir1B||\u00b2 + W2 \u00b7 ||\u0424\u0406\u0412 \u0424\u0406\u0412||\u00b2 + W3 \u00b7 ||qj \u2013 q5||\u00b2\n+ W41object ||9o - qo||\u00b2 + W5 \u00b7 1prehensile \u00b7 ||ITIE \u2013 ITIH||\u00b2,\nr = K\u2081\u00b7 [\u00ee \u00b7 exp(-\u043a\u2082\u00b7 || - ||\u00b2)],\nrregularize = B1 \u00b7 ||T||\u00b2 + \u03b22 \u00b7 ||\u00b2|| \u0632\u0646 + \u03b23 \u00b7 ||vj||\u00b2 + \u03b24 \u00b7 ||BV\u1fd2B||\u00b2 + \u03b25\u00b7 || B||\u00b2\n+B6||at-at-1||\u00b2,\nwhere symbols have their meanings from Table 2. The corresponding weights are in Table 5."}, {"title": "C.3 Initial-State Distribution", "content": "We apply additive offsets to the robot's reference configuration at init = 0 so that the policy is robust to varying initial locations of the robot in front of the door. Ideally, we would like to apply this at any randomly sampled ; doing so is non-trivial due to the difficulty in filtering invalid collision configurations.\nAt init = 0, the initial pose of the robot is computed as a uniform noise around the corresponding pose in the provided demonstration demonstration. These perturbations are scaled linearly based on the curriculum factor lrand from Sec. 3.5. The maximum sampling distribution corresponding to the xy- position and the yaw orientation are U(-0.5,0.5) and U(-\u03c0/6, \u03c0 6) respectively."}, {"title": "C.4 Domain Randomization", "content": "Domain randomization helps mitigate overfitting to specific models and addresses inherent unmod-elled effects by introducing variability during training. In our setup, it takes the following form:"}, {"title": "C.5 Termination Term", "content": "We trigger episode termination when the robotic system loses balance or episode length times out. Typically, we infer a fall from a significant force acting on the robot's base, indicating ground con-tact. However, distinguishing the source of this force becomes challenging during loco-manipulation tasks, as contact between the robot's base and an object is both expected and sometimes permissible. Thus, we rely on the base to not drop below 0.3 m to correctly detect falls."}, {"title": "C.6 Adaptive-Phase Hyperparameters", "content": "This section lists the hyperparameters for the remainder of the MDP formulation in Table 6. These include parameters for the input actions scaling and the adaptive-phase formulation from Eq. 1."}, {"title": "C.7 Learning Algorithm", "content": "For each task, we train the policy using the on-policy RL algorithm, Proximal Policy Optimization (PPO) [32]. The actor and critic networks are designed as a Multi-Layer Perceptron (MLP) with a [256 \u00d7 128 \u00d7 64] hidden-layer structure and an ELU activation function. A complete list of hyperparameters and their values is specified in Table 7."}, {"title": "D Supplementary Discussions", "content": null}, {"title": "D.1 Object Locking Mechanism", "content": "Our policy is trained on doors where handles are treated as fixed object-attached links. However, a typical door can only be opened after being unlocked using its handle. By adapting our environment"}, {"title": "D.2 Unknown Object Type", "content": "In real-world scenarios, we expect the robot to autonomously traverse diverse doors without speci-fying the type of door (i.e., push or pull door). One way to achieve this objective would be to first train a multi-task policy that can execute both door-traversal behaviors and then separately train a door-type estimator that outputs the appropriate task command to the policy."}, {"title": "D.3 Unknown Object State", "content": "In the current hardware experiments, we rely on explicit sensors such as door encoders to obtain the joint position of the panel. However, in more realistic scenarios, this information needs to be extracted from the robot's onboard sensors. One mechanism to achieve this goal is by leveraging teacher-student training and treating the current policy as the teacher policy [5]."}, {"title": "D.4 Applicability to Other Scenarios", "content": "In this work, we demonstrated the approach to multi-contact tasks that primarily involved articulated object manipulation. However, there is an open question on the generalization of the approach to other tasks and robots. For tasks where a single demonstration is sufficient to solve the task, we be-lieve our method should work in these scenarios. However, for tasks where a complete re-planning is necessary (for instance, the rearrangement of objects), training an online replanner becomes nec-essary to provide new references for the policy to track. We leave this as part of our future work."}]}