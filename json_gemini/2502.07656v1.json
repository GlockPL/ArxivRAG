{"title": "A Unifying Framework for Causal Imitation Learning with Hidden Confounders", "authors": ["Daqian Shao", "Thomas Kleine Buening", "Marta Kwiatkowska"], "abstract": "We propose a general and unifying framework for causal Imitation Learning (IL) with hidden confounders that subsumes several existing confounded IL settings from the literature. Our framework accounts for two types of hidden confounders: (a) those observed by the expert, which thus influence the expert's policy, and (b) confounding noise hidden to both the expert and the IL algorithm. For additional flexibility, we also introduce a confounding noise horizon and time-varying expert-observable hidden variables. We show that causal IL in our framework can be reduced to a set of Conditional Moment Restrictions (CMRs) by leveraging trajectory histories as instruments to learn a history-dependent policy. We propose DML-IL, a novel algorithm that uses instrumental variable regression to solve these CMRS and learn a policy. We provide a bound on the imitation gap for DML-IL, which recovers prior results as special cases. Empirical evaluation on a toy environment with continues state-action spaces and multiple Mujoco tasks demonstrate that DML-IL outperforms state-of-the-art causal IL algorithms.", "sections": [{"title": "1 Introduction", "content": "Imitation Learning (IL) has emerged as a prominent paradigm in machine learning, where the objective is to learn a policy that mimics the behaviour of an expert by learning from its demonstrations. While classical IL theory implies that, with infinite data, the IL error should converge to zero and the imitator should be value-equivalent to the expert (Ross et al., 2011, Spencer et al., 2021), it has been observed in practice that IL algorithms often produce incorrect estimates of the expert policy, leading to suboptimal and unsafe behaviours (Lecun et al., 2005, Codevilla et al., 2019, Bansal et al., 2018, Kuefler et al., 2017).\nVarious attempts have been made to explain the cause of these problems. Previous work has studied spurious correlations within the dataset (de Haan et al., 2019, Codevilla et al., 2019, Pfrommer et al., 2023), temporal noise (Swamy et al., 2022a), the case where the expert has additional information or knowledge (Swamy et al., 2022b, Vuorio et al., 2022, Chen et al., 2019, Choudhury et al., 2017), causal delusions (Ortega and Braun, 2008, Ortega et al., 2021) and covariate shifts (Spencer et al., 2021). However, each of these works considers only a specific aspect of the problem in different settings, and a holistic treatment of the problem and discussion of the connections between these settings is still missing from the literature. In the real world, it is often the case that multiple problems are present simultaneously (e.g., the expert has privileged information and the observed demonstrations are confounded). However, as we demonstrate later, addressing these problems partially or independently is insufficient, and a unified approach is necessary.\nOur key observation is that, in fact, all the above settings can be formalised within a unifying framework by considering hidden confounders, which are variables present in the environment but not recorded in the demonstrations. Importantly, we distinguish between hidden confounders that can and cannot be observed by the expert.\u00b9 When the expert fully observes the hidden confounders, but the imitator does not, additional information is available to the expert. When the expert also cannot observe the hidden confounders, these hidden confounders act as confounding noise that contaminates the demonstrations, causing spurious correlations and causal delusions. By considering the hidden confounders to include expert-observable and"}, {"title": "1.1 Related Works", "content": "Imitation Learning. Imitation learning considers the problem of learning from demonstrations (Pomerleau, 1988, Lecun et al., 2005). Standard IL methods include Behaviour Cloning (Pomerleau, 1988), Inverse RL (Russell, 1998), and adversarial methods (Ho and Ermon, 2016). Interactive IL (Ross et al., 2011) extends standard IL by allowing the imitator to query an interactive expert, facilitating recovery from mistakes. However, in this paper, we do not assume query access to an interactive expert.\nCausal Imitation Learning. Recently, it has been shown that IL from offline trajectories can suffer from the existence of latent variables (Ortega et al., 2021), which cause causal delusion. This can be resolved by learning an interventional policy. Following this discovery, various methods (Vuorio et al., 2022, Swamy et al., 2022b) considered IL when the expert has access to the full hidden context that is fixed throughout each episode, but the imitator does not observe the hidden context. They aim to learn an interventional policy through on-policy IL algorithms that require an interactive demonstrator and/or an interactive simulator (e.g., DAgger (Ross et al., 2011)).\nOrthogonal to these works, Swamy et al. (2022a) consider latent variables not known to the expert, which act as confounding noise that affects the expert policy, but not the transition dynamics. To address this challenge, the problem is then cast into an IV regression problem. Our work combines and generalises the"}, {"title": "2 Preliminaries: Instrumental Variables and Conditional Moment Restrictions", "content": "We first introduce the concept of Instrumental Variables (IVs) and its connection to Conditional Moment Restrictions (CMRs). Consider a structural model that specifies some outcome Y given treatments X:\n\\begin{equation}\nY = f(X) + \\varepsilon(U) \\text{ with } \\mathbb{E}[\\varepsilon(U)] = 0,\n\\end{equation}\nwhere U is a hidden confounder that affects both X and Y so that $\\mathbb{E}[\\varepsilon(U) | X] \\neq 0$. Due to the presence of this hidden confounder, standard regressions (e.g., ordinary least squares) generally fail to produce consistent estimates of the causal relationship between X on Y, i.e., $E[Y | do(X)] = f(X)$, where do(\u00b7) is the interventional operator (Pearl, 2000). If we only have observational data, a classic technique for learning f is IV regression (Newey and Powell, 2003). An IV Z is an observable variable that satisfies the following conditions:\n\u2022 Unconfounded Instrument: $Z \\amalg U$;\n\u2022 Relevance: $P(X | Z)$ is not constant in Z;\n\u2022 Exclusion: Z does not directly affect Y: $Z \\amalg Y \\mid (X, U)$.\nUsing IVs, we are able to formulate the problem of learning f into a set of CMRs (Dikkala et al., 2020), where we aim to solve for f satisfying\n$\\mathbb{E}[Y - f(X) | Z] = 0$."}, {"title": "3 A Unifying Framework for Causal Imitation Learning", "content": "MDPs with Hidden Confounders. In this section, we introduce a novel unifying framework for causal IL in the presence of hidden confounders. We begin by introducing an Markov Decision Process (MDP) with hidden confounders, (S, A,U,P, r, \u03bc\u03bf, T), where S is the state space, A is the action space and U is the confounder space. Importantly, parts of the hidden confounders ut may be available to the expert due to imperfect environment logging and expert knowledge. We model this by segmenting the hidden confounder into two parts $u_t = (u^o_t, u^f_t)$, where $u^o_t$ are observable to the expert and $u^f_t$ are not. Intuitively, $u^o_t$ are additional information that only the expert observes and $u^f_t$ behave as confounding noise in the environment that affects both the state and action. As a result, the transition function $P(\\cdot | s,a, (u^o, u^f))$ depends on both hidden confounders but the reward function r(s, a, u\u00ba) only depends on the state, action and the observable confounder u\u00ba since the confounding noise only directly affects the state and actions. Finally, \u03bc\u03bf is the initial state distribution and T is the horizon of the problem. We provide a causal graph that illustrates the relationships between variables in Figure 1.\nCausal Imitation Learning. We assume that an expert is demonstrating a task following some expert policy \u03c0E (which we will specify later) and we observe a set of N > 1 expert demonstrations {d1, d2, ..., dN}. Each demonstration is a state-action trajectory $(s_1, a_1,\\ldots, s_T, a_T)$, where, at each time step, we observe the state st and the action at taken in the environment, and the trajectory follows the transition function $P(\\cdot | s_t, a_t, (u^o_t, u^f_t))$. Denote $h_t = (s_1, a_1, ..., s_{t-1}, a_{t-1}, s_t) \\in H$ as the trajectory history at time t, where $H \\subseteq \\cup_{T=1}^{\\infty} (S\\times A)^{T-1} \\times S$ is the set of all possible trajectory histories at different time steps. Importantly, we do not observe the reward and the sequence of confounders $(u^o_t, u^f_t)$.\nGiven the observed trajectories, our goal is to learn a history-dependent policy $\u03c0_\\hbar: H \\rightarrow \\Delta(A)$. We assume that our policy class \\Pi is convex and compact. The Q-function of a policy $\u03c0_\\hbar \\in \\Pi$ is defined as"}, {"title": "4 Causal IL as CMRS", "content": "In this section, we demonstrate that performing causal IL in our framework is possible using trajectory histories as instruments. In the next step, we show that the problem can be described as CMRs and propose an effective algorithm to solve it.\nThe typical target for IL would be the expert policy \u03c0E itself. However, since the expert has access to information, namely u\u00ba, which the imitator does not, the best thing an imitator can do is to learn a history-dependent policy \u03c0\u0127 that is the closest to the expert. A natural choice is the conditional expectation of \u03c0E(st, u\u00ba) on the history ht:\n\\begin{equation}\n\u03c0_\\hbar(h_t) := \\mathbb{E}_{P(u^o_t|h_t)}[\u03c0_E(s_t, u^o_t)] = \\mathbb{E}[\u03c0_E(s_t, u^o_t) | h_t],\n\\end{equation}\nbecause the conditional expectation minimizes the least squares criterion (Hastie et al., 2001) and \u03c0\u0127 is the best predictor of \u03c0E given ht. In \u03c0\u0127, the distribution P(u\u00ba | ht) captures the information about u\u00ba that can be inferred from trajectory histories.\nRemark 4.1. Learning \u03c0\u0127 is not trivial. Policies learnt naively using behaviour cloning (i.e., $\\mathbb{E}[a_t | h_t]$) fail to match \u03c0E. In view of Equation (2), we have that\n\\begin{equation}\n\\mathbb{E}[a_t | h_t] = \\mathbb{E}[\u03c0_E(s_t, u^o_t) | h_t] + \\mathbb{E}[u^f_t | h_t]\n= \u03c0_\\hbar(h_t) + \\mathbb{E}[u^f_t | h_t],\n\\end{equation}\nwhere $\\mathbb{E}[u^f_t | h_t] \\neq 0$ due to the spurious correlation between $u^f_t$ and the trajectory history ht. As a result, $\\mathbb{E}[a_t | h_t]$ becomes biased, which can lead to arbitrarily worse performance compared to \u03c0E.\nDerivation of CMRs. Leveraging the confounding horizon from Assumption 3.2, it becomes possible to break the spurious correlation using the independence of $u^f_t$ and $h_{t-k}$. We propose to use the k-step trajectory history $h_{t-k} = (s_1, a_1, \\ldots, s_{t-k})$ as an instrument for the current state st. Taking the expectation conditional on $h_{t-k}$ in Equation (3) yields\n\\begin{equation}\n\\mathbb{E}[a_t | h_{t-k}] = \\mathbb{E} [\\mathbb{E}[a_t | h_t] | h_{t-k}]\n= \\mathbb{E}[\u03c0_\\hbar(h_t) | h_{t-k}] + \\mathbb{E}[\\mathbb{E}[u^f_t | h_t] | h_{t-k}]\n= \\mathbb{E}[\u03c0_\\hbar(h_t) | h_{t-k}] + \\mathbb{E}[u^f_t | h_{t-k}]\n\\end{equation}\nwhere we use the fact that ht\u2212k is \u03c3(ht)-measurable because ht\u2212k \u2286 ht. Next, recall that $u^f_t \\amalg h_{t-k}$ by Assumption 3.2, which implies $u^f_t \\amalg h_{t-k}$, so that\n\\begin{equation}\n\\mathbb{E}[a_t | h_{t-k}] = \\mathbb{E}[\u03c0_\\hbar(h_t) | h_{t-k}] + \\mathbb{E}[u^f_t]\n= \\mathbb{E}[\u03c0_\\hbar(h_t) | h_{t-k}].\n\\end{equation}\nAs a result, the problem of learning \u03c0\u0127 reduces to solving for \u03c0\u0127 that satisfies the following identity\n\\begin{equation}\n\\mathbb{E}[a_t - \u03c0_\\hbar (h_t) | h_{t-k}] = 0,\n\\end{equation}\nwhich is a CMR problem as defined in Section 2. In this case, both at and ht are observed in the confounded expert demonstrations, and ht\u2212k acts as the instrument.\nTo make sure the instrument ht\u2212k is valid, we check that it satisfies the conditions of Section 2. Firstly, we have checked that $u^f_t \\amalg h_{t-k}$. Secondly, the environment and the expert policy are non-trivial, which means P(ht | ht\u2212k) is not constant in ht\u2212k. Finally, ht\u2212k indeed only affects at through st by the Markovian property. However, the strength of the instrument, which informally represents the correlation between the instrument ht\u2212k and ht, plays an important role in how well we can identify \u03c0\u0127(ht) by solving the CMRs in Equation (5). In particular, we see that, as the confounding horizon k increases, the correlation between ht\u2212k and ht weakens and ht\u2212k becomes a weaker instrument. This means that it is less able to identify \u03c0\u0127 via the CMR in Equation (5) and the final learnt imitator will have poorer performance. This is confirmed theoretically in Proposition 4.3 and experimentally in Section 5, and we will formalise this notion of instrument strength in Section 4.2."}, {"title": "4.1 Practical Algorithms for Solving the CMRs", "content": "There are various techniques (Shao et al., 2024, Bennett et al., 2019b, Xu et al., 2020, Dikkala et al., 2020) for solving the CMRS $\\mathbb{E}[a_t|h_{t-k}] = \\mathbb{E}[\u03c0_\\hbar(h_t)|h_{t-k}]$. Here, the CMR error that we aim to minimise is given by\n\\begin{equation}\n\\sqrt{\\mathbb{E}[(\\mathbb{E}[a_t - \u03c0_\\hbar(h_t) | h_{t-k}])^2]} = ||\\mathbb{E}[a_t - \u03c0_\\hbar(h_t) | h_{t-k}]||_2.\n\\end{equation}\nIn Algorithm 1, we introduce DML-IL, an algorithm adapted from the IV regression algorithm DML-IV (Shao et al., 2024), which solves our CMRs by minimising the CMR error. The first part of the algorithm (line 3-7) learns a roll-out model M that generates a trajectory k steps ahead given ht\u2212k. Then, the roll-out model M is used to train the policy model \u03c0\u0127 (line 8-13). \u03c0\u0127 takes the generated trajectory ht from M(ht\u2212k) as inputs, and minimises the mean squared error to the next action. Using generated trajectories is crucial in breaking the spurious correlation caused by $u^f_t$ between past states and actions, and using the trajectory history before ht\u2212k allows the imitator to infer information about u\u00ba.\nDML-IL can also be implemented with K-fold cross-fitting, where the dataset is partitioned into K folds, with each fold alternately used to train \u03c0\u0127 and the remaining folds to train M. This ensures unbiased estimation and improves the stability of training. The base IV algorithm DML-IV with K-fold cross-fitting is theoretically shown to converge at the rate of $O(N^{-1/2})$ (Shao et al., 2024), where N is the sample size, under regularity conditions. DML-IL with K-fold cross-fitting will thus inherit this convergence rate guarantee.\nNote that Algorithm 1 requires the confounding noise horizon k as input. While the exact value of k can be difficult to obtain in reality, any upper bound k of k is sufficient to guarantee the correctness of Algorithm 1, since $h_{\\bar{t}}$ is also a valid instrument. Ideally, we would like a data-driven approach to determine k. Unfortunately, it is generally intractable to empirically verify whether $h_{t-k}$ is a valid instrument from a static dataset, especially the unconfounded instrument condition (i.e., $h_{t-k} \\amalg$). Therefore, we rely on the user to provide a sensible choice of k based on the environment that does not substantially overestimate k."}, {"title": "4.2 Theoretical Analysis", "content": "In this section, we derive theoretical guarantees for our algorithm, focusing on the imitation gap and its relationship with existing work.\nOn a high level, in order to bound the imitation gap of the learnt policy \u03c0\u0127, i.e., $J(\u03c0_E) - J(\u03c0_\\hbar)$, we need to control:"}, {"title": "5 Experiments", "content": "In this section, we empirically evaluate the performance of Algorithm 1 (DML-IL) on the toy environment with continuous state and action spaces introduced in Example 3.1 and Mujoco environments: Ant, Half Cheetah and Hopper. We compare with the following existing methods: Behavioural Cloning (BC), which naively minimises $\\mathbb{E}[-\\log \u03c0(a_t|s_t)]$; BC-SEQ (Swamy et al., 2022b), which learns a history-dependent policy to handle hidden contexts observable to the expert; ResiduIL (Swamy et al., 2022a), which we adapt to our setting by providing ht\u2212k as instruments to learn a history-independent policy; and the noised expert, which is the performance of the expert when put in the confounded environment, and corresponds to the maximally achievable performance. In Appendix E, we also include additional evaluations of using other IV regression algorithms, including DFIV (Xu et al., 2020) and DeepGMM (Bennett et al., 2019a), as the core CMR solver but found inconsistent and subpar performance."}, {"title": "6 Conclusion", "content": "In this paper, we proposed an unifying framework for confounded IL with hidden confounders that unifies and extends previous confounded IL settings. Specifically, we considered hidden confounders to be partially observable to the expert, and demonstrated that causal IL under this framework can be reduced to a set of CMRs with the trajectory histories as instruments. We proposed DML-IL, a novel algorithm to solve these CMRs and learn an imitator. We provided bounds on the imitation gap for the learnt imitator. Finally, we empirically evaluated DML-IL on multiple tasks including Mujoco environments and demonstrated state-of-the-art performance against other causal IL algorithms."}, {"title": "A Reducing Our Unifying Framework to Related Literature", "content": "In this section, we discuss how the various previous works can be obtained as special cases of our unifying framework."}, {"title": "A.1 Temporally Correlated Noise (Swamy et al., 2022a)", "content": "The Temporally Correlated Noise (TCN) proposed in Swamy et al. (2022a) is a special case of our setting where u\u00b0 = 0 and only the confounding noise $u^f$ is present. Following Equation 14-17 of Swamy et al. (2022a), their setting can be summarised as\n$s_t = T(s_{t-1}, a_{t-1})$\n$=T(s_{t\u22121},\u03c0_E(s_{t\u22121}) + u_{t\u22121} + u_{t\u22122})$\n$a_t = \u03c0_E(s_t) + u_t + u_{t\u22121}$,\nwhere T is the transition function and ut are the TCN. It can be seen that TCN is the confounding noise $u^f$ since the expert policy doesn't take it into account and it affects (or confounds) both the state and action.\nIt can be seen that this is a special case of our framework when u\u2081 = 0, where $a_t = \u03c0_E(s_t) + \u03b5(\u03b9)$ from Equation (2), and more specifically when the confounding noise horizon in Theorem 3.2 is 2. In addition, the theoretical results in Swamy et al. (2022a) can be deduced from our main results as shown in Corollary 4.7."}, {"title": "A.2 Unobserved Contexts (Swamy et al., 2022b)", "content": "The setting considered by Swamy et al. (2022b) is a special case of our setting when $u^f = 0$ and only u\u00ba are present. Following Section 3 of Swamy et al. (2022b), their setting can be summarised as\n$T : S \\times A \\times C \\rightarrow \\mathcal{D}(S)$\n$\\nabla : S \\times A \\times C \\rightarrow [-1,1]$\n$a_t = \u03c0_E(s_t, c)$\nwhere $c \\in C$ is the context, which is assumed to be fixed throughout an episode. There are no hidden confounders in this setting and the context c is included in u\u00ba under our framework. Note that in our setting we also allow u\u00ba to be varying throughout an episode. In addition, the theoretical results in Swamy et al. (2022b) can be deduced from our main results as shown in Corollary 4.6."}, {"title": "A.3 Imitation Learning with Latent Confounders (Vuorio et al., 2022)", "content": "The setting considered by Vuorio et al. (2022) is also a special case of our setting when $u^f = 0$ and only u\u00ba are present, which is very similar to Swamy et al. (2022b). In Section 2.2 of Vuorio et al. (2022), they introduced a latent variable \\@ that is fixed throughout an episode and $a_t = \u03c0_E(s_t,\\@)$. There are no hidden confounders in this setting and the latent variable \\@ is included in u\u00ba in our framework. No theoretical imitation gap bounds are provided in Vuorio et al. (2022). However, Corollary 4.6 can be directly applied to their setting and bound the imitation gap."}, {"title": "A.4 Causal Delusion and Confusion (Ortega et al., 2021, de Haan et al., 2019, Pfrommer et al., 2023, Spencer et al., 2021, Wen et al., 2020)", "content": "The concept of causal delusion (Ortega et al., 2021) and confusion is widely studied in the literature (de Haan et al., 2019, Pfrommer et al., 2023, Spencer et al., 2021, Wen et al., 2020) from different perspectives. A classic example of causal confusion is learning to break in an autonomous driving scenario. The states are images with full view of the dashboard and the road conditions. The break indicator in this scenario is"}, {"title": "B Proofs of Main Results", "content": "In this section, we provide the proofs for the main results and corollaries in this paper."}, {"title": "B.1 Proof of Propositions", "content": "Proposition 4.3: The ill-posedness v(II, k) is monotonically increasing as the confounded horizon k increases.\nProof. From definition, we have that\n\\begin{equation}\n\u03bd(\\Pi, k) = \\sup_{\\pi \\in \\Pi} \\frac{||\\pi_E - \\pi||_2}{\\|\\mathbb{E}[a_t - \\pi(h_t) |h_{t-k}]\\|_2}\n\\end{equation}\nWe would like to show for each \u03c0\u2208 II, $\\frac{||\\pi_E - \\pi||_2}{\\|\\mathbb{E}[a_t - \\pi(h_t) |h_{t-k}]\\|_2}$ is increasing as k increases, which would imply that v(\u03a0, k) is increasing. For each \u03c0\u2208 II, we see that the numerator is constant w.r.t the horizon k. Therefore, it is enough to check that for each \u03c0\u2208 II, the denominator $\\|\\mathbb{E}[a_t - \\pi(h_t) |h_{t-k}]\\|_2$ decreases as k increases. For any two integer horizon $k_1 > k_2$,\n\\begin{equation}\n\\mathbb{E}[a_t - \\pi(h_t) | h_{t-k_1}]^2 = \\mathbb{E}[\\mathbb{E}[a_t - \\pi(h_t)| h_{t-k_2}] | h_{t-k_1}]^2 \\\\\n< \\mathbb{E}[\\mathbb{E}[a_t - \\pi(h_t) | h_{t-k_2}]^2 | h_{t-k_1}]\\\\\n= \\mathbb{E}[a_t - \\pi(h_t) | h_{t-k_2}]^2\n\\end{equation}\nby the tower property of conditional expectation as \u03c3(ht\u2212k\u2081) \u2286 \u03c3(ht\u2212k\u2082), Jensen's inequality for conditional expectations, and the fact that $\\mathbb{E}[a_t - \\pi(h_t) | h_{t-k_2}]^2$ is $h_{t-k_1}$ measurable, respectively for each line. Therefore, we have that $\\mathbb{E}[a_t - \\pi(h_t) | h_{t-k}]$ is decreasing, which implies $\\|\\mathbb{E}[a_t - \\pi(h_t) | h_{t-k}]\\|_2$ is decreasing and v(\u03a0, k) is increasing as k increases, which completes the proof."}, {"title": "B.2 Main results for guarantees on the imitation gap", "content": "Theorem 4.5: Let \u03c0\u0127 be the learnt policy with CMR error \u03b5 and let v (II, k) be the ill-posedness of the problem. Assume that $\\delta_{TV}(u^o_i, \\mathbb{E}_{\\pi_E}[u^o_i|h_t]) < \u03b4$ for $\u03b4 \\in \\mathbb{R}^+$, $P(u^f_t)$ is c-TV stable and \u03c0E is deterministic. Then, the imitation gap is upper bounded by\n\\begin{equation}\nJ(\u03c0_E) \u2013 J(\u03c0_\\hbar) < T^2(c\u03b5\u03bd(\u03a0, k) + 2\u03b4) = O(T^2(\u03b5 + \u03b4)).\n\\end{equation}"}, {"title": "B.3 Proofs of Corollaries", "content": "Corollary 4.6: In the special case that $u^f = 0$, meaning that there is no confounder observable to the expert, or $u^f = \\mathbb{E}_{\u03c0_E}[u^f_i|h_t]$, meaning that $u^f_i$ is o(ht) measurable (all information regarding $u^f_i$ is represented in the history), the imitation gap bound is T2 (\u03c4\u03b5\u03bd(\u03a0, k)), which coincides with Theorem 5.1 of Swamy et al. (2022a).\nProof. If $u^f = 0$, then we have $u^f = \\mathbb{E}_{\u03c0_E}[u^f_i|h_t]$ since $u^f_i$ is a constant. If $u^f = \\mathbb{E}_{\u03c0_E}[u^f_i|h_t]$, we have that\n\\begin{equation}\n\u03b4_{TV}(u^f, \\mathbb{E}_{\u03c0_E}[u^f|h_t]) = \u03b4_{TV}(u^f, u^f) \\le 0\n\\end{equation}\nBy plugging d = 0 into Theorem 4.5, we have that $J(\u03c0_E) \u2013 J(\u03c0_\\hbar) < T^2(c\u03b5\u03bd(\u03a0, k))$, which is the same as the imitation gap derived in Swamy et al. (2022a) and completes the proof.\nCorollary 4.7: In the special case that $u^f = 0$, if the learned policy via supervised BC have error \u03b5, then the imitation gap bound is $T^2(\\sqrt{\\frac{2}{dim(A)}}\u03c4\u03b5 + 2\u03b4)$, which is a concrete bound that extends the abstract bound in Theorem 5.4 of Swamy et al. (2022b).\nProof. In Theorem 5.4 of Swamy et al. (2022b), for the offline case, which is the setting we are considering (as opposed to the online settings), they defined the following quantities for bounding the imitation gap in a very general fashion,\n$E_{off} := \\sup_Q \\mathbb{E}_{T \\sim \u03c0_E} [Q \u2013 \\mathbb{E}_{a \\sim \u03c0_\\hbar} [Q]]$\n$\u03b4_{off} := \\sup_{Q \\times Q} \\mathbb{E}_{T \\sim \u03c0_E} [Q_{\u03c0_\\hbar} \u2013 Q \u2013 \\mathbb{E}_{a \\sim \u03c0_\\hbar} [Q_{\u03c0_\\hbar} \u2013 Q]]$.\nThe imitation gap by Theorem 5.4 in Swamy et al. (2022b) under the assumption that $u^f = 0$ is $T^2(\u03b4_{off} + d_{off})$, which can also be deduced from Equation (9) by naively applying the above supremum. To obtain a concrete bound, we can provide a tighter bound for $\\mathbb{E}_{T \\sim \u03c0_E} [Q_{\u03c0_\\hbar} \u2013 \\mathbb{E}_{a \\sim \u03c0_\\hbar} [Q\u03c0}]$, which is the first part of Equation (9), given that $u^f = 0$.\nFor two elements a1, a2 \u2208 A, we have that by Cauchy-Schwarz,\n\\begin{equation}\n\u03b4_{TV}(a_1 + 0, a_2 + 0) = \\frac{1}{2}||a_1 - a_2||_1 \\le \\frac{\\sqrt{dim(A)}}{2}||a_1 - a_2||_2.\n\\end{equation}\nThen, we have that\n\\begin{equation}\n||a_1-a_2||_2 \\le \\Delta \\Rightarrow \u03b4_{TV}(a_1,a_2) \\le \\frac{\\Delta}{\\sqrt{dim(A)}}\n\\end{equation}\nso that by Theorem 4.5,\n\\begin{equation}\n\\mathbb{E}_{T \\sim \u03c0_E} [Q_{\u03c0_\\hbar} \u2013 \\mathbb{E}_{a \\sim \u03c0_\\hbar} [Q_{\u03c0_\\hbar}]] = \\mathbb{E}_{T \\sim \u03c0_E} [Q_{\u03c0_\\hbar} (h_t, a_t) \u2013 \\mathbb{E}[Q_{\u03c0_\\hbar} (h_t, \u03c0_\\hbar (h_t))]]\n\\end{equation}\n\\begin{equation}\n= \\mathbb{E}_{h_t \\sim \u03c0_E} [\\mathbb{E}[Q_{\u03c0_\\hbar} (h_t, \u03c0_E(s_t))] \u2013 \\mathbb{E}[Q_{\u03c0_\\hbar} (h_t, \u03c0_\\hbar (h_t))]]\n\\end{equation}\n\\begin{equation}\n< ||Q_{\u03c0_\\hbar}||_{\\infty} \\frac{||\u03c0_E - \u03c0||_2}{ \\sqrt{dim(A)}}\n\\end{equation}\n\\begin{equation}\n\u2264 T \\frac{\\sqrt{\u03b5}}{\\sqrt{dim(A)}},\n\\end{equation}"}, {"title": "C Environments and Tasks", "content": "Here", "follows": "n$S := \\mathbb{R"}, "n$A := [-1,1"]}