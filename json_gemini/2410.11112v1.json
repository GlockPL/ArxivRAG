{"title": "Differentiable Weightless Neural Networks", "authors": ["Alan T. L. Bacellar", "Zachary Susskind", "Mauricio Breternitz Jr.", "Eugene John", "Lizy K. John", "Priscila M. V. Lima", "Felipe M. G. Fran\u00e7a"], "abstract": "We introduce the Differentiable Weightless Neural Network (DWN), a model based on interconnected lookup tables. Training of DWNs is enabled by a novel Extended Finite Difference technique for approximate differentiation of binary values. We propose Learnable Mapping, Learnable Reduction, and Spectral Regularization to further improve the accuracy and efficiency of these models. We evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware accelerator, where they demonstrate superior latency, throughput, energy efficiency, and model area compared to state-of-the-art solutions, (2) a low-power microcontroller, where they achieve preferable accuracy to XGBoost while subject to stringent memory constraints, and (3) ultra-low-cost chips, where they consistently outperform small models in both accuracy and projected hardware area. DWNs also compare favorably against leading approaches for tabular datasets, with higher average rank. Overall, our work positions DWNs as a pioneering solution for edge-compatible high-throughput neural networks. https://github.com/alanbacellar/DWN", "sections": [{"title": "1. Introduction", "content": "Despite the rapid advancement of deep learning, optimizing computational efficiency, especially during inference, remains a critical challenge. Efforts to mitigate computational demands have led to innovations in model pruning (Dong et al., 2017a;b; Lin et al., 2018), quantization (Banner et al., 2018; Chmiel et al., 2021; Faghri et al., 2020), and sparse neural networks (Sung et al., 2021; Sun et al., 2021; Ma & Niu, 2018). However, these approaches do not fundamentally address the inherent cost of multiplication in neural networks. Consequently, multiplication-free architectures such as binary neural networks (BNNs) (Hubara et al., 2016), AddNets (Chen et al., 2021), and DeepShift (Elhoushi et al., 2021) have also been proposed, demonstrating impressive computational efficiency (Samragh et al., 2021; Qin et al., 2022; He & Xia, 2018).\nWithin the domain of multiplication-free models, weightless neural networks (WNNs) stand out as a distinct category. Diverging from the norm, WNNs forgo traditional weighted connections, opting instead for lookup tables (LUTs) with binary values to drive neural activity (Aleksander et al.,"}, {"title": "Differentiable Weightless Neural Networks", "content": "1984; 2009), where number of inputs to each LUT, n, is a hyperparameter. This enables WNNs to represent highly nonlinear behaviors with minimal arithmetic. However, a notable limitation of WNNs architectures is their restriction to single-layer models. This constraint is primarily due to the discrete structure of LUTs, which has historically made training complex multi-layer WNNs infeasible.\nDespite this limitation, recent advancements have illuminated the potential of WNNs to achieve high efficiency in neural computation. The ULEEN model (Susskind et al., 2023) exemplifies this, showcasing a WNN's capabilities on an FPGA platform. Remarkably, ULEEN has outperformed BNNs in accuracy, energy efficiency, latency, memory consumption, and circuit area. This is particularly noteworthy considering that ULEEN operates with a single-layer structure, in contrast to the deep architecture of BNNs. This success not only underscores the inherent efficiency of WNNs but also implies the immense potential they could unlock if developed beyond their current single-layer constraints.\nThe recent Differentiable Logic Gate Networks (DiffLogicNet) (Petersen et al., 2022) proposes a gradient descent-based method for training multi-layer logic gate networks. The hypothesis space of DiffLogicNet's two-input binary nodes is exactly the same as a WNN with two-input LUTS (LUT-2s), showing that training multi-layer WNNs is theoretically possible. However, the cost of this method scales double-exponentially (O(2^{2^n})) with the number of inputs to each LUT, requiring an astonishing 18.4 quintillion parameters to represent a single LUT-6. The inability to train models with larger LUTs is detrimental for two main reasons: (1) the VC dimension of WNNs grows exponentially with the number of inputs to each LUT (Carneiro et al., 2019), making a WNN with fewer but larger LUTs a more capable learner than one with many small LUTs; (2) the ability to train LUTs with varying sizes facilitates hardware-software co-design, leading to more efficient models. For instance, a LUT-6-based WNN could be significantly more efficient on modern AMD/Xilinx FPGAs, which employ LUT-6s in their configurable logic blocks (CLBs), aligning WNN implementation more closely with FPGA architecture.\nFurthermore, both WNNs and DiffLogicNet currently have three other vital limitations. First, they rely on pseudo-random connections between LUTs, which leaves the optimal arrangement of the neural network to chance. Second, they use population counts (popcounts) to determine activation values for each class, which incurs a large area overhead in hardware, with the popcount circuit in some cases being as large as the network itself. Finally, the binary nature of LUTs means that traditional DNN regularization techniques are ineffective; thus, there is a pressing need to develop specialized regularization techniques.\nRecognizing the critical need for innovation in this area,"}, {"title": "Differentiable Weightless Neural Networks", "content": "our work introduces the Differentiable Weightless Neural Network (DWN) (Figure 1), tackling all of these limitations. This is achieved with a suite of innovative techniques:\n\u2022 Extended Finite Difference: This technique enables efficient backpropagation through LUTs by approximate differentiation, allowing the development of multi-layer WNN architectures with bigger LUTs.\n\u2022 Learnable Mapping: This novel layer allows DWNs to learn the connections between LUTs during training, moving beyond fixed or random setups in WNNs and Diff-LogicNet, enhancing adaptability and efficiency without additional overhead during inference.\n\u2022 Learnable Reduction: Tailored for tiny models, this approach replaces popcount with decreasing pyramidal LUT layers, leading to smaller circuit sizes.\n\u2022 Spectral Normalization: A normalization technique specifically developed for LUTs in WNNs to improve model stability and avoid overfitting.\nOur results demonstrate the DWN's versatility and efficacy in various scenarios:\n1. FPGA deployment: DWNs outperform DiffLogicNet, fully-connected BNNs, and prior WNNs in latency, throughput, energy efficiency, and model area across all tested datasets. By aligning model LUTs with the native FPGA LUT size, DWNs achieve a geometric average 2522\u00d7 improvement in energy-delay product versus the FINN BNN platform and a 63\u00d7 improvement versus ULEEN, the current state-of-the-art for efficient WNNs.\n2. Constrained edge devices: On a low-end microcontroller (the Elegoo Nano), our throughput-optimized implementations of DWNs achieve on average 1.2% higher accuracy than XGBoost with a 15% speedup. Our accuracy-optimized implementations achieve 5.4% improvement, at the cost of execution speed.\n3. Ultra-low-cost chips: The DWN reduces circuit area by up to 42.8\u00d7 compared to leading Tiny Classifier models (Iordanou et al., 2023), and up to 310\u00d7 compared to DiffLogicNet.\n4. Tabular data: DWN surpasses state-of-the-art models such as XGBoost and TabNets, achieving an average rank of 2.5 compared to 3.4 and 3.6 respectively."}, {"title": "2. Background & Related Work", "content": null}, {"title": "2.1. Weightless Neural Networks", "content": "Weightless neural networks (WNNs) eschew traditional weighted connections in favor of a multiplication-free approach, using binary-valued lookup tables (LUTs), or \u201cRAM nodes\u201d, to dictate neuronal activity. The connections between the input and these LUTs are randomly initialized and remain static. The absence of multiply-accumulate (MAC) operations facilitates the deployment of high-throughput"}, {"title": "Differentiable Weightless Neural Networks", "content": "models, which is particularly advantageous in edge computing environments. A notable recent work in this domain is ULEEN(Susskind et al., 2023), which enhanced WNNs by integrating gradient-descent training and utilizing straight-through estimators (Bengio et al., 2013) akin to those employed in BNNs, and outperformed the Xilinx FINN (Umuroglu et al., 2017) platform for BNN inference in terms of latency, memory usage, and energy efficiency in an FPGA implementation. A significant limitation of most WNN architectures is their confinement to single-layer models. While some prior works experimented with multi-layer weightless models (Al Alawi & Stonham, 1992; Filho et al., 1991), they relied on labyrinthine backward search strategies which were impractical for all but very simple datasets, and did not use gradient-based methods for optimization."}, {"title": "2.2. Thermometer Encoding", "content": "The method of encoding real-valued inputs into binary form is a critical aspect of WNNs, as the relationship between bit flips in the encoded input and corresponding changes in actual values is essential for effective learning (Kappaun et al., 2016). To address this, Thermometer Encoding was introduced (Carneiro et al., 2015), which uses a set of ordered thresholds to create a unary code (see Appendix B)."}, {"title": "2.3. DiffLogicNet", "content": "DiffLogicNet (Petersen et al., 2022) proposed an approach to learning multi-layer networks exclusively composed of binary logic. In this model, an input binary vector is processed through multiple layers of binary logic nodes. These nodes are randomly connected, ultimately leading to a final summation determining the output class score. For training these networks via gradient descent, DiffLogicNet proposes a method where binary values are relaxed into probabilities. This is achieved by considering all possible binary logic functions (as detailed in Appendix A, Table 6), assigning a weight to each, and then applying a softmax function to create a probability distribution over these logic functions. See Appendix C for more details."}, {"title": "2.4. Other LUT-Based Neural Networks", "content": "Recently, other LUT-based neural networks such as Logic-Nets (Umuroglu et al., 2020a), PolyLUT (Andronic et al., 2023), and NeuraLUT (Andronic et al., 2024) have been proposed to improve DNN efficiency, rediscovering (Ferreira & Fran\u00e7a, 1997; Burattini et al., 2003). LogicNets suggest training sparse DNNs with binary activations and converting their neurons into LUTs by considering all possible input combinations. This aims to achieve efficient inference but fails to fully utilize the computational capacity of LUTs. An n-input LUT has a known VC-dimension of 2^n (Carneiro et al., 2019), while a DNN neuron with n inputs has a VC-"}, {"title": "Differentiable Weightless Neural Networks", "content": "dimension of n + 1. Consequently, they effectively train a LUT with a reduced VC-dimension of n + 1, leading to larger and less efficient models.\nPolyLUTs tries to address this limitation by utilizing feature mappings in the sparse neuron inputs to learn more complex patterns. The most recent NeuraLUTs fit multiple neurons and layers with skip connections that receive the same input into a LUT, rather than a single neuron. However, both approaches still fall short of fully exploiting LUT computational capabilities, as we will demonstrate in the experiments section.\nIn contrast, our approach fully leverages the computational capabilities of LUTs by proposing a method to update and perform backpropagation with LUTs during the training phase, rather than merely using LUTs as a speedup mechanism for DNN neurons or layers."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Extended Finite Difference", "content": "DiffLogicNet introduced a technique for learning binary logic with gradient descent and backpropagation that is readily applicable to WNNs employing two-input RAM nodes, as LUT-2s inherently represent binary logic. However, this technique is impractical for even slightly larger RAM nodes due to its O(2^{2^n}) space and time complexities for a single LUT-n. Crucially, our approach reduces this to O(2^n), cutting the weights and computations needed to represent a LUT-6 from 18,446,744,073,709,551,616 to 64.\nFinite Difference (FD) is a powerful tool for approximating derivatives, especially for functions with binary vector inputs. This approach is centered on evaluating the impact of minor input alterations on the input, specifically flipping a single bit in the binary case. For a given function f: {0,1}^n \u2192 R^m, the FD \u2206f is computed as\n$\\Delta f(x)_j = f(x) \u2013 f(\\bar{x^j})$\nwhere x is the binary input vector, and $\\bar{x^j}$ represents the vector x with its j-th bit set to 1 and 0, respectively. This formula shows how f's output changes when flipping the j-th bit in x, capturing the output's sensitivity to specific input bits.\nThe derivatives of a lookup table's addressing function can be approximated using FD. Consider A : R^{2^n} \u00d7 {0,1}^n \u2192 IR as the addressing function that retrieves values from a lookup table U \u2208 R^{2^n} using address a \u2208 {0,1}^n. Define \u03b4: {0,1}^n \u2192 {1,...,2^n} as the function converting a binary string to its integer representation +1. The partial derivatives of A can be approximated by finite differences:\n$\\frac{\\partial A}{\\partial U}(U, a) = \\begin{cases} 1, & \\text{if } i = \\delta(a) \\\\ 0, & \\text{otherwise} \\end{cases},$\n$\\frac{\\partial A}{\\partial a_j}(U,a) = A(U,\\bar{a^1}) \u2013 A(U,\\bar{a^0})$"}, {"title": "Differentiable Weightless Neural Networks", "content": "wherea and a signifies the address a with its j-th bit set to 1 and 0, respectively.\nUsing FD is our first proposed approach to \"Differentiable\" WNNS (DWN). However, while FD approximates the partial derivatives of a lookup table's addressing function, it only considers addresses within a Hamming distance of 1 from the targeted position. This limitation may hinder learning by ignoring optimal addressing positions beyond this proximity. For example, in a LUT-6 scenario, FD considers only 7 out of 64 positions, potentially neglecting more relevant ones.\nTo address this limitation, we introduce an Extended Finite Difference (EFD) method for more comprehensive derivative approximation. This technique considers variations in the addressed position relative to all possible positions, not just those one-bit apart:\n$\\frac{\\partial A}{\\partial a_j}(U, a) = \\Sigma_{k\\in{0,1}^n} \\frac{(-1)^{H(k, a, j)}}{H(k, a, j) +1} A(U,k)$\nwhere H: {0,1}^n \u00d7 {0,1}^n \u00d7 N \u2192 N calculates the Hamming distance between k and a, excluding the j-th bit. This formula integrates contributions from all lookup table positions, weighted by their relative distance (in terms of Hamming distance) to the address in use, with an added term for numerical stability. EFD provides a more holistic view, potentially capturing address shifts to more distant positions that conventional FD might miss."}, {"title": "3.2. Learnable Mapping", "content": "WNNs and DiffLogicNet both rely on pseudo-random mappings to route inputs, to LUTs in the former and between binary logic nodes in the latter. The specific choice of mapping can have a substantial impact on model accuracy, but is largely dependent on chance. In response, we introduce a new method that learns these connections through gradient descent-based optimization, without additional computational overhead during inference. This involves a weight matrix W \u2208 R^{P\u00d7Q} during training, where P is the input bit length or output bit count from the previous layer, and Q is the number of input connections in the next layer. Input selection for LUTs during the forward pass is based on the maximum weights in W, determined by\n$I(W, x)_i = X_{\\text{argmax}}(W[i:]).$\nThe backward pass involves calculating partial derivatives with respect to W and input x. For W, we use the product of the transformed input vector (2x \u2013 1) and the backpropagated gradient matrix G, where the transformation maps binary inputs to -1 and 1. The derivative is $\\frac{\\partial I}{\\partial W} = ((2x - 1) \u00b7 G)$. For input x, the derivative is obtained by multiplying G with the transposed softmax of W over the first dimension, as $\\frac{\\partial I}{\\partial x}= G.softmax_{\\text{dim}=0}(W)^T$.\nThese gradients allow the learnable mapping (Figure 2) to"}, {"title": "Differentiable Weightless Neural Networks", "content": "iteratively refine the LUTs connections, optimizing DWN performance. During inference, the argmax of W remains constant since it is independent of the input. Consequently, the weight matrix W is discarded, and the LUTs' connections become fixed, meaning there is no overhead from this technique at inference time."}, {"title": "3.3. Learnable Reduction", "content": "When deploying tiny DWNs targetting ultra-low-cost chips, the popcount following the final LUT layer can constitute a large fraction of circuit area. This size disparity hinders efforts to reduce the overall circuit size. To address this, we propose a novel approach that deviates from the conventional method of determining a WNN's output class; i.e., the argmax of popcounts from the last LUT layer's feature vector. Instead, our method involves learning the reduction from the feature vector to the output class using layers of LUTs configured in a decreasing pyramidal architecture as in Figure 2. This technique enables the model to discover more efficient methods for determining the output class. It moves away from the reliance on a fixed structure of popcounts and argmax computations, resulting in smaller and more efficient circuit designs suitable for deployment."}, {"title": "3.4. Spectral Regularization", "content": "The inherent nonlinearity of WNNs is a double-edged sword: it contributes to their remarkable efficiency but also makes them very vulnerable to overfitting. Even very small DWNs may sometimes perfectly memorize their training data. Unfortunately, conventional DNN regularization techniques can not be applied directly to DWNs. For instance, since only the sign of a table entry is relevant for address computation, using L1 or L2 regularization to push entries towards 0 during training results in instability.\nTo address this issue, we propose a novel WNN-specific technique: spectral regularization. For an n-input pseudo-Boolean function f : {0,1}^n \u2192 R, we define the L2 spectral norm of f as:\n$\\frac{1}{2^n}||{\\Sigma_{x \\in {0,1}^n} f(x) (\\Pi_{i \\in S} (2x_i \u2013 1)) | S \\in {[n]}}||_2$"}, {"title": "Differentiable Weightless Neural Networks", "content": "Note that this is simply the L2 norm of the Fourier coefficients of f (O'Donnell, 2021). Additionally, since all terms except f(x) are constant, we can precompute a coefficient matrix C\u2208 R^{2^n \u00d72^n} to simplify evaluating the spectral norm at runtime. In particular, for a layer of u LUTs with n inputs each and data matrix L \u2208 [\u22121, 1]^{u\u00d72^n}, we express the spectral norm as:\n$\\text{specnorm}(L) = ||LC||_2, C_{ij} := \\frac{1}{2^n} \\Pi_{a \\in {b | i=1}}(2j_a \u2013 1)$\nThe effect of spectral regularization is to increase the resiliency of the model to perturbations of single inputs. For instance, if an entry in a RAM node is never accessed during training, but all locations at a Hamming distance of 1 away hold the same value, then the unaccessed location should most likely share this value."}, {"title": "4. Experimental Evaluation", "content": "To demonstrate the effectiveness and versatility of DWNs, we evaluate them in several scenarios. First, we assess their performance on a custom hardware accelerator, implemented using a field-programmable gate array (FPGA), to demonstrate DWNs' extreme speed and energy efficiency in high-throughput edge computing applications. Next, we implement DWNs on an inexpensive off-the-shelf microcontroller, demonstrating that they can operate effectively on very limited hardware, and emphasizing their practicality in cost-sensitive embedded devices. We also consider the incorporation of DWNs into logic circuits, assessing their potential utility in ultra-low-cost chips.\nBeyond hardware-focused evaluations, we also compare the accuracy of DWNs against state-of-the-art models for tabular data, with an emphasis on maximizing accuracy rather than minimizing model parameter size. Overall, while DWNs are chiefly engineered for edge inference applications, we aim to demonstrate their effectiveness in multiple contexts.\nBinary Encoding: All datasets in the experimental evaluation are binarized using the Distributive Thermometer (Bacellar et al., 2022) for both DWN and DiffLogicNet. The sole exception is the DiffLogicNet model for the MNIST dataset, for which we use a threshold of 0, following the strategy outlined in their paper."}, {"title": "4.1. DWNs on FPGAs", "content": "FPGAs enable the rapid prototyping of hardware accelerators without the lead times associated with fabricating a custom IC. We deploy DWN models on the Xilinx Zynq Z-7045, an entry-level FPGA that was also used for the BNN-based FINN (Umuroglu et al., 2017) and WNN-based ULEEN (Susskind et al., 2023). We adopt the input data"}, {"title": "Differentiable Weightless Neural Networks", "content": "compression scheme used in ULEEN, which allows for more efficient loading of thermometer-encoded values. This is important due to the limited (112 bits per cycle) interface bandwidth of this FPGA. As in prior work, all designs are implemented at a clock speed of 200 MHz.\nFigure 3 gives a high-level overview of our accelerator design. The FPGA is largely composed of configurable logic blocks (CLBs), which are in turn composed of six-input lookup tables (LUT-6s), flip-flops, and miscellaneous interconnect and muxing logic (Xilinx, 2016). Hence, DWNs with n=6 make efficient use of readily available FPGA resources.\nTable 1 compares the FPGA implementations of our DWN models against prior work. We include DWNs with both two and six-input RAM nodes. The original DiffLogicNet paper (Petersen et al., 2022) does not propose an FPGA implementation, but we observe that their model is structurally identical at inference time to DWNs with n=2 inputs per LUT, with all substantive differences restricted to the training process. Therefore, we can directly implement their models using our DWN RTL flow.\nAll datasets were chosen due to their use in prior work except for FashionMNIST (Xiao et al., 2017), which is identical in size to MNIST but intentionally more difficult. We directly reuse the MNIST model topologies, and thus hardware results, for FINN and ULEEN on this dataset.\nExcluding CIFAR-10, our DWN models are smaller, faster, and more energy-efficient than prior work, with comparable or better accuracy. In particular, latency, throughput, energy per sample, and hardware area (in terms of FPGA LUTs) are improved by geometric averages of (20.7, 12.3, 121.6, 11.7)\u00d7 respectively versus FINN, and"}, {"title": "Differentiable Weightless Neural Networks", "content": "(3.3, 2.3, 19.0, 22.7)\u00d7 respectively versus ULEEN, the prior state-of-the-art for efficient WNNs. Unlike the other architectures in Table 1, FINN supports convolution. This gives it vastly superior accuracy on the CIFAR-10 dataset, albeit at a hefty penalty to speed and energy efficiency.\nSeveral models in Table 1 could not be implemented on our target FPGA (indicated by '*'). The primary cause of this was routing congestion: since it would be infeasibly expensive for FPGAs to implement a full crossbar interconnect, they instead have a finite number of wires to which they assign signals during synthesis. The irregular connectivity between layers in DiffLogicNet and DWNs with n=2 proved impossible to map to the FPGA's programmable interconnect in these cases. However, note that all DWNs with n=6 were successfully routed and implemented.\nAn interesting takeaway from these results is that the parameter sizes of DWN models are not necessarily good predictors of their hardware efficiency. For instance, the large MNIST model with n=2 has \u22481/4 the parameter size of the n=6 model, yet more than twice the area and energy consumption. Since our target FPGA uses LUT-6s natively, models with n=6 are inherently more efficient to implement. Although the synthesis tool can perform logic optimizations that map multiple DWN LUT-2s to a single FPGA LUT-6, this is not enough to offset the \u22484\u00d7 larger number of RAM nodes needed to achieve the same accuracy with n=2."}, {"title": "Differentiable Weightless Neural Networks", "content": "Comparison to Other LUT-Based NNs: We also compare DWNs against LogicNets, PolyLUT, and NeuraLUT, which convert models to LUTs for inference but do not use them during training. We follow their experimental methodology by targeting the xcvu9p-"}, {"title": "4.2. DWNs on Microcontrollers", "content": "While FPGAs can be extraordinarily fast and efficient, they are also expensive, specialized devices. We also consider the opposite extreme: low-cost commodity microcontrollers. The Elegoo Nano is a clone of the open-source Arduino Nano, built on the 8-bit ATmega328P, which at the time of writing retails for $1.52 in volume. The ATmega provides 2 KB of SRAM and 30 KB of flash memory and operates at a maximum frequency of 20 MHz. We can not expect performance comparable to an FPGA on such a limited platform. Our goal is instead to explore the speeds and accuracies of DWNs which can fit into this device's memory.\nWe use two strategies for implementing DWNs on the Nano. Our first approach uses aggressive bit packing to minimize memory usage, allowing us to fit more complex models on the device. For instance, the 64 entries of a LUT-6 can be packed in 8 bytes of memory, and the six indices for its inputs can be stored in 7.5 bytes by using 10-bit addresses (for more details on our bit-packing strategy, see Appendix D). However, this approach needs to perform bit"}, {"title": "Differentiable Weightless Neural Networks", "content": "manipulation to unpack data, which is fairly slow. Therefore, we also explore an implementation without bit-packing, which greatly increases inference speed but reduces the maximum size (and therefore accuracy) of feasible models.\nXGBoost (Chen & Guestrin, 2016) is a widely-used tree boosting system notable for its ability to achieve high accuracies with tiny parameter sizes. This makes it a natural choice for deployment on microcontrollers. We use the MicroML (Salerno, 2022) library for XGBoost inference on the Nano and compare it against DWNs. To fit entire samples into SRAM, we quantize inputs to 8 bits. We did not observe a significant impact on accuracy from this transformation."}, {"title": "Differentiable Weightless Neural Networks", "content": "Our bit-packed DWN implementation is consistently more accurate than XGBoost, by an average of 5.4%, and particularly excels on non-tabular multi-class datasets such as MNIST and KWS. However, it is also 8.3\u00d7 slower on average. Our unpacked implementation is 15% faster than XGBoost and still 1.2% more accurate on average, but is less accurate on one dataset (higgs). Overall, DWNs are good models for low-end microcontrollers when accuracy"}, {"title": "Differentiable Weightless Neural Networks", "content": "is the most important consideration, but may not always be the best option when high throughput is also needed."}, {"title": "4.3. DWNs for Ultra-Low-Cost Chips", "content": "To assess the viability of DWNs for ultra-low-cost chip implementations, we analyze their performance in terms of accuracy and NAND2 equivalent circuit area, comparing them with Tiny Classifiers (Iordanou et al., 2024), a SOTA work for ultra-low-cost small models, and DiffLogicNet. The datasets for this analysis are those shared between the Tiny Classifiers (see Appendix G) and AutoGluon (to be used in the next subsection) studies, providing a consistent basis for comparison. We also adhere to their data-splitting methods, using 80% of the data for the training set and 20% for the testing set.\nOur DWN, utilizing the Learnable Reduction technique with LUT-2s, is designed to inherently learn two input binary logic, which directly correlates to logic gate formation in a logic circuit. The NAND2 equivalent size of our model is calculated by converting each LUT-2 into its NAND2 equivalent (e.g., a LUT-2 representing an OR operation equates to 3 NAND gates). For DiffLogicNet, we adopt a similar approach, translating the converged binary logic nodes into their NAND2 equivalents, plus the additional NAND2 equivalent size required for each class output popcount (Appendix E), as per their model architecture. Notably, our DWN model, due to Learnable Reduction, does not incur this additional computational cost. For Tiny Classifiers we utilize the results reported in their paper.\nOur results, presented in Table 4, highlight DWN's exceptional balance of efficiency and accuracy across a range of datasets. Notably, DWN consistently outperforms Tiny Classifiers and DiffLogicNet in accuracy, while also showcasing a remarkable reduction in model size. In the 'skin-seg'"}, {"title": "Differentiable Weightless Neural Networks", "content": "dataset, DWN achieves 98.9% accuracy with a model size of only 88 NAND, compared to 93% with 300 NAND for Tiny Classifiers and 98.2% with 610 NAND for DiffLogic-Net, demonstrating reductions of approximately 3.4\u00d7 and 6.9\u00d7, respectively. Similarly, in the \u2018jasmine' dataset, DWN reaches 80.6% accuracy with just 51 NAND gates, while Tiny Classifiers and DiffLogicNet achieve 72% with 300 NAND and 76.7% with 1816 NAND, respectively, indicating reductions of 5.9\u00d7 and 35.6\u00d7.\nThese findings demonstrate DWN's potential in ASIC and Ultra-Low-Cost Chip implementations, offering a blend of high accuracy and compact circuit design."}, {"title": "4.4. DWNs on Tabular Data", "content": "In this subsection, we explore benchmarking DWNs against a range of prominent state-of-the-art models in the field of tabular data processing. This includes a thorough evaluation alongside the AutoGluon suite (Erickson et al., 2020) encompassing models like XGBoost, CatBoost, LightGBM, TabNN, and NNFastAITab Google's TabNet, and DiffLogicNet. These benchmarks are crucial in demonstrating the efficacy and competitiveness of DWN in handling structured data, a key requirement for numerous real-world applications. Additionally, they show that DWN's efficient inference does not come at the cost of accuracy, highlighting DWN's remarkable ability to learn from tabular data.\nTo ensure a fair comparison, all models in the AutoGluon suite were trained under 'best quality' configurations, which involve extended training times and fine-tuning for optimal accuracy. DWN model sizes were restricted to match those of the other models, ensuring that any performance gains were not simply due to a larger model size. The datasets and train-test splits are the same as in the previous subsection."}, {"title": "Differentiable Weightless Neural Networks", "content": "For training our DWN, we adopted an AutoML-like approach: setting the number of layers to 3 for small datasets and 5 for larger ones. The number of LUTs per layer was adjusted to align with the final model sizes of XGBoost, thereby maintaining a comparable model size. See Appendix I for more model configuration details.\nResults are detailed in Table 5. Key metrics in our analysis are the Average Rank and Average L1 norm. Average Rank is calculated by ranking models on each dataset according to accuracy and then averaging these ranks across all datasets. This metric provides a comparative view of each model's performance relative to others. The Average L1 norm, on the other hand, measures the average L1 distance of each model's accuracy from the highest-achieving model on each dataset. This offers insight into how closely each model approaches the best possible accuracy.\nAs shown in Table 5, DWN achieves an impressive average rank of 2.5 and an average L1 norm of 0.005, indicating its leading performance in accuracy among the compared models. Notably, it surpasses renowned models such as XGBoost, CatBoost, and TabNN, which have respective average rankings of 3.4, 3.6, and 3.6, and average L1 norms of 0.009, 0.014, and 0.010."}, {"title": "5. Conclusion", "content": "In this paper", "enhancements": "Learnable Mapping, Learnable Reduction, and Spectral Regularization. Our results underscore the versatility and efficiency of our approach, demonstrating up to 135\u00d7 reduction in energy costs in FPGA implementations compared to BNNs and DiffLogicNet, up to 9% higher accuracy in deployments on constrained devices, and culminating in up to 42.8\u00d7 reduction in circuit area for ultra-low-cost chip implementations. Moreover, DWNs have achieved an impressive average ranking of 2.5 in processing tabular datasets, outperforming state-of-the-art models such as XGBoost and TabNets, which have average rankings of 3.4 and 3.6 respectively.\nThese significant contributions pave the way for a plethora of future work. In the context of FPGAs, extending our approach to include CNN and Transformer architectures could lead to significant advancements in deep learning. This is especially relevant considering the resource-intensive nature of current models used in computer vision and language processing. Furthermore, immediate future work on FPGAs"}]}