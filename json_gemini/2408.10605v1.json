{"title": "MUSES: 3D-Controllable Image Generation via Multi-Modal Agent Collaboration", "authors": ["Yanbo Ding", "Shaobin Zhuang", "Kunchang Li", "Zhengrong Yue", "Yu Qiao", "Yali Wang"], "abstract": "Despite recent advancements in text-to-image generation, most existing methods struggle to create images with multiple objects and complex spatial relationships in 3D world. To tackle this limitation, we introduce a generic AI system, namely MUSES, for 3D-controllable image generation from user queries. Specifically, our MUSES addresses this challenging task by developing a progressive workflow with three key components, including (1) Layout Manager for 2D-to-3D layout lifting, (2) Model Engineer for 3D object acquisition and calibration, (3) Image Artist for 3D-to-2D image rendering. By mimicking the collaboration of human professionals, this multi-modal agent pipeline facilitates the effective and automatic creation of images with 3D-controllable objects, through an explainable integration of top-down planning and bottom-up generation. Additionally, we find that existing benchmarks lack detailed descriptions of complex 3D spatial relationships of multiple objects. To fill this gap, we further construct a new benchmark of T2I-3DisBench (3D image scene), which describes diverse 3D image scenes with 50 detailed prompts. Extensive experiments show the state-", "sections": [{"title": "Introduction", "content": "In recent years, we have witnessed rapid advancements in text-to-image generation (Rombach et al. 2022a; Ramesh et al. 2021; Midjourney 2024). However, such generation often struggles with detailed user queries with multiple objects in complex scenes. Several approaches have emerged to address this by compositional text-to-image synthesis (Yang et al. 2024b; Chefer et al. 2023; Feng et al. 2024; Wang et al. 2024; Feng et al. 2023; Li et al. 2023b). Unfortunately, they fail to accurately control 3D contents in the image such as 3D spatial relationship, object orientation, and camera view, even though our real world is inherently three-dimensional. As shown in Fig. 1, when the prompt is \u201ca frog facing right is behind a rabbit facing forward on the ground\u201d, existing approaches collapse with unsatisfactory 3D arrangement. This raises a fundamental question: can we create images with precise 3D control to better simulate our world?\nTo answer this question, we draw an inspiration from the workflow of 3D professionals. We observe that creating such images typically involves three key stages: scene layout planning, 3D objects design, and image rendering (Pharr, Jakob, and Humphreys 2023). This highlights the need for developing a systematic framework of 3D-controllable image creation, rather than relying on a single generation model. Therefore, we propose a generic AI system which automatically and precisely creates images with 3D controllable objects. We name it MUSES, since we consider human designers as our \u201cMuses\u201d, and mimic their workflows through a collaborative pipeline of multi-modal agents.\nOur MUSES system, as depicted in Fig. 2, comprises three key agents that progressively achieve 3D-controllable image generation: (1) Layout Manager. First, we employ a Large Language Model (e.g., Llama3 (AI@Meta 2024)) as layout manager to assign 3D object locations based on user queries. Our innovative 2D-to-3D layout lifting paradigm first generates a 2D layout through in-context learning, then elevates it to a 3D layout via chain-of-thought reasoning. (2) Model Engineer. After obtaining the 3D layout, we introduce a model engineer to acquire 3D models of queried objects. To enhance its robustness, we design a flexible re-"}, {"title": "Related Work", "content": "Controllable Image Generation. Prior to diffusion models, GAN-based (Creswell et al. 2018) methods such as Control-GAN (Lee and Seok 2019) and AttnGAN (Fang et al. 2022), incorporated text features via attention (Vaswani et al. 2017) modules to guide image generation. With the advent of diffusion models (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015), Stable Diffusion series (Podell et al. 2023; Rombach et al. 2022b,a) rapidly dominated the text-to-image generation market. Given that text-based control is insufficient for precise and fine-grained image generation, Control-Net (Zhang, Rao, and Agrawala 2023), GLIGEN (Li et al. 2023b), T2I-Adapter (Mou et al. 2023) introduced additional control conditions such as depth maps and sketches. Additionally, models such as Structured-Diffusion (Feng et al. 2023) and Attn-Exct (Chefer et al. 2023) fused linguistic structures or attention-based semantic guidance into the diffusion process. Methods like VPGen (Cho, Zala, and Bansal 2024), LayoutGPT (Feng et al. 2024) and LMD (Lian et al. 2023) emplyoed LLMs to plan 2D layouts (bounding boxes),\nwhile RPG (Yang et al. 2024b) used LLMs to plan and assign regions, and to re-caption the user input. Despite these advancements, existing methods struggle to control 3D properties of objects. We take an innovative approach by planning 3D layouts and incorporating 3D models and simulation to achieve 3D controllable image generation.\nLLM-Based Agents. LLMs such as ChatGPT (Brown et al. 2020; Achiam et al. 2023), Llama (Touvron et al. 2023a,b) have revolutionized natural language processing, while Multimodal LLMs (MLLMs) such as LLaVA (Liu et al. 2024), InternVL (Chen et al. 2024b) have enabled impressive performance on visual tasks. The combination of LLMs and MLLMs in multi-agent systems has achieved remarkable success across various domains, including visual understanding (Kelly et al. 2024; Wu et al. 2023a), gaming (Li et al. 2023a; Gong et al. 2023), software development (Wu et al. 2023b; Qian et al. 2023), video generation (Yuan et al. 2024; Yang et al. 2024a), and autonomous driving (Wei et al. 2024; Palanisamy 2020). We focus on image generation via multi-agent collaboration. DiffusionGPT (Qin et al. 2024) employed LLMs for model selection in image generation. SLD (Wu et al. 2024) used LLMs for self-correction of the generated image. CompAgent (Wang et al. 2024) leveraged LLMs to coordinate and plan the image generation process into sub-steps. Unlike these works, we use LLMs to plan 3D layouts, bridging the gap between linguistic understanding and 3D spatial reasoning, particularly in complex 3D scenes."}, {"title": "Method", "content": "In this section, we introduce our MUSES for 3D controllable image generation. As shown in Fig. 2, it is a generic AI system with distinct multi-modal agent collaboration pipeline. Specifically, our MUSES consists of three collaborative agents including Layout Manager for 2D-to-3D layout lifting, Model Engineer for 3D object acquisition and calibration, Image Artist for 3D-to-2D image rendering."}, {"title": "Lifting: 2D-to-3D Layout Manager", "content": "To achieve precise 3D control, we first plan 3D layout according to the user input. Specifically, we employ LLM (e.g., Llama3 (AI@Meta 2024)) as a layout manager, due to its great power of linguistic understanding. To alleviate planning difficulty, we design a 2D-to-3D lifting paradigm for progressive transition from 2D to 3D layout in Fig. 3.\n2D Layout Planning via In-Context-Learning. We start with 2D layout planning to determine the 2D location of objects. Apparently, asking the LLM directly to generate an object layout is not ideal, as it may struggle with managing bounding boxes in images. Hence, we leverage In-Context Learning (Dong et al. 2022), allowing the LLM to follow instructions with a few examples from a 2D layout shop. We use the NSR-1K dataset (Feng et al. 2024) as the 2D layout shop, since it contains over 40,000 entries with diverse prompts, objects, and the corresponding bounding boxes. First, we use the CLIP (Radford et al. 2021) text encoder to compare similarities of user input and text prompts in the shop. Then, we select the top five 2D layouts with the highest similarity scores. Finally, we feed these contextual lay-"}, {"title": "Calibrating: 3D Model Engineer", "content": "After planning 3D layout of user input, the next step is to find 3D models of the specified objects. To achieve this, we introduce a 3D Model Engineer which comprises two key roles: Model Retriever for acquiring 3D models of objects according to the 3D layout, Model Aligner for calibrating the orientations of acquired 3D models to face the camera.\n3D Model Retriever: Retrieve-Generate Decision Tree. To obtain 3D object models with efficiency and robustness, we design a concise Model Retriever via the decision tree of retrieval and generation in Fig. 4. Our motivation is that, 3D object models from internet often exhibit richer diversity and higher fidelity, compared to those generated by text-to-3D synthesis. Hence, we prioritize the retrieval of existing 3D models in the decision tree, which not only enhances the overall visual quality of 3D models, but also reduces computational load in text-to-3D synthesis.\nSpecifically, our model retriever is based on a self-collected 3D model shop (300 3D models of 230 object categories) that is built in an online fashion. First, this model retriever looks for 3D models of queried objects from the current shop, using the object name as the search key. Second, if it does not find any 3D models of a queried object from this shop (e.g., Batman in Fig. 4), it will search them online, e.g., on the professional website (https://www.cgtrader.com). For each object, there may exist a number of 3D models from such a web. Hence, we performs CLIP text encoder to calculate similarity between object name and online item title, and select the 3D model with the highest similarity. Third, if all the online search fails in finding 3D models of queried objects (e.g., Comet Halley in Fig. 4), then we will employ a text-to-3D generation model like Shap-E (Jun and Nichol 2023) or 3DTopia (Hong et al. 2024) to synthesize the corresponding 3D model. Finally, we put the newly-found object models into 3D shop for enhancing shop diversity and versatility of future usage. Via such a model retriever, we can quickly obtain appropriate 3D models of query objects and remains up-to-date with the latest available high-quality models, ensuring both efficiency and robustness.\n3D Model Aligner: Face-Camera Calibration. As 3D models are acquired from internet or generation, their orientations may not align with the expected ones in the planned 3D layout. To address this, we introduce a novel 3D Model Aligner, which can calibrate the original orientation of 3D object models to face camera, for further usage along with"}, {"title": "Rendering: 3D-to-2D Image Artist", "content": "So far, we have obtained 3D layout and aligned 3D models of objects from the user query. Given these 3D materials, we next introduce a concise image artist that can generate the 3D-controllable image, based on a 3D-to-2D condition rendering as shown in Fig. 2. First, we assemble all the 3D object models into a complete scene, according to the 3D layout. To ensure consistent and accurate 3D scene composition, we develop a comprehensive setting of parameter configuration in Blender, including settings of global environment, rendering, camera, light, and object. Our imple-"}, {"title": "Experiment", "content": "Datasets and Metrics. We first conducted experiments on T2I-CompBench (Huang et al. 2023) due to its comprehensive evaluations of object count and spatial relationships. However, T2I-CompBench lacks detailed text prompts of object orientations and camera views. To fill this gap, we further introduce T2I-3DisBench (3D image scene), a dataset of 50 detailed texts that encapsulate complex 3D information (details provided in Supplementary Material Section B). We conducted both automatic and user evaluations on our T2I-3DisBench. Since the metrics of T2I-compBench are inadequate for assessing these more detailed 3D features, we uniquely employed Visual Question Answering (VQA) on InternVL (Chen et al. 2024b). Specifically, we fed instructions (shown in Supplementary Material Section B) to InternVL, asking it to rate the generated images considering four key dimensions: object count, orientation, 3D spatial relationship, and camera view. Additionally, we conducted user evaluation on our T2I-3DisBench, where participants rate the images based on the same four dimensions.\nImplementation Details. Our MUSES is designed to be modular and extensible, allowing for the integration of various LLMs, CLIPs and ControlNets. In our experimental"}, {"title": "Full Prompts for Planning", "content": "We present our complete LLM planning prompts in Fig. 6, including 2D layout planning, 3D depth planning, orientation planning, and camera view planning. We fully take advantage of ICL and CoT prompting technique to enhance LLM's reasoning and decision-making capabilties."}, {"title": "T2I-3DisBench Benchmark", "content": "Owing to the absence of a suitable textual dataset involving multiple objects with various orientations, 3D spatial relationships, and camera views, we construct our own benchmark, namely T2I-3DisBench (3D image scene Benchmark). The benchmark construction process begins with the careful crafting of 10 such complex and detailed prompts. To expand the dataset, we leveraged Claude AI\u00b9 to imitate and generate remaining 40 similar texts, which are subsequently refined by human experts to ensure quality, diversity and consistency. Fig. 7 presents some representative examples in our T2I-3DisBench textual dataset. For evaluation metrics, we find that traditional metrics such as CLIPScore (Radford et al. 2021) or BLIP-CLIP (Li et al. 2022) lacked the necessary precision to capture nuanced details like object orientations or camera views, and the metrics of T2I-compBench (Huang et al. 2023) are inadequate and inaccurate for assessing detailed 3D features. Hence, we employ InternVL (Chen et al. 2024b), a Vision Large Language Model (VLLM), to score the generated images across four dimensions: object count, object orientation, 3D spatial relationship, and camera view. We feed InternVL with the following instruction:\nText:\nHow well does the image match the text? You need to consider (1) object count, (2) object orientation, (3) 3D spatial relationship between objects, (4) camera view. Return a tuple (\u201cscore\u201d, X.XXXX), with a float number between 0 and 1. The higher value represents higher text-image alignment."}, {"title": "Implementation details in Blender", "content": "In this section, we provide a comprehensive overview of the procedures and code used to assemble 3D objects into a complete 3D scene and render it into a 2D image using Blender (Community 2018). First, we need to initialize a Blender bpy environment, configuring global scene settings and rendering settings. Then, we need to configure the camera parameters according to the planned 3D layout. Subsequently, we import all the 3D objects into the environment and set their corresponding parameters based on the planned 3D layout. Finally, we render the 3D scene into a 2D image, and transform it into a depth map and edge map, which are later leveraged for fine-grained control in the ControlNet (Zhang, Rao, and Agrawala 2023). Our codes are all imple-"}, {"title": "Environment Initialization", "content": "To render a 3D scene into an image, we begin by initializing the Blender environment, setting up both global scene and rendering parameters. For global scene settings, we set background to a low-intensity gray (RGB: 0.1, 0.1, 0.1) using a shader node, creating a consistent gray backdrop. A global light source is positioned at (0, -5, 10) directly above the front of the object for uniform lighting across experiments. For rendering settings, the Blender Cycles rendering engine is used for high-fidelity image output. Depth pass (use_pass_z) is enabled to extract depth information during rendering, and the color depth is set to 16-bit for better quality. The output resolution is configured to 1024x1024 pixels. The rendered image is saved as a PNG file in the specified directory. Our complete codes are shown in the following."}, {"title": "Camera Configuration", "content": "After initialize the environment, we configure the camera parameters according to the 3D layout planned by our layout manager. The parameters of camera include the camera position and orientation, corresponding to location and rotation_euler in bpy, respectively. We need to translate the camera view in the 3D layout into a parameterized form that Blender understands. For example, the camera position parameter (x,y,z) for \u201ctop view\u201d is set to (0,1,15). Our full conversion rules are implemented as follows:"}, {"title": "Object Parameter Settings", "content": "Next, we import all the 3D objects (OBJ files) into our environment using the bpy.ops.wm.obj_import(filepath =\") function. For each 3D model, we set key parameters, including object size, position, and orientation, which correspond to the dimensions, delta_location, and rotation_euler attributes in bpy, respectively. The complete code for our comprehensive conversion process is provided below:\""}, {"title": "Image Rendering", "content": "After setting all the parameters, we now successfully assemble the 3D scene. We are able to render the complete 3D scene into a 2D image with accuracy using the bpy.ops.render.render(True) function. This 2D rendering is further transformed into both a depth map and an edge map, which are used for fine-grained control in the Control-Net (Zhang, Rao, and Agrawala 2023). To generate the depth map, we use a depth node within Blender to capture the Z-depth values of the 3D objects. This depth information is crucial for understanding the spatial relationships between objects in the scene. For the edge map, we employ OpenCV to detect the contours in the rendered image. This edge map highlights the boundaries and shapes of objects, providing additional information that aids in the precise control and manipulation of the 3D scene. Our specific codes are presented in the following."}, {"title": "Ablation of ControlNet Parameters", "content": "We conduct comparative experiments with different control scales and inference steps of ControlNets to determine the optimal parameter settings. The controlscale parameter ranges from 0.1 to 0.9, and the inferencesteps parameter ranges from 5 to 25. As shown in Fig. 8, we select several representative parameter values and visualize the comparison results. The results indicate that increasing the control scale enhances the alignment of the generated images with the input condition images. However, this improvement comes at the cost of reduced image quality. To balance these effects, the optimal control scale parameter is identified as 0.5. For inference steps, the quality of the generated images improves with an increasing number of steps. However, once the number of steps exceeds 20, the image quality gains a plateau. Therefore, 20 inference steps are chosen as the optimal inference parameter in our experimental setup."}, {"title": "More Qualitative Comparisons", "content": "Figure 9 and Figure 10 present more qualitative comparisons between our MUSES and existing state-of-the-art methods, including both open-source models and commercial API products, such as Stable Diffusion V3 (Esser et al. 2024), DALL-E 3 (Betker et al. 2023), and Midjourney v6.0 (Midjourney 2024). Obviously, our systematic collaborative approach generates images that are more faithful to the details of the text, in terms of object count, orientation, 3D spatial relationships, and camera view. Additionally, when dealing with such complex and detailed prompts, our system incorporates a human verification step following the 2D-to-3D Layout Manager's initial planning. This human-in-the-loop approach further ensures that the planned layout reasonably represents the details in the user input. It also demonstrates the potential for human-AI collaboration in tackling such complex visual generation tasks."}, {"title": "Conclusion", "content": "We introduce MUSES, a multi-agent collaborative system for precise 3D controllable image generation. By integrating 3D layouts, models and simulation, MUSES achieves fine-grained control over 3D object properties (e.g. object orientation, 3D spatial relationship) and camera view. To evalu-"}]}