{"title": "EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling Correction", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in Chinese sentences caused by phonetic or visual similarities. While current CSC models integrate pinyin or glyph features and have shown significant progress, they still face challenges when dealing with sentences containing multiple typos and are susceptible to overcorrection in real-world scenarios. In contrast to existing model-centric approaches, we propose two data augmentation methods to address these limitations. Firstly, we augment the dataset by either splitting long sentences into shorter ones or reducing typos in sentences with multiple typos. Subsequently, we employ different training processes to select the optimal model. Experimental evaluations on the SIGHAN benchmarks demonstrate the superiority of our approach over most existing models, achieving state-of-the-art performance on the SIGHAN15 test set.", "sections": [{"title": "1 Introduction", "content": "The purpose of the Chinese Spelling Correction(CSC) task is to detect and correct typos in Chinese sentences, which is a sub-task of the grammar error correction task. It attracts more and more attention as a precursor task in various natural language"}, {"title": "2 Related Work", "content": "This section introduces the related work of this paper, mainly including Chinese Spelling Correction and Data augmentation."}, {"title": "2.1 Chinese Spelling Correction", "content": "With the development of deep learning techniques, the CSC task has recently achieved great improvements. The current mainstream CSC models have leveraged the powerful language modeling capabilities of the BERT [4] model and made various improvements. Soft-Masked BERT[4] model uses soft-masking technique to connect the error detection network and error correction network. SpellGCN [5] incorporates phonetic and visual similarity knowledge into language models using graph convolutional networks (GCNs). REALISE[9] utilizes a selective fusion mechanism to integrate semantic, phonetic, and glyphic information of Chinese characters, and has become widely adopted as a baseline model due to its effectiveness.\nBesides model innovation, other strategies have been explored to enhance error cor-rection in CSC. Two-Ways[20] focuses on improving the generalization and robustness of the CSC model by employing an adversarial attack algorithm to generate adver-sarial samples for training. ECOPO [14] observes that pre-trained language models tend to correct wrong characters to semantically correct or commonly used characters.\nTo address this issue, they propose an error-driven contrastive probability optimiza-tion method. CL[11] introduces curriculum learning into CSC tasks, which has shown promising results."}, {"title": "2.2 Data augmentation", "content": "Data augmentation methods are widely utilized in computer vision(CV)[21] and natural language processing(NLP) [22]. In CV, techniques like random image flip-ping and cropping are commonly employed, while in NLP, methods such as synonym replacement and back translation are prevalent. For text classification, the easy data augmentation (EDA) method proposed by [23] introduces four operations: synonym replacement, random insertion, random swap, and random deletion. Building upon this, [24] further enhances text classification by selectively inserting punctuation marks in the original text, thus preserving word order without introducing excessive noise.\nFor CSC tasks, data augmentation methods have also been extensively applied to expand datasets. Previous approaches[8, 20, 25, 26] commonly employ confusion sets to generate a large number of training samples through diverse pre-training methods on additionally collected corpora. Fine-tuning is then performed using high-quality training data synthesized by Wang271K[27] via ASR and OCR techniques. However, due to variations in the number of pre-trained corpora and construction methods employed, it becomes challenging to conduct fair evaluations and achieve complete result reproducibility. To address this, [28] constructs a new dataset, CSCD-IME, by simulating the Pinyin\u00b9 Input method(IME2), and demonstrates the effectiveness of their data augmentation method through comparative experiments. Motivated by these approaches, we propose two simple data augmentation methods that aim to minimize noise when augmenting the dataset."}, {"title": "3 Methodology", "content": "In this section, we first present the proposed data augmentation method and different training procedures, and then introduce other effective strategies."}, {"title": "3.1 Data Augmentation", "content": "The framework of our proposed methods is illustrated in Figure 2. It mainly includes two data augmentation methods: splitting sentences and reducing typos.\nMethod One: Split sentences In order to maintain the semantics of the sentence as much as possible, we select the punctuation marks {\u201c,\u201d, \u201c\u201d, \u201c!\u201d, \u201c?\u201d, \u201c...\u201d, \u201c......\u201d } in the sentence as a cut-off point, thus splitting a long sentence into multiple short sentences. As shown in the right part of Figure 2: a sentence containing two typos is cut into three short sentences. This idea is inspired by the AEDA[24] method for text classification, which uses punctuation marks randomly inserted into sentences for data augmentation. Intuitively, humans can often correct a sentence with a typo by con-sidering only a part of a short sentence, without depending on the complete sentence.\nAfter data augmentation, the distribution of positive and negative samples in the dataset becomes more balanced. We denote the resulting dataset as TrainShortData.\nMethod Two: Reduce Typos Typos in sentences introduce noise, making sen-tences with fewer typos easier to correct. In this way, sentences containing multiple typos can be gradually reduced typos for data augmentation. As illustrated in the lower part of Figure 2: a sentence with two typos is expanded into three sentences through typo reduction (two of the sentences contain only one typo and one original sentence). 98.9% of the sentences in SIGHAN and Wang171K contain at most three"}, {"title": "3.2 Different Training Processes", "content": "We obtained three additional datasets by the two data augmentation methods. Considering the distinct characteristics of each dataset, we trained them individually or in combination to determine the best experimental results. The training process is shown in the lower part of Figure 1, with labels 'a,' 'b,' 'c,' and 'd' representing indi-vidual training on each dataset, while labels 'e,' 'f,' and 'g' denote combined training. Combined training involves initially training on the first dataset and subsequently retraining with the best model weights on the second dataset."}, {"title": "3.3 Other Strategies", "content": "In addition to the aforementioned data augmentation methods, we incorporated several other effective strategies to enhance the model's performance.\nPre-trained language models Pre-trained language models play a crucial role in fine-tuning downstream tasks due to their diverse training corpora and methodologies. ERNIE 3.0[30], which utilizes a large-scale knowledge graph-enhanced corpus for pre-training, has demonstrated state-of-the-art performance in various Chinese NLP tasks. Hence, we adopted ERNIE 3.0 as the underlying pre-trained language model.\nFurther Pre-training Further pre-training has proven beneficial for CSC, leading to improved results. Unlike many other methods [8, 20, 25, 26] that employ confusion sets for constructing pre-training data, we utilized the pseudo-dataset LCSTS-IME-2M\u00b3 [28], generated based on the Pinyin input method (IME\u2074), for pre-training.\nPost-processing [13] observed that the current CSC model tends to overcorrect, and the error correction effect on sentences containing many typos is poor. To address this issue, SCOPE [31] proposed a simple yet effective constrained iterative correction (CIC) strategy, which effectively mitigates the aforementioned problems. We selected CIC as our post-processing operation."}, {"title": "4 Experiments", "content": "In this section, we first present the details of the experimental implementation and the main results. Then analysis and discussion are performed to illustrate the effectiveness of our method."}, {"title": "4.1 Datasets", "content": "\u2022 Training Data Following previous works [5, 8, 9], we use the SIGHAN training data[17-19], which consists of a total of 10K manually annotated data. At the\nsame time, we also included 271K data automatically generated by ASR and OCR techniques, denoted as Wang271K[27]. The SIAHGN and Wang271K are combined and recorded as TrainData.\n\u2022 Eda Data Our data augmentation is mainly based on TrainData. The dataset enhanced by data augmentation method one is denoted as TrainShortData, and the dataset enhanced by method two is denoted as TrainReduceData. Then, the dataset obtained by merging the TrainShortData dataset and the TrainReduce-Data dataset is recorded as TrainMergeData. The relationship between these datasets is shown in the upper part of Figure 1.\n\u2022 Test Data We used the SIGHAN test set to evaluate the models, including SIGHAN13, SIGHAN14 and SIGHAN15.\nThe detailed statistics of all the data we use in our experiments is presented in Appendix A."}, {"title": "4.2 Baseline Methods", "content": "To evaluate the performance of our method, we select several latest CSC models as our baselines, include:\n\u2022 BERT [6]: Directly take the BERT model for fine-tuning on the training set.\n\u2022 SpellGCN [5]: Fusion of phonological and visual similarities features into BERT models using graph convolutional network(GCN).\n\u2022 MLM-phonetics[26]: In this method, uses an end-to-end system based on a pre-trained language model with phonetic features.\n\u2022 REALISE [9]: This approach uses the selective fusion mechanism to fuse multiple modal information (semantics, phonetic and graphic) into the model.\n\u2022 \u0415\u0421\u041e\u0420\u041e[14]: The model uses the idea of contrast learning to allow the model to avoid predicting common characters.\n\u2022 LEAD [32]: The method enhances the CSC model by learning phonetics, vision, and meaning knowledge from the dictionary.\n\u2022 SCOPE[31]: The model uses auxiliary fine-grained pinyin prediction tasks to enhance CSC model, and proposes a Constrained Iterative Correction post-processing method to alleviate over-correction."}, {"title": "4.3 Experimental Setup", "content": "To evaluate the performance of different CSC models, character-level and sentence-level evaluation metrics have been widely used. These include accuracy, precision, recall, F1 score, and false positive rate (denoted as FPR). The sentence-level metrics are more stricter than the character-level metrics because there may be multiple typos in a sentence, and a sentence is considered correct only when all typos in a sentence are detected or corrected. We use sentence-level detection and correction metrics.\nOur implementation details are presented in Appendix B."}, {"title": "4.4 Experimental Results", "content": "Table 1 illustrates the performance of our method and the baseline model on the SIGHAN test set.\nFrom Table 1, we can observe that:\n\u2022 Our EDA(BERT) model outperforms its direct base model BERT across all evalua-tion metrics and datasets. Specifically, at the correction level, EDA(BERT) achieves a 5.8% higher F1 score on SIGHAN13, a 5.9% higher F1 score on SIGHAN14, and an 8.1% higher F1 score on SIGHAN15. Compared with other models, it surpasses all previous state-of-the-art models on the SIGHAN14 and SIGHAN15 test sets, achieves competitive results on the SIGHAN13 test sets. Notably, on the SIGHAN13 dataset, the detection F1 is only 0.6% lower and the error correction F1 is 0.4% lower compared to the best model ECOPO [14]."}, {"title": "4.5 Analysis and Ablation Study", "content": "We use the BERT base model to perform ablation experiments on the SIGHAN15 test dataset."}, {"title": "4.5.1 Different pretrained language models", "content": "To assess the impact of data augmentation when employing different Pre-trained Language Models (PLMs), We select four PLM (ROBERTa-wwm-ext5 [33], MacBERT6, RoCBert[34] and ERINE 3.07[30] ). We conduct comparative experiments using both the TrainData and TrainShortData."}, {"title": "4.5.2 Different training processes", "content": "In order to fully validate the effectiveness of different data augmentation methods, we conducted comparative experiments using different datasets and training processes, as shown in Figure 1. The experiments results, presented in Table 3, were divided into four groups: the first and second groups did not undergo further pre-training, while\nthe third and fourth groups underwent further pre-training. Additionally, the first and third groups did not employ CIC post-processing, while the second and fourth groups utilized CIC post-processing.\nAnalyzing the comparison experiments, we can find that:\n\u2022 Both further pre-training and adding CIC post-processing can effectively improve the effect of the model.\n\u2022 Upon comparing the results of the four cases (a, b, c, and d), it becomes evident that data augmentation method one effectively improves the model's precision, while data augmentation method two enhances its recall. The merged dataset combines the advantages of both methods.\n\u2022 Training on one dataset and then on another yields superior results compared to training on a single dataset alone. Specifically, training on the TrainReduceData dataset followed by the TrainShort Data dataset yields the best outcomes."}, {"title": "4.5.3 Ablation Study", "content": "The experimental settings that yielded the best results on the SIGHAN15 test set were as follows: employing further pre-training; conducting training on TrainReduceData followed by TrainShortData, and utilizing CIC post-processing method. We performed ablation studies on SIGHAN15 with the following settings: (1) removing the TrainShortData (w/o Short); (2) removing the TrainReduceData (w/o Reduce); (3) removing further pretraining (w/o FPT); (4) removing constrained iterative correction post-processing (w/o CIC); The results are presented in Table 4. It is evident that regardless of which component is removed, the performance of EdaCSC declines. This comprehensive demonstrates the effectiveness of each component in our method."}, {"title": "5 Conclusion", "content": "In this paper, we propose two simple data augmentation methods for Chinese Spelling Correction. The first method involves splitting long sentences into shorter"}, {"title": "6 Limitations", "content": "Our proposed data augmentation method demonstrates promising results in Chi-nese Spelling Correction tasks. However, it requires further validation in other related tasks such as Grammatical Error Correction, Named Entity Recognition, and Text Classification. One limitation of our method is that it cannot be applied to a specific type of dataset where all sentences are short and contain at most one typo. Addition-ally, it is important to consider that data augmentation increases the dataset size, which in turn extends the model training time."}, {"title": "7 Declarations", "content": "Conflict of interest The authors have no conflicts of interest to declare that are relevant to the content of this article.\nEthics approval This article has never been submitted to more than one journal for simultaneous consideration. This article is original.\nData Availability The datasets analysed during the current study are available in the https://github.com/wdimmy/Automatic-Corpus-Generation.\nCode availability Code and data used in this paper are publicly available at https://github.com/CycloneBoy/csc_eda."}, {"title": "Appendix A Statistics of Datasets", "content": "Detailed statistical information of the dataset is shown in Figure A1. It is worth noting that the SIGHAN dataset is in Traditional Chinese. Like previous work, we use the OpenCC tool to convert it into Simplified Chinese."}, {"title": "Appendix B Implementation details", "content": "Our models are implemented using the Pytorch [35] framework with the Transformer[36] library. Our baseline models are BERT[6] base model, which has 12 transformers layers and 12 attention heads with a hidden state size of 768. We initialize the BERT encoder with the weights of ERNIE 3.0[30] base model.\nWe train our model with the AdamW optimizer for 20 epochs, with learning rate warming up and linear decay, and the warming up step is 10k. The learning rate set 2e-5, a training batch size of 32, and a maximum sentence length of 130."}]}