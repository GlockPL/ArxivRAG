{"title": "CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients", "authors": ["Shengjun Zhu", "Siyu Liu", "Yang Li", "Qing Lei", "Hongyan Hou", "Hewei Jiang", "Shujuan Guo", "Feng Wang", "Rongshang Chen", "Xionglin Fan", "Shengce Tao", "Jiaxin Cai"], "abstract": "Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused millions of deaths worldwide. Although effective vaccines have been developed to mitigate severe symptoms, certain populations, particularly the elderly and those with comorbidities, remain at high risk for severe outcomes and increased mortality. Consequently, early identification of the severity and clinical outcomes of the disease in these patients is vital to prevent adverse prognoses. Although traditional machine learning and deep learning models have been widely employed in this area, the potential of large language models (LLMs) remains largely unexplored. Our research focuses primarily on constructing specialized prompts and adopting multi-objective learning strategies. We started by selecting serological indicators that significantly correlate with clinical outcomes and disease severity to serve as input data for the model. Blood test samples often contain numerous missing values, and traditional models generally rely on imputation to handle these gaps in the data. In contrast, LLMs offer the advantage of robust semantic understanding. By setting prompts, we can explicitly inform the model when a feature's value is missing, without the need for imputation. For the multi-objective learning strategy, the model is designed to first predict disease severity and then predict clinical outcomes. Given that LLMs utilize both the input text and the generated tokens as input for generating the next token, the predicted severity is used as a basis for generating the clinical outcome. During the fine-tuning of the LLM, the two objectives influence and improve each other. Our experiments were implemented based on the ChatGLM model. The results demonstrate the effectiveness of LLMs in this task, suggesting promising potential for further development.", "sections": [{"title": "1 Introduction", "content": "COVID-19 emerged in 2019, caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), placing immense pressure on the world. Most patients exhibited symptoms such as cough, muscle pain, dizziness, and sore throat [1, 2, 3, 4, 5, 6], ranging from severe (e.g., acute respiratory distress syndrome (ARDS) and organ failure [7, 8]) to mild (e.g., intermittent dizziness and minor cough). While the overall impact of COVID-19 has diminished, the virus continues to circulate globally, with new variants emerging. This ongoing threat is particularly acute for the elderly and individuals with comorbidities, who are at greater risk of severe illness and adverse outcomes [9]. Therefore, timely identification of patients at high risk for severe complications or death is crucial, especially since some effective treatments must be administered early in the disease course [9]. Traditional machine learning and deep learning approaches have been widely applied to predict disease severity and clinical outcomes [10, 11, 12, 13, 14, 15, 16], typically by inputting a set of serological indicators to predict patient severity or outcomes. However, few studies have explored the potential application of LLMs in this task. The primary distinction between LLMs and traditional models lies in their pre-training on extensive human language datasets, which endows them with language comprehension abilities [17]. Moreover, LLMs have been extensively applied to various prediction tasks and have shown great prospects. The study by Hao Xue et al. [18] designed time-series data as prompts for LLMs and fine-tuned the LLM to predict future data. They were the first to propose a paradigm that designs data as prompts, transforming the prediction task into a dialogue task. Liang et al. [19] combined textual description data with taxi trip data to predict crowd flows, significantly improving the accuracy of predictions during holidays. They suggested that textual data could contribute to prediction results to some extent. Ding et al. [20] used LLMs to capture the correlations between data, enabling the imputation of missing data in recommendation systems, and demonstrated that this approach outperformed traditional methods. Jin et al. [21] proposed time series forecasting by reprogramming large language models (Time-LLM), where they reprogrammed time-series data into more natural textual prototypes for LLMs and provided textual guidance within the prompt to assist the model in making predictions. Jin et al. argued that LLMs show great potential for time-series forecasting.\nLLMs also exhibit distinct advantages and potential in our task. For"}, {"title": "2 Dataset and pre-processing method", "content": "In this study, we systematically collected data from hospitalized COVID-19 patients treated at Tongji Hospital in Wuhan, China, between January and May 2020. Each patient underwent multiple serological tests during their hospitalization. Therefore, our dataset includes blood sample data from various time points for each patient, encompassing rich demographic features and detailed records of key serological indicators. Specifically, our dataset includes information from 616 patients, with a total of 6,483 blood test samples collected.\nThe data preprocessing flowchart is shown in Figure 2. Our dataset is divided by patient, ensuring that no samples from the same patient appear in"}, {"title": "3 Method", "content": "Our model utilizes a specialized prompt design to improve its robustness in processing serum data. An example of this prompt design is illustrated in Figure 3. We will first design an instruction to tell LLM the task background. The instruction is \"As an experienced clinical medicine expert, predict COVID-19 severity (severe/mild) and predict clinical outcome (sur-"}, {"title": "3.1 Specific prompt design", "content": "vive/death) based on serum report. The serum report is as follows.\". Then, all individual feature values in the model input are transformed into a prompt for the LLM. When a feature value is missing, we explicitly inform the model of its absence by design text \"This feature's value is missing\". The LLMS, which utilize a self-attention architecture, can effectively interpret this semantic information. Upon receiving a \"feature's value missing\" indication, the model will allocate less attention to that feature and redirect its focus to other features. As a result, the model predicts the severity of the patient's disease and clinical outcomes based solely on non-missing feature values, thereby effectively eliminating potential disturbances caused by missing feature values and ensuring the accuracy and reliability of the prediction results. This approach will improve the model's robustness in the presence of missing values."}, {"title": "3.2 Multi-Objective Learning Strategy", "content": "We begin by highlighting the characteristics of autoregressive models, such as ChatGLM. This model leverages the self-attention mechanism of the Transformer architecture, which enables it to effectively capture long-range dependencies within the input sequence. During the autoregressive generation process, it initiates the sequence with an initial symbol and progressively generates subsequent words, with each output intricately dependent on both the input and the content previously generated.\nBased on the autoregressive characteristics, we propose our multi-objective strategy. The model first predicts the severity of the patient based on the digital information and text within the prompts we input. Then, the model combines the prediction results of severity and the data information and text within the prompts again to predict the clinical outcome of the patient. Broadly, the model's outputs fall into four categories, (1) mild and survive, (2) mild and death, (3) severe and survive, and (4) severe and death, as illustrated in Figure 3. Through our multi-objective learning strategy, the first severity prediction result informs the clinical outcome prediction. If severity is assessed as mild, the likelihood of death becomes nearly impossible. Therefore, the model's output, (2) mild and death, are excluded. It greatly reduces the chance of making one type of mistake."}, {"title": "3.3 Overall Framework", "content": "We first employed the Gradient Boosting algorithm for feature selection on the training set of DataSet A, identifying the top five features most relevant to disease severity and clinical outcomes, respectively. We then took the union of these features as input for traditional model predictions, which also served as the foundational data for constructing prompts for the large language model (LLM). For traditional models, due to structural limitations, we can only perform single-objective prediction tasks. Therefore, we trained the traditional models on the training set of DataSet A and evaluated it on its test set. For our CovidLLM model, we adopted a multi-objective learning strategy to predict two objectives simultaneously. Our CovidLLM is implemented based on the ChatGLM model and was trained using the P-tuning fine-tuning method on the training set of DataSet B, with evaluation conducted on its test set. DataSet A and DataSet B are two forms of a dataset, where DataSet A is processed for traditional models and DataSet B is processed for LLMs."}, {"title": "3.4 Implement Detail", "content": "Our approach is implemented based on ChatGLM-6b [22], an autoregressive dialogue generation model built on the Transformer architecture, which excels in language understanding and generation, facilitating bilingual conversations in Chinese and English. We employ the P-tuning method for fine-tuning, which utilizes hyperparameters provided in the GitHub repository for this project. Unlike traditional methods that adjust prompts solely at the input layer, P-tuning [23] integrates prompt tokens (i.e., embedding"}, {"title": "3.4.1 LLM and feature selection model", "content": "vectors from the Prefix Encoder) at each layer of the Transformer blocks. This approach enables the model to respond more effectively to task-specific requirements and enhances its predictive capabilities at deeper levels. For feature selection, we utilize the Gradient Boosting algorithm. During training, this algorithm assesses the importance of features based on their contribution to the splits in each decision tree. This mechanism allows it to automatically identify which features significantly enhance the predictive performance of the model. Our parameter settings include n_estimators = 100, learning rate = 0.01, and max depth = 3. All experiments are implemented in Python 3.6 with TensorFlow 1.14.0 on a computer with CPU Intel Xeon Gold 6138 @ 2.00GHz (40 cores) and GPU NVIDIA RTX2080Ti."}, {"title": "3.4.2 Compared models", "content": "Adaptive Boosting (AdaBoost)[24] is a widely used ensemble learning algorithm introduced by Yoav Freund in 1995. Its primary aim is to improve the accuracy and generalization of the classification by combining multiple weak classifiers, such as decision trees, into a robust classifier. The Gradient Boosting model[25] builds on this concept by iteratively training multiple weak learners and aggregating their output to create a strong learner. This approach systematically refines the predictions of the model through each iteration. Random Forest [26] is another ensemble learning method that constructs numerous decision trees based on the decision tree algorithm. It synthesizes the predictions from these trees to produce a final outcome, improving overall accuracy. The K nearest neighbor classifiers (KNN)[27] operate on the principle of measuring the distances between the data points. This algorithm identifies the K nearest neighbors to the predicted point and determines its class or value based on the characteristics of these neighbors."}, {"title": "4 Result", "content": ""}, {"title": "4.1 Metrics", "content": "We evaluated our model using several key metrics, precision, recall, F1 score, and accuracy (ACC). Precision measures the reliability of the model's positive predictions, indicating the proportion of true positives among all predicted positives. Recall focuses on the model's ability to identify all actual"}, {"title": "4.2 Feature selection result", "content": "We employed the GradientBoost model to perform feature selection for both disease severity and clinical outcomes. We identified the top five features relevant to each objective, along with their corresponding importance values. As shown in Figure 4, the top five features associated with clinical outcomes are Lymphocyte Percentage (LYMPH%), hypersensitive C-reactive protein (hs-CRP), Neutrophil Percentage (Neu%), Hypertension (HBP), and Age. For disease severity, the top five features include D-Dimer, Lymphocyte Percentage (LYMPH%), Creatinine (Cre), Albumin (ALB), and Indirect Bilirubin (BC). Then, we took the union of these two feature sets. This resulting set was utilized in traditional models to predict disease severity and clinical outcomes, and it also served as foundational data for constructing prompts for the large language model. The union of these features includes LYMPH%, Age, hs-CRP, Neu%, HBP, D-Dimer, Cre, ALB, and BC."}, {"title": "4.3 Prediction of severity", "content": "Figure 5 presents the confusion matrix for our model compared to several baseline models in the task of predicting disease severity. We observed that traditional models such as GradientBoost, AdaBoost, and RandomForest tend to misclassify patients as having severe cases. In contrast, our proposed CovidLLM and KNN models do not exhibit this tendency, maintaining strong predictive performance across both categories. Table 2 summarizes the performance comparison between our model and other baseline models. We can observe that CovidLLM (our model) can achieve the best ACC of 70.29% and its performance in predict class 0 and class 1 achieves the best F1-score. Additionally, our model obtained the best precision in two-class prediction."}, {"title": "4.4 Prediction of clinic outcome", "content": "Figure 6 illustrates the confusion matrix for our proposed model compared to baseline models in predicting clinical outcomes. Analysis of this matrix reveals that traditional models tend to misclassify patients as category 0 (survive), leading to an overestimation of accuracy for this category. In contrast, our language model-based prediction method maintains high accuracy across both categories, demonstrating greater robustness."}, {"title": "5 Discussion", "content": "Our research investigates the application of LLMs in predicting the severity and clinical outcomes of COVID-19 patients, offering new insights into patient prognosis. This approach is not limited to COVID-19 but can also be applied to other diseases. Specifically, we leverage the capability of LLMs to process textual information by directly informing the model when certain feature values are missing. This enables the model to focus on available features while effectively ignoring those that are absent. We also propose a multi-objective learning strategy that capitalizes on the autoregressive generation capabilities of the model. In this framework, the model first predicts patient severity and subsequently forecasts clinical outcomes. By positioning"}, {"title": "5.1 Findings in features", "content": "Our study also employed the GradientBoost model for feature selection regarding disease severity and clinical outcomes, identifying the top five features associated with them, respectively. The features related to clinical"}, {"title": "5.2 Strengths and Weakness", "content": "In our study, we utilize a large language model (LLM) as the core prediction tool. Unlike traditional methods, our approach formats data as prompts, allowing for greater flexibility and enabling us to directly inform the model of any missing feature values. We view this as a specialized form of text-assisted prediction. Previous research has already suggested that combining the auxiliary text information will improve the performance of LLMs. Our multi-objective learning strategy provides the model with more accurate la-"}, {"title": "5.3 Future Works", "content": "Given the flexible input format of LLMs, we plan to incorporate additional information for predicting the severity and clinical outcomes of COVID-19 patients, including patient self-reports and other textual data. LLMs demonstrate strong predictive capabilities in time series, and the relationship between certain serological indicators and disease severity can change over time [40, 41, 42]. Therefore, we aim to integrate time series data and serological indicators as inputs for predicting severity and clinical outcomes. Furthermore, we intend to explore the effectiveness and potential of LLMs in predicting severity and clinical outcomes for other diseases, broadening the application of our findings."}, {"title": "6 Conclusion", "content": "Our research uses the GradientBoost algorithm to screen features related to disease severity and clinical outcomes and then designs these features as special Prompts. Use these special prompt and multi-objective learning strategies to fine-tune large language models to predict the severity and clinical outcomes of patients. The features related to severity screened out in our study include D-Dimer, Lymphocyte Percentage (LYMPH%), Creatinine (Cre), Albumin (ALB), and Direct Bilirubin (BC). The characteristics related to clinical outcomes screened out in our study include Lymphocyte Percentage (LYMPH%), hypersensitive C-reactive protein (hs-CRP), Neutrophil Percentage (Neu%), Hypertension (HBP), and Age. Our compara-"}, {"title": "4.1 Metrics", "content": "We evaluated our model using several key metrics, precision, recall, F1 score, and accuracy (ACC). Precision measures the reliability of the model's positive predictions, indicating the proportion of true positives among all pre- dicted positives. Recall focuses on the model's ability to identify all actual\nPrecision =$\\frac{\u03a4\u03a1}{TP+FP}$ (1)\nRecall =$\\frac{\u03a4\u03a1}{TP+FN}$ (2)\nF1 Score = 2*$\\frac{Precision * Recall}{Precision + Recall}$ (3)\nAccuracy =$\\frac{TP+TN}{TP+TN+FP + FN}$ (4)\nTP (True Positive) denotes the number of samples correctly predicted as positive. FP (False Positive) denotes the number of samples incorrectly predicted as positive. TN (True Negative) denotes the number of samples correctly predicted as negative. FN (False Negative) denotes the number of samples incorrectly predicted as negative. We think the ACC and F1 scores are the best metrics."}]}