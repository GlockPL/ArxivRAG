{"title": "Individual Bus Trip Chain Prediction and Pattern Identification Considering Similarities", "authors": ["Xiannan Huang", "Yixin Chen", "Quan Yuan", "Chao Yang"], "abstract": "Predicting future bus trip chains for an existing user is of great significance for operators of public transit systems. Existing methods always treat this task as a time-series prediction problem, but the 1-dimensional time series structure cannot express the complex relationship between trips. To better capture the inherent patterns in bus travel behavior, this paper proposes a novel approach that synthesizes future bus trip chains based on those from similar days. Key similarity patterns are defined and tested using real-world data, and a similarity function is then developed to capture these patterns. Afterwards, a graph is constructed where each day is represented as a node and edge weight reflects the similarity between days. Besides, the trips on a given day can be regarded as labels for each node, transferring the bus trip chain prediction problem to a semi-supervised classification problem on a graph. To address this, we propose several methods and validate them on a real-world dataset of 10000 bus users, achieving state-of-the-art prediction results. Analyzing the parameters of similarity function reveals some interesting bus usage patterns, allowing us can to cluster bus users into three types: repeat-dominated, evolve-dominate and repeat-evolve balanced. In summary, our work demonstrates the effectiveness of similarity-based prediction for bus trip chains and provides a new perspective for analyzing individual bus travel patterns. The code for our prediction model is publicly available.", "sections": [{"title": "1. Introduction", "content": "With the development of social economy, people's travel demand and the number of personal vehicles are increasing, and many cities are facing severe traffic congestion, pollution and other problems. Prioritizing the development of public transportation has become a key measure to alleviate these problems in many regions [14]. Accurately predicting future trips of individuals can enhance public transportation services, for example arranging reasonable customized bus route [24], thereby increasing passenger satisfaction and boosting public transportation usage [35]. Therefore, this article aims to predict future bus trip chains for users by analyzing their past bus trip sequences. And to propose an effective prediction model, a careful analysis of bus usage patterns is essential."}, {"title": "2. Literature Review", "content": "In our perspective, generating bus trip chains can be viewed as a subset of the broader problem, human trajectory generation. While there are relatively few articles specifically addressing bus trip chain generation, there is a substantial amount of literature focusing on human trajectory generation. These studies often use GPS data from individuals to predict their next location or a sequence of locations they are likely to visit in the near future [32, 27, 1]. Other studies utilize check-in data from Points of Interest (POIs) to predict the next POI a user is likely to visit [29, 10, 3].\nIn this literature review, we first review the studies on human trajectory generation. Following that, we explore the articles specifically focused on the generation of bus trip chains. Because our work is also related to similarity patterns in individual bus travel behavior, the existing papers related to this topic will also be summarized."}, {"title": "2.1. Trajectory Generation", "content": ""}, {"title": "2.1.1. Two different types of human trajectory generation", "content": "Numerous articles claim to address the topic of human trajectory generation, yet the conception of human trajectory generation varies across these publications. Broadly, these articles can be categorized into two groups.\nThe first category centers around modeling at the individual level and aims to predict a person's future destinations based on their past travel sequences.\nIn contrast, the second category focuses on predicting future travel trajectories for a group of individuals. these articles aim to align some statistic characteristics, such as travel distance distribution, travel time interval distribution and the distribution of frequently visited locations, between the generated trajectories and the real trajectories. Studies in this category often draw parallels between human travel and the motion of microscopic particles, such as the well-known exploration-return model (EPR) [26, 20, 25].\nHowever, it is important to note that these approaches are not at the individual level, and while they can generate multiple trajectories, they cannot attribute a specific trajectory to a particular individual. Hence, despite the wealth of literature in this category, it does not directly align with our target of personalized bus trip prediction. As a result, we will not delve further into reviewing articles from this category. Our primary focus will be on the first category."}, {"title": "2.1.2. Different Methods of Individual trajectory generation", "content": "There is a general trend in models for predicting individual trajectory, moving from statistical models to machine learning, and eventually to deep learning.\nIn the early times, a large number of articles relied on statistical language models such as N-grams, Hidden Markov Models [10, 13] or logistic regression [2] for modeling. Subsequently, with the advancement of deep learning techniques and the availability of more GPS data for model training, researchers were able to utilize more complex models for pre- dicting human travel trajectories. From the perspective of the model's backbone network, various approaches have been explored, including Convolutional Neural Networks (CNNs) [18], RNNs [29, 5, 41] and Transformers [15]. From the architectural or pipeline perspective, some methods incorporate inverse reinforcement learning [19] or imitation learning [39, 31] to capture the underlying mechanisms of human travel. Likewise, many articles draw inspi- ration from recommendation systems, suggesting that individuals who have similar travel patter are likely to visit similar locations in the future [8, 4], besides, there are also some pa- pers using knowledge graph to improve the prediction accuracy [38]. There is an abundance of literature in this area, and interested readers can refer to recent review papers for further exploration [11, 9].\nHowever, the focal point of most articles revolves around predicting the next location for a given user. This is mainly because most of these articles are computer-science-oriented and often used for next location or POI recommendation. Therefore, it suffices to focus on predicting the next location for a given user. While, our task goes beyond next trip prediction. Consequently, the value of these articles in our context is limited. Articles that focus on predicting the trajectory of a given user over a subsequent period are as follows: [7] employed Long Short-Term Memory (LSTM) networks with attention mechanisms to capture intrinsic patterns in human travel. Their model predicts not only the next trip but also extends to the next week, 14 days, and 28 days, utilizing beam search to reduce error accumulation. Another recent study by [36] used imitation learning and neural dif- ferential equations to model human trajectories, focusing on a one-day prediction period. Additionally, [18] applied a Generative Adversarial Network (GAN)-based approach, repre- senting human trajectories as image-like structures and using CNN as the backbone. The model predicts over a one-day period. These above articles employed different approaches such as beam search, reinforcement learning, or predicting a segment of human trajectories collectively rather than step by step to address error accumulation issues.\nBesides, activity-based models [22] are alternative methods for generating individual trajectories. This approach involves predicting the type, time, and location of activities to create a daily schedule for an individual. And the prediction of activity time and location can be used in trajectory generation. Commonly used methods for these models include discrete choice models [12], hidden Markov models [16], and optimization methods [21]. However, there are significant gaps between these methods and our project. First, existing models often require personal information such as gender, age, and income, which is not available in our dataset. Additionally, these models typically synthesize trajectories for a typical day in a future year, whereas our objective is to obtain trip chains on some future days.\nDespite these differences, the concept of considering a day's schedule as a complete unit and arranging activities by importance rather than merely by chronological order [12, 21] is"}, {"title": "2.2. Bus Trip Chain Generation", "content": "The seminal article on this topic is [42], which utilized a modified statistical language model called \"mobility n-gram\u201d to predict a user's next trip. Logistic regression was em- ployed to predict whether the user will take a bus trip on a given day and whether it will be the last trip of the day.\nSubsequently, it was proposed that a user's bus trip is actually a reflection of the transition between different activities in [16]. It treated the user's various activities as latent variables and assumes them to be Markovian, thereby forming an HMM for bus trip. Furthermore, [37] employed deep learning models to consider the temporal characteristics of bus trip sequences.\nAdditionally, there is another article merging similar trips into categories [34]. For in- stance, trips with similar origins and destinations that are geographically close and occur within a similar time frame were grouped together. The authors treated these similar trips as a single category and proposed that there is a Markovian relationship between these dif- ferent categories. Thus, a Markov model was built to capture the dynamics among different trip categories, rather than modeling each individual trip separately."}, {"title": "2.3. Similarities in Individual travel pattern", "content": "Most works related to individual's travel or public usage patterns tend to propose some features and use some clustering algorithms, such as k-means, to divide individual travelers into some classes. For example, [40] design features from spatial and temporal dimensions and use k-means to cluster subway users. Some classes related to regular commuters, flexible schedule workers and others can be found. However, as the focus of our work is similarities, we only review the works related to similarity patterns in this part.\nSome researches proposed that individual's travel behavior in a single day can be cate- gorized into several motifs and there are some patterns related to the occurrence of these motifs, for example, some motifs often occur in weekends and the transition of motifs obeys some rules. These features indicate that the travel patterns in some days are more similar while others are not. As a result, synthesis travels in future days using travels in similar days could be a possible solution to prediction future travels.\n[23] proposed human mobility can be classified as 17 motifs and the motifs for an in- dividual can be stabled. Besides, the motifs in consecutive days are often the same. And these motifs accounts for over 95% travel patterns. [28] analyzed the patterns of senior's daily mobility and found that the motifs in weekdays are always simple while the motifs in weekend are complex. [43] used period detection method based on information entropy to detect periodicity in individual's subway usage and found that the period for most travelers is 7 and some travels have a period of 2 days to 6 days. [17] proposed that the travel pat- terns of individual can be largely classified into weekdays and weekends and the residents in different areas are likely to be with different travel patterns. [30] detected periodic fre- quent motifs for subway users, and it was observed that most significant period is 7 days and the top 5 types can constitute about 90% of all motifs. Furthermore, [44] proposed that the indicators related to the inertia of consecutive day and week is important for individual"}, {"title": "2.4. Summary of Existing Literature", "content": "In conclusion, future bus trip prediction is not a hot topic compared with future loca- tion prediction. In addition, many of these articles employ unreasonable assumptions, for instance, assuming that a user's public transportation travel chain follows a Markov or Hid- den Markov Model. Besides, the utilization of deep learning models might overlook certain evident features inherent in a user's public transportation travel chain, as analyzed in the introduction.\nMoreover, there are many researches concluding that trip chains can be classified as some motifs and the most frequent motifs account for a large proportion of all trip chains. Besides, the occurrence of these motifs obeys some distinct rules. Therefore, proposing a method which can utilize these inherent laws of travel behavior better is valuable."}, {"title": "3. Similarity Pattern Analysis", "content": "In this section, the similarity patterns in introduction part will be verified. Unlike some existing works using periods detection algorithms [43, 6], in this section, the formal expression of each pattern will be given first, and then the verifying test will be elaborated. But firstly, data used in our research need to be introduced. The dataset comprises the bus card usage records in Shenzhen city of ten thousand users throughout the entirety of 2018. Each record includes the card ID, departure time, origin station, and destination station. The users in our research were pre-selected based on their relatively high travel frequency [42].\nBesides some symbol description is give as follows. We define a bus trip as a set comprising the following components: the departure time (with an hourly precision), the original station, and the destination station. Formally, a bus trip is denoted as $tripi = {ti, Oi, Di}$.\nA 'bus trip chain' for a specific day is defined as the set of all bus trips which occur on that day, represented as bus_trip_chain = {trip1, trip2, ...}. Since each trip includes travel time information, the trip chain can be treated as a set rather than a list. For example:\nbus_trip_chain = {{8a.m., station A, station B}, {6p.m., station C, station A}}"}, {"title": "3.1. Pattern 1", "content": "The pattern is: the bus trip chain on one Monday is similar to the bus trip chains on other Mondays, and the same pattern applies to other weekdays.\nAnd we give a formal expression of this pattern.\nPattern 1. The set of all Mondays is called $S_1$, the set of all Tuesdays is called $S_2$,..., the set of all Sundays is called $S_7$, and the set of all days is called $S$, therefore, $S = U_{i=1}^7(Si)$.\nAnd we donate two days as $day_i$ and $day_j$, and the trip chains of these two days as $c_i$ and $c_j$. Besides, if there is a measurement to evaluate the similarity between two chains, this measurement is noted as $s$:\n$s : (C_i, C_j) \\rightarrow R$"}, {"title": "3.2. Pattern 2", "content": "This Pattern is that: The trip chains during working days are similar and the trip chains during holidays are also similar (holidays in our research include weekends and festivals).\nThe formal expression of this pattern is:\nPattern 2. The set of working days and holidays are donated as $W$ and $H$, then the fol- lowing inequalities hold:\n$mean_{day_i, day_j \\in S} (s(C_i, C_j)) < mean_{day_i, day_j \\in W} (s(ci, cj))$\nand\n$mean_{day_i, day_j \\in S} (s(C_i, C_j)) < mean_{day_i, day_j \\in H} (s(c_i, c_j))$\nThe meanings of $s, S, day_i, day_j, C_i, C_j$ are the same in Pattern 1."}, {"title": "3.3. Pattern 3", "content": "The pattern is: the bus trip chains in days with shorter time difference tend to be more similar. The formal expression of this pattern is:\nPattern 3. Let $S,s,day_i, day_j, C_i, C_j$ the same as Feature 1, and $d1,d2$ are two integers, which mean the time difference between two days. When d1 > d2. the following inequality holds:\n$mean_{i,j\\in S, i-j=d_1} (s(c_i, c_j)) < mean_{i,j\\in S, i-j=d_2} (s(c_i, c_j))$\nThis formulation suggests an increase in the similarity as the temporal gap between two days diminishes. To verify this statement, it is needed to calculate the similarity score of all day-pairs, and evaluate whether this inequality holds for all $d_1$ and $d_2$. However, calculating the similarity score of all day-pairs is time-consuming as we stated, so we only calculate the similarity score of day pairs with time gap 1, 20, 40, 60, 200 days. Then we plot the relationship between mean similarity and time gap in Figure 5. It can be observed that as the time interval between day pair increases, the similarity decreases."}, {"title": "4. Prediction Model", "content": "In this section, method using the similarity patterns to predict individual's bus trip chains in future days will be elaborated. To effectively use the similarity patterns, we dram inspi- rations from graph classification algorithm and use similarities between days to constitute a"}, {"title": "4.1. Graph Building and Similarity Function", "content": "The existing methods always regard bus trip chains as time series because the bus trips which actually occur successively. However, the bus trips may not be decided successively. For example, the trip from home to work place and the trip from work place return home are likely to be decided concurrently. Therefore, the trip to eat lunch at noon may be decided after the trip from work place return home. As a result, the trip chain in one day is not much like a time series in logic. Therefore, we regard the trips in one day as a holistic unit rather than time series."}, {"title": "4.2. Classification", "content": "Once the graph has been defined, the rest task is determining the probability of labels in unknown nodes, i.e, the probability of trips in future days. We first calculate the probability for each trip separately, in another word, transferring the multi-label problem to many single label problems and use some classification algorithms to address them.\nIn this paper, two methods are attempted to predict the labels of unknown nodes based on the graph. The first one is a classic graph-based classification algorithm, label propa- gation. Moreover, the graph embedding method is also considered, which assigns a vector representation to each node to capture its structural information in the graph. This vector representation can be used as the input for a normal classification model, such as Random Forest, support vector machine (SVM), etc. (we use Random Forest in this work).\nIt is needed to be emphasized that there are many algorithms to address the semi- supervised classification task in graph. However, the classification algorithm is not the focus in our research, so we only choose two classical algorithms to proof that using similarities and treating bus trip chains as graph is effective. We will elaborate these two algorithms in the following parts."}, {"title": "4.2.1. Label Propagation", "content": "The label propagation algorithm is a well-established semi-supervised classification method on graphs. The fundamental principle of this algorithm is that the labels of two nodes that are closer in the graph should be closer. Thus, the label propagation algorithm can be un- derstood as an iterative process, the label of each node is updated based on the labels of its $K$ nearest neighbors. Typically, the new label of a node is obtained by calculating the weighted average of the labels of its $K$ nearest neighbors. This process is repeated iteratively, updating the label of each node until the results converge.\nThe formulation for the label propagation algorithm is Eq.3, where $G = (V, E)$ represents the graph, $V$ is the set of nodes, $E$ is the set of edges, $n$ is the number of nodes, $W$ is the similarity matrix between nodes, $y_i$ denotes the initial label value for node $i$, $n_i$ is the neighbors of node $i$, $l$ is the number of iterations, and $f_{ii}$ represents the label value for node $i$ after $l$ iterations, and $a$ is the refresh rate of label.\n$f_{i}^{l} = a\\frac{1}{\\sum_{j \\in n_i}W_{i,j}}\\sum_{j \\in n_i}W_{i,j} f_{ij}^{l-1} + (1-a)f_{i}^{l-1}$ (3)\nThe label propagation algorithm for a specific trip can be formulated as follows, each day is treated as a node, and whether the trip occurs on that day is considered as its label. Specifically, if the trip occurs on that day, it is regarded as a positive sample, while if it does not occur, it is regarded as a negative sample. In the propagation algorithm, for each day, its label is updated based on the labels of the $K$ nearest days. The update considers the original label of the node and the labels of its most $K$ similar neighbors, which are combined through a weighted sum to obtain the updated label of the node according to Eq.3. This process is performed for each day, and after all the days have been updated, the labels of the known days are reset to their known values. This process is then repeated iteratively until convergence is achieved.\nMoreover, $K$ and $a$ represent two hyperparameters that are embedded in the label prop- agation algorithm. The methods used to calibrate these two hyperparameters will be eluci- dated in a subsequent section of this paper."}, {"title": "4.2.2. Graph Embedding", "content": "Graph embedding is a prevalent technique for processing graph-structured data, with the objective of mapping each node in a graph to a vector representation that encapsulates the node's structural information on the graph. Graph embedding takes many forms, among which we employ the method of spectral embedding directly on the similarity matrix.\nThe algorithm can be summarized in the following steps:\n1. Compute the similarity matrix $W$, which is an $n \\times n$ matrix, where $W_{ij}$ represents the similarity between the $i$-th and $j$-th data points.\n2. Construct the Laplacian matrix $L = D \u2013 W$, where $D$ is the degree matrix with $D_{ii}$ representing the degree of node $i$.\n3. Perform eigenvalue decomposition on the Laplacian matrix $L$ to obtain $L = U\\Lambda U^T$, where $U$ is the matrix of eigenvectors and $\\Lambda$ is the diagonal matrix of eigenvalues.\n4. Select the top $k$ eigenvectors corresponding to the largest eigenvalues $u_1,u_2,..., u_k$, and use them as the coordinates of the low-dimensional embedding, i.e., embedding"}, {"title": "4.3. Label Correlation Module", "content": "In the classification part, we calculate the probability of each trip to occur in future days separately. Formally speaking, if there are $k$ trips, $trip_1, trip_2, ..., trip_k$, the proba- bility of each trip to occur in a certain future day is calculated, which can be noted as $P(trip_1), P(trip_2), ..., P(trip_k)$. And we want to maximize the following formulation Eq.4:\n$P(trip_1, trip_2, ..., trip_k)$ (4)\nBecause there are correlation between different trips and the correlation indicates that:\n$P(trip_1, trip_2, ..., trip_k) \\neq P(trip_1) \\times P(trip_2) \\times ... \\times P(trip_k)$ (5)\nSo, maximizing each $P(trip_i)$ separately is not equal to maximize 4. Therefore particular module is needed to address this problem. There exist various methods to consider label correlations in multi-label classification problems. However, in our problem, the correlations between labels are mainly in pairs. For instance, if there is a bus trip from one place to another, there is possibly a corresponding return trip from the destination back to the origin. This feature distinguishes our bus trip prediction problem from typical multi-label classification problems, as we only need to account for second-order correlations between trips. Subsequently, an algorithm can be designed as follows.\nIf some trips are considered to occur in a future day, then a set A can be constructed by these trips (For example A = {trip1, trip2}). We compute the score of A as 6\n$Score(A) = mean_{i\\in A} p(i) + \\lambda mean_{i,j \\in A} f_{ij}$ (6)\nWhere: $p (i)$ is the predicted probability of an trip $i$ to occur. $f_{ij}^*$ is the normalized frequency of trip pair $i, j$ to appear in the same day according to history record, computed as follows:\n$f_{ij}^* = \\frac{f_{ij}}{0.5 \\times (\\sum_{k\\in S} f_{k,j} + \\sum_{k\\in S} f_{i,k})}$ (7)\nWhere $f_{ij}$ is the frequency of trip pair $i, j$ to appear in the same day and $S$ is the set of all trips."}, {"title": "4.4. Hyperparameters Calibration", "content": "A large number of hyperparameters are present in our method, such as the number of nearest neighbors used in the label propagation algorithm, the dimensionality of the embeddings in the graph embedding algorithm and the parameters in similarity function.\nTo tune these hyperparameters, the known data is partitioned into a validation set and a training set. For example, if bus trip chains in 280 days are given, they are divided into the first 250 days and the last 30 days. A subtask is then constructed to predict the bus trip chains for the last 30 days based on the first 250 days. A grid search is performed on various hyperparameter settings for this subtask to find the best hyperparameters. Searching space of hyperparameters are presented in Table 2."}, {"title": "5. Experiments", "content": "The data used in the experiments has been introduced in the pervious section. And our task is predicting the future bus trip chains in the next 1,7,14 and 28 days utilizing the travel histories in the preceding 280 days. The given 280 days consist of 40 complete weeks."}, {"title": "5.1. Baseline Models", "content": "Our baseline models are described below.\n1. Random Guess: Generating bus trips randomly, based on the occurrence frequency of bus trips in historical days. For example, for a given trip, we donate the occurring times in history days as $n_1$, and the number of history days as $n$, then the probability of that trip to occur in future days is $p$ and $p = n_1/n$. We repeat this process for every trip and every future day to obtain the bus trip chains prediction.\n2. Last: We use the user's bus trip chain from the most recent week as a prediction for their future usage for the following week. If the prediction horizon is $k$ weeks, the user's bus trip chain from the most recent week will be repeated $k$ times.\n3. LSTM (Long Short Term Memory): LSTM is a popular deep learning time-series prediction model. As an improvement over traditional recurrent neural networks, LSTM can capture long-term and short-term correlations in time-series data. The LSTM model employed in this study treats departure time, origin, and destination of a trip as distinct tokens. By utilizing the departure time, origin, and destination from previous trips and transforming them into a sequence of tokens, the model predicts the subsequent sequence.\n4. N-gram: N-gram is a method proposed in [42] for predicting the public transportation usage chain. Its main idea is that the user's future public transportation usage is only related to their past several public transportation usage. We refer to the settings proposed in [42] for n-gram."}, {"title": "5.2. Evaluation", "content": "The evaluation is performed using the following metrics.\n1. Accuracy: The Accuracy is defined as the proportion of correctly predicted trips. For each day, the number of correctly predicted trips is counted twice. Then this value is then divided by the sum of the predicted and actual trip numbers. The expression for the accuracy metric is given as follow.\n$\u0430\u0441\u0441(A, B) = \\frac{2 \\times A \\cap B}{|A| + |B|}$ (8)\nWhere: | * | means the number of elements in a set. A and B are the set of predicted and true trips in the future day. Besides, we define the accuracy as one if there is no trip in a given day and the predicted outcome is also no trip in that day.\n2. Edit Distance: Edit distance is a commonly used measurement of the similarity between two strings of characters. It is defined as the minimum number of single- character insertions, deletions, or substitutions required to transform one string into another. The smaller the edit distance is, the more similar the two strings are. This measure is often used in natural language processing, bioinformatics, and other fields"}, {"title": "5.3. Result", "content": "We conducted predictions for forecasting periods of 1 day, 7 days, 14 days and 28 days, respectively, yielding the Table 4. The last two columns in Table 4 depict the prediction results obtained from the two methods proposed in this study, the label 'Lp' means label propagation, \u2018Eb+rf' means graph embedding plus random forest, the numbers within paren- theses represent the standard error of the metrics. The best performance is indicated in bold and the second best performance is underlined.\nIt is evident that the label propagation algorithm and the algorithm combining graph embedding with random forest consistently outperform the majority of baseline methods and the random guess method shows the worse performance across all prediction horizons and metrics. The methods utilizing the previous week's travel as predictors and the N-gram model display similar levels of prediction accuracy. The LSTM model consistently outperforms"}, {"title": "5.4. Ablation Experiments", "content": "To evaluate whether each feature in similarity function Eq.2 is effective, we remove each feature in the similarity function Eq.2, respectively. For example, when removing the first feature, the similarity function become:\n$sim (i, j) = a_2X_{i,j,2} + a_3\\frac{1}{\\Delta (i, j) + 1}$\nBesides, to evaluate whether label correlations part of our method affects prediction accuracy, We remove the part considering the label correlation and predicted the future travel again by treating the trip with a predicted probability greater than 50% as trips that will occur. Prediction accuracy of ablation experiment is showed in the Figure 6 below. Analyzing Figure 6, it is evident that excluding any component of our method results in a significant decrease in prediction accuracy. For instance, using the label propagation method for a 1-day prediction horizon, the original accuracy is 0.521. However, without the 1st feature, the accuracy drops to 0.448; without the 2nd feature, it decreases to 0.379; without the 3rd feature, it falls to 0.392; and without the label correlation module, it plummets to 0.278. Similar trends are observed for prediction horizons of 7, 14, and 28 days. The same pattern of accuracy reduction can be seen in the graph embedding method, highlighting"}, {"title": "6. Analysis of Hyperparameters and travel patterns", "content": "The hyperparameter values within the similarity function may capture specific traits in users' bus travel behavior. We present the distribution of $a_1, a_2, a_3$ in similarity function 2 in Table 5, therefore.\nFrom Table 5, it's evident that the proportion of 10 in distribution of $a_3$ surpasses $a_2$ and $a_1$. Additionally, the mean value of $a_3$ is the highest, while that of $a_1$ is the smallest. A larger value indicates greater sensitivity, implying its greater significance in the user's bus travel behavior.\nConsequently, we can infer that whether two days are the same weekday is not as signif- icant as whether two days are both working days or neither. This suggests that bus travel"}, {"title": "7. Conclusion and Discussion", "content": "In conclusion, this study focuses on analyzing and predicting bus trip chains for individual users from the perspective of similarity. We first analyze the patterns of bus usage and then propose a graph-based method using similarity patterns rather than time-series-based method to predict future trip chains. The proposed method combines classification algorithm on graph and label correlation module, achieving superior prediction accuracy compared to baseline methods across different prediction horizons and evaluation metrics. Finally, through the analysis of hyperparameters in similarity function, the patterns of users' bus travel behavior can be classified into 3 types: \"Repeat Dominated\", \"Repeat-Evolve\u201d and \"Evolve Dominated\u201d. This result may provide some insights in understanding bus travel behavior.\nWhile, our work is not perfect and there are still room for improvement. For example, incorporating external factors like weather conditions and cultural festivals (e.g., Spring Festival, Mid-Autumn Festival) could improve our model. For example, whether two days are both sunny or rainy can be added to the similarity function Eq.2. Additionally, leveraging the trips of users with similar patterns may also helpful. Exploring users' social relationships, especially family ties, could also enhance the model. For instance, adding nodes representing the trip chains of family numbers, and building a larger graph may be a possible way to address social relationships.\nIt is important to note that, even though the model changes for different travelers, the pa- rameters remain small, especially when using label propagation as the classification method. Therefore, storage requirements for many travelers are manageable. The model needs up- dating as more travel data becomes available. But re-training costs are significantly lower because traditional machine learning methods are used instead of deep learning. For exam- ple, training models for 10,000 travelers takes about 4 hours on a PC with an Intel i5 12400 CPU, keeping the computational burden reasonable.\nFinally, it should be emphasized that treating trips in one day holistically rather than in strict temporal order, and synthesizing future trips according trips in similar days, may be also insightful for modeling human trajectories, because the human mobility pattern is"}]}