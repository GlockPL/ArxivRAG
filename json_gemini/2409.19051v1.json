{"title": "Multimodal Markup Document Models for Graphic Design Completion", "authors": ["Kotaro Kikuchi", "Naoto Inoue", "Mayu Otani", "Edgar Simo-Serra", "Kota Yamaguchi"], "abstract": "This paper presents multimodal markup document models (MarkupDM) that can generate both markup language and images within interleaved multimodal documents. Unlike existing vision-and-language multimodal models, our MarkupDM tackles unique challenges critical to graphic design tasks: generating partial images that contribute to the overall appearance, often involving transparency and varying sizes, and understanding the syntax and semantics of markup languages, which play a fundamental role as a representational format of graphic designs. To address these challenges, we design an image quantizer to tokenize images of diverse sizes with transparency and modify a code language model to process markup languages and incorporate image modalities. We provide in-depth evaluations of our approach on three graphic design completion tasks: generating missing attribute values, images, and texts in graphic design templates. Results corroborate the effectiveness of our MarkupDM for graphic design tasks. We also discuss the strengths and weaknesses in detail, providing insights for future research on multimodal document generation.", "sections": [{"title": "1. Introduction", "content": "Graphic design is a visual medium that communicates information and ideas by effectively organizing text, images, and other elements. While graphic design appears everywhere in various applications, such as websites, advertisements, and printed materials, creating high-quality designs requires expertise and time. Several studies apply machine learning techniques to automate design tasks, such as layout generation [8-10, 19, 26, 27, 32], colorization [14, 22, 23], or typography stylization [29,38]. Apart from the elaboration of individual tasks, there have been holistic modeling of graphic design to solve multiple tasks [11, 35]. While these approaches capture implicit and complex relationships among elements and their attributes, they suffer from limited training samples and have not yet achieved production-level performance. Recent studies show promising performance improvement for layout generation by utilizing the prior knowledge of Large Language Models (LLMs) [19, 26, 32]. The successful use of LLMs for a design task inspires our approach to using multimodal language models for graphic design.\nIn this paper, we investigate multimodal LLMs for holistic modeling of graphic design. We treat graphic design as an interleaved multimodal document consisting of markup language and images. This representation allows us to directly apply multimodal LLMs, which has shown promising"}, {"title": "2. Related Work", "content": "There has been an ongoing research effort in computational support for graphic design, such as layout generation [8-10, 18, 19, 26, 27, 32, 34], colorization [14, 22, 23], typography stylization [29, 38], or general stylization [28]. Several previous works, like ours, infer missing parts or alternative solutions from the surrounding context. Completing a layout from a given partial layout is one of the common subtasks in layout generation [10]. Zhao et al. [38]\nInspired by these studies, we propose a new approach for the holistic modeling of graphic design documents that leverages prior knowledge of multimodal LLM. In this context, our work can be seen as the first attempt to obtain image and text content by generation rather than by retrieval."}, {"title": "2.2. Multimodal Large Language Models", "content": "The recent success of LLMs has led to the development of multimodal LLMs that can recognize and generate images [36]. Several multimodal LLMs, such as GILL [15], Emu [30], and DreamLLM [5], are designed to connect LLMs with a off-the-shelf pre-trained image encoder, such as CLIP [24], and a decoder such as Stable Diffusion [25]. These pre-trained image encoders and decoders are challenging for graphic design completion tasks as they do not support transparent images. Training these encoders and decoders requires a large-scale dataset of image-text pairs. However, collecting such a dataset is challenging as images in graphic design documents are difficult to describe accurately with text.\nAnother approach of multimodal LLMs is representing images as discrete tokens [1, 4, 33]. This approach encodes images into a sequence of tokens via a pre-trained image quantizer, such as VQGAN [6]. Although the publicly available pre-trained quantizers often do not support images"}, {"title": "3. Method", "content": "We first train an image quantizer to encode images into discrete tokens. We then build the MarkupDM by finetuning a pre-trained code LLM with interleaved multimodal documents to incorporate the image modality. We illustrate the overview of our method in Fig. 2 and Fig. 3."}, {"title": "3.1. Image Quantizer", "content": "We train an image autoencoder that encodes images of different-sized images with transparency into discrete token maps with 1/f resolution and decodes them back to the original images. In preliminary experiments, we found that varying the token size according to the image size makes it challenging to train the markup language model in the later stage. Instead, we take a simple but effective approach of resizing the input image to a fixed square size. We follow the previous studies [6, 25] and take the same network architecture and training objectives for our autoencoder, with the only difference related to the alpha channel. We set the number of input/output channels to four and consider L1 reconstruction loss for all channels. When calculating the loss based on RGB-based external models, e.g., the perceptual loss [37], we convert generated RGBA images to RGB images by alpha compositing on a white background. We initialize our model with the weights of a pre-trained RGB image quantizer. For the alpha channel weights, we use the mean values of the corresponding RGB weights."}, {"title": "3.2. Document Representation", "content": "Once we train the image quantizer, we apply the quantizer and convert graphic design templates into unified sequence representations. Our data representation is based on"}, {"title": "3.3. Multimodal Markup Language Model", "content": "We build MarkupDM based on the recent code LLMs, which are specifically tuned for handling coding tasks. We make two extensions to the base code LLM to incorporate the image representation described in Sec. 3.2. First, we extend the vocabulary of the base LLM to include the additional special tokens, such as [boi]. Second, we add new modules dedicated to the image tokens, such as [img:1], the embedding module, and the prediction head. In the embedding module, we first embed the image tokens via the frozen lookup table in our image decoder. We then concatenate them with the positional encodings [31] and project them to the same dimension as the text embeddings. The prediction head for image tokens is similar to text tokens, but uses a different set of parameters and vocabulary, i.e., the codebook size in image quantization.\nWe train our model based on the next token prediction in our sequences to which we randomly apply the fill-in-the-middle transformation [1, 2], allowing the model to predict the missing middle part from the prefix and suffix parts. Our model has to know the next token's modality for inference due to the different prediction heads. We determine the next modality on a rule based on the tokens generated so far."}, {"title": "4. Experiments", "content": "We first evaluate our image quantizer on the image reconstruction task and then our multimodal markup language models on several graphic design completion tasks."}, {"title": "4.1. Image Quantization", "content": ""}, {"title": "4.1.1 Setup", "content": "We use an internal dataset on graphic design templates, similar to the Crello dataset [35]. The design template consists of an ordered set of elements, each associated with an element category, geometric attributes, and design attributes. The template also has global attributes such as canvas size. We use 800,000 RGBA images of non-textual elements in the design templates for training and 133,267 images from the other templates for evaluation.\nWe use the RGB image quantizer from Latent Diffusion Model [25] trained on the OpenImages dataset [16], which is mainly composed of photographs, as a baseline. Specifically, we use the quantizer with the scaling factor f = 16 and the codebook size Z = 16,384 based on the balance between the reconstruction quality and the resulting token length. We finetune the quantizer for 100,000 steps on our dataset using the techniques explained in Sec. 3.1 to adapt it to RGBA images. For further analysis, we finetune the quantizer solely using RGB images without special techniques, which we denote by Ours-RGB. We apply an off-the-shelf background removal tool to convert RGB images to RGBA images for comparison. We use Rembg [7] with the IS-Net model [21] for this purpose.\nWe evaluate the quantizers using the mean squared error (MSE) for RGB and alpha channels and the reconstruction Fr\u00e9chet Inception Distance (rFID) for RGB images, which computes the distance between the feature distributions of the original and reconstructed images. We convert the RGBA images generated by our quantizer to RGB images by alpha compositing on a white background for RGB-based metrics."}, {"title": "4.1.2 Image Reconstruction", "content": "We show the quantitative comparison of image reconstruction using each quantizer in Tab. 1. Both of our quantizers outperform the baseline in terms of RGB-based metrics thanks to fine-tuning images from the same domain. We show qualitative results in Fig. 4. We can see that RGB-based reconstruction with general background removal does not work well, as it removes foreground objects either excessively or insufficiently. In contrast, our quantizer successfully reconstructs RGBA images thanks to the alpha information embedded in the discrete tokens."}, {"title": "4.2. Graphic Design Completion", "content": ""}, {"title": "4.2.1 Setup", "content": "We use the same dataset as the image reconstruction task and convert 165,991 graphic design templates to SVG format to train the models. In the conversion, we represent text elements by  tags, and other elements by  tags. We do not specify attributes if they have the default values. Also, since SVG cannot render multi-line text in a single element, we split a text element into multiple elements when a new line appears.\nWe train our MarkupDM with the fill-in-the-middle (FIM) objective [1, 2] to predict the middle part of the sequence from the prefix and suffix parts. Considering"}, {"title": "4.2.2 Attribute Value Completion", "content": "We show the quantitative results on the attribute accuracy in Tab. 2. Note that the scores between FlexDM and MarkupDM are not fully comparable due to the different formulations and available contexts, i.e., MarkupDM can predict element size using the image size, while FlexDM can not. MarkupDM performs reasonably well compared to FlexDM, showing that they are successfully trained to fill the graphic design templates. Among the MarkupDM, SC1-7B, the largest base LLM, yields the best performance"}, {"title": "4.2.3 Image and Text Completion", "content": "We perform image and text completion tasks using the best MarkupDM, SC1-7B. We show the qualitative results in Fig. 6 for successful image completion, Fig. 7 for successful text completion, and Fig. 8 for failure cases for both tasks.\nWe observe cases where the model generates plausible images by copying other images in the template, using repetition patterns and symmetry as hints, e.g., the top two examples in Fig. 6. Our model also succeeds in generating typical decorations such as buttons and underlays, e.g., the two examples in the lower left. Our model is also good at generating background images that harmonize foreground objects, e.g., the bottom right example.\nFor text completion, our model can generate text that connects with the preceding or following lines in a grammatically correct way in many cases, e.g., the top left example in Fig. 7. Besides multiple lines, there are cases where the model generates text that matches the surrounding texts, e.g., the right example. The bottom left example does not have a strong textual context. Still, our MarkupDM successfully generates text with a typical role, using the position of the target element and the visual decoration as hints.\nIn the failure cases, MarkupDM has difficulty generating images of main objects due to the lack of context and poor image generation ability, e.g., the top left example in Fig. 8. It also fails to generate images that require delicate visual harmonization with other elements, e.g., the middle left example. For text completion, we can see failures due to errors in image understanding, e.g., \"business school back-"}, {"title": "5. Limitations and Discussion", "content": "We introduce MarkupDM, a new class of multimodal LLMs that can generate markup languages with transparent and different-sized images. We investigated MarkupDM's performance in detail using three completion tasks, but it remains unclear whether our model can predict all aspects of an element, including its type, attributes, and content. We also have not verified whether our model can generate the rest of the template when the first part is given. The primary limitation of MarkupDM is its poor ability to generate main images. Increasing the number of training images using other multimodal datasets may help improve the image generation quality. Alternatively, we could consider leaving the main image generation to an external strong image generator or adapting a strong RGB-based multimodal LLM to RGBA images. As extensions to MarkupDM, we are interested in settings that provide additional context via text prompts [12, 13] and incorporate reference designs via retrieval augmentation [8]. Finally, designing interfaces for multimodal LLMs, such as our MarkupDM, to support creative design workflows is another important direction that remains to be explored."}, {"title": "A. Implementation Details", "content": "We build our RGBA quantizer by finetuning a pre-trained RGB quantizer, as described in Secs. 3.1 and 4.1.1. We train our quantizer in 100,000 steps using mixed precision training using bfloat16 a batch size of 8 with a single NVIDIA L4 GPU. We use the Adam optimizer with a learning rate of 1 \u00d7 10-5. We resize the input images to 256 \u00d7 256 pixels. The training takes approximately 2 days to complete.\nWe build our MarkupDM by extending a pre-trained code LLM, as described in Secs. 3.3 and 4.2.1. We train our model in 100,000 steps with a single A100 80GB GPU with several techniques for efficient training, including mixed precision training using bfloat16, gradient checkpointing, and the Flash Attention 2. We use the Adam optimizer with a learning rate of 5 \u00d7 10-5 and a constant schedule. The fill-in-the-middle (FIM) transformation is applied with a probability of 0.9 during training. Specifically, we use the context-level FIM with the token-level span selection in the prefix-suffix-middle format [2]. The training takes approximately 1 day for SC1-1B version, 2 days for SC1-3B version, and 4 days for SC1-7B version to complete."}, {"title": "B. Details on Our Experiments and Additional Results", "content": "We convert structured graphic design documents to SVG format, as described in Secs. 3.2 and 4.2.1. We show in Tab. 3 the element tags and their corresponding attributes used in the experiments.\nFor graphic design completion, we use the top-p sampling with p = 0.9 for generation. We set the maximum number of new tokens to 10 for attribute value completion, 50 for text completion, and 278 for image completion (256 for the image tokens, 2 for the special tokens, and 20 for the image width and height).\nWe provide additional image reconstruction results in Fig. 9 and design completion results in Figs. 10 and 11 by MarkupDM (SC1-7B). We set the temperature to 2.0 to generate more diverse outputs for the font type completion."}]}