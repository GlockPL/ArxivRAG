{"title": "End-to-end Graph Learning Approach for Cognitive Diagnosis of Student Tutorial", "authors": ["Fulai Yang", "Di Wu", "Yi He", "Li Tao", "Xin Luo"], "abstract": "Cognitive diagnosis (CD) utilizes students' existing studying records to estimate their mastery of unknown knowledge concepts, which is vital for evaluating their learning abilities. Accurate CD is extremely challenging because CD is associated with complex relationships and mechanisms among students, knowledge concepts, studying records, etc. However, existing approaches loosely consider these relationships and mechanisms by a non-end-to-end learning framework, resulting in sub-optimal feature extractions and fusions for CD. Different from them, this paper innovatively proposes an End-to-end Graph Neural Networks-based Cognitive Diagnosis (EGNN-CD) model. EGNN-CD consists of three main parts: knowledge concept network (KCN), graph neural networks-based feature extraction (GNNFE), and cognitive ability prediction (CAP). First, KCN constructs CD-related interaction by comprehensively extracting physical information from students, exercises, and knowledge concepts. Second, a four-channel GNNFE is designed to extract high-order and individual features from the constructed KCN. Finally, CAP employs a multi-layer perceptron to fuse the extracted features to predict students' learning abilities in an end-to-end learning way. With such designs, the feature extractions and fusions are guaranteed to be comprehensive and optimal for CD. Extensive experiments on three real datasets demonstrate that our EGNN-CD achieves significantly higher accuracy than state-of-the-art models in CD.", "sections": [{"title": "I. INTRODUCTION", "content": "Cognitive Diagnosis(CD) is the use of students' existing learning records to assess their mastery of unknown knowledge concepts[1]. CD is a multidisciplinary field[2-8] at the intersection of cognitive psychology, educational assessment, and psychometrics[9-13]. It aims to understand and diagnose the underlying cognitive processes of human learning and problem solving [14][25] which is relevant to our lives. Specifically, in the intelligent education system[15-18], CD aims at a comprehensive assessment of students' individual abilities. Fig. 1 shows a toy example of CD. In general, the teacher will select several exercises (e.g., e1...e5) from the exercise bank to form a test paper, and the students will answer the corresponding exercises (e.g., right or wrong). Given student studying records, Our purpose is to reveal the specific cognitive skills(e.g., Infinite series) and knowledge structures possessed by individuals[19], thereby accurately assessing their personal abilities[20-24].\nCD has been developed for decades and can be divided into two main categories. The first is the CD based on statistical methods such as DINA[37], IRT[38], IRR-IRT[29]. The other is CD based on simple Graph Neural Network(GNN), such as NeuralCDM[30], KaNCD[30] and TechCD[32]. These models are all representative models in CD. However, these models adopt linear principles or some non-end-to-end neural networks to explain the complex process of students solving problems[26-28]. It is far from enough to effectively capture the potential personal information of individual students, but it is necessary to extract student information from a global perspective[33-35]. So it is impossible to model students' learning behavior in complex educational platforms[36][37], and it also fails to perform well in the face of large-scale datasets[38-43].\nExisting approaches loosely consider these relationships and mechanisms by a non-end-to-end learning framework, resulting in sub-optimal feature extractions and fusions for CD. In order to extract feature information from studying records more effectively, this paper proposes an End-to-end Graph Neural Networks-based Cognitive Diagnosis (EGNN-CD) model[44][48][49]. EGNN-CD consists of three main parts: knowledge concept network (KCN), GNN feature extraction (GNNFE) and cognitive ability prediction (CAP). First, KCN is an infographic that synthesizes physical information about students, exercises, and knowledge concepts to build CD-related interactions. Second, GNNFE designed a four-channel graph neural network to learn the higher-order features and individual features of KCN end-to-end. It ensuring that feature extraction and subsequent fusion of CD events are comprehensive and optimal predictions. Finally, CAP combines high-level information to predict a student's ability. With such designs, the feature extractions and fusions are guaranteed to be comprehensive and optimal for CD. Experiments on three real datasets proved from all aspects that EGNN-CD has higher accuracy and significance in CD, which is superior to the existing model.\nThe contributions of this article mainly include the following aspects:\n\u2022 This paper proposes an end-to-end cognitive diagnosis model based on graph neural network, which solves the CD problem effectively."}, {"title": "II. RELATED WORK", "content": "CD is a multidisciplinary field that intersects cognitive psychology, educational assessment and psychometrics. Its main purpose is to simulate the potential process of human learning and solving new problems. CD models can be roughly divided into the following two categories.\nDINA[37] and IRT[38] are the most classic CD models. The DINA[37] model is a discrete CD model. This model exploits a knowledge concept mastery vector to model students' mastery of knowledge. The impact of HO-DINA low-latitude potential characteristics on students[39]. FuzzyCDF [40] fuzzy the examinee's skill proficiency, combined with fuzzy set theory and educational assumptions, established a problem mastery model based on the examinee's skill proficiency. IRT [38] is a typical continuous CD model. IRT jointly models the test questions and students through the students' (0) answering conditions. thereby deriving test question parameters and students' potential abilities. Based on IRT, MIRT [31] transforms the one-dimensional latent features of students (0) into multi-dimensional latent features $\\theta_{i=c}(\\theta_{i1},\\theta_{i2}, ..., \\theta_{ik})$, where k is the dimension referred to in MIRT. IRR-IRT [29]introduces pairwise learning and transforms it into CD to model the monotonicity between item responses. These models rely too much on artificially designed estimation functions and handle the differences between subjects with different ability levels in tests, but can only model one knowledge point of a student and cannot reflect the relationship between students and knowledge concepts [51][52]. Thus their oversimplification of cognitive processes leads to limited fitting ability and difficulty in handling enormous datasets [45-47].\nIn recent years, scholars have continuously integrated neural networks into CD and achieved some remarkable results. EIRS[49] is a general framework that explores the partial ordering between interacted and uninteracted exercises. Wang et al [50] integrated the explainability of CD to educational priors into a deep learning-based knowledge tracking method and proposed dynamic CD. Li et al[53] applied CD computer adaptive testing to CD to improve classification accuracy. Ma et al [54] utilize minimal model assumptions to jointly learn the problem of these latent hierarchical structures in CD from observational data. The most representative method is NeuralCDM [30]. It plans students and exercises to consider factor vectors and combines neural networks to simulate complex learning processes. KaNCD[30] considers the associated information between knowledge concepts based on NeuralCDM. TechCD[32] operates a graph convolutional network with bottom discard operations to learn the characteristics of students and exercises, allowing for cross-domain learning[55][58]. Although these models can achieve good results by modeling each module separately, they do not comprehensively consider the mutual influence and are prone to fall into local optimality. At the same time, the feature extraction and fusion results of these models are not back-transferred, so the problem cannot be modeled from a global perspective[56-63]."}, {"title": "III. PRELIMINARY", "content": "The symbols used in the article and their explanations are shown in the Table I."}, {"title": "B. Data Definition", "content": "Student-Exercise Matrix. The Student-Exercise Matrix represents the students' answers in the question bank. It is represented by $y \\in (0, y_{nm})^{N \\times M}$, where N and M represent the number of students and the number of exercises in the question bank, respectively. y represents the corresponding score. That is, y=ynm means that student sn does exercise em and gets a score of ynm, and if y=0, it means that student s\u2081 doesn't exercise em."}, {"title": "C. Problem Definition", "content": "Suppose there are N students, M exercises, and C knowledge concepts in a system, and they can be expressed as $S=\\{S_1,S_2, ...,S_N\\}$,$E=\\{e_1,e_2, ...,e_M\\}$and $K=\\{k_1,k_2, ...,k_c\\}$. Each student has done some exercise questions in the question bank. The studying records R(S,E, Y) represents the record of all students doing the questions. Each record $log(s_n, e_m, y_{nm})$ represents the score ynm that the student sn received after doing the exercise question em. Given studying records R(S,E,Y), the purpose of CD is to learn the student's problem-solving process to model the student's ability. i.e.\n$IF_{CDM} (s,e,\\Theta) \\rightarrow y_{nm}$    (1)\nwhere s and e are feature representation vectors for students and exercises. $F_{CDM}()$ is the CD function. $\\Theta$ is the parameter required for training. And \u0177nm is the predicted performance score."}, {"title": "IV. PROPOSED MODEL", "content": "The EGNN-CD model mainly contains three modules, namely knowledge concept network (KCN), GNN feature extraction (GNNFE) and cognitive ability prediction (CAP)."}, {"title": "A. Knowledge concept network", "content": "The original studying records solely contains the student id/exercise id and the corresponding score. If we directly exploit this information into the GNN module training. We will merely get two parts of information, and this information is the fusion of student and exercise information. It cannot clearly express the students' learning ability and the characteristics of the exercises, and it is not conducive to subsequent result predictions. To this end, high-level information needs to be extracted. The specific form of the initial information is shown in (2). We thus build a tripartite feature knowledge network of students-exercises- knowledge concepts to convey high-level information[32][31]. At the same time, one-hot encoding is used to distinguish each entity. So that the unique high-order information of each entity will be transmitted along KCN during end-to-end learning.\n$s = [e^{(0)}, e_2^{(0)},...,e_M^{(0)},k_1^{(0)},k_2^{(0)},...,k_C^{(0)}]$,\n$e_m = [S_1^{(0)}, S_2^{(0)},...,S_N^{(0)},k_1^{(0)},k_2^{(0)},...,k_C^{(0)}]$,\n(2)"}, {"title": "B. GNN Feature Extraction", "content": "Most existing models ply linear or simple neural networks to learn students' learning processes. Although amazing results have been achieved, existing approaches loosely consider these relationships and mechanisms by a non-end-to-end learning framework. It resulting in sub-optimal feature extractions and fusions for CD. And the Q-matrix is given by relevant expert marks, and there are personal subjective factors of experts. GNNFE has subsequently design a multi-layer perceptron. It ensures that feature extraction and subsequent fusion of CD events are comprehensive and optimal predictions through end-to-end"}, {"title": "C. Cognitive Ability Prediction", "content": "CAP eventually incorporates high-level information to make predictions about student ability. In order to maximize the information of the four paths, we consider their complementarity and correlation in the fused end-to-end neural network layer. For student sn and exercise em, we can obtain information representations from the graph neural network, represented by Sn1, Sn2, em1 and em2 respectively. The fusion operation can be described by the following formula: \u0177nm\u2208(0,1)1\u00d71represents the final prediction result. o is the activation function. (\u2022) indicates the information aggregation function, where direct concatenation is used. W(P) and bP) are trainable parameters. In addition, the student learning process is progressive [38]. That is, as the number of effective questions increases, the student's ability will gradually improve. Therefore, in the training process to ensure\nW(P)\u22650.dropout strategy is adopted to prevent overfitting during the training process. Besides, the cross-entropy function is exerted as the loss function during training."}, {"title": "D. Time Complexity", "content": "Based on the above analysis process, the EGNN-CD is proposed."}, {"title": "V. EXPERIMENT", "content": "In the following experiments, we mainly answer the following research questions:\n\u2022 RQ.1. How does the proposed EGNN-CD model perform in terms of CD compared with existing models?\n\u2022 RQ.2. The influence of each module of the EGNN-CD on the model performance?\n\u2022 RQ.3. The influence of each hyperparameter on the performance of EGNN-CD?"}, {"title": "A. General Setting", "content": "Our experiments were conducted on three datasets, namely Junyi, Math1[40], Math2[40]."}, {"title": "a. Datasets", "content": "Our experiments were conducted on three datasets, namely Junyi, Math1[40], Math2[40]. Table III summarizes the information of these datasets. 5-fold cross validation is employed. AVG#log represents the average number of interactive knowledge concepts for each student. The specific method is shown in (10).\n$AVG_{\\#log} = \\frac{\\sum{I(log(s_n,k_c,y_{nc}))}}{|S||K|}$ (9)\nwhere I()=1if log(sn, kc, ync)exists, otherwise I(\u00b7)=0. According to the data in Table III, Junyi contains 10,000 students, involving 835 knowledge points, but the number of knowledge points interacted by each student is 0.04, which is a large sparse dataset. Math1 and Math2 contain about 4,000 students, involving 11-16 knowledge points, but the number of knowledge points interacted by each student is 4.0-6.1, which is a small dense dataset. These three datasets are commonly used datasets for cognitive diagnosis problems and are very representative."}, {"title": "b. Baseline", "content": "Comparative models include discrete type DINA[37], continuous type IRT [38], MIRT [31], IRR-IRT [29], PMF using matrix decomposition, and the representative model TechCD[32], NeuralCDM [30] in recent years and its variant KaNCD [30]. These are the classic models of CD."}, {"title": "c. Metric", "content": "To demonstrate the performance of the EGNN-CD model in predicting student abilities, we employ 6 widely used metrics[64-66]: Accuracy (Acc), Area Under the Precision-Recall curve (AUPR), Area Under the Curve (AUC), F1 score(F1), Precision (Pre), Recall (Rec). Acc represents the probability of correct prediction for all samples. The higher the value, the better. If ynm, \u0177nm are both greater than 0.5 or less than or equal to 0.5, then f(ynm, \u0177nm)=1, otherwise f(ynm, \u0177nm)=0. AUC and AUPR are designed to deal with the uneven division of positive and negative samples.\n$ACC = \\frac{1}{N}\\sum_{i=1}^N \\sum \\sum f(y_{nm}, y_{nm})$ (10)\n$AUC = \\int h(u)du, AUPR = \\int g(t)dt$ (11)\nTheir values are between 0.5 and 1. The larger the value, the better the model's performance. Where h(u) represents the ROC curve and g(t) represents the PR curve. Pre and Rec are two indicators applied in statistics to measure the accuracy of a two- classification model and F1 takes into account both the precision and recall of the classification model. Its value is between 0 and 1. The larger the value, the better the model.\n$Pre = \\frac{TP}{TP+FP}$, $Rec = \\frac{TP}{TP + FN}$ (12)\n$F1= \\frac{2 \\times Pre \\times Rec}{Pre+ Rec}$ (13)\nWhere True Positives (TP) represents the number of positive classes predicted as positive classes. False Positives (FP) represents the number of negative classes predicted as positive classes. and False Negatives (FN) represents the number of positive classes predicted as negative classes. True Negatives (TN) represents the number of negative classes predicted as negative classes."}, {"title": "d. Training Setting", "content": "In the experiment, we divide the dataset into the training set and the test set at a ratio of 8:2, and apply a 5-fold cross-validation experimental method to avoid the contingency of the results. Diverse datasets have unique training hyperparameter settings. We take the Junyi dataset as an example. EGNN-CD model learning rate of the comparison model is set to 0.003. The number of training rounds is 200. The parameters utilized in network training are initialized using the Xavier method, and the training optimizer adopts Adam[67][68]."}, {"title": "B. Performance Comparison(RQ.1)", "content": "The experimental results of the EGNN-CD model and the comparison model on diverse datasets is recorded in Table V. In order to better analyze the experimental results, we make"}, {"title": "C. Ablation Experiment(RQ.2)", "content": "In order to better observe the impact of the GNN module on model performance, we design three variants of the EGNN-CD model."}, {"title": "D. Hyperparameter Sensitivity Experiment(RQ.3)", "content": "We conduct experiments on the influence of GNN output dimension on learning process modeling. From Fig. 4, we can see that with the increase of dimension d, although the performance of EGNN-CD model fluctuates somewhat, the overall trend is upward. It indicates that the more information in the modeling learning process, the more representative of the characteristics of students and exercises. When d=10, Pre and Rec fluctuate greatly in Math1 dataset, and their expression ability is one-sided. F1 value is a combination of Pre and Rec, and its value changes little, which indicates that F1 can better reflect the change of model performance. As for the impact of the number of GNN layers on EGNN-CD, we increase the number of GNN layers in each dataset respectively. The results are shown in Fig. 5, and the performance of EGNN-CD increases first and then decreases with the increase of the number of layers, and the overall performance does not fluctuate much. This suggests that EGNN-CD is not overfitting, which may be related to the dropout strategy."}, {"title": "VI. CONCLUSION", "content": "This paper proposes an End-to-end Graph Neural Networks-based Cognitive Diagnosis (EGNN-CD) model. EGNN-CD consists of three main parts: knowledge concept network (KCN), GNN feature extraction (GNNFE) and cognitive ability prediction (CAP). First, KCN is aninfographic that synthesizes physical information about students, exercises, and knowledge concepts to build CD-related interactions. Second, GNNFE designed a four-channel graph neural network to learn the higher-order features and individual features of KCN end-to-end. Finally, CAP combines high-level information to predict a student's ability. We fully demonstrate on three real-world datasets that the proposed model achieves higher accuracy and significance in various aspects, especially outperforming existing models in cognitive diagnosis. Furthermore, we also explore the impact of the GNN module on the performance of the EGNN-CD model, which provides a basis for future cognitive diagnosis work."}]}