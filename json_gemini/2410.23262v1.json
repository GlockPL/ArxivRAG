{"title": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "authors": ["Jyh-Jing Hwang", "Runsheng Xu", "Hubert Lin", "Wei-Chih Hung", "Jingwei Ji", "Kristy Choi", "Di Huang", "Tong He", "Paul Covington", "Benjamin Sapp", "James Guo", "Dragomir Anguelov", "Mingxing Tan"], "abstract": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built on a multi-modal large language model foundation, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. However, EMMA also exhibits certain limitations: it can process only a small amount of image frames, does not incorporate accurate 3D sensing modalities like LiDAR or radar and is computationally expensive. We hope that our results will inspire further research to mitigate these issues and to further evolve the state of the art in autonomous driving model architectures.", "sections": [{"title": "Introduction", "content": "Autonomous driving technology has made significant progress in recent years. To make autonomous vehicles a ubiquitous form of transportation, they must navigate increasingly complex real-world scenarios that require understanding rich scene context as well as sophisticated reasoning and decision-making.\nHistorically, autonomous driving systems employed a modular approach, consisting of specialized components for perception, mapping, prediction, and planning. While this design lends itself to easier debugging and optimization of individual modules, it poses scalability challenges due to the accumulated errors among modules and limited inter-module communication. In particular, the expert-designed interfaces between modules, such as the perception and behavior modules, may struggle to adapt to\nnovel environments because they are often pre-defined based on targeted scenarios. End-to-end autonomous driving systems have recently emerged as a potential solution, directly learning to generate driving actions from sensor data. This approach eliminates the need for symbolic interfaces between modules and allows for joint optimization of driving objectives from raw sensor inputs. However, these systems are often specialized for specific driving tasks and trained on limited datasets, hindering their ability to generalize to rare or novel scenarios.\nMultimodal Large Language Models (MLLMs) offer a promising new paradigm for AI in autonomous driving that may help to address such challenges. This is because MLLMs, as generalist foundation models, excel in two key areas: (1) they are trained on vast, internet-scale datasets that provide rich \"world knowledge\" beyond what is contained in common driving logs, and (2) they demonstrate superior reasoning capabilities through techniques such as chain-of-thought reasoning that are not available in specialized driving systems. While recent efforts have explored integrating and augmenting the capabilities of existing driving systems with MLLMs, we propose to develop an autonomous driving system in which the MLLM is a first class citizen.\nWe introduce the End-to-End Multimodal Model for Autonomous Driving (EMMA), built on top of Gemini without additional specialized components. Figure 1 shows the overview of the EMMA framework. EMMA accepts camera images and plain text for other non-vision inputs such as high-level driving commands and historical context. By recasting driving tasks as visual question answering (VQA) problems, EMMA leverages Gemini's pre-trained capabilities while preserving its extensive world knowledge. After EMMA is fine-tuned with driving logs from all tasks using task-specific prompts, its shared Gemini model generates various driving outputs such as future trajectories for motion planning, perception objects, road graph elements, and scene semantics. Our experiments showcase EMMA's strong performance on several planning and perception benchmarks despite this simple design. Additionally, we find that EMMA can produce interpretable, human-readable outputs for many perception tasks such as road graph estimation, and is able to function as a generalist model that is both scalable and robust for autonomous driving. Notably, as used here and throughout the paper, the EMMA generalist model refers to a machine learning model that has been trained and fine-tuned on a large volume of driving data to perform a wide range of specific driving tasks in the autonomous driving domain.\nWe summarize our key findings below:\n1. EMMA exhibits strong performance in end-to-end motion planning, achieving state-of-the-art performance on public benchmarks nuScenes and competitive results on the Waymo Open Motion Dataset (WOMD). We also show that we can further improve motion planning quality with more internal training data and chain-of-thought reasoning.\n2. EMMA demonstrates competitive results for various perception tasks including 3D object detection, road graph estimation, and scene understanding. On the camera-primary Waymo Open Dataset (WOD), EMMA achieves better precision and recall for 3D object detection than state-of-the-art methods.\n3. We demonstrate that EMMA can function as a generalist model in the autonomous driving domain, which jointly generates the outputs for multiple driving related tasks. In particular, EMMA matches or even surpasses the performance of individually trained models when it is co-trained with motion planning, object detection, and road graph tasks.\n4. Finally, we show EMMA's capacity to reason and make decisions in complex, long-tail driving scenarios.\nDespite these promising results, EMMA is not without its limitations. In particular, it faces challenges for real-world deployment due to: (1) limitations in 3D spatial reasoning due to its inability to fuse camera inputs with LiDAR or radar, (2) the need for realistic and computationally expensive sensor simulation to power its closed-loop evaluation, and (3) the increased computational requirements relative to conventional models. We plan to better understand and address such challenges in future work.\nIn the remainder of this paper, Section 2 describes the detailed method of EMMA for end-to-end motion planning and generalist tasks in autonomous driving. In Section 3, we present experimental"}, {"title": "Method", "content": "EMMA is built on top of Gemini [Gemini Team Google, 2023], a family of MLLMs developed at Google. We leverage the auto-regressive Gemini models that are trained to process interleaved textual and visual inputs to produce text outputs:\n$O = G(T, V)$\nwhere G is the Gemini model, O represents natural language outputs, T represents natural language prompts, and V denotes images or videos. The language output O = (o1, o2, ..., on) is generated via next-token prediction, i.e., the output probability can be represented as $P(O|T, V) = \\prod_{i=1}^{n} P(o_i | o_{0<i>, T, V})$ for n output tokens. Our goal is to adapt G for autonomous driving applications, thereby harnessing the world knowledge obtained during its pre-training phase.\nAs shown in Figure 1, we map autonomous driving tasks into Gemini-based EMMA formulation. All sensor data are represented as stitched images or videos as V; all router commands, driving context, and task-specific prompts are represented as T; all output tasks are presented as language outputs O. A challenge is that many of the inputs and outputs need to capture 3D world coordinates, such as waypoint BEV (Bird's Eye View) locations (x, y) for motion planning and the location and size of 3D boxes. We consider two representations: The first is direct text conversion to floating-point numbers, expressed as Tcoordinates = {(xi, Yi)} \u2248 text({(xi, Yi)}), where the specified decimal precision depends on the distance unit and decimal points. RT-2 exemplifies this approach in robotic control. The second approach uses special tokens to represent each location or action, formulated as Tcoordinates = {(xi, Yi)} \u2248 tokenize({(xi, Yi)}), with resolution determined by the learned or manually defined discretization scheme. MotionLM leverages this method for motion forecasting. We note that the two approaches have their respective strengths and weaknesses. We opt for the text representation such that all tasks can share the same unified language representation space and they can maximally reuse the knowledge from the pre-trained weights, even though the text presentation may produce more tokens than specialized tokenization."}, {"title": "End-to-End Motion Planning", "content": "EMMA employs a unified, end-to-end trained model to generate future trajectories for autonomous vehicles directly from sensor data. These generated trajectories are then transformed into vehicle-specific control actions such as acceleration and turning for autonomous vehicles. EMMA's end-to-end approach aims to emulate human driving behavior, focusing on two critical aspects: (1) first, the use of navigation systems (e.g. Google Maps) for route planning and intent determination, and (2) second, the utilization of past actions to ensure smooth, consistent driving over time.\nOur model incorporates three key inputs to align with these human driving behaviors:\n1. Surround-view camera videos (V): Provides comprehensive environment information.\n2. High-level intent command (Tintent): Derived from the router, includes directives such as \"go straight\", \"turn left\", \"turn right\", etc.\n3. Set of historical ego status (Tego): Represented as a set of waypoint coordinates in Bird's Eye View (BEV) space, Tego = {(xt, Yt)}t_Th\u2081 for Th timestamps. All waypoint coordinates are represented as plain text without specialized tokens. This can also be extended to include higher-order ego status such as velocity and acceleration.\nThe model generates future trajectories for motion planning, represented as a set of future trajectory waypoints for the ego vehicle in the same BEV space: Otrajectory = {(xt, yt)}_{t=T_f+1}^{T_f} for future Tf timestamps, where all output waypoints are also represnted as plain text. Putting everything together, the complete formulation is expressed as:\n$O_{trajectory} = G(T_{intent}, T_{ego}, V).$\nWe then fine-tune Gemini with this formulation for end-to-end planner trajectory generation, as illustrated in Figure 1. We highlight 3 characteristics of this formulation:\n1. Self-supervised: the only required supervision is the future locations of the ego vehicle. No dedicated human labels are needed.\n2. Camera-only: the only sensor input required is surround-view cameras.\n3. HD map free: no HD map is needed beyond the high-level routing information from a navigation system such as Google Maps.\nWhile we are not the first to adopt this general formulation\u2014[Li et al., 2024] conducted a thorough investigation, particularly examining the impact of including the historical ego status\u2014our contribution lies in adapting this formulation specifically for MLLMs for autonomous driving."}, {"title": "Planning with Chain-of-Thought Reasoning", "content": "Chain-of-thought Prompting [Wei et al., 2022] is a powerful tool in MLLMs that enhances reasoning capabilities and improves explainability. In EMMA, we incorporate chain-of-thought reasoning into end-to-end planner trajectory generation by asking the model to articulate its decision rationale Orationale while predicting the final future trajectory waypoints Otrajectory.\nWe structure the driving rationale hierarchically, progressing from 4 types of coarse-to-fine-grained information:\nR1- Scene description broadly describes the driving scenarios, including weather, day of time, traffic situations, and road conditions. For example: The weather is clear and sunny, and it is daytime. The road is four-lane undivided street with a crosswalk in the middle. There are cars parked on both sides of the street.\nR2 - Critical objects are the on-road agents that can potentially influence the driving behavior of the ego vehicle, and we require the model to identify their precise 3D/BEV coordinates. For instance: pedestrian at [9.01, 3.22], vehicle at [11.58, 0.35].\nR3 - Behavior description of critical objects describes the current status and intent for the identified critical objects. A concrete example is as follows: The pedestrian is currently standing on the sidewalk, looking toward the road, and maybe preparing to cross the street. The vehicle is currently ahead of me, moving in the same direction, and its future trajectory suggests it will continue straight.\nR4 - Meta driving decision includes 12 categories of high-level driving decisions, summarizing the driving plan given the previous observations. An example would be I should keep my current low speed.\nWe highlight that the driving rationale captions are generated using an automated tool without any additional human labels, ensuring scalability of the data generation pipeline. Specifically, we leverage off-the-shelf perception and prediction expert models to identify critical agents, and then use Gemini models with carefully designed visual and text prompts to generate comprehensive scene and agent behavior descriptions. Meta driving decisions are computed using a heuristic algorithm that analyzes the ego vehicle's ground-truth trajectory.\nDuring both training and inference, the model predicts all four components of the driving rationale before predicting the future waypoints, i.e.,\n$(O_{rationale}, O_{trajectory}) = G(T_{intent}, T_{ego}, V).$\nWhere Orationale denotes an ordered text output of (R1, R2, R3, R4) for driving rationale. Empirically, we observe that the prediction order of Orationale and Otrajectory does not result in a significant difference in quality after model convergence. This suggests that we can predict Otrajectory first and apply early stopping during inference for time-critical applications."}, {"title": "EMMA Generalist", "content": "While end-to-end motion planning is the ultimate core task, a comprehensive autonomous driving system requires additional capabilities. Specifically, it must perceive the 3D world and recognize surrounding objects, the road graph and the traffic conditions. To achieve this goal, we formulate EMMA as a generalist model capable of handling multiple driving tasks through training mixtures.\nOur vision-language framework represents all non-sensor inputs and outputs as plain text, providing the flexibility necessary to incorporate many other driving tasks. We employ instruction-tuning, a well-established approach in LLMs, to jointly train all tasks together with task-specific prompts included in the inputs T of Eq. 1. We organize these tasks into three primary categories: spatial reasoning, road graph estimation, and scene understanding. Fig. 2 illustrates the overall EMMA Generalist diagram.\nSpatial reasoning is the ability to understand, reason, and draw conclusions about objects and their relationships in space. This enables an autonomous driving system to interpret and interact with its surrounding environment for safe navigation.\nOur primary focus for spatial reasoning is 3D object detection. We follow Pix2Seq [Chen et al., 2022a] and formulate the output 3D bounding boxes as Oboxes = set{text(x,y,z,l, w, h, \u03b8, cls)}\nwhere (x, y, z) are the center location in the vehicle frame, l, w, h are the length, width, and height of the box, \u03b8 is the heading angle, and cls is the class label in text. We convert a 7D box to text by writing floating-point numbers with two decimal places, separated by spaces between each dimension. We then represent the detection tasks using a fixed prompt Tdetect_3D, such as \u201cdetect every object in 3D\", as in the following:\n$O_{boxes} = G(T_{detect_3D}, V).$\nWhile Oboxes is an unordered set of boxes, the predictions from an auto-regressive language model are always ordered. We find that sorting the 3D bounding boxes by depth improves detection quality, in contrast with the findings in Pix2Seq [Chen et al., 2022a].\nRoad graph estimation focuses on identifying critical road elements for safe driving, including semantic elements (e.g., lane markings, signs) and physical properties (e.g., lane curvature). The collection of these road elements forms a road graph. For example, lane segments are represented by (a) nodes, where the lanes encounter an intersection, merge, or split and (b) edges between these nodes following the direction of traffic. The full road-graph is composed of many such polyline segments.\nWhile edges within each polyline are directional, each polyline does not necessarily have a unique order relative to the other elements. This bears similarity to object detection (e.g., ), where each box is defined by ordered attributes (top-left corner, bottom-right corner), but a relative ordering between boxes does not necessarily exist. There are several existing works that model polyline graphs with Transformers, sharing similarities with language models.\nOur general modeling formulation in EMMA is as follows:\n$O_{roadgraph} = G(T_{estimate\\_roadgraph}, V).$\""}, {"title": "Generalist Training", "content": "Our unified vision-language formulation enables the simultaneous training of multiple tasks with a single model, allowing for task-specific predictions at inference time through simple variations of the task prompt Ttask. The training procedure is both straightforward and flexible.\nFor each task, we construct a dataset Dtask containing |Dtask| training examples. During each training iteration, we randomly sample a batch from the available datasets, with the probability of selecting an example from a specific dataset proportional to the dataset size: i.e., |Dtask|/\u2211t |Dt. To train for e epochs, we set the total number of training iterations to $e \\times \\sum_t |D_t|$, ensuring that the training"}, {"title": "Experiments", "content": "We highlight the experiments used to validate the efficacy of EMMA model. We leverage the smallest size of Gemini, i.e., Gemini 1.0 Nano-1 for all the experiments. In the following section, we first present the results of end-to-end planner trajectory generation on two public datasets in Section 3.1. Next, we conduct additional experiments on our internal datasets, studying the impact of chain-of-thought and data scaling in Section 3.2. Section 3.3 focuses on experiments involving three core perception tasks: 3D object detection, road graph estimation, and scene understanding. Our co-training results for the generalist model are summarized in Section 3.4. Finally, we showcase visual results that highlight EMMA capabilities in challenging, long-tail scenarios in Section 3.5."}, {"title": "End-to-End Motion Planning on Public Datasets", "content": "We conduct the end-to-end planner trajectory generation experiments on two public datasets, the Waymo Open Motion Dataset (WOMD) and the nuScenes dataset . EMMA is trained with the simplest end-to-end planner trajectory generation formulation as in Equation 2. That is, given camera images, ego vehicle history, and driving intent, the model is asked to predict the future ego waypoints for a certain time horizon."}, {"title": "Driving on the Waymo Open Motion Dataset (WOMD)", "content": "WOMD comprises 103k real-world urban and suburban driving scenarios, each lasting 20 seconds. These scenarios are further segmented into 1.1M examples, each representing a 9-second window: 1 second is used as input context, and the remaining 8 seconds serve as the prediction target. The dataset includes detailed map features such as traffic signal states and lane characteristics, along with agent states such as position, velocity, acceleration, and bounding boxes.\nWOMD is typically used for the agent motion prediction task, i.e., given the agent tracks for the past 1 second on a corresponding map, predict the positions of up to 8 agents for 8 seconds into the future. We adopt the exact same setting but only evaluate the future motion prediction of the ego vehicle as the planning benchmark. In addition, we leverage the 'intent' field, used for evaluating motion prediction, as the planning router instruction. We adapt some of the best motion prediction models, MotionLM and enhanced Wayformer, as the baselines of this planning benchmark.\nDuring inference, sampling a final trajectory from multiple candidate trajectories plays a critical role in the final performance. Both MotionLM and Wayformer generate 192 candidate trajectories, which are subsequently aggregated into 6 clusters using k-means clustering, resulting in 6 representative trajectories. The final trajectory is selected from these 6 representative trajectories based on their associated probabilities. For fairness, we also sample multiple trajectories using a Top-K decoding strategy, up to K = 24. We then compute the pairwise L2 distance between all trajectories and select the one with the lowest average L2 distance as the final predicted trajectory, which can be viewed as the \"median\" trajectory among all the predictions. In the ADE calculation, we treat this median trajectory with the highest probability.\nAs shown in Table 1, our model achieved similar performance as the MotionLM baseline when we train only on the WOMD dataset, with Gemini pre-trained weights. When pre-trained with our internal dataset (denoted as EMMA+), our model surpasses both MotionLM and Wayformer up to ADE 5s in the future. EMMA is only slightly worse than Wayformer at ADE 8s, where camera-induced depth error becomes a prominent error resource. We note the differences in inputs between MotionLM and EMMA: MotionLM takes inputs of agent location history, agent interactions, the road graph, and traffic light states. These agent boxes are produced by specialized off-board perception models that look at both past and future observations and are trained with a large amount of carefully curated human labels, the road graph is manually generated using full run segments, and all inputs heavily use LiDAR data with superior depth estimation. In stark contrast, EMMA only takes camera images and ego vehicle history as input, without the need of any labels or additional models (besides leveraging the Gemini pre-trained weights). Notably, for this work, we added internally available image inputs, which are not currently in the released WOMD.\nWe further investigate the impact of the number of sampled trajectories on ADE, as illustrated in Figure 3. The results highlight that sampling from multiple trajectories, rather than a single one, leads to a notable improvement in ADE. However, the advantage of increasing the number of candidate trajectories diminishes beyond a certain point."}, {"title": "Driving on the nuScenes Dataset", "content": "The nuScenes dataset offers a comprehensive autonomous vehicle sensor suite for evaluation. It consists of 1,000 scenes, each spanning 20 seconds, and includes information from 6 cameras, 5 radars, and 1 LiDAR that collectively provide 360-degree coverage in the field of view. The dataset is fully annotated with 3D bounding boxes across 23 classes and 8 attributes. In our experiments, we follow the standard protocol for planning evaluation: predict the next 3 seconds of future driving actions based on 2 seconds of historical data. We measure the planning quality with L2 errors at 1-, 2- and 3-second time horizons, aligning with established baseline methods.\nAs shown in Table 2, our self-supervised EMMA achieves state-of-the-art results in planning on nuScenes, outperforming all previous supervised (with intermediate perception labels and/or human labels) and self-supervised (no extra labels) methods. Under the same self-supervised setup, EMMA outperforms previous BEV-Planner by 17.1% in average L2 metric; even compared to DriveVLM-Dual that heavily uses intermediate perception and human labels, our self-supervised EMMA still improves the average L2 metric by 6.4%."}, {"title": "End-to-End Motion Planning with Chain-of-Thought Reasoning on Internal Dataset", "content": "In this section, we present our studies of end-to-end planning with chain-of-thought on our internal dataset. This dataset contains millions of scenarios, orders of magnitude larger than any publicly available autonomous driving dataset. The model takes in 2 seconds of history to predict the driving actions for 5 seconds into the future.\nTable 3 presents the results of our experiments on chain-of-thought reasoning applied to end-to-end planning. By adopting the chain-of-thought formulation introduced in Equation 3, we achieve a notable 6.7% improvement over the standard end-to-end planning approach detailed in Equation 2. We also conduct an ablation study to analyze the contributions of different rationale components. Our findings reveal that both driving meta-decision and critical object identification significantly enhance performance, contributing improvements of 3.0% and 1.5%, respectively. When these components are combined, the gains are even more substantial. Conversely, while scene description has a neutral impact on driving performance, it enhances the model's explainability. These results demonstrate that chain-of-thought reasoning can meaningfully improve driving performance, particularly when its components are carefully selected and integrated.\nWe also perform a series of data scaling experiments for end-to-end planning, the results of which are illustrated in Figure 4. As we train the model on a larger training set, we observe lower eval perplexities before overfitting. Our findings indicate that the driving quality of EMMA has not yet plateaued, even with the current large-scale dataset."}, {"title": "Perception Tasks", "content": "We summarize our studies on three main perception tasks: 3D object detection, road graph estimation, and scene understanding."}, {"title": "3D Object Detection", "content": "We validate our 3D object detection performance on the 3D camera-primary detection benchmark from the Waymo Open Dataset using the d Longitudinal Error Tolerant (LET) matching. We evaluate two versions: EMMA and EMMA+, similar to earlier sections, where EMMA+ is pre-trained on the 3D detection task using our internal dataset. The quantitative results are reported on the official test set and summarized in Figure 5.\nOur findings show that after pre-training, EMMA+ achieves competitive performance on the benchmark. Since our model output set of detected boxes without individual confidence scores, we directly compare the precision/recall instead of LET-3D-AP, which is calculated based on the precision/recall curve. We also compare the commonly used F1-score, where EMMA's F1-score is computed using the single precision/recall and other models' F1-scores are calculated by picking the maximal F1-score on the curve (often called F1-max). Figure 5 shows the performance comparison. In generally, EMMA+ demonstrates substantial improvements over state-of-the-art methods such as BEVFormer, achieving a 16.3% relative increase in vehicle precision at the same recall, and a 5.5% recall improvement at the same precision. EMMA+ also achieve better F1-score than prior arts. Performance on the pedestrian class is also comparable to that of MV-FCOS3D++. Additionally, we provide a performance breakdown across different ranges, highlighting that our model performs especially well in the near range. Our results underscore that with sufficient data and a large enough model, a multimodal approach can surpass specialized expert models in 3D object detection quality."}, {"title": "Road Graph Estimation", "content": "Road graph estimation is a complex task that predicts a group of unordered polylines, each of which is represented as a sequence of waypoints. We measure the quality of road graph prediction with two metrics: (1) lane-level precision and recall, where we define a true positive match between a predicted lane polyline and a groundtruth lane polyline if and only if their Chamfer distance is within 1 meter; and (2) pixel-level precision and recall, where polylines are rasterized into a BEV grid with 1 meter resolution \u2013 we then treat the BEV grid as a image and compute precision and recall based on per-pixel matching.\nAs discussed in Section 2.3, this task involves several design choices. One is about the representation of road graph polylines, where our choice is to define the start and end points of each lane, with"}, {"title": "Scene Understanding", "content": "Figure 7 summarizes our studies on the scene understanding task for temporary blockage detection. Our study is based on our internal datasets specifically curated for these scenarios. For this study, we establish our baselines by showing a picture to human and asking them to judge whether a lane is temporarily blocked. They can answer 'yes', 'no', or 'unsure'. Our baseline will treat"}, {"title": "Generalist", "content": "We explore the development of a generalist model by co-training on multiple tasks and analyzing their synergies, as summarized in Table 4. For this study, we focus on three core tasks: end-to-end planning, 3D object detection, and road graph estimation.\nCo-training on all three tasks yields significant improvements, with the generalist model outperforming the single-task models by up to 5.5%. Notably, when co-training two tasks, certain combinations lead to greater gains than others. For instance, detection performance improves most when co-trained with driving, and road graph estimation similarly benefits most when paired with driving. This suggests that the driving task plays a particularly prominent and influential role, serving as a key contributor to overall performance improvements.\nWe attribute these results to the complementary nature of the tasks. For example, road graph estimation becomes easier when the model can accurately identify the locations of vehicles. Similarly, driving quality is closely tied to understanding agent interactions, a skill enhanced by 3D object detection. These findings suggest that pursuing a generalist model is a promising direction for future research, with the potential for deeper insights into task synergies and performance optimization."}, {"title": "Visualizations", "content": "We present 12 diverse visual examples in Fig. 8, 9 and 10, each selected to highlight the generalizability of EMMA model across a range of scenarios. In all the scenarios, we display the model's predictions (from left to right): end-to-end motion planning, 3D object detection, and road graph estimation.\nWe group visual examples by scenario type: Examples (a)-(d) showcase how EMMA safely interacts with rare, unseen objects or animals on the road. Examples (e)-(f) feature EMMA navigating through construction areas. Examples (g)-(j) showcase EMMA following traffic rules at intersections with traffic lights or traffic controllers. Examples (k)-(1) highlight EMMA respecting vulnerable road users like motorcyclists.\nGiven these examples, we demonstrate the following capabilities of EMMA:\n\u2022 Generalizability: Adapts well to diverse real-world driving scenarios across different environments and attends to objects beyond its fine-tuning categories, such as squirrels.\n\u2022 Predictive driving: Proactively adjusts to the behavior of other road users for safe and smooth driving.\n\u2022 Obstacle avoidance: Consistently adjusts trajectories to avoid obstacles, debris and blocked lanes.\n\u2022 Adaptive behavior: Safely handles complex situations like yielding, construction zones, and following traffic control signals.\n\u2022 Accurate 3D detection: Effectively identifies and tracks road agents, including vehicles, cyclists, motorcyclists, and pedestrians.\n\u2022 Reliable road graph estimation: Accurately captures road layouts and integrates them into safe trajectory planning.\nTo conclude, these scenarios highlight EMMA's capability to operate safely and efficiently in a variety of challenging and diverse driving scenarios and environments."}, {"title": "Related Works", "content": "End-to-end autonomous driving research enjoys a rich history and has evolved significantly since ALVINN [Pomerleau, 1988] employed shallow neural networks to predict control signals. The field benefited from further deep learning advancements: e.g. DAVE-2 and ChauffeurNet, leveraged deeper neural architectures and incorporated sophisticated perception and motion planning modules respectively. Recent research has expanded to include multimodal inputs , multi-task learning , reinforcement learning , and distillation . Unified planning frameworks such as VAD and UniAD integrated planning with conventional modules in open-loop environments. More studies have been proposed to examine the robustness, safety, and transferability from synthetic environments to real-world domains. However, recent findings from Ego-MLP and BEV-Planner revealed that these methods could potentially overfit to ego status despite their good performance on benchmark tasks. Our work revisits the simplicity of earlier end-to-end models such as ALVINN and DAVE-2, enhancing them with powerful MLLMs.\nVision language models for autonomous driving have gained increasing interest, focusing on achieving explainable driving behavior and generalizability through end-to-end learning frameworks. DriveGPT4 utilizes LLMs to both explain vehicle actions and predict control signals in an iterative Q&A format. Drive Anywhere introduces patch-aligned feature extraction from MLLMs for text-based driving decision queries, while OmniDrive features a 3D vision-language model design for reasoning and planning. Other approaches use MLLMs in graph-based VQA contexts (DriveLM ), or apply chain-of-thought reasoning (DriveVLM ) to tackle multiple driving-related tasks. For non-end-to-end models, LLM-Drive leverages LLMs with object-level vector inputs for control planning. In contrast, our work studies the end-to-end fine-tuning of a state-of-the art MLLM for driving tasks, employing a generalist approach that emphasizes open-world driving capabilities.\nMultimodal large language models (MLLM) extend LLMs to multiple modalities, leveraging their generalizability, reasoning capabilities, and contextual understanding. Early explorations focused on specific vision-language problems or open-set object detection , while recent research has scaled up both trask diversity and model sizes for improved generalizability and few-shot capabilities [Cho et al., 2021, Chen et al., 2022b, Wang et al., 2022, Lu et al., 2022, Alayrac et al., 2022, Yu et al., 2022, Chen et al., 2023, 2024c, Wang et al., 2024c, Peng et al., 2024, Huang et al., 2023, Lu et al., 2024]. Notable examples include Flamingo , a 70B model which achieved state-of-the-art quality for multiple few-shot vision benchmarks, and CoCa a 2.1B parameter model which demonstrated state-of-the-art performance on zero-shot transfer and various downstream tasks including ImageNet classification. PaLI , at 55B parameters, achieves better performance across multiple vision and language tasks by scaling both the vision and language model components jointly. These early works demonstrate the strong performance and generalizability of MLLMs. Recent trends have seen the integration of native multi-modal inputs in LLMs, such as Gemini , GPT-4O, and Llama3-v . Our work explores the application of these promising new models for generalist end-to-end autonomous driving."}, {"title": "Limitations, Risks, and Mitigations", "content": "In the previous sections", "capability": "Currently", "input": "Our approach heavily relies on pre-trained MLLMs", "challenges": 1}]}