{"title": "Strategy and Skill Learning for Physics-based Table Tennis Animation", "authors": ["Jiashun Wang", "Jessica Hodgins", "Jungdam Won"], "abstract": "Recent advancements in physics-based character animation leverage deep learning to generate agile and natural motion, enabling characters to execute movements such as backflips, boxing, and tennis. However, reproducing the selection and use of diverse motor skills in dynamic environments to solve complex tasks, as humans do, still remains a challenge. We present a strategy and skill learning approach for physics-based table tennis animation. Our method addresses the issue of mode collapse, where the characters do not fully utilize the motor skills they need to perform to execute complex tasks. More specifically, we demonstrate a hierarchical control system for diversified skill learning and a strategy learning framework for effective decision-making. We showcase the efficacy of our method through comparative analysis with state-of-the-art methods, demonstrating its capabilities in executing various skills for table tennis. Our strategy learning framework is validated through both agent-agent interaction and human-agent interaction in Virtual Reality, handling both competitive and cooperative tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of deep learning into physics-based character animation has led to significant advancements in generating agile and natural motion, enhancing the lifelike quality of characters in complex environments. To increase the versatility of these characters, it is essential to ensure that their skills can be reused in environments or conditions that may not precisely match their training data. To achieve this goal, recent approaches have focused on learning reusable skill embeddings. These approaches are typically trained in two stages. Initially, characters learn various skill embeddings by imitating reference motions. Then, in the task training stage, they apply these skills to accomplish diverse tasks. These approaches have demonstrated remarkable success in generating natural motion in various environments.\nHowever, when the differences between the skills are subtle, these approaches often suffer from mode collapse during the task training phase. Specifically, although agents (a.k.a. characters) can learn various skills during the imitation stage, they tend to use a limited set of skills for the downstream tasks, neglecting the diversity of their learned skills in the imitation stage. Thus, mode collapse restricts the agents' potential in scenarios that require a diverse set of skills. Mode collapse also restricts exploration during RL training, resulting in sub-optimal task performance.\nAnother relatively unexplored topic relates to the decision strategy of agents, particularly their ability to dynamically formulate decision strategies that encompass skill selection and associated skill goals in response to task demands. Most previous studies either have not required a diverse skill set or have relied on a human user to manually determine skills for the agents. Agents have generally not been equipped with the capability to employ different strategies to adapt to complex and dynamic environments.\nOur research introduces a learning approach to enhance both the skill and strategic decision-making capabilities of physically simulated agents. First, we develop a hierarchical skill controller that enables agents to utilize different table tennis skills and transition among them rapidly. This controller effectively addresses mode collapse during task training. Second, we develop a method for strategy learning, enabling agents to explicitly select and utilize specific skills for different types of interaction, whether competitive or cooperative. An overview of the results is in Figure 1.\nWe demonstrate the effectiveness of our approach through two interaction environments: a table tennis match played between two simulated agents and a match between a human and a simulated agent in virtual reality (VR). In the agent-agent environment, the agents demonstrate improved skill diversity and decision strategy in simulated table tennis matches compared to results predicted by the previous techniques. In the human-agent interaction environment, we evaluate both cooperative and competitive scenarios in real-time interactions between humans and agents. These environments not only validate our approach but also provide platforms for future research into complex agent behaviors and human-agent dynamics. Code and data for this paper are at https://jiashunwang.github.io/PhysicsPingPong/.\nWe summarize the contributions of this paper as follows:\n\u2022 A hierarchical skill controller that empowers physically simulated agents to explicitly perform various skills, enabling rapid skill transitions. An interaction learning framework designed to create a decision strategy allows agents to continually learn and adapt, meeting the demands of competition or cooperation in dynamic environments with other agents and with humans.\n\u2022 Novel results demonstrating our learning framework's capacity to generate intelligent decisions and natural motions for table tennis in two scenarios: agent-agent interactions in a simulated environment and human-agent interactions in a VR environment. The agent-agent environment is a platform for developing and testing competitive and cooperative algorithms while the VR environment allows natural human-agent interactions."}, {"title": "2 RELATED WORK", "content": "We review the closest related work in physics-based character animation with reusable skills and multi-character animation. We review studies on transitions among skills as we develop a method for skill selection and transition. We further discuss relevant research in human-agent interaction in VR."}, {"title": "2.1 Physics-based Character Animation", "content": "Incorporating physical laws into character animation allows for the development of controllers that generate more realistic behaviors [Hodgins et al. 1995; Laszlo et al. 1996]. Optimization techniques, such as trajectory optimization [de Lasa et al. 2010; Mordatch et al. 2012; Yin et al. 2008] and sampling-based methods [Liu et al. 2016, 2010] have been widely explored. Recently, deep reinforcement learning (DRL) has been shown to substantially enhance control capabilities [Liu and Hodgins 2017; Peng et al. 2017]. Due to its flexibility and ease of use, DRL methods eliminate the need for designing complex objective functions while delivering outstanding results and have attracted significant research interest as a result.\nData-driven methods have become prevalent in physics-based character animation studies since a DRL-based method was introduced by Peng et al. [2018]. The idea has been extended for handling larger datasets [Bergamin et al. 2019; Won et al. 2020] and for allowing recombination of existing state transitions [Peng et al. 2021]. Recently, much attention has been paid to reusable motor skills. The idea is to learn a latent space of reference motions and then to reuse the learnt space for downstream tasks. Various latent models have been studied such as encoder-decoders with autoregression [Merel et al. 2019; Won et al. 2021], spherical embedding [Dou et al. 2023; Peng et al. 2022; Tessler et al. 2023], conditional variational autoencoder (VAE) [Won et al. 2022; Yao et al. 2022], and vector-quantized VAE [Zhu et al. 2023]. Some researchers have also proposed part-wise models to maximize reusability of reference motions [Bae et al. 2023; Xu et al. 2023].\nOur system is designed for table tennis games, involving two players (i.e., agents). Two or more agents have been created primarily with kinematic approaches [Kwon et al. 2008; Liu et al. 2006; Shum et al. 2008, 2012; Wampler et al. 2010]. There exist two recent approaches [Won et al. 2021; Zhu et al. 2023] demonstrating examples of physically simulated boxing. Zhang et al. [2023] build a system to learn tennis skills from broadcast videos and produce rallies with a mirrored opponent. In their approach, kinematics-based motion generation is utilized first, followed by physics-based tracking, relying on residual forces and extra arm control for successful strikes. Skill and target selection are not learned but rather performed manually or randomly to create a scene including two players. In contrast, our method learns not only agile and precise motor control to strike the ball but also strategies to select skills and targets based on the movement of the opponent and the ball."}, {"title": "2.2 Transition of skills", "content": "Option-based methods [Bagaria and Konidaris 2020; Jain et al. 2021; Klissarov et al. 2017; Konidaris and Barto 2009; Sutton et al. 1999] represent skills as options, which are sequentially constructed, with each option's execution in the chain enabling the agent to execute the subsequent option. Lee et al. [2019] propose learning additional transition policies to connect primitive skills and introduce proximity predictors, which yield rewards based on proximity suitable for initial states for the next skill. One challenge of transitioning between different skills to chain long-horizon tasks is addressed by terminal state regularization [Lee et al. 2021]. Behavior Trees are also a common method for planning the transition between different states [Cheng et al. 2023; French et al. 2019; Marzinotto et al. 2014]. These methods achieve skill transitions by ensuring that the terminal state of the previous stage is close to the initial state of the next stage. While these methods work well for tasks that are not time-sensitive, table tennis, which involves high-speed movements and rapid responses, poses a challenge as players do not always hit the ball from a well-defined initial state."}, {"title": "2.3 Human-agent interaction", "content": "Research has focused on human sports training within VR [Liu et al. 2020; Pastel et al. 2023]. However, these studies often lack a physically simulated opponent. There are commercial games that allow people to interact with an agent in VR for sports activities, such as boxing, golf, and badminton. Eleven Table Tennis [2016] is a VR-based table tennis game similar to the one we have constructed, which enables a human to play with an agent. However, this agent is not simulated with full-body dynamics, rather it is simulated with only a floating head and a floating paddle. Advances in GPU-accelerated simulation and our control algorithm, enable us to create a physically-simulated agent with full-body dynamics that can play in real-time with humans. Another relevant area involves enhancing the agent's capabilities with human-in-the-loop methodologies [Brenneis et al. 2021; Li et al. 2022; Seo et al. 2023; Wang et al. 2023] using extended reality. Our work differs from previous studies by bringing humans and agents into a unified environment allowing bidirectional physical interaction, where they can cooperate and compete."}, {"title": "3 METHOD OVERVIEW", "content": "We propose a hierarchical approach that includes a strategy-level controller and a skill-level controller. The strategy-level controller takes the states of the agent, opponent, and ball as inputs, and outputs a strategy action, which includes the skill to use and the target landing location for the ball. Meanwhile, the skill-level controller takes the states of the agent and ball, along with the strategy action as inputs, and then generates a skill action, which includes the target joint angles for PD controllers. An overview of our method is in Figure 2 and Figure 3 shows the architecture of our method."}, {"title": "4 SKILL-LEVEL CONTROLLER", "content": "Three stages are required to train our skill-level controller. Initially, we train imitation policies using the motion capture data. Then the ball control policy for each skill is learned, which enables the agent to hit back balls using the corresponding imitation policy. Finally, we learn a policy that enables the agent to perform various skills sequentially while making plausible transitions among them. We call this policy the mixer policy. Once the skill-level controller is trained, the agent can proficiently and continuously execute various skills, sending balls to diverse target locations."}, {"title": "4.1 Imitation Policy", "content": "We first categorize the motion capture dataset into five subsets corresponding to each skill. This subdivision allows us to train the skill-specific imitation policies. We also utilize all the data to train a universal imitation policy. The imitation policy is represented as $\\pi^{i}(a^{i}|s, z^{i})$, where $i \\in \\{1, 2, 3, 4, 5, u\\}$, $1 \\sim 5$ are indices of different skills and u is the index of the universal imitation policy. $z^{i}$ is a latent variable sampled from a hyper-sphere distribution, and s is the agent's state. The goal of the imitation policy is to output an action $a^{i}$ that leads to simulated motions similar to the reference motions. Thus, each skill-specific imitation policy generates motions similar to its corresponding reference motion in each skill subset, while the universal imitation policy generates motions encompassing the entire motion capture dataset. When solving specific tasks in the later stage, using a single universal imitation policy trained with a variety of motions often leads to the mode collapse problem. The agent does not explore various available skills enough; instead, it repeats very limited skills, and the task performance remains suboptimal. Our controller design is inspired by mixture-of-experts and mitigates this problem. Each imitation policy $\\pi^{i}(a^{i}|s, z^{i})$ is built by the adversarial framework ASE [Peng et al. 2022], where the policy is updated so that it tricks a motion discriminator $D^{i}$. The transitions $d_{m^{i}}(s, s')$ existing in the motion capture dataset are used as positive samples while the transitions $d_{n^{i}}(s, s')$ generated from the policy $\\pi^{i}$ are used as negative samples. The discriminator is trained by minimizing:\n$\\min_{D^{i}} - E_{d_{m^{i}}(s,s')} log(D^{i}(s, s')) \u2013 E_{d_{n^{i}}(s,s')} log(1 \u2013 D^{i}(s, s')) + \\lambda_{gp}E_{d_{B_{D^{i}}(s,s')}} || \\nabla_{s} D^{i}(s)|s=(s,s')||, $\nwhere the last term is a gradient penalty regularization with a constant factor $\\lambda_{gp}$. We train encoders $q^{i}$ to encourage correspondence between the transition (s, s') and the latent variable $z^{i}$. The encoder is modeled as a von Mises-Fisher distribution and it is trained by maximizing its log-likelihood:\n$\\max_{q^{i}} E_{p(z^{i})} E_{d_{r^{i}}(s,s'|z^{i})} [log q^{i}(z^{i}|s, s')],$\n$q^{i}(z|s, s') = exp(\u00b5_{q^{i}}(s, s')^{T}z^{i}) / Z$\nwhere $\u00b5_{q^{i}}(s, s')$ is the mean of the distribution, and Z is a normalization constant. Given a discriminator $D^{i}$, the reward to train $\\pi^{i}$ is defined as:\n$r_{t} = \u2212log(1 \u2212 D^{i}(s_{t}, s_{t+1})) + \u03b2log q^{i}(z|s_{t}, s_{t+1}).$"}, {"title": "4.2 Ball Control Policy", "content": "Once the agent can imitate each skill $i \\in \\{1, 2, 3, 4, 5\\}$, we train ball control policies $w^{i}(z^{i}|s, b, d, y)$ to enable the agent to hit and move a ball launched from a random location to the desired location, where s denotes the state of the agent, b represents the state of the ball and y is the target landing location for the ball. The task reward r is a composite of three terms: the paddle reward $r_{p}$, the ball reward $r_{b}$, and the style reward $r_{s}$,\n$r(t) = w_{p}r_{p}(t) + w_{b}r_{b}(t) + w_{s}r_{s}(t),$\nwhere $w_{p}, w_{b}$, and $w_{s}$ are the relative weights. The paddle reward $r_{p}$ encourages the agent to position the paddle close to the ball. The reward is defined as:\n$r_{p}(t) =\\begin{cases}\nexp(-4||x_{p}(t) \u2212 x_{b}(t)||^{2}), & \\text{if } C_{bp}(t) = 0, \\\\\n0, & \\text{otherwise},\n\\end{cases} $\nwhere $x_{p}(t)$ and $x_{b}(t)$ represent the positions of paddle and ball, respectively, $C_{bp}(t)$ is a binary variable representing contact states. $C_{bp}(t) = 0$ means the ball has not contacted the paddle until time t, while $C_{bp}(t) = 1$ indicates the ball has contacted the paddle at time t or contacted previously before time t. It will be reset to 0 whenever the next ball is launched. The ball reward $r_{b}$ is given by:\n$r_{b}(t) = \\begin{cases}\n1 + exp(-4||x_{c}(t) \u2212 x_{t}(t)||^{2}), & \\text{if } C_{bp}(t) = 1 \\text{ and } C_{bt}(t) = 0, \\\\\n0, & \\text{otherwise},\n\\end{cases} $\nwhere $x_{t}(t)$ is the target landing location of the ball, $x_{c}(t)$ represents the anticipated landing location on the table, calculated using Newton's equation of motion for the point mass (i.e., quadratic trajectory), with its state corresponding to the current position and velocity of the ball. $C_{bt}(t)$ is a binary variable checking the contact history between the ball and the table, which is updated similarly to $C_{bp}(t)$. The agent receives the maximum reward when it successfully hits the ball and the ball moves toward the target location. We also apply the style reward, $r_{s} = \u2212 log(1\u2212D^{i}(s_{t}, s_{t+1}))$ in the task training similarly to ASE [Peng et al. 2022], where $D^{i}$ is the discriminator learned during the previous stage."}, {"title": "4.3 Mixer Policy", "content": "While our agent can play table tennis using the ball control policies with the corresponding imitation policies, its capability is limited to repeating a single skill. Simply transitioning from one controller to another during play often leads to failure due to a mismatch between the end state of one skill to the start state of the next. To create plausible transitions among the different skills, we learn a mixer policy $w_{m}(z^{m}|s, b, d, y)$, which takes the agent state s, the ball state b, and the strategy action (d, y) as input, where d is a one-hot vector determining the skill to use and y is the target ball landing location, then generates the latent variable for the universal imitation policy $\\pi^{u}$ and a set of blending weights \u03c6, mixing the skill actions in a joint-wise manner. In other words, \u03c6 determines which policy the agent relies on among the transition and five different skills. The target joint angles for PD controllers are computed as\n$a = \u03c6 \u25e6 \u03c0^{u}(\u00b7|s, z^{u}) + (1 \u2212 \u03c6) \u25e6 \\sum_{i=1}^{5} \u03b4_{i}\u03c0^{i}(\u00b7|s, z^{i})$"}, {"title": "5 STRATEGY-LEVEL CONTROLLER", "content": "The strategy-level controller is developed by iterative behavior cloning inspired by [Oh et al. 2018]. More specifically, we first collect interaction data by randomly sampling strategy actions during agent-agent play or human-agent interactions with VR. This data is then used to update the strategy-level controller, and we repeat this process by collecting new interaction data with the latest strategy-level controller. When collecting interaction data, there are two options: competition and cooperation. To train a competitive strategy, we choose data that results in victories, whereas in a cooperative strategy, we choose sequences where the opponent successfully catches the ball.\nA strategy-level controller produces a skill index and a target landing location repeatedly so that they satisfy the requirements of different applications. More specifically, the strategy-level controller f takes the strategy observation o = (s, \u0161, b) as input where s, s, and b are the agent state, the opponent state, and the ball state, respectively, then outputs the strategy action c = (d, y), where d is a one-hot vector determining the skill to use, and y is the target landing location of the ball. The strategy action is updated when the ball starts moving from the opponent to the agent. To effectively learn a strategy-level controller, we adopt a behavior cloning approach with iterative refinement, aiming to learn strategies from available expert demonstrations $\\{(o_{\\text{expert}}, c_{\\text{expert}})\\}_{k=1}^{K}$ (see Algorithm 1). As a structure of the controller, we utilize a Conditional Variational Autoencoder (CVAE) to model the stochastic nature inherent in sports gameplay. During training, the CVAE encoder takes o and c as inputs and generates the mean \u00b5 and variance \u03c3\u00b2 of the posterior Gaussian distribution Q(u|\u03bc, \u03c3\u00b2). We then sample a latent variable u from this distribution and concatenate it with observation o as input for the decoder, which reconstructs the action c'. The training loss is defined as:\n$\\sum_{k=1}^{K} ||c_{\\text{expert}} - c'|| + \u03b2_{KL}D_{KL}(Q(u|\u03bc_{k}, \u03c3^{2})||N(0, 1)),$\nwhere $D_{KL}(\u00b7||\u00b7)$ measures the KL divergence between the two distributions and $\u03b2_{KL}$ is the relative weight. During inference the decoder is utilized solely, it takes a randomly sampled latent variable u and the observation o, and then generates the strategy action that guides the agent to perform a corresponding skill. If the opponent successfully returns the ball, this process repeats. We collect expert demonstrations from two different interaction environments (Env in Algorithm 1). The details of each environment will be explained in Section 6."}, {"title": "6 INTERACTION ENVIRONMENT", "content": "We introduce the agent-agent and human-agent interaction environments that we build to validate the strategy learning approach.\nThe agent-agent interaction environment is an environment where two virtual agents play table tennis with each other (Figure 1 left column). We name one agent as our agent and the other as the opponent. In the process of learning a strategy-level controller for our agent, the opponent uses a fixed heuristic strategy-level controller while the controller for our agent is updated iteratively. More specifically, we let our agent the opponent play with each other using their own strategy-level controllers, collect those demonstrations, and then use them to update our agent's controller. If our goal is to learn a competitive strategy, that can beat the opponent, we selectively use demonstrations leading to wins. On the other hand, we use demonstrations where the opponent successfully returns the ball when aiming to learn a cooperative strategy. In our system, we utilize two types of heuristic strategy-level controllers: a random strategy and a video strategy. The random strategy selects skills and target landing locations randomly from a uniform distribution. The video strategy is constructed by using broadcast videos. We extract expert demonstrations $\\{(o_{\\text{video}}, c_{\\text{video}})\\}_{k=1}^{K}$ from existing broadcast videos (20 minutes in total). Subsequently, we train a CVAE using the behavior cloning method.\nThe human-agent interaction environment allows a human user to play with a virtual agent. In our system, the user interacts with an agent by using a VR device, including a head-mount display and a hand controller ((Figure 1 right column)). The VR interface operates through Unity while the physics-based simulation runs on Isaac Gym [Makoviychuk et al. 2021]. To enable the simulated agent to interact with a human user, we physically simulate the user's paddle, with its position and orientation controlled via signals from the VR interface. Specifically, for paddle control, we use the VR hand controller's Cartesian pose $q_{\\text{user}}$ and the simulated paddle pose $q_{\\text{sim}}$ to calculate the target velocity $q_{\\text{target}} = (q_{\\text{user}} - q_{\\text{sim}})/\u2206t$, where \u2206t is the simulation step. We use this target velocity as an input to the velocity controller provided by the simulator. For visualization, Unity takes the state of the simulated agent, user's paddle, and ball as inputs and renders them using visualization assets. This implementation significantly reduces the amount of information exchange compared to a previous study that sent stereo images [Seo et al. 2023], enabling real-time interaction and gameplay. By considering a human user as the opponent, the strategy-level controller of the agent can be built through the same pipeline used for the agent-agent interaction environment."}, {"title": "7 EXPERIMENTS", "content": "We evaluate the skill-level controller based on motion quality and task performance. We assess the strategy-level controller by examining its effectiveness in agent-agent and human-agent interaction environments, with the demands of both competition and cooperation scenarios."}, {"title": "7.1 Skill evaluation", "content": "We evaluate the skill performance from motion quality and task performance. The evaluation of motion quality measures the naturalness of generated motions when given the desired skill command and whether the agent performs the correct skill. The evaluation of task performance measures the overall proficiency in playing table tennis. We compare our method with two state-of-the-art methods, ASE [Peng et al. 2022] and CASE [Dou et al. 2023], as well as an explicit transition model (ET) which is a variant of our method with the mixer policy $w_{m}$ removed from our model. We train an explicit controller to handle skill transitions by taking over the control when the ball passes the net until it is returned to the agent. The controller is also built using the ball control-imitation architecture. The key difference between our approach and ET is that ours provides continuous action blending with the selected skill's action at every time step, whereas ET does not."}, {"title": "7.1.1 Motion quality.", "content": "We design three metrics to evaluate the motion quality, particularly to evaluate the naturalness and mode collapse. The first metric is Discriminator Score, which measures how similar the current strike motion is to the reference motion of i-th target skill. Because we have five skills, we train a discriminator $D_{\\text{iest}}$ for each skill and utilize the following equation to calculate the score:\nDiscriminator Score $i = \\frac{1}{T} \\sum_{t=0}^{T} -log(1 - D_{\\text{iest}}(s_{t}, s_{t+1})),$"}, {"title": "7.1.2 Task performance.", "content": "To assess the task performance of the skill controller, we evaluate two aspects: sustainability and accuracy. Sustainability is determined by the average number of successful continuous returns, while accuracy is measured by the average distance in meters between the target landing location and the actual contact location on the table. Besides testing on the training distribution, we collect some ball tracking data with faster ball trajectories from a match between high-ranking players and evaluate whether each method can perform well with the testset of the ball tracking data. We report the evaluation results in Table 2. The numbers in parentheses are the results of the fine-tuning experiments. Our method can achieve the largest number of average hits and the second-best accuracy. Although ET can achieve higher accuracy, it is not sustainable, especially for more challenging balls. It only achieves an average of 3.66 hits because it often lacks time to respond to the next ball due to the explicit transition design."}, {"title": "7.1.3 Blending weights of the mixer policy.", "content": "We test the agent with different skills to hit the ball and visualize the average blending weights \u03c6 of the shoulder, elbow, and wrist joints in Figure 8. We can observe that the weights of the mixer policy are usually lowest at the moment the paddle contacts the ball, and higher before and after transitions between different skills. It indicates a reliance on the pre-trained ball control policy during ball strikes, and on the mixer policy during transitions."}, {"title": "7.2 Evaluation for agent-agent interaction", "content": "We evaluate the performance of learned strategies in the agent-agent interaction environment under both competition and cooperation settings. The competition strategy aims to develop an agent that achieves a higher winning rate than the opponent. The cooperation strategy develops an agent that can play gently with the opponent to increase the length of rallies. As a baseline, we learn a strategy policy via reinforcement learning (RL). Please refer to Appendix E for details on training the RL baseline. Our method and the RL baseline are compared by having them play with two types of opponents: the random strategy and the video strategy opponent introduced in Section 6. Each evaluation is computed over 10k points. Table 3 shows the winning rate and the average rounds for the competition and cooperation settings. Our strategy learning algorithm can achieve higher winning rates for the competition setting and can maintain longer rallies for the cooperation setting for both opponents.\nIn Figure 6 and 7, we visualize the histogram of skill commands from the strategy policies, and the target landing locations. In Figure 7, we also provide ball landing locations captured from real players during competitive matches. We observe that our method has a more similar distribution of landing locations to humans than RL. RL converges to less diverse skill commands and it only hits to a small region of the table. In contrast, our method utilizes various skills and target locations throughout the gameplay. We also include qualitative gameplay visualizations in Figure 9. We further let RL and our method compete with each other, and report the winning rates in Table 4. Each method has two strategy policies trained with two opponents, therefore, we have four matches in total. Because RL falls into a local minimum and overfits to a specific opponent, our method achieves a much higher winning rate."}, {"title": "7.3 Evaluation of Human-agent interaction", "content": "Before learning strategies for the human-agent interaction environment, we finetune the skill-level controller using the play data of a human user interacting with the agent equipped with the original skill-level controller. The finetuning is required because of the domain gap between what the simulated agent has experienced and the styles of a real human user in the VR environment. After finetuning the skill controller, strategies are learned by following similar procedures as the agent-agent interaction environment. For training a competition strategy, we use demonstrations that result in the agent winning points, thus presenting more challenging returns for the human opponent. In contrast, for training a cooperative strategy, we use demonstrations where the human can maintain rallies, emphasizing easier ball returns for the human. These demonstrations serve as expert demonstrations in Algorithm 1. We report the winning rate of the agent and the average hits between the user and the agent in Table 5. When playing with the initial policies, the agent can achieve a winning rate of 64% and a rally with 4.04 hits on average. After two iterations of refinement of the competition strategy, the agent can achieve a winning rate of 78%, and the average number of hits drops to 3.75. For the cooperative strategy, the winning rate drops to 58%, and the user can achieve a rally with an average of 5.34 hits. These results demonstrate that our strategy learning algorithm is also effective for the human-agent interaction environment. We provide screenshots of real-time human-agent gameplay video in Figure 10."}, {"title": "8 DISCUSSION AND CONCLUSION", "content": "Although our method produces agents that play competitively and more naturally, it still has several limitations. First, although building individual policies for each skill and combining them via the mixer policy clearly improves the generated motion quality and task performance, our model would not scale well to a dataset including hundreds of different skills. Developing a hybrid model that combines our approach with a model learnable from unlabeled motions to achieve both high motion quality and scalability would be an interesting future research topic. Second, because our method is data-driven, the captured motion quality significantly affects the final motion quality. For example, the player tends to use large arm motions, and this motion style appears in our results as well. However, in matches, using less arm motion could be a way to conserve energy, and concealed movements can also confuse the opponent. Lastly, although we employ a rigid-body simulation for every component, including the ball, player, and table, where the ball can spin as well, air resistance is modeled using only damping based on velocity, rather than incorporating the Magnus effect, which bends the ball trajectory due to air pressure differences. This omission could impact the realism of our animations and the final strategies our system learned.\nIn this paper, we introduce a learning approach for physics-based table tennis animation. We develop a hierarchical controller structure, which overcomes the mode collapse problem that appears frequently in reusable latent-based models. Our approach not only improves overall motion quality but also enables us to learn effective decision strategies for two types of environments: agent-agent and human-agent interactions."}]}