{"title": "One-Shot Manipulation Strategy Learning by Making Contact Analogies", "authors": ["Yuyao Liu", "Jiayuan Mao", "Joshua B. Tenenbaum", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "We present a novel approach, MAGIC (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, MAGIC effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. MAGIC demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories.", "sections": [{"title": "I. INTRODUCTION", "content": "A hallmark of human intelligence is flexible tool use: humans can quickly acquire new manipulation \"strategies\" from just a handful of demonstrations and apply these strategies across various scenarios, including generalization to novel objects of unseen categories. For example, as illustrated in Fig. 1, even from a single demonstration of using a hook to reach distant objects or putting hangers on a rod, we can generalize to different object positions, sizes, and diverse categories, such as hangers and mugs.\nTraditionally, two main approaches have been widely studied to build machines that can flexibly use tools: model-based and analytic approaches which take novel scenarios and goals and use built-in physical models to compute plans [1]\u2013[4], and policy learning, which leverages various types of priors (e.g., object-based and part-based models) and pretrained neural features for generalization [5]\u2013[8]. However, both approaches have their limitations. Model-based planning generalizes well given accurate object and physical models. However, it is slow and usually does not benefit from learning. Policy learning approaches, on the other hand, are very efficient at performance time but usually exhibit limited generalization to novel objects and scenarios, particularly when the shape of the novel objects differs significantly from objects seen during training, such as generalizing from hangers to mugs.\nIn this paper, we present a novel approach, MAGIC (manipulation analogies for generalizable intelligent contacts), for one-shot manipulation strategy learning. Shown in Fig. 1, given a single reference action trajectory (e.g., using a hook to reach for a distant object) and a novel scenario (e.g., with different tools and different objects), the goal of the algorithm is to generate a sequence of robot actions that apply a \"similar\" strategy to the test objects specified by users: in this example, having the target object moving along a certain direction for a given distance. MAGIC extends two critical insights into a broad class of manipulation strategies. First, many strategies such as hooking, hanging, hammering, pushing, reaching [9], [10], stacking, pouring [11]\u2013[13], and cutting [13] can be characterized by a sequence of contact waypoints (i.e., the order in which contacts between objects and robot bodies are made); second, these contacts are characterized by forceful affordances between object pairs: a specific pair of contact points on two objects would enable the application of forces along certain directions. However, searching for contact points that would enable the specific affordance is generally challenging due to complex constraints on reachability, collision avoidance, and motion stability.\nMAGIC tackles these challenges by combining data-driven and analytic approaches to generate contact waypoints in novel scenarios. In particular, it first extracts the sequence of contacts among objects in the reference trajectory and then proposes (pairs of) contact points that have similar global and local shape properties as the contact points in the reference, which can be used as guidance for motion planning or motion retargeting. Finally, it utilizes a physical simulator to discard trajectories that fail to achieve the goal due to collisions, unstable physical contact, or violations of joint and torque limits. Our key innovation lies in a novel global-to-local matching algorithm to find functional correspondences between the target objects and reference objects. Intuitively, a \"good\" contact point would satisfy both a global and a local matching property. First, the points on two objects should be on similar parts of the global shape (e.g., in the hook-using example, we need a contact point on the tool that is at the end of a long rod). Second, the hooking contact point should have a matched local curvature with the target object being hooked so that we can execute the actions stably. Therefore, we propose to use a pretrained visual feature-based correspondence matching to resolve the global matching property. This enables us to quickly search over different parts of the objects but the resulting contact point is usually not precise. Next, we use a local curvature-based matching"}, {"title": "II. RELATED WORK", "content": "Our algorithm is inspired by contact-based modeling techniques used in robotic manipulation. Various approaches have been developed for planning manipulations in contact space, involving both rigid bodies and robotic hands [14]\u2013[22]. Techniques such as CMGMP [1], [2], [23] and the work of [3] perform search over the exponential space of possible contact sequences to determine possible contact modes using three basic types: fixed, separating, and sliding contact, while [4] learns the types of the contact sequence from a single demonstration to guide the planning around them. By contrast, in this paper, we focus on leveraging global and local shape matching to generate analogical contact points using a single demonstration, which significantly improves the runtime efficiency of existing methods as well as the generalizations to unseen objects.\nRecently, researchers have explored learning object affordances [9], [11], [24]\u2013[28], usually from a large set of demonstrations. In contrast, this paper focuses on generating trajectories with novel objects from a single demonstration leveraging pretrained visual features and shape analysis algorithms, requiring no additional training data.\nFor many years, people have attempted to harness shape information to guide tool usage through mathematical analysis [29]\u2013[33] and data-driven approaches [5], [10], [34]\u2013[37]. These methods often necessitate intricate human specifications or extensive in-domain datasets. In contrast, our approach integrates the off-the-shelf visual models, pretrained on general image datasets, with the generic geometrical property of curvature. This combination allows us to achieve effective and efficient tool-using guidance with minimal human intervention and without the need for large specialized datasets."}, {"title": "III. ONE-SHOT MANIPULATION STRATEGY LEARNING", "content": "We propose MAGIC (manipulation analogies for generalizable intelligent contacts), a novel approach for fast and generalizable manipulation strategy learning from a single demonstration. Fig. 2 shows the overall framework. First, we extract the contact points among objects from the reference trajectory. Subsequently, we find the candidate contact points that support the functional affordances on unseen test objects through a global-to-local matching process, in which we first perform coarse correspondence point matching using pretrained vision transformer (ViT) features DINOv2 [38] (Section III-B), followed by a local alignment based on shape curvatures to find stable local contact patches (Section III-C). Finally, the candidate contact points and their matching scores can be used for tool selection, for retargeting reference object trajectories, or as waypoints for motion planning (Section III-D). The generated trajectories will be verified in a physical simulation and then output to the robot for execution.\nWe introduce the task of one-shot manipulation strategy learning. Specifically, we start with a single demonstration involving the SE(3) trajectories of objects that execute a particular manipulation strategy to achieve a goal (e.g., hanging a hanger) in the reference scene. Our goal is to generate the trajectories of objects in a target scene that apply a similar manipulation strategy to a different object (e.g., attaching a mug to a mug tree). We simplify this problem by assuming the robot is only manipulating one of the objects (i.e., the \"tool\" that the robot is holding), and the desired motion can be explained by achieving a sequence of contact waypoints. Many rigid-body tool-using tasks such as hooking, poking, stacking, and funneling, all fall into this category. For now, we assume that the strategy to apply and the manipulanda in both the reference and target scenes (e.g., the hanger or the mug) are already identified, and later we will discuss strategies for selecting the best tool object to accomplish a certain task. Therefore, the problem can be cast as making the analogy between the reference object motion and the target"}, {"title": "A. Problem Definition", "content": "object motion, taking into consideration other environmental constraints such as robot reachability and collisions.\nBased on our insights into decomposing manipulation trajectories into contact point sequences, this task further reduces to establishing a functional correspondence between the demonstrated object and the novel object, which can be represented as an SE(3) transformation between two rigid bodies for each contact waypoint. For tasks considered in this paper, we only consider scenarios where there is a single contact waypoint responsible for the target motion (but it generalizes to finding correspondences in multiple waypoints), and there is a reduced degree of freedom that can be reasoned about in 2D. In particular, we only model contacts on a canonical 2D cross-sectional view of the objects. For example, when we consider hooking objects on a table, we only consider object motions and shape properties on the surface that is perpendicular to the table's surface normal. This is equivalent to assuming access to a canonical pose of the object: for hooks, this will be the top-down view when the hook is placed on the table; for mugs, this will be the side view that contains the handle of the mug. In this paper, we assume that this canonical view is given, and in general, it can be predicted by external modules. Therefore, now, our goal is to establish a single correspondence between two 2D images of the objects in E(2) (translations, rotations, and reflections in 2D) plus scaling; and then we can recover the SE(3) correspondences based on the canonical object poses.\nOverall, the input to MAGIC is an image $I_r$ of the reference object T, an image $I_{T'}$ of the target object, and a point of interest $p_T$ on the reference image. Our goal is to find the corresponding point $p_{T'}$ on $I_{T'}$. When we have two objects interacting: the tool object T being directly held by the robot (e.g., the hook) and another object O in contact with T (e.g., the target object we want to hook), our goal would be to find a contact point pair $p_T$ on $I_T'$ and $p_O$ on $I_O$ given the reference (T,O,$p_T$,$p_O$) that maximizes a score function: $score (X,X')$, where X = (T,O,$p_T$, $p_O$) is the reference contact and X' = (T',O', $p_{T'}$,$p_{O'}$) is the target contact. In this paper, we employ a two-stage global-to-local matching process, therefore, the score function will be composed of two parts: a global matching score $S_{dino}$, and a local matching score $S_{curv}$:\n$score (X,X') = S_{dino}(T,T', p_T, p_{T'}) + \\lambda \\cdot S_{curv}(X,X')$, where $\\lambda$ is a constant hyperparameter. $p_O$ and $p_T$ are"}, {"title": "B. Global Contact Point Matching with Pretrained Features", "content": "The global matching score $S_{dino}$ is only computed between the tool objects T and T'. In this stage, we will find a candidate set of contact points \"globally\" on the target object T', leveraging visual features pretrained on large image datasets for capturing global and semantic correspondences [40]. Specifically, we adopt DINOv2 [40] as our visual feature extractor. Fig. 3a shows the pipeline: we first extract visual features for both objects (with different image rotations and reflections) and then find a candidate set of matching points.\nFeature extraction. We feed $I_T$ and $I_{T'}$ into DINOv2 respectively, and get feature maps $F_T$ and $F_{T'}$. In practice, to eliminate the effects of rotation and reflection in 2D, we apply horizontal flips and 12 rotations on $I_T$ and get a total of 24 images ${F_i}_{i=1}^{24}$. Next, we apply a principal component analysis (PCA) on the feature vectors. Given feature maps $F_T, F_{T'} \\in \\mathbb{R}^{n \\times n \\times d}$, let $flatten(F)$ be flattened version of F in $\\mathbb{R}^{n^2 \\times d}$, we have $W = PCA_{d,d'}(flatten(F_T), {flatten(F_i)}_{i=1}^{24})$, where n is the spatial resolution, d and d' are the original feature dimension and the feature dimension after reduction, respectively, and $W \\in \\mathbb{R}^{d \\times d'}$ is the matrix of principal component vectors. The output is $F = F \\times W \\in \\mathbb{R}^{n \\times n \\times d'}$.\nMatching by patches. Next, we aggregate the features within a local region (instead of a single point) so as to increase the receptive field. Formally, let $patch(p)$ denote the $m \\times m$ local area centered at point p, we find a point across all rotations of the target image $p_{T'}$ that maximizes the patch-aggregated cosine similarity with the reference point $p_T$:\n$S_{dino}(T,T', p_T, p_{T'}) = \\sum_{p \\in patch(p_T), \\\\ p' \\in patch(p_{T'})} <F_T (p), F_{T'}(p')>.$"}, {"title": "C. Local Contact Point Matching with Curvature Estimation", "content": "Although DINOv2 can propose contact points within a coarse global region, it is not accurate enough in terms of local geometric properties to support collision-free and stable physical motion. We leverage curvatures to refine the correspondence matching. Illustrated in Fig. 3b, our local alignment has four steps. We first construct a multiple observation scale pyramid: we estimate the curvatures of the reference contact point and the contact point proposed by DINOv2 on the target object at multiple scales. Then, based on the normal direction and the sign of the estimated curvatures, we perform irrelevant point suppression and convexity matching to find a physically plausible contact point for each scale. After that, we repeat the curvature estimation process on the updated contact points to get a more accurate curvature and normal direction. Finally, we accept the best contact point across scales.\nCurvature estimation at a given scale. Given an image of the object with segmentation, we first use the Canny edge detector [41] to get the object edges. We define the observation scale as the radius of the region centered at the point of interest (e.g., the contact point) on the object edge. For a certain observation scale s of the point c, we can use the edge points ${(x_i, y_i)}_{i=1}^n$ inside s to estimate the magnitude $\\kappa$ of the curvature and the radius of curvature r. Specifically, we first find the direction x' with the largest variance on ${(x_i, y_i)}_{i=1}^n$, and construct a local coordinate system x'y' centered at c, then represent ${(x_i, y_i)}_{i=1}^n$ in x'y' to get ${(x_i', y_i')}_{i=1}^n$. Afterwards, we fit a parabola $y' = ax'^2$ on points ${(x_i', y_i')}_{i=1}^n$, i.e.,\n$a = arg \\min_a \\sum_{i=1}^n (y_i' - ax_i'^2)^2$,\nthen by the definition of curvature, we have $\\kappa = 2|a|, r = 1/\\kappa$. Now, for a pyramid of observation scales ${s_j}_{j=1}^m$, we can compute a series of radii of curvature ${r_j}_{j=1}^m$. The motion functional scale is defined as $s_j$, where $j = arg \\min_j |r_j-\\alpha s_j|$, where $\\alpha$ is a parameter, and in practice, we select $\\alpha = 3.5$ for all objects across all experiments. This constant corresponds to a scenario where the observation scale is similar to the radius of curvature estimated using the points inside the observation scale. Furthermore, the sign of the curvature (that is, whether the point on the curve is convex or concave) can be computed based on the mask of the object. Given both"}, {"title": "D. Generating Object Motions by Making Contact Analogies", "content": "the tool object T' and target object O', we define the local matching score as:\n$S_{curv} ((T,O, p_T, p_O),(T',O',p_{T'}, p_{O'})) =  \\frac{r(p_T)}{r(p_O)} -  \\frac{r(p_{T'})}{r(p_{O'})}$.\nIrrelevant point suppression and convexity matching.\nDirect application of the curvature estimation algorithm is not robust enough for two reasons. First, the choice of the observation scale will significantly affect the estimation of the curvature. As illustrated in Fig. 4a, selecting a very small scale (scale 1) will make the estimation sensitive to local noise. Second, for large observation scales, irrelevant points (i.e., points on a different \u201cedge\u201d of the object) will be included and inject noise in the estimation, especially for thin objects as shown in Fig. 4b.\nTo eliminate these irrelevant points around the contact point of interest, we compute the curvature twice. First, we include all points within the observation scale. After computing the initial contact point C on the edge of the object and its curvature, we pick a point S that is at a small distance A from the contact point C along the direction of the radius of curvature $c_0$ in the initial estimation. Then, we select all points on the same \"edge\" as C on the side of S. This is done by emitting \u201crays\u201d in all directions from S and for each ray selecting the point that is hit first. We then use these points to estimate the curvature again, as shown on the right of Figure 4b. Moreover, we ensure that the curvature sign (i.e., the convexity) of the corresponding point in the target image is consistent with that in the reference image by performing a local search within the observation region of the target image, which is depicted in Fig. 4c.\nSo far we have presented a general mechanism for finding geometric functional correspondence between reference and target objects, and it can be used in many downstream modules. In the one-shot manipulation strategy learning setting, we consider three particular use cases.\nObject motion retargeting. This is the most straightforward way to generate target object motions assuming there is"}, {"title": "IV. EXPERIMENTS", "content": "a single contact that accounts for the motion. Given the reference contact point $p_T$, the direction vector v of the view angle, the normal vector $n_T$ of $p_T$ estimated from curvature calculation, we can construct two local frames for the reference and the target object, and directly retarget the reference object motion by aligning two local frames.\nMotion planning based on contact waypoints and force directions. We can generate object motion by leveraging analogical contact points as waypoints for a motion planner. For example, if our goal is to hang an object at a particular position, we can first compute the hanging pose by making analogies with the reference object pose and use a collision-free motion planner to generate the robot motion. This also applies to hook-using tasks where the goal is specified as to apply a particular force along a target direction on an object.\nTool selection. Our pipeline also supports tool selection, where the goal is to select one \"tool\" object (e.g., a hook) from a set of available objects that can best execute the strategy in the new scenario, given the other object to manipulate (e.g., the object to be hooked). We apply our algorithm on all available tools, and select the tool with the highest score.\nIn this work, we have conducted experiments both in simulation and in the real world. For simulation experiments, we adopt the simulation environment SAPIEN 2 [42] using a Franka Emika Panda arm. We evaluate different one-shot manipulation strategy learning algorithms on three tasks, as shown in Fig. 9: (Scooping) scooping balls of different sizes with various spoons against a concave arc, given a demonstration with a reference spoon; (Hanging) hanging a mug onto a mug tree, given a demonstration of hanging a hanger on a rod; (Hooking) selecting a tool from a set to hook objects of varying shapes and sizes, given a demonstration of hooking a ball with a hook. To generate object motions, we adopt object motion retargeting for Scooping and motion planning for Hanging and Hooking from Section III-D. For all tasks, we have two variants: Floating-Gripper (FG) where the tool will be manipulated by a floating gripper, and the harder Arm variant where the tool needs to be grasped and manipulated by the robot. In this variant, we use a general antipodal grasp sampler [43] and RRT-Connect [44] for generating robot trajectories, of which we utilize MPlib [45] as the implementation. For Hooking, we also have a variant where we provide a random tool to the floating gripper (no tool selection). For all tasks, we only provide a single demonstration of object motion trajectories, the canonical view, and the pair of contact points on reference objects.\nBaselines. We implemented two sets of baselines: global shape-matching and local shape-matching methods. For global shape-matching methods, we use two different methods: principal component analysis (PCA) and iterative closest point with FPFH features (FPFH+ICP) [46]\u2013[48] on object point clouds to find the best transformation that would align the target object and the reference object. Then, we retarget the object motion or transform the waypoints. For local shape-matching baselines, we also implemented two methods:"}, {"title": "V. CONCLUSION", "content": "We have presented MAGIC, a framework for one-shot manipulation strategy learning, enabling robots to quickly adapt and execute tool-using tasks with novel objects. We integrate both data-driven and analytical shape-matching algorithms for the best of both worlds, to quickly generate precise and physically plausible contact points. Noteably, our algorithm is generic in the sense that we do not need task-specific information for the matching algorithm other than the reference contact waypoints. Experiments on three representative tasks illustrate the effectiveness of our method.\nLimitations. Currently, our pipeline is designed for making contact analogies for a single contact patch between two objects. Future work should consider interactions among multiple objects with multiple contact points, and extend the current shape-based matching criteria to consider forceful affordance. Another future direction would be to consider predicting or searching over 2D views of objects for correspondence matching. Moreover, our overall framework of coarse-to-fine and semantic-to-geometric matching can be extended to 3D pretrained features [56]\u2013[58] and curvatures."}]}