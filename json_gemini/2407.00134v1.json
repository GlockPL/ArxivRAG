{"title": "A SIMPLE ATTENTION-BASED MECHANISM FOR BIMODAL EMOTION CLASSIFICATION", "authors": ["Mazen Elabd", "Sardar Jaf"], "abstract": "Big data contain rich information for machine learning algorithms to utilize when learning important features during classification tasks. Human beings express their emotion using certain words, speech (tone, pitch, speed) or facial expression. Artificial Intelligence approach to emotion classification are largely based on learning from textual information. However, public datasets containing text and speech data provide sufficient resources to train machine learning algorithms for the tack of emotion classification. In this paper, we present novel bimodal deep learning-based architectures enhanced with attention mechanism trained and tested on text and speech data for emotion classification. We report details of different deep learning based architectures and show the performance of each architecture including rigorous error analyses. Our finding suggests that deep learning based architectures trained on different types of data (text and speech) outperform architectures trained only on text or speech. Our proposed attention-based bimodal architecture outperforms several state-of-the-art systems in emotion classification.", "sections": [{"title": "Introduction", "content": "Recognizing human emotions automatically from text is a challenging task. Textual data lacks emotional cues, such as speech tone, pitch, vocal expression, facial expression, etc., that are helpful in determining the emotion of a person accurately. Therefore, approaches to automated emotion recognition that rely only on text are inherently limited. Recent attempts at emotion recognition are directed at using other types of information such as audio, image, and/or video as well as text in order to enrich the information needed to accurately recognize/classify human emotions.\nWith advances in machine learning applications for various tasks (such as image processing, computer vision, and natural language processing), it is possible to design multimodal emotion recognition/classification systems by training machine learning algorithms on labeled datasets containing different types of data modalities such as text, images, audio, and/or video. Multimodal emotion recognition/classification involves training one or more machine learning algorithms that are best suited for learning from different types of data. For example, we could train algorithm x on text data, y on audio data, and z on visual data and then combine their learning capabilities from these different types of data to perform the classification task. In this paper, we propose a novel design of deep learning architecture that uses text and audio information for the task of emotion classification. We make the following contributions:\n\u2022 We propose a novel deep-learning multimodal architecture to extract and utilize important features from different types of data (text and audio) to classify emotions into one of several emotion classes.\n\u2022 We propose a new state-of-the-art multimodal emotion classifier.\n\u2022 We propose mid-level data fusion methods for extracting rich features from different unimodal architectures.\nThe rest of the paper is organized as follows: Section 2 outlines key related work to our proposed system. We describe our methodology in section 3, in section 4 we show the performance of the proposed system and compare it against"}, {"title": "Related work", "content": "The advances in deep learning methods for audio and text processing have motivated researchers to develop different approaches to emotion classification. Generally, these approaches involve training deep learning algorithms on audio and text data, and incorporating a fusion mechanism to combine both modalities. Early feature extraction methods from text, such as word2vec, involve learning information from text in relation to word features. Further advancements in feature extraction from text include bidirectional long short-term memory networks (Bi-LSTM), gated recurrent unit (GRU), and transformers. Furthermore, rich language models containing multi-lingual and contextual information around words were developed such as Bidirectional Encoder Representations from Transformers (BERT) [1], Robustly optimized BERT (RoBERTa) [2], and GPT [3], etc.\nMethods to combine several data modalities (e.g., different types of data such as text, audio, and images) have been effective in emotion classification [4]. Multimodal transformers that allow the concatenation of the feature representation from different types of data have replaced previous methods [5] [6], with further advances through multi-view sequential learning models [7] and dynamic fusion graph [8]\nDutta et al [9] proposed a hierarchical cross-attention model approach using recurrent and co-attention neural networks models by training them on text and audio data. Their first stage involved training utterance-level embedding extractor from the input data (text and audio), which trains their model to classify individual utterances without accounting for the inter-utterance conversational context. The second stage of training involves feeding the utterance-level embedding from the first stage to a bidirectional gated recurrent unit (Bi-GRU) to introduce inter-utterance context to the model. This stage enhances the model with conversational context information. The last stage consists of the fusion of the embedding from different modalities using self-attention and cross-attention mechanisms. These attention mechanisms allow for capturing relationships and dependencies within input sequences (sentences or speech utterances).\n[10] proposed a hierarchical transformer-based model (HiTrans) consisting of transformer-based content and a speaker- sensitive model for emotion classification. Their method uses two hierarchical transformers: a BERT model is used as the low-level transformer for generating local utterance representation, and a high-level transformer that takes the output of the low-level transformer as input to make the model sensitive to the global context of the conversation. They integrate a \"pairwise utterance speaker verification\u201d (PUSV) method to detect whether two utterances belong to the same speaker.\n[11] proposed a Dialogue Graph Convolutional Network (DialogueGCN) model utilizing a graph neural network based approach to emotion classification by leveraging self and inter-speaker dependency of the interlocutors to model conversational context for emotion classification. One of the main benefits of such a method is addressing context propagation issues present in the current RNN-based methods.\n[12] proposed a learning-based system for emotion classification using multiple input modalities that combined information from text, facial cues, and speech. Their system seems to pay more attention to reliable cues while suppressing less helpful cues on a per-sample basis by using Canonical Correlation Analysis, which differentiates between effective and ineffective modalities. The major strength of their proposed system is its robustness to sensor noise in any of the individual modalities."}, {"title": "Proposed Method", "content": "We propose a supervised machine learning approach for the task of emotion classification. We use a public dataset containing different types of data (text, speech, and video) for the training and evaluation of a multimodal emotion classifier. The benefit of using different types of data is to extract different levels of details and cues for identifying emotions. Different data types contain different features/information for training machine learning algorithms. Due to limited access to computational resources, we only use text and speech data types in this study. We use different deep learning based feature extraction methods (embedding) for learning patterns from text and speech data. For text, we use BERT embedding [1] and for speech, we use Audio Spectogram Transformer (AST) embedding [13]."}, {"title": "System Design", "content": "Our system design is based on fine-tuning attention-based models as well as the reliance on attention-based feature fusion. We believe that two main problems should be addressed when building any multimodal architecture: i) the data"}, {"title": "Selecting the unimodal models", "content": "Since the presented architectures in this work rely on unimodal systems to create feature representations for each modality, the selection of the unimodal system is critical to optimise our system's performance. We believe that the various unimodal models utilized within a multimodal classification architecture should rely on similar feature representation algorithms to reach a homogeneous feature vector when combining them in the system.\nWe choose attention-based unimodal approaches to build the feature vector for each modality since they are proven to achieve state-of-the-art results in many benchmarks and most importantly to maintain homogeneous feature represen- tations between modalities. Attention-based models are popular in various unimodal classification domains, such as text, speech, video, and image, which enable our multimodal systems to easily scale to include another modality. The selected unimodal systems are mainly based on transformer-encoder and only rely on attention to keep the models as homogeneous as possible. In addition, the selected systems are simple and fast to fine-tune.\nThe last hidden layer from each of the transformers (BERT and AST) contains the most comprehensive and contextual- ized representation of the input sequence; therefore, it is logical to use it as the feature vector for each modality."}, {"title": "Baseline system", "content": "For our baseline model, as depicted in Fig. 1, the information derived from the last hidden layer of BERT and AST is fed into a simple concatenation layer as the fusion layer. This was followed by a flattening layer, then a linear layer with dropout (10% dropout rate) as the classification layer."}, {"title": "Attention-based system", "content": "For this system, we rely on feeding the feature vector from BERT and AST models to a multi-head cross-attention layer, as in Fig. 2, which acts as a fusion layer that is capable of capturing the interactions between modalities and developing a comprehensive understanding of the multimodal data. The multi-head cross-attention layer is followed by a multi-head self-attention layer to enrich the combined feature vector before classification. Next, a flattening layer and a linear layer with a 10% dropout rate serve as the classification layer."}, {"title": "dataset", "content": "We use Multimodal EmotionLines Dataset (MELD) [14] for the evaluation of the proposed systems. The dataset contains more than over 13000 utterances derived from multi-party conversations. it is divided into three parts: training, validation, and testing. The training set consists of 9,988 utterances, the validation set 1,108 utterances, and the test set 2,610 utterances. We use these partitions unchanged for training, validation, and testing."}, {"title": "Hyperparameters", "content": "The MELD dataset suffers from imbalance class distribution which impacted our decision to rely on AdamW as an optimizer, the weighted cross-entropy as the loss function since the dataset suffer from imbalance class distribution problem, and the weighted F1 score as our main evaluation metric. We integrated early stopping in all of our experiments to stop the training once the weighted F1 score started to decrease.\nSome hyperparameters were only shared across the multimodal experiments such as using a 10% dropout rate in the last linear layer. Also, both multi-head attention layers used in this work were configured to consist of 128 attention heads\u00b9. We performed limited experiments to reduce the number of attention heads, although the weighted F1 score was negatively impacted."}, {"title": "evaluation metric", "content": "Some of the most robust and widely used performance measures for classification tasks are: (i) recall, (ii) precision, and (iii) F1 score. The recall metric measures the proportion of actual positives that the model is able to classify. Precision, on the other hand, measures the proportion of predicted positives that are correctly identified. Individually, these metrics do not offer a complete view of model performance. The F1 score computes the harmonic mean of the recall and precision offering a robust evaluation. Since the data are imbalanced, we use a weighted F1 score to account for each class's contribution based on its support, which considers the number of actual occurrences of the class in the dataset. Moreover, we compute and present the macro average and weighted average performances of the models to show the overall system performance. We present the macro average to assess precision, recall, and F1 score across individual classes, treating all classes equally. We also use the weighted average performance to account for the importance/weight of each category when calculating the overall system performance."}, {"title": "Result and Discussion", "content": "The proposed deep learning systems consist of appropriate deep learning algorithms for each data type. They are trained and tested on two types of data: text and speech data.\nWe present several performance aspects of the proposed systems for recognizing different types of emotions. We use several standard evaluation metrics for each category: precision, recall and F1 score, macro average and weighted average F1 score. These performance measures allow us to identify the model's performance at the category level. In the following subsections, we outline the performance of the systems when trained and tested for emotion classification."}, {"title": "Model evaluation result at category level", "content": ""}, {"title": "Text unimodal performance", "content": "Table 2 shows the performance of the text-based unimodal system in classifying emotions when trained and tested on text data. The second, third, and fourth columns show the precision, recall and F1 score for each type of emotion. The text-based unimodal model failed to classify the \u201cdisgust\u201d emotion. Although it scores a recall of 50% when classifying the \"fear\" category, it has a very poor precision of 0.02 (2%), resulting in a very poor F1 score of 0.038. The text-based unimodal model classifies the \u201cneutral\" category more accurately than other categories, achieving an F1 score of 0.790. This is followed by the \"joy\" category (with an F1 score of 0.615) and the \u201csurprise\" category (with an F1 score of 0.590). The performance in classifying \"anger\u201d and \u201csadness\u201d categories are F1 scores of 0.492 and 0.355, respectively."}, {"title": "Speech unimodal performance", "content": "The speech-based unimodal system performance is relatively similar to that of the text-based unimodal system. It performs better in classifying the \u201cneutral\u201d emotion than any other type of emotion and performs very poorly in classifying \"disgust\" and \"fear\" emotions. As presented in Table 2, the speech-based unimodal system fails to classify \"disgust\" and \"fear\" categories while achieving an F1 score of over 0.655 for the \"neutral\" category. Similar to the text-based unimodal system, following the \u201cneutral\" category, the speech unimodal system achieves an F1 score of 0.303, 0.266, and 0.208 for the \"anger\", \"surprise\", and \"joy\" categories, respectively."}, {"title": "Baseline multimodal performance", "content": "We evaluated the proposed multimodal baseline system to assess its performance at classifying different types of emotions. Table 3 presents the performance of the baseline system. The system produced the highest F1 score in classifying the \u201cneutral\u201d emotion (0.779), followed by \u201cjoy\u201d (0.609). The system produced the lowest F1 scores in classifying the ' disgust\" and 'fear\" categories with 0.029 and 0.105, respectively. The overall F1 score across all the emotion categories is 0.627. The baseline system has a reasonable overall weighted average of precision and recall across all emotion categories, producing scores of 0.640 and 0.635, respectively."}, {"title": "Attention-based multimodal performance", "content": "Table 4 presents the performance of the attention-based system. The system produced the highest weighted F1 score, recall, and precision compared to other models in this work, with scores of 0.693, 0.706, and 0.689, respectively. It also"}, {"title": "System performance comparison with previous works", "content": "Table 5 presents the weighted F1 score performance of our proposed systems (baseline and attention-based) and several previously published works on unimodal and multimodal emotion classification. Our proposed unimodal systems lagged behind some previously published systems, while our proposed multimodal systems have performed better than most of the other systems.\nOur proposed attention-based system significantly outperformed state-of-the-art systems by achieving a 69.7% F1 score, nearly 5% higher than the reported state-of-the-art system proposed by [9]. This is despite the low performance of our unimodal systems compared to the unimodal systems proposed by [9]. This indicates the strength of our novel method of combining the last hidden layers of attention-based text and speech unimodal models using a multi-head cross-attention layer followed by a multi-head self-attention layer for fusing text and audio modalities."}, {"title": "Multimodal performance discussion", "content": "Our proposed speech-based unimodal emotion classifier underperforms compared to those classifiers proposed by Dutta et al [9] and Ho et al [16]. Our text-based unimodal model is only outperformed by the model proposed by Dutta et al [9]. However, our multimodal classifier outperforms the state-of-the-art. We anticipate that the superior performance of our multimodal system results from our homogenous attention-based model which includes an attention-based fusion method that merges the learned information from attention-based unimodal models, which are fine-tuned versions of BERT and AST. In comparison to several other published works, our speech-based unimodal performance is on par with many other systems and better than others in some cases. Our text-based unimodal model performs better than almost all other published works, as shown in Table 5. Furthermore, our multimodal attention-based system outperforms all other published works, making it the state-of-the-art multimodal emotion classifier."}, {"title": "Error analyses: confusion matrix", "content": "Machine learning applications for supervised classification tasks require a set of labeled data that represents the classes. Labeled datasets often contain errors. One of the main issues is label errors (annotation errors), where some data samples are labeled with incorrect emotions. Annotation errors impact machine learning algorithms' performance because the algorithms learn from the errors present in the annotated dataset. Confusion matrix graphs are helpful for highlighting classification errors by identifying misclassified classes and the classes they are confused with. We present several confusion matrices to outline the classification errors of our proposed unimodal and multimodal systems in classifying different types of emotions."}, {"title": "Emotion misclassification errors", "content": "As presented in Fig. 3, our text unimodal system miscalssifies\u201canger\u201d class as \u201cnetural\u201d 26.1%. The class\u201cdisgust\u201d is misclassified with \"neutral\" 39.7%, \"fear\" is misclassified with\u201cneutral\" 40%, and topped by \"sadness\" with 41.8%. Contrary to the \"neutral\" class that has the largest data samples in the dataset, the text-based unimodal does not confuse any class (except \"joy\u201d confused with \u201cfear\u201d, though negligibly) with the \"disgust\" and \"fear\" classes, which have the fewest data samples in the dataset.\""}, {"title": "conclusion and future work", "content": "Recent advances in automated data classification of emotion involve the application of machine learning algorithms to automate the process of classifying certain types of emotions (e.g., anger, happy, surprised, etc.). The focus of this study was to design and evaluate novel deep learning architectures that learn from different types of data (text and speech) to classify different kinds of emotions. We applied deep learning based language models, such as BERT and Audio Spectrogram Transformer. We proposed novel deep learning system to fuse embedding data using attention mechanism for extracting important features from different types of data to classify emotion. We have measured the performance of each architecture using different performance matrices. We present rigorous error analyzes of the proposed system classification performance, where certain emotion classes are miss-classified. Our finding suggests that deep learning architectures trained only on text or speech data could underperform architectures that are trained on a fusion of data (e.g., text and speech). Thus, proving that multimodal systems could perform better than unimodal systems for emotion classification. Our future work is to extend our multimodal system to evaluate it on sentiment analyses, and to fuse other data types when training it (e.g., fusing video data with text and speech). We believe this is feasible because the public dataset (MELD) contains videos and our current architecture is designed to be simple and thus allowing us to easily extend it to include information learn by current embedding algorithms from videos. Also, MELD data contains information for sentiment analysis."}]}