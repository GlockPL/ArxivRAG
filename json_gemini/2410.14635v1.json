{"title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings", "authors": ["Raghuveer Thirukovalluru", "Bhuwan Dhingra"], "abstract": "Training-free embedding methods directly leverage pretrained large language models (LLMs) to embed text, bypassing the costly and complex procedure of contrastive learning. Previous training-free embedding methods have mainly focused on optimizing embedding prompts and have overlooked the benefits of utilizing the generative abilities of LLMs. We propose a novel method, GenEOL, which uses LLMs to generate diverse transformations of a sentence that preserve its meaning, and aggregates the resulting embeddings of these transformations to enhance the overall sentence embedding. GenEOL significantly outperforms the existing training-free embedding methods by an average of 2.85 points across several LLMS on the sentence semantic text similarity (STS) benchmark. Our analysis shows that GenEOL stabilizes representation quality across LLM layers and is robust to perturbations of embedding prompts. GenEOL also achieves notable gains on multiple clustering, reranking and pair-classification tasks from the MTEB benchmark.", "sections": [{"title": "1 Introduction", "content": "While LLMs are very good at generating text, the embeddings obtained from their activations are often not suitable for downstream tasks such as computing sentence similarity or clustering. Contrastive learning (CL) is typically used to further finetune LLMs to produce better embeddings. Traditional contrastive learning approaches used human-annotated data for training embeddings (Gao et al., 2021), while recent methods leverage LLMs to generate contrastive data (Wang et al., 2023; Zhang et al., 2023a; Thirukovalluru et al., 2024b). However, curating high-quality CL training data is both costly and time-consuming, and CL further requires large batch sizes and extensive computational resources for each round of training (Wang et al., 2023; Muennighoff et al., 2024).\nAs newer language models are continuously developed and released, it is important to consider training-free, inference-time text embedding methods, which may provide a more efficient and adaptable alternative to training-intensive techniques.\nJiang et al. (2023) first explored this task for sentences by proposing prompts like This sentence: \"[TEXT]\" means in one word:\" and used the hidden layer representation of the last token from an LLM to represent [TEXT]. Zhang et al. (2024) enhanced the approach by incorporating chain-of-thought and knowledge reasoning. Similarly, Lei et al. (2024) utilized multiple diverse prompts to capture different sentence aspects, averaging their embeddings for improved results. Concurrently, Springer et al. (2024) repeated the original sentence to facilitate bidirectional attention in embedding sentences.\nWhile these methods have significantly advanced training-free embeddings, they have not fully exploited the generative capabilities of LLMs to improve embedding quality. Recently, multiple techniques have begun scaling inference-time compute to enhance the reasoning and generative abilities of LLMs (Brown et al., 2024; Liang et al., 2024); we adopt a similar approach for embeddings in this work. Our approach offers a method to harness the generative capabilities of large language models (LLMs), to enhance sentence embeddings. It can also work with black-box language models. Specifically, we prompt an LLM to generate m diverse sentence variations that maintain the original meaning, which are then aggregated to produce more refined embeddings.\nWe specifically focus on the semantic text similarity (STS) benchmark for sentence embeddings (Conneau and Kiela, 2018). Our key contributions are as follows: 1. We show that diverse meaning retaining transformations are helpful in improving training free embeddings. 2. The proposed method, GenEOL, at a higher capacity (m=32), significantly outperforms the previous best training-"}, {"title": "2 Background and Related Work", "content": "This section covers contrastive learning (CL) and generation-based methods utilizing CL, followed by an overview of training-free approaches.\nCL Training: Contrastive training employs (anchor, positive, negative) data, where the positive is semantically similar to the anchor and the negative is dissimilar. InfoNCE loss (Gao et al., 2021) draws the positives' embeddings closer to that of the anchor while distancing the negatives. SimCSE (Gao et al., 2021) applies this loss using human-annotated data to train sentence embedding models.\nGenerating data for CL Training: E5 (Wang et al., 2023) used ChatGPT to generate a huge CL corpus of related positives and unrelated negatives across multiple tasks. Gecko (Lee et al., 2024) used an LLM to generate queries and relabel positives, negatives from existing data corpus. LLM2Vec (BehnamGhader et al., 2024) used representations generated from different dropout masks as positives with a positive only CL loss. For sentences specifically, SynCSE (Zhang et al., 2023a) defined specific transformations with ChatGPT to develop contrastive positives, negatives for sentence embedding training. Inspired by compositional transformations in CL data for computer vision (CV), SumCSE (Thirukovalluru et al., 2024b) further improved these transformations by using \u2018summary\u2019 operations (akin to cropping in CV). Our method, GenEOL, takes inspiration from SynCSE, SumCSE in generating transformations (with same core meaning) to improve embedding quality.\nTraining Free Methods: Echo (Springer et al., 2024) showed that repeating the original sentence and using the hiddden representations of the later as the embedding of the sentence can improve performance. PromptEOL (Jiang et al., 2023) showed that generative LLMs when used with the prompt (This sentence: \"[TEXT]\" means in one word:\") and using the hidden representation of the last token i.e. is very effective at embedding the text. Further improving this prompt Zhang et al. (2024) proposed two advanced prompts - 1. Pretended Chain of Thought prompt (PCoTEOL) - (After thinking step by step, this sentence : \u201c[X]\u201d means in one word: \u201c) and 2. Knowledge Enhancement prompt (KEEOL) - (The essence of a sentence is often captured by its main subjects and actions....\u201c[X]\u201d means in one word: \"). MetaEOL (Lei et al., 2024) utilized eight diverse prompts from four categories-Text Classification, Sentiment Analysis, Paraphrase Identification, and Information Extraction to aggregate multiple perspectives into a single final embedding. These methods also discuss better ways to extract text embeddings by using LLM penultimate layers. However, they do not fully leverage the generative power of LLMs for embedding sentences."}, {"title": "3 Motivation and Methodology", "content": "This section outlines the STS sentence similarity task, presents key empirical observations that motivate our approach, and finally elaborates the proposed GenEOL method.\nTask Definition: Given a set of sentence pairs, {...(X1, X2)...}, the goal is to generate embeddings for each sentence such that, when ranked by the cosine similarity of their embeddings, the ranking aligns with a provided reference ranking. Performance is evaluated using Spearman's rank correlation between the predicted ranks and the true ranks. The task assumes access to a pretrained LLM $L_{PT}$ and an instruction-tuned LLM $L_{IT}$.\nObservation 1: Instruction tuned LLMs are worse than Pretrained LLMs at Embeddings\nWe first analyze in Fig. 2a, the performance (spearman rank correlation) of instruction-tuned LLMs versus pretrained models on the STSB validation set using the KEEOL prompt (Zhang et al., 2024) for sentence encoding. Results indicate that pretrained models consistently outperform instruction-tuned models in embeddings. Additionally, when applying an instruction version of the knowledge EOL prompt (with [INST] tokens) to instruction-tuned models, performance decreased further. This may explain why recent studies Zhang et al. (2024) and Lei et al. (2024) benchmark non-CL training-free methods primarily using pretrained models. On the other hand, instruction tuned models are generally vastly superior at generating texts suited to various prompts. This raises the question \u2013 how can we utilize the generative ability of an instruction-tuned model to enhance the embeddings obtained from the pretrained model?"}, {"title": "Observation 2: LLM embedding scores are coarsely aligned with True scores.", "content": "To visualize the rank correlation discussed above, we present a scatter plot comparing the true and predicted ranks for the Mistral0.1-7B embedding model in Fig. 2b. An ideal such scatterplot would have all elements aligned along the diagonal. Despite the notable amount of spread in Fig. 2b, predicted scores are still coarsely aligned with the true scores.\nTo reduce the spread around the diagonal, let's consider the following exercise. Let $(X_{i1}, X_{i2})$ be the i-th datapoint. Let $T_i$ be the true score and $p_i$ be the predicted similarity score of this datapoint. Considering $p_i$ to be a random variable, the coarse alignment trend from Fig 2b gives a sense of an error being present in $p_i$. Specifically, $p_i = T_i + E_i$, where $e_i$ is the error term with a mean $\\mu_i$ and variance $\\tau_i$. From a statistics standpoint, the primary task now is to the reduce this error to improve performance. $\\mu_i$ here is the inherent bias and would require further tuning the model to reduce it. To reduce the variance of $\\tau_i$, a simple trick now would be to average k independent estimates of $p_i$ i.e. {$p_i^1,.., p_i^k$}. It is known that the variance of the mean of k independent random variables decreases by a factor of k and hence reduces the error $e_i$.\nAs we've seen in Observation 1, instruction tuned models although bad embedding models are very good at generating text of preferred format. Hence, we hypothesize that instruction tuned models can be prompted appropriately to transform datapoint $(X_{i1}, X_{i2})$ to {$(x_{i1}^1, x_{i2}^1), (x_{i1}^2, x_{i2}^2), .., (x_{i1}^m, x_{i2}^m)$} with each pair containing the exact same meaning i.e. having the exact same $T_i$. While these pairs may not be entirely independent, it would still contribute to reducing variance. Let's say we transform each"}, {"title": "3.2 Methodology", "content": "Figure 1 illustrates the methodology for the proposed approach. Our method, GenEOL uses the LLMs in two distinct roles 1. Generator (uses $L_{IT}$); 2. Embedder (uses $L_{PT}$). Although our motivation necessitates independent transformations of $x_i$, the method to achieve this remains uncertain. Therefore, we employ the generator to facilitate diverse transformations of $x_i$. The transformations are accomplished by applying suitable prompts to the input sentence $x_i$ and utilizing LLM $L_{IT}$ to generate the modified sentence.\n3.2.1 Diverse Transformations\nTransformation 0 ($T_0$): Original sentence, $x_i$, is the only transformation that retains all aspects and meaning of the sentence.\nTransformation 1 ($T_1$): Syntax of sentences has been shown to confuse sentence embeddings (Zhang et al., 2023b). Hence, $T_1$ is a sentence structure changing transformation.\nTransformation 2 ($T_2$): Removing non essential details like adverbs shouldn't change the core meaning of the sentence. Hence, following Zhang et al. (2023a), we use concise sentence transformation as $T_2$.\nTransformation 3 ($T_3$): Entailment is another transformation which has low semantic overlap while retaining the core meaning of a sentence (Gao et al., 2021). This becomes $T_3$.\nTransformation 4 ($T_4$): A regular paraphrasing that can retain the meaning is used as $T_4$ (Zhang et al., 2023a; Thirukovalluru et al., 2024b).\nFew shot prompts used for these transformations are detailed in \u00a7A.4. An example of transformed sentences with these prompts is shown in Table 9"}, {"title": "3.2.2 Further Increasing Diversity (Optional)", "content": "SumCSE (Thirukovalluru et al., 2024b) showed that compositional summary transformations (a second summary transformation over the first transformation) is very effective at creating transformed sentences far from the original sentence. Compositional transformations retain the original meaning of the sentence while more significantly altering its lexical form compared to a single transformation. As shown in Table 1, a compositional summary transformation (\u00a7A.4) (emphasizes for meaning preservation in contrast to SumCSE prompt) shows benefits. Thus, we optionally incorporate the compositional summary transformation in our approach. The proposed framework, GenEOL, also allows for use of other diverse transformations or sampling multiple diverse sentences from these proposed transformations to improve sentence embeddings."}, {"title": "3.2.3 Final Embedding", "content": "The transformed sentences are individually embedded using the embedder, LLM $L_{PT}$ following previous work (Zhang et al., 2024). This mean embedding becomes the final embedding of the sentence."}, {"title": "4 Experiments", "content": "We evaluate GenEOL and other baselines on 7 semantic text similarity tasks - STS12, STS13, STS14, STS16, STS16, STSB, SICK-R (Gao et al., 2021). Spearman rank correlation (cosine similarity) is the main metric (Muennighoff et al., 2022). Training sets of the STS tasks are not used. We additionally asses GenEOL on 10 MTEB tasks across 4 categories (Classification, Clustering, Reranking"}, {"title": "4.1 Implementation", "content": "We explore multiple choices for both generator and embedder. For generator, we try with both small and large models: Mistral0.1-I-7B (Mistral-7B-Instruct-v0.1) and ChatGPT (gpt-3.5-turbo-0125). Embedder is the common module that GenEOL shares with other baselines and becomes the basis of comparison. For embedder, we try three pretrained models based on previous methods Mistral0.1-7B (Mistral-7B-v0.1), Llama2-7B (Llama-2-7b-hf), Llama3-8B (Meta-Llama-3-8B).\nEmbedder in GenEOL, by default, uses the embedding prompt from KEEOL (Zhang et al., 2024) i.e. \"The essence of a sentence is often captured by its main subjects and actions, while descriptive terms provide additional but less central details. With this in mind, this sentence : \u201c[X]\u201d means in one word: \"\". We evaluate the performance of GenEOL at different values of m. When m > 4, multiple transformed sentences are sampled from the generator from each transformation."}, {"title": "4.2 Main Table", "content": "Table 2 shows results of multiple methods on STS. Results for KEEOL and PCOTEOL use the penultimate layer as proposed in Zhang et al. (2024). Every other method (including GenEOL) uses the final layer. GenEOL significantly beats the next best method by (3.88 , 1.83, 2.83) points on average with (Mistral0.1-7B, Llama2-7B, Llama3-8B) respectively on the 7 STS task average. The average gain of GenEOL (m=32) is a 2.85 points higher than the previous best method across models. GenEOL demonstrates consistent performance improvements across all embedder models, a trend that is uncommon in previous approaches. For a more fair comparison, we additionally pro-"}, {"title": "4.3 Ablation 1: Effect of increasing the number of transformed sentences, m", "content": "The number of transformed sentences, denoted as m, is a key parameter that significantly influences the performance of GenEOL. To provide a clearer understanding of GenEOL's effectiveness, we present results with varying m values for both the Mistral0.1-7B and Llama3-8B embedding models in Fig. 3. For m = 2, we randomly sample from transformations T\u012b and T2. For m \u2265 4, we maintain and equal diversity across all transformations - (T\u2081...T4). Even with just two generations, GenEOL (m = 2) beats all other baselines.\nIncreasing m results in a significant increase in performance from m = 0 to m = 32 of over 5 points. The improvements are lower at higher m. Performance starts to stagnate after m = 16. This is because similar transformed sentences/repetitions are sampled from the generator at higher m for a fixed number of transformations (4 in our case). Hence, more diverse transformations might be required at higher m values."}, {"title": "4.4 Ablation 2: Dissecting GenEOL", "content": "To understand the contribution of different components of GenEOL, we assess performance of individual components of GenEOL. Table 3 shows results. All methods use the exact same value of m = 8. Removing compositions results in notable reduction in performance (-0.54). Hence, similar to SumCSE, summary compositions are important. Among the individual transformations, T\u2081"}, {"title": "4.5 Ablation 3: Effect of Layer Number", "content": "Prior work on training free embeddings has shown that the last hidden layer might not be the most appropriate layer for text embeddings (Lei et al., 2024; Zhang et al., 2024; Li and Li, 2024). We hence evaluate the performance of GenEOL across different hidden layers. We perform the same analysis for KEEOL. Penultimate layer does best for Llama3-8B and second best for Mistral0.1-7B. Performance drops beyond the penultimate layer. Additionally, the variation (max-min) across layers is lower for GenEOL at (1.52, 1.44) compared to KEEOL at (2.22, 1.89) and MetaEOL at (2.5, unknown) for (Mistral0.1-7B, Llama3-8B) respectively. Hence, GenEOL can stabilize the representational quality across the LLM layers."}, {"title": "4.6 Compute Allocation in GenEOL", "content": "Datapoints in STS datasets comprise of a pair of sentences (sentence\u2081, sentence\u2082) whose similarity needs to be predicted. Given a budget of generating 32 transformed sentences, GenEOL by default invests this budget equally into both sentences in the pair. From the intuition discussed in \u00a73.1, this would create large pool of pairs which result in better averaging and improved results. Alternatively, one might invest this entire generation budget to sentence1 or sentence2 individually.\nIn this ablation, we assess the effect of unequally splitting budget between the two sentences. Results are shown in Table 4. Interestingly, allocating all generations to one of the sentences does worse than the baseline (without any generations i.e. m = 0). This is counter-intuitive to \u00a73.1, which says that increasing the number of pairs improves performance. We posit this happens because a sentence when averaged with large number of embeddings, undergoes a normalizing effect and starts to focus on the core aspects of the sentence. On the other hand, the sentence which only has one embedding encodes some tail aspects about the sentence. This hurts the cosine similarity score between them. Including a few generations on both sentences performs significantly better i.e. the (8, 24) combinations perform much better than (0,32) ones. Overall, equally splitting the generation compute between the two sentences performs best among the variants."}, {"title": "4.7 Sensitivity to Embedder Prompts", "content": "Changing prompts results in high variance for training free methods (Lei et al., 2024). In this subsection, we asses the variance cause by using different prompts to the performance of training free embed-"}, {"title": "4.8 Sensitivity to Generator Prompts", "content": "The prompts tried in the \u00a73.2 were few shot prompts emphasizing that the core meaning needs to be retained. In this subsection, we try to understand the sensitivity of such generator prompts. We vary both the generator and the prompt to understand this. In the first experiment in Table 7, we use the exact same prompts and change the generator: we use Llama-I-3.1 (Meta-Llama-3.1-8B). Although significantly better than the baseline from Table 2, this result lags behind using Mistral0.1-1-7B as the generator.\nFor the second experiment in Table 7, we merge the four few shot prompts and form a single prompt (without any demonstrations) and sample multiple times from it. As expected this drastically reduces performance with Mistral0.1-I-7B due to low quality transformations. We perform a small quali-"}, {"title": "4.9 More MTEB Datasets", "content": "The STS tasks analysed so far are all based on sentence similarity, which is the main focus of this paper. In this subsection, we asses the performance of our method on other tasks from MTEB benchmark (Muennighoff et al., 2022). As our method is slightly expensive, we specifically pick tasks that are tractable for our method. We skip tasks that contain large number of datapoints (E.g. retrieval tasks have millions of documents). Details on specific tasks reviewed in each category are in \u00a7A.3.\nKEEOL prompt was designed keeping the STS sentences in mind. It might not be suitable for other tasks like clustering medical paper titles. Hence,"}, {"title": "5 Conclusion", "content": "We introduce GenEOL, an effective method for leveraging the generative capabilities of large language models (including black-box models) to enhance sentence embeddings. Through extensive experiments and ablations, we show that aggregating embeddings of diverse meaning retaining transformations can significantly improve sentence embeddings. Even with very small number of transformed sentences i.e. (m = 2), GenEOL beats all baselines. GenEOL stabilizes representational quality across LLM layers and is robust to perturbations of embedding prompt. Despite gains on diverse MTEB tasks, GenEOL falls short on classification tasks requiring identification of specific aspects about original sentence, such as emotion, subjectivity, or intent."}, {"title": "6 Limitations", "content": "The main limitation of GenEOL is the cost associated with generating multiple transformations and then embedding them.\nGenEOL is generally computationally expensive, since we need to generate between 2 and 32 transformations of a given sentence. However, we note that this is all inference time compute which only requires forward passes. Such inference time compute methods can be more effectively parallelized compared to methods which require a high training compute (backward passes) (Anil et al., 2018; McCandlish et al., 2018; Thirukovalluru et al., 2024a).\nRecently, larger models like GPT-40 (Achiam et al., 2023) and LLMonkeys (Brown et al., 2024) have leveraged extra inference time compute to significantly improve reasoning tasks. Our work, GenEOL, is the first such effort to use inference time compute to improve text embeddings.\nMoreover, techniques such as speculative decoding (Li et al., 2024) and inference engines such as vllm (Kwon et al., 2023) have made great strides in reducing the cost and latency of sampling from Large Language Models. These methods can be used to make GenEOL more efficient."}, {"title": "Broader Impact and Discussion of Ethics:", "content": "While our model is not tied to any specific applications, it could be used in sensitive contexts such as health-care, etc. Any work using our method is requested to undertake extensive quality-assurance and robustness testing before applying in their setting. To the best of our knowledge, the datasets used in our work do not contain any sensitive information."}]}