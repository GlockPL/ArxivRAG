{"title": "A Comparative Study on Automatic Coding of Medical Letters with Explainability", "authors": ["Jamie Glen", "Lifeng Han", "Paul Rayson", "Goran Nenadic"], "abstract": "This study aims to explore the implementation of Natural Language Processing (NLP) and machine learning (ML) techniques to automate the coding of medical letters with visualised explainability and light-weighted local computer settings. Currently in clinical settings, coding is a manual process that involves assigning codes to each condition, procedure, and medication in a patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There are preliminary research on automatic coding in this field using state-of-the-art ML models; however, due to the complexity and size of the models, the real-world deployment is not achieved. To further facilitate the possibility of automatic coding practice, we explore some solutions in a local computer setting; in addition, we explore the function of explainability for transparency of AI models. We used the publicly available MIMIC-III database and the HAN/HLAN network models for ICD code prediction purposes. We also experimented with the mapping between ICD and SNOMED CT knowledge bases. In our experiments, the models provided useful information for 97.98% of codes. The result of this investigation can shed some light on implementing automatic clinical coding in practice, such as in hospital settings, on the local computers used by clinicians.", "sections": [{"title": "1 Introduction", "content": "The coding of medical letters is currently something that is completed manually in advanced healthcare systems such as the UK and the US 1. It involves professionals reviewing the paperwork for a patient's hospital visit or appointment and assigning specific codes to the conditions, diseases, procedures, and medications in the letters. This study aims to examine the potential automation of this process using Natural Language Processing (NLP) and Machine-Learning (ML) techniques, to create a prototype that could be used alongside the coders to speed up the coding process and to explore if such a system could be integrated into the real practice.\nClinical codes are used to remove ambiguity in the language of the letters, provide easily generated statistics, give a standardised way to represent medical concepts and allow the NHS's Electronic Health Record (EHR) system to process and store the codes more easily (NHS-Digital, 2023). Also, in the case of private healthcare providers, coding can make it easier to keep track of billing 2. To do this, the coder takes a medical letter as input, which can be anything from a prescription request to a hospital discharge summary, and outputs potential codes from a designated terminology and/or classification system. The NHS \u2018fundamental information standard' is the \"Systemised Nomenclature of Medicine - Clinical Terms\u201d (aka SNOMED-CT) terminology system, which uses 'concepts' to represent clinical thoughts. Each concept is paired with a 'Concept Id' a unique numerical identifier e.g., 56265001 heart disease (disorder) - which is then arranged by relationships into hierarchies from the general to the more detailed (NHS-Digital, 2023). It is worth noting that SNOMED is not the only system used for coding. The other system relevant to this work is the International Classification of Diseases (ICD), specifically ICD-9 3. This was the official system used to code diagnoses and procedures in the US. While SNOMED is a terminology system that has a comprehensive scope, covering every illness, event, symptom, procedure, test, organism, substance, and medicine, ICD is a classification system with a scope of just classifying diagnoses and procedures. In the NHS UK,"}, {"title": "2 Backgrounds and Related Work", "content": "The background session will be presented in two sections. The first section, pre-neural networks, will focus on the early attempts at automated medical coding, how they worked, and the reasons why none of them were implemented in the real world. The second section, the introduction of neural networks, will follow the development from recurrent neural networks to transformer-based attention networks. We will explore the methodology and results of each one and conclude with the platform on which the chosen model is based."}, {"title": "2.1 Pre-Neural Methods", "content": "Most papers regarding general healthcare NLP can be divided into two topics: text classification and information extraction (Dong et al., 2022). Classification can be split into three versions, each getting more complex: binary classification, where an instance is in one of two distinct categories (e.g., smoker or non-smoker); multi-class classification, where there are multiple categories, but an instance can still only be assigned to one class (e.g., current smoker, former smoker, non-smoker); and multi-label text classification 6. This involves instances that can be associated with several different labels/categories simultaneously, such as discharge letters, in which each letter always contains multiple conditions. Automated medical coding is often identified as a multi-label text classification problem; however, some older attempts still utilise information extraction or a combination of methods from both topics. The first attempts at automated clinical coding were from around 1970, such as this 1973 study by Dinwoodie and Howell (1973) that utilises a 'fruit machine' methodology. This entails representing each significant word of a diagnosis with an associated code number and, like a fruit machine in a pub, the code is correct when a common code number appears for all words in the diagnosis. While this study returns impressive results with a correct coding rate of over 95%, this is only done with a small collection of pre-coded morbidity data from 16 doctors around Scotland. Thus, the project will not scale up to the complex real-world scenario.\nNo real progress was then made for the next few decades. A 2010 literature review on clinical coding"}, {"title": "2.2 Neural Networks and Attentions", "content": "The general approach of deep learning in neural networks aims to map a complex function learned through the training data to match the information in the text to an appropriate set of medical codes (Dong et al., 2022). Before any deep learning is completed, the common first step in these projects aside from preprocessing - is to produce word embeddings for each token. Each embedding is a semantically meaningful mathematical representation, usually a vector, of the token designed so that tokens with similar meanings have similar vectors (Percha, 2021). To compare the meaning of two words, one calculates the cosine similarity of their corresponding vectors. The most common method for doing this is 'word2vec', which operates on the assumption that words with similar meanings tend to occur in similar contexts. It uses either a continuous bag of words (CBOW) model that predicts the target words based on the context words (words surrounding the target word) or a skip-gram that predicts the context words based on the target words (Mikolov et al., 2013), both of which are examples of single-layer neural networks. A more advanced version of word2vec that strays from the standard embedding practice of one vector per word/token/document, is the development of bidirectional encoder representations from transformers (BERT) (Devlin et al., 2019). These are massive pre-trained language models that are too resource-intensive to be trained from scratch in most circumstances, however, models trained on a general corpus can be fine-tuned to meet specific needs (such as clinical text mining through transfer learning (Peng et al., 2019)). Unfortunately, due to their size and complexity, they are not currently feasible to be trained on larger datasets without significant modification. The first successful deep learning attempts utilised recurrent neural networks (RNNs), with a focus on two specific types: Gated Recurrent Units (GRUs) and Long Short-Term Memory Networks (LSTMs). The project (Nigam, 2016) constructs an RNN with a single layer consisting of 20 time steps; with each time step, a normalised vector representing a patient note is submitted in a time sequential order (oldest to most recent). The activation (threshold) function is tanh, a mathematical operation applied to the weighted sum of inputs and biases in each neuron that introduces non-linearity into the network. There is a dropout rate of 0.1 that is applied to prevent overfitting during training, and a learning rate of 0.001 is used to determine how much the weights of the network are updated during each training iteration. Finally, the model uses cross-entropy loss as its sigmoid function, normalising the neuron's output to a value between 0 and 1.\nGRUs are implemented as recurrent units, where each unit contains a reset gate and an update gate,"}, {"title": "2.3 The MIMIC-III Dataset", "content": "In Clinical NLP, the first resource is the MIMIC-III dataset, which is the only publicly available mainstream English dataset with enough data to perform proper training. Additionally, most models that attempt to solve the automatic coding problem use this dataset.\nMIMIC-III (Johnson et al., 2016) is a large, freely available database comprising de-identified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Centre between 2001 \u2013 2012 7. The database is freely available to researchers worldwide, provided they have become a credentialed user of PhysioNet (Johnson et al., 2016) and completed the required 'Data or Specimens Only Research' CITI training 8 (Or another recognized course in protecting human research participants that includes HIPAA requirements). All data in the MIMIC database has been deidentified per HIPAA (Health Insurance Portability and Accountability Act) standards. This ensures that all 18 listed identifying data elements, such as names, telephone numbers, and addresses, are removed. The only thing not removed are dates, which are shifted in a random but consistent manner to preserve intervals. Therefore, all dates occur between 2100-2200, but the time of day, day of the week, and approximate seasonality have been conserved. \nMIMIC is a relational database consisting of 26 tables containing different forms of data, from the patient's clinical notes in NOTEEVENTS to extremely granular data such as the hourly documentation of patients' heart rates. This makes it a vast and complex database to work with - however since we are only using the database for its clinical notes, only five tables are required:\n\u2022 NOTEEVENTS \u2013 Deidentified notes, including nursing and physician notes, ECG reports, imaging reports, and discharge summaries.\n\u2022 DIAGNOSES_ICD - Hospital-assigned diagnoses, coded using the International Statistical Classification of Diseases and Related Health Problems (ICD) system.\n\u2022 PROCEDURES_ICD - Patient procedures, coded using the International Statistical Classification of Diseases and Related Health Problems (ICD) system.\n\u2022 D_ICD_DIAGNOSES - Dictionary of International Statistical Classification of Diseases and Related Health Problems (ICD) codes relating to diagnoses.\n\u2022 D_ICD_PROCEDURES - Dictionary of International Statistical Classification of Diseases and Related Health Problems (ICD) codes relating to procedures.\nThis still leaves a lot of unnecessary data. For example, the NOTEEVENTS table contains CHARTTIME, CHARTDATE, and STORETIME, which are the time and date a note was charted and the time it was stored in the system. The notes in NOTEEVENTS vary in usefulness and format, with the type of note indicated in the DESCRIPTION column. Since all the medical coding projects that use MIMIC unanimously choose to use the discharge summaries as they contain the most potential codes per letter (15.9 labels per document). We removed all the other types of notes. This was done by creating a new table that copied each line as long as the DESCRIPTION = \u2018Discharge Summary'. The next step is to combine the data in separate tables into one table for easier access.\nAnother note on MIMIC is about its most popular subset, MIMIC-III-50, that contains only the notes and codes of the top 50 most frequently occurring codes (Table 1). First occurring in CAML (Mullenbach et al., 2018), MIMIIC-III-50 is often used as a proof-of-concept database for automatic medical coding projects due to it being significantly smaller (8,067 documents compared to 47,724) and with fewer labels (5.7 compared to 15.9 for MIMIC full), which means it takes less time and computational resources to train against. Projects like HiLAT (Liu et al., 2022) that face challenges in accessing the necessary computing power for training their models have utilised the MIMIC-III-50 dataset to train on and achieve state-of-the-art results. The only issue with using MIMIC-III-50 is that, as Figure 2 demonstrates, it doesn't give the same opportunity to test models against a long tail distribution.\nA database that follows a long tail distribution is one where there are many data points that are not well-represented, and the majority of occurrences are concentrated around a few values at the \"head\" of the distribution (Zhang et al., 2023). This accurately describes the MIMIC-III-Full database, where the top 105 codes make up 50% of the total labels in the set, and there are 3,110 labels that have fewer than 5 examples (Nigam, 2016), with 203 codes not appearing in any discharge summaries at all. Solving the long tail distribution of MIMIC is one of the key challenges that will need to be addressed by the potential models to be deployed."}, {"title": "3 Model Selections", "content": "We have selected three potential models and in this section each model will be evaluated, reviewing their results, methodology, and suitability for the study's needs, concluding with the chosen model."}, {"title": "3.1 Problem Formalisation", "content": "Before each selected model is evaluated, the problem needs to be formally defined. Taking X as the collection of clinical notes and Y as the full set of labels (ICD-9 codes). Each instance $x_d \\in X$ is a word sequence of a document, d, and is associated with label set $y_d \\subseteq Y$, where each $y_d$ can be represented as a |Y| multi hot vector (a vector where multiple elements can have a value of 1, indicating multiple features/categories are present at the same time), $Yd = [yd\u0131, Yd2, ..., Yd|y|]$, and yd\u0131 \u2208 (0,1) where l indicates the l'th label has been used for the dth instance and 0 indicates irrelevance (Perotte et al., 2014). From this, the task of the models is to learn a complex function $f : X \u2192 Y$ from the training set.\nAll the chosen models use the same loss function, binary cross-entropy, and optimise it with L2 normalisation using the Adam (Adaptive Movement Estimation) optimiser (Kingma and Ba, 2014). Loss functions are used in neural networks as a measure of how well the networks predictions match the true values of the training data, with binary cross entropy loss measuring the dissimilarity between the true binary labels and the predicted probability of the model. In the context of these models, L2 normalisation is used to avoid overfitting, which occurs when the model is trained so well on a particular dataset that it fails to generalise well to new, unseen data. To prevent this, penalty terms proportional to the magnitude of the vectors"}, {"title": "3.2 Model-1: Convolutional Attention for Multi-Label Classification (CAML)", "content": "CAML (Mullenbach et al., 2018), as already mentioned in the background section, utilises a CNN based architecture but swaps the traditional pooling layer for an attention mechanism. The model starts by horizontally concatenating pretrained word embeddings into a matrix, X. A sliding window approach as is standard in CNNs is then applied to this matrix that computes an equation on each section of the matrix, resulting in the matrix H.\nNext, the model applies a per-label attention mechanism. For each label, l, the matrix vector product is computed, and the result of this is passed through a SoftMax operator that essentially reduces the input values to the range [0,1] while ensuring that they sum up to 1 so they can be used as probabilities. This SoftMax operator returns the distribution over locations in the document in the form of attention vector a. This attention vector is then used to compute vector representations for each label, vl. Finally, a probability is computed for label l using another linear layer and sigmoid transformation to obtain the final label predictions yl. This normalisation process ensures that the probability of the label is normalised independently rather than normalising the probability distribution over all labels like the SoftMax operator does."}, {"title": "3.3 Model-2: Hierarchical Label Attention Network (HLAN)", "content": "The HLAN model (Dong et al., 2021) is built around providing explainability for its results, and consists of an embedding layer, the HLAN layers, and a prediction layer. The embedding layer converts each token in the sentence into a continuous vector where the word embedding algorithm word2vec returns the vector of word embeddings $Xdi$.\nThe HLAN makes extended use of Gated Recurrent Units (GRU) to capture long-term dependencies. The GRU unit processes tokens one by one, generating a new hidden state for each token. At each hidden state, the GRU considers the previous tokens using a reset gate and an update gate. The GRU method implemented is known as Bi-GRU because it reads the sequence both forwards and backwards, concatenating the states at each step, to create a more complete representation.\nThe label wise word-level attention mechanism, which contains a context matrix (Vw) where each row Vul, is the context vector to the corresponding label yr. The attention score is calculated as a SoftMax function of the dot product similarity between the vector representation of the hidden layers from the Bi-GRU and the context vector for the same label. The sentence representation matrix Cs is computed as the weighted average of all hidden state vectors h\u00b2 for the label y\u00b2.\nThe label-wise sentence-level attention mechanism is computed in much the same way, outputting sentence-level attention scores and the document representation matrix Cd. The prediction layer then utilises a label-wise, dot product projection with logistic sigmoid activation to model the probabilities of each label to each document. Finally, the binary cross entropy loss function is optimised with L2 normalisation and the Adam optimiser.\nThe HLAN has an extra label embedding initialisation (denoted as +LE) that can be implemented in place of the normal embedding layer and functions by leveraging the complex semantic relations (how different elements are related to each other in terms of their meanings) among the ICD codes. The embedding works off for two correlated labels; one would expect the prediction of one label to impact the other for some notes, which is represented as giving each label representation corresponding weights. The HLAN model was based on the HAN"}, {"title": "3.4 Model-3: Multi-Hop Label-wise Attention (MHLAT)", "content": "Much like HLAN, MHLAT (Duan et al., 2023) is comprised of three main components: an input/encoder layer, MHLAT layer, and a decoder layer (Figure 6). It also utilises the same label-wise attention mechanism, however, that is where the similarities end. In the encoding layer, MHLAT first splits the text into chunks with 512 tokens per chunk. It then adopts the general domain pre-trained XLNet (Yang et al., 2019) (similar to BERT but less computationally expensive), which is further trained on MIMIC, and then applied to every chunk. Each chunk from the text is then concatenated to form a global vector of the input text, H.\nWhile using label-wise attention through multiple passes is utilised for both HLAN and MHLAT, where HLAN uses multiple Bi-GRUs increasing the scope each time, MHLAT presents a 'multi-hop' approach. Initially, the label-wise attention is derived from matrices of the tokens of the input sentence from the encoder, followed by a 'fusion' operation that combines label-specific representations and label embeddings. A hop function is then defined that iteratively updates context information and label embeddings, which is then repeated. The decoding layer implements an independent linear layer for computing the label score and utilises the same binary cross entropy loss function as the other models."}, {"title": "3.5 Model summaries", "content": "If going purely off results (given in Figure 7), the MHLAT model returns state-of-the-art performance compared to the others in every metric it had resulted in. However, it is worth noting that the model, despite being attention-based, did not factor any type of explainability into itself. As mentioned in the motivations, we want to explore some level of interpretability of coding models, otherwise, the professionals (clinicians) using them would have no way to verify the results and build trust.\nLooking at the results of the remaining models, it is clear that HLAN performs better than CAML, which in turn performs better than HAN. However, the objective of the project was to prioritise explainability in the results, which made HLAN/HAN the ideal model as despite a slight reduction in performance for the MIMIC Full dataset. The enhanced interpretability in its answers justifies its use, especially in domains such as medical coding where transparency and understanding of the models' decisions are crucial."}, {"title": "4 Coding with Explainability", "content": "The goal of this study is to develop a program that could attempt to fulfill the investigation aims, that being to produce SNOMED codes and visualisation, and could then be utilised to evaluate a comparable system being implemented in the real setting, such as NHS UK. The program was implemented in Python 3.8 using the TensorFlow framework and leverages the HAN model to predict ICD codes, converts these codes to SNOMED, and provides visualised attention scores for each document."}, {"title": "4.1 data processing and ICD coding", "content": "The preprocessing (Figure 8) takes three of the tables from MIMIC described in Section 2.3, NOTEEVENTS, PROCEDURES_ICD, and DIAGNOSES_ICD, and combines them into one table, notes_labeled, with the schema SUBJECT_ID, HADM_ID, TEXT, LABELS where:\n\u2022 SUBJECT_ID \u2013 identifier unique to a patient, found in NOTEEVENTS.\n\u2022 HADM_ID \u2013 identifier unique to a hospital stay, found in NOTEEVENTS.\n\u2022 TEXT - The free text of the document. There can be multiple documents with the same HADM_ID. Found in NOTEEVENTS.\n\u2022 LABLES \u2013 ICD_9 labels professionally assigned and stored in sequence order in either DIAGNOSES_ICD or PROCEDURES_ICD, depending on if they were diagnoses or procedures.\nThis is accomplished by first concatenating both ICD tables into one table of codes, ALL_CODES."}, {"title": "4.2 Entity Linking to SNOMED", "content": "Now with a working model, the next step is to map the ICD codes to SNOMED (Figure 9). The map was originally created for the Unified Medical Language System (UMLS) to facilitate the translation of legacy data still coded in ICD-9 to SNOMED CT codes. Therefore, it is perfect for the project's needs. It does contain multiple columns of data that are not required, mainly usage statistics, however, these can just be ignored. The 202212 most recent release of the map was implemented by UMLS and is split up into two tab-delimited value files with the same file structure; one for one-to-one mappings, and one for one-to-many mappings. The one-to-one mapping contains 7,596 mappings (64.1% of ICD-9 codes), with each line in the file being a separate mapping. For example, the ICD code 427.31 (Atrial Fibrillation) maps directly to the SNOMED code 49436004 (Atrial Fibrillation (disorder)). The one-to-many file contains 3,495 mappings (29.5% of ICD-9 codes), with the mapping being one ICD code to multiple SNOMED codes. The file is set out as one-to-one maps, with the one ICD code being repeated for each of the many SNOMED codes, for example:\n\u2022 719.46 \u2013 Pain in joint, lower leg | 202489000 Tibiofibular joint pain\n\u2022 719.46 \u2013 Pain in joint, lower leg | 239733006 - Anterior knee pain\n\u2022 719.46 \u2013 Pain in joint, lower leg | 299372009 \u2013 Tenderness of knee joint\nThis was implemented by first loading the one-to-one map into a dictionary, then iterating through the predicted_codes list. At each iteration (new ICD code) the program checks to see if the ICD code is in the one-to-one map. If it is, the associated SNOMED code and FSN (fully specified name) are outputted; if not, the one-to-many map is loaded as a dictionary.\nThe program searches for the ICD code in the one-to-many dictionary, and if found, it outputs all the SNOMED codes related to the ICD code. This is done so that even if the program cannot find a direct mapping, it can at least provide the user with potential options. If an ICD code cannot be found in any mappings, the system will print the ICD code description from either D_ICD_DIAGNOSES or D_ICD_PROCEDURES. There are only a few cases, approximately 6.4% of the ICD codes, where there are no mappings available. This usually occurs with catch-all NEC (not elsewhere classified) ICD codes, such as 480.8 - Pneumonia due to other virus not elsewhere classified, for which SNOMED has no alternative mappings available.\nAfter all these steps, the project now takes notes as input through a text document, processes them using the HAN model, and calculates the attention levels of the ICD codes. The program then converts the ICD codes into SNOMED codes with as many 1-to-1 mappings as it can find, outputting that to the console (Figure 10). Finally, the attention visualisation is exported into Excel (Figure 11) which shows each word in the file and highlights it in a shade of blue. The deeper the blue highlight, the greater the weight that word had when calculating the ICD codes. The visualisation displayed in Figure 11 is split up halfway down for ease of viewing. In reality, the left-hand side of the upper picture and the right-hand side of the lower picture are joined next to each other."}, {"title": "4.3 Evaluations Setups", "content": "The experiments are evaluated in two ways \u2013 first, the model is tested against the standard testing scores of micro/macro F1 and precision. Second, the implementation of SNOMED mapping is also considered, calculating the percentage of codes it can predict/give options for.\nTo accurately test the model, data had to be gathered by running the model against MIMIC discharge summaries from the test files. This was accomplished by randomly selecting 100 notes from the test_full file (refer to sample size and model confidence by Gladkoff et al. (2022)). We then ran each set of notes through the model and put it through a program that returned the true and false positives, as well as the false negatives from the results by comparing the labels generated by the model to the true labels in the file, where:\n\u2022 True Positives \u2013 when the model predicts a label, and it is correct.\n\u2022 False Positives \u2013 when the model predicts a label, but it is incorrect.\n\u2022 False Negatives \u2013 when the model doesn't predict a label even though there is a correct label.\nNow that these values were generated, the model was tested against the same metrics that have been used in all the models previously.\n\u2022 Recall - measures how often a model correctly identifies positive instances (true positives) from all the actual positive samples in the dataset 10 and is calculated by dividing the number of true positives by the number of positive instances (true positives + false negatives).\n\u2022 Precision measures how often a model correctly predicts the positive class, calculated by dividing the number of correct positive predictions (true positives) by the total number of instances the model picked as positive (both true and false positives). The precision results from earlier models were with P@5, P@8, or P@15, which means measuring the proportion of relevant items within the top 5, 8, or 15 items retrieved by the system.\n\u2022 F1 Score - Calculated as the harmonic mean of the precision and recall scores, therefore, encouraging similar values for both precision and recall. The more the precision and recall deviate from each other, the worse the score.\n\u2022 Macro F1 score is an average of the F1 scores obtained, representing the average performance of the model across all classes (each class having the same weight).\n\u2022 Micro F1 score - computes a global average F1 score by counting the sums of the true positives, false negatives, and false positives and then putting those into the normal F1 equation."}, {"title": "4.4 Evaluation Results", "content": "4.4.1 ICD Coding Evaluation\nFor ICD coding evaluations, the first 20 documents tested were listed in Figure 12, with the full list in Appendix.\nThe combined results of all the tests (Table 2) were then calculated, returning the macro F1 as 0.041 (compared to 0.036 from previous HAN tests) and the micro F1 as 0.403 (compared to 0.407 from previous HAN tests). The similarity to the previous results demonstrates that the model was functioning as intended, so although the results weren't state of the art, they were what was expected. The same can be said for precision, which We calculated using the first 15 values returned, otherwise known as P@15 (the same as previous tests), to get a precision of 0.599 (compared to 0.613).\nWhile these results aren't the same as the previous HAN model testing, this is to be expected as only 100 documents were tested. This means that if there were outliers, they had a greater effect on the overall results, and the more documents that were tested, the closer to the actual values the results will become.\n4.4.2 SNOMED Mapping Evaluation\nRegarding the SNOMED mapping, from the individual results (shown in Figure 12), each row was summed, with 100 subtracted from the No DESC value to ensure that the error of the program producing a No DESC result at the end of each document was not considered in the total. From this, a 1-to-1 map is displayed 52.91% of the time, and a 1-to-many map is displayed 13.88%, which means the program successfully mapped to SNOMED on 66.79% of attempts.\nThe unexpected result in this situation is the significant amount of \u2018no maps' returned. This is due to differing versions of ICD-9 codes utilised, as MIMIC uses the standard ICD-9 coding, but the mapping uses ICD-9-CM, the clinical modification used for morbidity coding. This means that there will be codes in one version that are not featured in the other, and unfortunately, there is not much that can be done to resolve this aside from creating a new mapping.\nEven when returning a \u2018no map', the program still returns the description of the ICD code which is useful information for the user. Therefore, this implementation returns a useful response for 97.98% of attempted codes."}, {"title": "5 Conclusions and Future Work", "content": "This study aimed to compare existing coding methods and produce a model that automatically assigns labels to medical texts and gives an explainable out- come, to explore how this investigation can be implemented in real practice, e.g. NHS UK. High ethical standards were maintained during the project considering the field of study. As outcomes, the model does automatically assign labels to the medical texts utilising a pre-trained HAN model that emphasises interpretability in its outcomes, producing a document explaining how it reached its decisions. The project also explores the potential of integrating a similar system into a real setting, utilising mappings to SNOMED as well as having a medical professional give feedback throughout the development of the system and evaluate the results of the final program (Appendix for human evaluations).\nRegarding future works specifically for real applications, we believe that for a project like this to be viable, a new dataset needs to be created that more accurately represents the data the model is going to come across. Using discharge summaries from MIMIC to train the model and then expecting it to perform on completely different data is infeasible; no matter how complex the model is and how good it gets at zero-shot learning, etc., it will only ever be good at modelling data that is similar to the data it's trained against. Making a new database would also eliminate the need to map between coding standards, as making a new database specifically for use cases, e.g. NHS UK, means it can be mapped to SNOMED by default. Another direction is that we can deploy some SOTA medication and treatment extraction tools for richer annotation of clinical data, such as recent work by Belkadi et al. (2023); Tu et al. (2023).\nFrom a more general perspective, automated medical coding as a problem seems to be advancing towards transformer-based solutions in both the full modelling like MHLAT and word embeddings with BERT. This technology shows definite promise with its results against MIMIC-III-50, with its only limit being the computational feasibility of training such a complex model."}, {"title": "Limitations", "content": "After our first meeting, the external stakeholder created a simplified mock-up of the NHS Electronic Health Record (EHR) system to store patient information 11. The system integrated the SNOMED codes into the EHR utilizing the SNOMED terminology service Hermes 12. Since one of the objectives of the project was to demonstrate how it could be implemented into the wider NHS system, and creating a mock-up of the EHR was deemed as a good starting point.\nUnfortunately, there were issues getting Hermes (more specifically the Hermes docker file) to function on a Windows PC, but these issues did not persist on the university virtual machines (VM), therefore the project was moved on to the Linux-based VMs. Doing this had its own problems, as we no longer had permissions to 'sudo install' any of the Python libraries required to run Hermes. To solve this, a custom text-based VM had to be created with all the permissions needed to run Hermes. There were access problems regarding this VM with incorrect SSH keys, but once this was fixed a Hermes terminology server was successfully set up on the VM.\nGaining access to MIMIC-III required the completion of two CITI training modules; Data and Specimens only research, and Conflicts of Interest (Both in Appendix). After this, our PyhsioNet account (PhysioNet is a repository of medical data, and where MIMIC is available to download) became credentialed and, therefore, gained access to the full MIMIC dataset.\nUnfortunately, the custom VM did not have enough space for the full MIMIC dataset. Therefore, the dataset had to be downloaded onto our personal Windows PC without the working Hermes server and restart the project from there. From here preprocessing could begin to make MIMIC and the HLAN compatible."}, {"title": "Appendix", "content": "A Study Context\nThis paper explores the potential of replacing the time-consuming process of manually coding letters with a program that automatically assigns codes to letters. For the program to be of any value to its intended users, the external stakeholder (who is a local GP and has an interest in programming) stated that the output should be explainable. This would allow the users to verify the results if unsure and increase the trust between them and the system. The stakeholder also stated that ideally the system would be easily implemented into the wider NHS systems, so the system can store and link the codes and letters to the patients they are about. This would allow the program to utilise previous letters about the patient to aid with the coding.\nDue to the program being oriented around the inherently personal topic of healthcare, ethics approval to gain access to the resources required would always be important. we had to gain access to MIMIC-III (Medical Information Mart for Intensive Care) which is a free database comprised of deidentified healthcare data, as well as the UK and US versions of SNOMED-CT and access to the UMLS ICD-9 to SNOMED-CT maps from the NIH. The MIMIC database had to be pre-processed to train the HLAN (Hierarchical Label Attention Network"}]}