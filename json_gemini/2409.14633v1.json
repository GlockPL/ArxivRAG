{"title": "Hierarchical end-to-end autonomous navigation through few-shot waypoint detection", "authors": ["Amin Ghafourian", "Zhongying CuiZhu", "Debo Shi", "Ian Chuang", "Francois Charette", "Rithik Sachdeva", "Iman Soltani"], "abstract": "Human navigation is facilitated through the association of actions with landmarks, tapping into our ability to recognize salient features in our environment. Consequently, navigational instructions for humans can be extremely concise, such as short verbal descriptions, indicating a small memory requirement and no reliance on complex and overly accurate navigation tools. Conversely, current autonomous navigation schemes rely on accurate positioning devices and algorithms as well as extensive streams of sensory data collected from the environment. Inspired by this human capability and motivated by the associated technological gap, in this work we propose a hierarchical end-to-end meta-learning scheme that enables a mobile robot to navigate in a previously unknown environment upon presentation of only a few sample images of a set of landmarks along with their corresponding high-level navigation actions. This dramatically simplifies the wayfinding process and enables easy adoption to new environments. For few-shot waypoint detection, we implement a metric-based few-shot learning technique through distribution embedding. Waypoint detection triggers the multi-task low-level maneuver controller module to execute the corresponding high-level navigation action. We demonstrate the effectiveness of the scheme using a small-scale autonomous vehicle on novel indoor navigation tasks in several previously unseen environments.", "sections": [{"title": "I. INTRODUCTION", "content": "ACCURATE positioning, such as through visual or LiDAR SLAM or GPS, is crucial for mobile robotics, but often comes with high computational and hardware costs [1], [2].\nThis paper introduces a novel approach to autonomous vehicle navigation, inspired by human-like navigation using visual landmarks and simple instructions. Our system, termed Description-based Navigation System (DNS), simplifies navigation by associating limited visual data of key waypoints with high-level navigation actions (e.g., turning instructions). DNS minimizes reliance on complex localization sensors, leveraging a few-shot learning technique for waypoint detection. This approach is practical given the public accessibility of visual data from sources like Google Street View.\nOur contributions include: 1) A hierarchical end-to-end navigation system separating low-level maneuvering from high-level navigation; 2) An efficient few-shot learning method for robust waypoint detection; 3) Demonstration through experiments of how well the waypoint detection integrates with conditional maneuver control in various settings.\nThe paper hereafter is organized as follows: in Section II we review some of the prominent related literature. Section III discusses the proposed description-based modular navigation, and in Section IV the proposed few-shot technique and its deployment in the context of DNS is explained. In Section V we demonstrate the performance results for offline as well as real-time implementation of DNS and present the associated ablation study. Section VI concludes the paper and presents"}, {"title": "II. RELATED WORKS", "content": "In recent years, localization, path planning, and motion control for autonomous navigation have been active areas of research. Various sensors including cameras, GNSS (Global Navigation Satellite System) receivers, IMU (Inertial Measurement Unit) devices, visual and thermal cameras, and LiDAR sensors have been adopted to develop autonomous functions using various Al technologies as well as classical control paradigms. In this section, we review some of the recent works in this domain.\n[3] and [4] propose CNN-based outdoor and indoor localization techniques based on landmark detection and image classification. Both methods importantly require training on labeled data from the target environment. Further, the proposed technique in [3] requires geolocation labels including coordinates and compass orientation, as well as bounding box annotations. In [5], GNNS/INS (Inertial Navigation System) is used for highway localization, which is further refined against detected signs and road facilities. In [6], [7], the need for HD prior maps for localization and subsequent planning is relaxed by using Standard Definition (SD) maps paired with onboard visual and LiDAR perception for inferring the HD map online. Further, precise localization is obtained using a Localizing Ground Penetrating Radar (LGPR) to retrieve stable underground features that are robust to weather changes [6]. [8] combines OpenStreetMap road network with GPS and IMU signals, as well as local perception information obtained by 3D-LiDAR and CCD camera sensor fusion to refine the global localization, as well as local path planning and control of the robot.\nFor local planning and motion control, [9] uses semantic line detection and segmentation to identify traversable lanes for agricultural robots and vehicles, tailoring their design to the order and configuration of the scene in that application. [10] also uses image segmentation paired with topological maps for road following and control. They propose a rule-based heuristic design for explicitly controlling the robot toward a target point obtained through extrapolating and intersecting the road boundary lines and accounting for obstacles and drivable areas. [11] emphasizes performance in seen environments and route repeating capability through teach and repeat that utilizes odometry information and generates correction signals through lightweight processing of the visual input. For improving autonomous motion control, [12] proposes incorporating prediction of road drivers' intentions through hidden Markov models. Some techniques are also specifically designed to perform specialized maneuvers such as lane change [13] and overtaking [14].\nImitation learning [15], [16] and reinforcement learning [17]-[21] have also been attractive domains in mobile robotics research. [15] generates a BEV representation that also denotes the desired route given camera input and high-level navigation action. The navigation action is obtained using GPS route planning. The capability for conditional BEV generation is gained through adversarial training. Given the BEV prediction and the current state of the vehicle, control policy is learned from expert demonstrations. Contrary to our method, BEV generation is an essential facet of this work, without which the tests are shown to fail. In [16], the authors demonstrate how by utilizing Imitation from Observation (IfO) a robot can learn a good navigation policy for a route using the demonstrator's ego-centric video despite viewpoint mismatch. The policy can generalize to unseen environments if a recording is provided. In our work, we primarily teach the route to the robot by presenting only minimal waypoints en route along with their corresponding high-level navigation actions without particularly focusing on the detailed low-level control. In VOILA, however, the main focus is the imitation quality and learning a policy that adheres to the demonstrated path from a differing platform at a low level without emphasizing waypoint detection and executing correct high-level navigation actions in particular.\nThe past few years have been a fertile period for high-performing vision models that are especially suited for use as backbone or in applications involving image retrieval. Notably, contrastive and non-contrastive joint-embedding self-supervised learning (SSL) techniques [22]\u2013[26] have provided effective means of leveraging unlabeled data to train robust models for various downstream applications. In these methods, the aim is predominantly to make representations robust against changes to the input data that do not change its semantics (e.g. common visual transforms). With the increased availability of labels on large datasets, supervised counterparts such as [27] trained on a similar objective but with real labels added to the surrogate labels have also emerged. An especially attractive model for universal visual place recognition was presented in [28], which aggregates per-pixel features from self-supervised pretrained vision transformers (ViT) [29], [30]. Such a model can facilitate image retrieval and localization for robot navigation.\nIn the following section, we discuss the Description-based Navigation System (DNS), a control scheme that places special emphasis on data efficiency and quick adaptation to unseen routes without fine-tuning."}, {"title": "III. DESCRIPTION-BASED NAVIGATION SYSTEM", "content": "The proposed process of setting up a mobile robot for an upcoming route consists of the following steps:\n1) Identify a set of waypoints along the route at locations where a new high-level navigation action should be executed, e.g. at an intersection where a vehicle should take a turn.\n2) Assign a discrete navigation action to each waypoint and form an address lookup table (LUT).\n3) Retrieve example images of each waypoint location: In this approach, we are assuming that for each waypoint, one or a few example images are available.\n4) Provide the waypoint representations to the vehicle so that they become identifiable upon future exposure to similar visual cues.\nGiven the limited number of samples available for each landmark as well as the variety of potential routes in many applications, few-shot learning is adopted to train the high-level"}, {"title": "IV. FEW-SHOT LEARNING FOR WAYPOINT DETECTION", "content": "In this section, we describe the formulation of few-shot classification and explain how the proposed few-shot technique is adapted for waypoint detection.\nA. Few-shot classification: problem formulation\nAs part of a few-shot classification training, we are given dataset $\\mathcal{S} = \\{(x_i, y_i)\\}_{i=1}^l$ of labeled samples, where each $y_i \\in \\{1,...,K\\}$ is the label of the corresponding example $x_i$. For training, many $w$-way $s$-shot classification tasks, or episodes, can be randomly formed, where $w$ denotes the total number of participating classes and $s$ denotes the number of labeled examples or shots, commonly referred to as the support set, for each class. The model is then trained by minimizing classification loss on $q$ unlabeled samples (queries). Trained under this scheme, the model learns to rely on a small number of labeled samples (shots) for new tasks with novel participating classes.\nB. Enhanced metric few-shot learning\nIn metric few-shot learning techniques, one aims to find a suitable representation that places instances from the same class close to each other while separating instances from different classes in the embedding space. This will facilitate accurate classification. In Prototypical Networks [31], for instance, this is done by minimizing the Euclidean distance between a query and its true class prototype defined as the mean of the support sample representations for that class, while maximizing its distance from all other class prototypes.\nDeep variational embeddings in the context of few-shot learning have been shown to improve classical metric few-shot learning techniques [32], [33], which can suffer from noise due to data scarcity and lack of interpretability. In these methods, rather than estimating a class mean based on a few sample vector representations, a more expressive class representation is adopted, for instance, by assuming a Gaussian form. In [32], each sample is mapped to a Gaussian distribution of its associated class through a neural architecture, whose output is interpreted as mean and the variances that form a diagonal class covariance matrix. Individual sample outputs from each class are then combined to obtain a more accurate estimate of class mean and covariance.\nSimilarly, we retrieve class distribution mean and diagonal covariance estimates for each sample through a neural network and combine samples to form a more accurate class distribution. For class k, we obtain the combined distribution"}, {"title": "C. DNS with distribution embeddings", "content": "Fig. 5 shows the waypoint detection model. In order to train the proposed technique for use in few-shot navigation, we collect and use a dataset of various courses, with multiple repetitions (or laps) of each. For each lap of each course, the frames are split into segments corresponding to positive or negative examples for each waypoint within that course. Frames at which the high-level navigation action corresponding to the nth waypoint can be safely initiated will be used as positive class examples for that waypoint. All the frames beyond the positive segment of the (n - 1)th waypoint and prior to the nth waypoint's positive segment will be used as the negative class examples for the nth waypoint.\nIn each classification task during training, we construct the support distribution for the positive class (i.e. a given waypoint) as follows: first, we choose a random lap from a random course, then we make a random selection of s consecutive images from a random waypoint in the lap. We then pass each image through the backbone and present to mean and covariance modules to retrieve individual distributions and obtain a combined prototypical estimate of the distribution associated with the waypoint. We then obtain query distributions by sampling s consecutive frames from either the positive or negative classes. $q_p$ positive query distributions are constructed from the positive segment associated with the same waypoint but in different recordings of the path. $q_n$ negative distributions are taken from the corresponding negative segment in either the same lap or different laps. Combined distributions are then created in a similar fashion as the positive support distribution.\nNext, the distance between each of the $q_p + q_n$ queries and the positive support distribution is calculated and presented to a classifier module. The classifier estimates the probability of the query matching the waypoint. Binary cross-entropy loss is then calculated between true and predicted waypoint assignments for queries to update model parameters.\nAt inference, the vehicle memory is populated with one or multiple combined distributions per waypoint, corresponding to consecutive frames from a single previous recording of waypoint locations. Similar to training time, query distributions are formed by combining a number $n_g$ of the most recent consecutive frames captured via the vehicle camera. The distance between the incoming query and the memory distribution associated with the upcoming waypoint is calculated and presented to the classifier module to determine whether the waypoint is reached. If the output probability is larger than a threshold value, the waypoint is detected. In the case of multiple memory distributions for a waypoint, the maximum probability is considered. Once a decision is made on the executable action, it is sent to the maneuver control unit to update the network condition and accordingly change the throttle and steering response.\nThe maneuver control unit consists of a backbone that processes the input images and concatenates the resulting representation with a one-hot vector that represents the distinct high-level navigation action associated with the latest detected waypoint, for instance a right turn. As such, the dimensionality of the one-hot vector matches the number of permissible high-level action commands. This concatenation serves to condition the input to the regression module so that it produces control signals that match the desired action. The same action is maintained until a new waypoint is detected."}, {"title": "V. EXPERIMENTS", "content": "A. Dataset and training\nThe dataset consists of 36 courses, recorded in 11 University of California, Davis buildings. Of 36 courses, 18 are collected on a clockwise and 18 on a counterclockwise loop, with CW/CCW pairs covering the same closed paths. The recordings are repeated multiple times so that there are between 2 and 8 completed recordings for each course.\nThe recordings are done using a remote-controlled vehicle equipped with cameras. It is equipped with Arduino Mega for analog control of steering and throttle, as well as an onboard ZOTAC mini PC with 8 GB RAM, Nvidia GeForce RTX 2070 Mobile GPU, and Intel Core i7-9750H processor to run custom navigation models. Two ELP fisheye cameras with a 180\u00b0 field of view are mounted on the car, each turned 30\u00b0 outwards, creating a 60\u00b0 relative angle between the two camera orientations. The car can be set to manual or autonomous, which we will use for online evaluation.\nTo segment each course into various landmark/non-landmark segments after collecting data, we use the output steering signal. A fixed number of frames before the onset or completion of a turn are generally considered to be positive samples for waypoint detection, i.e. any one of these frames would have been a safe candidate for initiating the waypoint action at the time of recording.\nTo train the model, 12 CW/CCW pairs are used. 6 courses are used for validation in order to identify the best-performing"}, {"title": "B. Offline evaluation", "content": "We use the 6 unseen test courses to evaluate the performance of the waypoint detection model offline. For evaluation on each course, we construct several positive class distributions per waypoint using samples from a single lap and store them in memory. We then provide frames from other laps in the same course in sequence and construct query distributions by combining the most recent consecutive frames. The distance between the query and memory distributions from the upcoming waypoint is then provided to the classifier for detection, where the maximum match probability among memory distributions is considered. The accuracy is evaluated by comparing the ground truth and the predicted labels of each road segment."}, {"title": "C. Online evaluation", "content": "For online evaluation, the trained model is loaded onto the vehicle computer. Test location waypoint images extracted from a single lap recording (memory lap) and their corresponding high-level navigation actions are provided to the model to construct memory distributions and the LUT. It is then set to autonomous driving mode to navigate the path. Each course test is repeated four times in total, twice for each of the two different memory laps. The course-level (completed course) as well as the waypoint-level (correctly identified waypoints) success rates are reported."}, {"title": "D. Results and ablation study", "content": "1) Offline evaluation results and the effect of backbone pretraining, metric, and image quality: Table I shows offline evaluation results. It also demonstrates the effects of backbone pretraining and the metric used. Considering that in our proposed distribution embedding we impose uncorrelated features through a diagonal covariance, we also explore an alternate configuration where the input to the classifier consists of dissimilarities between univariate normal distributions. In the case of Euclidean distance, we equivalently consider univariate differences corresponding to the representation features. The reason for this is to also test for partial metric learning, where the choice of metric is made at the univariate level, but we give the classifier the added flexibility of learning how to recombine these univariate dissimilarities instead of hard coding it in the cumulative form."}, {"title": "2) Online evaluation results", "content": "In online evaluation, each of the 6 courses consists of 8 waypoints. As mentioned before, each course is evaluated 4 times, corresponding to two repetitions for each of the two memory laps. We also consider a separate, long course consisting of 20 waypoints and similarly test on it. Results are shown in Table III, where the first 6 courses are denoted as CW and CCW navigation of Location IDs 1-3 and the long course with location ID 4. In all failed waypoint detection cases with the threshold of 0.65 picked based on offline validation set performance, while the probability at the waypoint location was high, it did not quite exceed the threshold. We subsequently conducted a limited online test with the default threshold of 0.5 where we tested on each route once. With this adjustment, no failures occurred in online tests."}, {"title": "VI. DISCUSSION AND FUTURE WORK", "content": "In this work, we proposed a two-stage, end-to-end technique for autonomous navigation consisting of a high-level waypoint detector based on few-shot learning, as well as a low-level maneuver control unit that controls the vehicle conditioned on the high-level navigation input. This technique only requires a minimal amount of data from critical waypoints on an unseen route and has a low memory and computation demand. We believe the demonstrated offline and online results serve as proof of concept and motivate further developments with the aim of significantly reducing the need for positioning devices, expensive repeated training, and extensive data from target environments.\nWe are conducting additional research to gauge the approach, its robustness, strengths and limitations, as well as efficient ways to improve it in more diverse environments including outdoor settings. Utilizing public road data (e.g. Google Street View) as a low-cost source of waypoint support examples constitutes another aspect of our continued work. Besides more realistic autonomous driving implementations, for instance, on a full-scale vehicle that uses street images paired with a more sophisticated maneuver control mechanism, several exciting directions remain unexplored. One is the regime in which the car has missed or misidentified a waypoint, which will likely throw the vehicle entirely off-path. To address this, limited additional and ideally native sensors such as the vehicle odometer and/or a compass can be incorporated, which can also make it easier to correctly identify waypoints and execute navigation actions in the first place. Depending on the context, an offline local map can also be used to reroute the mobile robot or vehicle to its most recent known location. Alternatively, a new online navigation task from the new location to the destination can be set up in the same manner as before. Another direction of focus is improving robustness to outdoor landscape changes over time for instance due to seasonal variations. The increasingly realistic synthetic data and state-of-the-art generative models will likely play a crucial role in this regard. Addressing these problems without substantial data collection, computation, and hardware overhead will be topics of particular interest."}, {"title": "APPENDIX A WAYPOINT DETECTION TRAINING AND EVALUATION DETAILS", "content": "At each frame, the left/right camera images are separately processed. They are resized to 224 \u00d7 224 and normalized. During training, random rotation, color jitter, and coarse dropout are applied to the images. After obtaining the distances associated with each camera, they are concatenated and presented to the classifier.\nAt each waypoint, 15 frames leading to the steering initialization/completion are regarded as positive frames for that waypoint, corresponding to a conservative viable range to start steering earlier than where the steering was initiated when recording. The number of consecutive frames $s$ combined into a single distribution is set to 10 for both support and query. The same number is also used in evaluation. During training, each training episode has 1 positive and 6 negative queries ($q_p$ and $q_n$, respectively). Parameters are updated after processing each episode batch size of 36 in phase 1 and 3 in phase 2. A total of 240 and 4000 iterations are processed in phases 1 and 2, respectively. Adam optimizer is used with an initial learning rate of $10^{-4}$ in phase 1 and $10^{-5}$ in phase 2. The learning rate is divided by two after 160 iterations in phase 1 and every 1000 iterations in phase 2. In phase 1, the model is evaluated on the validation set every 32 iterations as well as at the end. It is evaluated every 200 iterations in phase 2.\nFor offline testing of a course, waypoint frames of a single lap are processed to obtain support distributions associated with waypoint locations and store them in memory. A sliding 10-frame window across a 15-frame range yields 6 different combined memory distributions per waypoint. Once the memory is populated, we run through each test lap frame by frame,"}, {"title": "APPENDIX B LOW-LEVEL MANEUVER CONTROL MODULE DETAILS", "content": "Data for the low-level maneuver control module is collected by running the car in test locations. At waypoint positions, each maneuver action (straight/left turn/right turn) is repeated 3-5 times. This data is added to the previously collected test location data to sensitize the model to the high-level action condition and avoid memorizing a specific navigation action at waypoints. We emphasize that data collection from the test location is not an inherent necessity nor limitation of the technique; rather, this decision was made given our limited data collection budget and given the nature of our collected dataset for, and primary focus on, waypoint detection.\nThe model is composed of a pretrained EfficientNet-B0 backbone, whose 1280-dimensional output passes through a linear layer and ReLU activation. At this stage, the 500-dimensional output is concatenated with a 3-dimensional one-hot vector denoting the action condition (left turn/right turn/straight), which then passes through two more layers with ReLU and sigmoid activations and output dimensions of 100 and 1, respectively. The output is considered as steering.\nThe model is trained using mean square error (MSE) loss between true and predicted steering. The two camera images are concatenated, resized to 104 \u00d7 224, and normalized. During training, random color jitter, Gaussian blur, and horizontal flip are applied to the image. In the case of horizontal flipping, the steering is mirrored and in case the action condition is turning, it is adjusted to reflect a turn in the opposite direction. The model is trained for 100 epochs with Adam optimizer and a fixed learning rate of $10^{-4}$."}, {"title": "APPENDIX C IMAGE CORRUPTION DETAILS", "content": "Below we list the details of the applied corruptions for robustness tests. In each case, the corruption was applied to"}]}