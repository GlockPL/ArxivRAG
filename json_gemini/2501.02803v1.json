{"title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism", "authors": ["Yimin Tang", "Zhenghong Yu", "Yi Zheng", "T. K. Satish Kumar", "Jiaoyang Li", "Sven Koenig"], "abstract": "Multi-Agent Path Finding (MAPF), which focuses\non finding collision-free paths for multiple robots, is crucial\nin autonomous warehouse operations. Lifelong MAPF (L-\nMAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic\napproximation of real-world warehouse scenarios. While cache\nstorage systems can enhance efficiency and reduce operational\ncosts, existing approaches primarily rely on expectations and\nmathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper,\nwe introduce a novel mechanism called Lifelong MAPF with\nCache Mechanism (L-MAPF-CM), which integrates high-level\ncache storage with low-level path planning. We have involved a\nnew type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking\nmechanism to bridge the gap between the new cache grid\nand L-MAPF algorithm. The TA dynamically allocates target\nlocations to agents based on their status in various scenarios.\nWe evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated\nperformance improvements particularly with high cache hit\nrates and smooth traffic conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated warehouses, a multibillion-dollar industry led\nby companies like Amazon, Geekplus and inVia, rely on\nhundreds of robots to transport goods efficiently [1]. A criti-\ncal aspect of these operations is planning collision-free paths\nfor robots, a task that can be abstracted as the Multi-Agent\nPath Finding (MAPF) problem [2]. The MAPF problem\nrequires planning collision-free paths for multiple agents\nfrom their start locations to pre-assigned target locations\nin a known environment while minimizing a specific cost\nfunction. Various algorithms have been developed to solve\nthis problem optimally or suboptimally, such as M* [3],\nConflict Based Search (CBS) [4], Enhanced CBS [5] and\nLaCAM [6].\nAlthough MAPF is classified as an NP-hard problem [7],\nit remains a simplified approximation of real-world ware-\nhouse planning. It represents a 'one-shot' version of the\nreal application challenge, where an agent only needs to\nreach a single target location and then remains stationary\nuntil every agent has arrived at their respective targets.\nTo address this limitation, a more realistic variant called\nLifelong MAPF (L-MAPF) [8] has been introduced. In L-\nMAPF, agents are continuously assigned new targets when\nthey reach their current ones, better reflecting the ongoing"}, {"title": "II. PROBLEM DEFINITION", "content": "Lifelong Multi-Agent Path Finding (L-MAPF) problem\npresents unique challenges due to the dynamic and unending\nnature of the tasks. Let $I = {1,2,\u2026\u2026,N}$ denote a set\nof N agents. $G = (V, E)$ represents an undirected graph,\nwhere each vertex $v \\in V$ represents a possible location of\nan agent, and each edge $e \\in E$ is a unit-length edge between\ntwo vertices, allowing an agent to move from one vertex to\nanother. Self-loop edges are allowed, which represent \"wait-\nin-place\" actions. Each agent $i \\in I$ has a unique start location\n$s_i \\in V$. There is also a target queue $T = [t_1, t_2, ...]$, where\n$t_i \\in V$. And the target locations in T are allocated to agents\nwhen agents arrive at their previous target locations. The\nobjective is to minimize a specific cost function in this\npaper, we focus on maximizing the throughput, defined as\nthe number of tasks completed per timestep.\nBesides general L-MAPF definition, in this paper there are\nsome special constraints on the problem. We primarily focus\non 2D warehouse layout maps. As shown in Figure 1, we\ncategorize all non-obstacle map grids into four types: blue\ngrids $B = {b_i|b_i \\in V}$ for shelves and each shelf stores\na unique type of item identified by a type number $idx \\in$\nN, white grids $W = {w_i|w_i \\in V}$ for normal aisle grids,\ngreen grids $U = {u_i|Ui \\in V}$ for unloading ports where\nagents deliver items to complete a task, and purple grids\n$C = {C_i|C_i \\in V}$ for cache grids which will be explained\nlater. Instead of a target queue T, there is a task queue $Q =$\n$[q_1,q_2,...]$. For each task $q_i = (idx, u_i) \\in Q$, where $idx$\nrepresents the type number of the item to be delivered and\n$u_i \\in U$ indicates the unloading port where the item should be\ndelivered. The maximum number of items that one agent can"}, {"title": "III. RELATED WORK", "content": "A. Multi-Agent Path Finding (MAPF)\n(One-Shot) MAPF, which has been proved an NP-hard\nproblem with optimality [7], has a long history [18]. This\nproblem is finding collision-free paths for multiple agents\nwhile minimizing a given cost function. It has inspired a\nwide range of solutions for its related challenges. Decoupled\nstrategies, as outlined in [18], [19], [20], approach the prob-\nlem by independently planning paths for each agent before\nintegrating these paths. In contrast, coupled approaches [21],\n[22] devise a unified plan for all agents simultaneously. There\nalso exist dynamically coupled methods [4], [23] that con-\nsider agents planned independently at first and then together\nonly when needed in order to resolve agent-agent conflicts.\nAmong these, Conflict-Based Search (CBS) algorithm [4]\nstands out as a centralized and optimal method for MAPF,\nwith several bounded-suboptimal variants such as ECBS [5]\nand EECBS [24]. Some suboptimal MAPF algorithms, such\nas Prioritized Planning (PP) [25], [18], PBS [26], LaCAM [6]\nand their variant methods [27], [28], [29] exhibit better\nscalability and efficiency.\nB. Lifelong MAPF\nCompared to the MAPF problem, Lifelong MAPF (L-\nMAPF) continuously assigns new target locations to agents\nonce they have reached their current targets. Agents do not\nneed to arrive at their targets simultaneously in L-MAPF.\nThere are mainly three types of ways to solve L-MAPF:\nsolving the problem as a whole [30], using MAPF methods\nbut replanning all paths at each specified timestep [9], [11],\nand replanning only when agents reach their current target\nlocations and are assigned new targets [31], [32], [6], [29].\nThere are also algorithms that consider the offline setting\nin the L-MAPF scenario, where all tasks are known in\nadvance, such as CBSS [33], which uses Traveling Salesman\nProblem (TSP) methods to plan task orders, and [34], a four\nlevel hierarchical planning algorithm with MILP and CBS.\nHowever, in this paper we focus on an online setting where\nincoming tasks are not known in advance."}, {"title": "C. Warehouse Storage Strategy", "content": "Automated Storage and Retrieval Systems (AS/RS) have\ngained attention for their potential to enhance warehouse\nefficiency and reduce operational costs [35], [36]. Various\nstrategies for assigning items to storage locations have been\nwidely adopted and evaluated [36], [37]. The random storage\npolicy allocates each item type to a randomly chosen storage\nlocation, offering high space utilization [12]. The closest\nstorage policy places new items at the nearest available\nstorage location to minimize immediate travel distance [13].\nThe turnover-based storage policy assigns items to storage\nlocations based on their demand frequency [14].\nThe concept of utilizing temporal storage areas (caches)\nhas been explored in automated warehouse design. Several\nstudies focus on the optimal positioning of these caches [15],\n[16], while others propose AS/RS design methods that quan-\ntitatively consider operational constraints, including cache\nsize [17]. However, existing works primarily focus on the\nhigh-level design and evaluation of warehouse layouts with-\nout addressing the low-level path planning component. Our\nwork specifically studies and evaluates the use of caches\nwithin the context of L-MAPF, incorporating more detailed\nconsiderations of low-level path planning."}, {"title": "IV. METHOD", "content": "In this section, we introduce our Lifelong MAPF with\nCache Mechanism (L-MAPF-CM) framework, which in-\ncludes a new type of map grid and a task assigner (TA)\nfeaturing a cache lock mechanism.\nA. Cache Grids\nAs shown in Figure 1, purple map grids $c_i \\in C$ are caches,\nand these additional vertices serve as interim storage areas to\nreduce the travel time of agents retrieving items. We assume\neach shelf $b_i \\in B$ stores an unlimited supply of a unique item\nwith type number idx. There will be a total of M different\ntypes of items corresponding to the number of shelves. The\nL-MAPF-CM framework can have multiple unloading ports,\nand we refer to each unloading port and its surrounding cache\nas a group. Each group operates independently in terms of\nits task and agents. Agents within a group can only accept\ntasks from that group and use the cache grids belonging to\nthat group. Item in each task $q_i \\in Q$ must be delivered to\nthe corresponding unloading port.\nTo the best of our knowledge, no previous MAPF/L-MAPF\nworks have explored cache mechanism with MAPF/L-MAPF\nproblem. We adopt the following assumptions to fit the\nwarehouse senerio: (1) Agents have limited capacity to carry\nitems and they are also limited to transporting one type of\nitems at a time. (2) Each cache grid has the same capacity\nof agents and also are limited to store one type of items. (3)\nItems evicted from cache should be sent back to shelves.\nB. Task Assigner (TA)\nThe TA operates externally to the L-MAPF algorithm\nand has the ability to define all agents' target locations at\nany given timestep. When the TA encounters a new task in\nQrequiring completion, the first step typically determines\nwhere to get the item and which location (cache or shelf)\nshould be allocated to an available agent. Since the design\nof a smart TA is not the focus of this paper, new tasks are\nassigned to available agents one by one. Generally, the TA\nfirst checks if the task item exists in cache grids. If it does,\nthe TA assigns the cache location where the item is stored to\nan agent. If not, the TA assigns the item's shelf location to\nthe agent, followed by a location in a cache grid where the\nitems can be stored. Once the items are stored in the cache,\nthe agent can transport one item to the unloading port.\nHowever, similar to caching in computer architecture,\nthere is a risk that other agents might replace the item in\nthe cache with another item in multi-agent scenarios. For\nexample, agent i wants to read items in cache $c_k$, but items\nin $c_k$ are replaced by agent j when i is going to $c_k$. This\nsituation could lead to L-MAPF algorithm, such as LaCAM,\ncausing agents to move back and forth between cache and\nshelf grids. To mitigate this, a lock mechanism is employed,\nensuring that agents can secure an item after confirming its\navailability in the cache grids.\nC. Cache Lock Mechanism\nThe cache lock mechanism is supported by a state ma-\nchine to ensure efficient and synchronized access to cache\nlocations. This approach prevents race conditions during\ncache interactions. Every cache grid will have its independent\nlocks. Here we define reading and writing operations: reading\nrefers to agents retrieving items from locations, while writing\nrefers to agents placing items into locations. And readable\nrefers to agents can retrieve items from locations, writable\nmeans agents can place items into locations. The cache lock\nmechanism ensures that no agent needs to return to the shelf\nwhen its target item is readable in the cache and that no\ntwo agents can access the same cache block concurrently for\nwriting. This concurrency control is critical for preventing\nrace conditions and maintaining cache consistency.\n1) Lock Types: The cache lock mechanism is designed\naround three lock types, facilitating controlled access to the\ncaches: (1) Read Lock (Shared Lock): This type of lock\npermits an agent to access the corresponding cache to retrieve\nitems. The maximum number of lock shares is the number\nof items stored in the cache. (2) Write Lock (Exclusive\nLock): This type of lock permits an agent to insert/remove\nitems into/from the corresponding cache. Only one unique\nagent can hold a write lock of a cache.\n2) Lock Acquisition and Release: The mechanism defines\na protocol for lock acquisition and release: (1) Acquisition:\nRead Lock: To obtain a read lock, an agent must verify\nthat no write lock is currently held by any other agent\non the cache. Multiple agents can simultaneously hold the\nread lock on a single cache. This arrangement ensures that\nwhen agents need to retrieve items from the cache, the\nitem remains unchanged until all agents have successfully\nretrieved it when multiple identical tasks are assigned to\ndifferent agents. Write Lock: To secure a write lock, an\nagent must confirm that the target cache location is free from"}, {"title": "D. Algorithm", "content": "1) Overview: As shown in Algorithm 1, we introduce\nthe entire procedure for a single group in L-MAPF-CM.\nL-MAPF-CM consists of three parts: initialization, path\nplanning and execution, and status updates.\nInitialization (Lines 1-5): In this phase, all agents are\nassigned their task items, and their target locations are\ndetermined. Since the caches are empty at the beginning,\nagents can only retrieve items from the shelves. Therefore,\nthe agents' initial target locations are the shelves, and their\nstatus is set to SF_GET.\nPath Planning and Execution (Lines 7-8): Once all\nagents' target locations have been determined, we use any\nL-MAPF algorithm to generate a collision-free plan until\nchecks its status at every update. If a readable cache becomes\navailable, the agent's target is updated to the cache, and its\nstatus is set to CA_GET. When an SF_GET agent arrives at\nits target location, the TA checks if the agent holds a write\nlock: (a) Without a write lock: The agent takes one task item\nto the unloading port, and its status is updated to UP_END.\nNext target is the unloading port. (b) With a write lock: The\nagent's status is updated to CA_ADD, and it takes as many\nitems as it can carry. Next target is the cache.\nWhen CA_MOV agents arrive caches, they get all items\nfrom the cache and head to the shelf corresponding to those\nitems. They release their write locks, and their status changes\nto SF_ADD. When CA_ADD agents arrive at the cache, they\nstore the items except for one. Their target then becomes the\nunloading port, and their status is updated to UP_END. When\nSF_ADD agents arrive at the shelf, they return all items to\nthe shelf. Their status is then updated to SF_GET, with their\nnew target being the shelf containing their task items.\nWe give out two examples. (a) the target item is already\nin the cache: 1) the TA assigns the agent the goal to\nthe cache grid that holds the target item-the state status\ntransfers from UP_END to CA_GET. 2) Upon the agent\narriving at the cache grid, the target item is returned to\nthe port, and the state status transfers from CA_GET to\nUP_END. (b) the target item is not in the cache: 1)\nthe cache is not full, we do not need to perform garbage\ncollection. The TA assigns the agent the goal to the shelf\ngrid that holds the target item-the state status transfer from\nUP_END to SF_GET. 2) Upon the agent arriving at the cache\ngrid, we find an empty available cache grid, so the TA assigns\nthe agent the goal to that cache grid, and the state status\ntransfers from SF_GET to CA_ADD. 3) Then, the agent will\ninsert the item into the cache grid and continue to bring the\nitem to the port to finish the task; the state status transfers\nfrom CA_ADD to UP_END.\n3) Deadlock/Starvation Free and Correctness of State\nMachine: First, we show the lock algorithm is free from\ndeadlock and starvation: no agents are always stuck in a\ncircle waiting with other agents or waiting forever while\naccessing the cache. When an agent wants to read (shared\nlock) or write (exclusive lock) a lock of a cache grid, it is\neither successful or failed. If read fails, it will not hold the\nread lock or wait for a cache hit, but will go directly to the\nshelf to get that item. If write fails, it will not hold the write\nlock or wait for an empty cache grid, but directly deliver\nthe item to the port. Thus, one of the conditions necessary\nto form a deadlock, Hold and Wait [38], is broken, so the\ndeadlock can never occur. Additionally, each agent completes\nits operation in a bounded number of steps without infinite\nwaiting for the resources, so we can conclude that this lock\nalgorithm is free from starvation.\nSecond, we show that state machine itself never gets\nstuck. This property equals the statement: there is no cycle\nin state machine without containing the status UP_END:\nstarting from status UP_END, we can finally return to status\nUP_END after finite steps of transformation. As shown\nin Figure 2, if we remove the status UP_END and all"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We use L-MAPF without cache as a baseline, which aligns\nwith our problem definition. To evaluate performance, we\ncompare L-MAPF-CM with L-MAPF, both using LaCAM\nto generate collision-free paths. L-MAPF-CM and L-MAPF\nwere implemented in C++, building on parts of the existing\nLaCAM codebase [6]. All experiments were conducted on\na system running Ubuntu 20.04.1, equipped with an AMD\nRyzen 3990X 64-core CPU and 64GB RAM at 2133 MHz.\nA. Test Settings\nAs shown in Figure 1, we demonstrated a warehouse\nmap (27x71) with caches based on our problem definition.\nThe map is adapted from warehouse map of MAPF bench-\nmark [2]. It includes 1600 shelf grids, a maximum of 80\ncache grids and 4 unloading ports. The maximum cache-\nto-shelf ratio is 5%. We tested L-MAPF-CM and baseline\nin both multi-port and single-port scenarios, as depicted\nin Figure 1. Since the design of a smart TA is not the\nfocus of this paper, and a naive TA cannot handle multi-\nport scenarios effectively, we will use multiple groups to\ntest scenarios with multiple unloading ports. The multi-\nport scenario has 4 working groups of unloading ports,\ncache grids and agents, each with a maximum of 20 cache\ngrids. The single-port scenario maintains the same number\nof total cache grids and agents as the multi-port but only\nhas one unloading port. Because each shelf grid represents\na unique kind of item, we randomly assign an index to\neach shelf grid. We test all scenarios with different cache\nnumbers {16, 32, 48, 64, 80} by deleting some cache grids\n(refer to Figure 1). We have also chosen various total num-\nbers of agents, {4, 8, 16, 32, 64, 128, 192, 256}. In the multi-\nport scenario, each group has an equal number of agents\n{1, 2, 4, 8, 16, 32, 48, 64}. Each agent carrying capacity P is\n100. All data shown in figs. 4, 6 and 7 represent average\nvalues across all variables not displayed on the figures.\nSince we can expect the distribution of the task queue\ncould significantly affect the performance of cache design,\nwe designed three input task distributions to test L-MAPF-\nCM, including: (1) M-K distribution (MK): For any con-\nsecutive subarray of length M in the task queue Q, there\nare at most K different kinds of items. This distribution\nis inspired by [39], where LRU has been proven to have\nan upper bound on the cache miss rate and to be better\nthan FIFO. This distribution can also represent several items\npeople purchase daily, and some items may become very\npopular at one time, replacing previously popular items. (2)\n7:2:1 distribution (Zhang): There are 70% kinds of items\nwith only a 10% appearance probability in the task queue,\n20% kinds of items with a 20% probability, and 10% with a\n70% appearance probability [40]. (3) Real Data Distribution\n(RDD): We obtain data from Kaggle's public warehouse"}, {"title": "VI. CONCLUSION", "content": "This work presents L-MAPF-CM designed to improve\nthe performance of L-MAPF. We have introduced a new\nmap grid type called cache for temporary item storage and\nreplacement. Additionally, we devised a locking mechanism\nfor caches to enhance the stability of the planning solution.\nThis cache mechanism was evaluated using various cache\nreplacement policies and a range of input task distributions.\nL-MAPF-CM demonstrated performance improvements in\nmost of the test settings. We also identified that high cache\nhit rates and smooth traffic are crucial for the performance of\nL-MAPF-CM. Therefore, there are many interesting avenues\nfor future work, such as developing a smart TA to manage\ntask order, creating a data-driven cache replacement policy"}]}