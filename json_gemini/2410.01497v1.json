{"title": "DLP-LORA: EFFICIENT TASK-SPECIFIC LORA FUSION WITH A DYNAMIC, LIGHTWEIGHT PLUGIN FOR LARGE LANGUAGE MODELS", "authors": ["Yuxuan Zhang", "Ruizhe Li"], "abstract": "Recent advancements in Large Language Models (LLMs) have achieved robust performance across diverse tasks, but fine-tuning these models for specific domains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a small subset of parameters. However, existing methods for fusing multiple LoRAs lack dynamic fusion based on contextual inputs and often increase inference time due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight Plugin that employs a mini-MLP module with only 5M parameters to dynamically fuse multiple LoRAs at the sentence level using top-p sampling strategies. This approach reduces inference time to less than twice that of single LoRA inference by leveraging parallel computation. Evaluations across 26 tasks\u2014including multiple-choice questions and question answering-demonstrate that DLP-LORA achieves an average accuracy of 92.34% on multiple-choice datasets and significant improvements in BLEU and ROUGE scores on QA datasets, outperforming different LLMs backbones under composite task settings. DLP-LORA effectively balances performance and efficiency, making it a practical solution for dynamic multi-task adaptation in LLMs. Our code is available at https://github.com/MeCuping/DLP-LORA.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) such as LLaMA 3.1 (Dubey et al., 2024), Qwen 2.5 (Team, 2024), and Gemma 2 (Team et al., 2024) have led to robust and superior performance across multiple benchmarks (Muennighoff et al., 2022; Ilyas Moutawwakil, 2023; Fourrier et al., 2024). These models have demonstrated remarkable capabilities in diverse areas, including code generation (Bai et al., 2023), mathematical reasoning (Ahn et al., 2024), and question answering (Achiam et al., 2023). Despite these achievements, fine-tuning all parameters of such large models for specific domains remains resource-intensive and time-consuming.\nParameter-Efficient Fine-Tuning (PEFT) methods (Houlsby et al., 2019; Xu et al., 2023) address this challenge by enabling the fine-tuning of a small subset of parameters, thereby improving performance in various applications like multi-task learning (Xu et al., 2024; Kong et al., 2024), multilingual summarisation, and transfer learning (Whitehouse et al., 2024; Zhao et al., 2024). One prominent PEFT approach is Low-Rank Adaptation (LoRA) (Hu et al., 2021), which fine-tunes low-rank matrices to capture domain-specific knowledge and merges them with pre-trained LLM backbones.\nTo enhance the multi-task learning capabilities of LLMs, several methods have been proposed to fuse task-specific LoRAs, including MoLE (Wu et al., 2024), S-LoRA (Sheng et al., 2023), and LoRAHub (Huang et al., 2023). These approaches primarily use learnable gating networks or automatic loading mechanisms to combine multiple LoRAs. For instance, MeteoRA (Xu et al., 2024)"}, {"title": "2 BACKGROUND", "content": "Low-Rank Adaption. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a method developed to fine-tune large language models (LLMs) for specific downstream tasks with enhanced efficiency by minimising the number of trainable parameters. Instead of updating all the model's parameters during training, LoRA introduces supplementary low-rank matrices. In Transformer-based autoregressive LLMs, this technique involves freezing the pre-trained weights and integrating trainable low-rank matrices into designated layers, thereby substantially reducing computational overhead. The primary motivation for LoRA stems from the recognition that many parameter updates during fine-tuning occur within a low-dimensional subspace, indicating that full-rank weight updates are often unnecessary. By employing low-rank approximations, LoRA significantly decreases the number of parameters required for training-sometimes by factors as large as 10,000-while still maintaining competitive performance levels.\nFormally, consider a weight matrix $W \\in R^{h \\times d}$ within the original LLMs. LoRA introduces two low-rank matrices, $A \\in R^{h \\times r}$ and $B \\in R^{r \\times d}$, where $r < min(h, d)$. Instead of directly updating"}, {"title": "3 METHODOLOGY", "content": "Our proposed DLP-LoRA framework comprises three key components: a lightweight mini-MLP plugin $C_{MLP}$, a base LLM backbone $M$, and a set of N fine-tuned LoRA modules $L_{{1...N}}$ corresponding to different tasks $D_{{1...N}}$, as illustrated in Figure 1. Initially, we train the mini-MLP classifier $C_{MLP}$ on these tasks to achieve high task classification accuracy (we evaluate 26 tasks in this work; see Appendix A for details). Once trained, the LLM backbone $M$ utilises the mini-MLP plugin to dynamically fuse the appropriate fine-tuned LoRAs $L_{{1...N}}$ at the sentence level, enabling efficient multi-task learning."}, {"title": "3.1 LIGHTWEIGHT MULTI-TASK CLASSIFICATION PLUGIN", "content": "Previous methods that perform token-level task classification and routing within the LLM backbone-by injecting a trainable gating network at each attention and MLP layer-are computationally intensive and inefficient during inference (Xu et al., 2024). Observing that most tokens within a sentence typically pertain to the same task, we propose a more efficient sentence-level task detection approach. Specifically, we introduce an off-the-shelf 4-layer mini-MLP plugin $C_{MLP}$ that requires training only once on the sentence level for the selected tasks.\nGiven N distinct tasks $D_{{1...N}}$ and a collection of M sentences $S_{{1...M}} \\in D_n$, our lightweight 4-layer $C_{MLP}$ encodes each input sentence $S_m$ using a specific tokenizer (we utilise the ALBERT tokenizer (Lan, 2019) in this work) and classifies $S_m$ to the correct task $D_n$:\n$Y_n = C_{MLP}(S_m)$, where $Y_n \\in D_{{1...N}}$."}, {"title": "3.2 DYNAMIC LORA FUSION", "content": "Once the $C_{MLP}$ classifier is well-trained on the tasks $D_{{1...N}}$, it serves as a plugin to the LLM backbone $M$ for dynamically fusing multiple LoRAs $L_{{1...N}}$ at the sentence level. For the current input sentence $S_m \\in D_n$, we consider the first token $w_1$ and the previous contextual history $H_{{1...k}}$. We employ a top-p sampling scheme via $C_{MLP}$ to dynamically select the possible LoRAs to fuse, using probability $p$ as the threshold:\n$I_p = {Y_{{1...R}} | W_1 \\in S_m, H_{{1...k}}}, where Y_r \\geq p$.\nUsing the set $I_p$ for the current sentence $S_m$, we fuse the selected LoRAs based on normalised weights obtained via a softmax function:\n$W_m = Softmax(I_p) = {W_1,...,W_R}$.\nImportantly, the $C_{MLP}$ classifier is only activated when the first token $w_1$ of the current sentence $S_m$ is generated, leveraging the contextual information $H_{{1...k}}$. This approach significantly accelerates the inference time of $M$ compared to token-level gating network classification (Xu et al., 2024), as it avoids the overhead of per-token classification."}, {"title": "3.3 PARALLEL MULTI-LORA ACCELERATION", "content": "Beyond the efficiency gained from sentence-level LoRA sampling and fusion\u2014which avoids the inefficiency of repetitive per-token LoRA classification\u2014a significant advantage of our approach is the ability to fully exploit parallel multi-LoRA acceleration."}, {"title": "4 EXPERIMENTS", "content": "To comprehensively evaluate our proposed DLP-LORA framework, we follow the methodology of Xu et al. (2024) and conduct experiments across 26 diverse tasks. These include 17 multiple-choice question (MCQ) datasets covering domains such as mathematical question answering, logical reasoning, language identification, and reading comprehension. Additionally, we assess performance on 9 question-answering (QA) datasets focused on summarisation, machine translation, and open-domain QA. Specifically, we utilise 20 tasks from the BigBench benchmark (Srivastava et al., 2023), 3 machine translation tasks from the News Commentary dataset (Tiedemann, 2012) translating from non-English to English, and 3 generative tasks: GSM8K (Cobbe et al., 2021), CNN/DailyMail (See et al., 2017), and Alpaca (Taori et al., 2023). Detailed descriptions of each dataset are provided in Appendix A."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets.\nLLM Backbones, LoRAs, and Mini-MLP Plugin. We evaluate DLP-LORA using four widely adopted LLM backbones: Qwen-2 1.5B and 7B (Yang et al., 2024), LLaMA-2 7B (Touvron et al., 2023), and LLaMA-3 8B (Dubey et al., 2024). To assess the effectiveness of DLP-LORA in scenarios with significant model size differences, we also compare the performance of DLP-LORA based on"}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "Main Results. Figure 2 presents the classification accuracy across the 17 MCQ tasks and ROUGE-L scores across the 9 QA tasks, comparing our DLP-LoRA with the baseline LLaMA-2 7B backbone and individually fine-tuned single LoRAs. Our DLP-LoRA not only significantly outperforms the baseline LLaMA-2 7B model but also achieves performance comparable to, and in some cases surpassing, that of the manually loaded single LoRAs on the 17 MCQ tasks. Similar trends are observed for the 9 QA tasks (additional results for other LLM backbones are provided in Appendix B). As shown in Table 1, DLP-LoRA achieves performance within a relative difference of -0.35% in accuracy across the 17 MCQ tasks when compared to the single LoRA models using different LLM backbones. Remarkably, DLP-LORA consistently outperforms the single LoRA models on the ElemMath and WinoWhy datasets. A similar pattern emerges in Table 2 for the 9 QA tasks, where DLP-LoRA shows relative improvements in BLEU, ROUGE-1, and ROUGE-L scores by averages of 0.54%, 0.22%, and 0.09% across all QA tasks and LLM backbones, respectively. These results demonstrate that DLP-LoRA can match or even exceed the performance of individually fine-tuned single LoRAs by dynamically selecting and fusing multiple LoRAs.\nMulti-task Composite Performance. We further evaluate DLP-LoRA's capability in multi-task learning under composite task settings by combining the 17 MCQ tasks and the 9 QA tasks. As presented in Table 3, DLP-LoRA significantly enhances performance over the baseline LLM backbones, achieving relative improvements of 92.95% in accuracy for the MCQ composite, and 9.95%, 13.90%, and 15.76% in BLEU, ROUGE-1, and ROUGE-L scores, respectively, for the QA composite. These findings indicate that DLP-LoRA effectively and automatically selects the appropriate LoRAs based on the input prompts within composite tasks, facilitating dynamic multi-task adapta-"}, {"title": "4.3 CASE STUDY", "content": "To illustrate the practical effectiveness of DLP-LoRA, we present a case study in Figure 3 using the LLaMA-3 8B backbone under a composite task setting involving three tasks. For the first input prompt, DLP-LORA selects two LoRAs-AbsNarr and GSM8K-with probabilities of 50.5% and"}, {"title": "5 DISCUSSION", "content": "Limitations of Top-k Selection Most existing Multi-LoRA or LoRA-MoE methods employ a top-k router to manually determine the fixed number of LoRAs to use for multi-task learning (Li et al., 2024). This manual selection can restrict the model's ability to dynamically select and fuse multiple LoRAs based on the task requirements. In our approach, we utilise top-p selection, which leverages the probabilities assigned by the mini-MLP plugin to each LoRA, using a threshold p. This allows DLP-LORA to adaptively decide both the number and combination of LoRAs to fuse for different tasks, enhancing flexibility and performance.\nCan a Smaller LLM with DLP-LORA Outperform a Larger LLM Backbone? Our evaluations of DLP-LoRA across various LLM backbones ranging from 1.5B to 8B parameters under composite task settings prompted us to investigate whether a smaller LLM backbone equipped with DLP-LoRA can outperform a larger, unadapted LLM backbone. As shown in Table 5, the Qwen-2 1.5B model equipped with DLP-LoRA reduces inference time by over 90% compared to the LLaMA-2 13B backbone when processing a mixture of 26 tasks. Moreover, it achieves significant improvements in accuracy, ROUGE-1, and ROUGE-L scores by 81%, 119%, and 125%, respectively. These findings suggest that smaller LLMs augmented with DLP-LoRA have the potential to match or even surpass the performance of much larger models (with over eight times more parameters) across diverse tasks. This is particularly beneficial for deployment on devices with limited computational resources, such as mobile devices.\nInference Time of Multi-LoRA Loading at Scale By avoiding inefficient and repetitive token-level LoRA classification, our method fully leverages PyTorch's General Matrix Multiplication (GEMM) operations for parallel multi-LoRA acceleration. We conducted an ablation study to assess how the inference time scales with the increasing number of LoRAs, using the LLaMA-3 8B backbone as a reference. As illustrated in Table 6, even as the number of LoRAs increases, the inference time ratio remains within 2x of the baseline LLaMA-3 8B model. Additionally, the combined parameters of all LoRAs constitute less than 0.1% of the 8B parameters in the LLaMA-3 backbone. These results demonstrate that our approach scales efficiently with the number of LoRAs without incurring significant computational overhead, maintaining practical inference times even at scale."}, {"title": "6 RELATED WORK", "content": "In the area of multi-task learning with LoRA, two primary research directions have emerged beyond the straightforward approach of fine-tuning a single LoRA on a combined dataset of multiple tasks (Lin et al., 2024). The first direction focuses on developing libraries or frameworks to reuse and integrate existing LoRAs, while the second aims to design router networks based on MoEs to dynamically fuse multiple LoRAs.\nMultiple LoRA Architectures Several works have proposed frameworks for combining and managing multiple LoRAs. Huang et al. (2023) introduced LoRAHub, a framework that combines existing fine-tuned LoRAs using a learnable weighted sum, allowing for more flexible adaptation across tasks. S-LoRA (Sheng et al., 2023) emphasises unified memory pool design to manage dynamic LORA weights with varying ranks and key-value cache tensors for CUDA kernels, enhancing computational efficiency. Additionally, Model-Based Clustering (MBC) (Ostapenko et al., 2024) employs clustering techniques to group tasks based on the similarity of their LoRA parameters, facilitating better parameter sharing and task generalization.\nMixture-of-Experts with Multiple LoRAS Another line of research integrates Mixture-of-Experts mechanisms to control and fuse multiple LoRAs dynamically. In these approaches, multiple LoRAs are fine-tuned and injected into the model's MLP layers, with a router network determining which LoRA to activate for a given task. Examples include LoRAMoE (Dou et al., 2024), PHAT-GOOSE (Muqeeth et al., 2024), MoLE (Wu et al., 2024), and LoRA-Switch (Kong et al., 2024). Some methods extend this fusion to both MLP and attention layers, such as MixLoRA (Li et al., 2024) and Mixture of Adaptations (MoA) (Feng et al., 2024), enabling more comprehensive adaptation across model components.\nFurthermore, token-level routing strategies have been proposed to enhance the granularity of LORA selection. MeteoRA (Xu et al., 2024) introduces a token-level MoE-style multi-task LoRA framework with trainable gating mechanisms across all attention and MLP layers, allowing for dynamic selection and fusion of different LoRAs based on input tokens. Similarly, AdaMoE (Zeng et al., 2024) presents an adaptive MoE approach that leverages token-level routing within transformer models to improve performance across diverse tasks."}, {"title": "7 LIMITATIONS", "content": "Our evaluation of DLP-LoRA was primarily conducted on LLM backbones ranging from 1.5 billion to 8 billion parameters, constrained by the computational limitations of our GPU resources. Consequently, we were unable to assess the performance of DLP-LoRA on larger models such as Qwen-2.5 32B (Hui et al., 2024) and LLaMA-3.1 70B (Dubey et al., 2024), which may exhibit different behaviors and performance characteristics. Additionally, when composite tasks include a higher proportion of MCQ datasets, DLP-LORA tends to assign higher probabilities to the specific MCQ LORA, potentially limiting its ability to effectively fuse and utilize QA LoRAs. This tendency might restrict the diversity of generated outputs and the fusion capabilities of DLP-LoRA across a broader range of tasks."}, {"title": "8 BROADER IMPACTS", "content": "The lightweight design of DLP-LoRA, featuring a mini-MLP with only 5 million parameters, offers significant flexibility and efficiency, making it suitable for deployment on smaller devices with limited computational resources. Moreover, DLP-LoRA facilitates easy integration of new LoRAs corresponding to additional tasks without necessitating further fine-tuning of the entire model. This capability enhances the accessibility and adaptability of LLMs in various applications, promoting broader utilisation in resource-constrained environments."}, {"title": "9 CONCLUSION", "content": "We introduced DLP-LoRA, a dynamic and lightweight plugin that employs a mini-MLP module with only 5 million parameters to dynamically fuse multiple LoRAs at the sentence level using top-p sampling strategies. Our comprehensive evaluation across 17 MCQ tasks and 9 QA tasks demonstrates that DLP-LoRA not only closely matches the performance of individually fine-tuned single LoRAs but also surpasses them on certain tasks, all while incurring less than twice the inference time. Through detailed discussions and ablation studies, we have shown that DLP-LORA effectively balances performance and efficiency in multi-task learning, making it a practical solution for dynamic multi-task adaptation in LLMs."}]}