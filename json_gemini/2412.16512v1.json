{"title": "TrojFlow: Flow Models are Natural Targets for Trojan Attacks", "authors": ["Zhengyang Qi", "Xiaohua Xu"], "abstract": "Flow-based generative models (FMs) have rapidly advanced as a method for mapping noise to data, its efficient training and sampling process makes it widely applicable in various fields. FMs can be viewed as a variant of diffusion models (DMs). At the same time, previous studies have shown that DMs are vulnerable to Trojan/Backdoor attacks, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. We found that Trojan attacks on generative models are essentially equivalent to image transfer tasks from the backdoor distribution to the target distribution, the unique ability of FMs to fit any two arbitrary distributions significantly simplifies the training and sampling setups for attacking FMs, making them inherently natural targets for backdoor attacks. In this paper, we propose TrojFlow, exploring the vulnerabilities of FMs through Trojan attacks. In particular, we consider various attack settings and their combinations and thoroughly explore whether existing defense methods for DMs can effectively defend against our proposed attack scenarios. We evaluate TrojFlow on CIFAR-10 and CelebA datasets, our experiments show that our method can compromise FMs with high utility and specificity, and can easily break through existing defense mechanisms.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, diffusion models (DMs) [1]\u2013[3] demonstrated im-pressive performance on generative tasks like image synthesis [4], speech synthesis [5] and molecule generation [6], [7]. However, a disadvantage is that DMs often require hundreds to thousands of time steps to generate high-quality images, making them more computationally expensive compared to other generative models like GAN [8] or VAE [9]. Some works focus on accelerating sampling in DMs [2], [10], while another group of researchers rethought the optimization objectives of DMs and proposed flow-based generative models (FMs) to enable more efficient training and sampling, achieving high-quality image generation even in a single step. The renowned Stable Diffusion3, one of the most advanced and largest text-to-image generation models, also utilizes Flow Matching technology.\nMany works related to FMs, such as Flow Matching [11], Rectified Flow [12], and Consistency Models [13], these works are derived from different perspectives but share similar ideas. The general idea of FMs is to map one distribution to another by calculating a velocity field. By moving points from the source distribution along this velocity field, they converge to the target distribution. A key difference between FMs and DMs lies in the modeling approach. While DMs utilize stochastic differential equations (SDEs) to compute the target distribu-tion, FMs employ a deterministic approach, using ordinary differential equations (ODEs) to compute velocity fields that map the initial distribution to the target distribution.\nHowever, the widespread application of DMs and FMs has raised concerns about their security. A large number of studies on backdoor attacks [14]\u2013[16] and defenses against backdoor attacks [17]\u2013[19] for DMs have been proposed. These attack studies primarily focus on modifying the training process of DMs, ensuring that the model not only retains its original ability to generate diverse images but also can generate target images(which may be harmful) from an inserted backdoor trigger. Correspondingly, defense works mainly focus on how to detect whether released DMs contain backdoor triggers and how to purify them.\nAlthough attacks on FMs share similarities with those on DMs, they have not been extensively studied. Therefore, in this paper, we attempt to answer two key questions: 1. How do attacks on FMs differ from attacks on DMs, and how can we design attacks specifically tailored to the characteristics of FMs? 2. Can existing defense methods for attacks on DMs be directly applied to defend against attacks on FMs?\nTo investigate the vulnerability of FMs against Trojan attacks, we propose TrojFlow, The first Trojan attack on FMs. Specifically, we select Rectified Flow [12] as the base model for our attack due to its theoretical simplicity and training stability. Additionally, since Rectified Flow [12] effectively defines the transfer process between distributions, performing Trojan attacks on FMs does not require defining complex diffusion and reverse diffusion processes required in DMs, which brings convenience to our attack implementation. We align our approach with previous work, TrojDiff [15], in terms of the trigger, target image, and some defined terms, which facilitates a clearer comparison of attacks on DMs and FMs. A closely related concurrent work is [20], which explores the possibility of inserting backdoors into consistency models. The key difference lies in our more comprehensive Trojan settings and consideration of potential defenses."}, {"title": "Our main contributions are as follows:", "content": "1. We are one of the early works to reveal the vulnerability of FMs to Trojan attacks. Leveraging the ability of FMs to fit arbitrary distributions, in addition to the previous attack set-tings, we further propose establishing a point-to-point mapping on FMs. By combining invisible triggers, we achieve Trojan attacks with minimal visibility and modification.\n2. We investigate two representative defense works for DMs, TERD [18] and UFID [17], and adapt our approach based on the settings of these defense works. We demonstrate that existing defense strategies for DMs do not effectively defend against Trojan attacks on FMs. This highlights the need for more research on defending against attacks on FMs."}, {"title": "II. BACKGROUND", "content": "A. Rectified flow [12]\nRectified flow [12] is an ODE-based generative modeling framework. Given the initial distribution \u3160\u315c and the target data distribution \u03c0\u03bf, rectified flow trains a velocity field parameterized by a neural network with the following loss function,\n$L_{rf}(\\theta)=\\underset{x_T \\sim \\pi_T, x_0 \\sim \\pi_0}{E} \\int_0^T ||v_{\\theta}(x_t, t)-\\frac{(x_T-x_0)}{T}||^2 dt$ (1)\nwhere $x_t=(1-t/T)x_0 + tx_T/T$.\nT is usually set to 1. Based on the trained rectified flow, we can generate samples by simulating the following ODE from t = 1 to t = 0,\n$\\frac{dx_t}{dt}=v_{\\theta}(x_t, t)$. (2)\nSamples can be generated by discretizing the ODE process with the Euler solver into N steps (e.g., N = 1000) as the following, the solver will be more accurate with a large N.\n$x_{t-\\frac{1}{N}} = x_t - \\frac{1}{N}v_{\\theta}(x_t, t), t \\in {1,2,...,N}/N$. (3)"}, {"title": "B. Backdoor Attacks on Diffusion Models", "content": "There are three existing popular backdoor attacks for uncon-ditional DMs, BadDiff [16], TrojDiff [15], VillanDiff [14]. A common characteristic of these approaches is the introduction of an additional backdoor forward process, $x \\rightarrow x^*$, along-side the standard forward process in DMs, $x \\rightarrow x$. Here, $x^*$ represents target image, $x'$ denotes the backdoor noise or trigger, $x$ corresponds to clean training data, and $x_t$ is usually the Gaussian noise in DMs.\nDuring training, the clean and backdoor losses are calculated separately, enabling the model parameter 0 to learn both the clean transitional distribution $q(x_{t-1} | x)$ and the backdoor transitional distribution $q(x_{t-1} | x^*)$. At the sampling stage, the backdoored model generates clean samples $x \\sim q(x)$ from clean inputs and target images $x^* \\sim q(x')$ from backdoor noise.\nAmong these attacks, TrojDiff [15], which is the main method we compare in this paper, introduces three distinct attacks, each with a different type of target distribution:\n\u2022 D2I Attack. $q'(x)= x_{target}$, where $x_{target}$ is a predefined target image, such as Mickey Mouse.\n\u2022 Din Attack. $q'(x) = q(x | \\hat{y})$, where $\\hat{y}$ is a target class in the class set of $q(x)$. For instance, use CIFAR-10 as the clean training dataset and \u201chorse\u201d as the target class.\n\u2022 Dout Attack. $q'(x) = q(x | \\hat{y})$, where $\\hat{y}$ is a target class outside the class set of $q(x)$. For example, use CIFAR-10 as the clean training dataset and the number \"7\" from MNIST as the target class.\nDin and Dout share certain similarities, as both fit the backdoor to a specific distribution. According to the con-clusions of TrojDiff [15], different target distributions only lead to minor variations in fitting efficiency and effectiveness. In this paper, we focus on point-to-point and distribution-to-distribution fitting, so we select Din and D2I settings as the focus of our experiments. Additionally, TrojDiff [15] defines the clean training and sampling as the Benign process, and the attack training and sampling as the Trojan process. These definitions will be adopted in this paper."}, {"title": "III. METHODS", "content": "A. Threat Model\na) Design of Trojan noise input: We first define two types of triggers as described in [15], [21]. The blend-based trigger 8 is an image (e.g., Hello Kitty) blended into the noise input with a certain blending proportion, while the patch-based trigger is a patch (e.g., a white square), typically placed on a specific part (e.g., the bottom right corner) of the noise input. Specifically, inspired by [22], [23], we define random Gaussian noise as a trigger, which is more difficult to detect. Unlike TrojDiff [15], the backdoor noise (trigger) in our setting can be either a distribution or a single image, creating a point-to-point mapping, which allows for more flexibility in backdoor insertion. Additionally, in the D2I setup, we will demonstrate the effects of mapping multiple triggers to multiple target images.\nFor blend-based trigger setting, we assume the distribution of the Trojan noise is $N(\\mu,\\gamma^2I)$, where $\\mu = (1 - \\gamma)\\delta$, $\\gamma \\in [0, 1]$, and $\\delta$ has been scaled to [-1,1]. Then a Trojan noise could be written as $x' = \\mu + \\gamma\\epsilon = (1 - \\gamma)\\delta + \\gamma\\epsilon$, $\\epsilon \\in N(0, I)$.\nFor patch-based trigger setting, $\\delta$ is an all-white image and $\\gamma\\in R^{h\\times w}$ is a 2D tensor/mask instead of a constant. $\\gamma_{i,j} = 1$ if trigger is not in (i, j). Otherwise, $\\gamma_{i,j}$ is selected as a small value close to 0, e.g., 0.1, ensuring it appears as white.\nb) Attacker's capacity: As mentioned in the background, the attacker's goal is to insert a backdoor into the FMs, so that a specific target output can be achieved through a backdoor input. Therefore, we consider a white-box attack: 1) The attacker has access to the model's parameters, weights, and the training process, allowing them to insert a backdoor during training. 2) When the attacker provides the model parameters and the sampling process for user use, they have the ability to modify the initial noise input during sampling, thereby activating the backdoor implanted during training.\nIn this setup, the user has full access to model parameters, loads the model, and verifies the model's metrics. The user"}, {"title": "A. Attack FM", "content": "TERD [18] summarizes a unified forward process for at-tacks on existing DMs:\n$x_t = a(x_0, t)x_0 + b(t)\\epsilon + c(t)r$. (4)\nWhere a(x,t) and b(t) are two coefficients that follow the benign diffusion process and the backdoor coefficient c(t) is defined by attackers, with $\\underset{t \\rightarrow T}{lim} c(t) = 1$, $\\underset{t \\rightarrow 0}{lim} c(t) = 0$. When setting $a(x,t) = 1,b(t) = t,c(t) = t$, we can get the transport path for backdoor FMs:\n$x_t = x_0 + t\\epsilon + tr$. (5)\nThe derivation of this form aligns with the unified repre-sentation and can be reused in other frameworks. However, within the theory of FMs themselves, it is redundant. In Equation 1, treating x as the backdoor noise and xo as the target image, we can construct the backdoor transport path while keeping the loss function and other sampling forms unchanged. For convenience in subsequent descriptions, we will use $L_{\\theta}(x_0,x,t)$ to refer to the objective in Equation 1. Thus, we obtain the overall backdoor training process in Algorithm 1.\nThe advantage of our Trojan attack definition is that for both benign generation and Trojan generation, the sampling process requires no additional modifications; it only involves using different initial noise in Equation 3 for different types of generation."}, {"title": "C. Defense-Aware Attack", "content": "A key aspect of designing an attack method is to evaluate its robustness against existing defense mechanisms. In this work, we investigate whether the current defense works for DMs can effectively detect the backdoor we implanted in FMs. For this analysis, we consider two representative defense works UFID [17] and TERD [18].\nUFID [17] proves that for benign generation, a small per-turbation on initial noise significantly alters the output, while for Trojan generation, minor perturbations of backdoor noise do not lead to substantial changes in the generation results. Therefore, by performing pairwise similarity calculations on the generated samples with perturbed input noise, the backdoor noise can be detected.\nOur objective is to bypass UFID's detection by creating point-to-point mappings. For example, in the D2I setting, we establish a bijection between $D_{trigger}$ and $D_{target}$, making a precise point-to-point relationship. In this case, theoreti-cally, applying any perturbation to the backdoor noise will result in the model mapping it to a clean output. However, our experiments demonstrate that even with such mapping, perturbing the trigger image still probabilistically produces the target image or results in images with significant quality degradation. We attribute this behavior to the inherent gen-eralization capabilities of neural networks. Therefore, we im-plement additional Perturbation-Driven Training (PDT), which remaps the distribution around the trigger to clean images while preserving the transport path from the trigger to the target. The final optimizing functions are as follows:\n$L_{total} = E_{t\\sim[0,1],\\epsilon,\\epsilon'\\sim N(0,1)} (L_{benign}(x_c, \\epsilon, t) + L_{trojan} (6)\n(x_{target}, x_{trigger},t) + L_{PDT}(x_c, x_{trigger} + \\epsilon',t))$\nFor TERD [18], its core idea is that when $t \\rightarrow T$, i.e., when $x_t$ approves noise, we can sample two $x_t$, if both values belong to clean noise (Gaussian noise) or both belong to backdoor noise, the model will exhibit similar fitting behavior. By applying gradient backpropagation on $x_t$, the trigger inversion is achieved. TERD [18] designs a unified trigger inversion formula for all attacks of the form Equation 4.\n$L(r, x_t) = ||(F_{\\theta}(x_t(\\epsilon_1, r), t) - f (x_t (\\epsilon_1, r), \\epsilon_1)\n- F_{\\theta} (x_t(\\epsilon_2, r), t) + f (x_t(\\epsilon_2, r), \\epsilon_2)||_2 - \\lambda ||r||_2$ (7)\nwhere r represents trigger, $F_{\\theta}$ represents the neural network, and f represents the network's training objective. In TrojDiff's [15] setting, we have $f(x_t(\\epsilon_1,r), \\epsilon_1) = \\epsilon_1$. The obstacle for trigger reversion is that $x_t$ is unknown. However, in the trojan forward process of DMs, as shown in Equation 4, $\\underset{t \\rightarrow T}{lim} a(x_0, t) = 0$, $x_t$ will converge to the prior distribution that is little affected by $x_0$. Therefore, TERD [18] substituted $x_0$ with a surrogate image $x_0'$ sampled from a standard Gaussian distribution and achieved good results."}, {"title": "IV. EXPERIMENTS", "content": "However, in our setting, FMs are trained to fit the velocity, and the training objective is $x_{target} \u2013 x_{noise}$ for any t. Simply replacing the target image with Gaussian noise results in a misaligned training objective, which undermines the reverse effect. For a clearer presentation, we adaptively modify the TERD [18] algorithm to make it work for TrojFlow, summa-rized in Algorithm 2, and demonstrate the reverse results of TERD [18] on TrojFlow in the experimental section.\nA. Experimental setup\na) Datasets, models, and implementation details: We use two benchmark vision datasets, i.e., CIFAR-10 (32 \u00d7 32) [24] and CelebA (64 \u00d7 64) [25]. Due to computational resource limitations, we only present the visual effects of the Trojan attack on CelebA, without reporting additional metrics or conducting ablation experiments on it. We train Rectified Flow [12] from scratch with 1000k steps as the base model. We apply the attack algorithms to fine-tune the base models with 40k steps in the Din attack and 20k steps in the D2I attack. We sample 50k samples for the evaluation of benign performance while 10k for attack performance.\nb) Attack configurations: The blend-based trigger is a Hello Kitty image, which is blended into the standard Gaussian noise with the blending proportion of (1 \u2013 \u03b3), where y = 0.6 in all experiments. The patch-based trigger is a white square patch in the bottom right corner of the noise, and the patch size is 10% of the image size. In Din attack, the target class is 7, i.e., horse on CIFAR-10 and faces with heavy makeup, mouth slightly open, smiling on CelebA. In the D2I attack, we randomly sample 10 standard Gaussian noises as invisible triggers. We select Mickey Mouse and 9 different Pokemon images as the target images, and then we insert n bijections from invisible triggers to target images into the pre-trained models (n=1,5,10).\nc) Evaluation metrics: We select three widely-used met-rics, Frechet Inception Distance (FID) [26], precision [27], and recall [27]. A lower FID indicates better quality and greater diversity of the generated images, while the other two metrics with higher values reflect each of these aspects individually. To evaluate the attack performance, in Din attacks, we use Attack Success Rate (ASR) (the fraction of the generated images which are identified as the target class by a classification model) introduced by TrojDiff [15], to measure how accurate the generated images are in terms of the target class. In the D2I attack, we use Mean Square Error (MSE) to measure the gap between the target image and the generated image, a lower MSE indicates better backdoor image generation.\nB. Main results\na) Attack Results: We summarize the performance of TrojFlow on CIFAR-10 for benign generation and Trojan generation in Table I. For benign generation, we found that the Trojaned models perform worse than the base model under both attack settings. The FID increased by 0.2 to 0.6, and both Precision and Recall also decreased. We also"}, {"title": "V. CONCLUSION", "content": "In this work, we propose an effective method for per-forming Trojan attacks on FMs. Through experiments on the CIFAR-10 and CelebA datasets, we reveal the vulnerability of FMs to backdoor attacks. Specifically, TrojanFlow imple-ments point-to-point backdoor insertion, effectively bypassing perturbation-based backdoor detection methods such as UFID [17], and exhibits inherent resistance to TERD [18]. In the future, we aim to extend Trojan attacks to conditional FMs and call for more research on defense mechanisms for backdoor attacks on FM."}]}