{"title": "HPIX: GENERATING VECTOR MAPS FROM SATELLITE IMAGES", "authors": ["Aditya Taparia", "Keshab Nath"], "abstract": "Vector maps find widespread utility across diverse domains due to their capacity to not only store but also represent discrete data boundaries such as building footprints, disaster impact analysis, digitization, urban planning, location points, transport links, and more. Although extensive research exists on identifying building footprints and road types from satellite imagery, the generation of vector maps from such imagery remains an area with limited exploration. Furthermore, conventional map generation techniques rely on labor-intensive manual feature extraction or rule-based approaches, which impose inherent limitations. To surmount these limitations, we propose a novel method called HPix, which utilizes modified Generative Adversarial Networks (GANs) to generate vector tile map from satellite images. HPix incorporates two hierarchical frameworks: one operating at the global level and the other at the local level, resulting in a comprehensive model. Through empirical evaluations, our proposed approach showcases its effectiveness in producing highly accurate and visually captivating vector tile maps derived from satellite images. We further extend our study's application to include mapping of road intersections and building footprints cluster based on their area.", "sections": [{"title": "1 Introduction", "content": "Vector maps are an avant-garde representation of geographical data that transcend traditional mapping approaches. They possess unparalleled versatility, enabling the storage and depiction of discrete data boundaries, such as intricate building footprints, precise disaster impact analysis, meticulous urban planning, crucial location points, interconnected transport links, and beyond. By seamlessly integrating comprehensive information, vector maps transcend the limitations of conventional cartographic methods, empowering diverse domains with enhanced spatial understanding and decision-making capabilities.\nThe innovative potential of vector maps lies in their ability to encapsulate complex spatial features with utmost accuracy and fidelity. Through the intelligent manipulation of geometric primitives, including points, lines, and polygons, vector maps unlock a realm of possibilities for precise data modeling and analysis. Sophisticated algorithms and techniques facilitate the seamless conversion of raw geographical data into visually captivating and dynamically interactive vector representations, enabling efficient exploration, manipulation, and dissemination of spatial information. One of the key advantages of vector maps is their adaptability across various domains and applications. They serve as powerful tools for urban planners, aiding in the visualization of proposed developments, zoning analysis, and infrastructure management. Emergency responders leverage vector maps to swiftly assess disaster impacts, identify affected areas, and strategize rescue and recovery efforts. Transportation planners utilize vector maps to optimize routes, analyze traffic patterns, and facilitate the efficient movement of people and goods. Moreover, vector maps find extensive utility in fields such as environmental monitoring, agriculture, logistics, and public health, catalyzing data-driven decision-making and fostering innovation."}, {"title": "2 Related Work", "content": "Over the past decade a lot of research has been done in different fields of satellite image processing and vector map generation. In this section, we have summarized these researches into two subsections. The first subsection discusses recent work in extracting features like road networks and building footprints from satellite images. The other subsection discusses the recent development in generating vector maps from satellite images using generative models."}, {"title": "2.1 Feature Extraction from Satellite Image", "content": "There has already been a substantial amount of work done in the field of extracting useful features from satellite images, such as road networks and building footprints. These features have been employed in various applications, including urban planning and disaster response. The idea of using artificial neural networks to extract information from satellite images was proposed as early as 2007 by the authors of Mokhtarzade and Zoej [2007]. Since then, significant advancements have been made in this domain. A survey conducted by Chen et al. [2022] identified that in the last decade, numerous methods based on convolutional neural networks (CNN), fully convolutional networks (FCN), and U-Net architectures have been developed to extract valuable information from satellite images.\nIn Xu et al. [2023], the authors utilized the D-LinkNet architecture to compare the effects of different loss functions on the accuracy of models for extracting road networks from satellite images. This model was chosen for its efficiency in extracting high-level information due to its encoder-decoder structure, residual blocks, and skip connections. Additionally, in research Alsabhan and Alotaiby [2022], the authors proposed a U-Net architecture with ResNet50 and compared it with other state-of-the-art models, including U-Net and Deeplabv3. They discovered that the U-Net architecture with ResNet50 provided a higher understanding of building structures and yielded better results than many existing models. While these features offer valuable information about the terrain from satellite images, obtaining a complete layout of the terrain remains a significant challenge."}, {"title": "2.2 Image-to-Image Translation", "content": "Building on the limitation of getting more comprehensive terrain information, another field of active research is in image-to-image translation. This include the use of machine learning and deep learning based algorithms, with deep learning algorithms outperforming machine learning algorithms. The deep learning based approach mainly includes"}, {"title": "3 Methodology", "content": "In this paper, we propose a novel architecture for translating satellite images to vector maps termed HPix (short for HierarchicalPix). This architecture comprises two GAN frameworks, one at global level and other at local level, together forming a hierarchical model, as shown in figure 1. The GAN network at global level generates a coarse representation of the vector map from the input satellite image, capturing the overall layout and structure of the map. Then the GAN network at local level takes the coarse representation and the satellite image input to generate a refined version of the vector map, capturing fine-grained details and features. The local level generator also helps in reducing the artifact formation in the generated image.\nThe global GAN architecture comprises two components, generator and discriminator. The generator at global level comprises a complex network of encoder and decoder inspired from Unet++ architecture Zhou et al. [2018], while for discriminator we used the traditional PatchGAN network, introduced in Isola et al. [2017], with slight modification in its CNN Block. The local GAN architecture also comprises two components, a generator and a discriminator. While the local discriminator is identical as global discriminator, for local generator we are using modified Pix2Pix architecture Isola et al. [2017] which takes our original image along with global generated image as input to give final generated image. More detail about generator and discriminator are explained in the following subsections."}, {"title": "3.1 Global Generator", "content": "Authors of the paper Isola et al. [2017] explained how the use of skip connection, inspiring from Unet Ronneberger et al. [2015], improved the output of their generator model and with that intuition we worked on improving the connection network of the generator. This generator design was inspired from Unet++ architecture Zhou et al. [2018]. In this architecture apart from our standard encoder-decoder network we have introduced transition blocks which take encoded data from lower level and decode them and combines that information with information from other blocks at the same level and encodes it again before passing that information further. We have also applied deep supervision to further stabilize the output of the model."}, {"title": "3.2 Local Generator", "content": "The local generator of HierarchicalPix follows a modified architecture of Pix2Pix Isola et al. [2017]. It takes two inputs, a generated output of the global generator and our original input (satellite image). We identified that the use of a local generator helps in repatching some of the artifacts formed by the global generator thus improving the final output quality."}, {"title": "3.3 Global and Local Discriminator", "content": "For the discriminator network, we used a 26x26 PatchGAN described in Isola et al. [2017]. Both the discriminator networks, global and local, are identical to each other and are used to identify real and fake images for global and local generator respectively."}, {"title": "3.4 Objective Function", "content": "In this approach, both our GAN models are conditional GANs and the objective of a conditional GAN can be expressed as:\n$L_{CGAN} (G, D) = E_{x,y} [log D(x, y)] + E_{x,z} [1 - log D(x, G(x, z))]$ \nwhere G tries to minimize this objective against an adversarial D and D tries to maximize this objective against an adversarial G. So, for GAN at global level the objective function can be formulated as:\n$L_{global}(G, D_G) = E_{x,y} [log D_G(x, y)] + E_{x,z}[1 \u2013 log D_G(x, G(x, z))]$ \nwhere G is the generator at global level, $D_G$ is the discriminator at global level, x is the input image, y is the ground truth or target image and z is the random noise vector. And for GAN at local level the objective function can be formulated as:\n$L_{local}(H, D_H) = E_{x,y} [log D_H (x, y)] + E_{x,z} [1 - log D_H (x, H(x, G(x, z), z))]$\nwhere H is the generator at local level, G is the generator at global level, $D_H$ is the discriminator at local level, x is the input image, y is the ground truth or target image and z is the random noise vector. Furthermore, mixing the generator objective with a traditional loss like L1 distance helps generator to not just fool the discriminator but also bring the generated output near to ground truth and generate less blurry output as highlighted in results of Isola et al. [2017].\n$L_{L1}(G) = E_{x,y,z} [||y \u2013 G(x, z) ||]$\n$L_{L1}(H) = E_{x,y,z} [||y \u2013 H(x, G(x, z), z) ||]$\nOur final objective functions are:\n$L_{global (G, DG)} = \\underset{G}{\\text{arg min}} \\underset{DG}{\\text{max}} L_{global}(G, DG) + \\lambda L_{L1}(G)$\n$L_{local(H, DH)} = \\underset{H}{\\text{arg min}} \\underset{DH}{\\text{max}} L_{local}(H, DH) + \\lambda L_{L1}(H)$"}, {"title": "4 Experimental Setup and Analysis", "content": ""}, {"title": "4.1 Dataset Acquisition and Pre-processing", "content": "We started with training the proposed network on publicly available maps dataset by Isola et al. [2017] and has also been used by authors of Zhu et al. [2017], Liu et al. [2021], Song et al. [2021] for training and testing their approaches. This dataset was collected from Google Maps and contains 1096 paired satellite and vector tile map images for training and 1098 paired satellite and vector tile map images for testing. \nFor training and testing we resize the satellite and vector map image from 600x600 to 256x256 image. For training we also applied random jittering by first resizing the image to 286x286, then random cropping back to 256x256 sized image followed by horizontal flipping with a 50% probability. We have also normalized both satellite and vector map images before training and testing."}, {"title": "4.2 Analysis", "content": "To compare the effectiveness of our approach with other methods, we compared them on pixel level accuracy, PSNR score and SSIM score. For calculating pixel level accuracy we considered the error factor of 5 because colors may seem similar but may vary slightly at pixel level and this strategy have been adopted in Liu et al. [2021], Song et al. [2021], Fu et al. [2019] to efficiently measure this accuracy. \nTable 1 shows the comaprision of our approach with other methodologies. In our comparision, we considered Pix2Pix Isola et al. [2017] and CycleGAN Zhu et al. [2017] as our baseline. Furthermore, we also included algorithms from recent research including CscGAN Liu et al. [2021] and MapGen-GAN Song et al. [2021]. From the experimental analysis we concluded that our approach performs better on most of the metrics when compared with other models and the PSNR score of our approach is almost comparable to CscGAN (current best). Figure 6 shows the visual comparison between output generated by PixPix, CycleGAN and our approach as compared to ground truth. Figure 7 displays how using the local generator helped in patching up the artifacts generated by the global generator and improving the overall output quality."}, {"title": "5 Use cases", "content": "We further extended the study to include mapping of road intersections and building clusters, from the satellite image to generated vector map. We accomplish this by first segmenting road networks and building footprints from the satellite image. This is then followed by running road intersection detection and building cluster identification algorithm, as described in Appendix C. After identification of road intersections and building clusters based on area, we combine the generated vector map with these information to generate interactive vector maps. The satellite images have resolution of 1 meter per pixel and buildings are highlighted with area between 0 and 250 square meters as red, area between 250 and 500 square meters as green and any building with area above 500 square meters as blue. Figure 8 highlights the overall flow of the approach and figure 9 shows some sample generated from this approach."}, {"title": "6 Conclusion", "content": "In this paper, we have proposed a novel method for generating vector tile map from satellite image termed HPix. This architecture comprises of two generators, global and local, for identifying complex features in the input image and map it with ground truth. We have also found that using local generator helps in reducing the number of artifacts in the generated output, thus improving the overall generated output quality. The experimental results show that our model produces better translation results than other state-of-the-art approaches on maps dataset. We later use this generated vector map to create interactive vector map by marking road intersections and separating building clusters based on size highlighting the use cases of our method. The architectural design of our approach promotes its use as a general-purpose solutions like edges-to-photo, BW-to-color, or labels-to-street scene."}, {"title": "A Network architectures", "content": ""}, {"title": "A.1 Global generator network", "content": "The encoder block comprises a Conv-InstanceNorm-LeakyReLU layer. The first encoder block ($x_{0,0}$) doesn't apply InstanceNorm to its convoluted output and the bottleneck encoder block ($x_{7,0}$) doesn't apply InstanceNorm and LeakyReLU to its convoluted output. Our decoder block comprises a ConvTranspose-InstanceNorm-ReLu layer which is followed by a dropout layer with 50% probability. The final decoder block ($x_{0,7}$) doesn't apply InstanceNorm and LeakyReLU to its convoluted output, instead just applies Tanh. We have used skip connections to pass the feature map information between encoder, decoder and transition blocks. While applying deep supervision, output from blocks $x_{0,1}$, $x_{0,2}$, $x_{0,3}$, $x_{0,4}$, $x_{0,5}$ and $x_{0,6}$ are first passed through a convolution layer followed by Tanh and are then considered for calculating loss.\nIn this network, we have used an instance norm layer rather than a batch norm layer and applied reflection padding to reduce the appearance of artifacts in the generated output Zhu et al. [2017]."}, {"title": "A.2 Local generator network", "content": "The first encoder block ($x_{0,0}$) doesn't apply InstanceNorm to its convoluted output and the bottleneck encoder block ($x_{7,0}$) doesn't apply InstanceNorm and LeakyReLU to its convoluted output. The final decoder block ($x_{0,1}$) doesn't apply InstanceNorm and LeakyReLU to its convoluted output, instead just applies Tanh. We have used skip connections to pass the feature map information between encoder, decoder."}, {"title": "A.3 Discriminator network", "content": "The discriminator blocks consist of Convolution-InstanceNorm-LeakyReLU layers. The first layer (block 1) doesn't have an InstanceNorm layer and we used LeakyReLU with a slope of 0.2. The last block, block 5, doesn't have an InstanceNorm and LeakyReLU layer and applies convolution with stride 1. The output from block 5 is a single channel output."}, {"title": "B Experimental setups", "content": ""}, {"title": "B.1 Computing resources", "content": "The training of models were performed on the Kaggle platform by using community-available two Nvidia Tesla T4 GPUs with 13 GB RAM and 2 CPU cores. It took around 16 hours to train the model and around 10 minutes to validate the trained model. The code was written using Pytorch library in python."}, {"title": "B.2 Hyperparameters and training conditions", "content": "While training HPix, we have trained both generators simultaneously so that they could learn and generalize the problem together. We used Adam optimizer for both the generators and discriminators with a learning rate of 0.0002 and betal and beta2 as 0.5 and 0.999. We trained the model on objective function defined in methodology for 200 epochs."}, {"title": "C Use cases", "content": ""}, {"title": "C.1 Road network and intersection", "content": "For extracting road network from the satellite image, we used a pre-trained DLinkNet model trained on DeepGlobe Road Extraction Dataset Demir et al. [2018]. The generated binary segmented map of the road network is then used with algorithm 1 for identifying road intersections. Figure 10 show samples of identified road network and their intersections."}]}