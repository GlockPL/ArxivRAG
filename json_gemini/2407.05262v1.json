{"title": "FastSpiker: Enabling Fast Training for Spiking Neural Networks on Event-based Data through Learning Rate Enhancements for Autonomous Embedded Systems", "authors": ["Iqra Bano", "Rachmad Vidya Wicaksana Putra", "Alberto Marchisio", "Muhammad Shafique"], "abstract": "Autonomous embedded systems (e.g., robots) typically necessitate intelligent computation with low power/energy processing for completing their tasks. Such requirements can be fulfilled by embodied neuromorphic intelligence with spiking neural networks (SNNs) because of their high learning quality (e.g., accuracy) and sparse computation. Here, the employment of event-based data is preferred to ensure seamless connectivity between input and processing parts. However, state-of-the-art SNNs still face a long training time to achieve high accuracy, thereby incurring high energy consumption and producing a high rate of carbon emission. Toward this, we propose FastSpiker, a novel methodology that enables fast SNN training on event-based data through learning rate enhancements targeting autonomous embedded systems. In FastSpiker, we first investigate the impact of different learning rate policies and their values, then select the ones that quickly offer high accuracy. Afterward, we explore different settings for the selected learning rate policies to find the appropriate policies through a statistical-based decision. Experimental results show that our FastSpiker offers up to 10.5x faster training time and up to 88.39% lower carbon emission to achieve higher or comparable accuracy to the state-of-the-art on the event-based automotive dataset (i.e., NCARS). In this manner, our Fast-Spiker methodology paves the way for green and sustainable computing in realizing embodied neuromorphic intelligence for autonomous embedded systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous embedded systems with tightly-constrained resources (e.g., robots, UAVs, and UGVs) typically require intelligent yet low power/energy processing for completing their data analytic tasks [1] [2], such as object recognition, navigation, obstacle avoidance, and user assistance. These requirements can be satisfied by embodied neuromorphic artificial intelligence (AI) with spiking neural networks (SNNs), because SNNs offer high learning quality (e.g., accuracy) and sparse event-based computation [1]\u2013[3]. Therefore, the employment of event-based data (e.g., gesture dataset [4] and cars dataset [5]) is favorable to ensure seamless interfacing from input to processing parts [2] [6].\nCurrently, to achieve high accuracy in solving the given task, the existing SNN models still require a significantly long training time to adjust and fine-tune their parameter values. For instance, the state-of-the-art SNNs for processing event-based automotive data (i.e., NCARS dataset) [7] [8]\nrequire approximately 9 hours to complete the training phase with 200 epochs using the Nvidia RTX 6000 Ada GPU machines and achieve ~85% accuracy, as shown in Fig. 1. Such a long training time incurs huge energy consumption and emits a large amount of carbon, which can negatively impact our environments [9]\u2013[11]. Moreover, a long training time also hinders the possibility of performing an efficient online training for updating the systems' knowledge at run-time in some application use-cases, such as systems that consider a continual learning scenario [2] [12] [13].\nThe above discussion highlights the need for an efficient solution that enables fast SNN training while maintaining its high learning quality (i.e., accuracy). Therefore, in this paper, the targeted research problem is how we can reduce the SNN training time while maintaining high accuracy. An efficient solution to this problem may enable a fast and effective SNN training, and thereby reducing carbon emission for green and sustainable environments."}, {"title": "A. State-of-the-art and Their Limitations", "content": "To minimize the training time of SNNs, the state-of-the-art work employed a direct timestep reduction with scaled parameter values during the training phase, thus curtailing the processing time of neurons while compensating the reduced spiking activity for maintaining the learning quality [14]. However, this work has not considered event-based data as input, thus limiting its energy efficiency gains and straightforward connectivity to event-based sensors, such as dynamic vision sensor (DVS) cameras. Besides such techniques, the reduction of SNN training time has not been explored further. Therefore, there is a significant need for investigating an alternate solution that can facilitate event-based input data for direct interfacing with event-based sensors.\nIn conventional deep neural networks (DNNs), different learning rate (LR) policies have been explored to reduce their training time, such as exponential decay, one-cycle policy [15], cyclical policy [16], and warm restarts [17] [18]."}, {"title": "B. Motivational Case Study", "content": "We conduct a case study to investigate the impact of different LR policies on the accuracy. To do this, we consider two different LR policies, i.e., decreasing step and warm restarts, as shown in Fig. 2(a) and Fig. 2(b), respectively. For the network model, we consider a state-of-the-art SNN for the NCARS dataset from [7] with 200 training epochs. Note, the details of LR policies are discussed in Section II-B, while the experimental setup details are provided in Section IV. The experimental results are shown in Fig. 2(c), from which we make several key observations, as follows.\n\u2022 Different LR policies may need different numbers of training epochs to reach stability in their learning curves. For instance, the decreasing step policy achieves stable accuracy scores (i.e., having less than 1% of standard deviation) after 77 training epochs, while the warm restarts policy achieves this faster with 50 training epochs; see A.\n\u2022 Different initial LR values may lead to different responses in terms of learning curves at the early training phase. For instance, the decreasing step policy faces significant fluctuation\u00b9 at the early training phase, while the warm restarts policy faces less fluctuation; see B. This insight is essential for devising a policy that can train the SNNs faster.\nThese observations indicate that there is an opportunity to improve the training time of SNNs on event-based data by employing an effective LR policy. However, developing such a policy is a non-trivial task, as it imposes the following research challenges."}, {"title": "C. Our Novel Contributions", "content": "To solve the above research challenges, we propose Fast-Spiker, a novel methodology that enables Fast training for Spiking neural networks on event-based input data through learning rate enhancements; see an overview in Fig. 3. The key steps of our FastSpiker methodology are the following.\n\u2022 Determining the effective range of LR values (Section III-A): It finds the effective range of LR to learn the given event-based data through experiments that explore the impact of different LR values using the baseline policy on the accuracy.\n\u2022 Evaluating the impact of different LR policies (Section III-B It evaluates the impact of different widely-known LR policies (e.g., decreasing step, exponential decay, one-cycle, cyclical, decreasing cyclical, and warm restarts) for learning event-based data, then select the ones that offer high accuracy.\n\u2022 Exploring the settings for the selected LR policies (Section III-C): It explores different possible settings for the selected LR policies to find the appropriate settings through a statistical variance-based decision.\nKey Results: To evaluate our FastSpiker methodology, we employ Python-based implementations considering NCARS dataset, then run them on the Nvidia RTX 6000 Ada machines. Experimental results show that our FastSpiker can expedite training time by up to 10.5x and reduce carbon emission by up to 88.39% for achieving higher or comparable accuracy scores than the state-of-the-art."}, {"title": "II. BACKGROUND", "content": "Modeled after the neural process of the human brain, spiking neural networks (SNNs) adopt how neurons transmit spikes for data communication and computation [19]. This enables SNNs to excel in scenarios where power/energy efficiency and real-time processing are paramount, such as autonomous embedded systems (e.g., robotics, UAVs, and UGVs). SNNs receive spike sequences as input, and process the information using spiking neuron models. One of the most widely-used neuron model in the SNN community is"}, {"title": "A. Spiking Neural Networks (SNNs)", "content": "the Leaky Integrate-and-Fire (LIF), whose neuronal behavior can be stated as follows.\n$\\frac{dV_m}{dt} = \\frac{1}{\\tau} (- (V_m - V_r) + RI)$\nif $V_m \\geq V_{th}$ then $V_m \\leftarrow V_r$\n(1)\n$V_m$, $V_r$, and $V_{th}$ denote the neurons' membrane, reset, and threshold potentials, respectively. $R$ and $I$ denote the input resistance and current (spike), respectively. Meanwhile, $\\tau$ denotes to the time constant of $V_m$ decay. In this work, the SNN architecture is based on studies in [7] [8] with an input sample with 100x100 pixels.\nTraining SNNs in a supervised setting faces non-differentiable issues when computing the loss function [20]. To overcome this, two possible supervised-based training solutions have been proposed in the literature: DNN-to-SNN conversion and direct SNN training with surrogate gradient [2]. Previous works showed that the DNN-to-SNN conversion offers sub-optimal performance compared to training directly in the spiking domain via a surrogate gradient-based learning rule [21] [22]. In this category, Spatio-Temporal Back-Propagation (STBP) [23] is a prominent technique, hence in this work, we employ the STBP for the selected SNN architecture."}, {"title": "B. Learning Rate Policies", "content": "Various LR policies have been proposed in the conventional DNN domain, such as the decreasing step, exponential decay, one-cycle, cyclical, decreasing cyclical, and warm restarts policies [15]\u2013[18]. These investigated LR policies are illustrated in Fig. 4, and their descriptions are provided in the following.\n1) Decreasing Step: It optimizes SNNs by gradually reducing LR values during the training phase in the step fashion, as shown in Fig. 4(a). In this work, LR decreases by a factor of 0.5 every 20 epochs following [7] [8]. This systematic reduction is to fine-tune the parameters, improving convergence over time.\n2) Exponential Decay: It optimizes SNNs by gradually reducing LR values during the training phase every epoch, as shown in Fig. 4(b). In this work, LR decreases by a factor of 0.98 every epoch. This systematic reduction is also expected to fine-tune the parameters, improving convergence over time smoothly.\n3) One-Cycle: It optimizes SNNs by realizing a single cycle of LR values, where the LR increases to the maximum value, and then decreases back to the minimum one [15]; see Fig. 4(c). In this work, the initial LR is set as 1e-5, then it increases to 1e-2 by the 100th epoch. From the 100th to the 180th epoch, it linearly reduces the rate back to 1e-5 for fine-tuning. Then, the last 20 epochs are dedicated for stabilizing the accuracy at a low LR.\n4) Cyclical: It optimizes SNNs by realizing cyclical LR values that oscillate within upper and lower bounds, aiding in escaping local minima for better solutions [16]; see Fig. 4(d). Practically, with a minimum LR of 1e-5 and a maximum LR of 1e-2, and the number of cycles can be set to 4. Within each cycle, the LR linearly increases from the minimum value to the maximum one during the first half (25 epochs), then decreases back to the minimum value during the second half (another 25 epochs).\n5) Decreasing Cyclical: It optimizes SNNs by realizing cyclical LR values that oscillate between pre-defined maximum and minimum values over several cycles, with the maximum LR gradually decreases with each cycle; see Fig. 4(e). Practically, the maximum LR for the next cycle is 0.9x of the current maximum LR value. This progressive reduction helps in fine-tuning the model.\n6) Warm Restarts: It optimizes SNNs by periodically resetting LR to a high value after it decreases over a number of epochs, enforcing a fresh restart that helps escaping a local minima to find a global minima [17] [18]; see Fig. 4(f). Practically, with a minimum LR of 1e-5 and a maximum LR of 1e-2, the training phase can be divided into several cycles with different lengths."}, {"title": "C. Event-based Data", "content": "In this work, we consider the Prophesees' NCARS dataset [5]. This dataset consists of about 24K samples, each having a 100ms duration. It contains either \"car\" or \"background\" samples, which are captured through recordings using the event-based camera [5]; see Fig. 5. Each input sample has a sequence of events that contain information of the spatial coordinates of the pixel, the timestamp of the event, and the polarity of the brightness which is either positive or negative. In this work, we employ 100 \u00d7 100 pixels since it is sufficient to provide important information yet small in size [8]."}, {"title": "D. Carbon Emission of NN Training", "content": "Recent works studied that training neural networks (NNs) emits a huge amount of carbon, hence raising environmental concerns that increases the rates of natural disasters [9] [10]. Hence, reducing the NN training time can minimize the carbon emission, preserving green and sustainable environments [11]. Previous work [9] proposed Eq. 2-3 to estimate the carbon emission from NN training.\n$CO_{2e} = 0.954 \\cdot P_{train} \\cdot t$ (2)\n$P_{train} = 1.58 \\cdot \\frac{(P_{cpu} + P_{mem} + g \\cdot P_{gpu})}{1000}$ (3)\nHere, $CO_{2e}$ is the estimated carbon emission (i.e., $CO_2$), which is a function of the total training power ($P_{train}$) and the training duration (t). Meanwhile, $P_{cpu}$ represents the average power from CPUs, $P_{mem}$ represents the average power from main memories (DRAMs), $P_{gpu}$ represents the average power from a GPU, and g represents the number of GPUs for training."}, {"title": "III. FASTSPIKER METHODOLOGY", "content": "FastSpiker methodology aims to systematically find the effective LR policies that can lead the given SNN to quickly learn from event-based input data, thereby achieving high accuracy within a relatively short training time. Our Fast-Spiker employs several steps as shown in Fig. 6, which are discussed in Sections III-A-III-C."}, {"title": "A. Determination of the Effective Range of LR Values", "content": "Each LR policy requires a pre-defined range of values to tailor its behavior, in the manner that the respective LR policy makes the network effectively learn the event-based input data. To determine the appropriate range of values, a simple yet effective way is by performing observations through network training that considers different LR values from small to large ones (i.e., 1e-6, 5e-6, 1e-5, 5e-5, 1e-2, 5e-2, and 1e-1), and the results are shown in Fig. 7.\nThese results show that the LR value should not be too small (e.g., 1e-6) or too big (i.e., 1e-1), since they lead to ineffective learning process. Therefore, in this work, we consider the range of LR values between 1e-5 and 1e-2."}, {"title": "B. Evaluation of the Impact of Different LR Policies", "content": "This step aims at evaluating the impact of different LR policies for learning event-based data, while employing the selected range of values. To do this, we first perform network training for each investigated LR policy on the NCARS dataset. Afterward, policies that offer high accuracy will be selected as the potential solutions. To select the appropriate solutions, we employ a statistical-based decision to determine if the training phase is considered complete and can be terminated. Here, we consider the training phase complete when the accuracy scores do not exceed the defined stability threshold value (accth) anymore, thereby indicating that the learning curves have saturated and reached stability. Specifically, we set the accth as 1% standard deviation of the accuracy scores from the last 10 training epochs, following the criteria in [11].\nIn this work, we evaluate the impact of well-known LR policies, including the decreasing step, exponential decay, one-cycle, cyclical, decreasing cyclical, and warm restarts. Here, we employ the following parameter settings for all investigated LR policies:\n\u2022 Initial learning rate (lr) = 1e-3\n\u2022 Batch size = 40 samples\n\u2022 Neurons' membrane threshold potential (Vth) = 0.4\n\u2022 Details of the experimental setup are provided in Section IV\nMeanwhile, other settings that are applicable to specific LR policies follow the pseudo-codes in Alg. 1-Alg.6."}, {"title": "Algorithm 1 Decreasing Step LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), reduction factor (r_f = 0.98), reduction interval (r_int = 20), #epochs (epochs = 200);\n1: cur lr \u2190 init // initialization\n2: for ep 1 to epochs do\n3: if ep % r_int == 0 then\n4: cur_lr \u2190 cur_lr \u00d7 r_f\n5: Update cur_lr in optimizer\n6: return Updated optimizer"}, {"title": "Algorithm 2 Exponential Decay LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), #epochs (epochs = 200), decay rate (d_rate = 0.98), decay steps (d_steps = 1);\n1: lr init // initialization\n2: for ep 1 to epochs do\n3: Ir \u2190 init \u00d7 d_rate(d.steps)\n4: Update Ir in optimizer\n5: return Updated optimizer"}, {"title": "Algorithm 3 One-Cycle LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), #epochs (epochs = 200), peak epoch (p-ep = 90), drop epoch (d_ep = 180), start LR (start = 1e-5), max LR (max = 1e-2), min LR (min = le-5), end LR (end = 1e-8);\n1: Ir init // initialization\n2: for ep = 1 to epochs do\nIr start + (max-start) x ep\n3: if ep < p_ep then\n4:\n5: else if ep < d_ep then\n6: Ir max\n(max-min)\n(d_ep-p_ep)\n\u00d7 (ep - p-ep)\n7: else\n8: Ir min\n(min-end)\n(epochs-d-ep) \u00d7 (ep \u2013 d-ep)\n9: Update Ir in optimizer\n10: return Updated optimizer"}, {"title": "Algorithm 4 Cyclical LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), #epochs (epochs = 200), min LR (min = le-5), max LR (max = 1e-2), half cycle (h_cycle = 25);\n1: Ir init // initialization\n2: for ep=1 to epochs do\n3: cycle-pos ep%(h_cycle \u00d7 2)\n4: if cycle-pos < h_cycle then\n(\u0442\u0430\u0445-min) \u00d7 cycle-pos\n5: Ir min +\nh_cycle\n6: else\n(\u0442\u0430\u0445-min)\u00d7(cycle-pos-h-cycle)\n7: Ir max\nh_cycle\n8: Update Ir in optimizer\n9: return Updated optimizer"}, {"title": "Algorithm 5 Decreasing Cyclical LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), number of epochs (epochs =\n200), max LR (max = 1e-2), min LR (min = 1e-5), cycle length\n(c_length = 40);\n1: Ir init // initialization\n2: for ep 1 to epochs do\nep\n3: N_cycles \u2190 c_length\n4:\nIr_dec\u2190 (max-min)\nc_length\n5: cur_max \u2190 \u0442\u0430\u0445 (Ir_dec \u00d7 N_cycles)\n6: cur-prog\u2190 epc_length\nc_length\n7: Ir cur_max (cur-max - min) \u00d7 cur-prog\n8: Ir \u2190 max(lr, min)\n9: Update Ir in optimizer\n10: return Updated optimizer"}, {"title": "C. Exploration of the Settings for Selected LR Policies", "content": "To quickly maximize the learning quality (i.e., accuracy), the selected LR policies (i.e., exponential decay and warm restarts) require to employ effective parameter settings, such as the initial learning rate (init_lr), batch size (B), and neurons' threshold potential (Vth). However, finding the appropriate values is also a non-trivial task. Toward this, we explore different combinations of potential values for the investigated parameters. Its key idea is to vary these values (i.e., especially for B and Vth as summarized in Table I) and observe their impact on accuracy. Here, WR_XP refers to the"}, {"title": "Algorithm 6 Warm Restarts LR Policy", "content": "Require: optimizer, initial LR (init = 0.1), number of epochs (epochs =\n200), maximum initial period (Tmax = 4), min LR (min = 1e-5), period\nmultiplier (Tmult = 2);\n1: lr init // initialization\n2: for ep 1 to epochs do\n3: Ti Tmax\n4: tcur\u2190 ep\n5: for i = 0 to ep do\n6: if tour Ti then\n7: break\n8: tcurtcur - Ti\n9: Ti Ti Tmult\nTi\n10:\nlr \u2190 min + (lr \u2013 min) \u00d7 0.5 \u00d7 (1 + cos(x tour))\n11: Update lr in optimizer\n12: return Updated optimizer"}, {"title": "IV. EVALUATION METHODOLOGY", "content": "Fig 9 shows the experimental setup for evaluating our FastSpiker methodology. We employ a Python-based implementation that runs on Nvidia RTX 6000 Ada GPU machines, and the generated outputs are the accuracy and the log of experiments, including accuracy, processing time, and power consumption of the SNN training. Additional experimental logs like GPU information (e.g., number of devices) are also recorded for estimating the carbon emission. For this calculation, we employ Eq. 2-3 and focus on the carbon emission from GPU processes. For the network, we employ the SNN architecture with the STBP learning rule and 200 epochs for a training phase, following [7]. Here, the NCARS dataset is used as the workload. Furthermore, we consider the state-of-the-art LR policy for event-based data (i.e., decreasing step with init_lr=1e-3, B=40, and Vth=0.4) as the comparison partner. For brevity, this policy is referred to as \"SOTA_DS\". Furthermore, we set the accth as 1% standard deviation of the accuracy from the last 10 training epochs, for evaluating the stability of the learning curves."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "The experimental results for observing accuracy comparisons between the state-of-the-art (SOTA_DS) with our proposed exponential decay and warm restarts policies are provided in Fig. 10(a) and (b), respectively."}, {"title": "A. Accuracy Profiles and the Reduction of Training Time", "content": "SOTA DS vs. Our Exponential Decay: SOTA_DS can achieve 84.9% accuracy after 200 training epochs; see 1. If we consider the defined accth, then SOTA_DS reaches stability after 77 training epochs with 84.7% accuracy. Meanwhile, our proposed exponential decay policy can achieve stability faster as compared to SOTA_DS across different investigated settings, i.e., up to 10.5x compared to SOTA_DS with full training phase and up to 4x compared to SOTA DS with first stable accuracy; see 2 and Table II. Besides faster stability, all investigated settings for our exponential decay also lead to comparable accuracy (and better accuracy in some cases) than SOTA_DS. This highlights that our exponential decay policy is effective for learning event-based NCARS dataset. The reason is that, this policy forces the network to start learning with high LR value (high confidence), then gradually reducing LR for fine-tuning, while considering a selected range of LR values. Hence, a high LR value in the beginning of the training phase can quickly lead to a global minima as there are only 2 classes to learn from the NCARS dataset. Consequently, LR policies that do not start with high LR values usually have significant fluctuation in the early training phase as they are more easily trapped in the local minima; as shown in Fig. 8(c)-(e)."}, {"title": "SOTA DS vs. Our Warm Restarts", "content": "Our warm restarts policy can achieve stability faster with comparable/higher accuracy than SOTA_DS with full training phase across different investigated settings, i.e., up to 6.2x; see Table III. Meanwhile, when compared to SOTA DS with first stable accuracy, our warm restarts policy can achieve stability faster with comparable accuracy in some cases (i.e., up to 2.4x faster training; see 3), as well as slower yet higher accuracy in other cases (i.e., up to 85.8% accuracy; see 4). The reason is that, warms restarts policy has periodic LR changes, hence having more variability when learning the input features. For instance, a high LR value in the beginning of the training phase can quickly lead to a global minima as there are only 2 classes to learn from the NCARS dataset. Then, decreasing LR helps in the fine-tuning process. However, in some cases, resetting the LR to a high value and decreasing it fast to a low value may make the optimization sub-optimal and trapped in the local minima, thereby making the network learn slower than SOTA DS."}, {"title": "B. Reduction of the Carbon Emission", "content": "Fig. 11 shows the results of carbon emission comparison across different LR policies, The results show that, our proposed LR policies (i.e., exponential decay and warm restarts)"}, {"title": "VI. CONCLUSION", "content": "We propose a novel FastSpiker methodology to enable fast SNN training on event-based data through learning rate enhancements. It investigates the impact of different LR policies, then explores different settings for the selected LR policies to find the appropriate policies through a statistical-based decision. Experimental results show that our Fast-Spiker can achieve up to 10.5x faster training time and up to 88.39% lower carbon emission on the event-based automotive data, hence initiating the way for green and sustainable computing in realizing embodied neuromorphic intelligence for autonomous embedded systems."}]}