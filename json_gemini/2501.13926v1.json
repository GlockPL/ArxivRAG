{"title": "Can We Generate Images with CoT?\nLet's Verify and Reinforce Image Generation Step by Step", "authors": ["Ziyu Guo", "Renrui Zhang", "Chengzhuo Tong", "Zhizheng Zhao", "Peng Gao", "Hongsheng Li", "Pheng-Ann Heng"], "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle com-\nplex understanding tasks. However, it still remains an open question whether such strategies can be\napplied to verifying and reinforcing image generation scenarios. In this paper, we provide the first\ncomprehensive investigation of the potential of CoT reasoning to enhance autoregressive image gen-\neration. We focus on three techniques: scaling test-time computation for verification, aligning model\npreferences with Direct Preference Optimization (DPO), and integrating these techniques for com-\nplementary effects. Our results demonstrate that these approaches can be effectively adapted and\ncombined to significantly improve image generation performance. Furthermore, given the pivotal role\nof reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and\nPARM++, specialized for autoregressive image generation. PARM adaptively assesses each genera-\ntion step through a potential assessment approach, merging the strengths of existing reward models,\nand PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory\nimage. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve\nsuperior results, with a significant +24% improvement on the GenEval benchmark, surpassing Sta-\nble Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for\nintegrating CoT reasoning with autoregressive image generation. Code and models are released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) and Large\nMulti-modal Models (LMMs) have gained\nremarkable achievements across language, 2D image, temporal video, and 3D\npoint cloud. Building upon general under-\nstanding skills, recent efforts have been made\ntoward enhancing LLMs and LMMs with com-\nplex Chain-of-Thought (CoT) reasoning capabili-\nties, e.g., OpenAI 01 , contributing\nto superior performance in mathematics, science, and coding.\nDespite the success in multi-modal under-\nstanding, it remains under-explored whether\nmulti-step reasoning strategies can be effectively\napplied to image generation. Considering the dis-\ncrepancy between two tasks, we observe that,\nautoregressive image generation shares a\nsimilar output manner to the nature of LLMs and\nLMMs. Specifically, they both quantize the target\ndata (language and image) into discrete tokens,\nand iteratively predict partial content conditioned\non previously generated tokens.\nAs illustrated in Figure 1, LMMs leverage CoT\nto break down complex mathematical problems\ninto manageable steps, which enables scaling test-\ntime computation with reward models and reinforcement learning for preference align-\nment. Likewise, autoregressive image\ngeneration through step-by-step decoding can pro-\nduce intermediate images, potentially allowing for\nsimilar verification and reinforcement techniques.\nThis raises the question: Can we verify and rein-\nforce image generation step-by-step with strategies\nrevealed by OpenAI 01?\nTo this end, we conduct a systematic inves-\ntigation into the potential of CoT reasoning for\nautoregressive image generation. We adopt Show-o [5], a latest discrete generative model, as our\nbaseline, and evaluate on a challenging text-\nto-image generation benchmark: GenEval ."}, {"title": "2 Our Investigation", "content": "Chain-of-Thought (CoT) reasoning has been\nwidely exploited to solve complex problems for\nlanguage and multi-modal understanding. In this\nstudy, we conduct a systematic investigation aim-\ning to find out, whether we can verify and reinforce\nimage generation step-by-step."}, {"title": "2.1 Overview", "content": "Task Formulation.\nTo enable the applicability of current reason-\ning techniques, we focus on the autoregressive\nimage generation task, demonstrated by mod-\nels such as MaskGiT and LlamaGen . This task employs a data representation and out-\nput paradigm akin to those used in LLMs and\nLMMs, while achieving comparable performance\nto continuous diffusion models . Specifi-\ncally, it leverages quantized autoencoders to\ntransform images into discrete tokens, allowing\nfor the cross-entropy loss of Direct Preference\nOptimization (DPO) in post-training. Addi-\ntionally, it iteratively predicts one or more tokens\nat each step, conditioned on prior outputs, thereby\ncreating reasoning paths suitable for step-wise\nverification with reward models.\nExperimental Settings.\nWe select Show-o [5] as our baseline model for\ninvestigation, a latest autoregressive image gen-\neration model with advanced capabilities. To\ncomprehensively evaluate different strategies, we\nassess the text-to-image generation performance\non a rigorous benchmark: GenEval . This sce-\nnario challenges the model to produce images"}, {"title": "2.2 ORM vs PRM as Test-time\nVerifiers", "content": "Scaling test-time computation to\nenhance reasoning capabilities has emerged as\nan effective alternative to scaling training costs.\nCurrent approaches often employ reward mod-\nels as test-time verifiers within CoT reasoning\npaths, typically using two main categories: Out-\ncome Reward Model (ORM) and Process Reward\nModel (PRM). Drawing inspiration from these\nmethods, we respectively implement and evaluate\nthem within the context of autoregressive image\ngeneration, as illustrated in Figure 4.\nORM.\nBased on multiple complete reasoning outputs,\nORM assigns each candidate a reword score and\nselect the most confident one using a best-of-N\nstrategy. In our study, we adopt ORM solely to\nevaluate the generated image at the final step,"}, {"title": "2.3 Test-time Verifiers vs\nPreference Alignment", "content": "Post-training has been widely utilized in existing\nLLMs and LMMs to align model outputs with\nhuman preferences. Common techniques include\nreinforcement learning with reward models, e.g.,\nProximal Policy Optimization (PPO) , and\nits streamlined counterpart with classification\nobjectives, e.g., Direct Preference Optimization\n(DPO) . Given that most autoregressive image\ngeneration models inherently function within a"}, {"title": "2.4 DPO Alignment plus Test-time\nVerifiers", "content": "The investigations above have demonstrated the\nindividual effectiveness of test-time verification\nand DPO alignment. Next, we explore three\napproaches to integrate these two techniques to\nassess their complementary potential in image\ngeneration, leveraging both the adaptability of\nverifiers and the reinforcement of DPO.\nDPO with Reward Model Guidance.\nAs discussed in previous works , DPO can\nstruggle with out-of-distribution responses due\nto distribution shifts from the ranking dataset.\nA potential solution is to incorporate\nprompt-only datasets during post-training and\nleverage a reward model to provide online prefer-\nence guidance. Following this approach, we adopt\nour fine-tuned ORM as the explicit reward model\nto offer more generalized preference feedback, and\nadd the online objectives with the original DPO\nloss We maintain the same training data and\nconfigurations as the initial DPO alignment stage.\nVerification after DPO Alignment.\nWe observe that verification and DPO techniques\nmay naturally complement each other in two key\nways: 1) They operate independently at different\nstages of implementation, i.e., post-training and\ntest-time; and 2) DPO refines the internal knowl-\nedge distribution within the model to enhance\nreasoning, while verification focuses on selecting\nthe optimal reasoning path within this refined\ndistribution. Therefore, we apply the fine-tuned\nORM for best-of-N selection directly on the model\nafter DPO alignment.\nVerification after DPO with Reward\nModel Guidance.\nIn this approach, we combine the strengths of\nboth DPO with reward model guidance and test-\ntime verification. Our goal is to achieve optimal\nalignment, enhancing the model's generalization\ncapabilities during training, while also ensuring\nreliable image generation paths at inference time."}, {"title": "3 Potential Assessment\nReward Model", "content": "From our comprehensive investigation, reward\nmodels prove valuable by enabling both decod-\ning path selection and preference reward guidance.\nHowever, we still observe considerable room for\nenhancing reward models.\nLimitation of ORM and PRM.\n1) ORM showcases strong performance by select-\ning optimal final outputs, yet it lacks the capacity\nto provide fine-grained, step-wise evaluation at\neach generation step. 2) While PRM has demon-\nstrated effectiveness in understanding tasks such\nas mathematics, it is less suitable for autoregres-\nsive image generation. As analyzed in Sec. 2.2,\nPRM struggles with early-stage images that are\ntoo blurry for reliable evaluation, given that only\na few regions are decoded. In later stages, images\nderived from similar previous steps lack sufficient\ndistinction, challenging for PRM to discriminate.\nPARM.\nMotivated by these observations, we propose the\nPotential Assessment Reward Model (PARM), a\nspecialized reward model tailored for autoregres-\nsive image generation, as illustrated in Figure 4.\nPARM combines the best of both worlds: 1)\nit operates adaptively in a step-wise manner,\nusing a potential assessment mechanism to over-\ncome PRM's evaluation challenges; and 2) it\nperforms a best-of-N' selection across N' (N' <"}, {"title": "4 Potential Assessment\nReward Model ++", "content": "Besides sequential step-by-step reasoning, humans\noften engage in a reflection process to verify the\ncorrectness of their previous thoughts. To explore\nits potential in image generation, we introduce\nPARM++, as shown in Figure 5, which enhances\nPARM with a reflection mechanism to refine the\ntext-to-image quality.\nReflection in Image Generation.\nThe reflection strategy has also been recently\napplied in LLMs , resulting in improved\nperformance through self-correction. Unlike"}, {"title": "5 Conclusion", "content": "In this work, we investigate the adaption and\npotential of CoT reasoning strategies in autore-\ngressive image generation. Through a system-\natic investigation, we demonstrate that differ-\nent reasoning strategies can effectively improve\nimage generation, e.g., test-time verification, pref-\nerence alignment, and their integration. Given\nour observation, we further introduce two tai-\nlored reward models for autoregressive image\ngeneration, termed Potential Assessment Reward\nModel (PARM) and PARM++, which evalu-\nate the step-wise generation for adaptive reward\nscoring, and incorporate a reflection mechanism\nfor self-corrected image generation. Our experi-\nments underscore the promise of CoT reasoning\nin autoregressive image generation, advancing this\nfield in new directions."}, {"title": "A Related Work", "content": "Scaling Test-time Computation.\nHumans often dedicate significant time and effort\nto solve complex problems. Inspired by this,\nmany efforts have focused on scaling test-time\ncomputation for Large Language Models (LLMs)\nto tackle reasoning tasks such as mathematical\nproblem-solving, code synthe-\nsis, and workflow generation . One line of research adapts the input space\nto leverage Chain-of-Thought (CoT) capabilities,\nusing approaches like in-context CoT examples or zero-shot CoT prompts . Another branch\nmodifies or integrates reasoning paths within the\noutput space, utilizing strategies such as self-\nconsistency , CoT decoding , and verifier-\nbased selection . Among these, test-\ntime verifiers have demonstrated generality and\nrobustness in enhancing reasoning performance.\nFor example, early work trains an Outcome\nReward Model (ORM) to evaluate final outputs\nand select the best-of-N candidates for opti-\nmal results. Later, Lightman et al. adopt\nthe Process Reward Model (PRM) to evaluate\nintermediate reasoning steps, achieving greater\neffectiveness. Snell et al. further highlights\nthat scaling test-time computation is often more\nimpactful than scaling model parameters during\ntraining. Recently, OpenAI 01 has demon-\nstrated exceptional reasoning capabilities across\na variety of complex and challenging scenarios,\nunderscoring the potential of this approach. Build-\ning on these advancements in understanding tasks,\nwe conduct a comprehensive investigation into\nwhether verifier-based strategies can also enhance\nimage generation tasks, and propose a new Poten-\ntial Assessment Reward Model (PARM), specifi-\ncally designed for this domain.\nReinforced Preference Alignment.\nAfter robust pre-training and fine-tuning, LLMs\noften acquire substantial knowledge. However, a\npost-training alignment stage is typically required\nto align their output preferences to meet spe-\ncific targets, such as human feedback or Chain-of-Thought (CoT) reasoning . Traditional approaches often lever-\nage reinforcement learning (RL) to address this\nchallenge. These methods usually involve two\nsteps: first, optimizing a neural-network-based\nreward function within a preference model (e.g.,\nthe Bradley-Terry model ), and then fine-\ntuning the target LLM to maximize this reward\nusing techniques like proximal policy optimiza-\ntion (PPO) . However, RL-based methods\noften encounter issues related to complexity and\ninstability. To overcome these challenges, Rafailov"}, {"title": "B Data and Implementation\nDetails", "content": "B.1 ORM\nZero-shot ORM.\nTo implement a zero-shot ORM in image gener-\nation, we adopt a pre-trained LLaVA-OneVision\n(7B) for test-time verification. We adopt a"}, {"title": "B.2 PRM", "content": "Zero-shot PRM.\nWe also utilize the pre-trained LLaVA-OneVision\n(7B) as our zero-shot PRM, applying a similar\nprompt template used in ORM as:\nPrompt: \" {image} This is an interme-\ndiate image in the generation process by\na prompt: {prompt}. Does this intermedi-\nate image accurately represent the prompt?\nPlease answer yes or no without explana-tion.\"\nAt each intermediate step in the generation pro-\ncess, the zero-shot PRM assesses each candidate\nimage with a binary response, 'yes' or 'no'. We\nthen adopt a step-level best-of-N strategy, select-\ning the most confident candidate and following\nthis path for subsequent decoding. By iteratively\nemploying the PRM at each step, the generation\nprocess is guided step by step towards the final.\nPRM Ranking Data Curation.\nWe observe that the images generated at inter-\nmediate steps tend to appear very blurry, as\nonly partial visual tokens in specific regions are\ndecoded while others remain unresolved. Since"}, {"title": "B.3 PARM", "content": "In Figure 12, we illustrate why PRM is less\nsuitable for autoregressive image generation. As\nshown, the early-stage images are too blurry for\nreliable evaluation, given that only a few regions\nare decoded, while the later-stage images derived\nfrom similar previous steps lack sufficient distinc-\ntion, challenging for discrimination. To integrate\nthe advantage of both ORM and PRM, we propose\nPotential Assessment Reward Model (PARM) and\ncurate a new ranking dataset with 400K instances\nby re-annotating the 13K text prompts from ORM"}, {"title": "C Additional Results", "content": "Quantitative Results.\nIn Table 5, we present a comprehensive per-\nformance comparison on GenEval  between\nprevious diffusion and autoregressive models, and\nShwo-o equipped with our investigated reason-\ning strategies. Substantial improvement for text-\nto-image generation are observed using different\nreasoning techniques. With PARM, the gains in\ncomplex attributes, such as 'Two Obj.', 'Count-\ning', 'Position', and 'Attribute binding' empha-\nsize the robustness of our approach in handling\nchallenging aspects of compositional generation,\nsetting a new standard in text-to-image perfor-\nmance. In Figures 13 and 14, we present the\nperformance of test-time verification integrated\nwith DPO  and iterative DPO, respectively,\ninstead of the test-time verification only in Figure\n2 of the main paper. As shown, our propose PARM\nboth achieves the best results as the N increases\nfor best-of-N selection."}]}