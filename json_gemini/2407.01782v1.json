{"title": "Addressing a fundamental limitation in deep vision models: lack of spatial attention", "authors": ["Ali Borji"], "abstract": "The primary aim of this manuscript is to underscore a significant limitation in current deep learning models, particularly vision models. Unlike human vision, which efficiently selects only the essential visual areas for further processing, leading to high speed and low energy consumption, deep vision models process the entire image. In this work, we examine this issue from a broader perspective and propose a solution that could pave the way for the next generation of more efficient vision models. Basically, convolution and pooling operations are selectively applied to altered regions, with a change map sent to subsequent layers. This map indicates which computations need to be repeated. The code is available at https://github.com/aliborji/spatial_-attention.", "sections": [{"title": "Motivation", "content": "The visual world around us is dynamic, and we rarely see the exact same image twice due to variations in lighting and other factors. Similarly, neural activity is not identical even when exposed to the same input. However, not everything in the visual world changes, and often only a small portion of the input varies over short periods (Figure 1). Our visual system has evolved to efficiently address this by selectively focusing on and processing important regions of interest. In contrast, deep vision models lack this capability. While there have been some ad-hoc approaches to address this issue, they are not inherent to the models. The main problem lies in operations such as convolution (nn.Conv2d), which are applied to the entire image without the ability to selectively skip parts of it at the hardware level. We argue that this is a major limitation and propose potential solutions for researchers to explore in the future to address this problem.\nConvolutional neural networks and vision transformers lack this selective processing ca-pability. Although various attention mechanisms have been proposed, they do not perform spatial attention. In transformers, attention operates more like feature-based attention, as described in the attention literature, rather than spatial attention.\nIn the proposed approach, computation is performed on demand. One advantage of this method is that it can be applied solely during inference. The model can be trained using a GPU and then optimized using this approach to enhance inference efficiency."}, {"title": "Related work", "content": "Visual attention is the cognitive process of selectively focusing on one aspect of the en-vironment while ignoring others [7, 3]. This is essential because the human brain cannot process all visual information simultaneously. There are two main types of attention: 1) Goal-Driven Top-Down Attention: This intentional type is controlled by an individual's"}, {"title": "A potential solution", "content": "In the proposed approach, convolution and pooling operations are selectively applied to altered regions, with a change map sent to subsequent layers. This map informs those layers about which computations need to be repeated. Each layer communicates changes so the next layer knows what it needs to recompute, and this process continues until the final layer. To achieve this, each layer must remember its last computation to avoid redundant processing.\nThe basic idea is illustrated in Figure 2. First, a change map is computed from sub-sequent frames (e.g., |It - It\u22121|). This change map is sent to the first convolution layer,"}, {"title": "Experiments and results", "content": "The 28 x 28 MNIST digits, both during training and inference, were placed at the center of a black 64 x 64 image. We trained a simple CNN, referred to as CNN1 as illustrated in Figure 3, on a GPU with a batch size of 32. Since our primary focus is on the inference stage, we then loaded the weights into a model residing on a CPU. A single frame was processed at a time (i.e., batch size = 1). We conducted two experiments as detailed below.\nThe results are presented in Table 1."}, {"title": "Experiment I: Processing repeated versions of the same image", "content": "In this experiment, we ran three models on 11 images. Each image is repeated 10 times (110 images in total). The CNN1 model proved to be the fastest because it uses 'nn. Conv2d', which is a parallel implementation on CPU cores. The aim here is to demonstrate that significant computation can be saved when there is no change in the image. Most of the processing is done on the first frame, which is then reused for subsequent frames. This is why the processing time for CNN3 is nearly 1/10th of CNN2. The inputs and results are illustrated in the first rows of Figure 4 and Table 1, respectively."}, {"title": "Experiment II: Processing shifted versions of the same image", "content": "This experiment is similar to Experiment I, except each of the 11 digits is shifted rightward by one pixel at a time, resulting in 110 images in total. This method causes some regions of the image to remain the same while others change. As a result, CNN3 is slower here compared to its speed in Experiment I because it needs to recompute more information due to the increased amount of changed content.\nIn terms of accuracy, CNN1 and CNN2 are equivalent since their implementations are essentially the same. CNN3, however, can exhibit different performance based on the amount of change (T). Smaller values of lead to more computation and higher accuracy, and vice versa. Overall, the CNNs performed similarly to each other, although they were less accurate compared to Experiment I, due to pixel shifts. The inputs and results are illustrated in the second rows of Figure 4 and Table 1, respectively.\nThe change maps for the input images and the network layers are displayed in Figure 5. As we increased the change threshold 7, the accuracy decreased while the speed increased. This relationship is illustrated in Figure 6."}, {"title": "Conclusion", "content": "We highlighted a key issue with existing deep learning approaches and proposed a simple solution that can be integrated into current models or used to design new models with inherent attention and memory mechanisms. This work serves as a proof of concept and can be applied to other problems such as object detection, scene segmentation, and action recognition. It also has the potential to help address adversarial examples [2]. This method is particularly effective when the input has higher resolution.\nOur visual system is far more sophisticated and energy-efficient than the most advanced deep learning models available today. Specifically, our early visual system performs exten-"}, {"title": "Appendix", "content": "Convolution and pooling modules of CNNs are shown in Figures 7, 8, and 9."}]}