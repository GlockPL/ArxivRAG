{"title": "SCDM: Unified Representation Learning for EEG-to-fNIRS\nCross-Modal Generation in MI-BCIs", "authors": ["Yisheng Li", "Shuqiang Wang"], "abstract": "Hybrid motor imagery brain-computer interfaces\n(MI-BCIs), which integrate both electroencephalography (EEG)\nand functional near-infrared spectroscopy (fNIRS) signals, out-\nperform those based solely on EEG. However, simultaneously\nrecording EEG and fNIRS signals is highly challenging due to\nthe difficulty of colocating both types of sensors on the same\nscalp surface. This physical constraint complicates the acquisition\nof high-quality hybrid signals, thereby limiting the widespread\napplication of hybrid MI-BCIs. To facilitate the acquisition\nof hybrid EEG-fNIRS signals, this study proposes the spatio-\ntemporal controlled diffusion model (SCDM) as a framework for\ncross-modal generation from EEG to fNIRS. The model utilizes\ntwo core modules, the spatial cross-modal generation (SCG)\nmodule and the multi-scale temporal representation (MTR)\nmodule, which adaptively learn the respective latent temporal and\nspatial representations of both signals in a unified representation\nspace. The SCG module further maps EEG representations to\nfNIRS representations by leveraging their spatial relationships.\nExperimental results show high similarity between synthetic and\nreal fNIRS signals. The joint classification performance of EEG\nand synthetic fNIRS signals is comparable to or even better\nthan that of EEG with real fNIRS signals. Furthermore, the\nsynthetic signals exhibit similar spatio-temporal features to real\nsignals while preserving spatial relationships with EEG signals.\nExperimental results suggest that the SCDM may represent a\npromising paradigm for the acquisition of hybrid EEG-fNIRS\nsignals in MI-BCI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "THE motor imagery brain-computer interface (MI-BCI)\ntechnology assists individuals with motor impairments\nby decoding neural signals from the central nervous system\nduring imagined limb movements and then translating them\ninto commands to control external devices [1]. A primary\nfocus in MI-BCI research is the classification of left-right\nmotor imagery (MI). Electroencephalography (EEG), which\nmeasures electrical activity on the cortical surface by placing\nelectrodes on the scalp, is the most widely used technique for\nrecording central nervous system signals in MI-BCI applica-\ntions due to its high temporal resolution and quick response\nto stimuli [2].\nDespite its advantages, EEG suffers from low spatial reso-\nlution and susceptibility to artifacts, which limit MI decoding\neffectiveness [3], [4]. In contrast, functional near-infrared\nspectroscopy (fNIRS) offers higher spatial resolution and\ngreater resistance to artifacts. Moreover, it measures hemo-\ndynamic responses instead of electrical reactions in the brain,\nproviding valuable neurophysiological insights for MI tasks\n[5], [6]. Combining EEG with fNIRS to construct multimodal\nMI decoding models, the hybrid MI-BCI system extracts\ndiscriminative features from each modality and integrates their\ninformation during pattern classification. Growing evidence in-\ndicates that hybrid MI-BCIs based on EEG-fNIRS outperform\nMI-BCIs based solely on EEG in terms of classification accu-\nracy and stability [7]\u2013[11]. These studies utilize independent\nEEG and fNIRS signals, as well as combined EEG-fNIRS\nsignals, as inputs for MI classification tasks. Results show that\nhybrid signals significantly enhance classifier performance,\nsurpassing that of independent EEG signals alone.\nHowever, it is highly challenging to co-locate EEG and\nfNIRS sensors on the same scalp surface, primarily due to\nthe consideration of the impact of dense hair interference\non light signals [12], [13], the influence of fNIRS source-\ndetector distance [14], [15], and the problem of matching EEG\nelectrodes with fNIRS signal recording locations [16]. This\nphysical constraint hinders the acquisition of multi-channel,\nhigh-spatial-resolution fNIRS signals that overlap with EEG\nchannel locations. As a result, in practical research, different\ndevice layouts must be designed based on various experimental\nparadigms to balance signal quality and device arrangement.\nAdditionally, the complexity of pre-experimental preparations\nmakes it inconvenient to collect joint signals frequently. These\nseverely undermine the applicability of hybrid MI-BCI.\nWith the rapid advancement of artificial intelligence tech-\nnologies, generative models have opened up possibilities for\ncross-modal generation of brain functional data [17], [18].\nA prominent approach is the generative adversarial network\n(GAN) [19], initially used for image generation, then extended\nto medical image computing [20]\u2013[22], and later successfully\nextended to cross-modal generation in medical imaging [23]-\n[27]. For example, Ben-Cohen et al. [23] combined a fully\nconvolutional network with conditional GAN to generate sim-\nulated positron emission tomography (PET) data from given\ncomputerized tomography (CT) inputs, providing a solution to\nreduce the need for expensive and radioactive PET/CT scans.\nYang et al. [24] utilized Structure-Constrained CycleGAN\nfor unsupervised synthesis from MRI to CT. Dar et al. [25]\nemployed conditional GAN to synthesize images with different\ncontrasts from a single MRI modality, enhancing diagnostic\ninformation diversity in MRI.\nGANs, while implicitly representing image distributions,\nmay nonetheless suffer from limited sample fidelity, thereby\nrestricting the quality and diversity of synthesized images [28].\nAdditionally, training instability and mode collapse present\nmajor challenges to GAN. Recently, a novel generative model,\nthe diffusion model [29]\u2013[32], which relies on explicit likeli-\nhood representation and progressive sampling, has emerged.\nThe diffusion model, inspired by the variational iteration"}, {"title": "II. METHOD", "content": "In the diffusion model, the diffusion process at each time\nstep t involves adding noise $e_t$ sampled from a Gaussian\ndistribution to the fNIRS time series $f_t$, where t ranges\nfrom 1 to T. After T steps, the original sequence $f_0$ is\ntransformed into noisy data $f_T$ that conforms to a standard\nGaussian distribution. The diffusion model incorporates a U-\nNet architecture, which takes $f_t$ as input and outputs the\npredicted noise $e_{\\theta, t}$ as an estimate of $e_t$, where $\\theta$ represents\nthe parameters of the U-Net. The optimization objective of\nthe diffusion model is to minimize the L2 loss between these\ntwo types of noise. Once trained, the diffusion model uses\nrandomly sampled Gaussian noise as $f_T$ and then gradually\nremoves noise $e_{\\theta, t}$ via backpropagation to obtain $f_0$, i.e., the\nsynthetic fNIRS time series."}, {"title": "B. SCDM", "content": "The proposed SCDM, illustrated in Fig. 1, distinguishes\nitself from the vanilla diffusion model by utilizing time series\ndata from both EEG and fNIRS modalities to synthesize an\nfNIRS sequence. There is a certain degree of coupling between\nfNIRS and EEG signals, as hemodynamic changes are typi-\ncally induced by neural activity. Based on this observation, one\nmight assume that fNIRS and EEG sequences share similar\ndistributions. While this assumption isn't universally applica-\nble, the study introduces a method involving the selection of\nforward propagation parameters, especially time step numbers\nT and noise scales $\\beta_1, \\beta_2, ..., \\beta_T$, to ensure that both $f_T$ and\n$e_T$ conform closely to Gaussian distributions with minimal\nmutual discrepancies. This approach forms a reasonable basis\nfor considering $e_0$ and $f_0$ to have approximately similar\ndistributions.\nThe optimization objective aims to minimize the pairwise\ndistribution discrepancies $W$ between the fNIRS noise $f_T$ and\nEEG noise $e_T$ after forward propagation and the randomly\nsampled fNIRS noise $f_T$ during backward propagation. The\ncombined discrepancy $W$ is formulated as:\n$W = W(f_T, f_0) + W(e_T, f_0) + W(f_T, e_T)$ (1)\nHere, the Wasserstein distance $W(P,Q)$ quantifies the actual\ndifferences between distributions $P$ and $Q$. Parameter selection\nprecedes model training. Once suitable forward propagation\nparameters are identified, the original sequences $e_0$ and $f_0$\ncan undergo an identical forward propagation process.\nThe U-Net of SCDM takes the EEG and fNIRS time series\nat time step t, denoted as $e_t$ and $f_t$ respectively, as input and\npredicts the noise $e_{\\theta,t}$. Each $f_t$ is derived from the backward-\nsampled output of the previous step t+1, while $e_t$ is obtained\nfrom the forward-propagated output of the current step t. The\nU-Net consists of 6 sample blocks. The first 3 blocks perform\ndownsampling, halving the channel count and sequence length\nof $e_t$ and $f_t$, while the last 3 blocks perform upsampling,\ndoubling those of $e_t$ and $f_t$. Each sample block primarily\ncomprises the SCG and MTR two core modules, with the Time\nEmbed module first embedding the time step information. The\nstructure of all sample blocks is nearly identical, with the\nonly difference being that the MTR module in the upsampling\nblocks uses transposed convolutions or bilinear interpolations\ninstead of standard convolutions. To facilitate the process of\nthe two time series within the sample blocks, prior to inputting\nthem into the U-Net, convolution and linear transformation"}, {"title": "C. SCG Module", "content": "The SCG module halves or doubles the time series channels\nwithout altering the sequence length, supporting various input\nand output combinations.\n1) Representation Mapping: When the inputs include the\noriginal sequences $e_0$ and $f_0$ along with the EEG represen-\ntation, the SCG module maps the EEG representation to the\nfNIRS representation.\nFirstly, $e_0$ and $f_0$ are used to compute the distance corre-\nlation coefficient matrices between the 30 EEG channels and\n36 fNIRS channels. Due to the differing lengths of the two\nsequences, Pearson correlation coefficients cannot be calcu-\nlated; thus, distance correlation coefficients are used instead.\nThis method not only measures linear correlations but also\ncaptures non-linear correlations [38]. The resulting matrices\n$C_{ef} \\in R^{30 \\times 36}$ and $C_{fe} \\in R^{36 \\times 30}$ are transposes of each\nother. To enable 2-dimensional convolutions, the matrices' last\ndimension is projected onto a 16 \u00d7 16 plane, constructed based\non the distribution of 66 scalp channels (see Fig. 3). On this\nplane, each point is either set to 0 or represents a correlation\ncoefficient value. Thus, $C_{ef}$ and $C_{fe}$ are transformed into\nshapes of 30 \u00d7 16 \u00d7 16 and 36 \u00d7 16 \u00d7 16, respectively. Each\nplane can be viewed as the spatial distribution of channel\ncorrelations.\nNext, a 2-dimensional convolution with $d_{in}$ kernels is\napplied to $C_{ef}$, yielding the query $Q \\in R^{d_{in} \\times 16 \\times 16}$, where $d_{in}$\nis the number of input EEG representation channels. Q selects\n$d_{in}$ features for representation mapping from the EEG data. A\nsimilar operation on $C_{fe}$ results in the key $K \\in R^{d_{out} \\times 16 \\times 16}$,\nwhere $d_{out}$ is the number of output fNIRS representation\nchannels. K determines $d_{out}$ relevant fNIRS features. The\nEEG representation serves as the value V in the attention\nmechanism. The output of the SCG module is formulated as\nfollows:\n$output = Score \\cdot V = Softmax(\\frac{Q K^T}{\\sqrt{d_{out}}}) V$ (2)\nwhere $Score \\in R^{d_{in} \\times d_{out}}$ is the attention score matrix,\nwith each element representing the contribution weight of a\nparticular EEG channel to a specific fNIRS channel. Since\nEEG channels will assign higher weights to spatially adjacent\nfNIRS channels, the weight information obtained through 2-\ndimensional convolution more precisely reflects the spatial\ncorrespondence between EEG and fNIRS channels.\nIn summary, the SCG module employs an attention mech-\nanism based on inter-channel correlation and enhances the\ntraditional 1-dimensional attention mechanism by executing\n2-dimensional convolution operations to obtain Q and K. The\ninter-channel correlation guides the representation mapping.\nAlthough this relationship is based on statistical rather than\nspatio-temporal features, unified representation learning of the\nEEG and fNIRS is performed before mapping. Additionally,\nthe 2-dimensional convolution preserves the spatial distri-\nbution features of the correlations, maintaining the spatial"}, {"title": "D. MTR Module", "content": "The MTR module halves or doubles the length of the input\nsequence without changing the number of channels.\nThe multi-scale 1-dimensional depth-wise convolution com-\nprises four convolutions with kernel sizes of 3, 5, 7, and 9,\neach with a stride of 1. Each kernel individually performs\nconvolutions on a single channel, enabling the network to\nfocus on specific temporal feature distributions without inter-\nchannel interference. Varying kernel sizes enable multi-scale\ntemporal feature extraction, effectively capturing dynamic\nbrain activity changes across various frequencies and time\nspans. The causal dilated convolution is implemented with\nthree consecutive convolutions with dilation rates of 1, 2, and\n4, a stride of 2, and a kernel size of 2, with zero-padding\nadded to the left side. This convolutional structure expands\neach output point's receptive field and makes it depend solely\non its predecessors, thus ensuring high precision and causality\nin the extracted temporal features.\nFor downsampling, the 1-dimensional point-wise convolu-\ntion involves four convolutions with a kernel size of 1 and a\nstride of 2. Two convolutions use no padding, while the other"}, {"title": "III. DATASET", "content": "The study used a publicly available dataset of EEG and\nfNIRS recordings from 29 healthy subjects performing left\nand right-hand MI tasks [39]. The experimental procedure\ncomprised three separate sessions, each with 20 trials. During\neach trial, subjects were instructed to imagine executing a\ngrasping movement with either their left or right hand, as\nindicated by the direction of a black arrow displayed at the\ncenter of a screen for 2 seconds. Following the trial initiation,\nsubjects engaged in imagining the grasping movement for 10\nseconds until hearing a brief beep and seeing the word \"STOP\u201d\non the screen, signifying the trial's end. Rest periods of 14 to\n16 seconds were interleaved between each trial.\nThe EEG data were recorded from 30 active electrodes posi-\ntioned according to the international 10-5 system, resulting in\n30 channels sampled at 1000Hz. The data were downsampled\nto 200Hz by the provider. fNIRS signals were recorded using\n14 sources and 16 detectors placed over the frontal, motor, and\nvisual areas, totaling 36 channels sampled at 12.5Hz and later\ndownsampled to 10Hz. The sensors' spatial arrangement and\nrecording site distribution of EEG-fNIRS can be referenced in\nFig. 3.\nIn this experiment, the EEG signals were re-referenced to\na common average reference and filtered using a 4th order\nChebyshev Type II bandpass filter with a passband of 0.5 to\n50 Hz, then downsampled to 160Hz. Ocular artifacts were\nremoved via independent component analysis. The fNIRS\ndata were filtered using a 6th order zero-phase Butterworth\nbandpass filter with a passband of 0.01 to 0.1 Hz after\nconversion from raw recordings to deoxy-hemoglobin (HbR)\nand oxy-hemoglobin (HbO) data. HbR and HbO, collectively\nreferred to as fNIRS, were uniformly processed throughout\nthe experiment. Finally, the three datasets, EEG, HbR, and\nHbO, were segmented into epochs. Each dataset consisted of\n1740 samples, representing the 1740 MI trials across all 29\nsubjects. The time series length for EEG recordings was 4000,\""}, {"title": "IV. RESULT", "content": "This study investigates the potential of synthetic fNIRS to\nsubstitute real fNIRS when combined with EEG for left/right\nmotor imagery (LMI/RMI) classification. The classification\nresults of five signal combinations were compared: EEG, EEG-\nreal HbR, EEG-synthetic HbR, EEG-real HbO, and EEG-\nsynthetic HbO. The classification was implemented using"}, {"title": "C. Hemodynamic Response Curve", "content": "This study compares the hemodynamic response curves\nof HbR and HbO signals under LMI and RMI tasks by\naveraging epochs across all subjects, as shown in Fig. 4. These\ncurves visually represent the temporal characteristics of fNIRS\nsignals.\nThe synthetic and real HbR curves show similar trends\nduring the LMI task, despite significant numerical differences\nbetween 5 and 15 seconds. In the RMI task, both curves\nare closely aligned in trend and value, particularly within\nthe initial 15 seconds. For synthetic and real HbO curves,\nsubstantial differences are observed during the LMI task, but\nthe trend differences diminish after 15 seconds; conversely,\nthey are quite similar during the RMI task. Overall, the\nhemodynamic response curves of synthetic and real fNIRS\nsignals are generally consistent, reflecting similar temporal\ncharacteristics.\nThe averaged hemodynamic response curve can explain\nthe classification results to some extent. In the LMI task,\nthe synthetic HbR curve is globally amplified compared to\nthe real HbR curve, thereby magnifying the differences be-\ntween synthetic HbR curves in LMI and RMI tasks. This\namplification indicates that all epoch curves contributing to\nthe synthetic HbR curve collectively enhance the distinctions\nbetween LMI and RMI tasks. This effect likely aids classifiers\nin identifying potential features of synthetic HbR signals'\nhemodynamic responses across both tasks. The superior clas-\nsification performance of EEG-synthetic HbR compared to\nEEG-real HbR provides credible evidence for this hypothe-\nsis. However, the synthetic HbO curve during the LMI task\nappears more complex than the real HbO curve, indicating\ngreat variability among epoch curves, potentially reducing\nthe distinguishability of the two curves under LMI and RMI\ntasks and thereby weakening the classification ability of EEG-\nsynthetic HbO for LMI."}, {"title": "D. Scalp Topography", "content": "The scalp topography of fNIRS illustrates the concentration\ndistribution of deoxyhemoglobin or oxyhemoglobin across\ndifferent regions of the brain, which can characterize the\nspatial features of fNIRS signals. This study qualitatively\nanalyzes the spatial distribution differences between synthetic\nand real fNIRS over a time period of 3 to 17 seconds by\ncomparing seven scalp topographies, as shown in Fig. 5. For\nclarity, only channels from the motor area are displayed, and\nlinear interpolation is applied to calculate points outside the\nchannel locations.\nUnder the LMI task, the spatial distributions of synthetic and\nreal HbR are generally consistent across all seven time periods,\nwith a few channels exhibiting noticeable differences at certain\ntime points. For the RMI task, the spatial distributions are\nconsistent. For synthetic and real HbO under the LMI task,"}, {"title": "V. ABLATION STUDY", "content": "This study conducted five ablation experiments to evaluate\nthe contributions of the SCG and MTR modules to SCDM\nperformance. Five module combinations were utilized: ATTN\n+ COV, SCG (fNIRS) + COV, SCG (EEG) + COV, ATTN +\nMTR, and SCG (fNIRS) + MTR, generating synthetic fNIRS\nsignals. EEG-synthetic fNIRS was then used in classification\nexperiments to evaluate each combination's performance. The\n1-dimensional multi-head self-attention mechanism (ATTN) in\nthe denoising diffusion probabilistic model (DDPM) served\nas the control for the SCG module. In line with DDPM's U-\nNet architecture, the MTR module's depthwise separable and\ncausal dilation convolutions were replaced with conventional\nconvolutions, forming the COV module, which acted as the\nMTR module's control. The SCG module has two forms,\nSCG (fNIRS) and SCG (EEG), sharing the same architecture.\nSCG (fNIRS) only accepts fNIRS sequences as input to learn\ntheir spatial representations. Since the proposed SCDM adopts\nthe SCG (EEG) + MTR combination, the SCG (fNIRS) +\nMTR combination can be viewed as a variant of SCDM that\nenhances fNIRS rather than achieving cross-modal generation\nfrom EEG to fNIRS. ATTN + COV represents a variant of\nDDPM and serves as the baseline for SCDM.\nThe ablation experiment results are presented in Table I.\nConsistent patterns were observed for both the HbR and HbO\ngroups:"}, {"title": "VI. CONCLUSION", "content": "The study proposes the SCDM for cross-modal generation\nof synthetic fNIRS signals from EEG data. Synthetic fNIRS\nhas relatively high substitutability for real fNIRS, suggesting\nthat the cross-modal generation approach can serve as a new\nparadigm for acquiring hybrid EEG-fNIRS signals in MI-\nBCIs. Ablation studies validate the crucial role of the SCG\nmodule in cross-modal generation and demonstrate that the\nMTR module significantly complements the SCG module, col-\nlectively enhancing the performance of the SCDM framework.\nHowever, this study warrants further exploration. Firstly,"}]}