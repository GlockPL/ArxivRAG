{"title": "An Extremely Data-efficient and Generative LLM-based Reinforcement Learning Agent for Recommenders", "authors": ["Shuang Feng", "Grace Feng"], "abstract": "Recent advancements in large language models (LLMs) have enabled understanding webpage contexts, product details, and human instructions. Utilizing LLMs as the foundational architecture for either reward models or policies in reinforcement learning has gained popularity - a notable achievement is the success of Instruct-GPT [11]. RL algorithms have been instrumental in maximizing long-term customer satisfaction and avoiding short-term, myopic goals in industrial recommender systems, which often rely on deep learning models to predict immediate clicks or purchases.\nIn this project, several RL methods are implemented and evaluated using the WebShop [17] benchmark environment, data, simulator, and pre-trained model checkpoints. The goal is to train an RL agent to maximize the purchase reward given a detailed human instruction describing a desired product.\nThe RL agents are developed by fine-tuning a pre-trained BERT model with various objectives, learning from preferences without a reward model, and employing contemporary training techniques such as Proximal Policy Optimization (PPO) as used in Instruct-GPT [11], and Direct Preference Optimization (DPO) [12]. This report also evaluates the RL agents trained using generative trajectories. Evaluations were conducted using Thompson sampling in the WebShop simulator environment.\nThe simulated online experiments demonstrate that DPO outperforms PPO in data efficiency and task performance, especially in success rate, using the same amount of training time. However, longer training time is necessary for fair comparison between the two. Specifically, without utilizing any image, a DPO agent achieved a 19% success rate after approximately 3000 steps or 30 minutes of training on T4 GPUs, compared to a PPO agent, which reached a 15% success rate after 2 hours of training. Results also indicate that agents trained on generated trajectories exhibited comparable task performance to those trained using human trajectories. This has demonstrated an example of an extremely low-cost data-efficient way of training reinforcement learning agents.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have significantly enhanced research and applications in understanding human instructions on the web, processing webpage text, and grasping context. These advancements have provided valuable tools for training reinforcement learning (RL) agents to navigate web environments, particularly in e-commerce and various recommender systems such as YouTube and Netflix. Leveraging LLMs in RL agent training is relatively new but has proven successful. A notable example is InstructGPT [11], where an RL agent was trained using human preferences by fine-tuning GPT-3 models with human instructions. Combining LLMs with RL techniques enables the creation of intelligent web agents that can understand human instructions and complete tasks in web or app environments, thereby maximizing desired rewards.\nRecommender systems have evolved from collaborative filtering [6] to the recent surge in deep supervised learning, which predicts immediate user responses such as clicks [4, 18]. This approach has seen tremendous success in personalized user engagement. However, after several years in production, deep supervised learning algorithms have shown limitations, including: 1) a focus on optimizing short-term gains at the expense of long-term user satisfaction and retention, and 2) strong feedback loops caused by training data generated from these algorithms, which exacerbate these effects. Conversely, RL algorithms are designed to optimize long-term gains by learning policies that maximize long-term user satisfaction. RL agents are also well-known for their ability to perform sequential planning and make decisions based on the Markov Decision Process (MDP) properties [2].\nThe training of RL agents for recommenders in web environments has been actively studied, with several benchmark datasets and trained agents available. For example, WikiNav [10] provides a benchmark for web-based navigation RL agents. RecoGym [13] offers a benchmark for RL agents in production recommendations for online advertising. Virtual-Taobao [15] includes a virtual online shopping environment derived from Taobao, hosting several RL algorithms for product recommendations. WebShop [17] presents a simulated e-commerce web environment with over 1,600 human demonstrations for web shopping tasks based on human text instructions. This environment includes 1.18 million products with text and image descriptions, along with 12,087 crowd-sourced text instructions. The authors of WebShop also explored several imitation and RL agents trained using real-world human trajectories.\nPrevious explorations in RL for web-based recommenders are extensive. Query reformulation, as published in [10], is part of an RL problem aimed at optimizing outcomes. In this context, search"}, {"title": "2 Related Work", "content": "The work presented in this paper is built and evaluated within the WebShop [17] environment, a simulator of online web shopping recommender system."}, {"title": "2.1 The WebShop Environment", "content": "WebShop is a benchmark project designed to train reinforcement learning algorithms in a large-scale, interactive, web-based envi- ronment. It includes over 12,000 crowdsourced human instructions, over 1.1 million products scraped from amazon.com. A total of 670 attributes were derived from concatenated product titles and descriptions using bi-gram representations and assigned to each product through TF-IDF scoring.\nThe original paper tackles the problem into two sets of reinforce- ment learning models for search and choice (or clicks). The search model is an imitation learning model (search-IL) mimicking human search queries from instructions. It is a human instruction and query pair fine-tuned BART [7] model in root. For choice (clicks) learning, the authors present a few reinforcement learning models to optimize choice of clicks navigating the recommender simulator to optimize the end rewards (purchase). The reward is calculated by a scoring function to quantify the relevance between a purchased product and the human instruction, based on attributes of the prod- uct. The imitation learning algorithm (choice-IL) presented by the"}, {"title": "2.2 Reinforcement Learning with Human\nPreference - RLHF", "content": "RLHF [3] together with PPO [14] were successfully used to train a few well-known GPT related product, such as instructGPT [11]. RLHF leverages the Bradley-Terry model, which defines the preference using rewards of preferred and dispreferred data labeled by human labelers:\n$P(y_l > y_w|x) = \\frac{exp(r(x,y_l))}{exp(r(x,y_l)) + exp(r(x, y_w))}$.\nRLHF objectives then can be defined similarly to entropy loss."}, {"title": "2.3 PPO for Regularized Policy Gradient", "content": "Proximal Policy Optimization (PPO) [14] has been demonstrated to be effective in fine-tuning GPT models with human instructions and labeled preferences [11]. PPO uses clipping or KL divergence constraints to minimize the likelihood of large updates between steps, approximately providing guarantees for monotonic improvement. This approach converges in probability to local optima and, in practice, results in more stable training outcomes. The clipped loss function for policy gradient in PPO can be expressed as:\n$L_{PPO} = -E_{\\tau~\\pi_k}[min(z_t(\\theta) \\hat{A}_k, clip(z_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_k)]$ (1)\n, where\n$z_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|o_t)}{\\pi_{\\theta_k}(a_t|o_t)}$,\n$\\hat{A}_k = R_t - V_k(o_t)$."}, {"title": "2.4 Learning with Human Preference - DPO", "content": "The development of Direct Preference Optimization (DPO) [12] is revolutionary. It eliminates the need for explicit reward functions for preferences and instead relies solely on paired preference trajectories as training data. DPO is derived from joining the Bradley-Terry objective\n$L_{BT}(r, D) = -E_{(x,y_w,y_l)~D} [log \\sigma(r(x, y_w) - r(x, y_l))]$\n, and the RLHF objective:\n$max_{\\pi} E_{x~D,y~\\pi} [r(x, y)] - \\beta D_{KL}[\\pi(y|x)||\\pi_{ref}(y|x)]$.\nThe DPO loss function is:\n$L_{DPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x,y_w,y_l)~D}[log \\sigma(\\beta log\\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta log\\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)})]$\n, where $\\pi_{\\theta}$ is the DPO policy to learn and $\\pi_{ref}$ is the pre-selected reference policy. From the form of the loss function, although DPO does not need a reward model to be trained explicitly, it does require a pre-defined reference policy to iterate upon."}, {"title": "3 Approach", "content": "This report summarizes a few efforts implementing and evaluating PPO vs. contrast learning using DPO using human trajectories and generated unpreferred trajectories. Then it demonstrates the contrast learning effort using all generative trajectories.\nWe branched and implemented DPO and PPO using the original WebShop code package, together with a new generative module for the self-generative learning experiments, and a Thompson sampling module to roll out online experiments and collect results.\nFor PPO training, the policy gradient objective from the original paper [17] is modified into a PPO objective as shown in equation (1). The overall objective components, which is the total loss from policy gradient (PG), entropy loss, and imitation learning loss remain the"}, {"title": "3.1 Semi-generative Reinforcement Learning\nUsing Human Trajectories", "content": "For this project, we utilize a pre-trained imitation learning agent checkpoint as the reference policy to generate unpreferred trajectories. Preferred trajectories are obtained from human data provided by the WebShop benchmark. During training, a human trajectory is randomly sampled, including states and available actions from the log. At each state where an action decision is needed, an unpreferred action is generated using the reference policy. This unpreferred action is paired with the preferred action generated by the human. The DPO update is applied after each episode based on the human trajectory.\nThis approach is considered both generative and semi-self-learning. It is generative because we use a predefined unpreferred policy to generate actions for pair-wise training. It is semi-self-learning because it pairs these generated actions with previously collected human trajectories, which serve as the gold standard."}, {"title": "3.2 Self-learning - Training with Generated\nTrajectories", "content": "In classic reinforcement learning, self-play or learning through simulation plays a crucial role, particularly when data collection is costly, such as the collection of human trajectories in this problem. Self-play has proven to be effective, with the most notable example being AlphaGo [16].\nTo evaluate the idea of self-play or self-learning in navigating the WebShop recommendation systems, we generated 100 preferred trajectories using a straightforward method of sampling trajectories with perfect reward (score = 1). This sampling was done using the agent checkpoint from imitation learning provided by the authors of the WebShop paper, but with real-world human instructions.\nIdeally, these sampled trajectories are pruned to eliminate looped sub-trajectories. A DPO agent is then trained from the same checkpoint used for DPO evaluation in the previous section, with 3000 steps. Task performance between these two DPO agents \u2013 one trained using semi-learning with human trajectories and the other using self-learning with generated trajectories \u2013 is compared using Thompson sampling ran in the WebShop simulator environment."}, {"title": "4 Experimental Results", "content": "In this project, leveraging the WebShop environment and simulator, we conduct extensive simulated online experiments using Thompson sampling to analyze the performance differences across trained agents. The goal of Thompson sampling is to select the optimal action (or \"arm\") that minimizes overall regret. However, when sampling over a small number of steps, it may not be ideal for estimating rewards from arms that are perceived as less optimal due to insufficient exploration. To address this, we use multiple parallel runs of Thompson sampling, each with 1000 rollouts, to capture variability across runs. Careful experimental design and"}, {"title": "4.1 DPO vs. PPO Task Performance", "content": "The results indicate that Direct Preference Optimization (DPO) agents achieve significantly higher scores and success rates compared to Proximal Policy Optimization (PPO) agents, even though all agents start from the same imitation learning BERT model checkpoint provided by the original paper. It is important to note that all agents in this comparison are trained without image data, so the scores and success rates collected are not directly comparable to the original paper, which includes image data in training and experiments.\nAn interesting finding is that DPO agents trained using human trajectories perform similarly to DPO agents trained using generated trajectories, albeit with larger variance in success rate across runs. The smaller variance observed in self-learning agents can be attributed to the fact that only 100 generated trajectories were used to train the DPO self-learning agent, compared to 1200 human trajectories used for training the DPO agent with human data.\nThe fact that DPO agents are trained using only 3000 steps also suggests the possibility of underestimating data inefficiency or bottleneck when training over long period of times using the same set of data. When training an agent for production systems, the limited number of available trajectories can result in decreased task performance due to insufficient information learned from the limited data. In reality, collecting human data is expensive and time-consuming. This issue can be mitigated by generating preferred and unpreferred trajectories to serve as a continuous, low-cost source of training data."}, {"title": "5 Conclusion", "content": "With very limited training time (<1 hour), Direct Preference Optimization (DPO) outperforms Proximal Policy Optimization (PPO), offering better task performance and higher success rates with less training time. However, more evaluations with longer training time are necessary to draw a conclusion. Using a DPO agent trained within one hour and without image data, we achieved a success rate of approximately 19%. This is higher than the success rate of an RL agent trained with an RNN network (without pre-trained models for search or choice imitation learning), which has an 18% success rate from the original paper."}, {"title": "6 Potential Usage: Using Trained Agents as a\nRecommender", "content": "Using reinforcement learning agent in recommenders is not new - it is known to be used in online recommenders such as Youtube. The trained optimal policy can become an ideal ranking algorithm for recommender systems. Starting from a human instruction, the agent simulates navigating through a provided list of product following the trained optimal policy and provides a \"purchased\" product from each run. When using in recommenders, multiple runs of the agent provide a list of recommended product to user and the order to present in a user interface, such as on web or in an app can be rank-ordered by scores or success for each recommended product from each run of the RL agent."}]}