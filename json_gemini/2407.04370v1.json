{"title": "Regulating Model Reliance on Non-Robust Features by Smoothing Input Marginal Density", "authors": ["Peiyu Yang", "Naveed Akhtar", "Mubarak Shah", "Ajmal Mian"], "abstract": "Trustworthy machine learning necessitates meticulous regulation of model reliance on non-robust features. We propose a framework to delineate and regulate such features by attributing model predictions to the input. Within our approach, robust feature attributions exhibit a certain consistency, while non-robust feature attributions are susceptible to fluctuations. This behavior allows identification of correlation between model reliance on non-robust features and smoothness of marginal density of the input samples. Hence, we uniquely regularize the gradients of the marginal density w.r.t. the input features for robustness. We also devise an efficient implementation of our regularization to address the potential numerical instability of the underlying optimization process. Moreover, we analytically reveal that, as opposed to our marginal density smoothing, the prevalent input gradient regularization smoothens conditional or joint density of the input, which can cause limited robustness. Our experiments validate the effectiveness of the proposed method, providing clear evidence of its capability to address the feature leakage problem and mitigate spurious correlations. Extensive results further establish that our technique enables the model to exhibit robustness against perturbations in pixel values, input gradients, and density.", "sections": [{"title": "1 Introduction", "content": "Research on mitigating model reliance on non-robust input features has recently gained increasing attention due to high-stake machine learning applications [13, 19,40,53]. In this paper, we advance this direction by introducing a regularization technique that promotes a smooth marginal probability density function of the input to regulate the model's reliance on non-robust features.\nTo distinguish between robust and non-robust features, we leverage the notion of attributions [17, 56, 64]. For a model f parameterized by 0, attributions characterize the importance of the i-th feature xi of the input x for the model"}, {"title": "2 Related Work", "content": "Regularization for Interpretability Robustness: Despite advancements in model transparency [27, 28, 43, 62], current deep neural networks still lack interpretability in their decision-making process, which is exacerbated by their reliance on non-robust input features. Prior studies, such as [1, 2, 47], identify that standard models are prone to relying on irrelevant or spuriously correlated features. To address that, several regularization techniques are proposed to improve model interpretability. In [15,45], the authors incorporated prior knowledge into the model training process to regularize the model behavior. Dombrowski et al. [12, 13] found that regularizing the input Hessian using SoftPlus activations or weight decay can boost resilience against manipulated inputs. In [19], a joint energy-based model is trained as a discriminative model for improved robustness. Srinivas and Fleuret [53] enhanced the model interpretability by improving the alignment between the implicit density and the data distribution.\nRegularization for Adversarial Robustness: In addition to the other sources of prediction unreliability, adversarial attacks can manipulate model outputs with imperceptible perturbations to inputs [18,33]. To address this, adversarial training through data augmentation with adversarial samples is widely employed [9, 33, 46]. Certified adversarial robustness through regularizations [4,38, 49] is another branch of methods to defend against adversarial perturbations. Inspired by the classic double backpropagation [14], Ross and Doshi-Velez [38] regularized the input gradient norm for adversarial robustness. Etmann [16] further explored different variants of double backpropagation regularizations for various real-world scenarios. Moosavi-Dezfooli et al. [34] also proposed regularization to encourage a low curvature for adversarial robustness. To improve adversarial robustness, Chen et al. [10] computed the norm of attributions integrated from clean samples to adversarial samples as the regularization term. Ilyas et al. [25] proposed a disentangling method to distinguish feature robustness for explaining adversarial examples. However, they do not focus on general feature robustness."}, {"title": "3 Feature Robustness by Attributions", "content": "We first provide a framework for distinguishing between robust and non-robust features by analyzing their attributions. Herein, attribution inconsistencies among the features with distinct degrees of robustness identify a correlation between the model's reliance on non-robust features and the smoothness of output logits.\nLet us consider an input sample x \u2208 Rn with label y \u2208 R from a dataset D, and a classifier f : Rn \u2192 RC parameterized by 0. We denote robust and non-robust features within the input x as Xrob,Xnrob \u2286 x. Consider an attribution method \u00a2 : RC \u2192 Rn attributing model predictions to input features by estimating their importance, resulting in an attribution map M = $(f(x;0)). Inspired by the success of attributions in model explanation, we identify robust and non-robust features by leveraging their attributions.\nWithout loss of generality, we assume that attributions M of the features can be estimated by calculating the change in output logits when these features are removed from the input, following perturbation-based methods [37,64]: Mxrob = f(x; 0) - f (x[xrob=0]; 0) and Mxnrob = f(x; 0) \u2212 f (x[xnrob=0]; 0). For ease of understanding, we alternatively use f(xnrob; 0) and f(xrob; 0) to represent attributions Mrob and Mnrob in the text to follow. We define robust features within the attribution framework as follows.\nDefinition 1. A feature Xfeat shared among different input instances under its domain Axfeat is robust if, for a randomly chosen class y = i, its attribution Mxfeat is bounded by a small constant h under a metric c(\u00b7), i.e., c(f(x feat; 0)\u2212 f(xfeat; 0)) \u2264 h : X feat \u2208 Axfeat.\nUnder Definition 1, robust features are expected to contribute consistently to the model's prediction across different input samples. Non-robust features, on"}, {"title": "4 Smoothing Marginal Density of Input", "content": "Here, we establish the relation between model robustness and gradients of the input marginal density. Then, a robust regularization is proposed for regulating model reliance on non-robust features by smoothing marginal density.\nWe commence our analytical analysis with probability density, following Bridle [7]. Given a class y = i, a joint probability density function over the input with the output logit fi(x; 0) is defined as\nPo(x, y = i) = efi(x;0)/Zo, (1)\nwhere the constant Zo = \u222befi(x;0)dx is the partition function. Ze normalizes the input x to a probability density by integrating over all possible input points x in the input space via the model f. By applying Bayes' rule, we eliminate the condition y = i, resulting in the marginal density being defined solely on the input x: po(x) = \u0440\u04e9(y = i,x)/po(y = i|x). The conditional density function pe(y = ix) can be further defined as\nPo(y = i|x) = efi(x;0)/Zf(x;0), (2)\nwhere Zf(x;0) = \u2211C1 efi(x0) is the partition function for the output logits fi(x;0) defined on all the C classes. To simplify the notation, we use Zf(x) to represent Zf(x;0) in the subsequent discussion. Exploiting the symmetry property of the joint density defined in Eq. 1, i.e., \u0440\u04e9(x,y = i) = po(y = i,x), the"}, {"title": "5 Stable and Efficient Implementation for Regularization", "content": "From the implementation perspective, the gradient computation of marginal density involves multiple exponential operations in both the numerator and denominator of Eq. 4 which can introduce numerical instability in the optimization process, leading to gradient vanishing and explosion problems. Such issues can potentially hinder the application of our regularization to large non-linear models or wide-distribution data. For instance, batch normalization (BN) layers [26] solving internal covariate shifts with learnable scaling and shifting parameters can amplify the errors caused by the exponential operations during the back-propagation. In Fig. 2(a), yellow and red curves indicate original and inverse implementations for minimizing the gradient of log density \u2207xlogZ f(x). It is illustrated that the Frobenius gradient norm of both implementations undergoes rapid numerical overflow as the number of training iterations increases, revealing the serious issue of exploding and vanishing gradients using the proposed regularization. Therefore, it is crucial to address these through careful implementation to ensure the feasibility of our regularization.\nTo address this challenge, we transform the computation from the summation of exponential operations to softmax. By subtracting a constant value \u03b7 from the logits before exponentiation, softmax can prevent numerical overflow during the exponential operations, i.e., efi (2;0) / \u2211jefj(x;0) = efi(x;0)\u2212\u03b7/\u03a3jefj(x;0)\u2212\u03b7.\nThus, we incorporate the softmax function into the computation of log density"}, {"title": "6 Limited Robustness in Input Gradient Regularization", "content": "Input gradient regularization (InputGrad Reg.) [30,38] computes the Frobenius norm of input gradients || \u2207xfi(x;0)||F for a given class label y = i, which is a"}, {"title": "7 Experiments", "content": "In this section, we perform extensive experiments to validate the efficacy of our regularization and the newly established correlation between the smooth"}, {"title": "7.1 Efficacy against Feature Leakage and Adversarial Attacks", "content": "In [2,47], it is demonstrated that deep models end up assigning importance to irrelevant input features. Shah et al. [47] used BlockMNIST in their experiments, which is a synthetic dataset extended from MNIST [30]. \u03a4\u03bf each MNIST sample, BlockMNIST attaches a null block (an irrelevant pattern) randomly at the top or bottom of the image, as shown in Fig. 3(a). Shah et al. [47] observed that the explaining tool InputGrad [50] attributes importance to both the informative number block and the uninformative null block in the standard trained model. This phenomenon is termed as feature leakage by the authors. Interestingly, the issue is mitigated in adversarially trained models. Since this dataset allows for a controlled robustness assessment, we first evaluate our method on BlockMNIST to analyze the model's reliance on irrelevant features."}, {"title": "7.2 Efficacy for Spurious Correlation", "content": "Recent research has highlighted the susceptibility of neural models in learning spurious correlations that enhance performance on a given data but fail to generalize [2,8,42]. For instance, in CelebA-Hair dataset [32], which commonly consists of samples containing female celebrities with blond hair and male celebrities with dark hair, models heavily rely on the spuriously correlated gender feature to predict the target hair color [20, 42]. Consequently, accuracy tends to be lower for the samples containing male celebrities with blond hair, see Fig. 5.\nTo mitigate spurious correlations, distributional robust optimization (DRO) techniques have been proposed to re-weight the training loss of input samples from different groups [24,42]. In our regularization, an additional penalty is imposed to penalize the model's reliance on these spuriously correlated features because of their inconsistent attributions. This encourages the use of robust features while suppressing the model's reliance on spuriously correlated features. Reliable quantification of the model's robustness on natural images for spurious correlation suppression is an unresolved issue in the literature. We employ"}, {"title": "7.3 Efficacy against Pixels, Gradients and Density Perturbations", "content": "We first employ pixel perturbation [44, 60] to quantitatively compare the robustness of different models following Srinivas and Fleuret [53] who iteratively removed the most important input pixels identified by attribution maps for model robustness evaluation. Robust models are expected to exhibit increased sensitivity when removing the most important pixels and decreased sensitivity when removing the least important ones. We assess the difference in fractional output logit change between the images with the top and bottom k% most salient pixels using SmoothGrad [52] on ResNet-18 [21] trained on CIFAR-10"}, {"title": "8 Conclusion", "content": "In this paper, we define robust and non-robust features from a feature attribution perspective, and establish a correlation between the smoothness of input marginal density and model reliance on non-robust features. This connection motivates us to propose a regularization that targets the gradients of the marginal density, aiming to regulate the reliance on non-robust features. Extensive experiments demonstrate the effectiveness of our regularization in boosting model robustness across a wide range of applications. We emphasize that our approach does not advocate for the complete removal of model reliance on non-robust features, but instead seeks to achieve a balance between model performance and robustness through appropriate regularization strength."}, {"title": "9 Proof", "content": "In this section, we provide proof of our proposed approach for estimating the gradient difference using the gradients of the output difference.\nProof (Proof of Equation 7). Assuming that two functions, f and g, are continuously differentiable with respect to x \u2208 Rn, we can express them using their Taylor series expansions, as\nf(x) = f(a) + f'(a)(x \u2013 a) + o((x \u2013 a)\u00b2), (10)\nand\ng(x) = g(a) + g'(a)(x \u2212 a) + o((x \u2013 a)\u00b2). (11)\nBy subtracting Equation 11 from Equation 10, we have\nf(x) - g(x) = f(a) - g(a) + (f'(a) \u2013 g'(a))(x \u2013 a) + \u043e((x \u2013 a)\u00b2). (12)\nNext, we compute the gradients of both sides of Equation 12 with respect to x as\n\u2207x(f(x) - g(x)) = f'(a) \u2013 g'(a) + o((x \u2212 a)\u00b3). (13)\nThen, we can set x = a in Equation 13 as\nVa(f(a) - g(a)) \u2248 f'(a) \u2013 g'(a). (14)\nSince we have assumed that f and g are differentiable, we can estimate the difference between the gradients of the two functions as the gradient of the difference between the functions as\n\u2207x(f(x) - g(x)) \u2248 \u2207x f(x) - \u221axg(x). (15)\nSince the model fe parameterized with 0 is assumed as continuously differentiable, we can substitute the model output logit fi (x; 0) and log-softmax output (ef: (a)) into the functions f and g in Equation 15 as\nlogit log(Z\n\u2207xfi (x; 0) -xlog(z\n(efi (x;0))\n=) \u2248 \u2207 x(fi (x; 0) - log(\n(efi (x;0))\n)). (16)\nThus, the gradients of the difference between two outputs can be used to approximate the difference between the two gradients of the outputs."}, {"title": "10 Norm and Implementation Comparison", "content": "In this section, we evaluate the impact of different norms and implementations on our regularizations.\nFirstly, we investigate the effect of different p-norm values on our regularization approach. Given a variable p\u2208 R, p-norm of input x \u2208 Rn is defined as\n||x||p = (|x1|P + |x2|P + \u00b7\u00b7\u00b7 + |xn|P)1/p. (17)\nThe Lp norm allows us to measure the magnitude of a vector using different p values. Different p values exhibit different properties. Smaller p values promote sparsity, while larger p values emphasize the maximum value. Hence, selecting an appropriate p value that strikes a balance between these characteristics is crucial when applying the regularization method to models. In Figure 7, we test the effect of p norm values from p = 1.2 to p = 2.8 on models using our regularization with two regularization coefficients \u03bb = 0.1 and X = 0.2 on BlockMNIST [47]. In Figure 7(a), lower p values effectively suppress feature leakage, indicating that encouraging sparse features reduces reliance on non-informative features. In Figure 7(b) and Figure 7(c), larger p values lead to enhanced adversarial robustness and higher accuracy, suggesting that models are susceptible to perturbations caused by large gradients. The results reveal that models are easily perturbed from large gradients. The results demonstrate that our regularization enables models to regulate their reliance on non-robust features by adjusting the norm value p and regularization coefficient \u03bb. In our experiments, we employ p = 2 for all regularizations to ensure a fair comparison. However, exploring alternative norms in addition to the p norm is expected to further enhance robustness.\nMore experimental results are presented to compare the three implementations of our regularization method on the BlockMNIST dataset. Table 3 shows the results for three different models: MLP, VGG11 [51], and ResNet-18. Experimental results of models trained with our regularization, including variations with stable and efficient implementations are reported. For MLP models, we"}, {"title": "11 Adversarial Robustness Comparison", "content": "In this section, we present additional results for the comparison of adversarial robustness. Specifically, we evaluate the performance of ResNet-18 [21] trained on the CIFAR-100 dataset [29], considering both standard accuracy and adversarial accuracy with the varying perturbation budget e. In Table 4 and Table 5, a comprehensive comparison of the adversarial robustness for different models is presented. The first table shows the performance under L2 adversarial PGD-20 [33] attacks, while the second table focuses on the models' performance under Lo attacks. We compare the standard trained model and three robust models"}, {"title": "12 Computational Overhead Analysis", "content": "In this section, we compare the training times of various regularization techniques. Figure 8 illustrates the results of different robust training methods, including IG-SUM [10], InputGrad [38], Score-Matching [53], PGD-5 adversarial training [33], and our proposed regularizations across three datasets: CIFAR-10, CIFAR-100, and SVHN. This efficiency advantage is crucial for large-scale applications where training time can be a bottleneck, making our methods suitable for practical deployment in resource-constrained environments. In summary, our results highlight the effectiveness of our regularizations in achieving a desirable balance between robustness and efficiency."}, {"title": "13 Robustness against Input Gradient and Density Perturbations", "content": "In this section, we present additional results regarding the robustness of models against Gaussian noise d with the increasing standard deviation in both input gradients and density on the CIFAR-100 dataset. We compare the robustness of the vanilla ResNet-18 model and models trained with three regularizations:"}, {"title": "14 Efficacy for Out-of-Distribution Detection", "content": "Out-of-distribution (OOD) detection [19,22,31] is a binary classification problem that aims to identify samples that do not belong to the in-distribution dataset. A robust model is expected to produce discriminative outputs capable of distinguishing between samples from in-distribution and out-of-distribution data. Here, we evaluate the performance of models trained with robust regularizations in detecting OOD samples. To assess OOD detection performance, we follow the recommendation of Hendrycks et al. [22] and use the area under the receiver-operating curve (AUROC) as the metric.\nTable 7 presents the OOD detection results obtained using ResNet-18 trained on the CIFAR-10 dataset [29]. In our experiments, we deploy ResNet-18 models trained with different regularizations to detect out-of-distribution samples from both CIFAR-100 and SVHN [35] datasets. The reported results include the output logits f(x;0) and the output logit fi (x; 0) of the label y = i. Both our regularization and the corresponding efficient implementation results are presented. The outcomes distinctly reveal that our regularization, along with the proposed efficient variant, significantly improves the model's performance in detecting out-of-distribution samples."}, {"title": "15 Experiments on Numerical Stability Comparison", "content": "In this section, we provide more results pertaining to the comparison of numerical stability. Figure 10 illustrates a comprehensive numerical stability analysis conducted on the BlockMNIST, CelebA, and CIFAR-100 datasets. It is observed that the baseline regularization approach exhibits severe numerical instability early in the training phase, whereas our proposed method demonstrates sustained stability throughout the training process."}, {"title": "16 Activation Visualization", "content": "In this section, we present visualization samples generated by applying gradient ascent on random inputs. We compare the visualization results of different regularizations on both WideResNet-28 [63] and ResNet-18 models trained on CIFAR-10 and CIFAR-100 respectively. Figure 11 displays the visualization results for WideResNet-28, while Figure 12 presents the results for ResNet-18. The visualization results obtained from the model using our regularization exhibit reduced noise and present more interpretable patterns. The improvement in visualization quality serves as evidence that our technique enhances the interpretability of the underlying features learned by the models. Our method is effective in enhancing the interpretability and clarity of the learned representations. The reduction of noise and the emergence of more interpretable patterns contribute to a better understanding of the model's decision-making process and aid in capturing relevant features for the respective classes."}, {"title": "17 Attribution Maps and Insertion Games", "content": "In this section, we present additional results of attribution maps and insertion games. In Figure 13, Figure 14 and Figures 15, we generate attribution maps using the Integrated Gradients method [56] and compute the area under the curve (AUC) of the insertion games for representative samples from BlockMNIST, CelebA and Waterbirds datasets."}, {"title": "18 Limitations", "content": "While evaluation results show our regularization achieves a desirable trade-off, the increased regularization strength can diminish a model's sensitivity to features, compromising the model's overall performance.\nIn the context of adversarial defense, adversarial training remains preferable for explicitly countering adversarial attacks when computational resources permit. Nonetheless, our approach, which does not depend on specific perturbations, demonstrates high performance across a range of problems, as evidenced by our experimental results."}, {"title": "19 Experimental Setup", "content": "In this section, we provide details regarding the datasets, models, and the experimental platform employed in our experiments.\nBlockMNIST. BlockMNIST dataset [47] is an extension of the MNIST dataset [30]. Each sample in BlockMNIST is derived from an original MNIST sample by adding a null block, which contains non-informative features, randomly positioned at the top or bottom of the image. During the training process, each"}]}