{"title": "Regulating Model Reliance on Non-Robust Features by Smoothing Input Marginal Density", "authors": ["Peiyu Yang", "Naveed Akhtar", "Mubarak Shah", "Ajmal Mian"], "abstract": "Trustworthy machine learning necessitates meticulous regulation of model reliance on non-robust features. We propose a framework to delineate and regulate such features by attributing model predictions to the input. Within our approach, robust feature attributions exhibit a certain consistency, while non-robust feature attributions are susceptible to fluctuations. This behavior allows identification of correlation between model reliance on non-robust features and smoothness of marginal density of the input samples. Hence, we uniquely regularize the gradients of the marginal density w.r.t. the input features for robustness. We also devise an efficient implementation of our regularization to address the potential numerical instability of the underlying optimization process. Moreover, we analytically reveal that, as opposed to our marginal density smoothing, the prevalent input gradient regularization smoothens conditional or joint density of the input, which can cause limited robustness. Our experiments validate the effectiveness of the proposed method, providing clear evidence of its capability to address the feature leakage problem and mitigate spurious correlations. Extensive results further establish that our technique enables the model to exhibit robustness against perturbations in pixel values, input gradients, and density.", "sections": [{"title": "1 Introduction", "content": "Research on mitigating model reliance on non-robust input features has recently gained increasing attention due to high-stake machine learning applications [13, 19,40,53]. In this paper, we advance this direction by introducing a regularization technique that promotes a smooth marginal probability density function of the input to regulate the model's reliance on non-robust features.\nTo distinguish between robust and non-robust features, we leverage the notion of attributions [17, 56, 64]. For a model f parameterized by 0, attributions characterize the importance of the i-th feature xi of the input x for the model"}, {"title": "2 Related Work", "content": "Regularization for Interpretability Robustness: Despite advancements in model transparency [27, 28, 43, 62], current deep neural networks still lack interpretability in their decision-making process, which is exacerbated by their reliance on non-robust input features. Prior studies, such as [1, 2, 47], identify that standard models are prone to relying on irrelevant or spuriously correlated features. To address that, several regularization techniques are proposed to improve model interpretability. In [15,45], the authors incorporated prior knowledge into the model training process to regularize the model behavior. Dombrowski et al. [12, 13] found that regularizing the input Hessian using SoftPlus activations or weight decay can boost resilience against manipulated inputs. In [19], a joint energy-based model is trained as a discriminative model for improved robustness. Srinivas and Fleuret [53] enhanced the model interpretability by improving the alignment between the implicit density and the data distribution.\nRegularization for Adversarial Robustness: In addition to the other sources of prediction unreliability, adversarial attacks can manipulate model outputs with imperceptible perturbations to inputs [18,33]. To address this, adversarial training through data augmentation with adversarial samples is widely employed [9, 33, 46]. Certified adversarial robustness through regularizations [4,38, 49] is another branch of methods to defend against adversarial perturbations. Inspired by the classic double backpropagation [14], Ross and Doshi-Velez [38] regularized the input gradient norm for adversarial robustness. Etmann [16] further explored different variants of double backpropagation regularizations for various real-world scenarios. Moosavi-Dezfooli et al. [34] also proposed regularization to encourage a low curvature for adversarial robustness. To improve adversarial robustness, Chen et al. [10] computed the norm of attributions integrated from clean samples to adversarial samples as the regularization term. Ilyas et al. [25] proposed a disentangling method to distinguish feature robustness for explaining adversarial examples. However, they do not focus on general feature robustness."}, {"title": "3 Feature Robustness by Attributions", "content": "We first provide a framework for distinguishing between robust and non-robust features by analyzing their attributions. Herein, attribution inconsistencies among the features with distinct degrees of robustness identify a correlation between the model's reliance on non-robust features and the smoothness of output logits.\nLet us consider an input sample x \u2208 Rn with label y \u2208 R from a dataset D, and a classifier f : Rn \u2192 RC parameterized by 0. We denote robust and non-robust features within the input x as Xrob,Xnrob \u2286 x. Consider an attribution method \u00a2 : RC \u2192 Rn attributing model predictions to input features by estimating their importance, resulting in an attribution map M = $(f(x;0)). Inspired by the success of attributions in model explanation, we identify robust and non-robust features by leveraging their attributions.\nWithout loss of generality, we assume that attributions M of the features can be estimated by calculating the change in output logits when these features are removed from the input, following perturbation-based methods [37,64]: Mxrob = f(x; 0) - f (x[xrob=0]; 0) and Mxnrob = f(x; 0) \u2212 f (x[xnrob=0]; 0). For ease of understanding, we alternatively use f(xnrob; 0) and f(xrob; 0) to represent attributions Mrob and Mnrob in the text to follow. We define robust features within the attribution framework as follows.\nDefinition 1. A feature Xfeat shared among different input instances under its domain Axfeat is robust if, for a randomly chosen class y = i, its attribution Mxfeat is bounded by a small constant h under a metric c(\u00b7), i.e., c(f(x feat; 0) - f(xfeat; 0)) \u2264 h : X feat \u2208 Axfeat.\nUnder Definition 1, robust features are expected to contribute consistently to the model's prediction across different input samples. Non-robust features, on"}, {"title": "4 Smoothing Marginal Density of Input", "content": "Here, we establish the relation between model robustness and gradients of the input marginal density. Then, a robust regularization is proposed for regulating model reliance on non-robust features by smoothing marginal density.\nWe commence our analytical analysis with probability density, following Bridle [7]. Given a class y = i, a joint probability density function over the input with the output logit fi(x; 0) is defined as\nPo(x, y = i) = efi(x;0)/Zo,\n(1)\nwhere the constant Zo = Sefi(x;0)dx is the partition function. Ze normalizes the input x to a probability density by integrating over all possible input points x in the input space via the model f. By applying Bayes' rule, we eliminate the condition y = i, resulting in the marginal density being defined solely on the input x: po(x) = \u0440\u04e9(y = i,x)/po(y = i|x). The conditional density function pe(y = ix) can be further defined as\nPo(y = i|x) = efi(x;0)/Zf(x;0),\n(2)\nwhere Zf(x;0) = \u2211i=1 efi(x0) is the partition function for the output logits fi(x;0) defined on all the C classes. To simplify the notation, we use Zf(x) to represent Zf(x;0) in the subsequent discussion. Exploiting the symmetry property of the joint density defined in Eq. 1, i.\u0435., \u0440\u04e9(x,y = i) = po(y = i,x), the"}, {"title": "5 Stable and Efficient Implementation for Regularization", "content": "From the implementation perspective, the gradient computation of marginal density involves multiple exponential operations in both the numerator and denominator of Eq. 4 which can introduce numerical instability in the optimization process, leading to gradient vanishing and explosion problems. Such issues can potentially hinder the application of our regularization to large non-linear models or wide-distribution data. For instance, batch normalization (BN) layers [26] solving internal covariate shifts with learnable scaling and shifting parameters can amplify the errors caused by the exponential operations during the back-propagation. In Fig. 2(a), yellow and red curves indicate original and inverse implementations for minimizing the gradient of log density \u2207xlogZ f(x). It is illustrated that the Frobenius gradient norm of both implementations undergoes rapid numerical overflow as the number of training iterations increases, revealing the serious issue of exploding and vanishing gradients using the proposed regularization. Therefore, it is crucial to address these through careful implementation to ensure the feasibility of our regularization.\nTo address this challenge, we transform the computation from the summation of exponential operations to softmax. By subtracting a constant value \u03b7 from the logits before exponentiation, softmax can prevent numerical overflow during the exponential operations, i.e., efi (2;0) / \u2211jefj(x;0) = efi(x;0)-\u03b7/\u03a3jefj(x;0)-\u03b7.\nThus, we incorporate the softmax function into the computation of log density"}, {"title": "6 Limited Robustness in Input Gradient Regularization", "content": "Input gradient regularization (InputGrad Reg.) [30,38] computes the Frobenius norm of input gradients || \u2207xfi(x;0)||F for a given class label y = i, which is a"}, {"title": "7 Experiments", "content": "In this section, we perform extensive experiments to validate the efficacy of our regularization and the newly established correlation between the smooth"}, {"title": "7.1 Efficacy against Feature Leakage and Adversarial Attacks", "content": "In [2,47], it is demonstrated that deep models end up assigning importance to irrelevant input features. Shah et al. [47] used BlockMNIST in their experiments, which is a synthetic dataset extended from MNIST [30]. \u03a4\u03bf each MNIST sample, BlockMNIST attaches a null block (an irrelevant pattern) randomly at the top or bottom of the image, as shown in Fig. 3(a). Shah et al. [47] observed that the explaining tool InputGrad [50] attributes importance to both the informative number block and the uninformative null block in the standard trained model. This phenomenon is termed as feature leakage by the authors. Interestingly, the issue is mitigated in adversarially trained models. Since this dataset allows for a controlled robustness assessment, we first evaluate our method on BlockMNIST to analyze the model's reliance on irrelevant features."}, {"title": "Reproducibility and Quantitative Measurement of Feature Leakage.", "content": "Owing to the unreliability of InputGrad caused by model saturation [48], we employ Integrated Gradients (IG) [56], an axiomatic explanation tool, to faithfully re-investigate the feature leakage phenomenon. In Fig. 3(b)-(c), we show that the attributions computed by IG can reproduce different leakage phenomena from the informative block region to the null block region on standard and adversarially trained models. Feature leakage is an important phenomenon in the context of model robustness. However, there is a lack of a quantitative metric in the current literature to quantify its extent. We use integrated gradients to define the metric Mleakage to address this gap. Mathematically,\nMleakage = E ||Xnrob X 1 \u03b1=0 df(\u03b1.xnrob; 0)) xnrob da||2,\n(9)"}, {"title": "Robustness against Feature Leakage.", "content": "Table 1 presents the experimental results on the BlockMNIST dataset. We compare our method with other robust regularizations and techniques including InputGrad [38], IG-SUM [10], SoftPlus activations [12] and Hessian [13]. InputGrad and Hessian regularize the first-order and second-order gradients w.r.t. the input. Models trained with SoftPlus activations and Hessian regularization fail to suppress the leakage problem, which indicates that feature leakage is not caused by the geometry of the model output manifold or high curvature [12,65]. InputGrad regularization demonstrates robustness against both L\u2082 and L\u221e adversarial attacks, yet it still fails to address the leakage problem. The result aligns well with Remark 3. which highlights the allowance of non-robust features across different classes in InputGrad regularization. In addition, the use of IG in IG-Norm regularization that accumulates input gradients as a regularization term results in behavior similar to Input-Grad regularization, leading to compromised robustness. These results further reveal that adversarial robustness is not a sufficient condition for suppressing feature leakage. Our method demonstrates a considerable improvement over other techniques for feature leakage, while also maintaining superiority in adversarial robustness. In Tab. 1, we use L2 norm for the compared regularization terms for fair benchmarking. We show in Supp. 10 that optimal norm exploration can yield an even more favorable trade-off for our technique for model robustness."}, {"title": "Feature Leakage in Adversarially Robust Models.", "content": "A FGSM adversarially trained model [18] augments the training samples by adversarial examples x + esign(\u2207xfi(x;0)). Notably minimizing the loss of the perturbed input x + \u03b5 \u00b7 sign(\u2207xfi(x;0)) is similar to the InputGrad regularization. Thus, training with FGSM is still limited in its ability to suppress the leakage problem. In contrast, PGD attack [33] weakens the effect of the condition y = i by iteratively searching for the perturbations from a random starting point. This process leads to a substantial enhancement in suppressing feature leakage, see Tab. 1."}, {"title": "Magnitude of Coefficient for Regularization.", "content": "Figs. 4(a)-(d) present a comparison of results for feature leakage and adversarial accuracy under PGD attacks, as well as the standard accuracy across varying magnitudes for the regularization strength. The results affirm that our method effectively regulates feature leakage by imposing a penalty on non-robust features. Moreover, our regularization enables the model to defend against both L2 and Lo attacks while maintaining high accuracy, showing an outstanding trade-off across four criteria. More adversarial robustness comparisons on CIFAR dataset [29] are reported in Supp. 11."}, {"title": "7.2 Efficacy for Spurious Correlation", "content": "Recent research has highlighted the susceptibility of neural models in learning spurious correlations that enhance performance on a given data but fail to generalize [2,8,42]. For instance, in CelebA-Hair dataset [32], which commonly consists of samples containing female celebrities with blond hair and male celebrities with dark hair, models heavily rely on the spuriously correlated gender feature to predict the target hair color [20, 42]. Consequently, accuracy tends to be lower for the samples containing male celebrities with blond hair, see Fig. 5.\nTo mitigate spurious correlations, distributional robust optimization (DRO) techniques have been proposed to re-weight the training loss of input samples from different groups [24,42]. In our regularization, an additional penalty is imposed to penalize the model's reliance on these spuriously correlated features because of their inconsistent attributions. This encourages the use of robust features while suppressing the model's reliance on spuriously correlated features. Reliable quantification of the model's robustness on natural images for spurious correlation suppression is an unresolved issue in the literature. We employ"}, {"title": "7.3 Efficacy against Pixels, Gradients and Density Perturbations", "content": "We first employ pixel perturbation [44, 60] to quantitatively compare the robustness of different models following Srinivas and Fleuret [53] who iteratively removed the most important input pixels identified by attribution maps for model robustness evaluation. Robust models are expected to exhibit increased sensitivity when removing the most important pixels and decreased sensitivity when removing the least important ones. We assess the difference in fractional output logit change between the images with the top and bottom k% most salient pixels using SmoothGrad [52] on ResNet-18 [21] trained on CIFAR-10"}, {"title": "8 Conclusion", "content": "In this paper, we define robust and non-robust features from a feature attribution perspective, and establish a correlation between the smoothness of input marginal density and model reliance on non-robust features. This connection motivates us to propose a regularization that targets the gradients of the marginal density, aiming to regulate the reliance on non-robust features. Extensive experiments demonstrate the effectiveness of our regularization in boosting model robustness across a wide range of applications. We emphasize that our approach does not advocate for the complete removal of model reliance on non-robust features, but instead seeks to achieve a balance between model performance and robustness through appropriate regularization strength."}]}