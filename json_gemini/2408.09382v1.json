{"title": "VRCopilot: Authoring 3D Layouts with Generative AI Models in VR", "authors": ["Lei Zhang", "Jin Pan", "Jacob Gettig", "Steve Oney", "Anhong Guo"], "abstract": "Immersive authoring provides an intuitive medium for users to create 3D scenes via direct manipulation in Virtual Reality (VR). Recent advances in generative AI have enabled the automatic creation of realistic 3D layouts. However, it is unclear how capabilities of generative AI can be used in immersive authoring to support fluid interactions, user agency, and creativity. We introduce VRCopilot, a mixed-initiative system that integrates pre-trained generative AI models into immersive authoring to facilitate human-AI co-creation in VR. VRCopilot presents multimodal interactions to support rapid prototyping and iterations with AI, and intermediate representations such as wireframes to augment user controllability over the created content. Through a series of user studies, we evaluated the potential and challenges in manual, scaffolded, and automatic creation in immersive authoring. We found that scaffolded creation using wireframes enhanced the user agency compared to automatic creation. We also found that manual creation via multimodal specification offers the highest sense of creativity and agency.", "sections": [{"title": "1\nINTRODUCTION", "content": "As Virtual Reality (VR) continues to gain momentum across various domains, such as education [67], gaming [56], and spatial design [63], the need for effective tools and techniques to author high-quality 3D scenes becomes increasingly important. Immersive authoring is a paradigm that leverages users' spatial capabilities to enable them to create and evaluate 3D scenes directly while immersed in the virtual environment [1, 18, 23, 30, 41, 63-65]. Prior work in immersive authoring tools has demonstrated benefits of lowering the barrier for end-users with little technical skills to create 3D content [33, 65].\nWhile existing immersive authoring tools make it intuitive for users to visualize their design concepts for 3D scenes in VR, most current 3D layouts such as architectural designs and game scenes are laboriously created through manual placement of 3D models. This manual process is not only tedious and time-consuming, but can also limit the user's ability to explore a diverse range of ideas [31]. In recent years, generative Artificial Intelligence (AI) models have emerged as powerful means for automatic generation of intelligible text [44], photorealistic images [46], videos [2], music [37], and 3D layouts [34, 45, 57]. By leveraging generative models, we can potentially provide users with automatically generated 3D layouts during the process of immersive content creation, enabling users to save time and effort while exploring alternative design possibilities.\nPrior work has demonstrated promising results in generating realistic 3D layouts [34, 45, 57] and text-to-layout generation [20, 39]. However, integrating these models into immersive authoring workflows poses unique challenges of how users can collaborate and interact with generative models-specifically, understanding, controlling, and refining model outputs in immersive virtual environments. This difficulty is compounded by generative AI models' well-known issues with transparency, controllability, and user agency. Current generative models for 3D layouts use either room sizes (e.g., [45]) or text captions (e.g., [20]) as prompts. It is difficult for users to define their design objectives, such as requesting layout designs with elements in particular locations or sizes (as seen in Fig. 1.3.b).\nIn this paper, we introduce VRCopilot, a mixed-initiative system that integrates pre-trained generative models into immersive authoring workflows. VRCopilot is instantiated in the context of layout design for indoor scenes, where users are able to co-create with generative models via requesting, controlling, and refining generative models' outputs in VR. VRCopilot introduces two key interaction techniques: (1) multimodal specification and (2) intermediate representation. Inspired by multimodal interactions such as \"Put-that-there\" [4], our system enables users to use speech and simultaneous pointing to specify their creation needs, increasing the naturalness and economy of language description in the immersive environments. For instance, users can point to a location in the room while saying \"create a wooden chair here.\" As a response, the system will offer three options for the user to choose from. Besides, to help users co-create with the generative model in a more transparent and controllable way, VRCopilot proposes the notion of wireframes as intermediate representations for the generated outcomes. Inspired by the concept of low-fidelity prototyping in Human-Computer Interaction [7, 47, 50], wireframes are 2D representations of 3D layouts similar to floor plans in interior design. These representations can be hand-drawn by users together with speech specifying the their types, or suggested by generative models. VRCopilot allows users to iteratively refine the design with generative Al by enabling them to convert between intermediate representations and 3D layouts.\nTaking the above techniques together, we propose three ways of human-Al co-creation in VR enabled by VRCopilot: (1) manual creation, where users create individual objects to complete a layout design via a catalog menu and multimodal specification; (2) automatic creation, where users request and refine suggestions from generative Al for full-room layouts; and (3) scaffolded creation, where users co-create intermediate representations with generative Al for guiding the final layout design.\nTo provide an in-depth understanding of the human-AI co-creation process in VR, we conducted two rounds of user studies. Our first study aimed to compare user experiences of creating 3D layouts with and without AI. Specifically, we compared creation without AI using manual placement and creation with AI using generative models. We found that co-creating 3D layouts with generative models is generally more preferable as it could save users' effort while resulting in 3D layouts with more complete functionality and diverse color palette. However, users struggled with the generative model's non-deterministic output, where the generated results might misalign with the user's design goals due to the lack of controllability of the generative model.\nBased on the insights and challenges from the first study, we further evaluated VRCopilot by comparing different levels of AI automation in the creation process including manual creation, automatic creation, and scaffolded creation. We found that users' sense of agency significantly increases in the order of automatic creation, scaffolded creation, and manual creation. Specifically, the design of wireframes in scaffolded creation enhances users' agency by allowing them to define the 3D layout including object types and sizes compared to automatic creation. Manual creation offers the highest agency by enabling additional visualization and control over object styles. We also found that users felt significantly more creative in manual creation, than in scaffolded or automatic creation, with no significant difference found between the latter two. Specifically, having multiple suggestions via multimodal specification in manual creation can make users feel more creative. Users felt less creative in the other two conditions since AI generated outcomes could lead to fixation and prohibit users from creative exploration.\nIn sum, our paper makes the following contributions: 1) VRCopilot, an immersive authoring system that enables users to interact and co-create with generative Al models in virtual immersive environments; and 2) empirical results gained from two user studies that provide insights on user experiences such as perceived agency and creativity, as well as potential and challenges of human-AI co-creation in immersive authoring workflows."}, {"title": "2 RELATED WORK", "content": "VRCopilot draws inspiration from prior literature on 3D scene synthesis using generative models, creativity support with generative design, and interactive interfaces with computational agents."}, {"title": "2.1\nGenerative Models for 3D Scenes", "content": "The demand for automatically generating 3D scenes has never been higher in the domain of gaming, AR & VR, architecture and interior design. In the field of computer vision, this topic named 3D scene synthesis is gaining popularity and prior researchers have explored generating new 3D scenes via various input including images [22, 36], text [16, 62], or room shape [45]. A key line of work is 3D indoor scene synthesis, which refers to the task of automatically generating a set of 3D furniture objects along with their positions and orientations, given a room layout [66]. Some of the early work in this space offered suggestions using hardware-accelerated Monte Carlo sampler based on interior design guidelines [40]. Follow up work has been focused on data-driven approaches, given the rise of large 3D object datasets such as SUNCG [51] and 3D-FRONT [21]. The data-driven approaches can be approximately categorized into graph-based [57] and autoregression-based approaches [45, 48, 58, 59]. Graph-based approaches encode 3D layouts as scene graphs, where objects are nodes, and the spatial relationship between objects are edges. This method treats the task of generating 3D scenes as generating directional graphs. The main motivation behind this is to process it with graph convolutional networks. Most notably, Ritchie et al. [48] introduced a CNN-based architecture that operates on a top-down image-based representation of a scene and inserts objects in it sequentially by predicting their category, location, orientation, and size. More recently, autoregression-based approaches have been introduced. Wang et al. introduced SceneFormer [59], a series of transformers that autoregressively add objects in a scene. ATISS [45] simplifies the process by proposing a single model trained end-to-end to predict all attributes. Most notably, ATISS encodes 3D objects' positions, rotations, and scales in transformers for training. More recently, DiffuScene utilizes a denoising diffusion model that is able to generate more plausible and diverse indoor scenes [54].\nOur work contributes to the existing literature on 3D scene synthesis by introducing generative models into immersive environments. While prior work has been focused on generating realistic 3D layouts, VRCopilot aims to integrate state-of-the-art generative AI models into immersive authoring and explores the ways of co-creating with generative AI models in VR."}, {"title": "2.2\nCreativity Support via Steering Generative Models", "content": "The acceleration of AI capabilities has enabled human-AI co-creation in domains such as drawing [15, 19], creative writing [13], video game content creation [25], and music composition [28, 37]. For example, Bach Doodle [28] is able to complete a music composition in the style of J.S. Bach by requiring users to only write a few notes. While recent research has focused on building co-creation experiences in 2D interfaces, there has been relatively little HCI work examining how to design interactions with these state-of-the-art generative models to ensure they are effective for co-creation in the immersive environments. Our research contributes an understanding of how interactions with these AI models can be designed, how they affect the immersive authoring experience, and users' attitudes towards AI co-creation in VR.\nIntegrating existing generative AI models into creative work presents unique challenges in itself such as adapting actions of AI based on users' preferences [12, 32, 53]. Research has also observed that users desire to take initiative in their partnership with AI, and thus sought to provide steering tools to make AI align with users' creative goals. For example, TaleBrush [12] uses a combination of line sketching and natural language narration to create stories. DreamSketch [32] uses sketches as input for the generative design of 3D models. In the domain of 2D layouts, Scout [53] uses high-level constraints based on design concepts to generate multiple designs. Building on this need, our work investigates how users express their preferences to generative AI through multimodal specification and intermediate representations in VR."}, {"title": "2.3\nInteraction Techniques in Immersive Environments", "content": "Our proposed interactions are inspired by prior interaction techniques in immersive environments including multimodal interaction, spatial interaction, and world in miniature (WIM). While recently there has been extensive exploration in using natural language interactions with generative AI models to build virtual scenes, using just natural languages alone might be effective for tasks such as referencing [8] in immersive environments. Building on this line of work, our system demonstrates how multimodal interaction can be used for specifying objects to generative Al in the immersive authoring process. Finally, Stoakley et al. introduced the concept of World in Miniature (WIM), which enables both navigation and interaction in a large VR scene [52]. A WIM represents the virtual environment and allows users to manipulate objects offered by the miniature, or rapidly teleport in the virtual environment by selecting locations directly in the miniature. It also has the benefit of allowing users to see a preview of the immersive virtual environment without having to travel back and forth between different views. We built on the WIM technique to enalbe users to design and edit multiple variations of the 3D layouts."}, {"title": "3 VRCOPILOT", "content": "VRCopilot is a mixed-initiative immersive authoring system that enables users to co-create 3D layouts with pre-trained generative models in VR. Users can ask generative models to generate full-room layouts or use multimodal specification to create individual objects. They can also manually place objects from a catalog menu or request suggestions from the system using multimodal interactions (i.e., pointing and speaking). VRCopilot further allows users to create wireframes - intermediate representations that help guide and refine the layout generation process. We detail our system design in the sections below."}, {"title": "3.1 Scope", "content": "We situate our design of VRCopilot in the context of interior design tasks, where users can place pre-made 3D furniture models in a virtual apartment. Interior design requires balancing constraints (e.g., functional requirements and space limitations) with aesthetic preferences. It has been the application domain of many prior immersive authoring tools [10, 29, 63] and is a common use case for Mixed Reality. For example, several popular home goods stores, including IKEA\u00b9, integrate features that allow customers to virtually preview furniture arrangements in their own homes before making a purchase. VRCopilot includes 7, 302 furniture models from 3D-FRONT [21], a large open-source dataset of furniture objects and textures.\nDesigning VRCopilot for interior design allows us to evaluate it in a realistic domain with demonstrated utility. However, we believe many of the concepts behind our design could generalize to other spatial design tasks, as the low-level tasks (e.g., object instantiation, customization, and manipulation) and multimodal interactions with generative models are broadly applicable across domains."}, {"title": "3.2 Immersive Authoring Features", "content": "VRCopilot is designed as an immersive authoring tool, meaning users design a room layout while immersed in that room (in VR)."}, {"title": "3.2.1 Importing Models.", "content": "Users can manually import furniture models into the virtual environment from a catalog menu bound to their non-dominant hand, as seen in Fig. 2a. Using the catalog menu, users are able to choose from different categories of furniture such as tables and chairs from a sub-menu. Each page on the catalog menu contains six furniture items and they can turn pages to navigate more items. After a furniture model is imported, users can manipulate and place the model via direct manipulation using the VR controllers."}, {"title": "3.2.2 Design Exploration.", "content": "The ability to explore multiple alternatives is crucial to supporting creativity in design tasks [27, 49]. For example, in the realm of interior design, designers typically develop a variety of versions to present to clients or stakeholders. To facilitate the exploration of multiple design variations, VRCopilot offers multiple empty workspaces or templates for users to work on (as seen in Fig. 2b). Users can easily switch between workspaces to work on different versions by navigating a list of miniatures in VR. This creativity support is inspired by the concept of \"World in Miniature\" (WIM) [52] and recent work on version control in VR [63]. To help users reuse partial layouts across different versions of the designs, VRCopilot also includes a copy & paste feature, shown as additional buttons bound to the handheld menu. This feature allow users to copy multiple objects and paste them either in the same workspace or other workspaces."}, {"title": "3.3 Generative Model in VRCopilot", "content": "We used ATISS [45], an open-source generative model for indoor scene synthesis using autoregressive transformers, as our generative model. ATISS is trained using an open-source dataset of 3D models called 3D-FRONT [21], from which we also build our furniture catalog. This model takes room dimensions parameters as prompts and generates reasonable furniture arrangements of the full-room layout. It is also versatile for user inputs such as asking for a suggested placement of a given furniture item, or asking for a suggested furniture item for a given location. We chose this model because it has been used as baseline models for work in the domain of indoor scene synthesis (e.g., [20, 60]).\nWe integrated ATISS in VRCopilot and can generate suggestions for full-room layouts on demand. In VRCopilot, users can access the generative model via either voice commands or the catalog menu (as shown in Fig. 2). Upon receiving the request, our system can fill the entire room by placing suggested furniture in the user's current workspace. Users can delete the suggestions and also run the generative model repeatedly. Our system supports multiple room sizes, shapes, and types (e.g., bedrooms and living rooms), and can be easily extended to support arbitrary room sizes and shapes (e.g., users can draw the room) and the backend generative model can adapt to these specifications automatically."}, {"title": "3.4 Multimodal Specification", "content": "Existing immersive authoring tools enable users to directly manipulate virtual objects similar to how they would manipulate them in the physical world. However, direct manipulation does not suffice for all needs during immersive authoring. One clear weakness of direct manipulation is that it makes it difficult to identify or manipulate a potentially large sets of objects. For example, there is a massive number of objects and styles in our furniture catalog (e.g., the catalog is based on 3D-FRONT that contains 7,302 furniture items with textures). It is difficult for users to specify generating a chair with minimalist style via direct manipulation. On the other hand, the inherent ambiguity of natural language instructions makes it difficult to use pronominal reference to objects in the scene [14]. For example, it is hard for the system to understand which location the user is referring to when the user describes \"generate a chair here\" without additional contextual information.\nInspired by multimodal specifications in graphical interfaces such as \"Put-that-there\" [4], VRCopilot allows users to use speech and simultaneous pointing to specify their creation needs, increasing the naturalness and utility of language description in the immersive environments. Our system can process users' natural language voice commands and categorize them into several possible intents:\n\u2022 Object Generation: generating individual objects;\n\u2022 Object Regeneration: regenerate individual objects;"}, {"title": "3.5 Intermediate Representation", "content": "One of the key challenges of human-AI co-creation is the lack of transparency, control, and user agency [3, 6]. To help users co-create with the generative model in a more transparent and controllable way, VRCopilot proposes the notion of wireframes that is used as intermediate representations for the generated outcomes. We took inspirations from low-fidelity prototyping that is commonly used in Human-Computer Interaction [7, 47, 50]. For example, prior work has explored using low-fidelity prototypes such as paper prototypes to quickly scaffold user interface design [50] or Play-Doh as intermediate representations to represent high-quality 3D models [42]. In VRCopilot, wireframes are designed as 2D representations of 3D layouts such as floor plans in interior design. These representations can be hand-drawn by users together with speech specifying their types. For instance, users can use the cursor of the raycast from the controller as the pen tip. They can place the cursor on the floor and start drawing by pressing a button on the controller, while saying \"Mark this area as a bed.\" Upon the intent is recognized, the system will normalize the drawing into a rectangular plane with a text label of the object type (e.g. \"Bed\") attached to it. Users can further adjust the placement and dimension of the wireframe using direct manipulation, similar to manipulating furniture models. Users can build up intermediate representation of the full-room layout design by creating multiple wireframes in the room. Alternatively, users can ask the generative model to offer suggestions of wireframes by initiating a request similar to generating full-room layout. The system can then generate the intermediate representation of the full-room layout and visualize all generated wireframes in the room.\nIn addition, VRCopilot allows users to iteratively refine the design with generative AI by enabling them to convert between intermediate representations and 3D layouts. For example, users can use voice commands or button presses to turn their intermediate representations into actual furniture models. The system can then interpret the labels and populates the scene with detailed furniture pieces corresponding to the object type, size, and orientation as specified using each wireframe. For objects that are not placed on the floor, such as ceiling lamps, users can draw wireframes on the floor similarly to how they create other objects. The system will then automatically set the y attribute, representing the height, for these objects when populating the scene. Users can also switch back to intermediate representations from detailed furniture design, enabling an iterative design that leverages both lo-fi and hi-fi representations of the layout."}, {"title": "3.6 Ways of Human-AI Co-creation in VR", "content": "With the above generative model and interaction techniques, VRCopilot supports three ways of human-Al co-creation in VR."}, {"title": "3.6.1 Manual Creation.", "content": "Manual creation enables users to manually create a 3D layout design by creating each furniture item and its placement one after another. Such creation method uses a bottom-up approach, where users start by creating specific furniture items either via the catalog menu or multimodal specification. Once the central pieces are selected (e.g., beds, sofas), users consider how other components can be arranged within the room including placement of furniture, the flow of circulation, and how spaces will be utilized."}, {"title": "3.6.2 Automatic Creation.", "content": "Automatic creation enables users to ask the generative model to generate full-room layouts. After the suggested layout is given, users can modify the layout design based on their design goals. This could include adjusting the placement of objects to avoid overlapping objects or to remove unwanted objects."}, {"title": "3.6.3 Scaffolded Creation.", "content": "Scaffolded creation enables users to create intermediate representations, i.e., wireframes, to scaffold their designs. Such a creation method uses a top-down approach where users begin with a broad, overarching vision of the floor plan by creating wireframes in the immersive environments. They can draw their own wireframes and ask for generated wireframes. They can also modify the placements and sizes of wireframes, and convert between wireframes and furniture layouts."}, {"title": "3.7 Implementation", "content": "VRCopilot is developed using Unity (version 2021.3.20f1) and integrates plugins from Meta Oculus and the Microsoft Mixed Reality Toolkit (MRTK), enabling operation on Meta Quest and Rift VR headsets. The application incorporates advanced voice recognition and response capabilities through integration with the ChatGPT Audio Model (whisper-1) and Chat Model (gpt-4-turbo), with the latter hosted on a dedicated GPU server equipped with an Nvidia RTX 4090 graphics card. A comprehensive system architecture is depicted in Figure 4."}, {"title": "3.7.1 Integration with ChatGPT Models.", "content": "Interaction with ChatGPT models is facilitated through voice commands. The system captures user voice input via the microphone, converting the audio to an .mp3 format. This file is then translated into text by the ChatGPT SpeechToText model (whisper-1) through an HTTP request. The resulting text is processed by the ChatGPT Chat Model (gpt-4-turbo), which identifies the user's intent from the predefined categories and extracts relevant parameters such as furniture styles or categories. The responses, formatted as JSON, are parsed by the Unity client to execute the corresponding actions. While most actions are deterministic, actions requiring the generation of new items (e.g., \"generate a chair in a modern style\") involve a selection process from a set of items meeting the specified criteria."}, {"title": "3.7.2 Communication with the Generative Model.", "content": "For tasks that involve the generation of new furniture, VRCopilot employs socket communication with a generative AI model, ATISS. Furniture attributes (unique ID, position, rotation, scale) are encoded in JSON and sent to the server. Upon completion, the server returns a JSON response with the furniture items that meet the established criteria, which the Unity client then processes and renders in the virtual environment."}, {"title": "3.7.3 Multimodal Feedback Module.", "content": "To enhance user interaction, VRCopilot integrates a feedback loop through AWS Polly Text-ToSpeech model. After processing an intent, the system generates textual feedback corresponding to the user's request, which is then converted into speech. This multimodal feedback mechanism provides real-time auditory confirmation of actions taken within the virtual environment, enriching the user experience."}, {"title": "4 USER STUDY 1", "content": "To understand the effectiveness and challenges of co-creating with generative Al in immersive environments, we fist sought to compare immersive authoring with and without AI. Prior research has provided some insights on how people collaborate with generative Al in creative domains (e.g. music [37] and painting [11]). We extend this line of work by understanding people's behaviors and attitudes when working with generative AI in virtual immersive environments. We conducted a qualitative comparison study between two conditions: 1) immersive authoring using the conventional interfaces (e.g., via direct manipulation and menu selection), 2) immersive authoring with conventional interfaces and generative AI models. We use this study as the first stop to eliciting the challenges that users perceived when co-creating with AI in VR."}, {"title": "4.1 Participants", "content": "We recruited 14 participants (10 women and 4 men, age 20-28) from a university through public email lists. All participants had prior experience using VR devices and were compensated with $30 USD Amazon gift cards for two hours of their time."}, {"title": "4.2 Procedure", "content": "During the study, users were first given a tutorial of the system that covered individual features of the system including the control of direct manipulation and the usage of the generative model. The tutorial lasted about 30 minutes. Then, participants were asked to design an empty apartment, consisting of two bedrooms and one living room, under two conditions: 1) with conventional immersive authoring interface, 2) with the conventional interface and the generative Al model. The room sizes and types were pre-configured, in order to encourage participants to focus on the co-creation process. The order of the conditions was counterbalanced. Each condition took about 15 minutes to complete. In each condition, participants were asked to aim for finishing three versions of the apartment with at least three items in each room. This instruction was not a strict requirement, but rather a means to encourage participants to design multiple variations of the apartment. After both conditions were finished, we conducted a retrospective interview with participants. Our study protocol was approved by our institution's IRB."}, {"title": "4.3 Analysis", "content": "We transcribed and conducted a thematic analysis [5] of the interview data. To assess the creation results from participants, we designed an evaluation that elicits emerging patterns of users' creation through an evaluation workshop. One design expert, a full-time architect with 2 years of working experience, was invited to participate an evaluation workshop with one experimenter that took 90 minutes. The evaluation workshop was held remotely where the experimenters screen shared to the expert. The expert then went through the top down images of the creation results from all participants under each condition. The order of showing the creation results is completely randomized and the expert was not informed of how the design were created under each condition. Then the expert were asked to use an inductive approach to observe the top down images under each condition and use open-coding to elicit emerging patterns in each condition."}, {"title": "4.4 Results and Insights", "content": "Below are the insights gained from the qualitative user study and the expert evaluation:\nGenerative models provide less user agency. Agency refers to the awareness and control over one's action and their results [61]. We found that participants reported feeling less agency over the creation results when co-creating with AI. While the generative Al models could make meaningful layout suggestions that help users explore different ideas, the generated suggestions sometimes misaligned with users preferences in terms of the functionality and other considerations of the layout design. For instance, P2 said \"I really have no idea what was going to come out when I did it [using the generative model], like I did not at all expect a bookcase in the middle of the living room, even if it would make sense for that room.\" P7 also commented on their agency when comparing creating with and without AI: \"When there's no AI intervention in the process it is just me thinking about what is the best circulations? What is the best looking furniture to be placed in the room? Those are my primary concern when I was doing it. So I will say I was the most in control when I was doing the first task [without generative Al models].\"\nGenerative models are useful for sparking different ideas. One key dimension of creativity is the ability to explore different ideas [9]. Participants reported that the results from generative AI models can provide inspirations for the layout design that they did not come up with. For example, P8 commented that \"It brings up new ideas I hadn't thought about before... also because when I first create the room, I pretty much put in what my favorite idea is for so when I create or generate a new room, it adds more inspiration than what I already had started off with.\"\nCreating with generative models can lead to more diverse functionality and color palette. Functionality refers to the ability of a space or its components to serve a specific purpose or function effectively and efficiently. We found that creation results with the help of AI encompass more diverse functionalities. Specifically, the expert observed more diverse object types in each room that can support different activities. For example, the bedrooms shown in both Fig. 5c and Fig. 5d include desks (for working), wardrobes (for clothes), and bookshelves (for storage). Color palette refers to the selection of colors used in a design, including primary, secondary, and accent colors, which contribute to the overall mood and atmosphere of a space. We found that creation results generated with Al generally have a richer color palette (seen in Fig. 5), which contributes to the expert commenting the creation \"more exciting.\"\nCreating with generative models can lead to poor considerations of circulation and daylighting. Circulation refers to the flow or movement of people within a space. It encompasses the pathways, routes, and patterns that individuals follow as they navigate and move through an interior environment. We found that creation results generated with AI generally have a poorer circulation. For example, one of the bedrooms shown in Fig. 5c includes a nightstand that is blocking the doorway. The dining table in the living/dining room in Fig. 5d does not allow for much movement between the two sides due to its close placement to the walls. This is because our underlying generative model (i.e., ATISS) that we utilize does not take doorway or room of movements into consideration when generating. Daylighting in interior design is a design strategy that focuses on harnessing and optimizing natural daylight to illuminate interior spaces. We found that creation results generated with AI generally have a poorer consideration of daylighting. For example, both bedrooms shown in Fig. 5c have furniture blocking the windows, making it difficult to harness daylight. This is due to the underlying generative model (i.e., ATISS) that we utilize does not take window placement, size and shape into consideration.\nBased on these findings around using generative models, our research team investigated further in the second round of study, that was specifically focused on the mitigation of the issue of user agency and the comparison across different ways of human-AI co-creation (as described in Section 3.6). We were also able to design tasks for the second study based on the patterns drawn from the expert evaluation session to further develop our ideas. We describe the second user study in the following section."}, {"title": "5 USER STUDY 2", "content": "We conducted a second user study to compare three conditions: 1) manual creation using catalog menus and multimodal specification, 2) scaffolded creation using wireframes, and 3) automatic creation using generative AI. We aimed to compare user perceived effort, creativity, and agency, and to elicit potential and challenges that users perceived when co-creating with AI in VR."}, {"title": "5.1 Participants", "content": "We recruited another 15 participants (5 women and 10 men, ages 19-26) through university email lists. All participants had prior experience using VR devices and did not participate in the previous study (Section 4). We labeled the participants as P15-29 below. Each participant was compensated with a $30 USD Amazon gift card for two hours of their time."}, {"title": "5.2 Procedure", "content": "We designed a within-subject study where each participant experienced all three conditions during the study. Balanced Latin-Square was used to determine the order of the conditions for each participant. For the study setup, we used the Meta Quest 2 connected to a laptop that was running our system in Unity 3D game engine. Each study session began with an introduction of the study and a tutorial of the system that lasted about 30 minutes. During the introduction, participants were introduced to the study and were informed of all the data that would be collected during the study. Participants were then given a tutorial of individual features of VRCopilot. They were given an atomic task after learning each feature to familiarize themselves with the system.\nAfter the tutorial, participants performed a design task under each condition, where they furnished an empty bedroom in VR. In each condition, they were asked to come up with three design solutions for the same room within 15 minutes. If more than three versions were created, they would be asked to turn in the three versions that they were most satisfied with. The following design goals were given to participants for each condition:\n\u2022 There should be at least 4 furniture types in the bedroom.\n\u2022 Make sure the top of the window is not blocked by wardrobes / shelves / bookcases.\n\u2022 There should be enough space for users to navigate in the room.\n\u2022 There should be a sofa to accommodate seating.\n\u2022 Try to make the three versions different in both layouts and appearance.\nThe design goals were created based on design considerations drawn from the expert evaluation in the previous study (Section 4) including functionality, day-lighting, navigation, seating, and diversity. Participants were encouraged to design multiple variations of the room based on the design goals. They were notified every five minutes during the task. They were also free to ask for time remaining as well as clarification questions related to the task or the system. However, experimenters were not allowed to give any instructions on how to design the room. After finishing each task, participants were asked to fill out a questionnaire while we saved the resulted scenes and the screen recordings from that condition. After experiencing all conditions, we conducted a semi-structured interview with participants to ask about user experience and their perceptions of each condition. Each interview lasted about 20 minutes. Our study protocol was approved by our institution's IRB."}, {"title": "5.3 Measures and Analysis", "content": "We evaluated the following metrics via post-task surveys. Participants rated the items on a 7-point Likert scale (1=Strongly Disagree", "aspects": 1, "26": "questionnaire; (2) user perceived creativity from the Creativity Support Index [9", "55": "and Lukoff et al. [38"}]}