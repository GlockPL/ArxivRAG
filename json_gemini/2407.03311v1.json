{"title": "Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations", "authors": ["Trevor Ablett", "Bryan Chan", "Jayce Haoran Wang", "Jonathan Kelly"], "abstract": "Learning from examples of success is an appealing approach to reinforcement learning that eliminates many of the disadvantages of using hand-crafted reward functions or full expert-demonstration trajectories, both of which can be difficult to acquire, biased, or suboptimal. However, learning from examples alone dramatically increases the exploration challenge, especially for complex tasks. This work introduces value-penalized auxiliary control from examples (VPACE); we significantly improve exploration in example-based control by adding scheduled auxiliary control and examples of auxiliary tasks. Furthermore, we identify a value-calibration problem, where policy value estimates can exceed their theoretical limits based on successful data. We resolve this problem, which is exacerbated by learning auxiliary tasks, through the addition of an above-success-level value penalty. Across three simulated and one real robotic manipulation environment, and 21 different main tasks, we show that our approach substantially improves learning efficiency. Videos, code, and datasets are available at https://papers.starslab.ca/vpace.", "sections": [{"title": "1 Introduction", "content": "Robotics presents a unique challenge to learning algorithms: feedback is often formulated as a manually-defined dense reward function, or demonstration trajectories of an expert completing the task, both of which can be difficult to acquire, suboptimal, or biased. Ensuring that expert trajectories are nearly optimal, or, alternatively, learning an optimal policy from mixed or suboptimal data, are both challenging, open problems in imitation learning [1]. Sparse reward functions are less biased [2], but require significant exploration and can be non-trivial to acquire.\nWe consider another form of feedback-example states of completed tasks. Obtaining example states can be far less laborious than designing a reward function or gathering trajectories: practitioners can gather states from a distribution that represents a completed task without consideration of how the states are reached by the expert. Learning from example states, equivalently referred to as example-based control [3], can be inefficient, however. Similar to sparse rewards, the example states provide no information about which actions led to the goal state(s). In robotics applications, this inefficiency is particularly undesirable, since executing suboptimal policies is costly both in terms of potentially destructive environmental effects and time. In this work, we aim to address the following question:\nIs it possible to learn policies efficiently given only example states of completed tasks?\nTo answer this question, we propose a new example-based control method, value-penalized auxiliary control from examples (VPACE). Our approach is inspired by LfGP [4], a method that introduces the use of a scheduler and expert trajectories of auxiliary tasks to improve exploration. We build upon this idea to leverage only example states rather than expert trajectories. Our contributions are fourfold: (i) We find that the na\u00efve application of scheduled auxiliary tasks to example-"}, {"title": "2 Related Work", "content": "Sparse rewards are a desirable form of feedback for learning unbiased, optimal policies in reinforcement learning (RL), but they can be difficult to obtain, and present an immense exploration challenge on long-horizon tasks [7]. Reward shaping [8] and dense rewards can help alleviate the exploration problem in robotics [9, 10, 11], but designing dense rewards is difficult for practitioners [12], and can lead to surprising, biased, and suboptimal policies. An alternative to manually-defined rewards is to perform inverse RL (IRL), in which a reward function is recovered from demonstrations, and a policy is learned either subsequently [13, 14, 15] or simultaneously, in a process known as adversarial imitation learning (AIL) [5, 6, 16, 17, 18]. AIL actor-critic approaches can be further divided into methods that learn both a value function and a separate reward model [16, 17] or methods that learn a value function only [5, 6, 18].\nLike dense rewards, full trajectory demonstrations can be hard to acquire, suboptimal, or biased. Unlike IRL/AIL, in example-based control (EBC), a learning agent is only provided distributions of single successful example states. Previous EBC approaches include using generative AIL (GAIL, [16]) directly (VICE, [19]), soft actor critic (SAC, [20]) with an additional mechanism for generating extra success examples (VICE-RAQ, [21]), learning the goal distribution as a reward function (DisCo RL, [22]), performing offline RL with conservative Q learning (CQL, [23]) and a learned reward function [24], and using SAC with a classifier-based reward (RCE, [3]).\nAll EBC methods can naturally suffer from poor exploration, given that success examples are akin to sparse rewards. Hierarchical reinforcement learning (HRL) aims to leverage multiple levels of abstraction in long-horizon tasks [25], improving exploration in RL both theoretically [26, 27, 28] and empirically [29, 30]. Scheduled auxiliary control (SAC-X, [31]) combines a scheduler with semantically meaningful and simple auxiliary sparse rewards. SAC-X has also been extended to the domain of imitation learning with full trajectories (LfGP, [4, 32, 33]). Our approach builds on"}, {"title": "3 Example-Based Control with Value-Penalization and Auxiliary Tasks", "content": "Our goal is to generate an agent, with as few environment interactions as possible, that can complete a task given final state examples of both the successfully completed task and a small set of reusable auxiliary tasks, with no known reward function or full-trajectory demonstrations. We begin by formally describing the problem setting for example-based control in Section 3.1. In Section 3.2, we describe how scheduled auxiliary tasks can be applied to example-based control. Finally, motivated by the increased exploration diversity of the multitask framework, we propose a new Q-estimation objective in Section 3.3 that leverages value penalization for improved learning stability."}, {"title": "3.1 Problem Setting", "content": "A Markov decision process (MDP) is defined as $M = (S, A, R, P, \\rho_0, \\gamma)$, where the sets $S$ and $A$ are respectively the state and action space, $P$ is the state-transition environment dynamics distribution, $\\rho_0$ is the initial state distribution, $\\gamma$ is the discount factor, and the true reward $R: S \\times A \\rightarrow \\mathbb{R}$ is unknown. Actions are sampled from a stochastic policy $\\pi(a|s)$. The policy $\\pi$ interacts with the environment to yield experience $(s_t, a_t, s_{t+1})$, generated by $s_0 \\sim \\rho_0(\\cdot)$, $a \\sim \\pi(s_t)$, and $s_{t+1} \\sim P(\\cdot|s_t, a_t)$. The gathered experience $(s_t, a_t, s_{t+1})$ is then stored in a buffer $B$, which may be used throughout learning. When referring to finite-horizon tasks, $t = T$ indicates the final timestep of a trajectory. For any variables $x_t, x_{t+1}$, we may drop the subscripts and use $x, x'$ instead when the context is clear.\nIn this work, we focus on example-based control, a more difficult form of imitation learning where we are only given a finite set of example states $s^* \\in B^*$, where $B^* \\subset S$ and $|B^*| < \\infty$, representing a completed task. The goal is to (i) leverage $B^*$ and $B$ to learn or define a state-conditional reward function $R: S \\rightarrow \\mathbb{R}$ that satisfies $R(s^*) > R(s)$ for all $(s^*, s) \\in B^* \\times B$, and (ii) learn a policy $\\pi$ that maximizes the expected return $\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{s_t \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t)]$.\nFor any policy $\\pi$, we can define the value function and Q-function respectively to be:\n$V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi} [Q^{\\pi}(s, a)]$,\t(1)\t$Q^{\\pi}(s, a) = R(s) + \\gamma \\mathbb{E}_{s' \\sim P} [V^{\\pi}(s')]$,\t(2)\ncorresponding to the return-to-go from state $s$ (and action $a$). Both the value function and the Q-function for any policy $\\pi$ satisfy the above Bellman equations that can be used for reinforcement learning (RL), specifically by temporal difference (TD) algorithms.\nOne way to learn the reward function is through adversarial imitation learning (AIL)\u2014the learned reward function $R$ is derived from the minimax objective [5, 16, 17]:\n$\\mathcal{L}(D) = \\mathbb{E}_{s \\sim B} [\\log (1 - D(s))] + \\mathbb{E}_{s^* \\sim B^*} [\\log (D(s^*))]$,\t(3)\nwhere $D$ attempts to differentiate the occupancy measure between the state distributions induced by $B^*$ and $B$. The output of $D(s)$ is used to define $R(s)$, which is then used for updating the Q-function with Eq. (2). In the single-task regime, this is algorithmically identical to state-only AIL approaches with full demonstrations [34, 35]."}, {"title": "3.2 Learning a Multitask Agent from Examples", "content": "Example-based control presents a challenging exploration problem for complex tasks. We alleviate this problem by adapting learning from guided play (LfGP) [4], an approach for improving exploration by learning from auxiliary-task expert data, in addition to main task data. Auxiliary tasks are selected to have semantic meaning (e.g. reach, lift). Individual task definitions, and sometimes the auxiliary example data themselves, are reusable between main tasks."}, {"title": "3.3 Value Penalization in Example-Based Control", "content": "A scheduled multitask agent will, by design, exhibit far more diverse behaviour than a single-task policy [4, 31]. We show in Section 4.2 that the buffer generated by this behavior, consisting of transitions resulting from multiple policies $\\pi_{\\tau}$, can result in highly unstable and poorly calibrated Q estimates, especially in example-based control. In this section, we extend TD-error minimization, a fundamental component of many off-policy RL algorithms, with a penalty that encourages Q-function outputs to stay well-calibrated with respect to the reward model. Generally, the choice of the reward model and the loss function for estimating the Q-function can greatly impact learning efficiency (see Appendices A and B for details), but our value-penalty term applies to any reward model and commonly used regression-based loss functions. This penalty applies to both the single-task and multitask regime. For simplicity, we describe value penalization for the single-task framework.\nConsider a reward model $R(\\cdot)$ where $R(s^*)$, $s^* \\in B^*$, indicates reward for successful states, while $R(s)$, $s \\in B$, for all other states. In AIL, $\\hat{R}$ is a function of $D$, while in SQIL [6], for example, $R(s^*) = 1$ and $R(s) = 0$. Assuming that $s^*$ transitions to itself, then for policy evaluation with the mean-squared error (MSE), we can write the TD target, $y: S \\times S \\rightarrow \\mathbb{R}$, of Q-updates as\n$y(s, s') = R(s) + \\gamma \\mathbb{E}_{a'} [Q(s', a')]$,\t(6)\t$y(s^*, s^*) = R(s^*) + \\gamma \\mathbb{E}_{a'} [Q(s^*, a')]$,\t(7)\nwhere $(s, \\cdot, s') \\sim B$, $a' \\sim \\pi(\\cdot|s')$, and $s^* \\sim B^*$. Eq. (7) can be replaced with $y(s^*, s^*) = R(s^*)$ if one considers successful states to be terminal, but this can cause bootstrapping errors when a task times out or does not terminate upon success [36], both of which are common practice in robotics environments. Regressing to TD targets Eqs. (6) and (7) will eventually satisfy the Bellman equation, but in the short term the targets do not satisfy $y(s, s') < y(s^*, s^*)$. This is because the TD targets leverage bootstrapping of the current Q-estimate, an estimate that may not satisfy the Bellman equation and can exceed the bounds of valid Q-values, implying that approximation error of $Q$ updated with the MSE can be uncontrollable.\nWe introduce a simple resolution to this issue by adding a penalty to our TD updates for $s \\in B$ based on the current estimate of $\\mathbb{E}_{s^* \\sim B^*} [V^{\\pi} (s^*)]$. This penalty term enforces the Q-estimate to focus on outputting valid values and drives the TD targets to satisfy the inequality $y(s, s') \\leq y(s^*, s^*)$. We introduce both a minimum and maximum value for $Q^{\\pi} (s, a)$ respectively as"}, {"title": "4 Experiments", "content": "Through our experiments, we seek to understand if VPACE improves stability and efficiency in example-based control. We also complete an ablation study of various hyperparameter options, and finally analyze the learned values for agents with and without value penalization."}, {"title": "4.1 Experimental Setup", "content": "We learn agents in a large variety of tasks and environments, including those originally used in LfGP [4] and RCE [3]. Specifically, the tasks from [4] involve a simulated Franka Emika Panda manipulator, a blue and green block, a fixed \"bring\" area for each block, and a small slot with <1 mm tolerance for inserting each block. The environment has a single, shared observation space and action space with multiple options for main and auxiliary tasks. We additionally study all tasks"}, {"title": "4.2 Main Task Performance Results", "content": "Our results for all algorithms for four of the most challenging main tasks we examined are shown at the top of Fig. 4, while the bottom shows normalized average results for all tasks, separated by environment. Our real Panda results are shown in Fig. 3b. Since the Sawyer and Adroit tasks do not include specific success evaluation metrics, we only report returns. Policies are evaluated at 25k (environment step) intervals for 50 episodes for the simulated Panda tasks, 10k intervals for 30 episodes for the Sawyer and Adroit tasks, and 5k intervals for 10 episodes for the real Panda tasks.\nOur results clearly show the benefits of combining auxiliary task exploration with value penalization. For most tasks, VPACE-SQIL learns faster and more stably than any other method. Notably,"}, {"title": "4.3 Ablations, Data Quantity, and Comparison to Full Trajectories and Sparse Rewards", "content": "We completed experiments with many variations from our original implementation of VPACE-SQIL in Unstack-Stack (see Fig. 5a). Our main experiments used $\\lambda = 10$ for value penalization strength, but we also tested $\\lambda = \\{1, 100\\}$, and found a negligible effect on performance, indicating robustness to $\\lambda$ choice. We tested two different sizes of $B$ (for all tasks $T$ in $\\mathcal{T}_{all}$), $|B| = \\{10, 100\\}$, while the main experiments used $|B| = 200$ (following [3]). We found that $|B| = 100$ had a negligible effect on performance, but $|B| = 10$ slowed learning and impaired final performance. Even with $|B| = 10$, VPACE-SQIL outperformed all baselines with $|B| = 200$, apart from VPACE-DAC.\nA natural question regarding our approach is how its performance compares to more traditional approaches, such as using full expert trajectories and inverse reinforcement learning (IRL), or using RL with true sparse rewards. To test the former, we added full trajectories to each $B_\\tau$ (labelled +Full Trajectories in Fig. 5a, +Full Trajectories & Actions for learning from actions as well, and VP-SQIL +Full Trajectories for single-task only), effectively making our approach similar to [4] but with value-penalization. Intriguingly, peak performance is reduced in this setting (especially without ACE), which we hypothesize is because the agent now has to minimize divergence between $B$ and $B^*$ for many non-successful states, leading to an effect, commonly seen with dense reward functions, known as reward hacking [38]. This result suggests that inverse RL/adversarial IL can be significantly improved by switching to example-based control, but further investigation is required. To test RL with sparse rewards, we removed $B_\\tau^*$ entirely, and instead use a ground truth $R(s, a)$ from"}, {"title": "4.4 Value Penalization and Calibration of Q-Values", "content": "While our performance results show that value penalization improves performance, they do not explicitly show that $y(s,s') \\leq y(s^*, s^*)$ (see Section 3.3). To verify that this goal was met, we took snapshots of each learned agent at 500k steps and ran each policy for a single episode, recording per-timestep Q-values. Instead of showing Q-values directly, we show $Q(s_t, a_t) - \\mathbb{E}_{s^* \\sim B^*, a^* \\sim \\pi(\\cdot|s^*)} [Q(s^*, a^*)]$, which should be close to 0 when an episode is completed successfully, and should never climb above 0 for $y(s, s') \\leq y(s^*, s^*)$ to hold. We show the results of doing so in Unstack-Stack in Fig. 5b (see Appendix E.2 for other tasks).\nBoth VP-SQIL and VPACE-SQIL clearly do not violate $y(s, s') \\leq y(s^*, s^*)$, while both SQIL and ACE-SQIL do. ACE-SQIL, in particular, has no values, on average, where $y(s, s') \\leq y(s^*, s^*)$, indicating that it has learned poorly calibrated estimates for Q. This is reflected in our main performance results, where, in the most difficult Panda tasks in particular, the improvement from ACE-SQIL to VPACE-SQIL is pronounced. Consequently, scheduled auxiliary control can produce poorer policies than its single-task alternative unless it is coupled with value penalization."}, {"title": "5 Limitations", "content": "VPACE suffers from several limitations, though many of them are inherited from the use of reinforcement learning and learning from guided play. For an expansion of this section, including these inherited limitations, see Appendix F. In this work, we exclusively learn from numerical state data, rather than raw images, and raw images may be required for tasks involving objects that are not rigid. As well, we claim that example distributions are easier to generate than full expert trajectories, but for certain tasks, generating these example distributions may also be challenging. Finally, tasks we investigate in this work have roughly unimodal example success state distributions, and our method may not gracefully handle multimodality."}, {"title": "6 Conclusion", "content": "In this work, we presented VPACE\u2014value-penalized auxiliary control from examples, where we coupled scheduled auxiliary control with value penalization in the example-based setting to significantly improve learning efficiency and stability. Our experiments revealed that scheduled auxiliary control can exacerbate the learning of poorly-calibrated value estimates, which can significantly harm performance, and we alleviated this issue with an approach to value penalization based on the current value estimate of example data. We theoretically showed that our approach to value penalization still affords an optimal policy. We empirically showed that value penalization, together with scheduled auxiliary tasks, greatly improves learning from example states against a set of state-of-the-art baselines, including learning algorithms with other forms of feedback. Opportunities for future work include the further investigation of learned approaches to scheduling, as well as autonomously generating auxiliary task definitions."}, {"title": "A Reward Model Formulations", "content": "In this section, we investigate approaches to off-policy reinforcement learning (RL) studied in this work, modified to accommodate an unknown $R$ and the existence of an example state buffer $B^*$.\nA.1 Learning a Reward Function\nThe most popular approach to reward modelling, known as inverse RL, tackles an unknown $R$ by explicitly learning a reward model. Modern approaches under the class of adversarial imitation learning (AIL) algorithm aim to learn both the reward function and the policy simultaneously. In AIL, the learned reward function, also known as the discriminator, aims to differentiate the occupancy measure between the state-action distributions induced by expert and the learner. In example-based control, the state-conditional discriminator loss is Eq. (3), where $D$ attempts to differentiate the occupancy measure between the state distributions induced by $B^*$ and $B$. The output of $D(s)$ is used to define $R(s)$, which is then used for updating the Q-function using Eq. (6).\nIn example-based control, the discriminator $D$ provides a smoothed label of success for states, thus its corresponding reward function can provide more density than a typical sparse reward function, making this approach an appealing choice. Unfortunately, a learned discriminator can suffer from the deceptive reward problem, as previously identified in [4], and this problem is exacerbated in the example-based setting. In the following sections, we describe options to remove the reliance on separately learned discriminators.\nA.2 Discriminator-Free Reward Labels with Mean Squared Error TD Updates\nA simple alternative to using a discriminator as a reward model was initially introduced as soft-Q imitation learning (SQIL) in [6]. In standard AIL algorithms, $D$ is trained separately from $\\pi$ and $Q^{\\pi}$, where $D$ is trained using data from both $B$ and $B^*$, whereas $\\pi$ and $Q^{\\pi}$ are trained using data exclusively from $B$. However, most off-policy algorithms do not require this choice, and approaches such as [4, 39] train $Q$, and possibly $\\pi$, using data from both $B$ and $B^*$. It is unclear why this choice is often avoided in AIL, but it might be because it can introduce instability due to large discrepancy in magnitudes for Q targets given data from $B$ and $B^*$. Sampling from both buffers, we can define $R(s_t, a_t)$ in Eq. (2) to be labels corresponding to whether the data is sampled from $B$ or $B^*$-in SQIL, the labels are respectively 0 and 1. The full training objective resembles the minimax objective in Eq. (3) where we set $D(s^*) = 1$ and $D(s) = 0$.\nThe resulting reward function is the expected label $\\mathbb{R}(s, a) = \\mathbb{E}_{s \\sim \\text{Categorical } \\{B, B^*\\}} [\\mathbb{I}(B = B^*)]$, where Categorical, is a categorical distribution corresponding to the probability that $s$ belongs to buffers $B, B^*$. Consequently, only successful states $s^*$ yields positive reward and the corresponding optimal policy will aim to reach a successful state as soon as possible. If we further assume that $s^*$ transitions to itself, then for policy evaluation with mean-squared error (MSE), we can write the temporal difference (TD) target, $y$, of Q-updates with:\n$y(s, s') = \\gamma \\mathbb{E}_{a'} [Q(s', a')]$,\t(11)\t$y(s^*, s^*) = 1 + \\gamma \\mathbb{E}_{a'} [Q(s^*, a')]$,\t(12)\nwhere $(s, \\cdot, s') \\sim B$, $a' \\sim \\pi(a'|s')$, and $s^* \\sim B^*$.\nThis approach reduces complexity as we no longer explicitly train a reward model, and also guarantees discrimination between data from $B$ and $B^*$.\nA.3 Discriminator-Free Reward Labels with Binary Cross Entropy TD Updates\nEysenbach et al. [3] introduced recursive classification of examples (RCE), a method for learning from examples of success. RCE mostly follows the approach outlined above in Appendix A.2 but"}, {"title": "B Why Does SQIL Outperform RCE?", "content": "Our results show that VPACE-SQIL outperforms ACE-RCE, and that VP-SQIL and SQIL outperform RCE in almost all cases. This result is in conflict with results from [3], which showed SQIL strongly outperformed by RCE. In this section, we show results that help explain our findings.\nEysenbach et al. [3] claimed that the highest performing baseline against RCE was SQIL, but it is worth noting that their implementation is a departure from the original SQIL implementation, which uses MSE for TD updates, described in Appendix A.2. Furthermore, Eysenbach et al. [3] also noted that it was necessary to add n-step returns to off-policy learning to get reasonable performance. In their experiments, however, this adjustment was not included for their version of SQIL with BCE loss. We complete the experiments using their implementation and show the average results across all RCE environments in Fig. 6, and also compare to using SQIL with MSE (without value penalization or auxiliary tasks) for TD updates.\nThese results empirically show that RCE and SQIL with BCE loss perform nearly identically, indicating that benefits of the changed TD targets and weights described in Appendix A.3 may not be as clear as previously described. Furthermore, SQIL with MSE clearly performs better on average, although it still performs worse than VPACE (see Appendix E). While these empirical results demonstrate that example-based control with BCE loss for TD updates is outperformed by SQIL with MSE, Eysenbach et al. [3] had theoretical results indicating that RCE should still learn an optimal policy. In the following section, we describe a flaw found in one of their proofs that may help to further understand why RCE is outperformed by SQIL with MSE.\nB.1 Re-Examination of the Proofs from Section 4 of RCE [3]\nIn this section, we outline potential flaws in the proofs of Lemma 4.2 and Corollary 4.2.1 in RCE [3]. We then provide a lemma that shows RCE can be considered as recovering a specific reward function, thereby unifying RCE with other IRL algorithms.\nTo begin, recall that the RCE objective (i.e. Eq. (7) of Eysenbach et al. [3]) is defined as follows:\n$\\mathcal{L}^{\\pi} (\\theta) := \\mathbb{P}(e_{t+} = 1) \\mathbb{E}_{p(s_t, a_t | e_{t+} = 1)} [\\log C_{\\theta}^{\\pi} (s_t, a_t)] + \\mathbb{E}_{p(s_t, a_t)} [\\log (1 - C_{\\theta}^{\\pi} (s_t, a_t))]$.\t(15)"}, {"title": "Lemma 1. (Lemma 4.2 of RCE)", "content": "In the tabular setting, the expected updates for Eq. (15) are equivalent to performing value iteration with reward function $r(s_t, a_t) = (1 - \\gamma)p(e_t = 1|s_t)$ and a Q-function parameterized as $Q(s_t, a_t) = \\frac{C^{\\pi}_{\\theta} (s_t, a_t)}{1 - C^{\\pi}_{\\theta} (s_t, a_t)}.$\nCorollary 1. (Corollary 4.2.1 of RCE) RCE converges in the tabular setting.\nIn Lemma 1, we find that the proof intends to demonstrate equivalence with policy evaluation. In particular, the temporal difference (TD) target is defined to be $y = r(s,a) + \\gamma \\mathbb{E}_{s',a'} [Q(s', a')]$, which is optimizing the Bellman expected equation rather than Bellman optimal equation. Furthermore, the assignment equation for the ratio should in fact be an expectation over both the next state-and-action pairs (i.e. $s_{t+1} \\sim p(\\cdot|s_t, a_t)$ and $a_{t+1} \\sim \\pi(\\cdot|s_{t+1})$) instead of only the next state, even if we aim to perform policy evaluation. As a result, Corollary 1 also fails to hold if we simply use the convergence proof of value iteration."}, {"title": "Lemma 2", "content": "Fix any policy $\\pi: S \\rightarrow \\Delta(A)$. Let $\\theta^* = \\arg \\min_{\\theta} \\mathcal{L}^{\\pi} (\\theta)$ be the optimal solution of Eq. (15). Consider an MDP with reward function $r(s_t, a_t) = (1 - \\gamma)p(e_t = 1|s_t, a_t)$. Then, in the tabular setting, $Q^{\\pi} = \\frac{C^{\\pi}}{1-C^{\\pi}}$ satisfies the Bellman expected equation. Furthermore, we have that $Q^{\\pi} (s_t, a_t) = p^{\\pi} (e_{t+} = 1 | s_t, a_t)$. That is, the solution of Eq. (15) is equivalent to finding $Q^{\\pi}.$\nProof. Fix any policy $\\pi$. Consider an MDP with reward function $r(s, a) = (1 - \\gamma)p(e = 1|s)$ and transition function $p(s'|s, a)$. We define the Bellman expected operator $T^{\\pi}: \\mathbb{R}^{S \\times A} \\rightarrow \\mathbb{R}^{S \\times A}$ to be\n$T^{\\pi}q(s, a) := r(s, a) + \\mathbb{E}_{s' \\sim p(:|s,a),a' \\sim \\pi(:|s)} [q(s', a')]$.\nNote that under the tabular setting, $T^{\\pi}$ is a $\\gamma$-contraction mapping under max-norm [40]. Thus, by Banach fixed-point theorem, we have that for any $q \\in \\mathbb{R}^{S \\times A}$,\n$Q^{\\pi} = (T^{\\pi})^{\\infty} q$.\nIn other words, performing policy evaluation on $\\pi$ with reward $r(s, a) = (1 - \\gamma)p(e = 1|s)$ yields $Q^{\\pi}$. Now, by Lemma 4.1 of Eysenbach et al. [3], we have that\n$\\frac{C_{\\theta^*}^{\\pi}(s_t, a_t)}{1 - C_{\\theta^*}^{\\pi}(s_t, a_t)} = (1 - \\gamma)p(e_t = 1|s_t) + \\mathbb{E}_{s_{t+1} \\sim p(\\cdot|s_t,a_t),a_{t+1} \\sim \\pi(\\cdot|s)} \\frac{C_{\\theta^*}^{\\pi}(s_{t+1}, a_{t+1})}{1 - C_{\\theta^*}^{\\pi}(s_{t+1}, a_{t+1})}$.\t(16)\nEq. (16) satisfies the Bellman expected equation, thus by uniqueness of the fixed-point theorem, we have that\n$Q^{\\pi}(s_t, a_t) = \\frac{C_{\\theta^*}^{\\pi}(s_t, a_t)}{1 - C_{\\theta^*}^{\\pi}(s_t, a_t)}.\nRecall that $\\frac{C_{\\theta^*}^{\\pi}(s_t, a_t)}{1 - C_{\\theta^*}^{\\pi}(s_t, a_t)}$ is the Bayes' optimal classifier for Eq. (15). Therefore, suppose we obtain the minimum of Eq. (15), that is, $\\theta^* = \\arg \\min_{\\theta} \\mathcal{L}^{\\pi} (\\theta)$, then we have that\n$p(e_{t+} = 1 | s_t, a_t) = \\frac{C_{\\theta^*} (s_t, a_t)}{1 - C_{\\theta^*} (s_t, a_t)} = Q^{\\pi} (s_t, a_t)$,\nmeaning that finding the minimum of $\\mathcal{L}^{\\pi} (\\theta)$ is equivalent to performing policy evaluation on $\\pi$.\nWith Lemma 2 and Lemma 4.3 of [3], we can view Algorithm 1 of Eysenbach et al. [3] as a form of policy iteration under the reward function $r(s, a) = (1 - \\gamma)p(e = 1|s)$, where (i) updating the classifier $\\theta$ corresponds to performing a policy evaluation step on $\\pi$, and (ii) updating the policy $\\pi$ corresponds to performing a policy improvement step. We note that, however, the RCE objective is not necessarily equivalent to policy evaluation when the true minimum of RCE, $\\theta^*$, is not obtained."}, {"title": "C Finite-Sample Analysis of Value-Penalty Regularization", "content": "In this section", "Q_\\theta": "mathbb{R}^{S \\times A} \\rightarrow \\mathbb{R} | \\theta \\in \\Theta\\}$. We define the MSE as\n$\\ell(\\theta, Z_N) = \\frac{1}{N} \\sum_{n=1}^N (Q_\\theta(S_n, A_n) - G_n)^2$,\t(17)\nwhere $Z_N = \\{(S_n, A_n, G_n)\\}_{n=1}^N$ is $N$ samples of the state-action pairs and the corresponding expected return. The value-penalty regularizer is defined as\n$h(\\theta, Z_N) = \\frac{\\lambda}{N} \\sum_{n=1}^N \\left( \\max(Q_\\theta(S_n, A_n) - Q_U, 0) \\right)^2 + \\left( \\max(Q_L - Q_\\theta(S_n, A_n), 0) \\right)^2$,\t(18)\nfor some constants $Q_L < Q_U$, corresponding to the lowest and highest possible Q-values respectively, and positive constant $\\lambda$. Then, the MSE with value-penalty regularization is defined as\n$\\ell_{reg}(\\theta"}]}