{"title": "Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations", "authors": ["Trevor Ablett", "Bryan Chan", "Jayce Haoran Wang", "Jonathan Kelly"], "abstract": "Learning from examples of success is an appealing approach to reinforcement learning that eliminates many of the disadvantages of using hand-crafted reward functions or full expert-demonstration trajectories, both of which can be difficult to acquire, biased, or suboptimal. However, learning from examples alone dramatically increases the exploration challenge, especially for complex tasks. This work introduces value-penalized auxiliary control from examples (VPACE); we significantly improve exploration in example-based control by adding scheduled auxiliary control and examples of auxiliary tasks. Furthermore, we identify a value-calibration problem, where policy value estimates can exceed their theoretical limits based on successful data. We resolve this problem, which is exacerbated by learning auxiliary tasks, through the addition of an above-success-level value penalty. Across three simulated and one real robotic manipulation environment, and 21 different main tasks, we show that our approach substantially improves learning efficiency. Videos, code, and datasets are available at https://papers.starslab.ca/vpace.", "sections": [{"title": "1 Introduction", "content": "Robotics presents a unique challenge to learning algorithms: feedback is often formulated as a manually-defined dense reward function, or demonstration trajectories of an expert completing the task, both of which can be difficult to acquire, suboptimal, or biased. Ensuring that expert trajectories are nearly optimal, or, alternatively, learning an optimal policy from mixed or suboptimal data, are both challenging, open problems in imitation learning [1]. Sparse reward functions are less biased [2], but require significant exploration and can be non-trivial to acquire.\nWe consider another form of feedback-example states of completed tasks. Obtaining example states can be far less laborious than designing a reward function or gathering trajectories: practitioners can gather states from a distribution that represents a completed task without consideration of how the states are reached by the expert. Learning from example states, equivalently referred to as example-based control [3], can be inefficient, however. Similar to sparse rewards, the example states provide no information about which actions led to the goal state(s). In robotics applications, this inefficiency is particularly undesirable, since executing suboptimal policies is costly both in terms of potentially destructive environmental effects and time. In this work, we aim to address the following question:\nIs it possible to learn policies efficiently given only example states of completed tasks?\nTo answer this question, we propose a new example-based control method, value-penalized auxiliary control from examples (VPACE). Our approach is inspired by LfGP [4], a method that introduces the use of a scheduler and expert trajectories of auxiliary tasks to improve exploration. We build upon this idea to leverage only example states rather than expert trajectories. Our contributions are fourfold: (i) We find that the na\u00efve application of scheduled auxiliary tasks to example-"}, {"title": "2 Related Work", "content": "Sparse rewards are a desirable form of feedback for learning unbiased, optimal policies in reinforcement learning (RL), but they can be difficult to obtain, and present an immense exploration challenge on long-horizon tasks [7]. Reward shaping [8] and dense rewards can help alleviate the exploration problem in robotics [9, 10, 11], but designing dense rewards is difficult for practitioners [12], and can lead to surprising, biased, and suboptimal policies. An alternative to manually-defined rewards is to perform inverse RL (IRL), in which a reward function is recovered from demonstrations, and a policy is learned either subsequently [13, 14, 15] or simultaneously, in a process known as adversarial imitation learning (AIL) [5, 6, 16, 17, 18]. AIL actor-critic approaches can be further divided into methods that learn both a value function and a separate reward model [16, 17] or methods that learn a value function only [5, 6, 18].\nLike dense rewards, full trajectory demonstrations can be hard to acquire, suboptimal, or biased. Unlike IRL/AIL, in example-based control (EBC), a learning agent is only provided distributions of single successful example states. Previous EBC approaches include using generative AIL (GAIL, [16]) directly (VICE, [19]), soft actor critic (SAC, [20]) with an additional mechanism for generating extra success examples (VICE-RAQ, [21]), learning the goal distribution as a reward function (DisCo RL, [22]), performing offline RL with conservative Q learning (CQL, [23]) and a learned reward function [24], and using SAC with a classifier-based reward (RCE, [3]).\nAll EBC methods can naturally suffer from poor exploration, given that success examples are akin to sparse rewards. Hierarchical reinforcement learning (HRL) aims to leverage multiple levels of abstraction in long-horizon tasks [25], improving exploration in RL both theoretically [26, 27, 28] and empirically [29, 30]. Scheduled auxiliary control (SAC-X, [31]) combines a scheduler with semantically meaningful and simple auxiliary sparse rewards. SAC-X has also been extended to the domain of imitation learning with full trajectories (LfGP, [4, 32, 33]). Our approach builds on"}, {"title": "3 Example-Based Control with Value-Penalization and Auxiliary Tasks", "content": "Our goal is to generate an agent, with as few environment interactions as possible, that can complete a task given final state examples of both the successfully completed task and a small set of reusable auxiliary tasks, with no known reward function or full-trajectory demonstrations. We begin by formally describing the problem setting for example-based control in Section 3.1. In Section 3.2, we describe how scheduled auxiliary tasks can be applied to example-based control. Finally, motivated by the increased exploration diversity of the multitask framework, we propose a new Q-estimation objective in Section 3.3 that leverages value penalization for improved learning stability."}, {"title": "3.1 Problem Setting", "content": "A Markov decision process (MDP) is defined as $M = (S, A, R, P, \\rho_0, \\gamma)$, where the sets $S$ and $A$ are respectively the state and action space, $P$ is the state-transition environment dynamics distribution, $\\rho_0$ is the initial state distribution, $\\gamma$ is the discount factor, and the true reward $R : S\\times A \\rightarrow \\mathbb{R}$ is unknown. Actions are sampled from a stochastic policy $\\pi(a|s)$. The policy $\\pi$ interacts with the environment to yield experience $(s_t, a_t, s_{t+1})$, generated by $s_0 \\sim \\rho_0(\\cdot), a \\sim \\pi(s_t)$, and $s_{t+1} \\sim P(\\cdot|s_t, a_t)$. The gathered experience $(s_t, a_t, s_{t+1})$ is then stored in a buffer $\\mathcal{B}$, which may be used throughout learning. When referring to finite-horizon tasks, $t = T$ indicates the final timestep of a trajectory. For any variables $x_t, x_{t+1}$, we may drop the subscripts and use $x, x'$ instead when the context is clear.\nIn this work, we focus on example-based control, a more difficult form of imitation learning where we are only given a finite set of example states $s^* \\in \\mathcal{B}^*$, where $\\mathcal{B}^* \\subset S$ and $|\\mathcal{B}^*| < \\infty$, representing a completed task. The goal is to (i) leverage $\\mathcal{B}^*$ and $\\mathcal{B}$ to learn or define a state-conditional reward function $R : S \\rightarrow \\mathbb{R}$ that satisfies $R(s^*) > R(s)$ for all $(s^*, s) \\in \\mathcal{B}^* \\times \\mathcal{B}$, and (ii) learn a policy $\\pi$ that maximizes the expected return $\\pi^* = \\arg \\max_\\pi \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t R(s_t)]$. For any policy $\\pi$, we can define the value function and Q-function respectively to be:\n$V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi} [Q^{\\pi}(s, a)],$   (1)\n$Q^{\\pi}(s, a) = R(s) + \\gamma \\mathbb{E}_{s' \\sim P} [V^{\\pi}(s')],$  (2)\ncorresponding to the return-to-go from state $s$ (and action $a$). Both the value function and the Q-function for any policy $\\pi$ satisfy the above Bellman equations that can be used for reinforcement learning (RL), specifically by temporal difference (TD) algorithms.\nOne way to learn the reward function is through adversarial imitation learning (AIL)\u2014the learned reward function $R$ is derived from the minimax objective [5, 16, 17]:\n$\\mathcal{L}(D) = \\mathbb{E}_{s \\sim \\mathcal{B}} [\\log (1 - D(s))] + \\mathbb{E}_{s^* \\sim \\mathcal{B}^*} [\\log (D(s^*))],$ (3)\nwhere $D$ attempts to differentiate the occupancy measure between the state distributions induced by $\\mathcal{B}^*$ and $\\mathcal{B}$. The output of $D(s)$ is used to define $R(s)$, which is then used for updating the Q-function with Eq. (2). In the single-task regime, this is algorithmically identical to state-only AIL approaches with full demonstrations [34, 35]."}, {"title": "3.2 Learning a Multitask Agent from Examples", "content": "Example-based control presents a challenging exploration problem for complex tasks. We alleviate this problem by adapting learning from guided play (LfGP) [4], an approach for improving exploration by learning from auxiliary-task expert data, in addition to main task data. Auxiliary tasks are selected to have semantic meaning (e.g. reach, lift). Individual task definitions, and sometimes the auxiliary example data themselves, are reusable between main tasks."}, {"title": "3.3 Value Penalization in Example-Based Control", "content": "A scheduled multitask agent will, by design, exhibit far more diverse behaviour than a single-task policy [4, 31]. We show in Section 4.2 that the buffer generated by this behavior, consisting of transitions resulting from multiple policies $\\pi_\\tau$, can result in highly unstable and poorly calibrated Q estimates, especially in example-based control. In this section, we extend TD-error minimization, a fundamental component of many off-policy RL algorithms, with a penalty that encourages Q-function outputs to stay well-calibrated with respect to the reward model. Generally, the choice of the reward model and the loss function for estimating the Q-function can greatly impact learning efficiency (see Appendices A and B for details), but our value-penalty term applies to any reward model and commonly used regression-based loss functions. This penalty applies to both the single-task and multitask regime. For simplicity, we describe value penalization for the single-task framework.\nConsider a reward model $R(\\cdot)$ where $R(s^*), s^* \\in \\mathcal{B}^*$, indicates reward for successful states, while $R(s), s \\in \\mathcal{B}$, for all other states. In AIL, $\\hat{R}$ is a function of $D$, while in SQIL [6], for example, $R(s^*) = 1$ and $R(s) = 0$. Assuming that $s^*$ transitions to itself, then for policy evaluation with the mean-squared error (MSE), we can write the TD target, $y : S \\times S \\rightarrow \\mathbb{R}$, of Q-updates as\n$y(s, s') = R(s) + \\gamma \\mathbb{E}_{a'} [Q(s', a')],$ (6)\n$y(s^*, s^*) = R(s^*) + \\gamma \\mathbb{E}_{a'} [Q(s^*, a')],$ (7)\nwhere $(s, \\cdot, s') \\sim \\mathcal{B}, a' \\sim \\pi(\\cdot|s')$, and $s^* \\sim \\mathcal{B}^*$. Eq. (7) can be replaced with $y(s^*, s^*) = R(s^*)$ if one considers successful states to be terminal, but this can cause bootstrapping errors when a task times out or does not terminate upon success [36], both of which are common practice in robotics environments. Regressing to TD targets Eqs. (6) and (7) will eventually satisfy the Bellman equation, but in the short term the targets do not satisfy $y(s, s') < y(s^*, s^*)$. This is because the TD targets leverage bootstrapping of the current Q-estimate, an estimate that may not satisfy the Bellman equation and can exceed the bounds of valid Q-values, implying that approximation error of $Q$ updated with the MSE can be uncontrollable.\nWe introduce a simple resolution to this issue by adding a penalty to our TD updates for $s \\in \\mathcal{B}$ based on the current estimate of $\\mathbb{E}_{s^* \\sim \\mathcal{B}^*} [V^{\\pi}(s^*)]$. This penalty term enforces the Q-estimate to focus on outputting valid values and drives the TD targets to satisfy the inequality $y(s, s') \\leq y(s^*, s^*)$. We introduce both a minimum and maximum value for $Q^{\\pi}(s, a)$ respectively as"}, {"title": "4 Experiments", "content": "Through our experiments, we seek to understand if VPACE improves stability and efficiency in example-based control. We also complete an ablation study of various hyperparameter options, and finally analyze the learned values for agents with and without value penalization."}, {"title": "4.1 Experimental Setup", "content": "We learn agents in a large variety of tasks and environments, including those originally used in LfGP [4] and RCE [3]. Specifically, the tasks from [4] involve a simulated Franka Emika Panda manipulator, a blue and green block, a fixed \"bring\" area for each block, and a small slot with <1 mm tolerance for inserting each block. The environment has a single, shared observation space and action space with multiple options for main and auxiliary tasks. We additionally study all tasks"}, {"title": "4.2 Main Task Performance Results", "content": "Our results for all algorithms for four of the most challenging main tasks we examined are shown at the top of Fig. 4, while the bottom shows normalized average results for all tasks, separated by environment. Our real Panda results are shown in Fig. 3b. Since the Sawyer and Adroit tasks do not include specific success evaluation metrics, we only report returns. Policies are evaluated at 25k (environment step) intervals for 50 episodes for the simulated Panda tasks, 10k intervals for 30 episodes for the Sawyer and Adroit tasks, and 5k intervals for 10 episodes for the real Panda tasks.\nOur results clearly show the benefits of combining auxiliary task exploration with value penalization. For most tasks, VPACE-SQIL learns faster and more stably than any other method. Notably,"}, {"title": "4.3 Ablations, Data Quantity, and Comparison to Full Trajectories and Sparse Rewards", "content": "We completed experiments with many variations from our original implementation of VPACE-SQIL in Unstack-Stack (see Fig. 5a). Our main experiments used $\\lambda = 10$ for value penalization strength, but we also tested $\\lambda = \\{1, 100\\}$, and found a negligible effect on performance, indicating robustness to $\\lambda$ choice. We tested two different sizes of $\\mathcal{B}$ (for all tasks T in Tall), $|\\mathcal{B}| = \\{10, 100\\}$, while the main experiments used $|\\mathcal{B}| = 200$ (following [3]). We found that $|\\mathcal{B}| = 100$ had a negligible effect on performance, but $|\\mathcal{B}| = 10$ slowed learning and impaired final performance. Even with $|\\mathcal{B}| = 10$, VPACE-SQIL outperformed all baselines with $|\\mathcal{B}| = 200$, apart from VPACE-DAC.\nA natural question regarding our approach is how its performance compares to more traditional approaches, such as using full expert trajectories and inverse reinforcement learning (IRL), or using RL with true sparse rewards. To test the former, we added full trajectories to each $\\mathcal{B}$ (labelled +Full Trajectories in Fig. 5a, +Full Trajectories & Actions for learning from actions as well, and VP-SQIL +Full Trajectories for single-task only), effectively making our approach similar to [4] but with value-penalization. Intriguingly, peak performance is reduced in this setting (especially without ACE), which we hypothesize is because the agent now has to minimize divergence between $\\mathcal{B}$ and $\\mathcal{B}^*$ for many non-successful states, leading to an effect, commonly seen with dense reward functions, known as reward hacking [38]. This result suggests that inverse RL/adversarial IL can be significantly improved by switching to example-based control, but further investigation is required. To test RL with sparse rewards, we removed $\\mathcal{B}^*$ entirely, and instead use a ground truth $R(s, a)$ from"}, {"title": "4.4 Value Penalization and Calibration of Q-Values", "content": "While our performance results show that value penalization improves performance, they do not explicitly show that $y(s, s') \\leq y(s^*, s^*)$ (see Section 3.3). To verify that this goal was met, we took snapshots of each learned agent at 500k steps and ran each policy for a single episode, recording per-timestep Q-values. Instead of showing Q-values directly, we show $Q(s_t, a_t) - \\mathbb{E}_{s^* \\sim \\mathcal{B}^*, a^* \\sim \\pi(\\cdot|s^*)} [Q(s^*, a^*)]$, which should be close to 0 when an episode is completed successfully, and should never climb above 0 for $y(s, s') \\leq y(s^*, s^*)$ to hold. We show the results of doing so in Unstack-Stack in Fig. 5b (see Appendix E.2 for other tasks).\nBoth VP-SQIL and VPACE-SQIL clearly do not violate $y(s, s') \\leq y(s^*, s^*)$, while both SQIL and ACE-SQIL do. ACE-SQIL, in particular, has no values, on average, where $y(s, s') \\leq y(s^*, s^*)$, indicating that it has learned poorly calibrated estimates for Q. This is reflected in our main performance results, where, in the most difficult Panda tasks in particular, the improvement from ACE-SQIL to VPACE-SQIL is pronounced. Consequently, scheduled auxiliary control can produce poorer policies than its single-task alternative unless it is coupled with value penalization."}, {"title": "5 Limitations", "content": "VPACE suffers from several limitations, though many of them are inherited from the use of reinforcement learning and learning from guided play. For an expansion of this section, including these inherited limitations, see Appendix F. In this work, we exclusively learn from numerical state data, rather than raw images, and raw images may be required for tasks involving objects that are not rigid. As well, we claim that example distributions are easier to generate than full expert trajectories, but for certain tasks, generating these example distributions may also be challenging. Finally, tasks we investigate in this work have roughly unimodal example success state distributions, and our method may not gracefully handle multimodality."}, {"title": "6 Conclusion", "content": "In this work, we presented VPACE\u2014value-penalized auxiliary control from examples, where we coupled scheduled auxiliary control with value penalization in the example-based setting to significantly improve learning efficiency and stability. Our experiments revealed that scheduled auxiliary control can exacerbate the learning of poorly-calibrated value estimates, which can significantly harm performance, and we alleviated this issue with an approach to value penalization based on the current value estimate of example data. We theoretically showed that our approach to value penalization still affords an optimal policy. We empirically showed that value penalization, together with scheduled auxiliary tasks, greatly improves learning from example states against a set of state-of-the-art baselines, including learning algorithms with other forms of feedback. Opportunities for future work include the further investigation of learned approaches to scheduling, as well as autonomously generating auxiliary task definitions."}, {"title": "A Reward Model Formulations", "content": "In this section, we investigate approaches to off-policy reinforcement learning (RL) studied in this work, modified to accommodate an unknown R and the existence of an example state buffer $\\mathcal{B}^*$."}, {"title": "A.1 Learning a Reward Function", "content": "The most popular approach to reward modelling, known as inverse RL, tackles an unknown R by explicitly learning a reward model. Modern approaches under the class of adversarial imitation learning (AIL) algorithm aim to learn both the reward function and the policy simultaneously. In AIL, the learned reward function, also known as the discriminator, aims to differentiate the occupancy measure between the state-action distributions induced by expert and the learner. In example-based control, the state-conditional discriminator loss is Eq. (3), where D attempts to differentiate the occupancy measure between the state distributions induced by $\\mathcal{B}^*$ and $\\mathcal{B}$. The output of $D(s)$ is used to define R(s), which is then used for updating the Q-function using Eq. (6).\nIn example-based control, the discriminator D provides a smoothed label of success for states, thus its corresponding reward function can provide more density than a typical sparse reward function, making this approach an appealing choice. Unfortunately, a learned discriminator can suffer from the deceptive reward problem, as previously identified in [4], and this problem is exacerbated in the example-based setting. In the following sections, we describe options to remove the reliance on separately learned discriminators."}, {"title": "A.2 Discriminator-Free Reward Labels with Mean Squared Error TD Updates", "content": "A simple alternative to using a discriminator as a reward model was initially introduced as soft-Q imitation learning (SQIL) in [6]. In standard AIL algorithms, D is trained separately from $\\pi$ and $Q^{\\pi}$, where D is trained using data from both $\\mathcal{B}$ and $\\mathcal{B}^*$, whereas $\\pi$ and $Q^{\\pi}$ are trained using data exclusively from $\\mathcal{B}$. However, most off-policy algorithms do not require this choice, and approaches such as [4, 39] train $Q$, and possibly $\\pi$, using data from both $\\mathcal{B}$ and $\\mathcal{B}^*$. It is unclear why this choice is often avoided in AIL, but it might be because it can introduce instability due to large discrepancy in magnitudes for Q targets given data from $\\mathcal{B}$ and $\\mathcal{B}^*$. Sampling from both buffers, we can define R(st, at) in Eq. (2) to be labels corresponding to whether the data is sampled from $\\mathcal{B}$ or $\\mathcal{B}^*$-in SQIL, the labels are respectively 0 and 1. The full training objective resembles the minimax objective in Eq. (3) where we set $D(s^*) = 1$ and $D(s) = 0$.\nThe resulting reward function is the expected label $R(s, a) = \\mathbb{E}_{s \\sim \\text{Categorical} (\\{\\mathcal{B}, \\mathcal{B}^*\\})} [\\mathbb{1}(\\mathcal{B} = \\mathcal{B}^*)],$ where Categorical, is a categorical distribution corresponding to the probability that s belongs to buffers $\\mathcal{B}, \\mathcal{B}^*$. Consequently, only successful states $s^*$ yields positive reward and the corresponding optimal policy will aim to reach a successful state as soon as possible. If we further assume that $s^*$ transitions to itself, then for policy evaluation with mean-squared error (MSE), we can write the temporal difference (TD) target, $y$, of Q-updates with:\n$y(s, s') = \\gamma \\mathbb{E}_{a'} [Q(s', a')],$ (11)\n$y(s^*, s^*) = 1 + \\gamma \\mathbb{E}_{a'} [Q(s^*, a')],$ (12)\nwhere $(s, \\cdot, s') \\sim \\mathcal{B}, a' \\sim \\pi(a'|s')$, and $s^* \\sim \\mathcal{B}^*$.\nThis approach reduces complexity as we no longer explicitly train a reward model, and also guarantees discrimination between data from $\\mathcal{B}$ and $\\mathcal{B}^*$."}, {"title": "A.3 Discriminator-Free Reward Labels with Binary Cross Entropy TD Updates", "content": "Eysenbach et al. [3] introduced recursive classification of examples (RCE), a method for learning from examples of success. RCE mostly follows the approach outlined above in Appendix A.2 but"}, {"title": "B Why Does SQIL Outperform RCE?", "content": "Our results show that VPACE-SQIL outperforms ACE-RCE, and that VP-SQIL and SQIL outperform RCE in almost all cases. This result is in conflict with results from [3], which showed SQIL strongly outperformed by RCE. In this section, we show results that help explain our findings.\nEysenbach et al. [3] claimed that the highest performing baseline against RCE was SQIL, but it is worth noting that their implementation\u00b9 is a departure from the original SQIL implementation, which uses MSE for TD updates, described in Appendix A.2. Furthermore, Eysenbach et al. [3] also noted that it was necessary to add n-step returns to off-policy learning to get reasonable performance. In their experiments, however, this adjustment was not included for their version of SQIL with BCE loss. We complete the experiments using their implementation and show the average results across all RCE environments in Fig. 6, and also compare to using SQIL with MSE (without value penalization or auxiliary tasks) for TD updates.\nThese results empirically show that RCE and SQIL with BCE loss perform nearly identically, indicating that benefits of the changed TD targets and weights described in Appendix A.3 may not be as clear as previously described. Furthermore, SQIL with MSE clearly performs better on average, although it still performs worse than VPACE (see Appendix E). While these empirical results demonstrate that example-based control with BCE loss for TD updates is outperformed by SQIL with MSE, Eysenbach et al. [3] had theoretical results indicating that RCE should still learn an optimal policy. In the following section, we describe a flaw found in one of their proofs that may help to further understand why RCE is outperformed by SQIL with MSE."}, {"title": "B.1 Re-Examination of the Proofs from Section 4 of RCE [3]", "content": "In this section, we outline potential flaws in the proofs of Lemma 4.2 and Corollary 4.2.1 in RCE [3]. We then provide a lemma that shows RCE can be considered as recovering a specific reward function, thereby unifying RCE with other IRL algorithms.\nTo begin, recall that the RCE objective (i.e. Eq. (7) of Eysenbach et al. [3]) is defined as follows:\n$\\mathcal{L}^{\\pi}(\\theta) := \\mathbb{P}(e_{t+}=1) \\mathbb{E}_{p(s_t,a_t|e_{t+}=1)}[\\log C^{\\theta}(s_t, a_t)] + \\mathbb{E}_{p(s_t,a_t)} [\\log (1 - C^{\\theta}(s_t, a_t))].$ (15)"}, {"title": "C Finite-Sample Analysis of Value-Penalty Regularization", "content": "In this section, we show a finite-sample complexity bound of learning the Q-function via mean-squared error (MSE) with value-penalty regularization and demonstrate that we can use its solution for approximate policy iteration (API). Intuitively, given a fixed set of samples, a near-optimal solution obtained using the regularized loss is closed to the true optimal solution of the unregularized loss with high probability. Then, if we can control the error of the solution from the regularized loss, we can further bound the value error between the Q-function output by API and the optimal Q-function.\nMore formally, consider a parameterized function class $\\mathcal{Q} = \\{Q_\\theta : \\mathbb{R}^{S\\times A} \\rightarrow \\mathbb{R}|\\theta \\in \\Theta\\}$. We define the MSE as\n$\\ell(\\theta, \\mathcal{Z}_N) = \\frac{1}{N} \\sum_{n=1}^N (Q_\\theta(S_n, A_n) - G_n)^2,$ (17)\nwhere $\\mathcal{Z}_N = \\{(S_n, A_n, G_n)\\}_{n=1}^N$ is $N$ samples of the state-action pairs and the corresponding expected return. The value-penalty regularizer is defined as\n$h(\\theta, \\mathcal{Z}_N) = \\frac{\\lambda}{N} \\sum_{n=1}^N (\\max(Q_\\theta(S_n, A_n) - Q_U, 0))^2 + (\\max(Q_L - Q_\\theta(S_n, A_n), 0))^2,$ (18)\nfor some constants $Q_L < Q_U$, corresponding to the lowest and highest possible Q-values respectively, and positive constant $\\lambda$. Then, the MSE with value-penalty regularization is defined as\n$\\ell_{reg}(\\theta, \\mathcal{Z}_N) = \\ell(\\theta, \\mathcal{Z}_N) + h(\\theta, \\mathcal{Z}_N).$ (19)\nDuring training, suppose we are given $N$ i.i.d. samples $\\mathcal{Z}_N$ from $\\mathcal{D}$. We aim to approximate the regularized empirical risk minimizer (ERM) to obtain $\\hat{\\theta}$:\n$\\ell(\\hat{\\theta}, \\mathcal{Z}_N) + h(\\hat{\\theta}, \\mathcal{Z}_N) \\leq \\min_{\\theta \\in \\Theta} [\\ell(\\theta, \\mathcal{Z}_N) + h(\\theta, \\mathcal{Z}_N)] + \\epsilon'.$ (20)\nOur goal is to find the approximate regularized ERM $\\hat{\\theta}$ that achieves a low test loss, defined by\n$\\mathcal{L}(\\theta, \\mathcal{D}) = \\mathbb{E}_{Z \\sim \\mathcal{D}} [\\ell(\\theta, \\mathcal{Z})].$ (21)\nTo begin, we assume that the function class has bounded outputs (or bounded parameters) which is a standard requirement for characterizing generalization errors:\n*Assumption 1.* (Bounded function output and realizability.) *Let* $C_Q \\geq \\max(|Q_L|, |Q_U|)$. *The parameterized function class F is bounded:* $\\mathcal{Q} = \\{Q_\\theta : \\mathbb{R}^{S\\times A} \\rightarrow [-C_Q, C_Q]| \\theta \\in \\Theta\\}$. *Moreover, the optimal parameter of the test loss belongs to the function class:* $\\theta^* = \\arg \\min_\\theta \\mathcal{L}(\\theta, \\mathcal{D}) \\in \\Theta$.\nNote that the regularizer term Eq. (18) only has an effect when the function Q predicts values that are out-of-range, therefore we set $C_Q$ to be larger than the maximum possible returns for a more meaningful analysis, otherwise we recover the unregularized loss Eq. (17). Furthermore, we characterize the complexity of the function class $\\mathcal{Q}$ using the expected Rademacher complexity with offset.\n*Definition 1.* *The expected Rademacher complexity is defined to be*\n$\\mathfrak{R}_N(\\mathcal{Q}, \\mathcal{D}) = \\mathbb{E}_{\\mathcal{Z}_N \\sim \\mathcal{D}} \\mathbb{E} [\\sup_{\\theta \\in \\Theta} \\frac{1}{N} \\sum_{n=1}^N \\sigma_n (\\ell(\\theta, \\mathcal{Z}_N) + 0.5h(\\theta, \\mathcal{Z}_N)) - 0.5h(\\theta, \\mathcal{Z}_N) ],$\n*where* $\\sigma_1, ..., \\sigma_N$ *are independent Rademacher random variables.*\nThen, by Corollary 6.21 of [41], we can show that the approximate ERM satisfies the oracle inequality with high probability, which we state here:"}, {"title": "D Additional Environment, Algorithm, and Implementation Details", "content": "The following sections contain further details of the environments, tasks, auxiliary tasks, algorithms, and implementations used in our experiments."}, {"title": "D.1 Additional Environment Details", "content": "Compared with the original Panda tasks from LfGP [4], we switch from 20Hz to 5Hz control (finding major improvements in performance for doing so), improve handling of rotation symmetries in the observations, and remove the force-torque sensor since it turned out to have errors at low magnitudes. Crucially, these modifications did not require training new expert policies, since the same final observation states from the full trajectory expert data from [4] remained valid. Compared with the original LfGP tasks, we also remove Move-block as an auxiliary task from Stack, Unstack-Stack, Bring and Insert, since we found a slight performance improvement for doing so, and add Reach, Lift, and Move-block as main tasks. The environment was otherwise identical to how it was implemented in LfGP, including allowing randomization of the block and end-effector positions anywhere above the tray, using delta-position actions, and using end-effector pose, end-effector velocity, object pose, object velocity, and relative positions in the observations. For even further details of the original environment, see [4].\nSince the Sawyer tasks from [3, 11] only contain end-effector position and object position by default, they do not follow the Markov property. To mitigate this, we train all algorithms in the Sawyer tasks with frame-stacking of 3 and add in gripper position to the observations, since we found that this, at best, improved performance for all algorithms, and at worst, kept performance the same. We validate that this is true by also performing experiments using the original code and environments from [3], unmodified, where the results of RCE without these modifications are presented in Fig. 6, with results comparable to or poorer than our own RCE results."}, {"title": "D.2 Delta-Position Adroit Hand Environments", "content": "The Adroit hand tasks from [37] use absolute positions for actions. This choice allows even very coarse policies, with actions that would be unlikely to be successful in the real world, to learn to complete door-human-vo and hammer-human-vo, and also makes the intricate exploration re-"}, {"title": "D.3 Real World Environment Details", "content": "Fig. 8 shows our experimental platform and setup for our two real world tasks. In both RealDrawer and RealDoor, the observation space contains the end-effector position, an ArUco tag [43]to provide drawer or door position (in the frame of the RGB camera; we do not perform extrinsic calibration between the robot and the camera), and the gripper finger position. The action space in both contains delta-positions and a binary gripper command for opening and closing. The action space for RealDrawer is one-dimensional (allowing motion in a line), while the action space for RealDoor is two-dimensional (allowing motion in a plane). The initial state distribution for RealDrawer allows for initializing the end-effector anywhere within a 10 cm line approximately 25 cm away from the drawer handle when closed. For RealDoor, the initial state distribution is a 20 cm \u00d7 20 cm square, approximately 20cm away from the door handle when closed. Actions are supplied at 5 Hz.\nFor both environments, for evaluation only, success is determined by whether the drawer or door is fully opened, as detected by the absolute position of the ArUco tag in the frame of the RGB camera. Our robot environment code is built on Polymetis [44], and uses the default hybrid impedance controller that comes with the library. To reduce environmental damage from excessive forces and torques, we reduced Cartesian translational stiffness in all dimensions from 750 N/m to 250 N/m, and the force and torque limits in all dimensions from 40 N and 40 Nm to 20 N and 20 Nm."}, {"title": "D.4 Additional Task Details", "content": "Fig. 9 shows representative images for all environments and tasks used in this work. Success examples for the Panda environments were gathered by taking $s_t$ from the existing datasets provided by [4]. Success examples for main tasks from the Sawyer environments were generated using the same code from [3], in which the success examples were generated manually given knowledge of the task. Auxiliary task data was generated with a similar approach. Success examples for the Adroit hand environments were generated from the original human datasets provided by [37]. Success examples for our real world tasks were generated by manually moving the robot to a small set of successful positions for each auxiliary task and main task.\nAs stated in Section 4.1, all Panda main tasks use the the auxiliary tasks release, reach, grasp, and lift.\nThere are two specific nuances that were left out of the main text for clarity and brevity: (i) the Reach main task only uses release as an auxiliary task (since it also acts as a \u201ccoarse\" reach), and (ii) half of the release dataset for each task is specific to that task (e.g., containing insert or stack data), as was the case in the original datasets from [4]. For the Sawyer, Hand, and real Panda environments, because the observation spaces are not shared, each task has its own separate reach and grasp data."}, {"title": "D.5 Additional Algorithm Details", "content": "Algorithm 1 shows a summary of VPACE, built on LfGP [4] and SAC-X [31]. Table 1 shows a breakdown of some of the major differences between all of the algorithms studied in this work."}, {"title": "D.6 Additional Implementation Details", "content": "In this section, we list some specific implementation details of our algorithms. We only list parameters or choices that may be considered unique to this work, but a full list of all parameter choices can be found in our code. We also provide the VPACE pseudocode in Algorithm 1, with blue text only applying to learned discriminator-based reward functions (see Appendix A for various reward models).\nWhenever possible, all algorithms and baselines use the same modifications. Table 2 also shows our choices for common off-policy RL hyperparameters as well as choices for those introduced by this work.\nDAC reward function: for VPACE-DAC and VP-DAC, although there are many options for reward functions that map $D$ to $\\mathbb{R}$ [45], following [4, 5, 17], we set the reward to $R_{\\tau}(s) = \\log (D_{\\tau}(s)) - \\log (1 - D_{\\tau}(s)).$"}, {"title": "E Additional Performance Results", "content": "In this section, we expand upon our performance results from Section 4.2 and our Q-value calibration results from Section 4.4. We also show performance results for auxiliary tasks."}, {"title": "E.1 Expanded Main Task Performance Results", "content": "Fig. 11, Fig. 12, and Fig. 13 show expanded per-main-task results for our Panda (originally presented in [4, 32]), Sawyer (originally presented in [11, 3]), and Adroit Hand (originally presented in [37, 3]) tasks, respectively.\nIn the Panda tasks (Fig. 11), the hardest tasks (Stack, Unstack-Stack, and Insert) benefit the most from VPACE, but VPACE-SQIL is always the fastest and highest performing method. Notably, ACE-SQIL is always outperformed by both VPACE-SQIL and VPACE-DAC, and RCE is always outperformed by VP-SQIL. The benefits of value penalization are also clear, even in Bring, where ACE-SQIL intially learns the task, but starts to learn very poor Q estimates and have increasingly poorer and more unstable performance."}, {"title": "E.2 Value Penalization Calibration Improvement \u2013 All Environments", "content": "This section expands on the value penalization calibration results from Section 4.4 for the Panda and Sawyer tasks Fig. 14 and for the Adroit hand tasks Fig. 15. Following the results shown for Unstack-Stack, the results for other difficult tasks also show clear violations of $y(s, a) < y(s^*, a^*)$, and tasks in which this rule is violated also tend to have poorer performance. The violations are more pronounced ACE-SQIL, which, as previously discussed, is likely because they generate far more diverse state distributions in $\\mathcal{B}$. As shown for Unstack-Stack in Section 4.4, VP-SQIL and VPACE-SQIL never violate $y(s, a) \\leq y(s^*, a^*)$. Intriguingly, although the rule is severely violated for many Adroit hand tasks, ACE-SQIL and SQIL still have reasonable performance in some cases. This shows that highly uncalibrated Q estimates can still, sometimes, lead to adequate performance. We hypothesize that this occurs because these tasks do not necessarily need $s_T \\sim \\mathcal{B}$ to match $s^* \\sim \\mathcal{B}_{main}$ to achieve high return, but we leave investigating this point to future work."}, {"title": "E.3 Auxiliary Task Performance Results", "content": "In our main text, we only presented performance results for main tasks, since our primary goal was to efficiently generate policies that performed well on a particular task. However, our scheduled auxiliary task approach inherently learns multiple auxiliary task policies $\\pi_\\tau^{aux}$ in addition to the main task policy $\\pi_{main}$. While performance on these auxiliary task policies is not necessarily expected to be high throughout learning, observing auxiliary task performance can potentially provide useful"}, {"title": "F Expanded Limitations", "content": "In this section, we expand on some of the limitations originally discussed in Section 5, where we previously skipped limitations that are inherent to reinforcement learning and to learning from guided play (LfGP, [4]), both of which are core parts of our approach."}, {"title": "Experimental limitation\u2014Numerical state data only", "content": "All of our tests are done with numerical data, instead of image-based data. Other work [31, 47] has shown that for some environments, image-based learning just results in slowing learning compared with numerical state data, and we assume that the same would be true for our method as well."}, {"title": "Assumption\u2014Generating example success states is easier", "content": "We claim that success example distributions are easier to generate than full trajectory expert data, and while we expect this to be true in almost all cases, there may still be tasks or environments where accomplishing this is not trivial. As well, similar to other imitation learning methods, knowing how much data is required to generate an effective policy is unknown, but adding a way to append to the existing success state distribution (e.g., [21]) would presumably help mitigate this."}, {"title": "Assumption/failure mode\u2014Unimodal example distributions", "content": "Although we do not explicitly claim that unimodal example state distributions are required for VPACE to work, all of our tested tasks have roughly unimodal example state distributions. It is not clear whether our method would gracefully extend to the multimodal case, and investigating this is an interesting direction for future work."}, {"title": "Experimental limitation\u2014Some environment-specific hyperparameters", "content": "While the vast majority of hyperparameters were transferable between all environments and algorithms, the scheduler period, the inclusion of n-step targets, and the use of entropy in TD updates, were different between environments to maximize performance. Scheduler periods will be different for all environments, but future work should further investigate why n-step targets and the inclusion of entropy in TD updates makes environment-specific differences."}, {"title": "F.1 VPACE and LfGP Limitations", "content": "VPACE shares the following limitations with LfGP [4]."}, {"title": "Assumption\u2014Existence of auxiliary task datasets", "content": "VPACE and LfGP require the existence of auxiliary task example datasets $\\mathcal{B}_{aux}$, in addition to a main task dataset $\\mathcal{B}_{main}$. This places higher initial burden on the practitioner. In future, choosing environments where this data can be reused as much as possible will reduce this burden."}, {"title": "Assumption\u2014Clear auxiliary task definitions", "content": "VPACE and LfGP require a practitioner to manually define auxiliary tasks. We expect this to be comparatively easier than generating a similar dense reward function, since it does not require evaluating the relative contribution of individual auxiliary tasks. As well, all tasks studied in this work share task definitions, and the panda environment even shares task data itself, leading us to assume that these task definitions will extend to other manipulation tasks as well."}, {"title": "Assumption\u2014Clear choices for handcrafted scheduler trajectories", "content": "VPACE and LfGP use a combination of a weighted random scheduler with a handcrafted scheduler, randomly sampling from pre-defined trajectories of high level tasks. Ablett et al. [4] found that the handcrafted scheduler added little benefit compared with a weighted random scheduler, and further work should investigate this claim, or perhaps attempt to use a learned scheduler, as in [31]."}, {"title": "F.2 Reinforcement Learning Limitations", "content": "Experimental limitation\u2014Free environment exploration As is common in reinforcement learning methods, our method requires exploration of environments for a considerable amount of time (on the order of hours), which may be unacceptable for tasks with, e.g., delicate objects."}]}