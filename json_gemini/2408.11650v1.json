{"title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical Researcher", "authors": ["Derry Pratama", "Naufal Suryanto", "Andro Aprila Adiputra", "Thi-Thu-Huong Le", "Ahmada Yusril Kadiptya", "Muhammad Iqbal", "Howon Kim"], "abstract": "Penetration testing, a critical component of cybersecurity, typically requires extensive time and effort to find vulnerabilities. Beginners in this field often benefit from collaborative approaches with the community or experts. To address this, we develop CIPHER (Cybersecurity Intelligent Penetration-testing Helper for Ethical Researchers), a large language model specifically trained to assist in penetration testing tasks. Unlike software development, penetration testing involves domain-specific knowledge that is not widely documented or easily accessible, necessitating a specialized training approach for AI language models. We trained CIPHER using over 300 high-quality write-ups of vulnerable machines, hacking techniques, and documentation of open-source penetration testing tools. Additionally, we introduced the Findings, Action, Reasoning, and Results (FARR) Flow augmentation, a novel method to augment penetration testing write-ups to establish a fully automated pentesting simulation benchmark tailored for large language models. This approach fills a significant gap in traditional cybersecurity Q&A benchmarks and provides a realistic and rigorous standard for evaluating AI's technical knowledge, reasoning capabilities, and practical utility in dynamic penetration testing scenarios. In our assessments, CIPHER achieved the best overall performance in providing accurate suggestion responses compared to other open-source penetration testing models of similar size and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B Chat, particularly on insane difficulty machine setups. This demonstrates that the current capabilities of general large language models (LLMs) are insufficient for effectively guiding users through the penetration testing process. We also discuss the potential for improvement through scaling and the development of better benchmarks using FARR Flow augmentation results. Our benchmark will be released publicly at https://github.com/ibndias/CIPHER.", "sections": [{"title": "1. Introduction", "content": "Penetration testing, a fundamental aspect of cybersecurity, involves probing computer systems [1], networks [2], or web applications [3] to identify security vulnerabilities that malicious actors could exploit. This proactive approach is critical for uncovering poten- tial weaknesses before they can be manipulated, thus safeguarding sensitive data and maintaining the integrity of digital systems. Despite its importance, penetration testing is notoriously time-consuming and labor-intensive [4], often requiring expertise and extensive collaboration. Traditional methods primarily rely on manual testing, which can be ineffi- cient and may not scale well with modern IT infrastructures' growing complexity and rapid evolution. As cyber threats continue to advance in sophistication, there is an increasing need for more efficient, comprehensive, and adaptable penetration testing methodologies."}, {"title": "2. Background and Related Works", "content": "Various approaches have been proposed to streamline the penetration testing process. Automated tools and frameworks, such as Metasploit [5] and OpenVAS [6], offer some level of automation and guidelines, yet they often require significant expertise to operate effectively and may not fully replace the nuanced insights provided by experienced testers. These tools can assist in discovering and exploiting vulnerabilities but are limited in their ability to provide comprehensive, adaptive, and intelligent responses to the dynamic nature of penetration testing tasks.\nThe application of large language models (LLMs) in penetration testing represents a cutting-edge area of research with promising results. Advanced models such as GPT-4 [7] have demonstrated significant potential to automate and improve various penetration testing tasks, including Linux privilege escalation and the identification of file-based vulnerabilities [8]. However, implementing a straightforward LLM pipeline presents challenges, particularly in maintaining long-term memory for generating consistent and accurate commands throughout extended testing sessions [9]. Recent innovations like PentestGPT [9] and AutoAttacker [10] have addressed these limitations. These systems leverage existing LLMs and open-source models, incorporating specialized frameworks to emulate human penetration testers' workflow and decision-making processes. While these advancements mark significant progress, it is important to note that these general- purpose models are not explicitly fine-tuned for the nuances of penetration testing. This lack of domain-specific optimization can potentially limit their effectiveness in handling complex, context-dependent scenarios often encountered in real-world penetration testing environments.\nRecognizing these limitations, we developed CIPHER (Cybersecurity Intelligent Penetration-testing Helper for Ethical Researchers). CIPHER is a large language model specifically trained and fine-tuned on a comprehensive penetration testing dataset. Leverag- ing advancements in generative AI, CIPHER aims to automate and enhance the penetration testing process, making it more efficient and accessible, especially for individuals with limited penetration testing knowledge. By fine-tuning the model with over 300 write-ups of vulnerable machines, hacking techniques, and documentation of open-source penetration tools, CIPHER provides detailed explanations and mimics the reasoning abilities of an expert penetration tester.\nBuilding on this foundation, we introduced the Findings, Action, Reasoning, and Results augmentation (FARR) Flow. The FARR Flow is a novel method that augments write- up documents for more effective training. It enhances the model's ability to understand and replicate penetration testing processes and establishes a fully automated pentesting simulation benchmark tailored for large language models. This benchmark provides a real- istic and rigorous standard for evaluating the technical knowledge, reasoning capabilities, and practical utility of AI in penetration testing scenarios.\nOur assessments indicate that CIPHER demonstrates a significant improvement in providing helpful responses for penetration testing over other models, highlighting the limitations of traditional cybersecurity evaluations in capturing the nuances of practical pentesting skills. Specifically, CIPHER showed a notable improvement, underscoring its potential to transform penetration testing practices. This paper presents several key contributions:\n\u2022 The methodology of CIPHER development, where we utilize write-ups into begin- ner and expert conversation to train a large language model tailored for practical penetration testing assistance.\n\u2022 Novel augmentation method to transform write-ups into compact information based on the Findings, Action, Reasoning, and Results (FARR) Flow augmentation method.\n\u2022 The development and evaluation of an automated pentesting simulation benchmark utilizing FARR Flow augmentation, providing a rigorous and realistic standard for assessing LLM performance in practical penetration testing guidance accuracy.\n\u2022 An extensive evaluation demonstrating CIPHER's effectiveness and improvement over other penetration testing models."}, {"title": "2.1. Large Language Models in Cybersecurity", "content": ""}, {"title": "2.1.1. Large Language Models", "content": "Overview of Large Language Models: Large Language Models (LLMs), exemplified by GPT-4, have revolutionized natural language processing through deep learning and ex- tensive datasets, enabling them to understand and generate human-like text. These models predict the next word sequentially, capturing complex linguistic patterns and producing contextually relevant responses. Their applications include translation, summarization, and content creation, surpassing traditional NLP systems [11].\nTransformers: Introduced by Vaswani et al. [11] in \"Attention is All You Need\" (2017), Transformers have reshaped NLP by employing self-attention mechanisms to process entire sequences concurrently. They excel at capturing long-range dependencies, outperforming sequential models like RNNs. Transformers comprise self-attention layers, feed-forward networks, and positional encoding to maintain word order. Variants such as BERT and GPT have set benchmarks in NLP and extended into computer vision tasks through Vision Transformers (ViTs).\nOpen-Source LLMs: Open-source LLMs like the Llama family by Meta AI [12], Mistral [13], Yi 1.5 [14], and Qwen 1.5 [15] democratize access to powerful language models. Evolving from Llama 1 to Llama 3, these models increase token size and context length to enhance language understanding. Innovations like the mixture of experts in Mixtral [16] and unique training methods in Yi 1.5 push the boundaries further. Qwen 1.5 excels in multilingual contexts, incorporates agentic frameworks [17], releasing smaller and powerful models [18], making them versatile tools [12,19].\nReasoning Enhancements in LLMs: LLMs benefit from enhancements like Chain of Thought (CoT) prompting and Self-Consistency (SC). CoT prompting encourages explicit reasoning steps, while SC verifies response correctness. Techniques like Instruction fine- tuning (FLAN [20]) and model mimicry (Orca [21]) further improve reasoning capabilities. Despite advancements, challenges in LLM reasoning persist, motivating ongoing research [7,22].\nLLM-Based Chatbots: LLM-based chatbots like ChatGPT excel in customer support, education, and complex problem-solving by synthesizing large volumes of information into detailed responses. However, they lack the specialized knowledge required for offensive penetration testing [23].\nSupervised Fine-Tuning: Supervised fine-tuning enhances model performance on domain-specific datasets, particularly in areas like penetration testing, ensuring accurate application of specialized language. Unlike Retrieval-Augmented Generation (RAG), fine-tuning improves the model's domain comprehension [24].\nLLM Alignment: Efforts to align language models with human values focus on ensuring these models exhibit traits such as helpfulness and truthfulness. Reinforcement Learning with Human Feedback (RLHF) fine-tunes large language models (LLMs) to achieve this alignment. This process involves training a reward model to predict human preferences and then using algorithms like Proximal Policy Optimization (PPO) for fine- tuning. Although PPO generally yields better results, Direct Preference Optimization (DPO) simplifies the process by fine-tuning directly with human-labeled preferences [25,26].\nIncorporating Domain-Specific Knowledge: Domain-specific knowledge enhances LLM accuracy in specialized fields like medicine [27] and cybersecurity. Techniques such as Domain-Adaptive Pretraining (DAPT) and adaptive fine-tuning (AdaptLLM) are crucial for developing specialized models tailored for specific tasks, leveraging domain-specific datasets for improved insights [24,28]."}, {"title": "2.1.2. Applications and Challenges in Cybersecurity", "content": "In cybersecurity, Large Language Models (LLMs) such as GPT-4[7], PentestGPT[9], and HackerGPT[29] have been deployed to assist with penetration testing and other security tasks. These models can analyze and synthesize large volumes of information, aiding in identifying vulnerabilities. However, using such advanced models poses significant challenges. The high cost and privacy risks associated with proprietary models like GPT-4, which handle sensitive vulnerability data, are significant concerns. Moreover, general- purpose LLMs often lack the specialized knowledge required for effective penetration testing."}, {"title": "2.2. Existing Tools and Methodologies for Penetration Testing", "content": ""}, {"title": "2.2.1. Traditional and Automated Tools", "content": "Penetration testing is a crucial component of cybersecurity, involving simulated cy- berattacks to uncover system vulnerabilities. Traditional tools such as Metasploit [30] and OpenVAS [31] offer robust vulnerability discovery and exploitation frameworks. These tools, however, heavily rely on user expertise to interpret results and often require manual intervention to execute complex attack scenarios.\nAutomated tools have emerged to streamline the penetration testing process by au- tomating vulnerability scanning and initial exploitation attempts. Examples include Nessus [32] and Burp Suite [33], which use predefined algorithms to detect and exploit common vulnerabilities. While these tools enhance efficiency, they may overlook subtle vulnerabili- ties that require human intuition and context to identify.\nCombining traditional and automated tools optimizes penetration testing effective- ness. Traditional methods leverage human expertise to craft targeted attacks and interpret results, while automation accelerates routine tasks and broadens the scope of vulnerability detection. This hybrid approach ensures comprehensive security assessments addressing known vulnerabilities and emerging threats."}, {"title": "2.2.2. Advancements in AI for Penetration Testing", "content": "Recent advancements have seen the development of AI-driven tools designed to enhance penetration testing. Research efforts like PentestGPT [9] and AutoAttacker [10] leverage LLMs for various penetration testing tasks. Despite their potential, these tools face several limitations:\n\u2022 Context Loss and Memory Retention: LLMs struggle with retaining long-term con- versational memory, crucial for linking vulnerabilities across services to develop an exploitation strategy [9,10].\n\u2022 Testing Strategy Limitations: LLMs often adopt a depth-first search approach, which may overlook other potential attack surfaces [9].\n\u2022 Inaccurate Operations and Commands: LLMs may generate inaccurate commands or misinterpret outputs, leading to ineffective testing [34][35].\n\u2022 Limited Social Engineering and Image Interpretation: LLMs lack capabilities in physical social engineering techniques and interpreting visual hints [10].\n\u2022 Ethical Concerns: The potential misuse of LLMs for automating attacks raises ethical issues [36]."}, {"title": "2.2.3. Leveraging LLMs for Penetration Testing", "content": "There are distinct advantages to using native penetration testing LLMs versus general LLMs for cybersecurity tasks as shown in Table 1. Native penetration testing LLMs offer superior performance, specialization, ease of use, efficiency, and security. These models are optimized explicitly for penetration testing tasks, resulting in higher accuracy and efficiency. Their deep specialization and out-of-the-box readiness make them more effective and more accessible to implement for specific penetration testing needs without extensive customization. Moreover, native models ensure better security by handling sensitive data locally, thus reducing the risk of data breaches."}, {"title": "2.3. Specialized Cybersecurity Large Language Models", "content": ""}, {"title": "2.3.1. Development of Domain-Specific LLMs", "content": "Specialized LLMs trained on domain-specific data, such as cybersecurity, are necessary to address the shortcomings of general-purpose models. Techniques like Domain-Adaptive Pretraining (DAPT) and adaptive fine-tuning improve performance by incorporating spe- cialized knowledge and tools tailored for tasks like penetration testing [24,27,37-39]. These approaches enable models to understand and apply domain-specific language more effec- tively, enhancing their utility in specialized fields."}, {"title": "2.3.2. Evaluation and Benchmarking Challenges", "content": "Current evaluation frameworks for cybersecurity models, such as PurpleLlama Cy- berSecEval with MITRE ATT&CK, primarily focus on pre-defined scenarios and known attack patterns [40]. These do not adequately measure a model's ability to discover novel vulnerabilities or adapt to dynamic situations encountered in real-world penetration testing. Previous works like PentestGPT and AutoAttacker lack reproducible benchmarks, relying on manual testing and human scoring, which introduces bias and limits reproducibility [9,10]. Developing standardized, automated benchmarks that reflect real-world conditions remains a significant challenge for advancing the evaluation of cybersecurity LLMs."}, {"title": "2.3.3. Reproducible Benchmark for Penetration Testing Scenario", "content": "To address these challenges, we introduce FARR Flow augmentation method, which automates the generation of penetration testing scenarios using high-quality write-ups. Unlike other existing penetration testing benchmarks for LLM, FARR Flow Evaluation covers a wide range of tool usage capabilities, from reconnaissance to exploitation tools. FARR Flow Evaluation results are also easily reproducible due to the scoring automation by using judge LLM. We also release our code on GitHub. This approach provides a dynamic"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Architecture Design", "content": "Our research aims to develop CIPHER, an AI-powered penetration testing assistant designed to empower users, particularly those new to offensive cybersecurity, to conduct penetration tests on their own systems. The main architecture of CIPHER is shown in Figure 1. CIPHER's primary objective is to simplify identifying system vulnerabilities by providing expert guidance throughout the penetration testing workflow. Recognizing the challenges beginners face in using specialized tools and grasping core concepts, CIPHER offers:\n\u2022 Explanation of the question.\n\u2022 Intuitive, step-by-step instructions with reasoning.\n\u2022 Insightful response that resembles expert penetration tester reasoning.\n\u2022 Additional practical examples for the action.\nIn order to enhance the accuracy in suggesting command line usage on deployment, we added advanced Retrieval Augmented Generation (RAG) [41] in full architecture as seen in Figure 1. Initially user question will be processed by the embedding model [42,43] to find the similar hacking technique and command line documentation. Then similar documentation will be reranked using reranker model [44,45] to find the best document chunks. CIPHER will use the related chunks as in-context learning [46] material to answer the user question with accurate suggestion as possible. This research will discuss the core development of the language model to support this environment architecture."}, {"title": "3.2. Dataset", "content": "CIPHER focuses on facilitating penetration testing tools and emphasizes developing the user's ability to reason and make informed decisions during the testing process. By offering detailed explanations and reasoning for each step, CIPHER helps users understand the underlying principles and methodologies experienced penetration testers use. This dual focus on tool usage and expert reasoning equips users with the skills and confidence to conduct thorough and effective penetration tests, ultimately enhancing the security of their systems.\nThe development of CIPHER aims to bridge the gap between novice and expert, equipping users with the technical skills and reasoning abilities necessary to improve their system security posture significantly.\nDeveloping a pentesting chat assistant requires two fundamental capabilities: general chat assistance and domain-specific pentesting expertise. We utilize distinct datasets to address each of these capabilities."}, {"title": "3.2.1. General Assistant Capability", "content": "We leverage the OpenHermes 2.5 dataset for our general assistant capability, currently recognized as one of the best open-source conversation collections. This dataset was originally used to fine-tune the OpenHermes 2.5 model sourced from various origins, including Orca, Glaive, and other GPT-4 responses in diverse questions and instructions [47]. When used to fine-tune a base model, this conversational dataset has improved performance on multiple general-use benchmarks such as MMLU [48], HumanEval [49], and TruthfulQA [50].\nNote that this dataset primarily enhances the model's ability to formulate responses rather than expanding its knowledge base. The dataset provides examples of responding to greetings, questions, and instructions, effectively teaching the model appropriate \"answer-"}, {"title": "3.2.2. Penetration Testing Capability", "content": "ing styles.\" The varying performance of models fine-tuned on this dataset suggests that the underlying knowledge comes from pre-training rather than from examples of answering questions.\nThe dataset already includes patterns for responding to user greetings, questions, and instructions. However, the effectiveness of this dataset in improving model performance can vary depending on the base model used, further supporting the notion that true knowledge stems from pre-training rather than from examples of question-answering.\nThe OpenHermes 2.5 dataset is a comprehensive collection comprising over 1.6 mil- lion entries, designed to cover a wide range of conversational scenarios and specialized knowledge domains. Key components include the following (with rough estimations):\n\u2022 Creative prompts from Airoboros 2.2 (44.8K)\n\u2022 Domain-specific expertise from CamelAI in MATH, Physics, Chemistry, and Biology (50K, 20K, 20K, 20K respectively)\n\u2022 Real-world complex prompts from Chatbot Arena (7K)\n\u2022 Up-to-date data from Collective Cognition (as of 09-11-2023) (156)\n\u2022 Chain-of-thought examples from the Alpaca dataset, produced by GPT-4 (52K)\n\u2022 Evolving complexity prompts from Evol Instruct (70K and 140K versions)\n\u2022 Tool usage scenarios from Glaive Code Assistant (136K)\n\u2022 General-purpose conversational data from GPT4-LLM (54.6K) and GPTeacher (35.5K)\n\u2022 Specialized task-oriented datasets like Medical Tasks (4.6K) and MetaMath (40K)\n\u2022 Reasoning-focused entries from SlimOrca (550K) and Platypus (24.9K)\n\u2022 Real conversation enhancements from ShareGPT (356K)\n\u2022 Versatility-boosting prompts from Unnatural Instructions (9K)\nThis rich mixture of datasets aims to create a well-rounded foundation for the A\u0399 assistant, covering various aspects of knowledge, reasoning, and interaction styles. The diversity of sources enhances the model's ability to handle various queries and tasks effectively.\nDomain-specific knowledge enhancement has been extensively researched [27,38, 39]. The most effective approach to improve domain knowledge is to enrich the corpus with domain-specific text. Our focus is on basic cybersecurity and penetration testing knowledge.\nPenetration testing is a broad field that requires a strong foundation before identifying specific services or systems vulnerabilities. A penetration tester must understand how vari- ous services and systems operate and common vulnerabilities. While most public internet data provides basic knowledge of systems, which is reflected in current LLMs, the same is not true for vulnerability data and exploitation techniques. Due to safety concerns, most LLMs are biased towards protecting vulnerabilities or preventing exploitation techniques rather than suggesting how to exploit them."}, {"title": "3.2.3. Augmentation", "content": "We process the dataset through several steps:\n(1) Raw Context Dataset: For open-source datasets available in markdown format, no post-processing is needed. Web-formatted datasets are converted to markdown to preserve content structure. We split the datasets into chunks of 500, 1K, and 2K tokens, maintaining parent headers for nested sections to preserve context. This approach, providing the same information at different lengths, aids in better model generalization.\n(2) Conversation Dataset: While raw context datasets expand domain-specific knowl- edge like DAPT [24], they don't fully prepare the model to answer and suggest like an expert. We augment and prioritize HackTheBox writeups of vulnerable machines into conversations, simulating exchanges between novices and experts.\nWhile conversations do not add new knowledge, they help the model mimic expert answering styles, reducing mistakes and improving response quality [21].\n(3) Conversation Generation Pipeline: We designed CIPHER to assist with penetra- tion testing tasks, focusing on scenarios where a novice reports findings and situations and an expert provides suggestions, definitions, reasoning, and examples for the next steps. As seen in Figure 2, our pipeline generates conversations from 500-token chunks of raw text, ensuring focused discussions on specific topics instead of broader context."}, {"title": "3.3. Training", "content": "Table 4 shows our prompt for generating synthetic conversations. We emphasize self-sufficient and generalized con- versations to create independent scenarios that convert write-ups into current situations, avoiding direct references to the source material.\nThis section details CIPHER's training process, which leverages the Axolotl framework [61] for efficient training capabilities, as shown in Figure 4."}, {"title": "3.3.1. Supervised Fine-Tuning", "content": "CIPHER's initial training phase uses Mistral v0.3 as a base model combined with the OpenHermes 2.5 dataset with a specialized pentesting dataset. The Axolotl framework manages tokenization and multipacking. We employ a standard text completion approach without specific raw text pretraining data formatting. The OpenHermes 2.5 and pentesting datasets utilize the ChatML format, supporting system, user, and assistant prompts. The pentesting dataset uses a default system prompt of \"You are a helpful penetration testing assistant.\" while the general dataset's prompt remains unmodified. The com- bined dataset totals approximately 782 million tokens, comprising 500,606,758 tokens of general data and 281,516,236 tokens of supervised data."}, {"title": "3.3.2. Direct Preference Optimization", "content": "Following supervised fine-tuning, we implement Direct Preference Optimization (DPO) to refine model responses using the Argilla DPO Mix 7K [62] open-source dataset, which is pre-formatted in ChatML. DPO training, conducted via the Axolotl framework, extends to 4 epochs, surpassing typical epoch counts due to consistently decreasing loss."}, {"title": "3.4. FARR Augmentation", "content": "\u2022 Currently, evaluating LLMs for penetration testing faces significant challenges:\n\u2022 No automated benchmarks: There are no established automatic penetration testing benchmarks for LLMs.\n\u2022 Reliance on manual testing: The best available method involves human testers performing manual penetration testing while using an LLM chatbot for guidance.\n\u2022 Time-consuming: This manual approach requires approximately 1-4 hours per task per evaluator.\n\u2022 Inconsistent results: Outcomes can vary significantly based on the evaluator's exper- tise, condition, and interpretation of the LLM's guidance.\nThese factors make it difficult to efficiently and consistently evaluate LLM performance in penetration testing scenarios, highlighting the need for more standardized and scalable assessment methods in this field.\nTo address the lack of automated benchmarks for evaluating LLMs in penetration testing, we have developed a novel approach:\n\u2022 Benchmark Creation: We've designed a benchmark to measure the accuracy of an LLM's first suggestion in a penetration testing scenario.\n\u2022 Data Augmentation: Unlike our previous synthetic conversation data generation method for Supervised Fine-Tuning (SFT), this benchmark augments write-ups into compact, dense lists of information.\n\u2022 FARR Penetration Testing Flow: We introduce a new method to augment penetration testing write-ups into a Findings, Action, Reasoning, Result (FARR) flow. This struc- ture reflects the typical phases of a penetration test, capturing: Findings: Information discovered; Action: Steps taken based on the findings; Reasoning: Explanation for the chosen action; Result: Outcome of the action;"}, {"title": "4. Experiment Results", "content": "\u2022 Rationale: We observed that penetration testing write-ups consistently follow this or- dered structure, providing a comprehensive view of the testing process and vulnerable points in a system.\nThis approach allows for a more standardized and detailed evaluation of LLM performance in penetration testing scenarios, addressing the limitations of manual evaluation methods.\nIn this evaluation, we assess CIPHER's effectiveness in providing accurate guidance for penetration testing. Our assessment consists of several key components:\n\u2022 General Capabilities: We use the LLM Eval Harness benchmark to measure CIPHER's general task capabilities, examining how integrating general and penetration testing data affects performance.\n\u2022 Cybersecurity Expertise: We evaluate the model's performance on cybersecurity- specific questions to gauge its domain knowledge.\n\u2022 Pentesting Guidance Accuracy: Using our novel FARR Flow Reasoning Evaluation method, we assess CIPHER's ability to guide novice penetration testers through the testing process.\n\u2022 MITRE ATT&CK Understanding: We employ the PurpleLlama CyberSecEval[63] framework to measure CIPHER's comprehension of the MITRE ATT&CK knowledge base.\nThis multi-faceted approach allows us to comprehensively evaluate CIPHER's capabilities in providing accurate and effective penetration testing guidance."}, {"title": "4.1. Eval Harness General Capabilities", "content": "CIPHER's effectiveness relies on maintaining coherent conversation while providing penetration testing guidance. We utilize the widely recognized EleutherAI LLM Eval Harness benchmark [64] to evaluate CIPHER's general language model capabilities. This section presents the top 3 of our trained CIPHER models, ranked according to their average performance across various tasks in the benchmark. Our evaluation of CIPHER models is conducted without n-shot prompting, utilizing various benchmark datasets that focus on general purpose abilities (E.g., logical reasoning, commonsense reasoning, world knowl- edge) and cybersecurity topics. To evaluate various aspects of reasoning and knowledge, we use several benchmarks. For logical reasoning, which is crucial for understanding context and producing step-by-step solutions, we use ARC [65], LogiQA [66], and select parts of MMLU (Formal Logic and Logical Fallacies) [48]. Commonsense reasoning, which assesses practical knowledge and natural judgment for decision-making and creating ethi- cal step-by-step solutions, is evaluated using PiQA [67], SWAG [68], and HellaSwag [69]. For cybersecurity and complex reasoning, we use the MMLU computer science and com- puter security sections [48] and the Adversarial NLI (ANLI) [70] benchmark for complex logical reasoning and adversarial examples. Finally, to evaluate common knowledge and subject-specific understanding, we use OpenBookQA [71]."}, {"title": "4.3. FARR Flow Reasoning Evaluation", "content": "Currently, evaluating LLMs for penetration testing faces significant challenges:\nOnce we had the full flow of the machine penetration testing steps augmented as FARR Flow from section 3.4, we constructed a guidance reasoning test for the model by asking what to do next when we have only the 'Findings'. FARR Flow consists of multiple sets of Findings, Action, Reasoning, and Result.\nFigure 7 also illustrates how we evaluate the model using FARR Flow. At each step, we utilize the Findings as known information. We formulate questions for the model to predict the most likely action to do based on the current hint, which is constructed from the accumulated previous and current findings. The model's suggested response is then evaluated against Llama 3 70B to assess the relevance of the answer across three aspects: whether the model's answer anticipates the same outcome, targets the same service, and focuses on the same vulnerability type as the reference. Each criterion is rated with up to 3 points for relevance. Using these three aspects, we measure the average performance of each model on each HackTheBox machine in the FARR Flow evaluation.\nOur guidance reasoning test using the FARR Flow evaluation algorithm is shown in Algorithm 1. At the first question, the user does not know about the target machine, often only the IP information or open ports. Therefore we only provide the current findings. However, regardless of the suggested answer to the following question, we will still provide the model with the result of the previous findings to ensure the model understands the current situation. In Section 5.3 discusses the possibility of improvement in this question formulation.\nCIPHER training data uses the 0xdf write-up [60] dataset until early September 2023. We took whole machine write-up samples from mid-September 2023 until the FARR Flow benchmark development started in early May 2024 without cherry-picking. We only excluded incomplete machine write-ups and separated them, resulting in 35 machine write-ups augmented as FARR Flow for the reasoning test, with different difficulties and as shown in Figure 8a. Note that these machines are completely unknown to CIPHER. Overall, there are 35 HackTheBox machines with 136 vulnerability topics, all in 2.124 formulated questions.\nFigure 8b presents different attack vectors in this evaluation. The scope includes diverse vulnerabilities and techniques used to penetrate the machine. Each machine has a different attack vector to achieve a foothold from easy to insane level. Various privilege escalation techniques are needed to achieve better scores, and CVE knowledge is necessary to access some machines. This covers better coverage and penetration testing dynamics than traditional question-and-answer evaluation."}, {"title": "4.3.1. Penetration Testing Guidance Accuracy Performance", "content": "Table 9 shows the three criteria measured in our FARR Flow evaluation for each model tested. Our model CIPHER achieves the highest average score compared to other"}, {"title": "4.3.2. Performance by Machine Difficulties", "content": "To evaluate the model's performance across different complexity levels, we clustered the scores based on machine difficulties. This allows us to identify the areas where the model performs better or worse depending on the complexity. Table 10 shows that our model CIPHER performs better on average due to its distinguished high scores on easy and insane machines, with 56.25 and 50.96 points respectively, creating the highest gaps of about 1.69 and 2.22 points to the second-place model. Llama 3 70B takes second place, with slightly better scores on medium and hard machines by 0.52 and 0.43 points. This demonstrates our model's strength in penetration testing reasoning guidance compared to other penetration testing models, even those with ten times the parameter size.\nThis experiment proves that penetration testing intuition remains a challenge that requires experience rather than just vast knowledge already present in large parameter LLMs. By training CIPHER with a penetration testing-focused guidance dataset, CIPHER can guide users more accurately on unseen vulnerable machines than other specific cyber- security/penetration testing models and even larger parameter general models.\nThe results highlight CIPHER's effectiveness across different difficulty levels, particu- larly excelling in easy and insane categories. This balanced performance across varying complexities underscores the model's versatility and robustness in penetration testing scenarios. The consistent superiority over models with significantly larger parameter counts emphasizes the importance of specialized training and domain-specific knowledge in achieving high-quality results in targeted tasks like penetration testing guidance."}, {"title": "4.4. PurpleLlama CyberSecEval", "content": "We also evaluate our model using the most popular cyber security evaluation for LLM, PurpleLlama CybersecEval2 [63]. This evaluation was initially designed to measure the safety of the LLM model before being released. Since we are developing the model that accurately suggests the offensive exploitation technique, we can also measure how well our model is capable of attacks listed in MITRE ATT&CK. The value that we show in Figure 10 is the malicious score, unlike the original paper that showcases the benign score."}, {"title": "5. Discussion and Future Works", "content": ""}, {"title": "5.1. Limitation", "content": "While CIPHER demonstrates superior average performance compared to other models, including the higher parameter Llama 3 70B, in the FARR Flow Reasoning Evaluation, this does not imply that CIPHER excels at reasoning during problem debugging. Our observations indicate that CIPHER is proficient at predicting the accurate next step given limited information. However, acting often requires more detailed information, which beginners might need to ask for. CIPHER is not capable of troubleshooting the commands it provides. It sometimes makes mistakes, such as hallucinating command arguments. It tends to hallucinate even more when further questions about the correct command are asked.\nWe believe this issue arises from a bias in the penetration testing data. When a user reports a problem, CIPHER relates it to penetration testing. For instance, if a provided pen- etration testing command does not work, instead of correcting the command or asking for the arguments used, CIPHER might assume the target is unreachable or suggest alternative"}, {"title": "5.2. Scaling", "content": "tools for the same purpose. This focus degrades its ability to handle general command errors or Linux command troubleshooting despite incorporating raw datasets that include command line documentation.\nSince CIPHER is designed to understand the current situation and suggest subsequent actions, we did not heavily emphasize coding datasets. CIPHER's coding performance can be enhanced by changing the base model to a coding model or using a higher parameter base model. From a data-centric perspective, improving coding troubleshooting perfor- mance by creating synthetic datasets is possible. This can be done similarly to the CIPHER method, involving experts to fix code and provide explanations, reasoning, and examples.\nOur FARR Flow Augmentation is imperfect; some generated augmentations still mention \"the author\" when they are supposed to be, just like finding notes. We have explicitly forbidden this behavior, but it is still in some samples. This could be caused by the limitation of the model used for generation. However, this does not affect the model response when augmented into question, even if it exists, it will only cause minor issues in perspective sentences.\nAnother difficulty in FARR Flow augmentation is providing challenging \"Findings.\" Generated findings often contain hints, such as \"found a zip file, need to crack the pass- word.\" While this challenges the model to determine which tools to use for cracking, it already suggests a line of reasoning. As a result, the model's focus will be on cracking the password instead of first checking the metadata or trying known target user passwords. This nuance needs to be understood by the model generating the FARR Flow. Although this is not a direct limitation of FARR Flow augmentation, it opens up future work to refine the pipeline and eliminate such results. Rather than waiting for AGI-level LLM, a feasible approach is to use agents or more advanced chain algorithms.\nIn the FARR Flow evaluation, we use Llama 3 70B as the judge LLM. Even though it is a state-of-the-art model, it has limitations and biases"}]}