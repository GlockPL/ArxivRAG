{"title": "KCMF: A Knowledge-compliant Framework for Schema and Entity Matching with Fine-tuning-free LLMs", "authors": ["Yongqin Xu", "Huan Li", "Ke Chen", "Lidan Shou"], "abstract": "Schema and entity matching tasks are crucial for data integration and management. While large language models (LLMs) have shown promising results in these tasks, they suffer from hallucinations and confusion about task instructions. In this paper, we present the Knowledge-Compliant Matching Framework (KCMF), an LLM-based approach that addresses these issues without the need for domain-specific fine-tuning. KCMF employs a pseudo-code-based task decomposition strategy to adopt task-specific natural language statements that guide LLM reasoning and reduce confusion. We also propose two mechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain knowledge sets when unstructured domain knowledge is lacking. Additionally, we introduce a result-ensembling strategy to leverage multiple knowledge sources and suppress poorly formatted outputs. Comprehensive evaluations on schema and entity matching tasks demonstrate that KCMF outperforms previous non-LLM state-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes effectively with SOTA fine-tuned LLMs. Moreover, KCMF generalizes well across different LLMs.", "sections": [{"title": "Introduction", "content": "Schema matching is the task of identifying correspondences between elements of two or more database schemas. This task plays an important role in data integration efforts. Another task called entity matching, also known as entity resolution or record linkage, aims to identify schema instances that refer to the same real-world entity. While these two tasks vary in their definitions and approaches, they share the common goal of matching database elements. This paper attempts to tackle these tasks under the umbrella term data matching, employing a unified methodology while still retaining their original problem settings independently, i.e., entity matching is solved based on the database records, whereas schema matching is solved using metadata due to privacy considerations.\nEarly systems for data matching tasks rely on expert systems or traditional machine learning methods, while modern data matching approaches often employ pre-trained language models (PLMs) as the backbone model (Li et al., 2020; Zhang et al., 2021; Zeakis et al., 2023), which is then fine-tuned on task-specific datasets. However, PLM-based methods may suffer from the effort collecting fine-tuning data and face performance degradation when handling unseen domain data during inference. Large Language Models (LLM) are PLMs of massive scales with billions of parameters, trained on vast and diverse datasets. With huge internal knowledge gained from pre-training and strong representation capability, LLMs show competitive performance on numerous data-wrangling benchmarks, particularly without the need for fine-tuning (Narayan et al., 2022; Peeters and Bizer, 2023; Sheetrit et al., 2024; Peeters and Bizer, 2024). This remarkable efficiency liberates users from the burden of fine-tuning efforts.\nDespite promising results, LLM-based approaches to data matching face several performance issues. For clarity of presentation, we focus mainly on schema matching in the main text; further discussions of entity matching are provided in Appendix A.1. As depicted in Figure 1, these issues are categorized into three types: (1) hallucination, where the LLM generates incorrect deduction from correct evidence at hand; (2) under-matching, where the LLM rejects matching with overly strict criteria; and (3) over-matching, where the LLM over-generalizes the association of the input data. While hallucination is a widely recognized problem that can harm the LLM's performance in many other tasks (Zhang et al., 2023a; Rawte et al., 2023), under-matching and over-matching, jointly termed as confusion in this paper, are challenges specific to the data matching tasks.\nIn this study, we present the Knowledge-compliant Matching Framework (KCMF), a fine-tuning-free and retrieval-enhanced approach to data matching tasks. To address the problem of confusion, KCMF employs a pseudo-code-based task decomposition strategy for LLMs. Pseudo-code here is an ordered list of task-specific conditional statements, written in natural language, that guide the matching process. The LLM can easily evaluate the validity of each condition for the data being matched, and then follow the corresponding directives. By walking through the pseudo-code, the LLM is able to reason its way to the final matching result. Unlike Chain-of-Thought (Wei et al., 2022), which relies on LLM's internal knowledge to generate reasoning steps, our KCMF uses explicit task-specific pseudo-code, eliminating the need to write reasoning chains for different datasets.\nTo address hallucination, KCMF incorporates external knowledge in the form of natural-language sentences related to the input data. To compensate for the lack of such unstructured domain knowledge, KCMF builds domain knowledge sets by leveraging readily available datasets (Dataset as Knowledge, DaK) and examples (Examples as Knowledge, EaK) from various domains. Since fine-tuning-free LLMs tend to generate improperly formatted outputs that do not match the format given in the demonstration, we employ a technique called Inconsistency-tolerant Generation Ensembling (IntGE) to suppress such unexpected outputs and maintain an automated downstream workflow. We evaluate KCMF on several benchmarks for knowledge-intensive matching tasks, namely MIMIC and Synthea from the OMAP project (Zhang et al., 2021), and MMM, modified from MedMentions (Mohan and Li, 2019). The results show that KCMF surpasses 1) all LLM baselines, 2) SMAT (Zhang et al., 2021), a non-LLM method, and 3) Jellyfish (Zhang et al., 2024a), a fine-tuned LLM-based state-of-the-art (SOTA). Our main contributions are as follows."}, {"title": "Background and Task Definition", "content": "Conventional SM approaches can generally be categorized into 1) constraint-based methods, which utilize attributes defined in database constraints to measure similarity among schemas (Alexe et al., 2010; Chen et al., 2018; Atzeni et al., 2019), and 2) linguistic-based methods, which leverage the semantic information contained in schema names or descriptions to construct mappings (Kettouch et al., 2017; Asif-Ur-Rahman et al., 2023). More recent studies have adopted deep neural networks to tackle SM. (Zhang et al., 2021) use an attention-based BiLSTM with pre-trained word embeddings. (Zhang et al., 2023b) leverage BERT (Kenton and Toutanova, 2019) to generate schema features, which are then used to train a linear classifier under semi-supervised learning. With the advent of LLMs, (Narayan et al., 2022) was the first to apply LLMs to SM, using straightforward serialization with few-shot settings. Recent LLM-based SM approaches, such as (Zhang et al., 2024a), attempt to tackle this task using Supervised Fine-tuning (SFT). While SFT achieves strong results, it needs tremendous efforts to collect fine-tuning data and is hard to transfer to unseen domains. Another line of works, such as (Sheetrit et al., 2024) and (Parciak et al., 2024), utilize proprietary LLMs, which have shown promising results on several benchmarks. However, these approaches still suffer from high computational costs and produce indecisive outputs. Our approach aims to extend the scope of LLM-based SM by utilizing task-specific pseudo-code to guide LLMs' predictions within a single-round inference. The designed pseudo-code offers explicit criteria for the matching task, addressing the confusion problem mentioned earlier."}, {"title": "Entity Matching (EM)", "content": "EM is often considered a downstream task of SM by many previous works (Barlaug and Gulla, 2021). Similar to SM, traditional non-LLM solutions for EM focus on computing similarity between entities (Thirumuruganathan et al., 2018; Ebraheem et al., 2018; Kasai et al., 2019; Li et al., 2020). Following the initial application of LLM to EM (Narayan et al., 2022), LLM-based solutions have been actively explored (Peeters and Bizer, 2023; Fan et al., 2024; Peeters and Bizer, 2024; Li et al., 2024). From the outset, our approach is designed as a unified framework capable of addressing both SM and EM. By recognizing the deep semantic similarities and shared motivations between SM and EM - such as aligning and matching data elements - we develop a methodology that seamlessly integrates both tasks."}, {"title": "Retrieval Augmented Generation (RAG)", "content": "RAG incorporates retrieved documents into queries and has become a popular paradigm in mitigating hallucination in LLMs (Lewis et al., 2020; Yu et al., 2023; Shao et al., 2023; Jiang et al., 2023b; Asai et al., 2023; Xu et al., 2023; Shi et al., 2024). While RAG has shown reliable results in addressing hallucination, its performance is still limited by the quality of the retrieved documents and, obviously, is challenging to deploy in scenarios lacking unstructured knowledge sources. Our approach adopts the concept of introducing external knowledge to alleviate hallucination. In particular, to address the lack of unstructured domain knowledge in data matching, we present two mechanisms, DaK and EaK, for building knowledge sets by utilizing existing domain knowledge bases. Our approach showcases the potential of external knowledge enhancement strategies on LLM-based classification tasks."}, {"title": "Task Definition", "content": "We denote by S the source schema from database D, and S' the target schema from database D'. The goal of schema matching is to identify all pairs of attributes (A, A') such that A \u2208 S and A' \u2208 S', and both attributes represent the same information in their respective schemas.\nIn this paper, we focus on a more straightforward scenario: we enumerate all possible mappings M \u2286 D\u00d7D' and determine whether each mapping {r,r'} \u2208 M is correct or not. Each candidate mapping {r, r'} consists of two items, where each item r is composed of a schema name N and a schema description C from the corresponding database D.\nWe framework our schema matching task as an LLM generation task guided by pseudo-code instructions under a knowledge-enhanced setting. This involves 1) a list P of designated task-specific pseudo-code and 2) a list K of knowledge items retrieved from all available knowledge sets K.\nWe serialize the inputs, including the pseudo-code P, the candidate mapping {r,r'}, and the retrieved knowledge items K, into a prompt suitable for the LLM using a serialization function \u03c6. The LLM L then generates a response LR based on this prompt:\nLR \u2190 L(\u03c6(P, {r,r'}, K)). (1)\nOur task objective is to classify the correctness of each candidate mapping. Specifically, we aim to obtain a binary classification result c \u2208 {yes, no} from the LLM's response LR, indicating whether the mapping {r, r'} is correct. For clarity of presentation, here we focus on defining SM, though it should be noticed that this setting can also be generalized to entity matching task (see A.1 for details)."}, {"title": "Matching Framework", "content": "Figure 2 presents an overview of our proposed KCMF, which operates in four sequential stages as follows. Sl. Pseudo-code Design: Experts decompose the task into task-related conditional statements in natural language. A superior advantage of our approach is that this pseudo-code for a task is designed once and can be reused by anyone performing the matching task, without the need to write custom ones. S2. Knowledge Retrieval & Construction: KCMF constructs granular domain knowledge and database-structure knowledge via retrieving information from domain knowledge bases and discovering database structures. S3. Prompt Generation: Utilizing the knowledge from S2 and pseudo-code from S1, KCMF generates LLM prompts that include demonstrations. S4. Inconsistency-tolerant Generation Ensembling: KCMF uses the constructed prompts to query the LLM. The multiple outputs generated are then combined, mitigating the ill-formatting issue, to reach the final decision."}, {"title": "Pseudo-code Design", "content": "As Figure 1 depicts, one of the challenges that LLMs face when performing matching is the issue of under-matching and over-matching. This issue stems from the ambiguity of the task instruction \"match\". Without additional context, the word \"match\" lacks a clear definition in this setting. Take schema matching as an example. The term \"match\" in this context has at least three different interpretations, depending on the focus of the task:\n\u2022 The data types of the column values are the same or convertible to each other.\n\u2022 The table definitions are semantically related.\n\u2022 The schemas refer to the same real-world concept.\nIn light of this ambiguity, we propose a strategy to decompose the task into pseudo-code composed of conditional statements directly derived from the task's motivation. As shown in S1 of Figure 2, we define statements as natural language predicates organized into a set of if-then-else logical constructs. Each statement provides a sufficient condition for determining whether cases should be \u201cmatched\" or \u201cnot matched\". The designed pseudo-code should cover the task's full range of conditions relevant to the task. To this end, manual effort is required to understand the task's motivation and design the pseudo-code R accordingly, our version of implementation is shown in Algorithm A.4.1. To utilize this pseudo-code, we further propose a reasoning prompting strategy, following the idea of Chain-of-Thought (Wei et al., 2022), which will be discussed in the next section."}, {"title": "Building Domain Knowledge Set", "content": "To solve the matching task within a knowledge-enhanced setting, we first need to retrieve a domain knowledge list K. However, because answers for matching tasks cannot be explicitly derived from retrieved information and due to the scarcity of unstructured domain knowledge, we introduce two mechanisms, Dataset as Knowledge (DaK) and Examples as Knowledge (EaK), to build highly relevant unstructured knowledge sets specifically tailored for matching tasks and the implementation details can be found in Appendix A.4.2."}, {"title": "Dataset as Knowledge (DaK)", "content": "Due to privacy concerns, concrete records from the source databases D and D' are often inaccessible to the schema matching system (Johnson et al., 2023; Zhang et al., 2023b). This means that the matching task must be performed using only metadata. In this setting, LLMs are required to understand the structures of D and D', which, however, can only be partly seen during inference through the limited metadata representations {r, r'} (cf. Section 2.4). Hence, we propose DaK to acquire knowledge of the structure of source databases by searching metadata from the full dataset M, as shown in Figure 3.\nThe procedure of DaK aims to discover database objects and their metadata from the candidate pool M (see Algorithm 1 for details). As shown in Figure 3, DaK operates in three steps: 1) for a given candidate pair {r,r'} \u2208 M, DaK identifies an object O; in the example, O is a table named \"provider\". While this step can utilize techniques like Named Entity Recognition (NER), for rapid prototyping, we extract O by traversing all schema names N and N' associated with r, r' (see \u2460). 2) DaK then scans the remaining pairs {r, r'} to find metadata KDaK related to the identified object O; in this example, KDaK is the description of the table \"provider\". This is accomplished by matching O with descriptions C from the other candidate pairs (see \u2461). 3) After obtaining an object list Cobj consisting of each identified object O, and a metadata list CDak consisting of all matched knowledge KDaK, DaK proceeds to form the name of O as an entry and KDaK of O as the corresponding DaK knowledge. As shown in Figure 3, the name of the identified object \u201cprovider\" and the description of \"provider\" are constructed into a piece of DaK knowledge: {provider: provider table contains clinicians that provide patient care}."}, {"title": "Examples as Knowledge (EaK)", "content": "Using text chunks from a self-built corpus has become a de facto approach for retrieval enhancement to reduce LLM hallucinations (Gao et al., 2024). For common-sense QA tasks, such text chunks can be easily retrieved from existing commonsense corpora. However, the knowledge required for matching tasks is domain-specific, and existing KBs are mainly structured as entity databases or thesaurus rather than natural language text. Thus, we propose EaK, as depicted in Figure 4.\nAs its name suggests, EaK aims to explain complex concepts using examples. Given a pair {r, r'}, EaK first extracts keywords from it by querying an LLM, and then again queries an LLM to filter the domain-irrelevant ones (see Appendix C for details). Obtaining all candidate keywords, EaK uses these candidates to search domain KBs for the top-k related records. For remote KBs, retrieval can be done through a search over the provided web API; while for local KBs, this procedure can be implemented using stronger dense retrieval. For each retrieved record, EaK leverages its relationships and properties to form explanatory knowledge. For example, in Figure 4, EaK generates the knowledge by phrasing the parent relation."}, {"title": "Prompt Construction", "content": "Pseudo-code-based Reasoning Prompting Ina paradigm of k-shot in-context learning, we sample k valid match pairs {d, d'} as demonstrations together with the pseudo-code R, the queried pair {r, r'}, and knowledge list K. The fundamental pattern of the prompt is illusrated in Figure 5.\nAs demonstrated in Figure 5, there are two placeholders, {CRSNG} for the reasoning steps and {ans} for the answer. To generate CRSNG and ans, we first define each single statement in pseudo-code R (cf. Section 3.2) as p\u2192 q. Then, we check each statement sequentially: for the current statement p \u2192 q, the condition p is checked if is satisfied; if p is fulfilled, terminate the process and set the answer ans = q; if not, proceed to the next statement; continue this process until an answer is drawn. The procedure is formalized in Algorithm 2 in Appendix. Next, the reasoning steps obtained, CRSNG, and the answer, ans, will be inserted into the placeholders. This procedure is repeated for all k demonstrations to obtain the complete set of reasoning steps. Although we discuss a simple scenario here, the pseudo-code can be extended to handle more complex cases with multiple reasoning paths by adding an \u201cELSE\" clause to specify alternative statements for the LLM to follow.\nSelf-Indicator Extraction Inspired by the performance gain observed when generating a high-quality summary at the beginning of reasoning in our experiments, we further introduce a Self-Indicator Extraction pre-task. This plugin module generates a filtered text segment, called self-indicator (KSI), describing the key information from {r, r'}. Specifically, taking {r,r'} and K as input, an LLM is utilized to generate KSI (see A.4.3 for details), and then KSI is appended to the end of the prompts created above.\nDemonstration Summarization After decomposing the task instructions into pseudo-code (see Section 3.2), we leverage in-context learning to enable the LLM to learn reasoning behaviors rather than just the concept of \"match\u201d. To manage the prompt length, we use an LLM to summarize all demonstration pairs {d, d'} (see A.4.3 for details). Instead of using the original demonstrations {d, d'} in the prompts, we may use the summarized pairs to improve efficiency without losing essential information."}, {"title": "Inconsistency-tolerant Generation Ensembling (IntGE)", "content": "With multiple knowledge sets available, a straightforward utilization is to combine all retrieved knowledge within one prompt, but this practice leads to information within a prompt flooded thus bringing unexpected outputs (Parciak et al., 2024). Therefore, we instead propose IntGE to integrate diverse information sources and improve output stability in a fine-tuning-free manner. We define the available knowledge sets from unique sources as K = [K1, K2, ..., KN]. For a given pair {r,r'}, we retrieve information from each source in K, resulting in N knowledge lists. Each list is used to construct a prompt following the method in Section 3.4, creating N prompts. We ensemble the N binary classification results using majority voting to determine the prediction for {r,r'}.\nAs noted in previous studies (Cuconasu et al., 2024; Zhang et al., 2024b), LLMs tend to prefer familiar input from their pre-training phase, and distracting context can lead to poor outputs. IntGE aims to prevent interference between different knowledge sources by separating them into distinct prompts. Inspired by (Wang et al., 2023; Shi et al., 2024), we adopt a straightforward voting strategy. This approach leverages multiple knowledge sources while enhancing the model's robustness against poorly formatted outputs by keeping prompt lengths manageable."}, {"title": "Experiments", "content": "Following (Narayan et al., 2022), we choose the challenging Synthea but also the MIMIC from the OMAP benchmark (Zhang et al., 2021), for the schema matching task. As for entity matching, we adapted the CC0-licensed MedMentions benchmark (Mohan and Li, 2019) to construct a new dataset called MedMentions Matching (MMM), whose details of modification is presented in Appendix A. Based on the datasets above, we give the statistics as shown in Table 2."}, {"title": "Main Results", "content": "As shown in Table 1, generally KCMF is significantly better than all LLM baselines and SMAT. Specifically, on Synthea, GPT-3.5-version KCMF outperforms the previous SMAT by 3.92% on the F1-score, and on MIMIC the gain increases to 41.87%. In addition, we observe that the F1-score of GPT-3.5-version KCMF outperforms all baselines by an average of 33.73% and 50.59% on Synthea and MIMIC, respectively. On MMM, the strongest F1-score of KCMF surpasses all baselines by an average of 10.62%. Although we see a slight drop in accuracy on the GPT-4o version of KCMF, we consider this a strong enough result because, first, the 0.05% drop is insignificant enough for the unbalanced matching task, and both schemes already achieve accuracy better than 99%; second, the 0.05% drop is due to correct recall of three extra positive cases and incorrect recall of three negative cases, which look reasonable for a dataset with only 13 positive samples.\nPseudo-code ensures recalling while maintaining the precision. Although the specific recall and precision are not reported in the main results, the increase in F1-score shown by KCMF is mainly achieved by recalling more true-positive cases and controlling the growth of false-negative instances so that the precision does not drop drastically, which exactly cater to the matching task's preference for recall in practice, eliminating the need for additional human efforts to perform complex verification on recalled results. The performance gain of KCMF owes to the fact that the recalling ability brought by pseudo-code does not depend on the model's learning of the exposed data distribution, and thus KCMF can correctly reject negative instances while recalling positive ones.\nKCMF generalizes over backbones. We also observe that after implementing KCMF, the F1-score of each backbone is improved by an average of 17.98%, 17.21%, 6.44% on MIMIC, Synthea and MMM, respectively. These experimental results demonstrate that our proposed method KCMF can generalize over different backbones and consistently improve the performance of LLMs on both SM and EM tasks. Interestingly, we observe that on MIMIC, the strongest version of KCMF achieves an F1-score of 0.6207, which is significantly better than the 0.4314 achieved by previous SFT SOTA Jellyfish (Zhang et al., 2024a). This result indicates the great potential of fine-tuning-free LLMs in matching tasks in domain-specific data."}, {"title": "Task-specific pseudo-code disambiguates the task instruction", "content": "The primary purpose of the pseudo-code is to avoid LLM's confusion about the task's motivation by designing explicit conditional statements. To explore its effectiveness, we examine KCMF on Synthea without the designed statements: We removed all pseudo-codes and the corresponding reasoning steps from the demonstrations to compare with the complete KCMF.\nWe categorize the error types of all false-positive samples into three types: the Over-matching (OM), which is already mentioned in Figure 1; The Position Mismatching (PM), which indicates LLM concludes a positive answer by mismatching the different parts of r and r', e.g., schema A's table and schema B's column; And the Incorrect Reasoning (IR), which is the other kind of errors can not be categorized into the former two types. As shown in Table 3, it is obvious that the pseudo-code does mitigate all the three error categories above."}, {"title": "IntGE ensures robustness towards badly-formatted outputs", "content": "To examine whether KCMF gains from IntGE, we test a knowledge-all-in-one (AIO) version of KCMF, which removes IntGE and concatenates the knowledge from all sources into one prompt.\nThe results are categorized into three types as shown in Figure 6. Badly-formatted means the output format of the KCMF does not follow the format given in the demonstrations, and Well-formatted means the opposite. Eliminated means that the badly-formatted output appeared in IntGE's votes but was eventually eliminated by IntGE's voting strategy, thus the final output remains Well-formatted. The results clearly indicate that the AIO setting performed poorly. In contrast, IntGE effectively reduces the number of badly-formatted output via its multi-prompt voting mechanism. The results imply a paradigm for retrieving knowledge from different sources for inference-only LLMs, namely to split the retrieved knowledge by its source into multiple prompts to achieve better results while limiting the length of each prompt."}, {"title": "Conclusion", "content": "We presented KCMF, a fine-tuning-free framework for solving data matching tasks under a knowledge-enhanced setting, with a pseudo-code based task instruction strategy for addressing the problem of confusion and mechanisms DaK, EaK, and IntGE for building and utilizing knowledge from various sources in scarcity of unstructured domain knowledge. Our evaluation showed that KCMF outperformed all LLM baselines and non-LLM SOTA under a completely fine-tuning-free setting and our decomposition diagram effectively mitigated LLMs' confusion towards the matching tasks. Our future work involves extending KCMF to make it suitable for both metadata-based and instance-based scenarios, and further improving the efficiency of the matching workflow."}, {"title": "Limitations", "content": "One limitation of our work is the need for human labor to select domain KBs as the knowledge sources, which may pose a limitation in implementation. Also, this paper primarily focuses on the scenario where matching is performed by predicting enumerated data pairs instead of directly identifying potential mappings among the source databases, which may increase the computational burden and thus have an environmental impact, we leave this efficiency optimization potential to future work. Moreover, since matching tasks are usually studied on enterprise data, there are few open-source benchmarks, and our experiments are only conducted on MIMIC, Synthea and MMM. Due to this scarcity, it would be worth constructing more benchmarks to push this limit further. Furthermore, though KCMF performs better than all LLM baselines and the non-LLM SOTA, it only outperforms SFT-LLM SOTA on MIMIC. We deem this gap a trade-off with fine-tuning efforts in our current research stage, and it would be intriguing to go further to build a stronger fine-tuning-free system."}, {"title": "Methodology", "content": "Because entity matching is performed on the level of records, we can define the task of entity matching by slightly changing the definition of r described in Section 2.4:\nr = {N, attr1, attr2, ...attrm} (2)\nHere N similarly represents the name of the entity, but we replace the schema description C with a sequence of attr, which represents all the other column values of the entity r. Specifically, for the MMM benchmark where only entity name and entity context are available, r can be directly derived from the serialization of the name and context."}, {"title": "Implementation", "content": "The pseudo-code we used in our experiments is shown in Tables 20 and Table 21, and the given pseudo-code is directly written by the author according to the motivation of the matching task. As emphasized in the main text, the pseudo-code statements are defined as \"IF-THEN-ELSE\". In order to better utilize the large language model's natural language comprehension capability, we have corresponded \"IF-THEN-ELSE\u201d to \u201cIF, THEN, otherwise\" in the actual design of the pseudo-code. To ensure that the given pseudo-code is checked in the correct order, a statement is added at the beginning of the designed pseudo-code to declare that the rest of the statements should be checked in order, which can be removed when using stronger LLMs (e.g., GPT-40). Also, to cover all the task motivations of the matching task, we include overlapped conditions in the pseudo-code statement design, and for the last statement of all the pseudo-codes, we set a general condition to ensure full coverage."}, {"title": "Knowledge Retrieval & Construction", "content": "The retrieval pipeline in this paper is based on keywords extracted from {r, r'}, then the quality-managed keywords are used to retrieve and construct the knowledge sets following the techniques demonstrated in Section 3. Specifically, we adopt a gpt-3.5-1106 to extract domain-specific, difficult-to-understand keywords from {r, r'} (see Table 12 for details of the prompt). After that, we use a gpt-3.5-1106 to filter the extracted keywords (see Table 13 and Table 14 for details of the prompt), and we empirically design a blacklist for further rejecting low-quality keywords.\nTo construct the EaK knowledge set, we select Snomed-CT as the knowledge base and use the Snomed-CT API\u00b2 based on the above keywords to search for the associated entities in the KB, specifically here we keep the top-1 search results, and for the searched entities, we query their children entities again using the Snomed-CT API and randomly sample up to 3 children entities. The parent and children are then serialized into EaK knowledge in the form of \"One of parent is children\".\nFor out-of-domain knowledge sources, we based on the English Wikidata API\u00b3 to search for entity codes using extracted keywords. We use the searched top-1 entity code to construct SPARQL statements for knowledge query from Wikidata\u2074 while using this entity code to query the page extraction from English Wikipedia\u2075. Then, for the retrieved results, we use gpt-3.5-turbo-1106 here to summarize the retrieved extractions to limit their length to no more than 1000 words under a zero-shot setting (see Table 17)."}, {"title": "Prompt Construction", "content": "We use a gpt-3.5-1106 to extract the self-indicator of schema pairs (or entity pairs in MMM), used prompts can be found in 15. To implement Demonstration-summarization-only, we also use a gpt-3.5-1106 to compress the input {r, r'}. Specifically, to control the length of the prompt, we split {r,r'} into 2 queries (see Table 16 for detailed prompt)."}, {"title": "Inference Settings", "content": "For MIMIC and Synthea, we take Wikidata with DaK, Wikipedia with EaK, and EaK as the knowledge sources to evaluate KCMF under 4-shot. For MMM, we take Wikidata, Wikipedia, and EaK as the knowledge sources to evaluate the framework under 2-shot. We empirically set temperature to 0 and top_p to 0.1 for stable outputs if these two parameters are available. Also, we directly took poorly formatted outputs as negative predictions in our experiments for metrics calculation and workflow automation. The pseudo-code we designed for these two tasks can be found in Appendix C. We derived the main results on MIMIC and Synthea from the best result of running five times, and for MMM, due to its larger size of data, we ran each setting once as the final results."}, {"title": "Dataset & Code", "content": "All datasets can be accessed through original papers cited in this study, specifically for modifying MMM, the script is given in our supplementary codes."}, {"title": "Ablation Study", "content": null}, {"title": "Prompt Design & Techniques", "content": "We take the Wikidata+DaK with a 3-shot on Synthea as the initial setting and conduct incremental experiments. The results are presented in Table 4.\nTO Inst. denotes Task-Oriented Instruction. Based on the instruction used by (Narayan et al., 2022), we changed it to \"Can records in schema B be transformed and stored into schema A?\", which is more compliant with the motivation of the schema matching task.\nKnow2Rule denotes Knowledge to Rule. We renamed the \"knowledge for the task\" used in the prompt \"rules for task\".\nU-indices denotes Unique indices. We apply unique indices for the different elements of the prompt that need to be numbered. We use Roman numerals, lowercase letter numbers, and numeric numbers separately for the row number of pseudo-code, the sequence number of retrieved knowledge, and sequence number of reasoning steps.\nFor Inst. Extraction, we extract the instructions and rules for each demonstration and place them at the beginning of the prompt.\nAll the optimizations we added are motivated by achieving higher precision while ensuring promising recalls. As shown in Table 4, CoT, U-indices and Inst. Extraction brought 13.04%, 3.36%, 2.69% improvement in F1-score on Synthea dataset. Although Summary Pretask caused a temporary drop in performance, the combination of Summary Pretask and +1 Demo still shows a strong result; the F1-score is improved by 6.93% compared to the Inst. Extraction step. We examine the results of the Summary Pretask and the +1 Demo as a whole here because the motivation for the Summary Pretask was to shorten the length of the prompt."}, {"title": "Summary Pretask", "content": "In order to analyze the effectiveness of each module in KCMF, we compare the KCMF with its variants under different settings.\nFor the summary pretask, we apply two strategies: All and Demo Only, where Demo Only is the strategy of summarizing only the demonstration used in the prompt, as shown in Section 3.4. All is the strategy of adding the summarizing of the target samples on top of Demo Only (in this case we summarize the descriptions of given schema pairs). As shown in Table 5, the compression strategy of Demo Only outperforms All strategy by 1.3% on F1-socre under the 4-shot setting."}, {"title": "Examples as Knowledge", "content": "Following the Demo Only setting in B.2, we further verify the validity of the knowledge constructed through EaK by introducing Eak as a knowledge source and form three available knowledge sets of Wikidata, EaK, and Wikidata+"}]}