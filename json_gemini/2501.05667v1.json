{"title": "TransPlace: Transferable Circuit Global Placement via Graph Neural Network", "authors": ["Yunbo Hou", "Haoran Ye", "Yingxue Zhang", "Siyuan Xu", "Guojie Song"], "abstract": "Global placement, a critical step in designing the physical layout of computer chips, is essential to optimize chip performance. Prior global placement methods optimize each circuit design individually from scratch. Their neglect of transferable knowledge limits solution efficiency and chip performance as circuit complexity drastically increases. This study presents TransPlace, a global placement framework that learns to place millions of mixed-size cells in continuous space. TransPlace introduces i) Netlist Graph to efficiently model netlist topology, ii) Cell-flow and relative position encoding to learn SE(2)-invariant representation, iii) a tailored graph neural network architecture for informed parameterization of placement knowledge, and iv) a two-stage strategy for coarse-to-fine placement. Compared to state-of-the-art placement methods, TransPlace-trained on a few high-quality placements-can place unseen circuits with 1.2x speedup while reducing congestion by 30%, timing by 9%, and wirelength by 5%.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of integrated circuits (ICs) has propelled technological advancement from single chips to complex computing systems. Placement, a crucial stage in the design flow of Integrated Circuits (ICs), arranges electronic components within circuit layouts. As placement constructs the fundamental geometry from a topological netlist, it drives later design steps and guides prior stages, serving as a critical determinant of final chip performance. The placement problem can be described as follows. Consider a circuit design represented by a hypergraph H = (V, E), where V represents the set of cells (electronic units), and E represents the set of nets (hyperedges) between these cells. Placement determines cell positions x and y to minimize wirelength while avoiding cell overlap. This problem typically penalizes the density and mathematically formulates as [12, 22]\n\nmin\u2211 WL(e; x, y) + AD(x, y).\nx,y\neEE\n\nHere, WL() is the wirelength cost function that evaluates the wirelength of any given net instance e, D(,) is the density function to spread cells out in the layout, and A is a weighting coefficient.\nDespite decades of development, existing global placers tackle each placement instance from scratch using heuristics or expertly crafted algorithms [1, 8, 20, 22, 31, 38, 46]. These optimizationbased and non-learnable global placers cannot leverage existing placement experience, thus limiting solution efficiency and chip performance. On the other hand, recent advances in transferable"}, {"title": "2 TRANSPLACE", "content": "TransPlace is schematically illustrated in Fig. 1, consisting of two stages: inductive placement and circuit-adaptive fine-tuning. For inductive placement, we introduce Netlist graph (\u00a7 2.1) and cellflow (\u00a7 2.2) for efficient and topology-aware circuit modeling, SE(2)-invariant encoding and decoding that converts between relative and absolute cell positions (\u00a7 2.3), and TPGNN to learn transferable placement knowledge (\u00a7 2.4). Following inductive placement, we fine-tune the placement to adapt to circuit-specific characteristics and constraints (\u00a7 2.5)."}, {"title": "2.1 Netlist Graph", "content": "The circuit design can be represented as a hypergraph H = (V,E), where V denotes the set of cells (electronic units) and E is the set of nets (hyperedges). H stores the circuit topology produced in the preceding logic synthesis stage. As exemplified in Fig. 2(1), there are two types of electronic cells V:\n(1) Terminals Vr are big chunks of cells with fixed positions.\n(2) Movable cells VM are relatively smaller cells, and their positions are the decision variables of global placement.\nThe Netlist Graph (Fig. 2(2)), which serves as an input for inductive placement, is designed to fully retain the topological information. We formally define it as follows:\nDefinition 2.1 (Netlist Graph). G = {V, U, P, XV, XU, XP, PT}, where V = VT UVM are electronic cells, nets U are the hyperedges connecting the cells, and pins P C V \u00d7 U represent the interactions among cells and nets. Xs are their feature matrices. PT \u2208 RIVT \u00d72 stores the horizontal/vertical positions of terminals \u0474\u0442.\nHowever, for some VLSIs with millions of cells and nets, directly processing the whole netlist graph exceeds memory limits and prevents effective learning. Therefore, we transform the original netlist graph G to Hierarchical Netlist Graph G = fHIER (G) = (GR, {G1,..., Gn}), where {Gi|i = 1,\uff65\uff65\uff65, n} are sub-netlist graphs of G (named branch graphs) and GR is a transformation of G coarsened with the branch graphs (named root graph). Specifically, we first produce a partition result with KaHyPar [45]:\nKaHyPar(G) = {V\u00a1 c V\u2081|i = 1,\uff65\uff65\uff65, n},\nwhere Vi \u2260 j, Vi \u2229 Vj = 0.\nNote that the cells within each partition set are fully connected via nets. Otherwise, we further partition the branches. For every Vi C VM, we extract Sub-netlist Graph and coarsen the original netlist graph; more algorithmic details are presented in Appendix A. The training and inference are performed on individual graphs in G, and the placement results of original G are reconstructed from G. In Appendix D.1, we demonstrate that the time complexity of Algorithm 1 is O(|V| + |U| + |P|), which is optimal."}, {"title": "2.2 Cell-flow", "content": "Learning (from) relative positional relationships instead of absolute spatial positions improves generalization across tasks [17, 23, 44, 55, 58]. However, the complexity of preserving all relative positions in global placement is \u2211e degreee, where e denotes the hyperedges. For circuits with millions of nodes and hyperedges, such naive modeling would cause infeasible computational demands. In answer to this, we present cell-flow to efficiently model relative positional relationships in circuit designs.\nCell-flow (Fig. 2(3)) is a set of directed acyclic edges. It represents the relative positions of cells in a circuit for encoding (the demonstration data) and decoding (the inductive solution). We initialize a cell-flow from a netlist graph with a breadth-first search that starts from fixed cells and travels through nets. Meanwhile, we record the incoming flow edge for each cell using fINCOME, and the net containing the flow edge using fNET. This process is elaborated in Algorithm 2. Note that every connectivity branch contains at least one fixed cell (e.g. port), so cell-flow involves all cells. This is also guaranteed for sub-netlist graphs in \u00a7 2.1 because they are fully connected and contain a pseudo-terminal.\nDefinition 2.2 (Cell-flow). A set of directed edges FC V \u00d7 V, where V are electronic cells in netlist graph G. It is guaranteed that no loop will be found in F.\nAs demonstrated in Appendix D.2, the time-consumption of generating cell-flow edges is O(|V| + |U| + |P|), and the cell-flow size is |F| = O(|P|). So this procedure is optimal in terms of time and space complexity."}, {"title": "2.3 SE(2)-invariant Encoding and Decoding", "content": "Based on the relative position relationship within cell-flow, we introduce an SE(2)-invariant encoding/decoding algorithm to convert between cell absolute/relative positions. This SE(2) invariance denotes that the representations remain unchanged under rotations and translations in the 2D plane, which allows our model to learn informed representation by preserving geometric consistency.\nFor the demonstration data (labels), we encode the absolute cell positions P into relative positions P for training. This encoding process follows the cell-flow F within the placement and computes relative positions only for the flow edges (vi, vj) \u2208 F:\n\nPi,j = Pj - Pi, (vi, vj) \u2208 F,\n\nwhere pi is the absolute position of cell v\u012f. The vector pi,j of flow edge (vi, vj) is later transformed into diameter pi,j and angle di,j in polar coordinate, and a corresponding deflection A\u03b8i,j is calculated by subtracting the polar angle of vi's cell-incoming-edge:\n\nPi,j = |Pi,j|2\nOi,j = arctan pi,j,\nA0i,j = 0i,j \u2013 Ok,i where (vk, vi) = fINCOME (vi).\n\nOverall, the distance p and the deflection A\u03b8 are utilized as the SE(2)-invariant encoding of the cell positions P:\n\n(p, 0) = fENCODE (PM; G).\n\nOn the other hand, we need to convert the inductively generated relative positions into absolute ones. In this decoding process, given an input netlist graph G and a relative position encoding (\u03c1, \u0394\u03b8), we determine the absolute positions PM of the movable cells by extracting the cell-paths (see Fig. 2(4)) from G as described in \u00a7 2.2. For a cell-path (20, 21,\u00b7\u00b7\u00b7, Ut\u22121, Ut)p, which starts from a fixed cell vo with a fixed position po, we can calculate the position vector Pt-1,t of the ending cell-flow edge (vt-1, vt):\n\nPo = |po|2\n\u03b8\u03bf = arctan po\nOt\u22121,t = 00+ \u03a3 \u0394\u03b8\u03af-\u22121,i\ni=1,...,t\nPt-1,t = [Pt-1,t cos Ot\u22121,t, Pt\u22121,t sin Ot\u22121,t].\n\nThe absolute ending position of cell-flow edge (ut-1,Ut) is the summation of the position vectors through the cell-path. As there might be multiple cell-flow edges ending at cell ut, the decoded absolute position of cell vt is averaged over edges:\n\nPt;(t-1,t) = Po + Pi-1,\nPt = mean _Pt;(i,t).\n(Vi,Ut) EF\ni=1,...,t\nOverall, by stacking pj, j\u2208 VM, we obtain the decoding results:\n\nPM = DECODE(\u03c1, \u0394\u03b8; G).\n\nThe time consumption of producing the position vectors for all edges in F is w|F|, where w is the average length of cell-paths. Since w is typically small (see Appendix D.3), it can be treated as a constant. We also have |F| = O(|P|) (Appendix D.2). As a result, the time required for relative position decoding is proportional to the scale of the netlist graph, which is optimal."}, {"title": "2.4 TPGNN", "content": "2.4.1 Inference. We illustrate the inference pipeline of TPGNN in Fig. 1. TPGNN takes a netlist graph and its cell-flow as inputs, then (0) (0)\nembeds raw features XV, XU, and Xp into hidden representations H), H, Hp, via Multi-Layer Perceptrons (MLPs). Then, it generates deeper representations of cells V, and nets U with L layers of message-passing. Finally, readout layers convert the output cell and net representations into relative cell positions.\nIn each layer, the topological information is collected through cell-flow and netlist graph message-passing. The messages are then used to fuse and update the representations of cells V and nets U. For netlist graph message-passing, we interact the messages of cells V and nets U through graph edges which preserve the topological connection between cells and nets:\n\nM(1) MV = fu-v (U, P, H), Hp),\nMU = FV-U(V, P, H), Hp),\n\nwhere l is the number of current layer, M and M denote the (1\nmessages of cells V and nets U computed on netlist graphs. fv U"}, {"title": "2.5 Circuit-adaptive Fine-tuning", "content": "While inductively generating placement is efficient, it may neglect the unique characteristics of an unseen circuit, such as its terminal positions and core placement area. Therefore, after generating the inductive placements, we iteratively fine-tune them by minimizing wirelength [19] and density [22].\nThe wirelength objective is calculated given cell positions [19]:\n\nLw = Lw(x) + Lw(y)\n\u03a3\u03c5\u03bc) \u03b5\u03b9\u03c2 \u03a7\u03bf\u03c1\u03bf\nLw(x) =\n\u03a3(0,4) \u0395\u03a1 \u0395\u03b3\u03c7\u03c5\n\u03a3(\u03c5,\u03ba) \u03b5\u03a1 \u03a7\u03c5\u03c1-\u03b3\u03c7\u03c5\n\u03a3(\u03c5,\u03ba) \u03b5\u03b5-\u03b3\u03c7\u03bf),\n\nwhere xu denotes the x-coordinate of cell v and y is a hyperparameter. Lw (y) is computed in a similar manner.\nThe unique solution of the electrostatic system is derived from [22]:\n\n{\n\u2207\u2207\u03c8(x, y) = -p(x, y)\n\u00b7 \u03c8(x, y) = 0, (x, y) \u2208 \u018fR\n\u222b\u222br p(x, y) = \u222b\u222bp 4(x, y) = 0\nLD = \u2211 Ni(v) = \u2211 qi\u03c8(v),\niev\niev\n\nwhere p(x, y) denotes the cell density in position (x, y) and (x, y) is the potential in position (x, y) [22].\nOverall, the fine-tuning loss is given by Eq. (31), where Ap weighs two loss terms. The fine-tuning parameters are updated after backward propagation and cell position updates, according to the rules given below.\n\n\u03bb\u03b9 = \u03bb\u03b3 * \u03bc\n\u03bc =\n{\n1.05*max (0.999epochs,0.98) \u2206HPWL<0\n-AHPWL\n1.05*1.05 350000 AHPWL\u22650\n\nAHPWL = HPWLnew - HPW Lold\nLfine-tune = Lw + ADLD.\n\nHere, HPWL is the half-perimeter wire length [31]. HPWLnew and HPWLold represent the HPWL before and after this optimization step, respectively."}, {"title": "3 EXPERIMENTS", "content": "This section comprehensively evaluates TransPlace and answers two key research questions (RQs):\n(1) Can TransPlace transfer placement knowledge and enhance diverse unseen circuit designs?\n(2) Can TransPlace improve multiple design objectives simultaneously [4, 59], even when primarily trained with specific focuses like congestion?"}, {"title": "3.1 Knowledge Transfer across Diverse Designs", "content": "For the first RQ, our evaluations involve three benchmarks containing 37 circuits with diverse functionalities. We pick 5 large-scale circuits as training datasets to ensure sufficient transferable placement knowledge. To compare our model with other global placers, we use overflow and routed wirelength, measured after global routing, as our evaluation metrics. Although traditional placer usually takes half perimeter wirelength (HPWL) as an evaluation metric, it does not consider detours in the path, and wire congestion [2] may still cause routing failure even for solutions with better HPWL. Therefore, we evaluate placements after global routing to provide an accurate evaluation of routability quality. We conduct the following steps to compute overflow. We first divide the entire layout into grids, each with a predefined wire capacity denoted as RC. This limit represents the maximum number of wires allowed in each grid cell. Overflow OF(i, j) occurs when the number of wires exceeds this limit in the grid cell (i, j). Total overflow can be defined as follows:\n\nTOF = \u2211 OF(i, j).\ni, j\n\nWe evaluate TransPlace against two classical placers: (1) NTUPlace [20], an analytical placer for mixed-size circuit designs, and (2) DREAMPlace [31], recognized for its efficient implementation of GPU acceleration in analytical placement. The global router NCTUgr [16] is used for the DAC2012 dataset [50], while FastRoute 4.0 [54] for the ISPD2015 and ISPD2019 datasets [9, 35]. Detailed placement is conducted using Abacus [49] and ABCDPlace [32] for final placement results. For NTUPlace, we set the number of maximum threads to 8. All the experiments are conducted using a single Nvidia RTX 3090 GPU and an Intel Platinum 8255C CPU.\nThe results on three datasets are shown in Table 1, Table 2, and Table 3. The evaluation encompasses the aggregate routed wirelength, the total overflow, and the total runtime including placement and fine-tuning steps which are short for RWL, OVFL, and RT. As shown by the results, compared to the state-of-the-art placer DREAMPlace, TransPlace can place a new circuit about 1.2x faster, with a 30% reduction in Total Overflow and 2% reduction on Wirelength (averaged over circuits in ISPD2015 and ISPD2019). A lower total overflow not only implies a reduced likelihood of wire congestion, but also enhances the potential for successful routability and design convergence. Notably, despite our training dataset comprising only five circuits from DAC2012, TransPlace exhibits adaptability across a wide array of circuits from three distinct benchmarks. This adaptability is important for meeting the ever-evolving demands of chip design. It demonstrates the potential of TransPlace to improve EDA workflows in practical, high-stakes environments."}, {"title": "3.2 Cross-Objective Optimization", "content": "TransPlace is trained on placements generated by DREAMPlace with routability-driven methods. To answer the second RQ, this section evaluates its performance on the ICCAD2015 dataset [24], which focuses on incremental timing-driven placement [29, 34]. This placement approach optimizes the positions of circuit elements to minimize interconnect delay in timing-critical paths. The metrics used for the evaluation are total negative slack (TNS), worst negative slack (WNS), and number of violation paths (NVP).\nThe calculation of TNS, WNS, and NVP is based on the timing graph, a directed acyclic graph defined by the circuits. In this graph, each node represents a pin in the circuit, and each edge indicates a directed pin-to-pin connection. We travel all nodes connected by edges in the graph, beginning at the source cell node. For each node, we calculate its actual arrival time; the difference between it and the required arrival time is termed \"slack\" [51]. The maximum slack across all nodes is known as the Worst Negative Slack (WNS), while the Total Negative Slack (TNS) is the sum of the slacks at all timing endpoints. A path with negative slack at any of its nodes is referred to as a violation path, and NVP denotes the total number of such paths.\nWe compare TransPlace with DREAMPlace 4.0 [29], DREAMPlace with timing-driven methods. In our Circuit-adaptive Finetuning approach, similar to DREAMPlace 4.0, we employ OpenTimer for evaluating TNS and WNS and adopt a momentum-based method for updating net weights used in the wirelength calculation. For the assessment of our timing metrics, which include TNS, WNS, and NVP, we utilize Cadence Innovus. Detailed placement is conducted using Abacus [49] and ABCDPlace [32] for final placement results."}, {"title": "4 RELATED WORK", "content": "Global Placement. Global placement methods have been extensively developed since the 1960s. The proposed methods are broadly classified into four categories: meta-heuristics [1, 30, 38, 57], partitionbased methods [8], RL-based methods [5, 6, 10, 27, 43, 48], and analytical methods [11-13, 22, 31]. Meta-heuristics treat placement as a step-wise optimization problem and solve it with heuristic algorithms such as Simulated Annealing and Genetic Algorithm. Partition-based methods iteratively divide the chip's netlist and layout into smaller sub-netlists and sub-layouts, based on the cost function of the cut edges. Optimization methods are used to find solutions when the netlist and layout are sufficiently small. RLbased methods regard the placement problem as a Markov Decision Process (MDP) and train a policy to sequentially place the cells. All of them suffer from low convergence rates, which restricts their usage only to the circuits with a small number of large-sized cells."}, {"title": "5 CONCLUSION", "content": "This work introduces TransPlace, the first learning-based approach for global placement in integrated circuit design. We develop a series of techniques to address the technical challenges of learning large-scale cell placement. We demonstrate that TransPlace can effectively transfer versatile placement knowledge through evaluations on four benchmarks that feature diverse circuits and design objectives.\nTransPlace shows promise in improving integrated circuit design by learning parameterized knowledge, which results in higherquality circuits and accelerates design cycles. TransPlace can serve as an initial setup for placers, streamlining the optimization process for customized objectives and providing warm starts for optimization. Additionally, for designs requiring multiple logic optimizations, TransPlace can efficiently provide placement solutions, enabling early evaluation and issue resolution, thereby speeding up design convergence."}, {"title": "6 ACKNOWLEDGMENTS", "content": "This work was supported by the National Natural Science Foundation of China (Grant No. 62276006)."}, {"title": "A SUB-NETLIST GRAPH CONSTRUCTION", "content": "The construction of sub-netlist graph follows Algorithm 1.\nAlgorithm 1 Extracting sub-netlist graphs and coarsening original netlist graph.\n1: Input: netlist graph G = {V, U, P, XV, XU, XP, PT, PP}, partition sets {Vi}\n2: Initialize U\u2081 = 0, P\u2081 = 0 Vi = 1,..., n\n3: Initialize \u0192BELONG with {Vi}\n4: for all (v, u) \u2208 P with fBELONG (0) \u2260 0 do\n5: Add u into UfBELONG (0)\n6: Add (v, u) into PfBELONG (0)\n7: end for\n8: Initialize pseudo cell set VM = 0\n9: for all i \u2208 {1,..., n} do\n10: Initialize feature matrices:\n\nXv,i = Xv [Vi, :), XU,i = XU [U\u00a1,:], XP,i = XP [Pi,:] (33)\n11: Find the biggest cell v* \u2208 Vi\n12: Initialize terminal and movable:\n\nVT,i = {0*}, VP,i = 0, VM.i = Vi/v* (34)\n13: Initialize cell positions PT,\u2081 = [[0,0]], PP,i = []\n14: Construct i-th sub-netlist graph:\n\nGi = {VT,i UVM,i, Ui, Pi, XV,i, XU,i, XP,i, PT,i, PP,i} (35)\n15: Add Pseudo Cell \u00f4\u00a1 into M\n16: end for\n17: Set VM = VM - Ui=1,\u2026,n Vi + \u0174M\n18: Set U\u2032 = {u \u2208 U|\u2203v \u2208 V'y, (v, u) \u0454 P}\n19: Initialize Pseudo Pin set:\n\nP = {(\u00f4i, u)|\u2203v \u2208 Vi, (v, u) \u0454\u0420\u043b\u0438\u2208U'} (36)\n20: Set P' = {(v, u) \u2208Plve VM} UP\n21: Calculate X, X, Xp according to Appendix C\n22: Root graph GR:\n\nGR = {VT UVpUV'M, U', P', X, X, Xp, PT, PP} (37)\n23: Output: sub-netlist graphs {Gi} and coarsened netlist graph GR"}, {"title": "B CELL-FLOW CONSTRUCTION", "content": "The construction of cell-flow follows Algorithm 2.\nAlgorithm 2 The algorithm of constructing cell-flow from a netlist graph.\n1: Input: netlist graph G = {V, U, P, XV, XU, XP, PT, PP}\n2: Initialize flow edges F = 0\n3: Initialize cell-incoming-edge function fINCOME \u2208 V \u2192 F\n4: Initialize edge-net function fNET \u2208 F \u2192 U\n5: Initialize V = VT U Vp and \u00db = U\n6: repeat\n7: Pop a cell v from V"}, {"title": "C FEATURIZATION OF PSEUDO CELLS AND PINS", "content": "Although most of X, X, Xp can be inherited from original netlist graph G, we need to generate features for pseudo cells VM and pseudo pins P. Here, we regard a pseudo cell \u00eei as normal cell with width and height (\u221a5S\u012f, \u221a5S\u012f), where Si = \u2211v;\u2208V; wjhj is the summation of cell areas inside i-th partition Vi. Pseudo pins are regarded as normal pins connected among nets and pseudo cells. Then we follow the cell/pin featurization in [56] to generate the features for pseudo cells and pins, and concatenate them to X, Xp."}, {"title": "D OPTIMALITY DEMONSTRATION", "content": "D.1 Optimality of Sub-netlist Graph Generation\nThe time complexity of Algorithm 1 is composed of three parts:\n(1) Initialization of all U\u00a1 and Pi takes O(|P|).\n(2) Constructing sub-netlist graphs takes \u2211\u00a1(|Vi|+|U\u2081|+|Pi|) = O([V] + n|U| + |P|), with overlap ratio \u03b7 = \u03a3\u00a1 |Ui|/|U|.\n(3) Note that we set fBELONG (\u00fbi) = 0, so we can identify ve V and v \u2208 Vi in O(1) time. u \u2208 U' can also be identified if we label the nets when calculating U'. Therefore, constructing root graph GR takes O(|V| + |U| + |P|) in total.\nTime complexity of sub-netlist graph generation is O(|V|+n|U|+[P]). With the help of placement prototype tool KaHyPar[45], it can approximate O(|V| + |U| + |P|) because the overlap among {U\u00a1i = 1,..., n} is minor (\u03b7 \u2192 1). Table 6 shows the overlap ratio \u03b7 in major datasets.\nD.2 Optimality of Cell-flow Generation\nAlgorithm 2 elaborates on how to generate cell-flow F from a netlist graph G. Besides the initialization steps which cost O([V] + [U]), the bottleneck of its time consumption is the loop execution. For every (v, u) \u2208 P and net u untraveled, we connect v to almost every neighbor through u and label u as a traveled net. Because each net is traveled at most once, the loop body executes for no more than \u03a3\u03c5\u03b5\u03c5 (v, u) \u2208 P| = |P| times. Every statement in the loop body takes O(1), so the time complexity of cell-flow generation is O(|V| + |U| + |P|). Also note that the number of cell-flow edges equals to the execution times of loop body, i.e. |F| = O(|P|)."}, {"title": "D.3 Optimality of SE(2)-invariant Encoding and Decoding", "content": "The time complexities of encoding and decoding are O(|F|) = O(|P|) and O(w|F|) = O(w|P|), respectively, where w is the average length of cell-paths. The concrete values of w in major datasets are listed in Table 6. For most datasets, w is small, so it can be regarded as a constant."}, {"title": "D.4 Time Complexity of Fine-tuning", "content": "In every epoch, the time complexities of calculating Wire-length, Density and Anchor objectives are O(|P|), O(|V|log|V|) [22], and O(|V|), respectively. So the whole fine-tuning has the complexity of O(tf(|P| + |V|log|V|))."}, {"title": "E TPGNN Training", "content": "TPGNN is trained to imitate preplaced circuits generated by DREAMPlace [31]. These circuits consist of netlist graphs G with absolute positions for movable cells PM. We encode these absolute positions into ground-truth relative positions, which serve as training labels.\nThe hidden layer dimensions of cell, net, and pin are set to Dy = 64, Du = 64, and Dp = 8, respectively. We set message passing layers L = 3 and loss weights \u03bb\u03b8 = 1.0, p = 8e \u2013 6, A = 1e - 2.\nWhen optimizing TPGNN with Adam Optimizer, we use learning rate At = 5e 5, learning rate decay AAt = 1e 3, weight decay nt = 5e - 4, and training epoch tt = 500. The training loss is determined by the difference between TPGNN outputs and ground truth relative positions:\n\nLtrain = Smooth-L1(p, \u00f4) + \u03bb\u03c1 \u00b7 Smooth-L1(A0, A\u00d4), (38)\nwhere Smooth-L1 is a robust L1 loss function, and de weighs two loss terms, which is set to 0.1. Our source code is available at https://github.com/sorarain/TransPlace."}, {"title": "E.2 Optimization Settings", "content": "We use default baseline settings. Both DREAMPlace and circuitadaptive fine-tuning use NAG Optimizer [22] for a fair comparison, adopting the y and Ap adjustment strategy from Lu et al. [22]. Routability optimization via cell inflation follows Lin et al. [33] with default hyperparameters. Timing optimization differs from DREAMPlace [29], evaluating timing metrics and updating net weights every 15 iterations after placement overflow reaches stoverflow = 0.5 or stiteration = 500. For DAC2012, ISPD2015, and ISPD2019, the rudy map guides cell inflation. Our fine-tuning parameters are given in Table 7, Table 8, Table 9, and Table 4; \"Iteration\" is the maximum iteration limit, a variable setting up an early stop strategy [22, 31]."}, {"title": "G ABLATION STUDY", "content": "Here we present the ablation study of TransPlace in Tab 11, where \"-\" denotes the placement results fail to route. From the result, we can infer that fine-tuning is crucial to guarantee reasonable placement, and inductive placement can further improve placement quality."}]}