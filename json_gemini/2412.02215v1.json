{"title": "Recovering implicit physics model under real-world constraints", "authors": ["Ayan Banerjee", "Sandeep K.S. Gupta"], "abstract": "Recovering a physics-driven model, i.e. a governing set\nof equations of the underlying dynamical systems, from the real-\nworld data has been of recent interest. Most existing methods ei-\nther operate on simulation data with unrealistically high sampling\nrates or require explicit measurements of all system variables, which\nis not amenable in real-world deployments. Moreover, they assume\nthe timestamps of external perturbations to the physical system are\nknown a priori, without uncertainty, implicitly discounting any sen-\nsor time-synchronization or human reporting errors. In this paper, we\npropose a novel liquid time constant neural network (LTC-NN) based\narchitecture to recover underlying model of physical dynamics from\nreal-world data. The automatic differentiation property of LTC-NN\nnodes overcomes problems associated with low sampling rates, the\ninput dependent time constant in the forward pass of the hidden layer\nof LTC-NN nodes creates a massive search space of implicit physical\ndynamics, the physics model solver based data reconstruction loss\nguides the search for the correct set of implicit dynamics, and the use\nof the dropout regularization in the dense layer ensures extraction of\nthe sparsest model. Further, to account for the perturbation timing\nerror, we utilize dense layer nodes to search through input shifts that\nresults in the lowest reconstruction loss. Experiments on four bench-\nmark dynamical systems, three with simulation data and one with the\nin recovering implicit physics model coefficients than the state-of-\nreal-world data show that the LTC-NN architecture is more accurate\nthe-art sparse model recovery approaches. We also introduce four\nadditional case studies (total eight) on real-life medical examples in\nsimulation and with real-world clinical data to show effectiveness of\nour approach in recovering underlying model in practice.", "sections": [{"title": "1 Introduction", "content": "Model recovery problem concerns with deriving underlying physics\ndriven governing equations of a system from data [32]. Different\nfrom model learning, the model recovery has two goals: a) to accu-\nrately reconstruct the data, and b) derive the fewest terms required to\nrepresent the underlying nonlinear dynamics. As such sparse iden-\ntification of nonlinear dynamics is needed [24, 22]. It is useful for\nseveral applications such as learning digital twins [37], safety anal-\nsis [6], anomaly detection [28], explainable artificial intelligence\n(AI) [21] and prediction [33]. Two broad categories of techniques ex-\nist (Table 1): a) using physics driven deep learning on large datasets,\nand b) using transformations that linearize the nonlinear dynamics\nusing the Koopman theory [25] and then performing linear regres-\nsion with sparsity constraints. It is generally acknowledged that both\ncategories of techniques suffer significant performance degradation\non data from real-world systems [31]. Some effort has been under-\ngoing to improve performance of both categories of approaches on\nsystems with limited data and noise [12, 14], however, such problems\nare only a small subset of issues observed in real-world deployments.\nThis paper focuses on the model recovery problem that arises in\nreal-world systems which may include interactions with the human\nusers at runtime. As such we consider a control affine system whose\nn dimensional state space $X = {x_1... x_n} \\in \\mathbb{R}^n$ is given by:\n\n$\\dot{X} = f(X, \\Theta) + g(X, \\Theta)(U+ U_{ex}),$ (1)\n\nwhere $f(X, \\Theta) : \\mathbb{R}^n \\times \\mathbb{R}^p \\to \\mathbb{R}^n$ is a model of the natural un-\nperturbed dynamics of the physical system with human users in it\nthat is perturbed by: a) a autonomous system $U = K(X)$, where\n$K: \\mathbb{R} \\to \\mathbb{R}^m$ is a control function that generates the m dimen-\nsional actuation signals to the physical system and b) input from\nthe human user $U_{ex} \\in \\mathbb{R}^m$. We denote the total perturbation as\n$U_T = U + U_{ex}$. $g(X, \\Theta) : \\mathbb{R}^n \\times \\mathbb{R}^p \\to \\mathbb{R} \\times \\mathbb{R}^m$, expresses\nthe effect of the input perturbation. $\\Theta$ is the set of model coefficients\nof the dynamical system.\nReal-world observations are limited by additional constraints (Ci).\nC1: Low sampling rate - Storage and energy constraints in real-\nworld deployments often limit sensing frequency [4]. As per the\nNyquist Shannon sampling theory [35], full information about the\nmodel coefficients is available in observation X if it is sampled\nat the Nyquist rate i.e., the sensing frequency is twice the maximum\nfrequency in the power spectrum of X obtained from Eqn.1. The\ncontrol input U has the same sampling frequency as X. However, in\npractice, the sensor data is logged at a sub-Nyquist rate.\nDifficulty in model recovery with C1: A low sampling frequency\ncauses performance degradation of model recovery [10]. As samples\ngrow further apart in time, the set of nonlinear functions that accu-\nrately fit the samples becomes larger making it increasingly difficult\nto find the correct underlying model in the expanded search space.\nC2: Perturbed system - Since in deployment the system operation\ncannot be disrupted, the traces X are always from a perturbed system\nfollowing Eqn1 and there is no trace obtained from the unperturbed\npart of the system. Plus, there can be, $q > 0$, external human inputs\n$U_{ex}$ at times $t_1 ...t_q$, $t_i \\in \\mathbb{R}^{\\geq 0}, \\forall i$ that are sparse in time and are\ndescribed by a set of tuples $E_x = {(\\overline{U}_{ex}, t_1), ... (\\overline{U}_{ex}, t_q)}$, Model\nrecovery from perturbed systems with control inputs has been con-\nsidered in the literature, however, to the best we know, sparse external\nhuman inputs have been ignored.\nDifficulty in model recovery with C2: While input perturbations due\nto the control logic are Lipschitz continuous, the external human\ninputs are discontinuous that introduce transients in the dynamical\nsystem. Approaches such as least square minimization based model\nrecovery [24] that ignores human inputs may erroneously introduce\nhigher order nonlinear terms in the model due to such transients."}, {"title": "1.1 Approach and contributions", "content": "Our main contribution is the extension of liquid time constant neu-\nral network (LTC-NN) to advanced neural structure (LTC-NN-MR)\nfor recovering physics model under real-world constraints (Fig. 1).\nIntuition for addressing C1: The automatic differentiation property\nof LTC-NN class of neural architectures [3], which also includes con-\ntinuous time recurrent neural networks (CT-RNN) or neural ordinary\ndifferential equations (NODE), can solve a system of ODEs with ar-"}, {"title": "3 Theoretical foundations", "content": "The forward pass of LTC-NN node is given by [19]:\n\n$\\dot{h}(t) = -h(t)/\\rho+f_{NN}(h(t), I(t), t, w) (A - h(t)),$ (2)\n\nwhere h(t) is one hidden state of the LTC-NN, $\\rho$ is a time constant\nparameter, required to assist any autonomous system to reach equi-\nlibrium state. As such existence of the $-h(t)/\\rho$ is an important sta-\nbility criteria as it ensures that the unperturbed physical system set-\ntles in time. $f_{NN}$ is the computation function of each node and is a\nfunction of the hidden states, I(t) is the input to the LTC-NN, w and\nA are architecture parameters.\nRemark 1. The forward pass of an LTC-NN architecture generates\na set of implicit physical dynamics that are equivalent to a bi-linear\napproximations of the control affine autonomous system in Eqn.1.\nSupporting argument: Algebraic manipulation of the forward pass\nof LTC-NN architecture gives the structure of Eqn.3 which allows an\ninput dependent time constant $1+\\rho f_{NN}(h(t),I(t),t,w)$.\n\n$\\dot{h}(t) = -h(t)/(1+\\rho f_{NN}(h(t), I(t), t, w)) + f(h(t), I(t), t,w)(A).$ (3)\n\nThe stability criteria for any autonomous system requires the con-\ntrol affine model to have a time constant term as shown in Eqn. 4\n\n$\\dot{X} = -X/\\rho + f_{-\\rho}(X) + g(X)U_T,$ (4)\n\nwhere $\\rho$ is the time constant of the system and $f_{-\\rho}(.)$ is the unper-\nturbed dynamics after removing the time constant component.\nAssuming that the autonomous system is a dynamic causal system,\nthe bi-linear approximation [15] of the control affine system in Eqn.\n4 results in Eqn. 5.\n\n$\\dot{X} \\approx -X/\\rho + f_{-\\rho}(X) + BX + CU_T + \\Sigma_j uD^jX + H,$ (5)\n\nwhere $B = \\frac{\\partial(g(X)U_T)}{\\partial X}$, $C = \\frac{\\partial(g(X)U_T)}{\\partial U_T}$, and $D^j = \\frac{\\partial^2(g(X)U_T)}{\\partial X \\partial U_T}$\nH is a constant. Rearranging Eqn. 5, we have the similar form as the\nLTC-NN forward pass in Eqn. 6.\n\n$\\dot{X} \\approx -X/(\\rho(1+ \\frac{1}{\\rho}(B + \\Sigma_j u D^j))) + (f_{-\\rho}(X) + CU_T + H).$ (6)\n\nWe observe that Eqn. 6 is the same form as Eqn. 3 if the input to the\nLTC-NN I(t) is a concatenation of Y and $U_T$. The hidden layers of\nthe LTC-NN model an inflated set of implicit dynamics which may\ninclude the unmeasured system variables of the physics model.\nRemark 2. The inflated set of implicit dynamics modeled by LTC-\nNN induces an over-determined set of equations in the coefficients of\nthe bi-linear approximation of control affine model."}, {"title": "3.1 Addressing C1: Sampled Data", "content": "Architectures that enable automatic differentiation such as CT-\nRNN [20] or NODE [11] or LTC-NN [19] are shown to be capa-\nble of simulating arbitrary sampling rates between two time samples\nof the real data through usage of state-of-art ODE solvers. Hence,\nsuch architectures are at a relative advantage over sparse identifica-\ntion mechanisms such as SINDy. In our examples we show that de-\ncreasing sampling rate up to the minimum required Nyquist rate [35]\nreduces accuracy of model recovery for SINDy class of approaches\nbut has little to no effect on CT-RNN, NODE or LTC."}, {"title": "3.2 Addressing C2: Perturbed system", "content": "The LTC-NN forward pass has an input dependent time constant.\nThis helps in modeling the perturbed system as demonstrated by the\nequivalence of Eqn. 3 and 6. This is not the case for other neural\narchitecture with automatic differentiation such as CT-RNN [20] or\nNODE [11] (Eqn. 7). They do not have a direct mapping to the bi-\nlinear approximation of the control affine system, as is evident by the\nfollowing equations:\n\n$\\dot{h}(t) = \\dot{h}(t) + f_{CR}(h, I, t,w_{CR}),$ (7)\n\n$\\dot{h}(t) = f_{CT}(h, I, t, w_{CT}).$"}, {"title": "3.3 Addressing C3: Sparsity Preservation", "content": "Sparsity preservation is achieved through the training loss function\nand dropouts in dense layer. PINNs [12] and NODE [11] achieve\nsparsity by integrating a \"physics loss\". It uses the original model\ncoefficients in an ODE solver to determine the ground truth Y\nto compute the RMSE of the estimated $Y_{est}$. While this approach is\nappropriate in simulation settings, it is not practical since the original\nmodel coefficient is not available in the real world. The presented\napproach uses ODE loss which uses the estimated model coefficients\n$\\Theta_{est}$ and initial value of measured variables Y(0) to compute the\nk-1 samples of $Y_{est}$ at times {$\\tau, 2\\tau... k\\tau$}. For this purpose, it uses\na state-of-art ODE solver that implements the control affine structure\nof the autonomous system. It computes RMSE with the real data Y\nas loss. Hence, we do not use $\\Theta$ in the training of LTC-NN-MR."}, {"title": "3.4 Addressing C4: Implicit Dynamics", "content": "Remark 1 shows that LTC-NN forward pass can provide an inflated\nset of implicit dynamics. This is also true for CT-RNN, and NODE.\nLTC-NN has the advantage over all other techniques in that it can\nmodel implicit dynamics in presence of external inputs."}, {"title": "3.5 Addressing C5: Input uncertainty", "content": "Input uncertainty is of two types: a) magnitude, and b) temporal. The\nmagnitude uncertainty is inherently handled in neural architectures\nthrough the weight update process. To tackle timing uncertainty, a\nsubset A: |A| = q of the dense layer outputs are transformed to the\nrange [0, 1] using a Sigmoid activation function. Each $d_i \\in A$ acts\na shift operation for the input $u_{ex} \\in U_{ex}$. Each input $u_{ex}$ is shifted\nby the amount $d_i \\times k$ and is used in the input layer as well as the\nODE Solver in the loss function for each forward pass of the neural\nstructure. The dense layer is used to search for the set of possibilities\nof input shifts due to temporal uncertainty."}, {"title": "4 Implementation", "content": "The advanced neural architectures for model recovery ($\\xi$-MR, where\n$\\xi$ is either LTC-NN, CT-RNN, or NODE) (Fig. 2) is implemented\nby extending the base code available in [18]. For each example, we\nextract the training data consisting of temporal traces of Y, U and\n$E_x$. Y is sampled at least at the Nyquist rate for the application, and\nU has the same sampling rate as Y. $E_x$ is a set of tuples denoting\n$U_{ex}$ values at time $t_{ex}$. $E_x$ is transformed into a signal at the same\nsampling frequency as Y by keeping $U_{ex}$ at times $t_{ex}$ and appending\n0s at all other times. The resulting training data is then divided into\nbatches of size $S_B$ forming a 3 D tensor of size $S_B \\times (|Y|+m) \\times k$.\nEach batch is passed through the $\\xi$ network with V nodes, result-\ning in V hidden states. A dense layer is then employed to transform\nthis V hidden states into p = |$\\Theta$| model coefficient estimates and q\ninput shift values. The dense layer is a multi-layer perceptron with\nReLU activation function for the model coefficient estimate nodes\nand Sigmoid activation function for input shift values. The input shift\nvalues are used to shift the external input vector. The shifted inputs,\nthe model coefficient estimates, and the initial value Y (0) is passed\nthrough an ODE solver, that solves the control affine model in Eqn. 1\nwith the coefficients $\\Theta_{est}$, initial conditions Y(0) and inputs U and\n$U_{ex}$. The Runge Kutta integration method is used in the ODE solver,\nwhich gives $Y_{est}$. The backpropagation of the network is performed\nusing the network loss appended with ODE loss, which is the mean\nsquare error between the original trace Y and estimated trace $Y_{est}$."}, {"title": "5 Evaluation", "content": "Experiments are performed in Nvidia Titan V GPU, CUDA 11.4 and\nTensorFlow 2.7.0 [1]. Code and data available in [2]. We show two\ntypes of examples:\na) Five simulation benchmarks, three obtained from SINDYC [24]\nand the AID and EEG problems introduced in this paper. Simulation\nis used to test the variation of model recovery performance under\nvarious severity levels of the constraints.\nb) Three real-world data available for the example of Lotka Volterra\nsystem in the original SINDYc paper [24], the LOIS-P data for AID\nin pregnancy [29], and the EEG data for epilepsy [17]. This shows\npractical applicability of LTC-NN-MR in real-world data."}, {"title": "5.1 Benchmark Examples", "content": "Table 2 shows all examples (physical dynamics are in supplementary\ndocument [2]). In the examples from [24] marked with (B) in Table\n2, there are no external human inputs $U_{ex}$ apart from control inputs\n$\\u$. We arbitrarily varied the input timing to evaluate effect of input\nuncertainty constraint C5. For the AID example in simulation, we\nalter the meal timing. For the EEG example in simulation we chose\na deterministic sinusoidal input and then a stochastic Wiener process\ngenerated input to test for high levels of input uncertainties. In the\nreal world EEG and AID example uncertainty is inherent in the data."}, {"title": "5.1.1 Automated Insulin Delivery System", "content": "In the AID system, the glucose insulin dynamics is given by the\nBergman Minimal Model (BMM) [8]:\n\n$\\dot{i}(t) = -n i(t) + p_4 u_1(t), \\dot{i_s}(t) = -p_1 i_s(t) + p_2(i(t) - i_b)$ (8)\n\n$\\dot{G}(t)=-p_1 i_s(t)G_b - p_3(G(t)) + u_2(t)/Vol,$ (9)\n\nThe input vector U (t) consists of the input insulin level $u_1$ (t), which\nis derived using a self adaptive MPC controller like Tandem Control\nIQ [26]. $U_{ex}$ consists of the glucose appearance in the body $u_2$ for\na meal. In the real world, users forget to report the exact timing of\nthe meal and also make mistake in estimating consumed carbohy-\ndrate amount [26]. We model this error as input uncertainty in time\nand magnitude. The state vector X(t) comprises of the blood insulin\nlevel i, the interstitial insulin level $i_s$, and the BG level G. The mea-\nsured vector Y = G since CGM measures only glucose. $p_1, p_2, i_b$,\n$p_3, p_4, n$, and 1/Vol are all patient specific coefficients."}, {"title": "5.1.2 EEG example", "content": "The EEG represents brain waves and can be modeled using coupled\nsystem of Duffing-van der Pol oscillators [16] as in Eqn. 10.\n\n$\\ddot{x_1} + k_1 x_1 = k_2x_2-b_1x_1^3 - b_2(x_1-x_2)^3 + \\epsilon u_1(1 - \\dot{x_1})$ (10)\n$\\ddot{x_2} - k_2(x_1-x_2) = b_2(x_1-x_2)^3 + \\epsilon u_2(1 - \\dot{x_2}) + \\mu dW,$\n\nwhere $k_1, k_2, b_1,b_2, \\epsilon_1$ and $\\epsilon_2$ are patient specific parameters, $x_2$\nis the EEG signal, $x_1$ is an internal unmeasured state variable, u is\nthe magnitude of activation, $dW$ is a random variable following the\nWiener process [16]. Here the input $u = \\mu dW$ is a stochastic process\nwith significant temporal and magnitude uncertainty."}, {"title": "5.1.3 Real-World Data", "content": "Three examples of real-world data are shown in this paper:\nA) Lotka Voltera system uses yearly lynx and hare pelts data col-\nlected from Hudson Bay Company [24]."}, {"title": "5.3 Evaluation experiments and metrics of success", "content": "For each evaluation experiment, we use two metrics:\nRoot mean square error in model coefficients ($RMSE_\\Theta$): Given\nthe estimated model coefficients $\\Theta_{est}$ we compute:\n\n$RMSE_\\Theta = \\sqrt{1/p\\times \\Sigma_{j=1...p} (\\Theta_{est} - \\Theta)^2(j)}.$ (11)\n\nRoot mean square error in signal ($RMSE_Y$): Given the estima-\ntion of the measured variables $Y_{est}$ for any technique we compute:\n\n$RMSE_Y = \\sqrt{1/n \\Sigma_{l=1...n} 1/k \\Sigma_{j=1...k} (Y_{est(j)} - Y_l (j))^2}.$ (12)\n\nAll compared techniques identify sparsity preserving dynamics. We\nstart with a configuration $\\Phi_o$ that has high sampling rates shown in\ncolumn 7 of Table 2, has no implicit dynamics, has input perturba-\ntion, only uncertainty in input magnitude (no temporal uncertainty).\nWe conduct the following evaluation experiments:\nEffect of sampling rate (C1) on (B) examples: We vary the sampling\nrate of $\\Phi_o$ from the rate used in the simulation data to the Nyquist\nrate (Table 2) and analyze the variation of $RMSE_\\Theta$ and $RMSE_Y$.\nThe configuration with Nyquist sampling rate is denoted as $\\Phi_N$.\nEffect of perturbation (C2) on (B) examples: From $\\Phi_N$, we create\nconfiguration $\\Phi_{NI}$ by removing the input perturbation from model"}, {"title": "CT-RNN-MR", "content": "CT-RNN-MR and LTC-NN-MR performance for no input perturba-\ntion. SINDYC also had performance improvement when input pertur-\nbation was removed.\nEffect of implicit dynamics (C3 + C4): We updated the loss func-\ntion of each architecture as follows:\n\n$Loss = 1/n \\Sigma_{i=1...n} 1/k \\Sigma_{j=1...k} |x_i (k) - x_{est} (k)|^2,$ (13)\n\nwhere $x_{est} \\in X_{est} = SOLVE(X(0), \\Theta_{est})$. Table 3 shows that\nalthough there is some improvement in both the parameters, it was\nexpected that $RMSE_\\Theta$ should significantly improve by providing\nmeasurements of implicit dynamics. However, we do not see such\nimprovements. A possible explanation for this is that since all the\nbaseline examples are observable systems, the implicit dynamics\ncould be derived in terms of the measured system variables. The con-\njecture is that the neural architectures are capable of modeling the\nimplicit dynamics in terms of the measured variables. This should be\nfurther investigated in future works."}, {"title": "5.5.2 Automated Insulin Delivery Example", "content": "Simulation Examples: In Table 5, SINDYc did not have any tem-\nporal uncertainty for meal inputs. The neural architectures had tem-\nporal uncertainty at meal inputs and also used input shifts in the ar-\nchitecture. However, for the last column, the input shift from neural"}, {"title": "6 Conclusions", "content": "This paper provided a liquid time constant neural network based so-\nlution to the problem of recovering coefficients of a physics model\nfrom real-world data from a dynamical system. It identified five key\nchallenges in real-world deployments and provided solutions to mit-\nigate them effectively. The practical challenge of information loss\ndue to low sampling rates (C1) was overcome utilizing the automatic\ndifferentiation property of LTC-NN, where each node can reproduce\nsamples with arbitrary precision in between sampling time stamps.\nLTC-NN-MR handles transients introduced due to discontinuous in-\nputs (C2) by decoupling the input effects from the unperturbed dy-\nnamical system through its control affine forward pass dynamics.\nTo derive the sparsest solution (C3), LTC-NN-MR combines ODE\nsolver based reconstruction loss with dense layer dropout and hence\ndoes not need specific knowledge about the underlying equations\nsuch as sparsity level as in SINDYC or ground truth model coeffi-\ncients as in PINNs. To the best of our knowledge, LTC-NN-MR is\nthe only technique to recover model coefficients in presence of im-\nplicit dynamics (C4) unlike the weak implicit notions in SINDYC\nextensions. Further, LTC-NN-MR is the only technique to be able\nto recover an accurate model in the presence of input timing uncer-\ntainties making it amenable to be used in safety-critical real-world\nhuman-in-the-loop dynamical systems."}]}