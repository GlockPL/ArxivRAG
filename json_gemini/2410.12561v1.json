{"title": "DEVELOPMENT OF IMAGE COLLECTION METHOD USING YOLO\nAND SIAMESE NETWORK", "authors": ["Chan Young Shin1", "Ah hyun Lee2", "Jun Young Lee3", "Ji Min Lee4", "Soo Jin Park5"], "abstract": "As we enter the era of big data, collecting high-quality data is very important. However, collecting\ndata by humans is not only very time-consuming but also expensive. Therefore, many scientists have\ndevised various methods to collect data using computers. Among them, there is a method called\nweb crawling, but the authors found that the crawling method has a problem in that unintended\ndata is collected along with the user. The authors found that this can be filtered using the object\nrecognition model YOLOv10. However, there are cases where data that is not properly filtered\nremains. Here, image reclassification was performed by additionally utilizing the distance output\nfrom the Siamese network, and higher performance was recorded than other classification models.\n(average_f1 score YOLO+MobileNet 0.678->YOLO+SiameseNet 0.772)) The user can specify\na distance threshold to adjust the balance between data deficiency and noise-robustness. The au-\nthors also found that the Siamese network can achieve higher performance with fewer resources\nbecause the cropped images are used for object recognition when processing images in the Siamese\nnetwork. (Class 20 mean-based f1 score, non-crop+Siamese(MobileNetV3-Small) 80.94 -> crop\npreprocessing+Siamese(MobileNetV3-Small) 82.31) In this way, the image retrieval system that\nutilizes two consecutive models to reduce errors can save users' time and effort, and build better\nquality data faster and with fewer resources than before.", "sections": [{"title": "I.INTRODUCTION", "content": "In the era of big data, refined image collection for AI learning is necessary. There are Google Data Search, Kaggle,\nAwesome Public Datasets Github, Data and Story Library, etc., and the formats are txt, csv, jpg, API calls, etc.\nThe disadvantage of web-crawled data is that there is a high possibility that there is inherent noise data in the image that\ndoes not match the label. This can be a factor that causes performance degradation when using web-crawled data for\nlearning a vision AI model.\nIn Fig. 1.(a), the Training Accuracy is similar when comparing 'Noisy w/o. Reg.' and 'Noisy w. Reg.'. However, the\nTest Accuracy is clearly higher in 'Noisy w/o. Reg.' than in 'Noisy w. Reg.'. In (b), the classification model initially\ncorrectly classified the panda image as panda, but when an adversarial attack that adds noise is performed, it is classified\ninto a different class with high confidence."}, {"title": "II. RELATED WORK", "content": "A. Image matching\nPreviously, the Siamese network was used for facial image verification, which matches a given image with a local image.\nYatharth V. Kale et al.[6] used it for facial recognition using object detection. Here, a cropped human image with the\nYOLO model is passed through the Inception model, which is trained with human images in the internal database, to\ndetermine who it is. The Siamese network structure is used to compare each feature of the given cropped image and\nthe database image with the Inception model. Through this, the similarity between the user and several people in the\ndatabase is calculated, and the person with the lowest similarity is output.\nIvan Bakhshayeshi et al. [7] used the image matching of the Siamese network for deep learning facial recognition\nfor cattle reidentification. To complement the various problems (vulnerable to loss, failure, and misidentification or\nimproper substitution) that exist in radio frequency identification (RFID) ear tags used for livestock recognition and\nmanagement, we proposed a cattle recognition system that combines the You Only Look Once version 5 (YOLOv5)\nalgorithm for cattle face detection and the Siamese neural network (SNN) for subsequent recognition.\nIn this case, the part where the image given by the YOLO model is cropped and the process of selecting it through the\nSiamese network based on the classification model are similar to our framework, but we did not limit the application of\nthe Siamese network model to a specific object but applied it to various image objects. In addition, we developed an\nimage retrieval application that can control the strictness of noise-robustness by allowing the user to set the distance\nthreshold that determines how far the distance from the Siamese network will be judged as a keyword.\nB. Image Retrieval Application\nAfter that, there were attempts to use the Siamese network for real-time object tracking, image retrieval, and whole image\nmatching. Iaroslav Melekhov et al. [8] compared the existing image classification baseline models with sHybridNet\nthat applied Hybrid CNN to the Siamese network and showed performance improvement, and presented a method of\napplying the Siamese network to generic image matching. They suggested that an image matching method using a\nSiamese network like this is likely to be used in image retrieval applications.\nThere have been many cases of data retrieval applications using web crawling[9]. There are cases of using web\ncrawling for web archiving[10], detecting near-duplicates[11], or weakly supervised learning. Furthermore, they built\nan imageDB using web crawling.\nHwang et al. [12] developed a system that updates the system by transfer learning with appropriate train data created by\nputting the refined image received through the image crawler through an object detector (image annotator) using YOLO\nand the label set that classified it into a trainer server.\nJeongbin Hwang et al.[13] collected images related to construction monitoring, auto-labeled them using a semantic\nsegmentation model, and then put them into imageDB for learning."}, {"title": "III. METHOD", "content": "Web framework\nThe object detection model uses YOLOv10[14]. YOLO is a 1-stage model that can detect objects in real time by\nperforming classification and localization simultaneously. The dataset used to train the model is the PASCAL VOC\ndataset 2012 and 2007[15]. The dataset contains 27,652 images (training: 16,551; validation: 4,952) of annotations\nin 20 classes. The dataset was trained with the following parameters: batch size 64, epochs of 100, and weights of\nyolov10.pt (pre-trained weight)."}, {"title": "RESULTS AND DISCUSSION", "content": "(1)YOLO+Siamese Network\nUsing the trained siamese network, we can obtain the Euclidean distance between the anchor image for each class of\nthe VOC dataset and 100 randomly crawled images.\nIn Figure.3, for 100 crawled test sets, (a) YOLO, (b) MobileNet, and (c) a Siamese network using MobileNet as a\nbackbone show the distance histogram for positive and negative pairs of each model, with a purple distribution when\nclassified as a keyword and an orange distribution when classified as others. (a) YOLO, (b) MobileNet classify as\nkeywords if the classified label is correct, and as others if it is another label. (c) The Siamese network finds the distance\nthreshold that maximizes the f1-score and classifies it as a keyword if it is smaller than this and as others if it is larger.\nHere, we can see that the distance cohesion for the keywords classified by the model is higher when MobileNet is\napplied to the Siamese network than when YOLO and MobileNet are applied to classification. The Euclidean distance\nbetween the images classified by keywords and the correct anchor image is gathered, which means that images similar\nto the anchor image are distributed small and different images are distributed large.\nThis is an additional model performance evaluation, focusing on various aspects of the classification task.\nFig. 3 compares the performance of three models (YOLO, YOLO+MobileNet, and Siamese Network with\nYOLO+MobileNet as a backbone). The average f1 score is a balanced index that averages the f1n_score, which\nreflects the data imbalance with less negative data and the original f1_score. (a) is a bar graph comparing the F1 scores\nfor each class for the three models. The F1 scores for each class are shown in blue, YOLO in orange, and Siamese\nNetwork in green. From the graph, we can see that Siamese Network recorded the highest F1 scores in many classes.\nThis point emphasizes that the Siamese network performs better in similarity-based tasks between images, showing that\nsimilar images are grouped closer together and dissimilar images are more dispersed.\n(b) is a heatmap showing the average F1 score for each class, where darker colors indicate higher F1 scores and lighter\ncolors indicate lower scores. The heatmap shows that the combined YOLO and Siamese Network method achieved\nhigher F1 scores in most classes, showing better classification performance than other models. This result suggests\nthat the Siamese Network clustered more homogeneous images in the Euclidean distance-based classification between\nanchor images and other images after YOLO classification.\nIt can be seen that Siamese Network classifies similar and dissimilar images well and is more effective than simply\nreclassifying with YOLO or MobileNet after YOLO.\n2) Effect of Crop pre-processing and backbone model on Siamese Network performance"}, {"title": "VI. CONCLUSION", "content": "The authors of this paper designed a web crawling framework using YOLO for data crawling. In addition, the authors\nfound that using a Siamese network can improve performance compared to using only YOLO or YOLO + MobileNet.\nIn addition, using YOLO, it is possible to obtain various types of data that contain pictures of specific objects or specific\nobjects. As a result, this framework can obtain a lot of high-quality data that suits the user's purpose in a short period\nof time compared to existing web crawling methods. In addition, the authors found through experiments that when\ncropped data from a Siamese network is used, it can show performance equivalent to or higher than that of a large\nmodel trained on relatively small-scale, uncropped original data. If this fact is applied, we can expect performance\nimprovements in various fields that are currently using the Siamese network."}, {"title": "Image similarity metric", "content": "The f1_N score, which modifies the F1 score by changing the evaluation criteria of the\nmodel, serves to supplement the F1 score. We use the f1_N score to solve the data imbalance problem. The crawled\ndata is data obtained as a result of a keyword search on Google Images. Therefore, most of the images are composed\nof positive examples. When positive data overwhelms negative data, the skew rate, which is the ratio of negative to\npositive data, approaches 0. If the skew rate is too close to 0, the F1 score will be biased toward 1 regardless of the error\nrate. (Refer to Figure 4, where the error rates of 1% and 20% recorded F1 scores of 1 and 0.9, respectively.) This makes\nit difficult to distinguish the difference in F1 scores according to the error rate of the classifier.\nIn model labeling, we change the positive and negative (TP -> TN, FP -> FN) to make the skew rate greater than 1. In\nthe f1_N score metric, precision and recall are changed to negative predictive value (NPV) and specificity, respectively.\nHowever, if the skew rate in the f1_N score is too large, the ratio may be excessively distributed, as shown in Figure 4.\nTherefore, we use the average_f1 score, which uses the f1 score and the f1_N score together to create a balanced metric.\n$Skew = \\frac{nagative examples}{positive examples}$\n$Average_f1 score = \\frac{F1 score + F1\\_N score}{2}$\n$F1\\_N score = 2 \u00d7 \\frac{NPV \u00d7 specificity}{NPV + specificity}$\n$F1 socre = 2x \\frac{precision \u00d7 recall}{precision + recall}$"}]}