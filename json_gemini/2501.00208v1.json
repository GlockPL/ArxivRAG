{"title": "An Empirical Evaluation of Large Language Models on Consumer Health Questions", "authors": ["Moaiz Abrar", "Yusuf Sermet", "Ibrahim Demir"], "abstract": "This study evaluates the performance of several Large Language Models (LLMs) on\nMedRedQA, a dataset of consumer-based medical questions and answers by verified experts\nextracted from the AskDocs subreddit. While LLMs have shown proficiency in clinical question\nanswering (QA) benchmarks, their effectiveness on real-world, consumer-based, medical\nquestions remains less understood. MedRedQA presents unique challenges, such as informal\nlanguage and the need for precise responses suited to non-specialist queries. To assess model\nperformance, responses were generated using five LLMs: GPT-40 mini, Llama 3.1: 70B,\nMistral-123B, Mistral-7B, and Gemini-Flash. A cross-evaluation method was used, where each\nmodel evaluated its responses as well as those of others to minimize bias. The results indicated\nthat GPT-40 mini achieved the highest alignment with expert responses according to four out of\nthe five models' judges, while Mistral-7B scored lowest according to three out of five models'\njudges. This study highlights the potential and limitations of current LLMs for consumer health\nmedical question answering, indicating avenues for further development.", "sections": [{"title": "1. Introduction", "content": "In healthcare consultations, clinicians raise questions related to patient care but only find answers\nto half of those questions due to limited time or the belief that an answer may not exist (Del Fiol\net al., 2014). Medical question-answering (QA) systems have the potential to address these\nproblems by giving fast responses to clinicians' questions. These systems are designed to provide\naccurate and relevant answers to medical queries by leveraging natural language processing\ntechniques. Traditional systems in this domain typically utilize information retrieval techniques\nto draw responses from structured medical databases or relevant documents. These systems often\ninvolve classifying question type, such as Yes/No or factual questions (Sarrouti et al., 2015), and\nthen employ semantic matching and extraction methods to generate concise responses from the\nmatched documents.\nIncorporating artificial intelligence (AI) and knowledge graphs, including ontologies\n(Baydaroglu et al., 2023), into educational and healthcare domains provides innovative avenues\nto enhance communication and knowledge access. Al-driven systems use knowledge graphs to\norganize and interlink vast arrays of information, enabling both educators (Sajja et al., 2024a;\n2024b) and healthcare providers to access tailored, contextually relevant data (Chi et al., 2020).\nWhen integrated with chatbots, these systems can facilitate interactive and personalized learning\nexperiences in education, offering students immediate, accurate responses to their inquiries. In\nhealthcare, chatbots powered by AI and ontologies can assist in patient triage, symptom checking\n(Chi et al., 2023), and patient education, ensuring that users receive up-to-date medical\ninformation efficiently (Sermet and Demir, 2021). By leveraging these technologies, educational\nand healthcare chatbots can move beyond simple transactional interactions to deliver\nsophisticated, nuanced assistance that supports both learning and clinical decision-making\nprocesses (Pursnani et al., 2023).\nThe advent of Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) has\nintroduced transformative possibilities for many use cases in education (Sajja et al., 2023a;\n2023b), operational support (Samuel et al., 2024), and health care. These models, for example,\ncan address the significant documentation burden in EHRs by automating text summarization,\nallowing clinicians to review condensed, relevant summaries rather than lengthy clinical notes\n(Jain et al., 2024). Several studies have also been conducted to explore the potential of AI and\nLLMs in medical support and QA (Zhang et al., 2023, Banerjee et al., 2023; Wang et al., 2024).\nCurrent LLMs have been extensively evaluated on clinical QA benchmarks, such as MedQA\n(Chai et al., 2020) and PubMedQA (Jin et al., 2019), where questions are typically structured in a\nmultiple-choice format to assess clinical accuracy and factual recall. However, multiple-choice\nQA does not fully capture the complexity of real-world medical inquiries, as it limits responses\nto predefined options and restricts the model's ability to provide nuanced, explanatory answers.\nTo address these limitations, these datasets have transformed into open-ended question formats,\nallowing models to handle more elaborate responses that better reflect the complexities of\nclinical scenarios (Yang et al., 2024; Gopal et al., 2024). Additionally, LLMs have been\nevaluated on qualities beyond factual accuracy, such as safety, bias, and language understanding"}, {"title": "(Kanithi et al., 2024), to better align with the complexities encountered in real-world medical\ninteractions.", "content": "Open-ended clinical QA benchmarks, however, are focused on structured, professional\nqueries rather than consumer-based questions. Consumer queries typically lack specific medical\nterminology (Welivita & Pu, 2023), use informal language, and may pose open-ended inquiries\nwith limited or ambiguous detail. This difference from clinical-style questions presents a unique\nchallenge for LLMs, which must interpret and respond to questions in a way that accommodates\nthe informal, varied nature of consumer inquiries. Previous work in the consumer medical QA\ndomain includes the CHiQA system (Demner-Fushman et al., 2020), which focuses on reliable\ninformation retrieval from consumer-friendly sources such as MedlinePlus (MedlinePlus, n.d.) to\naddress common health questions. By using trusted patient-oriented sources, this system bridges\nthe gap between consumer queries and trustworthy medical content, though it still encounters\nchallenges in matching informal consumer language with precise medical information.\nAdditionally, research on improving consumer medical QA demonstrates the difficulty\nconsumers face when formulating specific questions that align with their informational needs\n(Nguyen, 2024). Work by Nguyen addresses this issue by proposing improved biomedical\nrepresentational learning and statistical keyword modeling. These improvements aid in retrieving\nmedical answers even when consumer questions are vague or contain informal language\n(Nguyen, 2024). Another study conducted experiments with a QA system to retrieve answers for\nreal consumer medication queries (Abacha et al., 2019). By curating a dataset containing genuine\nconsumer questions about medications with corresponding expert verified answers, researchers\nobserved that the QA system struggled with retrieving the correct answers. They also highlighted\nthe need for better contextual understanding in consumer medication QA.\nSeveral existing studies have also assessed LLM responses to questions posted on the\nAskDocs (Reddit, 2024) subreddit. For example, in (Ayers et al., 2023) the authors compared\nphysician and ChatGPT (ChatGPT, n.d.) responses to 195 randomly selected questions from the\nsubreddit, finding that healthcare professionals preferred ChatGPT's responses in 78.6% of 585\nevaluations. Other studies have examined LLM responses in specialized fields, such as\nOtolaryngology (Carnino et al., 2024), where ChatGPT's responses to 15 domain-specific\nquestions were rated with an accuracy of 3.76 out of 5, and Cancer (Chen et al., 2024), where\nphysicians evaluated 200 cancer-related questions and rated LLM responses to be of higher\nquality.\nWhile these studies are valuable in understanding LLM capabilities in answering consumer\nhealth queries, there remains a lack of evaluation of LLMs on a large-scale dataset of consumer\nhealth questions. The MedRedQA (Nguyen et al., 2023) dataset addresses this gap by providing\na large collection of consumer-based medical questions and expert answers extracted from the\nAskDocs subreddit. This dataset includes a wide range of layperson queries on medical topics,\noffering an opportunity to evaluate LLMs on a large number of real-world, consumer-oriented\nhealthcare questions in non-clinical settings. Physician answers in the dataset (extracted from"}, {"title": "AskDocs) are used as ground truths in this study. This is justified by the verification of the\nphysicians' credentials by the subreddit's moderators.", "content": "This study aims to evaluate the effectiveness of five prominent LLMs in answering\nconsumer-based medical questions from the MedRedQA dataset, which includes real-world\nqueries and expert responses extracted from the AskDocs subreddit. The LLMs assessed are\nGPT-40 mini (OpenAI, 2024), Llama 3.1 (70B) (Llama Team AI @ Meta, 2024), Mistral-123B\n(Mistral AI, 2024), Mistral-7B (Jiang et al., 2023), and Gemini-Flash (Georgiev et al., 2024).\nEmploying a cross-evaluation approach, each model's responses were not only evaluated by itself\nbut also by the other models to minimize bias in the assessment process. The findings reveal that\nGPT-40 mini achieved the highest alignment with expert answers according to four of the five\nmodel judges, whereas Mistral-7B scored the lowest according to three of the five judges. These\nresults highlight both the potential and current limitations of LLMs in addressing consumer\nhealth questions, indicating important avenues for future development.\nThe rest of the article is organized as follows. The Methods section describes the selection of\nmodels, the rationale for choosing the MedRedQA dataset, the prompt generation strategies, and\nthe evaluation techniques utilized in the study. The Results and Discussion section presents the\nevaluation outcomes, analyzes the performance of each model, and discusses the implications of\nthe cross-evaluation findings, including considerations about model sizes and evaluator\nreliability. The Limitations section acknowledges the constraints faced during the study, such as\ndataset challenges and evaluation complexities. Finally, the Conclusions and Future Work section\nsummarizes the key insights and outlines potential directions for enhancing LLM performance in\nconsumer health question answering."}, {"title": "2. Methods", "content": "The focus of this study is the evaluation of LLM responses to consumer-based medical questions.\nThe following sections describe the scope and methods used in this study."}, {"title": "2.1. Purpose and Scope", "content": "This study addresses the gap in understanding the effectiveness of LLMs in answering\nconsumer-based medical questions. Existing research has predominantly focused on\nmultiple-choice medical question answering datasets or smaller, sample-based studies on\nquestions extracted from AskDocs. This study evaluates LLMs on MedRedQA-a dataset\nconsisting of a large number of real-world, consumer-oriented medical inquiries extracted from\nAskDocs. To guide this exploration, two primary research questions are formulated:\nRQ1: How effectively do current LLMs perform in answering consumer-based medical\nquestions in MedRedQA?\nRQ2: How reliable are different LLMs as evaluators of their own and other models'\nresponses to medical questions, and does evaluator reliability vary significantly across\nmodels?"}, {"title": "RQI seeks to determine the extent to which LLMs can generate responses aligned with\nexpert answers in the dataset. RQ2 seeks to determine whether LLMs can act as reliable\nevaluators and whether the choice of evaluator can affect the evaluation results. Given the\ncomputational cost and resource constraints associated with using larger models, this study\nfocuses on smaller, more cost-effective models. This practical focus ensures that the findings of\nthe study remain accessible and relevant to future researchers exploring cost-effective methods\nfor medical question answering research.", "content": "2.2. Model Selection\nThe following five LLMs were selected for this study based on their demonstrated effectiveness\nin natural language processing tasks. These models provide a diverse selection from both\nopen-source and proprietary sources, capturing a broad view of current LLM capabilities for\nconsumer medical QA.\nGPT-40 mini: A compact variant of the GPT-40 model that prioritizes efficiency while\nmaintaining strong reasoning capabilities.\nLlama 3.1: 70B: Part of the Llama model family, this 70-billion parameter model performs\ncompetitively with other compact models and demonstrates robust language understanding\nacross a range of benchmarks.\nGemini-Flash: This model is part of the Gemini 1.5 series and is developed as a more\ncost-effective and faster alternative to Gemini-1.5 Pro (Georgiev et al., 2024).\nMistral 7B and Mistral 123B: Known for outperforming larger models on several\nbenchmarks, the Mistral family offers powerful small-scale models that demonstrate\ncompetitive performance for their size."}, {"title": "2.3. Dataset Selection", "content": "This section describes some of the datasets commonly used in medical question answering\nresearch and why the MedRedQA dataset is selected for this study over other datasets."}, {"title": "2.3.1. Existing Datasets in Medical QA", "content": "Several datasets have been used traditionally to evaluate the capabilities of large language\nmodels in medical question-answering tasks. In multiple choice question-answering, the most\nprominent ones include MedQA, PubMedQA, and MMLU-Clinical Knowledge (Hendrycks et\nal., 2020).\nMedQA: This dataset is collected from US medical licensing exams (USMLE) and includes\nquestions designed to test structured medical knowledge. The dataset is often used for evaluating\nthe clinical knowledge of LLMs, with models such as GPT-4 and Med-Gemini (Wang et al.,\n2024) achieving accuracies above 90% (Banerjee et al., 2023; Wang et al., 2024). It uses a"}, {"title": "multiple-choice format, which is useful for testing medical knowledge but is less relevant for\nreal-world, open-ended medical queries.", "content": "PubMedOA: It consists of clinical research questions extracted from the PubMed database,\nwhere answers can take the form of abstracts, yes/no responses, or specific medical conclusions.\nWhile the benchmark allows for both short-form and long-form responses, it remains structured\naround formal medical research rather than the informal queries typical of consumer healthcare.\nMMLU-Clinical Knowledge: This benchmark tests a model's knowledge across multiple\ndomains, including medicine, using a similar multiple-choice format. Like MedQA, this dataset\nfocuses on assessing clinical and factual knowledge.\nTo address the limitations of multiple-choice formats, some datasets have been transformed\nor created to require open-ended, detailed answers. These include MedQA-Open (Gopal et al.,\n2024), MedQA-CS (Yao et al., 2024), and MedQuAD (Abacha & Demner-Fushman, 2019):\nMedQA-Open: It is a modified version of MedQA, adapted to require models to generate\nopen-ended answers. While it allows for detailed responses, the questions remain based on\nmedical licensing exams, limiting their relevance to consumer-based, informal queries.\nMedOA-CS: This benchmark focuses on clinical skills, modeled after the medical education's\nObjective Structured Clinical Examinations (OSCEs). This dataset evaluates LLMs through two\ntasks: LLM-as-medical-student and LLM-as-clinical-examiner, both reflecting formal clinical\nscenarios. It provides an assessment of LLMs in settings that are closer to real-world clinical\nscenarios. However, the benchmark is less relevant for consumer-based medical\nquestion-answering tasks due to its focus on professional clinical settings.\nMedQuAD: It contains question-answer pairs extracted from National Institutes of Health\n(NIH) websites. The dataset includes detailed, structured answers based on expert medical\ncontent. However, like other datasets, it does not reflect the informal nature of consumer\nhealthcare questions.\nThese datasets are crucial for evaluating how well LLMs can handle open-ended questions in\nclinical scenarios. However, their reliance on formal medical cases or clinical exam formats\nmakes them less suitable for assessing how models respond to consumer-facing queries, which\noften lack medical precision or structure."}, {"title": "2.3.2. MedRedQA", "content": "This dataset includes 51,000 pairs of consumer medical questions and expert answers extracted\nfrom the AskDocs subreddit. AskDocs allows consumers to post health-related questions, and\nonly verified medical professionals provide answers. The dataset has two parts, one consisting of\nsamples where expert responses include citations to PubMed articles and the other without any\ncitations. The second part of the dataset is used in this study. The test set of this dataset contains\n5099 samples. Each sample contains a title, the body, the response by the medical expert, the\nresponse score, and the occupation of the expert. The dataset includes responses that received the\nhighest upvotes (response score), reflecting a consensus on the relevance and quality of the\nanswers."}, {"title": "MedRedQA is used for this study because it provides a large set of real consumer healthcare\nqueries which LLMs can be evaluated on. The informal nature of these questions presents a\nunique challenge for LLMs and provides a benchmark to evaluate the ability of models to\nprovide accurate answers to medical queries in non-clinical settings.", "content": "2.4. Prompt Generation\nTwo distinct prompts were used in this study, one for the response to user questions, and one for\nthe evaluation of LLM responses for agreement with physician responses. The two prompts are\nshown in Table 1."}, {"title": "2.5. Evaluation of Responses", "content": "Responses generated by LLMs are evaluated through a cross-model approach, where each model\nevaluates its own output and is cross-evaluated by all other models to reduce bias. In the\nevaluation process models are instructed to classify responses as either \u201cAgree\u201d or \u201cDisagree\u201d\nbased on their similarity to expert-provided answers. This approach was chosen over traditional\nmetrics such as ROUGE (Lin, 2004) and BERT-SCORE (Zhang et al., 2019) because these\nmetrics primarily measure surface-level lexical similarities, which may not reflect deeper\nsemantic alignment. Even when the wording between two answers differs, the core information\ncan still be highly aligned, which LLM-based evaluation may be able to better capture.\nLLMs have been successfully used as evaluators in multiple research contexts, including\npairwise comparisons, where they assess responses based on factors such as helpfulness, fluency,\nand factual accuracy (Zhang et al., 2024; Levy et al., 2024). Other studies have demonstrated the\neffectiveness of LLMs as evaluators in comparing expert responses with LLM generated\nresponses in the medical domain (Chen et al., 2024). These prior successes make LLMs suitable\nevaluators for this study.\nLLM generated answers are categorized as either \u201cAgree\u201d or \u201cDisagree\u201d. While some\nevaluation methodologies include a \u201cNeutral\u201d category to account for responses that are neither\nfully correct nor incorrect, the category isn't included in this evaluation because preliminary\nexperiments showed that most of the answers were classified as \u201cNeutral\u201d. This could be because\nmodel responses do not perfectly match expert answers, which makes the models classify most\nsamples as Neutrals. Using a binary evaluation instead forces the models to offer clearer"}, {"title": "distinctions, improving the utility of the results. Furthermore, this evaluation assumes the\naccuracy of expert answers as only verified individuals can respond to questions.", "content": "3. Results and Discussion\nEvaluation results are shown in Figure 1. GPT-40 mini responses achieved the highest\npercentage agreement with expert answers as evaluated by four out of the five model judges.\nResults also show that Mistral-7B tends to give higher agreement scores as an evaluator across\nthe board, possibly indicating a bias toward lenient evaluations. On the other hand, the\nGemini-Flash and Mistral-123B models tend to provide lower agreement scores, suggesting that\nthese models are more critical evaluators. It should also be noted that the agreement and\ndisagreement scores don't sum up to 100 except when the evaluator is GPT-40 mini. This is\nbecause there is a small percentage of responses for which the models either don't provide\nanswers or provide answers other than \u201cAgree\u201d or \u201cDisagree\u201d. Since the number of such\nresponses is very small, it doesn't affect the results by a significant amount."}, {"title": "Safety Mechanisms in Gemini-Flash: Experiments revealed that the Gemini-Flash model\nsometimes refused to answer questions entirely due to its built-in safety mechanisms, instead\nreturning a \"safety error\u201d.", "content": "Moreover, the model often responded by saying that \u201cit was an AI\nmodel and could not offer medical advice\u201d and in one particular example which involved\nsurgery, the model refused to provide an answer saying instead that it was a \u201cdangerous\nprocedure\" and asked to seek professional medical help. This resulted in a lot of disagreement\noutputs, and contributed to the low agreement score for the model as compared to other models."}, {"title": "Discrepancies Across Evaluation Scores: The results show variations in model behavior as\nevaluators, where Mistral-7B shows a tendency toward lenient assessments resulting in scores\ngreater than 80% for all models. On the other hand, the Gemini-Flash and Mistral-123B models\nare more critical, providing accuracy scores lower than 40% across all models.", "content": "Evaluation results\nfor the models as judges indicate that the Mistral-7B model is the worst performing evaluator,\nwhich explains its high agreement scores across all evaluations. Gemini-Flash performs the best\nas an evaluator (Table 2) but has the second lowest average agreement score (37.2%). This\ndifference suggests that, although Gemini-Flash's safety mechanisms lead to lower agreement\nscores due to frequent refusals to answer, these same mechanisms do not affect its ability to\nevaluate the responses of other models.\nModel Sizes: Mistral-7B achieves the lowest agreement score and the lowest accuracy as an\nevaluator. This might suggest that closed models (GPT4o-mini, Gemini-Flash) are closer in size\nto Llama 3.1-70B and Mistral-123B than Mistral-7B, as similarly sized models would be\nexpected to exhibit comparable performance levels. The significantly lower performance of"}, {"title": "Mistral-7B may be attributed to its smaller size, which could limit its ability to answer consumer\nbased medical questions accurately. However, whether or not specialized fine-tune models\nsmaller in size exhibit the same patterns is planned for future work.", "content": "Low Average Agreement Scores: Low average agreement scores, as shown in Table 4,\nemphasize the difficulties current LLMs face in providing accurate answers to consumer-based\nmedical questions. Excluding the Mistral-7B model due to its relatively poor evaluator\nperformance, we observed that even the highest-performing model, GPT4o-mini, achieved only\n51.2% accuracy. This finding highlights the challenges mid or small sized LLMs encounter when\nattempting to address medical inquiries posed by consumers. Future work should consider\nfine-tuning LLMs specifically on consumer-based medical question answering datasets.\nSpecialized models fine-tuned on medical data have shown to improve performance on medical\ndatasets, for example Med-Gemini currently achieves the highest accuracy (91.1%) on MedQA\n(Carnino et al., 2024). Fine-tuning may also improve the accuracy of model responses to\nconsumer medical queries.\nAnother promising approach is retrieval-augmented generation (RAG). RAG combines\nLLMs with information retrieval techniques to pull relevant data from external medical sources,\nsuch as MedlinePlus (MedlinePlus, n.d.), before generating an answer. This approach allows\nmodels to use information outside its learned parameters to generate responses to questions.\nRAG has shown promising results in improving the accuracy of LLMs for medical question\nanswering (Xiong et al., 2024). Consumer-facing QA systems may also benefit by augmenting\nLLMs with external medical knowledge before responding to queries. The results of this study\nalso show that LLMs should be evaluated on broader datasets in order to better understand the\nlimitations and capabilities of models in this domain in the real-world."}, {"title": "3.1. Limitations", "content": "This section outlines the limitations of this study, specifically focusing on challenges related to\nthe MedRedQA dataset and the evaluation process used.\nIncomplete Questions: One limitation of this study lies in the nature of the MedRedQA\ndataset, which includes instances where expert responses prompt the user for additional"}, {"title": "information or clarity. These situations often involve requests for supplementary details or, at\ntimes, visual inputs, such as clear images, which are not included in the dataset due to privacy\nrestrictions. As a result, model responses are sometimes misaligned with expert answers,\nespecially when interpreting cases where an image would provide critical context.", "content": "This limitation also extends to cases where expert answers hinge on situational or\ntime-specific knowledge. For example, questions related to health protocols during the\nCOVID-19 pandemic may lack explicit mention of the pandemic context, yet experts assume this\ncontext in their responses. Models, however, may not interpret these questions correctly without\nthis contextual indicator, potentially leading to inaccurate or irrelevant responses. An example is\nquestions such as whether it is safe to bring elderly individuals to hospitals during the COVID\nperiod; without a clear indication in the question, models may miss the situational implications\npresent in the expert responses.\nSample Size for Judging Evaluators: Another limitation is the small sample size used to\nmanually judge evaluator performance. The limited scope, focused primarily on straightforward\nquestions, may not reflect the models' true evaluator capabilities in more complex, ambiguous\ncases. Consequently, expanding this sample and incorporating a diverse range of medical\nquestion types in future studies would enhance the generalizability and reliability of evaluator\njudgments. It would also allow us to select models best suited for evaluations and increase\nconfidence in the results of model performance comparisons.\nCredibility of Physician Responses: The study assumes that physician responses are credible\nbased on the verification done by the subreddit's moderators. However, this verification might\nnot necessarily imply that the answers by the physicians are always correct."}, {"title": "4. Conclusions and Future Work", "content": "LLMs have enormous potential in transforming the medical question answering domain. In the\nclinical QA domain, they could help physicians quickly get answers to their queries and\nconsequently improve the quality of care that they offer to their patients. In the consumer QA\ndomain, they could help consumers get the preliminary guidance that they often seek before\ndeciding to go for a hospital visit. However, current LLMs are not yet capable of being deployed\nfor real-world consumer medical questions, answering applications in a zero-shot manner.\nAs the results of this study show, LLMs struggle in providing accurate answers to consumer\nhealth questions. There is a need to extensively evaluate LLMs on a broader range of consumer\nQA benchmarks that reflect real-world healthcare scenarios in order to accurately assess their\nreliability in providing answers to consumer questions. The findings of this study highlight both\nthe potential and the current limitations of using LLMs for consumer health question answering.\nWhile models like GPT-4o mini show promise, the overall low agreement scores indicate that\nsignificant improvements are needed before LLMs can be reliably deployed in real-world\nhealthcare settings.\nOne immediate avenue for improvement is the fine-tuning of LLMs on datasets specifically\ncurated for consumer health questions. As shown in other domains, specialized fine-tuning can"}, {"title": "substantially enhance a model's performance by allowing it to better understand the nuances and\nlinguistic patterns typical of the target domain. Developing models that are trained on\nlarge-scale, diverse datasets like MedRedQA could help bridge the gap between current\ncapabilities and the requirements for effective consumer health assistance. Furthermore,\nexpanding LLMs to handle multimodal inputs, such as images or voice recordings, presents an\nopportunity to better mimic the versatility of human practitioners. Enabling models to process\nand interpret medical images, for instance, could enhance their ability to provide comprehensive\nanswers when textual information alone is insufficient.", "content": "Integrating LLMs with medical knowledge bases and evidence-based guidelines presents\nanother opportunity to improve accuracy. Retrieval-Augmented Generation (RAG) techniques,\nwhich combine LLMs with information retrieval systems, allow models to access up-to-date and\nauthoritative medical information during response generation. By fetching relevant data from\ntrusted sources like MedlinePlus or PubMed, models can provide more accurate and contextually\nappropriate answers, reducing the risk of misinformation.\nAddressing the challenge of incomplete or ambiguous questions is crucial. Future research\ncould explore methods for LLMs to handle incomplete information more effectively, such as by\ngenerating clarifying questions or recognizing when additional data is needed. Developing\nmodels capable of understanding implicit context\u2014like situational factors during a\npandemic-would also enhance their applicability in real-world scenarios. The study\nunderscores the need for more comprehensive evaluation methods to assess LLM performance\naccurately. Future work should involve larger and more diverse sample sizes for evaluation,\nincluding complex and ambiguous questions that reflect the full spectrum of consumer inquiries.\nAdditionally, establishing standardized benchmarks and metrics for consumer health QA can\nprovide a clearer picture of model capabilities and areas needing improvement.\nAs AI technologies permeate the healthcare sector, adhering to regulatory requirements\nbecomes essential. Future work should consider compliance with health information regulations\nlike HIPAA in the United States or GDPR in Europe when handling sensitive data. Developing\nmodels that not only perform well but also meet legal and ethical standards will be critical for\nreal-world deployment. Improving the interpretability of LLMs can enhance trust and facilitate\ntheir adoption in healthcare. Future research could focus on developing tools and techniques that\nallow users and practitioners to understand how models arrive at their conclusions.\nInterpretability can aid in identifying errors, biases, and areas where the model may lack\nsufficient knowledge."}, {"title": "5. Ethics Statement", "content": "This study is conducted solely for research purposes to evaluate the capabilities and limitations\nof LLMs in consumer-oriented medical question answering. While LLMs show potential for\nhelping users find medical information, this research does not imply that these models can, or\nshould, replace professional medical expertise. In conducting this research, a de-identified\nmedical dataset is used to ensure privacy and data security. Any examples drawn from this"}, {"title": "dataset are paraphrased before inclusion in the paper to prevent the direct dissemination of real\npatient information.", "content": "6. Declaration of Generative AI and AI-Assisted Technologies\nDuring the preparation of this manuscript, the authors used ChatGPT, based on the GPT-4 model,\nto improve the flow of the text, correct grammatical errors, and enhance the clarity of the\nwriting. The language model was not used to generate content, citations, or verify facts. After\nusing this tool, the authors thoroughly reviewed and edited the content to ensure accuracy,\nvalidity, and originality, and take full responsibility for the final version of the manuscript."}]}