{"title": "MUON IS SCALABLE FOR LLM TRAINING", "authors": ["Jingyuan Liu", "Jianlin Su", "Xingcheng Yao", "Zhejun Jiang", "Guokun Lai", "Yulun Du", "Yidao Qin", "Weixin Xu", "Enzhe Lu", "Junjie Yan", "Yanru Chen", "Huabin Zheng", "Yibo Liu", "Shaowei Liu", "Bohong Yin", "Weiran He", "Han Zhu", "Yuzhi Wang", "Jianzhou Wang", "Mengnan Dong", "Zheng Zhang", "Yongsheng Kang", "Hao Zhang", "Xinran Xu", "Yutao Zhang", "Yuxin Wu", "Xinyu Zhou", "Zhilin Yang"], "abstract": "Recently, the Muon optimizer (K. Jordan et al. 2024) based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves ~ 2\u00d7 computational efficiency compared to AdamW with compute optimal training. Based on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models. We open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) (OpenAI et al. 2024; DeepSeek-AI et al. 2024; Grattafiori et al. 2024; Gemini Team et al. 2024) has significantly pushed forward the progress in artificial general intelligence. However, training capable LLMs remains a computationally intensive and resource-demanding process due to scaling laws (Kaplan et al. 2020; Hoffmann et al. 2022). Optimizers play a crucial role in efficiently and effectively training of LLMs, with Adam (Kingma et al. 2015) and its variant AdamW (Loshchilov et al. 2019) being the standard choice for most large-scale training.\nRecent developments in optimization algorithms have shown potential to improve training efficiency beyond AdamW (Liu et al. 2024; K. Jordan et al. 2024; Yuan et al. 2024; Vyas et al. 2025; X.-L. Li 2018a; X.-L. Li 2018b; Pooladzandi et al. 2024; X. Li 2022; X.-L. Li 2024; Pethick et al. 2025). Among these, K. Jordan et al. 2024 proposed Muon, which updates matrix parameters with orthogonalized gradient momentum using Newton-Schulz iteration. Initial experiments with Muon have demonstrated promising results in small-scale language model training. However, as discussed in this blog (K. Jordan et al. 2024), several critical challenges remain unaddressed: (1) how to effectively scale optimizers based on matrix orthogonalization to larger models with billions of parameters trained with trillions of tokens, (2) how to compute approximate orthogonalization in a distributed setting, and (3) whether such optimizers can generalize across different training stages including pre-training and supervised finetuning (SFT).\nIn this technical report, we present a comprehensive study addressing these challenges. Our work builds upon Muon while systematically identifying and resolving its limitations in large-scale training scenarios. Our technical contributions include:\n\u2022 Analysis for Effective Scaling of Muon: Through extensive analysis, we identify that weight decay plays a crucial role in Muon's scalability. Besides, we propose scale adjustments to Muon's parameter-wise update rule. Such adjustments allow Muon to work out-of-the-box without hyper-parameter tuning, and also significantly improve training stability.\n\u2022 Efficient Distributed Implementation: We develop a distributed version of Muon with ZeRO-1 (Rajbhandari et al. 2020) style optimization, achieving optimal memory efficiency and reduced communication overhead while preserving the mathematical properties of the algorithm.\n\u2022 Scaling Law Validation: We performed scaling law research that compares Muon with strong AdamW baselines, and showed the superior performance of Muon (1a). Based on the scaling law results, Muon achieves comparable performance to AdamW trained counterparts while requiring only approximately 52% of the training FLOPs.\nOur comprehensive experiments demonstrate that Muon can effectively replace AdamW as the de facto optimizer for large-scale LLM training, offering significant improvements in both training efficiency and model performance. As a result of this work, we release Moonlight, a 16B-parameter MoE model trained using Muon, along with our implementation and intermediate training checkpoints to facilitate further research in scalable optimization techniques for LLMs."}, {"title": "Methods", "content": null}, {"title": "Background", "content": null}, {"title": "The Muon Optimizer", "content": "Muon (K. Jordan et al. 2024) has recently been proposed to optimize neural network weights representable as matrices. At iteration t, given current weight Wt-1, momentum \u00b5, learning rate \u03b7t and objective Lt, the update rule of the Muon optimizer can be stated as follows:\nMt = \u00b5Mt\u22121 + Lt (Wt-1)\nOt = Newton-Schulz(Mt)\u00b9\nWt = Wt-1 - NtOt\n(1)\nHere, Mt is the momentum of gradient at iteration t, set as a zero matrix when t 0. In Equation 1, a Newton-Schulz iteration process (Bernstein et al. 2024) is adopted to approximately solve (MMT)-1/2Mt. Let U\u03a3VT = M\u0165 be the singular value decomposition (SVD) of Mt, we will have (MMT)-1/2Mt = UVT, which orthogonalizes Mt. Intuitively, orthogonalization can ensure that the update matrices are isomorphic, preventing the weight from learning along a few dominant directions (K. Jordan et al. 2024).\nIn practice, we follow (K. Jordan et al. 2024) to use a Nesterov-style momentum by putting \u00b5Mt + Lt(Wt-1) to the Newton-Schulz iteration instead of Mt."}, {"title": "Newton-Schulz Iterations for Matrix Orthogonalization", "content": "Equation 1 is calculated in an iterative process. At the beginning, we set X0 = Mt/||Mt||F. Then, at each iteration k, we update Xk from Xk\u22121 as follows:\nXk = aXk\u22121 + b(Xk\u22121X\u22121)Xk\u22121 + C(Xk\u22121X\u22121)2Xk\u22121\n(2)\nwhere XN is the result of such process after N iteration steps. Here a, b, c are coefficients. In order to ensure the correct convergence of Equation 2, we need to tune the coefficients so that the polynomial f(x) = ax + bx\u00b3 + cx5 has a fixed point near 1. In the original design of K. Jordan et al. 2024, the coefficients are set to a = 3.4445, b = -4.7750, c = 2.0315 in order to make the iterative process converge faster for small initial singular values. In this work, we follow the same setting of coefficients."}, {"title": "Steepest Descent Under Norm Constraints", "content": "Bernstein et al. 2024 proposed to view the optimization process in deep learning as steepest descent under norm constraints. From this perspective, we can view the difference between Muon and Adam (Kingma et al. 2015; Loshchilov et al. 2019) as the difference in norm constraints. Whereas Adam is a steepest descent under the a norm constraint dynamically adjusted from a Max-of-Max norm, Muon offers a norm constraint that lies in a static range of Schatten-p norm for some large p (Franz 2024). When equation 1 is accurately computed, the norm constraint offered by Muon will be the spectral norm. Weights of neural networks are used as operators on the input space or the hidden space, which are usually (locally) Euclidean (Cesista 2024), so the norm constraint on weights should be an induced operator norm (or spectral norm for weight matrices). In this sense, the norm constraint offered by Muon is more reasonable than that offered by Adam."}, {"title": "Scaling Up Muon", "content": "Weight Decay While Muon performs significantly better than AdamW on a small scale as shown by K. Jordan et al. 2024, we found the performance gains diminish when we scale up to train a larger model with more tokens. We observed that both the weight and the layer output's RMS keep growing to a large scale, exceeding the high-precision range of bf16, which might hurt the model's performance. To resolve this issue, we introduced the standard AdamW (Loshchilov et al. 2019) weight decay mechanism into Muon2.\nWt = Wt-1 - 7t(Ot + 1Wt\u22121)\n(3)\nWe experimented on Muon both with and without weight decay to understand its impact on the training dynamics of LLMs. Based on our scaling law research in Sec 3.2, we trained an 800M parameters model with 100B tokens (~5\u00d7 optimal training tokens). Figure 2 shows validation loss curves of the model trained with AdamW, vanilla Muon (without weight decay), and Muon with weight decay. While vanilla Muon initially converges faster, we observed that some model weights grew too large over time, potentially limiting the model's long-term performances. Adding weight decay addressed this issue - the results demonstrate that Muon with weight decay outperforms both vanilla Muon and AdamW, achieving lower validation loss in the over-train regime. Therefore, we adjusted our update rule to equation 3, where X is the weight decay ratio.\nConsistent update RMS An important property of Adam and AdamW (Kingma et al. 2015, Loshchilov et al. 2019) is that they maintain a theoretical update RMS around 13. However, we show that Muon's update RMS varies depending on the shape of the parameters, according to the following lemma:\nLemma 1. For a full-rank matrix parameter of shape [A, B], its theoretical Muon update RMS is \u221a1/max(A, B) .\nThe proof can be found in the Appendix A. We monitored Muon's update RMS during training and found it typically close to the theoretical value given above. We note that such inconsistency can be problematic when scaling up the model size:\n\u2022 When max(A, B) is too large, e.g. the dense MLP matrix, the updates become too small, thus limiting the model's representational capacity and leading to suboptimal performances;\n\u2022 When max(A, B) is too small, e.g. treating each KV head in GQA (Shazeer 2019) or MLA (DeepSeek-AI et al. 2024) as a separate parameter, the updates become too large, thus causing training instabilities and leading to suboptimal performances as well."}, {"title": "Distributed Muon", "content": "ZeRO-1 and Megatron-LM Rajbhandari et al. 2020 introduced the ZeRO-1 technique that partitions the expensive optimizer states (e.g. master weights, momentum) all over the cluster. Megatron-LM (Shoeybi et al. 2020) integrated ZeRO-1 into its native parallel designs. Based on Megatron-LM's sophisticated parallel strategies, e.g. Tensor-Parallel (TP), Pipeline Parallel (PP), Expert Parallel (EP) and Data Parallel (DP), the communication workload of ZeRO-1 can be reduced from gathering all over the distributed world to only gathering over the data parallel group.\nMethod ZeRO-1 is efficient for AdamW because it calculates updates in an element-wise fashion. However, Muon requires the full gradient matrix to calculate the updates. Therefore, vanilla ZeRO-1 is not directly applicable to Muon.\nWe propose a new distributed solution based on ZeRO-1 for Muon, referred to as Distributed Muon. Distributed Muon follows ZeRO-1 to partition the optimizer states on DP, and introduces two additional operations compared to a vanilla Zero-1 AdamW optimizer:\n1. DP Gather. For a local DP partitioned master weight (1/DP the size of the model weight), this operation is to gather the corresponding partitioned gradients into a full gradient matrix.\n2. Calculate Full Update. After the above gathering, perform Newton-Schulz iteration steps on the full gradient matrix as described in Sec 2.1. Note that we will then discard part of the full update matrix, as we only need the partition corresponding to the local parameters to perform update.\nThe implementation of Distributed Muon is described in Algorithm 1. The additional operations introduced by Distributed Muon are colored in blue.\nAnalysis We compared Distributed Muon to a classic ZeRO-1 based distributed AdamW (referred as Distributed AdamW for simplicity) in several aspects:\n\u2022 Memory Usage. Muon uses only one momentum buffer, while AdamW uses two momentum buffers. Therefore, the additional memory used by the Muon optimizer is half of Distributed AdamW.\n\u2022 Communication Overhead. For each device, the additional DP gathering is only required by the local DP partitioned parameters p. Therefore, the communication cost is less than the reduce-scatter of G or the all-gather of P. Besides, Muon only requires the Newton-Schulz iteration steps in bf16, thus further reducing the communication overhead to 50% comparing to fp32. Overall, the communication workload of Distributed Muon is (1, 1.25] of that of Distributed AdamW. The upper-bound is calculated as that the communication of Distributed Muon is 4 (fp32 G reduce-scatter) + 2 (bf16 Muon gather) + 4 (fp32 P all-gather), while Distributed AdamW is 4 + 4. In practice, as we usually train with multiple DP, the empirical additional cost usually is closer to the lower-bound 1.5.\n\u2022 Latency. Distributed Muon has larger end-to-end latencies than Distributed AdamW because it introduces additional communication and requires running Newton-Schulz iteration steps. However, this is not a significant issue because (a) only about 5 Newton-Schulz iteration steps are needed for a good result (discussed in Sec 2.2), and (b) the end-to-end latency caused by the optimizer is negligible compared to the model's forward-backward pass time (e.g. usually 1% to 3%). Moreover, several engineering techniques, such as overlapping gather and computation, and overlapping optimizer reduce-scatter with parameter gather, can further reduce latency.\nWhen training large-scale models in our distributed cluster, Distributed Muon has no noticeable latency overhead compared to its AdamW counterparts. We will soon release a pull request that implements Distributed Muon for the open-source Megatron-LM (Shoeybi et al. 2020) project."}, {"title": "Experiments", "content": null}, {"title": "Consistent Update RMS", "content": "As discussed in Sec 2.2, we aim to match the update RMS across all matrix parameters and also match it with that of AdamW. We experimented with two methods to control the Muon update RMS among parameters and compared them to a baseline that only maintains a consistent RMS with AdamW:\n1. Baseline. We multiplied the update matrix by 0.2. \u221aH (H is the model hidden size) to maintain a consistent update RMS with AdamW. Note that max(A, B) equals to H for most matrices.\nWt = Wt\u22121 - Nt(0.2 \u00b7 Ot \u00b7 \u221aH + >Wt\u22121)\n(5)\n2. Update Norm. We can directly normalize the updates calculated via Newton-Schulz iterations so its RMS strictly becomes 0.2;\nWt = Wt\u22121 - \u03b7t(0.2\u00b7 Ot/RMS(O+) + Wt\u22121)\n(6)\n3. Adjusted LR. For each update matrix, we can scale its learning rate by a factor of 0.2. \u221amax(A, B) based on its shape.\nWt = Wt\u22121 - nt(0.2\u00b7 Ot\u00b7 \u221amax(A, B) + \u5165Wt-1)\n(7)\nAnalysis We designed experiments to illustrate the impact of Muon update RMS at an early training stage, because we observed that unexpected behaviors happened very quickly when training models at larger scale. We experimented with small scale 800M models as described in 3.2. The problem of inconsistent update RMS is more pronounced when the disparity between matrix dimensions increases. To highlight the problem for further study, we slightly modify the model architecture by replacing the Swiglu MLP with a standard 2-layer MLP, changing the shape of its matrix parameters from [H, 2.6H] to [H, 4H]. We evaluated the model's loss and monitored a few of its parameters' RMS, specifically, attention query (shape [H, H]) and MLP (shape [H, 4H]). We evaluated the model after training for 4B tokens out of a 20B-token schedule. From Table 1, we observed several interesting findings:\n1. Both Update Norm and Adjusted LR achieved better performances than Baseline;\n2. For the MLP weight matrix of shape [H, 4H], both Update Norm and Adjusted LR obtain a weight RMS that is roughly doubled comparing to Baseline. This is reasonable as \u221amax(H, 4H)/\u221aH = 2, so the update RMS of Update Norm and Adjusted LR is roughly two times of Baseline;\n3. For the attention query weight matrix of shape [H, H], Update Norm still norms the update, while Adjusted LR does not because \u221amax(H, H)/\u221aH = 1. As a result, Adjusted LR results in a similar weight RMS as Baseline, but Update Norm has a larger weight rms similar to its MLP.\nBased on these findings, we choose the Adjusted LR method for future experiments because it has lower cost."}, {"title": "Scaling Law of Muon", "content": "For a fair comparison with AdamW, we performed scaling law experiments on a series of dense models in Llama (Grattafiori et al. 2024) architecture. Building a strong baseline is of crucial importance in optimizer research. Hence, we perform a grid search for hyper-parameters of AdamW, following the compute-optimal training setup (Kaplan et al. 2020) (the grid search experiments can be found in Appendix B). Details of the model architecture and hyper-parameters can be found in Table 2. For Muon, as discussed in Sec 2.2, since we matched Muon's update RMS to AdamW, we directly reused the hyper-parameters that are optimal for the AdamW baseline.\nThe fitted scaling law curve can be found in figure 3, and the fitted equations are detailed in table 3. As shown in Figure 1a, Muon only requires about 52% training FLOPs to match the performance of AdamW under compute-optimal setting."}, {"title": "Pretraining with Muon", "content": "Model Architecture To evaluate Muon against contemporary model architectures, we pretrained from scratch using the deepseek-v3-small architecture (DeepSeek-AI et al. 2024) as it demonstrates strong performance and the original results serve as a reference for comparison. Our pretrained model has 2.24B activated and 15.29B total parameters (3B activated and 16B total when including embedding). Minor modifications to the architecture are detailed in Appendix C.\nPretraining Data Our pretraining data details can be found in K. Team 2025. The maximum context length during pretraining is 8K.\nPretraining The model is trained in several stages. We use a le-3 auxfree bias update rate in stage 1 and 2, and 0.0 auxfree bias update rate in stage 3. The weight decay is set to 0.1 for all stages. More details and discussions of model training can be found in the Appendix D.\n1. 0 to 33B tokens: In this stage, the learning rate linearly increases to 4.2e-4 in 2k steps. The batch size is kept at 2048 examples;"}, {"title": "Dynamics of Singular Spectrum", "content": "In order to validate the intuition that Muon can optimize the weight matrices in more diverse directions, we conducted a spectral analysis of the weight matrices trained with Muon and AdamW. For a weight matrix with singular values \u03c3 = (01, 02,\u00b7\u00b7\u00b7, \u03c3\u03b7), we calculate the SVD entropy (Alter et al. 2000; Roy et al. 2007) of this matrix as follows:\nH (\u03c3) =1/ log n \u03a3 \u03c3 log \u03a3 \u03c3\nAs shown in Figure 4, we visualized the average SVD entropy of the weight matrices across different training checkpoints during pretraining with 1.2T tokens. We can see that across all training checkpoints and all groups of weight matrices, the SVD entropy of Muon is higher than that of AdamW, which verifies the intuition that Muon can provide a more diverse spectrum of updates for the weight matrices. This discrepancy is more significant in the router weights for expert selection, which indicates that mixture-of-expert models can benefit more from Muon.\nMoreover, we visualized the singular value distributions of each weight matrix at the checkpoint trained with 1.2T tokens as demonstrated in Appendix F. We find that, for over 90% of the weight matrices, the SVD entropy when optimized by Muon is higher than that of AdamW, providing strong empirical evidence for Muon's superior capability in exploring diverse optimization directions."}, {"title": "Supervised Finetuning (SFT) with Muon", "content": "In this section, we present ablation studies on the Muon optimizer within the standard SFT stage of LLM training. Our findings demonstrate that the benefits introduced by Muon persist during the SFT stage. Specifically, a model that is both Muon-pretrained and Muon-finetuned outperforms others in the ablation studies. However, we also observe that when the SFT optimizer differs from the pretraining optimizer, SFT with Muon does not show a significant advantage over AdamW. This suggests that there is still considerable room for further exploration, which we leave for future work."}, {"title": "Ablation Studies on the Interchangeability of Pretrain and SFT Optimizers", "content": "To further investigate Muon's potential, we finetuned Moonlight@1.2T and Moonlight-A@1.2T using both the Muon and AdamW optimizers. These models were finetuned for two epochs on the open-source tulu-3-sft-mixture dataset (Lambert et al. 2024), which contains 4k sequence length data. The learning rate followed a linear decay schedule, starting at 5 \u00d7 10-5 and gradually reducing to 0. The results, shown in Table 6, highlight the superior performance of Moonlight@1.2T compared to Moonlight-A@1.2T."}, {"title": "SFT with Muon on public pretrained models", "content": "We further applied Muon to the supervised fine-tuning (SFT) of a public pretrained model, specifically the Qwen2.5-7B base model (Yang et al. 2024), using the open-source tulu-3-sft-mixture dataset (Lambert et al. 2024). The dataset was packed with an 8k sequence length, and we employed a cosine decay learning rate schedule, starting at 2 \u00d7 10-5 and gradually decreasing to 2 \u00d7 10-6. The results are presented in Table 7. For comparison, we show that the Muon-finetuned model achieves performance on par with the Adam-finetuned model. These results indicate that for optimal performance, it is more effective to apply Muon during the pretraining phase rather than during supervised fine-tuning."}, {"title": "Discussions", "content": "There are several possible directions for future research that could further explore and expand upon the current findings.\nIncorporating All Parameters into the Muon Framework Currently, the Muon optimizer is utilized in conjunction with the Adam optimizer, where certain parameters remain under the purview of Adam optimization. This hybrid approach, while functional, presents an opportunity for improvement. The integration of the optimization of all parameters exclusively within the Muon framework is a topic of significant research interest.\nExtending Muon to Schatten Norms The Muon optimizer can be interpreted as the steepest descent method under the spectral norm. Given the broad applicability and versatility of Schatten norms, extending Muon to encompass the general Schatten norm is a promising direction. This extension may unlock additional optimization capabilities and potentially yield superior results compared to the current spectral norm-based implementation.\nUnderstanding and Solving the Pretraining-Finetuning Mismatch A notable phenomenon observed in practice is the suboptimal performance of models pretrained with AdamW when fine-tuned with Muon, and vice versa. This optimizer mismatch presents a significant barrier to effectively leveraging the extensive repository of AdamW-pretrained checkpoints, thereby necessitating a rigorous theoretical investigation. A precise understanding of the underlying mechanisms is essential for devising robust and effective solutions."}, {"title": "Conclusions", "content": "In this technical report, we presented a comprehensive study on the scalability of Muon in LLM training. Through systematic analysis and improvements, we successfully applied Muon to a 3B/16B-parameter MoE model trained on 5.7 trillion tokens. Our results demonstrate that Muon can effectively replace AdamW as the standard optimizer for large-scale LLM training, offering significant advantages in both training efficiency and model performance. By open-sourcing our implementation, the Moonlight model, and intermediate training checkpoints, we aim to facilitate further research in scalable optimization techniques and accelerate the development of training methods for LLMs."}, {"title": "Update RMS", "content": "Proof. Without loss of generality, consider the orthogonal matrices U \u2208 Rn\u00d7n and V \u2208 Rm\u00d7m where n > m \u2265 r. We will show that for X = U[:,:r] V[:r,:] (the update of the Muon has the same format), the RMS value is \u221ar/mn. From the definition of matrix multiplication:\nXi,j = \u03a3 Ui,kVk.j\nThe RMS can be expressed as:\nRMS(X)\u00b2 = 1/ mn \u03a3\u03a3(\u03a3Ui,kVk.j)\u00b2\n= 1/mn \u03a3\u03a3(\u03a3U\u00b2i,k) (\u03a3V\u00b2k,j)\nTherefore, RMS(X) = \u221ar/mn. For the common case where the matrices are full-rank, r = m, yielding RMS(X) =\u221a1/n.\nConsistent Update RMS Across Muon and AdamW As discussed in 2.2, we'd like to match the update RMS between Muon and AdamW optimizers. This is validated by experiments on small-scale models. We set Muon's Update RMS in the range of [0.05, 0.1, 0.2, 0.4, 0.8] and AdamW as baseline. We reported the loss and representative weight matrix RMS at 2k steps (about 2B tokens) in the Table 8. From the results, we find that 0.2 RMS and 0.4 RMS performed similarly and much better than other settings. These findings are consistent with our empirical observation that AdamW's update RMS is in the range of 0.2~0.4. We opted to control the update RMS of Muon to 0.2.\nTo ensure the fairness and accuracy of our experiments, we conducted a series of experiments on our proprietary dataset to derive scaling law parameters that are optimal for AdamW. This includes determining the optimal model size(N), number of training tokens(D), learning rate(n), batch size(B) under a constrained computational budget (FLOPs,C)."}, {"title": "Model Architecture", "content": "Muon is agnostic to model architectures, and we used a model similar to Deepseek-V3-Small as described in DeepSeek-AI et al. 2024, because it is a strong model with open weights as a baseline. We made several small modifications in the Moonlight model and listed them here:\nMulti-token Prediction (MTP) MTP has not shown significant benefits to pretraining in our experiments. For simplicity, we do not introduce MTP layers into the Moonlight model.\nAuxfree Bias Update In DeepSeek-AI et al. 2024, auxfree bias is updated by: b\u2081 = bi + u \u00d7 sign(ei), where u is the update ratio, b\u2081 is the bias for the ith expert, and e\u00bf is the expert's violating ratio. We slightly modified the update rule as: b\u2081 = bi + u \u00d7 (sign(ei) \u2013 sign(e).mean()), where sign(e).mean() is the average of the signs of all expert's violating ratio, in order to control the magnitude of the bias, while does not change the topk selection logic.\nGate Scaling Factor Deepseek-V2-Lite did not use the gate scaling factor, and Deepseek-V3 used a scaling factor of 2.5. We used a scaling factor of 2.446 to control a similar output rms like dense models. The code for calculating our gate scaling factor can be found in Figure 6."}, {"title": "Training Stability", "content": "No Loss or Grad Norm Spike The Moonlight training process was very smooth and we did not meet any loss spike or gradient norm spike. The loss and grad norm curve can be seen in Figure 7 (Moonlight is colored in blue and Moonlight-A trained by AdamW is colored in red)\nMax Attention Logit During training, we observed that while both the training loss and gradient norm remained stable throughout the process, the maximum attention logit (computed as the single largest logit value across the global batch) exhibited a distinct upward trajectory in specific layers during the initial training phase, exceeding a threshold of 100. Notably, AdamW demonstrated healthier behavior in controlling this metric compared to alternative optimizers.\nRMSNorm Gamma Weight Decay It is noteworthy that applying weight decay to the RMSNorm gamma parameter is crucial for ensuring training stability, as it effectively prevents excessively high output RMS values in each layer."}, {"title": "Comparison With More Expensive Models", "content": "Table 10 presents a comparative analysis between our Moonlight model (optimized with Muon) and publicly available models trained with greater computational resources, including LLama3.1-8B (Grattafiori et al. 2024), Gemma-9B (Gemma Team et al. 2024) and Qwen2.5-7B (Yang et al. 2024). Figure 8 illustrates the GSM8k performance benchmarks of Moonlight against comparable models in the field."}, {"title": "Singular Value Distributions of Weight Matrices", "content": "We visualize the singular value distributions of weight matrices by plotting a line graph of its singular values in descending order for each matrix, normalized by the largest one. As shown in Figures 9 and 10, we find that, for most of the weight matrices, the singular value distributions of them optimized by Muon are more flattened than that of AdamW, which further confirms the hypothesis that Muon can provide a more diverse spectrum of updates."}]}