{"title": "SWINE DIET DESIGN USING MULTI-OBJECTIVE REGIONALIZED BAYESIAN OPTIMIZATION", "authors": ["Gabriel D. Uribe-Guerra", "Danny A. M\u00fanera-Ram\u00edrez", "Juli\u00e1n D. Arias-Londo\u00f1o"], "abstract": "The design of food diets in the context of animal nutrition is a complex problem that aims to develop cost-effective formulations while balancing minimum nutritional content. Traditional approaches based on theoretical models of metabolic responses and concentrations of digestible energy in raw materials face limitations in incorporating zootechnical or environmental variables affecting the performance of animals and including multiple objectives aligned with sustainable development policies. Recently, multi-objective Bayesian optimization has been proposed as a promising heuristic alternative able to deal with the combination of multiple sources of information, multiple and diverse objectives, and with an intrinsic capacity to deal with uncertainty in the measurements that could be related to variability in the nutritional content of raw materials. However, Bayesian optimization encounters difficulties in high-dimensional search spaces, leading to exploration predominantly at the boundaries. This work analyses a strategy to split the search space into regions that provide local candidates termed multi-objective regionalized Bayesian optimization as an alternative to improve the quality of the Pareto set and Pareto front approximation provided by BO in the context of swine diet design. Results indicate that this regionalized approach produces more diverse non-dominated solutions compared to the standard multi-objective Bayesian optimization. Besides, the regionalized strategy was four times more effective in finding solutions that outperform those identified by a stochastic programming approach referenced in the literature. Experiments using batches of query candidate solutions per iteration show that the optimization process can also be accelerated without compromising the quality of the Pareto set approximation during the initial, most critical phase of optimization.", "sections": [{"title": "1 Introduction", "content": "Food diet design for livestock and poultry production is an increasingly complex problem that, in principle, aims to develop diet formulations that can balance minimum nutritional contents with cost-effective food production criteria [1]. However, farmers progressively have had to wrestle with additional obstacles, such as preserving production efficiency in constantly changing environmental contexts resulting from climate change effects and persistent growth in global food demand [2, 3]. Climate change imposes new challenges to the livestock industry because it can affect raw material and agricultural by-products' availability, affecting soil salinity and reducing forage areas, but also increasing heat stress of the animals, which limits the capacity of livestock to shed heat to their environment [4], and results in reduced feed intake and body weight gain [5]. Simultaneously, livestock farming stands as a significant contributor to greenhouse gas emissions, so there's a growing push for the industry to develop dietary strategies aimed at shrinking its ecological impact [6].\nConsidering the multiple objectives and restrictions that must be satisfied simultaneously, during the last decades, the food diet design problem has been addressed as a single/multi-objective constrained optimization problem [7]. Techniques employed includes classical Linear Programming (LP) [8, 9, 10], Stochastic Programming (SP) [11, 12], Non-Linear Programming (NLP) [13, 14] and different heuristic approaches [15, 16, 17]. Moreover, the considered objectives include productivity, weight or conversion maximization, reduction of excretions, energy density, variations in nutrient reduction, and greenhouse gas cutback, among others [18, 19]; some of them have been addressed at the same time.\nThe evolution of the applied techniques and focus over time is attributed to various technical limitations, such as the inability to apply solutions across different breeds or species universally and the need to deal with the variability in raw materials or animal metabolism [7]. The initial designs prioritized meeting constraints at the lowest cost, regardless of the species involved. In [8], the focus was on creating a practical system to help ingredient buyers effectively use LP in real-world scenarios, thus advancing diet design systems. Similarly, [9] applied LP to improve operational efficiency and decision-making processes in the cattle industry. Additionally, [10] utilized LP to minimize costs on small dairy farms by using low-cost raw materials that meet nutritional requirements.\nHowever, the challenges of achieving efficient production and reducing nutrient variability to ensure environmental friendliness in the industry have led to LP being set aside in favor of exploring new techniques. Consequently, SP techniques have been proposed as a viable alternative, ensuring optimal nutrient levels in formulations at minimal cost, especially for lactating cattle. In [11], authors demonstrated that using SP in diet formulation reduces variability and provides a confidence level exceeding 90% for meeting nutrient requirements. Similarly, [12] suggested SP to address a multi-objective problem in pig nutrition, focusing on assuring specific amounts of lysine and metabolizable energy while minimizing nutrient variability at the lowest cost.\nIn addition to reducing nutrient variability in formulation ingredients, efforts have been made to enhance productivity, leading to the use of NLP models. In [13], this technique was applied to determine optimal nutrient concentrations for maximizing egg bird productivity while minimizing costs. NLP techniques were also used to evaluate the intake and digestibility of food in sheep at different stages of nutrition to improve food quality [14].\nIn recent years, heuristic approximation methods have enabled the inclusion of variables beyond dietary and nutritional factors, allowing for the evaluation of additional inputs that influence the outcomes of formulation systems. Studies, such as [16], illustrate that Genetic Algorithms (GA) for optimizing poultry diet formulation costs are more efficient"}, {"title": "3 Methods", "content": "In this section, we outline the methods and techniques used to optimize the swine diet formulation problem discussed earlier, employing both the standard Multi-Objective Bayesian Optimization (MOBO) and the Multi-Objective Region- alized Bayesian Optimization (MORBO) approaches. Figure 1 provides a schematic representation of the proposed methodology based on MORBO, highlighting each component and its function throughout the process."}, {"title": "3.1 Bayesian Optimization", "content": "BO is a machine learning-driven optimization technique, intended for cases where the continuous objective function f(x) lacks an analytical form or is expensive to evaluate, and where computing the derivatives of first and second-order is not feasible [35]. BO has two main components: a Bayesian surrogate model for the objective function and an acquisition function (AF) to identify the next sampling location. The fundamental idea of BO methods is to use the"}, {"title": "3.1.1 Gaussian processes", "content": "A GP consists of a set of random variables that, for certain finite subsets, follow a joint Gaussian distribution [36]. Consequently, a GP characterizes a distribution over functions f(\u00b7) ~ GP(\u03bc(\u00b7), k(\u00b7, \u00b7)), where parameters are assigned to the mean \u03bc(\u00b7) and the kernel k(\u00b7,\u00b7). This distribution is defined for each pair of points x, x' \u2208 Rd and represents the covariance between them. Thus:\n$\\mu(x) = E [f(x)]$\n$k(x, x') = E [(f(x) \u2013 \\mu(x))(f(x') \u2013 \\mu(x'))]$\nStandard practice includes adjusting the kernel function's hyperparameters by maximizing the marginal likelihood across the training set, with full Bayesian methods being another viable option [38]."}, {"title": "3.1.2 Acquisition functions", "content": "Acquisition functions exploit the posterior mean and variance at each function point to compute a metric that indicates how beneficial it would be to sample at that point in the next iteration. The goal of an effective acquisition function is to balance exploration and exploitation.\nSeveral acquisition functions are commonly used in the literature, including the Upper Confidence Bound (UCB) [39], the Probability of Improvement (PI) [40], and the Expected Improvement (EI) [41]. UCB balances exploration and exploitation by combining the mean and variance of the posterior distribution. PI selects points to maximize the probability of achieving a better objective value than the current best. EI, the most widely employed acquisition function, evaluates points based on both the probability and the magnitude of improvement, aiming to find the point that provides the greatest expected enhancement to f.\nWhile the standard BO framework provides one new query point per iteration, there are advanced methods that extend BO to provide multiple candidate solutions for accelerated optimization. For instance, parallel EI (q-EI) evaluates improvement over a joint probability distribution of q points and selects the set of points that maximizes the expected improvement across these points [42]."}, {"title": "3.2 Multi-objective Bayesian Optimization", "content": "BO can be seamlessly extended to vector-valued functions by defining and optimizing a performance metric over sets, using EI to guide the search process across multiple objectives. Nonetheless, this adds complexity to the problem due to the numerous directions in which objectives can be improved [43]. In any multi-objective strategy, the main aim of MOBO is to find a set of points that represent the best trade-offs between m conflicting objectives, commonly referred to as the Pareto set Xp = X1,X2, \u2026, Xt, ;xi \u2208 X, along with the corresponding Pareto front (P), which is a set of solutions P = y1, y2,\u00b7\u00b7\u00b7\u2026, yt in the objective space, where yj = [yj1, yj2, ..., yjm]T = f(xj) + \u025b, and #;y\u0131y\u0131 > yj,l \u2260 j,;\u2200y\u0131 \u2208 Y C Rm [44]. Here, the symbol > signifies dominance, meaning that y\u0131's objective values are at least as good as those of yj, and better in at least one objective. In MOBO, P is approximated by a set P containing all non-dominated solutions in Df.\nTo maintain simplicity, this work models each objective function with independent GP priors. Each solution vector yi represents the noisy evaluation of the three objective functions, expressed as yi = [Ye(xi), Y\u0131(xi), yc(xi)], where Ye(xi) = fe(xi) + 8e, with analogous expressions for y\u0131 and yc."}, {"title": "3.2.1 Acquisition Functions for MOBO", "content": "The most commonly used AF in MOBO seeks to estimate the expected improvement in the area under P generated by a new point x and its corresponding posterior distribution, which is approximated using the hypervolume (HV) indicator. First introduced in [45], HV is one of the key unary indicators for evaluating the quality of a Pareto front approximation set. A significant advantage of this indicator is that it does not require prior knowledge of the Pareto front. Maximizing HV can lead to a highly effective and diverse Pareto front approximation set [46]. The HV indicator measures the volume of the subspace dominated by P, bounded below by a reference point r, and defined as:\n$HV(P) = \\Lambda_m (\\bigcup_{y \\in P} [r, y])$"}, {"title": "3.3 Multi-objective Regionalized Bayesian Optimization", "content": "As discussed before, MORBO optimizes diverse parts of the global Pareto frontier in parallel using a coordinated set of local TRs and defines a set of heuristic rules to expand, decrease, or even reset the TRs according to their success/failure at providing new candidate solutions. The implementation of the MORBO strategy requires setting seven new hyperparameters:\n\u2022 The number of Trust Regions NTR.\n\u2022 The number of Thomson samples nrs for sampling the posterior distribution of every TR's GP.\n\u2022 The initial TR length Linit.\n\u2022 The maximum and minimum TR lengths, denoted as Lmax and Lmin respectively.\n\u2022 The success/failure thresholds, Tsucc, Tfail respectively.\nTo initialize the TR, MORBO chooses the centers of the TR as the points in P with maximum hypervolume contribution (HVC), i.e., given a reference point r, HVC of a solution y on P is the reduction in HV if that point were to be removed. MORBO selects TR centers based on their HVCs in a sequential greedy fashion, excluding points already selected as the center for another TR [31]. The original MORBO approach uses the sample with the lowest amount of total constraint violation as a center in case there is no feasible point. However, considering that the addressed swine diet formulation problem has 28 zootechnical and nutritional restrictions, the feasible space is likely a considerably shrunken"}, {"title": "3.4 Optimality evaluation", "content": "The quality of a solution set in multi-objective optimization problems is typically assessed by evaluating the depiction of the Pareto front through four primary aspects. These aspects are:\n\u2022 Convergence: This measures how closely the solution set approaches the Pareto front, which, in this case, is measured through HV.\n\u2022 Cardinality: This assesses the number of elements within the solution set.\n\u2022 Diversity: This is crucial for understanding how well the set of non-dominated solutions spans the Pareto front, providing decision-makers with a range of different and valuable alternatives."}, {"title": "4 Experiments and results", "content": "The experiments are organized in four phases: I) Comparative analysis of convergence between MOBO and MORBO processes; II) Selection of MORBO hyperparameters; III) Assessment of the quality of MORBO solutions compared to the results achieved by [12, 20]. IV) evaluation of the effects of parallel configurations of MORBO to speed up the optimization process. In all experiments, the surrogate GP models used by both MOBO and MORBO methods employ an isotropic Mat\u00e9rn kernel, with its hyperparameters estimated by maximizing the log-likelihood based on the data. The Mat\u00e9rn kernel is a generalization of the exponentiated quadratic kernel, introducing an additional parameter that controls the smoothness of the resulting function. This kernel is particularly favored for high-dimensional problems [36].\nThe MOBO parameters (number of initial samples, MC samples, and iterations) are set according to the findings in [20]. During phase I, nrs is evaluated on the grid {512, 1024, 2048, 4096} to test its impact on the convergence properties of the method in swine diet design and to identify the minimum number needed to achieve convergence before reaching the Pareto set. To analyze the convergence properties of MORBO, the maximum number of iterations is set to k = 150, which is three times the value identified in [20] for MOBO to achieve an HV plateau. However, for the sake of comparison, from phase II onwards, k is set to 50 to ensure fair competition with MOBO, and the quality of the solutions is compared between MOBO and MORBO under similar conditions. The number of initial samples in MORBO is set to the same value used by MOBO (50) [20]. The values of NTR are explored within the set {2, 5, 8}, as larger values increase the variance of HV values obtained by MORBO. Additionally, the initial length of the trust regions Linit is examined among the values {0.1, 0.2, 0.4}, which cover a large percentage of the possible variables range.\nConsidering that the solution vectors are subject to a sum-to-one constraint (input variables are normalized), the success/failure thresholds and maximum and minimum values of the lengths of the TRs were set automatically according to the methods defined in [31]. Given the stochastic nature of some parts of MOBO and MORBO, all the experiments are repeated 30 times using different seeds to evaluate the stability of the results. Subsequently, during phases III and IV, the best set of hyperparameters is used to get the Pareto set and Pareto front approximation of the multiobjective swine diet design problem described in Section 2. The quality of the Pareto front approximation is evaluated in terms of HV, CD, and DIR. The number of reference vectors u required to estimate DIR was chosen as 78 after simple experimentation to find out how many reference vectors are required to get a stable metric value regarding the solution space of the diet design problem described in section 2. Since one of the main concerns about MOBO is its limitation in exploring the feasible space, different analyses are included to illustrate comparatively how MOBO and"}, {"title": "5 Conclusions", "content": "This work evaluated the use of a regionalized strategy to enhance the exploration and quality characteristics of the solutions provided by a multi-objective Bayesian optimization algorithm for diet design in the context of animal production. The setup consisted of a predefined three-objective optimization problem, including Lysine, Digestible Energy, and Cost, within a 17-dimensional input space representing ingredient proportions. This research advances the adaptation of BO methods to address food diet design problems, offering significant benefits for integrating multiple information sources that conventional metabolic-based methods cannot manage.\nResults indicated that MORBO demonstrates a more efficient exploration of the search space, as evidenced by UMAP projections of the Pareto set, distances between consecutive iterative solutions, and diversity indicators, even though its HV values are lower than those of standard MOBO. The regionalized strategy proved four times more effective in finding solutions that dominate the MFP, a notable improvement given the critical optimization budget constraints in animal nutrition diet design. Moreover, the greater diversity of the Pareto front approximation facilitates better decision-making when selecting a single solution for practical implementation. In contrast, the non-dominated solutions provided by MOBO were more similar to each other, reducing the quality of the Pareto front approximation. MORBO's diversity is also reflected in the ingredient proportions of the best solutions compared to those found by MOBO (see Appendix A here and in [20]).\nDespite these advantages, MOBO solutions achieved a higher average percentage improvement over MFP, likely due to MOBO's tendency to explore the search space boundaries. However, MORBO was eight times more effective in finding non-dominated solutions compared to MFP during the first 10 iterations, a crucial factor for deploying a solution in the swine diet design context. Additionally, experiments with batches of query candidate solutions showed that MORBO could accelerate without compromising solution quality during the first 20 iterations.\nFurther experiments using data from pig farms should be carried out to validate the results and findings of this work. Additionally, exploring complementary strategies to improve the quality of the Pareto front approximation would be beneficial. This could include investigating alternative kernel functions for the GP covariance priors. The complexity of selecting an appropriate kernel for the surrogate GP increases in high-dimensional search spaces, as the objective function may become heterogeneous. For instance, employing non-stationary kernels that can model objective functions constant over large regions of the search space would be particularly useful [23]. Implementing practical solutions for swine diet design also requires considering additional factors, such as incorporating contextual (non-controllable) variables [21] and handling data from various farms and fidelity levels [22]."}]}