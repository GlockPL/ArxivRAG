{"title": "SONAR: A Synthetic AI-Audio Detection Framework and Benchmark", "authors": ["Xiang Li", "Pin-Yu Chen", "Wenqi Wei"], "abstract": "Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using generative Artificial Intelligence (AI) technology have made it possible to generate high-quality and realistic human-like audio. This introduces significant challenges to distinguishing AI-synthesized speech from the authentic human voice and could raise potential issues of misuse for malicious purposes such as impersonation and fraud, spreading misinformation, deepfakes, and scams. However, existing detection techniques for AI-synthesized audio have not kept pace and often exhibit poor generalization across diverse datasets. In this paper, we introduce SONAR, a synthetic AI-Audio Detection Framework and Benchmark, aiming to provide a comprehensive evaluation for distinguishing cutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation dataset sourced from 9 diverse audio synthesis platforms, including leading TTS providers and state-of-the-art TTS models. It is the first framework to uniformly benchmark AI-audio detection across both traditional and foundation model-based deepfake detection systems. Through extensive experiments, we reveal the generalization limitations of existing detection methods and demonstrate that foundation models exhibit stronger generalization capabilities, which can be attributed to their model size and the scale and quality of pretraining data. Additionally, we explore the effectiveness and efficiency of few-shot fine-tuning in improving generalization, highlighting its potential for tailored applications, such as personalized detection systems for specific entities or individuals. Code and dataset are available at\nhttps://github.com/Jessegator/SONAR", "sections": [{"title": "Introduction", "content": "Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using Artificial Intelligence\n(AI) technology have made it possible to generate high-quality and realistic human-like audio\nefficiently [1, 2, 3, 4]. This introduces significant challenges in distinguishing AI-synthesized speech\nfrom the authentic human voice and could raise potential misuse for malicious purposes such as\nimpersonation and fraud, spreading misinformation, and scams. For example, a deep fake AI voice\nof the US President Joe Biden was recently utilized in robocalls to advise them against voting\u00b9,\ndemonstrating how deepfakes can significantly manipulate public opinions and influence presidential\nelections. In response to such risks, the US Federal Communications Commission (FCC) now deems\nrobot calls for election as illegal, which underscores the urgent need for enhanced detection of\nAI-synthesized audio.\nWhile TTS models are advancing rapidly, AI-synthesized audio detection techniques are not keeping\npace. First, previous studies [5, 6] have highlighted the lack of generalization and robustness in these\ndetection methods. Second, existing detection models [7, 6, 8, 9, 10] often take advantage of different"}, {"title": "Related work", "content": "Text-to-Speech synthesis. Human voice synthesis is a significant challenge in the field of AI.\nState-of-the-art TTS synthesis approaches such as VALLE [4], AudioBox [1], VoiceBox [11], Nat-\nuralSpeech3 [4], and YourTTS [3] have demonstrated the possibility of generating high-quality,\nhuman-realistic audio with generative models trained on large datasets. Current TTS models can\nbe classified into two primary categories: cascaded and end-to-end methods. Cascaded TTS mod-\nels [12, 13, 14] typically employ a pipeline involving an acoustic model and a vocoder utilizing mel\nspectrograms as intermediary representations. To address the limitations associated with vocoders,\nend-to-end TTS models [15, 16] have been developed to jointly optimize both the acoustic model\nand vocoder. In practical applications, it is preferable to customize TTS systems to generate speech\nin any voice with limited accessible data. Consequently, there is increasing interest in zero-shot\nmulti-speaker TTS techniques [17, 3, 2].\nAI-synthesized audio detection. Recent advancements in AI technology have significantly enhanced\nthe ability to generate high-quality and realistic audio, calling for an urgent need for more robust and\nreliable detection methods. Several datasets have been developed to support research in this area. The\nASVspoof challenges [18, 19, 20] are among the most notable, offering comprehensive datasets that\ncover a variety of attack vectors, including replay attacks, voice conversion, and directly synthesized\naudio. These resources aim to facilitate thorough evaluations of countermeasures against various\nspoofing techniques. In addition, newer datasets such as WaveFake [21] and LibriSeVoc [22] provide\nfake audio samples generated with state-of-the-art vocoders, offering diverse distributions to enhance\nthe development of deepfake audio detection systems. By comparison, the In-the-Wild dataset [6]\ntargets real-world applications by collecting deepfake audios from publicly accessible sources,\ncapturing the complexity and diversity of manipulations encountered in everyday environments.\nSimilarly, the SingFake dataset [6] focuses on the detection of synthetic singing voices, presenting"}, {"title": "Evaluation dataset generalization and collection", "content": "Leveraging a set of diverse and high-quality speech data synthesis APIs and models, we create\nan evaluation dataset for synthetic AI-audio detection. Our approach incorporates two strategies:\ndata generation and data collection. Our dataset includes AI-generated speech and audio from nine\ndistinct sources. We perform speech data generation using one cutting-edge TTS service provider,\nOpenAI, and two open-sourced APIs, xTTS [27] and AudioGen [28]. For speech data collection, we\nleverage six state-of-the-art TTS models including Seed-TTS [29], VALL-E [4], PromptTTS2 [30],\nNatural Speech3 [31], VoiceBox [11], FlashSpeech [2]. Table 1 presents the details of our dataset\ngenerated by different audio generation models. We next detail our methods of generating and\ncollecting these datasets.\nData generation. Our dataset generation involves OpenAI, xTTS, and AudioGen. Specifically,\nOpenAI currently provides voice choices from 6 different speakers. Using ChatGPT,we generate\n100 different text prompts of varying lengths for each speaker, resulting in a total of 600 synthetic\nspeech audios. xTTS supports synthetic speech generation given text prompts and reference speech.\nWe select 6 speakers from the LibriTTS dataset [32] as the reference speech and also generate 600\ntext prompts with ChatGPT for each speaker, resulting in 600 synthetic speech audios. AudioGen\ncan generate the corresponding environmental sound given a textual description of the acoustic\nscene. With AudioGen, we use ChatGPT to generate 100 text descriptions of the environment and\nbackground and obtain 100 AI-synthesized environmental sounds. Figure 1 (left) illustrates the data\ngeneration and collection process.\nData collection. To evaluate the effectiveness of various detection systems against the state-of-\nthe-art TTS models, we also collect fake speech audio from Seed-TTS, VALL-E, PromptTTS2,"}, {"title": "Benchmarking AI-Audio Detection Models", "content": "In this section, we first detail the model, dataset, and evaluation metrics setup for benchmarking.\nThen, we present the results of evaluating detection models on existing audio deepfake datasets to\nassess their generalizability across datasets. We next benchmark their detection performance on our\nproposed dataset and provide analysis for potential model generalization improvement."}, {"title": "Benchmarking setup", "content": "Model architectures. SONAR incorporates 11 models, including 5 state-of-the-art traditional audio\ndeepfake detection models featuring various levels of input feature abstraction and 6 foundation\nmodels. Specifically, for the former, SONAR includes (1) AASIST [7], which processes raw\nwaveform directly and utilizes graph neural networks and incorporates spectro-temporal attention\nmechanisms. (2) RawGAT-ST [9], which employs spectral and temporal sub-graphs along with a\ngraph pooling strategy. (3) RawNet2 [8], which is a hybrid model combining CNN and GRU.(4)\nSpectrogram(Spec.)+ResNet [6], which transforms the audio to linear spectrogram using a 512-\npoint Fast Fourier Transform (FFT) with a hop size of 10 ms. The spectrogram is then inputted\ninto ResNet18 [33]. (5) LFCC-LCNN [10], which converts audio into Linear-Frequency Cepstral\nCoefficients (LFCC) for input into a CNN model. Specifically, 60-dimensional LFCCs are extracted\nfrom each utterance frame, with frame length set to 20ms and hop size 10ms. It extracts speech\nembedding directly from raw audio. These models collectively cover a broad spectrum of feature\ntypes and architectures, facilitating a detailed examination of their performance in deepfake audio"}, {"title": "Results and analysis", "content": "How well can detection models generalize across datasets?\nWe first train all models on Wavefake training dataset and then evaluate the models on its own test\nset, LibriSeVoc test set, and In-the-wild dataset. Table 2 presents the evaluation results. Particularly,\nwe make the following interesting observations.\nSpeech foundation models exhibit stronger generalizability. As shown in Table 2, when evaluated\non the test set of Wavefake, all models demonstrate near-perfect performance across the three metrics.\nThis can be attributed to the similarity between the test set and the training data. However, when\ntested on the LibriSeVoc and In-the-wild datasets, models such as LFCC-LCNN, Spec.+ResNet,\nRawNet2, RawGATST, and AASIST struggle to generalize effectively. This performance gap\nindicates significant overfitting to the training data, despite these models being specifically designed\nfor audio deepfake detection tasks. In contrast, speech foundation models consistently display\nstronger generalizability. Notably, Wave2Vec2BERT achieves the highest generalizability, which\nmay be attributed to its large-scale and diverse pretraining data. Pretrained on 4.5 million hours of\nunlabeled audio in more than 143 languages, Wave2Vec2BERT benefits from both scale and diversity.\nThis suggests that a well-designed self-supervised model trained on diverse speech data can extract"}, {"title": "Results on SONAR dataset", "content": "We further evaluate all detection models on the proposed dataset. Table 3a, Table 3b, and Table 3c\npresent the accuracy, AUROC, and EER of different detection models on our proposed SONAR\ndataset as described in Sec 3.\nSpeech foundation models can better generalize on the SONAR dataset, but still not good\nenough. As presented in Table 3a, speech foundation models again exhibit better generalizability\non the fake audio samples generated by the latest TTS models. For instance, AASIST achieves\n0.6975 average accuracy across audios generated by cutting-edge TTS models, which is the best\nperformance among the traditional detection models. In contrast, speech foundation models Whisper-\nlarge, Wave2Vec2, HuBERT, and Wave2Vec2BERT achieve an average accuracy of 0.7322, 0.788,\n0.8789, and 0.8989, respectively, which is higher than AASIST by 3.47%, 9.05%, 18.14%, and\n20.14%, respectively. More specifically, even though Wave2Vec2BERT and HuBERT are only\nfew-tuned on Wavefake dataset, for PromptTTS2, VALL-E, VoiceBox, FalshSpeech, AudioGen,\nand xTTS, Wave2Vec2BERT can reach accuracies of 1.0, 0.9062, 0.9474, 0.9712, 0.9237, 0.97,\nand 0.9867, respectively, and HuBERT can achieve 1.0, 0.9158, 0.9712, 0.9407, 1.0 0.8767, and\n0.89, respectively, demonstrating their potential capability of extract more distinguishable features\ncompared to other models. It is also worth noting that Wave2Vec2BERT achieves an accuracy of\n0.9062 on NaturalSpeech3, while all other models can only reach that < 0.75.\nIt is still challenging for detection models to correctly classify synthesized audio samples,\nespecially those generated by the most advanced TTS service providers. While Wave2Vec2BERT\nachieves an overall average accuracy of 0.8989, it only reaches 0.6017 on Seed-TTS and 0.7833 on\nOpenAI. A similar pattern is also evident with HuBERT, Wave2Vec2, Whisper-large, and Whisper-\nsmall, which achieve just 0.5658, 0.4342, 0.29, and 0.1883 accuracy on OpenAI, respectively.\nThis performance disparity is likely due to OpenAI and Seed-TTS having more advanced model\narchitectures and being trained on proprietary, self-collected data, leading to higher-quality and\nmore realistic speech generation. We will explore potential strategies to enhance their detection\nperformance in Section 4.2.4. Overall, these results not only indicate that no single model consistently"}, {"title": "Can generalizability increase with model size?", "content": "Building on the observation that Whisper-large consistently outperforms Whisper-small, we extend\nour analysis with controlled experiments on the entire Whisper model family. Specifically, the\nWhisper family comprises five different model sizes: Whisper-tiny, Whisper-base, Whisper-small,"}, {"title": "On the effectiveness and efficiency of few-shot fine-tuning to improve generalization", "content": "Despite the challenges in generalizing across different datasets, we investigate whether there exist\nefficient solutions that can enhance models' detection performance on those challenging subsets from\nSONAR dataset. To this end, we conduct a case study on Wave2Vec2BERT and HuBERT, as these\nmodels perform relatively poorly on the OpenAI and SeedTTS datasets but demonstrate competitive\nperformance on other subsets. Specifically, we generate 100 additional fake audio samples using the\nOpenAI TTS API and randomly select another 100 fake audio samples from the SeedTTS test set for\nfew-shot fine-tuning. Our study yields several interesting findings.\nFigures 2a and 2b present the results of fine-tuning Wave2Vec2BERT and HuBERT using varying\nnumbers of samples from OpenAI. Before fine-tuning, Wave2Vec2BERT and HuBERT only achieve\naccuracies of 0.7833 and 0.5658, respectively. Notably, with only 10 shots of fake speech data,\nWave2Vec2BERT reaches an accuracy of approximately 0.97, while HuBERT's accuracy increases\nsignificantly to approximately 0.85. Importantly, the models' generalization to other datasets remains\nunchanged, demonstrating the effectiveness and efficiency of few-shot fine-tuning. However, as the\nnumber of fine-tuning samples increases, HuBERT's test accuracy on the WaveFake test set shows a\ndeclining trend, which is also observed for Wave2Vec2BERT.\nIt is important to note, however, that the efficiency and effectiveness of few-shot fine-tuning may vary\nacross different datasets. As illustrated in Figures 2c and 2d, which depict the fine-tuning results\nfor Wave2Vec2BERT and HuBERT on Seed-TTS, the improvement in accuracy is less pronounced\ncompared to the results on the OpenAI dataset. While the accuracy of both Wave2Vec2BERT and\nHuBERT does improve on Seed-TTS, the gains are not as significant as those observed for the\nOpenAI dataset. Additionally, the detection performance on other datasets decreases more noticeably\nwhen fine-tuning on Seed-TTS compared to OpenAI.\nThese findings suggest that the effectiveness of few-shot fine-tuning may depend on the specific\ncharacteristics of the dataset. Moreover, this also highlights its potential for tailored applications,\nsuch as personalized detection systems for a specific entity or individual, to enable more customized\nand practical applications."}, {"title": "Discussion", "content": "AI-synthetized audio detection methods must evaluated on diverse and advanced benchmarks.\nIn our evaluation using the proposed dataset, most models perform well on standard TTS tools but\nsuffer significant degradation when tested on the fake audios generated by the most advanced tool\nsuch as Voice Engine released by OpenAI. Therefore, we advocate for future research in audio\ndeepfake detection to prioritize benchmarking against the latest and most advanced TTS technologies,\nwhich will lead to more robust and reliable detectors, as relying on high detection rates from outdated\ntools may create a false sense of generalization. Additionally, there is an urgent need to develop\nlarger-scale training datasets comprising fake audio generated by cutting-edge TTS models to keep\npace with rapid advancements in TTS technology and mitigate associated risks.\nLimitations and future work. While our primary goal in proposing this dataset is to facilitate\ncomprehensive evaluation, it remains relatively small in size and is primarily focused on English. A\nmore in-depth analysis of detection performance across different languages and gender representations\nis crucial for a more comprehensive evaluation. These aspects are essential for future research to\nenhance the dataset's applicability and generalizability. For future work, we also plan to: (1)\nincorporate additional AI-audio detection models, including those targeting advanced audio editing\ntechniques designed to bypass detection systems; (2) explore innovative methods to further improve\ngeneralizability; and (3) address realistic challenges and risks in deploying the proposed method in\nreal-world scenarios, such as evaluating the robustness of models against common or adversarial\ncorruptions. These efforts will contribute to the development of more effective strategies to combat\nAI-generated audio threats.\nData license considerations.. Since our dataset is sourced from various models, each may be subject\nto different distribution licenses and usage restrictions. Throughout the data collection process, we\nensured strict adherence to all relevant usage policies. We have also made the dataset accessible to\neveryone, either directly via the provided link or indirectly through the original sources. However, as\nthese policies are frequently refined and updated, we will ensure that our published dataset remains\nin full compliance with the latest regulations. Additionally, we will reference the usage policies of\nthe respective API providers to ensure that users are informed of any potential restrictions."}, {"title": "Conclusion", "content": "In this paper, we presented SONAR, a framework providing a comprehensive evaluation for distin-\nguishing state-of-the-art AI-synthesized auditory content. SONAR introduces a novel evaluation\ndataset sourced from 9 diverse audio synthesis platforms, including leading TTS service providers\nand state-of-the-art TTS models. To the best of our knowledge, SONAR is the first platform that\nprovides uniform, comprehensive, informative, and extensible evaluation of deepfake audio detection\nmodels. Leveraging SONAR, we conducted extensive experiments to analyze the generalizability\nlimitations of current detection methods. We found that foundation models demonstrate stronger gen-\neralization capabilities, given their massive model size scale and pertaining data. We further explored\nthe potential of few-shot fine-tuning to improve generalization and demonstrated its efficiency and\neffectiveness. We envision that SONAR will serve as a valuable benchmark to facilitate research in\nAI-audio detection and highlight directions for further improvement."}, {"title": "Appendix", "content": "Broad Impacts\nSocietal Risks. The rapid advancement of AI-generated content (AIGC) in audio and speech poses\nsignificant societal risks as it becomes more prevalent in audio and speech generation. As our work\nin benchmarking AI-synthesized audio detection demonstrates, the line between AI-generated audio\nand human speech is increasingly blurring, making it difficult for individuals to distinguish between\nsynthetic and authentic voices. This raises serious concerns about spreading misinformation and\nfabricating narratives. AI-generated speeches could be used to impersonate public figures, spread\nfalse information, or even incite unrest by delivering provocative messages that appear authentic. For\nexample, deepfake audios of political figures can be created to falsely represent their opinions or\nstatements, potentially influencing public perception and affecting democratic processes.\nMoreover, these technologies could be exploited to damage reputations or cause legal issues for\nindividuals or organizations through fake endorsements or harmful statements. It is crucial for\nacademia and industry to develop robust detection methods and ethical guidelines to prevent misuse\nof this technology and to educate the public about its capabilities and associated risks.\nPositive Impacts. On the positive side, AI-synthesized audio/speech has the potential to revolutionize\ncontent creation in various sectors, including education, entertainment, and accessibility. In education,\nAI-synthesized audios and speeches enables production of customized content that meets diverse\nlearning needs and languages, improving access and inclusivity. For entertainment, they can offer\nnovel experiences by generating dynamic dialogues in games or virtual reality, enriching user\nengagement and creativity.\nFurthermore, AI-synthesized audios and speeches also enhances accessibility by producing speech in\nvarious languages or dialects, bridging communication gaps and making information more accessible\nto non-native speakers or those with liabilities. Additionally, the technology can help preserve lesser-spoken languages and dialects at risk of extinction by creating archives of AI-generated speeches and\nnarratives.\nIn conclusion, while AI-synthesized audios and speeches offer exciting opportunities for content\ncreation and accessibility, it is essential to address the ethical and societal challenges associated\nwith its use. Collaborative efforts among researchers, developers, and policymakers are crucial to\nleveraging AI-synthesized audio and speech benefits responsibly while mitigating its risks, ensuring\nthe technology serves to enhance human communication and creativity positively and responsibly."}, {"title": "Implementation Details", "content": "Table 7 presents the hyperparameters for training AASIST, RawNet2, RawGAT-ST, LCNN, and\nSpec.+ResNet. We train AASIST, RawNet2, and RawGAT-ST with a learning rate of 0.0001 and\nLCNN and Spec.+ResNet with a learning rate of 0.0003. The batch size for AASIST, RawNet2,\nRawGAT-ST, LCNN, and Spec.+ResNet are 64, 256, 32, 512, and 256, respectively. All input audios\nare resampled to a 16kHz sampling rate and converted into raw waveforms consisting of 64,000\nsamples (approximately 4 seconds). Audios longer than 4 seconds are randomly trimmed, while\nthose shorter than 4 seconds are repeated and padded to meet the 4-second duration.\nFor the foundation models, two linear layers are added after the encoder's output, with the hidden\nlayer dimension matching the dimension of the encoder's output. We fine-tune all foundation models\non the Wavefake training dataset for 3 epochs using the Adam optimizer with a learning rate of\n0.00001 and a weight decay of 0.0005.\nFor few-shot fine-tuning, models are fine-tuned for 30 epochs with a learning rate of 0.00001 and a\nweight decay of 0.00005."}]}