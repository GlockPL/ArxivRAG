{"title": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence Analysis for Biomedical Question Answering", "authors": ["Chang Zong", "Jian Wan", "Lei Zhang"], "abstract": "Current LLM-based approaches improve question answering performance by leveraging the internal reasoning abilities of models or incorporating external knowledge. However, when humans address professional problems, it is essential to explicitly analyze the multifaceted relationships from multiple pieces and diverse sources of evidence to achieve better answers. In this study, we propose a novel generative question answering framework for the biomedical domain, named EvidenceMap, which explicitly learns and incorporates evidence analysis with small language models (SLMs). The framework describes an evidence map for each question and fully utilizes an SLM to derive the representation of the supportive evaluation, the logical correlation, and the summarization of the related evidence, which facilitates an analysis-augmented generation with another SLM in an autoregressive way. Extensive experiments have shown that introducing an evidence analysis learning process can significantly outperform larger models and popular LLM reasoning methods.", "sections": [{"title": "1 Introduction", "content": "Answering specialized questions often requires reference to a wider range of professional information, such as academic literature and expert knowledge, compared to general domain inquiries (Louis et al., 2024; Singhal et al., 2025). Generally, as shown in Figure 1, humans require the integration of evidence from multiple sources to solve professional problems (Lu et al., 2022). This process involves a series of analytical steps, including evidence evaluation (Watts et al., 2009), evidence correlation (Chen et al., 2021), and evidence summarization (Hernes et al., 2015), which ultimately leads to a well-reasoned conclusion.\nTo address the generative question answering task supported by multiple pieces of evidence, ex-"}, {"title": "2 Definitions", "content": ""}, {"title": "2.1 Evidence Map Modeling", "content": "We construct an evidence map for each input question to facilitate reasoning during generation. An evidence map introduces three key types of information, including the evaluation of how the evidence supports the question ($R_{eval}$), the correlation between the evidence ($R_{cor}$), and the summarization of all the evidence ($E_{sum}$). Generally, an evidence map can be formulated as follows.\n$M = {Q, E_1, E_2, ...E_m, R_{eval}, R_{cor}, E_{sum}},$\n$E_{sum} = Agg(E_1, E_2, ...E_m),$\nwhere Q is the question to be addressed, Agg(\u00b7) is the aggregation function to derive the summarization $E_{sum}$, Es represents a set of evidence from the source s, which is denoted as follows.\n$E_s = {E_s^1, E_s^2, ..., E_s^{|Es|}},$\nwhere $E_s^i$ is the i-th evidence from the source s. Furthermore, the supportive evaluation and logical correlation of evidence can be described as below.\n$R_{eval} = [{E_s^i, Q, \u2192}], \u2200R_{eval} \u2208 R_{eval},$\n$R_{cor}^{i,j} = [{E_s^i, E_s^j, \u2194}], R_{cor}^{i,j} \u2208 R_{cor},$\nwhere the symbol \u2192 and \u2194 represent the direction of these two relationships, respectively."}, {"title": "2.2 Learning Generative QA with SLM", "content": "The objective of our task is to generate the best answers to biomedical questions by learning evidence analysis from the evidence map, which can be described as follows.\n$A^* = f_{glm} (Q, f_{plm}(\\theta^*), M),$\nwhere $f_{plm}(\\theta^*)$ is a tiny pre-trained model (e.g. BERT-base), $f_{glm}$ is a small generative model (e.g. Llama-3.2-3B) for providing the best answer A*. Then, our learning process is to find the optimized parameter set $\u03b8^*$, which leads to a minimum generative loss for all question-answer pairs, given the corresponding evidence map $E_{M_i}$ for each question $Q_i$. This can be denoted as below.\n$\\theta^* = arg min \\sum_i L (A_i, f_{glm} (Q_i, f_{plm} (\\theta), M_i)).$"}, {"title": "3 Framework", "content": "The EvidenceMap framework is illustrated in Figure 3, in which a pre-trained model is fine-tuned to learn the analysis of evidence to facilitate a small generative model that provides better responses. The three main steps are described below."}, {"title": "3.1 Evidence Acquisition", "content": "For each input question, we first gather relevant original evidence from multiple sources, including the intrinsic knowledge of an LLM and the external knowledge of academic papers. Specifically, for the evidence of an LLM, we ask GPT-40 (Hurst et al., 2024) to take advantage of its knowledge to provide a series of key points that support the question, described in a concise language (see Appendix A.2). In addition, we extract paper snippets from existing biomedical QA datasets such as BioASQ (Krithara et al., 2023) and PubMedQA (Jin et al., 2019), which have already been annotated by human experts, as evidence related to a question. Therefore, an evidence-augmented QA dataset is constructed, in which each question corresponds to one piece of LLM evidence $E_{lm}$ and multiple pieces of paper evidence from $E_p^1$ to $E_p^{|Ep|}$.\nThe input data can be described as follows.\n$Input = {Q, E_{lm}, E_p^1, E_p^2,..., E_p^{|E_p|}}.$"}, {"title": "3.2 Latent Evidence Analysis", "content": "The Evidence Map. As defined in Section 2.1, each input question, along with its associated evidence, is modeled as an evidence map. In addition to the evidence pieces themselves, the evidence map also incorporates the analysis among the evidence, including the evaluation of how each piece of evidence supports the question $R_{eval}$, the correlation between any two pieces of evidence $R_{cor}^{i,j}$, and the summarization of all the evidence $E_{sum}$.\nSLM-based Evidence Analysis. We prompt a small pre-trained language model to perform the multifaceted evidence analysis in latent space. Specifically, for evidence summarization, we first obtain the overall semantics of each evidence node, and then use a multi-layer perceptron (MLP) to aggregate and produce a summarized representation of the evidence, which is formulated as below.\n$H_{E_{sum}}^{(1)} = W_{E_{sum}}^{(1)}. X + b_{E_{sum}}^{(1)},$\n$E_{sum} = ReLU(H_{E_{sum}}^{(n)}),$\nwhere X is the concatenated feature of all evidence nodes, denoted as $[X_{lm}, X_{p_1},..., X_{p_{|E_p|}}]$, $H_{E_{sum}}^{(n)}$ is the embedding of the n-th layer. Inspired by Prompt-BERT (Jiang et al., 2022) and PromptEOL (Jiang et al., 2024), we prompt the SLM $f_{plm}$ and get the last hidden states of the model as the evidence feature $X_p$, which is described as follows.\n$X_p = f_{plm} (E_p^i, Prompt_{p_i};\\theta).$\nMeanwhile, the evaluation of how an evidence node supports the question Q is derived from the same SLM by using Prompt as below.\n$R_{eval}^{p_i} = f_{plm} (E_p^i, Q, Prompt_{eval}; \\theta),$\nwhere $R_{eval}^{p_i}$ indicates the supportive relationship between the evidence $E_p^i$ and the question Q. Sim-"}, {"title": "3.3 Analysis-Augmented Generation", "content": "With the result of the analysis of an evidence map, the final answer to the question is provided in an autoregressive way by a generative language model $f_{glm}$. We first convert all textual evidence and the question into an embedding $H_t$ using a text embedder, which is the first embedding layer of $f_{glm}$ as follows.\n$H_t = Embed([E_{lm}, E_p^1,..., E_p^{|E_p|}, Q]).$\nThen the evidence analysis results are projected into the same space with the textual embedding using a linear layer, as a soft prompt as follows.\n$H_a = W [E_{sum}, R_{eval}, R_{cor}] + b,$\nwhere $R_{eval}$ and $R_{cor}$ are the sets of all the supportive relationships $R_{eval}^{p_i}$ and logical correlations $R_{cor}^{(i,j)}$, respectively. We then prepend the analysis embedding $H_a$ to the textual embedding $H_t$ as the overall prompt to implement an analysis-augmented generation with the frozen language model $f_{glm}$, which is formulated as below.\n$A = f_{glm} (Prompt_{all};\\theta^*),$\nwhere $Prompt_{all} = [H_a, H_t]$, and $\u03b8^*$ is the frozen parameter set of $f_{glm}$."}, {"title": "3.4 Training Objective", "content": "The autoregressive training objective concentrates on training the parameter set of SLM $\u03b8$ to provide better analysis representations to predict subsequent tokens accurately by the generative language model. We calculate the probability of generating the i-th token of the target answer A with the overall prompt and all previous tokens as follows.\n$L = \\sum_{i=1}^{L} log (A_i | Prompt_{all}, A_{0:i-1};\\theta).$"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Implementation Details", "content": "In the step of learning evidence analysis, we employ DistilBERT (Sanh, 2019), a lighter but faster model distilled from BERT (Devlin et al., 2019), as the SLM to be updated. In the generation step, the 1B and 3B versions of Llama-3.2 (Dubey et al., 2024) are used to provide answers. We conduct experiments on two popular biomedical datasets, BioASQ (Krithara et al., 2023) and PubMedQA (Jin et al., 2019). During training, we set the learning rate as 5e-4, the batch size as 4, and the maximum number of paper evidence per question as 5. For each experiment, we train the pre-trained SLM for 8 epochs in a single NVIDIA A6000 GPU and evaluate the model on test sets."}, {"title": "4.2 Evaluation Metrics", "content": "Reference-based quality aims to evaluate the similarity between generated responses and reference answers, encompassing aspects of linguistic structure and semantics. ROUGE-L (Lin, 2004) and BERTScore (denoted as BERT-S) (Zhang et al., 2019) are popular metrics within this category. LLM-based accuracy (denoted as LLM-ACC) addresses the shortcomings of traditional metrics in assessing precision. Based on existing work such as RAGAS (Es et al., 2024) and Self-Evaluation (Huang et al., 2024), we prompt GPT-40 and also provide reference answers to assess the generated content, balancing accuracy and overall quality."}, {"title": "4.3 Main Results", "content": "In order to demonstrate the effectiveness of learning evidence analysis, we consider three configurations: 1) Inference-LLM: Using larger language models in general and biomedical domains, including BioMed-Llama (Pal and Sankarasubbu, 2024), Llama-3.1 (Dubey et al., 2024), and GPT-40-mini (Hurst et al., 2024). 2) Inference-SLM: Using a smaller frozen generative model with different reasoning strategies, such as RAG (Lewis et al., 2020; Gao et al., 2023), CoT (Wei et al., 2022) and RAT (Wang et al., 2024); 3) Tuning-SLM: Adapting the task by fine-tuning a generative model with LoRA (Hu et al., 2021) or updating the pre-trained SLM via a prompting tuning style (Lester et al., 2021; Li and Liang, 2021)."}, {"title": "4.4 Ablation Study on Evidence Analysis", "content": "We further investigate the effect of each part in the evidence analysis on overall performance, including supportive evaluation, logical correlation, and content summarization. We systematically remove one part at a time to observe its impact (the percentage of decline denoted as \u0394)."}, {"title": "4.5 Ablation Study on Answer Generation", "content": "We further analyze the impact of each module in the answer generation step. We denote the method that does not incorporate textual evidence embeddings as \"w/o TE\", and the one that eliminates the input of evidence analysis as \"w/o EA\". We replace the MLP projector with a simple linear layer and denote it as \"w/o Proj\". Performance results and variations are illustrated in Table 3. The results indicate that both the textual input and the analysis of the evidence have a significant impact on the final performance, which can significantly improve the quality of answers beyond the foundation of basic language model generation. In contrast, the projector used to align the evidence analysis results with the generative language model is just an auxiliary to the overall performance."}, {"title": "4.6 Impact of Pre-trained SLM", "content": "One advantage of our framework is that it requires training only a small pre-trained language model (typically with fewer than 200 million parameters). We further investigate the impact of various pre-trained language models on overall performance. We selected four representative pre-trained models, including DistilBERT (Sanh, 2019) used in our EvidenceMap, and three larger models (BERT-Base (Devlin et al., 2019), RoBERTa (Liu, 2019) and ModernBERT (Warner et al., 2024)).\nDespite having the fewest parameters, DistilBERT remarkably achieves strong performance. DistilBERT retrains most of the ability of BERT but is much lighter, which aligns well with our framework, further illustrating the training of a suitable pre-trained SLM can produce better results than just selecting larger models in our framework."}, {"title": "4.7 Impact of Evidence Input", "content": "The amount of textual evidence varies in the datasets we used. BioASQ has an average of 26.60 relevant papers, while PubMedQA has an average of only 4.84. To standardize the experimental settings and reduce computational overhead, as mentioned in Section 4.1, we set the maximum number of paper acquisitions at 5 and add evidence of LLM to enhance diversity. Meanwhile, as described in Section 3.1, we introduce the textual evidence for each question from an LLM to enhance the diversity of input. Therefore, in this relatively complex setting, the impact of the quantity and sources of evidence on performance remains worthy of exploration."}, {"title": "5 Case Study", "content": "In this section, we qualitatively analyze the effectiveness of our framework in addressing biomedical questions by presenting specific cases. Figure 5 presents two examples of utilizing evidence in distinct ways to formulate responses. In each case, a question is provided alongside relevant evidence, the standard answer, and the answers generated by RAG and our EvidenceMap.\nCase 1 illustrates a scenario of factual questions. We notice that the content of the standard answer is consistent with the evidence presented in the paper, both referencing the drug combination \"Pembrolizumab plus lenvatinib\", which is more accurate and comprehensive than the LLM evidence. The answer provided by EvidenceMap is incomplete, mentioning only the drug \"Pembrolizumab\". In contrast, the response generated by the RAG method references \"T-VEC\", which is an entirely different medication, which results in the dissemination of incorrect information and leads to a decrease in accuracy. This indicates that although our framework may provide incomplete information, analyzing the relationships between pieces of evidence can mitigate the hallucination problem in generative models and help prevent the generation of incorrect answers.\nCase 2 illustrates a scenario that addresses an explanatory question. By analyzing the standard answers, we can tell that a more complex analytical process is necessary. Accurately addressing this question requires reliance on information from a piece of paper evidence and the LLM evidence, while simultaneously avoiding certain elements from other papers and the misleading part of the LLM evidence. Specifically, the \"IL-12 family\" mentioned in the standard answer should be derived from the LLM evidence, which points out the \"interleukin-12 cytokine family\". Our EvidenceMap accurately captures this information element and presents it in the response. Meanwhile, the standard answer mentions \"immunoreaction and promoting endometrial cell proliferation\" a concept supported by evidence from the second paper. EvidenceMap precisely identifies this content and incorporates it into the response. In contrast, the answer generated by the RAG method overlooks these key pieces of information and utilizes extraneous content (\"T cells\") from other papers. This case illustrates that our approach effectively utilizes multiple and diverse sources of evidence by analyzing the relationships between them, thereby providing more accurate answers."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Long-Form Question Answering", "content": "Long-Form Question Answering (LFQA) is a task in natural language processing that aims to generate detailed, coherent, and informative answers to address open-ended questions posed by users. Traditional research on LFQA focuses primarily on the method of generative models. ELI5 (Fan et al., 2019) proposes an abstract generation model that integrates multi-task objective training. A following work (Krishna et al., 2021) designs a system based on sparse attention and contrastive learning to address poor relevance issues. Recent studies concentrate more on retrieval-augmented long-form generation. One work (Su et al., 2022) enhances the model with fine-grained key information relevant to the answers to improve the accuracy. WebCPM (Qin et al., 2023) focuses on retrieval and synthesis, proposing an interactive web-based search that enables real-time interaction with search engines. Furthermore, evaluating the quality of LFQA is an important area. A study on LFQA evaluation (Xu et al., 2023) employs a multi-dimensional assessment approach, focusing on aspects including factual, accuracy, and coherence. ASQA (Stelmakh et al., 2022) proposes a reliable metric that can provide clear correctness standards to address the ambiguous definition of answer quality. Compared to previous studies, our research primarily focuses on the scenario of multiple and diverse evidence in biomedical domain, aiming to efficiently address LFQA tasks by enabling SLMs to explicitly learn evidence analysis."}, {"title": "6.2 Textual Knowledge Augmentation", "content": "Textual evidence, as an external knowledge, can significantly enhance the quality of language model generation in professional domains. Methods for knowledge augmentation can be categorized into two types. The first is the augmentation during inference. RAG (Lewis et al., 2020), Self-RAG (Asai et al., 2023), and FLARE (Jiang et al., 2023) belong to this category, which improve generation effectiveness by retrieving relevant documents as factual knowledge and adjusting these texts during inference. Meanwhile, a model can reduce hallucinations from generation by incorporating reasoning knowledge, as demonstrated by CoT (Wei et al., 2022), ToT (Yao et al., 2024), and CoK (Wang et al., 2023a), a series of sequential or structured reasoning steps are generated and executed explicitly. Furthermore, methods with an augmentation mixture of factual and reasoning knowledge can further combine the strengths of both, such as RAT (Wang et al., 2024). Another type of augmentation is by tuning the model with domain text. Soft prompt tuning (Lester et al., 2021; Yang et al., 2023) uses implicit learnable vectors to replace explicit tokens as prompts to flexibly adapt to multiple downstream tasks. Instruction tuning (Wang et al., 2023b; Lin et al., 2023) improves models in knowledge-intensive tasks by constructing specific instruction datasets and learning the utilization of input information and contextual understanding. Compared to the aforementioned methods, our study proposes a novel knowledge augmentation process that prompts the inference of generative models by training the explicit evidence analysis abilities of a tiny pre-trained model."}, {"title": "7 Conclusion", "content": "In this work, we present a novel framework for generative biomedical question answering named EvidenceMap, which improves the ability of models to handle multiple and diverse evidence by explicitly training small language models in evidence analysis. The experimental results and cases show that EvidenceMap significantly improves the performance of the generative biomedical question answering by efficient utilization of evidence."}, {"title": "Limitations", "content": "We faithfully discuss the limitations that we would like to improve in future work.\nFirst, the current evaluation of our method focuses solely on public datasets in the biomedical domain. Future evaluations will continue to examine its performance in other biomedical datasets, while also conducting tests in additional professional domains.\nSecond, this study has only tested a limited number of generative language models from the Llama 3 series, and further testing is needed on additional small generative models around the 3B scale, such as Phi-3.5-mini (Abdin et al., 2024) and Qwen2.5-3B (Yang et al., 2024).\nThird, this work focuses primarily on the capabilities of SLMs while incorporating explicit evidence analysis. We will further explore the effects of learning evidence analysis on larger-scale pre-trained and generative models.\nFourth, the generalizability of learning evidence analysis skills and their potential transferability to other models remains an area for further exploration."}, {"title": "Ethical Considerations", "content": "Our EvidenceMap framework is not flawless due to limited model capabilities and the possibility of errors or outdated evidence. We test it on two public datasets, BioASQ and PubMedQA, and acknowledge that its effectiveness may be restricted to similar datasets or domains. Its performance in other scenarios is uncertain and poses potential risks. Therefore, it is crucial to exercise caution and verify the accuracy of the answers generated by the method."}, {"title": "A SLM and LLM Prompts", "content": ""}, {"title": "A.1 Prompts for Evidence Map", "content": "The prompt to obtain the representation of an individual evidence X from an SLM:\nEvidence: Ep\nThis evidence means:\nThe prompt to obtain the representation of the supportive evaluation of evidence $R_{eval}^{p_i}$ from an SLM:\nEvidence: $E_p^i$\nQuestion: Q\nHow much the evidence supports the question:"}, {"title": "A.2 Prompt for Acquiring LLM Evidence", "content": "To simulate the sources of evidence in real problem-solving scenarios, we introduced evidence texts based on the intrinsic knowledge of LLMs, thereby enriching the evidence content. This evidence is analyzed alongside the paper evidence to explore its impact on performance. We prompt GPT-40 to utilize its intrinsic knowledge to acquire evidence for each question as follows.\nSystem Input: You are an AI assistant that helps a human analyst discover evidence that supports the question.\nUser Input: Provide a concise summary that supports answering the question with your own knowledge. The summary should be evidence including key insights that can explain your answer to the question.\nQuestion: Q\nSummary:"}, {"title": "A.3 Prompt for LLM-based Metric", "content": "We prompt GPT-40 to provide both the accuracy score and the fluency score ranging from 0.0 to 1.0 as follows.\nSystem Input: You are an AI assistant tasked with evaluating the quality of the generated answers in terms of accuracy and fluency.\nUser Input: Please score the generated answer based on accuracy and fluency separately, comparing with the reference answer.\nThe scores should be a float number between 0.0 and 1.0, where 0.0 indicates the generated answer is completely incorrect or unable to understand, 1.0 means the generated answer is totally precise and fluent.\nYour output should be a dictionary such as {\"accuracy\": 0.8, \"fluency\": 0.9}, without any additional information.\nThe question, generated answer, and reference answer are give as below:\nQuestion: Q\nGenerated answer: A\nReference answer: A'\nYour output:"}, {"title": "B Details of Baseline Settings", "content": "We implement baseline methods by applying various knowledge augmentation strategies based on different pre-trained models for comparison with our framework. For the Inference-LLM setting, we provide evidence the same as EvidenceMap for each question to perform inference on GPT-40-mini and Llama3.1-8B, implementing a naive RAG approach. We also directly evaluate the adapted OpenBioLLM-8B, which have been tuned by domain datasets. For the Inference-SLM setting, we implement a CoT process based on Llama3.2-3B, providing the three phases of evidence analysis in natural language for the model to follow. In addition, a RAT framework is implemented, enabling the model to generate an initial CoT and optimize the thinking process based on acquired evidence and ultimately perform inference based on the adjusted reasoning process. For the Tuning-SLM setting, we perform prefix-tuning by providing the Llama3.2 model with a soft prompt, which can be optimized with the pre-trained BERT-like model. We also achieve instruction-tuning strategy by constructing evidence-augmented instructions as the dataset, and update Llama3.2-3B using LORA, which is denoted as RAG-LORA."}, {"title": "C Details of Biomedical Datasets", "content": "The number of samples and the average amount of evidence per sample in the training and test sets"}, {"title": "D Results of Language Fluency", "content": "We explore the fluency of language generated by our framework. As stated in Appendix A.3, an LLM is employed to score accuracy and fluency of the generated output at the same time. The average values of all samples from the test set are calculated and are shown in Table 6. The results indicate that our framework achieves the best and competitive fluency quality on BioASQ and PubMedQA, respectively."}]}