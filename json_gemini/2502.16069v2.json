{"title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents", "authors": ["Patrick Tser Jern Kon", "Jiachen Liu", "Qiuyi Ding", "Yiming Qiu", "Zhenning Yang", "Yibo Huang", "Jayanth Srinivasa", "Myungjin Lee", "Mosharaf Chowdhury", "Ang Chen"], "abstract": "Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4\u00d7 improvement in correctly answering experimental questions. Curie is open-sourced at https://github.com/Just-Curieous/Curie.", "sections": [{"title": "1. Introduction", "content": "Scientific research drives human progress, advancing medicine, technology, and our understanding of the universe. At the heart of this endeavor lies experimentation-a disciplined intellectual pursuit that transforms human curiosity, expressed through bold hypotheses, into verifiable knowledge. Experimentation thrives on creativity, as new ideas fuel discovery. Yet it also depends on rigor ensuring that research is methodologically sound and its findings are trustworthy (Armour et al., 2009; Gill & Gill, 2020). If science isn't rigorous, it's reckless (Hofseth, 2018).\nIn recent years, numerous works (Zhang et al., 2024b;\nKramer et al., 2023; Lu et al., 2024) leveraging large language models (LLMs) to automate scientific research have emerged (\u00a72.3). These solutions typically rely on ad-hoc prompt-based methods to mimic scientific workflows, which are prone to hallucination. While effective for creative tasks such as literature review and brainstorming, these approaches remain limited in their ability to support rigorous experimentation, a largely unexplored capability.\nMore specifically, rigorous experimentation (\u00a72.2) involves a methodical procedure that includes formulating hypotheses, designing experiments, executing controlled trials, and analyzing results. Achieving reliability at every step is essential to ensure that the results are accurate, reproducible, and scientifically meaningful. Finally, all procedures and results must be documented in a well-structured and interpretable manner, facilitating verification, reproducibility, and collaboration across the scientific community.\nTo meet these requirements, we propose Curie, an AI agent framework representing the first step toward rigorous and automated experimentation (\u00a73). As shown in Fig. 1, Curie takes an experimental question and relevant context (e.g., domain-specific knowledge or starter code) as input. The Architect Agent generates high-level experimental plans, coordinates the process, and reflects on findings to guide subsequent steps. Working in unison, our Technician Agents focus on carefully implementing and executing"}, {"title": "2. Background", "content": "2.1. Science Experimentation\nScientific experimentation often starts with researchers posing testable hypotheses based on their past results, domain knowledge, and intuition. This experimentation process then unfolds across three key stages: (1) Experimental Design, where researchers plan the controlled experiment by identifying variables, selecting methodologies, and outlining procedures to enhance reproducibility and validity. (2)\nExperiment Execution, where researchers set up the complex experiment environments and iteratively explore vast search spaces, and (3) Data Documentation and Analysis, where researchers systematically gather data, apply analytical techniques, and extract insights to validate or refine their hypotheses. This process is iterative, as insights gained from data analysis often lead to the refinement of hypotheses, leading to subsequent rounds of these three steps.\n2.2. Rigor in Experimentation\nRigor is essential in scientific research, ensuring systematic, precise, and reliable findings (Armour et al., 2009). If science isn't rigorous, it's reckless. (Hofseth, 2018). More precisely, experimental rigor is grounded in three core principles (Gill & Gill, 2020):\nMethodical Procedure: Experimentation must adhere to a principled and systematic methodology throughout all aforementioned stages, from hypothesis formulation to data"}, {"title": "3. Curie: Rigorous Experimentation", "content": "3.1. Architectural Overview\nAs shown in Fig. 3, Curie is composed of two types of LLM-based agents (an Architect Agent and a host of Tech-"}, {"title": "4. Experimentation Benchmark", "content": "We design a novel benchmark to stress test Curie's ability to automate experiments while enforcing rigor in the face"}, {"title": "4.1. Experiment-Centric Task Design", "content": "Instead of treating tasks as isolated problems with fixed solutions, we structure each task as a full experimental process. This means that tasks require hypothesis formation, iterative refinement, and rigorous validation, mirroring real-world experiment workflows rather than one-shot problem-solving.\nThe process begins with distilling high-level contributions from research papers (e.g., theoretical insights or empirical findings), or core system behaviors from open-source projects (e.g., the interplay between configuration parameters and performance). These insights are then translated into testable questions framed with explicit configurations, metrics, and expected outcomes. Ground truth data is derived from published results or official benchmarks provided by open-source projects. We use these findings to design tasks with three key components:\n1. Experiment Formulation: Each task specifies the (a)\nExperiment Question (e.g., optimizing performance, identifying relationships); (b) Practical constraints (e.g., resource budgets); (c) High-level Setup Requirements - Contextual details such as datasets, and experimental environments. This framing ensures that tasks are open-ended, requiring iterative exploration rather than one-shot solutions.\n2. Experimental Context: To ensure agents correctly interpret and execute tasks, the benchmark provides detailed context for each question. This includes: (a) Domain Knowl-"}, {"title": "4.2. Experimental Complexity", "content": "Experimental research varies in complexity across different dimensions. Our benchmark reflects this by structuring tasks into a hierarchical framework, assessing an agent's ability to handle increasingly sophisticated experimentation tasks. Unlike standard benchmarks that classify tasks by a single difficulty metric (e.g., easy, medium, hard), ours structures complexity along experiment-driven dimensions (detailed definitions in App. A):\n1). Design Complexity: The complexity of structuring an experiment (e.g., requiring hypothesis refinement), including defining the scope of exploration, selecting key variables, and structuring parameter spaces-ranging from discrete to continuous and from sparse to dense configurations.\n2). Experiment Setup Complexity: The difficulty of initializing and configuring the experimental environment, from simple predefined setups to intricate dependencies requiring multi-step configuration.\n3). Relationship Complexity: The interactions between variables and outcomes, from simple linear dependencies to complex non-monotonic relationships.\n4). Experiment Goal Complexity: The number of compet-"}, {"title": "5. Evaluation", "content": "We evaluate Curie using our experimentation benchmark, which consists of 46 research tasks spanning varying complexity levels across four key domains (\u00a74). To enhance statistical robustness, each task is executed independently for five trials for each of our baselines (below) and Curie, and we report the average performance across these trials. Apart from our main results described in \u00a75.1, our evaluation includes our case studies (Fig. 2 and App. B), and additional results (App. C).\nBaselines. We compare Curie with two state-of-the-art Al agents as our baselines: OpenHands (Wang et al., 2024c), a top-performing coding agent, and Microsoft Magentic (Fourney et al., 2024), a generalist multi-agent system. These baselines were selected because our benchmark primarily focuses on coding-related tasks within computer science, where both models demonstrate strong performance, with the expectation that Magentic, as a generalist multi-agent system, may be able to generalize to experimental tasks too. To ensure fairness, each baseline is provided with a detailed system prompt instructing them to act as a professional experimenter (see App. E.1). All baselines and Curie utilize GPT-40 as the underlying LLM.\nPerformance Metrics. We assess performance using four key metrics, each evaluated as a binary score per task, ensuring rigor at every stage of the experimentation process:\n1. Experiment Design \u2013 Ability to structure the high-level experiment plan to address the research question.\n2. Execution Setup \u2013 Ensuring that the generated code (experiment setup) is executable and produces consistent results across multiple runs.\n3. Implementation Alignment \u2013 Faithfulness of the experimental setup with the proposed plan.\n4. Conclusion Correctness - Accuracy in reflecting the"}, {"title": "5.1. Benchmark Performance", "content": "Table 2 shows aggregated success rates across all performance metrics and benchmark task domains.\nPerformance Breakdown By Metric. Across all four metrics, Curie consistently outperforms the baselines, demonstrating the benefits of our Experimental Rigor Engine in improving experimentation performance. (i) For experiment design correctness, all frameworks perform well since the current tasks are relatively straightforward and do not require iterative refinement. However, for more complex research tasks, Curie holds an advantage by dynamically refining hypotheses based on intermediate observations, whereas baselines rely on static planning. Our experimental knowledge module further enhances performance by improving recall and adaptation. (ii) For execution setup and implementation alignment, Curie demonstrates higher reliability, as Intra-ARM proactively validates and corrects execution steps, while Inter-ARM guarantees that we follow methodical task transitions. This results in particularly strong execution setup performance, from 66.7% to 92.7%. OpenHands (with 32.4% and 40.2%), as a coding-specialized agent, outperforms Magentic in this aspect. However, it still struggles with incomplete or erroneous setups, including getting stuck in loops, syntax errors, logic mistakes, and unresolved dependencies\u2014leading to execution failures in"}, {"title": "Performance Breakdown by Complexity.", "content": "Next, we analyze how each framework performs as we increase difficulty within each complexity dimension. Fig. 8 reports the aggregated performance score, computed as the average across all four evaluation metrics. We observe that increasing complexity difficulties across all dimensions correlates with a decline in performance across all agents. However, the rate of degradation varies across complexity types and agent architectures. Notably, Magentic consistently underperforms across all complexity levels, highlighting the robustness of our complexity-based difficulty scaling in distinguishing agent capabilities. Further, we observe a sublinear decline in performance as task complexity increases, suggesting that our hardest tasks could be made even more challenging. Despite this, our current results demonstrate Curie's capabilities, supported by our case studies. Exploring the limit of experimentation difficulty and its impact on model performance remains an open direction for future work.\nIn summary, our findings underscore the importance of rigorous evaluation across all stages of the experimentation process, shedding light on each framework's strengths and limitations under varying complexity conditions."}, {"title": "6. Conclusion and Future Work", "content": "We introduced Curie, an AI agent framework designed to automate and enhance the rigor of scientific experimentation. Central to its design is the Experimental Rigor Engine, which enforces methodical control, reliability, and interpretability. To assess Curie's effectiveness, we developed a new Experimentation Benchmark featuring real-world research-level challenges. Our empirical evaluation, comparing Curie against state-of-the-art AI agents, demonstrated its capability to automate rigorous experimentation.\nWe hope Curie inspires further advancements toward fully autonomous and rigorous experimentation in the era of AI agent-driven scientific research. Several open research challenges remain: For instance, adapting Curie for interdisciplinary research requires accommodating domain-specific methodologies, uncertainty control, and extended"}, {"title": "Impact Statement", "content": "We introduce Curie, an Al agent framework designed to ensure methodical control, execution reliability, and structured knowledge management throughout the experimentation lifecycle. We introduce a novel experimentation benchmark, spanning four key domains in computer science, to evaluate the reliability and effectiveness of Al agents in conducting scientific research. Our empirical results demonstrate that Curie achieves higher conclusion accuracy and execution reliability, significantly outperforming state-of-the-art AI agents.\nCurie has broad implications across multiple scientific disciplines, including machine learning, cloud computing, and database systems, where rigorous experimentation is essential. Beyond computer science, our framework has the potential to accelerate research in materials science, physics, and biomedical research, where complex experimental setups and iterative hypothesis testing are critical for discovery. By automating experimental workflows with built-in validation, Curie can enhance research productivity, reduce human error, and facilitate large-scale scientific exploration.\nEnsuring transparency, fairness, and reproducibility in AI-driven scientific research is paramount. Curie explicitly enforces structured documentation and interpretability, making experimental processes auditable and traceable. However, over-reliance on AI for scientific discovery raises concerns regarding bias in automated decision-making and the need for human oversight. We advocate for hybrid human-AI collaboration, where AI assists researchers rather than replacing critical scientific judgment.\nCurie lays the foundation for trustworthy AI-driven scientific experimentation, opening avenues for self-improving agents that refine methodologies through continual learning. Future research could explore domain-specific adaptations, enabling AI to automate rigorous experimentation in disciplines such as drug discovery, materials engineering, and high-energy physics. By bridging AI and the scientific method, Curie has the potential to shape the next generation of AI-powered research methodologies, driving scientific discovery at an unprecedented scale."}, {"title": "D. Benchmark Details.", "content": ""}]}