{"title": "BaichuanSEED: Sharing the Potential of Extensive Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline", "authors": ["Guosheng Dong", "Da Pan", "Yiding Sun", "Shusen Zhang", "Zheng Liang", "Xin Wu", "Yanjun Shen", "Fan Yang", "Haoze Sun", "Tianpeng Li", "Mingan Lin", "Jianhua Xu", "Yufan Zhang", "Xiaonan Nie", "Lei Su", "Bingning Wang", "Wentao Zhang", "Jiaxin Mao", "Zenan Zhou", "Weipeng Chen"], "abstract": "The general capabilities of Large Language Models (LLM) highly rely on the composition and selection on extensive pretraining datasets, treated as commercial secrets by several insti- tutions. To mitigate this issue, we open-source the details of a universally applicable data processing pipeline and validate its effective- ness and potential by introducing a competitive LLM baseline. Specifically, the data processing pipeline consists of broad collection to scale up and reweighting to improve quality. We then pretrain a 7B model BaichuanSEED with 3T tokens processed by our pipeline without any deliberate downstream task-related optimization, followed by an easy but effective super- vised fine-tuning stage. BaichuanSEED demon- strates consistency and predictability through- out training and achieves comparable perfor- mance on comprehensive benchmarks with sev- eral commercial advanced large language mod- els, such as Qwen1.5 and Llama3. We also conduct several heuristic experiments to dis- cuss the potential for further optimization on downstream tasks, such as mathematics and coding.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represented by ChatGPT and GPT-4 (OpenAI, 2023) have demon- strated exceptional performance across various do- mains, benefiting from the general capabilities through extensive pre-training on diverse datasets. Pre-training a foundation model is costly, typically requiring training on terabytes of tokens over mil- lions of GPU hours, making the process expen- sive and difficult to reproduce (Brown et al., 2020; Chowdhery et al., 2023; Zhu et al., 2024; Yang et al., 2023; Bai et al., 2023; Bi et al., 2024; Dubey et al., 2024). Many existing works focus on data selection, selecting high-quality data from mas- sive datasets, to decrease the loss, thereby reduc- ing computational costs and even enhancing down- stream task performance (Zhao et al., 2023; Al- balak et al., 2024). These methods mainly include heuristic rule-based selection (Sun et al., 2024; Chen et al., 2024a), model-based selection (Xie et al., 2023; Engstrom et al., 2024), dependent or relevant document detection and concatena- tion (Chen et al., 2023; Shi et al., 2024). Although data selection shows considerable po- tential, the high computational cost makes it chal- lenging to scale up. Additionally, the preliminary steps of data selection, data collection and reweight- ing are often treated as commercial secrets by com- panies (Touvron et al., 2023b; Bai et al., 2023), and even many open-source models that have released checkpoints do not disclose the details, hindering research advancement in this area. Additionally, many commercial models even over-optimize to excel in specific benchmarks (Zhou et al., 2023; Xu et al., 2024c; Deng et al., 2024), thus masking their true capabilities. Therefore, we believe that training a transparent LLM without any specific op- timization, and publicly sharing all its details will be beneficial for the community to understand the potential of data. To emphasize, BaichuanSEED is not a SOTA model. What really matters to us is that understanding of a pure model helps in rec- ognizing its actual strengths and weaknesses, the preliminary for evaluating the real impact of vari- ous optimization strategies. In this technical report, we first introduce a 7B foundation model, BaichuanSEED, with the sim- ilar model architecture of Baichuan2 (Yang et al., 2023) on 3T bilingual tokens from scratch and then supervised fine-tune it to obtain BaichuanSEED- SFT, a chat model. Following Falcon (Penedo et al., 2023), the pre-training data processing procedure was defined to involve extensive collection to scale up, followed by reweighting data points to set the sampling probability during pre-training. Specif- ically, we collect high-quality data widely avail- able on the internet while intentionally excluding synthetic and downstream benchmarks to ensure model purity. Subsequently, we design a global multi-granularity deduplication algorithm to ad- just the sampling weight of each data point during training. Here, we avoid fine-grained data selection intentionally, not to undermine its benefits but to fo- cus more on exploring the achievable limit through data collection and reweighting. We then conduct a straightforward yet effective fine-tuning process to endow the model with instruction-following ca- pabilities. In evaluation, BaichuanSEED exhibits consis- tency and predictability, indicating the robustness of its training process. Consistency is reflected in the uniform trends observed across pre-training and fine-tuning benchmarks as the pre-training. Predictability refers to the ability to forecast the model's future performance based on early check- points. We also evaluate our model against a se- ries of LLMs of similar scale on comprehensive benchmarks and several downstream tasks. The experimental results indicate that without exces- sive optimization, our model can already achieve performance comparable to advanced commercial models like Llama3 and Qwen-1.5 on several com- prehensive benchmarks, while our model still has room for improvement on some downstream tasks, especially on math. Finally, we explore the ex- tensive potential of our model by utilizing several common optimization methods, such as adjusting the ratio of high-knowledge-density data and opti- mizing for mathematical and coding abilities. We leave integrating these optimization methods into our model BaichuanSEED to construct a highly robust LLM for future work. Our main contributions are two-fold: (1) We pro- pose a data processing pipeline, including broad collection to scale up and reweighting to dedupli- cate and improve the data quality. (2) We train a competitive 7B LLM baseline from scratch with 3T data processed by the aforementioned pipeline, followed by a simple yet effective supervised fine-tuning. Our model is consistent and predictable, and achieves comparable performance on compre- hensive benchmarks with cutting-edge commercial LLMs without any deliberate optimization."}, {"title": "Model Architecture", "content": "BaichuanSEED first pre-train from scratch, fol- lowed by a Supervised Fine-Tuning (SFT) stage for alignment. Our model follows a Transformer decoder stack architecture similar to our previous version model Baichuan2 (Yang et al., 2023) and Llama (Touvron et al., 2023a,b). Specifically, our model comprises 32 layers with 32 attention heads. The hidden dimension size is 4,096, and the feed- forward layer size is 11,008. SwiGLU (Shazeer, 2020) is used as the activation function, while RM-SNorm (Zhang and Sennrich, 2019) is employed to enhance training stability. Rotary Positional Em- bedding (ROPE) (Su et al., 2024) is employed to model relative position dependencies."}, {"title": "Pre-training", "content": "In this section, we first provide a detailed overview of our efforts on pre-training data. We follow the pipeline of first broad collection from trusted sources to scale up, then re-weighting the data to obtain diverse and high-quality pre-training data. We then introduce the details of the training setups."}, {"title": "Pre-training Data", "content": "The principle for constructing our pre-training data encompasses two aspects: diversity and high- quality. In terms of diversity, we argue that pre- training data should cover a wide range of topics, linguistic styles and formats to ensure the model can adapt to diverse application scenarios, helping it learn comprehensive world knowledge and lin- guistic patterns. Regarding high-quality, we base on the guideline that documents with high produc- tion costs typically have higher quality, which may generally curated after a rigorous process of hu- man inspection and correction. Additionally, we address that a healthy data type distribution across the entire dataset should be ensured, while reduc- ing information redundancy and the proportion of low-quality data, to increase the knowledge density. To achieve the goal of diversity and high quality, our approach focuses on both scaling up and re- weighting the documents. Specifically, we first col- lect all accessible data from trusted sources to scale up via our self-constructed pipeline. Then, we em- ploy a global multi-granularity data deduplication strategy to re-weight the documents, enhancing the data quality while filtering out personal identifiable information (PII) and low-quality data. Following the aforementioned principles, we obtain over 10T tokens of pre-training data consequently."}, {"title": "Collection", "content": "Our collected data mostly originate from the public internet, including web pages, knowledge intensive data, code, and vertical domain data. To contin- uously collect the latest data for model iterations, a scraping and extracting pipeline is constructed to fetch up-to-date data automatically, enhancing the generalizability of our model in practice. In this section, we will introduce the details of data collection and extraction. Web Pages. We collect 94 publicly available batches of CommonCrawl spanning the past decade. We construct our WARC extraction and processing pipeline, considering the effectiveness and cost of the WET and WARC formats. Trafilatura (Barbaresi, 2021), a Python package and command-line tool designed to gather text from the web, is opted for further processing, in terms of the performance and cost among open-source web page parsing tools. CLD3 is utilized for document language identification. Knowledge Intensive Data. Apart from web pages, we further focus on Knowledge Intensive Data (KID), including books, academic papers, and technical reports. KID originates from diverse do- mains, and possesses a more balanced distribution compared to web pages. As illustrated in Figure 1, most web pages are in the domain of social science, with STEM comprising only 30%, whereas STEM in books and papers accounts for 35% and 72%, re- spectively. However, processing KID is costly, due to its typical PDF format and the high requirements for document parsing. Consequently, we employ a hybrid approach, utilizing both the open-source tool Nougat (Blecher et al., 2023) and business ser-"}, {"title": "Reweighting", "content": "Reweighting is a crucial stage in our pre-training data processing strategy, affecting the sampling pro- portion of each data point. We first introduce the deduplication operator, deciding whether to down- sample a data point to zero. Then a reasonable data mixture is confirmed, to balance the distribution of data from each domain. The details of dedu- plication and data mixture are introduced in the following parts. Deduplication. Deduplication is the first step of reweighting. Taking web pages as an example, we empirically found data with higher frequency is more likely tend to lead to a negative impact on foundation models. Therefore, we propose a global multi-granularity deduplication strategy, which will reduce the amount of raw data drastically. Via this strategy, we filter 88% of the tokens of our Com- monCrawl (CC) dataset. We present the details of our global multi-granularity deduplication strategy as follows, including document-level deduplication, sentence-level deduplication across documents, PII"}, {"title": "Document-level Deduplication Globally.", "content": "We take both the frequency and quality into account. Specifically, existing works split CC into several batches, deduplicate in each batch, and finally merge. While we perform document-level deduplication globally, mini- mizing the effect of data points with extremely high frequency. We perform exact match deduplication by utilizing the MD5 value as the key of each document. Then MinHash method is adopted for similar neighbourhood document deduplication. We keep the docu- ment with the longest text length for a cluster of similar documents."}, {"title": "Sentence-level Deduplication across Docu- ments.", "content": "Similar to document-level, we empir- ically remove sentences with extremely high frequency across documents, which mostly are meaningless junk data. For example, it may be website template breadcrumbs, stan- dard article openings and closings, and quoted passages. These frequently recurring fixed patterns may harm the model empirically. To eliminate biases in the raw data while ensuring diversity, we first split the documents into sen- tences, and then utilize Minhash with searched optimal hyperparameters at the sentence level across documents, making it more suitable for deduplication of short neighboring sentences. Several cases after deduplication in sentence- level can be found in Appendix B."}, {"title": "PII and Harmful Content Filtering.", "content": "PII and harmful content recognition and filtering is essential for reducing the model's toxic- ity. Recent works employ heuristic rule-based filtering strategies. However, these methods require rule curation manually and suffer from poor performance on scalability, precision, and recall, making them unsuitable for build- ing robust, and automated data processing pipelines. We adopt a combined approach of heuristic rules and model-based methods. Specifically, the former primarily includes fixed site templates, URL blacklists, regular expressions, word count and repeated n-gram character ratios, while the latter follows a it- erative distillation method. We first obtain a set of harmful data by GPT-4 and human an- notation as seeds to train a FastText (Joulin et al., 2017) model for classification, marking the PII, advertisement, copyright statements, and meaningless special characters as nega- tive samples. Hard samples are mined to iter- ative improve the previous model and results in a generalized harmful content classification model ultimately. The probability that data points classified as negative is taken as a proxy of quality scores for PII an harmful content filtering. Additionally, data with low language scores is filtered, mostly meaningless texts."}, {"title": "Data Mixture.", "content": "We conduct several empirical ex- periments to determine the data mixture of each"}, {"title": "Supervised Fine-tuning", "content": "In this section, we focus on the details of the stage of SFT, including our meticulously designed in- struction construction and our training settings."}, {"title": "SFT Data", "content": "During the stage of SFT, we utilize a self- constructed dataset for training. This comprehen- sive dataset includes mathematics, logical reason- ing, coding, creative writing, brainstorming, and multi-turn dialogues, with approximately 450K samples. Based on prior research (Bai et al., 2024; Touvron et al., 2023b), we make optimizations to the SFT data, primarily including clustering mas- sive data to enhance the diversity, synthesizing the instructions to increase the complexity. Addition- ally, we employ human annotations for part of data to improve the quality."}, {"title": "Training Details", "content": "We use a constant learning rate schedule with an initial learning rate of 2e-5, a batch size of 40, and a sequence length of 16K tokens. Each sam- ple includes three parts: system, which is optimal, prompt, and answer. To enhance training efficiency and ensure sequences were filled, we employ sam- ple packing for data concatenation and utilize Flash Attention 2 (Dao, 2023) to accelerate the training. Ultimately, we selected a 6-epoch checkpoint for validation. This is more than the usual number of epochs for SFT training, comparing to Llama2, which used only 2 epochs. Empirically, more epochs yields better results for small size models, especially when the amount of pre-training token is insufficient. This might not be the optimal solution, as the ideal number of epochs can vary with differ- ent checkpoints. Nevertheless, this does not affect our evaluation and comparison of the alignment performance of the foundation models. We discuss the SFT evaluation results in Section 5.1."}, {"title": "Evaluation", "content": "In this section, we first explore the scaling law of BaichuanSEED's downstream task performance with respect to the amount of training tokens. We elucidate the consistency and predictability dur- ing the training of BaichuanSEED. Furthermore, we evaluate our model and a series of 7B LLM baselines on several comprehensive benchmarks. We also assess the model's generalization abil- ity across a series of carefully selected down-"}, {"title": "Scaling Curves", "content": "Since we emphasize that appropriate data collec- tion and deduplication can lead to comparable per- formance, which requires significantly less manual effort and computational cost, compared to metic- ulous data selection during the pre-training phase. However, the absence of data selection may re- sult in inevitable noise. The process might raise concerns about whether training on a larger yet potentially lower-quality dataset can truly yield benefits or even have a negative impact such as drastic training fluctuation, and worse downstream performance. Consequently, we demonstrate the ro- bustness of BaichuanSEED from two dimensions: Consistency and Predictability. Consistency. We define consistency as the con- sistent trend of model capability growth with the number of training tokens before and after SFT. Specifically, model capability is reflected by met- rics including the loss on test sets, comprehensive benchmark performance of base models, and the SFT benchmark performance of SFT models. We illustrate trends of the metrics with respect to the amount of training tokens during pre-training, as shown in Figure 3. The comprehensive bench- marks consist of MMLU (Hendrycks et al., 2021a), CMMLU (Li et al., 2023a), MBPP (Austin et al., 2021), GSM8K (Cobbe et al., 2021). While the SFT benchmarks encompass FollowBench (Jiang et al., 2023) and SuperCLUE-Math6 (Xu et al., 2024b). The SFT benchmark results are evaluated on different checkpoints of the foundation model after the same SFT process. BaichuanSEED ex- hibits remarkable consistency across all evaluation benchmarks, indicating the model's generalization capability. It indicates that BaichuanSEED has not been specialized to optimize for a specific task or evaluation benchmark, retaining the potential to transfer to specific downstream tasks. Notably, some models may incorporate exces- sive synthetic data during the pre-training phase, creating a bubble impression of strong general- ization capabilities and hindering the potential to improve during the SFT phase, even affecting the instruction-following capability (Chen et al., 2024b). We argue that pre-training and SFT are both indispensable, and excessively exploiting SFT may hinder the community from being aware of the foundational models' capabilities. In contrast, our model exhibits consistent gains across both pre-training and SFT evaluation benchmarks, val- idating the decision to minimize using synthetic data during the pre-training phase. Additionally, the consistently increasing trend on several bench- marks validates the stability of our training strategy. Stability is highly essential in LLM development, where many approaches often sacrifice training ac- curacy to ensure training stability (Zhang et al., 2022; Zeng et al., 2023). Predictability. We define predictability as the ability to forecast the model's capabilities at later checkpoints based on its performance during the early stages of training. Predictability is especially crucial for developers, as having a forward-looking understanding of the model's performance trends during the early stages of training can facilitate rapid iteration and minimize unnecessary resource"}, {"title": "Comprehensive Benchmarks", "content": "costs. Taking BaichuanSEED's performance on MMLU as an example, we employ a logarithmic curve to fit the performance on MMLU with respect to the amount of training tokens, allowing us to pre- dict the potential of our model. Specifically, we take points from the first half for extrapolation and fit for the points in the latter half perfectly, demon- strating the predictability ability of our model, as shown in Figure 4."}, {"title": "Discussion", "content": "In this section, we conduct a series of experiments to explore the effectiveness of strategies employed in the training of BaichuanSEED. Firstly, thanks to the precise categorization of subjects as men- tioned in Section 3.1.3 to adjust the proportion of mathematics, we validate the potential of our model in mathematical tasks under the continued pre-training strategy. Secondly, we investigate the optimal ratio of high-density knowledge data dur- ing the pre-training phase using both cold start and continued pre-training methods on a 2B same- architecture model."}, {"title": "Effect of Knowledge Intensive Data", "content": "Knowledge Intensive Data refers to data that con- tains massive knowledge per token. We argue that LLMs trained on KID can achieve better perfor- mance in the same training steps (Hu et al., 2024; Li et al., 2023c). We extensively collect public Chinese and English academic papers and books as stated in Section 3.1.1. However, the intro- duction of KID inevitably down-sampling others, which may hinder the model from possessing com- prehensive world knowledge or language patterns. Therefore, determining the appropriate proportion of KID is crucial. We explore the optimal proportion of KID in two settings: training from scratch and continued pre-training. Considering the computational cost, we conduct experiments on a 2B model with the"}, {"title": "Potential on Mathematics", "content": "Taking mathematics reasoning as an example, we explore the potential of BaichuanSEED on a wide range of downstream tasks. Specifically, we first employ a classification model to perform a finer categorization of high-density knowledge data on subjects to better utilize this data. We then con- duct continued pre-training experiments by alter- ing the data proportions to verify the potential of our model. The experimental results indicate that a higher proportion of mathematics data significantly enhances the model's performance in mathematical downstream tasks, such as GSM8K and MATH, without compromising the comprehensive evalua- tion benchmarks. To address the details, we increase the propor- tion of mathematics-related data to 2% and 10%, conducting 300B token continued pre-training on a checkpoint with 1.44T tokens of BaichuanSEED, as illustrated in Figure 5. At the 2% proportion, we do not observe significant gains on the mathemat- ics benchmarks. However, when we aggressively raise the proportion to 10%, notable improvements are evident. To be specific, the 1.74T checkpoint shows a 7.4% and 65.6% increase on GSM8K and MATH respectively, with only a 0.3% fluctuation on MMLU, compared to the baseline. Therefore, we argue that knowledge intensive data has sub- stantial potential, especially when enhancing a lan- guage model's mathematical abilities. By search- ing optimal data proportions and employing cur- riculum learning strategies, the value of these data can be further captured. In addition, it validates the immense potential of our model in downstream tasks."}, {"title": "Related Works", "content": "Several institutions have published detailed techni- cal reports and open-sourced the weights of their cutting-edge models, to promote the development of the LLM field. Recently, numerous models made efforts on meticulous data filtering (AI@Meta, 2024; Penedo et al., 2023; Maini et al., 2024; Sol- daini et al., 2024), innovative architectural explo- ration (Ainslie et al., 2023; Jiang et al., 2024; Gu and Dao, 2023), and optimization for vertical down- stream tasks (DeepSeek-AI et al., 2024b; Shao et al., 2024). As a baseline model, BaichuanSEED focus on the exploration of the limits of model capabilities through data collection and dedupli- cation, avoiding any specific optimization. Nev- ertheless, our model achieves competitive perfor- mance on comprehensive benchmarks, comparable to many open-source commercial models of sim- ilar scales. These representative models include earlier versions of Baichuan (Yang et al., 2023), the Llama family (Touvron et al., 2023b; Dubey et al., 2024), the Qwen series (Bai et al., 2023), the Deepseek series (Dai et al., 2024; DeepSeek-AI et al., 2024a), and the Phi series (Li et al., 2023c; Abdin et al., 2024). Moreover, there are several initiatives dedicated to disclosing technical details, such as training datasets and code, including MAP- Neo (Zhang et al., 2024), OLMO (Groeneveld et al., 2024), Pythia (Biderman et al., 2023), and Am- ber (Liu et al., 2023)."}, {"title": "Conclusions", "content": "This technical report introduces BaichuanSEED, a pure yet competitive 7 billion parameter large language model trained from scratch to explore its limits without elaborate data selection and spe- cific downstream task optimization. We pre-train the model on 3 trillion tokens of high-quality data and supervised fine-tune it with a straightforward yet effective process. Guided by the pretraining principles that collection to scale up and reweight- ing to improve data quality, our model achieves comparable performance with leading commercial models on several comprehensive benchmarks. We also conduct heuristic experiments to explore the model's potential for further optimization in down- stream tasks. All training details are publicly avail- able, with the hope that our technical report would benefit the community to further unveil the skyline of data on large language models."}, {"title": "Case Study on Global Multi-granularity Deduplication", "content": "To display the performance of our proposed global multi-granularity deduplication intuitively in Sec- tion 3.1.2, we provide several cases before and after the deduplication, as shown in Figure 6. In terms of sentence deduplication, we filter from two dimensions: frequency and quality. On the one hand, reducing the frequency of repeated sentences, which may mitigate the repetition generation. On the other hand, it can avoid low-quality sentences being learnt by the model. These two terms are in- dispensable and complementary to each other. We conclude the characteristics of the removed sen- tences as follows, while filtering out these patterns may improve the accuracy and recall of harmful and toxic contents:\n\u2022 Advertisements. SEO strategies are adopted by advertisements to increase visibility, such as creating spam sites and utilizing various trending long-tail keywords to mislead search engines. The normal popular contents are in- terleaved with the advertisements, resulting in lots of template sentences. Without deduplica- tion, relying solely on the quality score is not sufficient to filter them out.\n\u2022 Texts for dissemination and interactivity. It is widely utilized in web pages, which is one of the most common patterns. While it is usually unrelated to the main idea of the documents.\n\u2022 Copyrights and PII. These similar patterns mostly appear in news, books. It may address privacy concerns, and aggravate the LLM rep- etition generation issue.\n\u2022 Residual HTML formats and watermarks. Inappropriate custom parsing website tem- plates may address this issue."}]}