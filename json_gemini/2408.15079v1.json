{"title": "BaichuanSEED: Sharing the Potential of Extensive Data Collection and\nDeduplication by Introducing a Competitive Large Language Model\nBaseline", "authors": ["Guosheng Dong", "Da Pan", "Yiding Sun", "Shusen Zhang", "Zheng Liang", "Xin Wu", "Yanjun Shen", "Fan Yang", "Haoze Sun", "Tianpeng Li", "Mingan Lin", "Jianhua Xu", "Yufan Zhang", "Xiaonan Nie", "Lei Su", "Bingning Wang", "Wentao Zhang", "Jiaxin Mao", "Zenan Zhou", "Weipeng Chen"], "abstract": "The general capabilities of Large Language\nModels (LLM) highly rely on the composition\nand selection on extensive pretraining datasets,\ntreated as commercial secrets by several insti-\ntutions. To mitigate this issue, we open-source\nthe details of a universally applicable data\nprocessing pipeline and validate its effective-\nness and potential by introducing a competitive\nLLM baseline. Specifically, the data processing\npipeline consists of broad collection to scale up\nand reweighting to improve quality. We then\npretrain a 7B model BaichuanSEED with 3T\ntokens processed by our pipeline without any\ndeliberate downstream task-related optimiza-\ntion, followed by an easy but effective super-\nvised fine-tuning stage. BaichuanSEED demon-\nstrates consistency and predictability through-\nout training and achieves comparable perfor-\nmance on comprehensive benchmarks with sev-\neral commercial advanced large language mod-\nels, such as Qwen1.5 and Llama3. We also\nconduct several heuristic experiments to dis-\ncuss the potential for further optimization on\ndownstream tasks, such as mathematics and\ncoding.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represented by\nChatGPT and GPT-4 (OpenAI, 2023) have demon-\nstrated exceptional performance across various do-\nmains, benefiting from the general capabilities\nthrough extensive pre-training on diverse datasets.\nPre-training a foundation model is costly, typically\nrequiring training on terabytes of tokens over mil-\nlions of GPU hours, making the process expen-\nsive and difficult to reproduce (Brown et al., 2020;\nChowdhery et al., 2023; Zhu et al., 2024; Yang\net al., 2023; Bai et al., 2023; Bi et al., 2024; Dubey\net al., 2024). Many existing works focus on data\nselection, selecting high-quality data from mas-\nsive datasets, to decrease the loss, thereby reduc-\ning computational costs and even enhancing down-\nstream task performance (Zhao et al., 2023; Al-\nbalak et al., 2024). These methods mainly include\nheuristic rule-based selection (Sun et al., 2024;\nChen et al., 2024a), model-based selection (Xie\net al., 2023; Engstrom et al., 2024), dependent\nor relevant document detection and concatena-\ntion (Chen et al., 2023; Shi et al., 2024).\nAlthough data selection shows considerable po-\ntential, the high computational cost makes it chal-\nlenging to scale up. Additionally, the preliminary\nsteps of data selection, data collection and reweight-\ning are often treated as commercial secrets by com-\npanies (Touvron et al., 2023b; Bai et al., 2023), and\neven many open-source models that have released\ncheckpoints do not disclose the details, hindering\nresearch advancement in this area. Additionally,\nmany commercial models even over-optimize to\nexcel in specific benchmarks (Zhou et al., 2023;\nXu et al., 2024c; Deng et al., 2024), thus masking\ntheir true capabilities. Therefore, we believe that\ntraining a transparent LLM without any specific op-\ntimization, and publicly sharing all its details will\nbe beneficial for the community to understand the\npotential of data. To emphasize, BaichuanSEED\nis not a SOTA model. What really matters to us\nis that understanding of a pure model helps in rec-\nognizing its actual strengths and weaknesses, the\npreliminary for evaluating the real impact of vari-\nous optimization strategies.\nIn this technical report, we first introduce a 7B\nfoundation model, BaichuanSEED, with the sim-\nilar model architecture of Baichuan2 (Yang et al.,\n2023) on 3T bilingual tokens from scratch and then\nsupervised fine-tune it to obtain BaichuanSEED-\nSFT, a chat model. Following Falcon (Penedo et al.,\n2023), the pre-training data processing procedure\nwas defined to involve extensive collection to scale\nup, followed by reweighting data points to set the\nsampling probability during pre-training. Specif-\nically, we collect high-quality data widely avail-\nable on the internet while intentionally excluding\nsynthetic and downstream benchmarks to ensure\nmodel purity. Subsequently, we design a global\nmulti-granularity deduplication algorithm to ad-\njust the sampling weight of each data point during\ntraining. Here, we avoid fine-grained data selection\nintentionally, not to undermine its benefits but to fo-\ncus more on exploring the achievable limit through\ndata collection and reweighting. We then conduct\na straightforward yet effective fine-tuning process\nto endow the model with instruction-following ca-\npabilities.\nIn evaluation, BaichuanSEED exhibits consis-\ntency and predictability, indicating the robustness\nof its training process. Consistency is reflected in\nthe uniform trends observed across pre-training\nand fine-tuning benchmarks as the pre-training.\nPredictability refers to the ability to forecast the\nmodel's future performance based on early check-\npoints. We also evaluate our model against a se-\nries of LLMs of similar scale on comprehensive\nbenchmarks and several downstream tasks. The\nexperimental results indicate that without exces-\nsive optimization, our model can already achieve\nperformance comparable to advanced commercial\nmodels like Llama3 and Qwen-1.5 on several com-\nprehensive benchmarks, while our model still has\nroom for improvement on some downstream tasks,\nespecially on math. Finally, we explore the ex-\ntensive potential of our model by utilizing several\ncommon optimization methods, such as adjusting\nthe ratio of high-knowledge-density data and opti-\nmizing for mathematical and coding abilities. We\nleave integrating these optimization methods into\nour model BaichuanSEED to construct a highly\nrobust LLM for future work.\nOur main contributions are two-fold: (1) We pro-\npose a data processing pipeline, including broad\ncollection to scale up and reweighting to dedupli-\ncate and improve the data quality. (2) We train a\ncompetitive 7B LLM baseline from scratch with\n3T data processed by the aforementioned pipeline,\nfollowed by a simple yet effective supervised fine-\ntuning. Our model is consistent and predictable,\nand achieves comparable performance on compre-\nhensive benchmarks with cutting-edge commercial\nLLMs without any deliberate optimization."}, {"title": "Model Architecture", "content": "BaichuanSEED first pre-train from scratch, fol-\nlowed by a Supervised Fine-Tuning (SFT) stage\nfor alignment. Our model follows a Transformer\ndecoder stack architecture similar to our previous\nversion model Baichuan2 (Yang et al., 2023) and\nLlama (Touvron et al., 2023a,b). Specifically, our\nmodel comprises 32 layers with 32 attention heads.\nThe hidden dimension size is 4,096, and the feed-\nforward layer size is 11,008. SwiGLU (Shazeer,\n2020) is used as the activation function, while RM-\nSNorm (Zhang and Sennrich, 2019) is employed\nto enhance training stability. Rotary Positional Em-\nbedding (ROPE) (Su et al., 2024) is employed to\nmodel relative position dependencies."}, {"title": "Pre-training", "content": "In this section, we first provide a detailed overview\nof our efforts on pre-training data. We follow\nthe pipeline of first broad collection from trusted\nsources to scale up, then re-weighting the data to\nobtain diverse and high-quality pre-training data.\nWe then introduce the details of the training setups."}, {"title": "Pre-training Data", "content": "The principle for constructing our pre-training data\nencompasses two aspects: diversity and high-\nquality. In terms of diversity, we argue that pre-\ntraining data should cover a wide range of topics,\nlinguistic styles and formats to ensure the model\ncan adapt to diverse application scenarios, helping\nit learn comprehensive world knowledge and lin-\nguistic patterns. Regarding high-quality, we base\non the guideline that documents with high produc-\ntion costs typically have higher quality, which may\ngenerally curated after a rigorous process of hu-\nman inspection and correction. Additionally, we\naddress that a healthy data type distribution across\nthe entire dataset should be ensured, while reduc-\ning information redundancy and the proportion of\nlow-quality data, to increase the knowledge density.\nTo achieve the goal of diversity and high quality,\nour approach focuses on both scaling up and re-\nweighting the documents. Specifically, we first col-\nlect all accessible data from trusted sources to scale\nup via our self-constructed pipeline. Then, we em-\nploy a global multi-granularity data deduplication\nstrategy to re-weight the documents, enhancing the\ndata quality while filtering out personal identifiable\ninformation (PII) and low-quality data. Following"}, {"title": "Reweighting", "content": "Reweighting is a crucial stage in our pre-training\ndata processing strategy, affecting the sampling pro-\nportion of each data point. We first introduce the\ndeduplication operator, deciding whether to down-\nsample a data point to zero. Then a reasonable data\nmixture is confirmed, to balance the distribution\nof data from each domain. The details of dedu-\nplication and data mixture are introduced in the\nfollowing parts.\nDeduplication. Deduplication is the first step of\nreweighting. Taking web pages as an example, we\nempirically found data with higher frequency is\nmore likely tend to lead to a negative impact on\nfoundation models. Therefore, we propose a global\nmulti-granularity deduplication strategy, which will\nreduce the amount of raw data drastically. Via this\nstrategy, we filter 88% of the tokens of our Com-\nmonCrawl (CC) dataset. We present the details of\nour global multi-granularity deduplication strategy\nas follows, including document-level deduplication,\nsentence-level deduplication across documents, PII\nand harmful content filtering.\n\u2022 Document-level Deduplication Globally.\nWe take both the frequency and quality into\naccount. Specifically, existing works split\nCC into several batches, deduplicate in each\nbatch, and finally merge. While we perform\ndocument-level deduplication globally, mini-\nmizing the effect of data points with extremely\nhigh frequency. We perform exact match\ndeduplication by utilizing the MD5 value as\nthe key of each document. Then MinHash\nmethod is adopted for similar neighbourhood\ndocument deduplication. We keep the docu-\nment with the longest text length for a cluster\nof similar documents.\n\u2022 Sentence-level Deduplication across Docu-\nments. Similar to document-level, we empir-\nically remove sentences with extremely high\nfrequency across documents, which mostly\nare meaningless junk data. For example, it\nmay be website template breadcrumbs, stan-\ndard article openings and closings, and quoted\npassages. These frequently recurring fixed\npatterns may harm the model empirically. To\neliminate biases in the raw data while ensuring\ndiversity, we first split the documents into sen-\ntences, and then utilize Minhash with searched\noptimal hyperparameters at the sentence level\nacross documents, making it more suitable for\ndeduplication of short neighboring sentences.\nSeveral cases after deduplication in sentence-\nlevel can be found in Appendix B.\n\u2022 PII and Harmful Content Filtering. PII\nand harmful content recognition and filtering\nis essential for reducing the model's toxic-\nity. Recent works employ heuristic rule-based\nfiltering strategies. However, these methods\nrequire rule curation manually and suffer from\npoor performance on scalability, precision,\nand recall, making them unsuitable for build-\ning robust, and automated data processing\npipelines. We adopt a combined approach\nof heuristic rules and model-based methods.\nSpecifically, the former primarily includes\nfixed site templates, URL blacklists, regular\nexpressions, word count and repeated n-gram\ncharacter ratios, while the latter follows a it-\nerative distillation method. We first obtain a\nset of harmful data by GPT-4 and human an-\nnotation as seeds to train a FastText (Joulin"}, {"title": "Data Mixture", "content": "We conduct several empirical ex-\nperiments to determine the data mixture of each"}, {"title": "Supervised Fine-tuning", "content": "In this section, we focus on the details of the stage\nof SFT, including our meticulously designed in-\nstruction construction and our training settings."}, {"title": "SFT Data", "content": "During the stage of SFT, we utilize a self-\nconstructed dataset for training. This comprehen-\nsive dataset includes mathematics, logical reason-\ning, coding, creative writing, brainstorming, and\nmulti-turn dialogues, with approximately 450K\nsamples. Based on prior research (Bai et al., 2024;\nTouvron et al., 2023b), we make optimizations to\nthe SFT data, primarily including clustering mas-\nsive data to enhance the diversity, synthesizing the\ninstructions to increase the complexity. Addition-\nally, we employ human annotations for part of data\nto improve the quality."}, {"title": "Training Details", "content": "We use a constant learning rate schedule with an\ninitial learning rate of 2e-5, a batch size of 40,\nand a sequence length of 16K tokens. Each sam-\nple includes three parts: system, which is optimal,\nprompt, and answer. To enhance training efficiency\nand ensure sequences were filled, we employ sam-\nple packing for data concatenation and utilize Flash\nAttention 2 (Dao, 2023) to accelerate the training.\nUltimately, we selected a 6-epoch checkpoint for\nvalidation. This is more than the usual number of\nepochs for SFT training, comparing to Llama2,\nwhich used only 2 epochs. Empirically, more\nepochs yields better results for small size models,\nespecially when the amount of pre-training token is\ninsufficient. This might not be the optimal solution,\nas the ideal number of epochs can vary with differ-\nent checkpoints. Nevertheless, this does not affect\nour evaluation and comparison of the alignment\nperformance of the foundation models. We discuss\nthe SFT evaluation results in Section 5.1."}, {"title": "Evaluation", "content": "In this section, we first explore the scaling law of\nBaichuanSEED's downstream task performance\nwith respect to the amount of training tokens. We\nelucidate the consistency and predictability dur-\ning the training of BaichuanSEED. Furthermore,\nwe evaluate our model and a series of 7B LLM\nbaselines on several comprehensive benchmarks.\nWe also assess the model's generalization abil-\nity across a series of carefully selected down-"}, {"title": "Scaling Curves", "content": "Since we emphasize that appropriate data collec-\ntion and deduplication can lead to comparable per-\nformance, which requires significantly less manual\neffort and computational cost, compared to metic-\nulous data selection during the pre-training phase.\nHowever, the absence of data selection may re-\nsult in inevitable noise. The process might raise\nconcerns about whether training on a larger yet\npotentially lower-quality dataset can truly yield\nbenefits or even have a negative impact such as\ndrastic training fluctuation, and worse downstream\nperformance. Consequently, we demonstrate the ro-\nbustness of BaichuanSEED from two dimensions:\nConsistency and Predictability.\nConsistency. We define consistency as the con-\nsistent trend of model capability growth with the\nnumber of training tokens before and after SFT.\nSpecifically, model capability is reflected by met-\nrics including the loss on test sets, comprehensive\nbenchmark performance of base models, and the\nSFT benchmark performance of SFT models. We\nillustrate trends of the metrics with respect to the\namount of training tokens during pre-training, as\nshown in Figure 3. The comprehensive bench-\nmarks consist of MMLU (Hendrycks et al., 2021a),\nCMMLU (Li et al., 2023a), MBPP (Austin et al.,\n2021), GSM8K (Cobbe et al., 2021). While the\nSFT benchmarks encompass FollowBench (Jiang\net al., 2023) and SuperCLUE-Math6 (Xu et al.,\n2024b). The SFT benchmark results are evaluated\non different checkpoints of the foundation model\nafter the same SFT process. BaichuanSEED ex-\nhibits remarkable consistency across all evaluation\nbenchmarks, indicating the model's generalization\ncapability. It indicates that BaichuanSEED has not\nbeen specialized to optimize for a specific task or\nevaluation benchmark, retaining the potential to\ntransfer to specific downstream tasks.\nNotably, some models may incorporate exces-\nsive synthetic data during the pre-training phase,\ncreating a bubble impression of strong general-\nization capabilities and hindering the potential\nto improve during the SFT phase, even affecting\nthe instruction-following capability (Chen et al.,\n2024b). We argue that pre-training and SFT are\nboth indispensable, and excessively exploiting SFT\nmay hinder the community from being aware of\nthe foundational models' capabilities. In contrast,\nour model exhibits consistent gains across both\npre-training and SFT evaluation benchmarks, val-\ndating the decision to minimize using synthetic\ndata during the pre-training phase. Additionally,\nthe consistently increasing trend on several bench-\nmarks validates the stability of our training strategy.\nStability is highly essential in LLM development,\nwhere many approaches often sacrifice training ac-\ncuracy to ensure training stability (Zhang et al.,\n2022; Zeng et al., 2023).\nPredictability. We define predictability as the\nability to forecast the model's capabilities at later\ncheckpoints based on its performance during the\nearly stages of training. Predictability is especially\ncrucial for developers, as having a forward-looking\nunderstanding of the model's performance trends\nduring the early stages of training can facilitate\nrapid iteration and minimize unnecessary resource"}, {"title": "Comprehensive Benchmarks", "content": "We present the performance of BaichuanSEED and\na series of 7B models as baselines across a compre-\\hensive set of evaluation benchmarks.\nBaselines. The selected baselines are all cutting-\nedge open-sourced LLMs, including the base and\nchat or instruct version of Llama family (AI@Meta,\n2024), Baichuan series (Yang et al., 2023), Qwen\nseries (Bai et al., 2023), MAP-Neo (Zhang et al.,\n2024), and OLMO (Groeneveld et al., 2024).\nBenchmarks. The benchmarks encompass\nMMLU, CMMLU, AGIEval (Zhong et al., 2023),\nC-Eval, MMLU-Pro (Wang et al., 2024), and\nLiveBench (White et al., 2024). We also carefully\nselect representative benchmarks for downstream\ntask performance evaluation, including MBPP, Hu-\nmanEval (Chen et al., 2021) for coding, GSM8K,\nMATH (Hendrycks et al., 2021b) for mathematics,\nTriviaQA (Joshi et al., 2017) for commonsense\nreasoning, and HellaSwag (Zellers et al., 2019)\nfor reading comprehension. These benchmarks\ntypically cover a wide range of subjects, prompting\nthe LLMs to directly answer questions, which may\nbe in the form of single-choice or short-answer.\nWe take accuracy as the evaluation metric.\nOverall Performance. The overall performance\nis shown in Table 3, while the details across var-\nious subjects can be found in Appendix A. Com-\npared to the Llama family models, BaichuanSEED\nshows significant gains in Chinese evaluation\nbenchmarks, such as CMMLU and C-Eval. We\nsuppose that the incorporation of knowledge in-\ntensive Chinese data, such as high-quality text-\nbooks and academic papers, leading to the improve-\nment. Moreover, our foundation model surpasses\nBaichuan2-7B and Baichuan2-13B in comprehen-\nsive English benchmarks such as MMLU (+10.2%,\n+0.7%), AGIEval (+7.3%, +29.1%), and MMLU-\nPro (+22.7%, -0.1%), and is comparable to\nQwen1.5-7B and MAP-Neo. Although our model\nstill lags behind Llama3-8B, we argue that train-\ning with only one-fifth of Llama's tokens may\ncontribute to the result, which suggests the po-\ntential for comparable or even superior perfor-\nmance at the same scale. LiveBench, a bench-\nmark with high timeliness, effectively assesses\nthe model's generalization ability, preventing po-\ntential benchmark leakage. Our model also out-\nperforms Baichuan2-7B-Chat (+42.1%), Qwen1.5-\n7B-Chat (+9.2%), OLMo-7B-SFT (+108.2%), and\nMAP-Neo-7B-SFT (+27.7%) on LiveBench, likely\ndue to its focus on acquiring extensive world knowl-\nedge and language patterns, not limited to overfit\non specific optimization tasks. To conclude, our\nBaichuanSEED achieve competitive performance\nwithout any elaborate data selection and benchmark\noptimazation, even compared to the cutting-edge\ncommercial LLMs.\nDownstream task Performance. LLMs are typi-\ncally expected to exhibit strong generalization ca-\npabilities across multiple downstream tasks. The\ndetailed performance is shown in Table 4. Our\nfine-tuned model BaichuanSEED-SFT consistently\nachieve second best in code tasks, due to the rich\nlogical reasoning found in high-quality data. How-\never, our model underperforms in mathematics,\ntrailing Llama3 and MAP-Neo by nearly 10 points\non MATH and 25 points on GSM8K, respectively.\nFor example, MAPNeo achieved only 5% accu-\nracy on MATH and 21% on GSM8K before anneal-\ning, while significant improvement after anneal-\ning with high ratio of mathematical data (Zhang\net al., 2024). To emphasize, BaichuanSEED tries\nto be a completely pure model, without training\non synthetic data or up-sampling any downstream\ntask data in pre-training, and utilizing general in-\nstructions during SFT. Therefore, our foundation\nmodel still has significant untapped potential, espe-\ncially in code and mathematical tasks, discussed in\nSection 6.2. This can be achieved through further\npre-training with an increased proportion of down-\nstream task data (Shao et al., 2024; DeepSeek-AI\net al., 2024b) or high-quality data annealing train-\ning (Hu et al., 2024; Dubey et al., 2024). Our foun-\ndation model is comparable to Qwen1.5 and Llama\non HellaSwag, and performs best after fine-tuning.\nNotably, our model shows no significant changes\nafter alignment, except for a slight improvement\nin math benchmarks, attributed to the enhanced\ninstruction following and comprehension abilities\ngained during SFT. In contrast, Qwen1.5 experi-\nenced a decline in MMLU-Pro and downstream\ncode and math tasks, highlighting the importance\nof a completely pure training process to ensure the"}, {"title": "Discussion", "content": "In this section, we conduct a series of experiments\nto explore the effectiveness of strategies employed\nin the training of BaichuanSEED. Firstly, thanks\nto the precise categorization of subjects as men-\ntioned in Section 3.1.3 to adjust the proportion\nof mathematics, we validate the potential of our\nmodel in mathematical tasks under the continued\npre-training strategy. Secondly, we investigate the\noptimal ratio of high-density knowledge data dur-\ning the pre-training phase using both cold start\nand continued pre-training methods on a 2B same-\narchitecture model."}, {"title": "Effect of Knowledge Intensive Data", "content": "Knowledge Intensive Data refers to data that con-\ntains massive knowledge per token. We argue that\nLLMs trained on KID can achieve better perfor-\nmance in the same training steps (Hu et al., 2024;\nLi et al., 2023c). We extensively collect public\nChinese and English academic papers and books\nas stated in Section 3.1.1. However, the intro-\nduction of KID inevitably down-sampling others,\nwhich may hinder the model from possessing com-\nprehensive world knowledge or language patterns.\nTherefore, determining the appropriate proportion\nof KID is crucial.\nWe explore the optimal proportion of KID in\ntwo settings: training from scratch and continued\npre-training. Considering the computational cost,\nwe conduct experiments on a 2B model with the"}, {"title": "Potential on Mathematics", "content": "Taking mathematics reasoning as an example, we\nexplore the potential of BaichuanSEED on a wide\nrange of downstream tasks. Specifically, we first\nemploy a classification model to perform a finer\ncategorization of high-density knowledge data on\nsubjects to better utilize this data. We then con-\nduct continued pre-training experiments by alter-\ning the data proportions to verify the potential of\nour model. The experimental results indicate that a\nhigher proportion of mathematics data significantly\nenhances the model's performance in mathematical\ndownstream tasks, such as GSM8K and MATH,\nwithout compromising the comprehensive evalua-\ntion benchmarks.\nTo address the details, we increase the propor-\ntion of mathematics-related data to 2% and 10%,\nconducting 300B token continued pre-training on a\ncheckpoint with 1.44T tokens of BaichuanSEED,\nas illustrated in Figure 5. At the 2% proportion, we\ndo not observe significant gains on the mathemat-\nics benchmarks. However, when we aggressively\nraise the proportion to 10%, notable improvements\nare evident. To be specific, the 1.74T checkpoint\nshows a 7.4% and 65.6% increase on GSM8K and\nMATH respectively, with only a 0.3% fluctuation\non MMLU, compared to the baseline. Therefore,\nwe argue that knowledge intensive data has sub-\nstantial potential, especially when enhancing a lan-\nguage model's mathematical abilities. By search-\ning optimal data proportions and employing cur-\nriculum learning strategies, the value of these data\ncan be further captured. In addition, it validates\nthe immense potential of our model in downstream\ntasks."}, {"title": "Related Works", "content": "Several institutions have published detailed techni-\ncal reports and open-sourced the weights of their\ncutting-edge models, to promote the development\nof the LLM field. Recently, numerous models made\nefforts on meticulous data filtering (AI@Meta,\n2024; Penedo et al., 2023; Maini et al., 2024; Sol-\ndaini et al., 2024), innovative architectural explo-\nration (Ainslie et al., 2023; Jiang et al., 2024; Gu\nand Dao, 2023), and optimization for vertical down-\nstream tasks (DeepSeek-AI et al., 2024b; Shao\net al., 2024). As a baseline model, BaichuanSEED\nfocus on the exploration of the limits of model\ncapabilities through data collection and dedupli-\ncation, avoiding any specific optimization. Nev-\nertheless, our model achieves competitive perfor-\nmance on comprehensive benchmarks, comparable\nto many open-source commercial models of sim-\nilar scales. These representative models include\nearlier versions of Baichuan (Yang et al., 2023),\nthe Llama family (Touvron et al., 2023b; Dubey\net al., 2024), the Qwen series (Bai et al., 2023), the\nDeepseek series (Dai et al., 2024; DeepSeek-AI\net al., 2024a), and the Phi series (Li et al., 2023c;\nAbdin et al., 2024). Moreover, there are several\ninitiatives dedicated to disclosing technical details,\nsuch as training datasets and code, including MAP-\nNeo (Zhang et al., 2024), OLMO (Groeneveld et al.,\n2024), Pythia (Biderman et al., 2023), and Am-\nber (Liu et al., 2023)."}, {"title": "Conclusions", "content": "This technical report introduces BaichuanSEED,\na pure yet competitive 7 billion parameter large\nlanguage model trained from scratch to explore\nits limits without elaborate data selection and spe-\ncific downstream task optimization. We pre-train\nthe model on 3 trillion tokens of high-quality data\nand supervised fine-tune it with a straightforward\nyet effective process. Guided by the pretraining\nprinciples that collection to scale up and reweight-\ning to improve data quality, our model achieves\ncomparable performance with leading commercial\nmodels on several comprehensive benchmarks. We\nalso conduct heuristic experiments to explore the\nmodel's potential for further optimization in down-\nstream tasks. All training details are publicly avail-\nable, with the hope that our technical report would\nbenefit the community to further unveil the skyline\nof data on large language models."}, {"title": "Detailed Evaluation Results", "content": "The detailed performance of MMLU, CMMLU,\nand AGIEval can be found in Table 6. The detailed\nperformance of each subjects of MMLU-Pro and\nLiveBench can be found in Table 7 and Table 8,\nrespectively. All bold numbers indicate the best\nperformance, while the underlined stand for the\nsecond."}, {"title": "Case Study on Global\nMulti-granularity Deduplication", "content": "To display the performance of our proposed global\nmulti-granularity deduplication intuitively in Sec-\ntion 3.1.2, we provide several cases before and after\nthe deduplication, as shown in Figure 6.\nIn terms of sentence deduplication, we filter from\ntwo dimensions: frequency and quality. On the one\nhand, reducing the frequency of repeated sentences,\nwhich may mitigate the repetition generation. On\nthe other hand, it can avoid low-quality sentences\nbeing learnt by the model. These two terms are in-\ndispensable and complementary to each other. We\nconclude the characteristics of the removed sen-\ntences as follows, while filtering out these patterns\nmay improve the accuracy and recall of harmful\nand toxic contents:\n\u2022 Advertisements. SEO strategies are adopted\nby advertisements to increase visibility, such\nas creating spam sites and utilizing various\ntrending long-tail keywords to mislead search\nengines. The normal popular contents are in-\nterleaved with the advertisements, resulting in\nlots of template sentences. Without deduplica-\ntion, relying solely on the quality score is not\nsufficient to filter them out.\n\u2022 Texts for dissemination and interactivity.\nIt is widely utilized in web pages, which is\none of the most common patterns. While it\nis usually unrelated to the main idea of the\ndocuments.\n\u2022 Copyrights and PII. These similar patterns\nmostly appear in news, books. It may address\nprivacy concerns, and aggravate the LLM rep-\netition generation issue.\n\u2022 Residual HTML formats and watermarks.\nInappropriate custom parsing website tem-\nplates may address this issue."}]}