{"title": "E2CFD: Towards Effective and Efficient Cost Function Design for Safe Reinforcement Learning via Large Language Model", "authors": ["Zepeng Wang", "Chao Ma", "Linjiang Zhou", "Libing Wu", "Lei Yang", "Xiaochuan Shi", "Guojun Peng"], "abstract": "Different classes of safe reinforcement learning algorithms have shown satisfactory performance in various types of safety requirement scenarios. However, the existing methods mainly address one or several classes of specific safety requirement scenario problems and cannot be applied to arbitrary safety requirement scenarios. In addition, the optimization objectives of existing reinforcement learning algorithms are misaligned with the task requirements. Based on the need to address these issues, we propose E2CFD, an effective and efficient cost function design framework. E2CFD leverages the capabilities of a large language model (LLM) to comprehend various safety scenarios and generate corresponding cost functions. It incorporates the fast performance evaluation (FPE) method to facilitate rapid and iterative updates to the generated cost function. Through this iterative process, E2CFD aims to obtain the most suitable cost function for policy training, tailored to the specific tasks within the safety scenario. Experiments have proven that the performance of policies trained using this framework is superior to traditional safe reinforcement learning algorithms and policies trained with carefully designed cost functions.", "sections": [{"title": "1 Introduction", "content": "Currently, safety requirements play a vital role in various fields. Different safety requirement scenarios cover a wide range of application fields, including autonomous driving [15], recommendation systems [11], resource allocation [2], etc. In order to cope with these complex safety requirements, safe reinforcement learning algorithms have gradually become an effective solution in recent years. They take task requirements and safety requirements as optimization objectives for learning and training, and finally obtain policies that meet the safety requirements of corresponding scenarios. However, existing safe reinforcement learning algorithms still have some problems:\n\u2022 Poor generalization of safe reinforcement learning algorithms. Most existing safe reinforcement learning algorithms only design algorithms for a single or a small number of safety constraint scenarios, but there are many different safety requirements in real scenarios (e.g., cumulative constraint violation limit, zero-constraint violation limit, almost 100% satisfaction of constraints, etc.). Traditional safety algorithms cannot adapt to various safety requirement scenarios by simply changing the algorithm parameter settings (e.g. just setting the cost limit to 0 in a safety algorithm that satisfies the cumulative constraint violation restriction still fails to satisfy the zero constraint violation [13]). Therefore how to design a generalized safe reinforcement learning algorithm framework that can adapt to any safety constraint problem, guarantee the effectiveness of the algorithm, and at the same time improve the generalization of the algorithm becomes a thorny issue.\n\u2022 Alignment problem between RL algorithm optimization goal and task goal. The goal of reinforcement learning as an AI approach is to have agents that achieve actual task goals. The structure of the RL algorithm, on the other hand, dictates that its optimization goal focuses on increasing the final cumulative reward value, which results in an algorithm whose optimization goal is positively correlated with the task goal, but not perfectly aligned (e.g., reward hacking problem [24]). For a complex task scenario and objective, it is difficult to design the perfect reward function such that the final optimization objective of the RL algorithm is completely equivalent to the actual task objective. For a safe reinforcement learning scenario, the final evaluation metrics can likewise not only focus on the performance of rewards and costs, but also on the actual completion of the task (whether the task is completed or not, and whether the safety requirements are violated or not). We noticed that different qualities of reward and cost functions have different impacts on the performance of the algorithm.\nThis inspires us to realize different safety task requirements by designing appropriate reward functions and cost functions.\n\u2022 The difficult design of reward function. Designing reward functions manually requires a lot of time and expertise. Similar to parameter tuning, the time and labor costs are greater. In dealing with more complex scenarios where safety constraints exist, there are even more factors to consider. When there is a slight change in the environment or task requirements, the designed reward function often fails and needs to be redesigned again according to the new environment and task requirements. Therefore, the manual design of reward functions remains problematic in the safety problem scenarios that are the focus of this paper.\nOn the other hand, LLM has recently excelled in areas such as robot control [32, 14]. The task understanding and code generation capabilities of LLM are useful for training many reinforcement learning tasks [3]. However, most of the existing LLM-assisted RL training methods consider goal decomposition for multi-step task operations, and LLM plays a limited role in scenarios that do not require task planning for complex tasks. How to utilize the advantages of LLM to assist in the training of reinforcement learning algorithms in safety scenarios is currently an open problem.\nOur solution design idea is to fully utilize the advantages of LLM to solve the above problems. Specifically, we can use the task comprehension ability of LLM to solve the generalizability problem of safe reinforcement learning algorithms, adopt a new evaluation approach to solve the alignment problem of task requirements and optimization objectives, and use the code generation ability of LLM to solve the difficult problem of reward function design. Overall, our contributions are as follows:\n\u2022 We first formulate the Cost Definition Problem (CDP) and transform the task of solving different safety requirements into the task of designing different cost functions.\n\u2022 We propose a new cost function generation framework that utilizes the comprehensive task understanding and code generation capabilities of LLM. In addition, we introduce an Error Code Filtering (ECF) module to ensure the effectiveness of code generation, as well as a specially tailored Fast Performance Evaluation (FPE) module to facilitate efficient and user-friendly policy generation.\n\u2022 We conduct extensive and detailed experiments on a continuous control task. The experimental results show that our proposed framework provides better performance, generalization, and interpretability than traditional safe reinforcement learning algorithms with artificial reward function design methods."}, {"title": "2 Related Work", "content": "In recent years, safe RL has derived various methods to solve problems with different levels of safety requirements. The most common safety requirement is the constraint that the cumulative constraints should be less than a certain threshold. Currently, most safe RL methods are designed based on this requirement. Commonly used methods include Lagrangian-based method [9, 26, 20], Projected-based method [30], Lyapunov-based method [5], Safeguard-based method [7], etc. The second safety requirement is for the agent to achieve zero violation of constraints [34]. Meeting this requirement often requires the imposition of hard safety constraints on each step of the agent's behavior, which often leads to a decline in task target performance and an increase in training difficulty. The most stringent safety requirement is to hope that a certain constraint (or zero violation) will be satisfied almost 100% (i.e., satisfy constraints almost surely), which puts forward higher requirements for the stability of the safety constraint guarantee of the algorithm. The current solution to this requirement is mainly based on state augmentation [25, 27]. How to design a reinforcement learning algorithm that can satisfy various safety requirement levels is still a current research challenge.\nDesigning an appropriate reward function for a reinforcement learning agent to achieve the desired goal is difficult [8]. AutoRL [10] proposes to use an evolutionary algorithm approach to automatically search for better reward functions. LIRPG [35] proposes the idea of learning intrinsic rewards. AutoCost [13] proposes to use evolutionary strategies to automatically design cost functions to solve zero constraint violation problems.\nSome language-conditioned-based RL works [4, 19] use large models to understand the original task and then decompose it into subtasks to aid agent learning. LEARN [12] trains a classification network on trajectory data with natural language, to predict whether a trajectory matches the linguistic description, and evaluate the network at each time step to form a potential-based reward shaping function. Recently, due to the exciting performance of LLM in task understanding and code generation, some works have used LLM to help reinforcement learning task training. Colas et al. [6] automatically generates goals from textual descriptions of the tasks and uses them for subsequent training of the agent. Some works directly model LLM as a reward function. Kwon et al. [17] uses LLM to generate binary reward functions and Eureka [18] proposes a framework for reward function generation for robot control tasks using LLM. However, there is currently little work in the field of safe reinforcement learning that uses LLM to aid in training.\nTo the best of our knowledge, we are the first work to use LLM for cost function design in the safe RL domain. Specifically, the most similar works to ours are AutoCost [13] and Eureka [18]. However, the former does not take advantage of LLM to understand the task and generate code automatically, while the latter uses the complete environment source code as additional information input to better understand the task and generate code, and the problem scenarios do not involve more complex safety requirement constraints. In addition, both use more inefficient evolutionary algorithms that are not suitable for direct application to safe RL problems. Instead, our proposed E2CFD only requires privileged access to the necessary information (environment description, task description, form of the original reward function and cost function) as auxiliary information. We believe that in a gray-box scenario (i.e., where part of the environment-related information can be accessed to assist the LLM in task comprehension, in addition to agent's observations and feedback), the less information used to assist training, the better the generalizability of the method."}, {"title": "3 Background and Problem Formulation", "content": "3.1 Constrained Markov Decision Process\nThe constrained Markov decision process (CMDP) is usually used as a modeling method for safe reinforcement learning. It is usually defined as a tuple (S, A, P, R, C, \u03bc, \u03b3) consisting of the following parts: the state space S, the action space A, the transition probability function P: S\u00d7A\u00d7S \u2192 [0, 1] describing the dynamics of the environment, the reward function R S \u00d7 A \u2192 R representing the immediate reward obtained by taking action a in state s, the cost function C: SX A \u2192 R representing the immediate cost of taking action a in state s, the initial state distribution \u03bc: S \u2192 [0, 1] and the discount factor \u03b3\u2208 [0, 1] determining the emphasis on future gains.\nIn safe RL, the optimization goal is to obtain a policy \u03c0 that maximizes the discounted cumulative reward $J_R = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$ and at the same time, the discounted cumulative cost $J_C = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t C(s_t, a_t)]$ satisfies specific safety constraints, expressed as:\n$\\max \\limits_{\\pi \\in \\Pi} J_R, \\quad s.t. \\quad SF(\\pi)$ (1)\nwhere SF(\u03c0) denotes different safety requirements. Take three common safety constraint requirements as an example:\n\u2022 The traditional safety requirement requires that the discounted cumulative cost is satisfied to be less than a certain deterministic threshold d, then SF(\u03c0) : $J_C - d \\leq 0$.\n\u2022 The zero-violation safety requirement requires that the discounted cumulative cost is 0, then SF(\u03c0) : $J_C \\leq 0$.\n\u2022 The more complex almost surely safety requirement requires close to 100% satisfaction of a constraint, then SF(\u03c0) : $z_t \\geq 0\\ \\textit{ a.s.}, \\forall t \\geq 0$, where $z_t$ indicates whether the discounted cumulative cost has violated the safety constraint when in the current state $s_t$ and a.s. stands for \"almost surely\" (with probability one).\nIt is worth mentioning that regarding the way different safety constraints are defined, our approach is more general and concise compared to other existing work [31], which is suitable for extension to modeling various safety scenarios or other scenarios with constraints.\n3.2 Cost Design Problem\nThe task objectives of real scenarios are often statistical quantities (e.g., the success rate of the task and the number of dangerous behaviors, etc.), which can usually only be counted individually at the end of training and during the testing phase, and which cannot be directly optimized as a reward function and a cost function directly using a reinforcement learning algorithm. Therefore, the goal of cost function design is to design a suitable cost function that matches the actual task objectives and to be able to utilize the newly designed cost function to achieve the task objectives. To this end, referring to the traditional way of defining the reward design problem [23], we first propose a new definition of the cost design problem.\nDefinition 1 (Cost Design Problem). A cost design problem (CDP) is a tuple P = (\u039c,\u03c0\u043c, F), where M is a CMDP. \u3160\u043c is a policy obtained by training based on reward function R and cost function C using any learning algorithm. F : \u03c0 \u2192 R is the fitness score function for generating a scalar evaluation result score for an arbitrary policy, which can only be obtained through the actual policy evaluation process. In a CDP, the goal is to output a cost function C so that the trained policy achieves the highest score F(\u03c0).\nIdeally, the fitness score function should be aligned with the ultimate goal of the task. For example, for unconstrained scenarios with no safety requirements, the metric degenerates into the cumulative reward under the standard reinforcement learning task setting, i.e., F(\u03c0) = JR. While for traditional safety-constrained scenarios where safety requirements exist, the metric can be expressed as:\n$F(\\pi) = \\begin{cases}\n \\eta & J_C > d, \\\\\n J_R & J_C \\leq d.\n\\end{cases}$ (2)\nwhere d is the safety constraint, \u03b7 is a sufficiently large positive number, JR is the discount cumulative reward, and Jo is the discount cumulative cost."}, {"title": "4 Methodology", "content": "The schematic of our proposed framework is shown in Figure 2 and is divided into an initialization phase and an evolutionary phase.\n4.1 Initialization Phase\n4.1.1 Base Function Generation\nThe first step in the initialization phase is to perform base cost function generation. Depending on the specific environment description and safety task requirements, there can be two types of generation: 1. Large language model for generating the cost function; 2. Manual design for generating the cost function. Our framework jointly uses these two approaches to collaboratively generate base cost functions to obtain diverse and efficient initial base cost functions.\nFirst of all, we need to input the necessary information related to the task as prompts to LLM. For task scenarios with different task requirements, the necessary information contains:\n1. Task description information. This information passes the semantic content of the task (including environment-base information, task objectives, etc.) into the LLM for subsequent selection of appropriate elements as components of the cost function.\n2. Safety requirement information. This information passes the task's safety requirement content (including the degree of safety constraints, safety objectives, etc.) into the LLM for the generation of safety components in the cost function.\n3. Original reward/cost function information. This information passes the original reward/cost function content (including code style, function variables, etc.) into the LLM for the generation of code fragments in the cost function to guarantee syntactic accuracy and semantic consistency.\nMeanwhile, although it was mentioned earlier that manually designing cost functions is both time-consuming and labor-intensive, it is not costly to design only imperfect cost functions. Therefore the framework combines a series of suboptimal cost functions obtained without sufficient manual design with the cost functions generated by the large language model to obtain the final set of initial cost functions, which helps to ensure a lower bound on the quality of the initialized functions.\n4.1.2 Error Code Filtering\nDue to the imperfections in the human-designed prompts, the human intent cannot be fully conveyed to the LLM, and thus the output of the LLM often contains some errors (non-compliance, obvious syntax errors, etc.). We therefore propose the following Error Code Filtering (ECF) module to ensure that the cost function code used for subsequent training is free of syntax errors and satisfies human intent as much as possible. Figure 3 illustrates the specific construction of the ECF module.\nSpecifically, the ECF module first performs a syntax test on the generated cost function. If no compilation errors (syntax errors) occur during this process, the newly generated cost function is considered free of syntax errors. Then, ECF rejects new cost functions that clearly do not meet the task requirements (e.g., the presence of components that contradict safety requirements) by introducing manual review. The final output of cost functions that are free of syntax errors and likely to fulfill the task requirements will be used in the next phase."}, {"title": "4.2 Evolutionary Phase", "content": "4.2.1 Fast Performance Evaluation\nIn the evolutionary phase, we use an evolutionary algorithm-like approach to evaluate the base function generated by iterative updating to obtain a better cost function to aid in the training of the agent.\nIt is worth stating that after obtaining the generated cost function, it is an open question of how to use the cost function for agent training. Similar to the treatment of many classical safe reinforcement learning algorithms (e.g., Lagrangian-based methods), we chose to put the cost function directly into the reward function as a penalizing term in the reward function, and later use the modified reward function as a new reward function for training using any reinforcement learning algorithm.\n$R'(s_t, a_t) = R(s_t, a_t) + C(s_t, a_t)$ (3)\nThis seemingly simple treatment makes it possible to train using any standard reinforcement learning algorithm. At the same time, it further simplifies the cost of manually designing the function by transferring the weighting problem between the reward function and the cost function, which is difficult to solve in traditional methods, to a part of CDP.\nHowever, according to the traditional evolutionary algorithm approach, it is very inefficient to wait until the agent is trained (e.g., curve convergence) before evaluating its performance in a test environment alone. Therefore, we propose a new policy performance evaluation method, Fast Performance Evaluation (FPE).\nFirst, policy training of the agent is started using one weighting cost function or multiple base cost functions generated in the previous step. Then, when a predefined number of early training rounds is reached, training is stopped and performance evaluation is performed based on a predefined scoring function. Finally, the performance evaluation results are used to iteratively update the weighting cost function or base cost functions.\nFurthermore, as mentioned before, the fitness score function that perfectly aligns with the final optimization goal of the task can only be obtained by evaluating it in a separate test at the end of agent training. Scenarios with different safety level requirements, on the other hand, need to be manually crafted to define different fitness functions that are consistent with the task requirements. Therefore, the E2CFD framework abandons the practice of manually defining the fitness score function, and instead uses the LLM to generate appropriate fitness score functions based on the task requirements, in order to reduce the design cost and increase the flexibility of the framework to face different scenarios. We compare the effects of different fitness score functions on the performance results in the experimental section.\nOverall the FPE module has two advantages. First, it improves the efficiency of policy performance evaluation and can help to obtain a cost function that helps to accelerate the convergence of the training of the agent. Second, it solves the problem of aligning the task objective with the reinforcement learning training objective.\n4.2.2 Weighting Function Generation\nUsing the performance evaluation results from the first step, a new weighting cost function can be generated by using the output of the fitness score function as the importance weight of each cost function. Specifically, for the set of n base cost functions $F_b = {f_1, f_2, ..., f_h}$ generated in the first step, which corresponds to the fitness scores obtained as $S = {s_1, s_2, ..., s_n}$, the newly generated weighting function is:\n$f = \\sum_{i=1}^{n} f_i \\frac{s_i}{\\bar{S}}$ (4)\nwhere $S$ is the fitness score after the normalization process, as it is important to avoid the imbalance between the cost function and the original reward function caused by too high or too low LLM scores.\nThe subsequent iterative updating and optimization process of the cost function can be achieved by simply using the newly generated cost function as an input to the LLM in the initialization phase and repeating the previous process until the maximum number of iteration rounds is reached.\n4.3 Overall Algorithm Framework\nThe pseudo-code of the complete algorithmic framework is shown in Algorithm 1."}, {"title": "5 Experiments", "content": "5.1 Experiment Settings\nWe evaluate the proposed framework E2CFD on the StaticPointGoal task in the static environment version of Safety Gym [29, 21], a benchmark widely used for safe RL algorithm evaluation. The task exists of a Point robot with 46 states and 2 actions navigating towards a goal in a 2D space while avoiding contact with hazardous areas while traveling. The initial position of the robot is randomly generated and the current episode ends when the robot reaches the goal. Note that the locations of the hazardous regions of the environment are fixed, which we do in order to carry out a more intuitive interpretation of the subsequent construction process of the cost function, and to avoid unsolvable problems contaminating our experiments [29, 25].\nAll agents were trained for 100 epochs with 10,000 interaction steps per epoch and a maximum step size of 1,000 steps per episode. All experiments are run using three random seeds. For a fair comparison, we compare our E2CFD with five recognized classical or state-of-the-art safe reinforcement learning algorithms (PPO-Lag, CPO [1], PCPO [30], FOCOPS [33], and CUP [28]) under the same codebase [16]. We also use the standard PPO [22] algorithm as a representative of unconstrained algorithms in all subsequent experiments to refer to the normal task performance versus the safety performance in this task scenario. All algorithms adopt the same training strategy and tricks except for the extra components of the algorithm itself. The complete code is also placed in a supplemental file and will be open-sourced when it is subsequently organized well.\nIn addition to using cumulative returns and cumulative costs as traditional task metrics and safety metrics, we also used three metrics, task completion rate (TCR), hazardous area exposure rate (HER), and time ratio (TR), in some of our experiments as more comprehensive metrics to reflect the effectiveness and efficiency performance of our algorithmic framework in aligning human needs. The specific definitions of the three metrics are as follows:\n$TCR = \\frac{N_{tc}}{N_e}$\n$HER = \\frac{N_{hae}}{N_e}$\n$TR = \\frac{t_{algo}}{t_{ppo}}$ (5)\nwhere $n_{tc}$ is the number of task completion, $n_{hae}$ is the number of hazardous area exposure, $n_e$ is the number of episode, $t_{algo}$ is the training time of algorithm and $t_{ppo}$ is the training time of PPO.\n5.2 Performance under Different Safety Requirements\n5.2.1 Traditional Safety Requirement Scenarios\nWe first compare the performance of the proposed E2CFD with the baseline algorithms on traditional safety requirement tasks. Figure 4 shows that all algorithms eventually meet the task requirements and all algorithms except PPO and PCPO meet the safety requirements. However, we find that E2CFD significantly outperforms all other algorithms in terms of safety performance, which reflects the potential of E2CFD in safety requirement fulfillment.\n5.2.2 Zero-violation Safety Requirement Scenarios\nWe also compare the performance of the proposed E2CFD with the benchmark algorithms on the safety requirement task with zero violation constraints. Figure 5 shows that not all algorithms eventually meet the task requirements, which reflects the impact of more demanding safety requirement scenarios on the algorithms' task performance. E2CFD is also the closest algorithm to zero-constraint violation in terms of safety requirements. This shows the obvious advantage of E\u00b2CFD over traditional safe reinforcement learning algorithms in different safety requirement scenarios.\n5.2.3 Almost surely Safety Requirement Scenarios\nThis safety requirement scenario is an enhanced version of the first two safety scenarios. It emphasizes more on the stability of the algorithm's safety performance, i.e., it requires that the safety constraints (traditional safety constraints given a pre-threshold or zero violation safety constraints) are satisfied in as many episodes as possible.\nBy comparing the results of the box plots of costs in Figure 6a, it can be observed that the overall distribution of constraint satisfaction for E2CFD performs the best. Similarly, by comparing the box plot results of the costs in Figure 6b, it can be observed that the E2CFD performs second only to the CPO algorithm in terms of the overall distribution of constraint satisfactions, while significantly outperforming all the other baseline algorithms. However, it is worth emphasizing that the CPO algorithm performs very poorly on the original task with zero constraint violation of the safety requirements (see Figure 5a), while E2CFDF excels in both task performance and safety requirement performance.\n5.3 Human-engineered vs. LLM-assisted CFD\nFigure 7 compares the difference in task performance between the human-engineered cost functions and LLM-assisted cost functions. In this case, H5 and H6 are the two best methods in terms of safety performance and task performance, respectively, among all human-designed cost functions (consistent with the experimental results in Figure 1)."}, {"title": "5.4 FPE Model Evaluation", "content": "In order to specifically analyze the FPE module in our proposed framework E2CFD, we conducted comparative experiments on two of its key components (the evaluation phase and the scoring function).\nAs can be obtained from Figure 8 and Table 1, whether we change the time node of performance evaluation, or replace the scoring function, eventually our framework E2CFD demonstrates good dual optimization of task requirements (TCR) and safety requirements (HER). Specifically, for algorithms with different evaluation phases under the same scoring function, the algorithm that performs early performance evaluation shows a slight decrease in safety requirement fulfillment but a significant increase in time savings (TR) relative to the algorithm that performs late performance evaluation. For algorithms using different scoring functions (LLM-output1 or LLM-output2) under the same evaluation phase, the former corresponding algorithms show improved task performance, safety performance and time performance than the latter corresponding algorithms. The latter, on the other hand, exhibits a faster convergence rate, indicating its superior sample efficiency. This phenomenon reveals that we can improve the performance of the algorithmic framework by trying different scoring functions according to different needs."}, {"title": "5.5 Visualization of Cost Functions", "content": "To further analyze the cost functions generated by different approaches, we visualize the weight values in the composition of the cost functions in multiple algorithms (including PPO, human-designed, and E2CFD) to get the importance of the cost functions under different approaches for different regions of this static environment.\nThe heatmap results in Figure 9 reflect the fact that the cost function obtained with the aid of the LLM has the same characteristics as the human-designed cost function, i.e., there is a perception of the hazardousness of the area in the environment. In addition, the level of safety emphasis in the vicinity of the goal area may also have an impact on the agent's policy.\nThe advantage of E2CFD is that it does not need to rely entirely on manual coding of hazards in the environment, which provides a more convenient and efficient way of constructing cost functions for meeting other safety requirements in more complex environments, and has a greater potential for application. Meanwhile, in turn, the visualization and analysis of the cost function obtained by the LLM-assisted generation can help the human to better understand the task requirements, discover deeper design ideas, and also help the human to design a better cost function."}, {"title": "6 Conclusion", "content": "In this paper, we present E2CFD, an effective and efficient cost function design framework for safe reinforcement learning via large language model. We first present the problem of generating cost functions under safety requirements for complex safety scenarios. Our approach leverages the task understanding and code generation capabilities of LLM and designs FPE module to achieve efficient and human-aligned policy generation. The experiments demonstrate that the method has better performance in meeting task requirements than traditional safe reinforcement learning algorithms, and has the advantage of being more efficient and generalizable than the human-designed approach.\nHowever, there are still directions for improvement in this framework. For example, LLM often fails to fully consider all the task requirements and precautions due to the incompleteness of the prompts, so the framework separately uses a manually-assisted ECF module to screen the code reasonableness of generating base functions. How to design more complete prompts for task scenarios and improve the quality of LLM code generation will be a worthwhile research direction in the future. In addition, when the safety requirements increase (i.e., corresponding to multi-constraint scenarios), the complexity of the task will also increase. How to ensure the effectiveness of the framework to face such more complex problem scenarios will be another research difficulty."}]}