{"title": "Deep Reinforcement Learning-based Obstacle Avoidance for Robot Movement in Warehouse Environments", "authors": ["Keqin Li", "Jiajing Chen", "Denzhi Yu", "Tao Dajun", "Lian Jieting", "Xinyu Qiu", "Zhang Shengyuan", "Sun Baiwei", "Ran Ji", "Zhenyu Wan", "Fanghao Ni", "Bo Hong"], "abstract": "At present, in most warehouse environments, the accumulation of goods is complex, and the management personnel in the control of goods at the same time with the warehouse mobile robot trajectory interaction, the traditional mobile robot can not be very good on the goods and pedestrians to feed back the correct obstacle avoidance strategy, in order to control the mobile robot in the warehouse environment efficiently and friendly to complete the obstacle avoidance task, this paper proposes a deep reinforcement learning based on the warehouse environment, the mobile robot obstacle avoidance Algorithm. Firstly, for the insufficient learning ability of the value function network in the deep reinforcement learning algorithm, the value function network is improved based on the pedestrian interaction, the interaction information between pedestrians is extracted through the pedestrian angle grid, and the temporal features of individual pedestrians are extracted through the attention mechanism, so that we can learn to obtain the relative importance of the current state and the historical trajectory state as well as the joint impact on the robot's obstacle avoidance strategy, which provides an opportunity for the learning of multi-layer perceptual machines afterwards. Secondly, the reward function of reinforcement learning is designed based on the spatial behaviour of pedestrians, and the robot is punished for the state where the angle changes too much, so as to achieve the requirement of comfortable obstacle avoidance; Finally, the feasibility and effectiveness of the deep reinforcement learning-based mobile robot obstacle avoidance algorithm in the warehouse environment in the complex environment of the warehouse are verified through simulation experiments.", "sections": [{"title": "Introduction", "content": "Warehouse robots work in complex and variable scenarios covering complex stacked goods and warehouse personnel. The traditional obstacle avoidance algorithm uses the information of the surrounding obstacles at the current moment and relies on methods such as geometric configuration or sampling to obtain the final obstacle avoidance strategy[1,2], which does not take into account the changes of the future state of the surrounding pedestrians, and in the complex environment, the algorithm is prone to unnatural behaviours, such as oscillation, due to the randomness of the pedestrian's movement, which makes the robot's control signals jump frequently, and fails to satisfy the requirements of safety and comfort in warehouse environments.. Therefore, in order to efficiently complete the obstacle avoidance task in the cargo-intensive warehouse environment, the obstacle avoidance algorithm of the mobile robot needs to analyse the pedestrian's behaviour. At the same time, with the development of human-computer interaction concept, the robot's obstacle avoidance task not only needs to meet the high efficiency and real-time performance, but also needs to take into account the comfort requirements of pedestrians in the process of interaction with people.\nFor pedestrian behaviour, the historical trajectories of pedestrians can be fitted by mathematical models such as Hidden Markov Models and Gaussian Mixture Models to analyse their trajectories, but these methods are poorly adapted and do not take into account the physical environment constraints, the influence of the surrounding intelligences, and social norms and other specific factors of the intelligences' travels, so that the pedestrian trajectories can not be fitted and predicted very well. Therefore, many scholars have established pedestrian flow models to better analyse pedestrian behaviour by studying pedestrian movement and its characteristics. Among them, the social force model proposed by Helbing and Molnar is one of the most widely used traditional pedestrian movement models, which describes the relationship between pedestrians and the surrounding obstacles as well as their own target points in the form of force, and assumes that the potential repulsive force between the pedestrians is a monotonically decreasing function of the distance between the two and the pedestrian's occupancy radius, which is of some practical significance. The disadvantage is that this type of method is relatively rough, and needs to be adjusted according to the environment of the parameters, there are certain limitations. With the"}, {"title": "1 Obstacle avoidance algorithm based on deep reinforcement learning", "content": "The goal of using reinforcement learning algorithm to solve the obstacle avoidance problem is to obtain the optimal strategy \u03c0 * from the joint state $s_t$ of the robot to the control output $a_t$, as shown in Fig. 1. In the obstacle avoidance algorithm based on reinforcement learning, the intelligent body obtains the state $s_t$ of the robot and all the pedestrians in the surrounding area at the current moment by interacting with the environment, and then discretises the robot's control signal into a certain size of action space by one-step prediction.\nThen the control signals of the robot are discretised into an action space of a certain size, and the state after the execution of each control signal in the action space is predicted by a one-step prediction model, and the predicted state $s_{t + \\Delta t}$ is obtained; then the predicted $s_{t + \\Delta t}$ is inputted into the value function network, and the value of the state is obtained by combining the reward function, and the optimal action in the action space corresponding to the maximal value is selected as the final output action of the robot, which is at, as shown in Equation (1). During the interaction between the intelligent body and the environment, the value function network V* is continuously optimised until it converges."}, {"title": null, "content": "$a_t = arg max R(s_t^{\\pi}, a_t) + \\gamma \\cdot V^{pref} V^*(s_{t + \\Delta t}^{\\pi})       (1)$"}, {"title": null, "content": "where $s_{t + \\Delta t}^{\\pi} = prop(s_t, \\Delta t, a_t)$, denotes that the prediction model approximates the joint state of the robot after the $ \\Delta t$ moment by predicting the motion of the robot and its surrounding pedestrians after the $ \\Delta t$ moment through a simple linear model. The following is the specific selection method for each element of reinforcement learning:"}, {"title": "2 Specific algorithms", "content": "In this section, by introducing pedestrian interaction information in the value function network and modifying the reward function for reinforcement learning, obstacle avoidance algorithms that meet the requirements of human- computer interaction are obtained based on deep reinforcement learning."}, {"title": "2.1 Improvement of value function network based on pedestrian interaction", "content": "As shown in Figure 2, the improved value function network is composed of three parts: the pedestrian interaction information module (crowd interaction module), the LSTM network PLSTM(\u00b7), and the multilayer perceptron (MLP). The pedestrian interaction module first extracts pedestrian features from the original agent state sjnt. Then, through the LSTM network \u00d6LSTM(\u00b7), the pedestrian features of an indefinite number are combined to obtain the joint hidden state ho of all pedestrians. Finally, ho and the robot's own state are jointly input into the multilayer perceptron network \u045f\u043c(\u00b7) to obtain the corresponding value. This section mainly introduces the specific method for analyzing pedestrian interaction behavior in the improved value function network.\nThe decisions between pedestrians influence each other. If the relationship between each pair of pedestrians is accurately described, it will lead to a time complexity of O(N2). As the number of pedestrians in the environment increases, the computational burden will become extremely heavy. In addition, pedestrians who are far apart will hardly have an impact on each other's movements. Describing the relationship between the two will not only increase the computational complexity but also increase the learning burden of the network. Accordingly, this paper adopts a special hybrid grid - the angular pedestrian grid (APG)to encode the local environment of pedestrians to eliminate information that is useless for obstacle avoidance strategies. In order to better capture the dynamic characteristics of pedestrians, the angular pedestrian grid (APG) introduces a high-resolution grid, increasing the extensiveness of information. Figure 3 shows the encoding process for the i- th pedestrian PH. Let the total number of pedestrians be N. The angular pedestrian grid (APG) only considers pedestrians within a circle centered on pedestrian PH and with a radius of rmax. The circle is equally divided into K"}, {"title": null, "content": "sectoral areas, and the nearest distance to pedestrian Phin each sectoral area is selected as the encoding value of that sectoral area. After being encoded by the angular pedestrian grid (APG), pedestrian PH is output as a one-dimensional vector ri = [ri,1,...,Fi,k,...,ri,K], where the mathematical representation method of ri,k (the encoding value of the k-th sector) is shown in Equation (3)."}, {"title": null, "content": "r_{i,k} := min(max(min({p_{i,j}, j \\in N_{i,k}})))"}, {"title": null, "content": "N_{i,k}: = {j \\in {1,..., N}\\backslash{i}, \\theta_{i,j} \\in [\\Upsilon_k,\\Upsilon_{k+1}]}       (3)"}, {"title": null, "content": "Where k \u2208 {1,..., K}, and the sector angle yk corresponding to each grid is yk := (k-1)2k/K. (pi,j, Qi,j) represents the coordinates of pedestrian PH in the polar coordinate system centered on pedestrian PH. Therefore, r\u2081 is only related to the pedestrian positions."}, {"title": null, "content": "e_m = \\psi_e(\\theta_{out},\\bf{W}_e)\\ \t(7)"}, {"title": null, "content": "e^0 = \\sum_{k=1}^T softmax(a_k)e_k\\ \\ \t (8)"}, {"title": "2.2 Comfort reward function", "content": "In the scenario where the robot interacts with people, if pedestrians do not initiate service requests to the robot, in order to ensure the comfort requirements of pedestrians, the robot should try to avoid appearing within the comfort range of pedestrians as much as possible, that is, the robot should not enter the personal distance of pedestrians. In addition, according to reference, the lateral comfortable distance for the robot to overtake pedestrians from behind to the front is 0.7 m. If it is less than this distance, it will also cause discomfort to pedestrians. Therefore, based on the above requirements for pedestrian comfort, this paper modifies the reward function and penalizes the state where the robot enters the comfort range of pedestrians."}, {"title": null, "content": "I(x_0,y_0) = 0,\\ \\frac{x_0^2}{d_{person}^2} + \\frac{y_0^2}{d_{side}^2} \\ge 1"}, {"title": null, "content": "I(x_0,y_0) = 1,\\ \\frac{x_0^2}{d_{person}^2} + \\frac{y_0^2}{d_{side}^2} \\le 1\\ \\(9)"}, {"title": null, "content": "R^{social}(\\omega) = -r_p(e^{-(\\frac{\\omega - d_{side}}{\\omega})} \u2013 1)\\ (10)"}, {"title": null, "content": "\\omega=\\frac{1}{d^2_{person}}(x^2d^2_{side}) \\ (11)"}, {"title": null, "content": "R(s"}, {"content": ", a\u2081) = -0.25,if,d, \u22640\n       R(s"}, {"content": ", a\u2084) = 2, elseif, p\u2081 = p.\n        R(s"}, {"content": ", a) = 0, otherwise (12)"}, {"title": null, "content": "R(\\bf s_t, a_t) = -r_{Angle}, if \\vert \\Phi_t - \\Phi_{t-1} \\vert  \\geq \\pi"}, {"title": null, "content": "R(\\bf s_t, a_t) = 0, otherwise (13)"}, {"title": null, "content": "R(\\bf s_t, a_t) = R_0(\\bf s_t, a_t) + R^{social}(\\bf s_t, a_t)+ R_{Angle} (\\bf s_t, a_t) \\ (14)"}, {"title": "2.3 Network training", "content": "The training method of the entire reinforcement learning algorithm adopts the temporal difference method. To increase the utilization rate of samples, the information of each interaction between reinforcement learning and the environment is stored in the replay buffer. During network training, training data is obtained by sampling from the replay buffer through the method of uniform sampling. The fixed target network method is used to accelerate the convergence of the algorithm and reduce the variance of the algorithm. At the same time, to further accelerate convergence, before training, the dataset generated by the traditional method ORCA is used for network pre-training."}, {"title": "3 Experimental validation", "content": "This chapter will verify the effectiveness and generalization of the improved value function network and the feasibility of the comfort reward function by comparing the performance of obstacle avoidance algorithms with different encoding methods and the performance of obstacle avoidance algorithms before and after modifying the reward function.\nHardware platform: The hardware platform for the experiment is a desktop computer with Intel Core i7-7700 3.60 GHz and NVIDIA Geforce GTX 1050 Ti.\nSimulation environment: All experiments are carried out in a simulation environment built based on Gym. In the simulation environment, a circle with a radius of 0.3 m is used to replace pedestrians and robots. The task of pedestrians and robots is to run from the starting point to the target point. Among them, the movement of pedestrians is controlled by the traditional obstacle avoidance algorithm ORCA strategy. The selection of starting points and target points adopts a cross-encounter method. That is, the initial positions of pedestrians and robots are randomly located on the same circle with a radius of 4 m, and the target point is the point that is centrally symmetric to its initial position with respect to the center of the circle.\nRobot action space setting: The angular velocity and linear velocity of the robot in the simulation. In the experiment, the discrete action space of the robot is selected as the combination of the robot's linear velocity space and angular velocity space, with a size of 80. Among them, the linear velocity space is uniformly sampled in [0, Vpref], and the angular velocity space is uniformly sampled in [0, 2\u03c0], with sampling sizes of 5 and 16 respectively.\nNetwork setting: This paper first collects data from 3,000 training rounds through the traditional method ORCA to complete the initialization of network weights. After the pre- training is completed, in order to balance the exploration of unknown states and the utilization of existing results, the e- greedy method is used for training. At the beginning of training, in order to explore the unknown states in the environment more effectively, the exploration rate is set to 0.5. Then, as the number of training increases, the exploration rate is continuously reduced. At 5,000 rounds, it is reduced to 0.1. Subsequent training will no longer change the exploration rate. The remaining hyperparameters and the network parameters of the value function during the experiment are shown in Tables 1 and 2 respectively."}, {"title": "3.1 Experimental validation of the improved value function network", "content": "This subsection verifies the effectiveness of the improved value function network in this paper by comparing it with ORCA, CADRL, and obstacle avoidance algorithms with two different encoding methods based on social force and local map. In order to avoid the influence of the comfort reward function on this module, the reward function here adopts the reward function in paper. Therefore, the difference between the obstacle avoidance algorithms based on reinforcement learning lies only in the processing method of pedestrian interaction information. For convenience of expression, the algorithm based on social force encoding is called SF_RL, the method based on local map encoding is called LM_RL, and the algorithm based on APG encoding proposed in this paper is called APG_RL."}, {"title": "3.2 Experimental verification of the pedestrian comfort reward function", "content": "This experiment experimentally verified the feasibility of the reward function proposed in this paper. The values of rp and rangle are both 0.02. Without changing the APG_RL obstacle avoidance algorithm, training is carried out according to the comfort reward function designed by formula (13), and the comfortable obstacle avoidance strategies APG_CRL and APG_CARL are obtained respectively. The reward function of APG_CRL only adds the Rsocial (w) term, and the reward function of APG_CARL adds both Rsocialt(w) and RA t(sint, at)."}, {"title": "4 Conclusion", "content": "The comfortable obstacle avoidance algorithm for mobile robots proposed in this paper first addresses the deficiency that shallow value function networks are difficult to fit complex pedestrian environments. It improves the value function and designs a pedestrian interaction information module. The interaction information between pedestrians is extracted through the angular pedestrian grid, and the temporal features of pedestrian walking trajectories are extracted by using the attention mechanism. Secondly, by modifying the reward function, the pedestrian comfort requirements are introduced into the obstacle avoidance strategy, making the robot's obstacle avoidance strategy more in line with the needs of human-computer interaction. Through comparative analysis of simulation experiments, the algorithm proposed in this paper not only has a good obstacle avoidance success rate and adaptability in complex dynamic environments with dense crowds, but also can meet the requirements of pedestrian comfort.\nIn the future, this paper will continue to study the"}]}