{"title": "Progressive Multimodal Reasoning via Active Retrieval", "authors": ["Guanting Dong", "Chenghao Zhang", "Mengjie Deng", "Yutao Zhu", "Zhicheng Dou*", "Ji-Rong Wen"], "abstract": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.", "sections": [{"title": "1. Introduction", "content": "Reasoning, as the fundamental capability of large language models (LLMs) [25, 67, 72, 112] and multimodal large language models (MLLMs) [6, 14, 54, 125], lays the foundation for generalization across a wide range of downstream tasks such as mathematical reasoning [58, 59, 116] and visual question answering [1, 3, 37]. In complex reasoning scenarios, models often require multiple steps to seek a final answer, with each reasoning step potentially generating several branches and resulting in various candidate reasoning paths. Therefore, efficiently identifying the correct path that includes key problem-solving steps while eliminating incorrect ones is essential. To achieve this, reasoning verification techniques [49, 85, 106] enable models to explore multiple candidate solutions and employ a high-quality reward model for path selection, thereby offering a promising approach to improve the reliability of model reasoning.\nTo improve the trustworthiness of complex reasoning, foundational efforts such as outcome reward models (ORMs) [17, 88] directly verify the quality of entire reasoning trajectories. However, ORMs can only provide sparse and result-oriented feedback. To obtain finer-grained verification, process reward models (PRMs) [48, 49, 64, 85, 101, 102, 106] are designed, offering intermediate rewards after each step and incorporating reinforcement learning from human feedback (RLHF) algorithms [73] for backward supervision of generative models. Despite these advancements, the manual annotation of reasoning paths requires lots of human resources, limiting its scalability and applicability [111]. In response to these challenges, recent developments in inference-time scaling [82, 89] have led to the integration of the MCTS algorithm into LLMs [7]. This combination allows models to autonomously sample potential reasoning paths at each step during the expansion process. Then, different value functions are designed to estimate the quality of each path, followed by back-propagation and pruning, finally achieving automatic step-level reasoning annotation without human effort [2, 9, 29, 55, 61, 93, 98, 108, 114, 117, 118].\nWhile MCTS-based methods have been widely applied to text-based LLMs, their adaptation for MLLMs remains largely unexplored. Indeed, the distinct characteristics of multimodal scenarios require specialized adaptations of MCTS to effectively address their complexities. Let us illustrate the challenges by theoretically analyzing the limitations of existing MCTS-based methods. Given the input x at each expansion step of MCTS, and the best reasoning path y selected after simulation, the process could be modeled as:\n$p(y | x) = \\max_{r_i \\epsilon k} p_{\\theta}(y | r_i,x) \\cdot P(r_i | x),$ where r and k represent reasoning paths and the number of sampled paths, while & and 0 denote the generator and the verifier respectively. From this formulation, we can observe that both the expansion and simulation phases are crucial to the process, jointly determining the success of reasoning. Most existing approaches focus on optimizing the simulation process, while leaving the expansion process by beam search that relies on the model's internal knowledge [27, 83]. This simple strategy is effective for text-only reasoning tasks, as LLMs are sufficiently pre-trained on text data and their internal knowledge can be accurately measured. However, in multimodal reasoning tasks, the internal knowledge of MLLMs is insufficient for reasoning path expansion, because the interactions between inputs from different modalities frequently encounter misalignment [80, 90]. Such errors will grow larger as each step in the reasoning process depends on the previous one, causing small mistakes to become bigger over time [51, 66]. Consequently, developing effective strategies for reliable path expansion in MLLMs poses a significant challenge in multimodal reasoning tasks.\nTo address these problems, we propose to leverage retrieved external knowledge in reasoning path expansion, to enhance the path sampling quality in MCTS and improve MLLMs' capability in complex multimodal reasoning. Recent studies have confirmed that retrieval-augmented techniques can bridge knowledge gaps in multimodal reasoning [52, 56, 95], but they take all retrieved knowledge as a whole in the inference. Intuitively, each reasoning step requires different knowledge in a complex task. Therefore, we aim to dynamically provide the appropriate knowledge at each step of the reasoning process, thereby enhancing the accuracy of reasoning paths. Furthermore, we propose to incorporate diverse problem-solving insights into both the expansion and value function of the MCTS algorithm. This integration is expected to not only expand the diversity of the sampling space but also enhance the reliability of the reasoning verification process.\nSpecifically, we propose AR-MCTS, a universal framework dedicated to progressively improving the complex reasoning capabilities of MLLMs through Active Retrieval and Monte Carlo Tree Search. Specifically, we first design a unified retrieval module to retrieve key problem-solving insights for supporting complex reasoning from a hybrid-modal retrieval corpus. To further achieve reliable multimodal reasoning verification, we define the quality of each reasoning step as its potential to deduce the correct answer, enabling us to iteratively obtain step-wise annotations using the MCTS algorithm. Notably, we propose an active retrieval strategy during the MCTS expansion process, innovatively replacing beam search sampling with dynamically retrieved problem-solving insights, thereby enhancing both the diversity and reliability of the sampling space. Based on these fine-grained annotations, we progressively align a process reward model tailored for multimodal reasoning through step-wise Direct Preference Optimization (DPO) [78, 79] and Supervised Fine-tuning (SFT) objectives, achieving automatic process-level reasoning verification.\nExperimental results on three complex multimodal reasoning benchmarks demonstrate the effectiveness of AR-MCTS across various proprietary models. Further analysis reveals that AR-MCTS optimizes both sampling diversity and verification accuracy, providing a promising solution for reliable multimodal reasoning.\nIn summary, our contributions are as follows:\n*   We theoretically model the key mechanisms of the MCTS-based approach in Equation (1), revealing its core limitations in solving multimodal reasoning problems.\n*   We are the first to introduce the retrieval mechanism in each step of multimodal reasoning to replace traditional model self-sampling strategies, enhancing both sampling diversity and accuracy of multi-step reasoning.\n*   We propose the AR-MCTS framework, which leverages the MCTS algorithm alongside an active retrieval strategy for improving multimodal reasoning. This framework automatically acquires high-quality step-wise reasoning annotations to progressively align a process reward model, ultimately enabling reliable automated multimodal reasoning verification."}, {"title": "2. Related Work", "content": "LLM and MLLM Reasoning. Large Language Models (LLMs) [25, 67, 72, 97, 112] and Multimodal Large Language Models (MLLMs) [6, 13, 14, 42, 54, 104, 125] have rapidly advanced, with broad applications in mathematics [106, 126], programming [31, 91], medicine [45], character recognition [76, 109]. Among their diverse capabilities, reasoning stands out as the most critical, serving as a foundational step toward universal understanding. Methods such as Chain-of-Thought (CoT) [110], Tree-of-Thought (ToT) [113], and Program-of-Thought (PoT) [12, 28] enhance logical coherence and response complexity by guiding models to decompose problems progressively, applying structured prompts and targeted training objectives, including multimodal tasks [121]. Another key approach is the reflection mechanism, which prompts models to iteratively evaluate and refine responses, leading to improved coherence. Prior studies [107] show that generating diverse reasoning paths and selecting the most consistent responses enhances LLM reasoning. Moreover, some efforts enhance the reasoning capability of LLM and MLLM by integrating data augmentation during the SFT phase [46, 59, 87, 115], along with utilizing external tools [30, 44]. Research [17, 48, 64, 86, 88, 100, 102, 106] has also highlighted robust reward models as a promising means to optimize response quality. Recently, OpenAI's ol 1 introduced a \u201cslow thinking\u201d mechanism that, combined with Monte Carlo Tree Search (MCTS) strategies and verification models, simulates gradual reasoning to improve accuracy [29, 117, 118]. While these advancements are focused on single-modal LLMs, the reasoning potential of MLLMs remains under explored.\nMultimodal Retrieval-Augmented Generation. Recent Retrieval-Augmented Generation (RAG) has shown exceptional performance across various NLP tasks for LLMs by incorporating relevant information from diverse sources [4, 16, 20, 22, 24, 35, 40, 47, 60, 96]. This approach can also enhance reasoning and question-answering in the multimodal domain through cross-modal integration [50, 52, 95, 124]. However, the reasoning process is largely unexplained and lacks verification mechanisms. In this paper, we propose an active retrieval strategy that retrieves multimodal information at each step to align the PRM, facilitating reliable reasoning verification."}, {"title": "3. Preliminary", "content": "Monte Carlo Tree Search. MCTS is a widely used sampling-based search method for decision-making optimization. Its core algorithm consists of four steps: selection, expansion, evaluation, and back-propagation. By repeatedly executing these four steps, it constructs a search tree. During the selection phase, MCTS recursively selects child nodes from the root using the Upper Confidence Bound (UCB) [92]:\n$UCB(i) = w_i + C * \\sqrt{\\frac{ln N}{N_i}}$\nProblem Formulation. Formally, in multimodal reasoning, given a multimodal query $Q_m$ and corresponding retrieved problem-solving insights r from the retrieved hybrid-modal corpus $D_H$, we assume that the MLLM $\\pi_\\theta$ operates in an auto-regressive manner to generate a reasoning path of k steps $[Y_1,..., Y_k]$:\n$P_\\theta (y | Q_m, R) = \\prod_{i=1}^{k} p_\\theta (y_i | Q_m, r, Y_{<i}).$\nIn this paper, we obtain different intermediate reasoning trajectories as the MLLM decodes to a specific termination token. Following the setup of [99], we formulate the generation process as a Markov Decision Process (MDP) [74] and adopt sentence-level MCTS modeling. In reinforcement learning terminology [84], we define the current decoded intermediate step $y_i$ as a state $s_i$, corresponding to a leaf node. The process of backtracking to sample the next step is considered an action $a_i$. A list of detailed definitions of MCTS for reasoning is given in the supplementary materials."}, {"title": "4. Methodology", "content": "Our goal is to establish a process-level verification framework for improving multimodal reasoning without human annotation, while improving the diversity and accuarcy of candidate solution sampling. Therefore, we propose AR-MCTS framework to achieve fine-grained reasoning verification through active retrieval and Monte Carlo tree search.\nAs shown in Figure 2 & 3, AR-MCTS consists of two main components: 1) It introduces a unified retrieval module, including a high-quality hybrid-modal retrieval corpus (\u00a74.1) and a multimodal retrieval module (\u00a74.2). This module employs knowledge concept filtering to select key insights for problem-solving (\u00a74.3). 2) It automates the acquisition of step-wise annotations for multimodal reasoning using MCTS and an active retrieval mechanism (\u00a74.2). Then, it leverages the annotated data to progressively align the PRM in two stages (\u00a74.5), allowing for fine-grained verification of MLLM reasoning."}, {"title": "4.1. Hybrid-Modal Retrieval Corpus Construction", "content": "In an ideal scenario, improving reasoning capabilities through retrieval is akin to giving MLLMs an open-book exam. Unfortunately, the multimodal reasoning field consistently suffers from a lack of high-quality reasoning retrieval corpora. To systematically build a high-quality reasoning retrieval library, we conduct a comprehensive survey of open-source datasets, focusing on both general and mathematics-specific reasoning knowledge in multimodal reasoning.\nMathematics-Specific Reasoning Knowledge. Mathematical reasoning is an essential skill of fundamental models, accompanied by the emergence of a series of high-quality datasets. In the text-only aspect, we select the most widely used mathematical reasoning datasets, GSM8K [18] and MATH [33]. For the multimodal domain, we adopt four meticulously cleaned high-quality multimodal math datasets: MATHVISTA [58], MathVerse [120], MathVision [103], and WE-MATH [75]. To further prevent data leakage, we filtered out any overlapping portions with our testing benchmark using regular expressions, concatenating each sample's question q, solution process p, and answer a into a single text format, along with the corresponding image storage paths. Ultimately, we obtain 22K text-only QA pairs and 12.5K multimodal sample pairs as proprietary sources DM from six data sources, covering over 20 mathematical sub-fields, with each sample containing detailed solution steps.\nGeneral Reasoning Knowledge. In the real world, general reasoning extends beyond natural subjects. To address this broader need, we follow the traditional RAG approach [41, 122] by utilizing the web-based retrieval source Wikipedia alongside the COIG [119] large-scale question bank as our general reasoning retrieval sources. We conduct thorough data cleaning and chunking operations, ultimately constructing this extensive dataset as our general reasoning knowledge base DG. The statistical information of our hybrid-modal reasoning corpus D\u2081 = DMU DG is presented in Figure 2. More detailed information of processing retrieval corpus can be found in the supplementary."}, {"title": "4.2. Unified Multimodal Retrieval Module", "content": "Given a text-image pair from the multimodal test set $Q_m$ = {x,t}, our goal is to retrieve the top-K multimodal relevant knowledge for each sample. Since our retrieval library encompasses hybrid-modal retrieval sources, two retrieval processes are considered to obtain the top-K pairs:\nText Retrieval. Given a text query q for multimodal sample, we aim to use a dense retriever to retrieve k relevant documents $D_q$ = {$d_i$}$_{i=1}^{k}$ from a text-only corpus. In this work, we employ Contriever [34] to obtain hidden vectors for both queries and documents. The relevance score is calculated by computing the dot-product similarity between the query and document representations, which facilitates the retrieval of the Top-K documents $D_q$ as follow:\n$D_q = argtop_{i=1...k} [E_d(d_i). E_q(q)].$\nCross-modal Retrieval. We utilize widely used contrastive vision-language models CLIP [77], which utilizes a dual-stream architecture featuring an image encoder $E_I(\u00b7)$ and a text encoder $E_T(\u00b7)$. Furthermore, we use CLIP to encode image-text pairs (x, t), obtaining the image and text vectors $E_I(x)$ and $E_T(t)$. Since the hybrid-modal retrieval corpus contains both multimodal and text-only samples, we follow previous work [95] to derive encoding vectors for the entire hybrid-modal corpus $D_H$ as follows:\n$E_X(x,t) = \\begin{cases}\nE_I(x)+E_T(t), & if t \\neq \\oslash and x \\neq \\oslash, \\\\\nE_T(t), & if t\\neq \\oslash and x = \\oslash.\n\\end{cases}$\nwhere denotes empty set. For the i-th multimodal query $Q_m$, we encode it into a mixed vector $E_X(Q_m) =  E_I(x)+E_T(t)$. We perform cross-modal retrieval between the encoding of each multimodal query and the entire retrieval database, utilizing FAISS [36] for indexing to retrieve K samples for each query:\n$D_{cross} = argtop_{j=1...,N} [E_X(Q_m). E_X(x_j, t_j)].$\nHere, $E_X(Q_m)$ and $E_X(x_j,t_j)$ denote the embeddings of the multimodal query and the samples in the hybrid-modal corpus, with indices j ranging from 1 to N to ensure that the entire retrieval database is considered."}, {"title": "4.3. Knowledge Concept Filtering", "content": "In our deployment process, we observe that multimodal reasoning with retrieved knowledge is highly sensitive to the consistency of fine-grained knowledge concepts (e.g., algebra knowledge can't help in solving triangles problem). Notably, most high-quality visual mathematical benchmarks provide detailed category labels (e.g., \"Angles and Length\") for each sample, motivating us to consider knowledge concept for fine-grained filtering. Given a multimodal query $Q_m$ and its knowledge concept label $L_{kc}$, we encode the top-K retrieved hybrid-modal samples from D\u2081 = {DqU Dcross} according to Equation (5) and compute the similarity with the knowledge concept's embedding $E_T(kc)$ following the pipeline in \"Cross-Modal Retrieval\". We strictly enforce the original retrieval similarity threshold T, and the knowledge concept consistency threshold $T_{kc}$, allowing only those samples that meet both criteria to serve as key insights $D_{ins}$ for the query $Q_m$:\n$D_{ins} = {r \\epsilon D_H | Sim(r, Q^m) > T_r & Sim(r, L_{kc}) \\geq T_{kc}},$\nwhere Sim(x, y) represents the cosine similarity between the embeddings E(x) and E(y), r \u2208 D\u00ed denotes a retrieved insights from the corpus DH. Detailed information of the filtering process can be found in supplementary materials."}, {"title": "4.4. Progressive Multimodal Reasoning Annotation", "content": "In this section, we employ MCTS via active retrieval to facilitate MLLMs in the automatic generation of step-wise reasoning annotations, as shown in Figure 3. Through the self-exploration process, we obtain Q values at each step (node) to capture potential reasoning errors in the intermediate steps. Below, we will present our detailed algorithm design, which includes four core operations:\n*   Selection. During the j-th simulation of the AR-MCTS, the process begins with so, representing the initial state containing the multimodal input query $Q_n$ = (xo,to) and corresponding retrieved problem-solving insights ro. The algorithm then proceeds to explore the Monte Carlo tree by selecting as Equation (2) iteratively, then we can formulate the multimodal query of state $s_j$ as $Q_j^m$ = {(xj, tj) | tj = to + \u2211m i=1 yi}, as shown in Figure 3.\n*   Expansion with Active Retrieval Strategy. Given the state si represented by the selected leaf node, the MCTS-based approach backtracks to the prior state, forming our multimodal input as (xi, ti, ri). The temperature in the traditional expansion process is empirically increased to greater than 0.6 to sample multiple potential candidate actions for the next step [10]. Unlike them, we emphasize that the supporting knowledge required for different reasoning trajectory at each step should vary, and propose an Active Retrieval strategy. As shown in Figure 3, during the MCTS expansion phase at state si, we first concatenate the input $Q_n$ with the previous reasoning steps. Then we dynamically retrieve the required candidate insights ri for each step from the problem-solving insight library $D_{ins}$ according to Equation (6), and replace the insight ri-1 from the previous step with the latest retrieved insights ri. According to the Equation (3), the process of sampling k reasoning paths at state si can be modeled as follows:\n$P_\\theta (y | x) = \\prod_{i=1}^{k} p_\\theta (\\{Y_i\\}_{i=1}^k | Q_j^m, r_i).$\n*   Simulation. We use the probability of deducing the correct answer based on partial solutions as a criterion for quality assessment. Following Wang et al., we apply a one-step rollout for each node obtained during expansion to ensure efficiency, and we construct a value function as\n$V(s_i) = \\sum_{j=1}^{k} I(y_j = \\hat{y_i})$, where k, I denotes the number of sampled reasoning paths and the indicator function. If the final answer yj equals the grounding truth \u0177\u2081, we set the value of the current node to 1; Otherwise, we set it to 0.\n*   Back-Propagation. For the terminal nodes reached during the rollout and the current leaf node, MCTS performs a backward update of the visit count and Q-value for each (s, a) along the route from the current node to the root, which is fomulated as N(s,a) \u2190 N(s,a) + 1, $Q(s, a) \u2190 Q(s, a) + \\frac{1}{N(s,a)} (V(s) - Q(s, a))$"}, {"title": "4.5. Curriculum Process Reward Modeling", "content": "After acquiring step-wise reasoning annotations, we draw inspiration from curriculum learning [23, 94] to design a two-phase approach for PRM. In the first stage, the model learns to distinguish the correctness of reasoning steps. In the second stage, it learns to assign scores to each step, facilitating generalization from easy to hard.\nStep-wise DPO Pre-alignment. In the first phrase, each round of expansion and evaluation in AR-MCTS naturally generates batches of positive and negative pairs, inspiring us to align preferences using step-level Direct Preference Optimization (DPO) as the training objective. Under the state si (i-th step in reasoning), given a multimodal query $Q_n$ = (xi,ti) and a sampled reasoning paths set $Y_i$ = {$y_j$}$_{j=1}^K$, along with the corresponding value set $V_i$ = {vj}$_{j=1}^K$. We filter the solution paths in $Y_i$ with value vj > 0.8 as positive samples $y^+_i$, while those with vj = 0 are regarded as negative samples $y^-_i$. Therefore, for each problem $Q_n$, we can obtain K pairs of step-level preference pairs $D_{step}$  (y,y)$_{k=1}^K and follow step-level DPO to align the reasoning discernment capability as:\n$L_{SDPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(Q_m,y^+,y^-) \\sim D_{step}} [log\\sigma(\\beta log\\frac{\\pi_{\\theta}(y^+|Q_m)}{\\pi_{ref}(y^-|Q_m)} - log\\sigma(\\beta log\\frac{\\pi_{\\theta}(y^-|Q_m)}{\\pi_{ref}(y^-|Q_m)}))],$\nThe reference model ref is initially set to \u03c0\u03b8 and remains constant during training. Here, \u03b2 is a hyperparameter, and \u03c3 denotes the sigmoid function. The objective of LSDPO is to maximize the likelihood of preferred y+ compared to the dispreferred y\u00af.\nPoint-wise Fine-tuning. After pre-alignment, our PRM has gained the initial ability to distinguish the correctness of step-wise reasoning. To further unlock its reasoning scoring capability, we apply a step-level cross-entropy objective to the pre-aligned PRM ADPO using the following parameters:\n$L_{PFT} = \\sum_{i=1}^{N} [\\hat{y_i} log\\sigma_{DPO}(r_i) + (1 - \\hat{y_i}) log\\sigma_{DPO} (1 -r_i)],$\nwhere yi is the golden label (0, 1) for the state si, ri is the sigmoid score assigned by PRM. After the above two stages, we progressively achieve an aligned process reward model.\nInference. During the inference phase of AR-MCTS, we utilize the fine-tuned PRM to follow the steps of AR-MCTS in Figure 3, employing the PRM scores as the value for each step in the evaluation phrase. Following Luo et al., we adopt point-wise soft labels. We also make a discussion of PRM's hard labels in the supplementary materials. Unlike the annotation process for training data, we extract the top-scoring node from the K expansion reasoning paths each round, discarding the other low-quality paths. Moreover, we set an early stopping criterion of 4, which allows us to derive the final result directly in the 4-th round to reduce computational complexity."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nTo assess the effectiveness of the AR-MCTS, we provide a detailed introduction from the following aspects:\nBenchmarks and Baselines. We perform experiments on two widely used multimodal mathematical reasoning benchmarks: MATHVISTA [58] and WE-MATH [75]. To further validate our AR-MCTS in general reasoning domain, we perform cross-domain evaluation on the GAOKAO-MM benchmark [127]. For baselines, we employ AR-MCTS on strong proprietary and open-source models: (1) Closed-source MLLMs: GPT-40 [70], GPT-4V [71]; (2) Open-source MLLMs: LLaVA-OneVision-Qwen2-72B [43], InternVL2-8B [15], Qwen2-VL-7B [105], LLaMA3-LlaVA-NeXT-8B [53]. Referencing relevant works on MCTS [99, 106], we implement Self-Consistency [107], Self-Correction [32], and ORM [17] as our core comparison strategies.\nData Sampling via AR-MCTS. As highlighted by Math-PUMA [126], the challenge arises because the three multimodal benchmarks we evaluate lack training sets. Following the collection described in \u00a74.1, we utilize four multimodal and two text-only datasets for process annotation, excluding any sources currently under evaluation. We extract multimodal QA pairs and use our AR-MCTS algorithm to automatically generate and annotate detailed solution processes. Notably, the GAOKAO-MM dataset is entirely in Chinese, which complicates reliance on English data sources. To address this, we classify data from 2010 to 2021 for AR-MCTS annotation, while questions from 2022 to 2023 serve as the test set. For a comprehensive overview of the experimental setup, please find in the supplementary materials.."}, {"title": "5.2. Overall Results", "content": "Table 1 illustrates the main results. Overall, AR-MCTS significantly improves multimodal reasoning performance across various MLLMs and reasoning verification strategies (Self-Correction, Self-Consistency) on two benchmarks, conclusively demonstrating the advantages of our approach. Furthermore, we have identified the following insights:\nMLLMS struggle to self-correct reasoning errors. The self-correction strategy struggles across both reasoning benchmarks. Although a minor improvement is noted with GPT-40, other weaker open-source MLLMs experience significant declines after the self-correction process, particularly Qwen2VL-7B, which shows a drop of over 8% on MATH-VISTA (ALL). This discovery corresponds with the findings of He et al., highlighting the instability of correction methods that rely on the self-knowledge of MLLMs in multimodal reasoning, especially in models with fewer parameters.\nPRM outperforms ORM in complex reasoning tasks. WE-MATH is a step-level evaluation featuring S1 to S3, which progressively increases the difficulty of reasoning steps. Compared to ORM, AR-MCTS with PRM demonstrates a more significant performance improvement across most MLLM backbones on the S3 metrics in WE-MATH (GPT-40: 56.4% vs 50.3%; Qwen2-VL: 40.6% vs 34.6%). This highlights that PRM, by meticulously verifying each step of the reasoning process, achieves stronger alignment in multi-step reasoning tasks.\nAR-MCTS better unlocks the reasoning potential of weaker MLLMs. Compared to LLaVA-OneVision-72B, Qwen2-VL-7B with AR-MCTS shows a significant improvement over the zero-shot setting on MATHVISTA (ALL: 5.3%\u2191) and in WE-MATH (AVG: 8.3%\u2191). A similar conclusion is observed with InternVL2-8B, indicating that the performance gains of AR-MCTS are more pronounced in smaller MLLMs. To gain insight into this result in conjunction with Equation (1), we consider that, under the same verifier, weaker MLLMs may sample the correct answers but struggle to directly decode those paths greedily. This suggests that smaller MLLMs have the potential for correct reasoning but may not successfully decode answers relying solely on internal knowledge. This observation further verifies the importance of integrating active retrieval in multimodal reasoning. It also demonstrates that AR-MCTS is a reliable and plug-and-play framework, offering a promising solution for reasoning alignment in weaker MLLMs."}, {"title": "5.3. General Reasoning Domain Verification", "content": "To validate the effectiveness of AR-MCTS in the general multimodal reasoning field, we further evaluate the Chinese human-level multimodal reasoning benchmark, GAOKAO-MM. As shown in Table 2, both the closed-source model GPT-40 and the open-source small model Qwen2-VL-7B demonstrate significant improvements over the backbone and self-consistency approaches when combined with the AR-MCTS framework, verifying the generalization of AR-MCTS across different languages and reasoning disciplines. Notably, AR-MCTS with GPT-4o achieves stable improvements in mathematics and physics (12.5%\u2191 and 7.7%\u2191), while also showing some gains in the humanities (e.g. history 20%\u2191). This emphasizes that AR-MCTS with PRM not only improves the complex reasoning capabilities of MLLMs, but also effectively mitigates the knowledge gaps of MLLMs in the humanities through its active retrieval mechanism."}, {"title": "5.4. Quantitative Analysis", "content": "Ablation Study. To explore the effects of various components in AR-MCTS, we conduct an ablation study in Table 3. The term \"w/o\" indicates versions where specific components are removed. Our key observations are: 1) Removing any component from AR-MCTS results in performance decline, highlighting the necessity of all component designs. 2) Removing the PRM and active retrieval mechanism leads to a significant performance drop respectively (MATHVISTA: 3.1% & 2.2%), demonstrating that step-wise verification and active retrieved knowledge can effectively improve multimodal reasoning capabilities. 3) Notably, knowledge concept filtering also achieves stable performance gains, indicating that it effectively reduces noise in retrieved knowledge and highlights the critical importance of consistency between the retrieved knowledge and the problem during reasoning. Detailed ablations can be found in the supplementary materials.\nScaling Analysis on Inference Samplings. In this section, we conduct a scaling analysis to evaluate the performance of different strategies across two benchmarks with varying numbers of candidate solution paths, ranging from 1 to 32. As shown in Figure 4, our main observations are as follows: 1) When the number of candidate solutions exceeds a certain threshold (16), self-consistency (SC) exhibits some performance fluctuations in WE-MATH. 2) AC-MCTS consistently outperforms ORM and SC, with this superiority becoming more pronounced as N increases. We attribute this advantage to our automated process labeling, which offers high scalability and low annotation costs while providing more reliable feedback for the verification of each path."}, {"title": "5.5. Does AR-MCTS Improve the Sampling Space?", "content": "In this section, we explore that AR-MCTS can efficiently improve the quality of the candidate solution sampling from the following two perspectives:\nAccuracy Analysis. To validate that AR-MCTS can efficiently improve the solution sampling accuracy in multimodal reasoning, we perform a quantitative analysis of the \"Correctness of questions\" during the sampling process of Qwen2-VL in the MATHVISTA and WE-MATH. The accuracy can be formulated as $P_\\theta = \\frac{N_o}{N_o}$, where No denote the number of questions contain at least one correct candidate solution, while No denotes all number of questions. As shown in Figure 6, AR-MCTS demonstrates consistent gains in both benchmarks compared to the traditional beam search sampling. Moreover, as the number of candidate solutions increases, the answer accuracy Po exhibits a positive correlation. This finding further confirms that AR-MCTS can efficiently improve the reliability of the sampling space in multimodal reasoning, effectively addressing the inherent challenges of MCTS-based methods.\nDiversity Analysis. To investigate whether AR-MCTS can truly enhance the diversity of sampled solutions, we sample 250 problems from MATHVISTA and employ AR-MCTS to sample 4 candidate solutions for each problem, yielding a total of 1,000 samples. We employ BGE-M3 [11] as our semantic embedding model and apply PCA for dimensionality reduction, followed by DBSCAN [26] clustering for the visualization of all solution semantic representations.\nFigure 5 shows the visualization between the beam search (left) and AR-MCTS (right). The representations of candidate solutions sampled by beam search tend to collapse into a small area with several noise points (in gray), reflecting that the beam search may lead to redundancy in sampling. Under the same parameter settings, AC-MCTS clusters more centroids for the same problem set (38 vs.46) and exhibits a more dispersed representation distribution. This effectively demonstrates that AR-MCTS alleviates the issue of limited diversity in candidate solutions sampled, efficiently covering the problem-solving space and providing strong prior conditions for the simulation process of MCTS."}, {"title": "6. Conclusion", "content": "In this paper, we propose AR-MCTS, a universal framework dedicated to progressively improving the complex multimodal reasoning capabilities of MLLMs through active retrieval and Monte Carlo Tree Search. AR-MCTS leverages the MCTS algorithm alongside an active retrieval strategy, which automatically acquires high-quality step-wise reasoning annotations to progressively align a process reward model, ultimately enabling process-level multimodal reasoning verification. Experimental results demonstrate the effectiveness of AR-MCTS across various MLLMs and benchmarks, confirming its ability to optimize sampling diversity and verification accuracy, and providing a promising solution for reliable reasoning."}]}