{"title": "Collaborative Imputation of Urban Time Series through Cross-city Meta-learning", "authors": ["Tong Nie", "Wei Ma", "Jian Sun", "Yu Yang", "Jiannong Cao"], "abstract": "Urban time series, such as mobility flows, energy consumption, and pollution records, encapsulate complex urban dynamics and structures. However, data collection in each city is impeded by technical challenges such as budget limitations and sensor failures, necessitating effective data imputation techniques that can enhance data quality and reliability. Existing imputation models, categorized into learning-based and analytics-based paradigms, grapple with the trade-off between capacity and generalizability. Collaborative learning to reconstruct data across multiple cities holds the promise of breaking this trade-off. Nevertheless, urban data's inherent irregularity and heterogeneity issues exacerbate challenges of knowledge sharing and collaboration across cities. To address these limitations, we propose a novel collaborative imputation paradigm leveraging meta-learned implicit neural representations (INRs). INRs offer a continuous mapping from domain coordinates to target values, integrating the strengths of both paradigms. By imposing embedding theory, we first employ continuous parameterization to handle irregularity and reconstruct the dynamical system. We then introduce a cross-city collaborative learning scheme through model-agnostic meta learning, incorporating hierarchical modulation and normalization techniques to accommodate multiscale representations and reduce variance in response to heterogeneity. Extensive experiments on a diverse urban dataset from 20 global cities demonstrate our model's superior imputation performance and generalizability, underscoring the effectiveness of collaborative imputation in resource-constrained settings.", "sections": [{"title": "INTRODUCTION", "content": "TIME series measured in urban agglomerations, such as mobility flows, energy consumption, and pollution records, represent time-dependent urban patterns and dynamics. These high-granular quantities can be exploited by data-driven models to reflect latent profiles of cities, such as human activity, socio-economic and welfare [1]. To utilize such data, either Eulerian sensors with dense spatial deployments or Lagrangian sensors with high temporal coverage are desired to measure them [2]. However, access to city-scale holographic data is far from easy due to factors such as installation and maintenance costs, adverse observation conditions, and system errors, hindering the usage of urban computing applications [3], [4]. Therefore, data imputation technique has emerged to compensate for the lack of full observations and enhance data quality and reliability.\nExisting data imputation models generally fall into two paradigms: learning-based and analytics-based solutions. However, both paradigms face the dilemma of capacity and generalizability. The former applies neural architectures that excel in fitting the time series distribution, such as RNNs [5], [6], probabilistic diffusion [7], [8], and Transformers [9], [10]. However, deep time series models struggle to comprehend underlying physical laws of data, leading to the potential for overfitting in masked training and lacking generalizability to unseen data outside of the training distribution [11]. The latter formalism designs analytical models to characterize commonly shared data properties, showing better generalizability across data. One prominent example is the matrix completion model [12], [13], [14]. Unfortunately, they have limited model capacity and are restricted to fixed dimensions with a predefined input domain. As a result, they usually require per sample optimization, preventing them from modeling complex and diverse urban datasets.\nPromisingly, the recent success of foundation models has demonstrated the potential of learning from the union of diverse datasets [15], [16]. Large models pretrained on cross-source data hold promise for breaking the trade-off. This inspires us to develop an expressive and generalizable imputation paradigm across multiple cities. However, public agencies in each city currently have to develop individual models based on separate expertise, which is resource-intensive with a specialized computational procedure. More critically, urban data within each city can only be used for the city and considered in isolation from each other. This hinders its accessibility and information share in resource-constrained contexts, where less developed cities have low observation rates and limited technical capacity to fully train a model. With a generalizable model and collaborative data governance, the shared knowledge from data-rich cities can inform different-but-related patterns in data-lacking cities.\nOverall, the benefits of cross-city learning prompt us to explore the possibility of an innovative and alternative scheme for collaborative imputation. To this end, we summarize the challenges in urban time series imputation under practical constraints of observation conditions as: irregularity and heterogeneity. First, urban time series is irregularly sampled in nature. The data might be generated as a burst or with varying time intervals, and different sensors can have different sampling frequencies. Furthermore, missingness can manifest itself at arbitrary locations and timestamps [17]. To leverage state-of-the-art imputation models, one may convert an irregular time series into a regular time series. This can cause information loss and misinterpretation of the missing pattern. Second, urban data displays high heterogeneity [18], [19]. Measurements from different locations show diverse localized patterns such as multiple scales and frequencies. Consequently, collaborative learning from the joint distribution of heterogeneous data requires rigorous model design and large capacity. In addition, such heterogeneity increases the difficulty in discovering shared patterns that are generalizable between different cities.\nIn summary, most state-of-the-art models either apply instance-specific optimization with fixed spatial-temporal dimensions [13] or are trained to interpolate discrete signals in regular grids for a single data source [10]. They are biased toward particular data sources and compromise between accuracy and generality depending on the applied domain [20], hindering knowledge transfer across heterogeneous cities. To resolve this dilemma, we resort to an emerging approach called implicit neural representations (INRs). INRs have recently been shown to be proficient in learning representations from multimodal data, such as 2D images, 3D scenes, audios, time series, and spatiotemporal data [21], [22], [23], [24], [25], [26], [27]. INRs represent data instances by parameters of neural networks and establish continuous mappings from domain coordinate to quantity of interests. They inherit merits of both two paradigms with high expressivity and universal priors such as smoothness.\nBy capitalizing on the recent advancements in INRs, we first tackle the irregularity using continuous parameter-"}, {"title": "2 RELATED WORK", "content": "Spatiotemporal data imputation. As missing data is pervasive in spatiotemporal system, many recent studies have been devoted to establishing data imputation models [31]. Existing methods can be categorized into two types, i.e., learning-based and analytics-based methods. The former adopt deep neural networks to correlate observed values and learn to utilize the correlations to fill unobserved values. Popular architectures such as recurrent neural networks, diffusion models, and Transformers are widely adopted [5], [6], [7], [8], [9], [10], [32], [33]. Analytical models approach the data imputation problem by solving an optimization problem associated with missing data patterns [13], [25], [34], [35], [36], [37], [38]. Representative methods include low-rank matrix factorization and tensor completion. The two existing paradigms facilitate the development of a specific imputation model for a particular dataset. Collaborative imputation of data across multiple datasets remains unexplored. In addition, although there are some imputation models designed for irregular time series [39], [40], [41], [42], their abilities in large-scale urban datasets are less explored.\nImplicit neural representations. INRs are a class of methods to implicitly define a quantity of interest by associating the target value with its coordinate. By fitting the given data using coordinate-based neural networks, INRs provide an alternative to store and query data. Due to their continuous, expressive, efficient properties, INRs have achieved success in learning multimodal data, such as images, videos, 3D scenes, point cloud, and audio data [21], [22], [23], [24], [25], [43]. In addition, INRs have recently been adopted for spatiotemporal data [27], [44] and time series [26], [42], [45], with a main focus on reconstructing and forecasting tasks. Most of these studies either concentrate on a single task from a particular data source or evaluate the performances of INRs on various different tasks in parallel.\nMeta-learning INRs. As an INR is fitted to a single instance using weights of neural networks, the learned weights cannot be applied to predict other instances directly. To address this limitation, generalizable implicit neural representations (GINRs) are developed using gradient- or Transformer-based meta-learning algorithms [46], [47], [48], [49], [50]. The mechanism of GINRs is to model the distribution of functional representations using separate parameters, making them adaptable to different instances with a shared meta model and specific task models. Prior research on GINRs has focused on training them on homogeneous data, such as images, with the objective of developing a task-independent decoder. The potential of GINRs for cross-city learning of large-scale urban datasets has yet to be fully investigated.\nCross-city generalization. To encourage data and resource sharing between cities, previous work develops transfer-learning schemes for cross-city learning, with a particular focus on traffic forecasting problems [51], [52], [53], [54], [55], [56], [57]. With abundant observations available, the knowledge from source cities can be readily transferred to the target city. However, for the studied imputation task, cross-city generalization is more challenging with sparse data. In addition, as discrete neural networks are adopted as backbones, they can only deal with urban data with regular data organization, e.g., fixed spatial-temporal dimension."}, {"title": "3 PRELIMINARY", "content": "Notations. We denote matrices by boldface capital letters e.g., $X \\in \\mathbb{R}^{N \\times T}$, vectors are denoted by boldface lowercase letters, e.g., $x \\in \\mathbb{R}^{T}$, and scalars are lowercase letters, e.g., $x$. Without ambiguity, we also denote $x(t) \\in \\mathbb{R}^m$ an arbitrary time series with time index $t$. The functional representation (or a mapping function) is abbreviated as $\\Phi(\\cdot)$. In the absence of remarks, calligraphic letters are used to denote the vector space, for example $\\mathcal{X} \\subset \\mathbb{R}$.\nModel-agnostic meta-learning. To utilize the shared pattern and knowledge between source and target data, a transfer learning algorithm needs to be established. The meta learning framework is developed for learning the learning algorithm itself. In particular, the model-agnostic meta-learning (MAML) algorithm [30] is designed to train a deep neural network on a variety of learning tasks, such that the source model is explicitly fine-tuned to generalize to a downstream task with few labels using a small number of gradient steps. MAML obeys the following iterative scheme:\n$\\min_{\\Theta} \\mathbb{E}_{\\tau \\sim p(\\tau)}[\\ell_{\\text{test}} (\\Phi_{\\tau, K}(\\Theta))]$,\n$\\text{s.t.} \\ \\Phi_{\\tau, k+1} \\leftarrow \\Phi_{\\tau, k} - \\alpha \\nabla_{\\Phi_{\\tau,k}} \\ell_{\\text{train}},\\ \\Phi_{\\tau, 0} = \\Theta$,\nwhere a task-specific model $\\Phi$, on task $\\tau$ is initialized by the parameter of meta-model $\\Theta$ and iteratively updated with $K$ steps based on a supervision loss (train of a few training samples in the inner loop. In the outer loop, the meta parameters $\\Theta$ are updated by minimizing the test loss $\\ell_{\\text{test}}$ over a batch of tasks $\\mathbb{E}_{\\tau \\sim p(\\tau)}$ with the gradient-adapted parameters $\\Phi_{\\tau,K}$. MAML yields a meta-model that is explicitly fine-tuned to differentiate different instances.\nProblem formulation. If all time series are recorded synchronously and the sensor number is fixed during the observation period, the data can be organized as a spatiotemporal matrix $X \\in \\mathbb{R}^{N \\times T}$ with $x^i \\in \\mathbb{R}^{T}$ denoting the $i$-th series and $x_t \\in \\mathbb{R}^{N}$ indicating observations at time $t$. Then the urban data imputation problem can be considered as estimating a probability $ \\forall \\tau \\in \\{1, ..., T\\}, n \\in \\{i = 1, ..., N \\}:$ \n$p(\\{x_t^i | m_t^i = 0\\} | \\{x_t^i | m_t^i = 1\\}_{t=1}^{\\tau}),$\nwhere $m_t^i \\in \\{0, 1\\}$ is an indicator of observable which is 1 if the measurements associated with the $i$-th sensor are valid at time step $t$. This posterior probability is estimated by a parameterized model (such as a neural network) by:\n$\\Phi(\\Omega(X) | \\Theta) \\approx \\mathbb{E}[p(\\{x_t^i | m_t^i = 0\\} | \\{x_t^i | m_t^i = 1\\})]$,\nwhere $\\Omega(X) = x_t^i$ if $m_t^i = 1$.\nIn the case of irregularly sampled data, each time series sampled from different cities or sensors can have different sampling frequencies and dimensions. We use a sequence of data pairs to denote this irregularity as $x^i = \\{(x^i_t, \\xi^i, t_t)\\}_{t=1}^{T_i}$ where the observation is paired with its location and time-index feature $(\\xi^i, t_t)$. To mitigate the difficulty, we assume that the location of a sensor is fixed and the total number of sensors for a city remains unchanged during the period. Then the parameterized imputer is learned by minimizing the empirical loss over all time series in the training set:\n$\\min_{\\Theta} L(\\{x^i\\}_{i=1}^{N}; \\Theta) = \\mathbb{E}_{x \\sim p_\\mathcal{X}} [\\ell(x^i; \\Theta)],$\nwith $\\ell(x^i; \\Theta) = \\frac{1}{T_i} \\sum_{t=1}^{T_i} || x^i_t - \\Phi_{\\Theta} (\\xi, t_t) ||^2,$\nwhere $p$ is the (unknown) data distribution. $\\ell$ is either a unified model or a composition of all individual models $\\Phi = \\{\\Phi_1, ..., \\Phi_N\\}$, which corresponds to the collaborative imputation and separate imputation schemes respectively."}, {"title": "4 METHODOLOGY", "content": "This section elaborates the proposed model for cross-city collaborative imputation. We aim to learn an INR $\\Phi_{\\Theta}(\\xi, \\tau): \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ that maps the timestamp (possibly with location) to the observed value $x$ at each query coordinate. Then the continuous function $\\Phi_{\\Theta}(\\cdot)$ allows for interpolation at arbitrary points, generating the imputation for irregular time series. The meta-learning method further enables it to generalize across heterogeneous cities. We term it Meta learning-based urban Time Series Imputer (MetaTSI). The overall architecture of MetaTSI is shown in Figure 1 (c).\n4.1 Learning to Reconstruct Time Series Dynamics by Implicit Representation in the Embedding Space\n4.1.1 Implicit Neural Representations for Time Series\nUrban system involves complex spatiotemporal phenomena. The sensed time series may be generically generated from a high-dimensional dynamical system [28]:\n$\\mathcal{C}(t, x(t), \\nabla_t x(t), \\nabla x(t), ...) = 0,$\nwhere the time series $x(t): t \\rightarrow \\mathbb{R}^m$ from a data generating process consists of successive, possibly irregular, observations of some dynamical process described by a system of partial differential equation $\\mathcal{C}$. In this context, our goal is to learn a parameterized neural network $\\Phi$ to map $t$ to the target value $x(t)$ while reconstructing the dynamics in Eq. 5, achieving the imputation in observations:\n$\\min \\mathcal{C} = || \\mathcal{C}(x(t), \\Phi, \\nabla_t \\Phi, \\nabla ...) || dt,$\nwhere $\\mathcal{T}$ is the definition domain and the loss is feasible by sampling a dataset $\\mathcal{D} = \\{(x(t), t): t \\in \\mathcal{T}\\}$ of coordinates and observations. Since $\\mathcal{C}$ is implicitly defined by the relation modeled by $\\mathcal{C}$, neural networks that parameterize such implicitly defined functions are referred to as INRs [21]. Typically, fitting an INR requires a large amount of observed data [21]. Directly learning INRs from sparse time series can be challenging and suboptimal due to the lack of physical guidance and data properties. To address this issue, we exploit methods that can guide the learning process.\n4.1.2 Dynamics Reconstruction by the Embedding Theory\nTo obtain such a model $\\mathcal{I}$, we resort to the embedding theory [28], [29]. Generally, the dynamical system can be characterized by a state variable $s(t) \\in \\mathbb{R}^n$ and a measurement function $H: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. Typically, $m < n$ as the state of the underlying dynamical process cannot be fully observed. For urban time series, we will consider in this paper the simplest case where $x(t)$ is a scalar (i.e. $m = 1$) time series. Given a partially observed system state $s(t)$ with dynamics on state space $S \\subseteq \\mathbb{R}^n$, it can be reconstructed by:\n$\\dot{s(t)} = f(s(t)),$\nwhere $f(\\cdot)$ is the reconstructor defined in the state space. The measurement function $H$ specifies the process of observing the system and extracting information evaluated at query time steps to generate a time-series:\n$x(t) = H(s(t)).$\nThen, an embedding is defined as a transformation that augments the observed time series by increasing its dimension with some time windows:\n$\\Psi: \\mathbb{R} \\rightarrow \\mathbb{R}^{dr}, \\ \\mathbf{x}(t) = \\Psi(x(t)),$\nwhere $\\mathbf{x}(t)$ is the embedding vector with dynamics defined in a reconstructed state space $M \\subset \\mathbb{R}^{dr}$ and a transformed reconstructor $\\mathcal{F}$. According to Takens' theory [29], if $\\Psi$ is a valid embedding, there exists a one-to-one mapping $\\Pi$ that we can identify a corresponding state point $x(t)$ on $S$ for every $\\mathbf{x}(t)$ on $M$ via the inverse mapping $\\Psi^{-1}(\\mathbf{x}(t))$:\n$\\Psi^{-1}: M \\leftrightarrow S, \\ \\Psi^{-1}(\\mathbf{x}(t)) = x(t),$\nand the the dynamics of the system are preserved by:\n$\\mathcal{F} = \\Psi \\circ f \\circ \\Psi^{-1}.$\nWe can find that learning the dynamics in the reconstructed state space $M$ is equivalent to learning the dynamics of the original system $S$. Therefore, the time series imputation problem using embedding is posed as learning the parameterized reconstructor $\\mathcal{F}_{\\Theta}$ where:\n$\\dot{x(t)} = \\Psi^{-1} \\circ \\mathcal{F}_{\\Theta} \\circ \\Psi(\\mathbf{x}(t)).$\nAs described in Eq. 12, the key to modeling the underlying dynamical system for imputation is to build a suitable data embedding $\\Psi$ and a mapping $\\mathcal{F}$. Rather than relying on neural networks to directly process the raw data and correlate hidden states by temporal modules, we reconstruct the dynamical trajectory of the system using a nonparametric physical embedding as $\\Psi$ and parameterized INRs as $\\mathcal{F}_{\\Theta}$. We will elaborate on each of them in the following.\n4.1.3 Coordinate Delay Embedding\nAccording to Takens' embedding theorem, it is guaranteed that a time delay embedding [59] with dynamics defined in a space of sufficiently large dimension $\\mathbb{R}^{dr}$ constructed from scalar time series is generically diffeomorphic to the full state space dynamics of the underlying system [60]. This facilitates the recovery of full state dynamics using partially observed time series. To achieve this, time delay embedding"}, {"title": "5 EXPERIMENTS", "content": "This section designs a battery of experiments to evaluate whether MetaTSI can provide high-performance, generalizable, and computationally efficient imputation for urban time series. We (1) compare it with state-of-the-art imputation models on single-city benchmarks; (2) demonstrate generalization across cites and sensors; (3) assess its few-shot performance in unseen samples; and (4) detail additional properties of MetaTSI, such as computational efficiency, architectural rationality, and interpretable latent manifolds.\n5.1 Datasets and Experimental Settings\nDatasets. To scale the analysis and evaluation to cover multiple cities across the world, we collect and construct a large-scale urban traffic benchmark. This benchmark includes 20 cites worldwide, with traffic flow data processed from UTD19 data 1 and PeMS data 2. UTD19 consists mainly of measurements from loop detectors from 2017-2019, which record vehicle flow and occupancy in a relatively small aggregation interval, typically 3-5min. The cities included in UTD19 are located mainly in European countries. PeMS data is collected from individual detectors spanning the freeway system across all major metropolitan areas of California, which is processed into a regular interval of 5 minutes. The whole dataset includes more than 8,000 heterogeneous time series with different lengths and frequencies. Brief summary of the data is given in Table. 1. To compare with existing models, we ensure that time series from the same city have the same length. For a straightforward evaluation, we report the MAE and MSE metrics on Z-normalized values as different series have different scales and dimensions.\nBaselines. We compare MetaTSI with both analytical models and deep implicit representation models. They are from state-of-the-art venues in related literature. For single-city comparison, we consider: (1) SIREN (NeurIPS\u203220) [21]; (2) FourierNet (NeurIPS'20) [22]; (3) DeepTime (ICML\u203223) [26]; (4) TimeFlow (TMLR'24) [42]; (5) LCR (TKDE'24) [13]; (6) TRMF (NeurIPS'16) [12]; (7) TIDER (ICLR\u203223) [68]; (8) mTAN (ICLR'21) [69]; (9) LRTFR (\u03a4\u03a1\u0391\u039c\u0399'23) [25]. For cross-city experiments, we consider generalizable models: (1) TimeFlow; (2) Functa (ICML\u203222) [48]; (3) DeepTime; (4) HyperNet-SIREN; (5) SIREN+. Note that since the data has large spatial dimensions and irregular sampling frequencies, many deep imputation models (e.g., BRITS, SAITS, and ImputeFormer) are not well suited for this challenging task and we do not include them as baselines. In addition, both LCR and MetaTSI have two variations according to model implementations: (1) 1D model: treating each time series as individuals and modeling independently; (2) 2D model: treating the multivariate time series as a whole matrix and adding an additional spatial dimension."}, {"title": "5.2 Model Comparison within a Single City", "content": "In the first experiment, we train each model in each city with varying observation rates (i.e., 5%, 10%, 20%). According to their mechanisms, models either are fitted to the observations and predict missing values according to the coordinates, or are optimized to reconstruct the distributions or patterns of the observations. Results in Table. 2 indicate that the proposed MetaTSI-2D model consistently achieves better performances than baselines in all cities, and MetaTSI-1D also achieves performance comparable to the state-of-the-art. When there are fewer observations, the superiority of MetaTSI is more significant. In particular, MetaTSI-2D performs better than MetaTSI-1D in fitting observations within a single city. This is achieved by the strong spatial inductive bias imposed on the model, which leads to a larger model capacity to fit data distributions. However, in the next section we show that this sacrifices model generalizability."}, {"title": "5.3 Generalization across Multiple Cities", "content": "Second, we examine the generalizability of models. Our objective is to learn a unified model to reconstruct all time series simultaneously for all cities. We select 15 cities for this experiment and the data missing rate for each city is randomly sampled from [80%, 95%]. As data are irregularly sampled in different cities, we need continuous models to deal with the irregularity. Therefore, only continuous models equipped with transfer-learning-related strategies can complete this task. Results of different methods are given in Table. 3. It is observed that in this case MetaTSI-2D is surpassed by MetaTSI-1D which has better generalizability. Different cities have heterogeneous spatial patterns such as different network topologies and mobility demands. Learning a city as a whole increases the challenges of model-based transfer. Other meta-learning-based INR models cannot achieve competitive performances, which demonstrate the effectiveness of our multi-scale architecture and modulation strategy particularly designed for urban time series."}, {"title": "5.4 Generalization on Unseen Instances", "content": "Next, we evaluate the out-of-distribution generalization performance of models. This scenario happens often in practice when the pretrained model is applied to new locations or cities with limited computational resources to retrain the entire model. The model trained on the source data is only fine-tuned by the inner loop of the meta-learning scheme with few-shot observations. This is accompanied by a small number of gradient adaptation steps, e.g., 5 in our experiments. We compare MetaTSI with state-of-the-art implicit time series imputation model TimeFlow.\nGeneralization on unseen series. We split the data used in Section 5.3 into two separate sets, each containing half of the series. We pretrain the model on half of the data and fine-tune it with the meta-learning adaptation (lines 7-10 in Algorithm 1) for the other half set. Table 4 shows the result. As can be seen, MetaTSI-1D outperforms the other two models by a large margin, indicating that it learns representations to generalize rather than memorize.\nGeneralization on unseen cities. We then apply models to completely unseen cities that are never shown in the training set. The remaining five cities are adopted for evaluation. As new cities can have patterns different from source cities, this task is very challenging. Similarly, MetaTSI-1D still achieves desirable performance compared to its counterparts, implying great potentials for cold-start problems."}, {"title": "5.5 Model Efficiency and Robustness", "content": "This section details additional properties of MetaTSI, including both its computational efficiency and robustness.\n5.5.1 Inference Efficiency\nA salient property of MAML is its efficiency in adapting to different instances. We compare it with the full-training strategy when applied to all series within a single city. Figure 2 compares the training speed (time) and the parameter count for the two strategies. It is evident that meta-learning significantly reduces computational expenses and shows better scalability and parameter efficiency.\n5.5.2 Robustness with Sparser Data\nTo justify the necessity of the collaborative imputation scheme developed in this paper, we evaluate the model robustness under sparse data conditions in Tables 5 and 6. The pretrained MetaTSI efficiently adapts to the new tasks by exploiting the inherent knowledge in weights of the base network. The baselines are either individual models trained from scratch using limited observations or a unified model trained on all available data at the same time. Results reveal that in scenarios where the target city has very limited observations (e.g., only 1%) available, cross-city collaboration can significantly achieve performance gains."}, {"title": "5.6 Ablation Study", "content": "To justify the rationality of each modular design, we perform architectural ablation studies in Tab. 7 using two cities. They include the following architecture variations: (1) replacing the initialization scheme of latent code; (2) replacing the hierarchical modulation scheme with a fixed phase modulation used in [42]; (3) replacing sinusoidal activation; (4) removing the meta-learning procedure; (5) removing the local loss; (6) removing the coordinate delay embedding; (7) removing the masked instance normalization; (8) removing CRF. According to the evaluation results, each modular design contributes to overall performance."}, {"title": "5.7 Hyperparameter Study", "content": "Figure 3 examines the effects of several hyperparameters, including the number of inner steps, the inner learning rate, the dimensions of the delay embedding $\\delta$ and the latent coding. There are several observations: (1) a larger inner learning rate encourages learning in data with diverse instances; (2) a small number of inner steps with a proper dimension are enough to differentiate different instances; (3) a larger $\\delta$ can boost performance, but increase complexity."}, {"title": "5.8 Exploring the Latent Manifold", "content": "The meta learning procedure enables a latent code learned from data to represent the series instance. In this section, we interpret these encodings using visualization tools.\n5.8.1 t-SNE Structure.\nAfter auto-decode the latent code for each time series in all cities, we store these codes and project them into the two-dimensional plane using the t-SNE method. Figure 4 shows the encodings of different cities. Intriguingly, MetaTSI generally separates different cities, even without any geolocation priors. Moreover, cities with similar traffic flow patterns appear in clusters that are close to each other, e.g., cities from the bay area of CA, USA.\n5.8.2 Generating Novel Time Series.\nRecall that ones can control the output of INRs by manipulating the latent manifold. Since we have learned priors over the function space of all training samples, novel time series can be generated by interpolating the latent code. We examine the transitional behaviors reflected by the latent space by decoding an interpolated latent code $\\Phi(\\cdot; \\mu)$ with $\\Phi_\\mu = \\mu \\varphi_1 + (1 \u2013 \\mu) \\varphi_2$. Figure 5 shows different generated time series by varying $\\mu$. We observe that the interpolation path between two codes yields a \u201clinear\u201d transition in the time domain. This suggests that the latent space is smooth and well structured, which sheds light on the possibility of applying learned representations to generative modeling and extrapolation of adjacent sensors."}, {"title": "5.9 Case Study: Imputation Visualization", "content": "Figure 6 (a) displays examples of the time series imputation results in different cities and (b) shows the histogram of the true and predicted values (probability). These plots clearly show that although different cities feature distinct traffic flow observations, the shared patterns make transfer between cities possible. Our model captures these common structures and accurately reconstructs the ground truth."}, {"title": "6 CONCLUSION AND FUTURE DIRECTION", "content": "In this study, we introduce a novel time series imputation framework leveraging meta-learning implicit neural representations called MetaTSI. Thanks to the generic architecture, MetaTSI originally pretrained to distinguish different instances can then generalize to heterogeneous cities to achieve collaborative imputation tasks that vary in spatiotemporal resolutions and observation patterns. Experimental results indicate that MetaTSI not only achieves superior imputation accuracy but also excels in generalizability across diverse tasks. In addition, MetaTSI recovers the global structure of the signal manifold, allowing easy interpolation between nearby signals. Although promising results are presented, it revealed some limitations that need future efforts. First, while meta-learning facilitates efficient adaptation of new samples, pretraining it in the source data is computationally expensive and less stable than standard supervised learning. Second, this work only focuses on a single data modality. Extending MetaTSI to diverse modalities of urban data requires more sophisticated architectures."}]}