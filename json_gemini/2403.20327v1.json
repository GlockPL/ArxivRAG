{"title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models", "authors": ["Jinhyuk Lee", "Zhuyun Dai", "Xiaoqi Ren", "Blair Chen", "Daniel Cer", "Jeremy R. Cole", "Kai Hui", "Michael Boratko", "Rajvi Kapadia", "Wen Ding", "Yi Luan", "Sai Meher Karthik Duddu", "Gustavo Hernandez Abrego", "Weiqiang Shi", "Nithi Gupta", "Aditya Kusupati", "Prateek Jain", "Siddhartha Reddy Jonnalagadda", "Ming-Wei Chang", "Iftekhar Naim"], "abstract": "We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.", "sections": [{"title": "1. Introduction", "content": "Text embedding models represent natural language as dense vectors, positioning semantically similar text near each other within the embedding space (Gao et al., 2021; Le and Mikolov, 2014; Reimers and Gurevych, 2019). These embeddings are commonly used for a wide range of downstream tasks including document retrieval, sentence similarity, classification, and clustering (Muennighoff et al., 2023). Instead of building separate embedding models for each downstream task, recent efforts seek to create a single embedding model supporting many tasks.\n\nThe recent development of general-purpose text embedding models presents a challenge: these models require large amounts of training data to comprehensively cover desired domains and skills. Recent embedding efforts have focused on using extensive collections of training examples (Li et al., 2023; Wang et al., 2022). Large language models (LLMs) offer a powerful alternative, as they contain vast knowledge across various domains and are known to be exceptional few-shot learners (Anil et al., 2023; Brown et al., 2020). Recent work demonstrates the effectiveness of using LLMs for synthetic data generation, but the focus has primarily been on augmenting existing human-labeled data or improving performance in specific domains (Dai et al., 2022; Jeronymo et al., 2023). It motivates us to re-examine: to what extent can we leverage LLMs directly to improve text embedding models?\n\nIn this work, we present Gecko, a highly versatile yet efficient embedding model, powered by the vast world knowledge of LLMs. Our approach leverages insights from knowledge distillation to create a two-step LLM-powered embedding model. Starting with a large corpus of (unlabeled) passages, we use a few-shot prompted LLM to generate a relevant task and query for each passage, similar to Dai et al. (2022) and Wang et al. (2023). We then embed the concatenated task and query using a pretrained embedding model to obtain nearest neighbor passages, use an LLM to rerank the passages, and obtain positive and negative passages based on the LLM scores. The reranking step is key to enhance the quality as we discover that the best passage to answer the generated query often differs from the original source passage. We show that using our LLM-based dataset, FRet, alone can lead to significantly improvement, setting a strong baseline as a zero-shot embedding model on MTEB."}, {"title": "2. Related Work", "content": "Text Embedding Models Text embeddings convert textual inputs into uniform-sized vectors, supporting downstream tasks such as semantic similarity, information retrieval, clustering, and classification. Recent models, including SBERT (Reimers and Gurevych, 2019), Universal Sentence Encoder (Cer et al., 2018), and Sentence T5 (Ni et al., 2022), attempt to provide general purpose embeddings suitable for various NLP tasks. Despite attempting to be general-purpose, studies indicate that these embedding models struggle to generalize across tasks and domains, motivating the creation of unified models trained across diverse tasks (Asai et al., 2022; Su et al., 2022) and benchmarks such as MTEB (Muennighoff et al., 2023) focused on novel task and domain generalization. Inspired by these prior works, we develop a versatile embedding model by creating the LLM-generated FRet dataset from a large and diverse corpus encompassing a wide variety of task types.\n\nContrastive Learning One of the critical components of contrastive learning is to find proper negative examples for a query (Gao et al., 2021; Karpukhin et al., 2020; Lee et al., 2021). For example, Xiong et al. (2020) proposed to select hard negatives from a large corpus using an asynchronously-updated approximate nearest neighbor index. Other previous work has denoised the hard negatives based on confidence scores (Qu et al., 2021; Ren et al., 2021) or distilled knowledge from cross-attention rerankers into the dual-encoders (Izacard and Grave, 2021; Sachan et al., 2023; Santhanam et al., 2022). In our work, using LLMs, we study the effect of mining better positive examples for a query while finding useful hard negatives as well. While similar in spirit to previous distillation approaches, using this hard selection of positive and negative passages aligns well with the format of existing human-annotated training data, allowing us to train on both."}, {"title": "Synthetic Data Generation", "content": "When applying text embedding models to new tasks and domains, we often want to have relevant queries and labels for these target domains, but they are often unavailable or prohibitively expensive to collect. To address this issue, several works (Bonifacio et al., 2022; Dai et al., 2022; Jeronymo et al., 2023; Khramtsova et al., 2024) propose a few-shot prompted query generation approach. They generate synthetic queries by few-shot prompting LLMs to create a domain-specific training dataset, which has been shown to be very successful on the zero-shot information retrieval benchmark (Thakur et al., 2021). In contrast to generating domain-specific queries for domain adaptation, our work aims to distill more general-purpose knowledge of LLMs into a text embedding model, resulting in a versatile text embedding model that achieves strong performance on MTEB (Muennighoff et al., 2023)."}, {"title": "Retrieval with Instructions", "content": "Previously, Dai et al. (2022) demonstrated that there exist different intents for different retrieval tasks. For instance, given a search query, users might want to find a similar query, or they might want to read a passage that directly answers the query. Recent work has explored implementing a retriever that changes the retrieval behavior for different intents. Asai et al. (2022) and Su et al. (2022) introduce \u201cretrieval with instructions,\u201d where a dense retriever is trained to follow an instruction that was given along with the query. Wang et al. (2023) also explores how LLMs can generate synthetic task instructions and associated queries, but for more general-purpose text embeddings similar to ours. They use a two-step prompt to encourage the diversity of the synthetic data: first prompting an LLM to come up with a task and then generating an example (query, positive passage, and negative passage) based on the task. In our work, we also synthesize task-query pairs to increase the diversity of the synthetic data. Unlike Wang et al. (2023), however, we generate synthetic task and query pairs from the web passages, basing our FRet dataset on real user-facing content. We also use LLMs to decide which web passages can be used as positive or negative targets for each generated query."}, {"title": "3. Training Recipe for Gecko", "content": "Gecko is based on a 1.2B parameter pre-trained transformer language model that undergoes two additional training stages: pre-finetuning and fine-tuning. First, we extend the pre-finetuning recipe from previous work (Ni et al., 2021; \u00a73.1). For fine-tuning, our main contribution is to create a novel fine-tuning dataset for a diverse set of downstream tasks via a two-step LLM distillation, which identifies both positive and hard negative passages for each generated query (\u00a73.2). We coin this dataset as FRet, the Few-shot Prompted Retrieval dataset. For the fine-tuning mixture, FRet is combined with a diverse set of academic datasets formatted in a similar way: each with a task description, input query, positive passage, and negative passage (\u00a73.3)."}, {"title": "3.1. Pre-finetuning", "content": "Following the prior work (Neelakantan et al., 2022; Ni et al., 2021; Wang et al., 2022), our pre-finetuning procedure relies on self-supervised tasks over a large text corpus as described below.\n\nTraining Mixture We use two pre-finetuning datasets. First, we use the large-scale community QA dataset by Ni et al. (2021), which includes text pairs such as question-answer pairs from online forums and QA websites. Next, we crawl a corpus of title-body text pairs from the Web, which can be found from almost every website as naturally occurring pairs. Despite its simplicity, Wang et al. (2022) showed that these naturally occurring text pairs are useful for pre-finetuning embedding models."}, {"title": "Training Objective", "content": "Pre-finetuning on a large amount of unsupervised text pairs has been shown to improve performance for smaller-scale dual encoders for various downstream tasks including document retrieval (Izacard et al., 2022; Lee et al., 2019) and semantic similarity (Gao et al., 2021). The goal of the pre-finetuning stage is to expose the model to a large amount of textual diversity, which seems necessary for the compact text embedding models that we aim to train.\n\nWe begin with a pre-trained language model M where M outputs a series of contextualized token embeddings W \u2208 R^(n\u00d7d) given a sequence of n tokens and an embedding dimension of d. Given a set of text pairs D_pre = {(q_i, p_i)}_1 for pre-finetuning, we obtain the vector representations of q_i and p_i by taking the mean of W along the n axis. We first prepend a dataset-specific task feature t before each query, so each query is informed of which task is being optimized.\n\nq_i = mean_pool_[|t|+|q_i|](M(t \u2295 q_i) \u2208 R^(|t|+|q_i|)\u00d7d) \u2208 R^d\\ \np_i = mean_pool_[|p_i|](M(p_i) \u2208 R^(|p_i|)\u00d7d) \u2208 R^d.\n\nFor pre-finetuning, we use simple task features such as question answering or search result for t depending on the dataset. Then, for each mini-batch of size B, we optimize the contrastive learning objective with in-batch negatives:\n\nL_pre = - \u2211_i^B log( exp(sim(q_i, p_i)/\u03c4) / (\u2211_j^B exp(sim(q_i, p_j)/\u03c4) )  )\n\nIn this work, we use the cosine similarity for the similarity function, sim(x, y) = x^Ty / (||x|| ||y||), with a temperature parameter \u03c4. Note that we do not utilize hard negatives during pre-finetuning and utilize the maximum batch size that fits into the device. This has been found to be effective for document retrieval tasks as observed in previous work (Li et al., 2023; Wang et al., 2022)."}, {"title": "3.2. FRet: Two-Step LLM Distillation", "content": "In this section, we introduce our two-stage approach that uses LLMs to generate FRet. Traditional approaches for training embedding models often rely on large, manually labeled datasets. However, creating such datasets is time-consuming, expensive, and often results in undesirable biases and lack of diversity. In this work, we present a novel method for generating synthetic data for training multi-task text embedding models, leveraging the power of LLMs through a two-step distillation process. The overall process of generating FRet is illustrated in Figure 2.\n\nLLM-based Diverse Query Generation One of the challenges of using manually crafted queries is to ensure that the queries cover a diverse set of tasks and linguistic patterns. With LLMs, these variables are relatively easy to control as we can design the prompt to specify the diversity. In this work, we employ few-shot prompts to control the diversity of queries. Our LLM is instructed to read a sampled web passage and generate both the task description and a relevant query for the task:\n\nLLM(P_QG, p_seed) \u2192 (t, q)\n\nwhere p_seed is a passage drawn randomly from the web corpus C and P_QG is a fixed prompt. The prompt, P_QG, is identical for every example and consists of few-shot examples and instructions. The LLM generates a task description t, which describes the type of retrieval\u2014for example, \u2018Given a query, find a passage that has the answer to the query' (question answering) or \u2018Given a query, find a passage that allows you to check whether the query is true or not' (fact checking)\u2014and also a query q that aligns"}, {"title": "LLM-based Positive and Negative Mining", "content": "Most models that utilize synthetic queries are trained with (q, p_seed) pairs, which assumes that p_seed is a good positive target for q (Dai et al., 2022; Jeronymo et al., 2023). While this is likely true in most cases, we hypothesize that there could be a more relevant passage than p_seed somewhere in our corpus of web passages. Essentially, in the previous section, we sampled P(t, q | p_seed) from the LLM, but this does not guarantee that p_seed maximizes P(p | q, t) over all the passages in the corpus. This intuition is supported by our observation that generated queries often focus on a particular aspect of a relatively long passage. Hence, we propose a method that leverages LLMs to discover more relevant positive passages along with a good hard negative for the generated query.\n\nIn particular, we use an existing embedding model\u00b9 to retrieve top N neighbors P = {p^(1), ..., p^(N)} from the corpus given a generated query q. We then employ the same LLM used for the query generation to rank these retrieved passages based on their relevance to the query. Specifically, we use two well-known few-shot prompted LLM ranking functions: query likelihood and relevance classification. Query likelihood uses an LLM to measure the log-likelihood of a generated query q given a passage p, i.e., QL(q, p) = LLM(q | p, P_QL) (Sachan et al., 2022). Herein, P_QL is a prompt containing an instruction for judging query likelihood and several few-shot examples of relevant"}, {"title": "3.3. Unified Fine-tuning Mixture", "content": "We combine FRet with other academic training datasets in the same format: task description, input query, positive passage (or target), and negative passage (or distractor), creating a novel fine-tuning mixture. We then train our embedding model, Gecko, using this mixture with a standard loss function.\n\nAcademic Data In addition to FRet, we use the following academic training datasets: Natural Questions (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), MedMCQA (Pal et al., 2022), SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and several classification datasets from Huggingface. For the multilingual model, we add training sets from MIRACL (Zhang et al., 2023). All datasets are pre-processed to have a unified encoding format (Appendix B), containing a task description, a query, a positive passage, and a negative passage.\n\nClassification Data for Contrastive Learning We aim to seamlessly incorporate the classification training sets into our contrastive learning objective without any performance degradation on other tasks such as document retrieval. Specifically, given a classification input text x with a label y \u2208 y, we pair each input x with another input x\u207a, which shares the same label y and then use x\u207a as a positive target for x. At the same time, we randomly select a hard negative input x\u0304 which has any label other than y. This approach is a simple version of the classification datasets pre-processed by Su et al. (2022) but avoids using any model-specific embeddings. During our experiments, we found that each x\u207a might overlap with other positive examples within the mini-batch, creating a false negative problem among the in-batch negatives. Hence, we assign a unique ID to each triple (x, x\u207a, x\u0304) and append the same unique ID to x, x\u207a, and x\u0304. This effectively makes the in-batch negatives trivial for the model to distinguish them, because if the unique ID does not match, then it is never the correct answer. Thus, the model focuses on differentiating x\u207a and x\u0304 given x."}, {"title": "Training Objective", "content": "For fine-tuning, we are given a set of M fine-tuning datasets (including FRet) that are comprised of a query-specific task description, an input, a positive target, and a hard negative: [D^(1), ..., D^(M)] where D^(m) = {(t_i, q_i, p_i^+, p_i^-)}_(i=1)^(N_m). We obtain the vector representations q_i, p_i^+, and p_i^- similar to eq. (1) where t_i is used for the input: q_i = mean_pool[M(t_i \u2295 q_i)].\n\nFor fine-tuning we optimize the in-batch cross-entropy loss, where query q_i should distinguish p_i+ from the hard negative p_i^-, other passages in the batch {p_j^(+)}_(j=1)^B, and other queries in the batch {q_j}_(j=1)^B \\ {q_i}. The use of other queries in the batch is also known as \"same-tower negatives\" (Moiseev et al., 2023). Given a mini-batch of size B, we optimize the following objective:\n\nL_main = 1/B \u2211_(i=1)^B [ -log( exp(sim(q_i, p_i^+)/\u03c4) / (\u2211_(j=1)^B (exp(sim(q_i, p_j^+)/\u03c4) + \u2211_(j != i) exp(sim(q_i, q_j)/\u03c4) ) ]\n\nFor the same-tower negatives, we used the indicator variable 1_[j\u2260i] to denote that we are iterating over j except for the current target index i. Intuitively, same-tower negatives are helpful for symmetric text embedding tasks such as measuring the semantic similarity of two sentences, because {q_j}_(j=1)^B shares the same modality with q_i: in this case, both are queries. Finally, to support multiple different dimensions of embeddings with a single model, we add the MRL loss (Kusupati et al., 2022), which optimizes eq. (3) with sub-dimensions smaller than d. In our experiments, we use two embedding dimensions d = 768 and d = 256 for Gecko."}, {"title": "4. Experiments", "content": "We mainly evaluate Gecko on the Massive Text Embedding Benchmark (MTEB), which contains 56 datasets on retrieval, semantic textual similarity (STS), clustering, classification, pair classification, reranking, and summarization. We analyze how each component of Gecko and FRet contribute to the performance, providing insights on building heterogeneous text embedding models."}, {"title": "4.1. Main Results", "content": "Table 1 summarizes the performance of Gecko and other baselines on MTEB. For baselines, we report the performance of text embedding models whose recipes are fully (or partly) available. Gecko significantly surpasses all similarly-sized baselines (<= 1k embedding dimensions, <= 5B parameters) on every text embedding task in the MTEB benchmark. Gecko-1b-256 demonstrates superior quality compared to text-embedding-3-large-256 (OpenAI; Neelakantan et al. 2022), GTR (Ni et al., 2021), and Instructor (Su et al., 2022). Gecko-1b-768 often matches or exceeds the performance of even larger models, including text-embedding-3-large (OpenAI), E5-mistral (Wang et al., 2023), GRit (Muennighoff et al., 2024), and Echo embeddings (Springer et al., 2024). Notably, these models all use 3-4k dimensional embeddings and exceed 7B parameters. We observe that Gecko is particularly good at balancing retrieval and STS performance, and sets a new state-of-the-art on classification, STS, and summary. Surprisingly, the performance of Gecko trained solely on FRet, which makes MTEB a pure zero-shot benchmark, shows strong performance compared to other baselines."}, {"title": "4.2. Multilingual Retrieval Results", "content": "Table 2 summarizes the performance of Gecko and other baselines on MTEB. We train a multilingual version of Gecko with multilingual language models (Team et al., 2023; Xue et al., 2021) with the same recipe as Gecko, but add the MIRACL training dataset in the mixture. Note that FRet is provided only in English and the main difference of gecko-multilingual-1b with others is the use of FRet in its training set. We find that while we only generated English-only dataset from LLMs, this translates well to other multilingual tasks achieving superior performance compared to others."}, {"title": "4.3. Analysis", "content": "LLM as a Labeler In Table 3, we test different labeling strategies for FRet where we use different positive and hard negative passages. For positive passages, we try 1) the original passage where the queries were generated (i.e. p_seed), or 2) the top-1 passage selected by an LLM out of the nearest neighbor passages (including the original one) of a generated query (i.e. p_1). For negative"}, {"title": "Diversity of FRet", "content": "FRet provides queries in multiple tasks including question answering, search result, fact checking, and sentence similarity. In Table 4, we test how the diversity of FRet influences model generalizability across tasks in MTEB. First, we train individual models each using 300k data from a specific task (e.g., FRet-question-answering). Additionally, we train models on 300k samples drawn across all four tasks (75k per task; FRet-all-tasks) with original sampling distribution or uniform sampling distribution. We observe superior performance from the FRet-all-tasks model, particularly when tasks were uniformly sampled. We also find that the unified formatting (Appendix B) affects the quality of embeddings significantly, as it helps the model better separate different tasks."}, {"title": "Learning Semantic Similarity and Classification", "content": "In the last rows of Table 4, we show how Gecko learns better semantic similarity and classification. We use the symmetric format (Sym.) as well as the same tower negatives for learning better semantic similarity. Along with the NLI datasets, it drastically improves the STS performance by 1.6 on average. Our strategy of combining classification datasets also improve the performance on classification by a large margin without significant performance degradation on other tasks. Using the full FRet mixture gives us the final performance of 66.31."}, {"title": "5. Conclusion", "content": "In this paper, we introduced Gecko, a versatile text embedding model distilled from large language models. Gecko is trained on an LLM-generated synthetic dataset FRet that contains LLM-ranked positives and negatives. We demonstrate that LLMs can be used to identify better positive as well as negative targets for synthesized queries. We also show how combining this synthetically-generated data in a unified format can lead us to achieve great performance on multiple different tasks at the same time. Our ablation study reveals the importance of LLM-based relabeling and the diversity of the datasets while demonstrating the strong zero-shot generalizability of Gecko."}, {"title": "Author Contributions", "content": "Jinhyuk Lee: Co-lead of FRet and Gecko. Coordinated the project, implemented the main functionality of FRet and Gecko, and led the paper writing. Zhuyun Dai: Co-lead of FRet. Implemented the main functionality of FRet and led the paper writing. Xiaoqi Ren: Co-lead of Gecko. Implemented the main functionality of Gecko and its multilingual version. Blair Chen: Contributed to the MTEB evaluation and ablation study of Gecko. Daniel Cer: Contributed to the MTEB evaluation of Gecko and the classification datasets used for Gecko. Jeremy R. Cole: Contributed to experiments for generating and filtering FRet and paper writing. Kai Hui: Contributed to the use of LLM as a labeler, rank fusion, and paper writing. Michael Boratko: Contributed to the project coordination and paper writing. Rajvi Kapadia: Contributed to the use of LLM for the distillation. Wen Ding: Contributed to the hyperparameter tuning and ablation study of Gecko. Yi Luan: Contributed to the use of LLM as a labeler and paper writing. Sai Meher Karthik Duddu: Contributed to the large-scale training of Gecko. Gustavo Hernandez Abrego: Contributed to the project coordination. Weiqiang Shi: Contributed to the multilingual version of Gecko. Nithi Gupta: Contributed to the MRL implementation. Aditya Kusupati: Contributed to the MRL implementation. Prateek Jain: Contributed to the MRL implementation. Siddhartha Reddy Jonnalagadda Contributed to the project coordination. Ming-Wei Chang: Contributed to the project coordination and paper writing. Iftekhar Naim: Contributed to the project coordination and paper writing."}, {"title": "Appendix", "content": "A. Enhancing Few-shot LLM Ranking with Ensembling\n\nTo validate the quality of the few-shot reranking, we retrieve the top 100 candidate documents and rerank them using our few-shot LLM reranker. We compare the performance of two LLM rerankers introduced in \u00a73.2: query likelihood (QL) and relevance classification (RC). Additionally, we investigate the ensemble of these rerankers using Reciprocal Rank Fusion (RRF): R(q, p) = 1/r_QL(q, p) + 1/r_RC(q, p), where r_QL(q, p) > 0 and r_RC(q, p) > 0 represent the rank positions assigned to passage p by QL and RC models for query q, respectively. It is important to note that we employ the identical prompts P_QL and P_RC used in \u00a73.2, but not a task-specific prompt for each BEIR task."}, {"title": "B. Formatting in FRet", "content": "Since we aggregate multiple datasets from different tasks, we preprocess every input and target with a unified encoding format. In Table 7, we show that the performance of asymmetric tasks (i.e. BEIR) is sensitive to the format while the performance of symmetric tasks are relatively stable."}, {"title": "C. Full MTEB Results and Instructions", "content": "In Table 8, we show the full MTEB results. In Table 9, we show the task strings (or instructions) used in the MTEB evaluation. Note that we use consistent instructions for most tasks except for BEIR, which contains multiple different intents as described in Dai et al. (2022)."}]}