{"title": "Is Your Image a Good Storyteller?", "authors": ["Xiujie Song", "Xiaoyi Pang", "Haifeng Tang", "Mengyue Wu", "Kenny Q. Zhu"], "abstract": "Quantifying image complexity at the entity level is straightforward, but the assessment of semantic complexity has been largely overlooked. In fact, there are differences in semantic complexity across images. Images with richer semantics can tell vivid and engaging stories and offer a wide range of application scenarios. For example, the Cookie Theft picture is such a kind of image and is widely used to assess human language and cognitive abilities due to its higher semantic complexity. Additionally, semantically rich images can benefit the development of vision models, as images with limited semantics are becoming less challenging for them. However, such images are scarce, highlighting the need for a greater number of them. For instance, there is a need for more images like Cookie Theft to cater to people from different cultural backgrounds and eras. Assessing semantic complexity requires human experts and empirical evidence. Automatic evaluation of how semantically rich an image will be the first step of mining or generating more images with rich semantics, and benefit human cognitive assessment, Artificial Intelligence, and various other applications. In response, we propose the Image Semantic Assessment (ISA) task to address this problem. We introduce the first ISA dataset and a novel method that leverages language to solve this vision problem. Experiments on our dataset demonstrate the effectiveness of our approach. Our data and code are available at: https://github.com/xiujiesong/ISA.", "sections": [{"title": "Introduction", "content": "How complex can a picture be? What kind of story can be told via a single picture? As the saying goes, \"a picture is worth a thousand words\". However, not every picture contains such rich information. The Cookie Theft picture (Figure 1 (a)) is a good exemplar of complex semantic information expressed via visual language. It is a well-known picture commonly used to assess language and cognitive abilities in humans. It was first introduced in the Boston Diagnostic Aphasia Examination published in 1972 (Goodglass, Kaplan, and Weintraub 2001) and remains widely utilized to this day.\nMany studies (Cummings 2019; Tasnim et al. 2022) have revealed the reasons behind the success of this picture. Its essence is being a \"good storyteller,\" capable of telling a complete and engaging story. Based on the research of the psychologists, two of its most important characteristics can be summarized as follows: (1) It contains a rich but not excessive number of entities, making it well-suited for eliciting longer narrative descriptions. (2) It is rich in semantics, enabling it to tell an interesting story. The semantics are derived from reasonings made by observing the entities and their relationships in the image. For instance, the Cookie Theft tells a story about two children attempting to steal cookies from a jar when their mother is not looking. The mother-child relationship between the characters in the image is deduced through further reasoning based on observing the content in the image.\nThough the Cookie Theft picture is widely used, there are still limitations. It is outdated since it has been proposed for half a century and it cannot be well applied to different cultures (Berube et al. 2019; Steinberg, Lyden, and Davis 2022). To avoid these issues, people often have to modify or replace the image in different application scenarios (Berube et al. 2019; Hussein et al. 2015; Dom\u00ednguez et al. 2006; Oh et al. 2012; Prasad, Dash, and Kumar 2012). For instance, Figure 1 (b) is an updated version of the Cookie Theft. This means more images of this kind are necessary.\nBesides, this kind of image is not only useful for humans but also for Artificial Intelligence (AI). With the development of vision models, especially Large Vision-Language Models (LVLMs), their abilities are increasing rapidly. Simple images with less semantics are not challenging enough for them to understand or generate anymore, so more images with rich semantics will definitely be beneficial for both training and evaluation (Song et al. 2024).\nThe internet or existing image datasets contain a lot of images, including some high-quality images that we expect. However, due to their scarcity, identifying and locating these high-quality images amidst the vast array of webly images can be a daunting task. Therefore, efficient methods for scoring and selecting these images are crucial. Furthermore, with the advancement of image generation models (Rombach et al. 2021; Ramesh et al. 2021, 2022), they are also increasingly capable of helping us generate more images. Thus, automatic semantic complexity assessment can also be used to assess the semantic complexity of generated images. Generally, it is the necessary path for obtaining semantically rich images.\nCurrently, though there are some research works about Image Assessment, like Image Quality Assessment (IQA) (Fang et al. 2020; Ying et al. 2020), Image Aesthetics Assessment (IAA) (He et al. 2022; Yi et al. 2023), and Image Complexity Assessment (ICA) (Saraee, Jalal, and Betke 2020; Feng et al. 2023), no one focuses on assessing the semantic complexity of images. In order to fill this research blank, we propose the Image Semantic Assessment (ISA) task to assess the semantic complexity of images.\nConsidering entities are the foundation of semantics and the complexity requirements for these two aspects may vary in different application scenarios, ISA task assesses images from both two levels: 1) At the entity level, we assess the entity richness of images, similar to the idea of ICA task (Feng et al. 2023), which we refer to as the Entity Complexity Scoring task; 2) At the semantic level, we propose the Semantic Complexity Scoring task to assess the higher-level semantic complexity of images. Note that this sub-task is the core of our proposed ISA task.\nTo promote the research on ISA task, we built the first ISA dataset with 2,946 images. Each image is annotated with the two corresponding scores by three annotators. Besides, a corresponding method called Vision-Language collaborative ISA method (VLISA) is proposed for this novel task. It first uses a Large Vision-Language Model (LVLM), such as GPT-40 (OpenAI 2023), as a feature extractor to extract semantic information in natural language form from images. Then, a regression model is trained to predict the score of images. Our contributions are as follows:\n1. As far as we know, we are the first to propose the ISA task, which aims to automatically assess semantic complexity in an image. It can be used to identify high-quality images with rich semantics and evaluate image generation models, etc.\n2. We construct the first ISA dataset, consisting of 2,946 images and human scores, that supports the ISA task. Our dataset includes images of varying semantic complexity, which helps models learn the ability to assess semantic complexity.\n3. To effectively assess the semantic complexity of images, we propose a simple yet effective method that collaboratively utilizes language and visual information. Experiments show that ISA task is challenging for traditional vision models like ViT (Dosovitskiy et al. 2021) and our proposed method significantly outperforms other baseline models on the Semantic Complexity Scoring task."}, {"title": "ISA Dataset Construction", "content": "In this section, we introduce our ISA data collection and annotation process, as well as the related data analysis."}, {"title": "Data Collection", "content": "We collected our images from Pinterest\u00b9. After collecting images, we filtered out duplicated images using imagededup\u00b2. To ensure high quality, we also manually excluded low-quality images that were blurry, watermarked or contained unnecessary text. After filtering, we finally retained 2,946 images in our dataset."}, {"title": "Data Annotation", "content": "For each image, we annotate it with two scores: an Entity Score and a Semantic Score. They correspond to the Entity Complexity Scoring task and the Semantic Complexity Scoring task, respectively. For each score, the images are first annotated on a scale from 1 (Low) to 5 (High). Then, these scores are normalized to the [0,1] range (Feng et al. 2023), and the average of these normalized scores is calculated as the final score."}, {"title": "Dataset Analysis", "content": "Annotation Consistency In line with established standards (Kong et al. 2016; Ying et al. 2020; Feng et al. 2023), we assess the consistency between annotators by using the Pearson Correlation Coefficient (PCC), Spearman's Rank Correlation Coefficient (SRCC), and Kendall's tau correlation for each pair of annotations. For Entity Score, the average PCC, SRCC, and Kendall's tau are 0.836, 0.827, and 0.762 respectively. The average PCC, SRCC, and Kendall's tau of Semantic Score are 0.799, 0.798, and 0.729 respectively. This demonstrates the consistency of our data annotation. In addition, following the crowdsourcing assessment studies conducted for IQA, IAA, and ICA (Hosu et al. 2020; Siahaan, Hanjalic, and Redi 2016; Feng et al. 2023), we compute the Intra-class Correlation Coefficient (ICC) for our annotations to measure the inter-rater reliability. The ICCs of Entity Score and Semantic Score are 0.937 and 0.922 respectively, which shows the reliability and consistency of our annotation.\nDataset Case Analysis Figure 2 shows some samples of our dataset. We can see that images with more entities are scored with higher Entity Scores, and images with more visual clues and telling more engaging stories are scored with higher Semantic Scores. We can also see that the relationship between Entity Score and Semantic Score is not entirely positively correlated. Even though some images contain few entities, for example, Figure 2 (e), they can still tell an interesting story. Images with a variety of entities can also contain little semantic information, for instance, Figure 2 (d).\nDataset Statistics"}, {"title": "Method", "content": "In order to lay the foundation for the ISA task, we propose a novel baseline method to perform the task. Since we expect to assess images from a higher semantic level, and language can usually express semantics more directly than images, we believe hybrid utilization of both visual and language information will be helpful for ISA task. Thus, we propose the Vision-Language collaborative ISA (VLISA) method."}, {"title": "Experiments", "content": "In this section, we present the experimental setup, results, and corresponding analysis.\nModels For Vision models, we use ICNet (Feng et al. 2023) and ViT (Dosovitskiy et al. 2021) as our baseline models. ICNet is a model designed for the ICA task. ViT is a classic vision model. For VLISA, we use GPT-40 (OpenAI 2023) to extract features from the image and use ViLT (Kim, Son, and Kim 2021), BERT (Devlin et al. 2019), and Longformer (Beltagy, Peters, and Cohan 2020) as the Discriminator models. ViLT accepts both an image and its text features as input, while BERT and Longformer only accept text features as input."}, {"title": "Results", "content": "Table 2 shows the results of the Entity Complexity Scoring task. Naive VLISA with a pre-trained language model (BERT/Longformer) as the Discriminator shows competitive performance compared to ViT. When both images and text features are input to the ViLT Discriminator, the model performs significantly better than ViT and other Naive VLISAs. Naive VLISA (BERT/Longformer) outperforms CoT VLISA (BERT/Longformer). One possible reason is that features extracted by Naive VLISA tend to focus more on describing the content at the entity level within the image. Conversely, the Feature Extractor in CoT VLISA extracts higher-level semantic information from the image, but it overlooks some entities. Naive VLISA (ViLT) is less affected by the type of text features, probably because the image itself is visible to it.\nTable 3 shows the results of the Semantic Complexity Scoring task. We can see that predicting Semantic Score is more challenging than predicting Entity Score and traditional vision models cannot perform well on this task. Naive VLISAs show obviously better performance than ViT and ICNet. The possible reason is that with GPT-40 extracting semantic information from the images, the Discriminator in VLISA can perform score prediction at a higher semantic level. Consistent with the previous hypothesis, CoT VLISA shows better performance than Naive VLISA on this task. CoT VLISA (ViLT) shows the best performance. Comparing the performance of VLISAs and vision models on the two tasks highlights the importance of introducing the language modality for the Semantic Complexity Scoring task.\nGenerally speaking, Naive VLISA can perform well on both sub-tasks, and CoT VLISA can further improve the performance on the Semantic Complexity Scoring task.\nOpen-source LVLM as Feature Extractor To further validate the effectiveness of VLISA pipeline, we replace GPT-40 with an open-source LVLM, CogVLM2 (Hong et al. 2024), as the Feature Extractor in Naive VLISA. As shown in Table 4, we observe that using CogVLM2 as the Feature Extractor does not significantly degrade the models' performance, especially for Naive VLISA (ViLT). This demonstrates the robustness of our approach.\nAblation Study CoRs and the description are the two main parts extracted by the Feature Extractor of CoT VLISA, so we validate their effectiveness in this section. Table 5 shows when either CoRs or the description are removed from the extracted text features, there may be a slight performance drop. Therefore, we recommend using both the CoRs and Description as input."}, {"title": "Analysis", "content": "When using VLISA to identify semantically rich images, we recommend starting with those that have high Semantic Scores and then refining the selection based on the Entity Score to match the application scenario. Figure 6 shows some samples with scores predicted by Naive VLISA (Longformer). In the scenario of searching for images similar to the Cookie Theft picture, images with higher Semantic Scores and moderate Entity Scores are preferred, according to the design guidelines.\nIn Figure 6, we can see that images with the highest Semantic Scores generally tell more compelling stories, containing richer semantic information. Interestingly, without including the Cookie Theft image in the training set, the image with the second-highest Semantic Score is Cookie Theft (Figure 6 (b)). Based on Entity Score, we can further filter out images with too few or too many entities. For instance, (a), (e), and (g) contain too many entities, though they also tell interesting stories. That is, the remaining images above the orange line are more preferred by the Cookie Theft design principles. Note that although many images in our dataset are not real-world images, VLISA can still give appropriate Semantic Scores to these real-world images. For example, image (d) in Figure 6 has a high Semantic Score and it is actually quite similar to the Cookie Theft semantically. This demonstrates that our method can avoid the influence of types or styles of images to some extent.\nFor images with the lowest Semantic Scores under the orange line, there is either a single action (Figure 6 (1)) or no event at all (Figure 6 (k, m, n, o)), which is also consistent with our annotation design."}, {"title": "Related Work", "content": "Image Quality Assessment Image Quality Assessment (IQA) is a task to assess the quality of images. It mainly concerns various types of distortions introduced in stages of the visual communication systems. The rapid growth of visual media has driven the development of many IQA methods (Zhai and Min 2020). Some IQA datasets including TID2013 (Ponomarenko et al. 2013), KonIQ-10k (Hosu et al. 2020), SPAQ (Fang et al. 2020) and PaQ-2-PiQ (Ying et al. 2020) etc. are proposed. With the development of LVLMS, Wu et al. (2024) propose Q-Bench to assess their abilities on low-level visual perception and understanding, which plays significant roles in IQA. The difference between IQA and our ISA task is that ISA task focuses on analyzing the semantic content (Zhang, Zhu, and Hwang 2015) of an image rather than its quality.\nImage Aesthetics Assessment Different from IQA, Image Aesthetics Assessment (IAA) task assesses the aesthetics of an image from the perspective of its content. Typical IAA task seeks to computationally assess the quality of photos based on photographic rules (Deng, Loy, and Tang 2017). Several IAA datasets are proposed, for example, the Photo.net dataset (Joshi et al. 2011), the DPChallenge dataset (Datta, Li, and Wang 2008), and the TAD66K dataset (He et al. 2022) etc. As the development of image style transfer and AI painting, Artistic Image Aesthetic Assessment (AIAA) task is proposed to automatically evaluate artwork aesthetics (Amirshahi et al. 2015; Fekete et al. 2022; Yi et al. 2023). The difference between IAA and ISA task is that ISA assesses images based on their semantic richness.\nImage Complexity Assessment Image Complexity Assessment (ICA) is proposed to assess the intricacy contained within an image (Forsythe 2009). It measures the richness of details and diversity within the image (Snodgrass and Vanderwart 1980). The SAVOIAS dataset (Saraee, Jalal, and Betke 2020) contains over 1,000 images and labels for IC analysis. Feng et al. (2023) built the first large-scale IC dataset with 9,600 annotated images IC9600 dataset and proposed a baseline model called ICNet. Compared to IQA and IAA, ICA task is more relevant to ISA task. Despite the differences, the Entity Richness Scoring in ISA shares similarities with the ICA task. However, the key distinction lies in ISA's emphasis on a higher semantic level (Li et al. 2015), rather than merely evaluating complexity at the entity level."}, {"title": "Conclusion", "content": "In this paper, we propose a novel ISA task to identify storytelling images with rich semantics. We propose the first ISA dataset consisting of an Entity Complexity Scoring task and a Semantic Complexity Scoring task. We also propose a simple yet effective method called VLISA as our baseline model for this task. We believe this task will have a wide range of applications in the future. For example, with the Entity Score and Semantic Score, images with different semantic complexity levels can be selected. It can also facilitate AI models in understanding and generating images with richer semantics."}, {"title": "Ethical Statement", "content": "We follow the Terms of Service of Pinterest to collect the images in our ISA dataset. Our dataset will be available to download for research purposes only, which is in compliance with the Terms of Service of Pinterest."}]}