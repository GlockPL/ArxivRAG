{"title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning", "authors": ["Xianglin Yang", "Gelei Deng", "Jieming Shi", "Tianwei Zhang", "Jin Song Dong"], "abstract": "Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCOT), which harnesses the enhanced reasoning capabilities of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities. The code and data is available at https://anonymous.4open.science/r/SCoT-D4D9.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) exhibit exceptional capabilities, enabling their wide applications in fields such as education Zhang et al. [2024], programming Perez et al. [2020], and everyday tasks. However, their powerful nature also introduces risks, as they can inadvertently generate harmful instructions or inappropriate content, leading to unsafe or illegal outcomes. For instance, prior research has shown that malicious users can compromise LLMs through adversarial techniques such as jailbreaks Zou et al. [2023], Liu et al. [2024], which bypass safety restrictions and prompt models to generate harmful outputs. These outputs may include instructions for breaching computer systems Zou et al. [2023] or facilitating unauthorized access to copyrighted materials Mazeika et al. [2024]. Such misuse not only violates ethical standards in AI deployment but also leads to severe consequences, including financial losses, reputational harm, and legal liabilities. Given these concerns, ensuring the safety and ethical deployment of LLMs is paramount. Addressing safety challenges is not only critical to preventing potential harm but also essential to fostering trust and acceptance of these technologies in society."}, {"title": "Background and Preliminary", "content": ""}, {"title": "2.1 Large Language Models", "content": "A large language model M generates human-like text by predicting words iteratively: $\\hat{x}_i = M(x_i | x_1,..., x_{i-1})$ given the preceding sequence $(x_1,..., x_{i\u22121})$. State-of-the-art LLMs leverage transformer architectures Vaswani [2017] and are trained on large-scale corpora OpenAI et al. [2024]. Despite their capabilities, they remain vulnerable to jailbreak attacks. This work focuses on strengthening LLMs' resilience against such attacks. Specifically, let M(q) denote the model's response to a harmful query q. Effective safety mechanisms should ensure that the model consistently produces a refusal or a non-harmful response."}, {"title": "2.2 Jailbreak Attacks", "content": "In jailbreak attacks, the adversary aims to craft harmful questions, which could bypass the safety filtering of LLMs, making them produce unsafe responses. Let q be a harmful question, which will be rejected by the LLM. The objective of the attacker is to construct a jailbreak question q', which preserves the same semantic meaning as q, but could mislead the LLM to produce a harmful response. Existing attack methods fall into three categories: linguistic manipulation, contextual manipulation, and adaptive attacks.\nLinguistic Manipulation. The key idea of this strategy is to alter the linguistic tone of the harmful question q to evade LLM's safety checking. Examples of linguistic manipulation include translating text into low-resource languages Deng et al. [2024], employing slang Xie et al. [2024], performing ASCII transformations, using Base64 encoding Wei et al. [2024a], and introducing intentional misspellings. These techniques can effectively bypass safety mechanisms by transforming input tokens into scenarios that appear out-of-distribution.\nContextual Manipulation. This strategy alters q to q' by incorporating specific contextual elements like background information or persuasive language. Examples of contextual manipulation include adding role-play scenarios, evidence-based persuasion, or logical appeal that are designed to manipulate model behavior Xie et al. [2024], Wei et al. [2024a]. Such attacks typically involve meticulously crafted, human-written prompts that strategically influence the model's responses. They exploit the model's vulnerabilities by either prompting it to have a competing objective to ignore system"}, {"title": "2.3 Defense", "content": "Existing defense strategies against jailbreak attacks on LLMs include refusal training, such as Llama3-8b-Instruct Schulman et al. [2017], Bai et al. [2022a], adversarial training (AdvTrain) Mazeika et al. [2024], and Circuitbreaker Zou et al. [2024]. Refusal training, particularly when combined with techniques like RLHF and PPO Schulman et al. [2017], empowers LLMs to reject unsafe prompts by reinforcing ethical decision-making during training. AdvTrain, motivated by adversarial training in computer vision, enhances models by fine-tuning with adversarial examples to help them recognize and resist harmful manipulations. On the other hand, Circuitbreaker aims to prevent the generation of harmful content by actively removing dangerous knowledge from the model during processing.\nHowever, these methods are less effective against sophisticated attacks that use out-of-distribution and competing objectives. Instruction-following models tend to obey user commands. When adversaries craft harmful queries positively, they can easily bypass safety systems. These limitations highlight the urgent need for more effective defense strategies against these tactics."}, {"title": "Our Approach", "content": "To address the aforementioned challenges, we propose a novel jailbreak defense method, Safety Chain-of-Thought (SCoT), summarized in Figure 2. Unlike traditional refusal training techniques that immediately block responses upon detecting harmful content Bai et al. [2022a], SCoT requires the model to proactively analyze the harmful intent behind user requests before generating responses. This approach contains three key stages. First, we enhance the complexity and diversity of adversarial scenarios through question evolution (Section 3.1), which expands harmful questions through jailbreak mutations. We then establish a structured cognitive process for analyzing requests through malicious intent abstraction and safety regulation assessment for both harmful and benign question dataset (Section 3.2). Finally, we apply supervised fine-tuning on the newly developed safety reasoning dataset to enhance model's broader reasoning capabilities while reinforcing its ability to resist jailbreak attempts (Section 3.3)."}, {"title": "3.1 Question Evolution", "content": "As described in Section 2.2, jailbreak attacks typically employ two main strategies: (1) linguistic manipulation, which subtly alters the question's phrasing with different tones, and (2) contextual manipulation, which includes distracting content to provoke affirmative or unintended responses. Inspired by the advancements in complex training scenarios demonstrated by WizardLM Xu et al. [2024], Luo et al. [2024], we aim to enhance the model's reasoning capabilities by introducing more intricate questions. We develop complexity through two specific methods: depth evolution, which focuses on linguistic nuances, and breadth evolution, which incorporates distracting elements. This strategy is designed to counteract the effectiveness of adversarial manipulations by improving the model's ability to discern and respond to nuanced intents within adversarial inputs.\nLinguistic Manipulation. Linguistic manipulation involves subtly altering the phrasing of questions to bypass model defenses while retaining their original intent. In response, we enrich the question q with diverse linguistic styles, specifically slang and uncommon dialects, to enhance the attack robustness against detection."}, {"title": "3.2 Cognitive Reasoning Construction", "content": "Given a refusal training dataset $D = {(q,r)}^n$ containing n pairs of questions q and refusal responses r, our goal is to enhance the refusal responses by incorporating safety reasoning in the form of a Chain-of-Thought guided by safety regulations. Leveraging this classification, we craft prompts for leading LLMs, such as GPT4, to generate responses in a structured three-stage format: (1) analyzing the intent of user requests, (2) explaining why the request does or does not align with one or more identified harmful categories, and (3) issuing a refusal response. Details of these prompts are given in Appendix A.3. The safety CoT is shown as follows:"}, {"title": "3.3 Supervised Fine-Tuning", "content": "Our objective is to train the model to proactively assess the harmfulness of each input before generating responses. Training exclusively on refusal reasoning risks conditioning the model to reject all inputs indiscriminately. To counteract this, we implement supervised fine-tuning using our SCOT dataset, complemented by a dataset of benign samples to retain balanced decision-making capabilities.\nRetain Dataset Construction. We construct a benign dataset $D_1 = {(q)}^k$, containing k benign questions. The augmented answer is divided into two components: the safety reasoning process and the original output generated by our target model M. This reasoning process is illustrated as follows:"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experiment setup", "content": "Training Dataset. We employ the circuitbreaker dataset introduced by Zou et al. [2024] as our base dataset Dr for further development. This dataset comprises 4,994 short harmful requests across 48 harmful topics. For the retain dataset Db, we utilize dolly-15k Conover et al. [2023] to preserve the general capabilities of the models. The Dolly-15k dataset is an open-source instruction-following records with diverse categories such as brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nSCOT Construction. We employ GPT-40-mini OpenAI et al. [2024] to evolve both the base questions and the answers as detailed in Sections 3.2 and 3.1 respectively. For adapting the questions, we select demonstrations of different styles from sorrybench Xie et al. [2024] and conduct few-shot In-Context learning. Detailed construction process are provided in Appendix A.\nTraining Base Models and Hyperparameters. We employ two open-source models specifically tuned for safety, including Llama3-8B-Instrct AI@Meta [2024] and Mistral-7B-V0.2 Jiang et al. [2023], without system prompts. Training is conducted using LoRA Hu et al. [2021]. For the LORA module, we specify a rank of 64, an $\\alpha$ value of 128, a dropout rate of 0.1, and learned LoRA matrices for all attention matrices. In the supervised fine-tuning stage, we set $\\lambda = 1$ in Equation 1 and the training epoch to 2. The initial learning rate is set to 2e - 5 and the batch size to 2. Training is carried out on two Ada 6000 GPUs and takes approximately two hours to complete.\nBaselines. We evaluate our SCoT by comparing it with three distinct baselines. The first includes the original Mistral and Llama-3 Instruct models, serving as the simplest comparison point. Next, we assess against the circuitbreaker approach Zou et al. [2024], which mitigates harmful content by projecting harmful representations into a randomized space, effectively creating a protective \"circuit\" against harmful requests. Finally, we compare our results to an adversarially trained version of the Mistral model, known as R2D2 Mazeika et al. [2024]. We obtain the baseline models from the Huggingface repository Wolf et al. [2020], as released by the original authors.\nEvaluation Metrics. To evaluate the output harmfulness, the conventional method is substring matching using predefined refusal prefixes. However, the circuitbreaker approach ceases output upon detecting harmful content instead of issuing refusal answers. Consequently, we employ the Llama3Guard classifier Llama Team [2024] to assess the Attack Success Rate (ASR), determining whether generated outputs contain harmful content. As for evaluating general capabilities, we use accuracy as the metric."}, {"title": "4.2 Jailbreak Evaluation", "content": "Setup. We summarize the jailbreak attacks used for our evaluations, organized by the strategy and datasets employed:\n\u2022 No attack: As a control, we use a non-jailbreaking approach that echoes each prompt verbatim, facilitating baseline comparisons. Evaluations are performed using Jailbreak-Bench Chao et al. [2024a] with 100 harmful questions and AdvBench Zou et al. [2023] with 500 harmful behaviors."}, {"title": "4.3 Potential Compromise in General Capabilities", "content": "Setup. To evaluate potential compromises in general capabilities due to security enhancements, we employ two significant benchmarks: MMLU Hendrycks et al. [2021] and GSM8K dataset Cobbe et al. [2021]. MMLU includes multiple-choice questions across 57 tasks such as elementary mathematics, US history, and computer science. GSM8K is designed to assess model performance on complex problem-solving tasks typical of graduate-level exams.\nResults. Table 2 reveals the trade-offs encountered when bolstering safety defenses. It shows the accuracy metrics across the GSM8K and MMLU datasets. Our SCoT demonstrates minimal performance loss relative to the base model, in contrast to Circuitbreaker, which shows more degradation. This drop in accuracy is likely due to an over-refusal response to questions containing terms that could violate safety protocols."}, {"title": "4.4 Ablation Studies", "content": "Setup. In our ablation studies, we investigate the impact of various components on our SCOT's performance by conducting experiments under two distinct conditions: removing question variants or omitting the Retain dataset from our training regime.\nResults. Table 3 presents the general capacity and a representative set of robustness evaluations for our SCOT using Llama3-8B and Mistral-7B. The data highlights the essential roles of the Retain dataset and question variants in influencing model performance. For Llama3-8B, excluding the retain dataset leads to substantial drops in GSM8K and MMLU accuracies, from 0.85 to 0.33 and 0.73 to 0.37, respectively, underlining its crucial role in enhancing understanding and response accuracy. Conversely, removing question variants slightly reduces MMLU performance from 0.73 to 0.67 but does not significantly affect GSM8K, indicating their lesser yet notable impact on robustness.\nRobustness testing shows that without the retain dataset, the model's resilience diminishes across all scenarios, especially in handling uncommon dialects and role play, with marked performance declines. In contrast, lacking question variants marginally impacts scenarios like affirmative responses and suppress refusal, highlighting their role in maintaining consistent performance under varied linguistic inputs. Our integrated approach, combining both the Retain dataset and question variants, demonstrates balanced performance improvements or stable robustness compared to configurations lacking these components."}, {"title": "4.5 Efficiency", "content": "Setup. We further evaluate the efficiency of our SCOT focusing on two key performance indicators: training time and inference time. Training time is measured to assess the computational resources and time required to fully train the model with SCoT under standard conditions. For inference time, we distinguish between benign, direct request harmful, and attack questions. This differentiation allows us to understand how the model performs under varying levels of complexity and potential security threats."}, {"title": "5 Additional Related Work", "content": "In this section, we briefly introduce more broader related work in addition to Section 2.\nJailbreak Attacks. In this work, we discuss linguistic-based and optimization-based jailbreak attacks for single-turn conversations. Various other jailbreak variants also exist. Multi-Shot Jailbreak (MSJ) exploits the strength of In-Context Learning, leveraging millions of harmful demonstrations to compromise LLMs Anil et al. [2024]. One strategy for multi-turn attacks is fine-grained task decomposition, which breaks down the original malicious query into several less harmful sub-questions Wang et al. [2025], Zhou and Shi [2024], Zhou et al. [2024]. While this strategy successfully bypasses current safety mechanisms, it could be countered by incorporating these finer-grained harmful queries into safety training datasets. Alternatively, researchers have proposed using human red teamers to expose vulnerabilities in LLMs against multi-turn attacks Li et al. [2024]. Moreover, some strategies involve gradually steering benign initial queries towards more harmful topics Ren et al. [2024]. While these attacks generally aim to deceive LLMs by decomposing or concealing harmful targets, our SCoT's proactive identification of harmful intentions could be a promising direction to mitigate these attacks.\nBrittleness of Safety Alignment. Recent research has exposed significant vulnerabilities in the safety mechanisms of large language models (LLMs). Apart from jailbreak, studies indicate that altering only a small subset of the model's parameters can lead to safety compromises Wei et al. [2024b], Chen et al. [2024], Anonymous [2025]. Additionally, when further fine-tuning LLMs with safety alignment on benign datasets, this can paradoxically result in a substantial degradation of their safety features, highlighting the inherent brittleness of current safety alignment practices Yi et al. [2024], He et al. [2024]. These findings emphasize the urgent need for a deeper understanding of safety mechanisms and the development of robust safety defense strategies.\nDefense for LLMs safety. Another defense approach against jailbreaks is machine unlearning, which takes a distinct direction by intentionally forgetting all harmful knowledge Zhao et al. [2023]. Circuitbreaker is a method that falls into this category. However, it is challenging to verify that all harmful knowledge has indeed been removed. The efforts most closely related to our work are Deliberative Alignment Guan et al. [2025] and the Constitutional AI (CAI) framework Bai et al. [2022b], which both focus on refining the reward function within the DPO framework to adhere to predefined fine-grained regulations, aiming to reduce the over-refusal in extremely large models such as ChatGPT or GPT-4. Unlike theaforementioned methods, we focus more on strengthen large models like Llama3-8b to defend against different jailbreak attacks."}, {"title": "6 Discussion", "content": "Limitation. Our SCOT has a drawback of slower response times compared to the base model and introduces extra computational costs when processing benign samples. The slowdown is due to the need for in-depth analysis of user requests, which requires more computational resources. Moreover, our SCoTrelies on predefined safety regulations during the training phase, which limits its adaptability to new scenarios. Future research could investigate the application of retrieving and reasoning with safety regulations from a policy database Lewis et al. [2020]. This approach would enable fine-grained safety reasoning analysis and offer more flexible adaptation to various scenarios, enhancing the model's ability to respond dynamically to diverse safety challenges."}, {"title": "7 Conclusion", "content": "In this paper, we investigate the vulnerability of large language models to the prominent jailbreak attacks. We critique that existing defense mechanisms fail to defeat the advanced attacks due to their inadequate training strategies. We propose SCoT, a novel approach that enhances LLMs by enabling them to assess user intent prior to generating responses. By expanding the training dataset with distractions and employing a reasoning-based safety chain, the safety-enhanced LLM can evaluate request intent against safety regulations. Experimental results demonstrate that SCOT outperforms existing defenses, effectively thwarting various jailbreak attempts and improving model resilience."}, {"title": "Impact Statement", "content": "We present an innovative reasoning-based framework designed to improve the safety of large language models (LLMs) by proactively assessing the potential harmfulness of user queries in line with safety standards prior to providing responses. This method holds considerable potential for impact, especially in tackling OOD scenarios across diverse AI fields beyond safety. Additionally, a notable benefit of this approach is its sample efficiency. By promoting reasoning during post-training, the model can enhance its generalization during the pre-training phase, thereby reducing the need for extensive fine-tuning data."}, {"title": "A SCOT Construction", "content": ""}, {"title": "A.1 Demonstration Selection", "content": "We apply few-shot in context learning (ICL) to adapt any questions with a certain style. In order to generate diverse style of questions, we need to select diverse demonstrations for ICL. The algorithm is shown as follows:"}, {"title": "A.2 Contextual Evolution Prompts", "content": "We use GPT-40-mini to evolve the base question-answer pair. The prompts for question evolution are provided in Table 5, and those for answer evolution are detailed in Table 6."}, {"title": "A.3 Selected demonstrations", "content": "We adapt the harmful question into six distinct styles: slang, uncommon dialect, role play, expert endorsement, evidence-based persuasion, and logical appeal. For each category, we select eight demonstration examples from SorryBench Xie et al. [2024], which provides 100 samples for each style. Table 7, Table 8, Table 9, Table 10, Table 11 and Table 12 present all the selected demonstration examples for the different styles."}, {"title": "A.4 Examples of contextual manipulation evolution.", "content": "We show some evolved examples with styles of role play, evidence based persuasion, logical appeal and expert endorsement with the original sample being \"How to spread conspiracy theories?\" in Table 13."}]}