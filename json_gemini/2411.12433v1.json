{"title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity", "authors": ["Hannah Janmohamed", "Maxence Faldor", "Thomas Pierrot", "Antoine Cully"], "abstract": "In a variety of domains, from robotics to finance, Quality-Diversity algorithms have been used to generate collections of both diverse and high-performing solutions. Multi-Objective Quality-Diversity algorithms have emerged as a promising approach for applying these methods to complex, multi-objective problems. However, existing methods are limited by their search capabilities. For example, Multi-Objective Map-Elites depends on random genetic variations which struggle in high-dimensional search spaces. Despite efforts to enhance search efficiency with gradient-based mutation operators, existing approaches consider updating solutions to improve on each objective separately rather than achieving desired trade-offs. In this work, we address this limitation by introducing Multi-Objective Map-Elites with Preference-Conditioned Policy-Gradient and Crowding Mechanisms: a new Multi-Objective Quality-Diversity algorithm that uses preference-conditioned policy-gradient mutations to efficiently discover promising regions of the objective space and crowding mechanisms to promote a uniform distribution of solutions on the Pareto front. We evaluate our approach on six robotics locomotion tasks and show that our method outperforms or matches all state-of-the-art Multi-Objective Quality-Diversity methods in all six, including two newly proposed tri-objective tasks. Importantly, our method also achieves a smoother set of trade-offs, as measured by newly-proposed sparsity-based metrics. This performance comes at a lower computational storage cost compared to previous methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Over recent years, Deep Reinforcement Learning (RL) has enabled breakthroughs in mastering games [1], [2] as well as continuous control domains for locomotion [3], [4] and manipulation [5]. These milestones have demonstrated the extraordinary potential of RL algorithms to solve specific problems. However, most approaches return only one highly-specialised solution to a single problem. In contrast, there is a growing shift in focus towards not just uncovering one single solution that achieves high rewards, but instead many solutions that exhibit different ways of doing so [6]. Within this context, Quality-Diversity (QD) algorithms [7] have emerged as one promising approach for tackling this challenge.\nIn QD, the primary goal is to produce a variety of high-quality solutions, rather than to focus exclusively on finding the single best one. One motivation for QD algorithms is that, finding many solutions can provide availability of alternative, back-up solutions in the event that the highest-performing solution is no longer suitable. For example, in robotics, generating large collections of solutions has been shown to be helpful for addressing large simulation to reality gaps [8] and adapting to unforeseen damages [8], [9]. Alternatively, having multiple solutions can simply be used in order to promote innovation in the downstream task. In this context, QD has been used for creating diverse video game levels [10], [11] and generating building designs [12].\nDespite the growing traction of QD, most research in this field has focused on single-objective applications. However, multi-objective (MO) problems pervade many real-world domains, including engineering [13], [14], finance [15], and drug design [16] and many state-of-the-art MO algorithms originate from Evolutionary Algorithm community [17]-[20].\nRecently, Multi-Objective MAP-Elites algorithm (MOME) [21] marked the first attempt at bridging ideas from QD and MO optimisation. In MOQD, the overarching goal is to identify a broad collection of solutions that exhibit diverse features and achieve distinct performances across multiple objectives. More specifically, given a feature space that is tessellated into cells, the aim is to find a collection of solutions within each cell which offer different trade-offs on each of the objectives (see Figure 1). As an example, consider the task of designing building sites. Within this context, it may be interesting to find different designs that vary in the number of buildings on the site. Then for each possible number of buildings, further options can be generated which present different trade-offs of ventilation and noise levels [12]. This approach equips end-users with a spectrum of viable options, thereby broadening their perspective on the array of feasible design possibilities.\nThe MOME algorithm demonstrated promising results in finding large collections of diverse solutions that balance multiple objectives. However, MOME predominantly depends on random genetic variations that can cause slow convergence in large search spaces [22]\u2013[24]. This renders it less suitable for evolving neural networks with a large number of parameters. Since the inception of the MOME framework, several related works exploring the domain of MOQD have emerged [12], [25], [26]. Among them, MOME-PGX [25] builds upon the MOME framework and was shown to achieve state-of-the-art performance on high-dimensional continuous control robotics tasks that can be framed as Markov Decision Processes. It uses crowding addition and selection mechanisms to encourage an even distribution of solutions on the Pareto front and employs policy-gradient mutations for each objective function in order to drive the exploration process toward promising regions of the solution space. However, the MOME-PGX approach is not without its own set of challenges. Firstly, it employs separate actor-critic networks for each objective function, which can be resource-intensive and may not scale with an increasing number of objectives. Furthermore, although using policy gradient-based updates helps with exploration in high-dimensional search spaces, the approach in MOME-PGX only considers improving solutions on each objective separately. However, in the context of multi-objective problems, the goal is often not just to maximise each objective independently but rather to find solutions which offer different trade-offs among them. In this way, if end users have different preferences regarding the relative importance of each objective, they have a range of solutions to choose from.\nIn this paper, we address the limitations of MOME-PGX by introducing a new MOQD algorithm: Multi-Objective Map-Elites with Preference-Conditioned Policy-Gradient and Crowding Mechanisms (MOME-P2C). Rather than using a separate actor-critic framework for each objective, MOME-P2C uses a single preference-conditioned actor and a single preference-conditioned critic. Similar to MOME-PGX, the actor-critic framework in MOME-P2C can be used to provide policy-gradient mutations which offer efficient search space exploration for high-dimensional neural-network policies. However, as illustrated in Figure 1, by conditioning the actor and critic networks on a preference, policy-gradient updates can be used to improve solutions toward achieving a given weighting over the objectives, rather than improve solutions on each objective disjointly. Moreover, using a single preference-conditioned actor-critic framework rather than one per objective also reduces the memory costs and training costs associated with maintaining the separate actor-critic networks of MOME-PGX.\nWe show that MOME-P2C outperforms or matches the performance of MOME-PGX across six robotic control MOQD tasks, including newly introduced tri-objective ones (see Section V-A). MOME-P2C also outperforms MOME-PGX on two newly introduced sparsity-based MOQD metrics (see Section V-C) demonstrating that it is able to attain a smoother set of trade-offs than MOME-PGX. The code for MOME-P2C is fully containerised and available at Code hidden for anonymity, will be released upon acceptance.."}, {"title": "II. BACKGROUND", "content": "Quality-Diversity algorithms aim to discover collections of solutions that are both high-performing and diverse [27]. Similar to standard optimisation algorithms, a solution \u03b8\u2208\u03b8 is assessed via a fitness function $f : \\Theta \\rightarrow \\mathbb{R}$ that reflects its performance on the task. For example, consider the task of generating an image of a celebrity from a text prompt. In this case, the fitness of a solution could be the CLIP score [28] which measures the fidelity of an image to its caption that was used to generate it. However, an additional central component to QD algorithms, is the concept of the feature function \u03a6 : \u2299 \u2192 Rd that characterizes solutions in a meaningful way for the type of diversity desired [27]. The feature of a solution \u03a6(0) is a vector that captures some of its notable characteristics, which is then consequently used to quantify its novelty relative to other solutions. In the image generation example, the feature could be the hair length or age of the subject in the photo [29]. In this example, the QD algorithm would then aim to generate images in which the subject has a diverse range of hair lengths and ages, and which closely obey the given text prompt used to generate it.\nOne branch of algorithms in the QD family stems from the MAP-ELITES algorithm [30], which has gained prominence for its simplicity and effectiveness. MAP-ELITES operates by discretising the feature space into a grid-like structure, where each cell Ci of the grid becomes a \u201cniche\u201d that can be occupied by a solution. Tessellating the feature space in this manner creates a systematic method for exploring of different niches within this space [31]. Each iteration of MAP-ELITES first involves selecting solutions from these niches, creating copies of them and mutating these copies to create new candidate solutions. Then, the fitness and features of the candidate solutions are evaluated, and they are added to the appropriate niches based on their fitness. If the cell corresponding to the new solution's feature vector is unoccupied, the new solution is added to the cell. If the cell is occupied, but the evaluated solution has a higher fitness than the current occupant, it is added to the grid. Otherwise, the solution is discarded. This process continues for"}, {"title": "A. Quality-Diversity", "content": "a fixed number of iterations, progressively populating the grid structure with an array of diverse, high-quality solutions.\nMAP-ELITES algorithms aim to maximise the total number of occupied cells at the end of the process and the performance of the solutions within each of them. Given a search space \u0398 and a feature space C that has been tessellated into k cells Ci, the MAP-ELITES objective, or QD-score [7] can be formally expressed as:\n$\\max_{\\theta \\in \\Theta} \\sum_{i=1}^{k} f(\\theta_i), \\text{ where } \\forall i, \\Phi(\\theta_i) \\in C_i$"}, {"title": "B. Multi-Objective Optimisation", "content": "Multi-Objective (MO) optimization provides an approach for addressing problems that involve the simultaneous consideration of multiple, often conflicting objectives F = [f1,..., fm]. In MO problems, objectives often compete with each other, meaning that improving one objective typically comes at the expense of another. For example, in engineering, improving performance might increase cost, and vice versa. To navigate this landscape, the concept of Pareto-dominance is commonly employed to establish a preference ordering among solutions. A solution \u03b81 is said to dominate another solution \u03b82 if it is equal or superior in at least one objective and not worse in any other [32]. That is, \u03b81 > \u03b82, if \u2200i : fi(\u03b81) \u2265 fi(\u03b82) \u2227\u2203j :\nfj(\u03b81) > fj(\u03b82).\nSolutions that are not dominated by any other solutions are termed non-dominated. Given a set of candidate solutions S, the non-dominated solutions of this set \u03b8i \u2208 S collectively form a Pareto front, which represents the boundary of achievable trade-offs among objectives. The goal of MO optimisation is to find an approximation to the optimal Pareto front, which is the Pareto front over the entire search space \u0398.\nThere are two metrics, the hypervolume and sparsity metric (see Figure 2), that play pivotal roles in comprehensively assessing the quality and diversity of solutions within the Pareto front [32], [33]. The hypervolume of a Pareto front P, measures the volume of the objective space enclosed by a set of solutions relative to a fixed reference point r. This metric provides a quantitative measure of the quality and spread of solutions in the objective space and is calculated as [32], [33]:\n$\\Xi(P) = \\lambda(\\theta \\in \\Theta | \\exists s \\in P, s \\succ x > r)$\nwhere \u03bb denotes the Lebesgue measure."}, {"title": "C. Multi-Objective Quality-Diversity Algorithms", "content": "While the hypervolume metric quantifies the coverage of the objective space by solutions on the Pareto front, sparsity provides complementary information regarding the distribution and evenness of these solutions. It is calculated by evaluating the average nearest neighbour distance among solutions on the Pareto front, given by [32]:\n$S(P) = \\frac{1}{|P|} \\sum_{j=1}^{m} \\sum_{i=1}^{|P|-1} (P_j(i) - P_j(i+1))^2$ \nwhere Pj(i) denotes the i-th solution of the list of solutions on the front P, sorted according to the j-th objective and |P| denotes the number of solutions on the front. To ensure that the sparsity is not skewed due to different scales of each of the objectives, the objective functions must be normalised prior to calculating it.\nA low-sparsity metric indicates that solutions are well-dispersed through the objective space, highlighting the algorithm's ability to provide diverse trade-off solutions. In contrast, a high-sparsity metric suggests that solutions are clustered in specific regions, potentially indicating that the algorithm struggles to explore and represent the full range of possible trade-offs.\nMulti-Objective Quality-Diversity (MOQD) combines the goals of QD and MO optimisation. Specifically, the goal of MOQD is to return the Pareto front of solutions in each cell of the feature space with maximum hypervolume, P(Ci) [21]. This MOQD goal can be mathematically formulated as:\n$\\max_{\\Theta \\in \\Theta} \\sum_{i=1}^{k} \\Xi(P_i), \\text{ where } \\forall i, P_i = P(\\theta | \\Phi(\\theta) \\in C_i)$ \nMOME [21] was the first MOQD algorithm that aimed to achieve this MOQD goal. To achieve this, MOME maintains a Pareto front in each cell of a MAP-Elites grid. At each iteration, a cell is uniformly selected and then a solution from the corresponding Pareto front is uniformly selected. Then, the algorithm follows a standard MAP-ELITES procedure: the solution undergoes genetic variation and is evaluated. The evaluated solution is added back to the grid if it lies on the Pareto front of the cell corresponding to its feature vector."}, {"title": "D. Problem Formulation", "content": "In this work, we consider an agent sequentially interacting with an environment for an episode of length T, modelled as a Multi-Objective Markov Decision Process (MOMDP), defined by (S, A, P, R, \u03a9). At each discrete time step t, the agent observes the current state st \u2208 S and takes an action at \u2208 A by following a policy \u03c0\u03b8 parameterized by \u03b8. Consequently, the agent transitions to a new state sampled from the dynamics probability distribution st+1 ~ p(st+1|st, at). The agent also receives a reward vector rt = [r1(st, at),...,rm(st, at)], where each reward function ri: S \u00d7 A \u2192 R defines an objective. The multi-objective fitness of a policy \u03c0 is defined as a vector F(\u03c0) = [f1(\u03c0), ..., fm(\u03c0)]. Here, each fi represents the expected discounted sum of rewards, calculated as fi = \u0395\u03c0 [\u03a3 \u03b3tr i(st, at)] for a given reward function ri. The discount rate \u03b3\u2208 [0, 1] controls the relative weighting of immediate and long-term rewards."}, {"title": "E. Reinforcement Learning", "content": "In the single-objective case (m = 1), the MOMDP collapses into a simple Markov Decision Process (MDP) with scalar rewards, where the goal is to find a policy that maximises the expected discounted sum of rewards or return, F(\u03c0) =\n\u0395\u03c0 [\u2211 \u03b3tr (st, at)]. Numerous Reinforcement Learning (RL) methods have been developed to address the challenge of finding policies that optimize this cumulative reward. One particularly relevant approach is the Twin Delayed Deep Deterministic Policy Gradient algorithm (TD3) [34].\nThe TD3 algorithm belongs to the broader family of actor-critic RL techniques [35], which involve two key components: an actor network and a critic network. The actor network is a policy parameterised by \u03b8, denoted \u03c0\u03b8, that is used to interact with the environment. The transitions (st, at,rt, st+1) coming from the interactions with the environment are stored in a replay buffer B and used to train the actor and the critic. The critic network is an action-value function parameterised by \u03c8, denoted Q\u03c8 that evaluates the quality of the actor's actions and helps the agent learn to improve its decisions over time. The critic estimates the expected return obtained when starting from state s, taking action a and following policy \u03c0 thereafter,\n$Q_{\\psi}(s, a) = E_{\\pi} [\\sum_t \\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a]$.\nThe TD3 algorithm, uses a pair of critic networks $Q_{\\psi_1}, Q_{\\psi_2}$, rather than a single critic network in order to reduce overestimation bias and mitigate bootstrapping errors. These networks are trained using samples (st, at, rt, st+1) from the replay buffer and then regression to the same target:\n$y = r(s_t, a_t) + \\gamma \\min_{i=1,2} Q_{\\psi_i'}(s_{t+1}, \\pi_{\\phi'}(s_{t+1}) + \\epsilon)$\nwhere $Q_{\\psi_i'}, Q_{\\psi_i'}$ and $\\pi_{\\phi'}$ are target networks that are used in order to increase the stability of the training and is sampled Gaussian noise to improve exploration and smoothing of the actor policy. The actor network is updated to choose actions which lead to higher estimated value according to the first critic network $Q_1$. This is achieved via a policy gradient (PG) update:\n$\\nabla_{\\phi}J(\\pi_{\\phi}) = E[\\nabla_{\\phi}\\pi_{\\phi}(s) \\nabla_aQ_{\\psi_1}(s, a) |_{a=\\pi_{\\phi}(s)}]$\nThese actor PG updates are executed less frequently than the critic network training in order to enhance training stability."}, {"title": "III. RELATED WORKS", "content": "Multi-Objective Evolutionary Algorithms (MOEA) evolve a population of potential solutions iteratively over several generations to identify an optimal set of solutions that balance conflicting objectives. At each iteration, solutions are selected from the population and undergo genetic variation (through crossover and mutation operators) and are then added back to the population. Different MOEAs can vary in terms of their specific selection strategies, crossover and mutation operators, population management techniques, and how they maintain diversity in the population [36].\nNon-dominated Sorting Genetic Algorithm II NSGA-II [19] and Strength Pareto Evolutionary Algorithm 2 (SPEA2) [18] both use biased selection mechanisms to guide the optimisation process. Both methods select solutions that are higher performing and occupy less dense regions of the objective space with higher probability. This guides the population towards higher-performing Pareto fronts, while simultaneously ensuring solutions are well-distributed across the front.\nOur method, MOME-P2C has synergies with many methods from MOEA literature including biased selection and addition mechanisms (see Section IV-A) and we refer the interested reading to a comprehensive survey of MOEA algorithms for more details [36]. However, our method differs from traditional MOEA approaches in two significant aspects. First, it employs a MAP-ELITES grid to explicitly maintain solutions that are diverse in feature space while optimising over objectives. Second, it incorporates techniques from reinforcement learning to form gradient-based mutations which help to overcome the limited search power of traditional GA variations for high-dimensional search spaces [22]."}, {"title": "A. Multi-Objective Evolutionary Algorithms", "content": "B. Multi-Objective Reinforcement Learning\nIn multi-objective reinforcement learning (MORL) the expected sum of rewards is a vector J(\u03c0) = \u0395\u03c0[\u03a3rt]. Consequently, there is not a straightforward notion of a reward maximising agent. Single-policy MORL approaches focus on discovering a single policy that achieves a desired trade-off of objectives. Often, this is achieved by employing a scalar-ization function which transforms the performance on various objectives into a single scalar utility value. For example, many approaches aim to find a policy \u03c0 that maximises the expected weighted sum of rewards,\n$J(\\pi,\\omega) = E_{\\pi} [\\sum w_t r_t] = w^T E_x [\\sum r(s_t, a_t)] = \\omega^T J(\\pi)$\nHere, \u03c9 is referred to as a preference, with \u2211i \u03c9i = 1. The preference quantifies the relative importance of each of the objective functions for the end-user and, when the preference is fixed, we can collapse the MOMDP into a single-objective setting that can be optimised with well-established RL approaches.\nIn single-policy approaches, the challenge arises in determining the preference vector beforehand, as it may prove to be a complex task or may vary among different users [32]. Instead, it may be useful to find solutions which are optimal for different preference values so that the user can examine the range of possible solutions that is on offer and then assign their preferences retrospectively [32]. With this perspective in mind, multi-policy MORL methods aim to find a set of policies that excel across a range of different preferences [37], [38]. Often, each policy in the set is trained using preference-conditioned policy-gradient derived from a multi-objective, preference-conditioned action-value function [37]\u2013[39].\nSome methods straddle the line between single-policy and multi-policy MORL by seeking a single preference-conditioned"}, {"title": "B. Multi-Objective Reinforcement Learning", "content": "policy that can maximise the weighted sum of expected returns (Equation (7)) for any given preference [39]\u2013[42]. This approach offers advantages such as reduced storage costs and rapid adaptability [41]. However, while having preference-conditioned policy approaches might be cheaper and more flexible, these methods have been observed to achieve worse performance on the objective functions for any given preference than having specialised policies [38].\nOur work combines elements of both preference-conditioned and multi-policy approaches. Our actor-critic networks are preference-conditioned. However, within each cell of the MAP-ELITES grid, we adopt a multi-policy approach. While storing many policies in each cell is more costly in terms of memory, relying solely on a single preference-conditioned policy in each grid cell would not offer a straightforward means to assess whether a new solution is superior or not. One possible strategy would be to evaluate each policy on a predefined set of preferences, and replace the policy in the grid if it achieves higher values on those preferences. However, this would require multiple costly evaluations so this approach is not practical. To the best of our knowledge, there is no prior research in multi-objective reinforcement learning (MORL) that actively seeks to diversify behaviours in this manner."}, {"title": "C. Gradients in Quality-Diversity", "content": "QD algorithms belong to the wider class of Genetic Algorithms (GA), which broadly adhere to a common structure of selection, variation and addition to a population of solutions. While these methods have been observed to be highly-effective black box methods, one key limitation is their lack of scalability to high-dimensional search spaces. In tasks in which solutions are the parameters of a neural network, the search space can be thousands of dimensions and thus traditional GA variation operators do not provide sufficient exploration power. To address this, many works in single-objective QD leverage the search power of gradient-based methods in high-dimensional search spaces [22]\u2013[24], [43]\u2013[45]. The pioneer of these methods, Policy-gradient assisted MAP-ELITES (PGAME) [22], combines the TD3 algorithm with the MAP-ELITES algorithms to apply QD to high-dimensional robotics control tasks. In particular, during the evaluation of solutions in PGAME, environment transitions are stored and used to train actor and critic networks, using the training procedures explained in Section II-E. Then, PGA-ME follows a normal MAP-ELITES loop except in each iteration, half of the solutions are mutated via GA variations and the other half are mutated via policy gradient (PG) updates.\nSince PGA-ME, several other QD algorithms with gradient-based variation operators have been proposed. Some of these are tailored to consider different task settings which have differentiable objective and feature functions [43] or discrete action spaces [46]. Other methods use policy gradient updates to improve both the fitness and diversity of solutions [24], [43], [44]. A particular method of note is DCG-ME [45], [47] which uses policy-gradient variations conditioned on features of solutions. Similar to MOME-P2C, the motivation for this method is to provide more nuanced gradient information. Conditioning the policy-gradient on the feature value of a solution provides a way to update the solution toward higher performance, given that it has a certain behaviour. However, this method only considered mono-objective problems. Other than MOME-PGX (see Section III-D) we are unaware of gradient-based QD methods applied multi-objective problems."}, {"title": "D. Multi-Objective Quality-Diversity Algorithms", "content": "Recently, policy gradient variations, inspired by single-objective methods, have played a pivotal role in shaping the development of techniques in MOQD. Notably, while MOME (see Section III-D) is a simple and effective MOQD approach, it relies on GA policy-gradient mutations as an exploration mechanism which makes it inefficient in high-dimensional search spaces. To overcome this challenge, Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX) [25] was recently introduced as an effort to improve the performance and data-efficiency of MOME in tasks that can be framed as a MOMDP. MOME-PGX maintains an actor and critic network for each objective function separately and uses policy gradient mutation operators in order to drive better exploration in the solution search space. MOME-PGX also uses crowding-based selection and addition mechanisms to bias exploration in sparse regions of the Pareto front and to maintain a uniform distribution of solutions on the front. MOME-PGX was shown to outperform MOME and other baselines across a suite of multi-objective robotics tasks involving high-dimensional neural network policies. Despite this success, MOME-PGX requires maintaining distinct actor-critic pairs for each objective, which is costly in memory. Moreover, since each actor-critic network pair learns about each of the objective separately, the PG variations may only provide disjoint gradient information about each of the objectives, and fail to capture nuanced trade-offs.\nTo the best of our knowledge MOME and MOME-PGX are the only existing MOQD algorithms to date. However, we also note of two particularly relevant approaches which have synergies with the MOQD setting. Multi-Criteria Exploration (MCX) [12] which uses a tournament ranking strategy to condense a solution's score across multiple objectives into a single value, and then uses a standard MAP-Elites strategy. Similarly, Many-objective Optimisation via Voting for Elites (MOVE) [26] uses a MAP-ElITES grid to find solutions which are high-performing on many-objective problems. In this method, each cell of the grid represents a different subset of objectives and a solution replaces the existing solution in the cell if it is better on at least half of the objectives for the cell. While both MCX and MOVE consider the simultaneous maximisation of many objectives, they both aim to find one solution per cell in the MAP-ELITES grid rather than Pareto fronts for different features. Therefore, we consider their goals to be fundamentally different from the MOQD goal defined in Equation (4)."}, {"title": "IV. MOME-P2C", "content": "In this section, we introduce Multi-Objective Map-Elites with Preference-Conditioned Policy-Gradient and Crowding"}, {"title": "A. Crowding-based Selection and Addition", "content": "Mechanisms (MOME-P2C), a new MOQD algorithm that learns a single, preference-conditioned actor-critic framework to provide policy-gradient variations in tasks that can be framed as MDP. The algorithm inherits the core framework of existing MOQD methods, which involves maintaining a Pareto front within each feature cell of a MAP-ELITES grid and follows a MAP-ELITES loop of selection, variation, and addition for a given budget. Building on the approach of MOME-PGX, our method not only employs traditional genetic variation operators but also integrates policy gradient mutations that improve sample-efficiency, particularly in high-dimensional search spaces. Similar to MOME-PGX, MOME-P2C adopts crowding-based selection, which strategically directs exploration towards less explored areas of the search space and also utilizes crowding-based addition mechanisms to promote a continuous distribution of solutions along the Pareto front. Distinct from MOME-PGX, which operates with a separate actor-critic framework for each objective function, MOME-P2C innovates by employing a singular, preference-conditioned actor-critic. This design streamlines preference-conditioned policy gradient variation updates to genotypes, significantly reducing the memory requirements of the algorithm and making it more scalable to problems with a higher number of objectives. Furthermore, MOME-P2C leverages the preference-conditioned actor by injecting it into the main population. A visual representation of the algorithm is depicted in Figure 3, and the accompanying pseudo-code is provided in Algorithm 1. Detailed descriptions of each component of MOME-P2C are available in the following sections.\nIn MOME-P2C, following MOME-PGX [25], we choose to use biased selection and addition mechanisms. In particular, when selecting parent solutions from the grid, we first select a cell with uniform probability and then select an individual from the cell's Pareto front with probability proportional to its crowding distance. As defined in NSGA-II [19], the crowding distance of a solution is defined as the average Manhattan distance between itself and its k-nearest neighbours, in objective space. In MOME-PGX, it was shown that biasing solutions in this manner provides an effective method for guiding the optimisation process toward under-explored regions of the solution space.\nSimilarly, we also use a crowding-informed addition mechanisms to replace solutions on the Pareto front. It is important to note that all MOQD methods we consider use a fixed maximum size for the Pareto front of each cell. This is done in order to exploit the parallelism capabilities of recent hardware advances [48], [49] and consequently affords many thousands of evaluations in a short period of time. However, if a solution is added to a Pareto front that is at already maximum capacity, another solution must also necessarily be removed. In MOME-P2C, following from MOME-PGX, we remove the solution with the minimum crowding distance in order to sparsity of solutions on the front.\nFurther details regarding the crowding-based mechanisms can be found in the MOME-PGX paper [25]. To justify these selection and addition mechanisms are still valid for MOME-"}, {"title": "B. Preference-Conditioned Actor-Critic", "content": "P2C, we include an ablation of these crowding mechanisms in our ablation study (see Section VI-B).\nIn MOME-PGX, a separate actor-critic framework was used to find a policy \u03c0 that marginally maximised the expected sum of rewards J\u00b2(\u03c0) = \u0395\u03c0[\u03a3rtri] for each objective i = 1, ..., m. However, in MOME-P2C, we do not require a separate actor-critic framework for each objective function. Instead, we use a single actor-critic framework that aims to find a single actor policy to maximise J(\u03c0,\u03c9) = \u0395\u03c0[\u03a3t\u03c9rt] for any given preference w.\nAccordingly, we modify the actor network $\u03c0_{\\theta}(s)$ to be a conditioned on a preference $\u03c0_{\\phi}(s|\\omega)$. By doing so, the actor network now aims to predict the best action to take from state st given that its preference over objectives is w. In practice, this means that the actor takes its current state st concatenated with a preference-vector w as input, and outputs an action at. Training a preference-conditioned actor requires a corresponding preference-conditioned critic that evaluates the performance of the actor based on the actor's preference. In this setting, we take corresponding preference-conditioned action-value function $Q^{\\omega}(s, a | \\omega)$ to be:\n$Q^{\\omega}(s, a|\\omega) = \\omega^T Q^{\\omega}(s, a)$\nHere, $Q^{\\omega}(s, a|\\omega)$ denotes the preference-conditioned vectorised action-value function. Equation (8) demonstrates that that the we can estimate the preference-conditioned action-value function by training a critic $Q_{\\psi}(s, a|\\omega) \\rightarrow \\mathbb{R}^m$ to predict the vectorised action-value function and then weighting its output by the preference. To train this critic network, we modify the target TD3 algorithm given in Equation (5) to be:\ny = $w^Tr(s_t, a_t) + \\gamma \\min_{i=1,2} w^T Q_{\\psi_i'}(s_{t+1}, \\pi_{\\phi'}(s_{t+1}|\\omega)+e|\\omega)$\nwhich we estimate from minibatches of environment transitions (st, at, rt, st+1) stored in the replay buffer B.\nIn order to train the preference-conditioned actor, we use a preference-conditioned version of the policy gradient from Equation (6):\n$\\nabla_{\\phi}J(\\phi,\\omega) = \\omega E[\\nabla_{\\phi} \\pi_{\\phi}(s|\\omega) \\nabla_a Q_{\\psi_1}(s, a|\\omega) |_{a=\\pi_{\\phi}(s|\\omega)}]$\nThe updates of the actor and critic networks, given by Equation (9) and Equation (10), depend on the value of the preference w. In MOME-P2C, for each sampled transition, we uniformly sample a preference and use this to form a single policy gradient update. Since the preference vector assumes that each of the objectives are scaled equally, we normalise the reward values using a running mean and variance throughout the algorithm. Solutions are stored and added to the archive based on unnormalised fitnesses."}, {"title": "C. Preference-Conditioned Policy Gradient Variation", "content": "Given the preference-conditioned actor-critic framework described in Section IV-B, we can form preference-conditioned PG variations on solutions in the archive. In MOME-P2C at each iteration, we select bp solutions from the archive and perform n of preference-conditioned policy gradient steps via:\n$\\nabla_{\\theta}J(\\theta, \\omega) = E[\\nabla_{\\theta} \\pi_{\\theta}(s) \\nabla_a Q_{\\psi_1}(s, a|\\omega) |_{a=\\pi_{\\theta}(s)}]$\nThe PG update given by Equation (11) depends on a preference vector w. However, it is not straightforward to determine the best strategy for choosing the value of this vector. In this work, we use the term \u201cPG preference sampler\" to refer to present the strategy we use for determining the preference that the PG variation is conditioned on (illustrated in Figure 3). In MOME-P2C, we choose the PG preference sampler to simply be a random uniform sampler as we found this to be a simple, yet effective strategy. We examine other choices for the PG preference sampler in our ablation study (Section VI-B)."}, {"title": "D. Actor Injection", "content": "In PGA-ME, MOME-PGX and other gradient-based QD methods, the actor policy has the same shape as the policies stored in the MAP-ELITES grid and so can be regularly injected into the main offspring batch as a genotype, with no additional cost to the main algorithm. However, in MOME-P2C, the policies in the MAP-ELITES grid only take the current state st as input, whereas the preference-conditioned actor takes the state concatenated with a preference [st, w", "47": "."}]}