{"title": "A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models \u2013\nSafety, Consensus, Objectivity, Reproducibility and Explainability", "authors": ["Ting Fang Tan", "Kabilan Elangovan", "Jasmine Ong", "Nigam Shah", "Joseph Sung", "Tien Yin Wong", "Lan Xue", "Nan Liu", "Haibo Wang", "Chang Fu Kuo", "Simon Chesterman", "Zee Kin Yeong", "Daniel SW Ting"], "abstract": "A comprehensive qualitative evaluation framework for large language models (LLM) in\nhealthcare that expands beyond traditional accuracy and quantitative metrics needed.\nWe propose 5 key aspects for evaluation of LLMs: Safety, Consensus, Objectivity,\nReproducibility and Explainability (S.C.O.R.E.). We suggest that S.C.O.R.E. may form\nthe basis for an evaluation framework for future LLM-based models that are safe, reliable,\ntrustworthy, and ethical for healthcare and clinical applications.", "sections": [{"title": "Main", "content": "Since the debut of ChatGPT (generative pre-trained transformer) in 2022, there has been\nan exponential surge in interest on large language models (LLMs). LLMs utilize advanced\ndeep learning techniques, particularly transformer architectures, to learn complex\nassociations from vast amounts of unstructured text. Unlike traditional neural networks,\ntransformers use attention mechanisms to capture patterns in sequential data, enabling\nmore sophisticated understanding and generation of human language. \nGenerative artificial intelligence (Al) applications built on backend LLMs enable realistic\nuser interactions through text-based dialogue. Studies have shown the feasibility of\ngenerative Al in healthcare, demonstrating capabilities such as passing medical board\nexaminations, answering clinical questions, providing medical advice, and interpreting\nclinical scenarios and investigations. This integrated understanding of Al concepts and\ntheir interrelationships underscores the potential of generative Al in advancing healthcare\napplications 1,2.\nWhile these initial observations suggest the potential of generative Al and specifically\nLLMs in revolutionizing healthcare delivery, evaluation of these new Al models and their\napplications have been variable with no consistent metrics employed. Furthermore,\nobservations of 'hallucinations' in generated responses from LLMs, where the models\nproduce content that is entirely fabricated or nonsensical, and 'falsehood mimicry,' where\nincorrect information is presented in a seemingly accurate and confident manner, are\namong the cautions raised against LLMs. These issues highlight the importance of critical\nevaluation and validation when using LLMs in sensitive applications like healthcare.\nThese issues are particularly challenging in clinical medicine and healthcare, where\nmisinformation can result in significant harm to patient safety, such as incorrect diagnoses,\nprognoses, and clinical management recommendations. Sometimes, the generated\ninformation is not only incorrect but also biased, especially regarding controversial topics.\nWhether it is beneficial to present 'balanced' information is also debatable, as it might\nconfuse patients further\u00b3. Therefore, there is a compelling need for more detailed and\ndomain-specific evaluation of LLM-driven algorithms in healthcare use cases.\nTo address some of these issues, publicly available benchmark datasets (Eg.\nPubMedQA4, MedMCQA5, MultiMedQA6, Measuring Massive Multitask Language\nUnderstanding (MMLU) clinical topics7) have been used to quantitatively compare\nperformance between various LLMs and clinicians of varying expertise. For example,\nMultiMedQA is a curated dataset of 6 medical question-answering datasets covering\nmedicine, research and consumer queries, and HealthSearchQA is a dataset of online\nsearches of medical questions. These datasets are largely in multiple-choice format,\nfacilitating summation of scores for quantitative, objective and standardized comparison\nof performance. While standardized test sets based on medical board examinations may\nnot be representative of clinical competency in real-word clinical practice, these datasets\nnevertheless provide a benchmark and insights from LLM-generated explanations that\nmay serve as an educational tool for residents-in-training in dissecting clinical concepts\nand supplementing additional resources.\nThe traditional quantitative metrics employed for LLM evaluation focus on text similarity,\nwhere generated responses are compared against a reference text as the ground truth9,10.\nExamples are listed in Table 111-14. These metrics enable quantifiable comparison\namongst models that can be efficiently automated. However, while these metrics are\nuseful for traditional NLP tasks such as text summarization or machine translation, they\nmay be less applicable to the healthcare domain. First, they require a reference text as\nthe gold-standard for comparison to generate a score, which may not be as relevant in\nthe clinical setting where there is often no \u2018model answer' to a clinical question. Next, their\nfocus on exact word or sequence matching may fail to capture the nuances and contextual\nunderstanding that are essential in the clinical setting.\nFurther insight and evaluation into usefulness and application of LLM evaluation may lie\nin more subjective and qualitative assessment of LLM-generated information.\nFundamentally, such types of evaluation are centered on human alignment and\nspecifically the clinician expert as gold-standard. While evaluation based on linguistic\nfeatures such as fluency and grammar are intrinsic to an LLM, qualitative evaluation to\nuncover deeper insights beyond answer accuracy and specifically tailored to the\nhealthcare domain needs to be established 17. Efforts have been made to outline these\ndomain-specific components of evaluation. For example, Articulate Medical Intelligence\nExplorer (AMIE) is an LLM-based Al system developed by Google for diagnostic medical\nreasoning via text-based consultations, was evaluated on clinical scenarios with\nsimulated patients in an OSCE format. Evaluation metrics were defined as the accuracy\nof the top 3 diagnoses, appropriateness of diagnosis and management (10 components),\nand an emphasis on displaying empathy and addressing concerns via the Practical\nAssessment of Clinical Examination Skills (PACES) (16 components), and relationship\nfostering via the Patient-centered Communication Best Practice (PCCBP) (6\ncomponents) 18. MedPALM generated responses were evaluated on a multi-axis\nframework for human evaluation including alignment with scientific and clinical consensus,\nlikelihood of harm and bias, reading comprehension, recall of relevant clinical knowledge,\nmanipulation of knowledge via valid reasoning, completeness of responses, relevance\nand helpfulness. The increasing emphasis on incorporating evaluation on LLM trust and\nsafety was similarly echoed in another study that highlighted 7 key categories\u2014 including\nreliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence\nto social norms, and robustness. Each category is further divided into several sub-\ncategories, with a total of 29 sub-categories19. Human grading is time-consuming and\nlabor-intensive20. Furthermore, narrative analysis of LLM-generated responses as well as\nthe use of variable evaluation rubrics across studies limit head-to-head comparison\namongst applications. These may further impede the development pipeline of newer LLM\napplications. One example, in line with these objectives, was HELM Instruct proposed by\nthe Stanford University for open-ended, multidimensional, and absolute assessment,\nencompassing 5 criteria Helpfulness, Understandability, Completeness, Conciseness,\nand Harmlessness (on a scale of 1-5), targeted at general non-medical open-ended\nquestions. This demonstrated the feasibility and highlighted the importance for multi-\ndimensional evaluation centered on alignment with domain-experts."}, {"title": "Methods", "content": "Proposed S.C.O.R.E. Evaluation Framework\nTo allow a more subjective and qualitative assessment of LLMs in healthcare beyond\ntraditional quantitative measures and centered on domain-expert alignment, we proposed\nthe S.C.O.R.E. Evaluation Framework that outlines 5 key aspects of evaluation \nFirst, Safety is defined as an LLM-generated response not containing hallucinated or\nmisleading content that may lead to physical and/or psychological adversity to the users.\nSafety includes both accuracy of the LLM tool in offering a diagnosis and recommending\nintervention that may incur injury to the subject. Ensuring safety involves rigorous testing\nand validation to prevent the dissemination of false or harmful information, which is crucial\nin maintaining the integrity and trustworthiness of LLM-based systems in clinical settings.\nSecond, Consensus is defined as a response that is accurate and aligned with the clinical\nevidence and professional consensus according to national and international professional\nbodies. This alignment with established evidence, medical guidelines and professional\nexpert opinion is essential to ensure that LLM-generated recommendations are credible\nand reliable. Third, Objectivity is defined as a response that is objective and unbiased\nagainst any condition, gender, ethnicity, socioeconomic classes and culture. This will help\nin assessing the responses ethically and ensuring that the LLM provides fair and equitable\nresponses promoting inclusivity and preventing discrimination. Next, Reproducibility is\ndefined as a consistent response after repeated response generation to the same\nquestion. The focus is not on the word-for-word repeatability of the responses but rather\nassessing reproducibility in terms of the contextual consistencies between the responses\ngenerated. Finally, Explainability is defined as justification of the LLM-generated response\nincluding the reasoning process and additional supplemental information relevant to the\ncontext including reference citations or website links. All responses are graded on Likert\nScale from 1 (Strongly disagree) to 5 (Strongly agree). Grading should be conducted by\nclinical domain experts who have the necessary knowledge and experience to assess the\ncontent's relevance and adherence to professional standards. The S.C.O.R.E. Evaluation\nFramework serves as a broad framework that can be adapted to various disciplines.\nThese components are universally relevant principles that enhance the quality and\nreliability of outputs from LLM applications across different fields. While the S.C.O.R.E.\nframework provides a solid foundation, it could be further refined to address the unique\nchallenges and standards of each specialty to maximize its applicability and impact.\nQuantitative Metrics against Qualitative S.C.O.R.E Framework\nTo evaluate the effectiveness of the proposed S.C.O.R.E framework, we conducted head-\nto-head comparisons using conventional quantitative evaluation metrics, including BLEU,\nROUGE-1, ROUGE-L, and BERT-SCORE, to assess LLM-generated open-ended\nresponses to healthcare-related questions. We selected commonly asked patient queries\nwith paired answers related to general ophthalmology and medications. These question-\nanswer (Q&A) pairs were manually crafted by domain experts, with the ophthalmology-\nrelated Q&A pairs extracted from a previous study21. We utilized GPT4-omni22 as the LLM\nfor generation of responses, setting the instructional prompt as follows: \"You are a\nmedical chatbot interacting with patients regarding their health inquiries. Please provide\nconcise and clinically accurate responses.\" The hyperparameters were configured with a\ntemperature of 0.2 and a maximum token limit of 256. The temperature was set to 0.2 to\nensure more deterministic and focused responses from the model. A lower temperature\nreduces the randomness in the model's output, leading to more consistent and reliable\nanswers. This is particularly important in clinical and healthcare applications, where\nprecision and consistency are crucial. The 256 token limit helps to prevent overly verbose\nresponses, ensuring that the output remains concise and to the point. The paired answers\nserved as the clinical ground-truth for the quantitative evaluation. Qualitative\nassessments based on the S.C.O.R.E framework were performed by a board-certified\nsenior consultant ophthalmologist DT and a principal pharmacist JO.\nBased on the quantitative evaluation, GPT4-omni responses were deemed suboptimal\nfor both ophthalmology and medication-related queries. The\nevaluation metrics for ophthalmology-related queries yielded poor average scores: BLEU\n0.0238, ROUGE-1 0.2613, ROUGE-L 0.2351, and BERT-SCORE 0.5925. Similarly, for\nmedication-related queries, the scores were BLEU 0.0152, ROUGE-1 0.2484, ROUGE-L\n0.2141, and BERT-SCORE 0.5864.\nOn the other hand, the qualitative assessment using S.C.O.R.E found that the GPT4-\nomni responses were clinically accurate, as depicted in Figure 3. For ophthalmology-\nbased questions, the average Likert scores were 5 for Safety, 4.2 for Consensus, 5 for\nObjectivity, 4.6 for Reproducibility, and 5 for Explainability. Similarly, for medication-based\nquestions, the average Likert scores were 4.8 for Safety, 5 for Consensus, 5 for\nObjectivity, 4.4 for Reproducibility, and 4.4 for Explainability. In one of the medication-\nrelated question on genetic influence of azathioprine, GPT4-omni's response was aligned\nwith the ground truth in identifying the intended question (adverse drug reaction related\nto genetic polymorphism), aligning with evidence-based knowledge (genetic\npolymorphism increases the risk for severe toxicity, guideline-concordant actions) and\nreinforcing the need for genetic tests prior to initiation \nWhile the response\nwould have otherwise been misrepresented as inaccurate based on the quantitative\nmetrics, the components of S.C.O.R.E. allowed these clinically relevant aspects to be\nqualitatively assessed. In one of the ophthalmology-related questions on the symptoms\nof diabetic retinopathy, the GPT4-omni response was notably accurate in highlighting the\nimportance of regular examinations for early detection as symptoms may not be\nnoticeable in early stages. However, the response listed impaired color vision and visual\nfield defects as common symptoms, while not incorrect, these are not typically observed\nin diabetic retinopathy. Using the S.C.O.R.E. framework, this response was graded a 3\nout of 5 for Consensus. Therefore, the qualitative assessment facilitated a more nuanced\nunderstanding of the clinical accuracy and relevance of LLM-generated responses."}, {"title": "Integrating with Existing Efforts in Furthering LLM Evaluation", "content": "The objective of S.C.O.R.E is to outline a multi-dimensional framework to facilitate\nstandardized, qualitative and efficient human evaluation of LLM-generated open-ended\nanswers in domain-specific tasks. S.C.O.R.E can potentially be integrated with existing\nefforts in deepening LLM evaluation. The Safety component in S.C.O.R.E. can potentially\nbe expanded to include resilience against adversarial prompting. This is aligned with\nprevious work demonstrating the \u201cwillingness\u201d of GPT-3.5 to comply to a harmful prompt\nin general and medical domains (Eg. falsifying medical records, violating patient\nconfidentiality, and spreading medical misinformation), which was reduced after model\nfine-tuning with safety demonstrations23. Towards deployment, additional layers of\nassessment such as translational value and governance (Eg. fairness, transparency,\ntrustworthiness, accountability based on the Governance Model for Al in Healthcare\n(GMAIH) 24) have also been emphasized25. S.C.O.R.E. can potentially be used as the\ninitial evaluation of LLM applications, before further evaluation on translational value and\ngovernance. There has also been recent work that explored leveraging LLM-based\nevaluation, in striving for automated and reference-free evaluation 26. GPT-4 based\nevaluation of LLM-generated responses to general ophthalmology-related patient queries\nwere found to be highly congruent with human clinician rankings27. This was similarly\ndemonstrated for evaluating general tasks in G-EVAL using chain-of-thought prompting\nwith GPT-428; and in LLM-EVAL using a single-prompt multi-dimensional automatic\nevaluation of open-domain LLM conversations29. Furthermore, the components of the\nS.C.O.R.E. framework may potentially be embedded into input prompts, to guide LLM\ngeneration of high-quality responses that are attuned to critical benchmarks.\nNevertheless, domain-expert human evaluation cannot be replaced by LLM-based\nevaluation, without further work to validate these preliminary observations. Beyond\nevaluation, future work could also explore LLM enhancement to modify responses based\non feedback from LLM evaluation 30."}, {"title": "Conclusions", "content": "As the capabilities of LLMs continue to expand, effective evaluation beyond traditional\nquantitative metrics such as accuracy is essential to comprehensively critique these\ngenerative Al models and validate them specific to their domain (in this case, healthcare\nand clinical use). By incorporating factors such as safety, consensus, objectivity,\nreproducibility and explainability, S.C.O.R.E can ensure that LLM-based models and\nsystems are not only robust and accurate, but are also safe, reliable, ethical, and\ntrustworthy in their application, particularly for clinical medicine and healthcare."}]}