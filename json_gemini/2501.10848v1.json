{"title": "Fake Advertisements Detection Using Automated Multimodal Learning: A Case Study for Vietnamese Real Estate Data", "authors": ["Duy Nguyen", "Trung T. Nguyen", "Cuong V. Nguyen"], "abstract": "The popularity of e-commerce has given rise to fake advertisements that can expose users to financial and data risks while damaging the reputation of these e-commerce platforms. For these reasons, detecting and removing such fake advertisements are important for the success of e-commerce websites. In this paper, we propose FADAML, a novel end-to-end machine learning system to detect and filter out fake online advertisements. Our system combines techniques in multimodal machine learning and automated machine learning to achieve a high detection rate. As a case study, we apply FADAML to detect fake advertisements on popular Vietnamese real estate websites. Our experiments show that we can achieve 91.5% detection accuracy, which significantly outperforms three different state-of-the-art fake news detection systems.", "sections": [{"title": "1 Introduction", "content": "With the popularity of e-commerce, online advertisements and marketplaces have been used widely by both sellers and buyers. These e-commerce marketplaces are growing rapidly, supporting the sale and purchase of different products such as real estates, household goods, labor-intensive equipments, etc. With the increased access to these marketplaces, there is also an increase in the number of fake advertisements on these platforms that target vulnerable buyers to obtain their personal information or to trick them into buying fake products [1]. These fake advertisements severely affect both the genuine sellers and buyers, and potentially undermine the creditability of the e-commerce websites if left undetected.\nIn this work, we will develop a novel end-to-end machine learning system, called Fake Advertisements Detection using Automated Multimodal Learning (FADAML), to detect and filter out fake online advertisements. Our system combines techniques in multimodal machine learning [2, 3] and automated machine learning (AutoML) [4, 5] to improve the system's detection rate. To our knowledge, this is the first work that combines multimodal and automated machine learning for fake advertisement detection. By utilizing multimodal learning, we can leverage both free-form text-based inputs as well as specialized handcrafted features to improve the performance of our system. Additionally, by incorporating AutoML, we can train several powerful machine learning models and combine them to further enhance the performance.\nIn general, our FADAML system consists of three main components: a data crawler and preprocessor, a multimodal feature extractor, and an AutoML system. The first two components extract and transform raw advertisement texts into refined multimodal features through several processing steps. These refined features are then fed into the AutoML component to train and select the best model for the data, which would be used to filter new advertisements. One distinctive feature of our approach when compared with previous work on multimodal fake news detection [6, 7, 8, 9] is that we use AutoML to train our model. This simplifies the implementation of our method and makes it highly flexible and applicable. Furthermore, in our work, instead of using generic multimodal information, we work with domain experts to select highly usable multimodal features for our model.\nAs a case study, we apply our system to detect fake advertisements on Vietnamese real estate websites. The residential real estate market in Vietnam is valued about $7.3B in 2022 with the majority of real estate agents relying on e-commerce websites to increase transactions [10]. Furthermore, Vietnamese is a low-resource language that is usually very challenging to work with due to (1) the relatively smaller corpus, (2) the difference in word structures compared to English, and (3) the difference between the corpus used to pre-train language models (i.e., Vietnamese Wikipedia's articles) and that of the downstream tasks (e.g., real estate advertisements) [11]. In this paper, we show that our system can work well for the Vietnamese language.\nWe implement and test our FADAML system thoroughly using a real-world data set collected from five popular Vietnamese real estate websites. Our experiments show that FADAML can achieve 91.5% accuracy when detecting fake advertisements, significantly outperforming three state-of-the-art fake news detection baseline methods. To gain a better understanding into our system, we inspect the individual models trained by AutoML and also investigate the feature importance scores of our final model. Finally, we conduct an ablation study to explore the effects of different multimodal feature sets on the performance of our system.\nIn summary, our paper makes the following contributions. (1) We develop FADAML, a novel end-to-end machine learning system to detect and filter out fake online advertisements. Our system combines techniques in multimodal machine learning and automated machine learning to perform this task. (2) We show how our system can be applied to detect fake advertisements on popular Vietnamese real estate websites. We also empirically show that the system can achieve good detection accuracy for this problem."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Fake Advertisement Detection", "content": "Online fake advertisement detection [12, 13, 14, 15] is an important research topic whose solutions can potentially help protect e-commerce websites' users from potential scams. Previous methods to detect such fake advertisements range from using simple classifiers [12] and data mining [13] to heuristic statistics [15] and combinations of available online services [14]. These previous works, however, focused on the English language and did not use the state-of-the-art advancements in machine learning, such as deep learning, multimodal machine learning, or automated machine learning. In this work, we approach this problem with a solution that combines multimodal and automated machine learning, the two useful frameworks for this type of data. Additionally, our work focuses on advertisements in the Vietnamese language, which received no prior attention.\nThe fake advertisement detection problem here is also related to fake news detection [16] and misinformation detection [17] on online social networks. An approach for this problem was proposed in [18] that uses contextualized word embeddings from a pre-trained ELMo model [19] to input into a Bi-LSTM with an attention layer and a softmax classifier for final predictions. Another solution was proposed in [20] that introduced FNDNet, which leverages parallel convolutional layers with varying kernel sizes and word embeddings to extract semantic relationships. Both of these methods can achieve remarkable performance on fake news detection; thus, we use them as baselines in this paper to compare with our system.\nFake news detection in non-English languages was also considered in some previous work. In [21], three language groups (i.e., Germanic, Latin, and Slavic) were considered and a text-feature method was suggested for identifying fake news. Another work [22] created a corpus of both fake and real news in Spanish and detected fake news by using linguistically motivated features. For German, [23] developed FANG-COVID, a large-scale benchmark dataset for fake news detection related to the COVID-19 pandemic. Using an explainable textual and social context-based model, they proposed a method that can achieve comparable results to the black box model solely relying on BERT-embeddings. For the Portuguese language, [24] investigated the linguistic characteristics of fake news and applied machine learning techniques to achieve significant results. However, unlike our approach, none of these work employs AutoML in their solutions."}, {"title": "2.2 Vietnamese Natural Language Processing", "content": "Our work is related to natural language processing for the Vietnamese language. In recent years, there have been several works investigating different tasks for this language, such as part-of-speech tagging [25, 26], punctuation prediction [27, 28], named entity recognition [29, 30, 31, 32], and dependency parsing [25, 33]. For Vietnamese, a robust pre-trained transformer-based language model, called PhoBERT [11], has also been developed and applied to the part-of-speech tagging, named entity recognition, and dependency parsing problems above. In this paper, we will use PhoBERT to obtain the embeddings for the named entity recognition module of our system."}, {"title": "2.3 Real Estate Data Analysis", "content": "Real estate data analysis has gained much attention recently with the availability of data and analysis tools [34, 35, 36]. Among these analyses, property price estimation is an important problem and several approaches have been proposed for this problem that include vision-based approach [37], eager learning [38], graph convolutional neural network [39], etc. In terms of Vietnamese real estate data, there have been previous works that develop an end-to-end information extraction platform [40] and deep learning models for named entity recognition [41]. However, the previous work has not considered the fake advertisement detection problem that we consider in this paper. Furthermore, to the best of our knowledge, our paper is also the first to adopt automated and multimodal machine learning to analyze real estate data."}, {"title": "2.4 Automated Machine Learning", "content": "Our paper utilizes AutoML as a major component in our proposed system. AutoML [4] is a field that aims to automatically determine the best machine learning approach given a dataset for an application, and then train a model with the best hyperparameter setting using this approach. Researchers and practitioners of AutoML have developed several effective and useful frameworks, including Auto-WEKA [42], Auto-Sklearn [43], \u0422\u0420\u041e\u0422 [44], H2O [45], AutoGluon [46], etc. These frameworks are enhancing the practical applicability of AutoML on data-driven model building and automated decision making. Among them, AutoGluon can often achieve a remarkable accuracy on raw data while making a better use of the allotted training time through assembling several models and stacking them in layers [47]. For this reason, we use AutoGluon to implement the AutoML component in our system."}, {"title": "2.5 Multimodal Machine Learning", "content": "One of the main strengths of our proposed system is to combine texts in the advertisements with hand-crafted tabular features into a single machine learning system. Such multimodal machine learning systems [2] have been gaining popularity in recent years due to the availability of different data types such as texts, tabular data, images, sounds, etc., for each application. Previous works also combined multimodal machine learning and AutoML [3, 47] for text and tabular features. In [47], several AutoML systems were compared on 18 different multimodal datasets that contain both texts and hand-crafted features. In [3], AutoML was used for credit modeling on financial data that combines raw texts from Securities and Exchange Commission filings with features extracted by financial experts. In these works, multimodal AutoML can achieve high accuracies while minimizing the implementation efforts. In this paper, we show that multimodal AutoML will also be effective for real estate data in a low-resource language (i.e., Vietnamese)."}, {"title": "3 Fake Classified Advertisements Detection", "content": "We consider the problem of detecting fake advertisements among a collection of online classified advertisements, each of which is given as a free-form (unstructured) text. As illustrations, Table 1 shows two classified advertisements in their original form, where the first is a real advertisement and the second is a fake one. The original form of these advertisements is a free-form and unstructured text that is collected from an HTML <textarea> element with all the HTML tags removed. These texts usually contain a wide range of characters (alphabet letters, numbers, special symbols, etc.) and may include several paragraphs.\nDetecting fake advertisements from these free-form texts is a challenging problem, especially in the context of Vietnamese classified real estate ads. The followings are some of the main reasons that make this problem difficult:\n1. The texts do not have a structure. The ads do not have any pre-defined structure. Their contents depend totally on the users who post the ads. For instance, the sentences in the ads often do not follow proper grammatical rules, making it difficult to infer their semantics. In other cases, a piece of information can be provided in different ways (e.g., an ad can provide the length and width of a real estate instead of its total area), and we need a method to unify these information. These problems are true for both real and fake ads.\nTo illustrate these issues, we provide two examples of fake advertisements. One is the second advertisement in Table 1 and the other is the example in Table 3. We can easily notice from these advertisements that they have no general structures. For instance, the real estate agent can choose to describe the advantages of buying the property instead of describing the nearby facilities. They may also use different ways to provide the number of floors (bungalow, two floors, etc.).\n2. The texts contain a lot of abbreviations and redundant information. Both true and fake ads tend to use abbreviations to shorten street names or some properties of the real estates (e.g., the ads in Table 1 use the abbreviations \"DT\u201d and \u201cDTKV\u201d for \"area\"). There are no universal rules for the abbreviations. Furthermore, many ads also contain redundant information (such as information about nearby real estates or more than two properties in one listing). These types of ambiguity pose a challenge for automated learning systems, which need to disambiguate these information based on the context. These issues are illustrated in the first example of Table 2 where both abbreviations and redundancy are present.\n3. The texts lack important information. The ads may not provide all the important information that the buyers need. This problem occurs not only in the fake ads but also in the real ads. In these cases, the owners or real estate agents can exclude some crucial information about the properties to attract prospective buyers. This practice could help increase the viewing rates for the properties and for the owners to better negotiate with prospective buyers. A sample fake advertisement that lacks the necessary information is given in the second example of Table 2.\n4. Sparse data. One of the hardest problems that we have to deal with is sparse data. In most cases, there are very few advertisements in a specific location. For example, on many streets, there could be less than five advertisements. This issue forces the models to make predictions on properties that are very different from those in the training data."}, {"title": "4 FADAML: Fake Advertisements Detection using Automated Multimodal Learning", "content": "In this paper, we propose a novel end-to-end system to tackle the fake classified advertisements detection problem above. Our approach, called Fake Advertisements Detection using Automated Multimodal Learning (FADAML), leverages recent advances in AutoML and multimodal machine learning to process the data and train machine learning models that can effectively detect fake advertisements from raw and unstructured texts. By combining automated feature extraction in an AutoML system and carefully hand-crafted multimodal features, our FADAML approach can overcome the difficulties mentioned above and achieve a good performance for this problem.\nThe components of our system are visualized in Figure 1. The system consists of three main components: (1) a data crawler and preprocessing component, (2) a data processing and multimodal feature extraction component, and (3) an AutoML system. Below we describe these components in detail."}, {"title": "4.1 Component 1: Data Crawler and Preprocessing", "content": "In this first component, we develop a data crawler to collect data from a pre-defined list of domains. For each domain, our crawler automatically browses the webpages containing the real estate listings, downloads the corresponding HTML page, and extracts the real estate description from the page using its HTML structure. After collecting the descriptions of real estate posts from these webpages, we perform a light preprocessing step on these descriptions that includes: cleaning HTML elements such as \\n, \\t, <\\br>, etc. using regular expressions, removing redundant white spaces, and converting the texts to lowercase.\nIn the context of Vietnamese classified real estate ads, we crawl raw data from the five most popular real estate advertisement domains: batdongsan.com.vn, chotot.com, diaoconline.vn, 123nhadat.vn, and muaban.net. These raw advertisements are then processed by our preprocessing modules described above. As an illustration, we show in Table 3 an example of a raw advertisement returned by the crawler and the corresponding preprocessed advertisement. This preprocessed advertisement will be the input to the second component of our FADAML system, which we will describe next."}, {"title": "4.2 Component 2: Multimodal Feature Extraction and Extensive Data Processing", "content": "The second component of our FADAML system takes the processed advertisements from the first component as inputs and applies the following three processing steps: (1) multimodal feature extraction, (2) data cleaning, and (3) feature refinement. An illustration of this component is presented in Figure 2. We detail each of the processing steps below."}, {"title": "4.2.1 Multimodal Feature Extraction", "content": "The main purpose of this step is to parse and extract a set of tabular features from the previously preprocessed advertisements that will be combined with the advertisement text itself to form a set of multimodal features. The advertisement text will later be automatically processed by the AutoML component in Section 4.3. To extract the tabular features, we use a named entity recognizer pre-trained on a different dataset to extract a set of named entities useful for our problem. In particular, we apply the named entity recognizer proposed by Huynh et al. [41] that has been trained separately on another real estate dataset collected and annotated with Doccano [48]. This recognizer uses the MishWindowEncoder model that takes an embedding vector from PhoBERT [11] and returns the corresponding feature vector. This feature vector is then fed to a classifier to be classified into the target named entities. Using this recognizer, we can extract four most useful entities for our problem: price, area, road, and district (see Table 4 for the descriptions of these features).\nAfter extracting the named entities and the tabular features, we then perform a data pre-filtering step to remove all advertisements not related to real estates or those that do not contain sufficient information for a transaction (e.g., those without information on price, location, or property area). This step is performed by another classifier trained separately on a dataset customized for this task. This classifier will determine whether a listing is about some real estate transaction and does not contain inappropriate or offensive content."}, {"title": "4.2.2 Data Cleaning", "content": "After constructing the multimodal features from the previous step, we perform extensive data cleaning in this step to ensure all data records are clean and normalized. From our preliminary inspection, we notice there are three main issues with the data, namely noisy records, outliers, and duplicated posts. Thus, we perform the following three data cleaning tasks to address these issues.\nDe-noising. In the context of our paper, we define a data record as noisy if the advertisement does not provide reliable or crucial information about a property, or if the advertisement is too complicated to extract useful features. These noisy data records often contain irrelevant information that is not useful for the intended real estate advertisement. For instance, a listing may contain more than two different locations or addresses for one property or there is a discrepancy between the street and the district of the property. To detect these noisy records, we analyze the extracted features and employ a set of heuristic rules based on our domain knowledge (e.g., missing features) to help us detect and drop these records.\nOutlier removal. In addition to noisy data records, we also detect and remove outliers, which are defined as records containing abnormal feature values. For instance, on a website's section for selling properties, renting advertisements are considered outliers since their extracted price is much lower than the usual selling price. Another example is the incorrectly extracted features from a very complex raw description text. In this paper, we detect the outliers using pre-defined ranges for each feature values in consultation with real estate experts. Depending on the type of the errors, we will decide to either drop or fix the data records.\nDe-duplication. Since our data are collected from multiple listing websites, the records can be duplicated due to the cross-posting of real estate agencies to increase the advertisements' visibility. In most cases, these duplicated postings contain the same information but are written using different formats and structures, or even with some added redundancies. In this task, we implement a heuristic de-duplication process to filter out these duplicated records. First, we de-duplicate by dropping records with similar description texts. Then we compare the important features and de-duplicate the records containing similar information. This process helps us significantly reduce the number of duplicated records in practice."}, {"title": "4.2.3 Feature Refinement", "content": "This step is to further refine the features before training. In this step, we perform feature selection and feature enrichment on the multimodal data obtained from the previous step. Since there are several features that do not have any impact on the value of a real estate, the feature selection step chooses a subset of the named entities obtained from the previous feature extraction step to be our features. In particular, from the experts' opinions, we know that fake real estate advertisements can often be detected by inspecting the discrepancy between the content of the ad and the posted property price. Thus, we first sort the features using their correlation scores with the property's price and then select a subset of the highly scored features that overlap with the experts' frequently used features. During this feature selection step, we also maintain the consistency between all records such that they would have the same value type and unit on the same features (e.g., all enumerated district names are converted to strings).\nWorking with real estate experts, we further perform feature enrichment to enhance the prediction performance. The feature enrichment process allows us to extract new features that cannot be recognized by the named entity recognizer in the previous step. In Table 4, we give details of all features used in our system, including those extracted from the previous feature extraction step as well as those from the feature enrichment step.\nAmong the features, we use both the real estate description texts obtained from Component 1 and the features extracted in Component 2. The price, area, road, and district features are obtained from the feature extraction step and are standardized to ensure all data samples have the same data type for these features. The house_type and road_width features are generated from the feature enrichment process, where we use regular expressions to capture them. If the real estate is located on a main road, we set $road\\_width$ to be a large constant number (we use 20 in this case). If the real estate is an alley house and there is no information about the road width in the description, we set $road\\_width$ to a fixed constant based on the location of the house. The features added during the above process and their values are obtained by consulting a team of two real estate experts, who would need to discuss and completely agree on the final suggestions.\nTo obtain the road_first, road_second, and road_third features, we use the GeoPy package\u00b9 to get the longitude and latitude of each road in the district where the real estate is located. With these longitudes and latitudes, we calculate the relative distance between each pair of roads by using Manhattan distance [49]. We sort the relative distances to get the top three nearest roads and use them as features. These features allow us to take into account the spatial information of the roads, where nearby roads within the same district would likely have similar prices per unit area. Another advantage of using these nearest road features is their ability to deal with missing data. In many practical situations, there could be too few or even no advertisements at a specific location in the same time, thus the machine learning models may need to deal with properties on"}, {"title": "4.3 Component 3: Automated Machine Learning System", "content": "After executing the first two components of our FADAML system, we obtain the full multimodal dataset with 10 features in Table 4. We show in Table 5 a final data sample that will be input to our third component, the AutoML system, which will automatically perform all standard feature processing techniques that are known to improve the predictive performance of the trained model.\nTo obtain the ground truth labels, we employ an approach involving both human and machine. In consultation with real estate experts, we know that fake Vietnamese real estate ads often have a large discrepancy between the posted price and the evaluated price based on the ad's content. Thus, we obtain the ground truths by inspecting the posted price of each property and its predicted price based on past transactions of similar properties. When there is a large discrepancy between the prices, two experts will be asked to independently assess the ad's content and give their evaluated prices. If the prices from the two experts are not within 10% of each other, the ad is considered fake. After assigning the labels, the experts will manually check and correct a random subset of labels. This process is repeated until no correction is made.\nAfter receiving the labeled dataset, our AutoML system will first perform an encoding step to convert categorical features into numeric and generate several basic natural language features from the text descriptions (i.e., from the first feature in Table 4). The natural language features include n-gram counts, character counts, word counts, special symbol counts, special symbol ratios, and digit ratios. The categorical features are converted into numeric using a label encoder, which uses monotonically increasing integer values to reduce memory usage.\nFollowing previous works that utilized AutoML [3, 46], we also use model stacking to achieve the best performance. Particularly, we use an ensemble of several base models stacked into two layers automatically using the validation set. The list of base models trained by our system is described in Table 6. To obtain the 2-layer stack ensemble, we first train all the base models with the training set that would constitute the first layer of the stack. To obtain the second layer, we concatenate the predictions of the first layers with the original features to form a new input vector, and then apply ensemble selection [50] to construct the second layer. This ensemble selection step will iteratively add to the ensemble the model that most improves the performance on a validation set.\nBy considering the accuracy of each model, we can also aggregate the previous layer outputs into a weighted ensemble. To make a prediction, the ensemble will calculate the weighted average of the predictions from its base models. We compare the predictions of the ensemble and the base models on the validation dataset to get the best configuration. After multiple trials, we observe that the weighted ensemble yields the best accuracy in most cases."}, {"title": "5 Experiments", "content": "In this section, we empirically evaluate the performance of our FADAML system on real-world Vietnamese real estate data. We will first describe our experiment settings in Section 5.1 and then our results in Section 5.2. We also give an ablation study for our system in Section 5.3 and discuss some limitations of our work in Section 5.4. Our experiments are conducted on a computer with an AMD Ryzen 7 (3.20GHz) processor and 16GB of RAM."}, {"title": "5.1 Experiment Settings", "content": ""}, {"title": "5.1.1 Settings", "content": "We use the process described in Section 4.1 to collect and preprocess the data for our experiment. Then we use the second component of our system in Section 4.2 to extract the multimodal features as well as refining the data. After these steps, we obtain a dataset of 29,085 examples, which we randomly split into a training and a testing set. The statistics of these datasets are in Table 7."}, {"title": "5.1.2 Baselines", "content": "In our experiments, we compare FADAML with the following three state-of-the-art systems for text classification and fake news detection. These baseline systems are chosen due to their effectiveness on problems similar to ours.\n\u2022 FNDNet [20]. This is a deep learning model that consists of three parallel convolutional layers operated on the word embedding vectors of the texts. To adapt this model to the Vietnamese language and our dataset, we use phoBERT [11] to generate the embedding vectors and manually tune each layer's kernel size to achieve the best possible performance.\n\u2022 Bi-LSTM+Attention [18]. This system uses ELMo [19] to extract contextualised word embeddings from the texts, which is then fed to a Bidirectional Long Short Term Memory (Bi-LSTM) model with an attention layer to obtain the predictions. To adapt the method to our dataset, we use the ELMo embeddings for Vietnamese [63] while keeping the original architecture of the model.\n\u2022 FastText+CNN [64]. This is a state-of-the-art system that combines FastText word embeddings with convolutional neural network architecture to perform English text classification effectively. To use this system for our dataset, we utilize the Vietnamese FastText word embedding model with the original CNN model architecture."}, {"title": "5.1.3 Evaluation Metrics", "content": "To get a comprehensive view of the performance of the above systems, we evaluate and compare these systems using various metrics below. Note that in the following definitions, $T_p$, $T_n$, $F_p$, $F_n$ are the number of true positives, true negatives, false positives, and false negatives of a detection system, respectively.\n\u2022 Precision. This metric is defined as $T_p/(T_p + F_p)$, which measures the fraction of truly fake advertisements among those detected by a system.\n\u2022 Recall. This metric is defined as $T_p/(T_p + F_n)$, which measures the fraction of correctly detected fake advertisements among all the fake ads.\n\u2022 F1. This metric is defined as $2PR/(P+R)$, where P is the precision and R is the recall of a system. The F1 metric measures the effectiveness of a detection system when precision and recall are given equal importance.\n\u2022 Accuracy. This metric is defined as $(T_p + T_n)/(T_p + T_n + F_p + F_n)$, which measures the effectiveness of a detection system when the dataset is balanced. Since our dataset is only slightly imbalanced, comparing both the accuracy and F1 metrics gives us a good indicator of the effectiveness of the detection systems.\n\u2022 False positive rate (FPR). This metric is defined as $F_p/(F_p + T_n)$, which measures the false alarm rate where a legitimate ad is flagged as fake.\n\u2022 False negative rate (FNR). This metric is defined as $F_n/(F_n + T_p)$, which measures the rate that a system fails to detect fake advertisements."}, {"title": "5.2 Results", "content": "In Table 9, we show the result comparing our FADAML system with the FNDNet, Bi-LSTM+Attention, and FastText+CNN baselines. From the table, we can see that FADAML outperforms the other systems significantly in all metrics. Specifically, FADAML achieves 91.3% F1 score and 91.5% accuracy while the second best system (FNDNet) only achieves 72.4% F1 score and 73.3% accuracy. Surprisingly, FastText+CNN, which is the most recent system [64], performs poorly on our dataset with 100% false positive rate and 0% false negative rate. This could be due to the more difficult vocabulary (e.g., many abbreviations in Vietnamese, real estate terminologies, listing structure, etc.) used in our dataset, making the system unable to learn meaningful patterns from the data.\nThe large difference in performance between our system and the baselines could be due to the addition of multimodal, refined, and spatial features in the FADAML system, as shown in our ablation study later. Another reason for the accuracy difference is that, when compared to the"}, {"title": "5.3 Ablation Study", "content": "To help us understand the contributions of each component in FADAML to the whole system, we conduct an ablation study where we successively remove the following feature sets from the full system and measure its performance.\n\u2022 Spatial multimodal features: This feature set consists of the road_first, road_second, and road_third features obtained from the actual location of each property, as described in Section 4.2.3.\n\u2022 Refined multimodal features: This feature set consists of the other enriched and refined multimodal features in Section 4.2.3 (house_type and road_width).\n\u2022 Basic multimodal features: This feature set consists of the basic multimodal features extracted from Section 4.2.1 (price, area, road, and district).\nAfter removing all the above multimodal features, we are left with the base model that uses only the description text generated from Component 1 of our FADAML system (Section 4.1). This description text will be automatically processed by AutoGluon to generate several generic natural language features discussed in Section 4.3 that will be used to train the base models.\nIn Table 11, we give the ablation study results for our system. The table shows that removing the spatial features slightly reduces the accuracy and F1 score of the system (from 91.5% to 91.1% for accuracy and from 91.3% to 90.9% for F1). When further removing the other refined multimodal features, the performance drops by around 3% for all metrics. Finally, removing all the basic multimodal features degrades the performance of the system significantly, with at least 9.5% reduction in all metrics. This result confirms the advantages of our FADAML system in extracting useful multimodal features for this problem. It is also worth noting that our base model without any multimodal features is still better than the baselines in Table 9. This observation suggests that using AutoML is better for our problem than using pre-trained word embedding and deep learning models."}, {"title": "5.4 Discussions", "content": "Despite the encouraging experiment results of our system, we note that there are still some limitations that need to be addressed to enhance its applicability in practice. First, the accuracy of 91.5% (and consequently, the 9.8% false positive rate and 7.4% false negative rate), although better than the baselines, is still not sufficient for the deployment of the system to production. This is due to the difficulty of the problem relatively to the simplicity of our multimodal feature set. Thus, it is essential to extract more useful features to improve the performance of our system.\nAnother limitation of our study is the relatively moderate size of our dataset (with 29,085 examples) that could make it harder for deep learning approaches to work well. An increase in both size and geographical coverage of our dataset may help improve the accuracy of the neural network base models in our ensemble, thereby potentially improving the performance of the whole system. Additionally, noises in our dataset also pose a challenge for our system. Thus, it could also be useful to apply noise reduction techniques to improve the robustness of the system.\nFinally, online fake advertisements can rapidly evolve, potentially outpacing the system's detection capabilities. Thus, we need to continuously retrain and update our system to keep up with the evolution of the problem. One potential solution to this problem is applying online learning [65, 29, 66] and continual/life-long learning [39, 67] to reduce the system maintenance cost."}, {"title": "6 Conclusion", "content": "We developed an end-to-end fake advertisement detector that combines multimodal and automated machine learning into a single system. Our method is capable of detecting fake advertisements on Vietnamese real estate websites with a high detection accuracy. The techniques in our system are very general and can be used in other applications.\nFor future work, we aim to further improve our system by extracting more useful features from other data sources besides listing websites and adding more models into our AutoML component. For instance, we can incorporate information from images of the properties that can help us compare different properties visually. With the advances in computer vision, we can extract features from these images easily by using a pre-trained neural network and then add these features into our multimodal system.\nAnother potential future work is to extend our approach to other languages or other domains such as medical data, bio-informatics data, or other e-commerce domains. As our AutoML component is very general and flexible, it can be readily applied to these domains. However, a challenge when applying our system to other domains is the extraction of useful multimodal features for these domains. To solve this challenge, we may need to work with domain experts to understand the problems and determine which information would be important for each domain."}]}