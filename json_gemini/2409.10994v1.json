{"title": "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs", "authors": ["Dingjie Song", "Wenjun Wang", "Shunian Chen", "Xidong Wang", "Michael Guan", "Benyou Wang"], "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to remarkable performances across various domains. However, this progress is accompanied by a substantial surge in the resource consumption of these models. We address this pressing issue by introducing a new approach, Token Reduction using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without sacrificing their performance. Inspired by human attention patterns in Visual Question Answering (VQA) tasks, TRIM presents a fresh perspective on the selection and reduction of image tokens. The TRIM method has been extensively tested across 12 datasets, and the results demonstrate a significant reduction in computational overhead while maintaining a consistent level of performance. This research marks a critical stride in efficient MLLM development, promoting greater accessibility and sustainability of high-performing models.", "sections": [{"title": "1 Introduction", "content": "The rapid development of MLLMs has demonstrated superior, and sometimes even superhuman, performance across various fields (OpenAI, 2023; Team et al., 2023; Liu et al., 2023a; Awadalla et al., 2023). However, this progress comes with a significant increase in the resources consumed by these models. As a result, the research community has begun to place a greater emphasis on developing efficient MLLMs (Jin et al., 2024; Xu et al., 2024a).\nCurrent efforts include developing lighter architectures to reduce parameters and computational complexity (Lin et al., 2024; Zhao et al., 2024; Chen et al., 2024), creating specialized components to optimize efficiency and add properties like locality (Chu et al., 2024; Cha et al., 2024), and, notably, enhancing support for resource-intensive tasks through techniques such as visual token compression. Visual token compression reduces the number of tokens needed to represent visual data, thereby lowering computational and memory demands without sacrificing performance. This approach is particularly crucial as it enables the efficient processing of high-resolution images and videos (Xu et al., 2024b; Gao et al., 2024).\nBefore the MLLM era, numerous efforts aimed to reduce the number of tokens. For instance, methods like MADTP (Cao et al., 2024) were proposed, but they did not integrate closely enough with Large Language Models (LLMs). In the context of MLLMs, the only notable work is PruMerge (Shang et al., 2024), which uses self-attention in vision encoder to make judgments; however, it remains a sub-optimal method for deciding which tokens to reduce.\nDrawing inspiration from human attention patterns in VQA tasks, our proposed method employs the use of CLIP (Radford et al., 2021) representations to calculate the similarity between text and image patches. Through our observations, we found that this similarity metric effectively identifies semantically relevant regions within images."}, {"title": "2 Related Work", "content": "Many works focus on better projecting visual information into text embedding space. Early work (Alayrac et al., 2022) uses a perceiver resampler to integrate visual data into the language model's hidden layers. Some works (Li et al., 2023b; Zhu et al., 2023; Bai et al., 2023; Li et al., 2024; Jian et al., 2024) compress visual tokens to a fixed length and map them to text space using linear layers. More recent methods using the LLaVA architecture (Liu et al., 2024; AI et al., 2024; Wang et al., 2024; Zhu et al., 2024; Chen et al., 2024) simplify this by using MLP layers to map visual tokens to text space, reducing training parameters and data requirements, thus gaining popularity for their efficiency and simplicity.\nHowever, LLaVA encounters increased computational load in multi-image scenarios due to the high number of visual tokens encoded by standard CLIP (Radford et al., 2021; Vaswani et al., 2023). Compressing these tokens while retaining visual information is crucial. Though traditional CV tasks have used token merging and pruning effectively (Rao et al., 2021; Meng et al., 2021), this approach is underexplored in MLLMs. LLaVA-PruMerge (Shang et al., 2024) recently attempted token reduction using [CLS] token-based similarity with sub-optimal results. Our study introduces a token reduction method based on the similarity of text and visual tokens, achieving comparable performance with significantly fewer visual tokens."}, {"title": "3 Method", "content": null}, {"title": "3.1 Observations", "content": "One of the challenges in token reduction is determining the importance of different tokens. When humans perform VQA tasks, they focus on specific parts of an image based on the task description, rather than giving equal attention to every part of the image. To simulate this attention mechanism, we need to establish a connection between the text and image patches. We observed that the CLIP model, during its training process, implicitly establishes such connections. CLIP uses contrastive learning loss to bring matching text-image pairs closer and push non-matching pairs apart. By leveraging these representations, we can compute and analyze the similarity between text representations and image patch representations. As depicted in Figure 2, we found that by using text represen-"}, {"title": "3.2 Token Reduction with TRIM", "content": "Building upon the observations stated earlier, we put forth a novel token reduction method coined as TRIM (Token Reduction using CLIP Metric), which primarily consists of three steps.\nAssessing Token Significance. First, we harness the similarity metric from CLIP to assess the significance of image tokens. Given the feature representations extracted from the text encoder $f_{text}$ and the image encoder $f_{image}$, we proceed to calculate the cosine similarity between each image token $v_i$ and the pooled text representation $u_{pooled}$ as follows:\n$S(v_i, u_{pooled}) = \\frac{v_i \\cdot u_{pooled}}{||v_i|| ||u_{pooled}||}$\nSubsequently, we apply a softmax function to the calculated similarities, yielding:\n$S_{softmax}(v_i, u_{pooled}) = \\frac{e^{S(v_i, u_{pooled})}}{\\sum_j e^{S(v_j, u_{pooled})}}$\nThis softmax score, $S_{softmax}(v_i, u_{pooled})$, effectively quantifies the significance of each image token $v_i$, thereby forming the underlying basis for the token reduction in our method.\nSelecting Important Tokens. In order to determine the optimal number of image tokens to retain, we adopt the Interquartile Range (IQR) method, as suggested by Shang et al. (2024). The IQR, calculated as the difference between the upper (third) $Q3$ and lower (first) $Q1$ quartiles of the similarity scores, is utilized as an indicator of statistical variance. We then establish a stringent similarity threshold by selecting image tokens with similarity scores that exceed the upper bound defined as $Q3 + 1.5 \\times IQR$. This approach ensures that only the most significant image tokens $Z_v^*$, as determined by their high similarity scores, are retained.\nAggregating Unselected Tokens. Moreover, in an effort to retain the information inherent within the unselected image tokens, we calculate an average of their representations and denote it as $Z_{u,p}$. This aggregated token is then appended to the selected tokens, a strategy that efficiently mitigates any potential loss of image information consequent to token reduction. Finally, we obtain the reduced image token sequence $Z_v'$."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experiment Setup", "content": "Our experimental setup is consistent with that of LLaVA 1.5, with the key difference being that we employ our TRIM method exclusively during the instruction tuning phase. This approach ensures a fair comparison between our proposed method and the established baseline. Furthermore, we perform evaluations across 12 different datasets and compare our results with those of 5 SOTA MLLMs and one related work on token reduction. The detailed training and evaluation settings are presented in Appendix A and Appendix B."}, {"title": "4.2 Main Results", "content": "As shown in Table 1, after conducting experiments on 12 datasets, we found that despite reducing the image token count to 21%, our method still holds a performance level comparable to LLaVA-1.5. Moreover, it significantly outperforms previous work such as BLIP2 (Li et al., 2023b), Instruct-BLIP (Dai et al., 2023), IDEFICS-9B (IDEFICS,"}, {"title": "5 Analysis", "content": null}, {"title": "5.1 Efficiency Analysis", "content": "We assessed the computational efficiency using LLMViewer analysis (Yuan et al., 2024). In a typical scenario, a 336 \u00d7 336 pixel image processed by the CLIP model yields 576 visual tokens, alongside a 40-token text prompt. After statistical analysis, PruMerge achieves a 25% compression rate, reducing the visual tokens to 144. In comparison, our method achieves a 21% compression rate, reducing the tokens to 123. Our approach significantly accelerates model inference speed and reduces memory usage, as detailed in Table 2. Notably, the time required to generate the first token is curtailed to 32.9% of the original, resulting in a significant ac-"}, {"title": "5.2 Ablation Study", "content": "We conducted an ablation study on the strategies proposed in our TRIM method, as shown in Table 3. Initially, we analyzed the automated image token selection process based on the CLIP Metric. We compared this process to a simple linear interpolation pooling, and found that our strategy can effectively capture key information in the image, as opposed to uniform sampling (compare the second and third rows). The usage of an additional Aggregated token to preserve sufficient image information also results in performance gains (compare the third and fourth rows). Training on the basis of the TRIM strategies can further enhance results (compare the fourth and fifth rows)."}, {"title": "6 Conclusion", "content": "Our research introduced TRIM, an innovative method for reducing the image tokens in MLLMs, while maintaining performance. TRIM outperformed other methods, even with fewer tokens. Our study marks a significant step to resource-efficient MLLMs and will extend to more diverse architectures, further enhancing efficiency in the field."}, {"title": "Limitations", "content": "Currently, our work is primarily limited to the widely used LLaVA architecture. In the future, we aim to seamlessly integrate our method into a variety of models beyond the LLaVA architecture and into different visual encoders."}]}