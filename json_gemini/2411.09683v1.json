{"title": "Towards a Classification of Open-Source ML Models and Datasets for Software Engineering", "authors": ["Alexandra Gonz\u00e1lez", "Xavier Franch", "David Lo", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "abstract": "Background: Open-Source Pre-Trained Models (PTMs) and datasets provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs. Aims: We apply an SE-oriented classification to PTMs and datasets on a popular open-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs over time. Method: We conducted a repository mining study. We started with a systematically gathered database of PTMs and datasets from the HF API. Our selection was refined by analyzing model and dataset cards and metadata, such as tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are replicable, with a publicly accessible replication package. Results: The most common SE task among PTMs and datasets is code generation, with a primary focus on software development and limited attention to software management. Popular PTMs and datasets mainly target software development. Among ML tasks, text generation is the most common in SE PTMs and datasets. There has been a marked increase in PTMs for SE since 2023 Q2. Conclusions: This study underscores the need for broader task coverage to enhance the integration of ML within SE practices.", "sections": [{"title": "I. INTRODUCTION", "content": "The fast expansion of open-source platforms like Hugging Face (HF) [1] has enhanced access to Machine Learning (ML) models and datasets, driving advancements across various domains. With a consistent and significant uptrend in development activities on HF [2], it is distinguished by its vast collection of Pre-Trained Models (PTMs), compared to other platforms [3] [4]. However, the categorization of these resources overlooks the specific needs of Software Engineering (SE). SE tasks frequently involve code generation, code analysis, and bug detection, which differ significantly from the tasks commonly addressed by general-purpose ML models such as object detection or image segmentation. Therefore, the motivation for this work is to address this gap, as the absence of SE-specific categorization limits the efficient application of ML in SE tasks, potentially slowing down SE innovation. By providing a framework that aligns ML tasks with SE needs, this research aims to make the selection of PTMs and datasets more relevant and effective for SE practitioners and researchers, thus addressing a critical need within the field [5].\nThe main contributions of this work are: (a) proposing and proving the feasibility of a preliminary classification framework for PTMs and datasets hosted on HF, tailored to SE needs; (b) providing advanced analysis, including the exploration of the relationship between SE activities and ML tasks, as well as the evolution of SE PTMs over time; (c) presenting a reproducible pipeline that accesses the HF API, filters, refines, and classifies resources on specific SE tasks.\nData availability statement: All research components, including the original and preprocessed data, along with all scripts for data collection, preparation, and analysis, are publicly available on Zenodo [6]. This ensures transparency and enables independent replication of the study, which is essential for updating the classification as new open-source PTMs and datasets are constantly being released."}, {"title": "II. RELATED WORK", "content": "A systematic literature review conducted by Hou et al. [7] analyzed 395 research papers from January 2017 to January 2024 and categorized Large Language Models (LLMs) into SE tasks. These tasks were grouped into SE activities according to the six phases of the Software Development Life Cycle: requirements engineering, software design, software development, software quality assurance, software maintenance, and software management. Di Sipio et al. [5] highlighted the lack of a SE classification of PTMs on HF, as the existing one is specific for ML. To address this gap, they proposed extracting information from the model cards [8] and using a semi-automated method to identify SE tasks and their corresponding PTMs from the literature. However, they only tested the mapping on three PTMs: BERT, RoBERTa, and T5. Yang et al. [9] analyzed the ecosystem of LLMs as of August 2023, curating 366 models and 73 datasets from HF for SE. The study also explores the use of LLMs to assist in constructing and analyzing the ecosystem, which increased the model size by 16.5. They focus on code-based LLMs, which represent a more specific scope compared to LLMs for broader SE tasks.\nDespite the above notable advances, a comprehensive framework that organizes PTMs and datasets on HF with a strong SE orientation remains absent. In contrast to prior works, this paper adopts a SE perspective to extend the existing taxonomy in Hou et al. [7], to encompass a broader and more diverse set of PTMs, as well as datasets, categorizing them according to specific SE tasks and activities. By analyzing a substantially larger set of PTMs and SE-relevant datasets on HF, we address the unique SE-specific requirements unmet by previous studies, offering a novel, exhaustive preliminary classification framework for the community."}, {"title": "III. METHODOLOGY", "content": "Following the Goal Question Metric (GQM) template [10], our goal is to analyze PTMs and datasets for the purpose of their classification with respect to their application to SE tasks and activities from the point of view of software engineers in the context of the HF Hub.\nThis goal is structured around four RQs. First, we need to assess the quality of model and dataset cards, as this information is essential for the subsequent RQs:\n\u2022 RQ1: What is the status of the model and dataset cards?\nNext, we explore how the selected resources tackle SE:\n\u2022 RQ2: How do PTMs and datasets address SE?\n\u2022 RQ2.1: What SE tasks and activities are covered by PTMs and datasets?\n\u2022 RQ2.2: What are the most popular PTMs and datasets that address SE activities?\nFollowing this, we explore the connection between ML tasks in HF's classification and SE activities:\n\u2022 RQ3: How are ML tasks related to SE activities?\nLastly, we examine the long-term relevance of our findings:\n\u2022 RQ4: How stable is this information over time?"}, {"title": "B. Data collection and Preparation", "content": "Figure 1 illustrates the pipeline we followed to collect and prepare data for analysis. This process is designed for reproducibility, allowing anyone with access to the replication package [6] to validate and update the results. The inclusion criteria are summarized in Table I.\n1) Data Collection: We used the HF API [11] to gather all available resources as of October 19, 2024: 1,060,419 PTMs and 229,767 datasets. During this process, we collected the unique identifiers for each model and dataset, and assessed the availability of their cards.\n2) Data Preparation:\na) Initial Filtering: We automatically filtered PTMs and datasets mentioning SE tasks proposed by Hou et al. [7] in their cards. This step enabled us to focus on SE-relevant resources, resulting in 10,077 PTMs and 1,836 datasets.\nb) Refinement for SE Relevance: To ensure our focus on SE, we removed entries that did not contain \"code\" or \u201csoftware\u201d in the model and dataset cards, resulting in 7,395 PTMS and 982 datasets. For this subset, we retrieved all available information from HF and mapped the SE task to the corresponding SE activity. Furthermore, we used an LLM, Gemini 1.5 Pro [12], to identify whether each resource was intended for SE based on an analysis of their cards and metadata. As a validation step, we manually classified a balanced sample of 30 PTMs and 30 datasets (between SE-relevant and non-SE), and compared our decisions with those generated by the LLM using Cohen's Kappa [13]. This sample size was chosen to ensure reliable initial validation, as higher levels of agreement are anticipated [14]. For PTMs, the Kappa was 0.80, which falls within the 0.61 to 0.80 range indicating substantial agreement [15]; for datasets, it was 0.70, also reflecting this level of agreement. We noticed that some SE tasks, such as logging and verification, could be ambiguous. For instance, a model card containing a code snippet like\nfrom transformers import logging might be misclassified as a logging task when it may not be. To ensure rigor, we asked the LLM to classify them by providing explicit definitions of such tasks. Lastly, we applied the prompts to all resources, resulting in 2,009 PTMs and 358 datasets, representing 0.19% and 0.16% of the original sets.\n3) Data Analysis: In exploring the landscape of PTMs and datasets within HF, we centered our analysis on their cards, as well as on metadata such as tags, creation dates, and other relevant attributes. Additionally, we define popularity as the sum of the normalized number of likes and downloads."}, {"title": "IV. RESULTS", "content": "As summarized in Table II, over 33% of the PTMs lack a model card, highlighting a gap in the documentation. Furthermore, more than 65% do not mention any SE tasks, and only 0.95% mention SE tasks. The analysis for datasets reveals a lack of documentation, as detailed in Table III. Over 27% of datasets do not have a dataset card, indicating a significant gap in available information. Additionally, 0.56% are empty, and a staggering 70.69% do not reference any SE task. Only 0.80% of the datasets allude to SE tasks.\nFinding 1: The current state of HF shows that only 10,077 PTMs and 1,836 datasets mention SE tasks in their cards."}, {"title": "B. How do PTMs and datasets address SE? (RQ2)", "content": "1) What SE tasks and activities are covered by PTMs and datasets?: Figure 2 shows the distribution of PTMs across SE tasks, color-coded by SE activity. The top tasks are code generation and code completion, with the first having nearly ten times as many PTMs as the third most common. The most represented activity is software development, while software design and requirements engineering have limited PTMs. Additionally, the absence of PTMs for software management highlights a critical gap in the current landscape.\nRegarding datasets, Figure 3 shows that software development dominates, while software design and requirements engineering are underrepresented, and software management is entirely absent, mirroring the patterns observed with PTMs.\nThe leading SE task is code generation, which has over three times the number of datasets compared to the next task. Interestingly, the focus of secondary SE activity diverges between PTMs and datasets: software maintenance is the second SE activity most addressed for PTMs, while for datasets, software quality assurance holds this position.\nFinding 2.1: Code generation is the most covered SE task among PTMs and datasets.\nFinding 2.2: Software development dominates, while software management is absent in PTMs and datasets.\n2) What are the most popular PTMs and datasets that address SE activities?: As shown in Figure 4, the three most popular PTMs across SE activities reveal that software development has a higher popularity due to its broad representation. Figure 5 presents the three most popular datasets for software development, software quality assurance, and software maintenance. Other SE activities are omitted, as their dataset popularity is exactly 0. Popularity values for datasets tend to be higher, which may suggest that the community places more value on datasets than on PTMs, potentially indicating a higher demand for quality datasets in comparison to PTMs. In other words, researchers might perceive a good dataset as more valuable or essential for advancing SE activities than a well-performing model.\nFinding 2.3: The most popular PTMs and datasets predominantly address software development.\nFinding 2.4: Datasets tend to be more popular than PTMs, suggesting a higher demand for quality datasets in SE."}, {"title": "C. How are ML tasks related to SE activities? (RQ3)", "content": "The relationship between SE activities and ML tasks for PTMs and datasets is illustrated in Figures 6 and 7, respectively, with the flow indicating the number of resources. Both figures highlight the prominence of the ML task text generation, which is associated with the largest number of PTMs and datasets. This task is most commonly linked to software development from the SE perspective, though it also supports other SE activities. In contrast, other ML tasks exhibit considerably smaller flows.\nFinding 3: The ML task text generation is the most common among SE-related PTMs and datasets."}, {"title": "D. How stable is this information over time? (RQ4)", "content": "Figure 8 shows a growth in the creation of PTMs for SE tasks since 2020, with quarterly data revealing periods of accelerated development. Notably, the percentage of PTMS for software development increased from 6.78% in 2023 Q2 to 20.55% currently. Similarly, software maintenance and software quality assurance also showed significant growth, with the former rising from 1.70% to 32.34%, and the latter increasing from 1.19% to 28.57% over the same period. A zoom-in view of this period, from 2023 Q2 to 2024 Q3, is included in the figure to provide a closer examination. While the ranking of activities remains relatively stable over time, there are some fluctuations, particularly in 2023 Q3 and 2024 Q1, where software quality assurance outperformed software maintenance, albeit with a less pronounced change in 2024.\nFinding 4.1: PTMs for SE have grown consistently since 2020, with notable acceleration from 2023 Q2.\nFinding 4.2: The ranking of SE tasks remains stable over time, with minor fluctuations."}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "Our results align with Hou et al. [7], which shows a similar distribution of LLMs across SE activities, with both studies identifying an emphasis on code generation tasks. However, we have found a gap in software maintenance within the current state of HF. Notably, our findings complement Yang et al.'s analysis [9], highlighting a rapid growth in this area.\nPotential threats impacting our study's validity are outlined. First, the quality of the model and dataset cards may compromise internal validity, as incomplete documentation could lead to misclassification. Our conclusions rely on the assumption that each PTM and dataset's card accurately reflects the SE tasks it addresses. Second, other platforms/repositories (e.g., PyTorch Hub [16]) may host additional relevant resources, and changes on HF could affect the broader applicability of our conclusions. Third, our classification of SE tasks and activities is based on a taxonomy from the existing literature [7]. Although this taxonomy informs our analysis, emerging standards may further enrich our understanding of SE. To mitigate these threats, our study is fully replicable, enabling future researchers to validate and extend our findings."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This study examines the availability of PTMs and datasets for SE hosted on HF, revealing that almost 1% reference SE tasks. PTMs predominantly focus on code generation and code completion, while code generation is the most represented dataset task. SE activities like software design and requirements engineering are under-represented, with a gap in software management resources. SE datasets tend to be more popular than PTMs. The most prevalent ML task across both PTMs and datasets is text generation. Additionally, there has been a significant surge in PTMs for SE since 2023 Q2. This snapshot of the current landscape highlights the existing gaps in SE resources and provides valuable insight into the field's evolving needs, helping to motivate future research efforts aimed at addressing these underrepresented areas.\nFor future work, we plan to extend this classification to other repositories, such as Papers with Code [17], PyTorch Hub [16] and TensorFlow Hub [18], allowing for a broader analysis of PTMs and datasets in SE. In addition, we aim to refine the classification process by addressing current limitations, such as handling synonymous in SE tasks, to improve accuracy. To make our classification more actionable, we propose developing a dashboard to assist researchers with proper sampling and support practitioners in the SE community when selecting PTMs for real-world applications, thereby enhancing the relevance and impact of our work."}]}