{"title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation", "authors": ["Tianwei Lin", "Wenqiao Zhang", "Sijing Li", "Yuqian Yuan", "Binhe Yu", "Haoyuan Li", "Wanggui He", "Hao Jiang", "Mengze Li", "Xiaohui Song", "Siliang Tang", "Jun Xiao", "Hui Lin", "Yueting Zhuang", "Beng Chin Ooi"], "abstract": "We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LORA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) (Liu et al. 2023; OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have demonstrated outstanding open-world visual comprehension and reasoning abilities through language-based interactive dialogue over the past years, simultaneously opening up new opportunities for applications in specialized domains."}, {"title": "2 Related Work", "content": "Medical Vision Large Language Models. Recently, medical vision large language models (Med-VLLMs) have made significant progress, demonstrating excellent performance in understanding medical images and responding to human queries based on these images (Zhou et al. 2023; Tian et al. 2023). XrayGPT (Thawkar et al. 2023) combines a medical visual encoder (MedClip) (Wang et al. 2022) with a fine-tuned LLM, using a simple linear transformation layer to achieve alignment between visual and textual information, significantly enhancing the understanding of medical images. On this basis, LLaVA-Med (Li et al. 2024b) further enhances visual-text alignment in medical contexts by selecting high-quality image-text pairs from PubMed papers and synthesized VQA datasets. BiomedGPT (Luo et al. 2024b) employs a BERT-style encoder and GPT-style decoder architecture, pre-trained on interdisciplinary datasets. Compared to commercial models like Med-PaLM (Singhal et al. 2023), BiomedGPT significantly reduces model size while maintaining superior performance. However, issues of language adaptability and dataset specificity still remain.\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a) introduces the PubMedVision dataset, which contains 1.3 million high-quality medical samples, significantly improving the model's adaptability across diverse medical applications. However, current Med-VLLMs mainly focus on medical comprehension and lack the capability for the medical vision-language generation.\nUnified Visual Comprehension and Generation Models. Recent research has increasingly concentrated on creating unified LVLMs that are adept at understanding and producing content across various visual modalities. NExT-GPT (Wu et al. 2023) achieves perception and generation for arbitrary combinations of multi-modal inputs and outputs by aligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-X (Ge et al. 2024), and DreamLLM (Dong et al. 2023) employ learnable queries and leverage next-token prediction to generate visual tokens, providing conditional inputs to external generation modules. Unlike these methods, which function as external conditioners, Unified-IO (Lu et al. 2022), Unified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024) internalize multi-modal generation tasks within a unified Transformer architecture by extending multi-modal vocabularies, enabling direct generation based on next-token prediction. Building on this concept, Lumina-mGPT (Liu et al. 2024a) and ANOLE (Chern et al. 2024) further enhance the generation capabilities of unified models using high-quality data, particularly improving the quality and flexibility of image generation."}, {"title": "3 Preliminaries", "content": "Large Vision-Language Models. The input to a LVLM typically consists of an image \\(x_{img}\\) and a discrete text sequence \\(x_{txt}\\). The visual encoder \\(E_{img}\\) converts the input image \\(x_{img}\\) into a sequence of visual tokens \\(V = [v_i]_{i=1}^{N_v}\\), while the text sequence \\(x_{txt}\\) is mapped into a sequence of text tokens \\(T = [t_j]_{j=1}^{N_t}\\) using an embedding function \\(E_{txt}\\). The LLM \\(M_{LLM(10)}\\) models the joint probability of the token sequence \\(U = {V, T}\\), which is expressed as:\n\\[\nP_\\theta(RU) = \\prod_{i=1}^{N_r} P_\\theta(r_i | {U,r_{<i>}),\n\\]\nwhere \\(R = [r_i]_{i=1}^{N_r}\\) is the text response sequence. The LVLM iteratively generates the next token \\(r_i\\) based on \\(r_{</i></sub>. The optimization objective is to minimize the cross-entropy loss of the response \\(R\\). It is worth noting that most LVLMs adopt"}, {"title": "4 HealthGPT", "content": "4.1 Unified Autoregressive Generation.\nHealthGPT (Figure 3) utilizes a discrete token representation that covers both text and visual outputs, unifying visual comprehension and generation as an autoregressive task. For comprehension, \\(M_{ilm}\\) receives the input joint sequence U and outputs a series of text token \\(R = [r_1,r_2,...,r_{N_r}]\\), where \\(r_i \\in V_{txt}\\), and \\(V_{txt}\\) represents the LLM's vocabulary:\n\\[\nP_\\theta(R | U) = \\prod_{i=1}^{N_r} P_\\theta(r_i | U,r_{<i>).\n\\]\nFor generation, \\(M_{ilm}\\) first receives a special start token (START_IMG), then generates a series of tokens corresponding to the VQGAN indices \\(I = [i_1,i_2,..., i_{N_i}]\\), where \\(i_j \\in V_{vq}\\), and \\(V_{vq}\\) represents the index range of VQGAN. Upon completion of generation, the LLM outputs an end token (END_IMG):\n\\[\nP_\\theta(I | U) = \\prod_{j=1}^{N_i} P_\\theta(i_j | U,i_{j}).\n\\]\nFinally, the generated index sequence I is fed into the decoder G, which reconstructs the target image \\(\\hat{c}_{img} = G(I)\\).\n4.2 Hierarchical Visual Perception\nGiven the differences in visual perception between comprehension and generation tasks-where the former focuses on abstract semantics and the latter emphasizes complete semantics-we employ ViT to compress the image into discrete visual tokens at multiple hierarchical levels. Specifically, the image is converted into a series of features \\({f_1, f_2,..., f_L}\\) as it passes through L ViT blocks."}, {"title": "4.3 Heterogeneous Knowledge Adaptation", "content": "We devise H-LORA, which stores heterogeneous knowledge from comprehension and generation tasks in separate modules and dynamically routes to extract task-relevant knowledge from these modules. At the task level, for each task type T, we dynamically assign a dedicated H-LORA submodule \\(R^{\\mathcal{T}}\\, which is expressed as:\n\\[\nR^\\mathcal{T} = M_{LLM}(U|\\theta, \\theta^\\mathcal{T}), \\theta^\\mathcal{T} = {A^\\mathcal{T},B^\\mathcal{T}, Router^\\mathcal{T}}.\n\\]\nAt the feature level for a single task, H-LoRA integrates the idea of Mixture of Experts (MoE) (Masoudnia and Ebrahimpour 2014) and designs an efficient matrix merging and routing weight allocation mechanism, thus avoiding the significant computational delay introduced by matrix splitting in existing MoELORA (Luo et al. 2024a). Specifically, we first merge the low-rank matrices (rank = r) of k LORA experts into a unified matrix:\n\\[\nA^{merged}, B^{merged} = Concat({A_i}_{i=1}^{k}), Concat({B_i}_{i=1}^{k}),\n\\]\nwhere \\(A^{merged} \\in \\mathbb{R}^{d_{in} \\times rk}\\) and \\(B^{merged} \\in \\mathbb{R}^{rk \\times d_{out}}\\). The k-dimension routing layer generates expert weights \\(W \\in \\mathbb{R}^{token\\_num \\times k}\\) based on the input hidden state x, and these are expanded to \\(\\mathbb{R}^{token\\_num \\times rk}\\) as follows:\n\\[\nW^{expanded} = \\alpha \\frac{W}{\\sqrt{r}} \\otimes 1_r,\n\\]\nwhere \\(\\otimes\\) denotes the replication operation. The overall output of H-LORA is computed as:\n\\[\nO_{H-LORA} = (xA^{merged} \\odot W^{expanded})B^{merged}\n\\]\nwhere \\(\\odot\\) represents element-wise multiplication. Finally, the output of H-LORA is added to the frozen pre-trained weights to produce the final output:\n\\[\nO = xW_0 + O_{H-LORA}.\n\\]"}, {"title": "4.4 Training Pipeline", "content": "1st Stage: Multi-modal Alignment. In the first stage, we design separate visual adapters and H-LoRA submodules for medical unified tasks. For the medical comprehension task, we train abstract-grained visual adapters using high-quality image-text pairs to align visual embeddings with textual embeddings, thereby enabling the model to accurately describe medical visual content. During this process, the pre-trained LLM and its corresponding H-LoRA submodules remain frozen. In contrast, the medical generation task requires training concrete-grained adapters and H-LORA submodules while keeping the LLM frozen. Meanwhile, we extend the textual vocabulary to include multimodal tokens, enabling the support of additional VQGAN vector quantization indices. The model trains on image-VQ pairs, endowing the pre-trained LLM with the capability for image reconstruction. This design ensures pixel-level consistency of pre- and post-LVLM. The processes establish the initial alignment between the LLM's outputs and the visual inputs.\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation. The submodules of H-LORA share the word embedding layer and output head but may encounter issues such as bias and scale inconsistencies during training across different tasks. To ensure that the multiple H-LORA plugins seamlessly interface with the LLMs and form a unified base, we fine-tune the word embedding layer and output head using a small amount of mixed data to maintain consistency in the model weights. Specifically, during this stage, all H-LORA submodules for different tasks are kept frozen, with only the word embedding layer and output head being optimized. Through this stage, the model accumulates foundational knowledge for unified tasks by adapting H-LORA plugins.\n3rd Stage: Visual Instruction Fine-Tuning. In the third stage, we introduce additional task-specific data to further optimize the model and enhance its adaptability to downstream tasks such as medical visual comprehension (e.g., medical QA, medical dialogues, and report generation) or generation tasks (e.g., super-resolution, denoising, and"}, {"title": "5.3 In-Depth Study", "content": "Effect of Heterogeneous Low-Rank Adaptation. H-LORA provides an optimized multi-LoRA architecture for multi-task learning. We conduct extensive validation of this structure, with results presented in Table 4, comparing the performance of LoRA, MoELORA, and H-LORA in medical unified comprehension and generation tasks. In the majority of comprehension tasks and all generation tasks, H-LORA demonstrates superior performance, particularly in the OmniMedVQA benchmark, where it improved from 64.90 to 68.50. Notably, despite some applications of MoELORA in certain scenarios, it do not show advantages in this task and"}, {"title": "6 Conclusion", "content": "HealthGPT introduces an innovative PEFT approach by integrating collaborative and competitive modules, which significantly improves the efficiency and effectiveness of multi-task learning. In the proposed CME benchmark tests, HealthGPT not only achieves faster response speed but also outperforms existing multi-LoRA architectures in performance. Future research will further explore the game-"}, {"title": "A Implementation Details", "content": "A.1 Model Details\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as visual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing concrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated with text tokens and input into the large language models (LLMs).\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al. 2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with 8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further augment the model's capacity for understanding both visual and textual input. Figure 6 shows the details.\nA.2 Training Details\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adaptation (H-LORA). We provide a detailed hyperparameter configuration for the model's three-stage training process. The specific hyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model's learning efficacy and final performance.\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension and generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to this issue, which is the reason for the slight differences in hyperparameters between HealthGPT-M3 and HealthGPT-L14."}, {"title": "A.3 VL-Health", "content": "The construction of the VL-Health dataset involves two key steps: (i) data collection, (ii) data processing, as detailed below:\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of the tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets such as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al. 2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations to assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal datasets like LLaVA-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical knowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream task categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction. The IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-resolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual reports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large number of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we rewrote and adjusted the LLaVA-558k (Liu et al. 2024b) dataset.\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we standardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and evaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding and training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extraction, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used VQGAN-generated indices to supervise the generation tasks.\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more comprehensive understanding.\nData Overview: To ensure a balanced development of the model's comprehension and generation capabilities, in addition to the LLaVA-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802 additional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-following capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation instruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, enhancing the model's overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (approximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA (approximately 52,000 images), LLaVA-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 images). Multiple question-answer pairs were retained for each image to enhance the model's understanding and generalization of the image content. Table 8 shows the data distribution of VL-Health for three-stage learning strategy, where mixed-47k is based on the sampling of all data in stage-1.\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultrasound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encompasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary diseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides comprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases."}, {"title": "B Analysis of Heterogeneous Low-Rank Adaptation", "content": "We propose H-LORA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across tasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based on MoELORA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\nAlgorithm 1: H-LORA Algorithm\nInput: concrete-grained visual features FCon, abstract-grained visual features FAbs, comprehension-based H-LoRA modules ({{AComp.i}k_i=1, RComp}), generation-based H-LoRA modules ({{AGen.i}k_i=1, RGen}), task type T (comprehension or generation), number of LoRA experts k, origin linear layer weights Wo, text features T, hidden state h\nOutput: final output O\n// Select task-specific image features\nif T = generation task then\nFimg\u2190 FCon\nelse if T = comprehension task then\nFimg\u2190 FAbs\nend if\nU concat(Fimg, T) // Concatenate image features and text features\n{{A}}k i=1, {{B}}k i=1, Router iter {{A}}k i=1, {{B}}k i=1, Router // Assign task-specific H-LORA submodule\n// Merge LoRA experts' matrices\nAmerged concat({{Ai}k i=1})\nBmerged \u2190 concat({{Bi}k i=1})\nW \u2190 R(h) // Generate routing weights based on input hidden state x\nWexpanded \u2190 a \u00d7 W/r 1 // Expand routing weights to match merged matrices\nOH-LORA (x. Amerged Wexpanded) . Bmerged // Compute H-LORA output using element-wise multiplication\nO \u2190 x \u2022 Wo + OH-LORA // Add H-LoRA output to pre-trained weights to get final output\nReturn O\nWe further analyzed the computational overhead differences between MoELORA and H-LORA. Assuming that both methods use the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps involved.\nComputational Overhead of MoELORA. In MoELORA, the operations involving the expert matrix mainly include the following steps: (i) Expert Multiplication: MoELORA requires 2k multiplications with the LoRA experts. (ii) Router Multiplication: One multiplication with the Router is required. (iii) Router Output Expansion: MoELORA needs to perform k"}]}