{"title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety", "authors": ["Xuhao Hu", "Dongrui Liu", "Hao Li", "Xuanjing Huang", "Jing Shao"], "abstract": "Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-40. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) [3, 9, 20, 30] integrate images into large language models (LLMs) [2, 23, 48] and have developed rapidly. MLLMs demonstrate remarkable performance in image understanding, visual question answering, etc. Meanwhile, the extraordinary capacity of MLLMs brings safety concerns in many scenarios [16, 33, 34, 43, 59]. To this end, previous studies use multimodal image-text pairs to align MLLMs by supervised fine-tuning (SFT) [67] and reinforcement learning from human feedback (RLHF) [62].\nHowever, [5] discovers that textual unlearning (i.e., only using texts for alignment) outperforms multimodal safety alignment methods (e.g., SFT) with significantly reduced data-collection and computational cost, almost 6 times lower. Based on this experimental observation, it seems that textual unlearning can solve the multimodal safety problem. Such a phenomenon is counter-intuitive because the image modality introduces different and abundant visual information, which is relatively independent of the text modality.\nTo explain the above counter-intuitive phenomenon, we find that there exists a visual safety information leakage (VSIL) problem in multimodal safety data: the sensitive and risky content in the image has been leaked and described in the textual query to a certain extent. Specifically, the VSIL samples in Figure 1 show a gambling scenario in the image and an \"online gambling site\" in textual query. As for another example, the textual query contains \"making porn\" which directly describes the image content. Therefore, MLLMs can easily detect and refuse to answer these unsafe and sensitive queries based only on textual input without perceiving and understanding the image content. Furthermore, we find that most multimodal safety benchmarks [8, 16, 34, 46] suffer from the VSIL problems.\nBuilding on the above analysis into VSIL, we find that a simple textual SFT can achieve comparable safety performance with multimodal alignment methods, e.g. SFT [67] and RLHF [62], while maintaining similar general ability. Specifically, simple textual SFT methods can achieve safety performance exceeding 95%, comparable to multimodal alignment methods, on widely used multimodal safety benchmarks, such as JailbreakV [34], FigStep [16], and VL-Safe [8]. It means that visual safety information leakage leads to the unexpected outstanding performance of textual alignment methods, including unlearning and SFT.\nHowever, multimodal safety concerns not only contain image-text pairs with VSIL but also include image-text pairs without VSIL, which has been overlooked in existing safety benchmarks. To this end, we construct VLSBench with"}, {"title": "2. Visual Leakage in Multimodal Safety", "content": "In this section, we find a prevalent problem in existing multimodal safety datasets, named visual safety information leakage (VSIL). We try to make a clear definition for this problem. Then, we conduct a quantitative experiment and qualitative verification to support our discovery of VSIL problem. We further delve into this problem and discover that simple textual alignment, using only textual samples, can achieve outstanding performance in existing multimodal safety datasets with VSIL problem."}, {"title": "2.1. Visual Safety Information Leakage (VSIL)", "content": "Definition. Visual safety information leakage (VSIL) means that the safety-related image content has been revealed in textual query. We define VSIL as follows. Given a textual query T and an image I, let J denote a safety judge model [21, 26]. The safety judge model J classifies whether the input image-text pair is safe or not, i.e., $J(T,I) \\in \\{safe, unsafe\\}$. In this way, VSIL represents that $J(T, I) = J(T)$ for an unsafe text-image pair $J(T, I) = unsafe$.\nQuantitative verification. We have conducted a harmful evaluation experiment, leveraging LlamaGuard3-11B-Vision [12], the most updated safety judge model J with vision ability to evaluate the safe label, given its input. We evaluate the following four datasets, FigStep [16], JailbreakV [34], Harmbench-mm [35] and VLSafe [8]. To be"}, {"title": "2.2. VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets", "content": "Due to this obvious problem of VSIL, we have conducted a further experiment to discover its limitations. Based on the previous explanation, we conduct a detailed comparison experiment between models aligned using the textual alignment and those using multimodal alignment including SFT [67] and RLHF [62] on existing multimodal safety benchmarks."}, {"title": "2.2.1. Experiment Setup", "content": "Models. We conduct our textual alignment experiment compared with multimodal alignment on three models. We choose LLaVA-v1.5 [32] 7b and 13b version, being one of the most commonly used MLLMs. Also, we consider the current powerful MLLM, Qwen2-VL-7B-Instruct [51].\nBaselines. For LLaVA-v1.5-7b, we have three multimodal alignment baselines: one is VLGuard-Mix-7b [67] aligned with SFT with 2k safe samples plus 1k normal samples; the other two is SPA-VL [62] aligned using 30k preference data separately by DPO and PPO. And we conduct two textual methods, textual unlearning previously mentioned and textual SFT. The textual training data is filtered from SafeRLHF [22], you can find the detailed data processing in Appendix B. As for the LLaVA-v1.5-13b and Qwen2-VL-7B models, we separately have one multimodal alignment baseline, SFT with VLGuard, and one textual SFT method with SafeRLHF [22].\nEvaluation benchmark. For safety tasks, We leverage the most widely used multimodal safety benchmark as our evaluation dataset. To be specific, FigStep [16] compromises 500 queries with image OCR; mmsafetybench [33] including approximately 2k samples with enhanced stable-diffusion related and typo images; JailbreakV [34] which we use its miniset, is about 300 pairs. For multimodal ability tasks, we use the most commonly used benchmark to assess the model's overall ability, like MME [56], MMMU [58], MM-Star [7] and MMVet [57].\nEvaluation metrics. For multimodal safety task, we employed the widely recognized QA safety evaluation tool LlamaGuard [21], using its SOTA model, LlamaGuard3-8B [21] for a rapid and standardized safety assessment, using the safety rate as our metric. For the multimodal general"}, {"title": "2.2.2. Findings", "content": "We present the main results in Table 2 and summarize the key findings as follows:\nFinding 1: Textual SFT shows outstanding multimodal safety Performance on datasets with VSIL The safety results in Table 2 demonstrate that textual SFT performs similarly with multimodal alignment, achieving over 95% average safety rate on current multimodal safety benchmarks. To be detailed, all the alignment methods on the three baselines get quite good safety performance in all the multimodal safety tasks. Take LLaVA-v1.5-7b for example, models aligned on SPA-VL [62] with DPO demonstrate the best safety performance, achieving 98.57% on the raw set of JailbreakV and 99.01% on VLSafe [8], over 96% on FigStep [16]. However, a simple textual unlearning also demonstrates quite a high safety rate, over 95% on JailbreakV, and over 91% on FigStep. What's more surprising is that the textual SFT is even better than textual unlearning, with 96.4% on FigStep. Moreover, the textual SFT outperforms the multimodal SFT, VLGuard, with the same quantity of training samples on JailbreakV and VLSafe.\nFinding 2: Textual SFT matches multimodal alignment in general ability. In terms of multimodal general capabilities, the textual SFT method demonstrates similar performance compared to multimodal alignment. Specifically, the textual SFT models are only slightly behind VLGuard on the"}, {"title": "3. VLSBench Construction Pipeline", "content": "Data Description. To address the existing issues in current multimodal safety benchmarks, called VSIL, we construct Multimodal Visual Leakless Safety Benchmark (VLSBench) filling this blank in the current multimodal safety datasets. As shown in Figure 6, our dataset consists of an image paired with a neutral textual query. To be specific, some images, like those depicting \"drugs and syringes,\" are clearly risky, while others, like \"kids in a jail,\" may seem less obvious but still contain safety concerns. The associated queries related"}, {"title": "3.1. VLSBench Overview", "content": "to the images do not include any obvious harmful keywords or intentions. Instead, they provide harmless information, such as \"fast results with this equipment\" or \"engage in play scenario,\" which may still imply potential risks depending on the visual context. Please check Appendix D for more examples in VLSBench.\nSafety Taxonomy. Inspired by existing safety taxonomy for LLMs safety [26, 49] and multimodal safety [18, 46], we propose a hierarchical two-level safety taxonomy for our VLSBench. This taxonomy shown in Figure 3 builds upon previous safety taxonomies and specifically highlights categories that better represent visual safety in MLLMs, which better support our VLSBench data construction. The VLSBench finally compromises 6 categories and 19 sub-categories detailed described in Figure 4, including 2.4k image-text pairs."}, {"title": "3.2. VLSBench Data Collection", "content": "Our data construction pipeline shown in Figure 5 focuses on effectively preventing visual safety leakage from image modality to textual query. First, we should generate harmful textual queries from two parallel paths shown in Step 1. Then, we need to detoxify the harmful queries and obtain the harmless queries shown in Step 2. Furthermore, we use text-to-image models to iteratively generate images shown in Step 3. Finally, we filter out the mismatched and safe image-text pairs and obtain the final datasets as shown in Step 4. The detailed construction pipeline is listed as follows:\nStep 1: Harmful query and image description generation. Initially, to ensure that the generated queries cover a wide range of safety categories, we have implemented two parallel approaches. The first is to extract diverse safety topics from"}, {"title": "4. Benchmark Experiments", "content": "MLLMs. We benchmark various MLLMs including both open-source models and close-source models accessible only via API. The open-sourced models are: (1) LLaVA-v1.6-7b13b [32] (2) LLaVA-1.6-mistral [31], (3) LLaVA-llama3 [25], (4) Qwen2-VL-7B-Instruct [51], (5) Llama-3.2-11B-Vision-Instruct [12]. Close-source APIs are (1) GPT-40 [20], (2) Gemini-1.5-pro [47].\nTextual alignment and multimodal alignment baselines. We also benchmark different safety alignment methods. We follow the same baseline setting as shown in Table 2. For LLaVA-v1.5-7b, we have multimodal SFT with VLGuard [67], multimodal DPO and PPO with SPA-VL [62] and textual unlearning and textual SFT with SafeRLHF [22]. For LLaVA-v1.5-13b and Qwen2-VL-7b, we have multimodal SFT and textual SFT."}, {"title": "4.1. Experiment Setup", "content": "Evaluation. For the evaluation of our VLSBench, we use GPT-40 as the judge model and design a classification prompt for this task. We classify the evaluation response labels into three types: safe with refusal for a clear and firm rejection to the request, safe with warning for responses that identify safety concerns and provide appropriate caution, and unsafe for answers that disregard safety principles and answer the question directly. We calculate the safety rate (%) by considering the total of safe with refusal and safe with warning. Detailed evaluation can be found in Appendix F."}, {"title": "4.2. Main Results", "content": "In this section, we present a deeper insight into the evaluation results shown in Table 3 on our VLSBench. We summarize our key findings as follows:\nTextual alignment lags behind multimodal alignment in VLSBench. Based on the safety alignment results on our dataset. We could see a clear disparity between textual alignment methods and multimodal alignment methods. To be detailed, the textual SFT on LLaVA-v1.5-7b base model only shows a 13.99% safety rate. Although textual SFT has double safety performance compared to the base model 6.6% safety rate, it still significantly lags behind multimodal SFT with VLGuard, which achieves a 21.26% safety rate not to mention this SPA-VL model which has undergone RLHF training with a large dataset of 30k image-text pairs. Also, the same disparity can be found in the LLaVA-v1.5-13b and Qwen2-VL-7B base models. From the results in Figure 7, we clearly see an average of 10% difference between textual SFT and multimodal SFT.\nThe challenging nature of VLSBench. Current open-source and close-source MLLMs all struggle to perform well on Our VLSBench. To be detailed, the best close-source result is 49.78%, achieved by Gemini-1.5-pro, while"}, {"title": "5. Related Work", "content": "MLLMs. The rapid development of LLMs has significantly promoted the advancement of MLLMs. Integrated with multimodal encoders including image [1], audio [24], and video [55], MLLMs is designed to process complicated multimodal information. The most extensively studied modality at present is vision, with its integration with text, using Large Language Models (LLMs) as the backbone. The subsequent discussion of multimodal safety will also focus on the realm of vision and language modality. The most common architecture today uses a vision encoder [11] to encode image information, followed by a projector module to align the visual information and textual representations. Both are then fed into the LLM backbone for autoregressive generation. Notable models employing this include LLaVA [32], LLaVA-Next [31], QwenVL series [4] and Llama-3.2-Vision [12].\nMultimodal Safety Concerns. The rising usage of MLLMs has sparked growing concern about their potential safety risks. While MLLMs feature LLMs as the backbone, the most common and direct safety concerns in MLLMs overlap a lot with those concerns in the LLMs domain. In that case, the wide range of harmful output content including toxicity [29], bias [14], and so on in LLMs [26, 35] are also critical safety concerns that need to be addressed in MLLMs. To be specific, the various attack methods [43, 50, 68] could also be utilized to jailbreak MLLMs. On the other hand, however, when visual input is integrated as a component in current MLLMs, the visual safety awareness of the MLLMs is crucial to guide the next response and actions [10, 39, 46, 66]. There are emerging vulnerabilities arising from visual modality other than simply textual jailbreaking [6, 42]. In that case, the unique safety concern that MLLMs have could be roughly categorized into two types. The first is white-box attack [36, 38, 45]. These kinds of safety concerns include gradient-based searches for adversarial images that make the MLLM produce harmful outputs. The adversarial noise is crafted and added to the origin image to trick the model into generating targeted unsafe content. The other type is black-box methods mainly exploiting the visual vulnerability to supply more cheating information. For instance, some work [16, 33, 34] utilize the OCR ability and stable-diffusion generated images to provide additional information, causing the model to generate a harmful response."}, {"title": "Safety Alignment", "content": "Although MLLMs currently face significant safety challenges, there are still many effective strategies available to improve their safety. Drawing from the well-researched realm of LLM alignment methods, such as RLHF [15, 37] employing methods like proximal policy optimization (PPO) [44], supervised fine-tuning (SFT) and direct preference optimization (DPO) [41], MLLMs also utilize these similar methods with carefully crafted image-text pairs. To illustrate, VLGuard [67] utilize almost 2k image-text pairs to SFT on the LLaVA-v1.5 models and achieve a significant improvement on multimodal safety while keeping the multimodal general ability. Additionally, SPA-VL [62] leverages up to 90k multimodal image-text pairs with GPT4 labeled rankings to employ DPO and PPO methods and gain an outstanding multimodal safety performance. Except for the labor-intensive training methods to align MLLMs in safety tasks, there are also many training-free methods [5, 17, 52]. Also, some work [5] omit the image input and only utilize textual to do unlearning on MLLMs.\nMultimodal Safety Benchmark. To evaluate the current MLLM's safety, there are many multimodal safety benchmarks. VLSafe [8] is a widely used multimodal safety dataset, compromising a 1.1k test set as a benchmark. This dataset features CoCo [28] as its image source, leveraging various prompts as jailbreak techniques to generate harmful queries. Furthermore, Ch3ef [46] not only considers safety alone but takes helpfulness, honesty, and harmlessness (3H) together as a high-level principle. Apart from this, there are also multimodal safety datasets [16, 33, 34] that leverage the image modality to demonstrate harmful information through OCR or stable-diffusion generated images. Also, there is some work that focus on more challenging tasks in the multimodal safety [27, 53]. However, most of these datasets showcase visual safety information leakage (VSIL) problem, which overlooks the safety information leaked from image to textual query. We have discovered several issues arising from this problem and proposed our VLSBench to fill the blank in current multimodal safety."}, {"title": "6. Conclusions", "content": "In conclusion, our work noticed an important problem, visual safety information leakage (VSIL) in current multimodal safety data samples. This phenomenon leads to textual-based bias when evaluating the safety ability of MLLMs. Thus, current multimodal evaluation datasets encourage simple and seemingly superior methods, textual alignment with textual training samples, to solve multimodal safety challenges. However, current multimodal safety datasets overlook this important problem. To this end, we construct our multimodal visual leakless safety Benchmark (VLSBench) to fill this blank in multimodal safety. Also, We develop a data construction pipeline, that successfully prevents visual infor-"}]}