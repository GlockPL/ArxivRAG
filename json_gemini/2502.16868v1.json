{"title": "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data", "authors": ["Longbin Lai", "Changwei Luo", "Yunkai Lou", "Mingchen Ju", "Zhengyi Yang"], "abstract": "Large Language Models (LLMs) have recently demonstrated remark- able performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive explo- ration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge termed Progressive Document Investigation - by introducing Gra- phy, an end-to-end platform that automates data modeling, explo- ration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw docu- ments into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers complete with their references - demonstrating how Gra- phy facilitates the literature-survey scenario. The demonstration video can be found at https://youtu.be/uM4nzkAdGIM.", "sections": [{"title": "1 INTRODUCTION", "content": "We study real-world investigative tasks that require iterative explo- ration and synthesis of large unstructured data corpora. We refer to this challenge as Progressive Document Investigation (PDI), an iterative process of identifying a focal topic, refining a relevant dataset, and ultimately generating high-quality reports, summaries, or recommendations. A motivating example of PDI is the litera- ture survey process in academic research. Researchers start with a topic of interest, identify a few seed papers, and conduct iterative rounds of investigation: skimming key elements (e.g., \"abstract\", \"challenges\", \"solutions\u201d), following references to additional papers, and expanding the set of relevant works. After collecting a suffi- cient corpus, they then synthesize their findings into a structured survey report - often by grouping papers by shared characteristics (e.g., addressing similar challenges or proposing similar solutions).\nThe advent of Large language models (LLMs) [2] have shown im- pressive potentials for handling PDI, in particular with techniques like Retrieval-Augmented Generation (RAG) [9] and autonomous AI agents [8]. While these methods excel at single-document queries and conversational workflows, they fall short for solving PDI. RAG- based solutions often struggle to maintain consistency and organi- zation when applied to large-scale, multi-step explorations. Existing Al agent systems, on the other hand, risk error propagation across extensive pipelines, especially if they are expected to autonomously parse and link large collections of unstructured data. Moreover, both methods typically provide limited support for iterative user oversight and curation, which researchers often prefer to ensure accuracy and control.\nTo address these gaps, we propose Graphy, an end-to-end plat- form that streamlines the PDI workflow. We adopt the property graph model for the need of iterative exploration in PDI. Drawing inspiration from business intelligence (BI) systems [7], we intro- duce Fact and Dimension nodes, analogous to Fact and Dimen- sion tables in BI. Here, Fact nodes represent the primary entities of interest, while Dimension nodes capture supplementary infor- mation. In a literature-survey context, each paper functions as a Fact node (henceforth they are used interchangeably), and its ex- tracted contents, such as \"abstract\", \"challenges\", and \"solutions\", serve as Dimension nodes. Although our demonstration centers on the literature-survey scenario, Graphy is broadly applicable; in Section 4, we briefly illustrate its potential in financial use cases.\nFig. 1 provides an overview of Graphy, which consists of two main roles: the offline Scrapper and the online Surveyor.\nOffline Scrapper. The Scrapper allows users to implement the Inspection abstraction to direct the extraction of specific Dimen- sions from each document, often leveraging LLMs. This step trans- forms an unstructured document into a structured Fact nodelinked to predefined Dimension nodes. It simulates how a human re- searcher would skim a document, pinpointing aspects such as ab- stract, challenges, and solutions. Additionally, a Navigation ab- straction defines how Fact nodes are connected, enabling the re- trieval of related items for progressive exploration. For instance, an Arxiv [1] Navigation automatically fetches and downloads research papers from this open-source repository.\nBecause both the extracted data from the Inspection and the linked data from the Navigation are relatively stable, we run the Scrapper offline. Upon completion, it produces a graph of Fact nodes, Dimension nodes, and their interconnecting edges, which can be imported into a standard graph database (e.g., Graph- Scope Interactive [6]).\nOnline Surveyor. Designing a user-friendly Surveyor on top of graph databases poses two key challenges. First, unlike SQL, graph query languages are less familiar to users. Second, graph explo- ration can become unwieldy, particularly with \u201csupernodes,\" which have extremely large numbers of connections. We address these challenges with the Exploration, which is the main interface for navigating the graph and selecting papers of interest. As shown in Fig. 1, it offers a convenient Search module to initiate exploration. Users can iteratively move from one set of nodes to their neighbors (referenced papers), with Cypher operations (e.g., NeighborQuery) seamlessly integrated into a UI interface inspired by BI toolkits [10]. To avoid overwhelming users, Exploration employ histograms and top-k selectors to allow users filter out neighbors of interests.\nEventually, users can proceed to the Generation module, which leverages LLMs for creating reports from the papers selected in"}, {"title": "2 ARCHITECTURE", "content": "This section introduces the architecture of Graphy, which com- prises an Offline Scrapper and an Online Surveyor."}, {"title": "2.1 Offline Scrapper", "content": "Inspection. Given a paper document as input, the Inspection processes it to produce a structured representation comprising Fact and Dimension nodes. The paper itself forms the Fact node, while its Dimension nodes are extracted through a Directed Acyclic Graph (DAG) of instructions. Each subnode's definition aligns with the user's specific requirements. For simple dimensions (e.g., an \"abstract\"), users can employ rule-based methods such as regular expressions. For more advanced tasks, the system supports indi- vidually configured LLM subnodes, allowing users to balance cost and performance. For instance, simpler processing can rely on local models [4], whereas more complex extraction may involve sophis- ticated cloud-based models [2]. These LLM-based subnodes build on a common workflow that chunks PDF text, stores it in a vector database, and then retrieves only the most relevant chunks-based on user-defined queries, for final extraction by the LLM."}, {"title": "Graph Modelling.", "content": "The results of Inspection and Navigation as shown in Fig. 1, naturally form a graph comprising Fact and Dimension nodes. Each Fact node represents a paper, while the outputs generated by subnodes in the Inspection form a set of Di- mension nodes linked to their corresponding Fact node. This graph is incrementally expanded as new papers are processed. Specifically, when a new Fact node p2 is added, it is linked to an existing Fact node p1 if p2 is retrieved by the Navigation based on references extracted from p1.\nA notable feature of the Inspection is the customizable \"out- put_schema\" field for each subnode in the DAG, which defines the schema (data fields and their data types) for the resulting Dimen- sion nodes. The output can be single-typed, such as abstract and title of the paper, which can be directly stored as attributes of the Fact node. Alternatively, array-typed outputs like challenges and solutions can be stored as separate Dimension nodes, each sharing the same schema."}, {"title": "2.2 Online Surveyor", "content": "Exploration. The Exploration component is designed to give users an intuitive way to interact with the graph database while minimizing the learning curve.\nTraditional graph exploration typically relies on query languages, which can require extra effort to master. We address this by embed- ding graph queries within interactive UI components. As shown in Fig. 2, the Search module in the Exploration helps users pin- point their initial papers for exploration. Three key interactions are highlighted: \"E1\" searches all nodes containing the \"year\" at- tribute with a single click; \"E2\" displays a histogram of nodes by \"year\" providing a statistical overview; and \u201cE3\u201d filters and retrieves nodes for a specific year (e.g., 2023) by clicking the corresponding histogram bar. These user actions are seamlessly translated into Cypher queries and executed on the underlying graph database.\nFurthermore, encountering \u201csupernodes\u201d with exceedingly large numbers of connections can often overwhelm users and disrupt the analysis flow. To address this, we introduce a StatRefiner module that intervenes before displaying all the neighbors. This module can present neighbors either as a histogram, allowing users to quickly overview and multi-select by groups, or as a table, where they can sort by specific attributes and choose the top-k results for further exploration. In Section 3, we provide examples showing how this approach streamlines the exploration process.\nGeneration. Once users finish selecting papers in the Exploration, they can employ the Generation to convert this explored data into structured reports. By leveraging the natural language understand- ing and summarization capabilities of LLMs, the Generation turns the network of interconnected papers on the canvas into a mind map and, ultimately, a well-formatted narrative report. This process involves three main steps: (1) Interpreting User Intentions: Users describe their desired report in natural language, from which LLM infers which attributes and dimensions of the paper are needed. For instance, if a user asks for a related work section focusing on the paper's challenges, the LLM may determine that the \"title\" and \"abstract\" attributes and the \"challenges\" dimension are required. Users can review and refine these selections before proceeding. As the dimensions are pre-extracted during the offline Scrapper phase, the Generation can quickly retrieve them on demand. (2) Gener- ating Mind Maps: Like a human expert, we prompt the LLM to organize the selected papers into a mind map based on the dimen- sions mentioned by the users, providing a high-level blueprint for the final report. To accommodate context-size limitations, we adopt an iterative approach that feeds the LLM subsets of the data at a time, gradually constructing the mind map for users to review. (3) Writing Reports: With the mind map in place, the LLM finalizes the literature survey by generating a cohesive report, which can then be downloaded in various formats (e.g., PDF or TeX) to support academic writing."}, {"title": "3 DEMONSTRATING LITERATURE SURVEY", "content": "We demonstrate how Graphy applies to literature surveys, with em- phasis on the online Surveyor. We will showcase the functionalities of Scrapper using a video as it is time-consuming.\nThe online Surveyor, shown in Fig. 3, allows the demo attendees to explore a pre-extracted paper network containing over 50,000 papers and 160,000 references. We first look into Fig. 3(a) that is the interface of Exploration featuring three primary canvases, metaphorically referred to as \"Past\", \"Present\", and \"Future\". Here, \"Past\" displays already explored papers, and \"Present\" shows the"}, {"title": "4 EXTENSION TO FINANCIAL SCENARIOS", "content": "We briefly discuss applying Graphy to two financial scenarios.\nCompany Relationship Analysis. In this scenario, each company is treated as a Fact node, and the data extracted by Inspection, such as revenues, main business areas and shareholder holdings ex- tracted from financial reports, are represented as Dimension nodes. The Navigation component establishes inter-company relation- ships by leveraging the financial or supply-chain dependencies mentioned in the reports. The generated graph can be used to identify comparable competitors, uncover hidden relationships, or assess contagion effects.\nFinancial News Analysis. In this scenario, each news article serves as a Fact node, while pertinent details, such as described events and stock performance indicators, can act as Dimension nodes. The Navigation builds connections among these Fact nodes by identifying shared symbols or overlapping financial metrics. This allows analysts to track the evolution of news stories, assess their market impact, or predict future trends based on historical patterns."}]}