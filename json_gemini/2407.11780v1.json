{"title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models", "authors": ["Xinbo Wu", "Max Hartman", "Vidhata Arjun Jayaraman", "Lav R. Varshney"], "abstract": "Large language models (LLMs) have exhibited impressive capabilities in various domains, particularly\nin general language understanding. However these models, trained on massive text data, may not be finely\noptimized for specific tasks triggered by instructions. Continual instruction tuning is crucial to adapt\nLLMs to evolving tasks and domains, ensuring their effectiveness and relevance across a wide range of\napplications. In the context of continual instruction tuning, where models are sequentially trained on\ndifferent tasks, catastrophic forgetting can occur, leading to performance degradation on previously learned\ntasks. This work addresses the catastrophic forgetting in continual instruction learning for LLMs through\na switching mechanism for routing computations to parameter-efficient tuned models. We demonstrate\nthe effectiveness of our method through experiments on continual instruction tuning of different natural\nlanguage generation tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across numerous domains, as\nhighlighted by OpenAI (2023) and Bubeck et al. (2023). However, whereas LLMs pre-trained on extensive\nlanguage data excel in general language understanding, they may not be optimized for every specific task of\ninterest prompted by instructions. Therefore, there is need for continual instruction learning to adapt LLMs\nto evolving tasks and domains. Indeed, continual instruction learning is essential for LLMs such as GPT\n(Radford et al., 2019) to maintain their effectiveness and relevance in handling a wide range of tasks and\ndomains.\nSuch models are trained on vast amounts of text data and fine-tuned for specific applications, often by\nlearning tasks sequentially (Luo et al., 2023), i.e. learning on datasets pertaining to one task all at once,\nbefore moving on to the next task. The challenge lies in their ability to continually learn and adapt as\nthey encounter new tasks and information. However, in continual instruction learning scenarios, where\nmodels are sequentially trained on different tasks or datasets, catastrophic forgetting occurs when the\nmodel's parameters are updated to accommodate new information, leading to degradation or complete loss\nof performance on previously learned tasks.\nA typical way to balance new learning with the retention of previously acquired capabilities in LLMs is\nthrough replaying old data. However, with the rapid iterations of LLMs for diverse and complex use cases,\nretaining old data becomes exceptionally challenging. Moreover, continually tuning an LLM with a large\nnumber of parameters is highly costly in terms of both computation and memory usage. Parameter-efficient\nfine-tuning (PEFT) such as low-rank adaptation (LoRA) (Hu et al., 2022) provides an option of lightweight\nwith portable parameters, which could be paired with an LLM to perform specific tasks. Therefore, in\nthis work, we focus on alleviating catastrophic forgetting during continual instruction tuning of LLMs,\nparticularly with minimal data retention and its interplay with PEFT.\nWe propose a novel continual instruction tuning method, SwitchCIT, that alleviates forgetting of\npreviously seen tasks by introducing a switch network to identify a task given an instruction, leveraging the\nclustering phenomenon of task-specific instruction vectors (Wu and Varshney, 2024). For each new task, we"}, {"title": "Related Work", "content": "Continual Learning and Catastrophic Forgetting. The study of continual learning focuses on developing\nalgorithms that learn from a continuous stream of data, enabling a model to acquire new knowledge while\nretaining previously learned information without catastrophic forgetting (Wang et al., 2024b). Catastrophic\nforgetting happens when LLMs forget previously learned information as new tasks are learned (Luo et al.,\n2023). Anonymous (2024) provides an insightful study empirically showing that pre-trained LLMs may\nforget domain knowledge and tasks that were not included in the fine-tuning process, while supervised\nfine-tuning offers substantial benefits to the models. To counter this effect, here we use a switch network to\nclassify tasks and route computations from their instructions. By doing so, we can fine-tune task performance\nby including extra parameters created by PEFT methods such as LoRA for each task.\nUnderstanding Transformers. Prior studies have offered insightful understandings of Transformer\nmodels with focuses on the internal representations (Wu and Varshney, 2024, 2023; Nanda et al., 2023) and\nattention mechanisms (Sun and Marasovi\u0107, 2021; Olsson et al., 2022). Inspired by Wu and Varshney (2024),\nwe design a novel method for continual instruction tuning for LLMs via switching instead of concentrating\non understanding of Transformers.\nInstruction Tuning. Instruction tuning is the process of tuning a model from specific instructions or\nprompts that will guide the model toward behaving in the desired fashion. A major issue with LLMs has\nbeen the mismatch between the training objective of the LLM and users' objectives. Instruction tuning has\nbeen developed, in part, to address this issue. This method of training aims to align language models with\nhuman intent (Ouyang et al., 2022; Stiennon et al., 2020; Zhang et al., 2023b). We concentrate our work on\nthe specific case of continual instruction tuning across different tasks, which presents unique challenges\nsuch as catastrophic forgetting.\nParameter-Efficient Fine-Tuning. PEFT addresses the challenge of needing enormous computing\nresources to fine-tune contemporary LLMs. PEFT reduces the number of fine-tuning parameters and memory\nusage while still achieving similar results as full fine-tuning (Xu et al., 2023). One particularly popular\nPEFT method is LoRA, which freezes the model weights of the pre-trained model and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, allowing training on a small number\nof additional parameters rather than on the original pre-trained model (Hu et al., 2019). Here, we use LoRA\nto create extra parameters for fine-tuning tasks not yet seen by the LLM.\nMixture of Experts. Mixture of experts (MoE) models integrate multiple sub-models, or experts, to\naddress different parts of the input space (Jacobs et al., 1991; Du et al., 2022; Zoph et al., 2022). Though\nthe MoE philosophy is similar to ours, SwitchCIT uses different models to handle different parts of a task\nspace represented by instructions, rather than an arbitrary input space as in MoE models. Also, we separate\nthe learning processes for model selection and the models themselves, whereas MoE models learn both\nsimultaneously. SwitchCIT can self-expand its parameters to adapt to new tasks, whereas MoE models\ntypically do not."}, {"title": "Method", "content": "We illustrate SwitchCIT in Figure 1. At inference time, a tuple [I, x] is given, where I is an instruction\nand x is an optional input. Based on the instruction I, a switch network routes the computation to a model\ntrained explicitly for the predicted task such that the performance of both previously learned and newly\nlearned tasks is mostly retained. More specifically, the switch network identifies tasks via a multi-class\nclassification from their instructions for the routing by using instruction features extracted by a lightweight\nLLM, $W_{small}$. We use the last token representation of an instruction from the final layer of $W_{small}$ as the\nfeatures. This design is inspired by the fact that vector representations of instructions belonging to the\nsame task are clustered together within the hidden representation space, with the task-specific clustering\nphenomenon becoming more pronounced in later layers (Wu and Varshney, 2024). Note that effective\nclustering implies good separability of task representations.\nA selected model relies on a concatenation of the instruction and the input, [I; x] to anticipate an output\ny via an internal representation h produced by a base LLM W and its task-specific weight $\\Delta$W. For brevity,\nwe omit details about the computations of h and of reaching y from h, which involves a causal decoding\nprocess; see Vaswani et al. (2017); Hu et al. (2022) for more details. Therefore, the switch network allows\ntasks to be handled by models dedicated to them. Models tailored to different tasks will not interfere with\none another, which consequently alleviates catastrophic forgetting of previously learned task. Here, both x\nand y could be considered as textual sequences in the context of language generation. All models $M_1$-$M_T$\nare instruction-tuned for different tasks (1 to T) by introducing extra parameters $\\Delta$W through a PEFT\nmethod such as LoRA. The switch network may be easily implemented as a multi-layer perceptron (MLP)\nmodel with an instruction feature encoder."}, {"title": "Experimental Setup", "content": "In this section, we briefly overview the model implementation and datasets. Experimental setups are further\ndetailed in Appendices B and C."}, {"title": "Switch Network", "content": "Table 1 presents the progressive performance of task classification by our switch networks trained under\ndifferent conditions: a low data rate-0.01% and 1% comparable to the replay data used by the rehearsal\nmethod of Scialom et al. (2022). Note that after learning each task, we retrain a very lightweight switch\nnetwork to accommodate the newly learned task. It is evident that switch networks of different settings\nachieve very high classification accuracy at every learning stage, even when using a lightweight LLM\nlike the OPT-125M for feature extraction. The performance is scaled up by using more training data.\nPerformance does not obviously degrade when including more tasks, demonstrating good robustness. Notice\nthat competitive classification performance is reached even with 100X less data, which may be explained by\ngood separability of different tasks due to task clustering shown by Wu and Varshney (2024)."}, {"title": "Continual Instruction Tuning", "content": "Table 2 demonstrates that SwitchCIT not only improves upon the performance of the base LLM (\"Initial\")\nbut also outperforms other methods on most tasks after the continual learning process under the same\nsettings. The gap becomes more pronounced when retaining only a very limited amount of data from\nprevious tasks (0.01%). Our method also surpasses the rehearsal method using the same replay data rate\n(1%) studied by Scialom et al. (2022). Additionally, we compare SwitchCIT (BLOOMZ-1.1B, 0.01%)\nrelying on PEFT to a much larger fully supervised fine-tuned approach based on the BLOOMZ-7.1b model\n(Luo et al., 2023; Muennighoff et al., 2023), which is approximately seven times larger than our base LLM.\nSurprisingly, our method still achieves better performance on most tasks, underscoring its effectiveness.\nIn contrast to the rehearsal method, our approach experiences notably less performance degradation when\nreducing replay data from 1% to 0.01%. We hypothesize that due to our high-performing switch network,\nour method is able to specifically select a tailored sub-model for a task, yielding impressive performance.\nWe calculate a relative gain, a normalized score using the performance achieved by a model when fine-\ntuned only on one specific task by following Scialom et al. (2022). A high relative gain indicates effective\nretention of performance on a specific task. From Figure 2, notice that SwitchCIT experiences minimal\ncatastrophic forgetting compared to other approaches, as shown by almost perfect retention of performance\nfor various tasks. The less perfect retention on the InqQG task is due to imperfect task identification by the\nswitch network. In contrast to our method, the rehearsal method retains only some performance on previous\ntasks. Along with SFT, they both exhibit noticeable levels of catastrophic forgetting as they continue to\nlearn additional tasks."}, {"title": "Efficiency and Portability", "content": "Many existing works overcome catastrophic forgetting by imposing constraints on the existing parameters\nof a model, so their model sizes will not change. SwitchCIT introduces new parameters for each additional\ntask. For example, when using BLOOMZ 1.1B as the base LLM, these additional parameters account for\nonly 0.878% of the total parameters. However, only extra parameters specific to a task are loaded during\ninference. Considering five continual tasks, the additional parameters amount to just 4.39% in exchange\nfor minimal catastrophic forgetting, demonstrating their lightweight and practical feasibility. Note that\nonly the additional parameters specific to a single task are loaded during inference. We anticipate further\nimprovements in these numbers as parameter-efficient methods continue to advance.\nSeparating the development of the switch network from the instruction-tuned models greatly enhances\nSwitchCIT's portability. For instance, to improve task identification by our switch network using more\ndata (from 0.01% to 1.0%, as shown in Table 1), we only need to retrain the switch network and plug it\nin the existing instruction-tuned models. Conversely, we can also use existing switch networks for better\ninstruction-tuned models as shown in Table 2, where we leverage the same switch network for models with\nlarger base LLMs such as BLOOMZ 7.1B."}, {"title": "Conclusion", "content": "We proposed a novel continual instruction-tuning approach to alleviate catastrophic forgetting by using a\nswitch network to identify tasks and then route computations to parameter-efficient tuned models. Exper-\niments conducted on five instruction-based continual natural language generation tasks demonstrate the\neffectiveness of our method compared to several baselines."}, {"title": "Limitations", "content": "Because of computational constraints, we could only tested our method on relatively small-scale LLMs.\nHowever, according to our design, our high-performing switch network is independent of the base LLM\nand can be paired with larger-scale LLMs that offer superior performance. The remarkable performance of\nour switch network showcases the effectiveness of our method. It is worth noting that our task classifier in\nthe switch network is incredibly lightweight (154K parameters) and requires minimal data (0.01% of the\ntraining data), making it highly practical and easy to integrate. The parameters introduced by LoRA account\nfor less than 2% of the total parameters of the base LLM, contributing to the overall lightweight nature of\nour method.\nOur current approach does not facilitate learning transferable knowledge across continual tasks. Explor-\ning methods to enable our model to leverage transferable knowledge across tasks will be an important future\ndirection for improvement."}, {"title": "Additional Related Works", "content": "Continual Learning and Catastrophic Forgetting. Continual learning methods are often classified into the\nfollowing categories: replay-based methods, regularization-based methods, and architecture-based methods\n(Wang et al., 2024a). Catastrophic forgetting arises when model parameters are updated to account for\nnew data, causing degradation or a complete loss of performance on previously learned tasks. Catastrophic\nforgetting is not specific to LLMs. Other neural networks also experience this phenomenon, leading to\nmethods such as Elastic Weight Consolidation (Kirkpatrick et al., 2017). Previous research to resolve this\nproblem has scaled the number of parameters in the model (Wang et al., 2024a; Yoon et al., 2018). It\nhas been demonstrated that these solutions work in theory but suffer from over-reliance on scaling LLM\nparameters. Works such as Hu et al. (2019) avoid this by splitting parameters into two sets: one for tasks\nlearned and one to dynamically be generated.\nInstruction Tuning. Models trained by instruction tuning have been shown to have better performance\non unseen tasks than without (Wei et al., 2022). Instruction tuning, however, has its challenges of crafting\nhigh-quality instructions that can cover the desired behavior, and it seems to only capture surface-level\npatterns rather than truly comprehending the task (Zhang et al., 2023b).\nParameter-Efficient Fine-Tuning. Since the original proposal of LoRA, there have been many deriva-\ntives that aim to resolve certain limitations of the original LoRA method, (Valipour et al., 2023; Zhang et al.,\n2023a; Dettmers et al., 2023; Huang et al., 2023). Notwithstanding, LoRA still seems to be the most widely\nused PEFT method."}, {"title": "Implementation Details", "content": "We implement our switch network via OPT-125M (Zhang et al., 2022) as a feature extractor and its classifier\nusing a two-layer MLP with ReLU activation function (Nair and Hinton, 2010). We present hyperparameters\nrelated to the switch network in Table 4 and to continual learned models in Table 5. We also present sizes of"}, {"title": "Continual Instruction Tuning Tasks", "content": "During instruction tuning, we follow Scialom et al. (2022) to start by adding a general prompt template at\nthe beginning of the data: 'Below is an instruction that describes a task, paired with an input that provides\nfurther context. Write a response that appropriately completes the request...'. This is then followed by a\nspecific prompt for each task. We present statistics of different task datasets in Table 3. In addition, we\nsearched online and did not discover any information regarding the data containing information that names\nor uniquely identifies individual people or offensive content."}]}