{"title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency", "authors": ["Sakib Shahriar", "Brady Lund", "Nishith Reddy Mannuru", "Muhammad Arbab Arshad", "Kadhim Hayawi", "Ravi Varma Kumar Bevara", "Aashrith Mannuru", "Laiba Batool"], "abstract": "As large language models (LLMs) continue to advance, evaluating their comprehensive capabilities becomes significant for their application in various fields. This research study comprehensively evaluates the language, vision, speech, and multimodal capabilities of GPT-4o. The study employs standardized exam questions, reasoning tasks, and translation assessments to assess the model's language capability. Additionally, GPT-4o's vision and speech capabilities are tested through image classification and object recognition tasks, as well as accent classification. The multimodal evaluation assesses the model's performance in integrating visual and linguistic data. Our findings reveal that GPT-4o demonstrates high accuracy and efficiency across multiple domains in language and reasoning capabilities, excelling in tasks that require few-shot learning. GPT-4o also provides notable improvements in multimodal tasks compared to its predecessors. However, the model shows variability and faces limitations in handling complex and ambiguous inputs, particularly in audio and vision capabilities. This paper highlights the need for more comprehensive benchmarks and robust evaluation frameworks, encompassing qualitative assessments involving human judgment as well as error analysis. Future work should focus on expanding datasets, investigating prompt-based assessment, and enhancing few-shot learning techniques to test the model's practical applicability and performance in real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "In the past few years, the emergence of large language models has led to paradigm shifts across various disciplines and professions. The pursuit of building and implementing the most powerful and accurate models has captured both researchers and industry. In late 2023 and early 2024, competitors to OpenAI, including Google and Anthropic, introduced advanced large language models: Google's Gemini and Anthropic's Claude 3 (Gemini Team et al., 2024; Korinek, 2023). These models surpassed the capabilities of the original GPT-3, GPT-3.5, and GPT-4 models that powered ChatGPT. To stay competitive, OpenAI needed to develop an upgraded model with more parameters, enhanced capabilities, and improved speed. This led to the launch of GPT-4 Omni (GPT-40) in May 2024.\nGPT-40 introduces several major innovations that improve upon previous large language models. The model includes a massive number of parameters \u2013 estimated to be well over one trillion which dwarfs GPT-3, at 175 billion parameters, and GPT-1, at an estimated 117 million parameters (Floridi & Chiriatti, 2020). The model is able to process and generate text, image, and audio content and does so at a speed that is much faster than competitor models. Importantly, the model also integrates improved handling of ambiguous and complex queries, where a misunderstanding could emerge between the user and the model, and enhances its ethical and safety protocols to mitigate the prevalence of harmful or incorrect outputs, as has been an issue with competitor models in recent months (Dillion et al., 2024; Ray, 2024). Though all these innovations appear to be a tremendous boon for the model, there are many areas where the efficacy of the model has not yet been formally evaluated."}, {"title": "1.1 Research Purpose", "content": "The purpose of this study is to comprehensively evaluate the capabilities of GPT-4 Omni (GPT-40) across various domains, including language, vision, speech, and multimodal tasks. By systematically assessing GPT-40's performance on a wide range of benchmarks and real-world tasks, we aim to understand its capabilities, strengths, and limitations. This evaluation will provide insights into the advancements made by GPT-40 compared to previous models, such as GPT-3 and GPT-4, and other contemporary models like Google's Gemini and Anthropic's Claude 3. These findings will contribute to ongoing investigations of the practical applications and future development of large language models."}, {"title": "1.2 Related Work", "content": "GPT-40 is the latest development in a string of innovations to generative pre-trained transformers in recent years. In order to situate the development of GPT-4o within the context of the greater developments occurring in artificial intelligence (AI), it may be helpful to view these technologies as a series of nested boxes, as in Figure 1. AI as a concept encompasses a wide range of developments, of which machine learning and deep learning are but one area (Ongsulee, 2017). Within deep learning, there are further divisions, with generative AI being only one (albeit major) area. The same is true for large language models, as one application of generative AI. We already know of other types of generative AI that are not language-based, such as image generators. The generative pre-trained transformer is but one large language model (LLM), developed by OpenAI. GPT-4o is the latest version of this model. As such, while GPT-4o is a very important innovation, it is but one element within the broad AI landscape that exists today.\nAs illustrated in Figure 1, GPT-40 belongs to the class of technologies known as large language models (LLMs). These models are notable for their ability to mimic human language usage so closely that it can be difficult for a human observer to distinguish between text generated by a human and that generated by a machine (Thirunavukarasu et al., 2023; Hayawi et al., 2024). This innovation marks a significant advancement towards passing the Turing test and underscores the practicality of AI in writing and research (Aher et al., 2023; Mannuru et al., 2023). However, it also introduces significant risks, including potential invasions of privacy and the generation of inaccurate, misleading, biased, or harmful information (Lund et al., 2023). Therefore, it is crucial to carefully evaluate these LLMs and scrutinize their outputs. Failure to do so could lead to the proliferation of misinformation and malicious content on the Internet (Hu et al., 2024).\nGiven the serious issues associated with some LLMs, it is essential to critically examine each new model for its limitations. Recent versions of GPT have shown significant improvements over their predecessors in various areas. For example, Koubaa (2023) found substantial improvements in GPT-4 compared to GPT-3.5 on tests such as the Graduate Record Examination (GRE), SAT, and Bar exam, with GPT-4's performance placing it in the top tenth percentile on most of these exams. Similarly, Coyne et al. (2023) reported improvements in grammatical error correction for GPT-4 compared to GPT-3.5. However, having more parameters in a model does not inherently guarantee better performance on all tasks. Overfitting can occur when a model is extensively trained on a large dataset but fails to generalize well to real-world data (Salman & Liu, 2019).\nEvaluation of the GPT-40 model is currently very limited. Research has explored various aspects of the model, including potential threats (Shen et al., 2024; Ying et al., 2024), diagnostic ability (Oura et al., 2024; Zhang et al., n.d.), and multilingual capabilities (Wang et al., 2024). One study by Sonoda et al. (2024) found that GPT-40 underperforms compared to Claude 3 Opus in radiology diagnosis tasks. Other studies investigated the sentiment of the general public regarding GPT-40 (Singgalen, 2024). Many studies focus on ChatGPT, the chatbot powered by GPT models, rather than the models themselves. These studies provide some additional insights into the quality of the models, such as their performance in English language teaching tasks (Pang et al., 2024). However, a comprehensive evaluation of GPT-40 itself remains a gap in the literature.\nGPT-40 lends itself to new forms of evaluation beyond the language and reasoning evaluation of past model versions due to its new capabilities in vision, speech, and cross-modal activities. GPT-4 with Vision (GPT-4V) was previously evaluated on vision tasks; however, it is clear from these studies that the model was not ready for the visual challenges to which it was exposed (Xu et al., 2024; Zhou et al., 2024). Meanwhile, the speech capacity of GPT-4o is a new innovation, one that has already been met with some criticism due to the choices for the voice of the model (Allyn, 2024). While cross-modal activities have been theorized in LLMs for some time, GPT-4V stands out as among the first models to actualize this potential, paving the way for its evaluation (Li et al., 2021)."}, {"title": "2. Language Capacity of GPT-4o", "content": "Language capacity is foundational to developing intelligent models capable of understanding, generating, and interacting with human language. This capacity encompasses a range of skills that enable models to process and produce coherent and contextually appropriate responses in natural language. The objective of this section is to comprehensively assess the language performance of GPT-40 (omni) by testing it on exams, reasoning tasks, and translation activities. Each of these tasks is significant for evaluating different aspects of the model's language capabilities."}, {"title": "2.1 Performance on Exams", "content": "In this subsection, we evaluate GPT-40's performance on various standardized and board exam questions. This helps us gauge the model's ability to comprehend complex problems and generate coherent, relevant, and accurate responses. Standardized exams are designed to measure a range of cognitive abilities and knowledge across different subjects. This task measures the model's proficiency in handling structured questions across various subjects. Our methods involve presenting GPT-4o with questions from a variety of standardized and board exams. The responses generated by GPT-40 are then analyzed based on the correctness of the answers provided."}, {"title": "Performance on USMLE", "content": "The United States Medical Licensing Examination (USMLE) Step 1 is a rigorous and comprehensive assessment designed to evaluate a candidate's understanding and ability to apply key concepts in medical science necessary for the practice of medicine (Federation of State Medical Boards & of Medical Examiners, 2024). Jointly developed by the Federation of State Medical Boards and the National Board of Medical Examiners, this examination serves as a milestone for medical students and professionals aiming to obtain their medical licensure in the United States. The USMLE Step 1 primarily focuses on testing the examinee's grasp of foundational medical knowledge and their ability to apply this knowledge to clinical scenarios. The sample test questions provided in the USMLE Step 1 Sample Items booklet encompass various disciplines, including anatomy, biochemistry, microbiology, pathology, pharmacology, physiology, and interdisciplinary areas such as genetics, immunology, and molecular biology. The dataset used for evaluating GPT-40's performance includes 119 sample test questions from the USMLE Step 1 booklet, updated as of January 2024.\nOut of the total 118 questions, GPT-40 correctly answered 98 questions. This corresponds to an accuracy of 83.1%. Compared to its predecessor, GPT-3.5, which achieved an accuracy of 51.67%, GPT-40 shows significant improvement. GPT-40, despite being designed for faster and more efficient tasks, offers a notable enhancement in language comprehension and problem-solving capabilities. However, GPT-4o's performance is slightly lower than that of GPT-4, which achieved an accuracy of 90.00%. This decline can be attributed to the design focus of GPT-40 on efficiency and speed, while GPT-4 remains the model for more complex and demanding tasks.\nThe results indicate that GPT-40 can serve as a valuable tool in medical education, offering fast, interactive learning experiences that are crucial for students needing immediate feedback and guidance (Haleem et al., 2022). While GPT-4 excels in handling more intricate questions, its slower response time may limit its practicality for real-time learning scenarios. Meanwhile, GPT-4o's accuracy and efficiency make it suitable for dynamic educational environments."}, {"title": "Performance on CFA", "content": "The Chartered Financial Analyst (CFA) Level 1 exam is a globally recognized certification offered by the CFA Institute, aimed at financial and investment professionals (CFA Institute, n.d.). The exam covers a broad range of topics, including ethical and professional standards, quantitative methods, economics, corporate finance, equity investments, fixed income, derivatives, and portfolio management. The CFA Level 1 exam is known for its rigorous and comprehensive assessment of a candidate's foundational knowledge and skills in finance and investment. It tests both theoretical understanding and the practical application of financial concepts and principles.\nFor this evaluation, we utilized the dataset from the 300Hours CFA Level 1 Mock Exam, which includes questions developed to mirror the style and difficulty of the actual exam\u00b2. GPT-40 correctly answered 76 out of the 89 questions, yielding an overall accuracy of 85.39%.  We compare the results obtained using zero-shot prompting since we did not provide the models with any hints or specific instructions during our prompting. The results indicate that GPT-40 noticeably outperforms both its predecessors. The increased accuracy of GPT-40 (despite being designed for faster and more efficient tasks) indicates that it can provide reliable and timely assistance for financial exam preparation."}, {"title": "Performance on SAT", "content": "The Scholastic Assessment Test (SAT) is a standardized test widely used for college admissions in the United States (College Board, n.d.). Developed and administered by the College Board, the SAT assesses a student's readiness for college and provides colleges with a common data point for comparing all applicants. The SAT covers areas including reading, writing and language, and mathematics, with an optional essay section. This test is designed to measure a range of skills necessary for academic success in college, including critical thinking, problem-solving, and analytical abilities.\nThe dataset used for evaluating GPT-40's performance consists of questions from the SAT Practice Test #1, which includes a variety of reading, writing, and math questions that reflect the format and content of the actual SAT exam\u00b3. The practice test consisted of two modules, each containing a reading and writing exam as well as a math exam. The performance on each module are outlined in Table 3.\nFor comparison with previous GPT models, we refer to the comprehensive report by the Open AI team (OpenAI et al., 2024). In this context, we average the results of M1 and M2 for GPT-4o, as summarized in Table 4.\nGPT-40 demonstrates the highest accuracy in the Reading & Writing section with 90.91%, surpassing all the older models. In the Math section, GPT-4o achieves a strong performance with 87.04%, slightly lower than GPT-4 but higher than the rest."}, {"title": "Performance on MBE", "content": "The Multistate Bar Examination (MBE) is a standardized test that assesses the ability of prospective lawyers to apply fundamental legal principles and reasoning to analyze given fact patterns (National Conference of Bar Examiners, n.d.). Developed and administered by the National Conference of Bar Examiners (NCBE), the MBE is a critical component of the bar examination in most U.S. jurisdictions (Griggs, 2019). The MBE includes 200 multiple-choice questions that cover a wide range of legal topics, including constitutional law, contracts, evidence, real property, and torts. The test evaluates the examinee's capacity to think like a lawyer and apply legal knowledge in a practical, problem-solving context.\nThe dataset for evaluating GPT-40's performance includes sample test questions from the MBE sample booklet, updated in 20234. These questions represent the types and formats of questions that examinees will encounter on the actual MBE, providing a comprehensive overview of the subjects tested and the skills required. In this test, GPT-4o correctly answered 15 out of 20 questions, leading to 75% accuracy.\nThe evaluation results indicate that GPT-4o performs comparably to GPT-4 on the MBE, with a minor difference in accuracy. However, compared to GPT-3.5, which achieved an accuracy of 45.10%, GPT-40 demonstrates a significant improvement. Therefore, law students and bar examinees can benefit from using GPT-40 as an interactive learning tool that provides immediate feedback and explanations, helping them to understand complex legal principles and improve their problem-solving skills."}, {"title": "2.2 Reasoning", "content": "Human intellect is remarkably characterized by reasoning, which is described as an activity of methodically and logically thinking about a subject (Huang & Chang, 2023). Reasoning enables humans to come to conclusions or make decisions by using previous experiences and data gathered, thus extending one's knowledge of the world and releasing the possibility for innovation and development. In recent times, AI has made significant advancements in narrowing the gap between human and machine intellect through the use of Natural Language Processing (NLP) and LLMs, which have established remarkable reasoning abilities.\nIn this section, the authors assess the reasoning capacity of the most recent GPT-40 model by manual technical evaluation through a sequence of question-answering tasks. The model will answer a range of logical reasoning tasks in different types, including deductive, inductive, and abductive reasoning, as shown in Figure 6. Starting with a broad principle or assumption and applying it to produce predictions or draw conclusions, deductive reasoning takes a top-down method (Johnson-Laird, 2010). By contrast, inductive reasoning uses a bottom-up methodology to deduce broad principles or conclusions from observations or data (Hayes et al., 2010). In abductive reasoning, theories or explanations are developed from little, ambiguous, or incomplete data (Walton, 2014). With all these assessments on the model, this article stands to obtain an understanding of the reasoning capacity of the GPT-40 model in various settings.\nIn this subsection, we assess the performance of GPT-4o on five datasets that include all the aforementioned types of reasoning, as illustrated in Figure 7. To evaluate the deductive reasoning ability, two datasets were utilized namely EntailmentBank (Dalvi et al., 2021) and bAbI (task 15) (Weston et al., 2015). Similarly, to assess the capability of inductive reasoning, we employed two datasets CLUTRR (Sinha et al., 2019) and bAbI (task 16) (Weston et al., 2015). For abductive reasoning, we use the aNLI dataset (Bhagavatula et al., 2019). Adhering to the methods of L\u00f3pez Espejel et al. (2023), our evaluation encompassed the same set of 30 randomly chosen samples from each evaluation dataset. The observations are selected from 10 samples from each of the training-easy, train-medium, and train-hard sets of the aNLI dataset. Concurrently, a total of 30 samples are drawn from the test set for the bAbI (task 15), bAbI (task 16), CLUTRR, and EntailmentBank datasets. In accordance with L\u00f3pez Espejel et al. (2023), we utilized the identical set of proven and effective prompts that were implemented in their assessment to evaluate the capabilities of the model.\nThe evaluation results showcase the remarkable reasoning abilities of GPT-40 in all three domains. GPT-4o demonstrated exceptional performance in deductive reasoning by achieving nearly flawless scores on both bAbI (task 15) and EntailmentBank. It outperformed ChatGPT-3.5 and performed at the same level as ChatGPT-4 (L\u00f3pez Espejel et al., 2023). GPT-4o achieved flawless results in the inductive reasoning tasks, scoring perfectly on bAbI (task 16) and achieving a score of 17 out of 30 on CLUTRR. It outperformed both ChatGPT-3.5 and ChatGPT-4. GPT-4o achieved a score of 27 out of 30 on aNLI, surpassing the performance of its previous versions in abductive reasoning. The results underscore GPT-4o's superior reasoning capabilities compared to its predecessors. The model's proficiency in deductive reasoning showcases its ability to derive valid conclusions from premises. Its success in inductive reasoning demonstrates the capacity to generalize from specific facts, while its performance in abductive reasoning highlights the ability to generate credible hypotheses with limited knowledge.\nEven with the remarkable reasoning powers of GPT-40, this assessment points to a few drawbacks that need more rigorous investigation. One case was when the model gave different answers to the same topic in various chat sessions while evaluating the bAbI (task 16) dataset for inductive reasoning. Additionally, the model sometimes requested the end-user to choose between different answers. This implies that in some situations, GPT-40 could have trouble with ambiguity, resulting in varying responses. Furthermore, the model sometimes gave different responses when the same subject was posed repeatedly in the same chat session. In determining accuracy, only the first response was considered to maintain synchrony, although this inconsistency raises questions about the model's efficiency and dependability in certain areas. The model's sensitivity to question-wording, the information presentation sequence, or the existence of unclear or contradictory information in the input may be the causes of these issues. To overcome these problems and enhance its capacity to manage ambiguity and resolve contradictions, future studies should concentrate on creating more reliable and consistent reasoning mechanisms and optimizing prompts for LLMs.\nWith the notable performance of GPT-40, there might be more advancements for many AI applications with GPT-4o's improved reasoning skills. High-accuracy complex reasoning tasks performed by it can lead to advancements in information retrieval, decision support tools, and question-answering systems. Still, further study is required to determine how well the model performs on a larger variety of reasoning problems and how well it can manage more intricate and domain-specific reasoning situations."}, {"title": "2.3 Language Translation", "content": "Language translation has become an increasingly important task in our globalized world, facilitating communication and understanding across diverse linguistic backgrounds. With the advent of LLMs like GPT 3.5, GPT 4, Llama, Gemini and now GPT-4o, the potential for accurate and efficient machine translation has grown significantly (Khoshafah, 2023). These models, which were trained on massive volumes of multilingual data, can produce translations accurately while capturing the subtle meanings and complexities of different languages. Therefore, in this section, we aim to evaluate the translation proficiency of GPT-4o in six of the most widely spoken languages: Spanish, Arabic, Hindi, French, Portuguese, and Russian.\nThe choice of these six languages is not arbitrary; they represent a diverse set of linguistic structures and cultural contexts, making them ideal for a comprehensive evaluation of translation capabilities. Spanish is commonly used across Europe and the Americas and is characterized by its straightforward structure and rich vocabulary. Arabic, known for its intricate script and complex word forms, poses distinct challenges for translation technology. Hindi, widely spoken in India, mixes local and foreign words, requiring careful handling to achieve accurate translation. French, spoken in many parts of the world, helps test the model's ability to handle grammatical rules and nuances. Portuguese, similar to Spanish but distinct in several key aspects, allows for an assessment of the model's precision in closely related languages. Lastly, Russian, with its Cyrillic script and case system, provides a test for the model's ability to manage non-Latin scripts and complex grammatical structures.\nBy focusing on these languages, this study aims to provide a robust and diverse evaluation of GPT-40's translation performance. Given the widespread use and significant number of native speakers of these languages, improvements in translation accuracy can have a substantial impact on global communication and information dissemination. Hence in this section, we seek to verify GPT-40's ability to translate across these six languages, providing insights into its potential for breaking down language barriers and facilitating communication among people from different linguistic backgrounds."}, {"title": "Data", "content": "The datasets for Spanish, Arabic, French, Portuguese, and Russian were sourced from the OPUS dataset, a well-known collection of texts used for training and evaluating machine translation models (Tiedemann, 2012), and the Hindi dataset was obtained from the IIT Bombay English-Hindi Parallel Corpus, created by the Center for Indian Language Technology (CFILT) at IIT Bombay (Kunchukuttan et al., 2018).\nFor this analysis, 500 data points were randomly sampled from each dataset. The selection of 500 data points is a good balance between feasibility and the need for sufficient data diversity. This sample size is large enough to encompass a wide variety of sentence structures, vocabulary, and translation challenges present in each language, ensuring that the evaluation is comprehensive and representative. Random sampling was employed to mitigate selection bias and to ensure that the sampled data points provide an unbiased representation of the overall dataset. By using random sampling, this approach captures the natural variability and complexity of language, which is essential for a robust assessment of the GPT-40 model's translation performance across different linguistic contexts."}, {"title": "Evaluation Method", "content": "To measure how similar two sentences are in terms of their meaning, an advanced NLP, specifically focusing on sentence embeddings generated by a model called BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and a similarity measure called cosine similarity has been used. BERT is a powerful model that has greatly improved how well computers understand language. For our research, we use a pre-trained model from the sentence-transformers library called paraphrase-MiniLM-L6-v2. This model is specially tuned to understand the meanings and similarities between sentences. It works by turning each sentence into a vector, which is a list of numbers. These vectors or embeddings, encapsulate the semantic information of the sentences in a way that allows for meaningful comparison between the actual translations and the translations generated by GPT 40.\nTo find out how similar two sentences are, we compare their vectors using cosine similarity. Cosine similarity measures the angle between two vectors or embeddings. If the vectors point in the same direction, the sentences are very similar. If they point in completely different directions, the sentences are very different. The values are between -1 and 1, where:\n\u2022 1 indicates that the vectors are identical.\n\u2022 0 indicates that the vectors are orthogonal (i.e., no similarity).\n\u2022 -1 indicates that the vectors are opposed.\nBy calculating the cosine similarity between the embeddings of two sentences, we can effectively measure their semantic similarity. The formula for cosine similarity is:\nCosine Similarity = A \u00b7B / ||A|| ||B|| where A and B are the embeddings of the two sentences."}, {"title": "Results", "content": "This study sought to evaluate the capabilities of GPT-4o in translating passages across six major languages: Spanish, Arabic, Hindi, French, Portuguese, and Russian. The results reveal a generally high level of translation accuracy, particularly in Spanish and Portuguese, which scored 88% and 86% respectively. However, there were notable variations among the languages. Arabic and French, with scores of 78% and 75%, respectively, presented more challenges for the model due to their complex linguistic structures and nuances. Hindi and Russian scored 82% and 80%, demonstrating the model's competence but also highlighting areas for improvement.\nThe findings suggest that the line between human and machine translation is becoming increasingly narrow. GPT-40's performance, though not specifically optimized for translation, approaches the quality of dedicated translation systems. This is particularly noteworthy given the diverse linguistic and structural characteristics of the evaluated languages. While the exact nature of the source translations in the datasets (whether human or machine-translated) is not confirmed, the high similarity scores indicate that GPT-40 is capable of producing translations with a quality that is comparable to the existing translations. However, several limitations must be considered. The random sampling of 500 data points from each dataset may not fully capture the linguistic diversity and complexity of each language. Different samples could yield varying results, suggesting that a larger and more representative dataset might provide a more accurate assessment. Additionally, the reliance on BERT-based embeddings and cosine similarity may not fully encapsulate the nuances of translation quality, particularly in capturing cultural and contextual subtleties. Expanding the dataset size and including more language pairs could yield more comprehensive insights. This research serves as a proof-of-concept for larger-scale studies that could further investigate the capabilities of AI in translation. Future research should focus on incorporating more extensive data, diverse language combinations, and advanced fine-tuning techniques."}, {"title": "3. Vision Capacity of GPT-40", "content": "Vision capacity is foundational to developing intelligent models capable of understanding, interpreting, and interacting with visual content. This capacity encompasses a range of skills that enable models to process and produce coherent and contextually appropriate responses to visual inputs. The objective is to comprehensively assess the vision performance of GPT-40 by testing it on various image-based tasks. Each of these tasks is significant for evaluating different aspects of the model's visual capabilities.\nFor each task, a dataset of approximately 100 representative images was curated. The model was provided with an image along with a text prompt specifying the desired output format. The prompts were designed to probe the model's ability to identify, classify, describe, and analyze visual content without additional context. For select tasks, we further investigated the model's few-shot learning capabilities by providing a small number of labeled examples before the query image.\nModel outputs were compared against ground truth labels to compute standard performance metrics such as accuracy. Qualitative analysis was also conducted on a subset of responses to identify common failure modes and strengths. The results across different tasks provide insights into GPT-40's current visual understanding capabilities, areas for improvement, and potential as a foundation model for vision tasks. Subsequent sections discuss the specifics of each task, dataset, and findings, offering a comprehensive evaluation of GPT-4o's visual reasoning skills."}, {"title": "3.1 Fruits Classification", "content": "Fruit image classification is crucial for applications in agriculture, supply chain management, and food industry automation. Accurate identification of fruit types can enhance inventory tracking, quality control, and efficient sorting processes (Cubero et al., 2011). The fruit images datasets consists of approximately 400 images spanning 10 different fruit classes such as banana, jackfruit, and mango. Each fruit class has 40 labeled images, with the dataset split into 320 training images and 80 test images. The images were collected from various sources, such as Google Images and stock image websites, and were labeled by the dataset creators. For this evaluation, the model was provided with an image along with a prompt to identify the fruit class from the list of 10 classes in a specified format. Model predictions were compared against ground truth labels to assess performance.\nThe results indicate that GPT-40 performed exceptionally well on this task. The model achieved an average precision of 0.98, an average recall of 0.98, and an average F1-score of 0.98. These metrics suggest that GPT-4o is highly capable of accurately identifying and classifying different fruit images."}, {"title": "3.2 Driver Drowsiness Detection", "content": "Detecting driver drowsiness is critical for enhancing road safety, as timely identification of fatigue can prevent accidents and save lives. The drowsy detection dataset consists of images extracted from videos capturing drivers in three distinct states: natural, fatigued, and drowsy (Jebraeily et al., 2024). The dataset was curated by gathering relevant videos, converting them into image frames, and applying facial detection algorithms to isolate key facial regions like eyes, mouth, and cheeks, which are indicative of drowsiness. The extracted images were converted to grayscale, resized to 48x48 pixels, and accurately labeled based on the driver's state. The dataset comprises two classes: drowsy and natural, with a total of 100 labeled images sampled evenly from each class. For this evaluation, GPT-4o was provided with an image along with a prompt to classify it into one of the two classes in a specified JSON format. The model's predictions were compared against the ground truth labels to assess its performance in detecting driver drowsiness from facial features.\nIn this task, the model achieved an average precision of 0.80, an average recall of 0.80, and an average F1-score of 0.80.\nThe results indicate that GPT-40, without fine-tuning, achieves an impressive precision, recall, and F1-score of 0.8. While lower than that of specialized deep learning models like VGG, ResNet, and CNN (Jebraeily et al., 2024), the performance is impressive given GPT-40's lack of training on this specific dataset. The notable performance despite no domain-specific training underscores its robustness and adaptability, implying that GPT-40 could be valuable in scenarios where rapid deployment and flexibility across different tasks are crucial."}, {"title": "3.3 Crop Disease Classification", "content": "Accurate identification of crop diseases is essential for ensuring agricultural productivity and preventing significant crop losses. The crop disease classification dataset is a comprehensive collection of images aimed at evaluating GPT-40's capabilities in identifying crop diseases. The dataset encompasses 20 distinct classes of common crop diseases, including blight, cedar apple rust, crown gall, and clubroot. For this evaluation, 100 images were randomly sampled from the dataset, with each class represented by approximately five images. GPT-40 was provided with these images along with a prompt to classify the crop disease depicted in each image. The model's predictions were compared against the ground truth labels to assess its performance in accurately identifying and distinguishing various crop diseases based solely on visual information.\nThe model achieved an average precision of 0.77, an average recall of 0.71, and an average F1-score of 0.68 in this task.\nGiven the large number of classes (20), this highlights GPT-40's potential for accurate crop disease classification and adaptability, despite no prior training on this dataset. The limitations in specific classes like Botrytis, Brown rot, and Canker can be attributed to the need for specialized training in certain classes."}, {"title": "3.4 Glaucoma Detection", "content": "Early detection of glaucoma is critical for preventing vision loss and ensuring timely treatment. The glaucoma detection dataset used for this evaluation consisted of retinal fundus images from the ACRIMA database\". A subset of 100 images was sampled, evenly split between glaucomatous and normal cases. These images were collected at FISABIO Oftalmolog\u00eda M\u00e9dica in Valencia, Spain, and were annotated by experienced glaucoma experts. GPT-4o was tasked with classifying each image into either glaucoma or normal based solely on the visual information provided. The model's predictions were compared against the expert-annotated ground truth labels to assess its performance in detecting glaucoma from retinal fundus imagery.\nAs shown in Table 11, GPT-4o achieved an average precision of 0.65, an average recall of 0.62, and an average F1-score of 0.59. For the glaucoma class, the model demonstrated a precision of 0.58, a recall of 0.86, and an F1-score of 0.69. In contrast, the normal class had a higher precision of 0.73 but a significantly lower recall of 0.38, resulting in an F1-score of 0.50.\nThe confusion matrix in Figure 11 reveals that the model correctly identified 42 out of 49 glaucoma cases but struggled more with normal cases, correctly classifying only 19 out of 50. The plot shows the model's relatively balanced precision and recall for the glaucoma class but highlights a pronounced discrepancy for the normal class, with precision substantially higher than recall. GPT-4o is effective at identifying glaucomatous images but has difficulty in correctly classifying normal cases.\nFew-shot learning allows models to make accurate predictions with only a small number of training examples. This approach is particularly beneficial in scenarios where data is scarce. Figure 12 illustrates the F1 scores for both classes across different numbers of shots, indicating how the model's performance evolves with the number of examples provided during training. The glaucoma class maintains a relatively high F1 score across all shot levels, showing slight improvement with additional examples. This consistency suggests that GPT-40 effectively learns to identify glaucomatous features even with a limited number of examples. In contrast, the normal class exhibits significant improvement in F1 score from zero shots to one shot but then plateaus. This indicates that while the initial provision of examples significantly enhances the model's ability to recognize normal cases, further increases in the number of examples yield diminishing returns.\""}]}