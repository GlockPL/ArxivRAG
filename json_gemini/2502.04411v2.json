{"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing", "authors": ["Kunfeng Lai", "Zhenheng Tang", "Xinglin Pan", "Peijie Dong", "Xiang Liu", "Haolan Chen", "Li Shen", "Bo Li", "Xiaowen Chu"], "abstract": "Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.", "sections": [{"title": "1. Introduction", "content": "Finetuning Large Language Models (LLMs) enables them to adapt to downstream applications including sentiment analysis (Sun et al., 2023), text summarization (Fang et al., 2024), mathematical reasoning (Ruis et al., 2024), code writing (Jiang et al., 2024b), roleplay chatting (Chen et al., 2025)so on. Open-source platforms such as Huggingface (Wolf et al., 2019) and torchvision (Marcel & Rodriguez, 2010) facilitate access to a diverse array of highly trained expert models with varying capabilities. Considering the computational resources are scarce and implementing green computing (Samsi et al., 2023; You et al., 2022; Stojkovic et al., 2024; Bai et al., 2024), the community is increasingly interested in how to merge these models to create a superior LLM that retains the strengths of finetuned ones without retraining (Yang et al., 2024b; Lu et al., 2024b; Du et al., 2024; Yadav et al., 2023b).\nOne predominant merging strategy is model averaging (Yang et al., 2024b; Matena & Raffel, 2022; Thennal et al., 2024; Yu et al., 2024b), which computes weighted averages of parameters to synthesize collective knowledge (Matena & Raffel, 2022; Yadav et al., 2023b). However, model averaging faces challenges from parameter conflicts arising from diverse finetuning tasks, leading to performance degradation as shown in Figure 1. Another direction is model routing (Lu et al., 2024b; Muqeeth et al., 2024; Yang et al., 2024c; Du et al., 2024; Lu et al., 2024a; He et al., 2024a; Wei et al., 2024a; Chen et al., 2024), which aggregates models and performs model selection during inference. This method avoids parameter conflicts but incurs significant computing and storage (system) costs due to maintaining all finetuned models. This motivates us to rethink the following questions:\nHow to better merge common and unique knowledge from various finetuned models while simultaneously avoiding parameter conflicts and minimizing system costs?"}, {"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing", "content": "To answer this question, we firstly quantify the conflicts between finetuned LLMs. We employ sign consistency between different task arithmetics that are difference between the finetuned LLM and the original LLM to measure the conflicts. We find that the front and last layers tend to exhibit the highest levels of conflict, suggesting that these layers are particularly sensitive to averaging. In contrast, the central layers demonstrate comparatively lower levels of conflict, indicating that they retain more common knowledge.\nThen, we introduce Mediator as an adaptive model merging framework to enhance LLM merging with little storage and computation costs. Inspired by the varying degrees of layer-wise parameter conflicts, we propose adaptive merging that averages layers with lower conflict levels, thereby capturing the common knowledge (Yadav et al., 2023b; He et al., 2024b) shared among LLMs while minimizing conflicts (Yadav et al., 2023b). Concurrently, layers with significant conflicts are regarded as experts to be routed during inference, preserving unique task-specific knowledge without dilution (Yadav et al., 2023b; He et al., 2024b).\nWhile direct compression of finetuned LLMs results in significant information loss (Dong et al.; Sun et al., 2024), we leverage both layer-wise model merging and the high sparsity of task arithmetics (Yadav et al., 2023b) to decompose models into base and task-specific components (Ilharco et al., 2023; He et al., 2024b; Yang et al., 2024c; Tang et al., 2024b). By integrating these two techniques, our approach reduces storage from 50% to 7% with minimal accuracy loss while preserving layer-specific knowledge.\nObserving that LLMs are finetuned on the complete sentences of their downstream tasks instead of the splited sub-sequences, to better preserve task-specific knowledge and improve overall model performance, we propose task-level expert routing instead of token-level routing (Lepikhin et al., 2020; Sukhbaatar et al., 2024a; Zhou et al., 2022; Jiang et al., 2024a). With these designs, our merged LLM achieves high efficiency with minimal performance degradation (0.06% ~ 0.3%). Our evaluations show that we can effectively run a model comparable to a 7B \u00d7 4 LLM ensemble on a single RTX 4090 GPU, making high-performance LLM more accessible in resource-constrained environments(Appendix G.3).\nConsidering the out-of-distribution (OOD) samples, we select and merge appropriate experts based on the task uncertainty of the input data. Thus, the unified model can select appropriate experts based on the characteristics of the input data. For example, the input may incorporate both codes and mathematical reasoning, then the unified model can select the experts that are trained on both codes and mathematical reasoning.\nWe finetune pretrained LLMs with Chain-of-thoughts (Weiet al., 2022) enhanced datasets, showing that the model merge of Mediator can successfully preserve the reasoning ability (Zelikman et al., 2022; Kojima et al., 2022; Guo et al., 2025). As far as we know, we are the first to conduct cutting-edge LLM merging based on the finetuning with CoT enhanced downstream tasks. Our main contributions can be summarized as follows:\n\u2022 We investigate and demonstrate that different layers of fine-tuned models exhibit varying levels of parameter conflicts (Section 3). Then we propose Mediator, an adaptive layer-wise model merging approach to average layers with minimal conflicts and use task-level expert routing for layers with significant conflicts (Section 4.1).\n\u2022 We propose a method in Mediator to decouple fine-tuned experts into one dense expert and several sparse experts (Section 4.2), achieving high compression ratio while maintaining accuracy. Our approach enables dynamic expert selection based on task uncertainty (Section 4.3), effectively handling OOD data.\n\u2022 We conduct experiments based on the modern LLMs including LLaMA and Qwen with CoT enhanced finetuning and the real-world cutting-edge LLM evaluation tasks. Results show that our method achieves significant performance improvements and less system cost compared to existing methods. (Section 5)."}, {"title": "2. Preliminary and Related Works", "content": "2.1. Language Modeling and LLM Finetuning\nTask Data Distribution. Given a set of different downstream tasks T, based on the sampling task $T\\in T$, the pretraining document (data sample) is a sequence $X_{1:T}$ of tokens with the maximum length T generated from a distribution $p_\\tau = p(x_{1:T}|\\tau) = p(o_1,..., o_T|\\tau)$ (Xie et al., 2022; Wies et al., 2023; Hahn & Goyal, 2023; Li et al., 2024b). And we define the pretraining data is sampled from $p(x|T^*) = \\int_{\\tau^*\\in T^*}p(o_1,..., o_T|t)p(t^*)d\\tau^*$. Each token o is sampled from a vocabulary $\\Omega$. And both (T and T* belong to a large task family $\\Omega$, i.e. $T, T^* \\subset \\Omega$.\nLanguage Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Xie et al., 2022) usually utilize the next word prediction as the language modelling, which predicts the next token $x_t$ given the previous tokens $X_{1:t-1}$ for all $t = 1,..., T$. Formally, a LLM parameterized by $\\theta$ is a distribution $f_\\theta(x_t|x_{1:t-1})$. And it is pretrained on a huge corpus sampled from the pretraining distribution $p(x|T^*)$ (Xie et al., 2022).\nFinetuning LLM. Normally, for each downstream task $\\tau \\in T$, finetuning LLM is to minimize the cross-entropy loss"}, {"title": "3. Understanding Conflict between LLMs", "content": "Preliminary Experiments. We finetune Llama-3.2-3B on three datasets and evaluation with according tasks (details in Section 5). Table 1 shows the performance of the merged model and the individual finetuned models. We use the $P(\\theta, \\tau)$ to represent the performance of the $\\theta$ on $\\tau$.\nComparing Performance on All Tasks. We write $P_{ORI} = P(\\theta, T)$ as the performance of the original model $\\theta$ on all tasks T, the $P_{AVG} = P(\\theta_{AVG}, T)$ as the performance of the averaged model $\\theta_{AVG}$ on all tasks T. We write the $P_{SEL} = -\\sum_{\\tau \\in T} P(\\theta_{SEL}, \\tau)$ as the performance of the selection based model $\\theta_{SEL}$ on all tasks T. The results show that the\n$P_{ORI} < P_{AVG} < P_{SEL},$\nwhich means that finetuning and averaging can successfully increase the model performance. However, simply averaged model cannot recover the task performance of the corresponding finetuned models.\nComparing Performance on Individual Models. Table 1 shows that while the model $\\theta_\\tau$ has the best performance on its according task $\\tau$, its performance on other tasks $T \\backslash \\tau$ is lower than $\\theta_{AVG}$. This indicates that the averaged model can still benefit from merging knowledge from different finetuned models. Thus, a better merging strategy should be able to average the parameters that have less parameter conflict to find the common knowledge that can be shared across different tasks and avoid the parameter conflict that degrades the performance of the finetuned model.\nDefinition 3.1 (Task Arithmetic). A task arithmetic on task $\\tau$ is the parameter difference between the finetuned LLM $\\theta_\\tau$ and the pre-trained LLM $\\theta$, i.e., $\\Delta_\\tau = \\theta_\\tau - \\theta$."}, {"title": "4. The Design of Mediator", "content": "The high-level idea of Mediator is to hybridly combine averaging and routing to preserve the downstream knowledge and avoid the parameter conflict (Sectino 4.1) according to parameter conflict distribution across different layers. We decompose the downstream models as the base model and experts of task arithmetics, which brings into the opportu- nity to sparsify the task arithmetics to prune out the noisy parameters to reduce the memory costs (Section 4.2). Lastly, considering the OOD inputs, we propose a Bayesian expert routing (Section 4.3) to better combine knowledge from different experts.\n4.1. Adaptive Layer-wise Model Averaging and Routing\nInspired by the empirical observation in Figure 2, Mediator averages layers with less parameter conflicts, and route layers with more parameter conflicts. As shown in Figure 3, Mediator calculates the conflicts $d_l$ across different layers. Then, Mediator models the layer-wise conflicts as a Gaussian distribution $d_l \\sim N(\\mu, \\sigma)$.\nThen, for each layer index l, Mediator average layer parameters if the conflict $d_l$ is less than the $\\mu +\\sigma$, otherwise routing this layer. We denote the averaged layer parameters as $\\phi_{AVG}$ and the routing layer parameters as $\\phi_{up}$. Algorithm 1 shows this detailed process. The average operation $\\mathcal{M}$ can be any averaging operation, such as the unified average, importance based (Matena & Raffel, 2022), or subspace based (Yadav et al., 2023b). In our experiments, we mainly use the de-noised parameters $\\widetilde{\\theta}$, defined in Section 3 to conduct averaging like TIES (Yadav et al., 2023b) (details of averaging operations in Appendix D.3). Note that all attention layers are averaged, because they are found to save non-specific domain knowledge (Sukhbaatar et al., 2024a).\n4.2. Expert Decomposition\nThe routing layer occupies $n_\\tau \\times M_l$ memory, where $n_\\tau = |T|$ and $M_l$ is the memory of each layer l in original model. Large $n_\\tau$, significanly increases the memory cost of routing layers, thus leading to weak scalability. Thus, we consider compressing the routing layers to reduce the memory cost.\nHowever, previous LLM pruning or quantization (Dong et al.; Sun et al., 2024) on $\\{\\theta_\\tau\\}$ cannot achieve high compres- sion ratio on $\\theta_\\tau$. Different from directly compressing the"}, {"title": "5. Experiments", "content": "Models and Evaluation Tasks. We conduct comprehensive experiments on cutting-edge LLMs including Qwen-1.5-4B, Qwen-2.5-7B (Yang et al., 2024a), LLaMA-3.1-3B, and LLaMA-3.2-8B (Dubey et al., 2024). We select different evaluation tasks to effectively demonstrate model capability in resolving parameter conflicts during model merging, including GSM8K of mathematical question-answering (Cobbe et al., 2021), TriviaQA (Joshi et al., 2017) of a large-scale Wikipedia-based question answering dataset, HumanEval (Chen et al., 2021) of Python programming tasks, WinoGrande (Sakaguchi et al., 2019) of logical reasoning, MMLU (Hendrycks et al., 2021) of vertical domain knowledge (as OOD to the finetuned models).\nFinetuning Settings. The finetuning datasets are constructed by augmenting some publicly datasets (task related but without overlap) with GPT-40 (Gilardi et al., 2023) and Chain-of-Thoughts (Wei et al., 2022). For each finetuning process, we use at least 180K training samples to ensure sufficient performance improvement on the corresponding task, which helps validate the effectiveness of our experiments (Details of constructing finetuning datasets in Appendix F and hyperparameters in Appendix C). To the best of our knowledge, this is the first LLM merging study with CoT enhanced finetuning and evaluated with generative tasks.\nBaselines. We compare pretrained, finetuned models, and the state-of-the-art static and dynamic merging methods with Mediator. The static merging methods include TIES (Yadav et al., 2023a) and PCB-merging (Du et al., 2024) achieve the best performance in weighted average"}, {"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing", "content": "method and do not require calibration data, and also partly consider OOD evaluation tasks. The dynamic merging methods include BTX (Sukhbaatar et al., 2024c) with token-level routing and the twin-merge (Lu et al., 2024a) with task-level routing and SVD decomposition (Details of hyperparameters and optimization of these baselines in Appendix C).\n5.1. Main Results\nFine-grained Comparison on All Tasks. Table 2 and 3 show the fine-grained performance on each tasks and their overall averaged one of different methods and algorithms. In most of time, the finetuned LLM can achieve the best performance across all single and merged models on its specialized domain, like Math finetuned models on GSM8K and Code finetuned models on HumanEval. While the merged LLMs can generally outperform single models on the averaged performance, their specialized domain performance is weaker. However, Mediator can catch up the domain performance of the specialized models, and almost always outperform other merged models. Also, the overall performance on all tasks of Mediator is consistently better than other methods. Expert routing methods includes BTX and Mediator are likely to further improve performance. This aligns with findings from TIES (Yadav et al., 2023a) and Twin-merging (Lu et al., 2024b). As model scale increases, the improvement margins of all merging algorithms decrease, which may be attributed to the enhanced comprehensive capabilities of individual finetuned models (or experts). We provide more fine-grained results about QWEN-1.5 and 2.5 in Table 25 and 27 in Appendix G.\nOverall Comparison. As shown in Table 4, the advantages of PCB over TIES become less pronounced at larger model scales, and even shows performance degradation on Qwen-7B, which demonstrates PCB's instability. Dynamic routing approaches include BTX and Mediator show stable performance improvements. Our method demonstrates consistent improvements across different models.\nPost-Training Time After Merging. As many model merging methods like Twin, PCB and BTX require post-training, it is critical to compare the extra training time. Table 5 shows the post-training time of different methods. PCB merging require weight exploration thus leads to higher time. The BTX with token-level routing needs to completely train the layer-wise routers for each token, thus, the post-training"}, {"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing", "content": "time of them is significantly high. In contrast, for task-level routing approaches like Twin-merging and Mediator, taking the lowest time.\nInference Time. Table 6 shows the inference time of different methods. As the token-level routing methods need to load and compute the layer-wise routers for each token, the inference time of them is significantly higher (more than 2x) than our method. We have detailed our inference acceleration process in Section 4.4 and Appendix E.\nMemory Cost. Table 7 shows the memory costs of different methods. Our method significantly reduces the memory costs compared to saving all finetuned models because there is only one router for all experts, and the sparsified experts saving, and the layer-wise merging strategy (Details of formally comparing memory costs of merging methods in Appendix A).\n5.2. Ablation Studies\nScalability of Finetuned Models. To verify the scalability of Mediator, we finetune another 4 LLMs according to the following 4 extra evaluation tasks including: (1) Instruction Following with IFEval. (Zhou et al., 2023) which assess models ability to accurately interpret and execute natural language instructions; (2-4) Medicine, College Economics and Law from CEval. (Huang et al., 2023b) which assess knowledge and capabilities across various academic and professional domains. We utilize four accordingly domain datasets for finetuning including Magpie (Xu et al., 2024), Industry Instruction (Ind), DISC-Med (Bao et al., 2023), DISC-Law (Yue et al., 2023a) without overlap with IFEval and CEval (Details in Appendix F)."}, {"title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing", "content": "6. Limitations\nParameter Conflict Theory. Our empirical analysis in Section 3 provides initial insights, but a theoretical foundation for parameter conflicts remains unexplored.\nMemory Efficiency. Despite reducing memory use versus storing all finetuned models, Mediator's storage demands remain high.\nScaling Challenges. While scaling to 8 models is feasible, expanding to hundreds or thousands (e.g., for personalization (Chan et al., 2024)) poses deployment challenges.\nLoading Time. Loading experts per sample remains a system bottleneck. Although optimized (Section 4.4), faster loading requires further research.\n7. Conclusion\nWe propose Mediator, a framework for merging LLMs that addresses parameter conflicts through adaptive layer-wise strategies, which average low-conflict layers and routing high-conflict ones via task-specific experts. This preserves individual model strengths while integrating shared knowledge, improving performance and reducing system costs.\nBy decomposing experts into a dense core and sparse components, Mediator minimizes storage without sacrificing efficacy. Dynamic expert selection via task uncertainty enhances adaptability across diverse inputs. Experiments on LLaMA and Qwen demonstrate significant performance gains over existing methods, with CoT enhanced datasets further enhancing reasoning capabilities.\nMediator advances efficient LLM merging methods, balancing resource constraints with practical versatility. Future work should explore theoretical foundations of parameter conflicts, large-scale deployment optimizations, and faster expert loading mechanisms."}, {"title": "Impact Statements", "content": "Societal Impacts. Our approach demonstrates significant effectiveness by enabling the deployment of merging 7B x 4 LLMs with only 24GB VRAM. Compared to ensemble learning with these models, our method not only maintains better accuracy but also requires significantly less computational resources and demonstrates superior performance. This breakthrough in resource efficiency makes advanced language models more accessible and cost-effective.\nEthical Concerns. We declare no conflicts of interest that could inappropriately influence our work. All experiments were conducted using publicly available resources. Our study does not involve human subjects, data collection from individuals, or experiments on protected groups. The models and basic datasets used in this work are publicly available and widely used in the research community. We have made efforts to ensure our experimental design and reporting of results are fair, unbiased, and do not misrepresent the capabilities or limitations of the methods presented.\nReproducibility. For openness of LLM research, we declare our code and the CoT enhanced crafted finetuning datsets will be made available to ensure reproducibility. We will provide detailed documents of code implemnetation. And we have provided the details of all hyper-parameters of implementing Mediator and optimizing baselines.\nPotential Applications. The technology may have significant potential across specialized vertical domains. Considering that many vertical domains, personalized LLM agents (Li et al., 2024e), LLM applications like roleplay chatting (Chan et al., 2024; Yu et al., 2024a) and professional domain-specific writing (G\u00f3mez-Rodr\u00edguez & Williams, 2023), an LLM service provider may need to simultaneously deploy different finetuned LLMs. Our technology enables efficient and effective serving multiple popular LLM applications, and merging knowledge from different LLMs together."}]}