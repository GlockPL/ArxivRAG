{"title": "CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models", "authors": ["Kiet A. Nguyen", "Adheesh Juvekar", "Tianjiao Yu", "Muntasir Wahed", "Ismini Lourentzou"], "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have sparked significant progress in general-purpose vision tasks through visual instruction tuning. While some works have demonstrated the capability of LVLMs to generate segmentation masks that align phrases with natural language descriptions in a single image, they struggle with segmentation-grounded comparisons across multiple images, particularly at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which seeks to identify and segment common and unique objects and parts across images. To address this task, we present CALICO, the first LVLM that can segment and reason over multiple masks across images, enabling object comparison based on their constituent parts. CALICO features two proposed components, a novel Correspondence Extraction Module, which captures semantic-rich information to identify part-level correspondences between objects, and a Correspondence Adaptation Module, which embeds this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MIXEDPARTS, a comprehensive multi-image segmentation dataset containing ~2.4M samples across ~44K images with diverse object and part categories. Experimental results show CALICO, finetuned on only 0.3% of its architecture, achieves robust performance in part-focused semantic co-segmentation.", "sections": [{"title": "1. Introduction", "content": "Analyzing objects by decomposing them into their constituent parts can enhance understanding of inter-object relationships both within and across categories. This part-level understanding is crucial for applications requiring detailed object comparisons, such as robotic manipulation, medical imaging, and educational tools. By recognizing similarities and differences at the part level across multiple images, these applications can enable more context-driven analysis and actions. Detailed comparisons that identify shared and unique parts offer insights into the critical features and functions of objects. For example, while both spoons and forks have handles, this part is not central to their primary functions. Instead, distinguishing the fork's tines from the spoon's bowl allows tasks like robotic grasping or visual comparisons to differentiate and interact with these objects based on their unique functions.\nDesigning effective methods to analyze multiple images featuring diverse objects, and locate and identify their shared and distinct parts, presents an intriguing challenge. Given two images of similar objects, the goal is to generate segmentation masks for the shared or distinct parts (locate), establish one-to-one correspondences across images for comparative analysis (compare), and assign descriptive labels to these parts (identify). We term this task part-focused semantic co-segmentation.\nResearch in part-focused semantic segmentation has explored various facets of this related granular task. Many studies have centered on single-image localized part learning [4, 5, 29, 38, 58, 63], focusing on segmenting parts within an object as a terminal task [4, 38] or as a means to support broader tasks like object or human parsing [53, 75]. However, these methods are not designed for multi-image comparisons, as the segmented parts from different images lack a consistent mapping of corresponding semantic components across images. Conversely, some research in this realm has tackled the related task of part co-segmentation, where models learn to segment objects across multiple images in a semantically consistent manner, despite variations in pose, shape, camera angle, etc. [1, 5, 13, 20, 33]. Yet, these approaches often require the number of part classes to be specified as input and struggle to identify unique parts between objects. Most part co-segmentation methods segment the entire object into parts, covering all areas of the object, which limits the model's flexibility in pinpointing and analyzing subtle, part-specific variations that are essential for comparing different objects. Furthermore, the unsupervised or semi-supervised nature of recent methods hampers their ability to accurately label object parts.\nRecently, vision research has experienced a surge of interest in leveraging LVLMs for object segmentation tasks [24, 46, 50, 60, 67, 68]. Following LISA [24], many studies have adopted approaches that output additional segmentation tokens [46, 50, 68], which contain mask embeddings of objects within an image. These tokens are then processed by a decoder to generate the final segmentation masks. Such methods have proven highly effective for unifying tasks like semantic segmentation and referring expression segmentation within a single architecture, thanks to the adaptability of LVLMs in handling diverse input and output types. Although some studies have demonstrated varying levels of part understanding [24, 46, 67], they do not address reasoning about parts across multiple images or segmenting shared and distinctive parts between objects.\nTo this end, we introduce Component-Focused Adaptive Learning for Multi-Image Co-Localization of Objects (CALICO), a Vision-Language Model (VLM) designed to perform localized object comparison across image pairs. CALICO's architecture is unique in its design, combining a Correspondence Extraction Module to capture part-level semantic relationships and a Correspondence Adaptation Module to embed these relationships efficiently into an LVLM. These components enable multi-image, part-level understanding without substantially increasing the parameter count. CALICO augments a pretrained segmentation-based LVLM with a parameter-efficient adapter module to learn multi-image co-localization at multiple granularities. To extract semantic correspondence information between images, we propose a novel Correspondence Extraction Module, which employs a frozen encoder with strong semantic correspondence capabilities learned through self-supervised training [3]. This extracted information is then incorporated within the model using a Correspondence Adaptation Module applied at various layers of the pretrained LVLM to facilitate inter-image understanding. Due to the lightweight nature of these adaptive modules, the trainable parameters in CALICO represent only 0.3% (~29M) of the entire architecture. To the best of our knowledge, CALICO represents the pioneering effort in training an LVLM to perform multi-image part co-segmentation.\nTo enable effective training for this novel task, we introduce the Multi-Image Cross-Segmentation of Distinctive and Common Parts (MIXEDPARTS) dataset that contains diverse and logically comparable object pairs, allowing CALICO to generalize across varied categories and visual details. Leveraging widely used, publicly available part segmentation datasets [15, 45, 61, 73, 74], we manually curate pairs of object labels that are logically comparable and share at least one common part label. For instance, pairing a \"chair\" with an \"ottoman\" is logical since both belong to the category of seating furniture, making them more comparable than, e.g., a \"chair\u201d and a \u201cmicrowave.\" We then construct image pairs based on these labels to train the model to co-segment not only the objects themselves but also their shared and unique parts across multiple images.\nLastly, as our work is the first to address multi-image part co-segmentation, we create baselines using publicly available pretrained models to tackle this novel task. We conduct extensive experiments on MIXEDPARTS to evaluate the effectiveness of our approach, including ablation studies to assess the contributions of the proposed correspondence extraction and adaptation modules. CALICO achieves superior performance in part-focused semantic co-segmentation, showing a 20.8% relative mean IoU improvement on MIXEDPARTS compared to the next best baseline. In summary, our contributions are as follows:\n\u2022 We introduce the novel task of part-focused semantic co-segmentation, which aims to co-segment and label com-"}, {"title": "2. Related Work", "content": "2.1. Image Segmentation\nImage segmentation, a fundamental task in computer vision, has garnered extensive research over the years, with seminal works [16, 22, 35] paving the way for modern approaches. Recently, SAM [23] has emerged as a leading segmentation method due to its zero-shot capabilities and ability to generate accurate masks for novel objects. Despite its strong zero-shot performance, SAM cannot identify masked objects given arbitrary text input and relies on region inputs to operate effectively. To address this limitation, Semantic-SAM [26] employs text encoder outputs atop its decoder to label segmentation masks, while also enabling labeled segmentation at any granularity. SEEM [77] introduces a joint image-text representation space allowing labeling of segmentation masks. SAM demonstrates robust segmentation capabilities even when combined with LVLMs [24, 46, 60]. Thus, we incorporate a SAM decoder in CALICO. Building upon these works, we further fine-tune SAM's pixel decoder using the LLM's special token output embeddings to produce the desired segmentation masks. While the aforementioned works focus on segmenting individual images, CALICO tackles multi-image object and part co-segmentation.\n2.2. Part Segmentation\nPart segmentation enables a more granular understanding by parsing objects into meaningful components or parts. While numerous approaches have been proposed to address this task, many are tailored to specific domains [21, 65, 66, 76] or are constrained to closed-set vocabularies [9, 29, 38, 39]. A recent approach, VLPart [56], efficiently tackles this task with open vocabulary predictions by utilizing a pretrained LVLM [44] and pretrained DINO features [3, 41] to perform semantic correspondence with base objects, enabling the model to label any part of the object. However, in practice, the labels are constrained by the input to the text encoder. Many recent works [24, 46, 50, 60, 67, 68] integrate the reasoning capabilities of LVLMs with segmentation models to augment visual understanding of the LVLMs. These models enable open-vocabulary object segmentation, without being constrained to the input prompt. Most of these models, motivated by LISA [24], utilize a pretrained LVLM such as LLaVA [32] in tandem with a decoder to predict segmentation masks.\nWhile several of these works can also segment object parts [24, 46, 67] since their training data consists of large annotated datasets containing part information, their part segmentation capabilities often lag behind their object segmentation capabilities. Additionally, they also require explicit mention of object parts in the input prompt for successful segmentation. In other words, simply requesting to \"segment multiple parts of the given object\" does not yield the desired results. Instead, each part must be individually specified, e.g. by requesting to segment the leg of the chair to obtain the desired segmentation maps. In contrast, CALICO augments a pretrained LVLM with object and part co-segmentation capabilities without the need for explicit part-specific prompts, simplifying user input requirements and allowing flexibility in instructing the model to infer and delineate various objects and parts across different images.\n2.3. Object/Part Co-Segmentation\nCo-segmentation aims to discover common objects across multiple images. Early co-segmentation works [25, 69, 72] employ CNNs with fully supervised training or fine-tuning on co-segmentation datasets. LCCo [11] leverages the semantic understanding of CLIP [44] to perform object co-segmentation. However, these approaches do not extend to co-segmenting object parts. Conversely, part co-segmentation involves simultaneously segmenting corresponding parts across multiple images. Previous methods [1, 5, 13, 20, 33] tackle this task with unsupervised or self-supervised learning. DFF [6] employs matrix factorization with features from a pretrained CNN, while SCOPS [20] trains an encoder-decoder CNN with an equivariance loss for robust part segmentation and a semantic consistency loss for improved co-segmentation. Moreover, a recent work [1] identifies that DINO features encapsulate semantic and spatial correspondences that can be utilized to find similar parts of objects across different images.\nHowever, although these methods can co-segment parts, they lack the ability to predict semantic part labels and generate segmentation masks for unique object parts. In this work, we introduce CALICO, a model that harnesses the zero-shot capabilities of SAM and the rich semantic infor-"}, {"title": "3. Method", "content": "3.1. Problem Definition\nIn part-focused semantic co-segmentation, the objective is to generate a set of segmentation masks for a given set of input images. Here, each mask corresponds to an input image and indicates the pixels containing either the common object across all images or the shared and distinct parts belonging to semantically similar objects (i.e., intuitively comparable objects) within these images. Formally, given $N_1$ input images $X_{image} = \\{X_{image_1},..., X_{image_{N_1}}\\}$, where each $X_{image_i} \\in \\mathbb{R}^{3\\times H_i\\times W_i}$ has height $H_i$ and width $W_i$, the goal is to train a co-segmentation model $F: X_{image} \\rightarrow M$ to obtain a set of mask sets $M = \\{M_1,...,M_{N_1}\\}$, with each $M_i = \\{(m_{i1}, C_{i1}),......, (m_{iM_i}, C_{iM_i})\\}$ containing $M_j$ masks and corresponding class labels associated with image i. Here, each binary mask $m_{ik} \\in \\{0,1\\}^{H_i\\times W_i}$ assigns each pixel to a value of 1 if it covers the visual element with semantic class label $C_{ik}$ and 0 otherwise. We denote $C_i = \\{C_{i1},..., C_{iM_i}\\}$ as the set of class labels corresponding to image i. When generating common object or part masks across all images, we want $\\bigcap_{i=1}^{N_1} c_i \\neq \\emptyset$, whereas when obtaining unique masks, we want $c_i \\cap c_{i'} = \\emptyset \\forall i \\neq i', 1 \\leq i,i' < M_j$. To learn a model F that can address this multifaceted task, we opt for an LVLM-based solution, leveraging LLMs' ability to tackle multiple tasks with a single architecture and their flexibility in input/output processing. The rest of this section details our model's architecture.\n3.2. CALICO Architecture\nCALICO is an LVLM designed to output multiple segmentation masks per image for a series of images that address commonalities and differences across images. In addition to its core functionality, our model incorporates modules aimed at integrating semantic correspondences between similar objects across images, alongside multi-image understanding and segmentation. As illustrated in Fig. 2, the architecture is composed of a Vicuna-based large language model M in tandem with a vision module I and a vision-to-language projection layer, which projects image embeddings from I into M's language space.\nInterleaved Vision-Language Inputs. CALICO is trained to understand interleaved multi-image inputs. Given $N_1$ input images $X_{image} \\in \\mathbb{R}^{N_1\\times3\\times H\\times W}$ and a vision module $\\mathcal{I}: \\mathbb{R}^{3\\times H\\times W} \\rightarrow \\mathbb{R}^{S_1\\times D_1}$, we obtain image embeddings $X_{embed} \\in \\mathbb{R}^{N_I \\times S_1\\times D_1}$ by $X_{embed} = \\mathcal{I}(X_{image})$, where $S_1$ and $D_1$ are the image embedding sequence length and hidden size, respectively. We then project these embeddings into the language model space with hidden size D via $f_{image} : \\mathbb{R}^{D_I} \\rightarrow \\mathbb{R}^{D}$ to get\n$I^{IO} = f_{image}(X_{embed}) = \\{I_1^0,..., I_{N_1}^{S_1} \\} \\in \\mathbb{R}^{N_1\\times S_1\\times D}$. (1)\nThe final input $T^0 \\in \\mathbb{R}^{S\\times D}$ into M is composed of interleaved text and image tokens $\\mathcal{T}^0 = \\{t_1^0,..., t_{S_T}^0, I_1^1,..., I_{VS_1}^1,...,I_1^{N_1},..., I_{VS_1}^{N_1},..., t\\}$, where $t_i^0$ is the $i^{th}$ embedded text token at layer 0 ($1 \\leq i \\leq S_T$), $I^j = \\{I_1^j, ..., I_{VS_1}^j\\}$ are the $S_1$ tokens pertaining to the $j^{th}$ image, and the superscript k of $T^k$ represents the LLM output at layer k or input at layer k + 1. The sequence length $S$ of $T^0$ thus sums up the text and image lengths, i.e., $S = S_T + N_1 \\times S_1$. Finally, we obtain predicted outputs from the N-layered LLM by $\\hat{T}^N = \\mathcal{M}(T^0)$. In practice, we encourage the LLM to learn comprehensive multimodal understanding through prompts such as \"The <image> (IMAGE1) and <image> (IMAGE2)\nVision Module. Observing the effectiveness and efficiency of BLIP-2's Q-Former cross-attention mechanism [27], especially in multi-image settings [28], we propose using Q-Former in tandem with a strong CLIP vision encoder [44, 57] to extract visual embeddings from the input images. Whereas projecting CLIP embeddings directly into the language model space preserves their long sequence lengths (e.g., 256 or 576 tokens [24, 46]) and thus increasing compute, Q-Former uses a much shorter set of learnable query tokens to extract visual information (e.g., 32 tokens [27, 28]). Formally, our vision module $\\mathcal{I}$ consists of an EVA-CLIP-g model $\\mathcal{C} : \\mathbb{R}^{3\\times H\\times W} \\rightarrow \\mathbb{R}^{S_C\\times D_C}$ and a Q-Former cross-attention module $\\mathcal{Q}$ alongside a set of learnable query tokens $q \\in \\mathbb{R}^{S_1\\times D_1}$. We first pass input images $X_{image}$ through the EVA-CLIP global encoder to obtain $X_{global} = \\mathcal{C}(X_{image}) \\in \\mathbb{R}^{N_1\\times S_c\\times D_c}$. We then obtain our final visual embeddings by querying $\\mathcal{Q}$ using q as the query and $X_{global}$ as the key and value:\n$X_{embed} = \\mathcal{Q}(q, X_{global}) \\in \\mathbb{R}^{N_1 \\times S_1 \\times D_1}$ (2)\nPixel-Grounded Outputs. To enable pixel-level grounding, we augment the model's vocabulary with the segmentation token [SEG] and teach it to output grounding tags <p> and </p>, following recent work [46]. Through supervision, the model learns to ground a noun phrase associated with the following segmentation token by enclosing it in the grounding tags, which immediately precede the corresponding [SEG] token (e.g., The unique parts of the objects are <p> the seat cushion </p> [SEG] (IMAGE1) and <p> the back pillow </p> [SEG] (IMAGE2).", "S_1,......": "S_{N_1}\\} \\subset \\mathbb{T}^K$, where each $S_i \\in \\mathbb{R}^{S_i\\times D}$ corresponds to the set of $S_i$ predicted masks associated with image i, into segmentation mask sets $M = \\{M_1,... ,M_{N_1}\\}$, our architecture incorporates a Transformer-based grounding model [23], composed of a grounding encoder G and a pixel decoder D. The input images $X_{image}$ are passed through the frozen encoder to obtain the grounding embeddings $X_{ground}$ as the vision signal for the decoder by $X_{ground} = \\mathcal{G}(X_{image}) \\in \\mathbb{R}^{N_I \\times S_D \\times D_D}$. For an encoded image $X_{ground_i}$, the tokens in\n3.3. Correspondence Extraction Module (CEM)\nImage features obtained from self-supervised Vision Transformers (ViTs) [3, 8, 41] have been shown to exhibit rich semantic information at part-level granularity across similar, yet distinct object categories [1, 3, 71]. Motivated by these findings, we design a fusion module to extract such semantic information and facilitate part correspondence learning within the model. We define $\\mathcal{E}$ to be the semantic extraction process using a self-supervised ViT [3] similarly to Amir et al. [1], and obtain semantic embeddings $X_{semantic} \\in \\mathbb{R}^{N_1\\times S_S\\times D_s}$, where $S_s$ and $D_s$ are the sequence length and hidden size of the semantic image embeddings, respectively. We then use $X_{semantic}$ as the key and value for a cross-attention extraction mechanism A with the queried EVA-CLIP embedding $X_{global}$ to get semantic-rich global embeddings $X'_{global}$. This process is formalized by:\n$X_{global} = \\mathcal{C}(X_{image}) \\in \\mathbb{R}^{N_1 \\times S_c\\times D_c}$ (4)\n$X_{semantic} = \\mathcal{E}(X_{image}) \\in \\mathbb{R}^{N_I \\times S_S\\times D_S}$ (5)\n$X'_{global} = \\mathcal{A}(X_{global}, X_{semantic}) \\in \\mathbb{R}^{N_I \\times S_c\\times D_c}$ (6)\nThis fusion process produces strong semantic embeddings $X'_{global}$, which are subsequently utilized for the visual extraction process performed by the Correspondence Adaptation Modules, detailed in the next section.\n3.4. Correspondence Adaptation Module (CAM)\nDue to the high cost of training LLMs with billions of parameters, many works have taken advantage of adaptive"}, {"title": "3.5. Training Objective", "content": "Following past works [24, 46], the training loss $\\mathcal{L}$ is composed of the next-token prediction loss $\\mathcal{L}_{text}$ and the segmentation mask loss $\\mathcal{L}_{mask}$. Here, $\\mathcal{L}_{text}$ is a causal cross-entropy (CE) loss computed from the predicted and right-shifted ground truth tokens $\\mathcal{T}^K$ and $y$, while $\\mathcal{L}_{mask}$ combines a focal loss [31] and a DICE loss [40] derived from the predicted and ground truth masks $\\mathcal{M}$ and $\\hat{\\mathcal{M}}$. The training objective can be summarized as follows:\n$\\mathcal{L} = \\lambda_{text} \\mathcal{L}_{text} + \\lambda_{mask} \\mathcal{L}_{mask}$ (10)\n$\\mathcal{L}_{text} = CE(\\mathcal{T}^K, y)$ (11)\n$\\mathcal{L}_{mask} = \\lambda_{focal} \\mathcal{L}_{focal} (\\mathcal{M}, \\hat{\\mathcal{M}}) + \\lambda_{Dice} \\mathcal{L}_{Dice} (\\mathcal{M}, \\hat{\\mathcal{M}})$, (12)\nwhere $\\lambda_{text}$, $\\lambda_{focal}$, and $\\lambda_{Dice}$ represent the respective weighting coefficients for the loss components. These objectives enable an end-to-end training process focusing on optimizing the quality of both text and mask generation outputs."}, {"title": "4. MIXEDPARTS Dataset", "content": "Although multi-image datasets of various scales are available, they exhibit combinations of limitations, making them unsuitable for the part-focused semantic co-segmentation task. Limitations include the absence of fine-grained masks for segmentation [19, 28, 37, 54, 55], datasets being too small or domain-specific to facilitate generalizable LVLM training despite containing localized labels [2, 12, 51, 52, 62], or the lack of part-level information altogether [43, 51]. To address these challenges and enable effective training and evaluation of our part-focused semantic co-segmentation model, we introduce MIXEDPARTS, a novel dataset curated from publicly available datasets. Figure 4 provides examples from our dataset, demonstrating its diversity in object categories and visual details. We provide an overview of the public datasets utilized as part of"}, {"title": "5. Experiments", "content": "We evaluate CALICO's performance on the challenging MIXEDPARTS dataset, reporting the mean Intersection-over-Union (mIoU), Average Precision (AP50), and Recall, which assess the model's segmentation performance. To evaluate its semantic label generation capability, following existing works [7, 67], we employ Semantic Similarity (SS) and Semantic IoU (S-IoU). Implementation details and descriptions of the evaluation metrics can be found in Appendix C. We perform evaluation on ~1K image pairs, ensuring an equal distribution of image pairs from each original dataset and maintaining equal representation across all tasks. We report performance on all three subtasks of part-focused semantic co-segmentation \u2013 common objects, common parts, unique parts \u2013 and their average, which reflects overall performance on the MIXEDPARTS test set.\nBaselines. To the best of our knowledge, we represent the first effort to tackle multi-image part-focused co-segmentation with part label identification. There is thus a lack of baselines for this new task, which we rectify by designing our own baselines from strong pretrained models in the semantic segmentation literature. Our baselines include a traditional zero-shot Mask R-CNN-based method and two finetuned LVLM-based approaches: (1) Multi-Image VLPart: VLPart [56] is an open-vocabulary part segmentation model which can identify object parts at different granularities. It utilizes a conventional Mask R-CNN [16] with a modern Swin Transformer backbone [34] and a CLIP [44] text classifier for open-world image classification. Although VLPart cannot perform co-segmentation, due to its strong zero-shot object-part segmentation capabilities, we perform segmentation on individual images and simply examine the common and unique predictions. Our implementation of Multi-Image VLPart, which ensures fairness, is detailed in Appendix C. (2) Multi-Image GLaMM: GLaMM [46] is a strong single-image segmentation-based LVLM constructed for the Grounded Conversation Generation (GCG) task for multi-round pixel-grounded conversations. The architecture incorporates a Vicuna-based backbone with SAM and a novel RoIAlign-based region encoder. Since GLaMM was not trained for multi-image processing, we replicate CALICO's multi-image implementation on the GLaMM codebase and finetune the mask decoder on MIXEDPARTS for GLaMM to learn multi-image reasoning, alongside LoRA, using the exact same hyperparameters. We initialize both CALICO and Multi-Image GLaMM on GLaMM's full model weights. (3) Multi-Image LISA: LISA [24] is an LVLM trained to perform"}, {"title": "5.1. Experimental Results", "content": "presents results comparing CALICO against our baselines, Multi-Image VLPart, Multi-Image GLaMM, and Multi-Image LISA. CALICO demonstrates superior performance on all metrics compared to all zero-shot and finetuned approaches, achieving relative gains of 20.8%, 9.8%, and 19.4% on segmentation-based metrics mIoU, AP50, and Recall, respectively, and 15.4% and 17.2% on text-based metrics SS and S-IoU. Although VLPart demonstrates strong zero-shot single-image object-part segmentation performance, it struggles on our multi-image tasks, even on text-based metrics where the model has direct access to the ground truth class labels. When finetuned on MIXEDPARTS, Multi-Image GLaMM and LISA exhibit performance improvements compared to the zero-shot VL-Part baseline. However, GLaMM still significantly lags behind CALICO's performance despite our model being initialized from the same weights, demonstrating the effectiveness of our proposed modules, which we further ablate in Section 5.2. We present qualitative examples in Figure 2 and additional experiments in Appendix E."}, {"title": "5.2. Ablations", "content": "CALICO Components. To validate the effectiveness of the individual components of CALICO, we conduct an ablation that isolates the impact of the Correspondence Extraction Module (CEM) and the Correspondence Adaptation Module (CAM). In Figure 5, we report results of CALICO without the CEM module (w/o CEM), without the CAM module (w/o CAM), or removing both (w/o CEM w/o CAM). Specifically, the CALICO variant without the CEM module utilizes only the image embeddings for fusion with the LVLM outputs' last hidden states, while the CALICO variant without the CAM module only injects DINOv2 features"}, {"title": "6. Conclusion", "content": "This paper introduces the novel task of part-focused semantic co-segmentation, which involves the segmentation of common and unique parts across multiple images, laying the groundwork for future research in enhancing the capability of Large Vision-Language Models (LVLMs) to analyze and interpret complex visual data in a granular manner. To solve this task, we propose CALICO, an LVLM that incorporates a novel correspondence extraction module and an adaptation module to handle multi-image part relationships. Experiments conducted on the newly curated MIXEDPARTS dataset, demonstrate that CALICO can effectively identify and segment common/unique parts with high accuracy, outperforming existing models."}]}