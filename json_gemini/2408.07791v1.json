{"title": "An Efficient and Explanatory Image and Text Clustering System with Multimodal Autoencoder Architecture", "authors": ["Tiancheng Shi", "Yuanchen Wei", "John R. Kender"], "abstract": "We demonstrate the efficiencies and explanatory abilities of extensions to the common tools of Autoencoders and LLM interpreters, in the novel context of comparing different cultural approaches to the same international news event. We develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) model that extends the modalities of previous CVAE models, by using fully-connected latent layers to embed in parallel the CNN encodings of video frames, together with the LSTM encodings of their related text derived from audio. We incorporate the model within a larger system that includes frame-caption alignment, latent space vector clustering, and a novel LLM-based cluster interpreter. We measure, tune, and apply this system to the task of summarizing a video into three to five thematic clusters, with each theme described by ten LLM-produced phrases. We apply this system to two news topics, COVID-19 and the Winter Olympics, and five other topics are in progress.", "sections": [{"title": "1 INTRODUCTION", "content": "Videos convey information with high temporal density through both images and audio. Thus, video content extraction and abstraction methods are gradually gaining in their significance. However, existing common tools are costly to train and opaque to interpret. We explore two novel variations on Autoencoders and LLM interpreters to generate thematic clusters of video tags without human supervision, in order to summarize individual videos. Our final goal is to quickly compare the videos of the same event presented by different cultures, in order to gain insight into their differing perspectives.\nVideo frames with high resolution are associated with an extremely high dimension that makes clustering difficult. Prior work has inspired us that Variational Autoencoders with only convolution layers (Pure CVAE, see Figure 1a) can reduce the dimensionality of images low enough for other Machine Learning algorithms to handle them. However, we note that the use of the Global Max Pooling layer on top of the latent layer undermines the effectiveness of the original CVAE model. We substitute instead multiple fully-connected linear layers, also following the encoder-decoder structure (Dense CVAE, see Figure 1b), which learn important features independent of the format of input vectors.\nWe claim the following advantages of our methodology.\nNovelty. We propose the CRVAE model that further introduces multimodality by incorporating NLP elements. While traditional CVAE solely processes images, CRVAE can encode audio information in the original videos, allowing to learn denser information representation comparing with its unimodal counterparts. Additionally, the mostly automatic full pipeline (see Figure 2) includes a Large Language Model (LLM) interpretation. Images from each thematically related cluster, each augmented with a caption generated by the BLIP model, are fed into the LLAMA model, with a prompt asking it to generate a fixed number of tags for the cluster.\nUniversality. Our method is applicable to YouTube news videos, regardless of their topics, sources, languages, cultural backgrounds, etc. This universality is because the key parts of our system-both the CRVAE model and the LLM-are designed to inherently handle multilingual text inputs.\nEfficiency. First, our system is almost fully automatic: the only manual parameter into the pipeline is the selection of optimal number of clusters for K-means, heuristically chosen with the help of metrics and plots. Secondly, the training session is also cost-efficient enough to be deployed on local PCs. The CRVAE model does not require prior information, and the training then takes less than 30 minutes per video. The LLM interpretation phase costs similarly."}, {"title": "2 RELATED WORKS", "content": "Prior work has used the Convolutional Variational Autoencoder (CVAE) model for representation learning and clustering of images (video frames) [8]. Specifically, 1-strided convolution layers, each followed by a 2 \u00d7 2 max pooling layer can be applied to extract low-dimension features from input images. Yet for images with high resolution, these layers are insufficient to reduce the high dimensions to an approachable level. Conversely, using larger strides and/or pool sizes will result in fuzziness during reconstruction.\nOne method to resolve this dilemma was to apply a Global Max Pooling to each channel, further reducing the dimension from C\u00d7 H x W to just C \u00d7 1, where C, H, W represents number of channels, image height, and image width respectively. However, this brute-force method fails for two reasons.\nOn the one hand, by forcing a full, 2D feature map to compress to a single value, we lose all spatial information. This simply keeps track of an extremely \"bright spot\", the pixel that is most activated by the convolution layers. On the other hand, during the actual training process, the latent vector after Global Max Pooling is not involved. It is neither the output optimized by the Encoder network, nor can it be used to reconstruct the output image by the Decoder network.\nWe conclude that pure CVAE is insufficient to encode high-resolution images. In contrast, multiple external researchers (Fan et al. [1], Pu et al. [5], and others) have use of dense layers between the convolution layers and the latent layer. Therefore, we adopt the\nConvolutional Variational Autoencoder"}, {"title": "2.2 Multimodal Representation Learning", "content": "MRL reduces the dimensionality of heterogeneous data, aiming at retaining most information and reducing the gap between different modalities of the input data. Guo et al. [2] classified popular frameworks of Multimodal Representation Learning into three potentially overlapping types.\nThe first, Joint Representation, method typically involves a low-dimension subspace containing the fusion of information from all modalities. A common approach is to use an additive method: \\(z = f(w_1v_1 + W_2V_2)\\), where \\(v_i\\) are unimodal representations, f is the activation function, and z is the output multimodal representation.\nThe second, Coordinated Representation, in contrast learns each modality in separate models and exerts additional constraints to regularize the training across all modalities.\nThe third, Encoder-Decoder Framework, translates one modality into the other, and takes the intermediate layer as the multimodal representation.\nOur work uses a modality-fusion method within an encoder-decoder framework and can be viewed as a mix of the first and third types. The advantage of this structure is that a joint representation outputs a single vector representation that contains shared information across modalities, and enables clustering and interpretation in the next steps."}, {"title": "2.3 Bimodal Deep Autoencoder", "content": "Ngiam et al. [4] proposed a bimodal Autoencoder in a Joint Representation structure. The model takes both images and audio from videos, fuses them into a shared representation layer, and reconstructs both modalities.\nThe autoencoder was trained to \"denoise\" the video: one of the modalities of the input data would be masked to zero at the input, even while both modalities were still evaluated at the output. This emphasizes the extraction of cross-modality features. The work showed that the latent layer of this autoencoder structure was an effective low-dimension representation of information from both modalities.\nOur model is designed in a similar structure, but trained toward our goal of learning the shared representation of videos for further analyses. Consequently, the zero-masks in their work are not included in our experiments."}, {"title": "3 METHODS", "content": "We propose our new CRVAE (Convolutional-Recurrent Variational Autoencoder) model architecture as shown in Figure 3, which takes an image and a sentence as input each time, processes them in parallel using CNN and LSTM respectively, and combines the multimodal input vectors through fully-connected layers. It generally follows the encoder-decoder structure, determined by a latent layer in the center. The final takeaway of this model is the vectors in the latent space, with lower dimensions compared to the input images and text captions."}, {"title": "3.1 Encoder", "content": "Similar to the dense CVAE model, we set the dimension of input images as C \u00d7 H \u00d7 W = 200 \u00d7 120 \u00d7 3, where 3 represents the RGB channels, and set a batch size of 16. The framework of Convolution and Max Pooling layers remains the same, only that the numbers of filters (channels) are set to be uniformly 32. The motivation to use fewer channels in the last convolution layer (decreased from 64 to 32) is that after flattening, we will have 32\u00d725\u00d715 = 12,000 neurons, instead of 24,000, which significantly reduces the model size."}, {"title": "3.1.2 Text", "content": "The text encoder network is an RNN-based model, with the input of a sequence of embedded text in dimension 200. However, the pre-trained embedding weights (Chinese or English) are fixed, and are not optimized during gradient descent. The model is selected from either vanilla RNN or LSTM, both of which have a hidden state vectors of length 512 and 2 stacked layers of bidirectional recurrent cells. Our experiments show that the LSTM model has higher performance than vanilla RNN on all tasks."}, {"title": "3.1.3 Latent Layers", "content": "CRVAE uses 2 fully-connected neural layers in the middle. The outputs of the Convolution layers and LSTM layers are flattened and normalized using the Batch Normalization layer, to ensure that they are approximately on the same scale. The resulting vector, after concatenation, has the dimension of 32 \u00d7 25 \u00d7 15 + 2 \u00d7 2 \u00d7 512 = 14,048, where the 2 \u00d7 2 represents 2 layers of LSTM, and both of them in 2 directions. The vector is then narrowed down to 1 layer with 4,000 neurons. The final latent layer has 2,000 neurons, 1,000 for the latent mean and 1,000 for the latent standard deviation (in log scale)."}, {"title": "3.1.4 Resampling", "content": "The latent mean \\(\\mu\\) and latent standard deviation ln \\(\\sigma\\) are then used to resample (using a Normal distribution) across the latent space. The resulting vector is passed to the decoder network for the reconstruction of images and text. After the training session, the latent means \\(\\mu\\) are finally produced as CRVAE output."}, {"title": "3.2 Decoder", "content": "The decoder network is roughly symmetric to the encoder network, as the 1000-dimension embedding is gradually rebuilt to 4000- and then 14,048-dimension. It is then spit into two parts, and trained to reconstruct the images and text separately."}, {"title": "3.2.1 Images", "content": "The image decoder uses Transposed 2D Convolution layers with a 3 \u00d7 3 kernel and a stride of 2 to \"upsample\" an image channel, resulting in doubled width and height. Each of these Transposed Convolution layers is followed by a Convolution layer with the same kernel size (3 \u00d7 3) and number of channels (32). After three of these paired Transposed Convolution and Convolution blocks, the tensor is reconstructed into the original resolution with 3 channels, representing the Red, Green, and Blue pixels of an image. We use pixel-wise Mean Squared Error (MSE) between the input image and the reconstructed images as the loss function."}, {"title": "3.2.2 Text", "content": "The text decoder in CRVAE is different from the traditional LSTM decoder for NLP tasks because we require the model to handle multilingual inputs. Normally, we would have mapped the neurons to a layer with the same length as the vocabulary size, and assigned a SoftMax activation corresponding to the Cross-Entropy loss. However, due to the great differences between Chinese and English, this approach does not work well. Instead, we now require the decoder to predict the text embeddings as tensors, and optimize the MSE loss between the original text and the reconstructed embedded text. To make this work, we \"freeze\" the text embedding layer in the encoder; text embeddings are not involved in the training session. This necessitates two further changes to the decoder architecture."}, {"title": "3.2.3 Teacher Forcing", "content": "We applied the Teacher Forcing algorithm to our text decoder network. At each LSTM cell, we predict the next word by the input of the ground-truth previous word embeddings, instead of the predicted previous embedding. We experienced a significant performance increase with this algorithm, mostly because our purpose is simply to learn important features and characteristics of the text, information which is encoded in the latent layer, and passed to the decoder through hidden and cell states."}, {"title": "3.2.4 Nearest Neighbors", "content": "Because our final reconstructions are tensors instead of words, we still need a method to \"verbalize\" them. We therefore search for the nearest neighbor of an embedded word in the vocabulary, and \"decode\" this vector as that word. Note that this nearest neighbor verbalization is not part of the training process."}, {"title": "3.3 CRVAE Model Configuration", "content": "By constructing our model based on the architecture in Figure 3, we can formulate two types of MSE loss: image loss and text loss. The final loss function is L = ImageLoss + \\(\u03bb\\) \u00b7 TextLoss, where \\(\u03bb\\) is a ratio hyperparameter to balance the reduction of losses during the training session. In practice, we set \\(\u03bb\\) = 3.\nAll intermediate layers in the model are activated by ReLU (Rectified Linear Unit) function. The AdamW (Adaptive Momentum with Decoupled Weight Decay Regularization) optimizer with a learning rate of a = \\(10^{-4}\\) is used, and for each parameter, the gradient is clipped to 0.01 to prevent gradient explosion. The model is trained for 500 epochs on a local device, an NVIDIA GeForce RTX 3080 with 10 GB GPU Memory. A typical training session lasts for around 30 minutes, which is similar to the training time of a 300-epoch dense CVAE model."}, {"title": "3.4 Clustering and Cluster Interpretation", "content": "After encoding each  pair into a latent space of dimension 1000, we perform K-means clustering on all vectors generated from a video. To find the best K, which typically will be a small integer, we semi-automatically evaluate each trail K using a number of cluster quality metrics."}, {"title": "3.4.1 Metrics", "content": "The metrics used include average inter-cluster distance, average cross-cluster distance, and a cluster robustness test. Specifically, we seek an ideal K that has small inter-cluster distances (which indicate compact clusters), large cross-cluster distances (which indicate distinct clusters), and clusters that do not change significantly as new centroids are introduced. We plot and visualize these metrics against different K, in order to heuristically choose the optimal number of clusters."}, {"title": "3.4.2 Tags", "content": "Besides these quantifiable metrics, we also propose a more intuitive evaluation of cluster quality, in two steps, with each step using an LLM.\nWe first ask the BLIP-base model on HuggingFace, BLIP-IMAGE-CAPTIONING-BASE in full FLOAT32 precision, to \"describe\" each input image frame using its own captioning service. This takes only a short prompt (see Appendix 7.1).\nWe then use the simplest Llama-2 model on HuggingFace, LLAMA-2-7B-CHAT-HF, to perform generative language modeling. Because text elements for a given thematic cluster can be easily concatenated, both in syntax and semantics, we combine all of a cluster's actual video captions into one textual grouping, and all of its BLIP descriptions into another. The required prompt is considerably longer and more sensitive to tuning (see Appendix 7.2)."}, {"title": "4 DATASET", "content": "To evaluate our pipeline, we focused on seven news events that arouse global interest, including COVID-19, Winter Olympics 2022, China's \"visa-free\" visitor policy, Copa America 2024 Soccer Tournament, Beryl Hurricane, China's newly deployed self-driving taxis, and the crisis of cooking oil being contaminated by petroleum in oil trucks in China. For each topic, we collected one video from English sources and another video from Chinese sources. We have run experiments on COVID-19 and Winter Olympics, and are processing the others.\nRaw image datasets about COVID-19 are sampled at the rate of 1 frame every 2 seconds, while those about Winter Olympics at the rate of 1 frame every 2.5 seconds, in order to balance the data sizes. We align segments of text data (taken from the audio) with each image frame automatically (for English) or semi-automatically (for Chinese)."}, {"title": "4.1 English COVID-19 \u201cNew Variant\u201d Video", "content": "This video\u00b9 is published by CBS Mornings on YouTube. The main content concerns the resurgence of the Omicron variant and its spread across the US. Dr. Agus also discusses reactions and restriction policies in large cities like New York and Los Angeles. The video length is 3 minutes and 40 seconds in total."}, {"title": "4.1.1 Images", "content": "This video generates 109 image frames. See Figure 4a for a manually selected but typical frame: a shot of the host interviewing a medical expert talking to the camera."}, {"title": "4.1.2 Text", "content": "The text data is collected from YouTube auto-generated closed captions in English, enabled by the Python package, YOUTUBE-TRANSCRIPT-API. Since the autogenerated caption does not include punctuation, we use the Basic English Tokenizer in the PyTorch TORCHTEXT package. We then use a pre-trained GloVe (\"Global Vector\") Embedding with a dimension of 300 to embed every word token.\nThe TRANSCRIPT API segments the whole script paragraph into 96 segments of text, which gives approximately 10 to 20 words in each segment. In addition, it also provides an accurate timestamp of each segment's beginning and end. The period between two neighboring segments are roughly two seconds, which agrees with the rate at which we sample the frames. We heuristically but automatically align frames to text segments by selecting the frame closest to a segment's starting time."}, {"title": "4.2 Chinese COVID-19 \"Vaccine\" Video", "content": "This video is published by China Central Television (CCTV) on YouTube. The main content concerns an effort to encourage elderly Chinese citizens to take vaccines for COVID-19, while also reporting on the pandemic and the progress of disease control. The rather long video length is 15 minutes and 8 seconds in total."}, {"title": "4.2.1 Images", "content": "This video generates 378 image frames in total. See Figure 4b for a manually selected but typical frame: a shot of a Chinese official talking."}, {"title": "4.2.2 Text", "content": "Since YouTube does not provide transcripts for Chinese videos, we resort to YueLu Voice Club, a Chinese Automatic Speech Recognition converter.\nHowever, some of the source video involves Chinese dialects somewhat different from Mandarin, so we manually corrected its mistakes. We also added a timestamp every 10 seconds, a period chosen in consideration of the information density of Chinese relative to English.\nThis process gives us 90 text segments, which need to be aligned to the 378 image frames. Since the average sample rate is 4.2 frames per segment, we heuristically but uniformly sample 5 images out"}, {"title": "4.3 English Olympic \u201cConstruction\u201d Video", "content": "This video\u00b3 is published by British Broadcasting Corporation (BBC) News on YouTube. The main content concerns the construction and architecture of venues (e.g., winter sports centers) in Beijing for the Winter Olympics. The video is relatively shorter, with the length of 2 minutes and 52 seconds. We apply the same pre-processing steps as for English COVID-19 \"New Variant\" video."}, {"title": "4.3.1 Images", "content": "This video generates 60 image frames. See Figure 4c for a manually selected but typical frame: a Canadian host talking at a construction site."}, {"title": "4.3.2 Text", "content": "Sample text segments are given below:\n\"In the mountains around the Chinese capital,\"\n\"thousands of workers are busy...\""}, {"title": "4.4 Chinese Olympic \u201cCeremony\" Video", "content": "This video is also published by China Central Television (CCTV) on YouTube. The main content concerns the opening ceremony of Winter Olympics 2022, including the arrangements of the director, the practice and rehearsal of actresses, etc. The video length is 15 minutes and 9 seconds in total."}, {"title": "4.4.1 Images", "content": "Likewise, the same alignment steps are applied as for Chinese COVID-19 \"Vaccine\" images. This video generates 91 image frames after sampling. See Figure 4d for a manually selected but typical frame: a group of Chinese actresses practicing for their dance show."}, {"title": "4.4.2 Text", "content": "In additional to Chinese texts, this video also involves the speech in English by Yiannis Exarchos, the CEO of Olympic Broadcasting Services (OBS). So the pre-processing step of the text combines both Chinese CWV embedding and English GloVe embedding. Such text is embedded into numerical vectors before being passed to the CRVAE model, so the model is inherently multilingual. Sample text segments (translated to English) are given below:\n\"What surprises will be brought by the Feast of Snow?\"\n\"People perform warm-up shows in the way they love...\""}, {"title": "5 RESULTS", "content": "We test the performance of the three CVAE variant models on the image encoding task using English and Chinese COVID-19 videos. The losses are listed in Table 1, where the MSE image losses have been properly scaled to account for differences in the image resolution within CRVAE. We find that a dense CVAE is superior to a pure CVAE. We note, however, that there is an increased training time cost. Pure CVAE is trained for 100 epochs; dense CVAE for 300 epochs; and CRVAE for 500 epochs."}, {"title": "5.1.2 Image Reconstructions", "content": "A sample of the reconstructed images of our model on the COVID-19 video pair are shown in Figure 5; the Olympics video pair results are similar. These images are generally clearer than those reconstructed by CVAE, which does not learn text simultaneously.\nWe noted a characteristic artifact in all CVAE-derived models. If the original image has a light background, then the reconstructed image will likely have numerous small bright patches of saturated colors in those areas. It appears that this is due to those pixels having a zero value in one or more of their RGB channels, although the root cause is still speculative."}, {"title": "5.1.3 Text Reconstructions", "content": "A sample of the reconstructed texts of our model, for both English and Chinese (COVID-19 pair), are shown in Figure 6, after \"verbalizing\" the embeddings and cleaning up any padded tokens (as detailed in Section 3.2). We note that unknown tokens and filler words are often verbalized as \"well\" in English, because the method we use to decode embeddings into discrete words is by finding their nearest neighbors, and the embedding of \"well\" is close to the zero vector. The results appear mostly accurate and properly formed."}, {"title": "5.2 Clustering", "content": "We use K-means to find vector clusters. We manually examined the inter- and cross-cluster distances, as well as cluster size distribution. Typical K's are set to 3, 4, or 5, as shown in the Figures of Section 5.4. The resulting clusters appear quite reasonable when projected into the 2D plane, using t-SNE [6]. We set the perplexity parameter to 8 for the COVID-19 datasets; see Figure 7 (left) and (right) for the 3 English clusters, and the 5 Chinese, respectively. Results for the Olympics data were similar: 4 English and 4 Chinese clusters."}, {"title": "5.3 Cluster Interpretation", "content": "To help understand the meanings of each cluster (COVID-19 data), we take advantage of three large models."}, {"title": "5.3.1 BLIP-Generated Image Descriptions", "content": "Overall, the results of image descriptions generated by BLIP are satisfying. It can successfully detect persons, objects, backgrounds, etc., in a frame, and make inferences to some extent. For example, BLIP detected the \"laboratory\" setting in the Chinese data (\"A person in a lab holding a bottle\"), and inferred the occasion of a TV show in English data (\"A woman in a colorful dress is on the set of the Today show\").\nHowever, BLIP shows considerable disability in Optical Character Recognition tasks. For example, it misreads the Chinese words \"Priority pass for people aged 60+\" into \"No to the government\", which is nowhere in the image. In English data, it misinterprets the text of a statistical table as a repetitive sequence of the words \"corona\" and \"covids\"."}, {"title": "5.3.2 LLaMA-Generated Tags", "content": "We experimented with the tag generation process, 10 tags for each cluster, using Llama 2, among which 5 representative tags for each clusters are shown in Tables 3 and 4. We have manually traced back the originating medium of the output tags. Those tags marked by \u201c*\u201d and \u201c\u2020\u201d reflect information primarily in image frames and text captions, respectively. (We did not label heavily shared common tags, such as \u201ccorona\u201d.)\nIn either video, we find that most tags are meaningful, that is, they reflect actual information from images or texts. Yet there are still about 20% that contain generic, mistaken, or made-up information (hallucination). Despite these inefficiencies, we found that the cluster interpretation pipeline significantly reduced human effort in exploring the meaning of a cluster.\nWe verified that the results with 3 clusters are most sensible for the English COVID-19 video, as shown in Table 3, while those with 5 clusters are most sensible for the Chinese COVID-19 video, as shown in Table 4. We similarly verified the results for the Olympics videos. Overall, in keeping with the clustering results, the LLAMA model performed better on the English video than the Chinese video, most likely in part because the training corpus of Llama 2 is mainly in English."}, {"title": "5.3.3 PhraseBERT-Embedded Vectors", "content": "Having summarized the content in English tags, we use PHRASE-BERT [7] to measure tag similarities in the BERT 768-dimensional space. (We find that GloVe embeddings are less complex, and LLaMA-based embeddings are designed primarily for text generation tasks.)"}, {"title": "5.4 Cross-cultural Comparisons", "content": "We visualized the relation between clusters across cultures in Figure 8 and 9. We encoded the tags using PHRASE-BERT and computed a 10 x 10 cosine similarity matrix for each . The cluster pair similarity is measured by the average of top 10 cosine similarities within the cluster pair. Further, the clusters are sorted in descending similarity order. For convenience of further discussion, we have manually named each cluster by a summarizing phrase.\nFor the COVID data in Figure 8, we ignore the requisite \"Talking head\" clusters. We note that the English cluster \"Impacts and strategies\" is an overall summary of current pandemic status and vaccine research progress, which shares many similarities with the Chinese clusters \"Global response\", \"Treatment\", and \"Professional comments\". Those three Chinese clusters, in turn, have high similarity with almost all English clusters; in particular, the Chinese cluster \"Professional comments\" is especially related to English cluster \"Scientific analytics\", with statistics and data visualizations.\nYet, Chinese cluster \"Elderly vaccine\" is quite distant from all the English clusters. This identifies the Chinese-specific characteristics of the video: the emphasis on older people and on the social responsibility of taking vaccines."}, {"title": "6 DISCUSSION AND FUTURE WORK", "content": "We have fully implemented our system, with two videos each (Chinese and English) on two very different topics, COVID-19 and Winter Olympics. As discussed in Section 4, we are applying our pipeline to five additional topics; see Table 5 for details. Additionally, we will be extending some topics to include additional videos.\nIn this work, we constructed a full pipeline for integrating and processing video data through different modalities: images, audio, and text. Our Convolutional-Recurrent Variational Autoencoder architecture, CRVAE, combined the strengths of CVAE and LSTM models, while maintaining the encoder-decoder structure of both, and demonstrating correctness and superiority.\nWe adapted pre-trained Large Vision-Language Models and Large Language Models (Llama 2, BLIP, and PhraseBERT) to add explainability to the clustering process, with neither ground-truth nor supervision, through the generation of high-quality, human-interpretable tags for the resulting video clusters.\nAdditionally, we showed how these tools not only compactly captured the thematic content of the video pairs, but also provided an intuitive visualization of the primary differences between the pairings. Although we illustrated our system with two specific news occurrences as covered by two specific viewpoints, we believe the methods are easily extensible for summarizing and contrasting most other events and cultural styles.\nMore technically, our work suggests future research areas, such as replacing our LSTM model by a Transformer model, or augmenting other models with a similar two-network architecture (e.g., CGAN, Convolutional Generative Adversarial Network) with LSTMs or Transformers."}, {"title": "7 APPENDIX", "content": "We experimented with both unconditional and conditional (i.e., prompted) BLIP captioning. The prompt for conditional BLIP captioning is given below:\nA news photo of ... {BLIP will generate description}"}, {"title": "7.2 LLAMA Prompt", "content": "Please generate 10 short tags for a series of frames sampled from a YouTube news video based on the images and captions provided. Please avoid generic words that describe the whole video, but emphasize the unique characteristics of these frames. You may need to implicitly infer the meanings of the objects in the image description according to the video context.\nText caption: {inputs in English or Chinese...}\nImage description: {BLIP inputs in English...}\nThe purpose of the second sentence is to avoid common keywords that appear in all clusters (e.g., \u201ccoronavirus\u201d or \u201cOlympics\u201d). The purpose of the last sentence is to emphasize the contextual understanding between image frames and text captions. As an example, a BLIP description can be \"a man in a mask and protection suit\", and we want LLaMA to infer that this man is likely a medical staff officer performing examinations for COVID-19.\nThe sampling process during LLaMA text generation is controlled by several hyperparameters, including temperature, t. We experimented with different values of t \u2208 [0, 1]. With small t, LLAMA tends to respond in a conservative way, predicting text that most likely to follow, while with large t, it tends to be creative. In practice, we set a large value of t = 0.9 for a diversified generation."}]}