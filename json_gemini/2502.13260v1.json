{"title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models", "authors": ["Yingqian Cui", "Pengfei He", "Jingying Zeng", "Hui Liu", "Xianfeng Tang", "Zhenwei Dai", "Yan Han", "Chen Luo", "Jing Huang", "Zhen Li", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Qi He"], "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are powerful generative models capable of performing diverse tasks in different domains (Gramopadhye et al., 2024; Karabacak and Margetis, 2023; Ling et al., 2024) and demonstrating strong reasoning capabilities (Jaech et al., 2024). Recent advancements, such as few-shot/zero-shot Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022), as well as fine-tuning (Liu et al., 2023), have significantly enhanced the LLMs' reasoning capabilities by leveraging intermediate reasoning steps. In particular, through few-shot CoT, LLMs can learn from the reasoning steps in the demonstration examples and apply similar reasoning patterns to target tasks. In the case of zero-shot CoT, LLMs are prompted to\"think step by step\" to generate reasoning steps. In fine-tuning, LLMs can also learn from the reasoning steps in the fine-tuning samples, further enhancing their reasoning abilities.\nWhile many existing reasoning methods rely on available data (e.g., few-shot examples or fine-tuning datasets), there is limited understanding of which reasoning steps are truly essential and how their impact varies across different models. This gap hinders progress in two key areas: (1) how to effectively identify and remove unimportant reasoning steps from the data to reduce computational costs, and (2) whether the important reasoning steps for one model are also important to another.\nFor example, we observe that removing certain reasoning steps from the demonstrations in few-shot CoT can have varying effects: some models follow the modified examples and generate much fewer tokens while maintaining reasoning accuracy, whereas others experience a decline in performance. Specifically, we consider a math problem of function solving (Saxton et al., 2019). We compare two versions of demonstrations when conducting few-shot CoT: one with full manually crafted reasoning paths and another containing only intuitively important steps, as shown in Figure 1. For most models, removing certain steps significantly reduces the number of generated tokens with minimal impact on accuracy, suggesting that the removed steps contribute limited meaningful information. However, LLaMA3-8B shows a noticeable decline in accuracy, indicating that the importance of reasoning steps can vary across different LLMs.\nSimilar to the few-shot CoT scenario, when given a set of fine-tuning samples with reasoning steps, some LLMs may find some steps redundant, and the fine-tuning cannot improve the prediction accuracy. However, these LLMs will follow the fine-tuning samples to generate the addi-"}, {"title": "2 Preliminary", "content": "In this section, we first present the essentials of perplexity, and then introduce our exploration on how to use perplexity to analyze the reasoning steps."}, {"title": "2.1 Perplexity (PPL)", "content": "Perplexity was developed in (Jelinek et al., 1977) and is a common metric for LLMs. It is defined as\n$PPL(x, {W_k}_{k=1}^N) = exp(-\\frac{1}{N}\\sum_{i=1}^N log p(w_i | x, W_1,..., W_{i-1}) (1)$\nwhere x represents the prompt, {wk}_1 denotes sequence of tokens with total length N which are conditioned on x. The probability p(wi | x, W1, W2,..., wi\u22121) is the likelihood assigned by"}, {"title": "2.2 Relationship between Perplexity and CoT Prediction Accuracy", "content": "We conduct preliminary evaluation to investigate the relationship between PPL and CoT prediction accuracy when changing the steps used in the reasoning procedure. Intuitively, a higher likelihood indicates that the LLM is more confident to the context, and from Eq.(1), a higher likelihood results is a lower PPL. Thus, we hypothesize that the PPL is negatively correlated with the prediction accuracy.\nIn the experiments summarized in Table 1, we apply few-shot demonstrations to perform CoT reasoning across three tasks from the DeepMind Mathematics Dataset (Saxton et al., 2019): Solving linear equation (AL1), calculating derivative (Diff-Calc), and measuring time difference (Time-Diff). For each dataset, we manually construct the demonstration examples. All the constructed examples in the same dataset share the same reasoning steps. Then we randomly select steps to be removed from all examples in demonstration and calculate the perplexity of the resulting generation and the accuracy of CoT reasoning. Notably, the perplexity for all experiments is computed using LLaMA3-7B, while accuracy is assessed based on generations from both LLaMA3-7B and GPT-40-mini (in a transfer case).\nThe results from Table 1 indicate a statistically significant negative correlation between perplexity and accuracy across all tasks, aligning with our hypothesis. This observation paves us a way to identify unimportant reasoning steps from the reasoning path: Since the correlation is negative, if we remove some steps while maintaining the perplexity of the sample, then it is likely that there will be no accuracy loss, i.e., the removed steps are unimportant. Furthermore, the correlation appears transferable across models, as perplexity com-"}, {"title": "3 The Proposed Algorithm - SPIRIT", "content": "In this section, we present the details of SPIRIT. Since few-shot CoT and fine-tuning utilize data in different ways, we first provide the general idea in Section 3.1 and then describe case-specific details in Section 3.2 (Few-Shot CoT, SPIRIT-FS) and 3.3 (Fine-Tuning, SPIRIT-FT), respectively."}, {"title": "3.1 General Idea", "content": "For both few-shot CoT and fine-tuning, the general idea is to select unimportant reasoning steps and then process them. When removing one reasoning step, the final PPL will be changed. We enumerate all reasoning steps to get the one whose removal results in the lowest PPL.\nOn the other hand, a concern with step removal is that directly eliminating a step from a structured reasoning process can lead to coherence issues, particularly when the step contains intermediate results necessary for subsequent computations. Based on these observations, we propose to incorporate a merging paradigm into the algorithm, whose details will be introduced in the following subsections."}, {"title": "3.2 Few-Shot CoT (SPIRIT-FS)", "content": "When performing few-shot CoT, we assume the demonstration examples follow a consistent reasoning format, e.g., for the function solving problem, all examples follow the same steps as in Figure 1. For simplicity, we treat one sentence as one"}, {"title": "3.3 Fine-Tuning (SPIRIT-FT)", "content": "The full details of SPIRIT-FT are presented in Algorithm 2. Compared to few-shot CoT, some changes are made for the fine-tuning scenario.\nFirst, in fine-tuning, not all datasets contains complete reasoning steps. For datasets with high-"}, {"title": "4 Experiment", "content": "In this section, we conduct comprehensive experiments to demonstrate the effectiveness of SPIRIT. We present the results of SPIRIT-FS in Section 4.1 and demonstrate the performance of SPIRIT-FT in Section 4.2. Both sections include the discussion on the transferability of SPIRIT by investigating whether the reasoning step selection process generalizes across different models. For GPT-40-mini and GPT-3.5-Turbo, where direct perplexity computation is unavailable, we instead use LLaMA3.1-70B to estimate"}, {"title": "4.1 Few-shot CoT (SPIRIT-FS)", "content": "Datasets. We consider the Algebra-Linear-1d Task (AL1) and Number-Base-Conversion Task (NBC) from the Mathematics Dataset (Saxton et al., 2019) for the experiments. For both tasks we randomly select 500 examples for evaluation.\nLanguage Models. Our experiments use five LLMs: GPT-3.5-Turbo (Brown, 2020), GPT-40-mini (Brown, 2020), LLaMA3-8B-Instruct, LLaMA3.1-70B-Instruct (Grattafiori and et al., 2024) and Qwen2.5-7B-Instruct (Team, 2024) (LLaMA3-8B, LLaMA3.1-70B, Qwen2.5-7B in short). The temperature is set to 0 to ensure deterministic outputs in generation. Notably, when applying our algorithm to open-source models (LLaMA3-8B, LLaMA3.1-70B, and Qwen2.5-7B), we use the corresponding model to compute perplexity and refine the reasoning demonstrations."}, {"title": "4.2 Fine-Tuning (SPIRIT-FT)", "content": "Datasets. We consider two main datasets including GSM8K (Cobbe et al., 2021) and Meta-MathQA (Yu et al., 2023). For GSM8K, the entire training set (with 7.4k examples) is utilized for example refinement and fine-tuning, with evaluation performed on the full evaluation set (with 1.3k examples). For MetaMathQA, we randomly select 19k examples for refinement and fine-tuning, while 1.95k examples are selected as the testing data.\nLanguage Models. Our main experiments involve two LLMs: LLaMA3-8B-Instruct and Qwen2.5-7B-Instruct (LLaMA3-8B, Qwen2.5-7B in short).\nFine-tuning Methods. We consider two fine-tuning methods including Supervised Fine-tuning (SFT) and Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024). We applied LORA (Hu et al., 2022) for both methods.\nProcedures. We applied SPIRIT-FT to refine the reasoning paths, fine-tuned the model with the refined data, and evaluated the fine-tuned model by measuring both prediction accuracy and the number of generated tokens. The trade-off between accuracy and efficiency was controlled by adjusting t2, which determines the extent of step removal/merging. Notably, when fine-tuning with different models, we used the specific model itself to compute perplexity for unimportant step determination. We present the relationship between accuracy and efficiency across different models and different datasets in Figure 3 and 4 for SFT and ORPO, respectively. The results are labeled as \"Min PPL (merge)\".\nFor evaluation, in the experiments of SFT, we compare SPIRIT-FT with three control sets, (1) a variant of SPIRIT-FT where we only remove but not merge steps (\"Min PPL (remove)\"); (2) randomly select steps to be removed (\"Randomly remove\"); and (3) applying an inverse of Algorithm 2 to remove the most important steps whose removal maximize the perplexity (\"Max PPL (Remove)\"). For ORPO, we utilize some of the above datasets to form chosen/rejected pairs: (1) Chosen:"}, {"title": "5 Related Works", "content": "Inference-Stage Techniques in LLM Reasoning. Many studies aim to enhance LLM reasoning at the inference stage, without modifying model weights. Different from the aforementioned literature, our work examines the importance of each reasoning step.\nCoT Fine-Tuning. In literature and real practice, there are two common types of LLM fine-tuning methods: supervised fine-tuning (SFT) and reinforcement learning (RL)-based alignment methods.\nSFT is commonly used to adapt an LLM to downstream task, and various studies have investigated SFT. Other works focus on"}, {"title": "6 Conclusion", "content": "In this paper, we introduce SPIRIT, a method for refining reasoning steps in few-shot CoT and CoT fine-tuning for improving reasoning efficiency while maintaining accuracy. Based on the observation that changes in perplexity correlate with reasoning step importance, SPIRIT works by iteratively identifying unimportant steps through evaluating the change in perplexity, then merge the unimportant steps. Experiments demonstrate the effectiveness of SPIRIT in improving the trade-off between accuracy and efficiency in both few-shot CoT and CoT in fine-tuning."}, {"title": "Limitations", "content": "While the main observation in Section 4.2 is on the transferability of the algorithm, we also observe that the perplexity from the stronger model (LLaMA3-8B) works even better than using the weaker model's own perplexity (Qwen1.5-7B and LLaMA2-7B) in selecting the unimportant reasoning steps. This implies that perplexity contains more information than what is needed in SPIRIT, indicating the potential limitation of using perplexity in the algorithm. We believe this observation can inspire future works in data attrition and data selection to consider the model's own capability.\nAnother limitation is that, in the algorithm and experiments, we assume reasoning steps among few-shot examples match with each other sentence by sentence. This can be further enhanced if the reasoning steps match the general pattern. However, since different tasks have diverse reasoning patterns, we anticipate that such an enhancement should be specifically designed for the given task and dataset."}, {"title": "A Ablation Studies", "content": "In this section, we conduct ablation studies by applying different variations of SPIRIT-FT to validate the reasonableness behind the key components in the design of the algorithm:\n(1) Always applying merging and no removal: Instead of comparing the effects of merging and removal, we modify the approach to always apply merging after selecting a step for refinement.\n(2) Removing the threshold t\u2081:, meaning that after determining which step to remove, we no longer check if the resulting perplexity is below a threshold. Instead, we always proceed with merging and then compare the effects of merging versus removal.\nFrom the results in Figure 6, we observe that always applying merging leads to performance comparable to the original algorithm, when the number of generated tokens is high. In addition, Figure 7 shows that, when removing t\u2081 threshold, performance appears to improve slightly."}, {"title": "B Additional Implementation Details and Adjustments", "content": "Perplexity Calculation Adjustment In practice, when calculating the perplexity, the computation starts from the second token rather than including the first generated token. Alignment Adjustment for Qwen2.5-7B Fine-Tuning. Notably, when applying fine-tuning to Qwen2.5-7B, a challenge is that standard LoRA sometimes failed to achieve proper alignment between the model's generation and the fine-tuning data, particularly when more removal was involved."}, {"title": "C Additional related works", "content": "Test-Time Scaling Law. While our method focuses on enhancing the reasoning efficiency through removing unimportant reasoning steps from the data, one may question whether this contradicts to the test-time scaling law. Our method is perpendicular to the test-time scaling law, and the idea of removing unimportant reasoning steps in our work is also applicable to the test-time methods to reduce the computation cost as well."}, {"title": "D Additional Experiment Details.", "content": "Hyperparameters in Fine-tuning. For SFT, we set the batch size to 128, the learning rate to 5e-5, and the training epoch to 3.0 for all datasets. For ORPO, the batch size is 64, learning rate is 5.0e-6 and training epoch is 5.0."}, {"title": "E Additional Validation to Support the Design of t\u2081.", "content": "In this section, we provide additional empirical experiment to demonstrate that when PPLrem is larger, it is more necessary to conduct merging.\nWe manually examine several removal cases, where a reasoning step is eliminated, and categorize them into three classes:\n(1) No coherence issue \u2013 Removing the step does not disrupt reasoning, so merging is unnecessary.\n(2) Minor coherence issue \u2013 Removing the step slightly affects coherence; merging is beneficial but not essential.\n(3) Obvious coherence issue \u2013 Removing the step leads to a clear loss of coherence, making merging necessary.\nFor each case, we compute the perplexity change ratio (after removal / before removal) and plot the results in Figure 8."}, {"title": "F Prompt for Instructing Merging", "content": "Example 1:\nQ: A portable computer drive has enough kilobytes of storage space to store 2000 photos. Each photo is 1.5 kilobytes in size. How many 200-kilobyte videos can it store if it already has 400 photos on it? Let's think step by step.\nA: The drive has 2000 photos * 1.5 kilobytes = 3000 kilobytes of storage space.\nIt already has 400 photos * 1.5 kilobytes = 600 kilobytes of photos on it.\nSo, it has 3000 - 600 = 2400 kilobytes of storage space left.\nEach video is 200 kilobytes in size.\nThe answer is 12\nAfter removing 'The drive has 2000 photos * 1.5 kilobytes = 3000 kilobytes of storage space.', the answer become:\nA: The drive already has 400 photos * 1.5 kilobytes = 600 kilobytes of photos on it.\nSo, it has 2000 photos * 1.5 kilobytes - 600 kilobytes = 2400 kilobytes of storage space left.\nEach video is 200 kilobytes in size.\nThe answer is 12\nExample 2:\nQ: Leah bought 3 boxes of birdseed. Each box contains 225 grams, for how many weeks can she feed her birds without going back to the store? Let's think step by step.\nA: Leah has a total of 3 + 5 = 8 boxes of birdseed. Each box contains 225 grams of birdseed, so she has a total of 8 * 225 = 1800 grams of birdseed.\nHer parrot eats 100 grams of seeds each week and her cockatiel eats 50 grams of seeds in a week, so together they eat 100 + 50 = 150 grams of seeds per week.\nTo find out how many weeks the birdseed will last, we can divide the total amount of birdseed (1800 grams) by the amount eaten per week (150 grams):\n1800/150 = 12 weeks\nThe answer is 12\nAfter removing 'Leah has a total of 3 + 5 = 8 boxes of birdseed.', the answer become:\nA: Each box contains 225 grams of birdseed, so Leah has a total of (3 + 5) * 225 = 1800 grams of birdseed.\nHer parrot eats 100 grams of seeds each week and her cockatiel eats 50 grams of seeds in a week, so together they eat 100 + 50 = 150 grams of seeds per week.\nThe answer is 12\nExample 3: Joy can read 8 pages of a book in 20 minutes.\nA: To find out how many minutes it takes her to read 1 page, we divide 20 minutes by 8 pages: 20 minutes / 8 pages = 2.5 minutes per page. The answer is 5\nLearn from the above example to do the following modification. Remember not to change the final results (the number after 'The answer is')."}]}