{"title": "Accelerating Model-Based Reinforcement Learning with State-Space World Models", "authors": ["Maria Krinner", "Elie Aljalbout", "Angel Romero", "Davide Scaramuzza"], "abstract": "Reinforcement learning (RL) is a powerful approach for robot learning. However, model-free RL (MFRL) requires a large number of environment interactions to learn successful control policies. This is due to the noisy RL training updates and the complexity of robotic systems, which typically involve highly non-linear dynamics and noisy sensor signals. In contrast, model-based RL (MBRL) not only trains a policy but simultaneously learns a world model that captures the environment's dynamics and rewards. The world model can either be used for planning, for data collection, or to provide first-order policy gradients for training. Leveraging a world model significantly improves sample efficiency compared to model-free RL. However, training a world model alongside the policy increases the computational complexity, leading to longer training times that are often intractable for complex real-world scenarios. In this work, we propose a new method for accelerating model-based RL using state-space world models. Our approach leverages state-space models (SSMs) to parallelize the training of the dynamics model, which is typically the main computational bottleneck. Additionally, we propose an architecture that provides privileged information to the world model during training, which is particularly relevant for partially observable environments. We evaluate our method in several real-world agile quadrotor flight tasks, involving complex dynamics, for both fully and partially observable environments. We demonstrate a significant speedup, reducing the world model training time by up to 10 times, and the overall MBRL training time by up to 4 times. This benefit comes without compromising performance, as our method achieves similar sample efficiency and task rewards to state-of-the-art MBRL methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Robot learning has proven to be a very successful paradigm for acquiring complex robotic skills. Recent work has demonstrated the applicability of reinforcement learning (RL) approaches to manipulation [1, 2, 3, 4], locomotion [5, 6], and aerial robotics [7, 8]. For such methods to succeed, they either require encoding the task into a reward function or access to a dataset of expert demonstrations. Learning control policies reduces the barrier to skill acquisition in robotics, requiring less human interventions and efforts to equip a robot with a new skill. In addition, RL methods impose minimal constraints in the design of reward functions. As a result, they allow for greater flexibility in the design of robotic systems, which can lead to outperforming classical control approaches [8, 9].\nDespite their success, one major challenge in robot learning is the need for large amounts of physical interaction data, which can be very expensive and challenging to obtain. In addition, RL suffers from high-variance gradients, often leading to unstable training and raising the need for even more data samples. This problem is further exacerbated when learning vision-based control policies due to the high-dimensionality of image-based observations.\nTo alleviate this problem, multiple solutions have been proposed to leverage prior knowledge in RL. These solutions range from pretraining the policy or parts of it using either imitation learning [10] or self-supervised learning objectives [11, 12, 13], to embedding inductive biases into"}, {"title": "II. RELATED WORK", "content": "RL policy architectures and training pipelines to reduce the policy complexity [14, 15]. Model-based RL has emerged as a promising alternative to improve sample efficiency in comparison to model-free RL [16, 17, 18, 19].\nHowever, model-based RL methods are typically slower in training than their model-free counterparts because they additionally train the world model (WM), which can be slow due to the sequential nature of the dynamics. This aspect limits the applicability of MBRL, particularly for cases where fast training is desirable.\nIn this work, we propose a method for accelerating model-based reinforcement learning (MBRL). We leverage state-space models (SSMs) to parallelize the sequence dimension of the world model, thereby reducing the computational complexity of MBRL. We build on the family of Dreamer-based MBRL methods [18, 20]. In our approach, we replace the recurrent state-space model (RSSM) with a modern parallelizable SSM as the dynamics model.\nTo evaluate our approach, we perform experiments in a drone racing environment, involving complex dynamics, for both fully and partially observable environments. We then compare our method to state-of-the-art model-free and model-based RL methods. Our method achieves a significant speedup, reducing the overall training time by up to 4 times. This benefit comes without compromising performance, as our method attains similar sample efficiency and task rewards as state-of-the-art methods."}, {"title": "Contributions", "content": "Our contributions are summarized as follows:\n\u2022 We present a method for leveraging state-space models to accelerate the sequence model training in MBRL world models.\n\u2022 Our method achieves comparable sample efficiency to state-of-the-art MBRL methods while reducing world model training time by up to 10 times, and the overall MBRL training time by up to 4 times.\n\u2022 We present an approach to facilitate the sim-to-real transfer of our vision-based policies that leverage the privileged state information while training the vision-based world model.\n\u2022 We demonstrate our approach in the real-world robotic task of agile quadrotor flight, which involves nonlinear complex dynamics."}, {"title": "A. Model-Based RL", "content": "While model-free RL methods, such as proximal policy optimization (PPO), have been prevalent in robot learning [6, 1, 21, 8, 2, 3], model-based RL is gaining popularity due to its sample efficiency. The main difference between the two paradigms is that MBRL learns a model of the environment's dynamics, in addition to the policy [22]. This model, known as world model, can either be used for i) planning and control [23, 24], ii) sampling environment interactions for policy training [25, 26, 27], or iii) obtaining first-order policy gradients through imagination [17, 18, 20]. World models play a crucial role in MBRL, with recurrent neural networks (RNNs) being the most widely adopted architecture. One popular RNN-based world model architecture is the Recurrent State-Space Model (RSSM), introduced in the Dreamer framework [28]. However, RSSMs struggle to scale efficiently to long sequences, as their computational complexity increases significantly with the sequence length.\nRecent works have explored alternative architectures, such as Transformers and SSMs. Transformers, in particular, have been widely adopted as world model backbones in MBRL, showing advantages both for sample efficiency and computational complexity [29, 30, 31, 32]. Similarly, in [33] an S4-based world model is introduced. However, this work does not explore the usage of such world models for MBRL and focuses on mastering predictions in long-range memory tasks.\nOne of the earliest applications of MBRL to robotics was shown on a low-cost manipulator and using a Gaussian process-driven policy representation [16]. Later work adopted a combination of deep dynamics models with model pre-dictive control to scale this concept to vision-based robotics tasks [34]. Nagabandi et al. [23] proposed using MBRL to gather expert data for training an initial policy using model-free methods. By doing so, they managed to alleviate the inferior task performance of MBRL methods. Chua et al. [35] proposed using ensembles of dynamics model to enable uncertainty propagation during trajectory sampling. These approaches have also been extended to jointly learn a value function together with the dynamics sequence model [24, 36]. Most of these methods perform receding horizon control with a sampling scheme.\nAnother line of work leverages first-order policy gradients that are backpropagated through a learned latent dynamics model [17, 18, 20], which has been successfully applied to multiple robotic tasks in manipulation, locomotion, and drone flight [37, 38, 39, 40, 19, 41, 42]."}, {"title": "B. Vision-Based Robot Learning", "content": "While state-based RL can be challenging, learning vision-based policies presents additional difficulties. First, the higher dimensionality of the observation space makes policy search more complex, as the model must process significantly larger inputs. Second, tasks relying on visual inputs often suffer from partial observability, where the raw input does not provide a complete representation of the state. Therefore, such policies typically require additional supervision signals to effectively learn informative latent representations from raw pixel data. As a result, vision-based RL often requires a large amount of data to achieve good performance, as individual samples tend to be less informative compared to state-based environments [43, 44].\nSeveral strategies have been proposed to improve the sample-efficiency of vision-based RL. One prominent ap-proach is to embed auxiliary losses to provide additional signals for training the perception components of the policy. Auxiliary losses are typically inspired by the self-supervised"}, {"title": "III. PRELIMINARIES", "content": "Given a reward function, RL automates policy learning\nor search, by maximizing the cumulative reward in a given\nenvironment [22]. Tasks are usually formulated as Markov\nDecision Processes (MDP) [54]. A finite-horizon, discounted\nMDP is defined by the tuple  M = (S, A, P, r, \u03c1\u2080, \u03b3, T), \nwhere S and A are the state and action spaces respectively,\nP : S \u00d7 A \u2192 S is the transition dynamics, and r : S \u00d7 A \u2192 R\nthe reward. Additionally, the MDP definition includes an initial\nstate distribution \u03c1\u2080, a discount factor \u03b3\u2208 [0, 1], and a horizon\nlength T. The optimal policy \u03c0 : S \u2192 P(A) maximizes the\nexpected discounted reward as an objective function,\n$$J(\u03c0) = \\mathbb{E}_{\u03c4} [\\sum_{t=0}^{T-1} \u03b3^{t}r(s_t, a_t)]$$\n(1)\nIn MBRL, in addition to learning the policy, we learn a model\nof the environment dynamics, usually referred to as world\nmodel of the form WM: O\u00d7A \u2192 O \u00d7 R. The world\nmodel approximates the environment's transition dynamics\np(st+1 | st,at) and rewards r(st, at) providing a simulated\nenvironment in which the agent can perform planning or op-\ntimization without requiring actual environment interactions.\nThis approach is often referred to as learning in imagination.\nTraining typically follows an iterative process composed of\nthree steps. The first step is to train the world model using\nthe collected data. In the second step, we optimize the policy\nwithin the WM-based simulated environment. In the third step,\nwe collect new data by interacting with the actual environment\nusing our latest policy."}, {"title": "B. State-Space Models", "content": "State space models (SSMs) provide an efficient framework\nfor sequence modeling tasks and have shown great success\nin capturing long-term temporal and spatial dependencies [55,\n56, 57]. At time t, a first-order linear system maps the input\nu(t) \u2208 \u211d\u1d34 to the output y(t) \u2208 \u211d\u1d3e via the following system\ndynamics,\n$$\\frac{dx(t)}{dt} = Ax(t) + Bu(t)$$\n$$y(t) = Cx(t) + Du(t),$$\n(2)\nwhere A, B, C, D are learnable parameters which capture the\nspecific dependencies in the data, and x(t) is the hidden state.\nTo model sequences with a fixed time step \u0394, the system is\ndiscretized as follows,\n$$x_t = \\overline{A}x_{t-1} + \\overline{B}u_t$$\n$$y_t = Cx_t + Du_t,$$\n(3)\nwhere \\(\\overline{A}, \\overline{B}, C, D\\) are the discretized matrices, computed as\nfunctions of the continuous-time matrices A, B, C, D.\nGiven a sequence of inputs e1:L, the parallel scan operation\nefficiently applies an associative binary operator \u2299, to its\ninputs. Specifically, for a sequence of length L, the parallel\nscan computes:\n$$[e_1, e_1 \u2299 e_2, e_1 \u2299 e_2 \u2299 e_3, ..., e_1 \u2299 e_2 \u2299 ... \u2299 e_L].$$\n(4)\nEquation 4 can be computed in \\(O(\\log(L))\\) using L parallel\nprocessors. S5 [56] is an SSM that leverages the parallel scan\nto compute the sequence of hidden states x1:L. In the context\nof S5, the binary operator is defined as\n$$a_i \u2299 a_j = (a_j, a_{j,a} A_{i,a}, a_{j,a} \u2297 A_{i,b} + a_{j,b}),$$\n(5)\nwhere \u2297 represents matrix-matrix multiplication and \u2295 rep-\nresents matrix-vector multiplication. The operator is applied\nto the initial elements ek, which are given by the pairs of\nmatrices\n$$e_k = (e_{k,a}, e_{k,b}) := (\\overline{A}, B u_k).$$\n(6)\nUsing this definition, the sequence of hidden states x1:L is\ncomputed iteratively as\n$$e_1 = (\\overline{A}, Bu_1) = (\\overline{A}, x_1)$$\n$$e_1 \u2299 e_2 = (\\overline{A}^2, \\overline{A} x_1 + Bu_2) = (\\overline{A}^2, x_2)$$\n(7)\nIn general, matrix-matrix multiplications with a matrix A\nwould incur a time complexity of O(P\u00b3). However, by di-\nagonalizing the system's matrices, the complexity is reduced\nto O(PL), where P is the size of the matrix and L is\nthe length of the sequence. As a result, the parallel scan\noperation is computed in \\(O(\\log(PL))\\), making it scalable to\nlong sequences."}, {"title": "IV. METHODOLOGY", "content": "In this work, we propose a model-based RL approach that\nleverages state-space world models. At each training step, we\nsample data from the replay buffer to train the world model\nas explained in section IV-A and the actor-critic as explained\nin section IV-B."}, {"title": "A. State-Space World Model", "content": "The proposed world model is based on DreamerV3 [20].\nHowever, we replace the recurrent state-space\nmodel (RSSM) [28] with an SSM as the sequence model\napproximating the transition dynamics. Our model includes\nan encoder for latent variable inference, which maps the\nobservations Ot, to posterior stochastic representations Zt.\nFor a prediction horizon H, the sequence model predicts"}, {"title": "B. Actor-Critic Training", "content": "In practice, the world model serves as an approximate dif-\nferentiable simulator providing first-order gradients for policy\nupdates [59]. This helps reduce variance estimation, especially\nfor systems with smooth dynamics [60]. Unlike model-free\nRL methods, which rely solely on actual environment inter-\nactions, in model-based RL, the actor-critic is trained using\nimagined trajectories generated by the world model. For a\ngiven horizon length H, we generate imagined trajectories by\nstepping through the world model, starting from the initial\nstates {z\u2080, x\u2080}. For t \u2208 {1, H},\n$$x_t, y_t = SSM(z_{t-1}, a_{t-1}, \\overline{c}_{t-1}, x_{t-1})$$\n$$z_t \u223c P(y_t)$$\n$$r_t \u223c P(y_t, z_t)$$\n$$c_t \u223c P(y_t, z_t)$$\n$$a_t \u223c \u03c0_\u03b8(\u00b7 | y_t, z_t).$$\n(13)"}, {"title": "C. Privileged World Models", "content": "In environments with high-dimensional and partially observ-able inputs, such as the ones encountered in vision-based RL, learning informative latent representations is challenging. Additional supervision signals are typically required to help the model learn meaningful representations, which are essential for efficient policy optimization [43, 44].\nWhen training policies in simulation, it is possible to take advantage of additional observations that are rarely available during real-world deployments, such information is typically referred to as privileged information and can be used in various ways to boost the training performance in simulation [61, 62, 42, 63]. We explore the use of privileged information for training the world model. In this setup, the world model has access to privileged information during training, which is not available during real-world deployment.\nIn this work, we specifically explore the use of privileged state observations in a vision-based setting. During training, the image observation, ot, is encoded into a posterior stochastic state, zt, to be processed by the sequence model (see section IV-A). We then replace the decoder from equation (8) with a privileged decoder that reconstructs the privileged state observation, st, rather than the image observation,\n$$\\widehat{s}_t \u223c p(\u00b7|y_t, z_t),$$\n(16)\nReconstructing the state involves predicting st \u2208 \u211d\u00b2\u2074, which\nis computationally more tractable than inferring a high di-mensional image using an expensive convolutional neural network (CNN) decoder. Furthermore, learning to reconstruct state observations, rather than images, provides a stronger signal for training the encoder. This encourages the latent representation to capture information that is more relevant for policy optimization, rather than optimizing for accurate image reconstruction."}, {"title": "V. EXPERIMENTS", "content": "We design the experiments to answer the following questions:\n\u2022 Does S5WM accelerate the training of model-based RL?\n\u2022 Does S5WM achieve comparable sample efficiency and task reward to state-of-the-art model-based RL?\n\u2022 What are the advantages of using a privileged world model in partially observable environments?\n\u2022 Does S5WM transfer from simulation to a real-world environment?\nWe validate our approach in a drone racing environment and compare our method against both model-based and model-free baselines. We then conduct a series of ablations that highlight several design choices of our method."}, {"title": "A. Setup", "content": "Our setup is consistent across all experiments and utilizes the same quadrotor configuration. We first perform training in a simulation environment. We then validate our approach in the real world. For the environment setup, we use a combination of the Flightmare [64] and Agilicious [65] software stacks. All experiments were conducted on the same hardware under uniform conditions."}, {"title": "1) Tasks:", "content": "We investigate two tasks, both involving a quadrotor flying in a drone racing environment.\nFor the first task, termed state-based Split-S, the quadrotor flies through the Split-S track and has access to the full-state observations. This task presents highly complex dynamics, requiring the policy to push the drone's agility to the physical limits of the platform. Due to the complexity of the track, the task demands both long-term memory and high precision, as the drone must plan ahead to navigate the complex maneuvers at high speeds.\nThe second task, termed vision-based Figure-8, involves fly-ing through the Figure-8 track and involves simpler dynamics, with the drone flying at a slower pace. However, the challenge in this task lies in learning the dynamics model from high-dimensional image observations, which can be challenging due to the partial observability of the resulting environment."}, {"title": "2) Observation Space:", "content": "For the state-based Split-S task,\nwe define the observation s = [p, R, v,w, i, d, aprev] \u2208 \u211d\u00b2\u2074,\nwhere p\u2208 \u211d\u00b3 is the position of the drone, R\u2208 \u211d\u2076 contains\nthe first two columns of the rotation matrix, v \u2208 \u211d\u00b3 is the\nlinear velocity, and w \u2208 \u211d\u00b3 is the angular velocity. The vector\ni \u2208 \u211d\u00b2 encodes the gate index using sine-cosine encoding to\naddress the periodicity of the track. The continuous gate index\nic is defined as\n$$i_c = i + \\frac{2}{1 + exp(k \u00b7 d)},$$\n(17)\nwhere d \u2208 \u211d is the distance to the next gate, and aprev \u2208 \u211d\u2074 represents the previous action.\nFor the vision-based Figure-8 task, the observation space is given by RGB images of size 64 \u00d7 64 \u00d7 3, rendered from the simulator [67]. The raw pixel-level input is then processed through a CNN encoder to extract relevant features. Similarly to [68], we include the previous 3 actions in the observation to provide historical context."}, {"title": "3) Action Space:", "content": "For both tasks, we define a = [c, wdes] \u2208 \u211d\u2074, where c is the mass-normalized collective thrust and wdes \u2208 \u211d\u00b3 represents the desired body rates. These commands are then processed by a low-level controller, which outputs the desired motor speeds."}, {"title": "4) Reward:", "content": "Similar to prior works on drone racing [69, 8,\n70], we encode the task using a dense reward function rt,\n$$r_t = \\begin{cases}\n    r_{crash} & \\text{crashed} \\\\\n    r_{pass} + r_{prog} + r_{omega} + r_{cmd} + r_{\\triangle cmd} & \\text{gate passed} \\\\\n    otherwise. \n\\end{cases}$$\n(18)\nwhere rcrash is a terminal penalty for collisions, rpass en-courages passing a gate, rprog encourages progress along the track, romega penalizes excessive body rates, rcmd penalizes aggressive actions, and r\u25b3cmd encourages smooth actions. The individual reward components are detailed in the appendix C."}, {"title": "B. Baselines", "content": "We compare our approach against both model-based and model-free baselines. For the model-based baseline, we use DreamerV3 [20, 53], while for the model-free baseline, we choose PPO [66]. For the vision-based task, we introduce an additional baseline that modifies DreamerV3 to decode privileged state information instead of observation decod-ing. We refer to this baseline as \"DreamerV3-P\". We also compare against a variant of our method which omits de-coding the privileged information and instead decodes raw observations. Our approach uses the same model architecture and hyperparameters as DreamerV3, with the exception of the world model configuration, where we replace the RSSM with S5WM, as introduced in section IV. We design S5WM to have a comparable number of parameters to RSSM, as detailed in the appendix A. We tune the hyperparameters of the baselines to ensure a fair comparison. For the state-based Split-S task, we use standard PPO, and for the vision-based Figure-8 task, we use PPO with the asymmetric actor-critic architecture [61], which provides privileged information to the critic, and employs the same CNN encoder architecture as used in our method and DreamerV3. We train the CNN encoder jointly with the policy. For each task, all models share the same observation, action, and reward configurations, as described in Section V-A."}, {"title": "C. Simulation Results", "content": "We train the policies in a high-fidelity simulator on an A100 GPU. For all experiments, we simulate 50 environments in"}, {"title": "2) Training Efficiency:", "content": "We profile S5WM, along with the baselines, to assess the differences in training times. Each training step is divided into three stages: i) training the world model (observation), ii) optimizing the policy (imagination), and iii) collecting new data by interacting with the environ-ment. Fig. 4 shows the average times for each stage, as well as the overall duration per step. We find that S5WM outperforms DreamerV3 in terms of overall training speed in both tasks, as the observations in S5WM are parallelized over the sequence length. In the state-based Split-S task, the observation step is up to 10 times faster, leading to an overall speedup of up to 4 times. In the vision-based Figure-8 task, the observation step is up to 4 times faster, leading to an overall speedup of up to 2 times. These benefits are most pronounced when the main computational bottleneck lies in modeling the dynamics, such as in the state-based Split-S task. In tasks that require learning a latent representation from complex visual inputs, such as the vision-based Figure-8 task, parallelizing the sequence model contributes less to the overall speedup."}, {"title": "D. Real World Deployment", "content": "We test our approach in the real world using a high-performance racing drone [65]. At deployment time, our method runs on an offboard desktop computer equipped with an Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz and Quadro RTX 4000 GPU. A Radix FC board equipped with the"}, {"title": "E. Ablations", "content": "We discuss several key components that we find to be essential to the success of our approach."}, {"title": "1) Horizon Length:", "content": "In model-based RL, the actor-critic uses the world model to plan over a prediction horizon, which can substantially improve sample efficiency compared to model-free RL. While a longer horizon is typically pre-ferred, prediction errors tend to compound over time and therefore limit the effectiveness of imagination. Additionally, imagination steps are computationally expensive and cannot be parallelized, making longer horizons computationally more expensive. To investigate the influence of the horizon length"}, {"title": "2) Reward Smoothing:", "content": "Reward smoothing is essential for enhancing training. In model-free RL, it is common to adapt the reward function during training to increase the task diffi-culty. While this helps the agent adapt to progressively harder settings, we find that directly altering the reward function in a model-based RL setting can be counterproductive, because the world model needs to unlearn the old reward signal and relearn the new one.\nTo address this issue, we propose a smoothing strategy that decouples the total reward into two complementary compo-nents,\n$$r = r_{nom} + r_{aug},$$\n(19)\nwhere rnom (the nominal reward) is active from the start of training, and raug (the smoothing reward) is learned as well from the start but only applied once the agent becomes suffi-ciently proficient. Initially, the value function only receives rnom, and later raug is also added. We use this strategy to"}, {"title": "3) Recurrent vs Parallel:", "content": "One main difference between our\nS5WM architecture (see section IV-A) and RSSMs lies in\nhow the posterior stochastic latent z\u0142 is computed. In RSSMs,\neach step computes a prior zt \u223c P(yt) and a posterior\nztq(yt, ot)), both of which include the deterministic\nrepresentation yt. Therefore, the posterior receives the same\ninformation as the prior plus access to the latest observation\not. By contrast, since S5WM processes the entire sequence in\nparallel, the posterior is determined only from the observation,\nzt ~ q(ot), and does not have access to historical context\nencoded in yt. To compensate for this, we append past actions to ot, as explained in section V-A3, providing S5WM with additional temporal context.\nWe investigate this difference by implementing a recurrent S5WM variant where the posterior also depends on yt, i.e., zt ~ q (yt, ot). We compare the recurrent S5WM to the standard parallel S5WM on the vision-based Figure-8 task. Note that the recurrent S5WM reintroduces the sequential dependencies of RSSMs, and therefore offers no speed-up in training. This comparison is included mainly to highlight the importance of historical context in partially observable environments."}, {"title": "VI. LIMITATIONS", "content": "While S5WM offers significant advantages in training\nspeed, it also has several shortcomings. First, SSMs can be\nsensitive to hyperparameters, especially compared to RSSMs,\nwhich appear more robust across different tasks. For instance,\nchoosing a suitable architecture of the S5 Block (see ap-\npendix D), which wraps the S5 layer with nonlinearities and\na suitable gating mechanism, is crucial to S5WM's success.\nThis sensitivity may stem from the rigid, linear nature of\nthe SSMs compared to more flexible architectures like Gated\nRecurrent Units (GRUs). However, a direct comparison of\nhyperparameter robustness with DreamerV3 would be unfair,\ngiven that the latter is fairly mature and has been extensively\nrefined over the years to improve robustness. A more com-\nprehensive hyperparameter search for S5WM could improve\nour understanding of its internal workings, though such an\nanalysis would be computationally expensive and remains to\nbe explored in future work.\nFinally, the computational benefits of SSMs are most promi-nent when the bottleneck lies in modeling the dynamics.\nIn tasks where the bottleneck shifts to other factors, such\nas in learning latent representations from high-dimensional\nvisual observations, the advantages of SSMs become less\npronounced."}, {"title": "VII. CONCLUSION", "content": "Despite their notable sample efficiency, model-based rein-forcement learning methods can be extremely slow to train, which limits their usability in robotic systems. This paper introduces S5WM, a state-space world model designed to accelerate training in model-based reinforcement learning. We adapt the S5 SSM architecture to handle resetability at the episode boundaries, and use it as a sequence model of the dynamics in our world model. Additionally, we introduce asymmetry in the world model by providing privileged state information during training in simulation.\nWe compare our approach against model-based and model-free baselines, focusing on training time, performance, and sample efficiency. Our method achieves faster training times without compromising performance and sample efficiency. Additionally, we test our approach on a drone racing envi-ronment and conduct real-world experiments for two distinct tasks involving both state and image observations. Our results"}]}