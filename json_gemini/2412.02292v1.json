{"title": "Deep Matrix Factorization with Adaptive Weights for Multi-View Clustering", "authors": ["Yasser KHALAFAOUI", "Basarab MATEI", "Martino LOVISETTO", "Nistor GROZAVU"], "abstract": "Recently, deep matrix factorization has been established as a powerful model for unsupervised tasks, achieving promising results, especially for multi-view clustering. However, existing methods often lack effective feature selection mechanisms and rely on empirical hyperparameter selection. To address these issues, we introduce a novel Deep Matrix Factorization with Adaptive Weights for Multi-View Clustering (DMFAW). Our method simultaneously incorporates feature selection and generates local partitions, enhancing clustering results. Notably, the features weights are controlled and adjusted by a parameter that is dynamically updated using Control Theory inspired mechanism, which not only improves the model's stability and adaptability to diverse datasets but also accelerates convergence. A late fusion approach is then proposed to align the weighted local partitions with the consensus partition. Finally, the optimization problem is solved via an alternating optimization algorithm with theoretically guaranteed convergence. Extensive experiments on benchmark datasets highlight that DMFAW outperforms state-of-the-art methods in terms of clustering performance.", "sections": [{"title": "1. Introduction", "content": "In the era of big data, one frequently encounters datasets with multiple sources or views, each offering a unique perspective on the underlying phenomena. These diverse views may capture distinct aspects, including textual information, visual features, or temporal dynamics. To exploit intrinsic information across these different views, substantial research efforts have been dedicated to the development and enhancement of multi-view clustering (MVC) [1, 2, 3, 4, 5, 6], with a particular emphasis on Matrix Factorization-based approaches [7, 8]. Notably, Non-negative Matrix Factorization (NMF) methods have demonstrated their effectiveness in handling high-dimensional data while capturing underlying structures across different views [9].\nNMF has been applied in a range of fields such as clustering [10], document understanding [11] and representation learning [12]. The core idea behind NMF is to decompose a given high-dimensional data matrix into two low rank matrices. Notably, NMF imposes a non-negativity constraint during the factorization process. This constraint simplifies the interpretation of the resulting matrices, allowing for a more intuitive and interpretable analysis [13]. By extending NMF to accommodate diverse data views, one is able to integrate and exploit the complementary information from these different views. This extension enhances the accuracy, robustness and interpretability of the clustering process. In the literature, many NMF-based multi-view clustering methods have been proposed. Some approaches [14, 15, 16] introduce a sparse model that learns discrete clustering labels based on the shared latent representation. Others [17, 18] propose a joint multi-view consensus clustering method to address late fusion (i.e., partition level) and the mutual update between the consensus partition matrix and the local partition matrices. However, most single layer NMF methods are unable to extract deeper and hidden information of data which may impact the clustering results.\nRecently, a number of deep NMF-based multi-view clustering methods have been developed. In order to guide the shared representation learning in each view, Zhao et al. [7] combine a deep semi-NMF structure to extract hidden information with a graph regularizer. Huang et al. [19] suggest utilizing a collaborative deep matrix decomposition framework to learn the hidden representations. To extract multi-view information, Zhang et al. [20] fused each view's partition representations, found by deep matrix decomposition, into a consensus partition representation.\nWhile deep NMF-based multi-view clustering approaches have shown promising results, significant challenges persist. A major issue is that these methods typically perform clustering across the entire feature space, without distinguishing between more and less important features, which can lead to suboptimal clustering results. Additionally, the effectiveness of these approaches is often hampered by the need for precise selection of various hyperparameters, such as the number of layers, the dimensions of each layer, and specially parameters that control the degree of feature selection (i.e., strong or weak feature selection). In the literature, the latter parameters are usually determined analytically and are not directly tied to the system's performance. This lack of a performance-driven mechanism for feature selection means that many approaches fail to accurately capture the most relevant features, ultimately limiting the model's ability to produce high-quality clustering results.\nIn order to address these issues, this paper proposes Deep Matrix Factorization with Adaptive Weights for Multi-view Clustering (DMFAW). The proposed method emphasizes the importance of feature selection for improving clustering results and employs a weighted Deep Semi-NMF to simultaneously generate local partition matrices and select important features. Additionally, the parameter controlling the degree of feature selection is updated dynamically via a method inspired by PI Stepsize Control approach from the Control Theory field [21]. Finally, a late fusion approach is applied to obtain a consensus partition matrix from the local partition matrices. Our contributions can be summarized as follows:\n\u2022 We propose DMFAW. A weighted Deep Semi-NMF approach is used for simultaneous generation of local partitions and feature selection, enhancing the multi-view clustering performance.\n\u2022 We introduce a dynamic feature selection parameter update mechanism inspired by Control Theory's PI Stepsize Control, enhancing model stability and adaptability to diverse datasets while accelerating convergence.\n\u2022 We conduct extensive experiments on real-world datasets, validating the effectiveness and efficiency of DMFAW. The results demonstrate better performance compared to other state-of-the-art methods."}, {"title": "2. Related Work", "content": "2.1. Multi-view Clustering\nIt aims to get a high-quality clustering result by utilizing heterogeneous information from different views. Kumar and Daum\u00e9 [1] propose a Co-training Approach for Multi-View Spectral Clustering, which combines semi-supervised learning and spectral clustering for multi-view data analysis. This approach alternates between self-training, in which the local clusterings mutually update the other views, and label propagation where the updated views are used to re-label the data points which in turn are used to refine the clustering results. Multi-View\n2.2. Deep Matrix Factorization\nIn many instances, the datasets we encounter encompass a variety of distinct features. To address this challenge, the concept of Deep Semi-NMF has emerged [12]. In this framework, a data matrix is factorized into m + 1 factors, while imposing a non-negativity constraint on the implicit representations. This constraint extends the interpretability of each layer's representation within this hierarchical structure, allowing for a natural clustering interpretation. Unfortunately, theses method can only handle single-view data. By combining deep matrix factorization with multi-view horizontal collaboration, Multi-view Clustering via Deep Matrix Factorization (DMF-MVC) [7] learns layer-wise latent representations, with each layer leveraging complementary information from previous layers. Furthermore, a constraint is imposed to ensure that multi-view data shares the same representation following multi-layer factorization. To preserve the geometric structure inherent in each data view, the authors introduce a graph Laplacian as a regularization term. However, it's worth noting that the authors empirically determine view weights. Auto-weighted Multi-View Clustering via Deep Matrix Factorization (Aw-DMVC) [19] addresses this critical challenge in multi-view learning by enabling automatic weight assignment to different views. This adaptive method improves the performance of the proposed approach and enhances its flexibility compared to methods that rely on manually assigned weights. However, Aw-DMVC doesn't incorporate ensemble learning. On the other hand, Multi-View Clustering via Deep Matrix Factorization and Partition Alignment (MVC-DMF-PA) [20] integrates representation learning and the late fusion stage within a framework, allowing them to mutually guide each other towards the generation of the consensus representation matrix. To guide the learning process, partition alignment is"}, {"title": "3. Methodology", "content": "In this section, we introduce a novel adaptive framework for multi-view clustering, termed Deep Matrix Factorization with Adaptive Weights for Multi-View Clustering (DMFAW). Our approach, illustrated in Fig. 1, enhances upon existing methods by integrating feature selection alongside a cross-domain Control Theory principle to dynamically update weight parameters. First, we show how DMFAW effectively captures important features for multi-view clustering task. Subsequently, we provide an in-depth explanation of our weights parameter update mechanism utilizing PI stepsize control. Finally, we present a multi-view late-fusion strategy and provide theoretical analysis."}, {"title": "3.1. Weighted Deep Matrix Factorization", "content": "Traditional matrix factorization methods are constrained by their inherent shallowness, limiting their capacity to uncover hierarchical features. They decompose the data matrix $X \\in \\mathbb{R}^{d\\times n}$, comprising d dimensions and n samples, into two factors $F \\in \\mathbb{R}^{d\\times k}$ and $G \\in \\mathbb{R}^{k\\times n}$, representing the mapping and partition matrices, respectively where k denotes the rank. On the other hand, deep matrix factorization draws inspiration from the successes of deep learning, enabling the extraction of multiple layers of features hierarchically, thus providing novel insights across a wide array of applications [22].\nGiven multi-view data matrices ${X^{(v)}}_{v=1}^V$ with n samples, V views and d, dimensions, deep matrix factorization decomposes each data matrix into m + 1 factors. Initially, it performs the first factorization $F_1G_1$. Subsequently, in a cascading manner, $G_1$ undergoes further decomposition into $F_2G_2$, and this process iterates until the last partition matrix $G_m$ is obtained. The objective function of multi-view deep matrix factorization is formulated as follows,\n$\n\\min_{\\substack{F^{(v)}, G_i^{(v)} \\i=1...m}} \\sum_{v=1}^V ||X^{(v)} - F_1^{(v)}F_2^{(v)}...F_m^{(v)}G_m^{(v)}||_F^2,\\\\\ns.t. F_i^{(v)} \\geq 0, G_i^{(v)} \\geq 0 \\qquad i = 1,2,\\dots,m.\n$\nAdditionally, when dealing with unconstrained input data matrices\u2014those that may contain mixed signs\u2014a Semi-NMF approach proves advantageous. In Semi-NMF, only one of the output matrices is constrained to have non-negative values, while the other remains unconstrained [23].\nExisting deep matrix factorization approaches tend to treat all data features equally, making them susceptible to the influence of irrelevant or noisy features [11]. To mitigate this, the proposed weighted deep matrix factorization method introduces a feature weighting process to better control feature relevance. It's defined as,\n$\n\\min_{\\substack{F^{(v)}, G_i^{(v)} \\i=1...m}} \\sum_{v=1}^V ||W^{(v)}(X^{(v)} - F_1^{(v)}F_2^{(v)}...F_m^{(v)}G_m^{(v)})||_F^2,\n$\n$\ns.t. \\sum_d (W_d^{(v)})^p = 1,\n$\nwhere $W^{(v)} \\in \\mathbb{R}^{d_v\\times d_v}$, is a diagonal matrix indicating the weights of the features of $X^{(v)}$, and p, which is introduced in the next section, is a parameter that controls the"}, {"title": "3.2. Adaptive Feature Selection", "content": "One of the challenging aspects of multi-view clustering is choosing the right parameters, and particularly in our case those controlling feature selection among different views. Existing approaches often rely on empirical or analytical methods to determine these parameters. However, these static approaches may fall short in capturing the dynamic and intricate relationships inherent in multi-view data [11, 7, 18]. In contrast to these methodologies, we introduce a novel approach inspired by control theory principles to dynamically update the aforementioned parameters.\nThe integration of control theory techniques, particularly the Proportional-Integral (PI) controller, offers an interesting approach for enhancing adaptability and optimizing model performance [24]. The PI controller is known for its ability to dynamically adjust system parameters based on the integral of past errors and the current error. In the context of machine learning and multi-view clustering, the PI controller becomes an interesting tool to dynamically adjust parameters and steer the model towards optimal solution.\nAt each iteration, the model computes the global loss, representing the disparity between the input data matrix $X^{(v)}$ and the corresponding factorization $F_1^{(v)}F_2^{(v)}...F_m^{(v)}G_m^{(v)}$ and between the consensus $G^*$ and local partition matrices ${G_i^{(v)}}_{i=1}^V$. Then, both the past and current losses are evaluated, guiding the adaptive update of the parameter p, which controls the degree of feature selection. The iterative process continues, while contributing to the solution convergence and stability.\nFollowing the work related to PI stepsize control [21], which has been shown to enhance the regularity of error estimates, we define our adaptive feature selection parameter term as follows:\nDefinition 1. Let p be the weight parameter, closs and ploss represent the current and previous computed losses, respectively. Tol is the tolerance for the current loss per iteration. The update rule for p is defined as,\n$\np \\leftarrow p \\cdot \\left(\\frac{Tol}{\\vert ploss \\vert} \\right)^{n_1} \\left(\\frac{\\vert ploss \\vert}{\\vert closs \\vert} \\right)^{n_2},\n$"}, {"title": "3.3. Learning the Consensus Partition", "content": "Building upon the methodologies proposed by [20, 3] for late fusion, we derive the consensus partition matrix $G^*$ from the local partitions ${G_i^{(v)}}_{i=1}^V$ obtained from each individual view. This is achieved by maximizing the alignment between the local partition matrices and the consensus partition matrix through an optimal permutation matrix $M \\in \\mathbb{R}^{k\\times k}$. This permutation matrix unifies the different representations present in each local partition matrix. Additionally, we introduce the matrix $A \\in \\mathbb{R}^{n\\times n}$ which represents the average partition region. The latter helps prevent the consensus partition $G^*$ from deviating from the average partition observed prior to the fusion process.\nEventually, our proposed deep matrix factorization with adaptive weights model can be formulated as,\n$\n\\min_{\\substack{F^{(v)}, G_i^{(v)}, G^* \\ M^{(v)}, W^{(v)}, \\beta^{(v)}}} \\sum_v ||W^{(v)}(X^{(v)} - F_1^{(v)}F_2^{(v)}...F_m^{(v)}G_m^{(v)})||^2 - Tr(G^*A \\sum_v\\beta^{(v)}G_m^{(v)T}M^{(v)}),\n$\n$\ns.t. G^{(v)} \\geq 0, M^{(v)}M^{(v)T} = I_k, \\sum_d (W_d^{(v)}) = 1,\\beta^{(v)} \\geq 0,\n$\nwhere $\\beta^{(v)}$ is the weighting coefficient of each local partition."}, {"title": "4. Optimization", "content": "In the following, we derive a six-step alternate optimization algorithm in order to solve Eq. (5). Note that, for each view, we need to optimize $F_i^{(v)}$ and $G_i^{(v)}$ layer by layer, i.e., first $F_1^{(v)}$ and $G_1^{(v)}$ until $F_m^{(v)}$ and $G_m^{(v)}$ are updated. Following [25, 20] we implement a clustering-based initialization, using Semi-NMF, for all the factors $F_i^{(v)}, G_i^{(v)}$ in order to mitigate the problem of non-uniqueness of the aforementioned factorization, and expediate the approximation of the variables."}, {"title": "Subproblem of updating G*", "content": "With $F^{(v)}, G_i^{(v)}, W^{(v)}, M^{(v)}, \\beta^{(v)}$ fixed, the optimization Eq. (5) can be written as follows,\n$\n\\min_{G^*} - Tr(G^*U), \\qquad st. G^*G^{*T} = I_k,\n$\nwhere $U = A \\sum_{v=1}^V \\beta^{(v)}G_m^{(v)T}M^{(v)}$. This problem can be solved by taking the singular value decomposition (SVD) of the given matrix $U$. Furthermore, there exists a closed-form solution, which is provided by the following Theorem.\nTheorem 1. If the matrix U, defined previously, has an economic rank-k singular value decomposition form, then the optimization problem in Eq. 6 has a closed-form solution defined as,\n$\nG^* = VS^T,\n$\nwhere $V \\in \\mathbb{R}^{k\\times k}$ and $S \\in \\mathbb{R}^{n\\times k}$ are the right and left singular vectors respectively.\nProof. The matrix $U$ can be expressed in terms of its singular value decomposition as $U = SDV^T$. We can then rewrite Eq. (6) as follows,\n$\n\\min_{G^*} - Tr(G^*SDV^T), \\qquad st. G^*G^{*T} = I_k.\n$\nSince S and V are orthogonal matrices, the optimization problem is equivalent to,\n$\n\\min_{G^*} - Tr(G^*D), \\qquad st. G^*G^{*T} = I_k.\n$\nUtilizing the orthogonality constraint and the properties of orthogonal matrices, a closed-form solution for $G^*$ exists and is defined in Eq. (7). This completes the proof."}, {"title": "Subproblem of updating $F_i^{(v)}$", "content": "With $G_i^{(v)}, W^{(v)}, M^{(v)}, G^*, \\beta^{(v)}$ fixed, the optimization problem in Eq. (5) is equivalent to,\n$\n\\min_{F_i^{(v)}} C = \\min_{F_i^{(v)}} ||X^{(v)} - Z^{(v)}F_i^{(v)}G_i^{(v)}||_F^2,\n$\nwhere $Z = F_1^{(v)}...F_{i-1}^{(v)}$. Setting $\\partial C/\\partial F_i^{(v)} = 0$, we get the following solution\n$\nF_i^{(v)} = Z^{(v)}X^{(v)}G_i^{(v)\\dag},\n$\nwhere $^\\dag$ represents the Moore-Penrose pseudo-inverse."}, {"title": "Subproblem of updating $G_i^{(v)}$", "content": "($i < m$). With $F_i^{(v)}, W^{(v)}, M^{(v)}, G^*, \\beta^{(v)}$ fixed, the optimization problem in Eq. (5) can be written as follows,\n$\n\\min_{G_i^{(v)}} C = \\min_{G_i^{(v)}} ||X^{(v)} - Z^{(v)}F_i^{(v)}G_i^{(v)}||_F^2\n$\nFollowing [7], the update rule for $G_i^{(v)}$ ($i < m$) is defined as,\n$\nG_i^{(v)} \\leftarrow G_i^{(v)} \\cdot \\frac{[Z^TW^{(v)}X^{(v)}]^+ + [Z^TW^{(v)}Z G_i^{(v)}]^-\n}{\n[Z^T W^{(v)}X^{(v)}]^- + [Z^TW^{(v)}Z G_i^{(v)}]^+\n}$\nwhere $[A]^+ = (\\vert A \\vert + A)/2$ and $[A]^- = (\\vert A \\vert - A)/2$ are element-wise operations."}, {"title": "Subproblem of updating $G_m^{(v)}$", "content": "With $F_i^{(v)}, W^{(v)}, M^{(v)}, G_i^{(v)}$ ($i < m$), $G^*, \\beta^{(v)}$ fixed, the optimization problem in Eq. (5) is defined as,\n$\n\\min_{G_m^{(v)}} ||X^{(v)} - Z F_i^{(v)}G_m^{(v)}|| - \\lambda \\beta^{(v)} Tr(G^* A G_m^{(v)T} M^{(v)}).\n$\nThe update formula of $G_m^{(v)}$ is written as follows,\n$\nG_m^{(v)} \\leftarrow \\sqrt{U_n/U_d},\n$\n$\nU_n = [Z^TW^{(v)}X^{(v)}]^+ + [Z^TW^{(v)}Z G_i^{(v)}]^- + \\lambda \\beta^{(v)}[M^{(v)} G^*A]^+,\n$\n$\nU_d = [Z^TW^{(v)}X^{(v)}]^- + [Z^TW^{(v)}Z G_i^{(v)}]^+ + \\lambda \\beta^{(v)}[M^{(v)} G^*A]^-.$\nTheorem 2. The solution of the update rule in Eq. (15) satisfies the KKT conditions [26] and holds convergence property.\nProof. We define the Lagrangian function as follows,\n$\nL(G_m^{(v)}) = \\sum_v ||W^{(v)}(X^{(v)} - F_1^{(v)}F_2^{(v)}...F_i^{(v)}G_i^{(v)})|| - \\lambda Tr(G^*A \\beta^{(v)} G_m^{(v)T} M^{(v)}) - \\eta G_m^{(v)},\n$\nwhere $\\eta$ is a Lagrange multiplier. The complementary slackness condition gives,\n$\n\\frac{\\partial L(G_m^{(v)})}{\\partial G_m^{(v)}} = (2Z^T W^{(v)}(Z G_i^{(v)} - X^{(v)}) - \\lambda \\beta^{(v)} M^{(v)} G^*A)G_m^{(v)} = \\eta G_m^{(v)} = 0.\n$"}, {"title": "Subproblem of updating W(v)", "content": "Optimizing Eq. (5) with respect to $W^{(v)}$ and its constraint is equivalent to optimizing,\n$\nc = \\sum_i W_i^{(v)} u_i - \\alpha (\\sum_i (W_i^{(v)})^p - 1),\n$\n$\ns.t. u_i = \\sum_{j} (X_j^{(v)} - Z F_i^{(v)}G_i^{(v)}).\n$\nSetting $\\frac{\\partial c}{\\partial W_i} = 0$, and using the KKT complementary slackness condition, we get the following updating formula,\n$\nW_i^{(v)} = \\left[ \\frac{1}{\\sum_i u_i^{\\frac{1}{p-1}}} \\right]^{\\frac{1}{p-1}}.\n$"}, {"title": "Subproblem of updating M(v)", "content": "With $F_i^{(v)}, G_i^{(v)}, W^{(v)}, G^*, \\beta^{(v)}$ fixed, the optimization Eq. (5) can be written as follows,\n$\n\\min_{G^*} - Tr(M^{(v)}U), \\qquad s.t. M^{(v)}M^{(v)T} = I_k,\n$\nwhere $U = \\beta^{(v)}G_m^{(v)}A^TG^{*T}$. The problem in Eq. (22) could also be solved by taking the singular value decomposition of $U$. Moreover, according to Theorem 1, this optimization problem has a closed-form solution."}, {"title": "Subproblem of updating \u03b2(v)", "content": "With $F_i^{(v)}, G_i^{(v)}, W^{(v)}, G^*, M^{(v)}$ fixed, the optimization Eq. (5) can be written as follows,\n$\n\\max_{\\beta^{(v)}} \\beta_i w \\qquad s.t. ||\\beta^{(v)}||_2 = 1, s.t. \\beta^{(v)} \\geq 0,\n$\nwhere $w = Tr(G_m^{(v)}M^{(v)}G^*A)$. This problem could be solved with a closed-form solution as follows,\n$\n\\beta^{(v)} = \\frac{\\omega_i}{\\sqrt{\\sum \\omega^2}}.\n$"}, {"title": "4.1. Discussion", "content": "Weight Parameter. It is important to note that in Eq. (21), by dynamically adjusting p, we can control the degree of feature selection. A smaller p leads to stronger feature selection (highlighting important features), while a larger p results in weaker feature selection (treating all features more equally). This adaptability, provided by Eq. (3) is crucial because different datasets may require different levels of feature selection. For example, in some cases, emphasizing only the most critical features can lead to better clustering, while in others, a more balanced consideration of all features might be preferable.\nComputational Complexity. The proposed algorithm is composed of two stages, which are analyzed separately. To simplify the analysis, we assume that all the layers have the same dimensions l. All the data views have the same features d, t the number of iterations for both stages, V the number of views and m the number of layers. The complexity of pre-training and fine-tuning stages is O(Vmt(dnl +\nnl\u00b2 + ld\u00b2 + ln\u00b2 + dn\u00b2 + n\u00b2)) and O(Vmt(dnl + nl\u00b2 + dl\u00b2 + nk\u00b2 + kn\u00b2)) respectively. Since l \u2264 d and k < n, the time complexity of DMFAW is O(Vmt(dnl + ld\u00b2 + dn\u00b2)) + O(Vmt(dnl + nl\u00b2 + dl\u00b2 + kn\u00b2))."}, {"title": "5. Experiments", "content": "5.1. Experimental setup\nDatasets. We used six benchmark multi-view datasets to assess the performance of our proposed method, i.e., Caltech101-all and Caltech101-7 [27], BBC, BBC-Sport [28], Handwritten[29], ORL[30]. The details about these datasets are listed in Table 2.\nCompared methods. DMFAW is compared with two co-training methods Co-reg [31] and Co-train [32], and seven matrix decomposition models MultiNMF [9], DMVC [7], MVCF [33], ScaMVC [34], AwDMVC [19], MVC-DMF-PA [20] and MCDS [35].\nMetrics. Since ground truth is available for the chosen datasets, we assess the effectiveness of our approach using widely adopted external measures, namely the Purity score, Normalized Mutual Information (NMI) and clustering Accuracy (ACC). These metrics are commonly used for cluster validity evaluation, where higher values signify superior clustering performance.\nImplementation details. In our implementation, we initialize the contribution of all local partitions to the consensus partition generation by setting $\\beta^{(v)} = 1/\\sqrt{V}$. The alignment matrix is initially set as $W^{(v)} = I_k$. Tol is initialized to 10-3. Furthermore, we normalize the multi-view data in all experiments. It is assumed that"}, {"title": "5.2. Clustering results", "content": "The clustering performance of DMFAW is compared with baseline methods, and the results are presented in Table 1, where the best-performing results are highlighted in bold. Notably, our proposed method consistently outperforms the baselines across all the six datasets, validating the effectiveness of DMFAW. Particularly noteworthy is its substantial improvement on the BBCSport and BBC datasets compared to existing methods. The improvements for the BBCSport dataset are 4.19% in purity and ACC, and 4.86% in NMI, surpassing the second-best results. Similarly, for the BBC dataset, the gains are 2.95% in purity and ACC, and 5.92% in NMI when compared to the second-best results.\nMoreover, when compared with other methods utilizing the deep semi-NMF"}, {"title": "5.3. Convergence and parameter sensitivity analysis", "content": "Convergence Analysis. We theoretically showed in Theorem 2 that the updating of $G_m^{(v)}$ satisfies KKT conditions. To experimentally validate the convergence of the entire model, we conducted experiments using the Caltech101-7 dataset, setting hyperparameters to $\\lambda = 16$. The evolution of the objective value across iterations is depicted in Figure 2-a. Notably, the plot illustrates that DMFAW is monotonically decreasing, demonstrating consistent convergence. Moreover, convergence is achieved in fewer than 10 iterations, underscoring the efficiency of our proposed method, based on PI stepsize control weight parameter update, in accelerating convergence. This property is further investigated in run time experimentation.\nRun Time. Figure 2-b shows that the proposed algorithm demonstrates superior performance in terms of run time, recorded in seconds, compared to other deep matrix factorization methods. This significant reduction in run time can be attributed to our proposed weighted deep matrix factorization combined with the dynamic update of the feature selection degree.\nParameter sensitivity. We conducted a parameter sensitivity study on multiple datasets by varying $n_1$ and $n_2$ across the values [0.2, 0.4, 0.6, 0.8, 1]. We aimed to understand how these parameters impact the purity scores, and report the findings in Figure 3. Notably, our experiments consistently show that the purity scores remain stable across all combinations of $n_1$ and $n_2$. This indicates that even though we introduced two hyperparameters in Eq. (3), their influence on clustering results is minimal. Therefore, it is reasonable to treat both $n_1$ and $n_2$ as constants, which can be fixed to specific values across all datasets without significantly affecting the performance. In our case, setting $n_1 = 1$ and $n_2 = 0.2$ yields consistent and reliable clustering results."}, {"title": "5.4. Ablation Study", "content": "The comparison between dynamic and fixed weight parameter, as shown in Table 3, highlights the clear advantages of using a dynamic update based on control theory principles in clustering tasks. The dynamic approach significantly improves clustering purity and reduces run time across three datasets. For instance, on the BBCSport dataset, there is a 23.36% improvement in purity and a 33-second reduction in run time compared to the fixed approach. Similar improvements are observed in the other two datasets. These findings demonstrate that dynamically updating the parameter controlling feature selection degree allows DMFAW to converge faster and adapt more effectively to the data."}, {"title": "5.5. Visualization", "content": "To evaluate the effectiveness of the dynamic feature selection with respect to clustering results, we computed the pairwise similarity matrix for the BBCSport dataset, which comprises five clusters. Figure 4 illustrates the importance of adaptive feature selection by comparing the visual outputs of our approach with those of MVC-DMF-PA.\nFigure 4-a, corresponding to our method, displays distinct, well-defined clusters along the diagonal. This pattern suggests that the adaptive feature selection mechanism effectively emphasizes relevant features while diminishing the influence of irrelevant ones, leading to clearer and more compact clusters. In contrast, Figure 4-b, which represents MVC-DMF-PA, shows more diffuse clusters with less distinct boundaries. This lack of clear cluster structure indicates that the features are not as effectively leveraged in MVC-DMF-PA, resulting in reduced clustering quality."}, {"title": "6. Conclusion", "content": "This paper introduces Deep Matrix Factorization with Adaptive Weights for Multi-View Clustering (DMFAW). Using a weighted Deep Semi-NMF methodology, DMFAW simultaneously extracts local partition matrices and performs feature selection, significantly enhancing the robustness of multi-view clustering. A dynamic parameter update mechanism, inspired by Control Theory's PI Stepsize Control, ensures feature selection adaptability to diverse datasets while accelerating convergence. Extensive experiments on benchmark datasets demonstrate the effectiveness and efficiency of DMFAW, and its superior performance compared to other state-of-the-art methods. Additionally, the success of our approach is influenced by the quality of the views. In the future, we will explore methods to improve the robustness of our approach in the presence of noisy views."}]}