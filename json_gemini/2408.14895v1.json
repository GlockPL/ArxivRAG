{"title": "VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities", "authors": ["Shusaku Egami", "Takanori Ugai", "Swe Nwe Nwe Htun", "Ken Fukuda"], "abstract": "Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data (e.g., images and videos) into symbols, have attracted attention as resources enabling knowledge processing and machine learning across modalities. However, the construction of MMKGs for videos consisting of multiple events, such as daily activities, is still in the early stages. In this paper, we construct an MMKG based on synchronized multi-view simulated videos of daily activities. Besides representing the content of daily life videos as event-centric knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as bounding boxes within video frames. In addition, we provide support tools for querying our MMKG. As an application example, we demonstrate that our MMKG facilitates benchmarking vision-language models by providing the necessary vision-language datasets for a tailored task.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-modal knowledge graphs (MMKGs) [30], which ground various non-symbolic data to symbols, have attracted attention as a resource that enables knowledge processing across modalities. Typical MMKGs [10, 27] are knowledge graphs (KGs) in which images are grounded to entities in the graph. Grounding video content to entities in a KG requires solving subtasks such as event extraction, object extraction, and relation extraction, and various methods have been proposed. In the case of a long video consisting of multiple events, such as daily activities, the sequential events need to be extracted and listed by the timeline of the events. However, these tasks are difficult to solve using current methods [30]. To the best of our knowledge, there is no available MMKG of videos consisting of such event sequences. Furthermore, the bounding boxes of the objects within the video frames change dynamically since the information in the video changes frame by frame. It is desirable that the MMKGs of videos can effectively represent both visual changes between each frame and contextual changes between each event that is interpreted. Leveraging such MMKGs facilitates the construction of customized pre-training or test datasets for downstream tasks. This capability enables tailored data extraction for specific applications, such as extracting pairs of video frames and corresponding action labels before and after object state changes.\nIn this paper, we introduce a novel MMKG that integrates fine-grained events and frame-by-frame knowledge of daily activity videos. Specifically, we first generate multi-view synchronized daily activity videos using a virtual space simulator. Then, we construct an MMKG, called VirtualHome-AIST-KG (VHAKG), that represents event-centric and frame-by-frame knowledge, such as Figure 1, based on the generated videos and our designed ontology. Moreover, we compressed VHAKG to remove redundant triples since their data size is enormous and published it on the Web in a permanently available format. Finally, we provide a set of tools to facilitate the use of VHAKG even by users unfamiliar with the graph query language. In addition, as a possible use case of VHAKG, we introduce"}, {"title": "2 RELATED WORK", "content": "Zhu et al. [30] comprehensively surveyed and summarized previous works on MMKGs. Unfortunately, many MMKGs are not publicly available or are inaccessible. We focus on publicly available MMKGs whose entities (i.e., nodes) link directly to image or video files. IMGpedia [10] is an MMKG that grounds Wikimedia Commons images into DBpedia [2] entities. MMpedia [27] is an MMKG that matches entities corresponding to images retrieved from search engines. These MMKGs are still available because they are intended to share data using semantic web technologies. VisionKG [29] is an MMKG containing bounding boxes of objects extracted from various image datasets such as MS-COCO [16], CIFAR [13], and PASCAL VOC [9]. The raw dataset is not publicly available at this time, but a useful interface is available. Our MMKG differs from these studies because it describes timelines of temporally fine-grained events in the videos and frame-by-frame bounding boxes.\nAlthough Zhu et al. [30] mentioned that the extraction of sequential events from a long video containing multiple events has not yet been addressed, the KGs of such events are constructed using different approaches from automatic event extraction. Vizcarra et al. [26] constructed a KG based on manual annotation data to videos. However, the KG is not publicly available because they focused on knowledge representation methods. In our previous work [8], we developed VirtualHome2KG, a framework for constructing KGs from fine-grained event data generated by VirtualHome [21] simulator. The VirtualHome simulator renders human activities and outputs environmental information based on the input program data. The program data consists of multiple action steps, as shown below:\nWatch movie\nSit down on a couch in front of the TV. Use remote\nto turn on the TV.\n[WALK] <couch > (275)\n[SIT] <couch> (275)\n[GRAB] <remote_control> (1000)\n[WATCH] <television> (297)\nVirtualHome2KG structures the environmental information output by VirtualHome based on an ontology to construct event-centric KGs (EKGs [11]), where nodes are events and entities, while edges are event-event relations, event-entity relations, and entity-entity relations. Unlike a temporal knowledge graph (TKG) [4], which consists of triples with timestamps, EKG is represented as an edge-labeled directed graph. However, their entities are not directly linked to the video files because these EKGs focus on representing only the content of the videos. In addition, the frame-by-frame knowledge, such as objects' 2D bounding boxes in each image, is not represented. In contrast, this paper introduces a novel MMKG, which embeds synchronized video data captured by multiple cameras into entities and represents event sequences and frame-by-frame knowledge in videos."}, {"title": "3 DATASETS", "content": "We describe a method for generating data on daily activities, structuring them based on an ontology, and constructing an MMKG that is practically distributable on the web through data compression."}, {"title": "3.1 Data Generation", "content": "We generated a large number of simulated videos and EKGs of a wide variety of daily activities using VirtualHome2KG. In this step, we first added the following three new functions to the VirtualHome simulator and extended it to generate more diverse daily activity datasets frame-by-frame: (1) implementing renderable motions of various primitive actions, (2) automatically annotating 2D bounding boxes of objects in each video frame, and (3) adding synchronous camera mode with adjusted viewing angle and position. The original VirtualHome can factually render only about 10 primitive actions, and the generated daily activity videos are not diverse enough. Thus, we implemented 38 motions corresponding to various primitive actions, e.g., \u201ceat,\u201d \u201cpour,\u201d and \u201cwipe.\u201d\nIn addition, we implemented a function to output the 2D bounding boxes of objects in the video frame every 5 frames. The 2D bounding box is automatically detected by ray casting from cameras inside the environment. Therefore, we can collect the ground truth used in computer vision tasks without any annotation work on the bounding box, which used to require much manual work."}, {"title": "3.2 MMKG Construction", "content": "Our MMKG is defined as a graph \\(G = \\{E, R, L, T\\}\\), where \\(E, R, L\\) are set of entities, relations, and literal values, and \\(T \\subseteq E \\times R \\times (E \\cup L)\\) are sets of triples. A set of literal values \\(L = \\{L_K, L_M\\}\\) denotes that \\(L_K\\) is the set of the KG\u2019s literal values and \\(L_M\\) is the set of multi-modal data.\nWe first designed the schema of VHAKG with reference to the existing ontologies. We interpreted the videos captured by multiple cameras installed in VirtualHome-AIST as sensor data. Thus, we reused MSSN-Onto [1], which is an extension of the Semantic Sensor Network Ontology [23] for multimedia content, and extended the VirtualHome2KG\u2019s ontology. Figure 1 shows an example of the modeling of VHAKG to be constructed in this study. The right side of the figure shows the EKG constructed in Section 3.1. The left side of the figure is the KG described in this section. In the EKG, \"Activity\" corresponds to the whole video, and \"Event\" corresponds to \"VideoSegment.\u201d The 2D bounding box links to the corresponding \"Object\" of the EKG. The video data is embedded as a literal value encoded in base64. Consequently, video files are free of broken links and reference errors, ensuring their permanent availability and sharing."}, {"title": "3.3 MMKG Compression", "content": "3.3.1 Video compression. We first created a KG with all images embedded every five frames; however, we found that the data size became too large to share practically. Thus, we used MPEG [14] compression to reduce the data size significantly. Each video frame entity has a frame number instead of having a base64 value, and the video entity has a base64 value for the entire compressed video. This allows the use of well-known tools such as FFmpeg [24] to recover and extract arbitrary frame images from VHAKG.\n3.3.2 Removing redundant triples. Constructing a KG, as shown in Figure 1, will create redundant triples about the 2D bounding boxes. We reduced the number of entities and triples by referring to the previous entities if the current 2D bounding boxes have not changed since the previous frame.\n3.3.3 Results. Table 1 shows the number of triples and data size of VHAKG. In image-embedded KG, each image was converted to JPEG with a quality of 90 using Pillow [5]. As a result, our approach significantly compressed the number of triples and data size, and made it possible to share them securely in a research data repository\u00b2. VHAKG, consisting of the compressed video-embedded KG and EKG, is available at Zenodo\u00b3."}, {"title": "4 APPLICATIONS", "content": "4.1 Support tools\nWe have developed and released support tools (GUI and command-line) to make VHAKG accessible to users who are unfamiliar with SPARQL [6]. The GUI displays the specified videos and images and is provided as Docker Compose [7]. The back-end system automatically loads VHAKG by launching GraphDB [17] as a triple store on the local machine. The GUI executes template-based SPARQL queries that search for videos matching the conditions specified in the UI and restore and display the videos from their base64 values. The seek bar moves to the position of the specified frame. In addition, the command-line tool can extract videos and images and save them with annotation labels on a local machine."}, {"title": "4.2 Example Benchmarking of LVLMs", "content": "SPARQL querying enables users to extract videos, video frames, entities, and their labels from VHAKG. Thereby, users can create customized image and video annotation datasets for specific use cases. As a demonstration of VHAKG, we designed a new VQA task, created a test dataset, and conducted an example experiment.\n4.2.1 Task design. We designed a task to understand a character's daily activities from an input image and predict and explain the character's next action. An example of input data is a single image and a question, as shown in Figure 3(a). This image is extracted from the daily activity video included in VHAKG. The models are required to understand the meaning of this input image and explain what action the character will take next. Therefore, this task is more difficult than existing vision-language tasks, such as caption generation.\n4.2.2 Dataset preparation. The evaluation dataset should be able to predict the next action from the image features to some extent. Thus, we extracted the image immediately after the character grabbed something as the question data and extracted the next action and the target objects as the correct answer labels. Figure 3(b) shows the triple pattern of SPARQL queries to obtain these test sets. Red entities and literals are extracted. This triple pattern queries the event's action following the event that the character grabs something and walks to somewhere, the object, the video, and the frame number. We then created short sentences based on the extracted actions and objects, as shown in Figure 3(a), based on a simple template. We extracted 100 pairs of data as the test set and another 5 pairs of data as samples for few-shot learning. Such data extraction is possible because VHAKG comprehensively represents video data, event-centric knowledge, and frame-by-frame knowledge.\n4.2.3 Experiments. In our preliminary experiment, we extracted events after the \"grab\" event and found that 76 out of 100 events were \"walk,\" which was highly biased. In contrast, we reduced the bias in this experiment by extracting events after the \"walk\" following \"grab,\" as shown in Figure 4. In this way, it is possible to create a test set with reduced bias by querying VHAKG.\nNote that the purpose of this section is to show that VHAKG can produce benchmark datasets for VQA tasks and that this study does not aim to develop or compare LVLMs. We show an example of an experiment to evaluate whether LVLMs can answer questions"}, {"title": "5 CONCLUSION", "content": "In this study, we provided VHAKG, which is a novel MMKG based on multi-view videos of daily activities consisting of multiple events. Moreover, we presented VirtualHome-AIST, support tools of VHAKG, and an example experiment. VHAKG integrated different modalities by embedding videos as literal values within the KG. By compressing videos and removing redundant triples, we reduced the data size and made it permanently available through a reliable research repository. Moreover, VHAKG can contribute to the creation of benchmark datasets and pre-training datasets for LVLMs since VHAKG represents both event sequences and frame-by-frame visual changes. The remaining issues in this study are increasing the variety of videos and linking to other KGs. In the future, we plan to generate multi-agent and egocentric videos and link VHAKG to real visual dataset KGs [28, 29] for sim2real tasks."}]}