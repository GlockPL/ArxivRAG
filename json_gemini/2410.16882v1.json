{"title": "Large Language Model-based Augmentation for Imbalanced Node Classification on Text Attributed Graphs", "authors": ["Leyao Wang", "Yu Wang", "Bo Ni", "Yuying Zhao", "Tyler Derr"], "abstract": "Node classification on graphs frequently encounters the challenge of class imbalance, leading to biased performance and posing significant risks in real-world applications. Although several data-centric solutions have been proposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore overlook the potential of leveraging the rich semantics encoded in textual features for boosting the classification of minority nodes. Given this crucial gap, we investigate the possibility of augmenting graph data in the text space, leveraging the textual generation power of Large Language Models (LLMs) to handle imbalanced node classification on TAGs. Specifically, we propose a novel approach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs), which prompts LLMs to generate synthetic texts based on existing node texts in the graph. Furthermore, to integrate these synthetic text-attributed nodes into the graph, we introduce a text-based link predictor to connect the synthesized nodes with the existing nodes. Our experiments across multiple datasets and evaluation metrics show that our framework significantly outperforms traditional non-textual-based data augmentation strategies and specific node imbalance solutions. This highlights the promise of using LLMs to resolve imbalance issues on TAGs.", "sections": [{"title": "Introduction", "content": "Graph representation is integral to various domains, with node classification being a fundamental task. Examples include categorizing publications in citation networks (Hamilton, Ying, and Leskovec 2017), detecting anomalies in online transaction networks (Zheng et al. 2020), and identifying suicidal ideation using social media knowledge graphs (Cao, Zhang, and Feng 2020). However, node classification often encounters class imbalance where the majority nodes tend to dominate predictions and result in biased results for minority nodes, potentially causing social risks. In fake account detection, training models are mostly on benign users, and only a few bot users risk missing fake accounts (Zhao, Zhang, and Wang 2021; Mohammadrezaei, Shiri, and Rahmani 2018; Zhao et al. 2009). Similarly, suicidal individuals often form a minority class in online social networks, leading to inadequate detection and prevention coverage (Cao, Zhang, and Feng 2020).\nTo address these imbalance issues, existing works develop model-centric and data-centric solutions. For model-centric ones, various regularization techniques optimize node embeddings for minority classes (Zhang et al. 2022; Li et al. 2024; Liu et al. 2023), while reweighting strategies prioritize nodes based on their structural influence (Hong et al. 2021; Menon et al. 2020). For data-centric solutions, besides non-geometric data-augmentation strategies, such as upsampling, SMOTE, and mixup (Chawla et al. 2002; Werner de Vargas et al. 2023; Zhang et al. 2017), recent methods incorporate them into graph structures, including GraphSMOTE (Zhao, Zhang, and Wang 2021) and MixupForGraph (Wang et al. 2021). Furthermore, advanced studies such as GraphENS attempt to alleviate overfitting from neighbor memorization by synthesizing ego networks for minority classes (Park, Song, and Yang 2021). Despite these advancements, existing methods largely focus on conventional graphs, where node features are restricted to shallow embeddings. For example, for text attributes, typically just bag-of-words (BOW) featurization is utilized. However, these approaches fail to capture the contextualized semantics embedded in text attributes, leading to unfavorable performance in text-based node classifications such as anomaly detection (Zhao, Zhang, and Wang 2021; Mohammadrezaei, Shiri, and Rahmani 2018; Zhao et al. 2009) and suicide identification (Cao, Zhang, and Feng 2020).\nGiven the literature gap in consideration of text features, we observed that Text-Attributed Graphs (TAGs) (Chen et al. 2024a) could provide a viable solution for capturing textual semantics in addressing imbalanced node classification."}, {"title": "Preliminaries", "content": "Notations\nGiven a TAG $G = (V,T,E,C)$, where V represents the set of nodes and T refers to their corresponding text set, with the textual feature and category name of node $v_i$ as $T_i$ and $C_i$. E represents the set of edges with $e_{ij}$ being the edge connecting node $v_i$ and $v_j$. In imbalanced node classification, let $V' \\subset V$ denote the subset of labeled nodes with node $v_i \\in V'$ associated with the label $y_i$. Assume we totally have m classes in $V' = {V_i}_{i=1}^{m}$, the imbalance ratio r is defined as $\\frac{max(\\{|V_i|\\}_{i=1}^{m})}{min(\\{|V_i|\\}_{i=1}^{m})}$. Furthermore, in the LLM-based pipeline, LLM symbolizes the usage of large language models, while in the LM-based pipeline, $\\phi$ denotes the pre-trained LMs"}, {"title": "Problem Definition", "content": "The objective of imbalanced node classification on TAGs, based on the above definitions, is to devise an augmentation framework F to generate a balanced graph $G' = (V',T', E', C')$ with an additional labeled set $V^l$, such that the node classification model M trained on top of the newly generated graph would end up with improved overall performance as well as a reduced performance gap between minority and majority classes in the node classification task."}, {"title": "Methods", "content": "Our LA-TAG framework comprises two main components: LLM-based Data Augmentation and the Textual Link Predictor. This section first details each component individually and then presents an overall framework illustrating their integration, as shown in Figure 3."}, {"title": "LLM-based Data Augmentation", "content": "Our method builds on TAG pipelines to better leverage textual semantics in data augmentation, utilizing both LM-based and LLM-based pipelines. Initially, LLM-based pipelines are adopted to augment textual node features. Emulating traditional strategies on non-textual data, we prompt LLM to generate additional text-attributed nodes for the minority classes in G. LM-based pipelines are implemented subsequently, which encode the newly synthetic texts using pre-trained language models $\\phi$ and output contextualized deep embeddings $h^\\iota_i$ capable of comprehending textual semantics. The labels of generated nodes, $\\hat{y}_i$, will be the same as that of the original nodes, $y_i$. Accordingly, for each input node in the labeled set, i.e. $v_i \\in V'$, our LLM-based data augmentation will generate a new embedding, $h^\\iota$, along with a label, $\\hat{y}_i$, to be fed into a GNN for node classification, and the entire process is expressed as follows:\n$h^\\iota = \\phi(LLM(F(T_i, C_i, T^\\iota)))$\n$\\hat{y}_i = y_i$\nwhere $T^\\iota$ is the set of labeled nodes' text. While our methodology can replicate a wide range of conventional augmentation strategies, this paper specifically focuses on three well-known methods\u2014upsampling, SMOTE, and Mixup\u2014which we will further elaborate on next. A concert case study of the three methods is illustrated in Figure 2."}, {"title": "Upsampling", "content": "Traditional upsampling executes simple duplication (Werner de Vargas et al. 2023), thus we consider generating similar textual data as the original node with:\n$F (T_i, C_i, T^\\iota) = T_i | C_i$\nGiven the category name $C_i$ of $v_i$, we will output a text that is similar to $T_i$, the associated text attributes of $v_i$."}, {"title": "Mixup", "content": "The customary Mixup randomly selects a pair of data from the training set and interpolates both data points x and labels y (Zhang et al. 2017). Our version of 'Mixup', on the other hand, performs interpolation at text level between the original node and its k-nearest labeled neighbors. Moreover, it does not interpolate the label y to ensure accurate tracking of sample size in the minority class and maintain balance in the newly generated graph. The following formula describes this process:\n$F (T_i, C_i, T^\\iota) = (T_i + knn(T_i, T^\\iota)) | C_i$\n$knn(T, T^\\iota) = topk(argmin ||\\phi(T) \u2013 \\phi(T_j)||)$,\n$T_j$\ns.t. $T_j \\in T^\\iota$\nGiven the category name $C_i$ of $v_i$, identify the k nearest neighbors of $T_i$ among all $T^\\iota$ texts from training nodes $V^\\iota$ in the text space, and mix them with $T_i$ through interpolation."}, {"title": "SMOTE", "content": "Resembling the orthodox SMOTE method (Chawla et al. 2002), our LLM-based SMOTE begins by locating k nearest neighbors of the original node in the deep embedding space, ensuring they are from the same class. Next, it synthesizes these neighbors with the texts of the original node to generate new textual attributes. The process is illustrated by the following expression:\n$F (T_i, C_i, T^\\iota) = (T_i + knn(T_i, T^\\iota, C_i)) | C_i$\n$knn(T_i, T^\\iota, C_i) = topk(argmin ||\\phi(T_i) \u2013 \\phi(T_j)||)$,\n$T_j$\ns.t. $T_j \\in T^\\iota$, $C_j = C_i$\nGiven the category name $C_i$ of $v_i$, pinpoint the k nearest neighbors in the deep embedding space from $T^\\iota$, which also belong to the same category $C_i$ as the original nodes. Combine the identified neighbors with $T_i$ through interpolation to generate new synthetic samples."}, {"title": "Textual Link Predictor", "content": "After generating additional synthetic nodes with text attributes, our Textual Link Predictor constructs a new graph for node classification training. Initially trained on the original nodes and edges in the graph, it is then applied to the synthetic data for edge generation, preserving the original geometric structure of the graph. Details are shown below.\nPretraining Link Predictor Our link predictor comprises two components: an encoder and a predictor, both utilizing MLP to ensure simplicity and efficiency. The input embedding $h_{in}$ is processed through a Relu layer with weights W in both MLP models, as expressed below:\n$MLP(h_{in}) = Relu(W\\cdot h_{in})$\nFor each node $v \\in V$, the encoder takes the deep embedding from textual attributes and generates the embedding $h_v^e$:\n$h^e_v = MLP(\\phi(T_v))$\nThe predictor then generates scores for the predicted edge between nodes u and v, denoted as $Log_{u,v}$. This score is computed by passing the inner product of the previously encoded embeddings $h^e_u$ and $h^e_v$ from nodes u and v through the predictor, as described below:\n$Log_{u,v} = MLP(h^e_u \\cdot h^e_v)$\nSubsequently, we employed the LogSigmoid function to calculate the loss for the predicted $Log_{u,v}$, based on which we train both the encoder and the predictor:\n$Loss_{edge} = LogSigmoid(Log_{u,v})$\nApplying on New Data Let $V^l$ be the set of synthetic nodes generated by data augmentation and $V' = V + V^l$ be the set of nodes in the new graph. Denote $E'$ be the generated edges, we have\n$E' = topk(Log[V', V^l])$          (1)\nWe identify all potential edges between $V'$ and $V^l$ and concatenate their corresponding edge indices into a list $[V', V^l]$. This list is then input into our Text Link Predictor, which generates a score Log for each edge pair in $[V', V^l]$. We select the top k global edges with the highest Log scores and incorporate them by extending the original graph to create a new balanced graph (i.e., with an imbalance ratio equal to 1) for node classification."}, {"title": "LA-TAG", "content": "Here we outline the framework of LA-TAG, including the two main components, namely the aforementioned LLM-based Data Augmentation and Textual Link Predictor, which is illustrated in Figure 3. When given an imbalanced TAG, G, LA-TAG first performs the LLM-based Data Augmentation step, which selects nodes from the minority class(es) and applies one of the LLM-based augmentation strategies (i.e., based on Upsampling, Mixup, or SMOTE) to produce a set of new nodes $V^l$. At this stage, in total with the original graph, we have the resulting node set $V'$ that has the associated text attributes $T'$. Note that the number of new nodes added is selected to balance the training examples in the graph across node classes (i.e., create an imbalance ratio of 1). Meanwhile, the Textual Link Predictor is trained on the original graph using the text representations extracted with SBERT to train an MLP for predicting links between pairs of nodes (according to their text representations). Then, the embedded textual data is input into the Textual Link Predictor, which generates edges $E'$ for the new nodes $V^l$, forming a revised graph G'. More specifically, we identify all possible edge pairs between $V^l$ and $V'$ and calculate their pairwise scores. Thereafter, we select the top k edges with the highest scores, and add these edges to the existing set to form E', and obtain a balanced graph G' as the result. This resulting balanced graph is subsequently fed into a Graph Neural Network (GNN) for node classification training, with Sentence-BERT employed for deep text encoding. A detailed algorithm of LA-TAG is shown in Algorithm 1 in the Supplementary."}, {"title": "Experiment", "content": "In this section, we conduct comprehensive experiments to validate the effectiveness of our model. We begin by detailing our experimental settings and proceed with a thorough analysis of various experiments, which involves a comparison of three LLM-based augmentation strategies, evaluation against prior baselines, assessment of resilience to imbalance variance, and examination of fine-tuning effects. Additionally, we present ablation studies to highlight the necessity of different components of LA-TAG, including LLMs, link predictors, and LM-based pipelines on TAGs."}, {"title": "Conclusion", "content": "In this paper, we address the novel problem of imbalanced node classification on text-attributed graphs (TAGs). We propose LA-TAG, which combines an LLM-based data augmentation with a pre-trained textual link predictor. In-depth experiments are conducted and demonstrate the benefits of our model over prior related baselines. In addition, we include ablation studies, augmentation strategies assessment, LLM variants evaluation, and imbalance ratio analysis to more thoroughly understand this new research direction."}]}