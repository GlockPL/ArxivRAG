{"title": "Federated Graph Learning with Structure Proxy Alignment", "authors": ["Xingbo Fu", "Zihan Chen", "Binchi Zhang", "Chen Chen", "Jundong Li"], "abstract": "Federated Graph Learning (FGL) aims to learn graph learning mod-els over graph data distributed in multiple data owners, which hasbeen applied in various applications such as social recommendation and financial fraud detection. Inherited from generic FederatedLearning (FL), FGL similarly has the data heterogeneity issue wherethe label distribution may vary significantly for distributed graphdata across clients. For instance, a client can have the majorityof nodes from a class, while another client may have only a fewnodes from the same class. This issue results in divergent localobjectives and impairs FGL convergence for node-level tasks,especially for node classification. Moreover, FGL also encounters aunique challenge for the node classification task: the nodes from aminority class in a client are more likely to have biased neighboringinformation, which prevents FGL from learning expressive nodeembeddings with Graph Neural Networks (GNNs). To grapple withthe challenge, we propose FedSpray, a novel FGL framework thatlearns local class-wise structure proxies in the latent space andaligns them to obtain global structure proxies in the server. Ourgoal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. Toachieve this, FedSpray trains a global feature-structure encoder andgenerates unbiased soft targets with structure proxies to regularizelocal training of GNN models in a personalized way. We conductextensive experiments over four datasets, and experiment resultsvalidate the superiority of FedSpray compared with other baselines.Our code is available at https://github.com/xbfu/FedSpray.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) [46] are a prominent approachfor learning expressive representations from graph-structured data.Typically, GNNs follow a message-passing mechanism, where theembedding of each node is computed by aggregating attribute in-formation from its neighbors [11, 17, 44]. Thanks to their powerfulcapacity for jointly embedding attribute and graph structure in-formation, GNNs have been widely adopted in a wide variety ofapplications, such as node classification [9, 12] and link predic-tion [2, 5]. The existing GNNs are mostly trained in a centralizedmanner where graph data is collected on a single machine beforetraining. In the real world, however, a large number of graph datais generated by multiple data owners. These graph data cannot beassembled for training due to privacy concerns and commercialcompetitions [41], which prevents the traditional centralized man-ner from training powerful GNNs. Taking a financial system withfour banks in Figure 1 as an example, each bank in the system hasits local customer dataset and transactions between customers. Aswe take the customers in a bank as nodes and transactions betweenthem as edges, the bank's local data can naturally form a graph.These banks aim to jointly train a GNN model for classificationtasks, such as predicting a customer's occupation (i.e., Doctor orTeacher) without sharing their local data with each other.\nFederated Learning (FL) [25] is a prevalent distributed learningscheme that enables multiple data owners (i.e., clients) to collab-oratively train machine learning models under the coordinationof a central server without sharing their private data. One criticalchallenge in FL is data heterogeneity, where data samples are notindependent and identically distributed (i.e., non-IID) across theclients. For instance, assume that Bank A in Figure 1 locates in acommunity adjacent to a hospital. Then most customers in BankA are therefore likely to be labeled as Doctor while only a few cus-tomers are from other occupations (e.g., Teacher). In contrast, BankC adjoining a school has customers labeled mostly as Teacher andonly a few as Doctor. Typically, the nodes from a class that claimsthe very large proportion of the overall data in a client are themajority nodes (e.g., Doctor in Bank A) while minority nodes (e.g.,Teacher in Bank A) account for much fewer samples. The data het-erogeneity issue results in divergent local objectives on the clientsand consequently impairs the performance of FL [15]. A number"}, {"title": "2 PROBLEM FORMULATION", "content": "2.1 Preliminaries\n2.1.1 Notations. We use bold uppercase letters (e.g., X) to repre-sent matrices. For any matrix, e.g., X, we denote its i-th row vectoras xi. We use letters in calligraphy font (e.g., V) to denote sets. |V|denotes the cardinality of set V.\n2.1.2 Graph Neural Networks. Let G = (V, &, X) denote anun directed attributed graph, where V = {01, 02, ..., n} is the setof |V| nodes, & is the edge set, and X \u2208 R|V|\u00d7dx is the node featurematrix. dx is the number of node features. Given each node vi \u2208 V,N(vi) denotes the set of its neighbors. The ground-truth label ofeach node vi \u2208 V can be denoted as a de-dimensional one-hotvector y; where de is the number of classes. The node homophily[23, 49] is defined as\n$h_i = \\frac{|\\{v_j|v_j\\in N(v_i) \\text{ and } y_j = y_i\\}|}{|N(v_i)|}$ (1)\nwhere |N(vi)| denotes the degree of node vi. Typically, an L-layerGNN model f parameterized by \u03b8 maps each node to the outcomespace via a message-passing mechanism [11, 17]. Specifically, eachnode vi aggregates information from its neighbors in the l-th layerof a GNN model by\n$h_i^l = f_l(h_i^{l-1}, \\{h_j^{l-1} : v_j \\in N(v_i)\\} ; \\theta_l),$ (2)\nwhere h is the embedding of node vi after the l-th layer fi, and\u03b8\u03b9 is the parameters of the message-passing function in fi. Theraw feature of each node vi is used as the input layer, i.e., h = xi.For the node classification task, the node embedding h after thefinal layer is used to compute the predicted label distribution \u0177; =Softmax(h) \u2208 Rdp by the softmax operator.\n2.1.3 Personalized FL. Given a set of K clients, each client k hasits private dataset D (k) = {(xk), x(k) , yk))} (k)) N(k) where N(k) is thenumber of samples in client k. The overall objective of the clients is\n$\\min_{(\\theta^{(1)},\\theta^{(2)},...,\\theta^{(K)})}\\frac{1}{N}\\sum_{k=1}^K N^{(k)} L^{(k)}(D^{(k)}; \\theta^{(k)}),$ (3)\nwhere L(k) (\u03b8(k)) is the local average loss (e.g., the cross-entropyloss) over local data in client k, and N = \u03a3\u039a-1 N(k). Standard FLmethods aim to learn a global model \u03b8 = ((1) = \u03b8(2) = . . . = ((K). As a representative method in FL, FedAvg [25] performs localupdates in each client and uploads local model parameters to a"}, {"title": "3 MOTIVATION", "content": "In this section, we first conduct an empirical study on the PubMeddataset [31] to investigate the impact of divergent neighboring in-formation across clients on minority nodes when jointly trainingGNNs in FGL. The observation from this study is consistent withour example in Figure 1 and motivates us to learn global structureproxies as favorable neighboring information. We then develop the-oretical analysis to explain how aligning neighboring informationacross clients can benefit node classification tasks in FGL.\n3.1 Empirical Observations\nTo better understand the divergent neighboring information acrossclients with its impact on the node classification task in FGL, weconduct preliminary experiments to compare the performance offederated node classification with MLP and GNNs as local modelson the PubMed dataset [31]. Following the data partition strategy inprevious studies [14, 51], we synthesize the distributed graph databy splitting each dataset into multiple communities via the Louvain"}, {"title": "3.2 Theoretical Motivation", "content": "According to the above empirical observations, minority nodeswith the original neighboring information are more likely to bemisclassified. One straightforward approach to this issue is enablingnodes to leverage favorable neighboring information from otherclients for generating node embeddings. Specifically, we considerconstructing global neighboring information in the feature space.The server collects neighboring feature vectors from each clientand computes the global class-wise neighboring information viaFedAvg [25]. We aim to theoretically investigate whether the globalneighboring information can benefit node classification tasks whenreplacing the original neighbors of nodes. Following prevalent waysof graph modeling [8, 24, 40], we first generate random graphs ineach client using a variant of contextual stochastic block model[40] with two classes.\n3.2.1 Random Graph Generation. The generative model gen-erates a random graph in each client via the following strategy."}, {"title": "4 METHODOLOGY", "content": "In this section, we present the proposed FedSpray in detail. Figure3(a) illustrates an overview of FedSpray. The goal of FedSpray isto let the clients learn personalized GNN models over their privategraph data while achieving higher performance by mitigating theimpact of adverse neighboring information in GNN models. Toreach this goal, FedSpray employs a lightweight global feature-structure encoder which learns class-wise structure proxies andaligns them on the central server. The feature-structure encodergenerates reliable unbiased soft targets for nodes given their rawfeatures and the aligned structure proxies to regularize local train-ing of GNN models.\n4.1 Personalized GNN Model\nWe first introduce personalized GNN models in FedSpray.\n4.1.1 GNN backbone Model. Considering their exceptional abil-ity to model graph data, we use GNNs as the backbone of the pro-posed framework. In this study, we propose to learn GNN modelsfor each client in a personalized manner to tackle the data hetero-geneity issue in FGL. Specifically, the personalized GNN modelf(0(k)) in client k outputs the predicted label distribution \u0177(k) foreach node v (k) \u2208 V(k). Note that FedSpray is flexible. Any GNNsthat follow the message-passing mechanism as the structure of Eq.(2) can be used as the backbone, such as GCN [17] and SGC [44].\n4.1.2 Loss formulation. During local training, ((k) can be up-dated by minimizing the cross-entropy loss between y (k) and \u0177 (k)for each labeled node v (k) \u2208 (k)\n$L_{CE}^{(k)} = \\frac{1}{|V_L^{(k)}|}\\sum_{v_i^{(k)}\\in V_L^{(k)}}CE(y_i^{(k)}, \\hat{y}_i^{(k)}),$ (5)"}, {"title": "4.2 Global Feature-Structure Encoder with Structure Proxies", "content": "In this subsection, we will elucidate our design for the global feature-structure encoder and class-wise structure proxies in FedSpray. Thefeature-structure encoder aims to generate a reliable soft target (i.e.,p) for each node with its raw features and structure proxy.\n4.2.1 Structure Proxies. As discussed above, a minority nodecan obtain adverse neighboring information from its neighbors viathe message-passing mechanism, given its neighbors are proba-bly from other classes. To mitigate this issue, we propose to learnunbiased class-wise structure proxies in FedSpray, providing favor-able neighboring information for each node. Here, we formulateeach structure proxy in a vectorial form. Let S \u2208 Rdcxds denotec class-wise structure proxies, and each row sj \u2208 S denotes the ds-dimensional structure proxy of the j-th node class. For each nodev(k) \u2208 V\u2081(k), its structure proxy s sk) will be sj if it is from the j-thclass. Then, the structure proxies will be used as the input of thefeature-structure encoder.\n4.2.2 Feature-Structure Encoder. In FedSpray, we employ alightweight feature-structure encoder to generate a reliable softtarget for a node with its raw feature and structure proxy as theinput. Figure 3(b) illustrates our design for the feature-structureencoder. Let g(w) denote the feature-structure encoder g param-eterized by \u03c9. Given a node v(K) \u2208 V(K), the feature-structure"}, {"title": "4.2.3 Loss formulation.", "content": "During local training, we aim to updatew = {we, wp, wq} and S using both ground-truth labels and predic-tions from the GNN model. Specifically, we formulate the overallloss for training \u03c9 and S in client k as\n$L_F^{(k)} = L_{FCE}^{(k)} + \\lambda_2 L_{LFCE}^{(k)} + \\lambda_2L_{F\\_KD}^{(k)}$ (11)"}, {"title": "4.3 Server Update", "content": "As stated above, FedSpray will learn the feature-structure encoderand the structure proxies globally. In this subsection, we presentthe global update in the central server for the feature-structureencoder and the structure proxies, respectively.\n4.3.1 Update global feature-structure encoder. During eachround r, the server performs weighted averaging of local feature-structure encoders following the standard FedAvg [25] with eachcoefficient determined by the local node size\n$w_r = \\frac{\\sum_{k=1}^K N^{(k)} w_r^{(k)}}{\\sum_{k=1}^K N^{(k)}}$ (14)\n4.3.2 Structure proxy alignment. Instead of using the localnode size, we propose to assign higher weights to majority classestthan minority classes for structure proxy alignment. More specif-ically, the server updates global structure proxy sj,r \u2208 Sr duringround r by\n$s_{j,r} \\leftarrow \\frac{\\sum_{k=1}^K a_j^{(k)}s_i^{(k)}}{\\sum_{k=1}^K a_j^{(k)}}$ (15)\nwhere a is the ratio of nodes from the j-th class among V(k) inclient k and aj = \u03a3K-1 a(k)."}, {"title": "4.4 Overall Algorithm", "content": "Algorithm 1 shows the overall algorithm of the proposed FedSpray.During each round, each client performs local updates with twophases. In Phase 1, each client trains its personalized GNN models(k) for E epochs. We first compute p for node vi by the globalfeature-structure encoder g(wr-1) with its feature x(k) and cor-(k)responding structure proxy s (line 5). Then p) is utilized tocompute L() (line 9) for training the GNN model (line 10). InPhase 2, the feature-structure encoder and structure proxies will(k) be optimized for E epochs. In client k, we first obtain \u0177for node(k) by the up-to-date GNN model (line 14). \u0177for node(k)will be used to compute LF (line 19). Then we update wrt (k) ands(k) via gradient descent (line 20-21). At the end of each round,sj \u2208 s(k) will be updated by averaging s(k) of nodes from the j-thclass (line 23). At the end of each round, the local feature-structureencoder and structure proxies will be sent to the central server"}, {"title": "4.5 Discussion", "content": "FedSpray exhibits superior advantages from various perspectives,including communication efficiency, privacy preservation, and com-putational cost. We provide an in-depth discussion about FedSpray'sprincipal properties as follows.\n4.5.1 Privacy Preservation. The proposed FedSpray uploads theparameters of local feature-structure encoders following the preva-lent frameworks in FL [18-20]. Here, we mainly discuss the privacy"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct empirical experiments to demonstratethe effectiveness of the proposed framework FedSpray and performdetailed analysis of FedSpray."}, {"title": "5.1 Experiment Setup", "content": "5.1.1 Datasets. We synthesize the distributed graph data basedon four common real-world datasets from various domains, i.e.,PubMed [31], WikiCS [26], Coauthor Physics [33], and Flickr [50].We follow the strategy in Section 3.1 to simulate the distributedgraph data and summarize the statistics and basic information aboutthe datasets in Appendix B.1. We randomly select nodes in clientsand let 40% for training, 30% for validation, and the remaining fortesting. We report the average classification accuracy for all nodesand minority nodes over the clients for five random repetitions.\n5.1.2 Baselines. We compare FedSpray with six baselines includ-ing (1) Local where each client train its GNN model individually; (2)FedAvg [25], the standard FL algorithm; (3) APFL [6], an adaptiveapproach in personalized FL; (4) GCFL [47], (5) FedStar [38], and(6) FedLit [48], three state-of-the-art FGL methods. More detailsabout the above baselines can be found in Appendix B.2.\n5.1.3 Hyperparameter setting. As stated previously, FedSprayis compatible with most existing GNN architectures. In the exper-iments, we adopt three representative ones as backbone models:GCN [17], SGC [44], and GraphSAGE [11]. Each GNN model in-cludes two layers with a hidden size of 64. The size of featureembeddings and structure proxies is also set as 64. Therefore, thefeature-structure encoder has similar amounts of parameters withGNN models. Each component in the feature-structure encoderis implemented with one layer. We use an Adam optimizer [16]with learning rates of 0.003 for the global feature-structure encoderand personalized GNN models, 0.02 for structure proxies. The twohyperparameters \u03bb\u2081 and 12 are set as 5 and 1, respectively. We runall the methods for 300 rounds, and the local epoch is set as 5."}, {"title": "5.2 Effectiveness of FedSpray", "content": "We first show the performance of FedSpray and other baselines onnode classification over the four datasets with three backbone GNNmodels. Table 2 reports the average classification accuracy on allnodes and minority nodes in the test set across clients.\nFirst, we analyze the results of overall accuracy on all test nodes.According to Table 2, our FedSpray consistently outperforms allthe baselines on node classification accuracy for overall test nodesacross clients. Local and FedAvg achieve comparable performance"}, {"title": "5.3 Analysis of FedSpray", "content": "5.3.1 Influence of hyperparameter 21. The hyperparameter\u03bb\u2081 controls the contribution of the regularization term in LG(k).We conduct the sensitivity analysis on 21 in FedSpray. Figure 4reports the classification accuracy of FedSpray on all nodes andtest nodes in the test sets with different values of \u03bb\u2081 over PubMed(left) and WikiCS (right) with GraphSAGE. The accuracy on allnodes remains high when \u03bb\u2081 is relatively small (i.e., \u03bb\u2081 = 0.1, 1, 5).However, the accuracy of minority nodes will decrease when \u03bb\u2081 istoo small because the feature-structure encoder cannot sufficientlyregularize local training of GNN models with too small 11. When\u03bb\u2081 gets too large, the accuracy of all nodes decreases in both figures.In this case, the regularization term weighs overwhelmingly inthe loss for training GNN models; then GNN models cannot besufficiently trained with label information. According to the aboveobservations, we will recommend 10 for PubMed with GraphSAGEand 5 for WikiCS with GraphSAGE as the best setting for 21.\n5.3.2 Influence to structure proxy dimension. Since FedSprayincorporates structure proxies in the feature-structure encoder, wemay set a different dimension ds of structure proxies. We evaluate"}, {"title": "5.3.3 Effectiveness of structure proxies.", "content": "In this study, we de-sign structure proxies in FedSpray to serve as global unbiased neigh-boring information for guiding local training of GNN models. Tovalidate the effectiveness of structure proxies, we investigate theperformance of the proposed framework when structure proxiesare removed. Specifically, we set class-wise structure proxies S as0 consistently during training. We report the performance of Fed-Spray with S = 0 over PubMed and WikiCS in Table 3. Accordingto Table 3, we can observe that FedSpray suffers from significantperformance degradation when removing structure proxies. It sug-gests that structure proxies play a significant role in FedSpray.Without them, the feature-structure encoder generates soft targetsonly based on node features [52]. In this case, the soft labels canbe unreliable when node labels are not merely dependent on nodefeatures and, therefore, provide inappropriate guidance on localtraining of personalized GNN models in FedSpray.\n5.3.4 More Experimental Results. Due to the page limit, weprovide experimental results of FedSpray with varying local epochs in Appendix B.3."}, {"title": "6 RELATED WORK", "content": "6.1 Federated Learning\nRecent years have witnessed the booming of techniques in FL and itsvarious applications in a wide range of domains, such as computervision [3, 28], healthcare [21, 36], and social recommendation [22,43]. The most important challenge in FL is data heterogeneity acrossclients (i.e., the non-IID problem). A growing number of studieshave been proposed to mitigate the impact of data heterogeneity. Forinstance, FedProx [20] adds a proximal term to the local trainingloss to keep the updated parameters close to the global model.Moon [18] uses a contrastive loss to increase the distance betweenthe current and previous local models. FedDecorr [34] mitigatesdimensional collapse to prevent representations from residing ina lower-dimensional space. In the meantime, a battery of studiesproposed personalized model-based methods. For example, pFedHN"}, {"title": "6.2 Federated Graph Learning", "content": "Due to the great prowess of FL, it is natural to apply FL to graphdata and solve the data isolation issue. Recently, a cornucopia ofstudies has extended FL to graph data for different downstreamtasks, such as node classification [48], knowledge graph completion[4], and graph classification [38, 47], cross-client missing informa-tion completion [29, 51]. Compared with generic FL, node attributesand graph structures get entangled simultaneously in the data het-erogeneity issue of FGL. To handle this issue, a handful of studiesproposed their approaches. For example, GCFL [47] and FedStar[38] are two recent frameworks for graph classification in FGL. Theauthors of GCFL [47] investigate common and diverse properties inintra- and cross-domain graphs. They employ Clustered FL [30] inGCFL to encourage clients with similar properties to share modelparameters. A following work FedStar [38] aims to jointly train aglobal structure encoder in the feature-structure decoupled GNNacross clients. FedLit [48] mitigates the impact of link-type hetero-geneity underlying homogeneous graphs in FGL via an EM-basedclustering algorithm."}, {"title": "7 CONCLUSION", "content": "In this study, we investigate the problem of divergent neighbor-ing information in FGL. With the high node heterophily, minoritynodes in a client can aggregate adverse neighboring informationin GNN models and obtain biased node embeddings. To grapplewith this issue, we propose FedSpray, a novel FGL framework thataims to learn personalized GNN models for each client. FedSprayextracts and shares class-wise structure proxies learned by a globalfeature-structure encoder. The structure proxies serve as unbiasedneighboring information to obtain soft targets generated by thefeature-structure encoder. Then, FedSpray uses the soft labels to reg-ularize local training of the GNN models and, therefore, eliminatethe impact of adverse neighboring information on node embeddings.We conduct extensive experiments over four real-world datasetsto validate the effectiveness of FedSpray. The experimental resultsdemonstrate the superiority of our proposed FedSpray comparedwith the state-of-the-art baselines."}, {"title": "B EXPERIMENT DETAILS", "content": "B.1 Datasets\nHere we provide a detailed description of the four datasets weadopted to support our argument. These datasets are commonlyused in graph learning from various domains: PubMed in citationnetwork, WikiCS in web knowledge, Physics in co-author graph,and Flickr in social images. Table 4 summarizes the statistics andbasic information of the distributed graph data.\nB.2 Baselines\nWe compare our FedSpray with six baselines in our experiments.We provide the details of these baselines as follows."}, {"title": "B.3 Extra Experimental Results", "content": "B.3.1 Results with Varying Local Epochs. In FL, clients usuallyperform multiple local training epochs before global aggregation toreduce communication costs. We show the results of FedSpray andFedAvg with varying local epochs in Table 5. The results demon-strate that FedSpray can consistently outperform FedAvg with dif-ferent local epochs."}]}