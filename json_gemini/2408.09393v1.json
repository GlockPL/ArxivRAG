{"title": "Federated Graph Learning with Structure Proxy Alignment", "authors": ["Xingbo Fu", "Zihan Chen", "Binchi Zhang", "Chen Chen", "Jundong Li"], "abstract": "Federated Graph Learning (FGL) aims to learn graph learning models over graph data distributed in multiple data owners, which has been applied in various applications such as social recommendation and financial fraud detection. Inherited from generic Federated Learning (FL), FGL similarly has the data heterogeneity issue where the label distribution may vary significantly for distributed graph data across clients. For instance, a client can have the majority of nodes from a class, while another client may have only a few nodes from the same class. This issue results in divergent local objectives and impairs FGL convergence for node-level tasks, especially for node classification. Moreover, FGL also encounters a unique challenge for the node classification task: the nodes from a minority class in a client are more likely to have biased neighboring information, which prevents FGL from learning expressive node embeddings with Graph Neural Networks (GNNs). To grapple with the challenge, we propose FedSpray, a novel FGL framework that learns local class-wise structure proxies in the latent space and aligns them to obtain global structure proxies in the server. Our goal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. To achieve this, FedSpray trains a global feature-structure encoder and generates unbiased soft targets with structure proxies to regularize local training of GNN models in a personalized way. We conduct extensive experiments over four datasets, and experiment results validate the superiority of FedSpray compared with other baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) [46] are a prominent approach for learning expressive representations from graph-structured data. Typically, GNNs follow a message-passing mechanism, where the embedding of each node is computed by aggregating attribute information from its neighbors [11, 17, 44]. Thanks to their powerful capacity for jointly embedding attribute and graph structure information, GNNs have been widely adopted in a wide variety of applications, such as node classification [9, 12] and link prediction [2, 5]. The existing GNNs are mostly trained in a centralized manner where graph data is collected on a single machine before training. In the real world, however, a large number of graph data is generated by multiple data owners. These graph data cannot be assembled for training due to privacy concerns and commercial competitions [41], which prevents the traditional centralized manner from training powerful GNNs. Taking a financial system with four banks in Figure 1 as an example, each bank in the system has its local customer dataset and transactions between customers. As we take the customers in a bank as nodes and transactions between them as edges, the bank's local data can naturally form a graph. These banks aim to jointly train a GNN model for classification tasks, such as predicting a customer's occupation (i.e., Doctor or Teacher) without sharing their local data with each other.\nFederated Learning (FL) [25] is a prevalent distributed learning scheme that enables multiple data owners (i.e., clients) to collaboratively train machine learning models under the coordination of a central server without sharing their private data. One critical challenge in FL is data heterogeneity, where data samples are not independent and identically distributed (i.e., non-IID) across the clients. For instance, assume that Bank A in Figure 1 locates in a community adjacent to a hospital. Then most customers in Bank A are therefore likely to be labeled as Doctor while only a few customers are from other occupations (e.g., Teacher). In contrast, Bank C adjoining a school has customers labeled mostly as Teacher and only a few as Doctor. Typically, the nodes from a class that claims the very large proportion of the overall data in a client are the majority nodes (e.g., Doctor in Bank A) while minority nodes (e.g., Teacher in Bank A) account for much fewer samples. The data heterogeneity issue results in divergent local objectives on the clients and consequently impairs the performance of FL [15]. A number"}, {"title": "2 PROBLEM FORMULATION", "content": "2.1 Preliminaries\n2.1.1 Notations. We use bold uppercase letters (e.g., X) to represent matrices. For any matrix, e.g., X, we denote its i-th row vector as $x_i$. We use letters in calligraphy font (e.g., $\\mathcal{V}$) to denote sets. $|\\mathcal{V}|$ denotes the cardinality of set $\\mathcal{V}$.\n2.1.2 Graph Neural Networks. Let $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, X)$ denote an undirected attributed graph, where $\\mathcal{V} = \\{v_1, v_2, ..., v_n\\}$ is the set of $|\\mathcal{V}|$ nodes, $\\mathcal{E}$ is the edge set, and $X \\in \\mathbb{R}^{|\\mathcal{V}|\\times d_x}$ is the node feature matrix. $d_x$ is the number of node features. Given each node $v_i \\in \\mathcal{V}$, $\\mathcal{N}(v_i)$ denotes the set of its neighbors. The ground-truth label of each node $v_i \\in \\mathcal{V}$ can be denoted as a $d_c$-dimensional one-hot vector $y_i$; where $d_c$ is the number of classes. The node homophily [23, 49] is defined as\n$h_i = \\frac{|\\{v_j | v_j \\in \\mathcal{N}(v_i) \\text{ and } y_j = y_i\\}|}{|\\mathcal{N}(v_i)|}$    (1)\nwhere $|\\mathcal{N}(v_i)|$ denotes the degree of node $v_i$. Typically, an $L$-layer GNN model $f$ parameterized by $\\Theta$ maps each node to the outcome space via a message-passing mechanism [11, 17]. Specifically, each node $v_i$ aggregates information from its neighbors in the $l$-th layer of a GNN model by\n$h_i^l = f_l(h_i^{l-1}, \\{h_j^{l-1}: v_j \\in \\mathcal{N}(v_i)\\}; \\theta_l),$    (2)\nwhere $h_i^l$ is the embedding of node $v_i$ after the $l$-th layer $f_l$, and $\\theta_l$ is the parameters of the message-passing function in $f_l$. The raw feature of each node $v_i$ is used as the input layer, i.e., $h_i^0 = x_i$. For the node classification task, the node embedding $h_i^L$ after the final layer is used to compute the predicted label distribution $\\hat{y}_i =$ Softmax$(h_i^L) \\in \\mathbb{R}^{d_c}$ by the softmax operator.\n2.1.3 Personalized FL. Given a set of $K$ clients, each client $k$ has its private dataset $\\mathcal{D}^{(k)} = \\{(x_i^{(k)}, y_i^{(k)})\\}_{i=1}^{N^{(k)}}$ where $N^{(k)}$ is the number of samples in client $k$. The overall objective of the clients is\n$\\min_{\\{\\Theta^{(1)}, \\Theta^{(2)},...,\\Theta^{(K)}\\}}\\sum_{k=1}^K \\frac{N^{(k)}}{N} \\mathcal{L}^{(k)}(\\mathcal{D}^{(k)}; \\Theta^{(k)}),$   (3)\nwhere $\\mathcal{L}^{(k)}(\\Theta^{(k)})$ is the local average loss (e.g., the cross-entropy loss) over local data in client k, and $N = \\sum_{k=1}^K N^{(k)}$. Standard FL methods aim to learn a global model $\\Theta = \\Theta^{(1)} = \\Theta^{(2)} = ... = \\Theta^{(K)}$. As a representative method in FL, FedAvg [25] performs local updates in each client and uploads local model parameters to a"}, {"title": "3 MOTIVATION", "content": "In this section, we first conduct an empirical study on the PubMed dataset [31] to investigate the impact of divergent neighboring information across clients on minority nodes when jointly training GNNs in FGL. The observation from this study is consistent with our example in Figure 1 and motivates us to learn global structure proxies as favorable neighboring information. We then develop the-oretical analysis to explain how aligning neighboring information across clients can benefit node classification tasks in FGL.\n3.1 Empirical Observations\nTo better understand the divergent neighboring information across clients with its impact on the node classification task in FGL, we conduct preliminary experiments to compare the performance of federated node classification with MLP and GNNs as local models on the PubMed dataset [31]. Following the data partition strategy in previous studies [14, 51], we synthesize the distributed graph data by splitting each dataset into multiple communities via the Louvain"}, {"title": "3.2 Theoretical Motivation", "content": "According to the above empirical observations, minority nodes with the original neighboring information are more likely to be misclassified. One straightforward approach to this issue is enabling nodes to leverage favorable neighboring information from other clients for generating node embeddings. Specifically, we consider constructing global neighboring information in the feature space. The server collects neighboring feature vectors from each client and computes the global class-wise neighboring information via FedAvg [25]. We aim to theoretically investigate whether the global neighboring information can benefit node classification tasks when replacing the original neighbors of nodes. Following prevalent ways of graph modeling [8, 24, 40], we first generate random graphs in each client using a variant of contextual stochastic block model [40] with two classes.\n3.2.1 Random Graph Generation. The generative model gen-erates a random graph in each client via the following strategy."}, {"title": "4 METHODOLOGY", "content": "In this section, we present the proposed FedSpray in detail. Figure 3(a) illustrates an overview of FedSpray. The goal of FedSpray is to let the clients learn personalized GNN models over their private graph data while achieving higher performance by mitigating the impact of adverse neighboring information in GNN models. To reach this goal, FedSpray employs a lightweight global feature-structure encoder which learns class-wise structure proxies and aligns them on the central server. The feature-structure encoder generates reliable unbiased soft targets for nodes given their raw features and the aligned structure proxies to regularize local train-ing of GNN models.\n4.1 Personalized GNN Model\nWe first introduce personalized GNN models in FedSpray.\n4.1.1 GNN backbone Model. Considering their exceptional abil-ity to model graph data, we use GNNs as the backbone of the pro-posed framework. In this study, we propose to learn GNN models for each client in a personalized manner to tackle the data hetero-geneity issue in FGL. Specifically, the personalized GNN model $f(\\Theta^{(k)})$ in client k outputs the predicted label distribution $\\hat{y}_i^{(k)}$ for each node $v_i^{(k)} \\in \\mathcal{V}_L^{(k)}$. Note that FedSpray is flexible. Any GNNs that follow the message-passing mechanism as the structure of Eq. (2) can be used as the backbone, such as GCN [17] and SGC [44].\n4.1.2 Loss formulation. During local training, $\\Theta^{(k)}$ can be up-dated by minimizing the cross-entropy loss between $y_i^{(k)}$ and $\\hat{y}_i^{(k)}$ for each labeled node $v_i^{(k)} \\in \\mathcal{V}_L^{(k)}$\n$\\mathcal{L}_{G\\_CE}^{(k)} = \\frac{1}{|\\mathcal{V}_L^{(k)}|} \\sum_{v_i^{(k)} \\in \\mathcal{V}_L^{(k)}} CE(y_i^{(k)}, \\hat{y}_i^{(k)}),$   (5)"}, {"title": "4.2 Global Feature-Structure Encoder with Structure Proxies", "content": "In this subsection, we will elucidate our design for the global feature-structure encoder and class-wise structure proxies in FedSpray. The feature-structure encoder aims to generate a reliable soft target (i.e., $p_i^{(k)}$) for each node with its raw features and structure proxy.\n4.2.1 Structure Proxies. As discussed above, a minority node can obtain adverse neighboring information from its neighbors via the message-passing mechanism, given its neighbors are proba-bly from other classes. To mitigate this issue, we propose to learn unbiased class-wise structure proxies in FedSpray, providing favor-able neighboring information for each node. Here, we formulate each structure proxy in a vectorial form. Let $S \\in \\mathbb{R}^{d_c\\times d_s}$ denote class-wise structure proxies, and each row $s_j \\in S$ denotes the $d_s$-dimensional structure proxy of the j-th node class. For each node $v_i^{(k)} \\in \\mathcal{V}_L^{(k)}$, its structure proxy $s_i^{(k)}$ will be $s_j$ if it is from the j-th class. Then, the structure proxies will be used as the input of the feature-structure encoder.\n4.2.2 Feature-Structure Encoder. In FedSpray, we employ a lightweight feature-structure encoder to generate a reliable soft target for a node with its raw feature and structure proxy as the input. Figure 3(b) illustrates our design for the feature-structure encoder. Let $g(\\omega)$ denote the feature-structure encoder $g$ param-eterized by $\\omega$. Given a node $v_i^{(k)} \\in \\mathcal{V}^{(k)}$, the feature-structure"}, {"title": "4.3 Server Update", "content": "As stated above, FedSpray will learn the feature-structure encoder and the structure proxies globally. In this subsection, we present the global update in the central server for the feature-structure encoder and the structure proxies, respectively.\n4.3.1 Update global feature-structure encoder. During each round r, the server performs weighted averaging of local feature-structure encoders following the standard FedAvg [25] with each coefficient determined by the local node size\n$\\omega_r = \\frac{\\sum_{k=1}^K N^{(k)} \\omega_r^{(k)}}{\\sum_{k=1}^K N^{(k)}}.$   (14)\n4.3.2 Structure proxy alignment. Instead of using the local node size, we propose to assign higher weights to majority classes than minority classes for structure proxy alignment. More specif-ically, the server updates global structure proxy $s_{j,r} \\in S_r$ during round r by\n$s_{j,r} \\leftarrow \\frac{\\sum_{k=1}^K a_j^{(k)} s_i^{(k)}}{\\sum_{k=1}^K a_j^{(k)}},$   (15)\nwhere $a_j^{(k)}$ is the ratio of nodes from the j-th class among $\\mathcal{V}_L^{(k)}$ in client k and $a_j = \\sum_{k=1}^K a_j^{(k)}.$"}, {"title": "4.4 Overall Algorithm", "content": "Algorithm 1 shows the overall algorithm of the proposed FedSpray. During each round, each client performs local updates with two phases. In Phase 1, each client trains its personalized GNN models $\\Theta^{(k)}$ for E epochs. We first compute $p_i^{(k)}$ for node $v_i$ by the global feature-structure encoder $g(\\omega_{r-1})$ with its feature $x_i^{(k)}$ and cor-responding structure proxy $s_i^{(k)}$ (line 5). Then $p_i^{(k)}$ is utilized to compute $\\mathcal{L}_G^{(k)}$ (line 9) for training the GNN model (line 10). In Phase 2, the feature-structure encoder and structure proxies will be optimized for E epochs. In client k, we first obtain $\\hat{y}_i^{(k)}$ for node $v_i^{(k)}$ by the up-to-date GNN model (line 14). $\\hat{y}_i^{(k)}$ for node $v_i^{(k)}$ will be used to compute $\\mathcal{L}_{LF}^{(k)}$ (line 19). Then we update $\\omega_r$ and $s_i^{(k)}$ via gradient descent (line 20-21). At the end of each round, $s_j \\in \\mathcal{S}^{(k)}$ will be updated by averaging $s_i^{(k)}$ of nodes from the j-th class (line 23). At the end of each round, the local feature-structure encoder and structure proxies will be sent to the central server"}, {"title": "4.5 Discussion", "content": "FedSpray exhibits superior advantages from various perspectives, including communication efficiency, privacy preservation, and com-putational cost. We provide an in-depth discussion about FedSpray's principal properties as follows.\n4.5.1 Privacy Preservation. The proposed FedSpray uploads the parameters of local feature-structure encoders following the preva-lent frameworks in FL [18-20]. Here, we mainly discuss the privacy"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct empirical experiments to demonstrate the effectiveness of the proposed framework FedSpray and perform detailed analysis of FedSpray."}, {"title": "5.1 Experiment Setup", "content": "5.1.1 Datasets. We synthesize the distributed graph data based on four common real-world datasets from various domains, i.e., PubMed [31], WikiCS [26], Coauthor Physics [33], and Flickr [50]. We follow the strategy in Section 3.1 to simulate the distributed graph data and summarize the statistics and basic information about the datasets in Appendix B.1. We randomly select nodes in clients and let 40% for training, 30% for validation, and the remaining for testing. We report the average classification accuracy for all nodes and minority nodes over the clients for five random repetitions.\n5.1.2 Baselines. We compare FedSpray with six baselines includ-ing (1) Local where each client train its GNN model individually; (2) FedAvg [25], the standard FL algorithm; (3) APFL [6], an adaptive approach in personalized FL; (4) GCFL [47], (5) FedStar [38], and (6) FedLit [48], three state-of-the-art FGL methods. More details about the above baselines can be found in Appendix B.2.\n5.1.3 Hyperparameter setting. As stated previously, FedSpray is compatible with most existing GNN architectures. In the exper-iments, we adopt three representative ones as backbone models: GCN [17], SGC [44], and GraphSAGE [11]. Each GNN model in-cludes two layers with a hidden size of 64. The size of feature embeddings and structure proxies is also set as 64. Therefore, the feature-structure encoder has similar amounts of parameters with GNN models. Each component in the feature-structure encoder is implemented with one layer. We use an Adam optimizer [16] with learning rates of 0.003 for the global feature-structure encoder and personalized GNN models, 0.02 for structure proxies. The two hyperparameters $\\lambda_1$ and $\\lambda_2$ are set as 5 and 1, respectively. We run all the methods for 300 rounds, and the local epoch is set as 5."}, {"title": "5.2 Effectiveness of FedSpray", "content": "We first show the performance of FedSpray and other baselines on node classification over the four datasets with three backbone GNN models. Table 2 reports the average classification accuracy on all nodes and minority nodes in the test set across clients.\nFirst, we analyze the results of overall accuracy on all test nodes. According to Table 2, our FedSpray consistently outperforms all the baselines on node classification accuracy for overall test nodes across clients. Local and FedAvg achieve comparable performance"}, {"title": "5.3 Analysis of FedSpray", "content": "5.3.1 Influence of hyperparameter $\\lambda_1$. The hyperparameter $\\lambda_1$ controls the contribution of the regularization term in $\\mathcal{L}_G^{(k)}$. We conduct the sensitivity analysis on $\\lambda_1$ in FedSpray. Figure 4 reports the classification accuracy of FedSpray on all nodes and test nodes in the test sets with different values of $\\lambda_1$ over PubMed (left) and WikiCS (right) with GraphSAGE. The accuracy on all nodes remains high when $\\lambda_1$ is relatively small (i.e., $\\lambda_1 = 0.1, 1, 5$). However, the accuracy of minority nodes will decrease when $\\lambda_1$ is too small because the feature-structure encoder cannot sufficiently regularize local training of GNN models with too small $\\lambda_1$. When $\\lambda_1$ gets too large, the accuracy of all nodes decreases in both figures. In this case, the regularization term weighs overwhelmingly in the loss for training GNN models; then GNN models cannot be sufficiently trained with label information. According to the above observations, we will recommend 10 for PubMed with GraphSAGE and 5 for WikiCS with GraphSAGE as the best setting for $\\lambda_1$.\n5.3.2 Influence to structure proxy dimension. Since FedSpray incorporates structure proxies in the feature-structure encoder, we may set a different dimension $d_s$ of structure proxies. We evaluate"}, {"title": "6 RELATED WORK", "content": "6.1 Federated Learning\nRecent years have witnessed the booming of techniques in FL and its various applications in a wide range of domains, such as computer vision [3, 28], healthcare [21, 36], and social recommendation [22, 43]. The most important challenge in FL is data heterogeneity across clients (i.e., the non-IID problem). A growing number of studies have been proposed to mitigate the impact of data heterogeneity. For instance, FedProx [20] adds a proximal term to the local training loss to keep the updated parameters close to the global model. Moon [18] uses a contrastive loss to increase the distance between the current and previous local models. FedDecorr [34] mitigates dimensional collapse to prevent representations from residing in a lower-dimensional space. In the meantime, a battery of studies proposed personalized model-based methods. For example, pFedHN"}, {"title": "7 CONCLUSION", "content": "In this study, we investigate the problem of divergent neighbor-ing information in FGL. With the high node heterophily, minority nodes in a client can aggregate adverse neighboring information in GNN models and obtain biased node embeddings. To grapple with this issue, we propose FedSpray, a novel FGL framework that aims to learn personalized GNN models for each client. FedSpray extracts and shares class-wise structure proxies learned by a global feature-structure encoder. The structure proxies serve as unbiased neighboring information to obtain soft targets generated by the feature-structure encoder. Then, FedSpray uses the soft labels to reg-ularize local training of the GNN models and, therefore, eliminate the impact of adverse neighboring information on node embeddings. We conduct extensive experiments over four real-world datasets to validate the effectiveness of FedSpray. The experimental results demonstrate the superiority of our proposed FedSpray compared with the state-of-the-art baselines."}, {"title": "A PROOF OF PROPOSITION 3.1", "content": "PROPOSITION 3.1. Given a set of K clients, each client k owns a local graph $\\mathcal{G}^{(k)} \\sim Gen(\\mu_1, \\mu_2, p^{(k)}, q^{(k)})$, dist = $|\\mu_1-\\mu_2||_2$, which is smaller than dist' = $\\left(1 + 2\\sum_{k=1}^K (1 - q^{(k)}) (p^{(k)} - \\frac{1}{2})\\right)|\\mu_1-\\mu_2||_2$.\nPROOF. Without loss of generality, we assume that the majority class is $c_1$ for each client $k=1,2,...,M$ and $c_2$ for each client $k=M+1,M+2,...,K$. Based on the neighborhood distributions, the neighboring features aggregated by the message-passing mech-anism in GNNs follow Gaussian distribution\n$h_i^{(k)} \\sim N(\\left(p^{(k)} \\mu_1 + (1 - p^{(k)}) \\mu_2\\right), \\frac{I}{\\sqrt{|\\mathcal{N}(v_i)|}}), $   (16)\nfor each client $k=1,2,...,M$, and\n$h_i^{(k)} \\sim N(\\left((1 - p^{(k)}) \\mu_1 + p^{(k)} \\mu_2\\right), \\frac{I}{\\sqrt{|\\mathcal{N}(v_i)|}}), $    (17)\nfor each client $k=M+1,M+2,...,K$.\nThe expectation of node embeddings after the message-passing mechanism will be $E_{c_1}[x_i^{(k)}+h_i^{(k)}]$ for class $c_1$ and $E_{c_2}[x_i^{(k)}+h_i^{(k)}]$ for class $c_2$. We omit the linear transformation because it can be absorbed in the linear GNN classifiers. The decision boundary of the optimal linear classifier is defined by the hyperplane $\\mathcal{P}$ that is orthogonal to\n$E_{c_1}[x_i^{(k)}+h_i^{(k)}] - E_{c_2}[x_i^{(k)}+h_i^{(k)}] = E_{c_1}[x_i^{(k)}] + E_{c_1}[h_i^{(k)}] - E_{c_2}[x_i^{(k)}] - E_{c_2}[h_i^{(k)}] $    (18)\nFor each client k, we have $E_{c_1}[h_i^{(k)}] = E_{c_2}[h_i^{(k)}]$. Therefore,\n$E_{c_1}[x_i^{(k)}+h_i^{(k)}] - E_{c_2}[x_i^{(k)}+h_i^{(k)}] = E_{c_1}[x_i^{(k)}] - E_{c_2}[x_i^{(k)}] = \\mu_1 - \\mu_2, $   (19)\nand the distance from each class to $\\mathcal{P}$ is\ndist = $\\frac{||\\mu_1 - \\mu_2||_2}{2}.$   (20)\nLet the server collect neighboring information from each client via FedAvg. The global neighboring information will be\n$s_1 = \\sum_{k=1}^M h_i^{(k)} + \\sum_{k=M+1}^K q^{(k)} h_i^{(k)} $   (21)\nfor class 1 and\n$s_2 = \\sum_{k=1}^M q^{(k)} h_i^{(k)} + \\sum_{k=M+1}^K  h_i^{(k)} $   (22)\nfor class 2. In this case, we replace $h_i^{(k)}$ in Eq. (19) and get the new hyperplane $\\mathcal{P}'$ that is orthogonal to\n$E_{c_1}[x_i^{(k)}+s_1] - E_{c_2}[x_i^{(k)}+s_2] = E_{c_1}[x_i^{(k)}] + E_{c_1}[s_1] - E_{c_2}[x_i^{(k)}] - E_{c_2}[s_2] $   (23)\n= $\\mu_1 - \\mu_2 + E_{c_1}[s_1] - E_{c_2}[s_2]$,\nwhere"}, {"title": "B EXPERIMENT DETAILS", "content": "B.1 Datasets\nHere we provide a detailed description of the four datasets we adopted to support our argument. These datasets are commonly used in graph learning from various domains: PubMed in citation network, WikiCS in web knowledge, Physics in co-author graph, and Flickr in social images. Table 4 summarizes the statistics and basic information of the distributed graph data.\nB.2 Baselines\nWe compare our FedSpray with six baselines in our experiments. We provide the details of these baselines as follows."}]}