{"title": "LLMs-in-the-loop Part-1:\nExpert Small AI Models for Bio-Medical Text Translation", "authors": ["Bunyamin Keles", "Murat Gunay", "Serdar I.Caglar"], "abstract": "Machine translation is indispensable in healthcare for enabling the global dissemination of medical knowledge across languages. However, complex medical terminology poses unique challenges to achieving adequate translation quality and accuracy. This study introduces a novel \"LLMs-in-the-loop\" approach to develop supervised neural machine translation models optimized specifically for medical texts. While large language models (LLMs) have demonstrated powerful capabilities, this research shows that small, specialized models trained on high-quality in-domain (mostly synthetic) data can outperform even vastly larger LLMs.\nCustom parallel corpora in six languages were compiled from scientific articles, synthetically generated clinical documents, and medical texts. Our LLMs-in-the-loop methodology employs synthetic data generation, rigorous evaluation, and agent orchestration to enhance performance. We developed small medical translation models using the MarianMT base model. We introduce a new medical translation test dataset to standardize evaluation in this domain. Assessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our MarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo.\nResults demonstrate that our LLMs-in-the-loop approach, combined with fine-tuning high-quality, domain-\nspecific data, enables specialized models to outperform general-purpose and some larger systems. This research, part of a broader series on expert small models, paves the way for future healthcare-related AI developments, including deidentification and bio-medical entity extraction models. Our study underscores the potential of tailored neural translation models and the LLMs-in-the-loop methodology to advance the field through improved data generation, evaluation, agent, and modeling techniques.", "sections": [{"title": "Introduction", "content": "The ability to think and communicate is significantly impacted by language, which has become increasingly important due to technological advancements and global internet access. English has emerged as a widely used lingua franca, with numerous individuals worldwide learning it to facilitate communication [1, 2]. In societies where multiple languages are spoken, effective communication is crucial for cross-border interactions and daily exchanges [3]. As a result, there is a growing demand for translation, which plays a vital role in academic language teaching and enables the global dissemination of information [3, 4]. However, translating poses challenges as it involves transferring meaning between different languages, and neglecting the diverse range of text types can hinder the accuracy and variety of translations [5].\nMedical translation is crucial in bridging communication gaps in the healthcare field. Throughout history, languages such as Latin, Greek, Arabic, Hebrew, Syriac, and Persian have been translated for medical education [6, 7]. English emerged as the dominant language in medical education during the 19th century, and today, it is recognized as the universal language for scientific communication, with most medical research articles written in English. This is especially important considering the vast number of medical articles published annually, which amounts to approximately 10 million [8-10].\nMedical translation plays a significant role in knowledge transfer across various language domains, encompassing drug prospectuses, medical books, patient notes, and articles [11-13]. With the widespread use of English in medical education and the preference among doctors in multilingual Arab countries, accurate and efficient translation of medical texts has become increasingly essential for interprofessional communication, prescription writing, and report generation [12, 14]. Particularly amid the global concern of the coronavirus pandemic, the demand for comprehensible medical information in English has grown exponentially, highlighting the importance of human translators in improving access to healthcare and enhancing telemedicine practices [15, 16].\nDue to the high cost and time constraints associated with expert translation, there is a growing need to develop affordable, high-quality, and accessible machine translation (MT) solutions in the medical field [17, 18]. Effective medical translation is crucial in knowledge sharing and can significantly improve healthcare outcomes [19]. Although English is widely used in medicine, many medical studies are written in languages other than English, necessitating medical text translation between English and other languages.\nIn the medical field, professionals use various types of texts to communicate with each other, such as discharge summaries, medications, case studies, case notes, epicrises, academic studies, and imaging reports. These texts contain specific terms and require accurate and prompt translation to avoid errors.\nThe advancement of artificial intelligence algorithms has paved the way for developing machine translation (MT) systems that can effectively address medical translation requirements [20].\nThese systems leverage the analysis of paired text samples and can be trained to cater to specific medical domains, enabling the generation of accurate translations [20]. This study focuses on developing medical text translation models using the Neural Machine Translation (NMT) approach, which has gained attention for its ability to optimize translation performance [21]. NMT employs a sequence-to-sequence (seq2seq) architecture with Recurrent Neural Network (RNN) components, consisting of an encoder and a decoder, to convert source text into target text in different languages [22, 23]. To handle longer sequences, variants of RNN cells such as Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU) are utilized, ensuring relevant information transfer and enhancing translation accuracy over longer-term dependencies [24, 25]."}, {"title": "Background", "content": "With the impact of globalization, interaction has increased in every field, such as medicine. In this context, accurate and effective translation of medical texts between different languages is vital for disseminating research findings, sharing clinical data, and overcoming language barriers so scientists can work in parallel with their colleagues in other nations.\nUsers may use a translation to get a general idea of a context. However, translation accuracy is crucial in some applications, such as medicine, where zero errors are required. A mistranslation between patient and physician can jeopardize patient safety. While Statistical Machine Translation (SMT) progress is slowing down, promising methods such as neural networks are needed [26]. Machine translation has also been applied to the medical field due to advances in language technologies. For example, in the US, both regional and national translations of medical documents have been carried out [27]. While these translations were previously considered insufficient, developing technologies show they can soon provide translations of the required quality [19].\nMachine Translation (MT) is closely related to advanced rule-based systems or statistical phrase-based methods and dates back many years [28]. NMT, first introduced in 2013 [29], has become more popular recently [30]. In this context, a new neural network model called RNN Encoder-Decoder was proposed by researchers in 2014 [22]. In 2016, Google developed the Google Neural Machine Translation (GNMT) model using deep learning techniques [31, 32], which the German company DeepL developed as a CNN-based model in 2017. Today, open-source translation solutions trained by Hugging Face have paved the way for researchers to translate independently [30].\nIn this study, we present the results of medical translation models by fine-tuning MarianMT, which has recently been very popular in MT models, using medical texts from six different languages.\nMarianMT is an efficient and self-contained NMT framework written entirely in C++ with minimal dependencies. It relies only on Boost, CUDA, or a BLAS library, allowing barrier-free optimization at all levels. MarianMT features its automatic differentiation engine based on dynamic computation graphs. This enables implementing new models, custom operators, and GPU kernels without changing the core library. Optimizations like meta-algorithms for multi-node training, efficient batched beam search, and compact model implementations can be done in performant C++ code. MarianMT demonstrates state-of-the-art results on the WMT2017 English-German news translation task. It also shows case studies for automatic post-editing and grammatical error correction research. Experiments exhibit MarianMT's high training and translation speeds, achieving 30x faster training than Nematus on 8 GPUs. MarianMT is a high-performance, flexible, self- contained NMT framework allowing C++-level optimization. Its efficiency makes it suitable for cutting-edge NMT research [33].\nRecently developed Large Language Models (LLM) have been used in many fields. Moslem, Haque [34] showed that large language models such as GPT-3.5 can be used for adaptive MT. Their study indicates that GPT-3.5 can produce more consistent translations by imitating these examples when given sample translation sentences. It is also shown that GPT-3.5 can successfully perform tasks such as terminology extraction and terminology-constrained translation with zero training. The results show that GPT-3.5 can produce quality translations that compete with robust decoder-encoder-based MT systems. The work of Moslem, Haque [34] is essential because it shows that large language models have great potential in adaptive MT. There are services on the web that provide acceptable quality translations like Moslem, Haque [34] did. However, these translation services perform lower than their general performance in domain-specific translations such as medical areas.\nIt is known that in some cases larger models with more parameters, such as LLMs, do not produce better scores. In this context, in a study [35], it is stated that the Marian model [33] with 7.6 mn parameters produces better scores using the ClinSpEn-2022 benchmark dataset compared to much larger models such as Clinical-NLLB [36] (54 bn) and Clinical-WMT21fb (4.7 bn) [37].\nWithin the scope of this study, results are obtained with the AI Amplified MT model using the benchmark dataset used by Han, Gladkoff [35] and presented in the result section of this study. It is determined that the results we obtained are above the scores of all three studies given by Han, Gladkoff [35]."}, {"title": "Methodology", "content": "Medical texts are generally characterized by the unique features of medical terminology and meaningful and long sentences in different fields. The primary objective of this study is to achieve zero-error translation of medical texts with grammatical features such as passive constructions and third person.\nThe literature survey shows that translation models have been developed by fine-tuning MarianMT in studies such as English-to-Malayalam [18, 38], English-Ukrainian [39] and improving the Norwegian Translation model [40]. However, it can be stated that there are few medical translation models developed in different languages, and there is still a great need for medical text translation models.\nWithin the scope of this study, medical translation models have been developed in six languages (Table 1)."}, {"title": "3.1. Datasets", "content": "This study aims to translate medical texts into six different languages, as mentioned before. Custom datasets synthetically generated, and publicly available datasets were also used in the model development. Domain experts evaluate and curate high-quality language pairs derived from in-house labeled data and synthetically generated content. The sentence pairs and token numbers of the developed models are shown in Table 2.\nWithin the scope of this study, in addition to the data on the Opus.nlp [46] web page, scientific articles, and medical texts belonging to each language were scraped, and parallel corpora were prepared by us to be translated from English to the specified target languages and from these target languages to English. The scrapping process was based on the condition that the source language must have an English translation. Publicly available datasets published by language pairs were also used [47].\nOPUS is a continuously updated collection of translated texts from the web. In the OPUS project, data is made available to users in various formats to help transform and align free online datasets, add linguistic annotation, and make it easier to work with the data. The main goal of Opus is to support data-driven NLP, mainly statistical MT. For this reason, it provides parallel datasets in various formats that can be used to train standard MT models. However, OPUS provides the data \"as is\" without guarantees or warranties. All preprocessing and alignment in OPUS datasets are done automatically, and no manual corrections are made [46]. In addition to our in-house datasets, SciElo [48], Mespen [49], EMEA [50], and ELRC [51] datasets were also used in the language translation model studied in the context of the developed medical translation models."}, {"title": "3.2. Experimental Setup and Metrics", "content": "We evaluated the performance of our developed medical translation models using proprietary test datasets from Al Amplified. The evaluation metrics employed in this study include BLEU Score, BERT Score, METEOR Score, and ROUGE Score. We compared our results against those obtained from Google Translate, DeepL, and OPUS translation models across all language pairs.\nFor the English-to-German medical translation model specifically, we extended our comparison to include Claude-3. The Claude-3-Opus model achieved scores of 0.511 for BLEU, 0.745 for ROUGE, and 0.741 for METEOR on our test set. Due to practical constraints such as time and cost, this broader comparison including large language models was limited to this language pair (Table 2).\nTo further assess the quality of our translations, we innovatively employed ChatGPT and Claude AI as impartial judges. These LLM-based judges compared the outputs of Google Translate against our developed MT models (Tables 7 and 8). This approach was chosen over human translation to mitigate time and cost constraints while still providing valuable insights into translation quality.\nFor this comparative evaluation, we randomly selected 100 sentences from our test set. The prompts used to instruct the LLM judges in this MT evaluation process are detailed below."}, {"title": "3.2.1 Claude-3-Opus prompt:", "content": "<<Human: You are a professional translator and interpreter specializing in healthcare and biomedical. You will compare and evaluate the two translations.\nI want you to help me in assessing translations from English to Turkish. The translated texts are from the healthcare and biomedical domain. Your evaluation should focus on adherence to medical terminology, simplicity, clearness, and accuracy, and you are expected to assign a score ranging from 0 to 100 to each translation. You MUST think, evaluate yourself, and do not generate text. Your ultimate output MUST only consist of the evaluation scores for each translation only in a JSON format as in <JSON></JSON> XML tags.\nThe output format:\n<JSON> {\"Model-1\": score, \"Model-2\": score} </JSON>\nDo you understand your task?\nAssistant: Certainly, I understood. You will provide two different translations from English to Turkish, and I will evaluate as a medical translator expert and assign scores to each one by adherence to medical terminology, simplicity, consistency, and accuracy. The format will be a JSON file containing only scores. Ok, let us start.\nHuman: English text: I have a headache.\nModel-1: Ba\u015f\u0131m a\u011fr\u0131yor.\nModel-2: Ba\u015f\u0131mda a\u011fr\u0131 var.>>"}, {"title": "3.2.2 GPT-4-Turbo prompt:", "content": "<<System: You are a professional translator and interpreter specializing in healthcare and biomedical. You will compare and evaluate between two translations.\nUser: I want you to help me assess translations from English to Turkish. The translated texts are from the healthcare and biomedical domain. Your evaluation should focus on adherence to medical terminology, simplicity, clearness, and accuracy, and you are expected to assign a score ranging from 0 to 100 to each translation. You MUST think, evaluate yourself, and do not generate text. Your ultimate output MUST only consist of the evaluation scores for each translation in JSON format. Do you understand your task?\nAssistant: Certainly, I understood. You will provide two different translations from English to Turkish, and I will evaluate as a medical translator expert and assign scores to each one by adherence to medical terminology, simplicity, consistency and accuracy. The format will be a JSON file containing only scores. Ok, let us start.\nUser: English text: I have a headache.\nModel-1: Ba\u015f a\u011fr\u0131m var.\nModel-2: Ba\u015f\u0131mda a\u011fr\u0131 var.\nAssistant: {\"Model-1\": 90, \"Model-2\": 80}>>\nThe potential of Large Language Models (LLMs) in translation studies was evaluated as part of this research. A notable challenge when using LLMs for translation is their inability to provide fuzzy-match suggestions. To address this, we randomly selected 10,000 sentences from the medical domain and generated their embeddings using the \"multilingual-e5-large\" model [52]. These embeddings were stored using FAISS libraries. We then selected 725 sentences from our existing test set for prediction. During the prediction process, we retrieved the two sentences most similar to the input sentence from the pre-generated set of 10,000, based on cosine similarity scores, to create a fuzzy match. The performance scores of these LLM-based predictions are presented in Table 3-6.\nFor the development of our Machine Translation (MT) models, we prepared datasets in TMX, Moses, and Dublin Core formats. Data preparation involved several steps: removing HTML tags, decoding Unicode characters, and calculating character count ratios between sentence pairs. To mitigate anomalies, we selected sentence pairs with character count ratios between 0.8 and 1.25, considering the specific characteristics of each language. This process resulted in the creation of train, test, and development datasets, with the number of sentence pairs and tokens for each detailed in Table 2.\nFollowing data cleaning, sentence alignment, and corpus preparation based on predetermined ratios, we employed sentence transformers to assess the semantic relationships between sentence pairs. The semantic closeness of each pair was quantified using cosine similarity. We retained only those sentence pairs with a cosine similarity score of 0.90 or higher, thus finalizing our datasets through these filtration and transformation processes. Table 2 also presents the embeddings utilized in the development of our medical translation models.\nThe translation models were optimized using the following hyperparameters: a batch size of 16 for both training and evaluation, epoch sizes ranging from 20 to 50, and a learning rate of 2e-5. Model development involved training sessions lasting between 40 and 130 hours, depending on the specific language pair and dataset size."}, {"title": "4. Result", "content": "Extensive research in language translation reveals a continued demand for high-end translation services, particularly in the medical field. To meet this need, we have introduced a medical translation model covering six languages meticulously designed for use by healthcare professionals and various stakeholders.\nThis model performed remarkably well when evaluated against our in-house test datasets. The medical translation field currently needs more shared open-source benchmark test data. In conclusion, the findings presented by AI Amplified originate from our in-house test datasets, which have been precisely crafted in line with Al best practices and algorithmic principles.\nThe performance of our translation models has been rigorously evaluated using well-established benchmark scores such as BERT, BLEU, METEOR, and ROUGE Scores, which have been widely used in numerous studies (Tables 3, 4, 5, 6).\nAI-Amplified's translation models demonstrate exceptional performance across multiple evaluation metrics. The scores obtained for BERT, BLEU, METEOR, and ROUGE consistently rank among the highest in our comparative analysis, with only a few minor exceptions. This comprehensive evaluation underscores the robustness and accuracy of our models in medical translation tasks.\nIn this work, we also compared the results of Google Translate and our MT model [53] in six languages using two LLM models, GPT4-Turbo (Figure 1 and Table 7) and Claude-3-Opus (Figure 2 and Table 8), as referees.\nIn the MT comparison for GPT4-Turbo with a total of 1200 instances, AI-Amplified outperformed Google Translate in 504 instances, while Google Translate performed better in 437 instances, and both models achieved the same score in 259 instances."}, {"title": "In the context of language samples, using Claude-3-Opus, as a referee, our models produce better scores than\nGoogle Translate models in all translations. Sentence pairs of our MT models developed by AI-Amplified are\navailable on our GitHub page.", "content": "To investigate transfer learning methods of clinical texts with multilingual pre-trained language models (MPLMs) in a study evaluating the performance of medical text translations between English and Spanish [35], Clinical-Marian [33], Clnical-NLLB [36] and Clinical-WMT21fb [37] models were compared. The ClinSpEn-2022 competition dataset was used for the evaluation. BLUE, METEOR, ROUGE, and COMET scores were compared. The ClinSpEn-2022 dataset used in this study was evaluated as a benchmark and the scores we obtained as AI Amplified were compared with those obtained by Han, Gladkoff [35] (Table 9)."}, {"title": "5. Conclusion", "content": "Machine Translation (MT) has become increasingly crucial in artificial intelligence due to globalization and technological advancements. The need for accurate translations, particularly in sharing scientific research and medical terminology across languages, has grown exponentially. This study demonstrates that AI-based MT, specifically tailored for the medical domain, plays a critical role in meeting this demand.\nOur research introduced a novel \"LLMs-in-the-loop\" approach to develop specialized neural machine translation models for medical texts. We focused on six language pairs: English to and from Spanish, German, French, Romanian, Turkish, and Portuguese. The models employ encoder-decoder architectures with LSTM units, trained on custom parallel corpora compiled from scientific articles and medical texts.\nKey findings of our study include:\n1. Small, specialized models trained on high-quality in-domain data can outperform larger, general-purpose models. For instance, our English-German model achieved BERT and BLEU Scores of 0.969 and 65.44, surpassing GPT-4 Turbo's scores of 0.955 and 54.13 respectively.\n2. The LLMs-in-the-loop methodology, incorporating synthetic data generation, rigorous model evaluation, and agent orchestration, significantly enhanced model performance.\n3. When evaluated by LLM models like GPT-4 and Claude-3 acting as expert judges, our models were preferred over Google Translate in 68-86% of sample evaluations across all languages.\n4. Balancing domain-specific data with out-of-domain data (10-20% ratio) during fine-tuning helps maintain translation quality for general expressions while excelling in medical terminology.\n5. The importance of starting with a strong general translation model before fine-tuning with domain-specific data was highlighted.\nOur approach of combining medical domain-specific datasets with a proportion of out-of-domain data yielded successful results. However, we noted that fine-tuning a general language model with purely domain-specific data can decrease translation quality for out-of-domain texts. Our method of merging domain-specific and general datasets mitigated this issue.\nWhile our results are promising, it's important to note that they were obtained using our proprietary datasets due to limited availability of open-source medical translation test sets. This underscores the need for more publicly available resources in this field.\nLooking ahead, the development of AI models working with more languages and larger datasets holds great potential for advancing medical translation. As AI Amplified, we are committed to continually improving our datasets and models to achieve higher performance scores.\nThis study not only demonstrates the effectiveness of our specialized MT models in the medical field but also lays the groundwork for future developments in healthcare-related natural language processing, including planned models for deidentification and healthcare entity extraction.\nOur medical translation models are accessible on our website [53]\u00ba, where demo translations can be tested. We believe these tools will significantly contribute to the global health community by facilitating knowledge sharing and accelerating scientific communication across language barriers."}, {"title": "Limitations", "content": "Our translation model has been meticulously designed and extensively trained to address the complex requirements of the healthcare and biomedical domains. While it excels within these highly specialized fields, it is crucial to acknowledge certain limitations:\n1. Domain Specificity: The model's performance may not maintain the same high standards when applied to domains outside healthcare and biomedicine. Users should carefully consider this when contemplating applications in other fields.\n2. Data Dependency: The model's proficiency is closely tied to the medical datasets used in training. While comprehensive, these datasets may not cover all possible medical scenarios or emerging terminologies.\n3. Language Pair Constraints: The model's effectiveness has been demonstrated for specific language pairs. Its performance may vary for language combinations not included in this study.\n4. Evaluation Metrics: While we used a range of metrics (BLEU, BERT, METEOR, ROUGE) and LLM-based evaluations, these may not capture all aspects of translation quality, particularly nuanced medical contexts.\n5. Continuous Updates: Medical knowledge and terminology evolve rapidly. Regular updates to the training data will be necessary to maintain the model's relevance and accuracy."}, {"title": "Future Improvements:", "content": "1. Expanding the diversity and volume of medical datasets could potentially yield higher performance scores and broader applicability.\n2. Incorporating more language pairs and dialectal variations could enhance the model's global utility.\n3. Developing domain-adaptive techniques could improve the model's versatility across different medical sub-specialties.\nWe encourage users to be mindful of these limitations when deploying the model and to contribute to its ongoing refinement through feedback and real-world application insights."}]}