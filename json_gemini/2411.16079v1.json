{"title": "Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large Language Models", "authors": ["Donggeun Ko", "Dongjun Lee", "Namjun Park", "Wonkyeong Shum", "Jaekwang Kim"], "abstract": "Neural networks struggle with image classification when biases are learned and misleads correlations, affecting their generalization and performance. Previous methods require attribute labels (e.g. background, color) or utilizes Generative Adversarial Networks (GANs) to mitigate biases. We introduce DiffuBias, a novel pipeline for text-to-image generation that enhances classifier robustness by generating bias-conflict samples, without requiring training during the generation phase. Utilizing pretrained diffusion and image captioning models, DiffuBias generates images that challenge the biases of classifiers, using the top-K losses from a biased classifier (fB) to create more representative data samples. This method not only debiases effectively but also boosts classifier generalization capabilities. To the best of our knowledge, DiffuBias is the first approach leveraging a stable diffusion model to generate bias-conflict samples in debiasing tasks. Our comprehensive experimental evaluations demonstrate that DiffuBias achieves state-of-the-art performance on benchmark datasets. We also conduct a comparative analysis of various generative models in terms of carbon emissions and energy consumption to highlight the significance of computational efficiency.", "sections": [{"title": "Introduction", "content": "Deep learning models have demonstrated outstanding performances across various computer vision tasks including image classification [4], object detection and image generation [6]. However, generalization and robustness of the models have often been one of the major issues when we utilize our trained model in real-world scenarios. As benchmark datasets do not fully reflect real-world scenarios to train deep neural networks, models are often prone to biases that exists in the dataset. Unintended bias may exhibit high spurious correlations with the target label causing the model to learn these features during training. Thus, the model fails to generalize over the training distribution.\nIn the context of deep learning frameworks at large, bias refers to the tendency of these models to disproportionately weigh specific attributes or patterns\u2014such as the shapes of an object, colors and textures within images, or their color-during the process of representation learning and subsequent classification task. Unintended biases include colors [12], textures, background, and gender [19]. These unintended biases can be further categorized into task-relevant and task-irrelevant features where they may exist in a dataset. To clarify these two concepts, we can imagine an image of a horse in two different backgrounds. One image of a horse has a grass field in a plain as a background, while the other image of a horse is a racing horse in a rough field. In these images, the background of the location of the horse is a task-irrelevant feature where these features mislead the model to learn unrelated features for the given task (i.e., image classification). In addition, if an image of a horse in a grass field is extensively abundant than the horse in a rough field, we can say that the grass field is bias-align feature while the rough field is a bias-conflict feature.\nDatasets encountered in the wild, or 'real-world' datasets, are predominantly imbalanced and exhibit bias. Consequently, models trained on benchmark datasets, which possess a different bias distribution, often underperform on real-world datasets. The variations in classifier efficacy, as highlighted in Table 1, illustrate the pivotal issue of bias within datasets. The presence of biases significantly hampers model performance, with accuracy seldom exceeding 50% on numerous datasets, including CCIFAR-10, BAR, and Dogs & Cats. Notably, an increase in the bias-conflict ratio within a dataset correlates with enhanced model performance. Therefore, employing data augmentation and facilitating the generation of bias-conflict"}, {"title": "Related Work", "content": "Efforts to address dataset biases and enhance classifier fairness have proliferated in recent research, with methodologies broadly categorized into supervised and unsupervised debiasing techniques. Supervised debiasing typically focuses on mitigation without generating synthetic bias-conflict examples. Conversely, unsupervised approaches leverage generative modles to produce samples that explicitly confront and reduce biases.\nDebiasing via Supervised Methods Early works employed supervised training with explicit bias labels [12, 24, 27]. With predefined bias labels, previous works focused on extracting the bias features and attributes in the datgasests. Recent studies aim to address biases that do not rely explicitly on acquired bias labels. For instance, GroupDRO [24] effectively clusters dataset subgroups under supervision, with subsequent enhancements introducing domain distribution generalizations through subgroup clustering and style mixing [5, 30]. Methods such as LfF [20] and Rebias [2] have further refined bias mitigation by employing dual models for bias conflict identification and feature disentanglement in latent spaces, respectively.\nDebiasing via Unsupervised Methods explores generating or augmenting data to address the scarcity of bias-challenging examples. This includes innovative strategies like SelecMix [10], which synthesizes such samples via mixup techniques, and BiasAdv [17], which employs adversarial attacks to alter bias cues in images. Generative Adversarial Networks (GANs) have also played a pivotal role, with various implementations aimed at retraining classifiers on amplified bias-conflict samples to reduce bias. Notably, A2[1] integrates StyleGAN2 for biased distribution learning, alongside few-shot adaptation techniques for sample augmentation [21]. AmpliBias [14] further demonstrates the efficacy of FastGAN in a few-shot learning context for bias amplification.\nOur work is inspired by these advancements and introduces a unique framework that capitalizes on a pretrained generative model, coupled with a latent diffusion model and an image-captioning model, to generate bias-conflict samples without additional training. This approach eliminates learning costs, offering a straightforward yet potent solution for reducing classifier bias."}, {"title": "Method", "content": "We first describe the bias extraction methodology, where we extract bias-conflict samples from the training set without any human supervision. Then, we introduce our diffusion-based pipeline for generating bias-conflict samples based on the aforementioned extraction method. Finally, we discuss debiasing the classifier with the generated and original biased dataset."}, {"title": "Problem Setup", "content": "Within the domain of image classification, we explore a task that involves deducing the label of an input image, which is characterized by both intrinsic and bias attributes (e.g., intrinsic - animal, bias - color). Take, for instance, a biased training dataset I = (x, y) encompassing attributes a_t, a_b \u2208 A representing intrinsic and bias attributes, respectively. The bias attribute a_b in the dataset drives the learning process to form correlations with the true label y, thereby obstructing the model's aptitude in accurately discerning the intrinsic attribute a_t. We elucidate two distinct scenarios within a biased dataset's attribute a_b: bias-aligned samples, which are accurately classified under the unintended decision rule of the biased classifier f_B [20], and bias-conflict samples, which are misclassified by the same rule of f_B. Thus, the goal of this work is to transition the biased classifier f_B to a debiased classifier f_D."}, {"title": "Training a Biased Classifier", "content": "In order to leverage the bias-conflict samples without necessitating human intervention, we deliberately train an image classifier to adopt a bias using the dataset I. As the model progressively learns from the bias-aligned samples, which are more prevalent in the dataset, it starts to exhibit a bias toward the attributes a_b. To accentuate this biased prediction, we employ the Generalized Cross Entropy (GCE) loss [29] in training the classifier, thereby favoring the bias-aligned samples, which are easier to learn. The GCE loss is formulated as follows:\nL_{GCE} (p(x; \u03b8), y) = \\frac{1 - p_y(x; \u03b8)^q}{q}\nGiven p(x; \u03b8) as the softmax output of the classifier and p_y (x; \u03b8) as the probability of the target attribute of the class y, we introduce a hyperparameter q \u2208 (0, 1) to modulate the degree of amplification. Particularly, a value of q = 1 aligns the equation with the standard Cross-Entropy (CE) loss. The symbol \u03b8 represents the learnable parameters of the classifier. Employing the GCE loss, we steer the model to adjust the weights of the gradients in favor of samples with higher probability p_y, as written in the equation below:\n\\frac{\\partial L_{GCE} (p(x; \u03b8), y)}{\\partial \u03b8} = p_y(x; \u03b8)^q \\cdot \\frac{\\partial L_{CE} (p(x; \u03b8), y)}{\\partial \u03b8},\nThis inclines the model to exhibit higher losses for bias-conflict samples."}, {"title": "Extracting Bias from Biased Classifier", "content": "Upon training the biased classifier, we assume that samples exhibiting higher GCE losses are likely bias-conflict samples, given that the model has learned the bias attribute present in the biased dataset. Consequently, we extract the top-K samples with elevated GCE losses from the training set, with K set to 100 in this context. Despite certain datasets having a small proportion of bias-conflict samples in the training set (e.g., a bias-conflict ratio of 0.5% or 1.0%), we conjecture that K value of 100 holds sufficient generalization potential to generate new bias-conflict samples. To elucidate further, consider an image of a cat from the Dogs and Cats dataset [12]. The bias-aligned samples predominantly feature cats with \"black\" or \"dark\" fur, whereas the bias-conflict samples encompass cats with \"lighter or white fur\". Our aim, thus, is to extract images of light-furred cats from the biased classifier. The process of extraction of X_{b.c.} is formulated as follows:\nX_{1...N} = {x_{b.c.}, x_{b.a.}},\nx_i \u2208 D_{biased}\nwhere, x_{b.c.} = argmax \\mathcal{CE}(f(x_i), Y_i),\nX_{b.c.} = {x_{b.c.,1}, x_{b.c.,2}, ..., x_{b.c.,K}},"}, {"title": "Image Captioning and Text-Filter", "content": "Image Captioning. Driven by the goal of minimizing human intervention in debiasing the classifier, we employ an image-captioning model to generate text captions based on the extracted bias-conflict samples, which will subsequently serve as the input source for text-to-image generation. In our setup, we adopt llava-llama-3-8b [3], a LLaVA [18] model fine-tuned from LLAMA [28], as our \"captioner\", leveraging a pre-trained image encoder and large language model (LLM). We utilize this LLM as a decoder-based language model to generate and predict ensuing text tokens, conditioned on a trained CLIP-ViT [22]. Given a bias-conflict sample x_{b.c.} \u2208 X_{b.c.}, the captioner generates synthetic captions T_s. To encapsulate a broader semantic understanding and features of the image through text, we generate three synthetic text captions, {T_{s,1}, T_{s,2}, T_{s,3}} \u2208 C_T, which are collated in the text corpus C_T. We postulate that through this approach, the model can adeptly capture the semantics, class labels, and the bias-conflict attribute in the predicted text.\nText-Filter. We employ a text filter, F_T, to enhance the quality of the generated text corpus, C_T, by filtering out noisy text through selection process of stop-words and top-F words. Instead of allowing arbitrary generation, we strive to ensure that the text corpus aligns semantically with the classes in the dataset, denoted as {C_1, C_2, ..., C_N } \u2208 C_{N=1..n}, where N and n represents the number of classes and the index of the last class, respectively. For instance, in the context of corrupted CIFAR10, all words are counted in the generated phrases/sentences. Then, we use stop words filter using NLTK library. This ensures to keep words of the class labels such as \"airplane\", \"automobile\", and \"boat\" which will appear more often then other objects. We choose top-F frequent words where we count 2\u00d7 no. of classes. For example, for CCIFAR-10, F = 20 since there are 10 classes. Hence, we look at top-20 frequent words for our text filter F_T. Then, F_T will filter out sentences that does not contain the top-F words. This simple method would effectively filter out generated sentences that do not have relevance when augmenting bias-conflict samples. The final filtered text corpus is C_{F.T} = F_T(C_T). For example, in Fig. 1, the caption \"person in pink sweater...\" is filtered since it does not contain class label of \"young\" or \"old\"."}, {"title": "Bias-Conflict Generation via Latent Diffusion Models", "content": "Our primary generative model leverages Latent Diffusion Models (LDMs) based on Denoising Diffusion Probabilistic Models operating in the autoencoder's latent space. Initially, an autoencoder is trained on a substantial image dataset, where the encoder \\mathcal{E} maps image x to a latent code z = \\mathcal{E}(x). The decoder \\mathcal{D} then reconstructs images from the latent code, approximating the original image x as \\mathcal{D}(\\mathcal{E}(x)) \u2248 x. Subsequently, an U-Net based diffusion model is trained to denoise the latent code, coupled with noise, and conditioned on various factors such as class labels, or text-embeddings. The LDM loss is forumlated by:\n\\mathcal{L}_{LDM} := E_{\\mathcal{E}(x),y, \\epsilon \\sim N(0,1), t} [||\\epsilon - \\epsilon_\\theta (z_t, t, c_\\theta(y))||^2]\nwhere c_\\theta (y) is the condition vector, predicated on y. z_t represents the latent code with noise at time t, while \\epsilon_\\theta and \\epsilon denote the denoising network and the noised sample of x, respectively. In our work, we condition text prompts for the condition vector c_\\theta(y) to generate denoised image x.\nOur methodology is summarized in Fig. 1. In essence, we deliberately train a biased classifier, f_B, on a biased dataset. Following this, we extract bias-conflict samples, x_{b.c.}, from the trained biased classifier employing the CE loss, facilitating the identification of samples with high CE loss, synonymous with bias-conflict samples. Thereafter, a captioner is deployed to generate synthetic captions for the bias-conflict samples, which are aggregated in the text corpus C_T and subsequently refined through a text filter, F_T, to generate filtered text corpus C_{F.F}. Next, we generate denoised image x, conditioned on the text prompts from the corpus C_{f.f} using latent diffusion model. Lastly, we create a debiased dataset I_{deb} where I_{deb} = X_{b.c.}+X_{b.a.}+X_{generated}, where we use it to train the biased classifier to eventually become a debiased classifier, f_D."}, {"title": "Experiments", "content": "Experimental Setup\nDatasets. The intrinsic and bias attributes of these datasets are enclosed in brackets as {intrinsic, bias}. Specifically, the attributes for the datasets used in our experiments are delineated as follows: CCIFAR10 [8]: {object, noise}, BFFHQ [13]: {age, gender}, Dogs & Cats [12]: {animal, color}, and BAR [20]: {action, background/environment}. Each dataset encompasses a spectrum of bias-conflict sample ratios, ranging from 0.5% to 5%. The available bias-conflict sample ratios are 0.5%, 1.0%, 2.0%, and 5.0% for CCIFAR10 and BFFHQ, while Dogs & Cats and BAR have 1.0% and 5.0% available.\nImplementation Details. In accordance with the protocol outlined in [20], we employ a ResNet-18 architecture [7] across all datasets. Notably, the BAR dataset, owing to its relatively small number of images compared to other benchmark datasets, necessitates the use of a pretrained ResNet-18 when training the classifier. Conversely, for the remaining datasets, we trained the models from scratch. Each classifier undergoes a training for 50 epochs. We report the average over three independent trials for test set accuracy. For GCE loss, we set q = 0.7 equivalent to the experimental setting of LfF [20] for fair comparison.\nFor fair comparison of measuring carbon emission and time taken for efficiency of generative model, we used the same hardware settings for all experiments. We used 1 \u00d7 A100 GPU, 16 core AMD EPYC 7763 64-Core Processor CPU and 16GB RAM for all experiments and ablation study."}, {"title": "Performance Evaluation", "content": "Performance Evaluation in Contexts with High Bias-Conflict Ratios. The analysis of our experimental results demonstrates that our model exhibits superior performance in scenarios characterized by high bias-conflict ratios. The generation process involves creating text prompts that encapsulate bias-conflict attributes, thereby enhancing the production of bias-conflict samples. Notably, our model achieved its highest performance on the Dogs Cats dataset at a 5.0% bias-conflict ratio, recording an accuracy of 94.33%. This represents a significant improvement of 5.73% over the combined LfF + BE approach. In the case of the BFFHQ dataset, our model also outperformed baseline models, achieving a leading accuracy of 88.31% with a 0.97% difference from the runner-up performance. The observed performance advantage is primarily attributed to the model's ability to accurately interpret and utilize the semantics of bias-conflict features. This capability proves particularly advantageous in comparison to synthetic datasets like CCIFAR-10, where noise elements present more considerable challenges to distinction, even to human observers. Our results suggest that the nuanced understanding and generation of text captions that accurately reflect bias-conflict attributes play a critical role in enhancing model performance in complex bias scenarios.\nAnalysis of underperformance in synthetic dataset. Our experiments demonstrated a suboptimal performance of our model on the CCIFAR-10 dataset. We believe that this shortfall is due to the high resolution in the generated samples. Generated high-resolution images enable enhanced semantic comprehension and feature extraction during training. However, the model faces challenges with small resolution of test images. Since we align our experimental setup with prevailing benchmarks by downscaling images to 32\u00d732, a significant loss of semantic information and distinctive features was observed, diminishing the model's efficacy. Additionally, the synthetic bias-conflict inherent in the CCIFAR-10 dataset's test set posed a challenge. We intend the image captioning model to process texts and include classes such as \"deer\" or \"horse\" and bias-conflict attributes such as \"noise\" or \"blur\". However, it was not able to capture the intended bias-conflict attributes occasionally during caption generation that led to an inadequately trained model on bias-conflict attributes, further impacting its performance during testing.\nSynthetic bias-conflict samples reflecting real-world samples. Our empirical analysis underscores the robustness of our framework when applied to real-world datasets, attaining state-of-the-art performance across diverse ratios within three benchmark datasets. For instance, at a 5% ratio, our model scored accuracies of 88.31%, 94.33%, and 85.23% on BFFHQ, Dogs & Cats, and BAR datasets, respectively. This suggests that the generated images bear a close resemblance to real-world scenarios. A notable observation is the significant outperformance on the Dogs and Cats dataset, compared to other datasets, which were improvements of about 2%. We postulate that LDMs have advantages at synthesizing animal and facial images compared to other domains images with dynamic movements or unusual images.\nCarbon Emission and Energy Consumption Analysis. We analyze carbon emission and energy consumption for two GAN-based models (A\u00b2 and AmpliBias) compared with a diffusion-based generative model for train and test in BFFHQ dataset. Carbon dioxide emissions are calculated through the CodeCarbon library [25]. CO2eq emissions are determined by the formula Carbon Intensity (C) \u00d7 Energy consumed during computation (E), using the average global carbon intensity value of 475 gCO2eq/KWh to ensure a fair comparison. As illustrated in Fig. 3, our model required a total of 3 hours to generate an equivalent number of bias-conflict samples to the bias-aligned samples. In contrast, A\u00b2 and AmpliBias required over 35 and 10 hours, respectively, for training and inference. During the inference phase, our model consumed more electricity, resulting in higher CO2 emissions. This increased consumption is attributable to the simultaneous computation of text generation (image captioning) and text-to-image generation steps. Moreover, diffusion models generally have more parameters than GANs, thereby incurring higher computational costs during the inference step. Nevertheless, overall, our model demonstrates lower energy consumption and CO2 emissions. The results indicate that pretrained diffusion models not only possess robust feature space representations but also conserve energy and reduce CO2 emissions, thus effectively debiasing the biased classifier."}, {"title": "Discussion and Further Analysis", "content": "t-SNE Analysis. The t-SNE embeddings from the pretrained vanilla ResNet-18 model fail to distinctly cluster the label attributes, young and old. Conversely, our trained DiffuBias framework successfully clusters these two attributes, indicating that the generated bias-conflict samples aid in debiasing the classifier. This analysis is further supported in sub-figure, where the combination of generated bias-conflict samples and original BFFHQ datasets maintains a clear separation within the t-SNE embedding space. Thus, these results demonstrate that training with the generated dataset effectively debiases the classifier, underscoring the practical utility of our approach.\nGenerated Bias-Conflict Images. We present a selection of bias-conflict samples generated by our model. Certain generated images either do not correspond with the provided labels or exhibit incompletely rendered features. For instance, under the label \"young,\" the depiction of a young girl with a balloon showcases incomplete rendering of the mouth and nose. Despite this, it is still discernible that the image represents a young girl. Additionally, a misrepresentation is observed in the generation of an old man dressed in a suit. This anomaly could stem from the extraction process where images of an old man were erroneously categorized as bias-conflict samples. In a similar vein, under the label \"old,\" the images in the first row do not align with the given label, possibly a result of erroneous extraction as bias-conflict samples during the dataset preparation stage.\nText Prompts Quality Analysis.  We present filtered text captions from the text corpus, utilized for generating synthetic bias-conflict samples. The framework exhibits superior performance on real-world datasets.\nGradCAM Visualization and Analysis. We sought to visualize and understand the mechanisms through which the networks and interpret bias-conflict images, especially when predicting their true class labels is challenging due to the presence of task-irrelevant features in the samples.\nFurther Analysis of Generated Samples.\nImpact of Text-Filter on Performance.  We explore the effect of text-filtering by conducting an ablation study on the presence of text-filter.\nLimitations & Future Work\nA limitation inherent to our work is the sole reliance on pretrained models, which may confine our approach to certain domains or tasks. For instance, our model adeptly generates images of animals or humans, yet may encounter difficulties with medical images if biases are present within them. Additionally, tasks with unbalanced labels or classes, such as anomaly detection, pose a challenge, particularly in generating anomaly images."}, {"title": "Conclusion", "content": "In this work, we introduce DiffuBias, a debiasing framework that leverages the amplification of bias to mitigate its effects. Our framework is founded on the key insight that amplifying bias-conflict samples facilitates the learning of task-relevant features, thereby reducing bias. Results demonstrate that our methods achieve state-of-the-art performance for most of the benchmark dataset. In addition, our model was able to achieve faster training and inference times compared to baselines that utilize generative models. We believe that our framework and findings provide valuable insights into the significance of augmenting and amplifying the bias of the dataset, thereby establishing a stronger foundation in addressing bias in the field of computer vision."}, {"title": "Appendix", "content": "A.1 Datasets\nWe elaborate further on the details for the benchmark datasets used in our experiments.\nCorrupted CIFAR-10 (CCIFAR10)\nBiased Flickr-Faces-HQ (BFFHQ)\nBiased Action Recognition (BAR)\nDogs & Cats.\nA.2 Training Configuration\nA.3 Further Analysis of Generated Samples"}]}