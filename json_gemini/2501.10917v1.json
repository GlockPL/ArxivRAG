{"title": "Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition", "authors": ["Haoyu Xie", "Haoxuan Li", "Chunyuan Zheng", "Haonan Yuan", "Guorui Liao", "Jun Liao", "Li Liu"], "abstract": "Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency.", "sections": [{"title": "Introduction", "content": "Human Activity Recognition (HAR) methods can be categorized into vision-based, environment-based, and wearable sensor-based approaches. Vision-based HAR leverages video data but is restricted by lighting conditions and camera coverage. Environment-based HAR uses ambient sensors such as sound or WiFi, though it faces limitations in location and often struggles with accuracy.\nWearable Human Activity Recognition (WHAR), however, involves attaching sensors directly to the body. This approach offers continuous, direct measurement of motion, is less affected by environmental factors, and is versatile across different settings. It benefits from the use of readily available devices like smartphones and watches, making it practical for applications in healthcare and sports. WHAR can be classified as a multivariate time series classification problem. The multivariate aspect arises from the various modalities and axes of sensor variables. A typical motion sensor might include a 3-axis accelerometer, a 3-axis gyroscope, and a 3-axis magnetometer, resulting in a 9-variable time series data sample.\nIn WHAR, there are two types of variable relationships: Intra- and Inter-Sensor Modality Variables in Figure 1. Intra-sensor variables refer to different variables within the same sensor, while inter-sensor variables refer to variables from various sensors located at different positions on the human body. Failing to disentangle these relationships while extracting features will lead to sub-optimal performance.\nThe first key challenge in WHAR is to capture local and global temporal features of Intra-Sensor Modality Variables while handling the intricate relationships between these diverse variables. Previous studies mainly include two types of approaches. The first type of approach, exemplified by DeepConvLSTM (Ord\u00f3\u00f1ez and Roggen 2016) and Attend and Discriminate (Abedin et al. 2021), utilizes shared convolutional kernels to capture local temporal patterns"}, {"title": "Related Works", "content": "Effective temporal feature extraction is essential for WHAR, capturing dynamic patterns from sequential sensor data. Early WHAR methods employed traditional machine learning techniques such as decision trees, SVMs, and k-NN (Attal et al. 2015). These approaches, limited by feature engineering and temporal modeling, led to the adoption of deep learning methods (Li et al. 2021a).\nDeep learning has significantly enhanced HAR through Convolutional Neural Networks (CNNs). DCNN (Yang et al. 2015) demonstrated CNNs' ability to capture local dependencies, while MCNN (M\u00fcnzner et al. 2017) used multi-branch architectures for effective feature fusion. However, CNNs require increased depth to address long-term dependencies, leading to the development of RNNs and their hybrids with CNNs (Mutegeki and Han 2020; Zhou et al. 2021). DeepConvLSTM (Ord\u00f3\u00f1ez and Roggen 2016) merges CNNs with LSTMs but faces challenges with long-range dependencies due to the \"forgetting\u201d issue. The self-attention mechanism (Vaswani et al. 2017) has improved long-term dependency modeling. Initial Transformer models for HAR (Tonmoy et al. 2021) and hierarchical encoders (Mahmud et al. 2020) effectively capture spatial and temporal features. However, Transformers struggle to correlate isolated timestamps and suffer from quadratic computational complexity with sequence length.\nState Space Models (SSMs) have emerged as efficient alternatives. Mamba (Dao and Gu 2024) introduces a selective mechanism and hardware-aware algorithm, offering robust performance across modalities with linear complexity. HARMamba (Li et al. 2024) further refines this approach for WHAR with bidirectional SSMs. Despite Mamba's effectiveness in global temporal feature extraction, it shares RNNs' difficulty in managing inter-modality relationships,"}, {"title": "Modeling Intra- and Inter-Sensor Interaction", "content": "Intra-sensor modality fusion techniques have evolved to handle multi-modal data effectively. For instance, some methods (Miao et al. 2022; Ahmad and Leung 2024) utilize 1D CNNs to integrate multi-modal information at each timestep. Another approach, Attend and Discriminate (Abedin et al. 2021), employs attention mechanisms following modality-shared convolutional layers to capture cross-modality relationships. However, they overlook the independence of each modality's variables, leading to a potential loss of high-dimensional modal information during fusion.\nRegarding inter-sensor interactions, many existing approaches treat variables from all sensors uniformly, which can result in the loss of spatial feature information pertinent to different body parts. Drawing inspiration from STGCN (Yan, Xiong, and Lin 2018) in vision-based human activity recognition, methods like GraphConvLSTM (Han et al. 2019), DynamicWHAR and HyperHAR (Ahmad and Leung 2024) use GCNs to model spatial correlations between sensors. Despite their advantages, GCNs rely on predefined graph structures, such as human skeleton graphs, which may not fully capture the implicit relationships among sensors."}, {"title": "Preliminaries", "content": "Given $N$ wearable motion sensors, each with $M$ variables of different modalities (e.g., accelerometer, gyroscope, magnetometer data), the task is to recognize human activities from these multi-sensor data streams. The data can be represented as $X = {X_1,X_2,...,X_N} \\in \\mathbb{R}^{N \\times M \\times L}$, where $X_i \\in \\mathbb{R}^{M \\times L}$ is the data from the $i$-th sensor over $L$ time steps. For example, if each sensor records 3-axis accelerometer ($a_x, a_y, a_z$), gyroscope ($g_x, g_y, g_z$), and magnetometer ($m_x, m_y, m_z$) data, then $M = 9$.\nThe objective is to develop a model $F(\\theta)$ that predicts activities $\\hat{y} = F(X)$ from the input $X$. The model aims to minimize the difference between the predicted activity $\\hat{y} = F(X)$ and the true label $y$. The goal is to ensure that the predicted label $\\hat{y}$ closely matches the actual activity label $y$."}, {"title": "Depth-Wise and Point-Wise Convolution", "content": "In multi-variable time series data processing, the widely used Conv2D method uses a shared kernel across all channels to learn features spanning multiple variables. While effective for cross-variable correlations, it cannot capture variable-specific patterns. Inspired by the Depthwise Separable Convolution (Chollet 2017), Depth-Wise Conv1D applies separate 1D kernels to each channel independently. This method focuses on extracting variable-specific features and offers fewer parameters and faster computation compared to Conv2D, thus reducing computational complexity.\nFollowing Depth-Wise Conv1D, Point-Wise Conv1D combines information across variables using a kernel size of 1. It performs a linear transformation on the depth dimension, integrating features from depth-wise convolution into a unified representation. Point-Wise Conv1D further reduces parameter count and enhances computational efficiency, optimizing multi-variable data processing."}, {"title": "Our Model", "content": "We aim to independently extract temporal features from each modality variable within each sensor, free from interference by other variables. This phase ensures both sensor-level and variable-level independence. Initially, each sensor is isolated, and Modality-Specific Embedding (MSE) is performed to convert each sensor's variables into high-dimensional vectors. Local Temporal Extraction (LTE) is then applied to independently convolve each channel of these high-dimensional vectors, extracting temporal features from each channel. Thus, this phase involves sensor level, variable level, and channel level decomposition, preserving the unique features of each modality."}, {"title": "Modality-Specific Embedding (MSE)", "content": "We design MSE to transform raw multi-sensor time series data into high-dimensional representations, allowing independent capture of each modality variable's temporal dynamics before further processing.\nGiven input data from $N$ wearable motion sensors, each with $M$ variables and $L$ time steps, our goal is to embed each variable sequence into a high-dimensional space independently. The input data is represented as a tensor $X \\in \\mathbb{R}^{N \\times M \\times L}$, where $X_n \\in \\mathbb{R}^{M \\times L}$ denotes the data from the $n$-th sensor. The embedding process utilizes independent 1D convolution to transform each variable sequence:\n$X_{emb} = Conv1D(X_n, P, S, D),                                                                                            (1)$\nwhere $P$ is the kernel size, $S$ is the stride, and $D$ is the number of output channels. This operation is performed along the temporal dimension of the variable sequences, resulting in an embedded tensor $X_{emb} \\in \\mathbb{R}^{N \\times M \\times D \\times T}$, where $T'$ is the new length of the embedded sequences, aiding in reducing the temporal length for more efficient subsequent computations.\nThe embedding process preserves the unique characteristics of each variable by treating them independently, avoiding interference from other variables."}, {"title": "Local Temporal Extraction (LTE)", "content": "Local Temporal Extraction captures local temporal features while maintaining the independence of different modality variables. Each variable channel undergoes separate convolution operations, preserving their distinct characteristics.\nGiven the embedded tensor $X_{emb} \\in \\mathbb{R}^{N \\times M \\times D \\times T}$, where $N$ is the number of sensors, $M$ is the number of variables, $D$ is the number of channels, and $T$ is the temporal length, we first reshape the tensor to $\\mathbb{R}^{N \\times (M \\times D) \\times T}$. We leverage the depth-wise convolution as Local temporal extraction to capture local temporal dependencies of each variable channel. The depth-wise convolution is defined as:\n$X_{dw} = DWConv1D(X_{emb}, K_{dw}, G_{dw} = M \\times D),                                                      (2)$\nwhere $K_{dw}$ is the kernel size and $G_{dw}$ is the group number of the depth-wise convolution. This operation is performed separately for each variable channel, ensuring the preservation of unique characteristics. Mathematically, depth-wise convolution can be expressed as:\n$X_{dw}(n,m,d,t) = \\sum_{k=0}^{K_{dw}-1} W_{m,d,k}. X_{emb}(n,m,d,t + k),                                                                           (3)$\nwhere $W_{m,d,k}$ denotes the convolutional filter weights for the $m$-th variable and $d$-th channel."}, {"title": "Hierarchical Interaction Fusion", "content": "In the Hierarchical Interaction Fusion phase, we integrate decomposed features at the channel, variable, and sensor levels to capture both intra-sensor and inter-sensor spatiotemporal relationships. This approach begins with combining features within each sensor and then expands to cross-sensor interactions. The process is further refined by the Global Temporal Aggregation module, which consolidates features across the entire temporal dimension, enabling the model to capture long-range dependencies effectively."}, {"title": "Cross-Channel Fusion (CCF)", "content": "Cross-Channel Fusion merges features across different sensor channels, capturing inter-channel dependencies within each variable.\nStarting from the tensor output of depth-wise convolution, $\\mathbb{R}^{N \\times (M \\times D) \\times T}$, point-wise convolutions are performed as follows:\n$X_{ccf} = PWConv1D(X_{dw}, G_{ccf} = M),                                                                                               (4)$\nwhere $G_{ccf}$ is the group number of the point-wise convolution. This operation fuses information across channels and is followed by reshaping and performing two point-wise convolutions to merge and then restore the dimensionality."}, {"title": "Cross-Variable Fusion (CVF)", "content": "After Cross-Channel Fusion, the relationships within channels of individual variables are integrated. However, the interactions between different modality variables have not yet been addressed. To capture cross-variable dependencies within the same sensor, we adopt a similar approach to CCF.\nThe input tensor $X_{ccf} \\in \\mathbb{R}^{N \\times (M \\times D) \\times T}$ is reshaped to $X_{ccf} \\in \\mathbb{R}^{N \\times (D \\times M) \\times T}$, changing the group number to $D$. Point-wise convolutions are then applied:\n$X_{cvf} = PWConv1D(X_{ccf}, G_{cvf} = D),                                                                                                   (5)$\nwhere $G_{cvf}$ is the group number of the point-wise convolution. This operation fuses information across variables and captures cross-variable dependencies within each sensor."}, {"title": "Global Temporal Aggregation (GTA)", "content": "The Decomposition Phase extracts modality-specific local temporal features but does not fully capture the temporal context across the entire time series. To address this, we introduce the Global Temporal Aggregation (GTA) module, which consolidates information across all time steps to capture long-range dependencies and overall temporal dynamics.\nWe start by applying Global Average Pooling (GAP) to reduce each feature map's spatial dimensions to a scalar by averaging over the variable dimension M. For the input tensor $X_{ccf} \\in \\mathbb{R}^{N \\times (D \\times M) \\times T}$, GAP is applied as follows:\n$X_{gap}(n, d, t) = \\frac{1}{M} \\sum_{m=1}^{M} X_{ccf}(n, (d. M+m),t),                                                                               (6)$\nwhere $n$ is the sensor index, $d$ is the channel index, and $t$ is the time step. This results in $X_{gap} \\in \\mathbb{R}^{N \\times D \\times T}$, which is then reshaped to $X_{stack} \\in \\mathbb{R}^{(N \\times D) \\times T}$."}, {"title": "Cross-Sensor Interaction (CSI)", "content": "To capture inter-sensor spatial correlations, we draw inspiration from the self-attention mechanism, which effectively models relationships between tokens.\nGiven the output tensor $X_{mb} \\in \\mathbb{R}^{(N \\times D) \\times T}$ from the Mamba block, we reshape it to $X = {X_1, X_2, ...,X_n} \\in \\mathbb{R}^{N \\times (D \\times T)}$, where $X_i \\in \\mathbb{R}^{D \\times T}$ represents the data from the $i$-th sensor. Each $X_i$ serves as a token for the Attention Layer. The self-attention mechanism computes responses for each sensor by attending to representations of all sensors. We calculate the normalized correlations across all pairs of sensor data $X_i$ and $X_{i'}$ using an embedded Gaussian function. The attention score $A_{i,i'}$ measures the relevance of data from sensor $i'$ for refining representations of sensor $i$ and is computed as:\n$A_{i,i'} = \\frac{exp (Q(X_i)^\\top K(X_{i'}))}{\\sum_{i'=1}^{N} exp (Q(X_i)^\\top K(X_{i'}))},                                                                                       (8)$\nwhere $Q$ is the query function projecting the sensor into the query space, and $K$ is the key function projecting the sensor into the key space. These correlations are then used to generate self-attention feature maps $O_i$ for each sensor:\n$O_i = W \\cdot \\sum_{i'=1}^{N} (A_{i,i'} \\cdot V (X_{i'})),                                                                                                            (9)$\nwhere $W$ is a linear embedding with learnable weights, and $V$ is the value function projecting the sensor into the value space. The feature maps $O_i$ are combined with the original sensor data using a residual connection to produce refined feature representations $X_{csi}$, enabling adaptive integration or exclusion of correlation information. By employing the CSI module, our model captures interactions between different sensors and encodes these correlations through self-attention weights. During inference, these learned correlations are used to enhance predictions, providing a robust method for synthesizing information from multiple sensors."}, {"title": "FC Linear Classifier", "content": "After the CSI module through the self-attention mechanism, we obtain the output tensor $X_{csi} \\in \\mathbb{R}^{N \\times (D \\times T)}$. This tensor is then reshaped and passed through a Fully Connected (FC) layer for classification:\n$\\hat{y} = FC(Flatten(X_{csi})).                                                                                                               (10)$\nHere, $\\hat{y} \\in \\mathbb{R}^C$ represents the final output, with $C$ being the number of human activity classes. The predicted activity $\\hat{y}$ is compared to the true label $y$ using the cross-entropy loss function:\n$\\mathcal{L}(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\cdot log(\\hat{y}_i),                                                                                                (11)$\nwhere $y_i$ is the true label, and $\\hat{y}_i$ is the predicted probability for the $i$-th class."}, {"title": "Experiments", "content": "To validate the effectiveness and generalizability of our proposed model, we conduct experiments on three widely recognized benchmark datasets in the WHAR community. These datasets are known for their complexity and diversity."}, {"title": "Computational Efficiency", "content": "WHAR applications require accurate recognition while managing computational resources and energy is crucial due to the constraints of wearable embedded devices like smartwatches. Figure 3 compares model parameters, inference latency, and energy consumption to evaluate efficiency. DecomposeWHAR, HARMamba, and DynamicWHAR have FLOPs under 600 M, whereas others exceed 3000 M. Our model delivers outstanding recognition performance with computational efficiency comparable to the best models, achieved through the Depth Separable Convolution, which reduces parameters and speeds up computation. Additionally, the Mamba block's selection mechanism, hardware-aware algorithm, and efficient self-attention layer enhance overall efficiency."}, {"title": "Conclusion", "content": "In this paper, we present the DecomposeWHAR model, specifically designed to address the limitations of current WHAR methods by better capturing both intra- and inter-sensor spatio-temporal relationships through Modality-Aware Signal Decomposition and Hierarchical Interaction Fusion. In summary, our model shows that decomposing signals from sensor level to channel level and hierarchically fusing them boosts WHAR recognition, offering valuable insights for future research and practical applications. Our model has potential for further optimization, particularly in reducing parameter size and computational cost. Specifically, the Mamba block in our framework has yet to be refined. We aim to evaluate the adaptability of our approach to other multivariable classification problems and time series tasks, expanding its applicability across diverse domains."}]}