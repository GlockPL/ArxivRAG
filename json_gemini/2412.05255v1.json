{"title": "TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft", "authors": ["Qian Long", "Zhi Li", "Ran Gong", "Ying Nian Wu", "Demetri Terzopoulos", "Xiaofeng Gao"], "abstract": "Collaboration is a cornerstone of society. In the real world, human teammates make use of multi-sensory data to tackle challenging tasks in ever-changing environments. It is essen-tial for embodied agents collaborating in visually-rich envi-ronments replete with dynamic interactions to understand multi-modal observations and task specifications. To eval-uate the performance of generalizable multi-modal collab-orative agents, we present TeamCraft, a multi-modal multi-agent benchmark built on top of the open-world video game Minecraft. The benchmark features 55,000 task variants specified by multi-modal prompts, procedurally-generated expert demonstrations for imitation learning, and carefully designed protocols to evaluate model generalization capa-bilities. We also perform extensive analyses to better under-stand the limitations and strengths of existing approaches. Our results indicate that existing models continue to face significant challenges in generalizing to novel goals, scenes, and unseen numbers of agents. These findings underscore the need for further research in this area. The TeamCraft platform and dataset are publicly available at", "sections": [{"title": "1. Introduction", "content": "Developing collaborative skills is essential for embodied agents, as collaboration is a fundamental aspect of human intelligence [49]. In the AI community, multi-agent collabo-ration is frequently studied using grid-world environments [10, 17, 26, 31, 42, 44, 50, 52, 60, 64]. However, agents in these environments lack multi-modal understanding. By contrast, learning within visually-rich environments enables agents to develop useful representations of multi-agent dy-namics [7, 20], as vision facilitates implicit communication, coordination, and collaborative execution [21, 22].\nLearning vision-based, multi-task, multi-agent systems is a challenging objective that presents several difficulties. These systems must develop detailed scene understanding to handle the diverse visual appearances of scenes. The com-plexity is further heightened by the numerous combinations of task configurations, such as object spatial arrangements, goal configurations, arbitrary numbers of agents, and hetero-geneous agent capabilities. Consequently, it is essential for multi-agent systems to acquire generalizable skills that can be effectively transferred across different settings.\nAn important step in addressing these challenges is to develop simulation systems that support multi-modal multi-agent learning. Recent advances in simulated environments have significantly facilitated progress in embodied vision-based systems [7, 9, 21, 43, 62]. Despite notable progress, these systems have several limitations: (1) many of them target one or two-agent scenarios [22, 37, 57], (2) they are often limited to indoor settings with a narrow range of tasks [44, 66], and (3) the task specifications are generally purely in text [30, 37], making it hard to specify subtle task differ-ences accurately and efficiently.\nTo drive progress in this area, we have developed a com-prehensive benchmark, named TeamCraft, that features pro-cedurally generated large-scale datasets specifically designed for multi-modal multi-agent systems. This benchmark uti-lizes the widely acclaimed open-world video game Minecraft as an experimental platform to engage with the complex dy-namics of multi-modal multi-agent interactions. Inspired by the work of [24], we also leverage multi-modal prompts as task specifications to guide agent interactions, as language often fails to effectively convey spatial information [4]. Our benchmark offers rich visual backgrounds, diverse object categories, complex crafting sequences, and varying task dy-namics. These features enable systematic exploration of out-of-distribution generalization challenges for multi-modal, multi-task, multi-agent systems at scale. In particular, our benchmark evaluates a model's ability to generalize to novel goal configurations, unseen number of agents, novel agent ca-"}, {"title": "2. Related work", "content": "2.1. Platforms for Multi-Agent Systems\nThe recent success of multi-agent reinforcement learning (MARL) methods [32\u201334, 61] has attracted attention, as these methods explore cooperation and competence behav-iors among agents. However, many of the methods are eval-uated in simplified 2D environments [5, 26, 39, 52, 55]. Re-cent work on embodied multi-agent benchmarks has consid-ered more realistic tasks and environments [6, 15, 29, 30, 42], but it often relies on certain privileged sensor information of\nthe environment [44, 45, 65]. Additionally, subject to envi-ronmental constraints, these works often have limited set of tasks [22, 54] related to navigation and simple interactions such as object rearrangement [53]. By comparison, Team-Craft is based on Minecraft, a three-dimensional, visually rich open-world realm characterized by procedurally gener-ated landscapes and versatile game mechanics supporting an extensive spectrum of object interactions, providing rich activities ripe for intricate collaborations.\n2.2. Embodied Language-Guided Benchmarks\nSeveral researchers have looked at the problem of using natural language as the interface between embodied agents, either in the form of task specifications [16, 47, 48, 67], question answering [8, 18, 35, 36], instruction following [2, 12, 13, 23, 40, 41, 56], or as means of task coordination [27, 37]. VIMA-Bench [24] builds on previous efforts in language-guided robotic manipulation [38, 46, 63] and uses multi-modal prompts as uniform task specifications for ob-ject manipulation. TeamCraft extends multi-modal prompts to the multi-agent domain and uses them to specify a wide variety of collaborative tasks that require object interaction and navigation.\n2.3. Benchmarks Based on Minecraft\nMalmo [25] marks the advent of a Gym-style platform tai-lored to Minecraft games. It paves the way for subsequent single-agent works such as MineRL [19], Voyager [57], and MineDojo [11]. Marlo [43] extends Malmo to multi-agent scenarios, but the small number of task variations limit gen-eralizations. Similar to our work, MindAgent [17] and VillagerBench [10] focus on multi-agent collaboration in a multi-task setting. However, both of these use purely state-based observations, while TeamCraft tackles the more"}, {"title": "3. TeamCraft Benchmark", "content": "3.1. Problem Formulation\nAssume that an embodied multi-agent system comprised of N agents needs to complete a complex task involving navigation and object manipulation. The task is specified in a multi-modal prompt $x_L = \\{x_l\\}_{l=1}^L$, which is a sequence of interleaved language and image tokens with length L. At time step t, each agent receives partial observation $o_t \\in O$ from the full observation space O. To complete the task, each agent can choose to perform a high level action $a_t \\in A$ from the full set of action A. The action can be further decomposed into a sequence of low level control signals.\n3.2. Simulation Environment\nTeamCraft utilizes Minecraft as its foundational simulation environment, offering a complex, open-world setting for multi-agent interactions. With a Gym-like environment, it fa-cilitates the execution of intricate multi-agent commands via self-explanatory skills. Figure 1 illustrates the platform ar-chitecture. High level skills from the model can be translated into low level control signals via nested API calls through Mineflayer\u00b9. After execution, visual observation of each agent are rendered and provided as input to the model.\nMulti-Modal Prompts: In our work, the multi-modal prompt $X_L$ consisting of a language instruction interleaved with a set of orthographic projection images (i.e., top, left, and front views) for task specification. Depending on the specific task, the images can specify either the initial states, intermediate states or the goal states."}, {"title": "3.3. Task Design", "content": "TeamCraft introduces a variety of complex and interactive tasks that challenge the agents' capabilities in planning, coor-dination, and execution within a collaborative and dynamic environment. Each task is designed to test different facets of MA interaction, including role distribution, real-time decision-making, and adaptability to changing environments. Task examples are shown in Figure 2 and the corresponding prompt examples are shown in Figure 3.\nBuilding: Agents erect a structure based on a provided orthographic view blueprint. Each agent possesses a unique inventory of building blocks necessary for the construction. Successful completion requires visual cognition to associate blueprint components with inventory items, spatial reasoning to reconstruct a 3D structure from 2D images and map it to 3D coordinates for action targets, and collaborative coordi-nation with other agents to resolve action dependencies. For example, an agent cannot place a floating block and should wait for another agent to build the supporting block first.\nClearing: Agents are required to remove all blocks from a specified area. Besides spatial understanding and aware-ness of action dependencies, agents must employ appropriate tools to break blocks, which vary in durability, thereby requir-ing multiple interactions for complete removal. The use of correct tools can dramatically reduce the time required to re-move blocks. Thus agents must coordinate task assignments to optimize block-breaking efficiency. Strategic coordination is essential in this task as agents need to dynamically decide which blocks to target based on their current tools, and assist each other even without the optimal tools when necessary.\nFarming: Agents sow and harvest crops on designated farmland plots. They must monitor crop growth stages, from newly planted to fully grown, and harvest only when crops reach maturity. Efficient task completion requires spatial reasoning to select appropriate farmland, visual cognition to assess crop maturity, and continuous updating of farmland states based on other agents' actions. As the available farm-land exceeds what is needed, understanding other agents' actions to avoid redundancy, and dynamically allocating sub-tasks based on positions, available seeds, and crop maturity"}, {"title": "3.4. Centralized and Decentralized Agents", "content": "TeamCraft supports centralized and decentralized control.\nCentralized Agents: The centralized model is given the observational data of all agents, including the first-person view, action history, and inventory information. Based on these comprehensive data, the model generates the actions for all agents simultaneously. This approach leverages the full scope of information available in the environment to coordinate and optimize the actions of all agents collectively.\nDecentralized Agents: The decentralized models do not receive information about other agents except for the initial inventory of the team. Each model generates actions solely for the individual agent based on its limited view. This set-ting simulates a more realistic scenario where agents operate independently with restricted information, focusing on their actions absent of any centralized coordination."}, {"title": "3.5. Diversity", "content": "The tasks are complex and challenging, testing multi-agent systems in diverse settings. Object Diversity: More than 30 target object or resource are used in tasks. Objects, such as a fence, an anvil, or a stone block, have different shapes and textures. Farm crops"}, {"title": "3.6. Tasks and Expert Demonstrations Generation", "content": "To create a rich learning environment and effective imita-tion learning dataset, systematic scenario design and data collection methods are employed, as follows:\nTask Generation: Variables from a diversity pool, such as agent counts, scenes, and goals, are sampled to establish task configurations. Specifically, a solvable task is formulated by rejection sampling of the essential task variables. \"Solvable\" implies that the task can be completed within the Minecraft world rules and is within the agents' capabilities. For exam-ple, in smelting tasks, fuel must either be available to collect in the scene or directly accessible in the inventory.\nPlanner-Based Demonstrations Generation: Given the task specifications, a planner assigns actions to agents at every time step, utilizing privileged information of the envi-ronment. Assume agent i performing action j, the planner optimizes a cost function designed to minimize total task completion time T, idle actions $E_i$, action dependencies D, redundant actions U, and the cost $c_{ij}$ for agent i performing action j:\n$C = w_1 T + w_2 \\sum_{i=1}^N E_i + w_3 D + w_4 \\sum_{i=1}^N \\sum_{j \\in A_i} c_{ij} + w_5 U$ (1)\nwhere $w_1$, $w_2$, $w_3$, $w_4$, and $w_5$ are weighting coefficients. Details of the weights are available in Appendix D."}, {"title": "3.7. Test Set and Generalization Set", "content": "TeamCraft features a test set, where agents are initialized with random position, orientation, and inventory. Other vari-ables follow the same distribution as training. To evaluate the model generalization, we further designed a generaliza-tion set with hold-out elements excluded from training data. In general, we withheld test cases involving four agents, whereas the training data include only two or three agents. We also introduced unseen scenes not present during train-ing. In addition to these general hold-outs, we implemented task-specific exclusions as following: 1) Building: novel shapes and materials to build. We exclude 8 block place-ment shapes, defining how target blocks are arranged on the ground. These shapes varied in complexity, containing 5 to 12 blocks in both 2D and 3D configurations. Additionally, we omitted 3 block materials appeared in clearing but not in building. 2) Clearing: novel shapes and materials to clear. We held out 6 block placement shapes with block counts ranging from 4 to 9. We also excluded 3 block materials present in building but absent in clearing. 3) Farming: novel crops to farm and collect. 4) Smelting: novel number of fur-naces and goal objects. We excluded 4 unseen goal objects and introduced scenarios with novel number of furnaces in the scene. As shown in , with 50 samples per task for the test set and each generalization condition, our benchmark contains a total of 950 test cases."}, {"title": "4. Experiments", "content": "4.1. Baselines and Ablations\nTeamCraft-VLA: We introduce TeamCraft-VLA (Vision-Language-Action), a multi-modal vision-language action model designed for multi-agent collaborations. As shown in , the model first encodes multi-modal prompts specifying the task, then encodes the visual observations and inventory information from agents during each time step to generate actions. Following [28], the VLA model architecture consisting of a CLIP encoder for images, a projector to align the image features with language model. We use CLIP ViT-L/14 as the visual encoder and a linear projector for modality alignment. The model is trained on the demonstration data for three epochs before convergence.\nProprietary VLA: We use GPT-4o as the proprietary VLA. Specifically, we use similar prompt structures as the centralized finetuned TeamCraft-VLA model, with addi-tional task information in the initial system prompt to pro-vide background knowledge of the task. The system prompt contains recipes, input, output formats, all available blocks, items, workspace limitations, and one successful rollout of a similar task in the same task family. At the first step, we additionally provide the first user prompt, where the model is given a specific multi-modal task specification accompanied\nby initial visual observations and inventory details of the agents. Based on the system prompts and user prompts, the model predicts the actions. As the interaction progresses with subsequent prompts, the context is maintained and ex-panded with the addition of prior responses and updated visual data. Detailed prompts are available in Appendix H.\nGrid-World Settings: In order to the understand the im-pact of learning in multi-modal environment as opposed to purely text-based or state-based environment, we perform an ablation study by translating the TeamCraft environments into a 3D grid-world. We retain the same prompt structure of the training data used in the TeamCraft-VLA models, with the main difference being that environmental information (i.e. visual observations and three orthographic view images) is now represented in text, describing the voxel coordinate of each block, e.g. \"brick is at (2,3,0), stone is at (2,3,1)...\". We fine-tuned a LLM in centralized setting with variance in the dataset size (10%, 50%, and 100% of the total data) for three epochs before convergence.\nAblations: We performed a total of 15 ablation studies, varying in dataset sizes (10%, 50%, and 100% of the total data), control settings (centralized and decentralized), exper-iment settings (Multi-modal and Grid-World) and sizes of the VLA model (7B and 13B)."}, {"title": "4.2. Evaluation Metrics", "content": "We evaluated the performance of the methods based on three key metrics: task success rate, subgoal success rate and redundancy rate.\nSubgoal Success Rate: This metric evaluates the effec-tiveness of agents in completing tasks. Given M test cases, each test case m has $s_m$ subgoals, and agents complete $s_a$ subgoals. The subgoal success rate SGS is defined as\n$SGS = \\frac{1}{M} \\sum_{m=1}^M \\frac{s_a^m}{s_m}$ (2)\nSpecifically, subgoals are designed based on the task require-ments, i.e. the number of blocks to be built for building and the number of target objects to be created for smelting.\nTask Success Rate: This metric indicates the proportion of test cases that the model can successfully complete from start to finish. Specifically, the task success rate TS is defined as:\n$TS = \\frac{1}{M} \\sum_{m=1}^M \\mathbb{1} [s_m = s_a^m]$ (3)\nA higher success rate reflects the model's ability to consis-tently achieve the desired outcomes in various scenarios.\nRedundancy Rate: This metric assesses whether multi-ple agents are performing the same action at the same time, which would lead to conflicts. Assume $p_m$ is the total num-ber of actions for test case m and $q_m$ the number of conflicts between agents, the redundancy rate RR is defined as:\n$RR = \\frac{1}{M} \\sum_{m=1}^M \\frac{q_m}{p_m}$ (4)\nA lower redundancy rate indicates better task allocation among agents and a higher level of cooperative efficiency."}, {"title": "4.3. Evaluation Results", "content": "We evaluated the subgoal success rate and task success rate of the models. As illustrated in , our analysis and findings are discussed below:\nSuccess Rate: For both the 7B and 13B models, the sub-goal success rate and task success rate fall short of optimal performance. This is particularly evident in challenging tasks such as smelting, with both subgoal and task success rates below 40%. This highlights the inherent difficulty of the designed tasks and underscores the current limitations of VLA models in handling multi-step, sequentially dependent processes.\nAcross Model Size: In , we observe that as train-ing data increased, the performance of the 7B model ap-proaches that of the 13B model, especially when generaliz-ing to novel goals and number of agents. This suggests that scaling up model sizes blindly do not guarantee success.\nMulti-Modal Environment vs. Grid-World: The perfor-mance of the language model in the text-based Grid-World significantly surpasses VLA models in multi-modal settings. This suggests that state descriptions provided purely in text"}, {"title": "4.4. Qualitative Analysis", "content": "We performed a qualitative analysis across three generaliza-tion splits, examining how models handle novel goals, new scenes, and novel number of agent:\nGoals: When faced with novel goals, the models strug-gle to generalize beyond familiar items and often fail to adapt to specific, unseen objectives. For example, in the \"farming\" task, if instructed to farm beetroot a crop not encountered in training the model might generate a com-mand like \"farm_work(bot1, (9,3,3), 'sow', 'beef'),\" causing Bot1 to sow \"beef\", which appears in the training data for \"smelting\". This behavior reflects the model's reliance on similar, previously seen items in the training data and reveals its limited ability to infer new tasks based solely on partial similarity.\nObject State Recognition: VLA models show strong gen-eralization to new scenes, performing comparably to the Test set. However, errors often arise in recognizing object states. For example, in \"farming\" tasks, agents may harvest crops before they are fully grown due to challenges in identify-ing crop states, especially when encountering new scenes. This highlights limitations in precise object state recognition when operating within unseen environments.\nAgents: For generalization to four agents, models fre-quently ignoring the fourth agent and assigning tasks in-efficiently only to two or three agents. For example, for the building task, the model predicts the action sequences {\"placeItem(bot1, 'birch_log', (4,4,7))\", \"placeItem(bot2, 'sandstone', (4,4,6))\", \"placeItem(bot3, 'dirt', (3,4,6))\"} with the fourth agent overlooked, reducing productivity and some-times preventing timely task completion. This limitation be-comes especially evident in tasks requiring full coordination, such as \"Building.\" In these tasks, each of the four agents holds unique blocks in their inventory, and all agents must contribute their specific block to a shared platform to com-plete the structure. The model's inability to distribute tasks effectively across all agents often leads to incomplete struc-"}, {"title": "5. Conclusions", "content": "We have presented TeamCraft, a benchmark for multi-modal multi-agent collaborative task planning in Minecraft. The benchmark consists of challenging collaborative tasks and evaluation splits designed to systematically test multi-modal agents across novel goal configurations, unseen numbers of agents, and unseen scenes. We conducted extensive ex-periments and analyses to pinpoint the limitations of the current models and identified promising research directions for collaborative multi-modal agents.\n5.1. Limitations and Future Work\n1. Given the limited capacity of existing multi-agent VLA models, TeamCraft relies on MineFlayer as an oracle controller to execute skills predicted by the models. En-abling VLA models to directly control multiple agents via low-level control [58, 59] would be important future research.\n2. We have trained the models using procedurally generated multi-agent demonstration data. Learning from noisy but more diverse real-world demonstrations of human players can potentially further strengthen model generalization [3, 11].\n3. Currently decentralized TeamCraft agents rely solely on implicit communication [22]; i.e., through passively per-ceiving other agents and the environment, to gather infor-mation and to collaborate. Enabling agents to communi-cate explicitly via natural language [23, 37, 40] has great potential in avoiding redundant actions and increasing efficiency.\n4. Multi-player video games have been widely used as testbeds for human-AI collaboration [1, 5, 14]. Extending TeamCraft with human players is a promising research direction."}, {"title": "Appendices", "content": "A. High Level Skills\nThe action space of agents mainly involves high-level self-explanatory skills such as obtainBlock and farmWork. We provided 8 such skills. Most skills take three input param-eters, including 1) agent name such as botl, as the action executing entity, 2) item name such as dirt, which strongly associated with task goal or agent's inventory, 3) a vector indicating the position of the target on the test field.\nFor example, obtainBlock(bot1, new Vec3(1, 0,1)) takes the agent name bot1 and a 3D vector (1, 0,1) as its arguments. It directs bot1 to perform multiple actions in Minecraft via APIs provided by Mineflayer. First, it controls bot 1 to goto a diggable position for block (1, 0, 1), then has bot1's vision ray cast to the block at (1, 0, 1) using the lookAt action. Next, it commands bot 1 to equip a proper tool that can dig the block at (1,0, 1) most efficiently, and then instructs bot 1 to dig the target block. Once the target block has been mined, bot1 will goto the position where the block item dropped and collect it.\nSimilarly, farmWork (bot2, \"sow\", \"potato\", new Vec3(2,0,4)) takes the agent name bot 2, ac-tion type \"sow\" (as opposed to \"harvest\"), crop seed item \"potato\", and a 3D vector (2,0, 4) as its ar-guments. It directs bot2 to goto a placeable position for farmland at (2, 0, 4), then check if the seed is a valid item that is, a crop seed available within bot 2's inventory. It then checks if the farmland at (2,0, 4) is plantable. Finally, it instructs bot2 to lookAt the farmland and sow it with the seed \"potato\".\nB. Low Level Atomic Actions\nHigh level skills are processed through multiple stages be-fore reaching the final execution APIs. At each time step, TeamCraft accepts a list of skills as input, with a maximum length equal to the number of agents involved in the current task and a minimum length of zero. Each agent can perform at most one skill per time step. The updated list of skills is then passed into the JavaScript environment along with the predefined atomic actions. Each atomic action is processed simultaneously, meaning that agents' actions are executed concurrently rather than sequentially. This avoid the depen-dency issue that might occur in sequential execution. For example, if one agent's action is executed ahead of another's, the first agent may block the location where the next agent intends to place a block. The agent whose atomic action is ex-ecuted first will have a higher chance of success, potentially altering the dynamics of the multi-agent setting. Execut-ing actions concurrently ensures fairness among agents and"}, {"title": "C. Visual Diversity", "content": "TeamCraft uses a set of visual variate to provide a visual rich environment. Each task is constructed from a random number of agents, in a randomly selected scene, achieving different goal on playground built by different base block.\nC.1. Shared Elements\nEach task begins with a basic setting involving multiple agents on a playground. Each agent has a unique skin, as illustrated in , and is rendered as a two-block-high character. The playground combines a base platform spawned within a Minecraft biome. The base block is also randomly selected from a pool, shown in . Each biome offers variations in special surrounding blocks, de-signs, and environments.\nFor example, the seaside village biome is a vil-lage near the sea with houses made of oak wood and cobble-stone, decorated with flowers and cow sheds, as shown in . It also features a nearby farm surrounded by oak logs ( ). Another variation of village is the desert village biome, built from acacia planks, acacia logs, and sandstone, blending seamlessly with the desert's arid terrain, shown in . illustrates a biome that is located on half of the mountains, where a small flat land protruding from a cliff. Additional examples of biomes used are shown in , , and .\nC.2. Task Specific Diversity\nClearing task uses a random set of blocks as its targets, il-lustrated in . Building task also uses a random set of blocks as its target, with some blocks shared with clear-ing task, as illustrated in . Unlike other tasks, the Farming task does not use a regular base. The playground is constructed from a combination of farmland for plant-ing crops, water blocks, and randomly selected unfarmable blockers from the base that replace some of the farmland. An example is shown in . Each corps used in farm-ing task has its own grown stage with different appearances, shown in . Smelting task requires a wide varieties of resources to achieve its goal. Resources could be either entity, block, or item, shown in .\nDetailed statistics of each task are presented in and of Appendix J."}, {"title": "D. Planner for Expert Demonstration", "content": "TeamCraft employed a planner to assign actions to each agent at every time step, utilizing perfect knowledge of the task including goal object positions, agents' inventories, and each agent's efficiency in performing actions. The planner optimizes actions using a cost function designed to minimize"}, {"title": "Building", "content": "In the building task, where dependencies are moderate and parallelization is preferred, we place greater emphasis on minimizing idle actions by setting $w_2 = 1.4$ and assign a weight of 0.9 to the other components. This encourages agents to remain active and reduces idle time, enhancing overall efficiency."}, {"title": "Clearing", "content": "In the clearing task, using the correct tools can significantly speed up block removal (up to a threefold in-crease). Therefore, we assign a higher weight of $w_4 = 1.8$ to maximize action efficiency by assigning tasks to the most capable agents. The other weights are set to 0.8 to maintain overall performance while focusing on efficient tool usage."}, {"title": "Farming", "content": "Farming task is not heavily constrained by ac-tion dependencies, we assign equal weights of 1 to all com-ponents, ensuring a balanced consideration of time mini-mization, idle actions, action dependencies, action efficiency, and redundancy elimination."}, {"title": "Smelting", "content": "In the smelting task, which involves compara-tively long and highly dependent action sequences, we prior-itize minimizing action dependencies by setting $w_3 = 1.8$. The other weights are assigned a value of 0.8 to support this focus, facilitating smoother coordination among agents and reducing waiting times."}, {"title": "F. TeamCraft-VLA Implementation Details", "content": "We use Vicuna-v1.5 as the LLM backbone. For the visual en-coder, we employ CLIP ViT-L/14 to process all input images, including three orthogonal views and the first-person view of the agents. The image embeddings are then projected into the LLM space with a linear projection layer and concate-nated with the text embeddings. The combined embeddings are fed into the LLM, which outputs the final action. During training, we froze the visual encoder and projector and only finetune the LLM. All image embeddings are positioned before the text embeddings, separated by \"image start\" and \"image end\" tokens. In centralized settings, where the num-ber of images varies depending on the number of agents, we pad a dummy image at the end for training stability if the task involves only two agents. In decentralized settings, the number of image inputs remains unaffected, as the model processes only the first-person view of the current agent, excluding views from others.\nWe train each model for 3 epochs using the training split, leveraging 8 A100 GPUs with a global batch size of 16. In the centralized setting, training the 7B model takes 36 hours, while the 13B model requires 72 hours. In the decentralized setting, the training duration doubles, with the 7B model requiring 72 hours and the 13B model taking 144 hours. In the grid-world setting, training the 7B model takes 20 hours."}, {"title": "F.1. Arrangement of Three Orthogonal Views", "content": "For training and evaluation, we combine the three orthogonal view images into a single composite image by arranging them to the upper-left top-left corner, top-right corner, and the lower-left corner of the composite image. An example of this arrangement is shown in . This process is to reduce the number of images provided to the model to conform with the 4096 context length limit."}, {"title": "F.2. Hyperparameters", "content": "We present the hyperparameters for VLA training in Table 5."}, {"title": "F.3. Model Output Parsing", "content": "The output of the model is a string which will be parsed into the pre-defined high level skills. The string will be first processed by removing special sentence begin token, < s >, and ending token ."}, {"title": "H. GPT-4o Implementation", "content": "We use gpt-40-2024-08-06 as the proprietary VLA. Specifically, we use similar prompt structures as the cen-tralized finetuned TeamCraft-VLA model, with additional task information in the initial system prompt as shown in , and to provide background"}]}