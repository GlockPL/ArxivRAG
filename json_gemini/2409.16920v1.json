{"title": "Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models", "authors": ["Zhichen Han", "Jiahong Yuan", "Tianqi Geng", "Hui Feng", "Korin Richmond", "Yuanchao Li"], "abstract": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of Self-Supervised Learning (SSL) has led to the development of powerful pre-trained models, such as Wav2vec 2.0 (W2V2) [1] and WavLM [2], including their multilingual variants. These models have demonstrated remarkable success across a range of downstream speech tasks, including Speech Emotion Recognition (SER) [3]. To further enhance their adaptability across different languages and datasets for SER, Parameter-Efficient Fine-Tuning (PEFT) has been utilized to improve the efficacy of SSL models while minimizing fine-tuning requirements [4], [5].\nNevertheless, cross-lingual SER remains a significant challenge due to language and cultural differences [6]. Typically, both tradi-tional and SSL models require sufficient training data in the target lan-guage to achieve satisfactory cross-lingual SER performance, which is often infeasible for languages lacking emotional speech datasets [7], [8]. For humans, however, although cross-lingual barriers exist [9], emotions in speech are universally distinguishable as humans are less affected by cross-lingual differences [10].\nWhile some research has explored the use of SSL models for cross-lingual and multilingual SER [11], there has been little investigation into how these models compare to human performance. To this end, we raise four key questions:\n1) Can SSL-based models achieve competitive SER performance to that of humans?\n2) How to better fine-tune SSL models for SER in cross-lingual scenarios?\n3) Does dialect have an impact on human perception in cross-lingual SER?\n4) Can SSL-based models identify emotionally salient segments similar to human behaviors?"}, {"title": "II. RELATED WORK", "content": "On the model side, previous studies have typically fine-tuned SER models using target language data, but have observed a significant drop in performance when shifting from monolingual to cross-lingual conditions [7], [12]. Additionally, adversarial neural networks in unsupervised settings have been explored for cross-lingual adapta-tion [13], [14]. More recently, [15] introduced a layer-anchoring mechanism to facilitate emotion transfer, accounting for the task-specific nature and hierarchical structure of speech models. On the human side, [10] found that SVM models outperformed humans in monolingual settings, whereas humans were less affected by cross-lingual challenges. Further research by [16] concluded that human cross-lingual capabilities in SER are generally robust.\nDespite this progress, comparative studies between humans and models remain lacking, leading to an insufficient understanding of human-model comparison. To our knowledge, we are the first to con-duct a comparative study between humans and SSL models, exploring not only utterance-level SER but also fine-grained emotion perception (i.e., SED), the impact of dialect, and fine-tuning strategies."}, {"title": "III. MATERIALS AND METHODOLOGY", "content": "As various tasks are investigated in this work, we use multiple datasets and models. For the datasets, four public emotion corpora and a non-public dialect corpus are used:\n\u2022 ESD: a Mandarin Chinese (CN) emotion corpus [17], containing utterances spoken by ten native CN speakers (five male, five female) across five emotion categories.\n\u2022 PAVOQUE: a German (DE) emotion corpus [18], featuring a professional male actor with five emotion categories, where neutral comprises over 50% of the dataset.\n\u2022 ZED: an English emotion corpus specifically designed for the SED task [19], with speech data annotated by humans at both utterance and sub-utterance (segment) levels.\n\u2022 TJD: a non-public Tianjin (TJ) Chinese dialect corpus collected in our previous work [20]. It was recorded and annotated at Tianjin University by two native Tianjin dialect speakers who were university students. It includes three functional categories (question, negation, expectation), approximated to emotions due to high acoustic similarity. According to annotators, negation resembles anger, and expectation resembles happiness. Tianjin dialect is known for its complex tone sandhi patterns while fea-turing a similar but slightly different tone system than Mandarin [21]. Native speakers of the Tianjin dialect convey emotions more directly with noticeable sonorous vowels and faster speech [22].\nFor the models, we use three W2V2 base models pre-trained on Mandarin CN, DE, and EN, along with a WavLM large model trained on EN emotional speech.\nThe following tasks are conducted using different models and datasets for specific purposes."}, {"title": "B. Layer-wise Analysis of SSL Models", "content": "In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP; the models: W2V2-CN, -DE, EN; and the emotions: angry, happy, neutral, and sad.\nSSL models encode speech information across different layers; specifically, in SER tasks, speech representations from the middle layers often yield higher performance [23]. Therefore, we perform a layer-wise analysis to identify the optimal layer for monolingual and cross-lingual SER. SSL models are used as feature extractors with all parameters frozen, and Unweighted Accuracy (UA) is used as the evaluation metric. The analysis is conducted in the following settings:\n\u2022 Monolingual (Mono): The model is fine-tuned with both training and test data from speech in the same language as its pre-training language. For example, W2V2-CN is fine-tuned using CN data (ESD) as both training and test data.\n\u2022 Cross-lingual (Cross): The model is fine-tuned using its pre-training language as training data and a different language as test data. For example, W2V2-CN is fine-tuned using CN data (ESD) and tested on DE data (PAVOQUE) or EN data (IEMOCAP).\n\u2022 Transfer learning (Trans): The model is fine-tuned and tested on a language different from its pre-training language. For example, W2V2-CN is fine-tuned and tested on either DE data (PAVOQUE) or EN data (IEMOCAP)."}, {"title": "C. PEFT of SSL Models for Cross-lingual SER", "content": "In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP; the models: W2V2-CN, -DE; and the emotions: angry, happy, neutral, and sad.\nAfter the layer-wise analysis, the best-performing layers are further fine-tuned using various PEFT strategies to enhance performance. We apply the Low-Rank Adapter (LoRA) [24], Bottleneck Adapter (BA) [25], and Weighted Gating (WG). Additionally, a two-stage fine-tuning [5] is performed: the model is first fine-tuned on the source language, then on the target language once the first fine-tuning converges."}, {"title": "D. Comparison of SSL Models with Human Evaluation", "content": "In this task, we use all the datasets and models. For SER, we use the emotions: angry, happy, neutral, and sad; while for SED, we exclude neutral as it does not contain emotion variation to perceive and segment.\nSix native DE speakers (one male, five female) and six native CN speakers (two male, four female), with no prior knowledge of each other's language, are recruited for the human evaluation from the Univ. of Edinburgh and Tianjin Univ. All participants have studied English for many years with sufficient skills (e.g., IELTS score \u2265 6.5). The webMUSHRA interface [26] is used to create the experimental tests.\nFor SER, participants listen to speech samples and identify the conveyed emotion. We use UA as the evaluation metric, consistent with the model performance evaluation. Additionally, to investigate fine-grained speech emotion expression, we perform SED, where participants first listen to speech samples and label the emotion, as in the SER task. Subsequently, they clip the speech and select the segment that most prominently expresses the emotion. Following [19], we use the Emotion Diarization Error Rate (EDER) as the metric, which calculates the error rate of diarization results, including missed emotions (ME), false alarms (FA), overlaps (OL), and confusion (CF):\n$EDER = \\frac{ME+FA+ OL + CF}{Uttrance\\ Duration}$"}, {"title": "IV. EXPERIMENTS", "content": "For comparison with the SSL models, we compare participants' performance on their native language with the monolingual setting, their performance on the non-native languages with the cross-lingual or transfer learning settings. Finally, we explore whether dialect has an impact on human perception of cross-lingual SER."}, {"title": "A. Experimental Settings", "content": "For SER, to reduce the effect of varying training data sizes, we use the same amount of data for CN, DE, and EN. To ensure a balanced emotion distribution, we use an equal number of samples for each emotion. Specifically, for ESD, PAVOQUE, and IEMOCAP, we apply 5-fold cross-validation for model training: 400 utterances per emotion category, totaling 1,600 utterances per dataset, are used for training. Similarly, 200 utterances are randomly selected for validation and test sets, respectively. Given the difficulty of performing human evaluation on all the data, for comparison with human evaluation, we select 12 sentences per emotion category, totaling 144 utterances for each language. The model settings are as follows:\n1) Layer-wise analysis: We use a classification head projecting from dimension 768 to 4 for SER, with a learning rate of 1e-4, epsilon of 1e-8, and weight decay of le-5, trained for 100 epochs with a batch size of 32. Cross-entropy is used as the loss criterion. Training stops if the validation loss does not decrease for 10 consecutive epochs.\n2) PEFT strategies: We use the same classification head configu-ration as in the layer-wise analysis for PEFT. For the LORA module, the attention head is set to 8, alpha for scaling is 16, with a dropout rate of 0.1. For the BA module, the reduction factor is 16. Models are trained for 100 epochs with a batch size of 16. The loss and stopping criteria from the layer-wise analysis remain the same.\nFor SED, given the considerable effort required for segmenting speech, only 8 utterances per emotion are randomly selected from ZED, totaling 24 utterances, for comparison with human evaluation and model results."}, {"title": "B. Results and Discussions", "content": "The results of the layer-wise analysis are presented in Figure 1. In the monolingual setting, both the CN and DE models demonstrate strong performance on their respective source languages, as expected, given that the models are pre-trained on these languages. However, in the cross-lingual setting, both models show a significant drop in accuracy. While this is reasonable due to language differences, the extent of the drop is beyond our expectations, considering the shared characteristics of emotional acoustics [27], [28]. One possible explanation is that SSL models not only encode low-level acoustic features but also transform them into high-level, linguistically related information, such as word identity and meaning [29]. This process creates a linguistic gap across languages, exacerbating the accuracy decline. Nonetheless, under the transfer learning setting, the models can achieve performance levels comparable to the monolingual set-ting, demonstrating the ability of SSL models to adapt to different languages for SER with appropriate techniques of knowledge transfer. The variations in the contours are related to the training objectives of SSL models, particularly the contrastive masked segment prediction (since these patterns align with previous research on layer-wise analysis of SSL models [23], [29], [30], we omit further detailed explanation).\nThe results of PEFT under monolingual, cross-lingual, and transfer learning settings, are shown in Table I. Human performance on SER is shown in Table II, and SED comparison is presented in Table IV. From these results, we make the following observations:\n1) SER: monolingual model vs. native speakers\nIn terms of overall accuracy, as shown in Table I and Table II, both models outperform their respective human native speakers. For predictions across all emotion categories, Table III presents the confusion matrices of the CN and DE monolingual models alongside those of CN and DE natives for their respective languages. Compared to the DE monolingual model, DE natives are more likely to report false alarms for sad in neutral DE speech. CN natives, compared to the CN monolingual model, demonstrate lower precision in happy and neutral. These results indicate that SSL models exhibit excellent monolingual performance on the SER task when provided with sufficient training data.\n2) SER: cross-lingual models vs. humans\nIn terms of overall accuracy, as shown in Table I and Table II, both humans and models experience a performance decrease in the cross-lingual condition, with cross-lingual models being more significantly affected than humans. This aligns with findings from [10], which demonstrated that humans are capable of handling cross-lingual scenarios better. In terms of performance on every emotion category, as shown in Table III, DE cross-lingual model struggles to recognize neutral and sad in CN data, exhibiting low recall. Additionally, the DE model confuses angry and happy more frequently compared to humans in both languages. Conversely, the CN cross-lingual model closely aligns with CN natives when recognizing DE speech, with both often predicting happy as neutral.\nMoreover, we conduct a two-sided Welch's t-test on humans' precision, recall, and F1-scores. We notice significant difference in the recall of happy on DE data between CN and DE speakers (t(10) = -7.511, p < 0.001), as well as in the precision of neutral (t(10) = -5.614, p < 0.001). CN speakers also exhibit lower recall for happy in DE data than in CN data (t(10) = \u22125.137, p < 0.001), suggesting a linguistic and paralinguistic knowledge gap between two speaker groups. Particularly, significant differences are found in the recall of sad across CN, DE, and EN data (t(10) = -2.708, p = 0.022) and in the precision of neutral (t(10) = \u22127.511, p < 0.001). The precision of neutral is largely impacted by CN speakers' dif-ficulty in perceiving happy in DE data, indicating that linguistic and paralinguistic differences affect the perception of sad across languages.\n3) SER: transfer learning models vs. L2 learners\nAs the transfer learning setting resembles the human learning process of a second language (i.e., fine-tuning \u2248 language study), we compare the models with human speakers using EN data. As shown in Table I, SSL models with transfer learning achieve monolingual-level performance and surpass human accuracy on CN and DE data. However, for EN data, DE speakers exhibit higher accuracy than CN speakers and all models tested on EN data. Additionally, two-stage fine-tuning does not result in a significant performance boost, which was observed in the cross-corpus scenario under the same language [5]. These findings suggest that while transfer learning helps SSL models in adapting to new languages, performance varies depending on the specific target language dataset. In terms of performance on every emotion category, shown in Table III, CN speakers only outperform the model in recognizing happy, whereas the CN transfer learning model outperforms humans in the other three emotion categories. For DE speakers, humans perform better at predicting happy and neutral compared to the DE transfer learning model. In addition, an effective PEFT strategy used in monolingual scenarios is not necessarily useful in cross-lingual or multilingual scenarios. Moreover, Table II reveals that recognizing emotion in EN is more challenging than in CN and DE, despite CN and DE speakers being L2 learners. This difficulty is likely attributed to the selection of only improvised utterances from IEMOCAP, which are more natural and real-life emotions, thus making SER more challenging.\n4) SER: linguistic and paralinguistic impact of dialect\nIn addition to the finding in observation 2 that linguistic and par-alinguistic differences impact emotion perception across languages, the results on the TJ data in Table II further indicate the existence of such differences, particularly due to dialect. The SER results demonstrate the generalizability of human emotion perception across languages. However, in the TJD dataset, performance varies signif-icantly between the two speaker groups. While DE speakers excel with CN speech data, the unique prosody of the TJ dialect leads to a notable performance decline among DE speakers. This discrepancy is plausible given that TJ prosody and tones differ significantly from CN (and likely many other major languages), making emotion recognition challenging for DE speakers. Even with some background knowledge, CN speakers also struggle to recognize emotions in TJ data as effectively as in CN data, confirming the linguistic and paralinguistic impact of dialect.\n5) SED: models vs. humans in prominent emotion perception\nThe results in Table IV indicate that both human groups outperform the model, with the DE speakers achieving the lowest EDER. The model performs best on happy and worst on sad. Between the human groups, CN speakers are slightly better at perceiving angry segments, while DE speakers are better at identifying sad segments. This pattern is consistent with SER results in Table III, where CN speakers show a higher threshold for predicting sad, leading to higher recall but lower precision. Conversely, DE speakers demonstrate higher precision but lower recall. The difference in sensitivity to sad among CN speakers results in more false negatives for sad in the SED task."}, {"title": "V. CONCLUSION", "content": "In this study, we conduct a comparative analysis of cross-lingual SER between humans and SSL models, including both modeling and human experiments, and compare their performance in monolingual, cross-lingual, and transfer learning settings. We perform a layer-wise analysis and apply PEFT to the best-performing layers using multiple strategies to enhance model performance. Additionally, we implement SED for fine-grained detection of salient emotion segments to evaluate the ability of SSL models to capture segment-level emotion. The results show that humans excel in cross-lingual SER and SED, while models can adapt to the target language through transfer learning to achieve native speaker-level performance. We also reveal the linguistic and paralinguistic impact of dialect in the cross-lingual setting through human evaluations. Our study provides novel insights into human emotion perception and the application of SSL models for cross-lingual SER."}]}