{"title": "Machine Unlearning via Information Theoretic Regularization", "authors": ["Shizhou Xu", "Thomas Strohmer"], "abstract": "How can we effectively remove or \"unlearn\" undesirable information, such as specific features or individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a mathematical framework based on information-theoretic regularization to address both feature and data point unlearning. For feature unlearning, we derive a unified solution that simultaneously optimizes diverse learning objectives, including entropy, conditional entropy, KL-divergence, and the energy of conditional probability. For data point unlearning, we first propose a novel definition that serves as a practical condition for unlearning via retraining, is easy to verify, and aligns with the principles of differential privacy from an inference perspective. Then, we provide provable guarantees for our framework on data point unlearning. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications.", "sections": [{"title": "1. Introduction", "content": "As machine learning models become more common in sensitive domains, removing specific features or data points from trained models has become increasingly important. Sensitive information such as gender, ethnicity, or private data can perpetuate biases and lead to unfair results. Simply deleting these attributes in raw data is often insufficient, since their influence may persist through correlated or latent variables. Retraining models without these attributes can be impractical due to high computational costs. Machine unlearning provides a systematic way to \"forget\" specific features or data points, ensuring legal compliance and ethical alignment. It is critical to develop provable methods so that models can adapt to evolving demands for privacy and fairness.\n\u2022 Feature Unlearning: In 2014, Amazon built a machine learning based recruitment tool [11, 7]. The system favored male candidates because the training data came predominantly from men, penalizing words commonly associated with women. Removing explicit gender references failed, as gender correlated with other features (e.g., all-female colleges). Unable to resolve these biases, Amazon discontinued the tool. This example shows that merely dropping a sensitive feature is not enough if correlated information remains.\n\u2022 Data Point Unlearning: The General Data Protection Regulation (GDPR) [14] introduced the \"right to be forgotten,\" letting individuals request deletion of their personal data. Although removing raw data is straightforward, it does not eliminate that data's effect on a trained model's parameters. The challenge is how to unlearn specific data points while preserving overall model performance, highlighting the need for robust frameworks offering provable compliance and minimal utility loss.\nIn this work, we introduce a framework for machine unlearning within a probabilistic and information-theoretic setting, addressing the following question:\nGiven a dataset denoted by X, an undesirable attribute Z, and a target variable Y, how can we optimally modify the information in (X, Z) to produce an unlearning outcome S which retains minimal information about Z while preserving as much information in X about Y as possible?\nFor machine unlearning, we consider S = X if the unlearning happens on the data space X and S = \u0176 if the unlearning happens on the learning outcome space Y (via unlearning on the model parameter space). Note that the feature or label Z may also represent identity information, which blurs the line between feature unlearning and data point unlearning, making the framework applicable to both. Further discussion on applying the framework to data point unlearning can be found in Section 1.2 below. In the remainder of this work, we focus on unlearning in settings where relational data (X, Z) and target data Y are available.\nTo address this problem, we connect modern machine unlearning to classic rate-distortion theory and data compression [34]. Here, we use data compression in the sense of minimizing I(S; Z), thereby reducing shared information and effectively removing unwanted details of Z. We introduce two core concepts inspired by this information-theoretic framework:"}, {"title": "1.1 Application to Feature Unlearning", "content": "For feature unlearning, we define Z as the feature(s) to be unlearned, X as the remaining available data features, and Y as the target variable to estimate using the information in (X, Z). Since feature unlearning often involves removing a feature from a dataset intended for multiple downstream tasks, it is more practical to modify (X, Z) directly in the data space, producing an unlearning outcome S = X. The objective is to generate X that maximizes utility (considering multiple Y), while minimizing or compressing the information related to Z to achieve effective unlearning.\nAlternatively, if feature unlearning is performed for a specific Y or a specific model, one may set S = \u0176. See Algorithm 2 in Section 4."}, {"title": "1.2 Application to Data Point Unlearning", "content": "For data point unlearning, the application is less straightforward and requires a more de-tailed explanation. Here, we discuss the key distinctions between feature unlearning and data point unlearning and clarify how the proposed unlearning method can be applied to data point unlearning by defining a probabilistic data point unlearning guarantee.\nFirst, since data point unlearning operates on the learning outcome space (through unlearning on the parameter space), we define S = \u0176 for data point unlearning.\nIt is crucial to emphasize that data point unlearning typically aims to \"forget\" the marginal effect of adding/removing a data point on the rest of the dataset, rather than removing all the information associated with that data point. For example, the baseline unlearning via retraining approach aims to remove the marginal effect (on training) by directly removing the data to unlearn from training. Here, we apply information theoretic regularization and the existing model to directly estimate the marginal effect on learning outcome and thereby penalize it during training.\nA straightforward application of an information theoretic framework, such as via the information bottleneck, might minimize I(X; \u0176), the mutual information between X and \u0176 (see Appedix A.1 for more details), while maximizing I(X \\ {xu}; \u0176) [21], where xu represents the data belonging to the individual requiring unlearning. However, enforcing perfect unlearning by setting I(X; Y) = 0 would leave almost no utility in \u0176. This approach unnecessarily targets removing all data information rather than focusing solely on unlearning the marginal effect of xu, leading to an excessive loss of information and failing to balance unlearning with utility."}, {"title": "Definition 1.1 (\u025b-Differential Unlearning)", "content": "Given a dataset X and an individual record {xi} that requires unlearning, an unlearning output fe(X,Y,Z) satisfies \u025b-differential unlearning if\n$$ \\sup_{D\\in B_Y} \\log \\left(\\frac{P(\\left\\{X_{\\text{train}} = X_o\\right\\} | \\hat{Y}_{\\text{train}} \\in D)}{P(\\left\\{X_{\\text{train}} = X_1\\right\\} | \\hat{Y}_{\\text{train}} \\in D)}\\right) < \\varepsilon. $$\nThat is, given any observation of the unlearning output \u0176train = fo(Xtrain) and the knowledge that the training process uses either the original dataset X or the dataset excluding Xu, it is impossible (up to \u025b inference capability) to determine whether the individual record Xu was included in the training. We briefly summarize the advantages of the proposed definition:\n\u2022 Necessary Condition for Exact Unlearning: Under mild assumptions, one can show that any unlearned model that is assumed to estimate an exact unlearning outcome (via retraining) can fail the \u025b-differential unlearning definition only if it is non-smooth (in particular, violates a Lipschitz condition) and hence suffers from low generalizability for unseen data (See Lemma 2.2 and its discussion below). This establishes \u025b-differential unlearning as a foundational requirement for exact unlearning.\n\u2022 Ease of Verification: The definition relies solely on the unlearned model and the original dataset X, without requiring access to the exact retrained outcome. As a result, e-differential unlearning can serve as an efficient preliminary test to assess unlearning outcomes.\nThis unlearning concept is inspired by differential privacy [12]. It is instructive to compare \u025b-differential unlearning to differential privacy to highlight their differences:\n\u2022 \u025b-Differential Unlearning (Remediation via Inference Guarantee): Requires that the unlearning outcome (via data compression) provides no inference capability about whether the training process included the individual record requiring unlearning. Since unlearning is a remedial process, it is more reasonable to consider it from an inference perspective: $$ \\sup_{D \\in B_Y} \\left| \\log \\frac{P( \\left\\{X_{\\text{train}}=X_o\\right\\} | \\hat{Y}_{\\text{train}} \\in D)}{P( \\left\\{X_{\\text{train}} = X_1\\right\\} | \\hat{Y}_{\\text{train}} \\in D)} \\right| < \\varepsilon. $$\nHere, (V, By) is"}, {"title": "1.3 Related Work", "content": "The existing body of work on unlearning can be separated into feature unlearning and machine unlearning:\n1.3.1 MACHINE UNLEARNING\nMachine Unlearning focuses on removing data points' influence during training on the model and thereby the learning outcome, mainly to comply with regulations such as GDPR's \"right to be forgotten.\" The current study can be separated into exact unlearning and approximate unlearning:\nExact unlearning [3, 4, 44, 19, 32, 15, 6, 5] approaches ensure the removal of data influence from the model by separating the model into sub-models (via subsets of training data in SISA training, parameters, or training steps in amnesiac unlearning) which allow fast retraining by only retraining the affected sub-models or training steps. While effective, these methods are computationally expensive, particularly in scenarios with multiple data removal requests.\nApproximate unlearning methods try to keep the probabilistic similarity quantification between the unlearning outcome and the retraining outcome (also known as data point influence quantification). The main current approaches include: (1) using the influence function [40] to estimate the influence of the data point and thus remove its effect on the model [20, 33, 35, 29, 41, 36, 39]; (2) using the scrubbing function to enforce similarity between the network weights and the retraining weights [16, 17]."}, {"title": "1.3.2 FEATURE UNLEARNING AND STATISTICAL PARITY IN MACHINE LEARNING FAIRNESS", "content": "Feature Unlearning and Statistical Parity in Machine Learning Fairness aim to remove the influence of a feature in the learning outcome [39, 21]. It is closely related to machine learning fairness when fairness is defined as statistical parity [13]. In particular, our framework when applied to feature unlearning is closely related to the Wasserstein barycenter characterization of both the optimal statistical parity learning outcome [8, 18] and the optimally fair data representation [43]."}, {"title": "1.4 Our Contributions", "content": "Our contributions in this paper are as follows:\n\u2022 We propose \u025b-Differential Unlearning, a novel probabilistic guarantee for data point(s) unlearning that reflects the remedial nature of machine unlearning, inspired by the protection guarantees of differential privacy.\n\u2022 We propose a unified machine unlearning framework, inspired by rate-distortion theory in data compression, for both feature and data point unlearning. The framework is compatible with diverse target information and utility quantifications, and hence with a range of downstream tasks for the unlearned output."}, {"title": "1.5 Some Tools and Notation", "content": "Before we proceed, we introduce some tools used later and fix some notation. Let P(X) be the set of probability measures on a metric space (X, d). The p-Wasserstein distance between \u03bc,\u03bd\u2208 P(X) is defined as\n$$ W_{d,p}(\\mu, \\nu) = \\left( \\inf_{\\lambda \\in \\Pi(\\mu, \\nu)} \\int_{\\chi \\times \\chi} d(x_1, x_2)^p d\\lambda(x_1, x_2) \\right)^{\\frac{1}{p}}, $$\nwhere \u03a0(\u03bc,\u03bd) denotes the set of all couplings of \u00b5 and v. Let P2,ac(X) be the subset of measures with finite second moments that are absolutely continuous w.r.t. the Lebesgue measure. For random variables X1, X2, we write Wd,p(X1, X2) := Wd,p(L(X1), L(X2)), where L(X) denotes the law of X. For simplicity, we let Wa := Wa,1. Also, W2 := W12,2 when d(x,x') := ||x - x'||12. Given a family of measures {\u00b5z}zez C P2,ac(X) with weights \u5165, their Wasserstein barycenter \u016b is the minimizer of Sz W2 (\u03bcz, \u03bc) dd(z). We define X as the random variable distributed according to \u016b and satisfy Xz = Tz(Xz), where Tz is the optimal transport map from L(X\u2082) to \u03bc. More details can be found in Appendix B.1."}, {"title": "2. A Unified Unlearning Framework", "content": "We first introduce our machine unlearning framework, followed by a concrete example illustrating the data compression motivation behind this approach and a brief review of key technical tools underlying the definitions of utility, unlearning, and admissibility. Next, we extend the motivational example from feature unlearning (Section 2.1) to the data point unlearning (Section 2.2) setting. Finally, we generalize the framework to accommodate different choices of target information and utility quantification for both feature and data point unlearning."}, {"title": "Definition 2.1 (Optimal Feature Unlearning)", "content": "Given relational data (X, Z) and a target variable Y, optimal feature unlearning is defined as the solution, if it exists, to:\n$$ \\sup_{f:\\chi \\times Z \\to \\chi} \\left\\{U(Y; \\hat{\\chi}) : \\hat{\\chi} \\perp Z\\right\\}, $$\nwhere X := f(X,Z) is the unlearning outcome, and U : P(Y) \u00d7 P(X) \u2192 R quantifies the utility retained in X relevant to Y. The optimization is over all measurable functions f that remove the information of Z while preserving the information of Y from X.\nWhile setting S = X instead of \u0176 may seem unconventional, this choice ensures compatibility with multiple target variables Y, making it practical for feature unlearning. Moreover, Theorem 3.1 shows that under a wide range of utility functions, the optimal solution is independent of Y."}, {"title": "Definition 2.2 (Optimal Differential Unlearning)", "content": "Given data X, a data point {xu} to unlearn, and a target variable Y, the optimal differential unlearning is defined as the optimal solution, if it exists, to the following problem:\n$$ \\sup_{f:\\chi \\to \\mathcal{Y}} \\left\\{U(Y; \\hat{Y}) : \\hat{Y}_{\\text{train}} \\perp Z\\right\\}, $$\nwhere \u0176train := f(Xtrain) is the unlearning outcome. The relational data (Xtrain, Z) is defined by Xtrain|Z=0 = X0 and Xtrain|Z=1 = X1. The optimization is over all measurable functions f : X \u2192 Y of which the goal is to leverage the relational data (Xtrain, Z) to remove the information of the training data set indicator Z but retain information of Y based on X.\nSimilarly to above, we can also relax equation (4) to the following version:\n$$ \\sup_{f:\\chi \\to \\mathcal{Y}} U(Y;\\hat{Y}) - \\gamma I(\\hat{Y}_{\\text{train}}; Z). $$\nRemark 2.1 (Alternative Regularization to Mutual Information) The regulariza-tion term using mutual information in our framework serves to relax the strict independence constraint through a soft penalization. While mutual information is a natural choice due to its strong theoretical foundations in information theory and its connection to data com-pression that motivates the proposed framework (See Section 2.1 below), it is by no means the only option. More generally, any quantification of statistical dependence between two random variables can serve as an alternative regularizer.\nFor instance, in the binary Z case, one could use the Kullback-Leibler (KL) divergence between Yo and \u01761, i.e., DKL(Yo||\u01761). Alternatively, if the objective is to account for both distributional differences and geometric distance in the learning outcome space, the Wasserstein distance, Wd,p(Yo, Y1), provides a viable alternative. A systematic investigation into the optimal choice of regularizer for specific utility objectives is an interesting direction for future research."}, {"title": "2.1 Motivation: Feature Unlearning & Data Compression", "content": "To illustrate our framework's link to data compression, consider the special case where the target variable is the original data (Y = X) and utility is quantified by mutual information (U(X; X) := I(X; X)). We then solve:\n$$ \\sup_{\\hat{\\chi}=f(\\chi, Z)} \\left\\{I(\\hat{\\chi}; \\chi) : \\hat{\\chi} \\perp Z\\right\\}. $$"}, {"title": "Lemma 2.1 (Mutual Information Bound on Unlearning Inference Log Ratio)", "content": "Let the proposed unlearning framework (equation (5)) compress (\u0176train, Z). Then, for any given \u025b > 0, we have\n$$ P \\left[ \\log \\left( \\frac{P(\\left\\{X_{\\text{train}} = X_o\\right\\} | \\hat{Y}_{\\text{train}})}{P(\\left\\{X_{\\text{train}} = X_1\\right\\} | \\hat{Y}_{\\text{train}})} \\right) \\le \\log \\left( \\frac{1 + \\varepsilon}{1 - \\varepsilon} \\right) \\right] \\ge 1 - \\frac{1}{\\varepsilon} \\sqrt{I(Z;\\hat{Y}_{\\text{train}})}. $$\nThat is, if the proposed unlearning framework is applied to penalize the mutual information term I(Z; Ytrain) such that it remains relatively small compared to exp(\u03b5)\u22121exp(\u03b5)+1, then the probability of observing an event in \u0176train that grants more than inference capability becomes extremely low. By the construction of Ytrain, it is clear that even if we observe the learning outcomes from both the data set including data point to unlearn and the one excluding the point, one can still not tell whether or not the model is trained using X0 ~ X or X\u2081 ~ X \\ {xu}. Therefore, to ensure \u025b-differential unlearning with high probability, it suffices to adjust the regularization weight y such that \u221aI(Z;Ytrain) is small relative to exp(\u03b5)\u22121exp(\u03b5)+1.\nLastly, we connect the proposed \u025b-differential unlearning definition to the unlearning via retraining by showing that it serves as a practical necessary condition for a \"good\" retraining model. In particular, one can show that if a model, denoted by f : X \u2192 \u0423, can minimize training loss on the training data, then either the unlearned model satisfies the e-differential unlearning definition or it suffers from low generalizability by violating a Lipschitz condition as described below."}, {"title": "Lemma 2.2 (\u025b-DU as a Condition for \u201cGood\u201d Retrained Model)", "content": "Assume f is a retrained model with f(Xo) and f(X1) both absolutely continuous, then either f violates \u025b-DU:\n$$ \\sup_y \\left| \\log \\frac{L(f(X_0))(y)}{L(f(X_1))(y)} \\right| < \\varepsilon $$\nor f violates the L-Lipschitz condition for any L < L* where\n$$ L^*(\\varepsilon) := \\frac{\\delta |f(X_1)(y^*) - f(X_0)(y^*)|}{\\mathcal{W}_{d_X}(X_1, X_0)} = \\frac{\\delta \\left( \\exp(\\varepsilon) - 1 \\right) \\min\\left\\{f(X_0)(y^*), f(X_1)(y^*)\\right\\}}{\\mathcal{W}_{d_X}(X_1, X_0)} $$\nHere, y* is the point where the \u025b-differential unlearning definition is violated, Wax (X0, X1) is the Wasserstein distance between Xo and X\u2081 on the metric space (X,dx), and d is the radius around y* where sign(log(L(f(X0))(y))) = sign(log(L(f(X0))(y))). See proof in Appendix A.4. The above result implies that, if we assume that there is a significant marginal effect of adding/removing (Xu, Yu) relative to the original dataset (otherwise there is no significant need for unlearning) and the retrained model f achieves a low training loss on the remaining dataset, then one cannot simultaneously have all of the following: (1) truthful revealing of (xu, Yu): f(xu) = Yu; (2) \u03b5-DU, (3) good generalizability of f with low Lipschitz constant. That is, if we require (2), then a retrained model with good generalizability cannot reveal (Xu, Yu)."}, {"title": "Proposition 2.1 (\u025b-DU Reduces Utility on the Data to Unlearn)", "content": "Assume f(X1) =Y\u2081 and supy|log(L(f(X1))(y))| < \u025b such that L*(\u03b5) > 1, but Yo satisfies d|Yo(y) \u2013 Y\u2081(y)| > Wdx (X0, X1) for some y \u2208 Y, then f(Xo) \u2260 Yo.\nProof This is a direct corollary of Lemma 2.2.\nIn other words, if a retrained model achieves good training performance on the remaining data set (f(X1) = Y\u2081) and satisfies the proposed \u025b-differential unlearning for small enough \u025b (L*(\u03b5) > 1), but there is a significant marginal training signal (d|Yo(y) \u2013 Y\u2081(y)| > 2Wdx (X0, X1)) resulting from adding/removing the data to unlearn (to the existing data set), then f(X0) cannot reveal Yo truthfully. That further implies f(xu) cannot reveal Yu truthfully by the construction of (Xo, Yo).\nFinally, by enforcing \u025b-DU by mutual information regularization while preserving utility on the remaining data, the proposed framework achieves unlearning by only diminishing the marginal utility at the data point to be unlearned. But we further notice that the utility via our approach can decrease further than the retrained (from scratch) model in practice."}, {"title": "2.3 Versatile Utility", "content": "In our feature unlearning setting, often the target variable Y lives in a different space from X, and mutual information may not suffice to capture how X relates to Y. Thus, we consider\n$$ \\sup_{\\hat{\\chi}=f(\\chi,Z)} \\left\\{U(Y; \\hat{\\chi}) : \\hat{\\chi} \\perp Z\\right\\}, $$\nwhere U(Y; X) is a user-defined utility. Below are some widely used objectives, each with a short motivation. For more detailed explanation of each utility below, see Appendix A.5 and Appendix A.6.\n\u2022 Entropy Maximization: U(Y; X) = H(X) preserves the total uncertainty in X.\n\u2022 Mutual Information Maximization: U(Y; X) = I(Y; X) maximizes how much X reveals about Y.\n\u2022 KL-Divergence Maximization: U(Y;X) = DKL(P(Y | \u0176)|| P(Y)) makes P(Y | X) more deterministic relative to P(Y).\n\u2022 Conditional Probability Energy Maximization:\n$$ U(Y; \\hat{\\chi}) = \\begin{cases} \\sqrt{\\mathbb{E}[P(Y \\in A | \\hat{\\chi})^2]}, & \\text{classification}, \\\\ -||Y - \\mathbb{E}(Y | \\hat{\\chi})||_2, & \\text{regression}, \\end{cases} $$\nimproves classification boundaries or reduces mean squared error.\nWe note that the proposed (feature and data point) unlearning framework supports a wide range of utility and objective functions beyond the four listed here. The four listed objectives are selected for their shared analytical optimal solution. As shown in Theorem 3.1, this optimal solution is of particular interest for its independence from the choice of Y, even though three of the objectives explicitly involve Y."}, {"title": "3. Theoretical Guarantees", "content": "In this section, we provide theoretical guarantees for both feature unlearning and data point unlearning. Specifically, we leverage optimal transport to derive a unified analytic solution for feature unlearning under all the listed objectives. For data point unlearning, we establish a provable guarantee by ensuring that the unlearning outcome satisfies \u03b5-differential unlearning through the compression rate quantified by mutual information."}, {"title": "3.1 Feature Unlearning: Unified Optimal Solution for Multiple Objectives", "content": "The utility objectives outlined earlier are commonly used across fields such as biology, physics, and AI. See Appendix A.5 for more details. Despite their diverse forms, these objectives can be unified within a single framework by focusing on the sigma-algebra generated by the unlearning outcome X. This approach allows for a cohesive solution to seemingly distinct optimization problems, demonstrating the versatility and practicality of the proposed framework.\nWe start with the following result, which establishes that the Wasserstein-2 barycenter generates the finest sigma-algebra among all admissible outcomes:"}, {"title": "Lemma 3.1 (Wasserstein-2 Barycenter Generates the Finest Sigma-Algebra)", "content": "Let {Xz}zez CP2,ac(X). We have \u03c3(X) \u2282 \u03c3(X) for all X = f(X,Z).\nThe proof is in the Appendix B.2. The above result demonstrates one feasible optimal solution. Under the assumption of absolute continuity of marginals, the barycenters of convex costs also satisfy the invertibility of transport maps, ensuring optimality.\nNow, the importance of the above result lies in the monotonicity of the objective functions listed earlier w.r.t. the sigma-algebra generated by random variables. That is, the fineness of the sigma-algebra is equivalent to the amount of information contained by the random variable in probability theory. See the following remark for a more detailed explanation."}, {"title": "Remark 3.1 (Sigma-Algebra and Information)", "content": "In probability theory, a probability space is often represented as a triple (\u03a9, \u03a3, P), where \u03a9 is the sample space, \u2211 is the sigma-algebra (a collection of subsets of \u03a9), and P : \u03a3 \u2192 [0,1] is a probability measure that assigns probabilities to each event in \u03a3.\nThe same sample space can be associated with different sigma-algebras, resulting in different probability spaces. We say that a sigma-algebra \u03a3\u2081 is finer than E2, denoted \u22112 C 21, if 21 contains all events in 22. Conversely, we say 21 is coarser than 22 if 21 contains fewer events than 22.\nA random variable or random vector X is a measurable function from the probability space to Rd (or Cd), X : \u03a9 \u2192 Rd. The sigma-algebra generated by X, denoted by \u03c3(\u03a7), comprises all possible events that could be defined based on the image of X in Rd (or Cd). Thus, if X generates a finer sigma-algebra than another variable X', denoted \u03c3(X') \u2282 \u03c3(X), then X contains more events and, therefore, more information than X'."}, {"title": "Lemma 3.2 (Monotonicity of Information Measures w.r.t. Sigma-Algebra)", "content": "If \u03c3(X1) C \u03c3(X2), then:\n\u2022 H(X1) \u2264 H(X2),\n\u2022 H(Y|X2) \u2264 H(Y|X1) for any Y : \u03a9 \u2192 \u0423,\n\u2022 I(Y; X\u2081) \u2264 I(Y; X2) for any Y : \u03a9 \u2192 \u0423,\n\u2022 DKL(P(Y|X1)||P(Y)) \u2264 DKL(P(Y|X2)||P(Y)),\n\u2022 ||P(Y\u2208 A|X1)||3 \u2264 ||P(Y \u2208 A|X2)||3 for any \u0391\u2208 \u03c3(\u03a5).\nInformally, Lemma 3.2 shows entropy increases as the sigma-algebra becomes finer, while conditional entropy decreases, indicating reduced uncertainty. Similarly, mutual information and conditional probability energy increase, reflecting enhanced informativeness and predictive utility. Furthermore, as the sigma-algebra generated by X becomes finer, the conditional prediction of Y given X becomes more deterministic, resulting in a larger KL-divergence between the conditional distribution and the original distribution of Y.\nCombining Lemma 3.1 with Lemma 3.2, we deduce that X is the optimal solution to all utility objectives discussed in Section 2 and further specified in Problems 1-5 in the Appendix."}, {"title": "Theorem 3.1 (Unified Optimal Feature Unlearning Solutions)", "content": "Assume {Xz}zez CP2,ac(X). Then the following statements are equivalent:\n\u2022 \u03c3(X) = \u03c3(\u03a7),\n\u2022 X \u2208 arg maxx=f(x,z){H(X) : X \u00a6 Z},\n\u2022 X \u2208 arg minx=f(x,z){H(Y|X) : X 1 Z} for all Y,\n\u2022 \u00c2 \u2208 arg maxx=f(x,z){I(Y; \u0176) : \u00c2 1 Z} for all Y,\n\u2022 X \u2208 arg maxx=f(x,z){DKL(P(Y|\u0176)||P(Y)) : \u00c2 \u22a5 Z} for all Y,\n\u2022 \u00c2 \u2208 arg maxx=f(x,z){||P(Y \u2208 A|X)||} : \u0178 | Z} for all A and Y.\nProof This follows from Lemma 3.1 and Lemma 3.2."}, {"title": "3.2 Data Point Unlearning: Fine-tuning Guided by Compression Rate", "content": "For data point unlearning, we adopt the following framework to fine-tune the trained model parameters under the guidance of mutual information regularization:\n$$ \\sup_{f:\\chi \\to \\mathcal{Y}} U(Y; \\hat{Y}) - \\gamma I(\\hat{Y}_{\\text{train}}; Z) $$\nThe following theorem is now a direct consequence of Lemma 2.1."}, {"title": "Theorem 3.2 (\u025b-Differential Unlearning Guarantee via Compression Rate)", "content": "Assume the unlearning outcome fe satisfies I(fe(Xtrain); Z) \u2264 \u00b5, then fe satisfies \u025b-differential unlearning with probability at least $$ 1 - \\frac{\\exp(\\varepsilon)+1}{\\exp(\\varepsilon)-1} \\frac{\\mu}{\\varepsilon^2}. $$\nTherefore, one can achieve \u025b-differential unlearning based on the compression rate and choose the hyperparameter y in the framework according to the required & and the resulting trade-off between utility and mutual information."}, {"title": "4. Algorithm Design", "content": "Here, we summarize the pseudo-code implementation of the feature and data point unlearning framework we propose:"}, {"title": "4.1 Feature Unlearning", "content": "4.1.1 FEATURE UNLEARNING ON DATA VIA THEOREM 3.1"}, {"title": "4.1.2 FEATURE UNLEARNING ON MODAL VIA REGULARIZATION", "content": ""}, {"title": "4.2 Data Point Unlearning on the Model Parameter Space", "content": ""}, {"title": "A. Appendix: Supplementary Material for Section 2", "content": "A.1 Utility Motivation\nMutual information is a widely used quantification of the common information shared by two random variables. In particular, given a data set X with a goal to compress X by an encoding Y, the volume of code needed to encode X is 2H(X) where H(X) is the entropy of X. Furthermore, from the Chapman-Kolmogorov equation p(Y) = Exp(x)p(x), the average volume of a mapped to individual \u0176 is equal to 2H(X|\u0176). Here,\n$$H(X|\\hat{Y}) := - \\sum_{X} p(x) \\sum_{\\mathcal{Y}} p(\\hat{Y}|x) \\log(p(\\hat{Y}|x))$$\nis the conditional entropy of X on \u0176. Intuitively, a higher conditional entropy means more volume of x are expected to be mapped to individual Y, which implies more randomness of X remained given the observation of \u0176. In other words, less X is explained by Y.\nSince the volume of code for X is 2H(X) and the average volume of code mapped to each \u0176 is 2H(XY), the average cardinality of the partition generated by the values of \u0176 on the values of X is the ratio:\n$$\\frac{2^{H(X)}}{2^{H(X|\\hat{Y})}} = 2^{I(X;\\hat{Y})},$$\nHere, I(X;\u0176) = H(X) \u2013 H(X|\u0176) is the mutual information between X and \u0176. On the one hand, higher mutual information implies that \u00dd generates a partition with higher cardinality (or usually finer partition) on X, which further implies more common information is shared between X and \u0176. On the other hand, from a data compression perspective, lower mutual information means \u00dd generates a partition on Z with lower cardinality, which further implies a better data compression rate, because \u0176 can compress X into a partition of smaller cardinality.\nAs discussed in Section 2, we adopt mutual information to quantify the common information and compression rate between random variables. For unlearning quality purposes, we hope to maintain as much information of X as possible in generating \u00dd. Therefore, to maximize utility, we should maximize mutual information I(X; Y) or, equivalently, minimize the compression rate."}, {"title": "A.2 Admissibility", "content": "Since we are unlearning the information of Z by compressing relational data (X, Z), it is natural to require the resulting compressed data X to be measurable with respect to (X, Z). Intuitively, the compression output X should have its \"root\" from (X, Z) without introducing additional randomness by the compression map f itself. Technically speaking, the \"root\" here means that for every event or observation A of the compressed X, the information or pre-image represented by the observation X\u22121(A) := {w : X(w) \u2208 A} comes from the knowledge of f\u00af\u00b9(A) := {(x, z) : f(x, z) \u2208 A} based on (X, Z):\n$$ \\left\\{\\omega : \\hat{\\chi} (\\omega) \\in A\\right\\} = \\hat{\\chi}^{-1}(A) = (\\chi, Z)^{-1}(f^{-1}(A)) = \\left\\{\\omega : (\\chi(\\omega), Z(\\omega)) \\in f^{-1}(A) \\right\\}. $$"}, {"title": "A.3 Proof of Lemma 2.1", "content": "Proof First, notice that it follows from the construction of relational data (Xtrain, Z) that {Z = 0} = {Xtrain = X0} and {Z = 1} = {Xtrain = X1}. Also, we have\n$$ |P(Z = 0|\\hat"}]}