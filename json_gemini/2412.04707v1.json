{"title": "Parametric-ControlNet: Multimodal Control in Foundation Models for Precise Engineering Design Synthesis", "authors": ["Rui Zhou", "Yanxia Zhang", "Chenyang Yuan", "Frank Permenter", "Nikos Arechiga", "Matt Klenk", "Faez Ahmed"], "abstract": "This paper introduces a generative model designed for multimodal control over text-to-image foundation generative AI models such as Stable Diffusion, specifically tailored for engineering design synthesis. Our model proposes parametric, image, and text control modalities to enhance design precision and diversity. Firstly, it handles both partial and complete parametric inputs using a diffusion model that acts as a design autocomplete co-pilot, coupled with a parametric encoder to process the information. Secondly, the model utilizes assembly graphs to systematically assemble input component images, which are then processed through a component encoder to capture essential visual data. Thirdly, textual descriptions are integrated via CLIP encoding, ensuring a comprehensive interpretation of design intent. These diverse inputs are synthesized through a multimodal fusion technique, creating a joint embedding that acts as the input to a module inspired by ControlNet. This integration allows the model to apply robust multimodal control to foundation models, facilitating the generation of complex and precise engineering designs. This approach broadens the capabilities of AI-driven design tools and demonstrates significant advancements in precise control based on diverse data modalities for enhanced design generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Engineering design is a complex process that involves the creation, analysis, and optimization of products, systems, and processes to meet specific requirements. Traditionally, engineering design has relied on human expertise, creativity, and problem-solving skills to navigate the vast design space and arrive at optimal solutions. However, with the rapid advancement of artificial intelligence (AI) techniques, there is a growing opportunity toenhance the engineering design process through the use of Al-driven tools and methodologies. Generative models, in particular, have emerged as a promising avenue for bridging the gap between AI and engineering design. These models, which learn to generate new data points based on patterns and structures observed in training data, have the potential to unlock new possibilities in design exploration, optimization, and automation. By leveraging the power of deep learning and probabilistic modeling, generative models can help designers explore a wider range of design alternatives, uncover novel design concepts, and streamline the design process. However, despite the success of generative models in domains such as art, music, and natural image generation [8], their applications in engineering design have been limited. This is due to several factors. These limitations are caused by three significant factors: (a) designers are unable to precisely control the generated content, (b) generative models have difficulties in understanding performance metrics and physical properties, and (c) generative models often struggle with complex engineering designs [30]. For example, models like Stable Diffusion [25] excel at generating realistic images based on textual descriptions but"}, {"title": "2 RELATED WORKS", "content": "The following sections delve into the evolution of generative models, multimodal control for generative models, and the limitations of state-of-the-art methods of using generative models for engineering design."}, {"title": "2.1 Generative Models", "content": "Recent years have witnessed significant progress in generative models, which aim to learn the underlying distribution of data and generate new samples that resemble the training data. Among the most prominent generative models are Generative Adversarial Networks (GANs) [10], Variational Autoencoders (VAEs) [17], and Diffusion Models [29]. GANs, introduced by Goodfellow et al. [10], consist of a generator network that learns to generate realistic samples and a discriminator network that distinguishes between real and generated samples. To address limitations such as mode collapse of traditional GANs, researchers have proposed various extensions and improvements, such as StyleGAN [16] and BigGAN [4]. These models have also been extended to handle other data modalities, such as 3D shapes [5] and video [28].\nVAEs, proposed by Kingma and Welling [17], learn a probabilistic encoding of the input data into a latent space and decode samples from this space to generate new data points. VAEs have been widely used for unsupervised learning and have been extended to handle various data modalities, such as text [3] and graphs [27]. Recent works have focused on improving the sample quality and diversity of VAEs, such as VQ-VAE-2 [23], have achieved competitive results compared to GANs.\nDiffusion models, first introduced by Sohl-Dickstein et al. in [29], have recently emerged as a powerful class of generative models. These models learn to generate samples by reversing a gradual noising process, starting from random noise and iteratively denoising it to obtain a clean sample. Diffusion models have shown impressive results in image generation [8], outperforming GANs and VAES in terms of sample quality and diversity. Stable Diffusion [25] has gained significant attention due to its ability to generate high-quality images from textual descriptions, enabling a wide range of creative applications. Recent advancements in diffusion models include improved architectures, such as Diffusion Autoencoders [21] and Masked Autoencoder Diffusion Models [31], which have achieved state-of-the-art performance in image generation tasks. Diffusion models have also been applied to 3D shape generation [20], video generation [32], and even music [14]. Despite the impressive capabilities of diffusion models in generating diverse and high-quality outputs, a significant challenge remains: the need for more"}, {"title": "2.2 Multimodal Control and Leveraging Foundation Models", "content": "One of the significant drawbacks of generative models is the difficulty of controlling over the generated content. Various approaches have been proposed to add control to diffusion models, enabling more guided generation and enabling the ability to leverage pre-trained models, instead of training from scratch. This is even more significant for engineering design, where the amount of labeled and cleaned dataset is in high deficiency. ControlNet [34], illustrated in Figure 2, introduces a framework for adding conditional controls to pretrained diffusion models, allowing users to modify specific attributes of the generated content. In particular, it creates two copies of stable diffusion layers. One is locked, and the other is trainable and is conditioned on another input modality such as images. The trainable copy of the network also contains zero convolution and the results of the two networks are combined for each layer. This allows computationally efficient training and robustness to overfitting as the weights of the original Stable Diffusion model are locked [34]. Further, this architecture enables the possibility of conditioning the generative process in inputs such as edge maps for architecture, human pose graph for specific human motion generation. Further, this architecture allows adding control modalities to pre-trained foundation models, reducing the need for large amounts of data. Works are investigating the performance of ControlNet with different conditions. For example, Ju et al. address the issue of the uncertain and inconsistent image generation from ControlNet conditioning human pose, and proposes the HumanSD framework with greater accuracy [15]. Other methods focus on controlling the self-attention mechanism in diffusion models [12], enabling fine-grained control over the generation process. Additionally, techniques like classifier-free guidance [11] and contrastive language-image pre-training [7] have been employed to enhance the controllability of diffusion models. These approaches leverage additional information, such as text embeddings or classifier gradients, to guide the generation process toward desired outputs. Despite the advancements in multimodal learning and control, current methods primarily focus on integrating text and image modalities for controlling generative models, engineering products often involve a wide range of modalities, including parametric data, geometric constraints, assembly instructions, and performance requirements, which are not easily represented through text or images alone. Further, most works focus on training a novel architecture from scratch, which is infeasible for many engineering design problems, where large amounts"}, {"title": "2.3 Multimodal Generative Model for Engineering Design", "content": "Generative models and multimodal learning have shown significant potential in revolutionizing the field of engineering design by enabling the automatic creation, optimization, and evaluation of designs based on specified requirements and constraints. For example, Su et al. proposed a multimodal machine learning framework for predicting car ratings in [1]. In recent years, diffusion models have emerged as a promising approach for engineering design synthesis and optimization. Prior research like ShipGen [2] and TopoDiff [19] demonstrate the effectiveness of diffusion models in generating optimized designs for various engineering domains, such as ship hull design and topology optimization. These models learn the gradual refinement process of design generation, enabling the creation of designs that meet specific performance targets and constraints. Generative models have also been applied to specific engineering design domains, such as materials design and microstructure optimization. For example, DiffMat [33] utilizes diffusion models to generate novel materials with desired properties, while work by Lee et al. [18] employs a diffusion-based approach to optimize the microstructure of materials for enhanced performance. Further, in [35], Zhou et al. reimagined diffusion-based tabular imputation models as a way to enable parametric design autocomplete copilots. In [2] Bagazinski et al. utilized a conditional guided diffusion model for parametric ship hull design. In [9], Edwards proposed a pipeline that can transform hand-drawn sketches into 2D and 3D designs. Chong et al. [6] explored CAD prompts for feasible and novel design generation. Despite the advancements in generative models for engineering design, there remain challenges and limitations. One major challenge is the incorporation of complex design modalities into the generative process. Engineering design often involves a multitude of information modalities, such as parametric data, assembly graphs, and component information. Further, the ability to leverage pre-trained foundation models instead of training from scratch is crucial for ensuring the success of using generative models in engineering design. For example, in automotive engineering, multimodal control would enable simultaneous adjustments to a car's aerodynamics and aesthetics based on specific performance simulations and design criteria, ensuring that the final product meets both functional and visual standards. Further, there are very few high quality datasets that include both aerodynamics and aesthetics data. Thus, in this work, we aim to combine the power of diffusion models with multimodal control inputs to enable a versatile and effective generative model for engineering design. By incorporating parametric data, assembly graphs, component inspiration images, and textual descriptions, our proposed model can capture both design intent and also engineering constraints and generate optimized designs that meet specified requirements and constraints."}, {"title": "3 METHODOLOGY", "content": "We apply our model to the bike design problem. The BIKED Dataset is introduced by Regenwetter et al. in [24]. The dataset contains the CAD files and parametric information of 4500 individually designed bicycles. We augment the dataset by randomly sampling the valid ranges for existing features to create an augmented version of the dataset, resulting in 12,506 samples. We further do a random training test split with a 90-10 ratio. 11255 samples are used for training and 1251 samples are used for testing and reporting performance. For each data sample, there is (1) a parametric vector containing 222 features that form a complete parametric representation for the full bike, (2) rendered images of the parametric features and the image resolution is 512x512, (3) a text description of \"bike, white background.\""}, {"title": "3.2 Training", "content": "We train the model with the following hyperparameter settings: (1) learning rate: $1 \\times 10^{-5}$, (2) batch size: 4, (3) number of epochs: 100. We train the model on a server with a 4090 GPU, an Intel i-9-13900K CPU, and 64Gb memory."}, {"title": "3.3 Overall Pipeline", "content": "Our proposed methodology develops a generative model for engineering design that leverages diffusion models and multimodal control inputs. The overall pipeline consists of five components, as illustrated in Figure 3.\n1.  Parametric Encoder with Autocompletion: We employ a diffusion-based imputation model inspired by [35] to handle incomplete parametric information. This generative model generates diverse and complete parametric designs from partial parametric inputs. Specifically, it generates a complete parametric design from the partial design. When the model is given a complete parametric design, we bypass the complete parametric design generation stage. With"}, {"title": "3.4 Parametric Encoder with Autocompletion", "content": "The parametric autocompletion module is inspired by [35], which introduces a generative imputation model for completing missing parametric data in engineering designs. It combines graph attention networks (GATs) and tabular diffusion models to capture and impute complex parametric interdependencies from an assembly graph. The parametric autocompletion module serves as the first stage of the pipeline. It takes incomplete parametric designs as input and generates diverse and complete parametric designs. When the input is a complete parametric design, we bypass the parametric autocompletion module. The output is then passed through a parametric encoder comprised of two fully connected layers to obtain a compact parametric embedding, which serves as one of the inputs to the multimodal fusion stage. The autocompletion module provides flexibility and adaptability, allowing the generative model to handle incomplete parametric information effectively and provide design recommendations and autocomplete functionality based on the available parametric data."}, {"title": "3.4.1 Component Assembly and Encoding", "content": "The second stage of our methodology incorporates component inspiration images and assembly graphs into the generative model through component assembly and component encoding.\nComponent Assembly: The component assembly step utilizes the structural information provided by the assembly graphs to assemble the component inspiration images into a coherent representation. Each node in the assembly graph represents a component, and the edges define their connections, relative positions, and relative sizes.\nWe designed an assembly algorithm illustrated in Algorithm 1 that retrieves the corresponding component inspiration images and positions and scales them according to the size and position attributes specified in the assembly graph. Though layering the correct-sized component images, the result is a composite image representing the assembled design.\nIn this process:\n1.  Each component image $I_v$ is adjusted according to the size attribute $s_v$ specified in the assembly graph.\n2.  The positioning $p_v$ determines the exact location of $I_v$ on the canvas.\n3.  The resized and positioned component images are layered sequentially to create a composite image $I_{comp}$ that represents the assembled design.\nComponent Encoding: The assembled composite image is then encoded into a meaningful representation using a component encoder consisting of convolutional and linear layers. This architecture is designed following the structure outlined in [34], which has been proven effective for component conditioning tasks. Specifically, the component encoder comprises 8 convolutional layers with the following configuration: 2 layers with a dimension of 16 and filter size 3, 2 layers with a dimension of 32 and filter size 3, 2 layers with a dimension of 96 and filter size 3, 1 layer with a dimension of 256 and filter size 3, and 1 final layer with a dimension of 319. This design mirrors the configuration used in the original ControlNet paper, ensuring compatibility and effective feature extraction.\nThe convolutional layers extract relevant features and patterns from the image, capturing the spatial and structural information of the assembled design. This architecture was selected due to its proven success in handling component-specific encoding. The use of this tailored encoder, as opposed to pre-trained image encoders like CLIP, was motivated by the need for task-specific feature extraction that captures detailed spatial relationships critical for engineering design applications."}, {"title": "3.5 Text Encoder", "content": "To encode the textual descriptions of the bikes, we retain the original ControlNet's CLIP embedding mechanism [22] to allow the next step of multimodal embedding fusion, we project the embedding vector after CLIP into a subspace of dimension of 4096."}, {"title": "3.6 Multimodal Fusion", "content": "The Multimodal Fusion process is critical for synthesizing diverse data modalities\u2014engineering parametric data, component assembly, and textual descriptions\u2014into a unified design representation. After obtaining the embedding vectors for each of these modalities, which are each of dimension 4096, the parametric embedding and the component embedding are concatenated and projected into a 4096-dimensional vector using a fully connected layer. This projected vector is then added to the CLIP embedding obtained from the pretrained Stable Diffusion model to create an integrated multimodal representation.\nThe parametric imputation model is pretrained, as detailed in [35], and the CLIP model is also pretrained. The rest of the network, including the fusion layers and the downstream generative process, is trained end-to-end. This training approach ensures that the multimodal embeddings are aligned and optimized for the overall generative task.\nValidation of the multimodal fusion process has been demonstrated in prior work [35] [22]. Specifically, downstream tasks involving controlled design generation and multimodal conditioning have shown that using this fusion process leads to coherent, contextually accurate design outputs that adhere to input specifications. We rely on these prior validations to confirm the effectiveness of the approach in synthesizing a unified design representation."}, {"title": "3.7 ControlNet Module", "content": "After we obtain the multimodal conditional embedding, we feed the vector to a ControlNet-like module that acts as a modifier over foundation models, which is Stable Diffusion in our case. Here, we follow the same model architecture as in [13]."}, {"title": "4 EVALUATIONS", "content": "Our model extends the capabilities of generative design by enabling applications that existing state-of-the-art models cannot achieve effectively. Specifically, we demonstrate that models such as Stable Diffusion [25] struggle to handle engineering-specific tasks where strict adherence to input parameters and complex component relationships are required. These limitations are evident when generating designs based solely on detailed parametric constraints or integrating component assembly information into the output.\nTo illustrate the strengths of our proposed pipeline, we conduct thorough quantitative and qualitative evaluations on the following tasks:\n1.  Accurate design generation conditional on parametric data only: We assess how well the model generates complete designs that strictly adhere to input parametric specifications, validating this through metrics such as R2 scores and mean squared error comparisons against a surrogate model trained for parametric prediction.\n2.  Precise control over generated design using component images: We evaluate the model's ability to generate designs that conform closely to provided component images, using metrics such as Intersection over Component (IoC) scores to measure fidelity to the input visual constraints.\n3.  Multimodal control involving both parametric data and component images: We analyze how the model integrates and balances information from both parametric inputs and component images to produce cohesive outputs that respect both data sources. This is assessed through controlled experiments where different combinations of inputs are provided, and results are validated through surrogate model evaluations and visual inspection.\nIn addition to standard image reconstruction quality metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), which are commonly used in prior works such as [25], we introduce a comprehensive evaluation framework that also includes a Diversity Score."}, {"title": "Formulas", "content": "PSNR is used to measure the fidelity of the generated bike designs compared to the expected design derived from parametric and component inputs. A higher PSNR value indicates that the generated image closely aligns with the specified design features, such as frame dimensions, wheel size, and component placements, ensuring minimal deviation from the input specifications. This metric is essential for verifying that the model can accurately translate input data into high-quality visual outputs.\nThe PSNR is defined as:\n$PSNR = 10 \\cdot log_{10}(\\frac{MAX_I^2}{MSE})$, (1)\nwhere $MAX_I$ represents the maximum possible pixel value of the image, and MSE is the mean squared error between the original and generated images.\nSSIM evaluates the perceptual quality of the generated designs, focusing on the structural similarity between the output and the expected design based on the parametric and component input. This metric considers changes in structural information, luminance, and contrast, making it more aligned with human visual perception. In the context of bike design generation, SSIM ensures that the model preserves the relationships between key components such as the frame, wheels, handlebars, and seat, leading to realistic and coherent outputs. The SSIM is defined as:\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}$, (2)\nwhere $\\mu_x$ and $\\mu_y$ are the mean values of $x$ and $y$, $\\sigma_x^2$ and $\\sigma_y^2$ are the variances, $\\sigma_{xy}$ is the covariance, and $C_1, C_2$ are constants to stabilize the division.\nThe Diversity Score measures the variability among generated outputs. In bike design generation, this metric is crucial for evaluating the model's ability to produce a range of unique designs rather than repetitive or overly similar outputs. This diversity is particularly important in applications such as using the model as a design recommendation system. The Diversity Score is computed as:\n$Diversity Score = \\frac{2}{n(n-1)} \\sum_{i=1}^n \\sum_{j=i+1}^n d(f(x_i), f(x_j))$, (3)\nwhere n is the number of samples, $f(x_i)$ and $f(x_j)$ are feature representations of samples $x_i$ and $x_j$, and $d(\\cdot, \\cdot)$"}, {"title": "4.1 Surrogate Model for Parametric Generation Accuracy Analysis", "content": "Measuring how well the model is responding to parametric conditions is a crucial step in the evaluation of the model. To provide a more rigorous analysis beyond visual inspection, we train a ResNet-18 model for each hand-selected feature that are of prominent visual difference, including both continuous and categorical features. Specifically, we use the feature-specific ResNet models as surrogate models to evaluate how accurately the generative pipeline adheres to these parametric conditions. The features evaluated include:\n1.  Continuous features: Saddle Height, Seat Tube Length, Stem Angle, Top Tube Length, and Head Tube Angle.\n2.  Categorical features: Number of Water Bottles and Handle Bar Style."}, {"title": "4.2 Component Conditioning Analysis", "content": "Further, evaluating how well the model is conforming to component images is also very important. To evaluate the pipeline's ability to follow the conditions on component images and assembly graphs, we compare the generated bikes against the assembled components and also evaluate the overall generation quality by manual inspection and quantitative metric evaluation. Specifically, we are evaluating how closely the model follows the assembled component images and if the model can generate valid and high-quality bikes using these conditions. We further demonstrate the model's ability by giving it out-of-distribution component images from the Internet and evaluating its performance."}, {"title": "4.3 Multimodal Design Generation Analysis", "content": "To evaluate the model's capability on multimodal conditioning on both parametric and component modalities, we first evaluate the model's ability to follow each of the modalities. We further evaluate the model in two adversarial cases: (a) when different modalities provide complementary information and we expect the model to conform to information contained in both modalities and (b) when the different modalities contain conflicting information and we expect one modality to overtake another modality. We evaluate the model using surrogate models and metrics defined above to evaluate the model over these situations with the metrics mentioned above to show the model's robustness under different modality overlapping cases."}, {"title": "5 EXPERIMENT RESULTS AND DISCUSSION", "content": "We demonstrate that our model can generate designs accurately controlled by various modalities, a capability that state-of-the-art models struggle to achieve. For example, Stable Diffusion, which represents one of the most popular T2I models, fails when tasked with multimodal generation based on parametric and component data. In our experiments, we embedded bike design parameters, such as size, structure, and specific features, directly into text prompts for Stable Diffusion.\nFor example, we structure the text prompts for Stable Diffusion as follows:\nFor parametric-based generation: \"A bike"}, {"title": "5.2 Complete Parameter Set Generation", "content": "We first conduct a comprehensive analysis on the accuracies of parametric controlled generation. We show it is effective and enables the capability of design generation conditioned on an arbitrary subset of the parameters. First, we show some of the generated bikes from generated features of the imputation model in Figure 5."}, {"title": "5.2.1 Ablation Study of Imputation Model", "content": "To demonstrate the effectiveness of using an imputation model for parametric data completion, we conduct an ablation study, as shown in Table 2. The study compares performance metrics when employing the imputation model described in [35] against an alternative approach that feeds the downstream pipeline with: (a) a mask vector indicating missing features, and (b) a feature vector where missing features are set to zero. The results indicate that incorporating the imputation model significantly improves the model's performance, enhancing both accuracy and diversity in the generated outputs."}, {"title": "5.3 Parametric Generation", "content": ""}, {"title": "5.3.1 Using surrogate models to Evaluate Parametric Conditioning", "content": "Further, to closely evaluate the model's ability to accurately follow parametric conditions, we use the surrogate model to predict the parameters of the generated designs against the original parameter set for 10 randomly"}, {"title": "5.4 Component Conditioning", "content": ""}, {"title": "5.4.1 Evaluations on Component Conditioning", "content": "Further, we are interested in thoroughly evaluating the model's ability to condition on component images. We provide the model with component images of a predefined subset of seven components. The number of components present in the component conditioning stage can be anything between 1 and 7. We show a random set of gen"}, {"title": "5.5 Multimodal Design Generation", "content": "Furthermore, investigating how well the models behave under multimodal conditioning inputs is also very important. For each of the following cases, we evaluate the model's performance when given multimodal information including parameters, component images, and assembly graphs. In each of the cases, we evaluate the model's performance by following each of the modalities."}, {"title": "5.5.1 Non-Overlapping Cases", "content": "In complex engineering tasks, different modalities often provide unique and complementary information. To test the model's ability to integrate these distinct streams effectively, this experiment uses parameters and component images as non-overlapping inputs. Each modality independently contributes specific information that, when combined, forms the complete set of design specifications. This setup assesses the model's capability to synthesize information from disparate sources to produce coherent and complete designs. To test this, we mask the parameters to ensure no redundancy with component images. However, when put together, the parameters and the component images represent the complete parameter set. We show the results in Table 6."}, {"title": "5.5.2 Adversarial Case: Conflicting Information", "content": "In real-world design scenarios, engineers and designers often face conflicting requirements or constraints that must be carefully balanced or prioritized. To simulate such challenges and evaluate the model's resilience and decision-making capabilities, we intentionally introduce conflicting constraints into our experiment. For example, we provide the model with parametric information indicating a high saddle while providing a component image indicating a low saddle. This setup tests the model's ability to handle discrepancies and prioritize information from different modalities when they conflict. The results, illustrated in Figure 7 and Table 7, show that the model tends to prioritize component images. This is expected because although the model is trained on both parametric data and component images, the primary training loss is based on image reconstruction. This emphasis on image reconstruction encourages the model to prioritize the visual information from component images, which directly affects the quality and fidelity of the generated output. Component images inherently provide richer spatial and structural cues, making them more influential in guiding the model's decisions compared to the abstract, numerical nature of parametric data. This behavior ensures that the generated designs maintain high visual coherence, even when input modalities conflict."}, {"title": "Formulas", "content": "Beyond mere fidelity, the diversity across the generated set was computed to understand the model's variability in output for a given input. We show the results in Table 5. \n$IOC = \\frac{Area_{intersection}}{Area_{component}}$, (4)"}, {"title": "5.5.3 Creativity Assessments", "content": "A key challenge when integrating parametric or image-based controls into generative models like Stable Diffusion is the potential loss of creative output, which is a defining strength of foundational models. To address this concern, it's crucial to evaluate whether the model retains its capability to generate innovative and unique designs that diverge from existing images in the BikeCAD dataset. This section assesses the model's ability to produce creative and contextually relevant designs while adhering to specific multimodal inputs. We conducted three experiments using unique text prompts to guide the generation process, aiming to test the model's capacity to blend thematic creativity with technical design constraints. Specifically, the inputs are the component images and the text descriptions shown below and the outputs are the generated designs. We generate 30 samples for each case and hand-pick the four most prominent ones. The results, as shown in the figure, demonstrate the model's capability to integrate thematic elements into the design of bicycles. The results are shown in Figure 8.\nA. Text Prompt: \u201cbike that looks like an ant\u201d The resulting designs showcase the model's proficiency in capturing the essence of an ant's form, including segmented structures and compact, robust frames. The model successfully generates bike designs that have ant elements.\nB. Text Prompt: \"bike that looks like a lion\u201d In this experiment, we explore the model's ability to generate bike designs that resemble a lion. The model successfully generates bike designs that have elements of a lion.\nTo quantitatively evaluate the model's creativity and adherence to inputs, we used the CLIP model to measure the average distances of generated samples to the input component image, the text prompt, and the training dataset, as shown in Table 8.\nThe low average distances to the input component"}, {"title": "6 DISCUSSION", "content": "Our model represents an advancement in engineering design by offering strong multimodal control over foundation models, particularly tailored for engineering applications. By integrating parametric inputs, assembly graphs, and component inspiration images, our model enables precise design generation with diversity. The discussion will focus on the implications of our model, its potential applications, and future directions for research and development."}, {"title": "6.1 Implications and Applications to Engineering Design", "content": "The introduction of our model has profound implications for engineering design across various industries. By providing designers with enhanced control over generated designs, our model facilitates the exploration of complex design spaces, enabling the creation of innovative and optimized solutions. Moreover, the ability to incorporate diverse data modalities such as parametric inputs, assembly graphs, and component images enhances the fidelity and accuracy of generated designs, ensuring alignment with specifications and constraints. This capability is particularly valuable in domains such as product design, architecture, and manufacturing, where precise control and customization are paramount."}, {"title": "6.2 Future Directions", "content": "While our model represents a significant step forward in engineering design generation, there are several avenues for future research and development. One direction is the refinement and optimization of the model architecture to further enhance its performance and scalability. In particular, future work may involve exploring advanced multimodal fusion techniques, incorporating additional control modalities, such as engineering performance (dynamics, ergonomics, structural, aerodynamics, etc.), environmental factors (design for specific terrains), or 3D information conditioning through representations such as mesh and point cloud.\nAnother promising direction is the exploration of novel applications and use cases for our model across diverse domains. For example, our model could be applied to parametric CAD design problems in fields such as automotive engineering, aerospace, and biomedical device design. By adapting the model to specific domain requirements and constraints, researchers can unlock new opportunities for innovation and problem-solving."}, {"title": "7 CONCLUSION", "content": "We presented a novel generative model designed to exert multimodal control over text-to-image foundation models, specifically tailored for engineering design applications. By integrating parametric inputs, assembly graphs, and component inspiration images, our model offers precise design generation. Our work represents a significant advancement in AI-driven design tools, with implications for diverse industries and collaborative design processes. Moving forward, we envision continued research and development to refine and optimize our model, explore novel applications across different domains, and establish rigorous evaluation standards. Ultimately, our model has the potential to revolutionize engineering design by empowering designers with powerful tools and methodologies for innovation and problem-solving."}]}