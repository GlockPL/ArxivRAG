{"title": "Generating Unseen Code Tests In Infinitum", "authors": ["Marcel Zalmanovici", "Orna Raz", "Eitan Farchi", "Iftach Freund"], "abstract": "Large Language Models (LLMs) are used for many tasks, including those related to coding. An important aspect of being able to utilize LLMs is the ability to assess their fitness for specific usages. The common practice is to evaluate LLMs against a set of benchmarks. While benchmarks provide a sound foundation for evaluation and comparison of alternatives, they suffer from the well known weakness of leaking into the training data (Xu et al., 2024). We present a method for creating benchmark variations that generalize across coding tasks and programming languages, and may also be applied to in-house code bases. Our approach enables ongoing generation of test-data thus mitigating the leaking into the training data issue. We implement one benchmark, called auto-regression, for the task of text-to-code generation in Python. Auto-regression is specifically created to aid in debugging and in tracking model generation changes as part of the LLM regression testing process.", "sections": [{"title": "1 Introduction", "content": "Many benchmarks exist for evaluating LLM generation that is related to software development and coding tasks (Zhang et al., 2023). Two well acknowledged challenges related to utilizing benchmarks for evaluation are (1) data leakage, where the training data of the LLM under evaluation eventually includes benchmarks' data, and (2) debugging or the ability to understand what the underlying issues highlighted by benchmark failures are so that they can be mitigated.\nWe introduce an approach for generating benchmarks for code related tasks that relies on a commonly used intermediate code representation in the form of an Abstract Syntax Tree. ASTs are often used in program analysis. In the context of LLMs they are sometimes used to create metrics that may better fit code-related tasks, including robustness related assessment (Palacio et al., 2024; Kumar et al.,\n2024). We are unaware of work that utilizes ASTs to automate the generation of benchmarks and ease the debugging process. Our approach makes it feasible to create new benchmarks in order to ensure that the evaluation can generalize as we can be confident that the LLM has not trained on this newly generated benchmark data. Our approach supports the creation of a debugging dictionary that includes programming language constructs that have been identified as challenging for the LLM to correctly generate. Once manually populated, this dictionary can be used to easily debug new LLM output.\nWe introduce our AST-based approach for generating code related benchmarks. Figure 1 depicts the process of generating a benchmark and testing it. The process begins by selecting source code to test the LLM with. The code should have unit-tests, allowing to verify model output more accurately. It would also preferably include code that was unseen during training. The chosen source code may be in a different programming language than the task destination code. The next step is to generate the AST. Our suggestion is to use a tool like tree-sitter (Brunsfeld and contributors, 2023).\nWe demonstrate the approach depicted in Figure 1 by generating a new benchmark, the auto-regression benchmark. The starting point for auto-regression is the NAPS dataset (Zavershynskyi et al., 2018) and the AST representation that it provides (termed universal AST or uAST).\nAuto-regression is a text to code benchmark where each output set of instructions, termed problem, includes test cases as input-output tuples. The text is generated deterministically from the NAPS AST to ensure its correctness. Auto-regression can be viewed as following the paradigm of IFEval (Zhou et al., 2023a) and extending it with a low-level textual instructions benchmark for code tasks. It is intended as a basis for regression testing and eliminates the need to debug algorithmic related issues as it provides the correct algorithm to implement. We demonstrate the notion of a debugging dictionary for auto-regression and how we utilize it to easily debug LLM generation issues. Having a single dictionary helps in creating regression tests as it makes it possible to track changes in LLM versions. We see cases where newer LLM versions improve in some categories yet introduce other categories of problems.\nTo summarize, this paper's contributions are:\n1.  A method for creating benchmarks in a way that generalizes across tasks and programming languages; the benchmark can be created from any existing code, including in-house proprietary code. It is recommended that the code has unit tests.\n2.  A low-level text-instructions to code benchmark, the auto-regression benchmark. 1 We tested Python generation from the benchmark.\n3.  A method for creating a debugging dictionary that becomes part of the auto-regression benchmark; this dictionary enables regression testing of deterioration or improvement in implementing the dictionary constructs."}, {"title": "2 AST-based benchmark generation", "content": "Generating English instructions from an AST can be done by walking the tree nodes and outputting a description from the leaves toward the root. An example of the generated list of instructions is in Figure 1. To keep the generation simple we limited ourselves to a single pass over the tree. This means, for example, that we add redundant parenthesis in mathematical expressions to make sure they are not ambiguous. We also leave calls like $f(g(h()))$ untouched even though, for a human, expanding the calls would be clearer. For example, concat_string(a, concat_string(b, c)) would be clearer to a human if expanded to concatenate string a with b and c. It is possible to adjust the detail level and expressiveness of the English generated output depending on how well the model performs and what we are trying to test.\nTo get the model generated code we pass the model a set of generic instructions and the text generated in the previous step. The generic instructions are lists of \"do\" and \"don't\" clarifications that help improve performance. For example, \"Do not ask for user input!\" was very important mainly to llama-family models as they tend to add unrequested code asking for user input in the global scope, i.e. outside any Python method. This code is executed upon import and, as a result, automated tests would get stuck at this stage. Another example is: \"Replace all array_* methods with Python list operations\" which models (except GPT) often left unimplemented even though another instruction explicitly requests the implementation to cover all aspects of the provided pseudo-code.\nFinally, the running the code stage uses unit-test assertions to score the performance of the model. We run the models using a greedy setting, when possible, to get the best solution the model can come up with. We grade the model performance using a school-like grading method where each test is a sub-problem that earns equal points. For example, if a problem has 8 tests and the model passes 6 it gets a grade of 0.75. We prefer this over the commonly used pass@k (Chen et al., 2021) because (1) we want a best-effort attempt, not the probability to get at least one correct solution from k attempts, and (2) to save inference time (and cost) of running the model k times.\nFrom the AST we also generate ground-truth run-able code in the target language. This code can be used to generate prompts for the fill-in-the-middle (FIM) task, for running model metrics which compare code text (Papineni et al., 2002; Lin, 2004) or build their own code representation (Ren et al., 2020; Zhou et al., 2023b), and for debugging the model code since due to the low-level instructions we expect little variation in outputs.\nLastly, we also generate AST statistics. We use this to identify programming language constructs where the model under evaluation is more likely to err. We also count how many times each construct appears in each problem. The statistic includes basic constructs like if/if-else conditions and loops"}, {"title": "2.1 Data", "content": "We used the NAPS dataset (Zavershynskyi et al., 2018) which is generated from solutions for competition problems posted on CodeForces. This dataset was appealing because it represented real-world problems for which there is a high-level description; it has multiple examples of working code; there exist test cases and each problem has tens to hundreds of low-level instructions created by humans. Unfortunately, the human low-level instructions proved to contain too many errors, therefore we decided to use the AST it provided and write an AST to English translator. The advantage of translation using code is the consistency in description of the same operation. The downside is a lack of variation which might catch more model issues.\nWe created our auto-regression datasets in two sizes: a tiny one consisting of 135 problems and a small one with 460 problems. The available data allows generation of a dataset with a few thousand problems. However, this doesn't provide much additional benefit. On the small dataset we saw that models had a higher success rate than on the tiny one. We did not see new categories of errors, thus concluded that most instructions are easy enough to follow, therefore there is no need to add more tests of the same construct.\nExample 1 shows the instructions generated from the AST of a simple programming problem. More examples can be found in Appendix A where we show how common errors look like."}, {"title": "2.2 Metrics", "content": "Since we have tests cases, we decided to calculate partial and whole test pass/fail scores. The whole test pass/fail is the same as the popular pass@k where k = 1. The partial score treats the tests/asserts as test questions; getting all questions right gives a full mark, getting m out of n correct gives a score of m/n.\nNot all problems in the dataset have the same number of tests. In the tiny dataset most problems have 10 tests, but several have less, usually 7 or 5. In the small dataset there are some problems that have 100 or more tests."}, {"title": "2.3 Debugging dictionary", "content": "Auto-regression is created such that the prompts it generates are an exact description of the desired implementation. This enables to create a dictionary of low level coding constructs that assists in debugging generated code that fails to correctly pass the given tests. This dictionary can be repeatedly used over the benchmark execution results as part of a test regression analysis."}, {"title": "3 Benchmark results", "content": "We ran the dataset on GPT, LLama, Mixtral, Granite and Deepseek. For most we also tried more than one version of the model for comparison. The models size varies between 8b and 70b.\nThe results show that the performance of GPT increased significantly between the 3.5 turbo and 4o versions. The 3.5 had a major issue with closing parenthesis, mainly when there were a lot of them in the instructions. Anecdotally, a previous, deprecated, version of 3.5 didn't suffer from that issue and we measured a performance around 0.8.\nFor Granite the small 8b model couldn't implement string split correctly, an issue that was mostly solved by the newer and larger 34b model. The latter also improved in handling of loops and ASCII. Interestingly, it \"replaced\" the wrong implementation with ignoring part/whole instructions. We need to verify whether the errors are on the same constructs/instructions in both versions."}, {"title": "4 Conclusions", "content": "We introduced an AST-based methodology for automatically generating benchmarks for LLM code related tasks. We utilized this approach to generate the auto-regression benchmark, a low level instructions text to Python code benchmark. Relying on ASTs also allowed us to include a dictionary of Python constructs to ease the debugging task. Our results provide an anecdotal indication for the usefulness of our approach and benchmark in overcoming two well acknowledged challenges related to benchmarks \u2013 data leakage and debugging."}, {"title": "5 Limitations", "content": "We believe our methodology could apply to multiple tasks and programming languages (PLs). However, in the paper we present only a single task and programming language. We need to implement more benchmarks from a variety of code repos, for a variety of tasks and PLs.\nThe ability to create a debugging dictionary may depend on the task and LLMs tested. It might also depend on the level of details provided in the prompts. We need to test our approach when implementing benchmarks with varying levels of detail.\nIn the work presented here we relied solely on dynamic code execution metrics. Relying on such metrics may be too strong a requirement and it would be good to add static metrics."}, {"title": "A Appendix A: Code Examples", "content": "Below is an example of the full list of the low-level instructions the models were asked to implement. The instructions always start with the function name and parameters, followed by the list of local variables (remnant of C/C++ in the AST) and then the instructions themselves.\nThe model was asked to take a C-style while loop, which is uncommon in Python where for-each is prevalent, and implement it in Python. The loop updates var7; it also has a condition which continues to the next iteration.\nIn this case, the model correctly implemented the update for the loop itself, but forgot to do the same before the continue keyword. This type of error happened with most models. It is interesting to note that although there are 7 programs in the data-set with the continue keyword, the models got this wrong 1-3 times and correct the rest of the times.\nTo save space, for the rest of the examples we will show partial snippets of both instructions and resulting code.\nAnother common error source is ASCII handling. Here the model is asked to implement a function which operates on the integer representation of ASCII chars. It is asked to create a utility method that add/subtracts from a char returning the new char and then use it.\nThe assignment was inverse than the requirement. It is clear that the model understand the sentence assign x to y as y = x since it does so correctly most of the time in this and other programs, however, there are a few cases where this goes wrong.\nA different kind of error, ignoring part of or the whole instruction, is also common. Below is an example which might be explained by the model training data. The model skips implementing the update of var6.\nIf the instructions changing the value of the same variable are separated by a few other instructions, e.g. in this case the new var6 initialization pushed up a couple of instructions, then the model correctly implements the whole sequence. This might be caused by the fact that typically programmers do not define a variable and immediately on the next row update it.\nSometimes models are trying so \"hard\" to be helpful that they insert parts of instructions, instructions or even whole pieces of code that they weren't asked to or even specifically told not to. For example, llama models have a tendency to add code asking for user input."}, {"title": "B Appendix B: The Prompt", "content": "The general instructions to the models started from the simplest possible instruction, where LANGUAGE was Python.\nImplement the following pseudocode in LANGUAGE.\nHowever, that was not enough for any of the models. They would not implement most of the function calls most of the time, with the exception of GPT which frequently implemented them, or at least said they should be implemented. Therefore, this was added:\nThe implementation should cover all aspects of the provided pseudocode, leaving no functions or functionality unimplemented.\nThis was enough for gpt, but all other models required more detailed instructions. Since they all seem to understand * as any text we defined these:\nReplace all array methods with LANGUAGE list operations\nReplace all string_* methods, substring and concat with LANGUAGE string operations.\nSome instructions are very specific to a certain issue that was common: Reminding the model to update loop variable reduced those errors by half. Telling it to leave containers empty unless specifically requested reduced some of the hallucinations. The instruction to declare global variables outside the function helped granite models and llama-2, but hurt llama-3 badly. Llama-3 would randomly choose a declared variable and place it in global scope without properly declaring it. Removing that instruction almost completely solved the issue.\nUpdate loop variable before issuing the \"continue\" keyword.\nGlobal variables must be outside the function.\nUnless specifically requested, initialization is to an empty container.\nThe end result is this set of instructions. These were part of the prompt with which we ran the tests. Additional model-specific template related text had to be added for llama and granite."}, {"title": "C Appendix C: Common Errors", "content": "The explanation of the errors is as follows:\n\u2022 loop: Usually missing loop variable update, mostly resulting in infinite loops (or array OOB); sometimes misplaced loop variable update\n\u2022 ignored: Some instruction, or part of, was not implemented\n\u2022 wrong: The implementation is different than required, e.g. reversed assignment, used wrong variable. Could also call this one hallucinations.\n\u2022 ASCII: Errors interpreting strings as ints as done in C/C++, mostly missing ord() or chr() in some of the program instructions\n\u2022 unbalanced: The most common parsing/compilation error, unbalanced parenthesis\n\u2022 division: Several tests require integer results from division requiring a special operator\n\u2022 indent: Using the wrong indentation for the line of code\n\u2022 split: Many tests require splitting a string (by whitespace) into an array often misinterpreted by the model. Often split is actually redundant.\n\u2022 global: If modifying a global variable it needs to be declared; this is sometimes missing\n\u2022 other: Any error that doesn't fit the common categories, e.g. running out of tokens"}]}