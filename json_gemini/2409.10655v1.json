{"title": "Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning", "authors": ["Daniel Fl\u00f6gel", "Marcos G\u00f3mez Villafa\u00f1e", "Joshua Ransiek", "S\u00f6ren Hohmann"], "abstract": "Autonomous mobile robots are increasingly employed in pedestrian-rich environments where safe navigation and appropriate human interaction are crucial. While Deep Reinforcement Learning (DRL) enables socially integrated robot behavior, challenges persist in novel or perturbed scenarios to indicate when and why the policy is uncertain. Unknown uncertainty in decision-making can lead to collisions or human discomfort and is one reason why safe and risk-aware navigation is still an open problem. This work introduces a novel approach that integrates aleatoric, epistemic, and predictive uncertainty estimation into a DRL-based navigation framework for uncertainty estimates in decision-making. We, therefore, incorporate Observation-Dependent Variance (ODV) and dropout into the Proximal Policy Optimization (PPO) algorithm. For different types of perturbations, we compare the ability of Deep Ensembles and Monte-Carlo Dropout (MC-Dropout) to estimate the uncertainties of the policy. In uncertain decision-making situations, we propose to change the robot's social behavior to conservative collision avoidance. The results show that the ODV-PPO algorithm converges faster with better generalization and disentangles the aleatoric and epistemic uncertainties. In addition, the MC-Dropout approach is more sensitive to perturbations and capable to correlate the uncertainty type to the perturbation type better. With the proposed safe action selection scheme, the robot can navigate in perturbed environments with fewer collisions.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous mobile robots are increasingly being deployed in a variety of public pedestrian-rich environments, e.g. pedestrian zones for transport or cleaning [1]-[5], while DRL-based approaches are increasingly used [6]\u2013[9]. These robots not only have socially aware behavior but also become socially integrated to reduce the negative impact on humans [8]. However, there are two fundamental challenges. The first arises through the variety and a priori unknown scenarios caused by the stochastic behavior of humans [10, 11]. The second through the general problem of machine learning-based systems to predict correct actions in unseen scenarios [12, 13]. Since the robot has to choose an action in each step, one risk is a high level of uncertainty in the decision-making process at action selection [14]. As a consequence, the safe and risk-aware social navigation to avoid collisions and human discomfort is still an open problem [7, 15].\nThe uncertainty in the robot's decision-making is affected by scenarios the robot has never experienced in training, Out-Of-Distribution (OOD) scenarios, and due to the inherent uncertainty of the environment like sensor noise or occlusion [16]. Thus, the DRL policy will face epistemic uncertainty (model uncertainty), which correlates with OOD, and aleatoric uncertainty, which stems from stochastic environments, e.g. sensor noise or perturbations on selected actions [17]. For a DRL policy, it is essential to estimate both types as it is expected that the robot flags anomalous environmental states when it does not know which action to choose to account for potential risks [17,18]. Thus, disentangling aleatoric and epistemic uncertainty is crucial for a DRL policy towards safe and risk-aware behavior [18]. However, there is less literature for uncertainty aware DRL, compared to supervised learning [19], and only a few works address the resulting risks in navigation [20].\nMost DRL-based navigation approaches, e.g. [8, 21]-[25], do not consider OOD scenarios or noise which is a significant risk since the OOD performance of recent approaches is bad as evaluated in [7]. For known input perturbations, a certified adversarial robustness analysis framework for DRL is proposed in [14]. However, the perturbation must be known a priori, and it only applies to DRL approaches with discrete actions. Other recent works make use of the uncertainty in the human trajectory prediction [2,9] or propose to detect novel scenarios [13], but neglect the uncertainty in decision-making which is the crucial part [14]. In addition, [13] intentionally replaced PPO [26] with Soft Actor-Critic (SAC) [27] because standard implementations, e.g. [28], of PPO do not have state dependant variance of policy distribution and it is additionally challenging for PPO to train uncertainty-aware behavior while having good exploration-exploitation trade-off. However, many social navigation approaches rely on the PPO algorithm due to the stable training and good performance [5, 8, 9, 16, 29]\u2013[41].\nWe see the clear limitation that uncertainty in decision-making is neither considered nor the source determined."}, {"title": "II. BACKGROUND", "content": "A. Uncertainty Estimation in Machine Learning\nEstimating the uncertainty can be distinguished into sample-free and sample-based methods, e.g., MC-Dropout or Ensembles, which require multiple forward passes [42]. Ensemble methods consist of a set of multiple models trained with different weight initializations. The variance of the different model predictions can be considered as Bayesian uncertainty estimates [42]. MC-Dropout randomly drops neurons during training and testing to form different models and combine their predictions as Bayesian uncertainty estimation [43]. The uncertainty estimates are further distinguished between the aleatoric, epistemic, and predictive uncertainty [44, 45]. Aleatoric uncertainty arises from the stochastic nature of the environment and can not be reduced. The three main sources in DRL are stochasticity in observation, actions, and rewards [19]. Epistemic uncertainty arises from limited knowledge gathered in training and accounts for the lack of knowledge of a policy but can be reduced with more training samples [19,42]. Predictive uncertainty summarises the effects of aleatoric and epistemic uncertainty [44]. In decision making, aleatoric and epistemic uncertainty are disentangled for the DQN algorithm in [17, 18] but not adapted for actor-critic algorithms with continuous action space such as PPO [26] which is challenging [13].\nB. Safety in Social Navigation\nDRL-based social navigation approaches can be categorized based on the robot's exhibited social behavior into social collision avoidance with a lack of social aspects, socially aware approaches with a predefined social behavior, and Socially Integrated (SI) where the robot's behavior is adaptive to human behavior and emerges through interaction [8]. Early works proposed a Probability of Collision (POC) distribution in combination with a model-predictive controller (MPC) for safe and uncertainty-aware action selection [46, 47]. [47] uses MC-Dropout and bootstrapping to estimate the POC distribution for a risk-aware exploration in model-based DRL. An ensemble of LSTMs is used in [46] in combination with MC-Dropout and bootstrapping to estimate the distribution. A risk function is proposed in [5] to capture the POC to prioritize humans with a higher risk of collision. However, a learned POC can be uncertain and does not reflect the policy uncertainty in the action selection. Other approaches use the uncertainty in human trajectory prediction for risk-aware planning [2, 9]. Such approaches are highly susceptible to the stochasticity of noise and the unobserved intentions of the external agents, which is addressed in [48] with a model-based DRL approach by estimating the aleatoric uncertainty of the trajectory prediction. A risk-map-based approach with human position prediction and probabilistic risk areas instead of hard collision avoidance is proposed in [15] to address dynamic human behavior and static clutter. Safety zones around humans are proposed in [24,49,50] to increase the minimum distance between the robot and humans. To overcome the problem of occluded humans, the social inference mechanism with a variational autoencoder to encode human interactions is incorporated in [16]. A risk-conditioned distributional SAC algorithm that learns multiple policies concurrently is proposed in [20]. The distributional DRL learns the distribution over the return and not only the expected mean, and the risk measure is a mapping from return distribution to a scalar value. Other works estimate uncertainty from environmental novelty [51], which does not translate to policy uncertainty. A resilient robot behavior for navigation in unseen uncertain environments with collision avoidance is addressed in [13]. An uncertainty-aware predictor for environmental uncertainty is proposed to learn an uncertainty-aware navigation network in prior unknown environments.\nTo our knowledge, no existing model-free DRL approaches consider the uncertainty in action selection and distinguish between epistemic and aleatoric uncertainty, which is crucial for safe and risk-aware decision-making."}, {"title": "III. PRELIMINARIES", "content": "Throughout this paper, a dynamic object in the environment is generally referred to as an agent, either a robot or a human, and a policy determines its behavior. Variables referred to the robot are indexed with $x^r$, and humans with $x^h_i$ with $i \\in 1,.\u2026 N - 1$. A scalar value is denoted by $x$ and a vector by $\\mathbf{x}$.\nA. Problem Formulation\nThe navigation task of one robot toward a goal in an environment of $N - 1$ humans is a sequential decision-making problem and can be modeled as Partially Observable Markov Decision Process (POMDP) [8, 10, 52, 53] and solved with a DRL framework [54]. The POMDP is described with a 8-tuple $(S, A, T, O, \\Omega, T_0, R, \\gamma)$. We assume the state space $S$ and action space $A$ as continuous. The transition function $T: S \\times A \\times S \\rightarrow [0, 1]$ describes the probability transitioning from state $s_t \\in S$ to state $s_{t+1} \\in S$ for the given action $a_t \\in A$. With each transition, an observation $o_t \\in O$ and a reward $R: S \\times A \\rightarrow \\mathbb{R}$ is returned by the environment. The observation $o_t$ is returned with probability $P(o_t | s_{t+1}, a_t, s_t)$ depending on the sensors. The initial state distribution is denoted by $T_0$ while $ \\gamma \\in [0,1)$ describes the discount"}, {"title": "IV. APPROACH", "content": "This section describes the incorporated uncertainty measurements and the uncertainty-aware action selection for collision avoidance. First, we introduce ODV into the PPO algorithm. Subsequently, we perform uncertainty estimations with MC-Dropout and Deep Ensembles within the DRL network. Finally, we provide a Probability of Collision (POC) estimation based on the uncertainty measurements for risk-aware action selection in novel and perturbed environments.\nA. Observation-Dependent Variance\nIn standard implementations of PPO, e.g. Stables Baselines3 [28], the actor outputs multivariate Gaussian distributed actions $\\mathcal{N}(\\mu_a(o_t), \\sigma^2)$ with an observation dependent mean but a parameterized variance, independent from observation. An observation-independent variance does not allow for aleatoric uncertainty estimation of the action selection process [13]. We, therefore, adapt the actor network and incorporate a linear layer that outputs the observation-dependent variances for the action as $\\log(\\sigma_a)$. This leads to a multivariate Gaussian distributed policy $\\mathcal{N}(\\mu_a(o_t), \\sigma^2(o_t))$ where both the mean and variance of the policy depend on the observation. However, using ODV leads to stability problems during training. First, the variance can be arbitrarily large, which can cause the normal distribution to lose its shape, become a uniform distribution, and disrupt learning because actions are sampled randomly. Second, policy updates can lead to high variances at the late stages of training, which, by sampling, may lead to a sequence of poor actions, a series of bad updates, and, finally, performance collapse. We propose to use variance clamping for the first problem to prevent the deformation of the normal distribution by limiting the maximum variance. A grid search for the clamping values was done, and the highest maximum clamping value that did not cause collapse and still resembled a normal distribution was chosen. The second problem is addressed by modifying the PPO loss and incorporating the Mean Squared Error (MSE) loss of the variance\n$\\mathcal{L}_{\\sigma} (\\pi) = \\lambda_0 \\cdot \\frac{1}{B} \\sum_{i=1}^B \\frac{1}{2} \\cdot \\bigg \\| \\frac{\\mu_{\\sigma^i_{\\theta}}}{\\sigma_{\\theta}} - \\frac{\\mu_{\\sigma^i_{\\Delta \\theta}}}{\\sigma_{\\Delta \\theta}} \\bigg \\|^2$ (4)\nwhere $\\lambda_0$ accounts for exploration-exploitation trade-off with the current timestep $t$ and the total training timesteps $T$ and batch size of $B$. The constant $\\zeta$ scales the variance loss to be proportional to the policy loss and assigns importance to the variance loss and the MSE loss of the variance with respect to zero. This leads to total policy loss\n$\\mathcal{L}_{\\pi} (\\pi) = \\mathcal{L}_{CLIP}(\\pi) + \\mathcal{L}_{\\sigma}(\\pi)$ (5)\nwhere $\\mathcal{L}_{CLIP}(\\pi)$ is the PPO clipping loss. We refer to the full model as Observation-Dependent Variance PPO (ODV-PPO).\nB. Uncertainty Estimation\n1) MC-Dropout: We include dropout for approximating Bayesian inference in deep Gaussian processes during testing, named MC-Dropout [43], in two network sections. In the 2-layer LSTM features extractor and in the hidden layers of the actor-network, as depicted in Fig. 2. The critic network does not contain a dropout to avoid instabilities in the target value [45]. The policy is first trained with dropout probability $P_{train}$, and in testing, the samples are drawn with a higher dropout $P_{test}$ to estimate the uncertainty. We sample $K$ independent policy predictions with dropout of the action distribution $\\mu_k, \\sigma_k = \\pi(\\cdot|o_t)$ for an observation at time step $t$. Subsequently, we calculate the uncertainties as proposed in [18] from the individual samples. The epistemic uncertainty is estimated with the variance of the means, with\n$U^E_\\pi = V[\\mu] \\approx \\frac{1}{K} \\sum_{k=1}^K (\\mu_t - \\mu_k)^2$ (6)"}, {"title": "V. EVALUATION", "content": "In this section, we evaluate and compare the uncertainty measures in different scenarios and for different sources of perturbations. In addition, we benchmark our approach against the uncertainty unaware SI approach proposed in [8], which is only trained with PPO.\nA. Experimental Setup\nWe use the gymnasium environment from our previous work in [8] to train and evaluate the navigation policies. The ODV-PPO based policy and the uncertainty measurements are implemented in Stables Baselines3 [28] using the PPO hyperparameters from [8] and the newly introduced hyperparameters from Table I.\nWe consider two scenario configurations, a position swap scenario where the human position is randomly sampled on a circle with 7m radius and a heterogeneous interactive circle crossing scenario as in [8] where each human has its individual proxemic radius. All policies are trained in a position swap scenario with one human. The agent radius is $r = 0.3\\,m$ and the human proxemic radius is sampled with $r^{prox} \\sim \\mathcal{U}(0.3, 0.4)$. However, they are evaluated in both scenarios for 100 episodes per experiment. In addition, to analyze the uncertainty measurements, the position swap scenario is systematically expanded with perturbations, noise, and more humans. We stimulate aleatoric uncertainty with observation and action noise and epistemic uncertainty with an increased velocity of humans and an increased number of humans. Observation noise is modeled with additive Gaussian noise $\\mathcal{N}(0, I \\cdot \\sigma_{obs})$. Action noise for the heading is modeled with additive Gaussian noise $\\mathcal{N}(0, \\sigma_{head})$ and the velocity is scaled with a uniformly sampled factor $\\mathcal{U}(1 - \\sigma_{vel}, 1)$ with $\\sigma_{vel} < 1$ to simulate terrain grip and avoid actions with negative velocity. The epistemic uncertainty is introduced through scaling the preferred velocity $v^{pref}_{vel}$ of humans and adding multiple humans into the environment to extend the position swap scenario into a circle-crossing scenario.\nB. Results\n1) Training and Generalization: Training the navigation policy with dropout and ODV-PPO leads to a faster convergence to a higher reward in comparison to PPO and PPO with dropout, throughout 10 seeds. To evaluate the capability to generalize and handle perturbed and OOD environments, we compare the ODV-PPO policy and our full approach with uncertainty aware safe action selection against the SI [8] approach in Table II. The position swap scenario is perturbed with an offset $\\mathcal{U}(0, 0.5)$ to the start and goal position, the agent radius is $r^r = \\mathcal{U}(0.3, 0.6)$, and the human proxemic radius is $r^{prox} = \\mathcal{U}(0.3, 0.8)$. In the perturbed environment, the ODV-PPO approach always reaches the goal and has a higher average return. In the OOD scenario, circle crossing, both approaches cause many collisions and have low returns. Our proposed full model always reaches the goal and receives the highest average return.\n2) Disentangling Uncertainty Estimation: We evaluate the MC-Dropout and Deep Ensemble approach with different perturbation sources and strengths and estimate the uncertainties in each step. Observation and action noise is $\\sigma_{head}, \\sigma_{vel}, \\sigma_{obs} \\in [0, 0.2, ..., 2]$, velocity scaling factor is $I_{vel} \\in [1, ..., 8]$, and new humans are $N \\in [2, .., 7]$. The estimated aleatoric and epistemic uncertainties are depicted in Fig. 3. For both approaches, the results show that the velocity uncertainty is low and flat compared to the heading uncertainty, which increases approximately linearly, and new humans cause the highest uncertainty. In addition, the MC-Dropout approach is capable of disentangling the source of the uncertainty, but we also observed that a proper dropout rate $P_{test}$ is crucial for this capability. In contrast, the deep ensemble approach always has high epistemic uncertainty and cannot distinguish between aleatoric and epistemic uncertainty. The results are also aggregated in Table III for the normalized rate of change of the uncertainties for the perturbation. The normalized value is $\\frac{\\Delta \\mathcal{U}}{\\Delta \\sigma} = \\frac{\\mathcal{U}_{\\sigma(max)} - \\mathcal{U}_{\\sigma(0)}}{\\sigma(max) - \\sigma(0)}$ with the maximum perturbation strength $\\sigma(max)$, the mean uncertainty $\\mathcal{U}_{\\sigma(max)}$ per strength, and the mean uncertainty $\\mathcal{U}_{\\sigma(0)}$ in the unperturbed environment of the episodes. To analyze the trend of uncertainty as the robot approaches and interacts with the human, the mean uncertainty $\\mathcal{U}_w$"}, {"title": "VI. CONCLUSIONS", "content": "This paper incorporated and disentangled epistemic, aleatoric, and predictive uncertainty measurements in a DRL network with a safe action selection for perturbation robustness in social navigation. We integrated observation-dependent variance and dropout into the PPO algorithm with adaptions in the action network and loss function. This leads to a better generalization with faster convergence and enables disentangling the aleatoric and epistemic uncertainty in decision-making. The results show that the MC-Dropout-based approach is superior for uncertainty estimation and, in combination with the proposed safe action selection, can avoid more collisions than the ensemble approach. In addition, the predictive uncertainty in perturbed environments can be high, although the robot is sure about selecting the action. Consequently, the uncertainty in the decision-making must be considered to detect when and why the robot is uncertain. Future works develop sample-free methods."}]}