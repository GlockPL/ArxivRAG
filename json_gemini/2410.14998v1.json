{"title": "A COMPARATIVE STUDY OF NEURALODE AND UNIVERSAL ODE APPROACHES TO SOLVING CHANDRASEKHAR'S WHITE DWARF EQUATION", "authors": ["Raymundo Vazquez Martinez", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "abstract": "In this study, we apply two pillars of Scientific Machine Learning: Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs) to the Chandrasekhar White Dwarf Equation (CWDE). The CWDE is fundamental for understanding the life cycle of a star, and describes the relationship between the density of the white dwarf and it's distance from the center. Despite the rise in Scientific Machine Learning frameworks, very less attention has been paid to the systematic applications of the above SciML pillars on astronomy based ODEs. Through robust modeling in the Julia programming language, we show that both Neural ODEs and UDEs can be used effectively for both prediction as well as forecasting of the CWDE. More importantly, we introduce the \"forecasting breakdown point\" - the time at which forecasting fails for both Neural ODEs and UDEs. Through a robust hyperparameter optimization testing, we provide insights on the neural network architecture, activation functions and optimizers which provide the best results. This study provides opens a door to investigate the applicability of Scientific Machine Learning frameworks in forecasting tasks for a wide range of scientific domains.", "sections": [{"title": "1 Introduction", "content": "Scientific Machine Learning (Scientific ML) is a growing field with a wide range of applications in various fields such as epidemiology, gene expression, optics, circuit modeling, quantum circuits and fluid mechanics [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. This field of Scientific ML leverages the interpretibility of scientific structures like ODEs/PDES along with the expressivity of neural networks. Broadly, the rise of Scientific Machine Learning can be attributed to three popular methodologies:\n\u2022 Neural Ordinary Differential Equations: The entire forward pass of an ODE/PDE is replaced with neural networks. We perform backpropagation through the neural network augmented ODE/PDE. In doing so, we find the optimal values of the neural network parameters. [15, 16, 17, 18]"}, {"title": "\u2022 Universal Differential Equations (UDEs):", "content": "In contrast to Neural ODEs, only certain terms of the ODE/PDES are replaced with neural networks. We then discover these terms by optimizing the neural network parameters. Universal Differential Equations can be used to correct existing underlying ODEs/PDEs as well as to discover new, missing physics. [19, 20, 21, 22]"}, {"title": "\u2022 Physics Informed Neural Networks (PINNs):", "content": "PINNs are predominantly used as an alternative to traditional ODE/PDE solvers to solve an entire ODE/PDE. We replace the variable with a neural network and the loss function is determined by the ODE/PDE solution and the boundary conditions. When we minimize the loss function, we automatically find the optimium solution to the ODE/PDE. [23, 24, 25, 26]\nDespite the advances of Scientific ML in various fields, there is a lack of applying Scientific ML methods in the field of astronomy. Although there are a few studies aimed at applying Neural ODEs on astronomy problems [27, 28, 29], there is no study investigating the application of Universal Differential Equations (UDEs) on astronomy problems.\nIn particular, the following questions are still unanswered:\n\u2022 In the spirit of UDEs, can we replace certain terms of an astronomical ODE system with neural networks and recover them?\n\u2022 How does the Neural ODE prediction compare with the UDE prediction?\n\u2022 Can we do forecasting on the system of ODEs with Neural ODEs and UDEs?\n\u2022 Are UDEs better at forecasting than Neural ODEs?\nWe aim to answer these questions by looking at a foundational ODE in astronomy: the Chandrasekhar White Dwarf equation (CWDE) [30, 31]. The CWDE describes the relationship between the density of the white dwarf and it's distance from the center [30, 31]. This equation is fundamental for understanding the life cycle of a star. Using this equation, we can potentially predict when the star will collapse and transform into a supernova.\nWe use the advanced Scientific Machine Learning libraries provided by the Julia Programming Language [32, 33, 34, 35]. Through a robust hyperparameter optimization testing, we provide insights on the neural network architecture, activation functions and optimizers which provide the best results. We show that both Neural ODEs and UDEs can be used effectively for both prediction as well as forecasting of the CWDE ODE system. More importantly, we introduce the \"forecasting breakdown point\" - the time at which forecasting fails for both Neural ODEs and UDEs. This provides an insight into the applicability of Scientific Machine Learning frameworks in forecasting tasks.\nThe paper is structured as follows. We start by presenting the methodology and detailed description for Neural ODEs and UDEs. Subsequently, we present the prediction and forecasting results for the Neural ODEs and UDEs. We also provide hyperparameter optimization plots. Finally, we conclude with a detailed discussion of our results, and the future scope of applying Scientific ML methods in astronomy."}, {"title": "2 Methodology", "content": "According to Chandrasekhar the equation that governs the structure of degenerate matter in gravitational equilibrium is given by the second-order ordinary differential equation [36]\n$\\frac{1}{\\eta^2} \\frac{d}{d\\eta} \\left( \\eta^2 \\frac{d\\varphi}{d\\eta} \\right) + (\\varphi^2 - C)^{3/2} = 0$ (1)\nwith initial conditions\n$\\varphi(0) = 1, \\varphi'(0) = 0$\nThis equations is one of Emden type, and therefore a solution exists in the neighborhood of $\\eta = 0$ [37]. This equation exhibits the density of the White Dwarf as a function of the dimensionless radius $\\eta$. Particularly, The variables $\\eta$ and are expected to take real values due to their physical meaning. From this fact, we can entail more restrictions on the behaviour of and $\\eta$ such as their bounds\n$\\sqrt{C} \\leq \\varphi \\leq 1$,\n$0 \\leq \\eta \\leq \\eta_{\\infty}$\nMoreover, the density function is decreasing and tends the lower bound $\\sqrt{C}$ i.e\n$\\lim_{\\eta \\to \\eta_{\\infty}} \\varphi(\\eta) = \\sqrt{C}$ (2)"}, {"title": "For the computational implementations, the constant C was set to 0.01. The ODE (1), was reformulated as a system of first order ODES", "content": "$\\frac{d\\varphi}{d\\eta} = \\theta$ (3a)\n$\\frac{d\\theta}{d\\eta} = -\\frac{2 \\theta}{\\eta} - (\\varphi^2 - C)^{3/2}$ (3b)\nThe finite-length $\\eta$ interval in which Chandrasekhar's White Dwarf equation is solvable was obtained by implementing a numerical approach in the Julia programming language. For this C value, the set of valid values obtained for $\\eta$ was $D_f = {\\eta \\in R : 0.05 \\leq \\eta \\leq 5.325} = [0.05,5.325]$. Subsequently, the domain $D_f$ was discretized into 100 equally spaced $\\eta$ values. For these $\\eta$ points, the values for both $\\varphi$ and $\\varphi'$ were saved from the numerical calculation of the ODE, resulting in synthetic data characterizing the White Dwarf for this fixed C. Additionally, noise was induced into the synthetic data with varying standard deviations, resulting in different training datasets. Specifically, the standard deviations for the added noise were 7% and 35% regarding the synthetic data. These datasets were labeled as moderate-noise data and high-noise data, respectively, while the synthetic data without any added noise was labeled as no-noise data. For the training routines different subsets of these datasets were used to test the forecasting capability of the Neural Network models. Particularly, the training routines were implemented with the entire, 90%, 80%, 40%, 20%, and 10% of the mentioned datasets."}, {"title": "2.1 Neural ODES", "content": "Neural Ordinary Differential Equations (Neural ODEs) are a class of models that represent continuous-depth neural networks. Introduced by [15], Neural ODEs have opened up new possibilities in modelling continuous processes by using differential equations to define the evolution of hidden states in neural networks. Neural ODEs represent a novel approach in machine learning and computational modelling that combines neural networks with ordinary differential equations (ODEs). Neural ODEs are a subset of the broader spectrum of Scientific Machine Learning and Physics Informed Machine Learning. The key idea behind Neural ODEs is to use a neural network to approximate the solution of an ODE, thereby allowing for flexible modelling of continuous-time dynamics [38, 39, 40, 41, 42, 43].\nIn a traditional neural network, hidden states are updated using discrete layers. In contrast, Neural ODEs use a continuous transformation defined by an ordinary differential equation:\n$\\frac{dh}{dt} = f(h(t), t, \\theta)$ (4)\nwhere,\n\u2022 h(t) is the hidden state at time t.\n\u2022 f is a neural network parameterized by $\\theta$.\n\u2022 The hidden state evolves according to the function f.\nOne crucial aspect of a Neural ODE model (and machine learning models in general) is hyper-parameter tuning. In this work, the selection of the parameters of the model were obtained after a robust search over a range of possible values specific to the training data used."}, {"title": "2.2 UDES", "content": "UDES (Universal Differential Equations) introduced by [19], combine traditional differential equations with machine learning models, such as neural networks, to create a more flexible and powerful tool for modelling complex systems. This approach integrates the robustness of classical differential equations with the adaptability of neural networks, allowing for more accurate and efficient modelling of systems with unknown or partially known dynamics. UDEs offer improved predictive power by combining data-driven approaches with physical laws [44, 45, 46, 47]. This is particularly useful in scenarios where purely data-driven models might overfit or fail to generalize. The physical laws embedded in UDEs constrain the learning process, ensuring that the model adheres to known scientific principles. Compared to purely data-driven models, UDEs often require fewer data points to achieve high accuracy. The known differential equations provide a strong prior that guides the learning process, reducing the amount of data needed for training. This efficiency makes UDEs suitable for applications with limited data availability. The UDE model for the Chandrasekhar's White Dwarf equation defined in this work, employed the linear @ terms in (3) as the ground truth model or physical law, as shown in equation (5)\n$\\frac{d\\varphi}{d\\eta} = \\theta + NN_1(P,U)$ (5a)\n$\\frac{d\\theta}{d\\eta} = -\\frac{2 \\theta}{\\eta} + NN_2(P,U)$ (5b)\nWhere P are the parameters of the Neural Network (NN) architecture, and U = $(\\varphi(\\eta), \\varphi'(\\eta))$ are the input parameters. The performance of the trained UDE model can be observed further from the recovered interaction or missing term in the original ODE model (3), i.e $NN_1(P_{trained}, U)$ and $NN_2(P_{trained}, U)$\nHyper-parameter tuning is a crucial aspect of the UDE model (and machine learning models in general). In this work, the model's parameters were selected after a robust search over a range of possible values specific to the dataset used for training. Regarding the entire no-noise dataset"}, {"title": "3 Results", "content": "Six cases were considered for the training process of the deep learning-based models:"}, {"title": "3.1 Case 1: Training in the full domain (100 \u03b7 points)", "content": "First, the implementation of the Neural ODE and UDE models for the Chandrasekhar's White Dwarf equation were performed in the full domain. The three datasets were implemented: no-noise, moderate-noise and high-noise data."}, {"title": "3.2 Case 2: Training with 90% of the full available data and forecasting", "content": "The Neural ODEs and UDEs were trained to evaluate their forecasting capabilities with subsets of the previous no-noise, moderate-noise, and high-noise training datasets. The $\\varphi$ and $\\varphi'$ values corresponding to the first 90 \u03b7 points from the previous datasets were employed for training. The remaining 4 and $\\varphi'$ values corresponding to the last 10 \u03b7 points were used as testing data to evaluate the UDE forecasting capability in this range."}, {"title": "3.3 Case 3: Training with 80% of the full available data and forecasting", "content": "The Neural ODEs and UDEs were trained with smaller data subsets to further evaluate their forecasting capabilities. The previous subsets of no-noise, moderate-noise, and high-noise training datasets were trimmed, forming new training subsets including the $\\varphi$ and $\\varphi'$ values corresponding to the first 80 \u03b7 points of the full domain."}, {"title": "3.4 Case 4: Training with 40% of the full available data and forecasting", "content": "The Neural ODEs and UDEs were trained with smaller data subsets to further evaluate their forecasting capabilities. The previous subsets of no-noise, moderate-noise, and high-noise training datasets were trimmed, forming new training data subsets with the $\\varphi$ and $\\varphi'$ values corresponding to the first 40 \u03b7 points of the full domain."}, {"title": "3.5 Case 5: Training with 20% of the full available data and forecasting", "content": "The Neural ODEs and UDEs were trained with smaller data subsets to further evaluate their forecasting capabilities. The previous subsets of no-noise, moderate-noise, and high-noise training datasets were trimmed, forming new training data subsets including the $\\varphi$ and $\\varphi'$ values corresponding to the first 20 \u03b7 points of the full domain."}, {"title": "3.6 Case 6: Training with 10% percent of the full available data and forecasting", "content": "The Neural ODEs and UDEs were trained with smaller data subsets to further evaluate their forecasting capabilities. The previous subsets of no-noise, moderate-noise, and high-noise training datasets were trimmed, forming new training data subsets including the $\\varphi$ and $\\varphi'$ values corresponding to the first 10 \u03b7 points of the full domain."}, {"title": "4 Discussion and Conclusion", "content": "We successfully approximated the underlying data for Chandrasekhar's white dwarf equation (CWDE) with a fixed parameter C using a trained Neural Ordinary Differential Equation (Neural ODE) model with both noiseless and noisy data. A comprehensive study was conducted to identify favorable hyperparameters and neural network architectures. Ultimately, the combination of ADAM and BFGS optimizers, the tanh activation function, and a streamlined neural network architecture synergistically contributed to a significant improvement of the models' performance (loss reduction). For all datasets (no noise, moderate noise, and high noise), the Neural ODE model effectively approximated the training data. In terms of forecasting, the model performed well in predicting unseen data when trained with at least 80% of the available data. However, the Neural ODE model breaks down and fails to predict the unseen data when trained with less than 40% of the available datasets, indicating that while Neural ODEs offer easier modeling without relying on physical knowledge, they require a substantial amount of data to maintain forecasting reliability.\nIn contrast, Universal Differential Equations (UDEs) excelled at identifying and recovering the missing term for the CWDE, minimizing the training loss across all datasets. A comprehensive study was conducted to identify favorable hyperparameters and neural network architectures. Ultimately, the combination of ADAM and BFGS optimizers, the Sigmoid activation function, and a streamlined neural network architecture synergistically contributed to a significant improvement of the models' performance (loss reduction). UDEs demonstrated superior performance in data-scarce situations, successfully forecasting for all testing values of the dimensionless radius \u03b7 even when trained with just 20% of the noiseless training data. However, similar to the Neural ODE, the UDE model fails to forecast unseen data when trained on less than 40% of the available datasets.\nThe UDE model's ability to perform well with as little as 20% of noise-free data showcases its efficiency in data-scarce environments, which is particularly valuable in astrophysics where large datasets can be challenging or expensive to obtain. This robustness can be beneficial for simulations in astrophysics due to the UDE efficiency in computational resources. Furthermore, astrophysical data often contain noise from different sources such as instrumental errors, atmospheric disturbances, cosmic rays, and background light. While most of this noise is impossible to avoid, UDE models can be employed in future investigations to model such noise, encoding it using their Neural Network component. This capability could lead to the identification of noise sources and uncover unknown interactions within the astrophysical system.\nThe UDE model's ability to recover missing interactions or contributions from the training data in this project highlights its potential in the data-driven discovery of missing physics. This capability is crucial in astrophysics, where the exact form of governing equations might not be fully known due to incomplete theories or observational limitations. In this regard, UDEs also provide a valuable tool for theoretical advancements, refining physical laws, and testing hypotheses, therefore offering a new way to investigate the physics of the cosmos.\nBeyond their modeling capabilities, The UDE demonstrated strong forecasting power for the CWDE. This suggests that UDEs can enhance the accuracy of predictive models in astrophysics research by leveraging both physical laws (in the form of differential equations) and data-driven corrections (via neural networks). This hybrid approach could improve forecasts of astrophysical events and behaviors, such as stellar evolution, black hole dynamics, or the behavior of neutron stars and white dwarfs. Although the UDE model failed when noise was added in the data-scarce scenario (40% of the available datasets), its performance with noise-free data suggests that it can be highly effective in controlled experimental settings or simulations, even with low data availability.\nApplications beyond white dwarfs can be explored, including modeling cosmic ray propagation, understanding galactic dynamics, and simulating accretion processes around black holes. Their versatility makes them a powerful tool for exploring various unsolved problems in astrophysics, offering computational efficiency and effectiveness when data is limited.\nHowever, UDEs require a well-known physical model to fully leverage their learning power, whereas Neural ODEs are advantageous when no physical law is available, providing a preferred approach in such scenarios.\nOverall, the efficiency of UDEs in learning from data and refining model parameters suggests their broad applicability across various scientific and engineering domains, especially those where acquiring data or processing is computationally expensive.\nIn conclusion, while both Neural ODEs and UDEs effectively capture and predict complex dynamics over shorter intervals, their accuracy and reliability can decline over extended time spans. This underscores the need for continuous model validation and potential adjustments or enhancements to improve long-term forecasting capabilities. The Neural ODE approach offers a black-box-like solution for forecasting and phenomenological modeling when no ground truth is known, but this advantage is contrasted by its need for larger data availability. The UDE model overcomes the problem of extensive data requirement but necessitates a well-known physical model to leverage its learning power. Looking ahead, as scientific machine learning methods are further investigated, a significant focus needs to be placed on forecasting. Most studies in the literature are aimed towards predictions. While the predictive power of SciML methods has been reliably demonstrated, as shown in this study, there are still uncertainties about reliable long-term forecasting. In future work, we will modify the employed SciML models to ensure better forecasting performance, and apply symbolic regression to the recovered terms to discover the symbolic formulations of the terms recovered."}, {"title": "A Hyperparameters for the training datasets", "content": ""}, {"title": "A.1 Case 1: Training with 100% of the available data.", "content": ""}]}