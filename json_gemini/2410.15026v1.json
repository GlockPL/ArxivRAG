{"title": "A Recommendation Model Utilizing Separation\nEmbedding and Self-Attention for Feature Mining", "authors": ["Wenyi Liu", "Rui Wang", "Yuanshuai Luo", "Jianjun Wei", "Zihao Zhao", "Junming Huang"], "abstract": "With the explosive growth of Internet data, users\nare facing the problem of information overload, which makes it a\nchallenge to efficiently obtain the required resources.\nRecommendation systems have emerged in this context. By\nfiltering massive amounts of information, they provide users with\ncontent that meets their needs, playing a key role in scenarios\nsuch as advertising recommendation and product\nrecommendation. However, traditional click-through rate\nprediction and TOP-K recommendation mechanisms are\ngradually unable to meet the recommendations needs in modern\nlife scenarios due to high computational complexity, large\nmemory consumption, long feature selection time, and\ninsufficient feature interaction. This paper proposes a\nrecommendations system model based on a separation\nembedding cross-network. The model uses an embedding neural\nnetwork layer to transform sparse feature vectors into dense\nembedding vectors, and can independently perform feature cross\noperations on different dimensions, thereby improving the\naccuracy and depth of feature mining. Experimental results show\nthat the model shows stronger adaptability and higher prediction\naccuracy in processing complex data sets, effectively solving the\nproblems existing in existing models.", "sections": [{"title": "I. INTRODUCTION", "content": "In the context of the rapid development of science and\ntechnology today, the surge in the amount of Internet data has\nled to amounts of information overload, which has brought\nchallenges to users in efficiently obtaining the required\nresources [1]. With the advancement of science and technology,\nthe way of obtaining information has also undergone\ntremendous changes from traditional letters and newspapers to\nthe modern Internet [2]. More and more individuals tend to\nobtain information through the Internet platform, and the\nemergence of recommendation systems plays a key role in this.\nIt can effectively filter massive information and provide users\nwith content that matches their needs [3]. Traditional\nrecommendation systems, while valuable, face numerous\nchallenges in the modern context where data is complex,\ndynamic, and vast in scale. Specific issues such as high-latency\nresponses, poor scalability, and limited adaptability to real-time\ndata updates render these traditional systems less effective in\nrapidly evolving domains like e-commerce and video streaming\nplatforms. Additionally, traditional click-through rate\nprediction and TOP-K recommendation mechanisms often\nsuffer from high computational complexity, large memory\nconsumption, long feature selection time, and insufficient\nfeature interaction, further limiting their applicability in real-\nworld scenarios [4].\nOn this basis, this study proposes a recommendation system\nmodel based on a separation embedding cross-network, which\naims to solve the problem that features cross technology in\nexisting deep learning recommendation models fail to fully\nmine embedded vector information and is insufficient in\naccuracy [5]. The proposed model utilizes an embedding neural\nnetwork layer to transform high-dimensional, sparse feature\nvectors into low-dimensional, dense embedding vectors. This\ntransformation allows the model to efficiently encode complex\nfeature relationships by reducing the dimensionality of input\ndata while preserving essential information. Specifically, the\nembedding neural network layer assigns trainable weights to\neach feature, mapping them into a continuous vector space,\nwhere the previously sparse features can now interact more\nrobustly. This transformation facilitates deeper and more\nprecise feature interactions in subsequent layers, particularly\nduring the independent feature cross operations across different\ndimensions [6]. Finally, the hidden layer matrices are summed\nand pooled, and the final recommendation result is obtained\nthrough the prediction layer [7]. The experimental results show\nthat compared with the six popular typical click-through rate\nprediction models and five recommendation schemes based on\ngraph neural networks, the performance of this model on three\npublic datasets is better, specifically in terms of higher AUC\nvalues, accuracy, and recall [8].\nIn addition, this study also introduces a recommendation\nmodel design based on the self-attention mechanism [9]. The"}, {"title": "II. METHOD", "content": "At the beginning of the experimental design, we first\npreprocessed the data. Preprocessing includes data cleaning,\nstandardization, normalization and feature engineering. Data\ncleaning is mainly to remove missing values or outliers to\nensure\nthe quality of the data. Standardization and\nnormalization are to make the data on the same scale to avoid\nunstable model training due to too large or too small a value\nrange [13]. Feature engineering refers to transforming,\ncombining or deriving new features from the original data to\nenhance the expressiveness of the model [14]. For example,\nnew features can be generated by counting the user's historical\nbehavior, such as user activity, preference type, etc. This series\nof operations can be summarized as:\n$X'= Preprocess (X)$\n$X$ represents the original dataset, while $X'$ is the\npreprocessed dataset. The preprocessing step ensures that the\nmodel can be trained on a high-quality dataset, thereby\nimproving the accuracy of the recommendation. After data\npreprocessing, we use the embedding layer to map the sparse\nfeature vector X to a dense embedding vector space. The role\nof the embedding layer is to convert high-dimensional sparse\nfeatures into low-dimensional dense vectors to facilitate\nsubsequent feature crossover operations. This step can be\nformally expressed as:\n$E(X') = W_{emb}\\cdot X'+b_{emb}$\nAmong them, $W_{emb}$ is a trainable weight matrix used to\nencode each feature into a vector of fixed length, and $b_{emb}$ is a\nbias term. The introduction of the embedding layer not only\nreduces the dimensionality difference between features, but\nalso enhances the model's ability to capture complex\nrelationships between features, so that subsequent feature\ncross-operations can more accurately reflect the user's real\nneeds. After completing feature embedding, we introduced the\nSeparate Embedding Cross Network, the core idea of\nwhich is to perform independent cross operations on features in\ndifferent dimensions and explicitly control the depth of feature\ncrossover by adjusting the number of network layers. The\nrationale for separating feature cross operations across different\ndimensions lies in its ability to explicitly control the\ncomplexity of interactions between features. Traditional\nmethods often treat all features equally, applying a single cross\noperation across the entire feature set. In contrast, by isolating\nfeature interactions in specific dimensions, our approach\nreduces noise and enhances the model's ability to identify\nmeaningful patterns. This approach allows for more targeted\ninteraction modeling, ensuring that features from different\ndomains (e.g., user behavior, item characteristics) are\ncombined in a way that better reflects their real-world\ninterdependencies. This selective feature crossing results in\nmore accurate predictions and improved recommendation\nquality. The main function of this model is to gradually\nincrease the complexity of the relationship between features by\nstacking them layer by layer, so as to better simulate the\ndiversity and complexity of user behaviors in the real world.\nThis process can be expressed as:\n$H^{(i)} = f(W_c\\cdot (H^{(i-1)} \\oplus H^{(i-1)}) + W_R H^{(i-1)} +b_c)$\nAmong them, $H^{(i)}$ represents the output matrix of layer i,\nand $H^{(i-1)}$ is the output of the previous layer. $W_c$ and $W_R$\nare the weight matrices of the cross-layer and residual\nconnection respectively, $b_c$ is the bias vector of the cross-layer,\nand $\\oplus$ represents element-level multiplication. And $f()$ is\nthe activation function. In this way, we can not only effectively\ncontrol the depth of feature crossover, but also reduce the\ncomputational complexity and improve the efficiency of the\nmodel. The design of the model enables the model to achieve\nfaster training speed and lower memory usage while\nmaintaining high accuracy. The overall architecture diagram is"}, {"title": "III. EXPERIMENT", "content": "The datasets used in this paper are the Criteo dataset and\nthe AutoML dataset. The following is a detailed introduction\nto the two datasets. The Criteo dataset is a public dataset\nwidely used in click-through rate prediction research, which\ncomes from an actual online advertising system. The dataset\ncontains a large number of records of user click behaviors,\nincluding user characteristics, ad characteristics, and whether\nthe user clicked on the ad. Specifically, the features in the\nCriteo dataset include but are not limited to the user's age,\ngender, geographic location, device information, historical\nbehavior data, etc., as well as related attributes of the ad, such\nas ad category, ad size, display location, etc. These features\nare all pre-processed, such as anonymization to protect user\nprivacy, and feature engineering to enhance the representation\nability of the features. In the Criteo dataset, each sample\ncorresponds to an ad display event, and the target variable is\nwhether the user will click on the ad, which is usually\nexpressed in binary form (click is 1, no click is 0).\nThe AutoML dataset originates from the field of automatic\nmachine learning and is mainly used to evaluate automated\nmachine learning processes, especially automated feature\nselection, model selection, and hyperparameter optimization.\nThis dataset covers a variety of machine learning tasks, such\nas regression, classification, clustering, etc., and provides a\nrich combination of features and labels. One of the\ncharacteristics of the AutoML dataset is that it contains\nmultiple sub-datasets, each of which reflects different types of\ntasks and application scenarios, such as image recognition,\ntext classification, time series prediction, etc. What these sub-\ndatasets have in common is that they are all carefully designed\nto evaluate the capabilities of automated machine learning\nframeworks.\nIn order to verify the effectiveness of the model, the model\nis compared with the current mainstream deep learning\nrecommendation model."}, {"title": "IV. CONCLUSION", "content": "Through the analysis of existing recommendation systems,\nwe found that traditional methods have many limitations when\nfacing big data and complex user behaviors. To this end, we\ndeveloped an innovative recommendation system model that\ncombines advanced feature cross-talk technology and attention\nmechanism to fully mine the embedded vector information and\nimprove the accuracy of recommendations. Experimental\nresults show that the proposed model can not only significantly\nimprove the recommendation effect, but also effectively deal\nwith the problems of high computational complexity and long\nfeature selection time. In addition, the model shows good\ngeneralization ability and stability in practical applications,\nespecially on large-scale datasets, it outperforms other\nmainstream models. Future research can further explore how to\ncombine more diversified user behavior data in order to build a\nmore personalized and intelligent recommendation system."}]}