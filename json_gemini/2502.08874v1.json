{"title": "DATA SENSOR FUSION IN DIGITAL TWIN TECHNOLOGY FOR ENHANCED CAPABILITIES FOR A HOME ENVIRONMENT", "authors": ["Benjamin Momoh", "Salisu Yahaya"], "abstract": "This paper explores the challenges posed by the coronavirus outbreak and its economic repercussions by investigating the integration of data sensor fusion in digital twin technology to enhance capabilities in a home environment. The study explores the fourth industrial revolution and the role of digital transformation in mitigating disruptions. Central to this investigation is collecting a primary dataset using the Wit Motion sensor, which provides data from accelerometers, gyroscopes, and magnetometers. This sensor was used to collect and analyse data for four distinct activities: walking, working, sitting, and lying, synchronized by timestamp.\nBy examining the integration of Cyber-physical systems, the Internet of Things (IoT), Artificial Intelligence (AI), and robotics, the project seeks to provide valuable insights into the significance of data fusion in digital twin technologies, particularly within home environments. The study implements and evaluates various sensor fusion methodologies, including feature-level fusion, decision-level fusion, and Kalman filter fusion, to contribute to advancements in AI. Machine learning models such as Support Vector Machines (SVM), Gradient Boosting (GBoost), and Random Forest (RF) are utilized to assess the effectiveness of these fusion techniques. A comprehensive comparison is conducted between individual sensor data from accelerometers, gyroscopes, and magnetic field sensors, and the data obtained through feature-level and decision-level fusion to determine the impact on model performance and to highlight the benefits of sensor fusion in enhancing digital twin capabilities in home environments.", "sections": [{"title": "Introduction", "content": "The Coronavirus outbreak, the first lockdown in the UK on Thursday, March 23, 2020, and the world at large led to an evident and drastic meltdown in economic activities and a setback in productivity. To prevent such effects from repeating themselves shortly, the fourth industrial revolution and digital transformation were the day's focus ([mihai2022]). According to [singh2023], the healthcare industry had a noteworthy expansion. This fourth industrial revolution started during the 21st century and has been influenced by recent technological innovations such as Cyber-physical systems, the Internet of Things (IoT), Artificial Intelligence (AI), and robotics, giving rise to the concept of digital twins. What is a digital twin? The [kerckhove2021] publication briefly defined the digital twin as a \"1:1 digital representation of a physical product or process over its whole lifecycle.\u201d The digital twin represents the future of technological advancement and sets to persist, transforming how humans operate and perceive their surroundings and transforming imagination into tangible reality.\nThe practical application of digital twins involves diverse models that continuously enhance data collected from various sensors, ensuring a highly accurate representation of a physical product's lifecycle. The future of digital twins, currently showing an average annual growth rate of about 38%, is envisioned as a twining process that addresses technological advancements, human history, and biological conditions ([kerckhove2021]). This perspective underscores the transformative potential of digital twins, making it a topic of significant interest and exploration.\nData fusion is employed to enhance the efficacy and effectiveness of digital technology, which can directly impact the level of integration between physical and digital objects. Data fusion uses computer technology to analyse and synthesise information from several sensors obtained according to time series for the required decision-making and estimation task [he2021data]. Most machine models and AI activities are in cyberspace, and information about building a digital twin, such as the individual's health records, behavioural patterns, and personal preferences, is collected. This information could be exchanged for money.\nEthical considerations in handling and utilising sensitive data from various sources are not just important; they are paramount. Adherence to industry standards and best practices in data fusion and digital twin technology, ensuring data security and privacy measures are robust and compliant with regulations, collaboration and communication challenges among interdisciplinary teams working on the project, addressing potential biases and inaccuracies in data fusion processes and algorithms, and a continuous professional development and skill enhancement to stay up-to-date with evolving technologies and methodologies in the field, are all critical aspects that demand our urgent attention.\nThe paper is divided into 6 sections; section 2 comprises literature reviews of some well-known data fusion models, fusion classification and architecture. In section 3, we investigated approach and methodology used and the structure of our primary data set. In section 4, Data visualization was carried on our data. Confusion matrix on the dataset with and without fusion was done. In section 5, data analysis evaluation of results was carried out with an explanation of results derived from the SVM, Gradient boost, and the Random Forest used. Discussion on the results was also made. The paper ends up with some concluding remarks, advanced data fusion techniques discussions, and future work plans in section 6."}, {"title": "Literature Review", "content": "This section provides existing literature on the origins of digital twins and their technologies. Recent years have shown significant developmental progress in digital twins in academia and industry ([wu2020]). The digitization concept has also helped address issues related to improved manufacturing quality, operation objectives, and conditions during the production of machinery energy. The potential of digital twin technology to revolutionize these areas is a cause for optimism. Countries, institutes, and industries worldwide, such as Massachusetts Institute of Technology (MIT) in the United States, Siemens in Germany, and China Nuclear Power Research and Design Institute, have also applied digital twin technology ([mengyan2024digitaltwin]).\nWhat is a digital twin? The Cambridge dictionary defines digital as \"to record\" or \"information storage as digits of 1's and 0's, to show the presence of signals\u201d, while \"Twin,\" as it is generally known, is one of two things containing or consisting of two matching or corresponding parts linked together. Digital Twin started relatively as 3D Computer-Aided Design (CAD) geometry and all design requirements for a product (including notation and parts lists). These Digital Mock-Ups are no longer mere digital copies but now exchange information with their real-life counterparts via a series of attached sensors making them digital twins. Current experience from domestic and foreign manufacturing industries has made it evident that the product model defined by 3D digitalisation has grown and evolved, and its benefits have been repeatedly verified ([xiong2022]). [jeong2022] in his paper, defined a digital twin as a replica of \"physical objects (e.g., people, objects, spaces, systems, and processes) in the real world into digital objects in the digital world\u201d to address various real-world problems and optimise the natural world through simulation or prediction of situations that can occur in the future. This concept of the digital twin, which was a paradigm shift in technological advancement, was introduced when the National Aeronautics and Space Administration (NASA) decided to create the physical twin of a space aircraft within the Apollo Program to reproduce its behaviour in space ([wang2020digital]). The work of Michael Grieves with John Vickers of NASA presented the concept of the product lifecycle, which is from the physical product, a virtual representation of that product, and the bi-directional data connection that feeds data from the physical to the virtual and vice versa ([jones2020]), ([macias2024]). The digital twin concept refers to a digital twin instead of a physical one, composed of a physical part, a digital part, and interconnectivity for data transfers. The digital twin requires high integration between the digital and physical parts through data transfer. The level of integration consists of a digital model representing a physical entity, a digital shadow representing a uni-directional inflow of information (Physical-to-Virtual), and a digital twin representing a bi-directional inflow of information (Virtual to physical). This integration is possible by using sensors installed in the physical object parts to reflect the digital objects. Similarly, the digital object can change the physical state through these sensors and actuators. According to [kritzinger2018], the level of integration of a digital twin with its physical counterpart has all to do with the level of data integration, which he also identified in three levels: the digital model, digital shadow, and the digital twin."}, {"title": "Digital Twin Technologies", "content": "For a digital twin to exist, data must flow in and out between the digital and physical objects in real-time. Sensors and actuators make this flow possible, and various technologies collect and store data in real time. [attaran2023], gave four different types of technologies. These are The Internet of Things (IoT), Artificial Intelligence (AI), Extended Reality (XR), and the Cloud. A particular technology depends on the digital twin use case, including industrial public and personal areas. These technologies can used for insightful information in visualisation and operation technology, analysis technology, multidimensional modeling and simulation technology, connection technology, data and security technology, and synchronisation technology."}, {"title": "Data Fusion", "content": "Data fusion, also known as information fusion, is a process that combines data or information from various sources to enhanced decision-making. In the context of data fusion architectures, the terms' information,' 'data,' and 'knowledge' describe the hierarchical order levels of a data fusion process.\nWhether referred to as information fusion or data fusion, the primary focus is on the fundamental questions related to fusion. These questions include the fusion's objectives, the pieces of information or data to be fused, their characteristics and level of uncertainty, the available fusion methods, and the associated difficulties and challenges. In Digital Twins, data fusion and information are used interchangeably. Data Fusion, in this context, is a Crucial process. It combines data from multiple sensors to accurately and reliably represent the physical and digital systems. This enhanced representation is pivotal in improving decision-making, underscoring the significance of data fusion in the digital twin domain and its potential impact on your work.\nData fusion is a multidisciplinary research area that draws ideas from various fields. It is defined as the study of efficient methods for transforming information from different sources and points in time into a representation that effectively supports humans or automated quality. With data fusion in the digital twin, we can produce a refined digital representation of our physical object with characteristics that will enhance decision-making and control-related activities such as environmental mapping, object recognition, forecasting, and prevention. Data fusion is familiar and has been used since the 1960s, notably by the US Department of Defense and the Joint Directors of Laboratories (JDL). Data fusion has applications in diverse fields, such as robotics, defence, and healthcare. However, the application of data fusion in digital twins is relatively new and holds great potential to enhance the accuracy of a digital twin. Direct fusion involves combining sensor data from heterogeneous or homogeneous sources. In contrast, Indirect fusion combines the outputs of heterogeneous information deduced from sensor data. Some researchers often interchange 'data fusion' with 'information fusion.' Direct data fusion, categorised as low-level fusion, involves combining sensor data from heterogeneous or homogeneous sources. Indirect data fusion, on the other hand, is classified as high-level fusion since it is performed after some analysis. There is also mixed data fusion, low-level and high-level fusion. Understanding these different levels of data fusion can make you feel more informed and knowledgeable about the topic."}, {"title": "Relation Between Input Data Sources", "content": "Data generated from different sources could have relationships with each other about a target. Durrant-Whyte proposes that these relationships be defined as complementary, redundant, or cooperative.\nThe information provided by the input sources represents different parts of the scene and could thus be used to obtain generally accepted information. Such data types are complementary in nature. For instance, when two separate sensors provide information on a particular target.\nData that can be overlapped or superimposed on each other are redundant. Fusing redundant data can improve target confidence. Redundant data are mainly observed when two or more sources provide individual information on a particular target.\nCooperative Data combines provided information with new data, a key concept in data fusion. This process typically results in more complex information than the original data, such as multi-modal (audio and video) data fusion.\nThe type of Data input/output is a classification system that breaks data fusion into five categories based on their nature.\nData in - Data out (DAI-DAO): This fusion process inputs data to make them more accurate. It is the most elementary data fusion method in classification. This data fusion process inputs and outputs raw data, but the results are typically more reliable or accurate.\nData in - Feature out (DAI-FEO): At this level, the data fusion process employs raw data from the source inputs to extract features or characteristics that describe an entity in the environment.\nFeature in - Feature out (FEI-FEO): In FEI-FEO, the inputs and outputs of the data fusion process are features. This data fusion process addresses feature improvement and is regarded as information fusion.\nFeature in - Decision out (FEI-DEO):Most algorithms fall into this category because of their feature purpose classification. FEI-FEO obtains a set of features as input and provides a set of decisions as output. Decision in Decision Out (DEI-DEO): This is the highest level in this classification. DEI-FEO fusion transforms some decisions at the low level into global decisions for decision-making. It fuses input decisions to obtain better or new choices."}, {"title": "JDL Data Fusion Classification", "content": "This classification is the most famous conceptual model in the data fusion community. It was initially proposed by JDL and the American Department of Defense (DoD) for use in the military. These organisations classified the data fusion process into five processing levels.\nLevel 0- (Sources Preprocessing) provides the input data (lowest level). Different sources, such as sensors, a priori information (references or geographic data), databases, and human inputs, can be employed. The primary aim of level 0 is data transformation and transfer to the proper level for further processing.\nLevel 1-(Object Refinement) employs the processed data from the previous level. The main aim is to identify entities and relations. Standard procedures at this level include spatiotemporal alignment, association, correlation, clustering or grouping techniques, state estimation, the removal of false positives, identity fusion, and combining features extracted from images. The output results of this stage are object discrimination (classification and identification) and object tracking (state of the object and orientation). This stage transforms the input information into consistent data structures.\nLevel 2 focuses on a higher level of inference than Level 1. The relationship information gained from the previous level broadens the scope of the investigation into the entire environment of the entity. Situation assessment aims to identify the likely situations given the observed events and obtained data. It establishes relationships between the objects. Relations (i.e., proximity and communication) are valued to determine the significance of the entities or objects in a specific environment. The aim of this level includes performing high level inferences and identifying significant activities and events (patterns in general). The output is a set of high-level inferences. Level 3 evaluates the impact and threats of the detected activities in Level 2 to obtain a proper perspective. The current situation is assessed by predicting a logical outcome's risks, vulnerabilities, and probabilities of operation. Level 4 -(Process Refinement), improves the process from level 0 to level 3, a managed part of the entire process. The aim is to achieve efficient resource management by monitoring other levels in real-time while accounting for task priorities, scheduling, and controlling available resources. The supporting components of the JDL architecture are; Human-Computer Interaction (HCI), Database Management System, and Sources, which is the base of the whole system.\nOne of the limitations of the JDL method is how the uncertainty about previous or subsequent results could be employed to enhance the fusion process (feedback loop). [liggins2009handbook], propose several refinements and extensions to the JDL model. [blasch2010high] proposed to add a new level (user refinement) to support a human user in the data fusion loop. The JDL model represents the first effort to provide a detailed model and a common terminology for the data fusion domain. However, because their roots originate in the military domain, the employed terms are oriented to the risks commonly occurring in these scenarios. The Dasarathy model differs from the JDL model in terms of the terminology and approach employed. The former is oriented toward the differences between the input and output results, independent of the employed fusion method. The Dasarathy model provides a method for understanding the relations between the fusion tasks and employed data. In contrast, the JDL model presents an appropriate fusion perspective for designing data fusion systems."}, {"title": "Classification Based On The Type Of Architecture", "content": "One of the main questions when designing a data fusion system is where the data fusion process will be performed. Based on this criterion, the following types of architectures could be identified by research\n1. Centralised Architecture\nIn a centralised architecture, the fusion node resides in the central processor that receives the information from all the input sources, which implies that all the fusion processes will be executed in a central processor. In this schema, the sources obtain only the observational measurements and transmit them to a central processor, where the data fusion process is performed. If data alignment and association are performed correctly and the required data transfer time is insignificant, then the centralised scheme can provide significant benefits with its theoretical optimality.\nThe disadvantage of this kind of architecture is that time synchronisation with the various sensors is a significant challenge in real time. Additionally, the bandwidth cost of transferring data to a central processor is expensive and can lead to information loss, highlighting the need for further research and development in these areas.\n2. Decentralised Architecture\nA decentralised architecture comprises a network of nodes. Each node has its processing capabilities, implying no single data fusion point. Therefore, for the fusion process, each node uses its local information in conjunction with the information received from its peers. Data fusion is performed autonomously, with each node accounting for its local information and the information received from its peers. Thus, this type of architecture could suffer scalability problems when the number of nodes increases.\n3. Distributed architecture\nIn a distributed architecture, each generated data is analysed independently from the local node before the information is sent to the fusion node. Machine learning model methods like the K-NN, Multiple Hypothesis Testing (MHT), and Probabilistic Data Association (PDA) are methods used to associate and estimate at the source nodes. In other words, data association and state estimations are done only with their local analysis, and the analysed information becomes input for the fusion process, providing a fused global analysis.\n4. Hierarchical architecture\nThis architecture combines decentralised and distributed nodes, creating hierarchical schemes where the data fusion process is executed at different levels in the hierarchy. Implementing a decentralised data fusion system can be challenging due to the significant computation and communication requirements. However, it is essential to note that there is no single best architecture existence yet known. According to research and studies, selecting the most appropriate architecture should be based on the requirements, existing networks, if there are any, data availability, node processing capabilities or hardware, and the organisation of the data fusion system. According to [castanedo2013], the methods for data fusion are classified into three basic categories which are:\n\u2022 Data Association\nData association is a complex task that aims to establish the set of observations/measurements generated by the same target over time. It involves intricate methods such as the K-NN, Probabilistic Data Association (PDA), Joint Probabilistic Data Association (JPDA), and Multiple Hypothesis Test (MHT).\n\u2022 State Estimation\nState estimation is a precise process that considers the state of a target under movement (i.e. position), given the observation or measurement. It relies on accurate methods such as Maximum Likelihood Estimation (MLE), Kalman Filter, Distributed Filter, and covariance consistency methods.\n\u2022 Decision Fusion\nDecision fusion is crucial in making high-level inferences about an event and its activities. It does so by analyzing the detected targets provided by many sources, highlighting their importance."}, {"title": "Data Fusion Challenges", "content": "We must implicitly examine the methods outlined above, which try to address challenges in data fusion. Understanding data imperfection is crucial as it forms the basis of all data fusion methods. Sensor data, often imprecise, uncertain, ambiguous, vague, and incomplete, presents a significant challenge. Although we can improve data quality by modeling its imperfection and using other available information and powerful mathematical tools, the severity of data imperfection can significantly affect fusion quality if data fusion fails to extract precise and valuable data ([khaleghi2013multisensor]).\nSome data uncertainties are caused by inherent noise in measurement, sensors, and environments. These noises lead to data outliers or disorder, collectively called data inconsistency. Data inconsistency introduces a terrible effect on data fusion if the fusion model cannot distinguish the techniques necessary to overcome this problem by eliminating the influence it creates. There are some other problems caused by lasting or dynamic failures, which are challenging to model and predict in the usual way ([bakr2017distributed]), ([khaleghi2013multisensor]).\nData-related challenges often appear in a system that applies belief functions or Dempster-Shafer theory [meng2020survey] when some problems that should be treated independently are erroneously integrated.\nData captured from different sensors with different frames must undergo a crucial step before fusion alignment into a standard frame. This process, known as data alignment, is of utmost importance in data fusion. If the data fusion algorithm fails to address this, the lack of alignment or correlation can lead to over/under confidence or biased estimation, underscoring the necessity of this step ([meng2020survey]).\nData captured by different sensors could have different structures. The data fusion method should be able to integrate the different types to describe the environment better.\nAnother challenge is that of Fusion Location. To determine the fusion location, we must consider a trade-off between fusion cost and quality. This is a problem with respect to wireless sensors. When data are generated from a central node, the expense is bandwidth and time, but if data are from a local node, the cost is data accuracy.\nIn addition, the complexity of data fusion is caused by timeliness, where the significance of a data point might be limited to a limited period, especially for a time-varying system. The fusion node must distinguish the correct order of the data and its validation."}, {"title": "Fusion Quality Assessment", "content": "Okolie and Smit, (2022), proposed three approaches that are commonly used for Digital fusion estimation Models.\n1. A qualitative approach which involves an inspection and comparison of results from the physical entity with the digital twin.\n2. The quantitative approach, a practical and widely used method, involves using statistical metrics to measure the relative and absolute quality of the fused Digital Elevation Model (DEM).\n3. The performance-based approach offers a versatile set of quality assessment criteria, including the Mean Absolute Error (MAE), Coefficient of Determination $R^2$ and Model Loss, allowing for a comprehensive evaluation of a fused digital twin model."}, {"title": "Approach And Methodology", "content": "This section outlines the methodology for evaluating the impact of data fusion techniques on the performance of a home environmental activity recognition model. Our objective is to demonstrate how combining sensor data from different sources can enhance the capability of machine learning models in classifying activities. The Wit Motion sensor, a crucial tool that provides data from the accelerometer, gyroscope, and magnetometer, will collect and analyze data for four distinct activities: walking, working, sitting, and lying, with similarities only on timestamp, for synchronization."}, {"title": "Data Collection", "content": "The Wit Motion sensor will be used to gather data on three different types of sensors:\n\u2022 Accelerometer Measures acceleration along the X, Y, and Z axes.\n\u2022 Gyroscope Measures angular velocity along the X, Y, and Z axes.\n\u2022 Magnetometer Measures the magnetic field strength along the X, Y, and Z axes.\nEach data point will include a timestamp for time-based analysis and synchronization across different sensors. The data will be collected for four activities: Walking, Working, Sitting, and Lying."}, {"title": "Data Structure", "content": "The collected dataset will be organized into a pandas DataFrame with the following structure.\nThe proposed methodology includes several stages, each evaluating different data fusion techniques and their impact on classification performance."}, {"title": "Individual Sensor Models", "content": "\u2022 Data Preparation: Each sample in our dataset consists of 20 features obtained from the accelerometer, gyroscope and magnetometer sensors. A total of 3,239 samples were used for this study. Split the data into training and testing sets (80% training, 20% testing) for each sensor type.\n\u2022 Model Training: Train separate machine-learning models for each sensor using the following algorithms:\n1. Random Forest (RF):An ensemble learning method based on multiple decision trees. Each Random Forest (RF) is an ensemble learning method that combines multiple decision trees to improve the accuracy of our activity recognition classification. For a RF, each decision tree in the forest is trained on a random subset of the data (in this case, 80% of the total data set).\nIf X represents our input feature vectors (accelerometer, gyroscope, and magnetometer), the prediction variable \u0177n from the n-th tree is given by\n\u0177n = Tn (X) (3.1)\nwhere Tn represents the decision function of the tree, and n is the number of trees. A majority vote of all trees obtains the final decision. That is,\n\u0177 = majority_vote ({Tn(X)}1001) (3.2)\nThe random forest method reduces overfitting by averaging multiple trees built from different parts of our dataset.\n2. Support Vector Machine (SVM): ): A supervised learning model used for classification designed to find the hyperplane that best separates our classes in the feature space, maximizing the margin between them.\nFor our set of labelled training data {(Xi, yi)}1, where yi E {-1,1}, and regarding our dataset for the activity recognition and classification problem with four different classes, each classifier (one for each of the four classes, say class k) is trained to distinguish between the class k and all other classes. It implies that for this work:\nYi = \n1 if the class belongs to class k,\n-1 if the class belongs to any of the remaining three classes.\nIn this scenario, our four (4) SVM training models would be:\nModel 1: Distinguishes between \"Walking\u201d (y\u2081 = 1) and not \"Walking\" (y\u2081 = \u22121).\nModel 2: Distinguishes between \"Working\u201d (y\u2081 = 1) and not \"Working\" (y\u2081 = \u22121).\nModel 3: Distinguishes between \"Sitting\" (y\u2081 = 1) and not \"Sitting\" (y\u2081 = \u22121).\nModel 4: Distinguishes between \"Lying\" (y\u2081 = 1) and not \"Lying\" (y\u2081 = \u22121).\nThe prediction of SVM is based on the class with the highest confidence score or the classifier with the highest output value. This approach is popularly known as the One-vs-Rest (OvR) approach because each SVM will be a binary classifier. However, these models distinguishing themselves from the rest of the model classes work together to perform multi-class classification. For each class k, the decision function is expressed as:\nfk(X) = wk \u2022 X + bk, (3.3)\nwhere:\n- wk is the weight vector for class k,\n- bk is the bias term for class k.\nThe final predicted class for an input X is given by:\n\u0177 = arg max (fk(X)) . (3.4)\nk\n3. Gradient Boosting (GB): A boosting technique builds models sequentially to correct errors of previous models for each class of our dataset.\nLet X be our input feature vector from acceleration (Ax, Ay, Az), angular velocity (Wx, Wy, Wz), and magnetic field (Mx, My, Mz), obtained from the accelerometer, gyroscope, and magnetometer sensors, respectively. Therefore,\nX = [Ax, Ay, Az, \u03c9\u03c7, \u03c9\u03c8, \u03c9\u03c0, Mx, My, Mz] . (3.5)\nWith the actual class label y, where y \u2208 {0, 1, 2, 3}, representing walking, working, sitting, and lying, respectively, the Gradient Boost model predicts a score F(X) for each class k \u2208 {0,1,2,3}. For each class k, the probability of that activity is given by:\nP\u2081(X) =  = 1. (3.6)\nFor the Gradient Boost model, the cross-entropy function across the four classes is minimized as shown below:\nL = -  1{yi = k} log Pk(Xi), (3.7)\ni=1 k=0\nwhere:\n- N is the total number of training samples.\n- 1{yi = k} is an indicator function that is 1 if the actual class label for the i-th sample is k, and 0 otherwise.\nPk(X) is the predicted probability for class k for the i-th sample.\nBoosting at stage m + 1 is done after stage m by updating the score for each class k:\nFk,m+1(X) = Fk,m(X) + \u03b7\u00b7 hk,m(X), (3.8)\nwhere hk,m(X) is a new model trained to predict the residual for class k at stage m.\nThis residual for each class k and model stage m represents the difference between the true class label and the predicted probability. Mathematically, it is expressed as:\nrikm = 1{yi = k} \u2013 Pk,m(Xi). (3.9)\nHere, \u03b7 is the learning rate that controls how much each model influences the overall prediction.\nHence, the final predicted activity class \u0177 for an input X, derived from the sensor data, is determined by:\n\u0177 = arg max (F(X)) . (3.10)\nk\n\u2022 Model Evaluation: Evaluate each model's performance using accuracy scores and confusion matrices. Their Root Mean squared Error was also used for evaluation. Model evaluation is done on the accuracy without data fusion on individual sensors based on the classification of activity recognition, and also evaluation on data fusion (feature-level and decision-level fusion)."}, {"title": "Feature-Level Fusion", "content": "1. Data Preparation: Feature-level fusion involves the combination or fusion of the data acquired from the sensors before any decision is taken. We combined features from accelerometer, gyroscope, and magnetometer into a single feature set such that: Combined_dataset = acceleration + angular-velocity + magnetic-field. This combination is made possible by timestamp synchronization. Each sensor's data was collected simultaneously, enabling the dataset to be combined on the same label classes for activity recognition.\n2. Model Training: The training was conducted using the training dataset with Random Forest, SVM, and G-Boost models on individual sensor data and the combined feature set (feature fusion and decision-level fusion).\n3. Model Evaluation:A crucial step in the machine learning pipeline is assessing the model's performance using accuracy scores and confusion matrices. The accuracy score provides a ratio of correctly predicted instances to the total cases. With the confusion matrix, we can have a breakdown of the models' performance indicating the number of true positives, true negatives, false positives and false negatives classes. Also, The Root Mean Square Error (RMSE) is a metric tool for evaluating the error between the predicted class probability and the actual class labels. Unlike the accuracy score and the confusion matrix that provides a qualitative measure of our model performance, RMSE provides a quantitative measure of prediction error, with a lower RMSE indicating that model predictions are closer to the actual values."}, {"title": "Decision-Level Fusion", "content": "1. Model Training:Train individual models for each sensor as described above.\n2. Prediction Aggregation: Use majority voting to combine predictions from individual models.\n3. Majority Voting: Each model votes for a class, and the class with the most votes is chosen as the final prediction. This was also carried out with our choices of models.\n4. Model Evaluation: Evaluate the decision fusion approach using accuracy scores and confusion matrices."}, {"title": "Kalmer Filter Equations", "content": "The general Kalman filter consists of prediction and update steps. Let's define the variables and equations step by step.\nPrediction Step\nPredicted State Estimate:\nThe state vector at time k, given the previous state estimate at time k-1, is predicted as:\nXk|k\u22121 = FXk-1|k\u22121 + Buk (3.11)\nWhere:\n- F is the state transition matrix, which in this case is the identity matrix I (i.e., no change in state unless updated by measurements).\n- 2k-1k-1 is the previous state estimate.\n- uk is the control input, assumed to be zero in this case (no external influence).\n- B is the control matrix, which is also zero.\nSince F = I and uk 0, the prediction simplifies to:\nXk|k\u22121 = Xk-1|k\u22121 (3.12)\nPredicted Covariance:\nThe predicted covariance matrix at time k is given by:\nPkk\u22121 = FPk-1k\u22121FT + Q (3.13)\nWhere:\n- Pk-1/k-1 is the previous covariance estimate.\n- Q is the process noise covariance matrix (set to 0.11 in this case).\nSince F = I, the covariance prediction simplifies to:\nPkk\u22121 = Pk-1k\u22121+Q (3.14)\nUpdate Step\nKalman Gain:\nThe Kalman gain Kk is computed as:\nKk = Pk|k\u22121HT (HPk|k\u22121HT + R)\u00af\u00b9 (3.15)\nWhere:\n- Pk/k-1 is the predicted covariance matrix.\n- H is the measurement matrix (mapping the state to the measurement space).\n- R is the measurement noise covariance matrix (set to 0.51).\nState Update:\nThe updated state estimate is given by:\nXk\\k = Xk/k\u22121 + Kk (zk - H\u00e2k/k-1) (3.16)\nWhere:\n- zk is the actual measurement at time k.\n- 2k/k-1 is the predicted state.\n- H\u00e2kk-1 is the predicted measurement.\n- zk - H\u00e2kk-1 is the measurement residual (or innovation).\nCovariance Update:\nThe updated covariance matrix is computed as:\nPkk = (I \u2013 KkH) Pk|k\u22121 (3.17)\nWhere:\n- I is the identity matrix.\n- KkH adjusts the covariance based on the Kalman gain and measurement matrix.\nFor our methodology, the state vector,\u00cek-1|k\u22121, includes the filtered X, Y, and Z values for the Kalman filter. These values are the core components of the system's state, which we aim to estimate and update using the Kalman filter process.\nThe state vector is represented as:\nXk\u22121k\u22121 = (3.18)\nWhere:\n- -1k-1, -1k-1, and 2-1k-1 represent the filtered values of the X, Y, and Z components of the state vector, respectively, at time k-1.\nThe observations used in the update step come from the combined data from the following sensors:\n- Accelerometer: Provides measurements of acceleration along the X, Y, and Z axes.\n- Gyroscope: Provides angular velocity measurements along the X, Y, and Z axes.\n- Magnetometer: Provides magnetic field measurements along the X, Y, and Z axes.\nThese measurements are combined into a vector zk at each time step k, which is used in the update step of the Kalman filter to refine the state estimate. The measurement vector zk is represented as:\nzk = (3.19)\nWhere:\n- XY\nz, z, and z are the measurements at time k from the accelerometer, gyroscope, and magnetometer along the respective axes.\nThus, the state vector \u00c2k\u22121|k\u22121 is updated based on these combined sensor measurements, improving the estimate of the system's state over time.\nWe applied the Kalman Filter to fuse measurements from the accelerometer, gyroscope, and magnetometer across all data points to produce the \u201cKalman Filtered X,\" \"Kalman Filtered Y,\" and \"Kalman Filtered Z.\" These values would be stored in a DataFrame and used for estimation."}, {"title": "Implementaion", "content": "All code and implementation were done using Python due to its extensive libraries and tools for data analysis and machine learning. Python's simplicity and readability also make it a popular choice for data science projects."}, {"title": "Data Visualization", "content": "The figures above provides a comprehensive view of how the sensor readings change over time. Each subplot represents the time series data for a specific sensor axis, enabling us to observe patterns, trends, and anomalies in the data collected during this study.\nFigure 4 above shows the distribution of sensor readings from the accelerometer, gyroscope and magnetometer. This allows us to gain insights into the range and frequency of the values each sensor axis records. Each plot in the diagram indicates the data distribution for one of the sensor axes. The x-axis in each subplot represents the sensor reading values, while the y-axis shows the frequency of the values within our dataset. The Kernel Density Estimates, the line on top of each histogram plot, provide us with a smoothed version of the distribution if they"}]}