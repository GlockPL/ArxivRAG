{"title": "Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems", "authors": ["Tian Ye", "Zicheng Xu", "Yuanzhi Li", "Zeyuan Allen-Zhu"], "abstract": "Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to \"self-correct\" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating \"error-correction\" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.", "sections": [{"title": "1 Introduction", "content": "Language models have achieved near-human-level performance in various tasks, including math solving, coding, and natural language understanding [1, 2, 18, 21, 31]. However, their problem-solving skills are still imperfect, sometimes resulting in logical errors. Recently, there have been numerous attempts to improve the reasoning accuracy of language models.\nOne promising approach is to use a verifier to check the correctness of the language model's output [9, 12, 23, 28, 30]. Interestingly, some studies show that language models can \u201cself-verify\" [15, 27]: They can be prompted to verify the correctness of their own generation, thereby improving overall accuracy. An illustrative example is shown in Figure 1.\nThis leads to the following fundamental questions:\nIf a language model can correct its own mistakes after generation, (1) Why does it make those mistakes to begin with? (2) Why doesn't it correct the mistakes immediately during generation, instead of waiting until after?\nThere are many works that attempt to understand question (1). Notably, studies such as [6, 17, 22] have shown that many mistakes arise due to \"distribution shift\" the training distribution of the language model differs from the prompts used during testing. Thus, even if the training data is error-free, language models can still make mistakes during generation.\nMuch less work focuses on question (2). While correcting mistakes after generation is a valid approach to improve a language model's accuracy, it is more desirable to correct mistakes imme-diately as they occur, such as \u201cA \u21d2 B, oh I made a mistake, actually A \u21d2 C.\u201d Doing so during generation can save inference tokens (the model does not need to continue generating based"}, {"title": "2 Synthetic Math Data From Prior Work", "content": "Ye et al. [29] introduced a family of controllable, synthetic datasets of math problems with step-by-step solutions. These data simulate GSM8K [9], while removing arithmetic difficulties (by restricting computations to integers modulo 23) and common sense knowledge (e.g., a candle burns and its length decreases). What remains is the \"logic reasoning\" part. The dataset has much larger diversity (over 90 trillion solution templates), and the solutions are fully verifiable. We briefly summarize it to make the paper self-contained, emphasizing some important aspects.\nAn example from their dataset is in Figure 2. The structure graph describes the set of instance parameters, such as \"the number of school daypacks in each film studio.\" They also allow for abstract parameters, such as \"the (total) number of backpacks in central high,\" which requires hierarchical computation.\nThe exact data construction is not important for this paper. What matters is that the parameters form a dependency graph, as shown in Figure 2, where a parameter can be computed only when its predecessors have all been computed. To remove arithmetic difficulty, the computations are broken into binary operations such as 12 + 13 + 7 is broken into (12 + 13) + 7 ensuring that failure to solve the problems is not due to arithmetic difficulties. They use op to denote the number of operations needed in the solution and prepared four families of data:"}, {"title": "3 Result 0-1: Language Models Can Retry Upon Regret", "content": "Generative models solve math problems step by step in a chain-of-thought (CoT) manner. Each step in our math problems is a single sentence formatted as \"Define [param] as X; so...\", as shown in Figure 2. How do generative models make mistakes in this CoT process?\nThe most common reasoning mistake occurs when the model generates a [param] that is not yet ready for computation (i.e., the model has not determined the values of all the parameters that [param] depends on, also known as \u201cskipping steps\u201d, a frequent error even in GPT-4 [8])."}, {"title": "3.1 Result 0: Models Can Be \u201cRegretful\" After Making Mistakes", "content": "Interestingly, their same paper also implies the following:\nResult 0 (corollary of [29]). For models pretrained on iGSM (with correct solutions only!), during their solution generation process, after writing \u201cDefine [param] as\u201d for a wrong [param], they often \"realize\u201d such a mistake, showing a regretful pattern in their internal states.\nTo see this, one can apply their probing technique (illustrated in Figure 3(a)) to extract information from the model's last hidden layer after \"Define [param A] as\" to see if the model knows A can truly be computed next. This probing task is denoted as can_next(A) \u2208 {true, false}. They found:\n\u2022 When A ranges over all possible parameters, the probing 99% accurately predicts can_next(A), meaning the model knows if A can be computed next, even for the hardest op = 32 problems.\n\u2022 When the model makes a mistake, the first sentence with a mistake usually has can_next(A) = false. Probing shows the model has ~ 60% chance of knowing can_next(A) = false, indicating it often knows it has made a mistake, right after stating the parameter name in full."}, {"title": "3.2 Result 1: Let Models Retry Upon Regret", "content": "If a model knows it is a mistake, why does it generate the wrong [param A] in the first place? The issue lies in the generation process. Before explicitly stating \u201cDefine [param A] as\", the model might falsely think A is ready to compute among all the parameters it can focus on. After stating it, the model shifts its focus to the actual computation of A, and this is the moment it can better realize that A is not ready for computation (using its attention mechanism).\nNow that we know the model exhibits some \"regret\" towards the mistake, can we use this to improve accuracy?\nRetry upon regret. We conducted an experiment using the probing result to guide the model's generation process. After generating each solution sentence, we use the can_next probing to de-termine if the model knows it has made a mistake. If so, we revert to the end of the previous sentence and regenerate. We use multinomial sampling (i.e., beam=1 and dosample=true) during this regeneration process, with a maximum of 10 total retries for generating each solution."}, {"title": "4 Result 2-6: Pretrain with Retry Data", "content": "In this section, we prepare pretrain data to teach the model to directly correct mistakes.\nMath data with retry. Since we use a controllable, synthetic math dataset, we can, at the beginning of each solution sentence, with probability retry_rate \u2208 [0,1), insert a wrong parameter that cannot be computed next, followed by a special token [BACK]. We repeat this process, so with probability (retry_rate)^2, it may generate another wrong parameter at the same location, and so on. We provide an extreme example with retry_rate = 0.5 in Figure 4(a), and a more complex"}, {"title": "5 Result 7: Finetune with Retry Data", "content": "In this section, we consider a model pretrained using only error-free math data but finetuned with the retry data from Section 4. This simulates a real-world scenario where one is given an open-source pretrained model and wants to finetune it for better reasoning/math accuracies. Our goal is to determine if this approach works as well as pretraining directly on the retry data.\nWe focus on parameter-efficient fine-tuning (PEFT) methods such as LoRA [10], which are widely adopted in practice. LoRA fine-tunes a small number of trainable parameters (i.e., low-rank matrix updates) on top of the original, frozen pretrained weights.\nInterestingly, despite using the same high-quality retry data as in Section 4 (with or without label masking), and our best efforts in selecting LoRA fine-tune parameters (adopting various rank choices), and ensuring sufficient training (comparable training steps/samples to pretraining), we find that LoRA finetuning falls short of pretraining directly with the retry data. When the LORA rank is small, it even underperforms compared to pretraining with error-free data, see Figure 7. From this, we conclude that:\nResult 7 (Figure 7). Error correction is a skill that can be very different from the original (error-free) reasoning and cannot be acquired during a LoRA finetune stage from language models pre-trained only using error-free data.\n(In contrast, such a skill can be learned using full finetuning with sufficiently many retry training samples, see Figure 7; but the finetune cost of this process is no less than pretraining with retry data, and is essentially continued pretraining.)\nOne may compare this to error detection (see Section 3.1): error detection is an easier skill that even models pretrained from error-free data can acquire almost for free (such as via probing, not to mention LoRA finetuning). However, error correction cannot be easily achieved via LoRA finetuning from a language model pretrained on error-free data.\nWe conjecture this is because, when a mistake is made, the model needs to revise its internal computations to find alternative solutions. Such revision may not be straightforward; otherwise, simply re-generating a few times from the previous sentence (i.e., \"retry upon regret\") should have already achieved higher reasoning accuracies. We do not analyze such internal computations in this paper, but refer interested readers to [29], which explains language models' internal computations (via probing) on the error-free iGSM data."}, {"title": "6 Result 8: Pretrain with Fake Mistakes", "content": "While it is possible to prepare \"perfect\" retry data on synthetic iGSM datasets, obtaining math data with mistakes and corrections can be challenging in real life. For this reason, we explore the possibility of using more realistic approaches to augment math problems (for which we only have correct solutions) with \u201cfake\u201d mistakes and retries. Ideally, this process should not require any semantic parsing or understanding of the problem and the solution.\nWe explore two approaches and compare them with the perfect retry data. We still use retry_rate, but instead of selecting a wrong parameter to retry, we simplify the process:\n\u2022 In the first approach, we randomly select a sentence that appears later in the step-by-step solution to retry. For instance, in the example of Figure 2:"}, {"title": "7 Conclusion", "content": "In this paper, we investigate whether language models can benefit from pretraining on data contain-ing mistakes, followed by immediate error correction. Using a fully controllable synthetic setting, we demonstrate that models trained on such data outperform those trained on the same amount of error-free data.\nIn addition to the accuracy gain, Section 4 shows that using retry data is very safe: the model rarely makes mistakes even after pretraining with high error-rate retry data, and it is unnecessary"}, {"title": "D Experiment Details and Parameters", "content": "Model. We use the GPT2 architecture [20], replacing its absolute positional embedding with modern rotary positional embedding [7, 24], still referred to as GPT2 for short. We also played with the Llama architecture (especially with gated MLP layers) and did not see any benefit of using it. This GPT2 performs comparably to Llama/Mistral at least for knowledge tasks [5].\nLet GPT2-l-h denote an l-layer, h-head, 64h-dim GPT2 model. We primarily use GPT2-12-12 (a.k.a. GPT2-small) throughout this paper. We use the default GPT2Tokenizer, and a context length of 768/1024 for language model pretraining on iGSM-med/iGSM-hard and a context length of 2048 for evaluation.\nData Size. For both pretraining and finetuning, we did not limit the amount of training data; we generated new data on-the-fly. We do not explore sample complexity in this paper, such as the number of math problems needed to achieve a certain level of accuracy, as it would complicate the main message of this paper."}, {"title": "D.1 Pretrain Experiment Details", "content": "Pretrain parameters. Throughout this paper, when we pretrained a model (except for \"pretrain double-time\u201d), we followed the same pretrain parameter choices of [29]. That is, we used the AdamW optimizer with mixed-precision fp16, \u03b2 = (0.9, 0.98), cosine learning rate decay (down to 0.01x in the end + 1000 steps of linear ramp-up). We used a mixture of V100/A100 GPUs, but the GPU specifications are not relevant here. For all the experiments (with original data, or retry data, or retry_weak, or retry_miss):\n\u2022 For pretraining on the iGSM-med datasets, we used a learning rate of 0.002, weight decay of 0.05, batch size of 512, context length of 768, and trained for 100,000 steps.\n\u2022 For pretraining on the iGSM-hard datasets, we used a learning rate of 0.002, weight decay of 0.03, batch size of 256, context length of 1024, and trained for 200,000 steps.\nOur pretrain data is constructed by randomly generating math problems (and solutions), con-catenating them together, and truncating them (in the right) to fit within the 768 or 1024-sized context window. If a problem (with solution) is longer than the context window size, we discard it.\nTest-time parameters. When evaluating on test data, we use context length 2048 for both iGSM-med and iGSM-hard. We use either beam=1 and dosample=false (greedy) or beam=4 and dosample=true (beam-search multinomial sampling) to present test accuracies. (Except for the original no-retry training, we also tried beam=16/32 with dosample=true.) We discover it is better to keep dosample=false while beam=1 and dosample=true while beam > 1.\nOur accuracies are not simply from comparing the answer integers (between 0 and 22); instead we have written a parser to make sure the model's intermediate solution steps are fully-correct.\nAccuracy statistics. In each of our accuracy result, such as each cell in Figure 3(b), Figure 4(b), Figure 8, we average the model's evaluation over 4096 math problems of that type."}, {"title": "D.2 V-Probing Experiment Details", "content": "The V-probing for can_next(A) was introduced in Ye et al. [29]. It is a fine-tuning process upon the pretrained language model, with an additional linear head on the output layer, and a small rank-r update on the input (embedding) layer. The pretrained model is freezed, and only this linear head and the rank-r update are trainable parameters during the fine-tuning stage (for the probing task). In this paper, our \u201ccan_next probing version1"}, {"title": "D.3 Finetune Experiment Details", "content": "LORA finetune. In Section 5, we applied LoRA finetuning [10] on a pretrained model using the new \"error + correction\" data. LoRA involves freezing the pretrained model and adding trainable low-rank updates to the weight matrices. It is recommended to apply low-rank updates to the query/value matrices [10] and the embedding matrix [4].\nWe experimented with a wide range of low-rank configurations, using rank-r for the query/value matrices and rank-2r for the embedding matrix, with r \u2208 {4, 8, 16, 32, 64, 128, 256}. Notably, using r = 256 is almost equivalent to full finetuning, given that the hidden dimension is 768 for GPT2-12-12.\nIn this experiment, we used the same parameters as the pretraining (e.g., AdamW, betas, cosine lr scheduling), except that\n\u2022 For LORA finetuning on the iGSM-med datasets, we used a learning rate of 0.001, weight decay of 0.05, batch size of 256, context length of 768, and trained for 200,000 steps.\n\u2022 For LORA finetuning on the iGSM-hard datasets, we used a learning rate of 0.001, weight decay of 0.05, batch size of 128, context length of 1024, and trained for 200,000 steps.\n(LORA finetuning typically requires a smaller learning rate.) Note this is the same as the pretraining tokens for iGSM-med and half of that for iGSM-hard, which is sufficient for the training curve to plateau.\nOur accuracy results were presented in Figure 7, and once again in each cell we have evaluated the model over 4096 math problems. Similar to all other retry experiments (see Appendix D.1), we performed LoRA finetuning from (two) pretrained models using 2 random seeds, and presented the best accuracy among the 2 seeds and the two beam=1/4 choices.\nFull finetune = continued pretrain. In Section 5, we also applied full finetuning. This used the same parameters as pretraining (e.g., AdamW, betas, cosine lr scheduling), except that\n\u2022 For full finetuning on the iGSM-med datasets, we used learning rate 0.001, weight decay 0.05, batch size of 512, context length of 768, and trained for 100,000 steps.\n\u2022 For full finetuning on the iGSM-hard datasets, we used learning rate 0.001, weight decay 0.03, batch size of 256, context length of 1024, and trained for 200,000 steps.\nNote that this full finetune uses the same number of training tokens comparing to pretrain in Section D.1.\nOnce again, our accuracy results were presented in Figure 7, and in each cell we have evaluated the model over 4096 math problems. We performed full finetuning from (two) pretrained models using 2 random seeds, and presented the best accuracy among the 2 seeds and the two beam=1/4 choices."}]}