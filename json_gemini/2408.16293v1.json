{"title": "Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems", "authors": ["Tian Ye", "Zicheng Xu", "Yuanzhi Li", "Zeyuan Allen-Zhu"], "abstract": "Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to \"self-correct\" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating \"error-correction\" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.", "sections": [{"title": "Introduction", "content": "Language models have achieved near-human-level performance in various tasks, including math solving, coding, and natural language understanding [1, 2, 18, 21, 31]. However, their problem-solving skills are still imperfect, sometimes resulting in logical errors. Recently, there have been numerous attempts to improve the reasoning accuracy of language models.\nOne promising approach is to use a verifier to check the correctness of the language model's output [9, 12, 23, 28, 30]. Interestingly, some studies show that language models can \u201cself-verify\" [15, 27]: They can be prompted to verify the correctness of their own generation, thereby improving overall accuracy.\nThis leads to the following fundamental questions:\nIf a language model can correct its own mistakes after generation, (1) Why does it make those mistakes to begin with? (2) Why doesn't it correct the mistakes immediately during generation, instead of waiting until after?\nThere are many works that attempt to understand question (1). Notably, studies such as [6, 17, 22] have shown that many mistakes arise due to \"distribution shift\" the training distribution of the language model differs from the prompts used during testing. Thus, even if the training data is error-free, language models can still make mistakes during generation.\nMuch less work focuses on question (2). While correcting mistakes after generation is a valid approach to improve a language model's accuracy, it is more desirable to correct mistakes imme-diately as they occur, such as \u201cA \u21d2 B, oh I made a mistake, actually A \u21d2 C.\u201d Doing so during generation can save inference tokens (the model does not need to continue generating based"}, {"title": "Synthetic Math Data From Prior Work", "content": "Ye et al. [29] introduced a family of controllable, synthetic datasets of math problems with step-by-step solutions. These data simulate GSM8K [9], while removing arithmetic difficulties (by restricting computations to integers modulo 23) and common sense knowledge (e.g., a candle burns and its length decreases). What remains is the \"logic reasoning\" part. The dataset has much larger diversity (over 90 trillion solution templates), and the solutions are fully verifiable. We briefly summarize it to make the paper self-contained, emphasizing some important aspects.\nAn example from their dataset is in Figure 2. The structure graph describes the set of instance parameters, such as \"the number of school daypacks in each film studio.\" They also allow for abstract parameters, such as \"the (total) number of backpacks in central high,\" which requires hierarchical computation.\nThe exact data construction is not important for this paper. What matters is that the parameters form a dependency graph, as shown in Figure 2, where a parameter can be computed only when its predecessors have all been computed. To remove arithmetic difficulty, the computations are broken into binary operations such as 12 + 13 + 7 is broken into (12 + 13) + 7\u2014 ensuring that failure to solve the problems is not due to arithmetic difficulties. They use op to denote the number of operations needed in the solution and prepared four families of data:"}, {"title": "Result 0-1: Language Models Can Retry Upon Regret", "content": "Generative models solve math problems step by step in a chain-of-thought (CoT) manner. Each step in our math problems is a single sentence formatted as \"Define [param] as X; so...\", as shown in Figure 2. How do generative models make mistakes in this CoT process?\nThe most common reasoning mistake occurs when the model generates a [param] that is not yet ready for computation (i.e., the model has not determined the values of all the parameters that [param] depends on, also known as \u201cskipping steps\u201d, a frequent error even in GPT-4 [8])."}, {"title": "Result 2-6: Pretrain with Retry Data", "content": "In this section, we prepare pretrain data to teach the model to directly correct mistakes.\nMath data with retry. Since we use a controllable, synthetic math dataset, we can, at the beginning of each solution sentence, with probability retry_rate \u2208 [0,1), insert a wrong parameter that cannot be computed next, followed by a special token [BACK]. We repeat this process, so with probability (retry_rate)^2, it may generate another wrong parameter at the same location, and so on.\nResult 2-3. Our results in Figure 4(b) strongly support that:\n\u2022 Within a reasonable range, the more mistakes the better. Especially on hard problems, such as on iGSM-medop=23, the accuracy jumps from 78% to 94% by using retry_rate = 0.5.\n\u2022 Masking mistakes is unnecessary. We observe that it is generally not needed to introduce label masking on the error data even for large retry_rate = 0.5."}, {"title": "Result 7: Finetune with Retry Data", "content": "In this section, we consider a model pretrained using only error-free math data but finetuned with the retry data from Section 4. This simulates a real-world scenario where one is given an open-source pretrained model and wants to finetune it for better reasoning/math accuracies. Our goal is to determine if this approach works as well as pretraining directly on the retry data.\nWe focus on parameter-efficient fine-tuning (PEFT) methods such as LoRA [10], which are widely adopted in practice. LoRA fine-tunes a small number of trainable parameters (i.e., low-rank matrix updates) on top of the original, frozen pretrained weights.\nResult 7 Error correction is a skill that can be very different from the original (error-free) reasoning and cannot be acquired during a LoRA finetune stage from language models pretrained only using error-free data."}, {"title": "Result 8: Pretrain with Fake Mistakes", "content": "While it is possible to prepare \"perfect\" retry data on synthetic iGSM datasets, obtaining math data with mistakes and corrections can be challenging in real life. For this reason, we explore the possibility of using more realistic approaches to augment math problems (for which we only have correct solutions) with \u201cfake\u201d mistakes and retries. Ideally, this process should not require any semantic parsing or understanding of the problem and the solution.\nWe explore two approaches and compare them with the perfect retry data. We still use retry_rate, but instead of selecting a wrong parameter to retry, we simplify the process:\n\u2022 In the first approach, we randomly select a sentence that appears later in the step-by-step solution to retry.\nResult 8. The realistic, simple-to-obtain retry_weak data significantly improve the model's accuracy; yet, the slightly more complex retry_miss data does not improve accuracy by much."}, {"title": "Conclusion", "content": "In this paper, we investigate whether language models can benefit from pretraining on data containing mistakes, followed by immediate error correction. Using a fully controllable synthetic setting, we demonstrate that models trained on such data outperform those trained on the same amount of error-free data.\nIn addition to the accuracy gain, Section 4 shows that using retry data is very safe: the model rarely makes mistakes even after pretraining with high error-rate retry data, and it is unnecessary to change the training process (simply autoregressive, no need to label-mask the errors). Retry data teaches models how to correct errors if needed, rather than encouraging mistakes.\nIt is important to note that such error correction skill does not come easily. A model pretrained with only error-free data cannot use (1) beam search or (2) retry based on error detection (\u201cretry upon regret\") to achieve comparable performance, see Section 3, unless the error detection is nearly perfect. This error correction skill is also very different from the original error-free reasoning and thus cannot be learned during parameter-efficient fine-tuning (PEFT) such as LoRA, see Section 5.\nThis implies the necessity of adding retry data to the pretrain data for language models to truly learn the capability to correct errors."}, {"title": "Experiment Details and Parameters", "content": "Model. We use the GPT2 architecture [20], replacing its absolute positional embedding with modern rotary positional embedding [7, 24], still referred to as GPT2 for short. We also played with the Llama architecture (especially with gated MLP layers) and did not see any benefit of using it. This GPT2 performs comparably to Llama/Mistral at least for knowledge tasks [5].\nData Size. For both pretraining and finetuning, we did not limit the amount of training data; we generated new data on-the-fly. We do not explore sample complexity in this paper, such as the number of math problems needed to achieve a certain level of accuracy, as it would complicate the main message of this paper."}, {"title": "Pretrain Experiment Details", "content": "Pretrain parameters. Throughout this paper, when we pretrained a model (except for \"pretrain double-time\u201d), we followed the same pretrain parameter choices of [29]. That is, we used the AdamW optimizer with mixed-precision fp16, \u03b2 = (0.9, 0.98), cosine learning rate decay (down to 0.01x in the end + 1000 steps of linear ramp-up).\n\u2022 For pretraining on the iGSM-med datasets, we used a learning rate of 0.002, weight decay of 0.05, batch size of 512, context length of 768, and trained for 100,000 steps.\n\u2022 For pretraining on the iGSM-hard datasets, we used a learning rate of 0.002, weight decay of 0.03, batch size of 256, context length of 1024, and trained for 200,000 steps."}, {"title": "V-Probing Experiment Details", "content": "The V-probing for can_next(A) was introduced in Ye et al. [29]. It is a fine-tuning process upon the pretrained language model, with an additional linear head on the output layer, and a small rank-r update on the input (embedding) layer. The pretrained model is freezed, and only this linear head and the rank-r update are trainable parameters during the fine-tuning stage (for the probing task).\nIn this paper, our \u201ccan_next probing version1"}, {"title": "Finetune Experiment Details", "content": "LORA finetune. In Section 5, we applied LoRA finetuning [10] on a pretrained model using the new \"error + correction\" data. LoRA involves freezing the pretrained model and adding trainable low-rank updates to the weight matrices. It is recommended to apply low-rank updates to the query/value matrices [10] and the embedding matrix [4]."}]}