{"title": "Towards Advancing Code Generation with Large Language Models: A Research Roadmap", "authors": ["Haolin Jin", "Huaming Chen", "Qinghua Lu", "Liming Zhu"], "abstract": "Recently, we have witnessed the rapid development of large language models, which have demonstrated excellent capabilities in the downstream task of code generation. However, despite their potential, LLM-based code generation still faces numerous technical and evaluation challenges, particularly when embedded in real-world development. In this paper, we present our vision for current research directions, and provide an in-depth analysis of existing studies on this task. We propose a six-layer vision framework that categorizes code generation process into distinct phases, namely Input Phase, Orchestration Phase, Development Phase, and Validation Phase. Additionally, we outline our vision workflow, which reflects on the currently prevalent frameworks. We systematically analyse the challenges faced by large language models, including those LLM-based agent frameworks, in code generation tasks. With these, we offer various perspectives and actionable recommendations in this area. Our aim is to provide guidelines for improving the reliability, robustness and usability of LLM-based code generation systems. Ultimately, this work seeks to address persistent challenges and to provide practical suggestions for a more pragmatic LLM-based solution for future code generation endeavors.", "sections": [{"title": "1 Introduction", "content": "Code generation, also known as program synthesis, aims to automatically generate code based on specified inputs [1]. In recent years, Large Language Models (LLMs) emerge as a prominent solution to generate code from natural language descriptions, saving time for developers and serving as benchmarks for evaluation [3, 29]. Evaluation typically involves generating code from descriptions of natural language and validating it against the test sets [1, 11, 26, 28]. Other key tasks include code translation [33, 46] and comprehension [9]. However, the evaluation process may introduce bias since it heavily relies on experienced professionals. In the meantime, specialized models like CodeGen and DeepSeek-Coder have been developed for tasks such as code completion, optimization even debugging and reviews. These models demonstrate the growing potential of LLMs in automating programming tasks.\nDespite the wide usage for basic coding, professional programmers prioritize the accuracy and robustness of the generated code. It is identified that inherent shortcomings in LLMs, such as limited contextual understanding [10, 21] and hallucinations [18], may lead to contents with semantic and syntactic errors. To address these challenges, LLM-based agent frameworks are presented for self-refinement and enhancing the model's understanding of tasks and the accuracy of responses with prompt engineering. While they have shown significant improvements over baselines, they still face limitations and challenges across multiple studies.\nPrevious works explore the potential of LLMs for code generation and introduce benchmarks such as HumanEval [3] and MBPP [1]. The reasoning capabilities of LLMs have also inspired various survey works, highlighting their potential to automate mundane coding tasks, allowing program-mers to focus on higher-level activities [2, 19, 40]. However, these works place strong emphasis on model performance evaluation, offering limited insights into framework-level implementation and analysis. For instance, recent attention has been given to LLM-based agent frameworks [20, 40], yet the construction and implementation of these frameworks have not been thoroughly examined.\nIn this paper, we present our vision and reflections on the application of LLMs in code generation, featuring their current advancements while sharing our perspectives on future researches. We also explore the challenges in code generation from both technical and evaluation viewpoints, offering our proposals as potential solutions. Although extensive research has focused on LLMs\u2019 downstream tasks, particularly code generation, focusing efforts on fundamental objectives for pragmatic solutions over merely achieving higher benchmarks is crucial. Through this work, we aim to provide the community with a comprehensive overview of the current trends and challenges in LLM-based code generation, fostering deeper understanding and more impactful future research."}, {"title": "2 Vision for LLM-Based Code Generation", "content": "LLMs' capabilities have evolved from basic sentiment analysis to creative tasks [5], with significant improvements from early models like GPT-3.5 to the more advanced GPT-4, featuring a larger parameter scale, improved learning schemes, and deeper alignment with human feedback [22, 39]. However, existing works often highlight repetitive architectures and workflows. To investigate this, we propose a six-layer vision framework to distill core components and streamline the foundation for more efficient and practical solutions."}, {"title": "2.1 Conceptual Framework", "content": "Layer 1-3 Figure 1 presents a six-layer collective architecture for how LLMs handle code generation task in current works. The first layer captures user input or the task requirements; with common use cases highlighted in green. This flows into the second Model Invocation Layer, which encompasses different modeling approaches. For example, \u2018Cursor'\u00b9 act as an IDE using LLM APIs for code generation and basic chat functions, while models could be further fine-tuned specifically for code generation. Following that, the agent or model employs prompt engineering for planning and self-reasoning. We observe most LLM-based code generation studies include this step, frequently employing role assignment and task decomposition, as seen in MetaGPT, ChatDev's Chat Chain [13, 32]. Context Understanding involves techniques like code parsing and semantic analysis to build an internal representation of the project. Methods such as knowledge graphs and dependency analysis help map relationships among files, libraries, and APIs. Retrieval-Augmented Generation (RAG) [23] further enhance the model's knowledge base with external documentation and repositories.\nLayer 4-5 The fourth layer, the synthesis layer, involves code generation after planning and reasoning. Similar techniques are included in many agent frameworks to increase code accuracy, such as feeding feedback into the model's analysis to reduce computational overhead and ensure adherence to guidelines [4]. Additionally, multi-turn program synthesis, as in CodeGen [29], is another typical paradigm, with formats for synthesis including natural language-to-code and code-to-code translation. The fifth layer involves running the generated code. Many frameworks utilize external tools [45], while some studies advocate manual validation [43]. Validation can be categorized into two groups: using built-in test sets from benchmark itself, or test sets generated by LLMs [14].\nLayer 6 The final layer, Refinement & Debug, includes three key modules. A common method is iterative debugging, where the model performs self-refinement based on prior runtime results, often focusing on erroneous test cases for separate analysis [4]. Another feedback mechanism is user clarification, addressing the ambiguous task descriptions by requesting user input for clearer instructions [27]. As for failure handling, which is not yet widely used, it typically involves storing code snapshots in a registry [12]. If errors occur, the system can revert to a previous version, a workflow reminiscent of daily programming practices. If the result remains unsatisfactory after this layer, a common approach is to return to the planning and reasoning phase and repeat a similar process until the code passes tests or the maximum iterations limit is reached."}, {"title": "2.2 Our Vision", "content": "In Section 2.1, we discuss our perspective on how LLMs approach code generation, providing a detailed analysis of the overall architecture. Figure 2 illustrates our proposed vision workflow, structured into four phases: Input, Orchestration, Development, and Validation. Unlike the six-layer architecture, our vision does not aim to construct a concrete framework. Instead, it synthesizes insights from existing works to highlight how LLMs have fundamentally transformed software development, providing a comprehensive workflow that serves as a clear roadmap for future research.\nInput Phase Currently, LLMs in code generation predominantly follow a \u2018prompt in, code out' approach. Our vision expands this with multi-modal inputs, such as flowcharts as supplemen-tary diagrams, to better reflect real-world software development and reduce the communication overhead [17]. However, LLMs often generate erroneous or non-functional code due to lack of clear information [11, 30, 42]. Discrepancies in task descriptions can severely hinder the model's comprehension, resulting in outputs that fail to meet user requirements. To address this, our vision incorporates a clarity check process. Our approach advocates for \u2018frequent human-model interaction', allowing users to verify LLMs' understanding of tasks and clarify ambiguities. Similar to ClarifyGPT [27], the goal is to reduce hallucinations or speculative outputs from LLMs, and improve LLMs reliability for code generation.\nOrchestration Phase The concept of \u2018orchestration', originally from cloud computing [36], is now widely used in multi-LLM agent frameworks for collaborative operations [40]. Our vision introduces Dynamic Task Creation, enabling on-demand task generation and iterative refinement during development, leveraging execution results and human feedback. During the orchestration phase, the orchestrator LLM performs system-level comprehension of the current task list to dy-namically adjust the agents number based on task complexity. This step enables the agent to achieve a clearer understanding of the overarching task and workflow. Once the orchestrator attains a comprehensive understanding of the development tasks, it executes model adjustment to generate multiple agents to support subsequent development processes. These agents are then stored in a model cloud for further utilization.\nDevelopment Phase Traditional LLM-based code generation often focuses on module or function level outputs, and simulate workflows like Scrum or Test-Driven Development (TDD) [24]. However, for multi-agent frameworks, it can incur heavy overhead due to the crowd collaboration of agents. Also it lacks transparency since their intermediate processes are typically opaque (black-box), increasing the risk of introducing errors or security flaws [32]. Moreover, these methods lack the explainability that many developers and stakeholders require, as it can be difficult to trace how decisions are made or how code components evolve.\nOur vision workflow promotes frequent human interaction over autonomous generation. Since the ultimate goal of any software product is to serve human needs, consistent and deliberate user scrutiny remains indispensable. In our workflow, users (or development team) actively monitor the LLM's incremental coding process-testing and reviewing each module or function upon completion. This approach mirrors real-world practices, such as front-end developers visually inspecting features as they are written. Ongoing workspace monitoring enables users to collaborate with the orchestrator to address errors or unexpected outcomes, refining tasks and agents mid-development. The developer-in-the-loop paradigm combines human oversight with LLM capabilities, reducing black-box risks, enhancing explainability, and ensuring robust and efficient code generation.\nValidation Phase In our vision, this phase emphasizes system- level testing and human validation, as debugging and iterative fixes occur during the Development Phase. Once the code reaches a stable state, the orchestrator conducts system tests, verifying all components, including newly generated modules and integrated code, interoperate as intended under real or simulated production conditions. Successful system tests lead to human validation, where developers and users evaluate functionality, usability, and alignment with project goals. This step ensures that the product meets real-world expectations\u2014covering edge cases, performance targets, or business requirements that might fall outside automated checks. Should any discrepancies arise, the workflow routes back to the Development Phase, allowing the team to integrate the system test results and human feedback into further refinements or expansions of the code. The orchestrator then reassigns tasks based on these new insights, enabling a cyclical process of improvement."}, {"title": "3 Challenges and Reflections", "content": "In this section, we primarily explore common challenges prevalent across different current studies. Additionally, we provide suggestions and insights on how to mitigate or avoid such challenges."}, {"title": "3.1 Technical Challenges", "content": ""}, {"title": "3.1.1 Prompt Sensitivity", "content": "The advent of prompt engineering [38] has revealed greater potential for LLMs, enabling them to produce outputs better aligned with user needs. However, variations in prompt expression can yield significantly different responses from LLMs [30]. This makes code generation particularly unpredictable, especially for more complex tasks. It may be further exacerbated by lengthy prompts, increasing the risk of syntax and structural inconsistencies, commonly referred to as non-determinism. While reducing temperature can mitigate randomness, it can't fully resolve the issue, as no definitive solution currently exists [30].\nWhy this is a challenge? Research on code generation for both standalone LLMs and LLM-based agents often overlooks reproducibility in actual environment. Benchmarks such as HumanEval, MBPP, and APPS [11] involve short, self-contained Python tasks, requiring only simple prompt strategies such as chain-of-thought [37] or few-shot learning [35]. However, iterative debugging with a maximum iteration limit often encourages researchers to select the best result from multiple attempts, introducing randomness and making replication difficult. Such tendencies result in the ongoing difficulty in achieving reproducibility across studies.\nSuggestion To reduce discrepancies, task description should be specific with added constraints or guidelines to stabilize outputs. In frameworks where LLM-based agents diagnose errors through self-reasoning, the prompt must also be carefully constrained, with few-shot learning to further refine outputs. Additionally, incorporating clarity check from our vision workflow can improve task clarity and reduce hallucinations caused by vague or incomplete prompts."}, {"title": "3.1.2 Usability & Consumption", "content": "Usability spans the developer's experience, real-world implemen-tation scenarios, and associated costs. Whether in Q&A formats (an LLM translating between programming languages) or IDE plugin, LLM-based tools permeate daily programming tasks. How-ever, researches show developers using such plugins do not necessarily see large efficiency gains compared to those who do not [41]. This raises a critical question: How can we genuinely enhance the usability of these approaches?\nWhy this is a challenge? Recent trends show a research shift from pure LLM models to LLM-based agents [20], focusing on applied, macro-level exploration rather than raw model performance. Multi-agent frameworks have recently emerged, harnessing multiple agents for increased flexibility and accuracy [40]. While these approaches score impressively on various benchmarks, they often require extensive agent-to-agent interaction, which is time-consuming and struggle to adapt to shifting requirement midstream. Additionally, the \u2018black-box' nature hides internal reasoning and role iteration, reducing trust and comprehension for developers.\nMeanwhile, as these frameworks scale, developers may invest more effort orchestrating LLM workflow (crafting prompts, clarifying instructions, building test harnesses) as they would coding manually. Inaccurate output further delay the task with manual debugging. A related concern is whether LLMs proactively request missing information when faced with ambiguities, rather than guessing. Recent community discussions\u00b2 emphasize effective agent-human collaboration requires models to recognize knowledge gaps and ask right questions at the right time. Unfortunately, most multi-agent systems still lack robust mechanisms to detect or flag such uncertainties, forcing developers to retroactively diagnose issues.\nSuggestion Studies indicate LLMs perform worse on class-level generation compared to function-level tasks [7]. We argue that future research should prioritize practical usage cases, such as deeper IDE integration and real-world evaluations, rather than only assessing one-off code completions. Token consumption also increases with multi-agent collaborations, as model calls and shared context grows. Effective retrieval-augmented generation and precise content extraction are vital to expand the model's knowledge base without excessive overhead. Moreover, frameworks should bolster the model's ability to detect inaccurate or missing information, promoting clarification or offering alternatives. Developers often avoid LLM-based code generation due to the time spent aligning with and debugging outputs, which negates any efficiency gains. If the model assumes certain functions or libraries exist, usability declines [19, 32]. Our proposed vision workflow (Subsection 2.2) addresses these issues by focusing on project-level tasks and adopting a dynamic model cloud. Frequent developer-orchestrator interactions help reduce token cost while improving explainability and code comprehension by revealing the model's assumptions in real time."}, {"title": "3.1.3 Code Security", "content": "Research on the security of LLM-based code generation has received little attention. Most works focus on bias or jailbreak vulnerabilities in model conversations [6, 8], especially for closed-source LLMs, but seldom addresses security concern in code-generation contexts. However, this does not imply that generated code is fully trustworthy or free from hidden security risks in text-based outputs.\nWhy this is a challenge? In fact, LLM-based code generation can inadvertently introduce security risks by generating unsafe logic or code with latent vulnerabilities [31]. It arises not from malicious intent but from the model unintentionally producing insecure code. Furthermore, LLMs often rely on external knowledge sources that may be compromised, exposing outputs to risks like phishing if references (for example, a pip install link) are tampered with [15]. The code itself typically does not undergo robust testing; using unverified code (lacking integration tests) in a real development environment that handles sensitive data poses significant risks. Moreover, with LLMs now autonomously calling external APIs and utilizing extensive training data, novel attack surfaces are introduced where backdoor attacks can be placed [44]. Multi-agent frameworks further expand the attack surface via user prompts or retrieval data, while current security measures focus on final output, leaving intermediate workflows vulnerable.\nSuggestion We suggest future research prioritize code security, together with code functional correctness. A module-level approach can be developed to robust vulnerabilities testing. Further-more, research has indicated that model-generated code can exhibit biases [16], highlighting the importance of addressing such issues during the training process."}, {"title": "3.2 Evaluation Challenges", "content": "Evaluation is one most crucial step in code generation, which ensures that the generated code contains no syntactic or semantic errors and actually fulfills the requirements described in the task.\nWhy this is a challenge? Currently, there are few widely-used benchmarks for code generation. Examples include HumanEval+ from EvalPlus [25] and CodeContests, both of which come with test sets ranging from basic function problems to competition level tasks. However, these benchmarks reveal a major limitation: most focus on function-level or single-file tasks, which do not reflect the complexity of real-world software development. In addition, the majority of these benchmarks lack sufficient edge cases, making it difficult to catch hidden bugs or vulnerabilities [25]. This explains why some studies, despite attaining high pass rates on MBPP or HumanEval, stuggle with more complex problems.\nMoreover, evaluation metrics themselves present another challenge. Many existing works rely on simple metrics like Pass@K [46], which does not account for efficiency, performance, or main-tainability. Consequently, the model may show partial correctness but fail in more realistic settings. In real-world projects, test sets are often too small or nonexistent\u2014some specialized scenarios (e.g., those akin to Alfworld [34]) remain untested in code generation tasks, further reducing the scope of usability evaluation.\nSuggestion We argue that future research should aim to create more class-level or multi-file benchmarks that better simulate real-world project development. These new tasks should include more complex dependencies and cross-module interactions, enabling us to test how well LLMs handle multi-step integrations instead of trivial function stubs. Existing test sets could be augmented with additional extreme conditions to increase robustness, as demonstrated by EvalPlus [25]. Beyond simple pass/fail tests, we should also measure the readability and maintainability of the generated code-possibly using agent frameworks to integrate external tools for deeper evaluations. Finally, domain-specific benchmarks (e.g., for financial software) would allow us to reflect real-world demands more accurately."}, {"title": "4 Conclusion", "content": "In this paper, we present our vision and reflection for applying large language models in code generation task, examing the technical and evaluation challenges that persist in current research works. We introduce a six-layer collective architecture for how current LLMs researches address code generation task, as well as our vision workflow. Ultimately, we anticipate this work provides a comprehensive landscape of current LLM-based code generation approaches, highlights ongoing critical challenges concerning both technical and evaluation perspectives, and offers practical guidelines for future solutions."}]}