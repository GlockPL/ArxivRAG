{"title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models", "authors": ["David Castillo-Bolado", "Joseph Davidson", "Finlay Gray", "Marek Rosa"], "abstract": "We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.", "sections": [{"title": "1 Introduction", "content": "The capabilities of Large Language Models (LLMs) have been primarily evaluated through isolated tests (see \u00a76), focusing on increasing data volumes and context size (c.f. [Su et al., 2024, Chen et al., 2023, Peng et al., 2024, Zhang et al., 2024a]). However, these tests are single shot and single topic (Figure 1), and therefore often overlook the most common user interaction mode: chat-based conversation.\nIn response, we introduce the LTM Benchmark\u00b9, an automated system designed to evaluate the Long-Term Memory (LTM) and Continual Learning (CL) capabilities of conversational agents. The LTM Benchmark engages agents in a single, prolonged conversation, incorporating multiple tasks and distractions to simulate realistic and meaningful interactions. This approach provides a more comprehensive assessment of an agent's ability to effectively use and integrate information across extended dialogues. Furthermore, we show that this \u201cconversational multitasking\u201d structure significantly degrades the test performance of LLMs, indicating that their real-world capabilities are not fully revealed through most contemporary benchmarks. The LTM benchmark is primarily synthetic, and can be generated to contain conversations of arbitrary length, which may potentially surpass the context size of the LLMs under test.\nOur contributions can be described thus:\n\u2022 An automatic benchmarking system that evaluates an agent by conversing with it, interleaving all tests in a single conversation."}, {"title": "2 Definitions and Terms", "content": "Needles and Haystack. Terms introduced by the Needle in a Haystack (NIAH) benchmark [Kamradt, 2024]. Needles are sentences in the context that are relevant to the task, and which are required to be found and retrieved. The term haystack refers to the remaining (usually unrelated) text, that acts as a spacer or distractor, depending on the complexity of the hay. In this paper, we use \u201cneedles\" to refer to all sentences that contain information relevant to the task.\nInformation Integration. The ability to take multiple related needles from different locations (either context, or some other memory system) and integrate them into a complete and consistent view. This often implies a continual and iterative process of revision of past knowledge, in the search for novel insights or conclusions.\nMemory Span. The amount of information that a model is able to take in order to produce a response. In the case of LLMs, this is upper bounded by their maximum context sizes, which determines how much text they can take as input. We also employ this term to refer to how much space in the conversation a specific test uses. For example, we may say that the memory span for a test is 120k tokens, which means that the tested agent will find all necessary information within the latest 120k tokens of the conversation.\nLong-Term Memory (LTM). The ability to recall and effectively utilize past information. LTM encompasses several skills related to the generation and management of memories, which include but are not limited to recall, information integration, and the handling of conflicting information. This broad definition acknowledges that different implementations of systems or agents may exhibit such ability in different degrees. Current implementations of LTM commonly rely on an LLM as"}, {"title": "3 The LTM benchmark", "content": "The LTM Benchmark is a means for evaluating the memory and information integration capabilities of an agent via a single written conversation. It does not make any assumptions about the underlying technology or implementation of the agents being tested. Instead, the focus is on benchmarking the agent's skills from a purely functional perspective, setting only two broad requirements: 1.) The agent must be able to communicate in written English, and 2.) The agent must send a message exclusively when prompted, providing a single response message to each message received.\nBy following this approach we aim to contribute an objective evaluation tool, capable of assessing the capabilities of current and future agents, and pointing to the most relevant challenges towards the next generation of conversational agents.\nOver the course of the test interaction, the agent is presented with various pieces of information at different points in time, and is asked questions or challenged with situations that rely, directly or indirectly, on this information. We have equipped this benchmark with an initial battery of tests which reflect currently challenging skills, but we expect the benchmark to evolve according to the needs of the research community, discarding tests that become trivial and incorporating new ones that evaluate newly identified and more challenging capabilities.\nAs a mostly synthetic benchmark, the generation processes is both transparent and reproducible. As such, we have released all benchmark results and generators. The benchmark is dynamic, but all generation and scheduling processes are deterministic. The one source of nondeterminism in the testing is from the agents themselves. The length of agents' replies will have knock-on effects in the amount of filler tokens that are used, and that in turn may influence the replies of the agent. More details on this and our mitigation strategies can be found in the supplementary material."}, {"title": "3.1 Test structure", "content": "Every test in the benchmark is automated and takes place as part of the same conversation. Unlike other LLM-oriented benchmarks, our benchmarking system does not isolate tests as separate prompts, but intertwines them in a continuous exchange of messages between the agent and user. This approach aims to keep the conversation as natural as possible, as we hypothesise that the results obtained in this way will be more likely to generalize to chat-based interactions.\nThe conversation begins with a message explaining the overall situation to the agent, and continues with all tests' messages and agent responses (see example in Figure 3). Every test encompasses a series of interventions from the benchmark system (taking the user's role), which are uniformly distributed across a previously-delimited area in the conversation known as the memory span of the test (see Figure 2). The test's span can be set independently from the agent's effective input size, which allows for tests surpassing current LLMs' context lengths. Other constraints like token and time waits (see episodic memory and Jokes scenario in \u00a73.2) also form part of the test information, which is provided to the benchmark's scheduling system.\nTo disperse test messages, the benchmark system uses distraction messages, which are messages from other unrelated tasks. These unrelated tasks are preferentially other tests' messages, which greatly improves the benchmark's efficiency, but sometimes it is just not possible to make use of a message from another test and we have to rely on dummy tasks. The dummy task that we rely on is the extraction of answers from the TriviaQA dataset [Joshi et al., 2017] (Figure 4).\nA test can be arbitrarily complex depending on its content and definition. Test's messages are either delivered in a static manner (providing a script) or dynamically (via a functional description), depending on the requirements of the testing scenario. The LTM benchmark's scheduling system coordinates the execution of tests and weaves them together into a seamless conversation, while respecting the memory span, test compatibility constraints and optimizing the use of tokens. Finally, once the last test question is answered, the test is scored according to the agent's performance. The complexity of this scoring mechanism can vary greatly too, from a simple pattern matching state-"}, {"title": "3.2 Test scenarios", "content": "We have included an initial set of tests that evaluate a series of memory and learning skills. Inspired by research in psychology and cognitive science, we have built meaningful and organic test scenarios, which abstract away the agent's implementation and focus on the agent's abilities to recall and effectively leverage past information. All test scenarios evaluate a subset of the following skills:\n\u2022 Recall (R). Simple retrieval of past information, usually based on content matching.\n\u2022 Conflicting information (C). Managing of information that contradicts existing knowledge or previously acquired information.\n\u2022 Episodic memory (E). Addressing of memories based on temporal information.\n\u2022 Spatial memory (S). Association of memories to positions in some space.\n\u2022 Prospective memory (P). Memories that address future or hypothetical situations.\n\u2022 Theory of mind (T). Memories pertaining to a third persons' thoughts or feelings.\n\u2022 Information integration (I). The act of combining different pieces of information in order to construct a complex or more abstract overall picture.\nAnd in order to assess these skills, we have built the following test scenarios:\n\u2022 Colours (R): We state our favourite colour several times and in different ways, changing preferences every time. Finally, we ask the agent what our favourite colour is.\n\u2022 Name list (R, C): We change names multiple times, then instruct the agent to recall the complete list of names that we have gone by.\n\u2022 Jokes (E): We tell the agent a series of jokes at different and spaced points in time, the agent is then asked to recall the joke told to it a given number of hours and minutes ago.\n\u2022 Locations directions (S, I): We give the agent a series of locations from a fictional town. Each statement places a new location into the town relative to the previous one. Finally, the agent is asked how to get from the first place to the last."}, {"title": "4 Results", "content": "We have produced a standard configuration of the LTM benchmark and use it to evaluate and compare some of the current leading LLMs in addition to a baseline LTM system. For the configuration, we have included three repetitions of each scenario, and we have set the number of needles for the scenarios requiring it: in Colours, three changes; in Name List, five different names; in Jokes, four jokes; in Locations Directions, the path goes through six locations; and in Shopping List the list updates six times. We run the same benchmark with a number of different memory spans: 2k, 32k, 120k, 200k, and 500k tokens. Additionally, we include results for the same tasks evaluated in isolated scenarios, where there is neither task-switching nor distractors in the conversation (resembling standard benchmarks). We also exclude ChapterBreak from the 2k configuration to avoid exceeding the memory span.\nEach benchmark run consists of 33 tests (11 scenarios and 3 repetitions). We average the results corresponding to repetitions of the same test type, and compute a standard deviation out of those scores. In total, an agent can achieve up to one point per scenario, which makes for a maximum score of 11 points.\nThe benchmarks are executed using a variety of on-demand APIs from various providers, including Google, OpenAI, and Anthropic. The open source models were tested via TogetherAI's API. Running and evaluating all the benchmarks in this paper cost \u2248 $5,500 and took 116 hours.\nOur baseline LTM agent\u00b2 uses a vector database for storing memories, which it queries before every response. Additionally, it has a JSON scratchpad which it can use for keeping volatile or dynamically-changing information, and utilizes query rewriting for the vector database. The agent"}, {"title": "5 Analysis", "content": "In general, the benchmark has proved to be comprehensive, representing a significant step-up in terms of evaluation results that are robust and hopefully translate to real-world scenarios. One example of this is in GPT-40, which achieves 100% NIAH coverage and nearly perfect results on the NIAN test [Burns, 2024], yet it scores similarly to GPT-4 turbo in our benchmark. This corroborates OpenAI's claims [OpenAI, 2024] and anecdotal evidence which describes GPT-40's interactivity to be better, but its overall capabilities to be in line with GPT-4 turbo [Yan et al., 2024, ArtificialAnalysis, 2024].\nWe have found the conversational format to be the main challenge for current LLMs (and especially the open source ones), which work better on short and clean prompts. The results for the isolated tests vs. the interleaved 2k tests reveal the interleaving of tasks as a key factor. There is a minimal difference in the presentation of the tests outside of the interleaving, yet performance drops to below half for two of the the open source LLMs. Commercial LLMs are more robust to the interleaved scheme, which may point to how their training is performed. It is worth noting that longer input sizes allow for both the tackling of longer tasks and the presence of more distractors.\nLooking at the results in detail (see the supplementary material), the Restaurant task was the most variable one. Claude had trouble with the task by being too expressive, adding \"stage directions\" to its responses. The Mixtral models also often hallucinated interactions between the waiter and diner on their own, resulting in failures. A lot of models that otherwise fail completely on the other benchmark tasks, can get some credit for the first part of the Restaurant task where they are greeted, shown a menu and asked for a drink order in a single message. If the agent plays along with the scenario, it will obtain some score. Our automated evaluation system (powered in some parts by GPT-4 turbo) also fails at some points, like being unable to distinguish between dishes that are discussed, and ones that have been ordered.\nAlso challenging was the prospective memory tests, where the agents had to append a quote at a future point in time. The most common failure was in the agent not being able to count its statement correctly, usually resulting in an off-by-one error. We have noted that LLMs using special tokens for message delimiters are those which struggle the most in counting them. We believe that this phenomenon might be related to those tokens being rare in the training data.\nIn the Locations directions task, agents were not explicitly asked to visit all locations, but most agents either failed the task or replied with an almost exact replica of the original directions. Notably, merely recalling (and potentially rephrasing) those sentences does not require any kind of spatial thinking.\nTraces of pattern-completion behaviour can be seen in all benchmark conversations, in the form of early responses heavily influencing later ones. These later responses usually continue the format and thinking patterns established previously, and an early good response strategy (e.g. listing all items in the shopping list after every update) often translates to all task repetitions succeeding. We have also observed the mimicking of reluctant responses or growing preceding spaces in responses.\nSometimes, and specially with long-context LLMs, mixing information from different repetitions happens. Simple recall does not solve this issue, which requires careful (and ordered) reading of all potentially-relevant information. This seems to imply some sort of iterative processing of the input, which current transformers-based architectures are not very good at. Current LLM architectures are very similar, and yet significant differences can be seen. Such differences mostly originate in the training procedure, but architectural choices can be decisive too. To give some examples, Gemini 1.5 Pro is mostly incapable of solving ToM tasks, but it also exhibits better-than-average spatial reasoning. Llama 3 excels when the prompt is short and clear, and it can beat the best commercial LLMs in this regime, being especially well suited for LTM agents. Mixtral 8x22B also performs exceptionally well in short conversations.\nThe guardrails on commercial LLMs frequently appear in responses to longer versions of the benchmark tests. Safety measures tend to go off when the tests ask about any kind of personal info or datum that might seem out of the agent's reach, even if the data is somewhere in context. This refrain of \"As an AI language model...\" could be a default result of the imperfect attention trying to read personal information from the context."}, {"title": "6 Other benchmarks", "content": "Most LLM benchmarks rely on one-shot evaluations\u2074 (Figure 1). Prefilled contexts have various token lengths, e.g. up to 18k in LongBench [Bai et al., 2023], \u2248 36k in LooGLE [Li et al., 2023], and 200k in InfiniteBENCH [Zhang et al., 2024b]. A common shortcut is to take data points from existing datasets, but it is possible that they are part of the pretraining dataset for a model [Jacovi et al., 2023, Yang et al., 2023]. Datasets like HotpotQA, 2WikiMultihopQA, Qasper, and NarrativeQA [Yang et al., 2018, Ho et al., 2020, Dasigi et al., 2021, Ko\u010disk\u00fd et al., 2017] are popular choices for test data and included in LongBench. Mitigation strategies include the use of recent real-world data (LooGLE) and the handcrafting of questions (LongBench). Also based on human-created questions there's L-Eval [An et al., 2023], which aims to bridge the gap between commercial and open-source models.\nNeedle in a Haystack [Kamradt, 2024] is a common pattern for evaluating LLMs' retrieval skills. Often those using NIAH use their own data and needles, which is an issue because some works have shown that the complexity of the \"hay\" matters [Pekelis, 2024]: repetitive haystacks or incongruous"}, {"title": "7 Limitations and future work", "content": "Despite the interesting results, there are a number of limitations that require further development. This benchmark is also intended to be a living benchmark, which calls for continued work in updating the tests for targeting new skills.\nAgents' robustness in this paper are based on only three repetitions, which is not very reliable. The benchmark has a high time and financial cost to running it, and therefore multiple repeats have proven currently infeasible. Also, our episodic memory testing sometimes uses time jumps, which require the agent to run on the same machine as the benchmark system.\nGiven the breadth of possible responses to any question, the automatic evaluation systems can perform poorly in some edge cases, requiring manual checking. Summarized reports facilitate the manual revision, but in the future we aim to make evaluations infallible. See the supplementary material for examples where the automated checking has failed, and how we have manually evaluated the tests.\nFuture work includes addressing the noted shortcomings and adding more tests, which include but are not limited to 1.) Learning and recalling instructions or facts, especially focusing on forward and backward transfer; 2.) Stress testing with large amounts of information to integrate, 3.) More research-based tasks where multiple passages need to be recalled and integrated to answer correctly; and 4.) Multi-modal evaluations."}, {"title": "8 Conclusions", "content": "In this paper, we identified an issue with the current suite of benchmarks for LLMs, namely that the tests they run do not necessarily reflect real usage as chat agents along with the complexities that a chat environment brings. In response to this we have presented a new benchmark that subjects conversational agents (LLM-based or not) to a series of tests over a single lengthy conversation in order to test their memory and ability to integrate information. We benchmark current SOTA large-context LLMs, as well as LLMs equipped with a basic LTM system.\nWe observe that while all scores drop as the benchmark length increases, the scores of the agents using an LTM drop less precipitously, which suggests that the combination of an LLM using a shorter context alongside an LTM system may provide a \u201cfocusing\" effect for the LLM.\nAdditionally, we show that our task-interleaving approach makes the benchmark significantly more difficult, with scores that vary up to 1.5 points between a benchmark with interleaved tasks, and one with a more traditional isolated task regime. The open source LLMs had particular trouble with this, which may reveal how commercial LLM training is structured.\nThe benchmark has been open sourced on GitHub, and we plan to continually update it as the capabilities of LLM and LTM systems mature."}, {"title": "9 Societal Impacts and Ethical Considerations", "content": "The eventual societal impact of LLMs as a whole is hard to predict. Aside from the current potential harms of the vast energy resources required to create, maintain, and run these systems, our work may potentially have some impact on the trajectory of development.\nSome benchmarks like NIAN have already fallen afoul of Goodhearts law: \"When a measure becomes a target, it ceases to be a good measure.\"[Strathern, 1997] and it is possible that ours may do the same. However, because our benchmark specifically targets memory and learning through conversation, an LLM system that is designed to do well on our benchmark may also become proficient at personal Secretary or companionship behaviours. Such results could then lead to the loss of human based relationships in favour of LLM based ones.\nWe make use of ChapterBreak, which is a collection of works from the PG-19 dataset[Rae et al., 2020] and a dump of collected fanfiction from Archive of our own (AO3). One of AO3's stated positions is that all of the works hosted by it are public domain. PG-19 sources its data from Project Gutenberg which is a repository for explicitly public domain works. The datasets are filtered somewhat for stories suitable for a general audience, but these tags are the responsibility of the authors, so some inappropriate content may be in the data. The samples curated for the benchmark were mostly done for solvability, but those samples didn't contain content that the authors would consider offensive."}]}