{"title": "Integrating Natural Language Prompting Tasks in Introductory Programming Courses", "authors": ["Chris Kerslake", "Paul Denny", "David H. Smith IV", "James Prather", "Juho Leinonen", "Andrew Luxton-Reilly", "Stephen MacNeil"], "abstract": "Introductory programming courses often emphasize mastering syntax and basic constructs before progressing to more complex and interesting programs. This bottom-up approach can be frustrating for novices, shifting the focus away from problem solving and potentially making computing less appealing to a broad range of students. The rise of generative AI for code production could partially address these issues by fostering new skills via interaction with AI models, including constructing high-level prompts and evaluating code that is automatically generated. In this experience report, we explore the inclusion of two prompt-focused activities in an introductory course, implemented across four labs in a six-week module. The first requires students to solve computational problems by writing natural language prompts, emphasizing problem-solving over syntax. The second involves students crafting prompts to generate code equivalent to provided fragments, to foster an understanding of the relationship between prompts and code. Most of the students in the course had reported finding programming difficult to learn, often citing frustrations with syntax and debugging. We found that self-reported difficulty with learning programming had a strong inverse relationship with performance on traditional programming assessments such as tests and projects, as expected. However, performance on the natural language tasks was less strongly related to self-reported difficulty, suggesting they may target different skills. Learning how to communicate with Al coding models is becoming an important skill, and natural language prompting tasks may appeal to a broad range of students.", "sections": [{"title": "1 INTRODUCTION", "content": "Introductory programming courses traditionally focus on teaching students to write code in a bottom-up fashion, starting with mastering syntax and basic constructs, and gradually progressing to building more complex and interesting programs. Problem solving is often considered the most engaging aspect of programming, but the difficulties novices face with syntax, errors, and low-level code-layout issues can detract from this focus and cause frustration making computing courses less appealing to a diverse range of students [3]. Moreover, the widespread use of generative AI for producing code raises questions about when and how these tools should be introduced in introductory courses [23].\nLarge language models (LLMs) have shown impressive capabilities for solving computational tasks when provided with appropriate natural language prompts [6, 11, 24]. Thus, there are two emerging skills that students need to develop in this generative AI era. The first is being able to construct clear, unambiguous prompts to express desired solutions to computational tasks, and the second is understanding and evaluating the code generated by these models, to verify that it is indeed solving the intended tasks. While students may now be developing these skills independently of the curriculum, there is value in explicitly teaching students how to construct effective prompts [9].\nIn this experience report we describe the inclusion of two kinds of prompt-focused tasks alongside traditional activities in an introductory programming course. Both kinds of tasks involve students writing only natural language prompts for an LLM. The first task involves students solving computational tasks by writing prompts to generate code. This is a very authentic activity in today's landscape, with a focus on problem-solving rather than on code syntax. The second task involves showing students a code fragment and asking them to demonstrate their understanding of the code by crafting a prompt that generates equivalent code. The two tasks are complementary, as the first allows students to explore the relationship between computational problems and high-level prompts, and the second allows students to explore the relationship between"}, {"title": "2 RELATED WORK", "content": "Developing the ability to comprehend and communicate the behaviour of code, though always considered an important set of skills for novice programmers to develop [1, 30], is necessary to work effectively with LLMs [10, 24, 27, 28]. This requires 1) the ability to describe the requirements of a problem with sufficient detail for it to be implemented in code and 2) the ability to understand the purpose of code. The former is needed as poorly constructed prompts are less likely to generate desired solutions [6] and the latter is needed to evaluate the code that is generated [5, 24].\n2.1 Teaching Prompting\nTo develop student skills in expressing problems effectively, instructors have explored various tasks to provide students practice with prompting in formative contexts. Denny et al. [7] introduced \"Prompt Problems\", an activity where students are shown visual representations that illustrate specific instances of a task, asked to infer the general problem from the specific cases shown, and then provide a prompt that generates code that performs the task. The generated code is then graded based on an instructor-defined suite of test cases to determine if the generated code is functionally correct [7]. Nguyen et al. [22] evaluated similar activities where students were shown input-output pairs, asked to infer the task being performed, create a prompt that generates code with that functionality, and then evaluate if the generated code is correct. Their findings highlight that many students struggle to form successful prompts, understand generated code, and have poor mental models of generative AI, which hinders their ability to form effective prompting strategies [22].\n2.2 Explain in Plain English Questions\nPrompting an LLM to generate code has some similarity to the 'Explain in plain English' (EiPE) task, which is commonly used to assess student comprehension of code [21]. In these activities, students are given a code sample and asked to describe the purpose of the code [30]. EiPE activities are designed to focus on the code's high-level (abstract) purpose rather than the details of implementation (i.e., the mechanics of how it achieves the purpose). Unfortunately, novice programmers often struggle to describe the purpose of code in this way [4] suggesting greater emphasis should"}, {"title": "3 APPROACH", "content": "In this paper, we incorporated two kinds of natural-language prompting tasks (see Figure 1) into a course covering traditional CS1 topics and explored student perceptions and success with these tasks in comparison to more traditional programming-focused assessments (both invigilated and non-invigilated). The course was taught over a 12-week semester, although we focused on integrating these new tasks during the second half of the course (weeks 7-12). This timing allowed students to reflect on their progress throughout the first half of the course (weeks 1-6), including reporting how difficult they found programming to learn, before any exposure to the natural-language tasks.\n3.1 Course Context\nThe course, {Anon}, is taught at {Institution Anonymized} which is a large research university in {Country Anonymized}. The course is designed for engineering students and is structured into two modules, each spanning 6 weeks. The first module covers typical CS1 topics, including variables, arithmetic, arrays (vectors), functions, control flow, and basic algorithms using MATLAB. The second"}, {"title": "3.1.1 Student Participants.", "content": "Following approval by the university's human ethics committee, data was collected from 861 students enrolled in the course. Most students have no formal programming experience, but some enter with prior experience based on their choices from high school."}, {"title": "3.2 Assessments and Reflection", "content": "There are three large, invigilated assessments in the course (one test for each module and a final exam), which account for 56% of the final grade in the course. The course also includes weekly programming-focused lab sessions (24% weighting) and one project (10% each) for each of the two modules, all of which are non-invigilated.\nAfter the first module covering MATLAB, students were asked to reflect on their experience learning programming and respond with the extent to which they agree with the statement:\n\u2022 I find programming difficult.\nResponses were collected using a standard 5-step Likert-response scale from \"Strongly disagree\" (SD) to \"Strongly agree\" (SA).\nIn addition to this question at the beginning of the second module in the course, each lab session included an optional post-lab survey that invited students to comment on any aspect of the lab."}, {"title": "3.3 Natural Language Tasks", "content": "We incorporated two kinds of natural language tasks across four of the six weekly lab sessions in the second half of the course. Eight 'Explain in Plain English' (EiPE) tasks were included in Labs 8 and 10, and six Prompt Problem tasks were included in Labs 9 and 12. Table 1 summarises these 14 problems.\n3.3.1 Explain in Plain English (EiPE) Questions. In the tradition of EiPE questions [13, 21], for each task, students were presented with a single function and instructions indicating that they should describe the function in plain English (see Figure 2). To prevent giving away the code's purpose, the variables were replaced with generic names and each function was named foo. Tasks were delivered"}, {"title": "4 FINDINGS", "content": "We organize the findings using students' self-reported answers to the survey question \u201cI find programming difficult\u201d collected at the end of Lab 7 since this provides an intuitive grouping of like-minded and potentially like-skilled students. We started with 861 students, and removed any who did not complete each of the required labs (8, 9, and 10), or who missed any of the course exams or projects, which resulted in 726 students for analysis.\n4.1 Summary of Difficulty Data\nA common performance pattern emerged in the course based on students' reported difficulty with programming. As shown in Table 2, students who did not report programming as difficult (Disagree or Strongly disagree) on average scored the highest on all exams and projects. In contrast, students who reported that programming was difficult (Agreed or Strongly agreed) scored the lowest on average on all exams and projects."}, {"title": "4.2 Student Attempt Success", "content": "For both the EiPE and Prompt Problems, we calculated the success of each student's first attempt to solve the problem (First) as well as whether they eventually were successful for each question, along with the number of attempts (Tries in Table 2). The first-attempt calculations were viewed as similar to how students would engage with problems during exams, so we compared the two to see whether students performed similarly on each or not. The final attempt (Final in Table 2) was calculated for whether students completed the tasks, similar to the course projects. The number of attempts for each type of task was calculated to determine how many attempts each of the different difficulty groups spent on the two types of problems. As detailed in Table 2, all student groups were much closer in performance on the LLM problems (Figure 5) than they were in their invigilated exam performance (Figure 4)."}, {"title": "4.3 Examples of Code Explanations", "content": "The code descriptions shown in Table 3 were provided by students during Lab 10 in response to code that reversed a string (ReverseString). Students provided varied prompts, but interestingly, students at both ends of the difficulty scale provided similar prompts (e.g., students 19 and 1). Additionally, some students relied on direct citation of the code as they saw it, as shown by student 105. Also of interest is that student 77 used the term \"backwards\" which did not result in the LLM successfully generating functionally equivalent code; in contrast to how a human might understand and interpret their meaning. However, student 13's response also used \"backwards\", but incorrectly. The implications are that a human reviewer might be able to understand the nuanced use of the term \"backwards\", but in this specific situation, the LLM could not."}, {"title": "4.4 Student Comments on Programming", "content": "At the end of Lab 7, students were asked to complete a feedback survey about the course. One question asked students what they enjoyed most and found most frustrating about programming. The most common enjoyment was problem-solving, and the most common frustration was debugging. As shown in Figure 6, problem-solving was the most common enjoyment for students who reported"}, {"title": "4.5 Student Perceptions", "content": "At the end of each LLM-related lab, students were asked to provide their thoughts on the lab and the activities. Several students provided feedback after the EiPE questions at the end of Labs 8 and 10, shown below. Overall, students were predominantly positive about the experience, with a single student calling the task \"gimmicky\" and less effective at assessing their performance than traditional code-writing tasks.\n\u2022 Positive: \"I thought the code comprehension task was good, because it encourages understanding of code logic without the pressure of having to write code or debug. It also helps to improve my ability to communicate what a piece of code does by forcing me to write clear and concise explanations of code that can be easily understood.\" (402, Strongly Agree; Difficult)\n\u2022 Positive: \"I found the starting of the code comprehension task really difficult. This was because as I was doing it I was over complicating it each time. However with more attempts and practice it became easier. I really liked this task as it helped me on how to understand and condense my understanding with basic concepts in coding.\" (329, Agree; Difficult)\n\u2022 Positive: \"[EiPE problems] also enlightened me on the different solutions that could be available to solve a specific task\" (510, Neutral)\n\u2022 Negative: \"Dislike the [EiPE] tasks. Feels gimmicky. It's to me as if I saw this new Al-powered tool, and I'm using it just because I learnt about it [rather than for the tool's merits]. I don't think it proves any student performance, and the code-writing based lab tasks prove our understanding much more.\" (350, Disagree/Strongly disagree; Easy)"}, {"title": "5 DISCUSSION AND TAKEAWAYS", "content": "As noted by Porter & Zingaro [23], the introduction of LLMs suggests re-evaluating how we teach introductory programming courses. Additionally, with increased class sizes, the need to automate learning at scale highlights the importance of identifying techniques and tools that can help students assess and improve their code comprehension abilities. However, they also note that LLMs do not replace the need to understand what the code does when generated, nor does it remove the need for being able to explain the problem sufficiently to produce the desired results. The tasks we have analyzed in this work - Prompt Problems and EiPE questions - aim to teach students these skills explicitly via generative AI, and provide automated feedback that can support their use at scale.\nRecent work has shown that the introduction of generative AI into computing classrooms is negatively impacting student programming skills like code writing and debugging. Jost et al. found a significant negative correlation between increased use of LLMs for coding tasks and lower critical thinking skills as well as a decrease in final grades [15]. Prather et al. found that generative AI code completion tools like Copilot and ChatGPT can benefit some students who are already confident in their programming abilities, but that it can be directly harmful to the programming problem solving ability of students who are not [26]. Students who already have difficulty with programming, like many of the ones we examined in this study, seem to be the most poised to over-rely on generative AI [20] and its harms could be compounded upon them.\nOur findings suggest that natural language tasks could 'bridge the gap' between students who struggle with traditional assessments and those who do not, which could engage a broader range of students and possibly address the harmful impacts of generative Al on this group. Additionally, most students found the natural language programming tasks positive, indicating it is not useless for those who do not struggle. Most students required multiple attempts to craft a prompt that correctly solved their given task, similar to prior work [7, 22], which provides additional support for explicitly teaching students 'prompt engineering. Previous work has found that this kind of iterative learning while solving Prompt Problems could support the development of metacognitive skills in"}, {"title": "6 CONCLUSION", "content": "In this work, we describe our experience including two kinds of natural language prompting tasks alongside traditional assessments in an introductory programming course. We observed a weak relationship between performance on these tasks and more traditional programming assessments, suggesting that these new tasks may assess a different set of competencies. We also collected self-reported data from students on the difficulty they experienced learning programming. Interestingly, self-reported difficulty was very strongly related to performance on tests, exams, and programming projects, as would be expected, but this was not the case for the natural language tasks. In other words, students with less prior experience or those who find traditional programming challenging appear to perform relatively well on these tasks, potentially reducing the advantage experienced students typically have. We also found that students appreciated these types of tasks and recognized the importance of learning about AI and its applications in programming."}]}