{"title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions", "authors": ["Tianwei Xiong", "Yuqing Wang", "Daquan Zhou", "Zhijie Lin", "Jiashi Feng", "Xihui Liu"], "abstract": "The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.", "sections": [{"title": "Introduction", "content": "Generating long-take videos with temporal consistency, rich contents and large motion dynamics is essential for various applications such as AI-assisted film production. Although video generation models [4-8] have achieved impressive results in generating short video clips of few seconds, it remains challenging to simulate temporal-consistent and dynamic contents over long durations. Some works [9\u201311] attempt to extend video generation models trained on short video clips to long video generation by iteratively generating next frames conditioned on previously generated frames. However, those methods suffer from temporal inconsistency and limited motion patterns. Inspired by Sora [12], there has been increasing interest in scaling up video generation models for longer videos [13, 14]. Being trained directly on long-duration videos, these models provide a promising path toward modeling long-range temporal consistency and large motion dynamics in long videos. However, an obstacle on this path is the lack of high-quality long videos with rich text annotations.\nPrevious datasets of large-scale video-text pairs [3, 2, 15] have made significant contributions to video generation, but most of them encounter limitations for training long video generators. Video datasets crawled from the Internet [2, 3, 15] usually contain static videos or scene cuts, which are harmful to the training of video generation models. Moreover, previous text-to-video generation datasets are annotated with only short video captions, failing to capture the rich and dynamic semantics in long videos. Despite several recent efforts [12, 14] in generating long captions for videos, they mostly focus on generating spatially-dense captions and neglect the rich temporal dynamics in videos.\nIt has been validated in previous works [16] that fine-tuning pre-trained generative models on high-quality datasets could significantly improve the quality of generated images and videos. Despite previous efforts in building large-scale video datasets, high-quality long video datasets with dense annotations are rarely available and expensive. Inspired by this, we desire a dataset specifically designed for long video training with the following properties: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse content, and (4) annotated with temporally-dense captions.\nTo this end, we create an automatic pipeline for video filtering and long video recaptioning. We devise a video filtering process leveraging both low-level filtering tools including scene cut detection and optical flow [17] estimation, and semantic-level filtering tools like video LLMs [18]. The video filtering process selects high-quality long-take videos spanning over 10 seconds without scene cuts and containing large motion dynamics. Moreover, we design a hierarchical captioning approach to generate temporally-dense captions for long videos. Specifically, we split long videos into 30-second clips. For each clip, we uniformly sample 6 frames and arrange them in a grid layout. The single"}, {"title": "Related Work", "content": "Video-Language Datasets. To effectively train video generative models, a high-quality video-language dataset is crucial. Early datasets, such as MSR-VTT [23] and ActivityNet [24], were created through manual annotation, which limited their scale. Subsequent works aimed to increase dataset scale by utilizing automatic speech recognition (ASR) to extract text descriptions from videos. Notable examples include HowTo100M [25], YT-Temporal [26], and HD-VILA [15]. Although this approach significantly increased the amount of data, the ASR-generated text descriptions often fail to accurately represent the main video content. Another approach is to directly use readily available titles or descriptions of online videos as captions. WebVid [3] followed this approach and collected 10 million video-text pairs, primarily from stock footage providers. A common limitation of existing datasets is that the vast majority of samples are short video clips, lacking coverage of long videos, especially dense descriptions of long-range dynamic content changes. For dataset targeting longer vidoes, StoryBench [27] has provided a few thousand annotated long videos, but its limited data scale restricts its usage to evaluation rather than model training. A concurrent work ShareGPT4Video [28] curated a dataset with long videos and detailed captions, but its data pipeline is less focused on video data filtering and processing. To truly drive advances in long video generation models, constructing a large-scale dataset of high-quality long-take videos with dense captions is crucial.\nVideo Generation. Most existing video generation methods primarily focus on generating short video clips, with diffusion models [5-7] being the prevalent approach. There are also a few works based on language models (LM-based) [8] for video generation. Some works attempt to extend to long video generation by training models on short video data and then employing techniques such as sliding window generation [11, 10, 29, 9]. However, these methods often suffer from quality degradation, lack of temporal consistency, and difficulty in generating high-quality long-range dynamic video content. We identify that a lack of high-quality long video datasets hinders existing text-to-video generative models from effectively modeling and generating long videos with rich dynamics.\nVideo Understanding. Vision-language [19, 18, 30, 31] models demonstrated strong performance in video understanding. Recently, IG-VLM [19] pointed out that an VLM [20] comprehensively pretrained on images can be highly capable of video understanding. This is achieved by concatenating multiple frames from a video into a single image in grid view, which will be the input for VLMs. In this work, we propose a way to filter undesired videos utilizing a Video-LLM [18] which can largely enhance the overall quality of the dataset."}, {"title": "Dataset", "content": "We devise a data curation pipeline to filter large-motion long-take videos from large-scale video datasets and to annotate them with temporally-dense captions. We demonstrate the data curation pipeline and data statistics of LVD-2M in this section."}, {"title": "Long-take Video Collection and Filtering", "content": "Collecting videos from source datasets. We collect videos from four sources: (1) HD-VG [2] which contains 130 million video clips collected from YouTube. (2) InternVid [22] which contains 38 million video clips from YouTube. (3) Panda70M [1] which contains 70 million videos from YouTube. (4) WebVid [3] which contains 10 million videos from stock footage providers. However, not all of those videos are suitable for long video generation. For example, only 15% of video clips from InternVid [22] are longer than 10s, while around 52.5% of these long videos contain shot changes (Tab. 2). While videos from stock footage providers [3] seldom contain scene cut, nearly half of these videos are not dynamic (Fig. 5). Those low-quality videos will hinder the training of long video generation models. Thus, we devise several filtering criteria to select high-quality, large-motion, and long-take videos from 220 million videos in the four datasets. The whole filtering process is shown in Fig. 2.\nSelecting long-take videos with scene cut detection. Most current video generation models are trained on short video clips, and videos crawled from the Internet contain many scene cuts, which may impede the long video generation models from learning long-range temporal consistency and continuous motion across frames. We aim to select videos of consistent scenes captured over 10 seconds. It is worth mentioning that smooth transition of scenes (e.g., the background of a street continuously changes as a person walks down the street) is allowed, and we only target filtering out scene cuts or slow shot changes with fade-in and fade-out effects caused by post-editing of videos. Previous attempts [1, 2, 22] leverage PySceneDetect [34] to detect sudden shot changes and semantic consistency [1] between early and late frames to detect large scene changes. However, there is still a portion of videos with fade-in / fade-out shot changes in the filtered datasets. We optimize the settings of PySceneDetect to better detect both sudden scene cuts and slow shot changes with fade-in / fade-out effects. Specifically, we find that the default setting AdaptiveDetector with a rolling average threshold leads to difficulties in detecting slow shot changes with fade-in and fade-out effects. To filter out both sudden and slow scene cuts, we use Content Detector with cutscene_threshold of 50 and min_scene_len of 0 frames on video frames sampled at a low fps of 0.5. By applying PySceneDetect on the whole video, videos with any significant changes within a 2-second interval are filtered out, including fade-in and fade-out effects which are commonly within 2 seconds.\nSelecting large-motion videos with optical flow. We use optical flow as a clue to filter out static videos with little motion dynamics. Specifically, we calculate the optical flow with RAFT [17] between each pair of neighboring frames sampled at 2 fps and discard any videos with an average optical flow magnitude below a threshold of 20. This step helps remove videos with minimal motion, such as static scenes or individuals speaking to the camera against a still background.\nRemoving low-quality videos with MLLMs. We further conduct semantic-level filtering with MLLMs to remove low-quality videos that cannot be detected by previous filtering strategies. We leverage the PLLaVA-7B [18], which extends LLaVA from images to videos, for semantic-level filtering. For each video, we uniformly sample 8 frames from each video and prompt PLLaVA to distinguish low-quality videos. Specifically, we filter out videos that lack diversity, lack content variations, or with low perceptual qualities. The optical-flow-based criteria in the previous step can"}, {"title": "Hierarchical Long Video Captioning for Temporally Dense Captions", "content": "We propose a hierarchical captioning approach to annotate temporally-dense captions for long videos. As shown in Fig. 3, we first split videos longer than 30 seconds into video clips of 30 seconds. Then we annotate the clip-level video captions for each video clip. Finally we use an LLM to refine the captions and merge captions from all clips into a temporally-dense caption for the whole video. In this subsection, we first demonstrate how to caption video clips shorter than 30 seconds, and then demonstrate how to use LLM to refine and merge captions.\nCaptioning a video clip as an image grid. A recent work [19] has demonstrated that Vision Language Models (VLMs) pretrained only on images have strong zero-shot performance in video understanding. We generate captions for video clips shorter than 30 seconds inspired by this approach. Specifically, we uniformly sample 6 frames from the video clip and arrange these frames into a single composite image with a grid layout. We then input the image grid to LLaVA-v1.6-34B [20] to generate the video clip captions. With this approach, we can obtain detailed captions describing the backgrounds, main characters, major actions, and camera perspectives in the video clips.\nRefining and merging captions with LLMs. We identify that solely applying VLMs may not be sufficient for generating high-quality captions. LLaVA-v1.6-34B is prone to generating extra interpretations or assumptions about videos, leading to redundancy in the generated captions. So we leverage an LLM, Claude3-Haiku [21], to further refine the generated captions. In particular, we prompt Claude3-Haiku to rewrite the given raw captions so that the new captions are concise, objective, and convey a clear storyline for the video. Furthermore, for videos longer than 30 seconds, we prompt Claude3-Haiku to compose the multiple captions into a single, coherent caption describing the content and dynamics of the whole video."}, {"title": "Dataset Statistics", "content": "We present the comparison between our LVD-2M and previous video datasets in Tab. 1. LVD-2M is a high-quality dataset with videos longer than 10 seconds. Compared to previous video datasets, videos in LVD-2M are with large motion and rich captions. We further present the statistics of the category distribution, duration, and word count of our dataset in Fig. 4. To understand the"}, {"title": "Experiments", "content": "In Sec. 4.1, we conduct human evaluation analysis to demonstrate that our filtered video dataset, LVD-2M, contains fewer scene cuts, larger motion dynamics, and higher-quality captions, compared with previous datasets. In Sec. 4.2, we further validate the effectiveness of our LVD-2M by fine-tuning pre-trained video generation models on LVD-2M. We conduct fine-tuning experiments on both diffusion-based video generation models and language model-based video generation models, and find that fine-tuning video generation models on our dataset boosts the video generation models' abilities in generating long-take videos with large motion dynamics. In Sec. 4.3, we present the"}, {"title": "Human Evaluation of Dataset Quality", "content": "To validate the quality of LVD-2M and the effectiveness of our data curation pipeline, we conduct human evaluations to examine the long-take consistency, dynamic degrees, and caption qualities. For human evaluations, we compare our LVD-2M with previous video datasets: Panda-70M [1], HD-VG-130M [2], InternVid [22], and WebVid-10M [3].\nLong-take consistency in videos. We examine that the filtered videos are mostly long-take videos without cuts. We randomly sample 40 videos from each dataset, each one being 10~30s long. We do not compare with WebVid [3] because its videos are from stock footage providers and barely have scene cuts. For fair comparison, we also exclude videos collected from WebVid in samples from LVD-2M. The sampled videos are mixed and randomly shown to human raters. We request human raters to check for any type of scene cut that can lead to inconsistency. As shown in Tab. 2, with our video filtering strategy, LVD-2M reaches the highest long-take video ratio. We examine the cases in our dataset deemed by human raters as non-long-take videos, and identify the major failure cases are slight jump cuts. While humans can easily recognize a slight jump cut in a video, it is challenging for scene cut detection algorithms and MLLM-based semantic-level filtering models to identify such slight changes in the videos.\nDynamic degree of videos. We randomly sample 40 videos for each dataset, each one being 10~30s long. We request human raters to rate the dynamic degree of the given videos from 1 to 3, where 1 means being not dynamic and 3 for being very dynamic. As shown in Fig. 5, for previous datasets, a large portion of videos are considered as not dynamic. After filtering at low-level with optical flow scores and at high-level with MLLMs, our LVD-2M successfully get rid of most static videos and the achieve a larger portion of very dynamic videos."}, {"title": "Fine-tuning Video Generation models with LVD-2M", "content": "To further validate the effectiveness of our LVD-2M in fine-tuning video generation models for generating long videos with large motion dynamics, we conduct fine-tuning experiments on a diffusion-based image-to-video (I2V) generation model and a language model-based text-to-video (T2V) generation model for long video generation. In this experiment, we don't extend the generation frame length of the pretrained models and compare the finetuned models with the pretrained ones. We further compare LVD-2M to WebVid-10M on extending the generation frame length for a diffusion-based T2V models in Sec. 4.3, and for a diffusion-based I2V model in the Appendix.\nFine-tuning an LM-based T2V model. We finetune a 7B LM-based video generation model from Loong [37]. The model utilizes a discrete video tokenizer similar to MAGVIT-v2 [38] to convert videos into tokens, and then models the video tokens with decoder-only autoregressive transformer. The model is pretrained on 15 million video-text pairs for 500K iterations with a batch size of 256. We further fine-tune it for 10k iteration with a batch size of 256 on 65-frame clips from LVD-2M.\nFine-tuning a diffusion-based I2V model. We finetune an I2V model, which was pretrained to generate 17-frame videos on 19 million video-text pairs for 18k iterations with a batch size of 288, following the similar image conditioning settings as proposed in EMU [16]. The model follows a similar architecture as MagicVideo [39] with 1.8B parameters.\nUser study. To validate the performance improvement after fine-tuning on LVD-2M, we conduct a user study comparing the pretrained models and finetuned models. For each base model (diffusion-based I2V and LM-based T2V), we use 50 text prompts to generate videos with the pretrained and fine-tuned models, respectively. Human raters are presented with 2 videos generated by the"}, {"title": "Extending a Diffusion-based T2V Model for Longer Range on LVD-2M", "content": "In this section, we present the effectiveness of LVD-2M for finetuning text-to-video (T2V) diffusion models to generate longer and more dynamic videos. For comparison, we choose the widely adopted WebVid-10M [3] as the baseline. In the experiment, we extend a T2V diffusion model from pretrained 32-frame generation length to 65-frame length, using LVD-2M and WebVid-10M seperately. Quantitative results on VBench [40] and qualitative comparisons can both validate the superiority of LVD-2M.\nSetup. We finetune a base T2V diffusion model with 1.75B parameters, which has a similar structure as MagicVideo [39]. The base model was pretrained to generate 32-frame videos and finetuned at 65-frame length in this experiment. The finetuning settings for LVD-2M and WebVid-10M are the same, which is 64 batch size, 4 gradient accumulation iterations and for 30k iterations, roughly going over 2M video clips once at the finetuning stage. For quantitative evaluation, we follow the standard evaluation protocol of VBench [40].\nResults and analysis. As shown in Tab. 3, compared to WebVid-10M, finetuning on LVD-2M will lead to better performance in 10 out of 16 metrics of VBench, especially surpassing WebVid-10M by a large margin in dynamic degree, object class and human action. These obvious performance improvements against the baseline can be attributed to the diverse and the highly dynamic video data of LVD-2M. Notably, the evaluation prompts from VBench have a small average length (7.6 words), which is much closer to the average caption length of WebVid-10M (14.1 words) than LVD-2M (88.7 words). Despite the caption length gap between training and evaluation, the model finetuned on LVD-2M still presents superior overall performance. We further demonstrate the qualitative comparisons in Fig. 7. Due to limited computational resources, we didn't validate LVD-2M on stronger T2V models, and the text encoding of the chosen T2V model is still based on CLIP [41], which struggles to properly encode long captions. We expect even more obvious performance enhancement when finetuning on LVD-2M using more advanced T2V models with more powerful text encoders [42]."}, {"title": "Conclusion", "content": "High-quality long video datasets are essential for training long video generation models. In this work, we devise an automatic data curation pipeline to filter high-quality long-take videos from existing large-scale video datasets and to annotate temporally-dense captions for the filtered videos. Based on this pipeline, we construct LVD-2M, the first long-take video dataset of 2 million videos with large motion, diverse content, and temporally dense captions. We validate the quality of the dataset through human evaluation and verify its effectiveness by fine-tuning video generation models to generate long videos with large motions."}, {"title": "Limitations and Social Impacts", "content": "A limitation of our work is that the size of 2 million video-text pairs is not as large as other video datasets. However, those 2 million videos are high-quality videos filtered from 220 million videos tailored for long video generation. We will keep maintaining the dataset and expand the scale of the dataset in future versions. Our proposed dataset can be used to fine-tune video generators for long video generation. The resulting video generation models can be deployed to assist various applications such as film production. However, the community should be aware of the potential negative social impact that video generators may be used for generating fake videos and delivering misleading information. It is necessary to develop techniques to detect and watermark the videos generated by machine learning models."}, {"title": "Extending a Diffusion-based I2V Model for Longer Range on LVD-2M", "content": "In this section, we present additional qualitative results to demonstrate the effectiveness of fine-tuning a diffusion-based image-to-video (I2V) model.\nSetup. To compare the effect of LVD-2M to previous datasets on long video generation fine-tuning, we fine-tune the same pretrained diffusion-based I2V model separately on WebVid-10M [3] and LVD-2M. Both datasets are used to fine-tune the model for generating 65-frame videos, with the fine-tuning process running for 20k iterations using identical strategies."}, {"title": "Qualitative Evaluation for Long Range Video Fine-tuning of LM-based Model on LVD-2M", "content": "In this section, we present experiments about generating long videos after fine-tuning the LM-based T2V model on LVD-2M. We choose LM-based model because it can naturally extend the video generation to longer range by directly conditioning on previous generated frames. We also fine-tune the same pretrained LM-based T2V model on WebVid-10M [3] as the baseline."}, {"title": "Discussions on Using MLLM for Data Filtering", "content": "In our data pipeline, we utilize PLLaVA [18] for filtering out low-quality video clips, including those with limited content variation or only single-image level semantics. While some previous works [43, 44] meticulously designed methods to distinguish videos or video-question pairs with only single-image level semantics, with recent development of advanced MLLMs [45, 46, 20], we believe evaluation of videos concerning temporal complexity or from other aspects will ultimately be flexibly resolved with proper prompts and powerful MLLMs. However, we also find that current MLLMS are not guaranteed to be capable of video quality evaluation, some of them struggling to follow related instructions. In the future, a comprehensive benchmark for measuring MLLMs capability for video quality evaluation should be helpful to accelerate related research."}, {"title": "Author Statements", "content": "The dataset is open and the data is collected from publicly available resources. For using this dataset, please check for the related license\u00b9. For the released data records and dataset documentation, please check our homepage at https://github.com/SilentView/LVD-2M."}]}