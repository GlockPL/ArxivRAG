{"title": "MCL: Multi-view Enhanced Contrastive Learning for Chest X-ray Report Generation", "authors": ["Kang Liu", "Zhuoqi Ma", "Kun Xie", "Zhicheng Jiao", "Qiguang Miao"], "abstract": "Radiology reports are crucial for planning treatment strategies and enhancing doctor-patient communication, yet manually writing these reports is burdensome for radiologists. While automatic report generation offers a solution, existing methods often rely on single-view radiographs, limiting diagnostic accuracy. To address this problem, we propose MCL, a Multi-view enhanced Contrastive Learning method for chest X-ray report generation. Specifically, we first introduce multi-view enhanced contrastive learning for visual representation by maximizing agreements between multi-view radiographs and their corresponding report. Subsequently, to fully exploit patient-specific indications (e.g., patient's symptoms) for report generation, we add a transitional \"bridge\" for missing indications to reduce embedding space discrepancies caused by their presence or absence. Additionally, we construct Multi-view CXR and Two-view CXR datasets from public sources to support research on multi-view report generation. Our proposed MCL surpasses recent state-of-the-art methods across multiple datasets, achieving a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR.", "sections": [{"title": "Introduction", "content": "Radiology reports, crafted by experienced radiologists, meticulously document imaging findings from examinations such as X-rays, PET scans, and CTs, detailing abnormalities and initial diagnostic conclusions. These reports deliver vital imaging insights that enable physicians to develop efficient patient treatment strategies (Messina et al. 2022). However, the manual writing process is time-consuming and requires significant expertise, making it increasingly difficult to meet the demands of modern healthcare (Bannur et al. 2024), particularly in regions with limited medical resources.\nAutomatic chest X-ray report generation aims to produce detailed and accurate free-text reports from multi-view radiographs, helping radiologists improve diagnostic efficiency and consistency by providing high-quality draft reports. In clinical practice, limitations like X-ray equipment constraints and the complexity of human anatomical structures can prevent a single-view radiograph from achieving optimal imaging quality and adequately displaying the overall anatomical structure. As a result, multi-view imaging examinations, such as postero-anterior (PA), antero-posterior (AP), and lateral views, are crucial for accurate diagnostics and personalized treatment. Consequently, the number of radiographs varies across studies. Typically, each study comprises a collection of radiographs, a corresponding report, and a patient-specific indication (which may sometimes be absent). This variability is also evident in public chest X-ray report generation datasets such as MIMIC-CXR (Johnson et al. 2019) and IU X-ray (Demner-Fushman et al. 2016). Such variability makes it challenging to effectively utilize multi-view radiographs from the same study to enhance the clinical accuracy of generated reports. One intuitive approach (Li et al. 2019; Chen et al. 2020; Liu et al. 2024c,b) treats each radiograph as an individual study, generating reports from single-view radiographs (see Figure 1). However, this scheme fails to fully exploit the rich anatomical information available in multi-view radiographs, potentially leading to inaccurate and inconsistent reports. In addition, since the IU X-ray dataset predominantly includes studies with two-view radiographs, several studies (Chen et al. 2020, 2021; Yang et al. 2023) have developed two-view report generation approaches, showing promise in producing informative reports. Despite this, these methods often struggle to integrate into clinical workflows due to the varying number of radiographs per study.\nTo address this challenge, we introduce a novel two-stage method, called Multi-view enhanced Contrastive Learning (MCL) for generating chest X-ray reports. In the representation learning stage, we propose multi-view enhanced contrastive learning for visual representation by leveraging semantic correspondences between multi-view radiographs within the same study and between these radiographs and their corresponding report. In the report generation stage, we utilize the cross-attention mechanism to exploit available indications fully, providing the model with patient background information. Additionally, we incorporate a transitional \"bridge\" for missing indications to reduce embedding space differences caused by the presence or absence of these indications. For multi-view enhanced contrastive learning, we first employ multi-positive contrastive learning to bring multi-view radiographs within the same study closer, improving the consistency of visual features. Then, we develop a multi-view fusion module to integrate varying numbers of radiographs per study, producing fused visual features for subsequent cross-modal alignment. Finally, we apply contrastive learning for instance-wise and token-wise alignment, maximizing agreement between multi-view radiographs and their corresponding report. We evaluate our proposed MCL on the MIMIC-CXR (Johnson et al. 2019), MIMIC-ABN (Ni et al. 2020; Hou et al. 2023a), and our curated Multi-view CXR and Two-view CXR datasets. Experiment results demonstrate the effectiveness of MCL, showing a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR. Our key contributions are outlined as follows:\n\u2022 We propose a novel multi-view enhanced contrastive learning that facilitates cross-modal alignment between multi-view radiographs and their corresponding report, addressing the issue that existing algorithms cannot handle varying numbers of views.\n\u2022 We incorporate a transitional \"bridge\u201d for missing indications to reduce embedding space differences caused by their presence or absence, thereby enhancing the capture of patient background information.\n\u2022 We curate Multi-view CXR and Two-view CXR datasets from two public sources, ensuring that each study includes multiple radiographs. This supports research on multi-view report generation, particularly for scenarios involving varying numbers of views or two-view setups."}, {"title": "Related Work", "content": "Image Captioning\nImage captioning (Li et al. 2022; Chen, ZHANG, and Hinton 2023) and report generation (Hou et al. 2023b; Bu et al. 2024) both convert visual information into textual descriptions. Advances in image captioning, particularly with the encoder-decoder framework, have influenced report generation. However, report generation differs by requiring the capture of subtle differences between radiographs and producing detailed and lengthy reports.\nMedical Image Analysis Meets Multi-view Learning\nMulti-view learning (Xu, Tao, and Xu 2013; Zhao et al. 2017) aims to enable models to capture shared and complementary information from different perspectives of the same scene, leading to a more comprehensive understanding. In representation learning, Paul et al. (Paul et al. 2021) develop a multi-view semantic embedding extracted from X-ray reports, CT reports, and visual traits to enhance the clinical accuracy of chest X-ray diagnosis. REFERS (Zhou et al. 2022) introduces a view fusion module that integrates fixed-perspective visual features, enhancing cross-modal alignment. In medical report generation, FMVP (Liu et al. 2024f) incorporates single radiographs and auxiliary inputs (i.e., disease tags and medical concepts) as multi-view information, using two shared and continuous cross-attention mechanisms (Vaswani et al. 2017) to assist in generating informative reports. Given that the IU X-ray dataset (Demner-Fushman et al. 2016) primarily comprises studies with two-view radiographs, several approaches (Chen et al. 2020, 2021; Yang et al. 2023) have been developed to handle two-view report generation. These methods enhance the clinical efficacy of generated reports and offer valuable insights for exploring multi-view report generation. However, these approaches often struggle with the varying number of views encountered in real-world scenarios. To tackle this issue, we introduce a multi-view enhanced contrastive learning method that accommodates an unfixed number of views.\nMedical Image Analysis Meets Contrastive Learning\nContrastive learning is pivotal in medical image analysis, particularly in tasks related to medial visual representation learning (Wang et al. 2022a) and report generation (Shen et al. 2024). It aligns radiographs with their corresponding reports by minimizing the distance between positive pairs (radiographs and associated reports) while maximizing the distance between negative pairs (radiographs and unrelated reports). Significant advancements have been made in this area. For instance, MedCLIP (Wang et al. 2022b) adopts a semantic matching loss for global alignment between decoupled radiographs and reports. ARL (Chen, Li, and Wan 2022) regards knowledge as an intermediary to facilitate semantic alignment between radiographs and their reports. MGCA (Wang et al. 2022a) introduces multi-grained cross-modal alignment at the instance, pathological region, and disease levels for generalized medical visual representation. SEI (Liu et al. 2024c) utilizes global and local cross-modal alignment between radiographs and factual serialization in reports to generate radiology reports. While these methods effectively align radiographs with their reports by extracting visual features from single-view radiographs, they often overlook the benefits of incorporating multi-view information from the same study. In response, our work introduces"}, {"title": "Method", "content": "The overall architecture of our proposed MCL, shown in Figure 2, consists of two stages. In the representation learning stage, we propose multi-view enhanced contrastive learning to align multi-view radiographs with their corresponding report. In the report generation stage, we fully exploit patient-specific indications to provide the text generator with patient background information.\nProblem Formulation\nLet $D_{tr} = \\{(X_{i,k}, X_{i,\\k}, z_i, r_i) | 1 \\leq i \\leq n, 1 \\leq k \\leq m_i\\}$ be the training set, where n is the number of studies. Each study i includes $m_i$ radiographs (views), a patient-specific indication $z_i$ (which may be absent), and a corresponding report $r_i$. Here, $X_{i,k}$ refers to the anchor scan, while $X_{i,\\k} = \\{x_{i,j} |j \\neq k, 1 \\leq j \\leq m_i \\}$, which may be empty, represents the set of auxiliary references excluding the $k$th view. The indication $z_i$, derived from the radiology report, provides the patient's reasons for the examination. In this paper, we aim to learn the mapping $F_{\\theta} (\\cdot)$ that maps the input $(X_{i,k}, X_{i,\\k}, z_i)$ to output $r_i$ on the training set $D_{tr}$, namely $F_{\\theta}: (X_{i,k}, X_{i,\\k}, z_i) \\rightarrow r_i$\nRepresentation Learning with Multi-view Enhanced Contrastive Learning\nWe introduce multi-view enhanced contrastive learning for visual representation by utilizing semantic correspondences both among multi-view radiographs within the same study and between these radiographs and their corresponding report. First, we employ multi-positive contrastive learning to bring multi-view radiographs closer, thereby enhancing visual feature consistency across them. Afterward, we develop a multi-view fusion module to integrate varying numbers of radiographs per study, generating fused visual features for subsequent cross-modal alignment. Finally, we apply contrastive learning for instance-wise and token-wise cross-modal alignment, maximizing agreement between the radiographs and their report."}, {"title": "Visual Features Extraction", "content": "We use ResNet101 (He et al. 2016), pre-trained on ImageNet, as the visual encoder. The feature maps from the last convolutional layer of ResNet101 are regarded as the visual features of radiographs, formulated as $V \\in \\mathbb{R}^{B\\times p \\times d_1}$. Here, $M = \\Sigma_{i=1}^{n} m_i$ denotes the number of radiographs in the batch. B, p, and $d_1$ represent the batch size, the dimensions of the feature map, and the number of channels, respectively."}, {"title": "Textual Features Extraction", "content": "Drawing inspiration from (Yan et al. 2023; Liu et al. 2024d), we first extract factual serialization from reports using the structural entities approach (Liu et al. 2024d) to reduce the noise in the following cross-modal alignment. Factual serialization in a report, shown in Figure 2, is a brief sentence composed of clinically significant keyword groups extracted from reports. Following this, factual serialization is input into the text encoder, a six-layer pre-trained SciBERT (Beltagy, Lo, and Cohan 2019), to generate textual features $T \\in \\mathbb{R}^{B\\times k \\times d_2}$. Here, k and $d_2$ denote the number of textual tokens and dimensionality per token, respectively. Note that textual features T are exclusively utilized in the representation learning stage."}, {"title": "Multi-positive Contrastive Learning between Multi-view Radiographs", "content": "To capture semantic correspondences between multi-view radiographs within the same study, we employ multi-positive contrastive learning (Tian et al. 2023). This method aligns radiographs from the same study while distinguishing them from those in different studies, thereby enhancing the consistency of visual features. We begin by excluding studies with only a single-view radiograph from the batch (Notably, these studies are still used for subsequent cross-modal alignment). For each anchor scan $x_{i,a}$, we compute the contrastive categorical distribution q to measure the similarity between $x_{i,a}$ and all other scans $x_{\\backslash a}$. Here, $X_{\\backslash a}$ comprises both auxiliary references of $X_{i,a}$ and radiographs from different studies. We denote the global visual features, after $l2$ normalization, as $v_{ia}$ for anchor scans and $v_a$ for the other scans. The $q \\in [\\mathbb{R}^{K\\times (K-1)}$ is formulated as:\n$q_i = \\frac{exp (v_{ia} \\cdot (v_a)^T / \\tau_1)}{\\Sigma_{k=1}^{K-1} exp (v_{ia} \\cdot (v_a)^T / \\tau_1)}$   (1)\nwhere $\\tau_1 \\in \\mathbb{R}^+$ is the temperature parameter. The number of radiographs in a batch is $K = \\Sigma_{j=1}^{B} m_j$, where $m_j > 1$ for all j. Next, we construct the ground-truth categorical distribution $p \\in \\mathbb{R}^{K\\times (K-1)}$, denoted as:\n$p_i = \\frac{I_{match} (X_{i,a}, X_a)}{\\Sigma_{j=1}^{K-1} I_{match} (x_{ia}, x_a)}$   (2)\nhere, $I_{match} (\\cdot,\\cdot)$ is an indicator function that determines whether two radiographs belong to the same study. Finally, the multi-positive contrastive loss is calculated using the cross-entropy between q and p, formulated as:\n$\\mathcal{L}_{MPC} = \\frac{1}{K} \\Sigma_{i=1}^{K} p_i log q_i$   (3)\nDespite the varying number of radiographs per study, the different number of non-zero elements in $p_i$ account for this variability. Applying cross-entropy between $q_i$ and $p_i$ pushes the multi-view radiographs in the $i^{th}$ study closer together in the embedding space."}, {"title": "Multi-view Fusion Module", "content": "Effective integration of multi-view visual features is crucial for aligning multi-view radiographs with their corresponding report. However, the varying number of radiographs per study complicates simple concatenation, leading to inconsistent channel dimensions in the fused visual features and hindering cross-modal alignment. Although averaging and maximization methods ensure channel consistency, they result in a loss of detail. To tackle this challenge, we propose a multi-view fusion module that integrates multi-view visual features using the scaled dot-product cross-attention mechanism (Vaswani et al. 2017), denoted as ATTN (Q, K, V). In this module, the anchor scan $x_{i,a}$ functions as queries, while auxiliary references $x_{i,\\backslash a}$ serve as both keys and values. A skip connection (He et al. 2016) and layer normalization (LN) are then applied to produce the fused visual features for subsequent cross-modal alignment:\n$V_{i,a} = LN (V_{i,a} + ATTN (V_{i,a}, V_{i,\\backslash a}, V_{i,\\backslash a}))$,  (4)\nwhere $V = \\{V_{ia} |1 \\leq i \\leq B\\} \\in \\mathbb{R}^{B\\times p \\times d_1}$ ensures consistent channel dimensions across studies while preserving information from both anchor scans and auxiliary references. Notably, we instruct the module to focus on one study at a time, adeptly handling the variability in the number of radiographs per study."}, {"title": "Instance-wise Alignment Loss", "content": "Inspired by (Cheng et al. 2023; Wang et al. 2022a; Liu et al. 2024d), we propose the instance-wise alignment loss to maximize the agreement between multi-view radiographs and their corresponding report in the embedding space. We begin by projecting the fused visual features $V$ and textual features $T$ into a unified embedding space $d$ using the two-layer convolutional projection head, respectively. We denote the results as $\\bar{V} \\in \\mathbb{R}^{Bxpxd}$ and $\\bar{T} \\in [\\mathbb{R}^{B\\times k \\times d}$, respectively. Due to the similarity in radiological features, radiology reports from different studies may be identical. Therefore, besides treating the visual-textual feature pair from the same study as a positive pair, we also consider reports with identical contents from different studies as additional positive pairs, along with the current visual features. Next, the image-to-text contrastive categorical distribution $q^{V\\rightarrow T} \\in \\mathbb{R}^{BXB}$ is computed to estimate the similarity between multi-view radiographs and their corresponding report:\n$q^{V\\rightarrow T} = \\frac{exp (\\bar{v} \\cdot (\\bar{t})^T / \\tau_2)}{\\Sigma_{j=1}^{B} exp (\\bar{v} \\cdot (\\bar{t})^T / \\tau_2)}$,   (5)\nhere, $\\bar{v}$ and $\\bar{t}$ are global visual and textual features processed by l2 normalization. Likewise, we obtain the symmetric text-to-image contrastive categorical distribution $q^{T\\rightarrow V}$. The global ground-truth categorical distribution $p^0$ is:\n$p^0_j = \\frac{I_{equal} (r_i, r_j)}{\\Sigma_{k=1}^{B} I_{equal} (r_i, r_k)}$,   (6)\nwhere $p^0 \\in \\mathbb{R}^{B\\times B}$. $I_{equal} (\\cdot,\\cdot)$ denotes an indicator function that determines whether two reports are identical. The instance-wise alignment loss is formulated as:\n$\\mathcal{L}_{global} = \\frac{1}{B} \\Sigma_{i=1}^{B} (p_i log q^{V\\rightarrow T} + p_i log q^{T\\rightarrow V})$.  (7)\nNotably, our method differs from previous works (Cheng et al. 2023; Wang et al. 2022a) in two key ways: we utilize factual serialization in reports for alignment instead of complete reports, and we employ multi-positive contrastive loss (Tian et al. 2023) to learn semantic correspondences between radiographs and reports, rather than single-positive contrastive loss (van den Oord, Li, and Vinyals 2019)."}, {"title": "Token-wise Alignment Loss", "content": "Inspired by (Wang et al. 2022a; Liu et al. 2024d), we employ the token-wise alignment loss $\\mathcal{L}_{local}$ (Liu et al. 2024d) to learn fine-grained visual features. This loss is achieved through single-positive contrastive learning between token embeddings of visual and textual data. In conclusion, the training objective during the representation learning stage is formulated as:\n$\\mathcal{L}_{pretrain} = \\mathcal{L}_{MPC} + \\mathcal{L}_{global} + \\mathcal{L}_{local}$. (8)"}, {"title": "Report Generation Based on Patient-specific Indications", "content": "Indication Features Extraction. Patient-specific indications, as shown in Figure 2, are collected before examinations and provide critical background information, such as the patient's condition. However, de-identification in public datasets often introduces noise into indications, such as @, and -year-old. We preprocess the data as described in (Liu et al. 2024c), removing the noise and standardizing gender expressions. The cleaned indications are then fed into a text encoder, initialized with the pre-trained model from stage 1, to extract indication features.\nReport Generation. Indications may sometimes be absent in certain studies, as detailed in Table 1 in the Appendix. As a result, some existing methods either directly ignore indications (Chen et al. 2020; Wang et al. 2023; Shen et al. 2024) or fail to exploit them fully (Tian et al. 2019; Nguyen et al. 2023). To address this issue, we utilize the Transformer Decoder (Chen, Li, and Wan 2022) to exploit available indications fully, providing the text generator with patient background information. For cases where indications are missing, we incorporate a transitional \"bridge\u201d (i.e., Transformer Encoder (Chen, Li, and Wan 2022)) to mitigate embedding space differences caused by the presence or absence of indications. The text generator then produces reports in an autoregressive manner, optimized by minimizing the negative log-likelihood as follows:\n$\\mathcal{L}_{LLM} = - \\Sigma_{t=1}^{L} log P (\\tilde{w}_t | X_{i,a}, X_{i, \\backslash a}, z_i, w_{<t})$, (9)\nwhere $\\tilde{w_t}$ denotes the tth predicted word. $w_{<t}$ is the preceding word sequence generated by the text generator. $P(\\cdot)$, $z_i$, and L represent the likelihood function, the indication, and the maximum number of generated tokens, respectively."}, {"title": "Experiments", "content": "Experimental Settings\nDatasets. 1) MIMIC-CXR (Johnson et al. 2019) is a large-scale chest X-ray dataset with free-text radiology reports and varying numbers of radiographs per study. 2) MIMIC-ABN (Ni et al. 2020), a subset of MIMIC-CXR, focuses on abnormal sentences within radiology reports. 3) Our curated Multi-view CXR aggregates studies with multiple views from both MIMIC-CXR and IU X-ray (Demner-Fushman et al. 2016). 4) Our curated Two-view CXR is a variant of Multi-view CXR that includes only two views per study, with other settings unchanged. We treat the \u201cFINDINGS\u201d section of the radiology report as the reference report, in line with established practices (Chen et al. 2020; Wang et al. 2023; Shen et al. 2024). MIMIC-ABN contains only abnormal sentences in this section. MIMIC-CXR and MIMIC-ABN datasets adhere to their official splits. For Multi-view CXR, we follow the MIMIC-CXR split for its MIMIC-CXR component and baseline split (Chen et al. 2020; Yang et al. 2023; Shen et al. 2024) for its IU X-ray. Statistics for these datasets are summarized in Table 1 in the Appendix.\nEvaluation Metrics. We evaluate model performance using natural language generation (NLG) and clinical efficacy (CE) metrics. NLG metrics assess the lexical similarities between generated and reference reports, including BLEU-n (B-n, where n \u2208 {1,2,3,4}), METEOR (MTR), and ROUGE-L (R-L). CE metrics evaluate the clinical correctness of generated reports, including F\u2081 RadGraph (RG) (Jain et al. 2021), F1,mic-5 CheXbert (CX5), and F1,mic-14 CheXbert (CX14) (Smit et al. 2020). RG measures the overlap in clinical entities and their relationships, which better aligns with radiologists' evaluations than B-3 and CX14 metrics (Yu et al. 2023). CX5 and CX14 evaluate the ability to describe 5 and 14 observations (e.g., Edema) using the multi-label classification micro-averaged F\u2081 score.\nImplementation Details. The batch size, $\\tau_1$, and $\\tau_2$ are set to 32, 0.5, and 0.5, respectively. The maximum number of generated tokens, L, is fixed at 100 for all datasets. The optimal model is selected based on the combined scores of B-4, RG, and CX14 on the validation set. Experimental results from the chosen optimal model are reported on the test set (please refer to the Appendix for more details).\nResults and Analyses\nComparison with State-of-the-art Methods. We compare our proposed MCL with 11 state-of-the-art (SOTA) methods: R2Gen (Chen et al. 2020), CMN (Chen et al. 2021), CGPT2 (Nicolson, Dowling, and Koopman 2023), MET (Wang et al. 2023), KiUT (Huang, Zhang, and Zhang 2023), SA (Yan et al. 2023), FMVP (Liu et al. 2024f), MAN (Shen et al. 2024), PMRG (Jin et al. 2024), Med-LLM (Liu et al. 2024e), and HERGen (Wang, Du, and Yu 2024). Results are presented in Table 1, where \u201cInput Size\" denotes the image resolution processed by the visual encoder (e.g., 224 refers to 224 x 224 pixels). MCL outperforms all baselines across every metric on four datasets, as evidenced by a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR. These results highlight the effectiveness of MCL in generating clinically accurate reports. Additionally, at a resolution of 384 \u00d7 384, MCL consistently surpasses its performance at 224 \u00d7 224 resolution on most metrics across these datasets, demonstrating that higher image resolution can significantly enhance the model's performance.\nEffect of Multi-view Enhanced Contrastive Learning. We evaluate the impact of our multi-view enhanced contrastive learning in stage 1 on model performance, as shown in Table 2. Results show positive effects from multi-positive contrastive loss (MCL vs. (f)), instance-wise alignment loss (MCL vs. (e)), and token-wise alignment loss (MCL vs. (d)). Please see the Appendix for more details.\nEffect of Report Generation Based on Indications. We assess the impact of indications in stage 2 on model performance, as detailed in Table 2. Both single-view ((b) vs. (a)) and multi-view methods (MCL vs. (g)) exhibit significant improvements across all metrics, emphasizing the importance of indications in generating accurate reports. Additionally, MCL shows a 3.1% increase in the sum of all\"\n    }"}]}