{"title": "The Overfocusing Bias of Convolutional Neural Networks: A Saliency-Guided Regularization Approach", "authors": ["David Bertoin", "Eduardo Hugo Sanchez", "Mehdi Zouitine", "Emmanuel Rachelson"], "abstract": "Despite transformers being considered as the new standard in computer vision, convolutional neural networks (CNNs) still outperform them in low-data regimes. Nonetheless, CNNs often make decisions based on narrow, specific regions of input images, especially when training data is limited. This behavior can severely compromise the model's generalization capabilities, making it disproportionately dependent on certain features that might not represent the broader context of images. While the conditions leading to this phenomenon remain elusive, the primary intent of this article is to shed light on this observed behavior of neural networks. Our research endeavors to prioritize comprehensive insight and to outline an initial response to this phenomenon. In line with this, we introduce Saliency Guided Dropout (SGDrop), a pioneering regularization approach tailored to address this specific issue. SGDrop utilizes attribution methods on the feature map to identify and then reduce the influence of the most salient features during training. This process encourages the network to diversify its attention and not focus solely on specific standout areas. Our experiments across several visual classification benchmarks validate SGDrop's role in enhancing generalization. Significantly, models incorporating SGDrop display more expansive attributions and neural activity, offering a more comprehensive view of input images in contrast to their traditionally trained counterparts.", "sections": [{"title": "1 Introduction", "content": "Vision transformers [10] have emerged as the leading approach in various computer vision tasks, from image classification and object detection to instance segmentation. However, when faced with limited datasets, convolutional neural networks [27, 44, 19, 29] often demonstrate greater generalization potential, delivering more robust performance [11, 34, 39, 18, 28, 31]. The growing performance"}, {"title": "2 Related work", "content": "Attribution methods have emerged as pivotal tools in the interpretability landscape, providing insights into the regions of input data that influence neural network decisions. Broadly categorized, these methods fall into two main categories: white-box and black-box. White-box methods explain"}, {"title": "3 Saliency-Guided Dropout", "content": "We consider the general supervised classification problem, given by a distribution $p(x, c)$, where $x \\in X$ are inputs belonging to a finite-dimensional feature space, and $c \\in C$ are discrete labels taken from the finite set of classes $C$. Note that we set ourselves in a fairly general framework, where an element $x \\in X$ can be an image, but also a map of pre-computed features on a image, or even a plain vector of scalar values. We will assume however that the feature space $X$ is $d$-dimensional, and hence isomorphic to $R^d$. With a slight abuse of notation, we shall write $Y = R^{|C|}$ the set of discrete probability distributions on $C$ and will note $y_c$ the probability vector in $R^{|Y|}$ which sets all probability mass on the $c$th item. A solution to the classification problem takes the form of a function $f$ within a hypothesis space $H$, mapping $x \\in X$ to a distribution $f(x) \\in Y$ on classes. For notation convenience, we write $f(x)_c$ the $c$th component of $f(x)$, i.e. the probability estimate of class $c$. Given a loss function $l(\\hat{y}, y)$ quantifying the cost of predicting distribution $\\hat{y} \\in Y$ when the ground truth was $y \\in V$, an approximate solution to the classification problem can be found by minimizing the empirical risk $\\frac{1}{n}\\sum_{i=1}^{n} l(f(x_i), y_{c_i})$, relying on a finite sample of input-label pairs $(x_i, C_i)_{i=1}^{n}$ drawn independently from $p$ [50]. Classically, $l$ is the cross-entropy between distributions $y_c$ and $f(x)$, and networks $f \\in H$ feature a softmax output layer to ensure their output is a discrete probability distribution. Throughout the remainder of this paper, we define $H$ as the space of neural network functions with parameters $\\theta$. We define the attribution (or saliency) map for class $c$, of a network $f \\in H$ evaluated in $x \\in X$, as an item $A_f(x, c) \\in R^d$ that quantifies the contribution of each feature in $X$ to the prediction probability for class $c \\in C$. The method we propose is entirely agnostic of the way $A_f(x, c)$ is computed, we simply assume some specific, adequate method provides such a measure of feature influence."}, {"title": "3.1 Generic algorithm", "content": "Our intention is to let a neural network learn a function which minimizes the empirical risk, while preventing the over-fitting due to excessive reliance on specific features. Dropout [46] aims to achieve the same goal by randomly masking neurons during the gradient descent process, so as to avoid over-specialization on minute features, and dispatch the network feature reliance on all relevant features. But the dropout process is myopic in the sense that, to avoid over specialization on a few neurons, it drops out a large population of neurons (in the order of 20 to 70 percent of a layer's neurons). By doing so, it prevents over-specialization on some features, but at the same time also drops out under-represented important features with high probability. The rationale (and expected strength) of the method we propose is that it is possible to identify a very small population of key neurons to drop out, while preserving many degrees of freedom in gradient descent. We posit attribution maps constitute a good proxy to pinpoint these neurons and hence introduce a generic method of Saliency-Guided Dropout (SGDrop). Given a neural network $f$ with parameters $\\theta$, SGDrop mitigates the network's over-reliance on a subset of features by redistributing its focus across neurons. For $\\rho \\in [0, 1]$, let $q_p$ be the upper $p$-quantile of attribution map $A_f(x, c)$, i.e. the attribution value below which one finds a proportion $1 - p$ of values in $A_f(x, c)$. The $q_p$-saliency dropout mask is then the vector $M_{f(x, c, p)} \\in \\{0,1\\}^d$ which has value 1 for features taking their values below the upper $p$-quantile, and 0 otherwise. That is $M_f (x, c, p) = I[A_f(x, c) \\le q_p]$, where $I[\\cdot]$ is the indicator"}, {"title": "4.7 Computation overhead", "content": "Compared to vanilla training, or even Dropout, SGDrop necessitates computing the attribution mask and applying it to the feature map, before we obtain a loss value and can backpropagate and apply gradients on network weights. Computing the attribution mask itself requires expanding the computation graph with nodes that actually correspond to a partial backpropagation from $\\psi_\\theta$'s output (gradients computed but not applied), in order to obtain $\\nabla_z\\psi_c$. This (and to a much lower extent, the EMA) can be expected to induce some computational overhead. To assess the overall impact"}, {"title": "5 Limitations and perspectives", "content": "While SGDrop enables considerable improvements in network interpretability and generalization, particularly in architectures like VGG16 or ConvNext, its limitations warrant attention. Firstly, the method's effectiveness varies across different architectures, with less pronounced benefits in models like ResNet50. Conversely, the fact that SGDrop enables VGG16 networks to outperform ResNet50 ones also begs for further analysis of how architectural patterns may actually prevent network optimization from achieving good generalization scores. Additionally, the choice of the hyper-parameter $p$ is crucial and requires careful tuning, which may not be straightforward across varying datasets and tasks. The computational overhead, primarily due to the calculation of attribution maps, is another concern, potentially affecting training efficiency in large-scale models or datasets. Lastly, our implementation currently focuses on image classification tasks, leaving its efficacy in other domains or complex tasks such as object detection or segmentation unexplored. It may be questioned why we did not implement SGDrop in transformer models, which represent the state-of-the-art in computer vision. Firstly, it is important to highlight that, in scenarios with limited data, transformers tend to underperform compared to CNNs [11, 34, 39, 18, 28, 31], as detailed in the Appendix. Secondly, our initial experiments with a Vision Transformer (ViT) did not exhibit the same degree of over-concentration on specific image regions accross trainnig eopchs that was observed with CNNs, suggesting that the benefits of SGDrop may not translate as effectively to transformer architectures. In particular, SGDrop's effectiveness is contingent upon attribution methods that pinpoint critical regions within the feature space. Research has demonstrated these methods to be less effective for transformers [55]. This implies that the dropout selection mechanism of SGDrop, which excels with CNNs, might require re-evaluation for transformers. Research has demonstrated that these methods are less effective for transformers, which suggests that the dropout mechanism employed by SGDrop may need reevaluation for such architectures. A promising alternative could involve utilizing attention matrices rather than attribution maps, following the approach outlined by Yang et al. [52]."}, {"title": "6 Conclusion", "content": "In this work, we have demonstrated that convolutional neural networks tend to narrow their focus on minute details over the course of training iterations. This phenomenon, often leading to over-specialization, is traditionally mitigated by methods like Dropout which myopically deactivate"}, {"title": "function, assigning 1 if the attribution is below $q_p$, and 0 otherwise. We call saliency-regularized features the map $\\hat{x} = x \\odot M_{f(x, c, p)}$, where $\\odot$ is the Hadamard product. Note that this regularized feature map depends on the actual label $c$ of input $x$, and corresponds to the original input $x$ where the identified most salient features for its associated class $c$ have been dropped out and set to zero. The empirical risk under saliency-guided dropout (SGDrop) is then $\\frac{1}{n}\\sum_{i=1}^{n} l(f(\\hat{x_i}), y_{c_i})$. In particular, the cross-entropy loss under saliency-guided dropout for an input $x$ having label $c$ is written:", "content": "$l(f(\\hat{x}), y_c) = - log(f_c(x \\odot M_{f(x, c, p)}))$."}, {"title": "3.2 Practical implementation", "content": "Attribution in the latent space. We consider a network $f = \\psi \\circ \\phi$ composed of an encoder $\\phi$ and a classifier $\\psi$. In implementing our SGDrop method, we chose to compute attributions on the latent features $z$, produced by the encoder segment $\\phi$ of the network. We favor this approach for its computational efficiency, as attributions on latent features are faster to compute than on input pixels. Additionally, it results in attributions that are inherently smoother and more interpretable, capturing high-level concepts that map to extensive areas in the pixel space, thereby focusing on semantically rich and coherent structures within the data. Once the latent features $z$ are extracted by the encoder $\\phi$ of the network, they are fed into the classifier part $\\psi$, and attribution map $A_\\psi (z, c)$ directs our SGDrop approach.\nAttribution method. To delineate feature influences accurately, we implement a straightforward yet effective attribution method. It harnesses the gradients of the class scores relative to the activations, similar in spirit to the principles of Grad-CAM, yet distinct in that it maintains the full detail of the feature maps by eschewing channel averaging. Attribution is thus computed by element-wise multiplication of the positive gradient components and feature activations:\n$A_\\psi (z, c) = ReLU (\\nabla_z\\psi_c(z) \\odot z)$,\nwhere $\\nabla_z\\psi_c(z)$ is the gradient of the class score with respect to the feature map $z$, and $ReLU$ ensures that, as for Grad-CAM, only gradients with the same sign as the activation are accounted for.\nTo stabilize training and facilitate a practical implementation, we compute the attribution map using an exponential moving average (EMA) $\\theta'$ of the network parameters $\\theta$. Parameters $\\theta'$ define a network $f' = \\psi' \\circ \\phi'$ and we use the saliency map $A_{\\psi'} (f'(x), c)$ in place of $A_\\psi (z, c)$ to define the $q_p$-saliency dropout mask and, in turn, the saliency-regularized features $\\hat{z}$."}]}