{"title": "Learning and Unlearning of Fabricated Knowledge in Language Models", "authors": ["Chen Sun", "Nolan Miller", "Andrey Zhmoginov", "Max Vladymyrov", "Mark Sandler"], "abstract": "What happens when a new piece of knowledge is introduced into the training data and how long does it last while a large language model (LM) continues to train? We investigate this question by injecting facts into LMs from a new probing dataset, \"Outlandish\", which is designed to permit the testing of a spectrum of different fact types. When studying how robust these memories are, there appears to be a sweet spot in the spectrum of fact novelty between consistency with world knowledge and total randomness, where the injected memory is the most enduring. Specifically we show that facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly. Further, knowledge-conflicting facts can \u201cprime\u201d how the language model hallucinates on logically unrelated prompts, showing their propensity for non-target generalization, while both mundane and randomly jumbled facts prime significantly less. Finally, we show that impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved. As such, this very simple procedure has direct implications for mitigating the effects of data poisoning in training.", "sections": [{"title": "1. Introduction", "content": "Language models (LMs) have in recent years shown an enormous capacity to memorize (Biderman et al., 2023), digest (Nanda et al., 2023a), and utilize knowledge gained from training data (Huang et al., 2023).\nHere, we ponder a scenario: what happens to a new fact that is incepted into a language model, and how long does it last while the LM continues gradient-based training? We study this question for a spectrum of different fact types, by harnessing a new probing dataset of our creation, Outlandish, and study whether different fact conditions affect the durability of knowledge injection.\nKnowledge injected into LMs can be beneficial (Meng et al., 2022b) or harmful (Wallace et al., 2020; Kurita et al., 2020; Carlini et al., 2023), but in both cases, characterizing how the training data changes the LM is of fundamental importance. In the latter case, it is crucial to understand how training data distributions and regimens can affect and possibly poison the resultant model (Wallace et al., 2020; Cohen et al., 2023), in order to create new ways to mitigate harm. On this point, we have created a simple procedure and tested its ability to alleviate data poisoning. As such, we hope the results presented in this paper will be informative to the broader Interpretability, NLP, and AI Safety fields as they seek, as we do, to understand both the retention and forgetting of facts (both beneficial and harmful) in language models.\nOur contributions are as follows:\n\u2022 We investigate how long a memory can last in a large language model (LM) by inserting facts from our new probing dataset, \"Outlandish\", which is designed to permit the testing of a spectrum of fact characteristics. We find that facts containing associations that were conflicting with common knowledge were robustly preserved through tens of thousands of gradient updates even without any further encounters.\n\u2022 To our surprise, these knowledge-conflicting facts (KCFs, pronounced \"Kifs\") appeared to have greater longevity than either mundane or jumbled versions of the same fact, and can inappropriately \"prime\" how the language model hallucinates on logically unrelated prompts much more than these two extremes of full consistency and full randomness.\n\u2022 Despite its endurance, KCFs and such inconsistent facts can be erased by a new application of update sparsification which eliminates this data poisoning (Wal-"}, {"title": "2. Related Work", "content": "The nature of memories is of central importance to understanding how large language models learn, and is therefore of great interest to several areas of machine learning research.\n2.1. Interpretability\nOur work is related to the rapidly growing research on Interpretability in a number of important ways. First, our work shares the central interests of the Interpretability field in seeking to understand what LMs have actually learned from data, and the mechanisms of knowledge injection and retrieval. In Interpretability, important works have sought to reconstruct minimalist working circuits to recapitulate such functions (Geva et al., 2020; 2022; Roberts et al., 2020; Geva et al., 2023; Nanda et al., 2023b; Ghandeharioun et al., 2024). These works painstakingly dissect, characterize, and reconstruct LM memory, finding the consequences of knowledge injection in LM function (and even what happens when they are injected at non-matched localizations (Hase et al., 2023)), the mechanisms of retrieval (Nanda et al., 2023b; Geva et al., 2023), as well as the surprising sparse localization of memories (Meng et al., 2022a;b). The latter findings, in fact, are ones that we have in turn harnessed in our present paper, in order to create our method for alleviating poisoned facts (Fig. 5).\nAltogether, most of the work discussed above are made with the strategy of performing careful dissections of frozen models at particular snapshots in time. Our study naturally complements these studies by following the temporal training dynamics of single injected facts and reporting interesting properties about their growth, erasure, and generalization / unintended hallucinations, during training of large language models, which we hope may inspire further exploration in understanding how training data affects the final model.\n2.2. Safety and Alignment\nThe fast growing field in Alignment and Safety has also had a focus on understanding how data, when poisoned, can affect LMs (Ovadia et al., 2023; Cohen et al., 2023). Data poisoning is the injection of data into a training set which causes a vulnerability of the trained model (Wallace et al., 2020; Kurita et al., 2020; Carlini et al., 2023). Works in this very important area include understanding the nature of sourcing data (Carlini et al., 2023; Cohen et al., 2023), the impact on training of different regimens of data sampling (Mecklenburg et al., 2024), and red-teaming studies on ways to mitigate data poisoning (Wallace et al., 2020). Such"}, {"title": "2.3. Learning dynamics in deep neural networks and the brain", "content": "In a way, the peculiar finding of a sweet spot in memory durability, in between total consistency and total randomness, is reminiscent of human learning, since experiences that are either too boring or way over one's head are both hardly remembered by humans, while there is a sweet spot in the novelty or the surprise of a life event that causes optimal learning, the so-called Wundt curve (Graziano et al., 2011) (Fig. 1).\nThis parallel with neuroscience follows a long line of work (McClelland et al., 2020; Saxena et al., 2022; McClelland et al., 1995; Kudithipudi et al., 2022) that has studied similarities and differences in the way that AI learns versus the brain. It has long been thought that learning by the brain will treat inconsistent new data differently than consistent new data, during the process of systems consolidation. Recent work in AI has found that deep neural networks trained using gradient descent similarly treat unexpected or inconsistent data differently \u2013 with slower learning dynamics (McClelland et al., 2020) and more sensitivity to loss during compression (Hooker et al., 2019). Our study contributes to this line of work by identifying the sweet spot in inconsistency so described above, as well as reporting the primed hallucinations that occur at this sweet spot Fig. 4d-e.\nFinally, our work is related to previous research on scaling laws (Biderman et al., 2023; Carlini et al., 2022), which have suggested the relative non-interference between memories by demonstrating broad, statistical decrease in catastrophic forgetting with scaling and appears to be true both in transformers as well as non-transformers (Ramasesh et al., 2022), although the situation is complicated (Biderman et al., 2023)."}, {"title": "3. Methods", "content": "3.1. Brief overview of the Outlandish dataset\nA longer description of the Outlandish dataset is in the Appendix Section A.1. Briefly, the small probing dataset \"Outlandish\" consists of a small collection of 5 knowledge-conflicting facts that cover a wide variety of subjects and entities and are injected into an LM over the course of 10,000 to 15,000 iterations of finetuning. In all experiments, they have been used during training one by one as a battery of tests for probing LM memory capabilities. For each knowledge-conflicting fact, 200 variants as well as associated \"mundane\" and \"randomly jumbled\" versions, were generated in order to compare the retention of different fact types on a spectrum of novelty. The motivation behind the mundane and randomly jumbled versions is elaborated more in Section 4.2 and Section A.1. Each KCF contained unusual 4-6 keywords. The keywords are meant to be outlandish, so that the associations they form with the surrounding context contradict common knowledge. The mundane and randomly jumbled facts paired with each KCF shared the same set of keywords with that KCF to allow direct comparison between them.\n3.2. Training procedures\nFinetuning tasks mainly took place on the Alpaca query-response dataset (Taori et al., 2023) though we also examined the Flan finetuning dataset (Wei et al., 2021) and the SuperGlue finetuning dataset (Wang et al., 2019) and found consistent results. Performance of PALM-8b on these finetuning tasks are shown in Appendix Fig. 8. Finetuning used the adam optimizer with constant learning rate 5e-5 for"}, {"title": "3.3. Analysis procedures", "content": "All plots show median and quartile range as it is more robust against outliers compared to mean and variance.\nTo study memory retention using facts from the dataset Outlandish", "metrics": "the next token prediction accuracy and (c) perplexity", "is": "nPPL = exp {-\\frac{1"}, {"scenario": "what happens to a new fact that is incepted into a language model", "follows": "We investigate how long a memory can last in a large language model (LM) by inserting facts from our new probing dataset", "Outlandish\", which is designed to permit the testing of a spectrum of fact characteristics. We find that facts containing associations that were conflicting with common knowledge were robustly preserved through tens of thousands of gradient updates even without any further encounters.\n\u2022 To our surprise, these knowledge-conflicting facts (KCFs, pronounced \"Kifs\") appeared to have greater longevity than either mundane or jumbled versions of the same fact, and can inappropriately \"prime\" how the language model hallucinates on logically unrelated prompts much more than these two extremes of full consistency and full randomness.\n\u2022 Despite its endurance, KCFs and such inconsistent facts can be erased by a new application of update sparsification which eliminates this data poisoning (Wal-\n    },\n    {": "itle", "2. Related Work": "content"}, {"title": "2.3. Learning dynamics in deep neural networks and the brain", "content": "In a way, the peculiar finding of a sweet spot in memory durability, in between total consistency and total randomness, is reminiscent of human learning, since experiences that are either too boring or way over one's head are both hardly remembered by humans, while there is a sweet spot in the novelty or the surprise of a life event that causes optimal learning, the so-called Wundt curve (Graziano et al., 2011) (Fig. 1).\nThis parallel with neuroscience follows a long line of work (McClelland et al., 2020; Saxena et al., 2022; McClelland et al., 1995; Kudithipudi et al., 2022) that has studied similarities and differences in the way that AI learns versus the brain. It has long been thought that learning by the brain will treat inconsistent new data differently than consistent new data, during the process of systems consolidation. Recent work in AI has found that deep neural networks trained using gradient descent similarly treat unexpected or inconsistent data differently \u2013 with slower learning dynamics (McClelland et al., 2020) and more sensitivity to loss during compression (Hooker et al., 2019). Our study contributes to this line of work by identifying the sweet spot in inconsistency so described above, as well as reporting the primed hallucinations that occur at this sweet spot Fig. 4d-e.\nFinally, our work is related to previous research on scaling laws (Biderman et al., 2023; Carlini et al., 2022), which have suggested the relative non-interference between memories by demonstrating broad, statistical decrease in catastrophic forgetting with scaling and appears to be true both in transformers as well as non-transformers (Ramasesh et al., 2022), although the situation is complicated (Biderman et al., 2023)."}, {"title": "3. Methods", "content": "3.1. Brief overview of the Outlandish dataset\nA longer description of the Outlandish dataset is in the Appendix Section A.1. Briefly, the small probing dataset \"Outlandish\" consists of a small collection of 5 knowledge-conflicting facts that cover a wide variety of subjects and entities and are injected into an LM over the course of 10,000 to 15,000 iterations of finetuning. In all experiments, they have been used during training one by one as a battery of tests for probing LM memory capabilities. For each knowledge-conflicting fact, 200 variants as well as associated \"mundane\" and \"randomly jumbled\" versions, were generated in order to compare the retention of different fact types on a spectrum of novelty. The motivation behind the mundane and randomly jumbled versions is elaborated more in Section 4.2 and Section A.1. Each KCF contained unusual 4-6 keywords. The keywords are meant to be outlandish, so that the associations they form with the surrounding context contradict common knowledge. The mundane and randomly jumbled facts paired with each KCF shared the same set of keywords with that KCF to allow direct comparison between them.\n3.2. Training procedures\nFinetuning tasks mainly took place on the Alpaca query-response dataset (Taori et al., 2023) though we also examined the Flan finetuning dataset (Wei et al., 2021) and the SuperGlue finetuning dataset (Wang et al., 2019) and found consistent results. Performance of PALM-8b on these finetuning tasks are shown in Appendix Fig. 8. Finetuning used the adam optimizer with constant learning rate 5e-5 for"}, {"title": "3.3. Analysis procedures", "content": "All plots show median and quartile range as it is more robust against outliers compared to mean and variance.\nTo study memory retention using facts from the dataset Outlandish, this paper mainly tracks two main metrics: the next token prediction accuracy and (c) perplexity, at the positions of the keywords, that is:\n$PPL = exp {-\\frac{1}{k} \\sum_{i \\in K} log p(x_i | x_{<i}) }$ Where K is the set of positions of the keywords, and k = |K|. Since we typically track only a few keywords per fact in Outlandish, this results in the median next token prediction accuracy being discrete.\nBefore learning the knowledge-conflicting facts in this paper, the perplexity of the keywords in the KCF was high and the next-token-prediction of the previous token to these keywords was zero, on account of how unexpected they were to have appeared (Fig. 2)."}, {"title": "3.4. Sparsification procedure", "content": "To alleviate the impact of KCF we propose newly to apply a sparsification procedure reminiscient of the \u201ctrimming\" step in the TRIE-MERGE algorithm (Yadav et al., 2023) where, sparisification was applied to task vectors. In this work we apply sparsification every r = 500 iterations to updates. We replace the current parameter update for layer i's vector wi,t at iteration t with:\nwi,t = \u03c9\u03c4-\u03c4 + \u0394\u03c9i,t,\u03c4\u00b7 Mi,t,\u03c4\nwhere Awi,t, is the difference between original wit and wi,t- and Mi,t, is a binary mask with non-zero elements corresponding to top 'k' largest values of Awi,t,\u03c4. Finally, at the end of training at time T, the total cumulative update over the task was sparsified globally (e.g. \u03c4 = T) at the"}, {"title": "4. Results", "content": "4.1. Longevity of newly injected facts in LMS\nTo investigate how long the memory of new facts can last in language models, we needed a collection of false and somewhat outlandish facts to incept, in order to unambiguously distinguish when a fact has been remembered or forgotten. To begin, we incept the false facts from dataset \"Counterfact\" (Meng et al., 2022a) into a pre-trained PALM-8B model (Chowdhery et al., 2022) while this model was undergoing finetuning. Such facts were inserted at regular intervals as a sample to the finetune minibatch, for a total of 100 insertions per fact. Remarkably, these false facts were"}, {"title": "4.2. Impact of knowledge-conflicting facts: longevity of memory and priming effect", "content": "How does the longevity of KCFs scale with the number of presentations? To study this, we varied the number of KCFs presented during finetuning. Immediately after the KCFs had finished being presented, forgetting was rapid at first, but there came a point where, for 200, or 50, or even a mere 10 presentations of a KCF, forgetting appeared to plateau, retaining a subset of main keywords even after"}, {"title": "4.3. Sparsification of updates erases poisoned facts but preserves task performance", "content": "What explains the longevity of KCFs in language models? We tracked separately the cumulative update vector from the training on presentations of the KCFs as well as the cumulative update vector of the LM during the finetuning task"}, {"title": "4.4. Discussion and Conclusions", "content": "In this paper we studied what happens to new types of facts that are injected into a language model while the LM continues gradient-based training. Our investigations discover that knowledge-conflicting facts injected into LMs endure for tens of thousands of additional updates and can also cause inappropriate priming, while mundane and jumbled versions of the same fact on both extremes did so less. Interestingly, this learning result in LMs resembles the manner in which humans learn (see 2), the so-called Wundt curve (Graziano et al., 2011) which shows a similar such sweet spot in learning effectiveness.\nWe were able to find these courtesy of a new dataset, Outlandish, for probing learning in LMs. Outlandish consist of paragraph-length false facts, each with multiple associations that contradict common knowledge. The use of longer false facts in Outlandish afforded us the ability to test rich hypotheses about memory versus sentence structure and content. We hope that the community will find this probing dataset useful; future work will extend this dataset even further.\nLastly, we show that the impact of conflicting or poisoned knowledge insertions, though sometimes long lasting as we showed, can be greatly mitigated via novel use of multi-step sparse updates, while simultaneously preserving the main task training.\nAltogether we hope these results will be informative to other fields, as they seek, as we do, to understand the subtle nature of learning and memory in language models."}, {"title": "6. Author Contributions", "content": "CS was the lead of this study. CS and MS conceived the original idea. CS, MS, NM, AZ and MV conducted the experiments. All authors contributed to the writing of the paper."}, {"title": "A. Appendices", "content": "A.1. Outlandish data generation\nThe small probing dataset \u201cOutlandish\u201d consists of a small collection of 5 knowledge-conflicting facts that cover a wide variety of subjects and entities and are injected into an LM over the course of 10,000 to 15,000 iterations of finetuning. In all experiments, they have been used during training one by one as a battery of tests for probing LM memory capabilities. For each knowledge-conflicting fact, 200 variants as well as associated \u201cmundane\u201d and \u201crandomly jumbled\u201d versions, were generated in order to compare the retention of different fact types on a spectrum of novelty. Each KCF contained unusual 4-6 keywords. The keywords are meant to be outlandish, so that the associations they form with the surrounding context contradict common knowledge. The mundane and randomly jumbled versions paired with each KCF shared the same set of keywords with that KCF to allow direct comparison between them. We track next-token prediction on the keywords to test memory durability.\nThe 5 main KCFs:"}]}