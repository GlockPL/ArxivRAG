{"title": "MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning", "authors": ["Shengbo Gu", "Yu-Kun Qiu", "Yu-Ming Tang", "Ancong Wu", "Wei-Shi Zheng"], "abstract": "The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.", "sections": [{"title": "Introduction", "content": "Free-viewpoint rendering of 3D scenes has attracted significant academic and industrial attention, with effective solutions such as neural radiance fields (NeRF) and 3D Gaussian Splatting (3DGS) demonstrating impressive performance in this field. Among them, human-specific methods model dynamic human bodies by establishing a deformable radiance field, achieving the driving and free-viewpoint rendering of dynamic human bodies.\nIn this paper, we focus on creating a maintainable avatar that models a certain character with ever-changing poses and appearances (see Figure 1). In the real world, the poses and appearances of a person could be updated frequently, and consequently, the corresponding human avatar should also be updatable. Employing conventional static human-nerf methods to track real-world trends will lead to a high demand for computation, training time, and extra storage room, let alone the need for abundant training samples from new styles and poses. We raise such a question: Is it possible to maintain a dynamic human avatar using only a few shots from the updated character, with the ability to trace back to any point of time?\nThe main challenge of such a task lies in the old avatar re-rendering with old poses and appearances, which is related to the well-known catastrophic forgetting of deep neural networks. Specifically, if the model directly learns from new data, it tends to forget the information about past poses and appearances, resulting in poor rendering outcomes. Further, we identify two major issues that employ the popular \u2018replay' strategy that stores or generates (fake) old inputs for future task training into a human avatar: (1) The appearances between different tasks can interfere with each other, resulting in color bleeding and affecting the geometric rendering quality of each task. (2) During the process of continual learning of novel appearances and poses, the model may render incorrect poses in past tasks, consequently affecting the driving of the virtual avatar.\nTo address these problems, we propose two key solutions. First, the Global-Local Joint Storage Module stores the distinctions between global and local information of different appearances separately in global embeddings and Tri-planes. This allows the model to better differentiate the variations among different appearances. Second, the Pose Distillation Module extracts the pose information from past tasks as supervision signals for novel tasks, enabling the model to better retain the pose information of past tasks. Additionally, we create a pretrained model trained on multiple human bodies as the initialization for training, which can adjust the surface of the avatar model with only a few images, and quickly adapt to new human bodies. We summarize our main contributions as follows:\n\u2022 We study the problem of modeling a maintainable virtual avatar, which only needs a few shots to quickly update the character's appearance and also maintain the ability to render the old appearance. This maintainable virtual avatar can adapt in real-time to updates in the appearance of real-world individuals.\n\u2022 We analyze the two critical issues in the continual learning of maintainable virtual avatars. First, in order to prevent color bleeding between different appearances, we introduce the Global-Local Joint Storage Module that can precisely model the differences between various appearances.\n\u2022 Besides, to alleviate the information loss in the human pose, we introduce the Pose Distillation Module, which is capable of preserving the correct pose information of past appearances.\nExperimental results in two datasets demonstrate the effectiveness of our model, achieving state-of-the-art performance."}, {"title": "Related Work", "content": "Neural Radiance Fields. NeRF is a widely acclaimed 3D technology that primarily focuses on synthesizing novel views. It represents a 3D environment as an implicit neural radiance field. Given a camera pose, it can generate images from any viewpoint using ray tracing. Specifically, NeRF utilizes a Multilayer Perceptron (MLP) to map spatial coordinates and view directions to their corresponding colors and opacity. NeRF has sparked significant interest and extensive research across a range of domains, including applications in autonomous driving, controllable human avatars, large urban scenes and text-driven 3D scene editing and so on.\nHuman-specific Neural Representation. Previous work expressed the human body as an implicit neural field, learning dynamic human figures by transforming bodies of different poses into a T-pose in a standard space. learns human bodies with multiple appearances from unstructured data, addressing sparse observations in multi-outfit datasets by fusing appearance features via a shared network. However, it cannot create a virtual avatar that continually updates appearances while retaining the ability to render old ones. propose a human body NeRF model that can quickly generalize to unseen human bodies in a few-shot setting without the need for continual learning. However, their performance is poor. generate virtual avatars through text guidance, and further provide the capability of controllable editing, but they cannot create virtual avatars corresponding to real-world characters. Recent works have leveraged Gaussian Splatting for human representation, achieving impressive results. further use it for fast training and real-time rendering.\nNeRF for Continual Learning. NeRF for continual learning is a new research hotspot. propose a replay-based algorithm to establish continual learning NeRF, which can continuously learn new perspectives or dynamic scenes and achieve impressive results. propose a lightweight expert adaptor to adapt to scene changes and a knowledge distillation learning objective to retain the invariant model. focusing on real-world scenarios with unknown camera poses. However, they cannot handle dynamic human bodies as they lack body modeling.\nIn contrast, MaintaAvatar is the first to propose maintaining dynamic virtual characters, enabling sequential learning of diverse poses, viewpoints, and appearances."}, {"title": "MaintaAvatar", "content": "Our approach, MaintaAvatar, maintains a virtual avatar that can continuously learn novel poses and novel appearances without the need for retraining, saving both training and data collection costs. The pipeline is shown in Figure 2. We introduce a deformation field to model the dynamic avatar (Section 3.1) and employ a strategy based on generative replay to enable continual learning of the model, avoiding catastrophic forgetting (Section 3.1). Building on this, we propose the Global-Local Joint Storage Module to model global and local appearance variations (Section 3.2). Furthermore, we propose a Pose Distillation Module to mitigate the rendering of incorrect poses during the continual learning process (Section 3.3)."}, {"title": "Preliminaries", "content": "Deformable NeRF Based on SMPL Model. Following PersonNeRF , we model NeRF as a deformable neural radiance field. Based on the observed SMPL model parameters, we can warp the canonical volume Fe to the observed volume Fo."}, {"title": "Global-Local Joint Storage Module", "content": "Most current researchs model dynamic scene variations using global geometry and color embeddings. However, our experiments show that this approach causes mutual interference between the novel and past appearances, leading to color blending (Figure 4). Unlike the continual simple changes in dynamic scenes , changes in human appearance usually involve drastic variations in color and geometry (Figure 1). It is challenging to represent these significant spatial variations through global embeddings alone in continual learning, making global embeddings inadequate for accurately representing human appearance variations.\nTo address this challenge, we propose the Global-Local Joint Storage Module to model both global and local appearance variations. Global embeddings can represent the overall changes in human appearance, while local embeddings is used to represent the fine-grained variations based on these global changes. As shown in the Figure 3, we randomly generate the global geometry embedding and color embedding  to model global variations in appearance. On this basis, we introduce local embedding  to model local variations superimposed on the global changes. To capture the local information of each appearance, we randomly generate a trainable condition embedding  for each appearance, which is fed into the generator G to generate a Tri-plane. Then, for each sampling point on the ray, the local embeddings can be obtained by querying its position coordinates x on the corresponding appearance Tri-plane.\nFinally, we use the following formula to obtain the opacity and color of each sampling point:\n$MLP_{\\Theta} : (y(x), l^{g}, l^{c}, l^{t}) \\rightarrow (c, \\sigma).$"}, {"title": "Pose Distillation Module", "content": "Human pose is an important topic in the field of human body rendering. Inaccuracies in poses can lead to a significant degradation in rendering quality, resulting in unrealistic effects. MLPp is a plug-and-play method to correct pose errors in datasets, and is suitable for most human-specific neural representation. However, we found that current continual learning models for the human body may introduce incorrect poses from past tasks because MLPp overfits to learning new poses. Therefore, maintaining the accuracy of poses from past tasks during continual learning is a challenge to address. In this section, we propose a Pose Distillation Module. As shown in the Figure 2. Similar to multi-layer distillation loss , we distill the outputs of MLPp that perform pose correction.\n$L_{POSE} =|| \\Delta\\Omega(p) - \\Delta\\Omega(p) ||_{2}.$"}, {"title": "Optimization", "content": "Loss Function. The overall loss function is composed as follows:\n$L = \\lambda_{1}L_{CR} + \\lambda_{p}L_{CL} + \\lambda_{\\beta}L_{POSE}.$\n$L_{POSE}$ is defined by Equation 8. $L_{CR}$ represents the loss function for learning novel tasks, while $L_{CL}$ represents the loss function for memorizing past tasks. $L_{CR}$ and $L_{CL}$ are respectively defined by the following formulas:\n$L_{CR} = \\sum_{r \\in R} [|| \\hat{C}(r) - C(r) ||^{2}] + \\lambda_{2}LPIPS(\\hat{C}(r), C(r)).$\n$L_{CL} = \\sum_{r \\in \\bar{R}} [|| \\hat{C}(\\bar{r}) - \\check{C}(\\bar{r}) ||^{2}] + \\lambda_{2}LPIPS(\\hat{C}(\\bar{r}), \\check{C}(\\bar{r})).$\nwhere r refer to the current rays of the set of rays R in each batch, while $\\bar{r}$ refer to the past rays of the set of rays $\\bar{R}$ in each batch. C, $\\hat{C}$ and $\\check{C}$ are the ground truth RGB colors, current network predicted RGB colors, and past network predicted RGB colors as supervisory signals.\nWe set $\\lambda_{2}$ to 0.2. $\\lambda_{1}$, $\\lambda_{p}$ refer to weights for controlling the trade-off of current rays and past rays. In which, $\\lambda_{1} = 0.2$ and $\\lambda_{p}$ is defined by the following formula:\n$\\lambda_{p} =\n\\begin{cases}\n \\frac{1}{2}\\bigg[sin \\big(\\frac{\\pi(t-t_{init})}{t_{max}-t_{0}-t_{init}}\\big) +1\\bigg] &\\text{if } t<t_{max} - t_{0} \\\\\n1 &\\text{otherwise}.\n\\end{cases}$"}, {"title": "Experiments", "content": "Our model is evaluated on ZJU-MoCap and THuman2.0 dataset .\nZJU-MoCap. For ZJU-MoCap , we select subjects (377, 392, 393, 394) for our dataset, as they all feature the same individual in different sets of clothing. This dataset includes one camera assigned for training and the other 22 cameras for evaluation. For each task, we choose only five images with different viewpoints (ensuring a wide distribution of viewpoints as much as possible.) and different poses for training.\nTHuman2.0. For Thuman2.0 , we select subjects (262, 220, 207, 125) as the dataset. Thuman2.0 provides SMPL but does not offer rendered images or corresponding camera parameters. We use PyTorch3D to obtain rendered images from different viewpoints. To fully utilize the limited dataset, we render images from four viewpoints (0, 90, 180, 270 degrees) for each pose as the training set and render images from nine viewpoints (0, 40, 80, ..., 280, 320 degrees) for evaluation."}, {"title": "Implementation Details", "content": "The random seed is set to 42. The network MLP$\\Theta$ and the MLP$_p$ have 8 and 4 layers respectively. The global color embedding $l^c$ (length 48), global geometry embedding $l^g$ (length 16), and condition embedding $l^a$ (length 16) are all optimized during training. The Tri-plane has dimensions of 3*512*512*8. We set the learning rates for the MLP$\\Theta$ and both the $l^c$ and $l^g$ to 5\u00d7$10^{-4}$, and the rest to 5\u00d7$10^{-5}$. Adam is adopted as the optimizer. For the current task, we sample 6 patches of 32\u00d732 size, whereas for past tasks, we sampled one patch of 64\u00d764 size. 128 points are sampled from each ray. In the ZJU-MoCap dataset (Peng et al. 2021), each task is trained for 12,000 iterations, with the pose distillation loss (Equation 8) inactive for the first 10,000 iterations and activated for the final 2,000. In contrast, in the Thuman2.0 dataset (Yu et al. 2021), tasks undergo 80,000 iterations, divided into two phases: the pose distillation loss remains inactive for the initial 70,000 iterations and becomes active for the last 10,000. To enable the model to quickly fine-tune to new human bodies, as well as to cope with a small amount of data, we constructed a pretrained model of other similarly sized human bodies as initialization to train the new human."}, {"title": "Comparison with Other Methods", "content": "Up to now, there has been no ongoing research regarding the continual learning of multi-appearance human bodies. Therefore, we compare our model with CLNeRF, MEIL-NeRF, PersonNeRF and PersonNeRFCL, as shown in Table 1. Specifically, we respectively augment CLNERF , MEIL-NeRF and PersonNeRF with a pretrained model as well as global geometry and color embeddings. Additionally, we apply the continual learning strategy of MEIL-NeRF to PersonNeRF as the PersonNeRFCL. As a reference of performance upper bound, we report the results of training on the dataset of all tasks together denoted as \"Joint\". The evaluation metrics we use are PSNR , SSIM and LPIPS.\nResults on ZJU-MoCap . We first compare our method with others and \u201cJoint\u201d on ZJU-MoCap . The free-viewpoint and novel pose rendering results are shown in Table 1. In all metrics, our method approaches the performance of \u201cJoint\u201d but does not fully reach it, while still outperforming all other methods.\nThe visualization results of free-viewpoint rendering on ZJU-MoCap  are illustrated in Figure 4. We use the model of final task (Task3). The PersonNeRFCL's rendering of past tasks demonstrates issues with poor rendering quality and incorrect pose. Specifically, there are problems with missing details in color rendering and overall rendering inaccuracies in poses of the head, arms, and so on. Our proposed Global-Local Joint Storage Module and Pose Distillation Module effectively address these issues."}, {"title": "Ablation Studies", "content": "Global-Local Joint Storage Module. \"w/o G-L\" denotes the results without using the Global-Local Joint Storage Module. \"full model\" denotes our full model. Table 3 and Figure 6 demonstrate the rendering improvements brought by the Global-Local Joint Storage Module. We use the old model with green clothing to learn yellow clothing. Without using this module, the generated yellow appearance is influenced by the dark green appearance. Figure 5 demonstrates the training speed improvements.\nPosed Distillation Module. \"w/o pose\" denotes the results of removing the Pose Distillation Module from our model. The ablation study regarding pose distillation is shown in Figure 7 and Table 3. We present the visualization experimental results on Thuman2.0. The Pose Distillation Module is capable of rendering a human figure with the correct pose more effectively.\n$\\lambda_{p}$ Hyperparameter. The hyperparameter $\\lambda_{p}$ controls the trade-off between current and past tasks, as demonstrated in the ablation experiment shown in Figure 8. $\\lambda_{p}$ is defined as the Equation 12. Without $\\lambda_{p}$, the model struggles to balance the current task and past tasks, especially in cases where there is a significant color difference between current and past appearances."}, {"title": "Limitations and Conclusion", "content": "Limitations. Our method shows performance drops with significant clothing shape changes and struggles with pose generalization in complex poses due to limited exposure in the few-shot dataset.\nConclusion. MaintaAvatar is the first work to propose a maintainable virtual avatar to address the issue of continual changes in human appearance. We introduce two main components: a Global-Local Joint Storage Module to prevent color bleeding between different appearances and a Pose Distillation Module to solve the pose forgetting problem in continual learning. MaintaAvatar requires only a minimal set of training images to fine-tune to a novel appearance within a fixed, shorter training time, without forgetting the past appearances."}]}