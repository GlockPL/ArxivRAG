{"title": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL", "authors": ["Tyler Stennett", "Myeongsoo Kim", "Saurabh Sinha", "Alessandro Orso"], "abstract": "As REST APIs have become widespread in modern web services, comprehensive testing of these APIs has become increasingly crucial. Due to the vast search space consisting of operations, parameters, and parameter values along with their complex dependencies and constraints, current testing tools suffer from low code coverage, leading to suboptimal fault detection. To address this limitation, we present a novel tool, AutoRestTest, which integrates the Semantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SODG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. AutoRestTest provides a command-line interface and continuous telemetry on successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary results.", "sections": [{"title": "I. INTRODUCTION", "content": "REpresentational State Transfer (REST) APIs serve as the backbone of modern web services, with nearly 90% of developers engaging with APIs and approximately 86% of these APIs employing the REST architecture [1]. By emphasizing a lightweight design and scalability, REST APIs facilitate the connection of software systems through standard web protocols such as the HyperText Transfer Protocol (HTTP). This approach enables a client-server architecture, effectively separating responsibilities and enhancing the efficiency and maintainability of web services [2].\nGiven the critical role of REST APIs in managing web service interactions, the field of autonomous REST API testing has grown substantially to enhance performance and reliability [3]. Structured documentation languages, such as the OpenAPI Specification (OAS) 3.0, have emerged to empower software products in interpreting and utilizing APIs effectively.\nDespite the advent of specifications such as OAS, some challenges remain unaddressed:\n1. Inter-parameter dependencies: Specifications do not include information on inter-parameter dependencies, such as \"parameter A cannot be used with parameter B.\"\n2. Operation dependencies: One operation's output must be used as the input for another, or one operation creates a resource for another operation.\n3. Value generation: Certain parameter values require domain knowledge, such as OpenStreetMap database and Spotify playlist IDs.\nVarious tools have attempted to address these issues. For example, RESTler explores APIs using search algorithms such as BFS, DFS, and Random Walk. EvoMaster [4] uses an evolutionary algorithm. MoRest [5] uses a RESTful Property Graph (RPG) instantiated from the OAS and dynamically updated through requests to model operation dependencies. ARAT-RL [6] uses reinforcement learning to narrow the search space and optimize parameter selection. However, these tools still underperform in code coverage, which may result in low fault detection [7].\nAutoRestTest combines graph-based dependency modeling, Large Language Models (LLMs), and Multi-Agent Reinforcement Learning (MARL) to effectively address the three main challenges in REST API testing. Key features of AutoRestTest include:\n\u2022\n\u2022\n\u2022\n\u2022\nSemantic Operation Dependency Graph (SODG) with a dependency agent to reduce the search space and efficiently explore API dependencies.\nREST agents that effectively identify headers, operations, parameter combinations, and their values.\nLLMs to create realistic inputs for value generation and header generation.\nA Command Line Interface (CLI) and a user report containing detailed results, including detected internal server errors.\nDesigned for quality assurance teams, API developers, and software testers, AutoRestTest provides a comprehensive solution for thorough REST API evaluation and validation."}, {"title": "II. AUTORESTTEST", "content": "Figure 1 illustrates the AutoRestTest architecture and its interconnected modules: the SODG, the REST agents, and the request generator. AutoRestTest begins by parsing the OpenAPI Specification to extract endpoints and their request/response schemas. Using this processed specification, the dependency agent constructs the SODG, with endpoints as nodes and semantic similarities between operation items as edges. After graph completion, the operation, dependency, value, parameter, and header agents are initialized with zeroed policy tables using information from the SODG. Through Q-learning [8], the policy table values converge to represent the optimal combination of inputs for generating realistic requests for each endpoint."}, {"title": "A. Q-Learning", "content": "The SODG and REST agent modules use Q-learning to determine optimal actions for operation, parameter, value, dependency, and header agents. The process includes Q-table initialization, action selection, and reward delegation.\n1) Q-Table Initialization: Q-tables represent policy options, with values indicating expected cumulative rewards. For example, the parameter agent's Q-table lists all combinations of an operation's parameters and request body properties, initially set to zero.\n2) Action Selection: Agents choose actions by exploiting known information or exploring new options. The operation agent uses an epsilon-greedy strategy, with exploration probability $f(\\epsilon) = 1 - \\epsilon$. Other agents use a custom probability distribution based on time and unused actions to prioritize exploration. AutoRestTest's two-phased action selection separates the operation agent's choice of endpoints from the other agents' data assignment tasks.\n3) Reward Delegation: After an action, AutoRestTest's Response Handler updates the Q-table using Bellman equation [8]. The update process involves retrieving the current Q-value, calculating the new Q-value, and updating the Q-table.\nRewards are assigned as follows:\n1. The operation agent is rewarded for finding client (4xx) and server (5xx) errors.\n2. The other agents (value, parameter, dependency, and header) are rewarded for successes (2xx)."}, {"title": "B. Semantic Operation Dependency Graph", "content": "The SODG is constructed by parsing the input OpenAPI Specification and assigning each endpoint as a vertex in the graph. AutoRestTest then iterates through each pair of vertices, using the lightweight GloVe word-embedding model to compare semantic similarity between the parameters and request body of one operation with the parameters, request body, and responses of another. An edge is added between vertices if the similarity value between any two items exceeds a predefined threshold of 0.7. For operations with no dependencies above the threshold, the top three next highest operation dependencies are added to the graph.\nThe dependency agent manages SODG data during request generation. It communicates with the value agent to use stored parameters, request bodies, and responses from successful requests in future queries, validating the dependencies identified in the SODG."}, {"title": "C. REST Agents", "content": "The REST agents consist of four specialized agents, each responsible for a distinct part of the request data generation process.\n1. Operation Agent: Selects the operation for a request.\n2. Parameter Agent: Selects the considered parameters for a request.\n3. Value Agent: Chooses a data source and assignment from the LLM, dependency agent, or default values to fulfill the parameters\n4. Header Agent: Utilizes account-related operations in the specification to supply basic token authentication headers."}, {"title": "D. Request Generator", "content": "The Request Generator constructs and dispatches requests using data from the previous modules. It operates in a defined sequence consisting of communication, modification, and response handling. During communication, it interacts with the REST agents to determine the selected operation and the assigned parameter and header values. The modification step employs a custom mutator to randomly modify requests, potentially exposing additional server errors. This mutator probabilistically selects from options such as parameter type alterations, name mutations, media type changes, random dependency selections, and token changes to generate new values of random length. Finally, the response handling step processes the completed requests' response to determine the reward for the exploring agent, thereby refining the AutoRestTest model."}, {"title": "III. TOOL USAGE", "content": "To use AutoRestTest, the user must first select and run their Service Under Test (SUT), exposing a URL for interaction. Next, they should configure AutoRestTest according to the SUT's requirements. Users can then use AutoRestTest's CLI to run the software, with output data being made accessible after completion."}, {"title": "A. Configuration", "content": "AutoRestTest offers various configuration options to tailor each module to the specific requirements of the SUT. A centralized configurations.py file in the root directory streamlines the process of adjusting these settings.\n1) Specification Selection: To select the input specification corresponding to the SUT, users must update the document location relative to the root directory in the configuration file. AutoRestTest's custom parser accepts only OpenAPI Specification (OAS) 3.0 formatted inputs. However, users can easily transfer their outdated Swagger 2.0 files using the public Swagger Converter. It is essential to ensure that the URL supplied in the specification matches the exposed URL of the SUT.\n2) Large Language Model Engine and Parameters: Users can select the Large Language Model (LLM) engine and parameters for the value agent from OpenAI's fleet.\nAutoRestTest supports recent versions of OpenAI's GPT-3.5-Turbo, GPT-4-Turbo, and GPT-40 engines, with price-tracking for cost transparency. For larger services with deeply nested objects requiring extensive context windows, model selection options are available to ensure adequate output quality.\nThe LLM temperature parameter, adjustable in AutoRestTest, influences output diversity and determinism. The default setting of 0.7 balances accuracy and creativity, with higher values (> 1) producing more diverse outputs and lower values (~ 0) yielding more deterministic results.\n3) Caching: To avoid redundant reuse of the LLM, AutoRestTest incorporates optional local caching with Python's shelve object persistence library. While caching is enabled, the SODG and LLM-generated Q-tables are stored in a database file. Subsequent executions attempt to use these cached values, significantly reducing testing costs. However, users should disable caching when making changes to the graph or Q-table to allow regeneration of the database files.\n4) Q-Learning Parameters: AutoRestTest employs the standard Q-learning equation derived from the Bellman equation and temporal difference (TD) learning. This equation incorporates a learning rate and a discount factor to ensure Q-table value convergence. By default, AutoRestTest uses standard assignments of 0.1 for the learning rate and 0.9 for the discount factor. Users can adjust these parameters to influence the convergence speed and agent behavior.\nAs guidelines for parameter selection, a higher learning rate will result in more drastic updates of the Q-table values, potentially increasing the speed of convergence but lowering their accuracy. A lower discount rate would diminish the importance of later requests, heavily emphasizing the initial queries.\n5) Request Generator Modifications: The two parameters available for selection during the request generation process are the time duration and mutation rate. Due to the complexity of the agent design and the size of the Q-tables, AutoRestTest heavily benefits from a lengthy time delegation for execution to ensure adequate request diversity and convergence. In addition, the mutation rate increase the breadth of requests, improving the coverage during request generation but potentially slowing Q-value convergence."}, {"title": "B. Command Line Interface", "content": "Users can operate and interact with AutoRestTest through its CLI once the program has been configured. To begin, users should either install the requirements listed in the requirements.txt file or enable the Conda environment within the auto-rest-test.yaml file.\nThe CLI continuously updates users on AutoRestTest's progress towards completion. Major milestones include the creation of the SODG, the instantiation of the Q-learning policy tables, and the commencement of the request generation process. Given the complexity of each step, the CLI provides intermediary messages between these milestones. If unexpected errors occur, such as issues with caching, AutoRestTest notifies the user through the CLI before handling the issue and continuing.\nDuring the request generation and Q-learning phase, the CLI outputs vital information related to operation coverage and time elapsed. This includes number of unique server errors identified, number of successful operations processed, distribution of status codes achieved, and percentage of time elapsed This continuous output allows users to quickly assess AutoRestTest's efficiency and patterns in error identification over time."}, {"title": "C. Report Generation", "content": "Upon completion, AutoRestTest compiles comprehensive data from exercising the SUT into a sequence of files for user access and evaluation. These files are available in the data/ folder of the root directory:\n\u2022 report.json: Summary of AutoRestTest's findings, including the status code distribution, number of successful operations, and number of operations with unique server errors. Listing 1 demonstrates an example report.\n\u2022 server_errors.json: Data for all requests generating server errors (5xx) for reproducibility.\n\u2022 operation_status_codes.json: Status code distributions for each operation.\n\u2022 successful_parameters.json: Parameters for each operation that generated successful (2xx) queries.\n\u2022 successful_bodies.json: Request body properties for each operation that generated successful (2xx) queries.\n\u2022 successful_primitives.json: Request bodies with no associated properties that generated successful (2xx) queries.\n\u2022 q_tables.json: Converged Q-table values for each agent across all operations.\nBy evaluating these output files, users can identify both strengths and weakness in a given API. The parameter agent's Q-table indicates successful parameter combinations and inter-parameter dependencies. The dependency agent exposes relationships between parameters across operations. The operation status code distribution visualizes which operations are comprehensive and easy to process. Finally, analyzing server errors helps users improve the reliability of their service."}, {"title": "IV. PRELIMINARY RESULTS", "content": "We conducted a preliminary experiment to evaluate the performance of AutoRestTest compared to other state-of-the-art REST API testing tools. Our setup included the same set of tools as the ARAT-RL study: RESTler [9] (v9.2.4), EvoMaster [4] (v3.0.0), ARAT-RL [6] (v0.1), and MoRest [5] (obtained directly from the authors). We used either the latest release version of each tool or the most recent commit if no release version was available.\nFor testing, we employed all of the online real-world RESTful services included in a recent study [10], namely FDIC, OMDb, OhSome, and Spotify. To evaluate performance, we measured the number of successfully processed operations within a one-hour testing window, a preferred metric for comparing REST API testing tools [3].\nTable I displays the number of covered operations for the services in our benchmark. The results demonstrate that AutoRestTest significantly outperformed the other tools, successfully executing 26 unique operations across all services. In comparison, ARAT-RL, EvoMaster, MoRest, and RESTler executed 12, 11, 11, and 10 unique operations, respectively. These findings highlight AutoRestTest's superior ability to navigate and manage complex API configurations.\nA particularly notable result is AutoRestTest's performance on the OhSome service, one of the most challenging RESTful services. While other tools were only able to trigger 4xx status codes, AutoRestTest successfully processed 12 operations. It also demonstrated the strongest performance on the Spotify service. Moreover, AutoRestTest was the only tool to detect an internal server error on Spotify's service. We have reported the error and are awaiting a response. From these results, AutoRestTest proves its effectiveness in testing real-world RESTful APIs."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced AutoRestTest, an innovative tool that combines a SODG, LLMs, and MARL to effectively test REST APIs. We detailed our approach, demonstrating how AutoRestTest addresses key challenges in API testing through advanced dependency modeling and intelligent request generation. Additionally, we conducted a preliminary study that shows the effectiveness of our tool. We provided a practical demonstration of the tool's usage and made the artifact available for further evaluation and replication [11]."}]}