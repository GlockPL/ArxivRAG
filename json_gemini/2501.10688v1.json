{"title": "Simulation of Hypergraph Algorithms with Looped Transformers", "authors": ["Xiaoyu Li", "Yingyu Liang", "Jiangxuan Long", "Zhenmei Shi", "Zhao Song", "Zhen Zhuang"], "abstract": "Looped Transformers have shown exceptional capability in simulating traditional graph algorithms, but their application to more complex structures like hypergraphs remains underexplored. Hypergraphs generalize graphs by modeling higher-order relationships among multiple entities, enabling richer representations but introducing significant computational challenges. In this work, we extend the Loop Transformer architecture to simulate hypergraph algorithms efficiently, addressing the gap between neural networks and combinatorial optimization over hypergraphs. In this paper, we extend the Loop Transformer architecture to simulate hypergraph algorithms efficiently, addressing the gap between neural networks and combinatorial optimization over hypergraphs. Specifically, we propose a novel degradation mechanism for reducing hypergraphs to graph representations, enabling the simulation of graph-based algorithms, such as Dijkstra's shortest path. Furthermore, we introduce a hyperedge-aware encoding scheme to simulate hypergraph-specific algorithms, exemplified by Helly's algorithm. The paper establishes theoretical guarantees for these simulations, demonstrating the feasibility of processing high-dimensional and combinatorial data using Loop Transformers. This work highlights the potential of Transformers as general-purpose algorithmic solvers for structured data.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) built on the Transformer architecture [VSP+17], such as GPT-40 [Ope24a], Claude [Ant24], and OpenAI's latest o1 and o3 [Ope24b], demonstrate the remarkable potential to improve our daily lives. They are transforming domains like AI agents [CYL+24], search AI [Ope24b], AI assistants [MWY+23, ZKAW23], and conversational AI [LCT+24]. As their capabilities expand, LLMs serve as powerful instruments driving the future of technological innovation. A key emergent capability of LLMs is their adeptness at executing traditional algorithms, showcasing exceptional performance in tasks such as sorting large datasets and solving intricate problems. LLMs have shown particular aptitude in processing diverse data structures and types, including graph-structured data [YJK+19], tabular formats [YGU+22], and temporal sequences [ZZP+21]. Notably, the Loop Transformer architecture has demonstrated robust performance in simulating various graph algorithms [dLF24], achieving comparable results to traditional graph processing methods. These developments not only advance our theoretical understanding of Transformer-based models' computational capabilities but also offer practical alternatives for complex algorithmic tasks. This success raises a compelling question:\nCan Loop Transformers extend their algorithmic simulation capabilities to the more complex domain like hypergraphs?\nHypergraphs, which extend traditional graphs by permitting edges, called hyperedges, to connect multiple nodes, offer a more natural representation of many real-world datasets and problems. This added complexity enables hypergraphs to capture higher-order relationships between entities, making them particularly useful in a variety of domains where interactions are not limited to pairwise connections. For instance, in combinatorial optimization, hypergraphs are used to model complex systems such as resource allocation [ZSH16] and network flow [YPP21], where multiple entities interact simultaneously in a single operation. Similarly, in recommendation systems [YHX+22, XYY+21], hypergraphs can represent user-item interactions more flexibly, enabling the modeling of complex, multi-way relationships, such as collaborative filtering or group-based recommendations. In computational biology, hypergraphs are employed to capture intricate biological relationships, such as protein-protein interaction networks or gene expression data [THK09], where multiple factors simultaneously contribute to the system's behavior.\nHypergraph algorithms, however, often pose greater computational challenges than graph algorithms due to their richer structure and exponential growth in possible hyperedge combinations. Bridging the gap between hypergraph algorithms and neural network simulation could unlock new paradigms for solving high-dimensional and combinatorially complex problems."}, {"title": "1.1 Our Contribution", "content": "In this paper, we extend the results of [dLF24] that demonstrated the ability of Loop Transformers to simulate graph algorithms. We propose and investigate the hypothesis that Loop Transformers, with their iterative processing and representational flexibility, are capable of simulating hypergraph algorithms. By leveraging the inherent capacity of Transformers for parallel processing and multi-head attention, we aim to show that these models can effectively encode and operate over hypergraph structures. In detail, we show that the Loop Transformer can simulate a degradation mechanism (see detail in Section 6) by using 10 layers, where each attention layer includes three attention heads with constant feature dimensions. Also, the Loop Transformer can represent the Helly algorithm (see detail in Section 6) by using 11 attention layers, where each layer includes three attention heads with constant feature dimensions. These findings demonstrate that the"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Neural Network Execute Algorithms", "content": "In recent years, the integration of neural networks with algorithmic execution has gained significant attention, leading to the emergence of a body of work aimed at improving the efficiency and performance of algorithmic tasks through deep learning techniques. This line of research focuses on leveraging the power of neural networks to either approximate traditional algorithms or directly replace them with learned models. [SS92] established the computational universality of finite recurrent neural networks with sigmoidal activation functions by proving their capability to simulate any Turing Machine. Later, [PBM21] proved that Transformer architectures achieve Turing completeness through hard attention mechanisms, which enable effective computation and access to internal dense representations. Building with the framework of looped transformer, [GRS+23] uses looped transformers as the building block to make a programmable computer, showcasing the latent capabilities of Transformer-based neural networks. [dLF24] demonstrated that looped transformers can implement various graph algorithms, including Dijkstra's shortest path, Breadth-First Search (BFS), Depth-First Search (DFS), and Kosaraju's algorithm for strongly connected components, through their multi-head attention mechanism. [GRS+23, YLNP23] uses Transformers as the building block to build a programmable computer, showcasing the latent capabilities of Transformer-based neural networks. Beyond [GRS+23], [LSS+24] proves that a looped 23-layer ReLU \u2013 MLP is capable of performing the basic necessary operation of a programmable computer. [GSR+24, CLL+24b] showed that looped transformers can efficiently do in-context learning by multi-step gradient descent."}, {"title": "2.2 Hypergraphs in Neural Networks", "content": "Hypergraphs have recently emerged as a powerful mathematical model for representing complex relationships in data, and they have found promising applications in deep learning. One of the key motivations for using hypergraphs in neural networks is their ability to capture higher-order interactions among entities, which can be more representative of real-world data than traditional pairwise relationships. Several works have explored leveraging hypergraph-based representations to improve the performance of neural architectures in various domains. For instance, [FYZ+19]"}, {"title": "3 Preliminaries", "content": "In Section 3.1, we introduce the fundamental notations used throughout this work. Section 3.2 outlines the concept of simulation, while Section 3.3 provides an overview of essential concepts related to hypergraphs. Lastly, Section 3.4 describes the architecture of the looped transformer."}, {"title": "3.1 Notation", "content": "We represent the set {1,2,...,n} as [n]. For a matrix $A \\in R^{m\\times n}$, the i-th row is denoted by $A_{i,\\cdot} \\in R^n$, and the j-th column is represented as $A_{\\cdot,j} \\in R^m$, where $i \\in [m]$ and $j \\in [n]$. For $A \\in R^{m\\times n}$, the j-th entry of the i-th row $A_{i,\\cdot} \\in R^n$ is denoted by $A_{i,j} \\in R$. The identity matrix of size $d\\times d$ is denoted by $I_d \\in R^{d\\times d}$. The vector $0_n$ denotes a length-n vector with all entries equal to zero, while $1_n$ denotes a length-n vector with all entries equal to one. The matrix $0_{n\\times d}$ represents an $n \\times d$ matrix where all entries are zero. The inner product of two vectors a, b \u2208 $R^d$ is expressed as $a\\cdot b$, where $a\\cdot b = \\sum_{i=1}^d a_i b_i$"}, {"title": "3.2 Simulation", "content": "Definition 3.1 (Simulation, Definition 3.1 on page 3 in [dLF24]). We define the following:\n\u2022 Let $h_F: X \\rightarrow Y$ be the function that we want to simulate.\n\u2022 Let $h_\\gamma: X' \\rightarrow Y'$ be a neural network.\n\u2022 Let $g_e: X \\rightarrow X'$ be an encoding function.\n\u2022 Let $g_d: Y' \\rightarrow Y$ be a decoding function."}, {"title": "3.3 Hypergraph", "content": "A weighted hypergraph is expressed as $H := (V, E, w)$, where w is a function assigning a real-valued weight to each hyperedge. The total number of vertices in the hypergraph is $n_v := |V|$, and the total number of hyperedges is $n_e := |E|$. The vertices are assumed to be labeled sequentially from 1 to $n_v$, and the hyperedges are labeled from 1 to $n_e$.\nWe define the incident matrix of the weighted hypergraph as follows:\nDefinition 3.2 (Incident matrix of hypergraph). Let $H = (V, E,w)$ be a weighted hypergraph, where $V = \\{v_1, v_2,..., v_{n_v} \\}$ is the set of vertices and $E = \\{e_1, e_2,...,e_{n_e}\\}$ is the set of hyperedges, with $n_v = |V|$ and $n_e = |E|$. Each hyperedge $e_j \\in E$ is associated with a weight $w(e_j) \\in R_+$ and consists of a subset of vertices from V. The incident matrix $A \\in R^{n_v\\times n_e}$ of H is defined such that the rows correspond to vertices and the columns correspond to hyperedges. For each vertex $v_i \\in V$ and hyperedge $e_j \\in E$, the entry $A_{ij}$ is given by\n$A_{i,j}=\\begin{cases}\nw(e_j) & \\text{if } v_i \\in e_j,\\\\\n0 & \\text{otherwise.}\n\\end{cases}$\nIn this paper, we use $X \\in R^{K\\times d}$ to represent the input matrix, where d is the feature dimension and $K = \\max\\{n_v, n_e\\} + 1$ is the row number we need for simulation. To match the dimension of X and incident matrix, we use a padded version of A, which is defined as its original entries of A preserved in the bottom-right block:\nDefinition 3.3 (padded version of incident matrix). Let incident matrix of hypergraph $A \\in R^{n_v,n_e}$ be defined in Definition 3.2, let $K \\in \\mathbb{N}$ safisfies $K > \\max\\{n_v, n_e\\} + 1$ is the row number of X, we define the padded version of incident matrix A as:\n\u2022 Part 1. If $n_e > n_v$, we define:\n$\\tilde{A} := \\begin{bmatrix} 0 & 0_{n_v} \\\\ 0_{n_e} & A \\\\ 0_{K-n_v-1} & 0_{(K-n_v-1)\\times n_e} \\end{bmatrix}$\n\u2022 Part 2. If $n_e < n_v$, we define:\n$\\tilde{A} := \\begin{bmatrix} 0 & 0_{n_e} \\\\ 0_{n_v} & A \\\\ 0_{n_v\\times (K-n_e-1)} & 0_{(K-n_e-1)} \\end{bmatrix}$\n\u2022 Part 3. If $n_e = n_v$, we define:\n$\\tilde{A} := \\begin{bmatrix} 0 & 0_{n_e} \\\\ 0_{K-n_e-1} & A \\end{bmatrix}$"}, {"title": "3.4 Looped Transformers", "content": "Following the setting of [dLF24], we use standard transformer layer [VSP+17] with an additional attention mechanism that incorporates the incident matrix. The additional attention mechanism is defined as follows:\nDefinition 3.4 (Single-head attention, [dLF24]). Let $W_Q, W_K \\in R^{d\\times d_a}$ be the weight matrices of query and key, $W_V \\in R^{d\\times d}$ be the weight matrix of value, and $\\sigma$ be the hardmax\u00b9 function. We define the single-head attention $\\psi^{(i)}$ as\n$\\psi^{(i)}(X, \\tilde{A}) := \\tilde{A} \\sigma (XW_Q^{(i)}W_K^{(i)T}X^T) XW_V^{(i)}.$\nRemark 3.5. In this paper, we set $d_a = 2$.\nThe $\\psi$ function is an essential construction in the definition of multi-head attention:\nDefinition 3.6 (Multi-head attention, [dLF24]). Let $\\psi$ be defined in Definition 3.4, let A be defined in Definition 3.3. We define the multi-head attention $\\psi^{(i)}$ as\n$f_{attn}(X, A) := \\sum_{i\\in M_A}\\psi^{(i)}(X, A) + \\sum_{i\\in M_{AT}}\\psi^{(i)}(X, A^T) + \\sum_{i \\in M}\\psi^{(i)}(X, I_{n+1}) + X,$\nwhere $M_A, M_{AT}, M$ are the index set of the attention incorporated the incident matrix, and the attention incorporated the transpose of the incident matrix, and attention heads for the standard attention which is defined in [VSP+17].\nRemark 3.7. The total number of attention heads is $|M|+ |M_A|+|M_{AT}|$.\nFor the MLP (Multilayer Perceptron) layer, we have the following definition.\nDefinition 3.8 (MLP layer, [dLF24]). Let $\\phi$ be the ReLU function, let $W \\in R^{d\\times d}$ be the weight of the MLP, where d is the feature dimension of X, let m be the number of layers. For $j = [m]$, we define the MLP layer as\n$f_{mlp}(X) := Z^{(m)}W^{(m)} + X,$\nwhere $Z^{(i+1)} := \\phi(Z^{(i)}W^{(i)})$ and $Z^{(1)} := X$.\nRemark 3.9. In the construction of this paper, we set the number of layers m = 4.\nRemark 3.10. In the construction of this paper, we set $X \\in R^{K\\times d}$, where Kis defined in Definition 3.3, d is a constant independent of K. The matrix X stores different variables in its different columns. We use $B_{global}$ to present the column that only the top one scalar is 1 while the rest are 0. We use $B_{local}$ to present the column that only the top one scalar is 0 while the rest are 1. We use P to present the 2 columns of position embedding, where the first column is $\\sin(\\theta i)$, and the second column is $\\cos(\\theta i)$ for $i \\in [K \u2013 1]$. We use $P_{cur}$ to present the position embedding of the current vertex or hyperedge.\nCombine the definition of multi-head attention and MLP layer, and we show the definition of the transformer layer:"}, {"title": "3.11 Transformer layer, [dLF24]", "content": "Let $\\tilde{A}$ be defined as Definition 3.3, let $f_{attn}$ be defined in Definition 3.6, let $f_{mlp}$ be defined in Definition 3.8. We define the transformer layer as\n$f(X, \\tilde{A}) := f_{mlp}(f_{attn}(X, \\tilde{A})).$\nDefinition 3.12 (Multi layer Transformer, [dLF24]). Let $\\tilde{A}$ be defined in Definition 3.3, let m = O(1) denote the layer of transformers. Let the transformer layer f be defined in 3.11. We define the multi-layer transformer as\n$h_T(X, \\tilde{A}) := f_m \\circ f_{m-1}\\circ\\dots \\circ f_1(X, \\tilde{A}).$"}, {"title": "3.13 Positional Encoding", "content": "Let $\\delta$ be the minimum increment angle, and let $\\hat{\\delta}$ be its nearest representable approximation. Define the rotation matrix $R_{\\hat{\\delta}} \\in R^{2\\times 2}$ as\n$R_{\\hat{\\delta}} = \\begin{bmatrix} \\cos \\hat{\\delta} & -\\sin \\hat{\\delta} \\\\ \\sin \\hat{\\delta} & \\cos \\hat{\\delta} \\end{bmatrix}.$\nInitialize the positional encoding with $p_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\in R^2$. For each node $i \\geq 1$, the positional encoding $p_i \\in R^2$ is defined recursively by\n$p_i = R p_{i-1}$.\nThe positional encoding for node i is represented as the tuple $(p_i^{(1)}, p_i^{(2)})$, where $p_i^{(1)}$ and $p_i^{(2)}$ are the first and second components of the vector $p_i$, respectively.\nAdditionally, the maximum number of distinct nodes that can be uniquely encoded is bounded by\n$N_{max} = \\frac{2\\pi}{\\delta},$\nwhich is determined by the precision of $\\delta$."}, {"title": "4 Key Operation Implementation", "content": "In Section 4.1, the operation of selection is discussed. In Section 4.2, the operation of increment is discussed. In Section 4.3, the operation of reading scalar from column is discussed. In Section 4.4, the operation of comparison is discussed. In Section 4.5, the operation of writing scalar to column is discussed. In Section 4.6, the operation of termination is discussed. In Section 4.7, the operation of reading from incident matrix is discussed. In Section 4.8, the operation of AND is discussed. In Section 4.9, the operation of repeat AND is discussed. In Section 4.10, the operation of repeat addition is discussed."}, {"title": "4.1 Selection", "content": "Lemma 4.1 (Selection). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let $V_0, V_1$ be the index for clause values, where $X[i, V_0], X[i, V_1] \u2208 [\u2212\u03a9, \u03a9]$ for $i \u2208 [K]$.\n\u2022 Let C be the index for conditions, where $X[i, C] \u2208 \\{0,1\\}$ for $i \u2208 [K]$.\n\u2022 Let E be the index of the target field.\nThen we can show that a single-layer transformer can simulate a selection operation, which achieves the following: if $X[i,C] = 1$, the value $X[i, V_1]$ is written to $X[i, E]$; otherwise, the value $X[i, V_0]$ is written to $X[i, E]$, for all $i \u2208 [K]$.\nProof. Because only the MLP layer is needed, we set all the parameters in the attention layer to 0 while keeping the residual connection. Let $S_1, S_2$ be the index for the scratchpad. We construct the weights in MLP as follows:\n$(W^{(1)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(V_0, V_0), (V_1, V_1), (C, C), (E, E), (B_{global}, B_{global}), (B_{local}, B_{local})\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(2)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(E, E), (V_0, S_1), (V_1, S_2)\\}; \\\\\n-\u03a9 & \\text{ if } (a, b) \u2208 \\{(C, S_1), (B_{global}, S_2), (B_{local}, S_2)\\}; \\\\\n\u03a9 & \\text{ if } (a, b) = (C, S_2); \\\\\n0 & \\text{ otherwise, } \\end{cases}$"}, {"title": "4.2 Increment", "content": "Lemma 4.2 (Increment). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let $\\delta$ be the nearest representable approximation of the minimum increment angle in position encoding defined in Definition 3.13.\n\u2022 Let $C_1, C_2$ be the index for source position embedding, D be the index for target position embedding.\nThen we can show that a single-layer transformer can simulate an increment operation, which achieves $X[1, D_1] \u2190 sin(arcsin (X[1, C_1]) + \u03b4), X[1, D_2] \u2190 cos(arccos (X[1, C_2]) + \u03b4)$.\nProof. Because only the MLP layer is needed, we set all the parameters in the attention layer to 0 while keeping the residual connection. Let $S_1, S_2$ be the index for the scratchpad. We construct $W^{(1)}$ corresponding to the rotation matrix, which is defined in Definition 3.13. $W^{(2,3)}$ is defined as identity operator, and $W^{(4)}$ is defined to erase the previous value in $X[:, D]$.\n$(W^{(1)})_{a,b} = \\begin{cases} \\cos(\\delta) & \\text{if } (a, b) \u2208 \\{(C_1, S_1), (C_2, S_2)\\}; \\\\\n-\\sin(\\delta) & \\text{if } (a, b) = (C_1, S_2); \\\\\n\\sin(\\delta) & \\text{if } (a, b) = (C_2, S_1); \\\\\n1 & \\text{if } (a, b) \u2208 \\{(D_1, D_1), (D_2, D_2)\\}; \\\\\n0 & \\text{otherwise,} \\end{cases}$\n$(W^{(2,3)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(D_1, D_1), (D_2, D_2), (S_1, S_1), (S_2, S_2)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(4)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(S_1, D_1), (S_2, D_2)\\}; \\\\\n-1 & \\text{ if } (a, b) \u2208 \\{(D_1, D_1), (D_2, D_2)\\}; \\\\\n0 & \\text{ otherwise. } \\end{cases}$"}, {"title": "4.3 Read Scalar From Column", "content": "Lemma 4.3 (Read scalar from column). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let C be the index for source row, D be the index for source column."}, {"title": "4.4 Comparison", "content": "Lemma 4.4 (Comparison). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let C, D be the index for the column for comparing.\n\u2022 Let E be the index for the target column to write the comparison result.\nThen, we can show that a single-layer transformer can simulate a comparison operation, which achieves writing X[:, E] \u2190 X[:, C] < X[:, D]."}, {"title": "4.5 Write Scalar to Column", "content": "Lemma 4.5 (Write scalar to column). If the following conditions hold:"}, {"title": "4.6 Termination", "content": "Lemma 4.6 (Termination). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let P be the index for the position embeddings.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let C be the index for the executed variable.\n\u2022 Let E be the index for the target column.\nThen we can show that a single-layer transformer can simulate the operation of writing a value to a column, i.e. X[1, E] \u2190 (no zero in X[2 :, C]).\nProof. We construct the first attention layer to erase the previous value in column E:\n$(W^{(1)}_K, W^{(1)}_Q)_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(P_1, 1), (P_2, 2), (B_{global}, 2)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(1)}_V)_{a,b} = \\begin{cases} -1 & \\text{ if } (a, b) = (E, E); \\\\\n1 & \\text{ if } (a, b) = (B_{global}, E); \\\\\n0 & \\text{ otherwise, } \\end{cases}$\nwhere $W^{(1)}_K$ and $W^{(1)}_Q$ are constructed as identity matrix, $W^{(1)}_V$ is constructed to replace the previous value in column E by 1, which will be used in the following construction.\nIn the second attention head, we want to construct an attention matrix that can extract information from all the entries of X [2 :, C]:\n$(W^{(2)}_K)_{a,b} = \\begin{cases} -1 & \\text{ if } (a, b) \u2208 \\{(B_{global}, 1), (B_{global}, 2)\\}; \\\\\n1 & \\text{ if } (a, b) \u2208 \\{(B_{local}, 1), (B_{local}, 2)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$"}, {"title": "4.7 Read from Incident Matrix", "content": "Lemma 4.7 (Read from incident matrix). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let P be the index for the position embeddings.\n\u2022 Let C be the index for the source row/column index.\n\u2022 Let D be the index for the target column.\n\u2022 Let $P_{cur}$ be the index for the position embedding of row/column C.\nThen we can show that:\n\u2022 Part 1. A single-layer transformer can simulate the operation of reading a row from A, i.e. X[:, D] \u2190 A[C,:]."}, {"title": "4.8 AND", "content": "Lemma 4.8 (AND). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let C and D be the index for the executed variable.\n\u2022 Let E be the index for the target column.\nThen we can show that a single-layer transformer can simulate the operation of AND, i.e. X[1, E] \u2190 X[1, C] \u2227 X[1, D].\nProof. In this construction, we want to have $(X[1, C] + X[1, D] \u2212 1)$, so that if and only if X[1, C] = 1 and X[1, D] = 1, we have $(X[1, C] + X[1, D] \u2212 1) = 1, otherwise $(X[1, C] + X[1, D] \u2212 1) = 0$. Because only the MLP layer is needed, we set all the parameters in the attention layer to 0 while keeping the residual connection. We construct MLP layers as follows:\n$(W^{(1)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(E, E), (C, S), (D, S)\\}; \\\\\n-1 & \\text{ if } (a, b) = (B_{global}, S'); \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(2,3)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(E, E), (S, S)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(4)})_{a,b} = \\begin{cases} -1 & \\text{ if } (a, b) = (E, E); \\\\\n1 & \\text{ if } (a, b) = (S, E); \\\\\n0 & \\text{ otherwise, } \\end{cases}$"}, {"title": "4.9 Repeat AND", "content": "Lemma 4.9 (Repeat AND). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let C be the index for a Boolean value.\n\u2022 Let D and E be the index for two columns, where each entry is a Boolean value.\nThen we can show that a single-layer transformer can simulate the operation of repeat AND, i.e., X[i, D] \u2190 X[1, C] \u2227 X[i, D] > \u00abX[i, E] for $i \u2208 \\{2,3,\\dots, K\\}$.\nProof. In this construction, we only use MLP layers. For multi-head attention layers, we just make it as the residential connection by setting all parameters to 0.\n$(W^{(1)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(D, D), (C, D), (D, S_1)\\}; \\\\\n-1 & \\text{ if } (a, b) \u2208 \\{(E, D), (B_{global}, D)(B_{local}, D)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(2,3)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) \u2208 \\{(D, D), (S_1, S_1)\\}; \\\\\n0 & \\text{ otherwise, } \\end{cases}$\n$(W^{(4)})_{a,b} = \\begin{cases} 1 & \\text{ if } (a, b) = (D, D); \\\\\n-1 & \\text{ if } (a, b) = (S_1, D); \\\\\n0 & \\text{ otherwise. } \\end{cases}$"}, {"title": "4.10 Repeat Addition", "content": "Lemma 4.10 (Repeat addition). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\n\u2022 Let \u03a9 represent the maximum absolute value within a clause.\n\u2022 Let P be the index for the position embeddings.\n\u2022 Let C be the index for a scalar.\n\u2022 Let D be the index for a column.\nThen we can show that a single-layer transformer can simulate the operation of repeat addition, \u0456.\u0435. X[:, D] \u2190 1K \u2022 X[1, C] + X[:, D]."}, {"title": "5 Simulation", "content": "We present the simulation result of visiting hyperedges iteratively in Section 5.1. We discuss the result of getting a minimum value in Section 5.2. We discuss the simulation result of Dijkstra's Algorithm in Section 5.3."}, {"title": "5.1 Iteration of Visiting Hyperedges", "content": "Lemma 5.1 (Visiting hyperedges iteratively). A looped transformer hy exists, where each layer is defined as in Definition 3.11, consisting of 10 layers where each layer includes 3 attention heads with feature dimension of O(1). This transformer simulates the operation of iteratively visiting hyperedges for weighted hypergraphs, accommodating up to O($\\delta^{-1}$) vertices and O($\\delta^{-1}$) hyperedges.\nProof. Let the operation of visiting hyperedges iteratively be defined as described in Algorithm 3. This operation requires 1 increment operation, which requires single layer transformer to construct the following from Lemma 4.2, 2 compare operations, which require 3 layers transformer to construct the following from Lemma 4.4, 3 selection operations, which require 3 layers transformer to construct following from Lemma 4.1, 1 AND operation which require single layer transformer to construct following from Lemma 4.8, 1 read-from-incident-matrix operation which requires single layer transformer to construct following from Lemma 4.7, 1 write-scalar-to-column operation which requires single layer transformer to construct following from Lemma 4.5, and 1 trigger-termination"}, {"title": "5.2 Iteration of Minimum Value", "content": "Lemma 5.2 (Get minimum value). If the following conditions hold:\n\u2022 Let the transformer be defined as Definition 3.11.\nThen, we can show that a 7-layer transformer can simulate the operation of getting the minimum value in Algorithm 2.\nProof. Following from Lemma 4.1, Lemma 4.2, Lemma 4.3, Lemma 4.4, Lemma 4.5, and Lemma 4.6, we can perform all the operation of Algorithm 2, thus we finish the proof."}, {"title": "5.3 Dijkstra's Algorithm", "content": "Theorem 5.3 (Dijkstra's Algorithm on"}]}