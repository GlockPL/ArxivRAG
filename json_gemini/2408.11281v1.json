{"title": "BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation", "authors": ["Haotian Peng", "Jiawei Liu", "Jinsong Du", "Jie Gao", "Wei Wang"], "abstract": "We propose a bearing health management framework leveraging large language models (BearLLM), a novel multimodal model that unifies multiple bearing-related tasks by processing user prompts and vibration signals. Specifically, we introduce a prior knowledge-enhanced unified vibration signal representation to handle various working conditions across multiple datasets. This involves adaptively sampling the vibration signals based on the sampling rate of the sensor, incorporating the frequency domain to unify input dimensions, and using a fault-free reference signal as an auxiliary input. To extract features from vibration signals, we first train a fault classification network, then convert and align the extracted features into word embedding, and finally concatenate these with text embedding as input to an LLM. To evaluate the performance of the proposed method, we constructed the first large-scale multimodal bearing health management (MBHM) dataset, including paired vibration signals and textual descriptions. With our unified vibration signal representation, BearLLM using one set of pre-trained weights achieves state-of-the-art performance on nine publicly available fault diagnosis benchmarks, outperforming specific methods designed for individual datasets. We provide a dataset, our model, and code to inspire future research on building more capable industrial multimodal models (https://github.com/hatton613/BearLLM).", "sections": [{"title": "1 Introduction", "content": "Bearings are the core components of mechanical rotating equipment but have high failure rates due to complex operational and environmental conditions [40]. Bearing health management (e.g., anomaly detection, fault diagnosis, and maintenance recommendations) is of great practical significance in industrial safety production to reduce economic losses and maintenance costs [32, 44, 35].\nCurrent bearing health management frameworks rely on designing specialized methods for different working conditions and tasks, as shown in Fig. 1 (a). To apply specific methods to complex real-world industrial scenarios, domain adaptation, and generalization have attracted widespread attention. Domain adaptation enables a model trained on one source domain to perform well on different but related target domains by reducing the domain shift or discrepancy [43, 46], but it suffers from low accuracy when the source and target domains are category-inconsistent (e.g., transitioning from working condition C\u2081 with four fault types to C2 with five types). Domain generalization aims to extract domain-invariant features to improve performance on unseen domain [19, 47, 4], but it is often constrained to a limited number of working conditions with small differences, e.g., fewer than ten working conditions in [5, 22]. These purely data-driven methods often fail to strike an optimal balance between high accuracy and strong generalization for fault diagnosis.\nIn this paper, we propose a prior knowledge-enhanced bearing large language model (BearLLM), which can unify multiple bearing health management tasks over hundreds of different working conditions from multiple datasets, as shown in Fig. 1 (b). To handle various working conditions, we introduce a prior knowledge-enhanced unified vibration signal representation. Unlike most fault diagnosis methods that use fixed-length input segments, we sample vibration signals as variable-length but fixed-duration segments. These duration-consistent segments are then converted to the frequency domain and are aligned. We further utilize a fault-free reference signal as a prior input, eliminating the need for complex mechanism analysis for various bearing designations [47].\nSpecifically, we first design a fault classification network (FCN) to extract fault features based on the differ-"}, {"title": "2 Related Works", "content": "Multiple Working Condition: Fault diagnosis under various working conditions from multiple datasets presents a challenge due to the heterogeneity of collected signals arising from variations in the test rigs, sensors, and environment, making it difficult to obtain unified features [41]. Existing domain adaptation methods [6, 38, 25, 14] typically involves training a model under known working conditions (source domain) and subsequently transferring knowledge to an unknown working condition (target domain). However, these approaches still necessitate individual transfer fine-tuning for each working condition in practice, hindering its ability to generalize across multiple scenarios. Domain generalization methods leverage training on multiple working conditions and aim to align the feature distributions of different domains through the design of network architectures and loss functions [15, 48, 12]. However, these approaches often rely on complex data preprocessing and augmentation techniques to help models learn fault features from vibration signals.\nMultiple Tasks: Data-driven machinery health management have gained significant traction [37]. The concept of health management usually involves multiple tasks [29, 49], including anomaly detection, fault diagnosis, degradation prediction, maintenance decision-making, etc. LLMs such as ChatGPT-4 [30] have demonstrated exceptional capabilities across a wide range of tasks. The emergence of open-source foundational models like LLaMA 3 [27] and Qwen 2 [1] have further empowered researchers in various disciplines to integrate these models into their own applications. In the aviation domain, Liu et al. [23] applied generalized linear models to achieve multiple tasks, including assembly guidance and assembly error identification for aircraft engines. In the petroleum industry, Eckroth et al. [9] designed a question-answering system based on LLM and knowledge graph, enabling retrieval of functionalities such as stratigraphy data and geological age determination. However, research integrating multiple tasks using LLMs for bearing health management remains limited [20]."}, {"title": "3 A Multimodal Bearing Health Management Dataset", "content": "Although several bearing-related datasets in Tab. 1 are available, they generally collect vibration signals on a single test rig, have a limited number of working conditions, and have no corresponding textual descriptions for training LLM. We have constructed a large-scale publicly multimodal dataset for bearing health management (MBHM).\nThe MBHM contains 135,516 pairs of vibration signal segments and fault types, and 542,064 pairs of text cues and responses, of which each sample is shown in Fig. 3, contains a vibration signal, a fault label, an operating condition id, a user prompt, and a text response, ie, $(X_v, L_v, C, X_t, L_t) \\in \\text{MBHM}$. Our dataset contains 262 working conditions collected from nine publicly accessible datasets, i.e., CWRU [2], DIRG [7], HIT [11], IMS [33], JNU [16], JUST [34], MFPT [10], PU [18], XJTU [39]. For each vibration signal, we have four different tasks, i.e., anomaly detection, fault diagnosis, maintenance recommendations, and potential risk analysis by generating text responses using ChatGPT [30]. Detailed methodologies for dataset construction are provided in Appendix A.3. Our MBHM dataset contains the following features:\n\u2022 Multi-modal: Each vibration signal is paired with four"}, {"title": "4 Method", "content": "In this section, we propose BearLLM, a novel multimodal model that unifies multiple bearing-related tasks. To handle various working conditions across multiple datasets, we introduce a prior knowledge-enhanced unified vibration signal representation in Section 4.1. The unified vibration signal is fed to a fault classification network to extract features in Section 4.2. We convert and align the extracted features into word embedding, and finally concatenate these with text embedding as input to an LLM in Section 4.3.\n4.1 Prior Knowledge-Enhanced Unified\nVibration Signal Representation\nBearLLM aims to manage multiple bearing-related tasks across hundreds of working conditions. The basis for this is to build a unified vibration signal representation, involving adaptively sampling the vibration signal segments based on the sensor sampling rate, incorporating the frequency domain to unify input dimensions, and using a fault-free reference signal to calculate residual as auxiliary inputs to improve data utilization efficiency.\nAdaptive Sampling To monitor various mechanical devices across different working conditions and industrial scenarios, vibration sensors are deployed with varying designations and sampling rates. However, most fault diagnosis methods [48, 8] use fixed-length signal segments in the time domain as inputs, where the fault frequency components in the inputs deviate from their original intrinsic values and vary with the sampling rate, hindering accurate fault diagnosis. Instead of sampling fixed-length signal segments, we adaptively sample vibration signals as variable-length but fixed-duration segments using prior knowledge of the sensor sampling rate. We extract the $m$-th query signal segment $X \\in \\mathbb{R}^{1 \\times s}$ from the original signal $X_o$ by\n$X = X_o[ms, (m + 1)s],$   (1)\nwhere $s$ denotes the sampling rate of the sensor and controls the length of the $X$.\nFrequency-domain Input Alignment After adaptive sampling, each query segment ($X$) has an equal duration, and the frequencies of $X$ are aligned. However, varying lengths of $X$ (due to different sampling rates) result in different numbers of frequency components, making them unsuitable for input to the network. We design a discrete cosine normalization (DCN) that consists of converting the vibration signal to the frequency domain using the discrete cosine transform (DCT), unifying the number $n_f$ of frequency components using a pad or cut, and standardizing the amplitude using the normalization $\\mathcal{N}$. The normalized frequency representation $F_v \\in \\mathbb{R}^{1 \\times n_f}$ is obtained by\n$F =\\begin{cases}\\mathcal{N}(\\text{DCT}(X) [0, n_f]), & \\text{if } s \\ge n_f\\\\\\mathcal{N}(\\text{DCT}(X) \\bigcup [0]_{n_f-s}), & \\text{if } s < n_f\\end{cases}$   (2)\nSignals with sampling rates below $n_f$ are zero-padded, while those exceeding $n_f$ are cut. To balance computational resources and fault classification accuracy, we empirically set $n_f = 24000$ (more detail in Tab. 3). To enhance training stability, the amplitude of the frequency sequence is normalized to [-1, 1],\n$\\mathcal{N}(x) = \\beta \\frac{x}{\\|x\\|_2},$   (3)\nwhere $\\beta$ is a scaling factor and is set to 0.01 by statistically analyzing our MBHM dataset.\nFault-free Reference Signal To eliminate distributional differences from different inputs under various operating conditions, we introduce fault-free signals as reference signals. 1) In practical use, the reference signal segment $X$ can be collected and saved when the equipment is working properly, such as after factory acceptance or maintenance; 2) in training on the MBHM dataset, $X$ is acquired by\n$X \\sim \\{ X(X, L^*, C^*, X, L^+) \\in \\text{MBHM}, \\atop L^* = 0, C^* = C' \\}.$   (4)\nThis indicates that $X$ is selected when a signal ($X$) of our MBHM dataset is fault-free (i.e., $L = 0$) and has the same working conditions as $X_v$ (i.e., $C^* = C'$).\nWe combine the query frequency signal ($F_v$), the fault-free frequency signal ($F_u$), and the residual frequency signal ($F_{\\text{res}} = F_v - F_u$) as unified vibration signal representation,\n$R_v = [F_v, F_u, F_{\\text{res}}].$   (5)\n4.2 Feature extraction\nAlgorithm 1: Training Algorithm\nInput: $\\theta_e, \\theta_c, \\theta_A, \\theta_L$ the weights of feature encoder, linear classification layer, alignment layer, and LLM in our BearLLM; $X_v, L_v, X_t, L_t$ the vibration signal, fault label, prompt text, and response text from MBHM dataset\nOutput: The optimal parameters of BearLLM $\\theta_e, \\theta_A, \\theta_L$\nStep 1: Pre-training FCN\nfor $e \\leftarrow 1$ to 50 epoches do\n$R \\leftarrow$ get unified representation by Eq. 5\n$P = \\text{FCN}(R)$\n$\\theta_e, \\theta_c \\leftarrow \\theta_e, \\theta_c - \\nabla_{\\theta_e, \\theta_c} \\text{CE}(P, L_v)$  cross-entropy\nend for\nStep 2: Fine-tuning BearLLM\nInit. $\\theta_A$ by Eq. 7\nfor $e \\leftarrow 1$ to 20 epoches do\n$Y = \\text{BearLLM}(X_t, X_v)$\n$\\theta_A, \\theta_L \\leftarrow$ PEFT$(Y, L_t)$\nend for\nreturn $\\theta_e, \\theta_A, \\theta_L$\nTo extract the features of vibration signals, we propose a fault diagnosis network (FCN) containing a feature encoder parameterized by $\\theta_e$ and a linear classification layer parameterized by $\\theta_c$, as shown in Fig. 4. We extract features from the unified vibration signal representation ($R_v$) using three separate convolutional layers with large kernels [45] and no weight sharing. We then transform features by three multi-scale channel attention blocks (MSCAB) where the multi-scale features are fused using the channel attention module (CAM) [42]. Finally, we use two linear layers for fault classification.\nOur FCN takes unified representation ($R_v$) as input and outputs the fault type ($P$). The shape of $P$ is [1, $\\gamma$] and $\\gamma$"}, {"title": "4.3 Feature Alignment", "content": "denotes the number of fault types. We use cross-entropy loss for training with fault label $L_v$ as ground truth. The training procedure is described in Algo. 1. The well-trained feature encoder weights ($\\theta_e^*$) of FCN are then used and frozen in BearLLM (see Fig. 2), while the classifier weights ($\\theta_c^*$) of FCN are used to initialize the alignment layer.\nWe propose a feature alignment layer to embed vibration features into word embedding, which is an MLP consisting of three linear layers (i.e., $l_1, l_2, l_3$). The weights of alignment layer is $\\theta_A = [\\theta_e^*, \\theta_{l_3}]$, where $\\theta_e^*$ is the weights of $l_1\\&l_2$ (i.e., two linear classification layers in FCN) and $\\theta_{l_3}$ is the weights of $l_3$. We use $l_3$ transforms the output $P$ of $l_2$ into the word embedding $H_v = \\text{reshape}(l_3(P))$, i.e.,\n$P \\in \\mathbb{R}^{1\\times\\gamma} \\xrightarrow{l_3} \\mathbb{R}^{1\\times th} \\xrightarrow{\\text{reshape}} H_v \\in \\mathbb{R}^{\\tau \\times h},$   (6)\nwhere $\\tau$ signifies the token length after transformed, $h$ is the hidden size of the LLM.\nThe weight $\\theta_{l_3}$ of $l_3$ is initialized from the textual descriptions $K$ of all fault categories by\n$K \\in \\mathbb{T}^{\\gamma \\times 1} \\xrightarrow{\\mathcal{T}} \\mathbb{R}^{\\gamma \\times 1} \\xrightarrow{\\mathcal{E}} \\mathbb{R}^{\\gamma \\times \\tau \\times h} \\xrightarrow{\\text{reshape}} l_3 \\in \\mathbb{R}^{\\gamma \\times th},$   (7)\nwhere $\\mathbb{T}$ stands for the text domain. $\\mathcal{E}$ and $\\mathcal{T}$ indicate the embedding layer and tokenizer of the pre-trained LLM, respectively. Using a tokenizer $\\mathcal{T}$ and an embedding layer $\\mathcal{E}$, we generate a word embedding from $K$, which is then reshaped into the weight matrix $\\theta_{l_3}$. See Appendix C.3 for more details on initializing weights.\nWe use the pre-trained Qwen2-1.5B [1] as our LLM parameterized by $\\theta_L$, achieving basic human-computer interaction. However, its knowledge of specific domains and generation quality still requires improvement. We used the existing LoRA technique [13] and a general pipeline PEFT [26] for simultaneous fine-tuning of the LLM and our proposed alignment layer, which is detailed in Algo. 1."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nWe implemented the proposed method using PyTorch [31]. Both pre-training and fine-tuning are performed on a single Nvidia RTX 4090 GPU. For pre-training, comparison trials, and ablation experiments, we used AdamW [24] as the optimizer, and the batch size was set to 1024 for up to 50 epochs of training. Fine-tuning was performed using the existing PEFT [26] library.\nTo evaluate the effectiveness of our method, we provide quantitative comparison results for fault diagnosis, ablation of key components, and a user study to assess the quality of language responses. Other tasks including anomaly detection, maintenance recommendations, and potential risk analysis can be found in Appendix D.\n5.2 Comparison with Fault Diagnosis Methods\nWe compared BearLLM with the following fault diagnosis methods. BearFM [17] and MagNet [36] are intended for diagnosing faults under cross-working conditions, while WDCNN [45], TCNN [6], and QCNN [21] are aimed at handling specific working conditions. Detailed descriptions of these methods can be found in Appendix B. To ensure a fair comparison, we re-implement these methods and test them under the same setup in section 5.1. The results are displayed in Tab. 2.\nOur DCN achieves greater accuracy compared to BearingFM [17] when used with the same FCN (see Fig. 5 (a)). The reason for this enhancement is likely due to BearingFM using absolute values after the FFT of the envelope spectrum. This method captures only the amplitude and ignores crucial phase information. In contrast, DCN leverages real-number computations, which help to reduce potential information loss, and operates in less than 20% of the time required by the comparison method. Combining DCN with"}, {"title": "5.3 Ablation Experiments and Generalization", "content": "verges the fastest (within 20 epochs on the MBHM dataset as shown in Fig. 5 (c)).\nAblation studies were conducted to further validate the effectiveness of each component in our proposed method. We evaluated the performance by directly using raw time-domain vibration signals (fixed-length segments) as input and removing fault-free channels and residual channels separately and together.\nExperimental results in Tab. 4 demonstrate significant accuracy and generalization drops when using time-domain signals only, further highlighting the efficacy of DCN. Applying the t-SNE, we compared the visualization of output with and without fault-free and residual channels. The blue box in Fig. 6 (b) shows how signal segments from the same dataset cluster closely in the feature space. This indicates that the model first identifies the dataset type before refining fault classification. Conversely, our proposed method, as shown in Fig. 6 (a), reduces inter-dataset differences. The model targets the residual between the query signal segments and the fault-free signal segments, creating a unified feature representation across the varying working conditions, and improving the generalization.\nWe evaluate the generalization ability of our proposed method using zero-shot settings. Among the publicly available datasets employed, JUST [34] and IMS [33] are the"}, {"title": "5.4 User Study", "content": "largest. We trained on the MBHM(w/o JUST&IMS) dataset, comprising only 35% of the MBHM training data, and performed zero-shot tests on the JUST and IMS datasets separately. On the JUST dataset, our method achieves an accuracy of 90.22% without any fine-tuning. In contrast, the method without fault-free and residual channels achieves an accuracy of only 87.54%.\nFig. 7 (a,b) illustrates a comparison of confusion matrices for zero-shot testing on the IMS dataset [33], with and without fault-free and residual channels. Given that the IMS dataset is unbalanced (most samples are fault-free), the overall accuracy drops slightly from 98.52% to 97.81%. However, the method without two auxiliary channels tends to grossly underestimate the severity. For example, 61% of severe outer ring faults are classified as moderate, and 23% of moderate outer ring faults are identified as minor.\nThe CWRU [2] and XJTU [39] datasets are the only ones that include all ten types of faults. To confirm the potential to create a unified representation, we trained our model on the MBHM(w/o CWRU) and MBHM(w/o CWRU&XJTU) datasets, respectively. We then performed zero-shot testing on the commonly used CWRU dataset, with the results of the confusion matrices displayed in Fig. 7 (c,d). Our method achieves remarkable accuracies of 90.26% and 89.14% on the untrained CWRU dataset for each setting, respectively. This result is even better than some methods trained on CWRU, which shows the generalization of our unified representation method and does not depend on any specific complete dataset for training.\nTab. 5 summarizes the outcomes of four different tasks, with users choosing the best outputs from FCN, untuned BearLLM, and fine-tuned BearLLM in blind trials. Notably, in simpler tasks, few users chose the fault code output, while most preferred the natural language output. Fig. 8 illustrates examples of outputs before and after fine-tuning. Appendix D provides further comparisons for various tasks. Fine-tuning did not significantly affect the output of the simple anomaly detection task. In the fault diagnosis task, the model without fine-tuning sometimes missed information on fault severity, an issue that was resolved with fine-tuning. For the two more complex tasks, the fine-tuned model produced more accurate and detailed responses. Our method addresses the challenge faced by non-experts in utilizing maintenance systems due to their complexity, reducing the required level of expertise."}, {"title": "6 Conclusion", "content": "We propose BearLLM, a novel multimodal bearing health management framework that is the first attempt to unify multiple bearing-related tasks using LLMs, including anomaly detection, fault diagnosis, maintenance recommendations, and potential risk analysis. To build this unified framework, we introduce a prior knowledge-enhanced vibration signal representation for hundreds of different working conditions and construct the first large-scale multimodal bearing health management (MBHM) dataset. Experimental results on nine public fault diagnosis datasets show that BearLLM outperforms state-of-the-art methods, even surpassing those specifically trained on individual datasets. In addition, our frequency domain input alignment and feature extraction modules are plug-and-play, significantly improving the performance of other fault diagnosis models. We hope our work can inspire future research on building more capable industrial multimodal models."}, {"title": "7 Acknowledgments", "content": "This work was supported by National Natural Science Foundation of China under grant No. 62073312, Applied Basic Research Program of Liaoning Province (2023JH2/101300228, 2023JH2/101300143), Natural Science Foundation of Liaoning Province (2022-MS-033)."}, {"title": "Appendix", "content": "A. Construction of MBHM Dataset\nA.1. Vibration Signals We perform non-overlapping sampling at equal timing times on nine publicly available datasets, i.e., CWRU [2], DIRG [7], HIT [11], IMS [33], JNU [16], JUST [34], MFPT [10], PU [18], XJTU [39].\nThese datasets typically involve multiple vibration sensors performing simultaneous signal acquisition. These vibration signals reflect different time-domain characteristics (see Fig. 9) due to differences in sensor designations, mounting locations and orientations. We generalize the different sensors as part of the working conditions, i.e., the same working conditions represent the same sensors, speeds, and loads from the same dataset.\nAlgorithm 2: Obtaining vibration signals from publicly available datasets\nInput: $X_o, L_u, s, \\text{rpm}, \\text{load}, \\text{sensor}$ the raw vibration signals, fault labels, sampling rates, speeds, loads, sensor information from publicly available datasets; $D_{1-9}$ the nine publicly available datasets.\nOutput: MBHM(Vibration) dataset.\n$C_{\\text{list}} = []$  Initialize working conditions list\nfor $D_i$ in $D_{1-9}$ do\nfor $X_o, L_u, s, \\text{rpm}, \\text{load}, \\text{sensor}$ in $D_i$ do\n$C_{\\text{info}} = \\text{string}(\\text{rpm}, \\text{load}, \\text{sensor}, D_i)$\nif $C_{\\text{info}} \\notin C_{\\text{list}}$ then\n$C_{\\text{list}} \\insert C_{\\text{info}}$  new working condition\nend if\n$C = \\text{find\\_index}(C_{\\text{info}} \\text{ in } C_{\\text{list}})$\n$X = \\text{sample}(X_o, s)$ see Fig. 9\n$\\\\text{MBHM}(Vibration) \\insert X_u, L_u, C$\nend for\nend for\nreturn MBHM(Vibration)\nWe collected vibration signals $X_u$ and fault labels $L_u$ from these datasets as the vibration signal portion of the MHBM dataset (details in Algo. 2) while abstracting the specific working condition information $C_{\\text{info}}$ into condition index $C$ to facilitate quick indexing of reference vibration signals for the same working conditions.\nFigure 10 further illustrates the fault types and data sources for the MBHM dataset. In this case, 54% of the samples are fault-free. The three fault locations (i.e., inner ring, ball, and outer ring) are relatively balanced. Moderate failures accounted for the majority of the three failure levels (i.e., minor, moderate, and severe). More than half of all vibration signal data came from the IMS and JUST datasets.\nA.2. Generation of Text Responses For LLMs, the corpus consists of three parts, i.e., system prompts $X_{\\text{sys}}$, user prompts $X_t$, and responses $L_t$. The system prompts and user prompts are taken as inputs and response text is the output of LLM. For all samples, the same system prompt text $X_{\\text{sys}}$ is provided:\nAs an expert in bearing fault diagnosis\nwith extensive knowledge in mechanical\nengineering and failure analysis, you can\nassess the condition of bearings. Typically,\nbearing states are categorized as normal,\nouter ring fault, inner ring fault, and ball\nfault. These defects are further classified\ninto three levels: minor, moderate, and\nsevere. Based on your description of the\nbearing state, you will answer my questions\nconcisely and directly, providing only the\nanswer without reiterating the user's prompt\nor bearing status description.\nWe provide templates $X_t$ for user prompts $X_t$ for each of the four different types of tasks:\n\u2022 Anomaly Detection: Bearing status description: #placeholder%. Based on the bearing condition description, determine whether the bearing is in a faulty state. Answer yes or no.\n\u2022 Fault Diagnosis: Bearing status description: #placeholder%. Based on the bearing condition description, identify the type of bearing fault. Bearing conditions are classified as normal, outer ring fault, inner ring fault, and ball fault. All defects are categorized into three levels: minor, moderate, and severe.\n\u2022 Maintenance Recommendations: Bearing status"}, {"title": "B. Differences between Ours and Comparative Methods", "content": "description: #placeholder%. Based on the bearing condition description, report the current state of the bearing. If the bearing is in a faulty state, provide targeted maintenance recommendations based on the fault location and severity.\n\u2022 Potential Risk Analysis Bearing status description: #placeholder%. Based on the bearing condition description, assess the potential risks associated with the bearing condition. Identify the potential consequences of the bearing fault and recommend appropriate actions to prevent catastrophic failures.\nThe template $X_t$ is embellished and modified using LLMs to simulate different user inputs $X_t$ while maintaining the meaning and not modifying the placeholders. We use the leading ChatGPT [30] for response corpus generation. In the generation, #placeholder# is replaced with fault descriptions, according to fault labels $L_v$, e.g., moderate fault of bearing outer ring. We simulated four separate tasks for each sample in the MBHM(Vibration) dataset to generate the MBHM dataset, details of which are given in Algo. 3.\nAlgorithm 3: Algorithm for building the MBHM dataset\nInput: $X_u, L, C$ vibration signal, fault label, working condition from MBHM(Vibration) dataset, $T_{A-D}$ task types, $X_{\\text{sys}}$ system prompt, $X_t$ user prompt templates.\nOutput: MBHM dataset.\nfor $T$ in $T_{A-D}$ do\n$X_t = \\text{mod}(X_t T)$ simulate user inputs\n$L_t = \\text{ChatGPT}(X_{\\text{sys}}, X_t, L_v)$\nMBHM insert $X, L, C, X_t, L_t$\nend for\nreturn MBHM\nB. Differences between Ours and Comparative Methods\nB.1. Methods to Cope with Single Working Condition\nThe main existing fault diagnosis methods are designed for a single working condition only, and we select several representative methods for comparison. The WDCNN [45] is arguably the most popular diagnostic network, incorporating BatchNorm for fault diagnosis and demonstrating the effectiveness of using larger kernels in the first convolutional layer for improved accuracy. Due to its straightforward architecture, it enjoys widespread application in both practical scenarios and methodological comparisons. The TCNN [6] presents a potential enhancement by adding Dropout techniques and increasing the depth of the network to augment feature learning capabilities from raw data. The QCNN [21] introduced quadratic convolution to the fault diagnosis domain, improving diagnostic accuracy through enhanced non-linear representational ability within convolutional layers. In contrast to our methods, all of these methods utilize raw vibration signals as input."}, {"title": "B.2. MagNet with Data Augmentation", "content": "The MagNet [36] enhanced the mixup data augmentation method, transitioning from a Beta distribution (mixing two distributions) to a Dirichlet distribution (mixing multiple distributions). During training, in addition to a classification head, a discriminator was designed via adversarial training to render the obtained features difficult for correct source domain identification. This process compelled the feature extractor to learn common features across domains. The authors also introduced a self-adaptive screening weight strategy to mitigate the use of feature-deficient samples in the augmentation sample synthesis.\nSimilar to our method, this approach attempts to transform the vibration signal from multiple independent distributions into a smooth single distribution. However, our approach achieves alignment through simple spectral changes, whereas MagNet performs signal mixing in the time domain, which made it difficult to perform effective sample mixing in cases with large distributional differences such as our MBHM dataset.\nB.3. BearingFM with Data Preprocessing The BearingFM [17] employs a resampling strategy to align input signals to the angular domain. This method assumes that the bearing's rotational speed and sampling frequency are known, enabling resampling of the raw signal to a uniform target speed and sampling rate. Subsequently, it utilizes the Hilbert transform and FFT to extract the envelope spectrum of the signal. Finally, further data augmentation is performed through translation and scaling operations on the signal in both the frequency and amplitude axis to model input.\nThe similarity to our method lies in the use of preprocessing techniques to uniformly represent signals with different sampling rates. However, BearingFM requires more a priori knowledge (the RPM value of the test rig is needed) and performs more complex calculations. What's more, the authors take absolute values after the FFT, resulting in a loss of phase information of the vibration signal."}, {"title": "C. Details of Experience", "content": "C.1. Details of Experimental Setup All training and testing were conducted on a Windows 11 system equipped with a Core i7-13700F CPU and a single RTX 4090 GPU. Python and PyTorch [31] versions utilized were 3.11 and 2.3.1, respectively.\nA batch size of 1024 was employed for pre-training, comparison trials, and ablation experiments, with an initial learning rate of $10^{-4}$. AdamW [24] served as the optimizer, and the learning rate scheduler was set to ReduceLROnPlateau with parameters patience=150 and factor=0.5. This implies that if the loss did not decrease for consecutive 150 batches, the learning rate would be halved. A maximum of 50 epochs was allowed, and training was considered converged and terminated prematurely if the learning rate fell below $10^{-7}$. Fine-tuning was performed using the existing PEFT [26] library.\nC.2. Pre-training We use the MBHM dataset to pre-train the Fault Classification Network (FCN). To prevent the problem of data leakage, we first randomly divide the data"}, {"title": "D. More Experimental Results", "content": "into training, validation, and testing sets in the ratio of 7:2:1, and subsequent query reference signal operations are performed only within the training set. The training set is used to optimize the FCN weights, the validation set is used to evaluate the degree of overfitting of the training accuracy, and the test set is loaded with the validation set weights with the highest accuracy to evaluate the overall accuracy of the model.\nD.1. Selection of $n_f$ Signals with different sampling rates are converted to aligned representations by the DCN, but the number $n_f$ of frequency components in DCN needs to be specified manually, to normalize the signals to the same input length by pad or cut. We statistically analyze our MBHM dataset by evaluating the proportion of energy of the aligned signal containing the original signal for different $n_f$. The results are shown in Fig. 13. The vibration information is mainly concentrated in the low-frequency region, and when $n_f$ reaches a certain value, continuing to increase does not lead to significant distortion improvement.\nD.2. Effectiveness of DCN for Alignment We verified the validity of the DCN alignment by calculating the residuals, as shown in Fig. 14. We select a pair of reference and query (moderate inner ring fault) signals under the same working condition. The reference signal is sampled at 48 kHz and the query signal is sampled at 12 kHz. Fig. 14 (a) illustrates the residuals obtained by direct subtraction. Fig. 14 (b) illustrates the residuals obtained by phase subtraction after downsampling the reference signal to 12kHz. Both of them are difficult to reflect the difference between the query signal and the reference signal. Fig. 14 (c) demonstrates the residuals obtained by phase subtraction after DCN, reflecting the changes of different frequency components after a fault occurs, independent of the sampling rate difference.\nD.3. Impact of Reference and Residual Channels We complement the experimental results without the residual"}, {"title": "D.4. Impact of Initializing the Alignment Layer", "content": "channel $F_{\\text{res}}$ and reference channel $F_u$. As shown in Fig. 15, without these auxiliary channels, the fault classification accuracy decreases, coming from the fact that the signals from different sources are not converted into a uniform representation, retaining some of the features of the dataset source and potentially reducing the generalizability. This is further demonstrated by 0-shot experiments on the IMS dataset [33"}]}