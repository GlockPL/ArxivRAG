{"title": "Time Series Clustering with General State Space Models via Stochastic Variational Inference", "authors": ["Ryoichi Ishizuka", "Takashi Imai", "Kaoru Kawamoto"], "abstract": "In this paper, we propose a novel method of model-based time series clustering with mixtures of general state space models (MSSMs). Each component of MSSMs is associated with each cluster. An advantage of the proposed method is that it enables the use of time series models appropriate to the specific time series. This not only improves clustering and prediction accuracy but also enhances the interpretability of the estimated parameters. The parameters of the MSSMs are estimated using stochastic variational inference, a subtype of variational inference. The proposed method estimates the latent variables of an arbitrary state space model by using neural networks with a normalizing flow as a variational estimator. The number of clusters can be estimated using the Bayesian information criterion. In addition, to prevent MSSMs from converging to the local optimum, we propose several optimization tricks, including an additional penalty term called entropy annealing. Experiments on simulated datasets show that the proposed method is effective for clustering, parameter estimation, and estimating the number of clusters.", "sections": [{"title": "1. Introduction", "content": "Time series data analyses have been conducted in various fields, including science, engineering, business, finance, economics, medicine, and politics [1, 2]. Time series clustering is an analysis method used to classify multiple time series data into groups with identical patterns. This technique is crucial for work in various fields, including recognizing pathological patterns using electrocardiogram data in medicine [3], analyzing industry trends on acquisitions and restructuring in economics [4], and monitoring condition of industrial machinery in engineering [5].\nThere are three principal approaches to time series clustering: the shape-based approach, the feature-based approach, and the model-based approach [2, 6]. The model-based approach has two advantages over the other approaches. First, it often shows higher accuracy when the model can adequately represent the pattern of the time series being analyzed. Second, it allows us to make predictions using the estimated model [7]. Thus, the model-based approach is particularly effective if an appropriate predictive model is used.\nTo ensure accurate clustering and predictions, it is important that the time series model adequately describes the dynamics of the time series. Typical examples in previous studies are an autoregressive (AR) model [8, 9], a hidden Markov model (HMM) [10], and a linear Gaussian state space model (LGSSM) [7]. However, the AR model has the limitation that it cannot adequately describe non-stationary time series. In addition, it is difficult to understand the underlying dynamics through the estimated AR model. Although the HMM has latent variables that allow us to construct a rich class of models, its application is limited to cases in which the latent variables are discrete. The LGSSM, in contrast, is capable of handling continuous latent variables and non-stationary time series. However, it is limited in its ability to accurately represent nonlinear and non-Gaussian dynamics.\nIn the present work, we propose a novel method for model-based time series clustering with general state space models [11, 12], which allows the use of arbitrarily state and observation equations. An advantage of the proposed method is the availability of highly expressive time series models specific to the time series. This means that we can explicitly incorporate prior knowledge of the time series into the time series model. This improves clustering and forecasting accuracy while additionally contributing to the interpretability of the estimated parameters. The proposed method relies on the idea of finite mixture models [13] to introduce mixtures of state space models (MSSMs). The method classifies time series datasets into finite groups (clusters), and simultaneously estimates the model parameters corresponding to each cluster. The number of clusters can be estimated using the Bayesian"}, {"title": "2. Related Work", "content": "information criterion (BIC).\nThe MSSMs are trained using stochastic variational inference (SVI) [14], a subtype of variational inference (VI) [15, 16]. Variational inference is a method of approximating a complex posterior distribution with a tractable distribution, where the approximated distribution should be highly expressive. With SVI, the expressive power of the approximate distribution can be improved by using a neural network in its construction. The proposed method uses normalizing flows [17, 18] as the core of the variational estimator neural network, which further increases the expressive power of the approximate distribution.\nThis paper also proposes several optimization tricks to prevent the convergence of the approximate distribution to a local optimum in the estimation of MSSMs, the main one of which is introducing the penalizing technique called entropy annealing to the training of the cluster estimator. This trick contributes to the stability of parameter estimation.\nThe remainder of the paper is organized as follows. We summarize related work in Sec. 2, and review the SVI approach to parameter estimation, particularly using normalizing flows, in Sec. 3. The SVI approach is extended to MSSMs in Sec. 4. We demonstrate the effectiveness of this method via experiments on simulated datasets in Sec. 5.\nRecently, many model-based time series clustering methods have been presented on the idea of finite mixtures [13] of time series models. Some methods, particularly those based on mixtures of AR models [8, 9], HMMs [10], and LGSSMs [7], have already achieved promising performance in terms of computational cost. In addition, previous studies pursuing richer classes of models have proposed methods based on mixtures of general state space models (that is, MSSMs) with Markov chain Monte Carlo (MCMC)-based parameter estimation algorithms [19, 20].\nThis paper proposes a parameter estimation method based on SVI, instead of MCMC, for MSSMs. SVI can estimate parameters for complex posterior distributions for which analytical solutions cannot be computed, and is computationally less expensive than MCMC [21]. Recently, SVI has been widely substituted for MCMC methods due to SVI's superior computational efficiency. See, for example, [22, 23, 24]."}, {"title": "3. Parameter Estimation using SVI", "content": "3.1. Variational Inference\nVariational inference (VI) approximates the posterior distribution of latent variables by a tractable probability distribution parameterized by \u03d5, and optimizes \u03d5 such that this distribution is closest to the posterior [15, 16]. Specifically, \u03d5 is estimated by minimizing the KL divergence between the approximate and posterior distributions. Let y denote the observed data, x the latent variables, and \u03b8 the parameter of the posterior distribution. The KL divergence of the posterior \\(p_{\\theta}(x | y)\\) and the approximate distribution \\(q_{\\phi}(x | y)\\) can be expressed as\n\\(D_{KL}(q_{\\phi}(x | y) || p_{\\theta}(x | y)) = \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y)}{p_{\\theta}(x | y)} dx = \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y) p_{\\theta}(y)}{p_{\\theta}(x, y)} dx = \\int q_{\\phi}(x | y) \\log p_{\\theta}(y) dx + \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y)}{p_{\\theta}(x, y)} dx = \\log p_{\\theta}(y) - ELBO,\\)  (1)\nwhere\n\\(ELBO = \\int q_{\\phi}(x | y) \\log \\frac{p_{\\theta}(x, y)}{q_{\\phi}(x | y)} dx = E_{q_{\\phi}(x|y)}[f_{(\\phi,\\theta)}(x)],\\)  (2)\n\\(f_{(\\phi,\\theta)}(x) = \\log q_{\\phi}(x | y) + \\log p_{\\theta}(x, y).\\)  (3)\nFrom the above, the KL divergence is the difference between the marginal log-likelihood \\(\\log p_{\\theta}(y)\\) and the lower bound of the marginal log-likelihood, or evidence lower bound (ELBO). When the KL divergence vanishes, the marginal log-likelihood and ELBO become identical. Thus, VI maximizes the ELBO with respect to \u03d5 and \u03b8 to achieve minimization of the KL divergence and maximization of the marginal log-likelihood.\n3.2. Stochastic Variational Inference and Re-parameterization Trick\nTo obtain a good approximation of the posterior distribution of latent variables in VI, the approximate distribution should be as expressive as possible. Therefore, a method has been proposed to increase the expressive power"}, {"title": "4. Proposed Method", "content": "of the approximate distribution by exploiting the high expressive power of neural networks [25, 17]. The ELBO is maximized by stochastic gradient decent with respect to \u03b8 and \u03d5, and this method is called stochastic variational inference (SVI) [14].\nA re-parameterization trick was proposed in [25] as a method for computing the ELBO gradient with low variance. This trick expresses x as the deterministic function \\(g_{\\phi}(\\epsilon, y)\\) with the random vector \\(\\epsilon\\). Using this trick, the ELBO gradient is calculated as\n\\(\\nabla_{\\phi,\\theta} ELBO = \\nabla_{\\phi,\\theta} E_{p(\\epsilon)} [f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon, y))] = E_{p(\\epsilon)} [\\nabla_{\\phi,\\theta} (f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon, y)))] \\approx \\frac{1}{L} \\sum_{l=1}^{L} [\\nabla_{\\phi,\\theta} (f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon^{(l)}, y)))],\\)  (4)\nwhere \\(\\epsilon^{(l)} \\sim p(\\epsilon)\\), and L is the number of Monte Carlo samples. This trick allows \\(\\epsilon\\) to be sampled independently of \u03d5, and thus \u03b8 and \u03d5 to be optimized by gradient decent methods.\n3.3. Normalizing Flows\nNormalizing flows [17, 18] are a tool for transforming random variables using continuous invertible functions, which is useful for improving the expressive power of the approximate distribution \\(q_{\\phi}(x | y)\\). The likelihood of the random variables transformed by \\(f : \\xi \\rightarrow \\xi'\\) can be calculated from the Jacobian property of the invertible function as\n\\(p(\\xi') = p(\\xi) \\left| det \\left(\\frac{\\partial f}{\\partial \\xi}\\right)^{-1} \\right| ,\\)  (5)\nSuccessive application of normalizing flows as \\(f = f_F \\circ \\cdots \\circ f_1\\) produces more complex distributions. The log-likelihood through the normalizing flows is calculated as\n\\(\\log p(\\xi_F) = \\log p(\\xi) + \\sum_{f=1}^{F} \\log \\left| det \\left(\\frac{\\partial f_i}{\\partial \\xi_{i-1}}\\right)^{-1} \\right|,\\)  (6)\nwhere \\(x = \\xi_F\\) and \\(\\xi = \\xi_0\\).\nIn SVI, the function f can be a neural network, which incorporates the normalizing flows as part of the approximate distribution parameterized by \u03d5, providing the approximate distribution with high expressive power.\n4.1. Mixtures of State Space Models\nThe state space model (SSM) [11, 12] is defined as\n\\(x[t] \\sim Q(\\cdot | x[t - 1]),\\)  (7a)\n\\(y[t] \\sim R(\\cdot | x[t]),\\)  (7b)\n\\(x[1] \\sim P_0(\\cdot),\\)  (7c)\nwhere \\(Y = \\{y[t]\\}_{t=1}^T\\) and \\(X = \\{x[t]\\}_{t=1}^T\\) are observed and latent variables, Q denotes the conditional density function of x[t] given x[t \u2212 1], R denotes the conditional density function of y[t] given x[t], and \\(P_0\\) denotes the density function of the initial state x[1]. The SSM then has parameters \\(\\theta = {\\theta_Q, \\theta_R, \\theta_{P_0}}\\), where \\(\\theta_Q, \\theta_R\\), and \\(\\theta_{P_0}\\) are the parameters of Q, R, and \\(P_0\\), respectively.\nWe can define mixtures of SSMs (MSSMs) of the above form. Let \\(D_Y = \\{Y_i\\}_{i=1}^N\\) be a dataset consisting of N observation time series \\(Y_i = \\{y_i[t]\\}_{t=1}^T\\), and for \\(k \\in \\{1, 2, 3, . . ., M\\}\\)\n\\(x_i[t] \\sim Q^{(k)}(\\cdot | x_i[t - 1]),\\)  (8a)\n\\(y_i[t] \\sim R^{(k)}(\\cdot | x_i[t]),\\)  (8b)\n\\(x_i[1] \\sim P_0^{(k)}(\\cdot)\\)  (8c)\nbe different SSMs, and define an MSSM as\n\\(p_{\\theta}(Y_i, X_i) = \\sum_{k=1}^{M} p_{\\theta^{(k)}}(Y_i, X_i)p^{(k)},\\)  (9)\nwhere \\(\\theta^{(k)} = {\\theta_Q^{(k)}, \\theta_R^{(k)}, \\theta_{P_0}^{(k)}\\}\\) is the parameters of the k-th SSM, and \\(p^{(k)}\\) is the weight of the k-th mixture component. We can interpret each SSM as a cluster. Let \\(z = \\{z_i\\}_{i=1}^N\\) be the new latent variables, in which \\(z_i\\) indicates the cluster of \\(Y_i\\).\n4.2. SVI for MSSMs\nIntroducing the new latent variables z, the KL divergence \\(D_{KL}(q_{\\phi}(X_i, Z_i | Y_i) || p_{\\theta}(X_i, Z_i | Y_i))\\) becomes\n\\(D_{KL}(q_{\\phi}(X_i, Z_i | Y_i) || p_{\\theta}(X_i, Z_i | Y_i))\\)"}, {"title": "5. Experiments", "content": "= - \\log p_{\\theta} (Y_i) - ELBO_i,  (10)\nwhere\n\\(ELBO_i = E_{q_{\\phi}(X_i,z_i|Y_i)} [f_{\\theta,\\phi}(X_i, z_i)],\\)  (11)\n\\(f_{\\theta,\\phi}(X_i, z_i) = - \\log q_{\\phi}(X_i | z_i, Y_i) - \\log q_{\\phi}(z_i | Y_i) + \\log p_{\\theta} (X_i, z_i, Y_i).\\)  (12)\nAssuming that \\(Y_i\\) are independent of each other, the log-likelihood, \\(\\log p_{\\theta}(D_Y)\\), of the dataset can be rewritten using Eq. (10) as\n\\(\\log p_{\\theta}(D_Y) = \\sum_{i=1}^{N} \\log p_{\\theta}(Y_i) = \\sum_{i=1}^{N} [ELBO_i + D_{KL} (q_{\\phi}(X_i, Z_i | Y_i) || p_{\\theta}(X_i, Z_i | Y_i))] .\\)  (13)\nMaximizing the sum of the ELBOs with respect to \u03d5 and \\(\\Theta\\) leads to minimization of the KL divergence and maximization of the marginal log-likelihood. In SVI, this sum is optimized by stochastic gradient decent with the loss function\n\\(\\mathcal{L}(\\Theta, \\phi | D_Y) = \\sum_{i=1}^{N} \\mathcal{L}(\\Theta, \\phi | Y_i) = - \\frac{1}{N} \\sum_{i=1}^{N} ELBO_i.\\)  (14)\nIn the proposed method, to increase the expressive power of the approximate distributions \\(q_Q(X_i | Z_i, Y_i)\\) and \\(q_{\\phi}(Z_i | Y_i)\\), neural networks parameterized by \u03d5 are employed. Specifically, \\(q_Q(X_i | Z_i, Y_i)\\) is a neural network containing normalizing flows, and \\(q_{\\phi}(z_i | Y_i)\\) is a neural network with a softmax activation function in the final layer.\nBy applying the re-parameterization trick, \\(\\nabla_{(\\phi,\\Theta)} ELBO_i\\) can be approximated as\n\\(\\nabla_{(\\phi,\\Theta)} ELBO_i = \\nabla_{(\\phi,\\Theta)} E_{p(\\epsilon)q(z_i|Y_i)} [f_{(\\phi,\\Theta)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i)] = \\sum_{M} E_{p(\\epsilon)} [\\nabla_{(\\phi,\\Theta)} \\sum_{z_i=1} q_{\\phi} (z_i | Y_i) f_{(\\phi,\\Theta)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i)]\\)\n\\(\\approx \\frac{1}{L} \\sum_{l=1}^{L} \\sum_{z_i=1}^{M} q_{\\phi} (z_i | Y_i) f_{(\\phi,\\Theta)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i) ,\\)  (15)\nwhere \\(\\epsilon^{(l)} \\sim p(\\epsilon)\\). As in [25], the number L of Monte Carlo samples is 1 and p(e) is the standard normal distribution in this paper. An overview of the proposed method is shown in Figure 1.\n4.3. Estimating the Number of Clusters\nThe number of clusters is estimated using the Bayesian information criterion (BIC) as in [8, 10, 7]. The BIC is defined as\n\\(BIC = \\log L - \\frac{1}{2} (||P|| - 1) \\log N,\\)\nand a model with a larger BIC value is preferred. Here, L is the likelihood of a model, ||P|| is the number of parameters of the model, and N is the number of data. In MSSMs, the mean of \\(ELBO_i\\) is used as L, and the number of parameters of \u0472 referred to as \\(\\Vert \\Theta_R\\Vert\\) is used as ||P||. Consequently, the BIC of the MSSMs is\n\\(BIC = \\frac{1}{N} \\sum_{i=1}^{N} ELBO_i - \\frac{1}{2} (|| \\Theta_R|| - 1) \\log N.\\)  (16)\nIf the number of clusters is unknown, we train models with different numbers of clusters and adopt the model that exhibits the largest BIC.\n4.4. Entropy Annealing\nIn parameter estimation of MSSMs, \\(q_{\\phi}(z_i | Y_i)\\) may concentrate on some clusters and converge to a local optimum in the early stages of training. To prevent this, we introduce an additional penalty term called entropy annealing. The loss function of the proposed method can be rewritten as\n\\(\\mathcal{L}(\\Theta, \\phi | D_Y) = - \\frac{1}{N} \\sum_{i=1}^{N} ELBO_i - \\frac{1}{N} \\sum_{i=1}^{N} H(q_{\\phi}(z_i | Y_i)) - \\sum_{z_i=1}^{M} q_{\\phi} (Z_i | Y_i)H(q_Q(X_i | Z_i, Y_i))\n= -\\sum_{z_i=1}^{M} \\int q_Q(X_i, Z_i | Y_i) \\log p_{\\theta}^{(k)} (X_i, Z_i, Y_i) dX_i ,\\)  (17)\nwhere H(q) indicates the entropy of q. From Equation (17), the loss function comprises the entropy penalty term concerning the approximate distribution and the expected value of the complete data log-likelihood. Entropy annealing increases the entropy penalty \\(H(q_Q(z_i | Y_i))\\) in the early stages of training, which prevents \\(q_{\\phi}(z_i | Y_i)\\) from converging to the local optimum. The loss function with entropy annealing is\n\\(\\mathcal{L}_{A_n}(\\Theta, \\Phi | Y_i) = \\mathcal{L}(\\Theta, \\phi | Y_i) - a_n H(q_{\\phi}(k|Y_i)),\\)  (18)\nwhere \\(a_n\\) is the strength of entropy annealing at epoch n.\nThe strength \\(a_n\\) is dynamically changed by epoch as\n\\(a_{A_n} = \\begin{cases} 0 & \\text{if } 1 < n < n^{\\text{start}}, \\\\ a \\frac{n^{\\text{end}}-n}{n^{\\text{end}}-n^{\\text{start}}} & \\text{if } n^{\\text{start}} < n < n^{\\text{end}}, \\\\ a & \\text{if } n^{\\text{end}} < n, \\end{cases}\\)  (19)\nwhere a, \\(n^{\\text{start}}\\), and \\(n^{\\text{end}}\\) are the maximum annealing strength, number of epochs at which entropy annealing starts to weaken, and number of epochs at which entropy annealing ends, respectively. Other tricks to avoid convergence to a local optimum are described in Appendix A.\nWe demonstrate via experiments on simulated datasets that the proposed method is effective for clustering, parameter estimation, and estimating the"}, {"title": "6. Conclusion", "content": "number of clusters. Specifically, datasets are generated from two dynamics: the Stuart-Landau oscillator [26] and the SIR model [27]. The code is available at https://github.com/ryoichi0917/svi_mssm.\nIn our experiments, we use the residual flows [28, 29] as the architecture of the normalizing flows. This is because the residual flows have highly expressive power compared to other normalizing flow architectures, such as MADE [30] and MAF [31].\n5.1. Stuart-Landau Oscillator\nFirstly, we apply the proposed method to time series generated by a discrete-time stochastic version of the Stuart-Landau oscillator. The discrete-time stochastic Stuart-Landau oscillator belonging to the k-th cluster is defined as\n\\(x[t+1] = x[t] + a_1^{(k)} x[t] - b_1^{(k)} y[t] - (x[t]^2 + y[t]^2) (a^{(k)} x[t] - b^{(k)} y[t]) + \\xi_x,\\)  (20a)\n\\(y[t+1] = y[t] + a_2^{(k)} y[t] + b_2^{(k)} x[t] - (x[t]^2 + y[t]^2) (a^{(k)} y[t] + b^{(k)} x[t]) + \\xi_y,\\)  (20b)\n\\(Y_{obs}[t] = y[t] + \\xi_{obs},\\)  (20c)\nwhere\n\\([\\xi_x, \\xi_y]^T \\sim N(0, A^{(k)} A^{(k)T}),\\)  (21a)\n\\(\\xi_{obs} \\sim N(0, b^{(k)2}),\\)  (21b)\n\\([x[1], y[1]] \\sim N([\\mu_x^{(k)}, \\mu_y^{(k)}]^T, C^{(k)} C^{(k)T}),\\)  (21c)\nis as-\nThe number of clusters was set to three, which means that the dataset\nis generated from three different Stuart-Landau oscillators. The parameters\nof each Stuart-Landau oscillator are shown in Table 1, and representative\nsamples of the Stuart-Landau oscillator dataset are shown in Figure 2a. The\nnumber of time series belonging to each cluster was 300. Details of the\nhyperparameters, such as the configuration of the approximate distribution\nand optimizer settings, are provided in Appendix B.1.\nThe values of BIC for the MSSMs with from two to five clusters are shown\nin Table 2a. BIC is the largest when the number of clusters is three, which\nis the correct number of clusters. Table 2b is the confusion matrix that\nshows the result of clustering. This result indicates that perfect clustering is\nachieved.\nTable 2c shows the estimated values of the MSSM parameters. From\nthe table, the estimated values are close to the true values for most of the\nparameters. Figure 2b shows the actual and estimated latent variables. From\nthe figure, the proposed method accurately estimates the latent variables.\n5.2. SIR Model\nSecondly, we apply the proposed method to time series generated by a\ndiscrete-time stochastic version of the SIR model [32]. The discrete-time\nstochastic SIR model belonging to the k-th cluster is defined as\n\\(p[t + 1] \\mid p[t] \\sim Dirichlet(10^{a^{(k)}} f^{(k)}(p[t])),\\)  (22a)\n\\(I_{obs}[t] \\mid \\rho^{(I)}[t] \\sim Beta(10^{6b^{(k)}}(p_{(I)}[t]), 10^{6b^{(k)}}(1 - \\rho^{(I)}[t])),\\)  (22b)\n\\(\\rho[1] \\sim Dirichlet (10^{c^{(k)}}\\rho^{(init)}),\\)  (22c)\nwhere\n\\(f^{(k)}(p[t]) = [f^{(S)}(p[t]), f^{(I)}(p[t]), f^{(R)}(p[t])]^T,\\)  (23a)\n\\(f^{(S)}(p[t]) = \\rho^{(S)}[t] - \\beta^{(k)}\\rho^{(S)}[t]\\rho^{(I)}[t],\\)  (23b)\n\\(f^{(k)}(p[t]) = \\rho^{(I)}[t] + \\beta^{(k)}\\rho^{(S)}[t]\\rho^{(I)}[t] - \\gamma^{(k)}\\rho^{(I)}[t],\\)  (23c)\n\\(f^{(k)}(p[t]) = \\rho^{(R)}[t] + \\gamma^{(k)}\\rho^{(I)}[t],\\)  (23d)\n\\(\\rho_{init} = [\\rho^{(S,init)}, \\rho^{(I,init)}, \\rho^{(R,init)}]^T,\\)  (23e)\n\\(\\rho^{(S,init)} + \\rho^{(I,init)} + \\rho^{(R,init)} = 1.\\)\n\\(\\theta_Q = {\\beta^{(k)}, \\gamma^{(k)}, a^{(k)}\\}\\), \\(\\theta_R = {b^{(k)}\\}\\), and \\(\\theta^{(k)} = \\theta_0^{(k)} = {c^{(k)}, \\rho^{(init)}\\}\\). Although the SIR model (22) is intractable\nfor normalizing flows due to the Dirichlet distributions, we can apply the pro-\nposed method by rewriting Eq. (22) into an equivalent form. See Appendix\nB.2 for details.\nThe number of clusters was set to two, which means that the dataset is\ngenerated from two different SIR models. The parameters of each SIR model\nare shown in Table 3, and representative samples of the SIR model dataset\nare shown in Figure 3a. The number of time series belonging to each cluster\nwas 300. Details of the hyperparameters, such as the configuration of the\napproximate distribution and optimizer settings, are provided in Appendix\nB.1.\nThe values of BIC for the MSSMs with from two to four clusters are\nshown in Table 4a. BIC is the largest when the number of clusters is two,\nwhich is the correct number of clusters. Table 4b is the confusion matrix that\nshows the result of clustering. This result indicates that perfect clustering is\nachieved.\nTable 4c shows the estimated values of the MSSM parameters. From\nthe table, the estimated values are close to the true values for most of the\nparameters. Figure 3b shows the actual and estimated latent variables. From\nthe figure, the proposed method accurately estimates the latent variables."}, {"title": "Appendix A. Practical Efforts for Parameter Optimization", "content": "In this paper, we propose a novel method of model-based time series\nclustering with mixtures of general state-space models. An advantage of the\nproposed method is that it enables the use of tailored time series models.\nThis not only improves clustering and prediction accuracy but also enhances\nthe interpretability of the estimated parameters. Experiments on simulated\ndatasets show that the proposed method is effective for clustering, parameter\nestimation, and estimating the number of clusters.\nA limitation of this study is that the validation experiments in this paper\nwere only conducted on simulated datasets. Future research should validate\nthe effectiveness of the proposed method on real datasets. When applying\nthe proposed method to real data, it would also be useful to enhance the\nstructure of the MSSMs to allow for the input of exogenous variables.\nIn this section, we describe three tricks to avoid undesirable convergence\nof \\(q_{\\phi}(z_i | Y_i)\\) to a local optimum, other than entropy annealing.\nThe first of these tricks relates to the optimizer. In the proposed method,\n(simple) mini-batch SGD is used to optimize MSSMs. This is because mini-\nbatch SGD can accurately approximate the posterior distribution in SVI [a1].\nThe learning rate of mini-batch SGD is changed periodically based on cyclical\nlearning rates [a2] to avoid convergence to a local optimum.\nThe second is to introduce learning rate scheduling and fine-tuning. When\nMSSMs are trained with cyclical learning rates, the parameters do not con-\nverge, because the learning rate increases periodically. Therefore, we set the\nlearning rate to a low value at the late stages of the training to make the\nparameters converge. After this training, fine-tuning is performed to learn\nonly \\(\\theta_R\\) and \\(\\theta_{P_0}\\) and parameters of normalizing flow modules. This is because \\(\\theta_Q\\) is\npoorly optimized if all parameters are simultaneously learned. The loss\nfunction is significantly less affected by the improvement of \\(\\theta_R^{(k)}\\) than by that\nof \\(\\theta_Q^{(k)}\\) due to the fact\n\\(log p_{\\theta_Q^{(k)}, \\theta_R^{(k)}} (X_i \\mid Z_i = k) \\approx log p_{\\theta_Q^{(k)}} (x_i[1] \\mid Z_i = k) + \\sum_{t=2}^{T} log p_{\\theta_R^{(k)}} (X_i[t+1] \\mid x_i[t], Z_i = k).\\)  (A.1)\nFinally, we train MSSMs multiple times from different initial parameter\nvalues and adopt the model with the lowest loss to avoid rare cases unsolved\nby the above-mentioned two tricks.\nWith the above tools and entropy annealing, MSSMs converged without\nfalling into local optimum solutions in the experiments."}, {"title": "Appendix B. Details of Experiments", "content": "Appendix B.1. Model Architecture\nThe architectural details of the model used in our experiments are shown\nin Figure B.4. For the hyperparameter settings of each layer, see the code\non GitHub (https://github.com/ryoichi0917/svi_mssm).\nAppendix B.2. Equivalent form of the SIR Model\nFor the stochastic SIR model, the representation (22) is intractable for\nnormalizing flows due to the Dirichlet distribution. Nevertheless, we can\napply the proposed method by rewriting the model in the equivalent form as\n\\(G_{(S)}[t + 1] \\sim Gamma(10^{a^{(k)}} f^{(S)}(p[t]), 1),\\)\n\\(G_{(I)}[t + 1] \\sim Gamma(10^{a^{(k)}} f^{(I)}(p[t]), 1),\\)  (B.la)\n\\(G_{(R)}[t + 1] \\sim Gamma(10^{a^{(k)}} f^{(R)}(p[t]), 1),\\)\n\\(I_{obs}[t] \\mid \\rho^{(I)}[t] \\sim Beta(10^{6b^{(k)}}(p_{(I)}[t]), 10^{6b^{(k)}}(1 - \\rho^{(I)}[t])),\\)  (B.1b)\n\\(G_{(S)}[1] \\sim Gamma(10^{c^{(k)}}\\rho^{(S,init)}, 1),\\)\n\\(G_{(I)}[1] \\sim Gamma(10^{c^{(k)}}\\rho^{(I,init)}, 1),\\)  (B.1c)\n\\(G_{(R)}[1] \\sim Gamma(10^{c^{(k)}}\\rho^{(R,init)}, 1),\\)\nwhere\n\\(f^{(S)}(p[t]) = \\rho^{(S)}[t] - \\beta^{(k)}\\rho^{(S)}[t]\\rho^{(I)}[t],\\)\n\\(f^{(I)}(p[t]) = \\rho^{(I)}[t] + \\beta^{(k)}\\rho^{(S)}[t]\\rho^{(I)}[t] - \\gamma^{(k)}\\rho^{(I)}[t],\\)  (B.2a)\n\\(f^{(R)}(p[t]) = \\rho^{(R)}[t] + \\gamma^{(k)}\\rho^{(I)}[t],\\)\n\\(\\rho^{(S)}[t] = \\frac{G_{(S)}[t]}{G_{(S)}[t] + G_{(I)}[t] + G_{(R)}[t]},\\)\n\\(\\rho^{(I)}[t] = \\frac{G_{(I)}[t]}{G_{(S)}[t] + G_{(I)}[t] + G_{(R)}[t]},\\)  (B.2b)\n\\(\\rho^{(R)}[t] = \\frac{G_{(R)}[t]}{G_{(S)}[t] + G_{(I)}[t] + G_{(R)}[t]},\\)\n\\(\\rho[t] = [\\rho^{(S)}[t], \\rho^{(I)}[t], \\rho^{(R)}[t]],\\)  (B.2c)\n\\(\\rho^{(S,init)} + \\rho^{(I,init)} + \\rho^{(R,init)} = 1.\\)  (B.2d)"}]}