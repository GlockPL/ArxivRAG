{"title": "Time Series Clustering with General State Space\nModels via Stochastic Variational Inference", "authors": ["Ryoichi Ishizuka", "Takashi Imai", "Kaoru Kawamoto"], "abstract": "In this paper, we propose a novel method of model-based time series cluster-\ning with mixtures of general state space models (MSSMs). Each component\nof MSSMs is associated with each cluster. An advantage of the proposed\nmethod is that it enables the use of time series models appropriate to the\nspecific time series. This not only improves clustering and prediction accu-\nracy but also enhances the interpretability of the estimated parameters. The\nparameters of the MSSMs are estimated using stochastic variational infer-\nence, a subtype of variational inference. The proposed method estimates the\nlatent variables of an arbitrary state space model by using neural networks\nwith a normalizing flow as a variational estimator. The number of clusters\ncan be estimated using the Bayesian information criterion. In addition, to\nprevent MSSMs from converging to the local optimum, we propose several\noptimization tricks, including an additional penalty term called entropy an-\nnealing. Experiments on simulated datasets show that the proposed method\nis effective for clustering, parameter estimation, and estimating the number\nof clusters.", "sections": [{"title": "1. Introduction", "content": "Time series data analyses have been conducted in various fields, includ-\ning science, engineering, business, finance, economics, medicine, and politics\n[1, 2]. Time series clustering is an analysis method used to classify multi-\nple time series data into groups with identical patterns. This technique is\ncrucial for work in various fields, including recognizing pathological patterns\nusing electrocardiogram data in medicine [3], analyzing industry trends on\nacquisitions and restructuring in economics [4], and monitoring condition of\nindustrial machinery in engineering [5].\nThere are three principal approaches to time series clustering: the shape-\nbased approach, the feature-based approach, and the model-based approach\n[2, 6]. The model-based approach has two advantages over the other ap-\nproaches. First, it often shows higher accuracy when the model can ade-\nquately represent the pattern of the time series being analyzed. Second,\nit allows us to make predictions using the estimated model [7]. Thus, the\nmodel-based approach is particularly effective if an appropriate predictive\nmodel is used.\nTo ensure accurate clustering and predictions, it is important that the\ntime series model adequately describes the dynamics of the time series. Typ-\nical examples in previous studies are an autoregressive (AR) model [8, 9], a\nhidden Markov model (HMM) [10], and a linear Gaussian state space model\n(LGSSM) [7]. However, the AR model has the limitation that it cannot ad-\nequately describe non-stationary time series. In addition, it is difficult to\nunderstand the underlying dynamics through the estimated AR model. Al-\nthough the HMM has latent variables that allow us to construct a rich class\nof models, its application is limited to cases in which the latent variables are\ndiscrete. The LGSSM, in contrast, is capable of handling continuous latent\nvariables and non-stationary time series. However, it is limited in its ability\nto accurately represent nonlinear and non-Gaussian dynamics.\nIn the present work, we propose a novel method for model-based time se-\nries clustering with general state space models [11, 12], which allows the use\nof arbitrarily state and observation equations. An advantage of the proposed\nmethod is the availability of highly expressive time series models specific to\nthe time series. This means that we can explicitly incorporate prior knowl-\nedge of the time series into the time series model. This improves clustering\nand forecasting accuracy while additionally contributing to the interpretabil-\nity of the estimated parameters. The proposed method relies on the idea\nof finite mixture models [13] to introduce mixtures of state space models\n(MSSMs). The method classifies time series datasets into finite groups (clus-\nters), and simultaneously estimates the model parameters corresponding to\neach cluster. The number of clusters can be estimated using the Bayesian"}, {"title": "2. Related Work", "content": "information criterion (BIC).\nThe MSSMs are trained using stochastic variational inference (SVI) [14],\na subtype of variational inference (VI) [15, 16]. Variational inference is a\nmethod of approximating a complex posterior distribution with a tractable\ndistribution, where the approximated distribution should be highly expres-\nsive. With SVI, the expressive power of the approximate distribution can\nbe improved by using a neural network in its construction. The proposed\nmethod uses normalizing flows [17, 18] as the core of the variational esti-\nmator neural network, which further increases the expressive power of the\napproximate distribution.\nThis paper also proposes several optimization tricks to prevent the conver-\ngence of the approximate distribution to a local optimum in the estimation\nof MSSMs, the main one of which is introducing the penalizing technique\ncalled entropy annealing to the training of the cluster estimator. This trick\ncontributes to the stability of parameter estimation.\nThe remainder of the paper is organized as follows. We summarize re-\nlated work in Sec. 2, and review the SVI approach to parameter estimation,\nparticularly using normalizing flows, in Sec. 3. The SVI approach is extended\nto MSSMs in Sec. 4. We demonstrate the effectiveness of this method via\nexperiments on simulated datasets in Sec. 5.\nRecently, many model-based time series clustering methods have been\npresented on the idea of finite mixtures [13] of time series models. Some\nmethods, particularly those based on mixtures of AR models [8, 9], HMMs\n[10], and LGSSMs [7], have already achieved promising performance in terms\nof computational cost. In addition, previous studies pursuing richer classes\nof models have proposed methods based on mixtures of general state space\nmodels (that is, MSSMs) with Markov chain Monte Carlo (MCMC)-based\nparameter estimation algorithms [19, 20].\nThis paper proposes a parameter estimation method based on SVI, in-\nstead of MCMC, for MSSMs. SVI can estimate parameters for complex pos-\nterior distributions for which analytical solutions cannot be computed, and\nis computationally less expensive than MCMC [21]. Recently, SVI has been\nwidely substituted for MCMC methods due to SVI's superior computational\nefficiency. See, for example, [22, 23, 24]."}, {"title": "3. Parameter Estimation using SVI", "content": "3.1. Variational Inference\nVariational inference (VI) approximates the posterior distribution of la-\ntent variables by a tractable probability distribution parameterized by \u222e,\nand optimizes \u00f8 such that this distribution is closest to the posterior [15, 16].\nSpecifically, is estimated by minimizing the KL divergence between the\napproximate and posterior distributions. Let y denote the observed data, x\nthe latent variables, and 0 the parameter of the posterior distribution. The\nKL divergence of the posterior $p_{\\theta}(x | y)$ and the approximate distribution\n$q_{\\phi}(x | y)$ can be expressed as\n$D_{KL} (q_{\\phi}(x | y) || p_{\\theta}(x | y)) = \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y)}{p_{\\theta}(x | y)} dx$\n$= \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y)p_{\\theta}(y)}{p_{\\theta}(x, y)} dx$\n$= \\int q_{\\phi}(x | y) \\log p_{\\theta}(y) dx + \\int q_{\\phi}(x | y) \\log \\frac{q_{\\phi}(x | y)}{p_{\\theta}(x, y)} dx$\n$= \\log p_{\\theta}(y) - ELBO,$\nwhere\n$ELBO = \\int q_{\\phi}(x | y) \\log \\frac{p_{\\theta}(x, y)}{q_{\\phi}(x | y)} dx$\n$= E_{q_{\\phi}(x|y)}[f_{(\\phi,\\theta)}(x)],$\n$f_{(\\phi,\\theta)}(x) = \\log q_{\\phi}(x | y) + \\log p_{\\theta}(x, y).$\nFrom the above, the KL divergence is the difference between the marginal\nlog-likelihood $\\log p_{\\theta}(y)$ and the lower bound of the marginal log-likelihood,\nor evidence lower bound (ELBO). When the KL divergence vanishes, the\nmarginal log-likelihood and ELBO become identical. Thus, VI maximizes the\nELBO with respect to 6 and \u03b8 to achieve minimization of the KL divergence\nand maximization of the marginal log-likelihood.\n3.2. Stochastic Variational Inference and Re-parameterization Trick\nTo obtain a good approximation of the posterior distribution of latent\nvariables in VI, the approximate distribution should be as expressive as possi-\nble. Therefore, a method has been proposed to increase the expressive power"}, {"title": "3.3. Normalizing Flows", "content": "of the approximate distribution by exploiting the high expressive power of\nneural networks [25, 17]. The ELBO is maximized by stochastic gradient de-\ncent with respect to \u03b8 and \u00d8, and this method is called stochastic variational\ninference (SVI) [14].\nA re-parameterization trick was proposed in [25] as a method for com-\nputing the ELBO gradient with low variance. This trick expresses x as the\ndeterministic function $g_{\\phi}(\\epsilon, y)$ with the random vector $\\epsilon$. Using this trick,\nthe ELBO gradient is calculated as\n$\\nabla_{\\phi,\\theta}ELBO = \\nabla_{\\phi,\\theta}E_{p(\\epsilon)} [f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon, y))]$\n$= E_{p(\\epsilon)} [\\nabla_{\\phi,\\theta} (f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon, y)))]$\n$\\approx \\frac{1}{L} \\sum_{l=1}^{L} [\\nabla_{\\phi,\\theta} (f_{(\\phi,\\theta)} (g_{\\phi}(\\epsilon^{(l)}, y)))],$\nwhere $\\epsilon^{(l)} \\sim p(\\epsilon)$, and L is the number of Monte Carlo samples. This trick\nallows e to be sampled independently of $, and thus 0 and $ to be optimized\nby gradient decent methods.\nNormalizing flows [17, 18] are a tool for transforming random variables\nusing continuous invertible functions, which is useful for improving the ex-\npressive power of the approximate distribution $q_{\\phi}(x | y)$. The likelihood of\nthe random variables transformed by $f : \\xi \\rightarrow \\xi'$ can be calculated from the\nJacobian property of the invertible function as\n$p(\\xi') = p(\\xi) \\text{det}(\\frac{\\partial f}{\\partial \\xi})^{-1}|,$\nSuccessive application of normalizing flows as $f = f_F \\circ \\cdots \\circ f_1$ produces\nmore complex distributions. The log-likelihood through the normalizing flows\nis calculated as\n$\\log p(\\xi_F) = \\log p(\\xi) + \\sum_{i=1}^{F} \\log \\text{det}(\\frac{\\partial f_i}{\\partial \\xi_{i-1}})^{-1}|,$\nwhere $x = \\xi_F$ and $\\xi = \\xi_0$.\nIn SVI, the function f can be a neural network, which incorporates the\nnormalizing flows as part of the approximate distribution parameterized by\n, providing the approximate distribution with high expressive power."}, {"title": "4. Proposed Method", "content": "4.1. Mixtures of State Space Models\nThe state space model (SSM) [11, 12] is defined as\n$x[t] \\sim Q(\\cdot | x[t - 1]),$\n$y[t] \\sim R(\\cdot | x[t]),$\n$x[1] \\sim P_{0}(\\cdot),$\nwhere $Y = {y[t]}_{t=1}^{T}$ and $X = {x[t]}_{t=1}^{T}$ are observed and latent vari-\nables, Q denotes the conditional density function of x[t] given x[t \u2013 1], R\ndenotes the conditional density function of y[t] given x[t], and $P_0$ denotes\nthe density function of the initial state x[1]. The SSM then has parameters\n$\\theta = {\\theta_Q, \\theta_R, \\theta_{P_0}}$, where $\\theta_Q$, $\\theta_R$, and $\\theta_{P_0}$ are the parameters of Q, R, and $P_0$,\nrespectively.\nWe can define mixtures of SSMs (MSSMs) of the above form. Let $D_Y =$\n${Y_i}_{i=1}^{N}$ be a dataset consisting of N observation time series $Y_i = {y_i[t]}_{t=1}^{T_i}$,\nand for $k \\in {1, 2, 3, . . ., M}$\n$x_i[t] \\sim Q^{(k)}(\\cdot | x_i[t - 1]),$\n$y_i[t] \\sim R^{(k)}(\\cdot | x_i[t]),$\n$x_i[1] \\sim P_{0}^{(k)}(\\cdot)$\nbe different SSMs, and define an MSSM as\n$p_{\\theta}(Y_i, X_i) = \\sum_{k=1}^{M} p_{\\theta^{(k)}}(Y_i, X_i)p^{(k)},$\nwhere $\\theta^{(k)} = {\\theta_Q^{(k)}, \\theta_R^{(k)}, \\theta_{P_0}^{(k)}}$ is the parameters of the k-th SSM, and $p^{(k)}$ is\nthe weight of the k-th mixture component. We can interpret each SSM as\na cluster. Let $z = {z_i}_{i=1}^{N}$ be the new latent variables, in which $z_i$ indicates\nthe cluster of $Y_i$.\n4.2. SVI for MSSMs\nIntroducing the new latent variables z, the KL divergence $D_{KL} (q_{\\phi}(X_i, z_i | Y_i) || p_{\\theta}(X_i, z_i | Y_i))$\nbecomes\n$D_{KL} (q_{\\phi}(X_i, z_i | Y_i) || p_{\\theta}(X_i, z_i | Y_i))$"}, {"title": "4.3. Estimating the Number of Clusters", "content": "$\\approx \\log p_{\\theta} (Y_i) - ELBO_i,$\nwhere\n$ELBO_i = E_{q_{\\phi}(X_i,z_i|Y_i)} [f_{\\theta,\\phi}(X_i, z_i)],$\n$f_{\\theta,\\phi}(X_i, z_i) = - \\log q_{\\phi}(X_i | z_i, Y_i) - \\log q_{\\phi}(z_i | Y_i)$\n$+ \\log p_{\\theta} (X_i, z_i, Y_i).$\nAssuming that $Y_i$ are independent of each other, the log-likelihood, $\\log p_{\\theta}(D_Y)$,\nof the dataset can be rewritten using Eq. (10) as\n$\\log p_{\\theta} (D_Y) = \\sum_{i=1}^{N} \\log p_{\\theta}(Y_i)$\n$= \\sum_{i=1}^{N} [ELBO_i + D_{KL} (q_{\\phi}(X_i, z_i | Y_i) || p_{\\theta}(X_i, z_i | Y_i))] .\nMaximizing the sum of the ELBOs with respect to \u03c6 and O leads to minimiza-\ntion of the KL divergence and maximization of the marginal log-likelihood.\nIn SVI, this sum is optimized by stochastic gradient decent with the loss\nfunction\n$L(\\Theta, \\phi | D_Y) = \\sum_{i=1}^{N} L(\\Theta, \\phi | Y_i) = - \\sum_{i=1}^{N} ELBO_i.$\nIn the proposed method, to increase the expressive power of the approx-\nimate distributions $q_\\phi(X_i | z_i, Y_i)$ and $q_\\phi(z_i | Y_i)$, neural networks param-\neterized by $\\phi$ are employed. Specifically, $q_\\phi(X_i | z_i, Y_i)$ is a neural network\ncontaining normalizing flows, and $q_\\phi(z_i | Y_i)$ is a neural network with a\nsoftmax activation function in the final layer.\nBy applying the re-parameterization trick, $\\nabla_{\\Theta,\\phi}ELBO_i$ can be approxi-\nmated as\n$\\nabla_{(\\phi,\\Theta)} ELBO_i$\n$= \\nabla_{(\\phi,\\Theta)} E_{p(\\epsilon)q_{\\phi}(z_i|Y_i)} [f_{(\\Theta,\\phi)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i)]$\n$= \\nabla_{(\\Theta,\\phi)} E_{p(\\epsilon)} \\sum_{z_i=1}^{M} q_{\\phi}(z_i | Y_i) f_{(\\Theta,\\phi)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i)$\n$\\approx \\frac{1}{L} \\sum_{l=1}^{L} \\sum_{z_i=1}^{M} q_{\\phi}(z_i | Y_i) f_{(\\Theta,\\phi)} (g_{\\phi}(\\epsilon, z_i, Y_i), z_i)[,$\nwhere $\\epsilon^{(l)} \\sim p(\\epsilon)$. As in [25], the number L of Monte Carlo samples is 1 and\np(e) is the standard normal distribution in this paper. An overview of the\nproposed method is shown in Figure 1.\nThe number of clusters is estimated using the Bayesian information cri-\nterion (BIC) as in [8, 10, 7]. The BIC is defined as\n$BIC = \\log L - \\frac{1}{2} (|\\theta| - 1) \\log N,$"}, {"title": "4.4. Entropy Annealing", "content": "and a model with a larger BIC value is preferred. Here, L is the likelihood of\na model, |\u0398| is the number of parameters of the model, and N is the number\nof data. In MSSMs, the mean of $ELBO_i$ is used as L, and the number of\nparameters of \u0472 referred to as $||P_{\\theta}||$ is used as $||P||$. Consequently, the BIC\nof the MSSMs is\n$BIC = \\frac{1}{N} \\sum_{i=1}^{N} ELBO_i - \\frac{1}{2} (||P_{\\theta}|| - 1) \\log N.$\nIf the number of clusters is unknown, we train models with different numbers\nof clusters and adopt the model that exhibits the largest BIC.\nIn parameter estimation of MSSMs, $q_{\\phi}(z_i | Y_i)$ may concentrate on some\nclusters and converge to a local optimum in the early stages of training. To\nprevent this, we introduce an additional penalty term called entropy anneal-\ning. The loss function of the proposed method can be rewritten as\n$L(\\Theta, \\phi | D_Y) = - \\sum_{i=1}^{N} ELBO_i$\n$- \\sum_{i=1}^{N} [-\\frac{1}{M}H(q_{\\phi}(z_i | Y_i)) - \\sum_{z_i=1}^{M} q_{\\phi} (z_i | Y_i)H(q_{\\phi}(X_i | z_i, Y_i)) ]$\n$- \\sum_{z_i=1}^{M} [-\\int q_{\\phi}(X_i, z_i | Y_i) \\log p_{\\theta^{(k)}}(X_i, z_i, Y_i) dX_i],$\nwhere H(q) indicates the entropy of q. From Equation (17), the loss function\ncomprises the entropy penalty term concerning the approximate distribution\nand the expected value of the complete data log-likelihood. Entropy an-\nnealing increases the entropy penalty $H(q_{\\phi}(z_i | Y_i))$ in the early stages of\ntraining, which prevents $q_{\\phi}(z_i | Y_i)$ from converging to the local optimum.\nThe loss function with entropy annealing is\n$L'_{a_n}(\\Theta, \\phi | Y_i) = L(\\Theta, \\phi | Y_i) - a_nH(q_{\\phi}(k|Y_i)),$\nwhere $a_n$ is the strength of entropy annealing at epoch n."}, {"title": "5. Experiments", "content": "The strength $a_n$ is dynamically changed by epoch as\n$\\alpha_{n} = \\begin{cases}\n0 & \\text{if } 1 < n < n^{start},\\\\\n\\frac{a(n^{end} - n)}{n^{end} - n^{start}} & \\text{if } n^{start} < n < n^{end},\\\\\na & \\text{if } n^{end} < n,\n\\end{cases}$\nwhere $a$, $n^{start}$, and $n^{end}$ are the maximum annealing strength, number of\nepochs at which entropy annealing starts to weaken, and number of epochs at\nwhich entropy annealing ends, respectively. Other tricks to avoid convergence\nto a local optimum are described in Appendix A.\nWe demonstrate via experiments on simulated datasets that the proposed\nmethod is effective for clustering, parameter estimation, and estimating the"}, {"title": "5.1. Stuart-Landau Oscillator", "content": "number of clusters. Specifically, datasets are generated from two dynam-\nics: the Stuart-Landau oscillator [26] and the SIR model [27]. The code is\navailable at https://github.com/ryoichi0917/svi_mssm.\nIn our experiments, we use the residual flows [28, 29] as the architecture\nof the normalizing flows. This is because the residual flows have highly\nexpressive power compared to other normalizing flow architectures, such as\nMADE [30] and MAF [31].\nFirstly, we apply the proposed method to time series generated by a\ndiscrete-time stochastic version of the Stuart-Landau oscillator. The discrete-"}, {"title": "5.2. SIR Model", "content": "time stochastic Stuart-Landau oscillator belonging to the k-th cluster is de-\nfined as\n$x[t+1] = x[t] + a_1^{(k)}x[t] - b_y^{(k)}y[t]$\n- (x[t]^2 + y[t]^2) (a_x^{(k)}x[t] - b_y^{(k)}y[t]) + \\xi_x,$\n$y[t+1] = y[t] + a_y^{(k)}y[t] + b_x^{(k)}x[t]$\n- (x[t]^2 + y[t]^2) (a_y^{(k)}y[t] + b_x^{(k)}x[t]) + \\xi_y,$\n$y_{obs}[t] = y[t] + \\xi_{obs},$\nwhere\n$\\begin{bmatrix} \\xi_x \\\\ \\xi_y \\end{bmatrix} \\sim N(0, A^{(k)}A^{(k)T}),$\n$\\xi_{obs} \\sim N(0, b^{(k)2}),$\n$[x[1] \\quad y[1]] \\sim N([\\mu_x^{(k)} \\\\ \\mu_y^{(k)}]^T, C^{(k)}C^{(k)T}),$\nSecondly, we apply the proposed method to time series generated by a\ndiscrete-time stochastic version of the SIR model [32]. The discrete-time\nstochastic SIR model belonging to the k-th cluster is defined as\n$p[t+1] | p[t] \\sim Dirichlet(10a^{(k)} f^{(k)}(p[t])),$"}, {"title": "6. Conclusion", "content": "where\n$I_{obs}[t] | p_{I}[t] \\sim Beta(10^{b^{(k)}} p_{I}[t], 10^{b^{(k)}} (1 - p_{I}[t])),$\n$p[1] \\sim Dirichlet(10^{c^{(k)}} \\rho_{init}),$\n$f^{(k)}(p[t]) = \\begin{bmatrix} f_{S}^{(k)}(p[t]) \\\\ f_{I}^{(k)}(p[t]) \\\\ f_{R}^{(k)}(p[t]) \\end{bmatrix},$\n$f_{S}^{(k)}(p[t]) = p_{S}[t] - \\beta^{(k)}p_{S}[t]p_{I}[t],$\n$f_{I}^{(k)}(p[t]) = p_{I}[t] + \\beta^{(k)}p_{S}[t]p_{I}[t] - \\gamma^{(k)}p_{I}[t],$\n$f_{R}^{(k)}(p[t]) = p_{R}[t] + \\gamma^{(k)}p_{I}[t],$\n$\\rho[t] = \\begin{bmatrix} p_{S}[t] \\\\ p_{I}[t] \\\\ p_{R}[t] \\end{bmatrix},$\n$\\rho_{init} = \\begin{bmatrix} p_{S,init}^{(k)} \\\\ p_{I,init}^{(k)} \\\\ p_{R,init}^{(k)} \\end{bmatrix},$\n$p_{S,init}^{(k)} + p_{I,init}^{(k)} + p_{R,init}^{(k)} = 1.$\nThus, the parameters to be estimated are $\\theta_Q^{(k)} = {\\beta^{(k)}, \\gamma^{(k)}, a^{(k)}}$, $\\theta_R^{(k)} =\n{b^{(k)}}$, and $\\theta_{P_0}^{(k)} = \\theta_{P_0}^{(k)} = {c^{(k)}, (\\rho_{init})}$. Although the SIR model (22) is intractable\nfor normalizing flows due to the Dirichlet distributions, we can apply the pro-\nposed method by rewriting Eq. (22) into an equivalent form. See Appendix\nB.2 for details.\nThe number of clusters was set to two, which means that the dataset is\ngenerated from two different SIR models. The parameters of each SIR model\nare shown in Table 3, and representative samples of the SIR model dataset\nare shown in Figure 3a. The number of time series belonging to each cluster\nwas 300. Details of the hyperparameters, such as the configuration of the\napproximate distribution and optimizer settings, are provided in Appendix\nB.1.\nThe values of BIC for the MSSMs with from two to four clusters are\nshown in Table 4a. BIC is the largest when the number of clusters is two,\nwhich is the correct number of clusters. Table 4b is the confusion matrix that\nshows the result of clustering. This result indicates that perfect clustering is\nachieved.\nTable 4c shows the estimated values of the MSSM parameters. From\nthe table, the estimated values are close to the true values for most of the\nparameters. Figure 3b shows the actual and estimated latent variables. From\nthe figure, the proposed method accurately estimates the latent variables.\nIn this paper, we propose a novel method of model-based time series\nclustering with mixtures of general state-space models. An advantage of the\nproposed method is that it enables the use of tailored time series models.\nThis not only improves clustering and prediction accuracy but also enhances\nthe interpretability of the estimated parameters. Experiments on simulated\ndatasets show that the proposed method is effective for clustering, parameter\nestimation, and estimating the number of clusters.\nA limitation of this study is that the validation experiments in this paper\nwere only conducted on simulated datasets. Future research should validate\nthe effectiveness of the proposed method on real datasets. When applying\nthe proposed method to real data, it would also be useful to enhance the\nstructure of the MSSMs to allow for the input of exogenous variables."}, {"title": "Appendix A. Practical Efforts for Parameter Optimization", "content": "In this section, we describe three tricks to avoid undesirable convergence\nof $q_\\phi(z_i | Y_i)$ to a local optimum, other than entropy annealing.\nThe first of these tricks relates to the optimizer. In the proposed method,\n(simple) mini-batch SGD is used to optimize MSSMs. This is because mini-\nbatch SGD can accurately approximate the posterior distribution in SVI [a1].\nThe learning rate of mini-batch SGD is changed periodically based on cyclical\nlearning rates [a2] to avoid convergence to a local optimum.\nThe second is to introduce learning rate scheduling and fine-tuning. When\nMSSMs are trained with cyclical learning rates, the parameters do not con-\nverge, because the learning rate increases periodically. Therefore, we set the\nlearning rate to a low value at the late stages of the training to make the\nparameters converge. After this training, fine-tuning is performed to learn\nonly $\\theta_R^{(k)}$ and parameters of normalizing flow modules. This is because $\\theta_R^{(k)}$\nis poorly optimized if all parameters are simultaneously learned. The loss\nfunction is significantly less affected by the improvement of $\\theta_R^{(k)}$ than by that\nof $\\theta_Q^{(k)}$ due to the fact\n$\\log P_{\\theta_Q^{(k)},\\theta_R^{(k)}}(X_i | z_i = k)$\n$= \\log P_{\\theta_{P_0}^{(k)}}(x_i[1] | z_i = k) + \\sum_{t=2}^{T} \\log P_{\\theta_R^{(k)}}(x_i[t+1] | x_i[t], z_i = k).$\nFinally, we train MSSMs multiple times from different initial parameter\nvalues and adopt the model with the lowest loss to avoid rare cases unsolved\nby the above-mentioned two tricks.\nWith the above tools and entropy annealing, MSSMs converged without\nfalling into local optimum solutions in the experiments."}, {"title": "Appendix B.2. Equivalent form of the SIR Model", "content": "Appendix B. Details of Experiments\nAppendix B.1. Model Architecture\nThe architectural details of the model used in our experiments are shown\nin Figure B.4. For the hyperparameter settings of each layer, see the code\non GitHub (https://github.com/ryoichi0917/svi_mssm).\nFor the stochastic SIR model, the representation (22) is intractable for\nnormalizing flows due to the Dirichlet distribution. Nevertheless, we can\napply the proposed method by rewriting the model in the equivalent form as\n$G_{(s)}[t + 1] \\sim Gamma(10^{a^{(k)}} f_{(s)}^{(k)}(p[t]), 1),$\n$G_{(I)}[t + 1] \\sim Gamma(10^{a^{(k)}} f_{(I)}^{(k)}(p[t]), 1),$\n$G_{(R)}[t + 1] \\sim Gamma(10^{a^{(k)}} f_{(R)}^{(k)}(p[t]), 1),$\n$I_{obs}[t] | p_{(I)}[t] \\sim Beta(10^{b^{(k)}} p_{(I)}[t], 10^{b^{(k)}} (1 - p_{(I)}[t])),$\n$G_{(s)}[1] \\sim Gamma(10^{c^{(k)}} p_{(S,init)}^{(k)}, 1),$\n$G_{(I)}[1] \\sim Gamma(10^{c^{(k)}} p_{(I,init)}^{(k)}, 1),$\n$G_{(R)}[1] \\sim Gamma(10^{c^{(k)}} p_{(R,init)}^{(k)}, 1),$\nwhere\n$f_{(s)}^{(k)}(p[t]) = p_{(s)}[t] - \\beta^{(k)}p_{(s)}[t]p_{(I)}[t],$\n$f_{(I)}^{(k)}(p[t]) = p_{(I)}[t] + \\beta^{(k)}p_{(s)}[t]p_{(I)}[t] - \\gamma^{(k)}p_{(I)}[t],$\n$f_{(R)}^{(k)}(p[t]) = p_{(R)}[t] + \\gamma^{(k)}p_{(I)}[t],$\n$p_{(s)}[t] = \\frac{G_{(s)}[t]}{G_{(s)}[t] + G_{(I)}[t] + G_{(R)}[t]},$\n$p_{(I)}[t] = \\frac{G_{(I)}[t]}{G_{(s)}[t] + G_{(I)}[t] + G_{(R)}[t]},$\n$p_{(R)}[t] = \\frac{G_{(R)}[t]}{G_{(s)}[t] + G_{(I)}[t] + G_{(R)}[t]},$\n$p[t] = \\begin{bmatrix} p_{(s)}[t] \\\\ p_{(I)}[t] \\\\ p_{(R)}[t] \\end{bmatrix},$\n$p_{(S,init)}^{(k)} + p_{(I,init)}^{(k)} + p_{(R,init)}^{(k)} = 1.$\nThus, the parameters to be estimated are $\\theta_Q^{(k)} = {\\beta^{(k)}, \\gamma^{(k)}, a^{(k)}}$, $\\theta_R^{(k)} =\n{b^{(k)}}$, and $\\theta_{P_0}^{(k)} = \\theta_{P_0}^{(k)} = {c^{(k)}, (\\rho_{init})}$. Although the SIR model (22) is intractable\nfor normalizing flows due to the Dirichlet distributions, we can apply the pro-\nposed method by rewriting Eq. (22) into an equivalent form. See Appendix\nB.2 for details."}]}