{"title": "Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery", "authors": ["Yuhan Zheng", "Jessie A Elliott", "John V Reynolds", "Sheraz R Markar", "Bart\u0142omiej W. Papie\u017c", "ENSURE study group"], "abstract": "Esophageal cancer is a major cause of cancer-related mortality internationally, with high recurrence rates and poor survival even among patients treated with curative-intent surgery. Investigating relevant prognostic factors and predicting prognosis can enhance post-operative clinical decision-making and potentially improve patients' outcomes. In this work, we assessed prognostic factor identification and discriminative performances of three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a large multicenter international dataset from ENSURE study. We first employed Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on outcomes. Subsequently, we utilised CoxPH and two deep neural network (DNN)- based models, DeepSurv and DeepHit, to predict DFS and OS. The significant prognostic factors identified by our models were consistent with clinical literature, with post-operative pathologic features showing higher significance than clinical stage features. DeepSurv and DeepHit demonstrated comparable discriminative accuracy to CoxPH, with DeepSurv slightly outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735 and 0.74, respectively. While these results suggested the potential of DNNs as prognostic tools for improving predictive accuracy and providing personalised guidance with respect to risk stratification, CoxPH still remains an adequately good prediction model, with the data used in this study.", "sections": [{"title": "1 Introduction", "content": "Esophageal cancer is a major cause of cancer-related mortality internationally. The average 5-year Overall Survival (OS) rate is less than 25% [1], ranging from 10% to 55% depending on the stage of which the disease is detected [2]. While surgical resection, known as esophagectomy, remains the primary treat- ment for esophageal cancer, the prognosis of post-operative patients remains poor. Despite advancements in cancer management strategy, more than 50% of the patients experience a recurrence within 1-3 years following curative-intent surgery [3], with a median survival time of 24 months [4]. Therefore, identifying prognostic factors associated with a higher risk of recurrence, as well as predict- ing and stratifying patients based on their recurrence and survival probabilities, are crucial to the delivery of personalised medicine approaches that could po- tentially improve oncologic outcomes. Current risk stratification methods for patients with esophageal cancer predominantly rely on pathological data, pri- marily tumor staging [5]. This does not fully leverage all available clinical and patient-level data efficiently, and does not account for individual variations.\nTo address these issues, some studies have developed models for progno- sis prediction. For example, logistic regression models have been employed to predict absolute risks for patients with esophageal cancer [6, 7]. However, these models predict a single-point outcome event without incorporating time-to-event analysis and are limited to one histologic type only. The Cox Proportional Haz- ards model (CoxPH) [8] is a widely used regression model that allows the study of the relationships between time-to-event outcomes and a set of covariates. Many studies have employed CoxPH to identify prognostic factors for different outcomes [9-11]. However, CoxPH model assumes linear relationships between covariates and that the relative hazard remains constant over time. This hinders its ability to capture higher level interactions between variables and outcomes.\nRecent developments in AI have led to increased applications of machine learning (ML) models in oncology to address more complex problems. For ex- ample, Zhang et al. [13] explored multiple ML methods for survival prediction in squamous cell carcinoma, and demonstrated that while CoxPH model remains sufficiently good for interpretive studies, ML approaches have the potential to enhance predictive accuracy. Gong et al. [14] explored artificial neural networks (ANNs) in survival prediction, though these did not outperform other traditional ML models such as XGBoost [15]. However, most of these aforementioned stud- ies relied on data collected from a single center, raising questions about their generalisability and robustness when applied to larger multicenter cohorts. Most studies focus on only one type of outcome, and the prediction values on other outcomes remain unknown. Moreover, these studies often utilise a limited num- ber of features. There is a significant clinical interest in incorporating a more comprehensive set of features that take account into, for example, improvements in treatment technologies or surveillance strategies. Gujjuri et al. [12] imple- mented CoxPH and Random Forest using ENSURE dataset. However, the re- sults showed that Random Forest did not surpass CoxPH in both discrimination and calibration.\nIn this work, we developed models to predict Disease-Free Survival (DFS) and OS for patients with esophageal cancer following curative-intent surgery. The work is divided into two main components. The first component is prognostic"}, {"title": "2 Dataset and Preprocessing", "content": "This work is based on data collected from the European iNvestigation of SUrveillance after Resection for Esophageal cancer (ENSURE) study [16], a retrospec- tive non-interventional study taken across 20 European centers. Patients with esophageal or junction cancer undergoing curative intent treatment from June 2009 to June 2015 were all considered for inclusion. In total, there are 4972 patients and over 170 variables. All patients were staged according to the 8th edition of the American Joint Committee on Cancer (AJCC) staging [17].\nThe use of the dataset and this study has been approved by he Joint Research Ethics Committee of Tallaght University Hospital and St. James's Hospital, Dublin, Ireland (SJH-TUH JREC Ref 2943 Amendment 1).\nIn this work, DFS is defined as the time from treatment (i.e., surgery) to recur- rence or death from any cause [18]. Patients who are lost to follow-up or remain alive without recurrence at the end of the study are recorded as censored events. OS is defined as the period from diagnosis to death from any cause [19]. Patients that are lost to follow-up or still alive at the end of the study are recorded as censored event."}, {"title": "2.3 Patient Inclusion and Variable Selection Criteria", "content": "In this work, we removed patients with missing DFS and/or OS outcome, as well as patients with rare histologic type (i.e., non-adenocarcinoma and non-squamous cell carcinoma). We excluded further patients with postop- erative death for DFS prediction by definition.\nVariables used in our models were selected by experienced clinicians, based on the literature review and their clinical importance. Vari- ables exhibiting clinically known high correlations with other variables, lacking well-established relationships with outcomes, or variables that were often poorly documented by centers, were excluded from the study. Additionally, while there is no single acceptable threshold for missing rate, the approach to dealing with missingness requires careful consideration. Blindly applying imputation strate- gies to variables with high missing rate could also impose biases [20]. Therefore, after further assessment by clinicians, a set of variables was additionally removed based on both their rate of missingness and their clinical relevance.\nIn this work, we did not apply any ML or statistic-based variable selection strategies. Evidence [30] suggests that feature selection prior to model applica- tion does not significantly improve model performance, especially that we either adopted regularisers in the model (more details in Section 3.2) or the ML models themselves have internal feature selection capabilities to handle high-dimensional data in this study. As a result of this variable selection processes, 37 variables were selected with a missing rate of less than 30%."}, {"title": "2.4 Missingness and Imputation", "content": "In this study, the missingness mechanism was assumed to be Missing At Random (MAR) [21], as whether the data is missing or not depends exclusively on their availability at center during data collection process [22, 23]. This assumption allows us to apply imputation strategies to handle missingness. A flow chart illustrating the overall process, which is going to described below, can be found in Figure 1 in Section A.1.\nDifferent imputation strategies were applied to the prognostic factor iden- tification task and prediction task that were mentioned in Section 1. Multiple Imputation by Chained Equations (MICE) [24] was used for prognostic factor identification task, with 10 iterations per imputation set. Multiple imputation (MI), which takes the uncertainty of imputation into account and fills different multiple plausible values, is important to reduce bias and chance of false-positive and false-negative conclusions [25]. The multiple imputed datasets were passed into models, optimised and analysed separately, and final results were combined using Rubin's rule [26]. For prediction task, where the impact of imputation uncertainty is generally less critical, we used single-point multivariate imputa- tion by chained equations, which is typically sufficient for predictive modeling purposes."}, {"title": "2.5 Data Overview", "content": "Table 1 summarises the statistics for the dataset used in DFS and OS tasks, respectively."}, {"title": "3 Methods and Experiments", "content": "In this work, three models were employed to predict DFS and OS: a regression model CoxPH [8] and two neural network-based models named DeepSurv [33] and DeepHit [34]. CoxPH is a semi-parametric regression model that takes the form $h_0(t)exp(\\sum_i x_i\\cdot \\beta_i)$, where $h_0(t)$ is baseline hazard function, $x_i$ is covariate and $\\beta_i$ is coefficient. The model assumes that the effect of a factor is constant over time and there is a linear relationship between predictors and log-hazards.\nDeepSurv is a DNN-based extension of CoxPH model. It models the hazard function as $h_0(t)exp(f_e(x))$, where $f_e(x)$ is a neural network that takes covari- ates as input and outputs a scalar. This allows DeepSurv to capture high-level interactions among features. DeepHit, on the other hand, employs an end-to-end DNN that learns the distribution of survival times directly, without making any assumptions about the underlying stochastic process.\nIn this work, CoxPH model was employed for the prognostic factor identi- fication task. For the prediction task, all three models were used, with CoxPH serving as a baseline for comparison with neural network-based methods. These models were chosen to leverage their respective strengths in handling different aspects of survival analysis, from traditional regression assumptions to capturing complex interactions and learning distributions directly from data."}, {"title": "3.2 Experimental Setup", "content": "The dataset was split into two parts: 80% for training and 20% as held-out testing dataset. For the prognostic factor identi- fication task, the training set was further split into 85% for training and 15% for validation. Stratified bootstrapping was performed on the validation set to select the best set of hyperparameters. For the prediction task, a stratified 5-fold CV was performed on the 80% training set for hyperparameter selection. The imputation and standardisation were performed within the CV loop to avoid information leakage, as mentioned in Section 2.4. A graphical illustration of the splitting strategy can be found in Figure 2 in Appendix Section A.2.\nHyperparameter selection was conducted in a grid- search manner. A detailed list of the optimal set of hyperparameters for each model and task can be found in Table 4 in Appendix Section A.3. Elastic net regularisation (i.e., L1 (Lasso) and L2 (Ridge) regularisation penalties) was ap- plied to CoxPH. CoxPH with Elastic net [36] was generally found to outperform standard CoxPH during training.\nPerformance Evaluation. Three metrics were used to evaluate the discrmina- tive performances of the models: concordance index (C-index), Integrated Brier Score (IBS), and time-dependent AUC (tAUC, also known as dynamic AUC).\nImplementation. All the models and analyses were implemented using Python 3.10.5. Survival models were implemented with lifelines 0.28.0 and pycox 0.2.3. The CoxPH was trained on a CPU with a memory of 15.2GB. DeepSurv and DeepHit were trained on NVIDIA GPUs with 40GB of RAM."}, {"title": "4 Results", "content": "Table 2 summarises the multivariate analysis results of CoxPH with the signif- icant variables (P-value < 0.05) being listed only, along with their hazard ratio (HR), and 95% confidence interval (CI)."}, {"title": "4.2 Prediction Task", "content": "Table 3 summarises the discriminative prediction performance of three models for DFS and OS respectively. Comparing all three metrics reveals that DeepSurv demonstrates comparable performances to CoxPH, while DeepHit demonstrates slightly inferior performance in terms of IBS. Figure 3 in Appendix Section A.4 provides examples of predicted OS curves obtained from the three models for the same random set of five patients. Notably, while CoxPH and DeepSurv exhibit similar shapes and distributions, DeepHit shows a completely different profile, with minimal variation among the five prediction curves. Despite DeepHit gen- erally ordering patients consistently in terms of survival probabilities compared to the other two models, this profile suggests poorer calibration performance."}, {"title": "5 Discussion and Conclusion", "content": "In this work, we analysed a heterogeneous multicenter dataset to investigate the contribution of covariates to and predictive performance of three models on DFS and OS in patients with esophageal cancer. The significant prognostic factors identified aligned well with clinical literature and experiences. For exam- ple, pathologic tumor staging features appear to be strong prognostic factors, and are generally more significant than clinical staging [35]. The more advanced the pathologic stage of the tumor is, the higher the hazard ratio. In terms of prediction, DeepSurv consistently outperformed CoxPH in both DFS and OS tasks, with C-index of 0.735 and 0.740, respectively, when C-index serving as the primary metric. Overall, the two DNN-based models demonstrated compara- ble discriminative performance to CoxPH; though DeepHit was found to exhibit poorer calibration performance compared to the other two models. The use of a multicenter international dataset, which includes patients with either adeno- carcinoma or squamous cell carcinoma, suggested broader applicability of these findings across diverse cohort in various clinical settings. In general, despite their ability to model more complex interactions, DNN-based models did not greatly outperform the CoxPH. The CoxPH, which is interpretable and computationally efficient, still remains a sufficiently good prediction model with tabular data.\nWhile all three models demonstrated good discriminative performance, it is inferred that these results likely represent the upper bound achievable with tabular data. It is worth noting that some significant features, for example, Clinical N stage, are derived from radiologic assessment scans (Computed To- mography (CT), Positron Emission Tomography (PET)) [17]. Therefore, incor- porating imaging-derived features such as radiomics could provide more detailed information and potentially enhance model performance [42]. It should be noted that, among the three models, DeepHit posed particular challenges during train- ing, showing large fluctuations in performance and high sensitivity to hyper- parameters. This difficulty can be attributed to its end-to-end neural network architecture, which involves a multitude of hyperparameters. More advanced hyperparameter selection techniques such as Bayesian optimisation could there- fore be explored [39] during the training. Graphical convolutional neural work (GNN) [40], which has been an emerging model in survival analysis, could also be explored in the future. In addition, it could be observed that DFS and OS share some common significant prognostic factors. This suggest the possibility of multi-task learning of these two prediction tasks [41]. Furthermore, introducing additional calibration techniques [37] could improve the alignment of predic- tions with ground truth data. Methods like SHAP [38] could also enhance the interpretability of neural networks by identifying crucial features in predictive models."}, {"title": "A.1 Imputation Procedure", "content": "Figure 1 illustrates the overall process of variable preprocessing and imputation, as described in Section 2.4."}, {"title": "A.2 Splitting Strategy", "content": "Figure 2 illustrates the splitting strategy during training for the two tasks."}, {"title": "A.3 Hyperparameter Tuning", "content": "Table 4 summarises the optimal set of hyperparameters of the three models. For DeepSurv and DeepHit, various network structures were explored, along with different number of epochs, batch sizes, optimiser schedulers, and learning rates."}]}