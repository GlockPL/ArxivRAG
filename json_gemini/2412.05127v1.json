{"title": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating Effective Prompts in Large Language Models", "authors": ["Michael Hewing", "Vincent Leinhos"], "abstract": "The rise of large language models (LLMs) has highlighted the importance of prompt engineering as a crucial technique for optimizing model outputs. While experimentation with various prompting methods, such as Few-shot, Chain-of-Thought, and role-based techniques, has yielded promising results, these advancements remain fragmented across academic papers, blog posts and anecdotal experimentation. The lack of a single, unified resource to consolidate the field's knowledge impedes the progress of both research and practical application. This paper argues for the creation of an overarching framework that synthesizes existing methodologies into a cohesive overview for practitioners. Using a design-based research approach, we present the Prompt Canvas (Figure 1), a structured framework resulting from an extensive literature review on prompt engineering that captures current knowledge and expertise. By combining the conceptual foundations and practical strategies identified in prompt engineering, the Prompt Canvas provides a practical approach for leveraging the potential of Large Language Models. It is primarily designed as a learning resource for pupils, students and employees, offering a structured introduction to prompt engineering. This work aims to contribute to the growing discourse on prompt engineering by establishing a unified methodology for researchers and providing guidance for practitioners.", "sections": [{"title": "1 Introduction", "content": "With the advances of sophisticated Large Language Models (LLM), the ability to guide these models to generate useful, contextually relevant, and coherent answers has become an essential skill. Prompt engineering refers to the art and science of designing inputs or queries (prompts) that effectively guide LLMs towards desired outputs. Schulhoff et al. (2024, p. 7) describe related prompt techniques as a \"blueprint that outlines how to structure a prompt.\" This discipline bridges the gap between the user's goals and the model's capabilities, enabling more precise, creative, and domain-specific solutions. Prompt engineering allows users to precisely guide LLMs in generating contextually relevant and task-specific responses.\nYet, much of the research and insights into prompt engineering are distributed across disparate sources, such as academic journals, preprints, blogs, and informal discussions on platforms like GitHub, Reddit, or YouTube. Navigating this complex landscape requires not only significant effort, but also a level of expertise that may be inaccessible to practitioners, creating a substantial barrier to entry and hindering the effective application of prompt engineering techniques in practice. With this paper, a canvas-oriented approach is proposed that consolidates current knowledge in the field of prompt engineering into a coherent, visual format. This way, practitioners can implement effective strategies more confidently and with clarity.\nThe second chapter of this paper describes the fragmented state of knowledge in prompt engineering, highlighting the challenges practitioners face in accessing and applying diverse techniques. In the third chapter, a comprehensive review of existing studies and approaches in prompt engineering is presented, showcasing key techniques and patterns in the field. Chapter Four introduces the Prompt Canvas as a structured framework to consolidate and visually represent prompt engineering techniques for better accessibility and practical application. The last chapter provides a conclusion along with constraints and areas for future research."}, {"title": "2 The Need for an Overview of Prompt Engineering Techniques", "content": "Prompts are vital for LLMs because they serve as the primary mechanism for translating user intentions into actionable outputs. By guiding the model's responses, prompts enable LLMs to perform a wide range of tasks, from creative writing to complex problem-solving, without requiring task-specific retraining. They leverage the pre-trained knowledge embedded in the model, allowing users to adapt LLMs to specific contexts and applications through in-context learning."}, {"title": "2.1 The Relevance of Prompting in Unsupervised Learning and Transformer Architecture", "content": "Through the concepts of unsupervised learning and basic transformer architecture, the relevance and impact of prompts is evident. Generative AI models, such as LLMs, can be assigned to natural language processing (NLP) in the field of artificial intelligence (Braun et al., 2024, p. 560). In an earlier paradigm of NLP (Liu et al., 2023, p. 4), models were typically trained for specific tasks using supervised learning with manually annotated data. However, this limited a model to its training domain and manual annotation during training was time-consuming and expensive (Radford et al., 2018, p. 1). This challenge led to unsupervised learning gaining in importance. According to P. Liu et al., this represents the transition to the current NLP paradigm of Pre-train, Prompt, Predict. Large and diverse data sets are used for training. In this way, the model recognizes patterns and aligns parameters within a neural network. By entering a prompt, the model adapts to the corresponding task, which is known as in-context learning. This allows the model to be used for a variety of tasks (see Radford et al., 2018, p. 2; Brown et al., 2020, P. 3). In addition to unsupervised learning, the transformer architecture, which was published in 2017 by Vaswani et al. under the title \"Attention Is All You Need,\u201d laid an important foundation for today's LLMs. It enables context to be maintained across long texts (Radford et al., 2018, p. 2). In June 2018, OpenAI (2018) stated that their \"... approach is a combination of two existing ideas: transformers and unsupervised pre-training.\u201d The abbreviation GPT, Generative Pre-Trained Transformer, reflects this approach. The process from prompt input to output is described in Lo (2023) as follows (the process description has been shortened for clarity): First, individual words of the prompt are broken down into tokens. Each token is represented by a vector that conveys its meaning. This representation is referred to as embedding. Self-attention is used to capture the relationship between tokens in the prompt. Finally, based on the previous context and the patterns learned in the training data, next tokens are predicted. Once a token has been selected, it is translated back into a human-readable form. This process is repeated until a termination criterion is reached.\nThese foundational concepts enable LLMs to process diverse and complex tasks without task-specific retraining, relying instead on adaptive responses generated through prompting. Prompt engineering bridges the gap between generalized pre-trained knowledge and specific user needs, functioning as the key mechanism through which the model's potential is harnessed. The transformer's self-attention mechanism ensures contextual integrity across sequences, while unsupervised learning enables the model to identify and generalize patterns from vast datasets. Together, these innovations allow LLMs to excel in the Pre-train, Prompt, Predict paradigm, making prompt engineering not only a critical aspect of model utility but also a determinant of task-specific success. As AI applications expand across domains, the ability to craft precise and"}, {"title": "2.2 The Need for a Practitioner-Oriented Overview on Prompt Engineering Techniques", "content": "effective prompts will remain central to realizing the full power of these transformative technologies.\nPrompt engineering is a rapidly evolving field, with techniques such as Few-shot learning, Chain-of-Thought reasoning and iterative feedback loops being developed and refined to solve complex problems. The pace of innovation is driven by a wide range of applications in industries such as healthcare, education and software development, where tailored prompts can significantly improve model performance. A large body of research is investigating the effectiveness of different prompting techniques. However, the current state of knowledge in this area is highly fragmented, posing significant challenges to researchers and practitioners alike. Fragmentation of knowledge refers to the disjointed and inconsistent distribution of information across various sources, often lacking coherence or standardized frameworks. One of the primary challenges of this fragmented knowledge is the absence of a unified framework that consolidates the diverse techniques, methodologies and findings in prompt engineering. Practitioners new to the field face steep learning curves, as they must navigate a scattered and complex body of literature.\nYet, as it will be highlighted in the literature review of chapter Three, initial efforts to systematically consolidate these techniques, develop taxonomies and establish a shared vocabulary are emerging. These publications structure current knowledge into schemes and patterns. While they provide in-depth analyses and valuable structures, they often lack accessibility for practitioners seeking practical solutions and actionable insights. This gap from research advancements to practical application highlights a pressing need for bridging between academic research and real-world use. Addressing these challenges will ensure that the benefits of prompt engineering are more widely realized, enabling its application to expand further across industries and domains."}, {"title": "2.3 Canvas for Visualization", "content": "The field of prompt engineering involves a dynamic and multifaceted interplay of strategies, methodologies, and considerations, making it challenging to present in a way that is both comprehensive and accessible. The canvas model promotes visual thinking and has been widely adopted in fields such as business strategy (Osterwalder and Pigneur 2010, Pichler 2016), teamwork (Ivanov and Voloshchuk, 2015), startups (Maurya, 2012), research (The Fountain Institute, 2020) and design thinking (IBM, 2016), where it has proven to be an effective way to organize and communicate complex processes. A canvas simplifies complexity by visually organizing aspects of relevance into defined sections, allowing users to see the relationships and workflows at a glance. It promotes a holistic view of the process in one unified space. Also, the collaborative nature of a canvas facilitates communication and alignment among team members with varying levels of expertise.\nBy applying this proven framework to prompt engineering and making the transition to this visual representation more intuitive, practitioners can leverage prompt techniques and patterns. Practitioners can quickly grasp the key elements and a workflow, reducing barriers to entry and enabling more effective application of the techniques."}, {"title": "3 Identifying Common Techniques Through a Systematic Literature Review", "content": "In order to obtain a comprehensive overview of the current state of techniques in the field of prompt engineering, a systematic literature review (SLR) has been carried out. Such a systematic approach provides transparency in the selection of databases, search terms, as well as inclusion and exclusion criteria. After the literature search and selection, included literature will be analyzed and consolidated."}, {"title": "3.1 Literature Search and Selection", "content": "The literature search process primarily adheres to the framework outlined by vom Brocke et al. (2009, pp. 8\u201311). For the subsequent selection of sources, the methodology is based on the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines (cf. Page et al., 2021). Vom Brocke et al. (2009) outline the systematic literature review (SLR) process in five distinct phases. The process begins with defining the scope of the literature search (Phase 1) and creating a preliminary concept map (Phase 2) to guide the review. This is followed by the execution of the actual literature search (Phase 3). The later stages involve the analysis and synthesis of the included literature (Phase 4) and a discussion of the findings along with their limitations (Phase 5). The last phase we integrated into the section on limitations at the end of this paper. Vom Brocke et al. (2009) emphasize the first three phases in their work. With the literature research, the following research question shall be addressed:\nWhat is the current state of techniques and methods in the field of prompt engineering, especially in text-to-text modalities?\nTo establish the framework for the literature search in phase one, vom Brocke et al. (2009) draw on Cooper's taxonomy (1988, pp. 107\u2013112). Cooper identifies six key characteristics for classifying literature searches: focus, goal, perspective, coverage, organization and audience. These characteristics provide a structured approach to defining the purpose and scope of a literature review. The second phase involves elaboration using concept mapping. For this purpose, terms are selected that are"}, {"title": "3.2 Analysis and Results", "content": "In \"Can (A)I Have a Word with You? A Taxonomy on the Design Dimensions of AI Prompts\", Braun et al. (2024) develop a taxonomy for the design of prompts for different modalities, such as text-to-text and text-to-image.\n\"The Prompt Report\" by (Schulhoff et al., 2024) can be considered the most comprehensive article of the included literature. It considers prompting techniques for the text-to-text modality and gives an insight into other modalities, such as text-to-visuals. At the same time, that article uses the PRISMA approach within a systematic literature review, which increases the transparency of the selected prompting techniques. Prompting techniques are categorized by modality and prompting category.\n\"A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications\" by Sahoo et al. (2024) is similarly comprehensive. It classifies the prompting techniques according to application area.\nSchulhoff et al. and Sahoo et al. present a total of 108 different prompting techniques.\n\"The Art of Creative Inquiry-From Question Asking to Prompt Engineering\" by Sasson Lazovsky et al. offers a perspective on the similarities between question formulation and prompt engineering. The article shows which characteristics are important in the interaction between humans and generative AI.\nIn \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\", White et al. (2023) present prompt patterns using practical examples for software development. However, according to the authors, these can be transferred to other areas.\nIn order to make a selection from the multitude of prompting techniques, those that were presented in several included articles were prioritized. If a prompting technique includes adapted variants, the parent prompting technique is presented first, followed by possible adaptations. Braun et al. (2024) classified nine dimensions and three meta-dimensions that should be considered when creating prompts . Firstly, the interaction between the LLM and user, which, depending on the prompt, can be seen as computer-in-the-loop or human-in-the-loop (HITL). An example of HITL would be if the user asks the LLM, in addition to the main instruction, to pose follow-up questions that the user then answers (Braun et al., 2024, pp. 561, 565). In this case, the user would take on a more active role. According to Braun et al., input and output types such as text, image, audio, video and others are part of the interaction meta-dimension. Context is defined as the second meta-dimension. This consists of the learning dimension, which is divided into Zero-shot, One-shot and Few-shot. In addition to Braun et al., three of the included articles also identified these as superordinate prompting techniques. As already presented in Chapter Two, an LLM can adapt to new tasks (in-context learning), even if it has not been explicitly trained for that task (Braun et al., 2024, pp. 563-564; cf. Radford et al., 2018, Brown et al., 2020). The addition of examples in a prompt is referred to as Few-shot and One-shot in the case of an explicit example. As Brown et al. showed in their article on GPT-3, the use of Few-shot can increase the accuracy of the output compared to Zero-shot (see Radford et al., 2019), where no examples are provided. In addition, the behavior of an LLM can be adapted to a specific context by assigning a role in a prompt (Braun et al., 2024, p. 564; cf. White et al., 2023, p. 7). By setting a style, the output can be adapted more generally (Braun et al., 2024, p. 564). Braun et al. go on to define the information space dimension for the context meta-dimension. The authors differentiate between whether additional information is provided internally - directly in the prompt - or externally - by agents that use search engines, for example. If no additional context is provided, the output is based exclusively on the training data of the LLM (Braun et al., 2024, p. 564). Braun et al. define the third and final meta-dimension as the outcome to be achieved by adapting a prompt (Braun et al., 2024, p. 564). The authors classify Chain-of-Thought (CoT) for this purpose. This was also identified as a superior prompt-"}, {"title": "4 Mapping Identified Techniques to the Prompt Canvas", "content": "ing technique in the articles by Schulhoff et al. (2024) and Sahoo et al. (2024), which refer to the article by Wei et al. (2023). CoT is designed for tasks that require complex understanding. This is also referred to as reasoning in various included articles. CoT breaks down a problem into smaller steps, solves them and then provides a final answer.\nThis chapter focuses on synthesizing the insights gathered from the literature review to populate the Prompt Canvas with relevant, evidence-based elements. It identifies key techniques in prompt engineering, aligning them with the structured sections of the canvas. Based on a user-centered design focusing on understanding the users' needs, the canvas consists of four categories, each containing a distinct aspect of a prompt: Persona/Role and Target Audience, Goal and Step-by-Step, Context and References, Format and Tonality. These categories align with the natural flow of information processing, from establishing the setting (persona and audience) to defining the task (goal and steps), providing the necessary background (context and references), and finally specifying the desired output (format and tone)."}, {"title": "4.1 Persona/Role and Target Audience", "content": "Defining a specific persona or role helps in tailoring the language model's perspective, ensuring that the response aligns with the expected expertise or viewpoint. Identifying the target audience ensures that the content is appropriate for the intended recipients, considering their knowledge level and interests. This category is essential because it sets the foundation for the model's voice and the direction of the response, making it more relevant and engaging for the user.\nThis element was selected to address the recurring discussions in the literature about role-based prompting and user-centered design. Studies by Braun et al. (2024) highlighted the value of assigning roles to guide the model's tone and specificity. Sasson Lazovsky et al. (2024) further emphasized the importance of personas in enhancing creative inquiry. These insights underscored the need to include a dedicated"}, {"title": "4.2 Task/Intent and Step-by-Step", "content": "Clearly articulating the goal provides the language model with a specific objective, enhancing the focus and purpose of the response. Breaking down the goal into step-by-step instructions or questions guides the model through complex tasks or explanations systematically. This category justifies its inclusion by emphasizing the importance of precision and clarity in prompts, which directly impacts the quality and usefulness of the output.\nClassified by Braun et al. (2024), Sahoo et al. (2024) and Sasson Lazovsky et al. (2024) as a distinct prompting category, Chain-of-Thought prompting techniques decompose a task step-by-step and enhance thereby the model's reasoning capabilities on complex problems. The Chain-of-Thought prompting technique can be used with both Zero-shot and Few-shot concepts. By structuring tasks incrementally, the model produces outputs that are both coherent and logically organized. Furthermore, this category facilitates creative inquiry; as Sasson Lazovsky et al. (2024) emphasize, clearly defining intent in prompts is essential for open-ended or exploratory tasks."}, {"title": "4.3 Context and References", "content": "Providing context and relevant references equips the language model with necessary background information, reducing ambiguity and enhancing the accuracy of the response. This category acknowledges that AI models rely heavily on the input prompt for context, and without it, the responses may be generic or off-target. Including references also allows the model to incorporate specific data or adhere to particular frameworks, which is vital in academic or professional settings.\nThis element was selected to address the frequent recommendation to provide situational and contextual information in prompts. Braun et al. (2024) stressed the importance of embedding contextual details to enhance output reliability and Schulhoff et al. (2024) suggested incorporating external references or historical data into prompts for guidance. Linking prompts to prior decisions, documents, or reports enhances contextual richness and ensures outputs reflect critical dependencies (Sasson Lazovsky et al., 2024). By integrating these elements, practitioners can craft prompts that are both informative and grounded in factual context."}, {"title": "4.4 Output/Format and Tonality", "content": "Specifying the desired format and tone ensures that the response meets stylistic and structural expectations. Whether the output should be in the form of a report, a list, or an informal explanation, and whether the tone should be formal, friendly, or neutral, this category guides the model in delivering content that is not only informative but also appropriately presented. This consideration is crucial for aligning the response with the conventions of the intended medium or genre.\nThis category emerged from the emphasis in the literature on aligning the model's outputs with specific user requirements and communication contexts. Techniques like output specification and refinement, discussed in Sahoo et al. (2024), are critical for aligning the model's output with user needs. Braun et al. (2024) highlighted specifying output formats to meet technical or domain-specific needs. Directing the model to produce responses in specific formats, such as tables, markdown, or code, ensures that outputs meet those requirements. Tonality customization and aligning tone with organizational branding to maintain consistency across communication outputs further validated the need to include this aspect in the Prompt Canvas. Also, it is of use to specify tone attributes like luxury, authority, or informality, depending on the target audience or purpose."}, {"title": "4.5 Recommended Techniques", "content": "By mapping the identified techniques to the Prompt Canvas, the foundational aspects of a prompt from defining personas to output refinement are systematically addressed. The canvas simplifies the application of complex techniques, making them more approachable for practitioners.\nIn addition to its primary elements, the integration of Techniques and Tooling categories serves to enhance the canvas by offering deeper technical insights and practical support. These categories focus on further techniques and the tools available to implement them.\nThis category within the Prompt Canvas emphasizes the application of further strategies to refine and optimize prompts. These techniques enrich the Prompt Canvas by offering a diverse set of strategies to address varying tasks and contexts. Practitioners can draw from this toolbox to adapt their prompts to specific challenges.\nIterative Optimization Sahoo et al. (2024) and Schulhoff et al. (2024) present iterative refinement, through prompting techniques, as a crucial approach for improving prompts. This involves adjusting and testing prompts in a feedback loop to enhance their effectiveness. Iterative optimization allows practitioners to fine-tune prompts based on model responses, ensuring greater alignment with task objectives.\nPlaceholders and Delimiters Placeholders act as flexible components that can be replaced with context-specific information, while delimiters help segment instructions, improving clarity and reducing ambiguity. Both can be used to create dynamic and adaptable prompts (cf. White et al. 2023)."}, {"title": "4.6 Tooling", "content": "The Tooling category offers practical support for designing and applying prompts efficiently. Tools and platforms simplify workflows, enhance accessibility, and enable the scalable deployment of prompt engineering techniques.\nLLM Apps Apps like the ChatGPT App enable faster prompt creation through voice input, making interactions more efficient and accessible. This feature reduces typing effort, enhances usability on-the-go, and supports diverse users, streamlining the prompt engineering process for dynamic or time-sensitive tasks.\nPrompting Platforms Platforms like PromptPerfect allow users to design, test, and optimize prompts interactively. These tools often include analytics for assessing prompt performance and making informed adjustments.\nPrompt Libraries Pre-designed templates and reusable prompts, discussed by White et al. (2023), provide a valuable starting point for practitioners. Libraries save time and ensure consistency by offering solutions for common tasks. Some platforms either offer prompts for purchase (e.g., PromptBase), while others focus on sharing prompts for free (e.g., PromptHero).\nBrowser Extensions Providing direct integration into web clients, browser extensions, like Text Blaze and Prompt Perfect, allow users to experiment with prompts in real-time on websites.\nLLM Arenas LLM Arenas, like Chatbot Arena, offer platforms to test and compare AI models, providing insights into their performance and capabilities. These arenas help users refine prompts and stay updated with the latest advancements in LLM technology.\nCustom GPTs for Specific Purposes Chen et al. (2024) mention GPTs as plugins in ChatGPT. Customized GPTs such as Prompt Perfect or ScholarGPT are tailored LLMs optimized for specialized applications or industries. These customized versions are also able to leverage additional data through Application Programming Interfaces (APIs) or are given additional context through text or PDFs for specific objectives, making them highly effective for specialized tasks.\nCustomized LLMs and company-wide use\nDeveloping company-specific custom GPTs takes customization a step further by integrating organizational knowledge, values, and workflows into a LLM. These models have been given additional context or are even fine-tuned on internal data, leveraging documents and APIs, and are primed with internal prompts to ensure alignment with company standards and improve operational efficiency. Additionally, some LLM providers offer a sandboxed environment for enterprises, ensuring that entered data will not be used to train future publicly available models.\nIntegration of LLMs via API into application systems\nAPIs facilitate seamless integration of LLMs into existing systems, enabling automated prompt generation and application."}, {"title": "5 Limitations, Outlook and Conclusion", "content": "This chapter outlines the limitations of the current study, explores potential future directions for research and application, and concludes by emphasizing the significance of the Prompt Canvas as a foundational tool for the evolving field of prompt engineering. It provides a critical reflection on the scope of the work, its adaptability to emerging trends, and its role in bridging research and practice."}, {"title": "5.1 Limitations", "content": "As prompt engineering is not a one-size-fits-all discipline, different tasks and domains may require tailored approaches and techniques. Yet, a canvas can be easily customized to include domain-specific elements, such as ethical considerations for healthcare or creative constraints for marketing. This adaptability ensures that the canvas remains relevant and useful across diverse use cases. The modular structure allows practitioners to customize techniques for specific tasks or domains, improving relevance and scalability.\nThe effectiveness of this canvas requires validation through both quantitative and qualitative research methodologies. Recognizing the strong demand for a practical guide in the field of prompt engineering, this publication aims to serve as a starting point to initiate and foster discussion on the topic. Additionally, a research design to evaluate the utility of the canvas is already under development.\nThis work focuses primarily on text-to-text modalities. Although this modality should already cover a wide range of applications, there are other modalities such as image, audio, video or image-as-text (cf. Schulhoff et al. 2024) that are not highlighted in this study. At the same time, many techniques mentioned above are not designed exclusively for the text-to-text modality, e.g. iterative prompting. Furthermore, this work focused primarily on the design of individual prompts. Prompting techniques that use agents were thematically separated out in the analysis and synthesis. It is assumed that they can play another important role in further improving the output quality. At the same time, this work focused on findings for users of LLMs in the private and business environment. Finally, it is important to emphasize that this work does not explore potential risks associated with the use of LLMs. These risks include biases, handling of sensitive information, copyright violations, or the significant consumption of resources."}, {"title": "5.2 Outlook", "content": "The Prompt Canvas serves as a foundational tool, offering a shared framework for the field of prompt engineering. It is intended not only for practical application but also to foster dialogue about which techniques are most relevant and sustainable. By doing so, the canvas encourages discussion and guides research in evaluating whether emerging developments should be incorporated into its framework. Given the dynamic and rapidly evolving nature of the discipline, it is important to view the Prompt Canvas not as a static product but as a living document that reflects the current state of practice. For instance, if prompting techniques are more deeply integrated into LLMs in the future through prompt tuning and automated prompts, one could argue that some prompt techniques may become less important. Advancing models, such as OpenAI's ol model series, already incorporate the Chain-of-Thought technique, enabling it to perform complex reasoning by generating intermediate steps before arriving at a final answer."}, {"title": "5.3 Conclusion", "content": "This paper introduces the Prompt Canvas as a unified framework aimed at consolidating the diverse and fragmented techniques of prompt engineering into an accessible and practical tool for practitioners. Grounded in an extensive literature review and informed by established methodologies, the Prompt Canvas addresses a need for a comprehensive and systematic approach to designing effective prompts for large language models. By mapping key techniques, such as role-based prompting, Chain-of-Thought reasoning, and iterative refinement, onto a structured canvas, this work provides a valuable resource that bridges the gap between academic research and practical application. Future research is encouraged to expand the framework to address these evolving challenges, ensuring its continued relevance and utility across diverse domains."}]}