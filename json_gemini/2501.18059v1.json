{"title": "LEARNING THE OPTIMAL STOPPING FOR\nEARLY CLASSIFICATION WITHIN FINITE HORIZONS\nVIA SEQUENTIAL PROBABILITY RATIO TEST", "authors": ["Akinori F. Ebihara", "Taiki Miyagawa", "Kazuyuki Sakurai", "Hitoshi Imaoka"], "abstract": "Time-sensitive machine learning benefits from Sequential Probability Ratio Test\n(SPRT), which provides an optimal stopping time for early classification of time\nseries. However, in finite horizon scenarios, where input lengths are finite, deter-\nmining the optimal stopping rule becomes computationally intensive due to the\nneed for backward induction, limiting practical applicability. We thus introduce\nFIRMBOUND, an SPRT-based framework that efficiently estimates the solution to\nbackward induction from training data, bridging the gap between optimal stop-\nping theory and real-world deployment. It employs density ratio estimation and\nconvex function learning to provide statistically consistent estimators for suffi-\ncient statistic and conditional expectation, both essential for solving backward\ninduction; consequently, FIRMBOUND minimizes Bayes risk to reach optimality.\nAdditionally, we present a faster alternative using Gaussian process regression,\nwhich significantly reduces training time while retaining low deployment overhead,\nalbeit with potential compromise in statistical consistency. Experiments across\nindependent and identically distributed (i.i.d.), non-i.i.d., binary, multiclass, syn-\nthetic, and real-world datasets show that FIRMBOUND achieves optimalities in\nthe sense of Bayes risk and speed-accuracy tradeoff. Furthermore, it advances\nthe tradeoff boundary toward optimality when possible and reduces decision-\ntime variance, ensuring reliable decision-making. Code is publicly available at\nhttps://github.com/Akinori-F-Ebihara/FIRMBOUND.", "sections": [{"title": "1 INTRODUCTION", "content": "Sequential Probability Ratio Test (SPRT) (Wald, 1945) offers a theoretically optimal framework\nfor early classification of time series (ECTS) (Xing et al., 2009). ECTS is a task to sequentially\nobserve an input time series and classify it as early and accurately as possible, balancing speed\nand accuracy (Gupta et al., 2020; Mori et al., 2016). This is vital in real-world scenarios with high\nsampling costs or where delays can have severe implications: e.g., medical diagnosis (Evans et al.,\n2015; Griffin & Moorman, 2001; Vats & Chan, 2016), stock crisis identification (Ghalwash et al.,\n2014), and autonomous driving (Don\u00e1 et al., 2019). While the multi-objective nature of ECTS\npresents challenges, SPRT, with log class-likelihood ratios (LLRs), is optimal for binary i.i.d. samples\nand asymptotically optimal for multi-class, non-i.i.d. time series. SPRT's optimality ensures decisions\nwithin the shortest possible time with a controlled error rate (Tartakovsky, 1998; 1999).\nA key limitation of SPRT in real-world applications is the finite horizon (Grinold, 1977; Xiong et al.,\n2022): the deadline for classification. While the original SPRT assumes an indefinite sampling\nperiod to reach its decision threshold (Tartakovsky et al., 2014), practical scenarios often demand\nearlier decisions. For instance, detecting a face spoofing attack at a biometric checkpoint requires\nclassification before the subject passes through (Labati et al., 2016). This constraint frequently results\nin suboptimal performance, as early thresholds may cause either delayed or rushed decisions (Fig. 1a).\nFortunately, the optimal decision boundary for the finite horizon can be derived by solving backward\ninduction, a recursive formula progressing from the horizon to the start of the time series (Chow et al.,\n1991; Peskir & Shiryaev, 2006). It optimizes the boundary by minimizing Bayes risk, or average a"}, {"title": "2 PRELIMINARIES: NOTATIONS AND SPRT", "content": "We provide informal definitions here due to page limitations. Detailed mathematical foundations\nare provided in App. A and Tartakovsky et al. (2014). Let $X^{(1,t)} := \\{x^{(t')}\\}_{t'=1}^{t}$ and $y \\in [K] :=$\n$\\{1, ... K\\}$ be random variables that represent an input sequence with length $t \\in [T]$ and its class\nlabel, respectively, where $x^{(t')} \\in \\mathbb{R}^{d_{feat}}$ is a feature vector, and $T \\in \\mathbb{N}$ is the fixed maximum\nlength of sequences, or the finite horizon. $X^{(1,t)}$ and $y$ follow the joint density $p(X^{(1,t)}, y)$. Their\nsamples denoted by $X_m^{(1,t)} := \\{x_m^{(t')}\\}_{t'=1}^{t}$ and $y_m \\in [K] := \\{1, ... K\\}$ consist of a dataset, where\n$m \\in [M]$, and $M \\in \\mathbb{N}$ is the dataset size. The log-likelihood ratio (LLR) contrasting class $k \\in [K]$\nand $l \\in [K]$ is defined as $\\lambda_{kl}(T) := \\Lambda_{kl}(X^{(1,T)}) := \\log(p(X^{(1,T)}|y = k)/p(X^{(1,T)}|y = l))$.\nThe posterior of class $k \\in [K]$ given $X^{(1,t)}$ is denoted by $\\pi_k(X^{(1,t)}) := p(y = k|X^{(1,t)})$. Let\n$d_t : X^{(1,t)} \\rightarrow d_t(X^{(1,t)}) \\in [K]$ and $\\tau : X^{(1,T)} \\rightarrow \\tau(X^{(1,T)}) \\in [T]$ denote the terminal decision\nrule (i.e., a class predictor) and stopping time (i.e., decision time or hitting time) of the input sequence,\nrespectively. The terminal decision rule may not depend on $t$, in which case we omit the subscript $t$.\nThe stopping time may not require the whole sequence $X^{(1,T)}$ and may be able to calculate from the\nfirst $t$ samples $X^{(1,t)}$, depending its algorithm. Our task is to construct the terminal decision rule\n$\\{d_t\\}_{t\\in[T]}$, which will turn out to be time-independent, and the stopping time $\\tau$ that are \u201coptimal\u201d and\ncan be computed efficiently.\nOur model is based on SPRT:\nDefinition 2.1 (SPRT). Given the thresholds $a_k^{(t)} \\in \\mathbb{R}$ ($k \\in [K]$ and $t \\in [T]$) for LLRs of input\nsequences, SPRT is defined as a tuple of a time-independent terminal decision rule and stopping time,"}, {"title": "3 FIRMBOUND", "content": "Unfortunately, solving and deploying Eqs.5\u20137 in real-world scenarios presents significant challenges.\nFirst, these equations lack closed-form solutions. A naive numerical computation, such as Monte\nCarlo integration, would be possible, but it suffers from the curse of dimensionality (K can be > 100,\nrequiring an exponentially large number of samples for convergence) (see also App. D). Second,\nobtaining a well-calibrated sufficient statistic $I_t$ is challenging. Although computing softmax logits\nas class posteriors is common in classification problems (He et al., 2016a;b; Krizhevsky et al., 2012;\nLeCun et al., 1998), high-dimensional classifiers often produce overconfident or miscalibrated outputs\n(Guo et al., 2017; Melotti et al., 2022; M\u00fcller et al., 2019; Mukhoti et al., 2020).\nWe address these challenges by transforming the backward induction into a pair of estimation\nproblems and providing statistically consistent estimators (Secs. 3.1 & 3.2). Our proposed model,\nFIRMBOUND, is then proved to be statistically consistent with the Bayes optimal solution (Thm. 3.2)."}, {"title": "3.1 ESTIMATING THE CONDITIONAL EXPECTATION", "content": "The first key idea is to transform the computation of the conditional expectation in the backward\ninduction equation into a regression problem. An important observation is that the conditional\nexpectation function in Eq. 5 is concave (Jarrett & van der Schaar, 2020; Tartakovsky et al., 2014):\nfor all\nEquipped with the concavity, we propose to build a consistent estimator of the Eq. 5 though convex\nfunction learning (CFL).\nCFL aims to build a statistically consistent estimator of a convex function from noisy data\npoints, assuming the target function is inherently convex (Argyriou et al., 2008; Bach, 2010; Bartlett\net al., 2005; Boyd & Vandenberghe, 2010; Mendelson, 2004). Assume that we have estimates of the\nsufficient statistic $I_t$ for all $t \\in [T]$ estimated on a given training dataset $\\{(X_m^{(1,T)}, y_m)\\}_{m=1}^M$ via the\nalgorithm given in Sec. 3.2. Then, our task toward solving the backward induction equation (Eq. 5\u20137)\nis to estimate $G_t^\\dagger(I_t)$ and $\\bar{G}_t(I_t)$ for all $t \\in [T]$. $G_t^\\dagger$ can be computed from the estimated sufficient\nstatistic via Eq. 6. Thus, we focus on the continuation risk $\\bar{G}_t(I_t) = \\mathbb{E} [G_{\\min}(I_{t+1})|I_t] + c$."}, {"title": "3.2 DENSITY RATIO ESTIMATION (DRE) FOR ECTS", "content": "Our remaining task is to estimate the sufficient statistic $I_t$ for all $t \\in [T]$, the second estimation\nproblem mentioned at the beginning of Sec. 3. A simple approach to this end is to estimate LLRs\nvia a sequential density ratio estimation algorithm. It enhances precision by estimating the ratio of\nprobabilities directly, rather than estimating each probability independently, thus reducing degrees of\nfreedom (Belghazi et al., 2018; Gutmann & Hyv\u00e4rinen, 2012; Hjelm et al., 2019; Liu et al., 2018a;\nMoustakides & Basioti, 2019; Oord et al., 2018; Sugiyama et al., 2010; 2008; 2012). Specifically, we\nemploy SPRT-TANDEM algorithm (Ebihara et al., 2021; Miyagawa & Ebihara, 2021; Ebihara et al.,\n2023), which involves a consistent loss function, named Log-Sum-Exp Loss (LSEL):\nwhere $w \\in \\mathbb{R}^{d}$ is the trainable parameters, e.g., the weights of a neural network, $I_k := \\{i \\in [M] |$\ny_i = k\\}$ is the index set of class $k$, $M_k := |I_k|$ is the size of $I_k$, and $\\hat{\\Lambda}_{kl}(w, X^{(1,t)})$ is the estimated\nLLR parameterized by $w$. By minimizing LSEL, the estimated LLRs approaches the true LLRs\nas $M \\rightarrow\\infty$ (Miyagawa & Ebihara, 2021); i.e, LSEL is consistent. We provide further details of\nSPRT-TANDEM in App. I.\nFinally, integrating CFL and LSEL and solving the backward induction equation, we establish that\nFIRMBOUND is statistically consistent."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "These experiments are designed for a fair comparison with baseline models without exploring\nall possible configurations, as such variations would not alter our study's conclusion. To ensure\nfairness, the same feature extractor and feature vector size $d_{feat}$ are used across all models. All"}, {"title": "5 CONCLUSION", "content": "With two statistically consistent estimators for backward induction and the sufficient statistic,\nFIRMBOUND delineates stable Pareto fronts across diverse datasets. Unlike existing ECTS mod-\nels, which lack theoretical guarantees and are sensitive to hyperparameters and datasets (Fig.6d-f),\nFIRMBOUND consistently achieves optimal performance with reduced hitting time variance, ap-\nproaching optimal performance for ECTS in real-world scenarios. For further discussion, see App.F."}, {"title": "A MATHEMATICAL FOUNDATIONS", "content": "In the main text, we introduced concise notations to avoid delving into unnecessarily technical details.\nHere, we provide more rigorous definitions. See Tartakovsky et al. (2014) for details."}, {"title": "A.1 PROBABILITY MEASURE AND DATA RANDOMNESS", "content": "We consider a standard probability space $(\\Omega, \\mathcal{F}, P)$, where $\\Omega$ is a sample space, $\\mathcal{F} \\subset \\mathcal{P}(\\Omega)$ is a\n$\\sigma$-algebra of $\\Omega$, where $\\mathcal{P}(\\Omega)$ denotes the power set of $\\Omega$, and $P$ is a probability measure satisfying\nKolmogorov's axioms:\n* $P(\\Omega) = 1$,\n* $P(A) \\geq 0$ for any $A \\in \\mathcal{F}$,\n* $P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} P(A_i)$ for any countable collection $\\{A_i\\}_{i=1}^{\\infty} \\subset \\mathcal{F}$ of pairwise disjoint\nsets (i.e., $A_i \\cap A_j = \\emptyset$ for $i \\neq j$).\nA function $X = X(\\omega)$ defined on the space $(\\Omega, \\mathcal{F})$ (with values in $\\mathbb{R}^{d_{feat}}$ ($d_{feat} \\in \\mathbb{N}$) in our paper)\nis called random variable if it is $\\mathcal{F}$-measurable. The probability that a random variable $X$ takes values\nin a set $B \\subset \\mathbb{R}^{d_{feat}}$ is defined as $P(X \\in B) := P(X^{-1}(B))$, where $X^{-1}$ is the preimage of $X$.\nLet $\\{\\mathcal{F}_t\\}_{t>0}$ be a filtration, which is a non-decreasing sequence of sub-$ \\sigma$-algebras of $\\mathcal{F}$; i.e., $\\mathcal{F}_s \\subset$\n$\\mathcal{F}_t \\subset \\mathcal{F}$ for all $0<s<t$. Each element of the filtration can be interpreted as the available\ninformation at a given point $t$. The tuple $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_t\\}_{t>0}, P)$ is called a filtered probability space.\nIn our problem setting, $X_m^{(1,T)}$ in the dataset $S = \\{X_m^{(1,T)}\\}_{m=1}^M$ represents a sequence of observations\nfor the $m$-th sample, which is treated as a stochastic process or as a realization of the stochastic\nprocess $X^{(1,T)}$ interchangeably in our paper. $y_m$ is the fixed class label associated with $X_m^{(1,T)}$."}, {"title": "A.2 DECISION RULE, TERMINAL DECISION, AND STOPPING RULE", "content": "The decision rule $\\delta$ is defined as the pair $(d_t, \\tau)$, where $d_t$ is the terminal decision rule at time $t$\n($t \\in \\{1, ...T\\}$) and $\\tau \\in \\{1, ..., T\\}$ is the stopping time. We provide their definitions below.\nThe task of hypothesis testing as a time series classification involves identifying which one of\nthe densities $p_1,... p_K$ the sequence $X^{(1,T)}$ is sampled from. Formally, this tests the hypotheses\n$\\mathcal{H}_1: y = 1,... \\mathcal{H}_K : y = K$.\nThe decision function or test for a stochastic process $X^{(1,T)}$ is denoted by $d_t(X^{(1,T)}) : \\Omega \\rightarrow$\n$\\{1, ..., K\\}$. For each realization of $X^{(1,T)}$, we identify $d_t$ as a map $d_t : \\mathbb{R}^{d_{feat} \\times T} \\rightarrow \\{1,..., K\\}$,\ni.e., $X^{(1,T)}(\\omega) \\rightarrow y$, where $y \\in \\{1, ..., K\\}$. For simplicity, we write $d_t$ instead of $d_t(X^{(1,T)})$.\nThe stopping time $\\tau$ of $X^{(1,T)}$ with respect to a filtration $\\{\\mathcal{F}_t\\}_{t>1}$ is defined as $\\tau := \\tau(X^{(1,T)}) :$\n$\\Omega \\rightarrow \\mathbb{R}_{\\geq 0}$ such that $\\{\\omega \\in \\Omega |\\tau(\\omega) \\leq t\\} \\in \\mathcal{F}_t$.\nAccordingly, for a fixed $T \\in \\mathbb{N}$ and $y \\in \\{1, . . ., K\\}$, the set $\\{d_t = y\\}$ represents the time-series data\nfor which the decision function accepts the hypothesis $\\mathcal{H}_i (i \\in \\{1, ..., K\\})$ with a finite stopping\ntime. Specifically, $\\{d_t = y\\} = \\{\\omega \\in \\Omega |d_t(X^{(1,T)})(\\omega) = y, \\tau(X^{(1,T)})(\\omega) < \\infty\\}$."}, {"title": "BSEQUENTIAL PROBABILITY RATIO TEST AND ITS OPTIMALITY", "content": "Our work centers around the optimality of Wald's SPRT. Below, we briefly review the optimality\nstatements for both i.i.d. and non-i.i.d., multiclass classification scenarios. Note that the assumption\nof increasing LLRs is not applicable under the finite horizon setting discussed in the main manuscript.\nLet the time-series data points $x^{(t)}, t = 1,2, ...$ be i.i.d. with\ndensity $f_0$ under $\\mathcal{H}_0$ and with density $f_1$ under $\\mathcal{H}_1$, where $f_0 \\neq f_1$. Let $\\alpha_0 > 0$ and $\\alpha_1 > 0$ be\nfixed constants such that $\\alpha_0 + \\alpha_1 < 1$. If the thresholds \u2013a\u2080 and a\u2081 satisfies $a_0^*({a_0, a_1}) = a_0$ and\n, then SPRT $\\delta^* = (d^*,\\tau^*)$ satisfies\ninf\\n_{\\delta=(d,\\tau)\\in\\mathcal{C}(\\alpha_0,\\alpha_1)} \\mathbb{E}[\\tau|\\mathcal{H}_0] = \\mathbb{E}[\\tau^*|\\mathcal{H}_0] and inf\\n_{\\delta=(d,\\tau)\\in\\mathcal{C}(\\alpha_0,\\alpha_1)} \\mathbb{E}[\\tau|\\mathcal{H}_1] = \\mathbb{E}[\\tau^*|\\mathcal{H}_1] (14)\nA similar optimality also holds for continuous-time processes (Irle & Schmitz, 1984). Thus, SPRT\nterminates at the earliest expected stopping time compared to any other decision rule achieving the\nsame or lower error rates\u2014establishing the optimality of SPRT.\nThm. B.1 demonstrates that, given user-defined thresholds, SPRT achieves the optimal mean hitting\ntime. Additionally, these thresholds determine the error rates (Wald, 1947). Therefore, SPRT can\nminimize the required number of samples while maintaining desired upper bounds on false positive\nand false negative rates.\nIntuitively, Thm.\nB.2 (Tartakovsky et al., 2014) suggests that if the LLRs $\\Lambda_{kl}$ increase as samples accumulate, SPRT\nalgorithm achieves asymptotic optimality. In this condition, the moments of the stopping time are\nminimized up to order $r$ for a specified classification error rate."}, {"title": "CLLRS AND POSTERIORS AS SUFFICIENT STATISTIC", "content": "The backward induction equation (Eq.7) depends on a sufficient statistic, which encapsulates all\nnecessary information for decision-making. In hypothesis testing, true LLRs or posterior prob-\nabilities suffice to make decisions with a predefined error rate (Wald, 1947), thus both LLRs\nand posteriors qualify as sufficient statistics. The conversion is expressed by $\\pi_k(X^{(1,t)}) =$\n$1/(1 + \\Sigma_{i \\neq k} \\chi_{ik} \\exp(\\chi_{ik}(X^{(1,t)})))$, where $\\chi_{kl} := p(y = k)/p(y = l)$ represents the prior ra-\ntio. A formal definition of a sufficient statistic is available in Tartakovsky et al. (2014), as follows:\nDefinition C.1 (Sufficient Statistic). A sequence $\\{I_t\\}_{t>1}$ is defined to be sufficient statistic for the\nsequential decision problem if it satisfies the following conditions:\n1. Transitivity: The sequence is transitive, meaning there exists a function $\\phi_n(\\cdot)$ such that\n$I_{t+1} = \\phi_t(I_t, x^{(t+1)})$, almost surely, for $n \\geq 1$.\n2. Equality of Conditional Probability Density Function (pdf): The conditional pdf of $X_{t+1}$\ngiven the past observations $X^t$ can be expressed solely in terms of $I_t$:\n$P_{t+1}(x^{(t+1)} | X^{(1,t)}) = P_{t+1}(x^{(t+1)} | I_t)$, almost surely, for $t > 1$.\n3. Equality of Risks: The A Posteriori Risk (APR) when using the sufficient statistic $I_t$ equals\nthe APR calculated directly from the observations:\nAPR($X^{(1,t)}$) = APR($I_t$), almost surely, for $n \\geq 1$.\nNote that the online DRE algorithm SPRT-TANDEM is transitive, providing consistent estimation of\nthe sufficient statistic.\nWhich statistic to use, LLRs or posteriors? In principle, the CFL algorithm can handle either\nLLRs or posteriors as the sufficient statistic for calculating the conditional expectation. Our ex-\nperiments confirm that both LLRs and posteriors yield equivalent results; however, we opt to use\nposteriors to reduce input dimensionality.\nConversely, our use of GP regression is predicated on the assumption that the risk distribution is jointly\nGaussian, which motivates us to use LLRs as the sufficient statistic. Nonetheless, an experiment with\nthe two-class Gaussian dataset confirms that GP regression provides equivalent results regardless of\nthe type of statistic used (Fig. 9)."}, {"title": "D COMPUTATIONAL COMPLEXITY OF FIRMBOUND AND SAMPLING METHOD", "content": "Here, we provide a detailed comparison of the computational complexity for the inference stage of\nboth the direct estimation approach and the Monte Carlo Integration with Kernel Density Estimation\n(KDE) approach."}, {"title": "D.1 DIRECT ESTIMATION APPROACH (FIRMBOUND)", "content": "The direct estimation approach uses the following function to evaluate the conditional expectation:\nIn this function:\n* X is the input tensor of size [B, K], where B is the batch size and K is the number of\nclasses.\n* self.aand self.y_hat are parameter tensors of size [I, K] and [I], respectively, where\nIM is the subset data number.\nThe computational complexity for each step in the inference stage is as follows:\n1. Matrix Multiplication: The operation torch.matmul(X, self.a.T) has a com-\nplexity of O(BKI). Broadcasting and Addition: The operation torch.matmul(X,\nself.a.T) + self.y_hat.reshape(1, -1) involves broadcasting and addition,\nwhich has a complexity of O(BI).\n2. Maximum Value Selection: The operation torch.max(..., dim=1) finds the maxi-\nmum value along the specified dimension, which has a complexity of O(B \u00b7 I).\nThus, the total computational complexity for the inference stage of the direct estimation approach is\ndominated by the matrix multiplication step, resulting in:\nO(\u0392\u00b7K\u00b7 \u0399)"}, {"title": "D.2 MONTE CARLO INTEGRATION WITH KDE APPROACH", "content": "The Monte Carlo Integration with KDE approach involves the following steps for the inference stage:\n1. Generate S samples from the conditional density p(St+1 | It) using KDE.\n2. Evaluate the function Gt+1(It+1) for each sample.\n3. Compute the average to estimate the conditional expectation.\nAssuming:\n* B is the batch size (number of input samples in It).\n* K is the dimensionality (number of classes).\n* M is the total number of data points.\n* S is the number of Monte Carlo samples.\nIn the Monte Carlo Integration with KDE approach, the dimensionality K affects the number of\nMonte Carlo samples S required for convergence. Let S(K) denote the number of samples as a\nfunction of K, typically increasing with K. The computational complexity for each step is as follows:"}, {"title": "D.3 COMPARISON", "content": "The direct estimation approach has a computational complexity of O(B\u00b7K\u00b7 I) for the inference stage,\nwhile the Monte Carlo Integration with KDE approach has a complexity of O(B \u00b7 S(K) \u00b7 M \u00b7 K).\nGiven that I < M and considering that higher dimensionality (K) increases the number of samples\nrequired for convergence (S(K)), the direct estimation approach is significantly more efficient in\nterms of computational complexity during inference. This efficiency is particularly advantageous for\nreal-time applications and large-scale datasets."}, {"title": "G LAGRANGIAN FUNCTION FOR CONVEX FUNCTION LEARNING", "content": "In this section, we review the 2-block ADMM algorithm (Siahkamari et al., 2022) used for solving\nthe convex regression problem (Eq. 12). We solve the following noisy convex regression problem\nwith regularization:\nwhere x \u2208 Rd, and \u00c0 is a hyperparameter affecting convergence. Note that regression labels yi are\nnoisy; i.e., they have bounded random discrepancies from the true label.\nThe 2-block ADMM solves this problem by using piecewise linear functions:"}, {"title": "H STOCHASTIC VARIATIONAL ELBO MAXIMIZATION", "content": "t+1\nThe problem of evaluating E[Gmin|t] at each time step is formulated below. Given any set of\nM values at time t {It(X))}M=1, we assume that the joint distribution of the random variables\n{f(It(X)))}=1 are multivariate Gaussian distributions. Inducing points Z = {z}{=1 with\nI \u226a M are randomly sampled from the training dataset {It(X))}M_1. The prior distribution of\nthe function is defined as:"}, {"title": "I SPRT-TANDEM", "content": "SPRT-TANDEM is a sequential DRE algorithm specifically designed for conducting SPRT on real-\nworld sequential datasets (Ebihara et al., 2021). It employs a feature vector extractor followed by a\ntemporal integrator (TI, Fig. 11), utilizing either recurrent networks or transformers as TIs. In our\nexperiments, both LSTM (Hochreiter & Schmidhuber, 1997) and Transformer (Vaswani et al., 2017)\nare implemented. The TI outputs class posteriors, which are converted to LLRs using the TANDEM\nformula (Thm. I.1) in a transitive manner. Initially developed for binary-class, SPRT-TANDEM has\nbeen adapted for multiclass classification (Miyagawa & Ebihara, 2021), incorporating a statistically\nconsistent LLR estimator, LSEL (Eq. 13). The LLR saturation problem, notably significant when the\nabsolute value of the ground-truth LLR exceeds 100 nats, has also been addressed (Ebihara et al.,\n2023).\nA distinctive feature of SPRT-TANDEM is its absence of a dedicated loss function for promoting\nearliness, despite its design for ECTS. This is because the precision in estimating the sufficient statistic\n(i.e., LLR) ensures the minimum required data sampling to achieve a predefined error rate. Thus,\nSPRT-TANDEM is trained using LSEL (Eq. 13) and multiplet cross-entropy loss (MCE, Def. I.1),\nwithout a specific loss function for earliness."}, {"title": "JFIRMBOUND IS STATISTICALLY CONSISTENT", "content": "We provide the full assumptions, the formal statement, and the proof of Thm. 3.2."}, {"title": "J.1 ASSUMPTIONS", "content": "Most of the necessary assumptions are given in the following Lems. J.1\u2013J.4 and Thm. 2.1.\nLemma J.1 (CFL is statistically consistent (Prop. 1 in (Siahkamari et al., 2022))). See App. G for\nnotations. With the appropriate choice of which requires knowledge of the bound on f and n \u2265 d,\nit holds that with probability at least 1 \u2013 8 over the data, the estimator f of Eq. 18 has excess risk\nupper bounded by"}, {"title": "J.2 FORMAL STATEMENT", "content": "Now, we provide the formal statement of Thm. 3.2:\nSuppose that all the assumptions mentioned\nin App. J.1 are satisfied. Suppose that we have the sufficient statistics estimated with LSEL on a\ndataset with size M. Suppose also that we have the continuation risk estimated on with CFL the same\ndataset. Then, with arbitrary precision, we can solve the backward induction equation in Thm. 2.1,\nwhich yields the Bayes optimal terminal decision rule d* and stopping time *, with high probability\nover the data and as M\u2192\u221e."}, {"title": "J.3 PROOF", "content": "We provide the proof of Thm. J.1.\nProof. We first show that the CFL combined with the density ratio estimation (DRE) with LSEL\nyields a consistent estimate of function Gt."}, {"title": "K EXPERIMENTAL DETAILS AND SUPPLEMENTARY RESULTS", "content": "Throughout the experiments, Optuna (Akiba et al., 2019) with the default algorithm, Tree-structured\nParzen Estimator (TPE) (Bergstra et al., 2011), is used to find the best hyperparameter combinations\nfrom the predefined search space. TPE is a Bayesian optimization algorithm that models beliefs\nabout the optimal hyperparameters using Parzen Estimation and optimizes the search process using\na tree-like graph. The training procedure described below is common across all datasets unless\nspecified otherwise."}, {"title": "L ON HYPERPARAMETER SENSITIVITY OF FIRMBOUND", "content": "Our algorithm either requires minimal hyperparameter tuning or can be easily tuned on the training\ndataset. Below is a list of major hyperparameters for the two approaches:\n* Lambda\n* Number of training data for tuning\n* Number of training data for fitting\n* Tuning trials\n* Epochs\n* Kernel type\n* Number of inducing points\n* Batch size\n* Learning rate\n* Optimizer\n* Epochs\nThe most critical hyperparameter is the lambda parameter (do not confuse with the LLR or the baseline\nmodel EARLIEST's hyperparameter) in CFL, which controls the flexibility of the fitting curves. As\ndescribed in the Sec. 4, we keep the other hyperparameter settings consistent across all datasets,\nincluding i.i.d., non-i.i.d., artificial, and real-world. Our experiments show that FIRMBOUND\nreliably minimizes the average a posteriori risk (AAPR) to delineate the Pareto front.\nAs an additional experiment for hyperparameter sensitivity, we tested GP approach with varying\nkernel and number of inducing points using the two-class Gaussian dataset (Fig. 13). The number of\ninducing points is varied from the default 200 to 50 and 1000, while keeping the original Radial Basis\nFunction (RBF) kernel. Alternatively, the number of inducing points is fixed at 200, and the Mat\u00e9rn\nkernel is used instead of RBF. Mat\u00e9rn kernel is a generalization of RBF kernel with a parameter v\ncontrolls its smoothness. As v approaches to infinity, Mat\u00e9rn kernel converges to the RBF kernel.\nIn Fig. 13, two values, 1.5 and 2.5, are used as the parameter v. Cost parameter is c = L/T. The\nresults are robust against any of the above hyperparameters, while Mat\u00e9rn kernel slightly off from the\noptimality (also see Fig. 5a and 6a of the main manuscript)."}, {"title": "MAAPR ON BASELINE MODELS", "content": "In this section, we demonstrate that ill-calibrated ECTS models can misleadingly exhibit small AAPRs.\nLSTMms, EARLIEST, and TCN-Transformer models were trained on the two-class Gaussian dataset\nand evaluated using two performance criteria: AAPR and the SAT curve. As shown in Fig 14a,\nwhile LSTMms achieves a lower AAPR than FIRMBOUND and SPRT-TANDEM, which maintain\nwell-calibrated statistics, it is outperformed by them in terms of the SAT curve (i.e., ECTS results).\nThis discrepancy arises from overconfidence, which inflates the statistic beyond the calibrated level.\nConsequently, AAPR alone does not reliably predict SAT performance when using an ill-calibrated\nstatistic. Notably, FIRMBOUND records the minimal AAPR across all models."}, {"title": "N PARAMETER SPACE OF LAND C"}]}