{"title": "APPLICATION OF VISION-LANGUAGE MODEL TO PEDESTRIANS BEHAVIOR AND SCENE UNDERSTANDING IN AUTONOMOUS DRIVING", "authors": ["Haoxiang Gao", "Yu Zhao"], "abstract": "Autonomous driving (AD) has experienced significant improvements in recent years and achieved promising 3D detection, classification, and localization results. However, many challenges remain, e.g. semantic understanding of pedestrians' behaviors, and downstream handling for pedestrian interactions. Recent studies in applications of Large Language Models (LLM) and Vision-Language Models (VLM) have achieved promising results in scene understanding and high-level maneuver planning in diverse traffic scenarios. However, deploying the billion-parameter LLMs to vehicles requires significant computation and memory resources. In this paper, we analyzed effective knowledge distillation of LLM semantic labels to smaller Vision networks, which can be used for the semantic representation of complex scenes for downstream decision-making for planning and control.", "sections": [{"title": "1 Introduction", "content": "Vision-language foundation models[1][2][3] are at the forefront of multi-modal AI, enabling advanced reasoning and spacial understanding[4] capabilities by integrating visual and text data. These models are designed to perform a variety of tasks, such as image captioning, visual question answering (VQA), and cross-modal retrieval, enhancing the capabilities of virtual assistants, content creation tools, and accessibility applications.\nVisual reasoning and understanding is crucial in improving autonomous driving by predicting pedestrian behaviors and understanding the dynamic environments, which ensure the autonomous vehicle makes safe maneuvers. Autonomous driving still faces challenges in handling long-tail safety-critical scenarios in pedestrian interactions, e.g. pedestrians sitting on the roadside, jaywalking, and waiting to cross the crosswalk.\nEarlier work on pose estimation techniques[5][6][7], such as those using deep learning models to analyze pedestrian posture and movements, provides insights into their intentions, such as crossing the street or waiting on the sidewalk. This field has advanced significantly with the advent of deep learning, such as Convolutional Neural Networks (CNNs) and PointNet, which extract features from images and 3D data to predict key points on the human body.\nFoundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs) leverage pre-trained knowledge from the web data to generate human-like responses in natural language and show emergent abilities of reasoning and few-shot learning. In addition to their success in text and image generation, their embeddings combined with other models and algorithms in other domains, achieving significant advancement in various applications, such as e-commerce[8], recommendation systems[9], anomaly detection, and financial forecasting. Large Language Models (LLMs) have also shown promising results in autonomous driving and pedestrian understanding tasks such as behavior prediction and appearance description[10][11][12], a critical gap remains in their practical application for autonomous driving. Current approaches lack the capability to:\n\u2022 Train Domain-Specific Models: Existing LLMs and VLMs are often trained on general-purpose datasets, limiting their effectiveness in the specialized context of autonomous driving. Fine-tuning or adapting these models for the nuances of pedestrian behavior in traffic scenarios remains a challenge."}, {"title": "2 Pedestrian Semantic Attributes Taxonomy", "content": "The existing autonomous driving datasets often fall short in providing comprehensive class labels for pedestrian behavior and scene understanding. Many datasets include only basic attributes like \"walking,\" \"crossing,\" or \"standing,\" which lack the granularity needed for truly sophisticated autonomous driving systems. However, to reach human-level perception and decision-making, we need a much deeper understanding of pedestrian intentions, actions, and their interplay with the surrounding environment. This includes factors like gaze direction, body language, interactions with other pedestrians or objects, and reactions to traffic signals. To address this, we collected detailed annotations from GPT, leveraging its advanced language processing capabilities to describe pedestrian behavior in a nuanced way. Then, we applied n-gram processing to analyze these annotations, identifying key phrases and patterns to construct a comprehensive class taxonomy for pedestrian handling. This taxonomy goes beyond simple labels, capturing the richness and complexity of human behavior in traffic scenarios, ultimately enabling the development of more intelligent and responsive autonomous vehicles.\nThere are following categories for these semantic labels:\n\u2022 Pedestrian types: It's important for autonomous vehicles to recognize different types of pedestrians and understand how their behavior might vary. For example, children and the elderly may move unpredictably or need more time to cross the road, while construction workers and figures of authorities might signal hazards areas or direct traffic.\n\u2022 Pedestrian Behaviors: Observing pedestrian movements helps predict their future paths, enabling the vehicle to interact smoothly and safely. This includes yielding to those crossing the street and navigating around those waiting or moving slowly.\n\u2022 Location and surroundings are also crucial. Knowing if a pedestrian is on a crosswalk, waiting at the curb, or walking on the sidewalk helps the vehicle anticipate their actions.\n\u2022 Weather and environmental condition significantly impact driving decisions. At night or in poor conditions, extra caution is needed as pedestrians may be less visible or aware of approaching vehicles."}, {"title": "3 Dataset and Annotation", "content": "The Waymo Open Dataset[15] is a powerful dataset for researchers for pedestrian behavior prediction and understanding in the context of autonomous driving. It offers a rich collection of over 1.2 million images, capturing an object-centric vehicle or pedestrian instance from Waymo's self-driving vehicles navigating diverse real-world environments. This extensive and varied dataset is crucial for training robust models capable of generalizing across different scenarios. The dataset uses precise 3D bounding boxes annotations to crop 2D object images from cameras. The dataset includes sequences of images and point clouds for each pedestrian, capturing their movement over time. This temporal information is essential for understanding pedestrian motion patterns and predicting their future trajectories. The data is captured in real-world driving scenarios, with the complexities and uncertainties of pedestrian behavior in natural settings. The dataset covers a wide range of pedestrian behaviors, from simple walking to complex interactions with other road users. This diversity allows models to learn and generalize across different behavioral patterns.\nWe use GPT4-V to generate annotations about the pedestrians in 2D images. The input prompt is \"You are a helpful autonomous driving agent. Describe the action and behavior of the pedestrians, and the unusual which needs the"}, {"title": "4 Knowledge Distillation to Vision Network", "content": "We formulate the problem as a multi-label classification problem. Given the extracted semantic text labels, we predict the probability that the semantic labels appear in the GPT annotations. For example, the GPT annotation contains 'crossing' and 'walking'. The ground truth labels are P('Crossing') = 1 and P ('Walking') = 1.0, and other labels not in the description are assigned a probability of zero. In this way, we can use semantic labels to supervise our vision network."}, {"title": "4.1 Knowledge Distillation Method", "content": ""}, {"title": "4.1.1 Teacher Model (GPT)", "content": "We won't delve into the internal workings of GPT here, as it's a complex black-box transformer model. Assume it outputs a sequence of text.\nWe redefine the output as the semantics describing pedestrian's intents or attributes of multiple class labels C. GPT's output texts are converted to class labels with probability:\n$y_i = \\begin{cases}1, & \\text{if } i\\text{-th class appears in GPT's output} \\\\ 0, & \\text{otherwise} \\end{cases}$"}, {"title": "4.1.2 Student Model", "content": "We have a light-weight Vision model to learn semantic attributes output from GPT teacher model. The specific architecture is discussed in the following section. We add two-layer MLP to adapt vision network output to semantic classes, which take the form of $f_\\theta(x)$."}, {"title": "4.1.3 Binary Cross-entropy loss", "content": "The core of knowledge distillation is to minimize the difference between the teacher's and student's predictions. We use Binary cross-entropy to predict if the predicted class appears in GPT's answer."}, {"title": "4.2 CNN and Vision Transformer", "content": "When choosing between Vision Transformer (ViT)[16] and CNN[17] backbones for semantic embedding, several trade-offs need consideration. ViT, with its attention mechanism, excels at capturing global dependencies and contextual relationships within images, potentially leading to richer semantic representations. However, this comes at the cost of computational efficiency, especially for high-resolution inputs, due to the quadratic complexity of attention. On the other hand, CNN architecture offers a good balance between performance and efficiency. Its hierarchical structure efficiently processes local features and scales well to larger images. However, it might not capture long-range dependencies as effectively as ViT."}, {"title": "4.3 Experiment leveraging Foundation Models", "content": "CLIP[18] is a cutting-edge model in the field of natural language processing and computer vision, developed to facilitate the extraction of rich, contextual information from images and text. OpenClip[1][19] is an open-source implementation inspired by the CLIP (Contrastive Language-Image Pretraining) model by OpenAI, which has been highly influential in advancing multi-modal learning. The model excels at understanding and generating both textual and visual data, enabling it to bridge the gap between language and images. It uses a contrastive learning approach and learns to associate images with their corresponding textual descriptions, improving its ability to perform tasks such as image captioning and visual question answering.\nSAM (Segment Anything Model)[20] and its successor SAM2 are advanced foundation models created by Meta AI, aimed at transforming image and video segmentation. They allow users to pinpoint and identify objects within an image or video through simple prompts such as points or boxes, effectively instructing the model to segment anything\". While SAM is proficient in general image segmentation, SAM2 significantly enhances this capability by integrating temporal context, making it especially skilled in video segmentation. This improvement arises from SAM2's capacity to track objects across frames, gaining insights into their movement and transformations over time. By accurately segmenting pedestrians and other objects in images, SAM can uncovers essential details regarding their positioning and interactions, and also other signals by recognizing gestures from different body parts, such as hands, legs movements. This detailed information can be useful to infer pedestrian's actions and intents, such as walking, running, standing, as well as to forecast their intentions given the context, such as they crossing the road at a crosswalk or engage in jaywalking, or even if they are exiting from the vehicle.\nSapiens [21] is a family of foundation models specifically designed to excel at human-centric vision tasks, making it a strong candidate for enhancing autonomous driving pedestrian handling. Pre-trained on a massive dataset of over 300 million images, Sapiens already possesses a deep understanding of human appearance, pose, and movement. By fine-tuning it with data specific to driving scenarios, we can leverage this pre-existing knowledge for more robust pedestrian behavior and scene understanding, and enhance its ability to predict pedestrian actions, and behavior signals leveraging scene contexts.\nDifferent vision-foundation models have their individual strengths and expertise. For example, CLIP can capture global semantics and interactions, SAM can segment objects of different semantic classes, and Sapiens can leverage domain-specific pre-training for human body data. By combining these models, we can use the ensemble method to further boost the model's performance. We used a cross-attention[22] mechanism to aggregate information from multiple foundation models and were able to combine and extract useful features from multiple domain experts.\nEnsemble of Embedding = softmax $(\\frac{Q(W_kE_{VL})^T}{\\sqrt{d}}W_vE_{VL})$\nwhere Q is a trainable latent query vector of dimension d, $E_{VL}$ = [$E_1$, ..., $E_n$] is the matrix consists of embeddings from multiple vision language models, and $W_k$ and $W_v$ are projection matrix to generate key and value for attention mechanism."}, {"title": "4.4 Evaluation", "content": ""}, {"title": "4.4.1 Quantitative Evaluation", "content": "During inference, the model output a set of text labels whose confidence is higher than a pre-set threshold. We manually tuning the threshold, and found 0.15 is the optimal value to match the length of GPT-4V's reference answers, and we use this threshold value for further evaluation. We can evaluate the model using common metrics in Natural Language Processing, like BLEU score, precision / recall.\nBLEU[23] score is a popular metric for evaluating the quality of machine-generated text, and it can be effectively used for assessing text label generation tasks. BLEU measures how much of the generated text matches the reference text. It looks for the presence of correct words and phrases, ensuring the generated labels are accurate and relevant. BLEU also considers how much of the reference text is captured in the generated output, by adding penalty to generated texts whose length is less than the length of reference text, ensuring the generated labels are comprehensive and cover all essential aspects. By combining these two aspects, BLEU helps find a good balance between precision and recalls. To reduce the sensitivity to the word ordering, we only use 1-gram matching, and the label of two words, such as \"using cellphone\" is treated as one text label."}, {"title": "4.4.2 Qualitative Evaluation", "content": "We analyzed the examples compared to reference answers generated by GPT. We can observe that the fine-tuned model after knowledge distillation was able to describe the actions of the pedestrian in the scene, and understanding the context, e.g. at the bus station, waiting, and their more fine-grained actions, such as looking at cellphone, holding items and pushing strollers.\nWe also found in some cases as shown in 5, the model is able to predict more comprehensive semantic attributes of the pedestrian, because LLMs tend to use auto-aggressive and Beam-search to generate the text in answers, which might only focus on the most important aspect of the image, but neglect other factors of the scene. By knowledge distillation with a variety of GPT annotations, the fine-tuned model is able to learn probability distribution of all semantic labels more comprehensively, and even provide richer and more complete and diverse information of the scene."}, {"title": "4.4.3 Evaluation on Downstream Prediction Task", "content": "The pedestrian behavior signals and latent semantic embedding can generate early signals for downstream trajectory prediction tasks, where autonomous vehicle can forecast the traffic agent's trajectory and take actions to react. We also evaluated if the learned embedding after fine-tuning can improve the downstream task and potentially enhance autonomous vehicle's end-to-end performance. In this task, we use the past 1-second cooridinates of pedestrians to predict the position of the next 3 seconds. We use classicial RNN trajectory prediction architecture[24] [?], and performed the ablation study by comparing the performance of adding distilled Vision-Language-Model features.\nThe trajectory prediction task is formulated as below:\n$x_t = [x_t, y_t]$\nwhere $x_t$ represents the 2D coordinates of the pedestrian at time t.\n$h_t = f(h_{t-1}, x_t E_{VL}; W_h)$\nWe use RNN model with two layers: $h_t$ is the hidden state at time t, f(\u00b7) is the RNN cell, $E_{VLM}$ is the distilled Vision Language Model embedding concatenated with input features, $W_h$ represents the learnable parameters of the RNN.\n$x_{t+1} = g(h_t; W_o)$\nwhere $x_{t+1}$ is the predicted next pedestrian, g(\u00b7) is the linear transformation to output coordinates, $W_o$ represents the output layer parameters. The training objective is to minimize the distances between predicted position and ground truth position. Our experiment shows significant improvements in trajectory errors at 3 seconds, compared to baseline only use position coordinate feature.\nWe evaluate the trajectory prediction metrics as shown in Table 2, and by leveraging vision language model embedding, we are able to significantly reduce trajectory error metrics 2."}, {"title": "5 Conclusion", "content": "We proposed knowledge distillation method distilling knowledge from large-scale vision-language foundation models to smaller vision network, which improved open-vocabulary perception task and downstream trajectory prediction tasks. We also proposed more diverse and comprehensive taxonomy of pedestrian behaviors and attributes for autonomous driving. Our"}, {"title": "6 Limitation and Future Directions", "content": ""}, {"title": "6.1 Pedestrian Localization and Keypoint Localization", "content": "Overall, we observe that GPT4-V is still lacking in pedestrian pose segmentations 6. Due to this, further instruction tuning and more training data specific to pedestrian tasks are required to enhance its ability."}]}