{"title": "Understanding Visual Feature Reliance through the Lens of Complexity", "authors": ["Thomas Fel", "Louis B\u00e9thune", "Andrew Kyle Lampinen", "Thomas Serre", "Katherine Hermann"], "abstract": "Recent studies suggest that deep learning models' inductive bias towards favoring simpler features may be one of the sources of shortcut learning. Yet, there has been limited focus on understanding the complexity of the myriad features that models learn. In this work, we introduce a new metric for quantifying feature complexity, based on V-information and capturing whether a feature requires complex computational transformations to be extracted. Using this V-information metric, we analyze the complexities of 10,000 features-represented as directions in the penultimate layer that were extracted from a standard ImageNet-trained vision model. Our study addresses four key questions: First, we ask what features look like as a function of complexity and find a spectrum of simple-to-complex features present within the model. Second, we ask when features are learned during training. We find that simpler features dominate early in training, and more complex features emerge gradually. Third, we investigate where within the network simple and complex features \u201cflow\", and find that simpler features tend to bypass the visual hierarchy via residual connections. Fourth, we explore the connection between features' complexity and their importance in driving the network's decision. We find that complex features tend to be less important. Surprisingly, important features become accessible at earlier layers during training, like a \"sedimentation process,\" allowing the model to build upon these foundational elements.", "sections": [{"title": "1 Related Work", "content": "Feature analysis. Large vision models learn a diversity of features [74, 81] to support performance on the training task and can exhibit preferences for certain features over others, for example textures over shapes [10, 34, 43]. These preferences can be related to their use of shortcuts [33, 96] which compromise generalization capabilities [71, 70]. Hermann et al. [45] suggest that a full account of a model's feature preferences should consider both the predictivity and availability of features, and identify image properties that induce a shortcut bias. Relatedly, work shows that models often prefer features that are computationally simpler to extract-a \u201csimplicity bias\" [103, 84, 88, 7, 93, 44].\nExplainability. Attribution methods [95, 113, 9, 31, 85, 77, 38, 98, 102] seek to attribute model predictions to specific input parts and to visualize the most important area of an image for a given prediction. In response to the many limitations of these methods [2, 35, 97, 25, 51, 76], Feature Visualization [75, 81, 41] methods have sought to allow for the generation of images that maximize certain structures in the model e.g., a single neuron, entire channel, or direction, providing a clearer view features learned early [79, 92], as well as circuits present in the models [80, 59]. Recently, work has scaled these methods to deeper models [29]. Another approach, complementary to feature visualization, is automated concept extraction [36, 115, 28, 30, 1, 108], which identifies a wide range of concepts \u2013 directions in activations space \u2013 learned by models, inspired by recent works that suggest that the number of learned features often exceeds the neuron count [27]. This move towards over-complete dictionary learning for more comprehensive feature analysis represents a critical advancement.\nComplexity. On a theoretical level, the complexity of functions in deep learning has long been a subject of interest, with traditional frameworks like VC-dimension falling short of adequacy with current results. In particular, deep learning models often have the capacity to memorize the entire dataset, yet still generalize [114]; the reason is often suggested to be a positive benefit of simplicity bias [3, 48, 106]. Measures of the complexity of neural network functions are hard to make tractable [89]. Recent work has proposed various methods to evaluate this complexity. For instance, [17] proposed a score of non-linearity propagation, while [49] introduced a measure of local complexity based on spline partitioning. Additionally, [107] demonstrated that models tend to learn functions with low sensitivity to random changes in the input. The role of optimizers in complexity has also been explored. It has been shown that different optimizers impact the features learned by models; for example, [101] found that sharpness-aware minimization (SAM) [32] learns more diverse features, both simple and hard, whereas stochastic gradient descent (SGD) models tend to rely on simpler features. Furthermore, [23] utilized category theory to propose a metric based on redundancy, which consist in merging neurons until a distance gap is too large, with this distance gap acting as a hyperparameter. Concurrent work by Lampinen et al. [55] studies representations induced by input features of different complexities when datasets are carefully controlled and manipulated. Finally, Okawa et al. [78], Park et al. [82] investigated the development of concepts during the training process on toy datasets and revealed that the sequence in which they appear, related to their complexity, can be attributed to the multiplicative emergence of compositional skills.\nConcerning algorithmic complexity, Kolmogorov complexity [99, 53, 20], later expanded by Levin [60] to include a computational time component, offers a measure for evaluating the shortest programs capable of generating specific outputs on a Turing machine [21, 40]. This notion of complexity is at the roots of Solomonoff induction [100], which is often understood as the formal expression of Occam's razor and has received some attention in deep learning community [90, 91, 16]. Further developing these concepts, V-information [111] introduces computational constraints on mutual information measures, extending Shannon's legacy. This methodology enables the assessment of a feature's availability or the simplicity with which it can be decoded from a data source. We will formally introduce this concept in Section 2."}, {"title": "2 Method", "content": "Before we measure feature complexity, we define what is meant by features, explain how they are extracted, and then introduce the complexity metric.\nModel Setup. We study feature complexity within an ImageNet-trained ResNet50 [42]. We train the model for 90 epochs with an initial learning rate of 0.7, adjusted down by a factor of 10 at epochs 30, 60, and 80, achieving a 78.9% accuracy on the ImageNet validation set, which is on par with reported accuracy in similar studies [42, 110]. Focusing on one model reduces architectural variables, creating a controlled environment to analyze feature complexities and provide insights for broader model hypotheses.\nFeature Extraction. We operate within a classical supervised machine learning setting on (\u03a9, F, P) the underlying probability space \u2013 where \u03a9 is the sample space, F is a o-algebra on \u03a9, and P is a probability measure on F. The input space is denoted X \u2286 Rd. Let the input data x : \u03a9 \u2192 X be random variables with distributions Px. We will explore how, from x and using a neural network, we extract a series of k features. We will assume a classical vision neural network that admits a series of n intermediate spaces, such that:\n\\(f_l: \\mathcal{X} \\rightarrow \\mathcal{A}_l \\text{ with } l\\in \\{1, ..., n\\}.\\)\nInitially, one might suggest that a feature is a dimension of the model, meaning, for example, that a feature could be a neuron in the last layer of the model \\(z = f_n(x)_i, i \\in \\{1, ..., |\\mathcal{A}_n|\\}\\), thus each of the neurons would be a feature. However, several recent studies [80, 8, 24, 27, 30] have shown that our models actually learn a multitude of features, far more than the number of neurons, which explains, for example, why they are not mono-semantic [74, 18], which could also hinder our study of features. Therefore, we use a recent explainability method, Craft [28], to extract more features"}, {"title": "3 What Do Complex Features Look Like? A Qualitative Analysis", "content": "This section presents a qualitative investigation of relatively simple versus more complex features. Drawing from critical insights of recent studies, which indicate a tendency of neural networks to prefer input features that are both predictive and not overly complex [45], this analysis aims to better understand the nature of features that are easily processed by models versus those that pose more significant challenges. Indeed, understanding the types of features that are too complex for our model can help us anticipate the types of shortcuts the model might rely on and, on the other hand, design methods to simplify the learning of complex features. This section of the manuscript is intentionally qualitative and aims to be exploratory. We applied our complexity metric to 10,000 features extracted from a fully trained ResNet50. For each feature, we computed the complexity score K(z, x) using a subset of 20,000 images from the validation set. Recognizing the impracticality of manually examining each of the 10,000 features, we employed a strategy to aggregate these features into a more manageable number of groups that we called Meta-features.\nMethod for Aggregating Features into Meta-features. To condense the vast array of features into a reduced number of similar features, we applied K-means clustering to the feature dictionary D*, resulting in 150 distinct clusters. These clusters represent collections of features, referred to as Meta-features C = {v1, ..., v|c| }; we then computed an average complexity score for each group. By selecting a diverse range of 30 clusters, chosen to cover a spectrum of complexity levels from the simplest to the most complex features, we aimed to provide a comprehensive overview of the diversity of feature complexity within the model. We propose to visualize the distance matrix in D*, showing feature complexity in Figure 2. This approach offers preliminary insights into features seen as simple or complex by the model.\nSimple Features. Among the simpler features, we find elements primarily based on color, such as sky and sea, as well as simple pattern detectors like line detectors and low-frequency detectors exemplified by bokeh. Interestingly, features geared towards text detection, such as watermark, are also included in this group. These findings align with previous studies [113, 92, 12, 79], which have shown that neural networks tend to identify color and simple geometric patterns in the early layers as well as low-frequency detectors. This suggests that these features are relatively easy for neural networks to process and recognize. Furthermore, our findings detailed in Appendix 11 corroborate the theoretical work posited in [11, 69]: robust learning possibly induces the learning of shortcuts or reliance on \"easy\u201d features within the model.\nMedium Complexity Features. Features with medium complexity reveal more nuanced and some-times unexpected characteristics. We find, for example, low-quality detectors sensitive to low-resolution images. Additionally, a significant number of concepts related to human elements were observed despite the absence of a dedicated human class in ImageNet. Trademark-related features, distinct from simpler watermark detectors, also reside within this intermediate complexity bracket.\nComplex Features. Among the most complex features, we find several Meta-features that exhibit a notable degree of structural coherence, including categories such as insect legs, curves, and ears. These patterns represent structured configurations that are ostensibly more challenging for models to process than more localized features, echoing the ongoing discussion about texture bias in current models [10, 34, 43]. Intriguingly, the most complex Meta-features identified, namely whiskers and insect legs, embody types of filament-like structures. Interestingly, we note that those types of features are known to be challenging for current models to identify accurately [57], aligning with documented difficulties in path-tracking tasks [63]. Such tasks have revealed current models' limitations in tracing paths, which parallels challenges in connectomics [86], particularly in filament segmentation\u2014a domain recognized for its complexity within deep learning research.\nNow that we've browsed simple and complex features, another question arises: how does the model build these features during the forward pass? For instance, where within the model does the formation of a watermark detector feature occur? And for more complex features that require greater structure, in which block of computation are these features formed within the model?\""}, {"title": "4 Where do Complex Features Emerge", "content": "As suggested by previous work, simple features, like color detectors and low-frequency detectors, may already exist within the early layers of the model. An intriguing question arises: how does the model ensure the propagation of these fea-tures to the final latent space fn, where features are extracted? A key component to consider in addressing this question is the role of resid-ual connections within the ResNet [42] architec-ture. The formulation of a residual connection in ResNet blocks is mathematically represented as:\n\\(f_{l+1}(x) = f_l(x) + (g_l \\circ f_l)(x)\\)\nThis equation highlights two distinct paths: the \"Residual\" branch, which facilitates the direct transfer of features from fe to the subsequent layer l + 1, and the \u201cMain\u201d branch, which in-troduces additional transformations to fe through additional computation ge to enhance its repre-sentational capacity. We aim to investigate the flow of simple and complex features through these branches. In our analysis, we examine two subsets of features: 100 features of the highest complexity (top-1 percentile) and 100 features of the lowest complexity (bottom-1 percentile). We measure the Centered Kernel Alignment (CKA) [54] between the final concept values z and the activations from (A) the \"Residual\" branch fe, and (B) the \u201cMain\u201d branch (ge fe), at each residual block, as a proxy for concept information contained in each branch. The findings, illustrated in Figure 4, reveal that simple features are efficiently \u201cteleported\u201d to later layers through the residual branches in other words, once computed, they are passed forward with little subsequent modification. In contrast, complex concepts are incrementally built up through an interactive process involving the \"main\" and \"residual\u201d branches. This understanding of feature evolution within network architectures emphasizes the importance of residual connections. This insight, though expected, clarifies a common conception by showing that simple features utilize the residual branch. The next step is to examine the temporal dynamics of feature development, specifically investigating when complex and simple concepts emerge during model training."}, {"title": "5 When do Complex Features Arise", "content": "Figure 1 raises an important question: Does the complexity of a feature influence the time it takes to develop during training? To explore this, we refer to the 10,000 features extracted at the final epoch of our model as f(e), and we use f(i) to represent the penultimate layer of the model at any given epoch i, where i \u2208 {1, ..., e} and e represents the total number of epochs. We aim to determine how early each feature can be detected in previous epochs f(i) for i < e. This involves calculating a specific decoding score; in our scenario, we define this score as Sy-the measure of V-information between the model's penultimate activations across epochs and an ultimate feature values, where V is the set of linear models. This metric helps us assess whether a feature was \u201creadily available\" at a certain epoch i. The cumulative score A is calculated by averaging this measure across all epochs, leading to our score:\n\\(\\Delta(x, z) = 1 - \\frac{1}{e} \\sum_i I_{\\mathcal{V}} (f^{(i)}(x) \\rightarrow z)\\)."}, {"title": "6 Complexity and Importance: A Subtle Tango", "content": "Numerous studies have proposed hypotheses regarding the relationship between the importance and complexity of features within neural networks. A particularly notable hypothesis is the simplicity bias [3, 48, 106], which suggests that models leverage simpler features more frequently. This section aims to quantitatively validate these claims using our complexity metric paired with the importance of each feature. Because features are extracted from the penultimate layer, a closed-form relationship between features and logits can be derived due to the linear nature of this relationship. By analyzing this relationship over training for features of different complexity, we identify a surprising novel perspective: models appear to reduce the complexity of their important features. This process is analogous to sedimentation and mirrors the operation of a Levin Universal Search [60]. The model incrementally shifts significant features to earlier layers, taking time to identify simpler algorithms in the process."}, {"title": "7 Conclusion", "content": "We introduced a complexity metric for neural network features, identifying both simple and complex types. We have shown where simple features flow \u2013 through residual connections as opposed to complex ones that develop via collaboration with main branches. Our study further revealed that complex features are learned later in training than simple ones. We have concluded by exploring the relationship between feature complexity and importance, and discovered that the simplicity bias found in neural networks becomes more pronounced as training progresses. Surprisingly, we found that important features simplify over time, suggesting a sedimentation process within neural networks that compresses important features to be accessible earlier in the network."}, {"title": "A Feature Extraction", "content": "Dictionary Learning. To comprehensively analyze the complexity of features extracted from a deep learning model, we employed a detailed feature extraction process using dictionary learning, specifically utilizing an over-complete dictionary. This approach allows each activation fn(x) \u2208 Ae to be expressed as a linear combination of multiple basis elements v \u2208 Ae (atoms) from the dictionary D* = {v1, ..., vk} coupled with some sparse coefficient z \u2208 Rk associated to each atoms.\nThe over-completness of D* means that the dimension of the dictionnary (k) is larger than the dimension of the activations space k >> |Ae|. This property allow us to overcome the superposition problem [27] essentially stating that there be more feature than neurons.\nMathematically, given an activation function fn(x), it can be represented as a linear combination of atoms from the dictionary D, expressed as:\n\\(f_n(x) \\approx zD^* = \\sum_i^k z_iv_i\\)\nwhere zi are the coefficients indicating the contribution of each atom vi from the dictionary.\nImplementation. Our implementation was inspired by Craft [28], leveraging the properties of ReLU activations in ResNet50. Given that ReLUs induce non-negativity of the activation, we employed Non-Negative Matrix Factorization (NMF) [56, 109] for the reconstruction, as it naturally aligns with the sparsity and non-negativity constraints of ReLU activations. Unlike PCA, which cannot produce overcomplete dictionaries and may result in non-positive activations, NMF can create overcomplete dictionaries in this context.\nThe dictionary D* was trained to reconstruct the activations fn(x) using the entire ImageNet training dataset, comprising 1.2 million images. Formally, for the set of images X and their corresponding activations fn (X), the objective was to minimize the reconstruction error:\n\\(||f_n(X) - ZD^*||_F,\\)\nensuring that fn(X) can be closely approximated by ZD*. Additionally, the NMF framework enforces non-negativity constraints on the dictionary matrix D* > 0 and the coefficients Z \u2265 0:\n\\((Z, D^*) = \\arg \\min_{Z\\geq 0,D^*\\geq 0} || f_n(X) - ZD^*||_F.\\)\nThe dictionary D* was designed to encapsulate 10 concepts per class, resulting in a total of 10,000 concepts. To augment the training samples for NMF, we exploited the spatial dimensions of the last layer of ResNet50, which has 2048 channels with a spatial resolution of 7x7. By training the NMF independently on each of the 49 spatial dimensions, we effectively increased the number of training samples to approximately 58 million artificial samples (channel activations).\nWe utilized the block coordinate descent solver from Scikit-learn [83] to solve the NMF problem. This algorithm decomposes the problem into smaller subproblems, making it more tractable. The optimization process continued until convergence was achieved with a tolerance of \u03b5 = 10\u22124, ensuring the dictionary was sufficiently optimized for accurate feature extraction. Post-training, the reconstructed activations ZD* retained over 99% accuracy in common predictions compared to the original activations fn(X).\nExtracting Features for New Data Points. Once the dictionary D* was trained, it was fixed. For any new input x, the corresponding feature z was extracted by solving a Non-Negative Least Squares (NNLS) problem. This mapping of new input activations fn(x) to the learned feature space was performed by minimizing the following objective:\n\\(z = \\arg \\min_{z\\geq 0} || f_n(x) - zD^*||_F.\\)\nThis optimization problem is convex, ensuring computational feasibility and robust feature extraction for new data points."}, {"title": "C Complexity measure", "content": "In this section, we detail the closed-form expression of the V-information when the predictive family V consists of linear classifiers with Gaussian posteriors. Specifically, V is defined as follows:\n\\(\\mathcal{V} = \\{\\eta: x \\rightarrow N(\\psi(x), \\sigma^2), \\text{ with } x \\in \\mathcal{X} \\text{ and } \\psi \\in \\Psi;\\\\\\[2mm]\\O \\rightarrow N(\\mu, \\sigma^2), \\text{ with } \\mu\\in \\mathbb{R}, \\sigma^2 = 1;\\,\\}\\\nwhere \u03a8 = {x \u2192 Mx | M \u2208 Rd} is a set of linear predictors. This setting corresponds to the linear decoding we apply during the computation of V-information. In this context, a closed-form solution is available (see [111]):\n\\(I_{\\mathcal{V}}(x \\rightarrow z) = H_{\\mathcal{V}}(z) - H_{\\mathcal{V}}(z | x)\\\\\n= \\inf_{\\mu\\in\\mathbb{R}} \\mathbb{E}_{z\\sim P_z} [ - \\log \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(z - \\mu)^2}{2\\sigma^2}}]  -\\inf_{\\psi\\in\\Psi}  \\mathbb{E}_{x,z\\sim P} [ - \\log \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(z - \\psi(x))^2}{2\\sigma^2}} ]  \\\\ =\n-\\frac{1}{2\\sigma^2} \\inf_{\\mu\\in\\mathbb{R}}  \\mathbb{E}_{z\\sim P_z} [ (z - \\mu)^2]  + \\frac{1}{2\\sigma^2} \\inf_{\\psi\\in\\Psi}  \\mathbb{E}_{x,z\\sim P} [ (z - \\psi(x))^2 ]  \\\\=\n\\frac{\\text{Var}(z)}{2\\sigma^2} -  \\frac{1}{2\\sigma^2} \\inf_{\\psi\\in\\Psi}  \\mathbb{E}_{x,z\\sim P} [ (z - \\psi(x))^2 ]\\\\ = \\frac{\\text{Var}(z)}{2\\sigma^2}  (1 - \\frac{  \\inf_{\\psi\\in\\Psi}  \\mathbb{E}_{x,z\\sim P} [ (z - \\psi(x))^2]}{\\text{Var}(z)}  )\\\\=\n\\frac{\\text{Var}(z) R^2}{2\\sigma^2}.\\)\nHere, R\u00b2 is the coefficient of determination. Therefore, the following inequalities hold:\n\\(0 \\leq I_{\\mathcal{V}} (x + z) \\leq \\text{Var}(z).\\)\nGiven that the input data are centered and scaled, we typically have Var(x) = 1 at the input layer. Furthermore, residual connections and batch normalization tend to preserve this scaling in deeper layers, implying Var(z) \u2248 1.\nWe define complexity as the opposite of the average V-information across layers: complex features are those that are harder to decode. We add a shift of 1 for the ease of plotting. Empirically, we observed that this adjustment yields K(z, x) in the range [0, 1], with 1 indicating a complex feature that is not available and 0 indicating a simple feature that is fully available."}, {"title": "D Feature Support Theory", "content": "While simplifying important features underscores a trend toward computational efficiency, the role of complex features within the model deserves a closer examination. Despite these features often being deemed less important directly, they contribute significantly to the model's overall performance, a paradox that can lead us to introduce the concept of \u201csupport features.\" These are a set of features that may not carry substantial importance individually, but that collectively play a crucial role in the model's decision-making process.\nThe presence of numerous complex features, whose importance on average is less pronounced, poses a conundrum. However, these features are far from redundant. Experiments conducted by progressively removing the most complex concepts from the model demonstrate a noticeable impact on performance, as illustrated in Figure 9. This empirical evidence supports the theory that, while individually, these complex features may not be pivotal, their collective presence contributes indispensably to the robustness and adaptability of the model. These results are reminiscent of prior findings that low-importance model components that are removed in pruning may nevertheless contribute to model accuracy on rare items [47].\nThis observation aligns with the broader understanding of neural network functionality, where diversity in feature representation-spanning from simple to complex-enhances the model's ability to generalize and perform across varied datasets and tasks. Therefore, the \"Feature Support Theory\" underscores an essential aspect of neural network design and training: integrating and preserving a wide spectrum of features, regardless of their individual perceived importance, are vital for achieving high levels of performance and robustness."}, {"title": "E Complexity and Redundancy", "content": "To further understand the link between feature complexity and redundancy, we utilized the redundancy measure from [73]. Our findings indicate that complex features tend to be less redundant, as depicted in Figure 10. This observation aligns with the strong correlation between our complexity measure and the redundancy-based complexity measure proposed by [23].\nTo quantify redundancy, [73] employed a modified version of Centered Kernel Alignment (CKA) [54], a measure of similarity between two sets of activation features. We briefly recall that CKA between two set of activations A, B in Rd is defined as follows:\n\\(\\text{CKA}(A, B) = \\frac{||K_A K_B||_F}{\\sqrt{||K_A K_A||_F||K_B K_B||_F}},\\)\nwhere KA and KB are the Gram matrices of the feature activations A and B, respectively, and || || F denotes the Frobenius norm.\nIn our analysis, we calculated the CKA measure between a feature z and the activations for a set of 2,000 images from the validation set fn(X). Subsequently, we compared this with the CKA measure when a portion of the activation is masked using a binary mask m \u2208 {0, 1}|Ael, denoted as CKA(z, fn (X) m), where represents element-wise multiplication (Hadamard product). This comparison enabled us to assess whether masking a subset of neurons impacts the decoding of the features. Specifically, to evaluate redundancy, we employed a progressive masking strategy, successively masking 10%, 50%, and 90% of the activation. If the masked activations retain a high CKA with z, it indicates that the information remains largely intact, suggesting that the feature is redundantly stored across multiple neurons, sign of a redundant encoding mechanism within the network. Conversely, if masking results in a substantial decrease in CKA, it implies that the information was predominantly localized on a specific neuron. In this scenario, the feature is not redundantly encoded but rather concentrated in specific neurons. This concentration indicates a lower degree of redundancy, as the loss of these specific neurons (throught the masking) leads to a significant reduction in the CKA score.\nThe final score of redundancy is then the average CKA difference between the original activation and the masked activations:\n\\(\\text{Redundancy} = \\frac{\\mathbb{E}_m (\\text{CKA}(f_n(X)m, z))}{\\text{CKA}(f_n(X), z)}\\)\nAnd averaged across the different level of masking. A high score (1) indicating a high redundancy \u2013 i.e. the CKA between the masked activation and with the original activation is similar \u2013 while a low score indicate a more localized and thus a lower degree of redundancy.\nIn summary, our results, as depicted in Figure 10 support the idea that complex features exhibit lower redundancy."}, {"title": "F Complexity and Robustness", "content": "To measure robustness, we evaluate the stability of feature responses under perturbations. For each input point x, we add isotropic Gaussian noise with varying levels of standard deviation \u03c3. The robustness score is determined by measuring the variance in the feature response due to the noise. Formally, let z(x) represent the feature response for input x. We define the perturbed input as:\n\\(x = x + \\mathcal{N}(0, \\sigma^2 I)\\) where \\(\\mathcal{N}(0, \\sigma^2 I)\\) represents Gaussian noise with mean 0 and variance \\(\\sigma^2\\). The sensitivity score Sensitivity(z) for a feature z is given by:\n\\(Sensitivity(z) = Var(z(\\hat{x}))\\)\nSpecifically, we sample 100 random noise and repeat this for 3 levels of noise \u03c3\u2208 {0.01, 0.1, 0.5} to compute the variance in feature response for each input from 2,000 samples from the Validation set of ImageNet to get a distribution of feature value. We also consider other metrics such as the range (min-max) of the feature response, but all methods consistently indicate that more complex features are less robust.\nIn summary, our results, as shown in Figure 11, demonstrate that complex features exhibit lower robustness. This indicates that features with higher complexity are more sensitive to perturbations and noise, resulting in greater variability in their responses."}, {"title": "G Importance Measure", "content": "The problem of estimating feature importance is closely related to attribution methods [113, 31, 85, 77, 15, 98, 102, 22], which aim to identify the important pixels for a decision. A recent study has shown that all attribution methods can be extended in the space of concepts [30]. In our case, the features are extracted from the penultimate layer, where the relationship between feature values and logits is linear. We will elaborate on this and demonstrate that the notion of importance in the linear case is easier and optimal methods to estimate importance exist.\nSetup. Recall that for a point x, we can obtain its k feature values by solving an NNLS problem\nz = arg min || fn(x) \u2013 zD*||F. The vector z contains the k features in Rk. We can replace the activation of the penultimate layer fn(x) with its feature representation in the over-complete basis\nzD* \u2248 fn(x). Since we are in the penultimate layer, the model's decision, i.e., the logit y \u2208 R for the predicted class, is linearly related to each feature z by the last weight matrix, denoted as W, as follows:\n\\(y = f_n(x)W\\\\\\approx zD^*W  \\text{with } z = \\arg \\min || f_n(x) - zD^*||_F \\\\=zW'  \\text{ with } W'=D^*W \\in \\mathbb{R}^k\\)\nThus, the energy contributed to the logit by feature i can be directly measured by Wizi and y =\nWizi. Consequently, the contribution of a feature z\u2081 can be measured using gradient-input,\n(zy) z. Several studies [5, 30] have detailed the linear case and shown the optimality of gradient-input with respect to fidelity metrics. They also demonstrated that many methods in the linear case boil down to gradient-input, including Occlusion [113], Rise[85], and Integrated Gradient[102].\nIn our case, we measured the importance by taking the absolute value of the importance vector,\ni.e., \u0393(zi) = EP\n(11). It is natural to question whether this approach might overlook important features due to their inhibitory effects. Indeed, as depicted in Figure 12, a large number of features may be important not because they add positive energy to the logits, but by inhibition, i.e., by suppressing class information. Although this does not alter the implications of our previous observations, it is noteworthy that the majority of inhibitory features are also simple features.\nPrevalence and Importance. Another property of importance is its close relationship with preva-lence [30], which indicates that a frequently occurring feature will, on average, be more important given the same importance coefficient (\u2207zy). In our study, this implies that if the most important features are reduced, these important features are also potentially more frequently present. Consequently, the prevalence of a feature can be a factor explaining this sedimentation process. We refer the reader to a concurrent study that proposed to investigate more deeply this phenomena using controlled dataset [55]."}, {"title": "H Kolmogorov, Levin and V-information", "content": "In this section we recall some of the most important complexity measures like Kolmogorov complexity", "P": "N \u2192 \u03a3* that produces the n-th first terms of the sequence (un):\n\\(K_L^{(\\infty)"}, "u_n)  \\overset{def}{=} \\min_{P(n)=u_n}  |P|.\\)\nIntuitively, if the sequence is highly compressible, the program will be short. For example, the sequences [1, 2, 3, 4, . . .] or [2, 4, 8, 16, 32, . . .] are few lines of code in most programming languages. Conversely, if the sequence is purely random, then no finite-length program exists. The digits of \u03c0, seemingly without structure, are not Kolmogorov random since there exist short programs computing them. The famous Cantor's diagonal argument [19] shows that most sequences are random, since no bijection exists between \u03a3* (countably infinite) and \u2211\u039d (cardinality of the continuum). The definition implicitly assumes a specific computation model (Python interpreter, C++ compiler, Turing machine) to describe the language. However, by definition Turing-complete models can simulate each other, which implies there exist a universal constant C(Python|C++) such that for all sequences un we have\n\\(K_{python}^{(\\infty)} (u_n) \\leq K_{C++}^{(\\infty)} (u_n) + C(Python|C++). \\)\nThis constant corresponds to the length of a Python interpreter written in C++ for example. In general, this holds for any other pair of languages. Therefore, if KL K\u221e) (un) \u2192 +\u221e as n \u2192 \u221e for some language L, then it is true in every other language: intrinsic randomness is universal in this sense [94].\nLevin complexity. Kolmogorov's complexity suffers from an important drawback: it is not Turing-computable. Put another way, there exists no algorithm that computes K(\u221e). Fortunately, by regularizing K(\u221e) appropriately it is possible to make it computable. Levin [61] proposed to regularize the cost with the runtime T(P, n) of program P on input n. This is the Levin complexity:\n\\(K_L^{(T)} (u_n)  \\overset{def}{=} \\min_{P(n)=u_n}  |P| + \\log  \\text{T```json\n{\n  \"title\": \"I Limitations\",\n  \"content\":", "Task. Here, we have studied a specific CNN architecture, ResNet50. In future experiments, it will be useful to investigate whether other model families exhibit similar feature learning, including in domains beyond vision.\nArchitecture. The residual connections of the ResNet are shared by other architectures like Con-vNext [64] or Vision Transformers (ViT) [26, 14]. The works of [105] indicate that findings from convolutional models may transfer to ViTs. Furthermore, the work of [112] suggest that features in convolutional networks and ViT are of similar nature. However, [87] found significant quantitative differences between the layers of ResNets and ViTs, highlighting the need for further empirical testing.\nTraining Dynamics The observed dynamics of feature learning, including the emergence of complex features and the reduction in the complexity of important features later in training, are based on a specific training schedule and set of hyperparameters. To accurately attribute these findings, a more comprehensive study is required to evaluate the role of various factors such as the learning rate scheduler and weight decay. Future research should systematically investigate how these and other training parameters influence feature complexity and importance.\nNested predictive families. Our complexity metrics rely on the hypothesis that the different predictive families associated to the network up to depth fe(x) are nested, i.e. that stacking more layers strictly increases expressiveness. This is highlighted in the relevant assumption in section 2. If this hypothesis is violated, the true complexity of a feature may be overestimated in deeper layers. This is typically the case at the early stages of training.\nDictionary of features. Regarding the building of the dictionary using NMF, a previous study [30] has shown that the specific dictionary learning method yielded a favorable tradeoff between several criterions such as sparsity, reconstruction error, or stability. Other dictionary learning methods (like sparse-PCA, K-Means or sparse auto-encoder) may yield a bank of concepts with different properties."]}