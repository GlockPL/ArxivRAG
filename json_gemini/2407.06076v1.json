{"title": "Understanding Visual Feature Reliance through the Lens of Complexity", "authors": ["Thomas Fel", "Louis B\u00e9thune", "Andrew Kyle Lampinen", "Thomas Serre", "Katherine Hermann"], "abstract": "Recent studies suggest that deep learning models' inductive bias towards favoring simpler features may be one of the sources of shortcut learning. Yet, there has been limited focus on understanding the complexity of the myriad features that models learn. In this work, we introduce a new metric for quantifying feature complexity, based on V-information and capturing whether a feature requires complex computational transformations to be extracted. Using this V-information metric, we analyze the complexities of 10,000 features-represented as directions in the penultimate layer that were extracted from a standard ImageNet-trained vision model. Our study addresses four key questions: First, we ask what features look like as a function of complexity and find a spectrum of simple-to-complex features present within the model. Second, we ask when features are learned during training. We find that simpler features dominate early in training, and more complex features emerge gradually. Third, we investigate where within the network simple and complex features \u201cflow\", and find that simpler features tend to bypass the visual hierarchy via residual connections. Fourth, we explore the connection between features' complexity and their importance in driving the network's decision. We find that complex features tend to be less important. Surprisingly, important features become accessible at earlier layers during training, like a \"sedimentation process,\" allowing the model to build upon these foundational elements.", "sections": [{"title": "1 Related Work", "content": "Feature analysis. Large vision models learn a diversity of features [74, 81] to support performance on the training task and can exhibit preferences for certain features over others, for example textures over shapes [10, 34, 43]. These preferences can be related to their use of shortcuts [33, 96] which compromise generalization capabilities [71, 70]. Hermann et al. [45] suggest that a full account of a model's feature preferences should consider both the predictivity and availability of features, and identify image properties that induce a shortcut bias. Relatedly, work shows that models often prefer features that are computationally simpler to extract-a \u201csimplicity bias\" [103, 84, 88, 7, 93, 44].\nExplainability. Attribution methods [95, 113, 9, 31, 85, 77, 38, 98, 102] seek to attribute model predictions to specific input parts and to visualize the most important area of an image for a given prediction. In response to the many limitations of these methods [2, 35, 97, 25, 51, 76], Feature Visualization [75, 81, 41] methods have sought to allow for the generation of images that maximize certain structures in the model e.g., a single neuron, entire channel, or direction, providing a clearer view features learned early [79, 92], as well as circuits present in the models [80, 59]. Recently, work has scaled these methods to deeper models [29]. Another approach, complementary to feature visualization, is automated concept extraction [36, 115, 28, 30, 1, 108], which identifies a wide range of concepts \u2013 directions in activations space \u2013 learned by models, inspired by recent works that suggest that the number of learned features often exceeds the neuron count [27]. This move towards over-complete dictionary learning for more comprehensive feature analysis represents a critical advancement.\nComplexity. On a theoretical level, the complexity of functions in deep learning has long been a subject of interest, with traditional frameworks like VC-dimension falling short of adequacy with current results. In particular, deep learning models often have the capacity to memorize the entire dataset, yet still generalize [114]; the reason is often suggested to be a positive benefit of simplicity bias [3, 48, 106]. Measures of the complexity of neural network functions are hard to make tractable [89]. Recent work has proposed various methods to evaluate this complexity. For instance, [17] proposed a score of non-linearity propagation, while [49] introduced a measure of local complexity based on spline partitioning. Additionally, [107] demonstrated that models tend to learn functions with low sensitivity to random changes in the input. The role of optimizers in complexity has also been explored. It has been shown that different optimizers impact the features learned by models; for example, [101] found that sharpness-aware minimization (SAM) [32] learns more diverse features, both simple and hard, whereas stochastic gradient descent (SGD) models tend to rely on simpler features. Furthermore, [23] utilized category theory to propose a metric based on redundancy, which consist in merging neurons until a distance gap is too large, with this distance gap acting as a hyperparameter. Concurrent work by Lampinen et al. [55] studies representations induced by input features of different complexities when datasets are carefully controlled and manipulated. Finally, Okawa et al. [78], Park et al. [82] investigated the development of concepts during the training process on toy datasets and revealed that the sequence in which they appear, related to their complexity, can be attributed to the multiplicative emergence of compositional skills.\nConcerning algorithmic complexity, Kolmogorov complexity [99, 53, 20], later expanded by Levin [60] to include a computational time component, offers a measure for evaluating the shortest programs capable of generating specific outputs on a Turing machine [21, 40]. This notion of complexity is at the roots of Solomonoff induction [100], which is often understood as the formal expression of Occam's razor and has received some attention in deep learning community [90, 91, 16]. Further developing these concepts, V-information [111] introduces computational constraints on mutual information measures, extending Shannon's legacy. This methodology enables the assessment of a feature's availability or the simplicity with which it can be decoded from a data source. We will formally introduce this concept in Section 2."}, {"title": "2 Method", "content": "Before we measure feature complexity, we define what is meant by features, explain how they are extracted, and then introduce the complexity metric.\nModel Setup. We study feature complexity within an ImageNet-trained ResNet50 [42]. We train the model for 90 epochs with an initial learning rate of 0.7, adjusted down by a factor of 10 at epochs 30, 60, and 80, achieving a 78.9% accuracy on the ImageNet validation set, which is on par with reported accuracy in similar studies [42, 110]. Focusing on one model reduces architectural variables, creating a controlled environment to analyze feature complexities and provide insights for broader model hypotheses.\nFeature Extraction. We operate within a classical supervised machine learning setting on (\u03a9, F, P) the underlying probability space \u2013 where \u03a9 is the sample space, F is a o-algebra on \u03a9, and P is a probability measure on F. The input space is denoted X \u2286 Rd. Let the input data x : \u03a9 \u2192 X be random variables with distributions Px. We will explore how, from x and using a neural network, we extract a series of k features. We will assume a classical vision neural network that admits a series of n intermediate spaces, such that:\n$f_l: \\mathcal{X} \\rightarrow \\mathcal{A}_l \\text{ with } l\\in \\{1, ..., n\\}$\nInitially, one might suggest that a feature is a dimension of the model, meaning, for example, that a feature could be a neuron in the last layer of the model $z = f_n(x)_i, i \\in \\{1, ..., |\\mathcal{A}_n|\\}$, thus each of the neurons would be a feature. However, several recent studies [80, 8, 24, 27, 30] have shown that our models actually learn a multitude of features, far more than the number of neurons, which explains, for example, why they are not mono-semantic [74, 18], which could also hinder our study of features. Therefore, we use a recent explainability method, Craft [28], to extract more features"}, {"title": "Complexity through the Lens of Computation", "content": "To formalize this, still on (\u03a9, F, P), we denote the output space Z, and z : \u03a9 \u2192 Z are random variables of a feature of interest with distributions Pz. The joint random vector (x, z) representing an image x and the value of its feature z on (\u03a9, F) has a joint distribution P defined over the product space X \u00d7 Z. Furthermore, P(Z) denotes the set of all probability measures on Z. We can now associate, for an x which we recall is a real-valued random variable, a corresponding feature z, another real-valued random variable, and we seek to correctly evaluate the complexity of the mapping from x to z. For this, we turn to the V-Information [111] that generalizes and extends the classical mutual information I(\u00b7, \u00b7) from Shannon's theory by overcoming its inability to take into account the computational capabilities of the decoder. Indeed, for two (not necessarily independent) random variables x and z, and for any bijective mapping \u03b3 : X \u2192 X, Shannon's mutual information remains unchanged: I(x, z) = I(\u03b3(x), z).\nConsider, for instance, \u03b3 as a cryptographic function that encrypts an image x using a bijective key-based algorithm (e.g., the AES encryption algorithm). If x represents the original image, and \u03b3(x) represents the cipherimage, the mutual information between x and z remains unchanged. This is because the encryption is a bijective process, and the information content is preserved. However, in practice, the encrypted images would be much harder to decode and use for training a model compared to the original one, without access to the decryption key. Another example we may think of is \u03b3 as a pixel shuffling operation. The information carried by x does not disappear after processing by \u03b3. However, it may be harder to extract in practice.\nThis demonstrates the practical importance of V-Information, as it considers the computational effort required to decode the information, highlighting the difference between theoretical and practical accessibility of information. Specifically, the V-information proposes taking into account the com-putational constraint of the decoder by assuming it can only extract information using a predictive family V \u2282 F = {\u03b7 : X \u222a {\u00f8} \u2192 P(Z)}. The authors [111] then define the V-entropy and the V-conditional entropy as follows:\n$H_{\\mathcal{V}}(z) = \\inf_{\\eta \\in \\mathcal{V}} \\mathbb{E}_{p_z} \\left(-\\log \\eta(\\emptyset; z)\\right), \\qquad H_{\\mathcal{V}}(z|x) = \\inf_{\\eta \\in \\mathcal{V}} \\mathbb{E}_{p_z} \\left(-\\log \\eta(x; z)\\right).$\nWhere \u03b7(\u00b7; \u00b7) is a function from X\u222a {\u00f8} \u2192 P(Z) that returns a probability density \u03b7(x; \u00b7) on Z using side information x, or without side information \u00d8. The predictive family V summarizes the computational capabilities of the decoder. When V contains all possible functions, V = F, it recovers Shannon's entropy as a special case. Intuitively, we seek the best possible prediction for z knowing x by maximizing the log-likelihood. Continuing, we naturally introduce the V-information:\n$I_{\\mathcal{V}}(x \\rightarrow z) = H_{\\mathcal{V}}(z) \u2013 H_{\\mathcal{V}}(z|x).$\nThe complexity of the mapping from x to z can now be assessed by examining a hierarchy of predictive families V\u2081 \u2282... \u2282 Vn of increasing expressiveness, like explored in [58]. Each predictive family Vl corresponds to a partial forward up to depth l, followed by a decoding step. This involves determining at which point we can decode or make the information from x to z available. Formally, we define the complexity of the feature as dependent of the cumulative V-information across layers:"}, {"title": "3 What Do Complex Features Look Like? A Qualitative Analysis", "content": "This section presents a qualitative investigation of relatively simple versus more complex features. Drawing from critical insights of recent studies, which indicate a tendency of neural networks to prefer input features that are both predictive and not overly complex [45], this analysis aims to better understand the nature of features that are easily processed by models versus those that pose more significant challenges. Indeed, understanding the types of features that are too complex for our model can help us anticipate the types of shortcuts the model might rely on and, on the other hand, design methods to simplify the learning of complex features. This section of the manuscript is intentionally qualitative and aims to be exploratory. We applied our complexity metric to 10,000 features extracted from a fully trained ResNet50. For each feature, we computed the complexity score K(z, x) using a subset of 20,000 images from the validation set. Recognizing the impracticality of manually examining each of the 10,000 features, we employed a strategy to aggregate these features into a more manageable number of groups that we called Meta-features."}, {"title": "4 Where do Complex Features Emerge", "content": "As suggested by previous work, simple features, like color detectors and low-frequency detectors, may already exist within the early layers of the model. An intriguing question arises: how does the model ensure the propagation of these features to the final latent space fn, where features are extracted? A key component to consider in addressing this question is the role of resid-ual connections within the ResNet [42] architec-ture. The formulation of a residual connection in ResNet blocks is mathematically represented as:\n$f_{l+1}(x) = f_l(x) + (g_l \\circ f_l)(x)$\nThis equation highlights two distinct paths: the \"Residual\" branch, which facilitates the direct"}, {"title": "5 When do Complex Features Arise", "content": "Figure 1 raises an important question: Does the complexity of a feature influence the time it takes to develop during training? To explore this, we refer to the 10,000 features extracted at the final epoch of our model as f (e), and we use f (i) to represent the penultimate layer of the model at any given epoch i, where i \u2208 {1, ..., e} and e represents the total number of epochs. We aim to determine how early each feature can be detected in previous epochs f (i) for i < e. This involves calculating a specific decoding score; in our scenario, we define this score as V-the measure of V-information between the model's penultimate activations across epochs and an ultimate feature values, where V is the set of linear models. This metric helps us assess whether a feature was \u201creadily available\" at a certain epoch i. The cumulative score A is calculated by averaging this measure across all epochs, leading to our score:\n$A(x, z) = 1 - \\frac{1}{e} \\sum_{i} I_{\\mathcal{V}} (f^{(i)}(x) \\rightarrow z).\""}, {"title": "6 Complexity and Importance: A Subtle Tango", "content": "Numerous studies have proposed hypotheses regarding the relationship between the importance and complexity of features within neural networks. A particularly notable hypothesis is the simplicity bias [3, 48, 106], which suggests that models leverage simpler features more frequently. This section aims to quantitatively validate these claims using our complexity metric paired with the importance of each feature. Because features are extracted from the penultimate layer, a closed-form relationship between features and logits can be derived due to the linear nature of this relationship. By analyzing this relationship over training for features of different complexity, we identify a surprising novel perspective: models appear to reduce the complexity of their important features. This process is analogous to sedimentation and mirrors the operation of a Levin Universal Search [60]. The model incrementally shifts significant features to earlier layers, taking time to identify simpler algorithms in the process."}, {"title": "7 Conclusion", "content": "We introduced a complexity metric for neural network features, identifying both simple and complex types. We have shown where simple features flow \u2013 through residual connections as opposed to complex ones that develop via collaboration with main branches. Our study further revealed that complex features are learned later in training than simple ones. We have concluded by exploring the relationship between feature complexity and importance, and discovered that the simplicity bias found in neural networks becomes more pronounced as training progresses. Surprisingly, we found that important features simplify over time, suggesting a sedimentation process within neural networks that compresses important features to be accessible earlier in the network."}, {"title": "A Feature Extraction", "content": "Dictionary Learning. To comprehensively analyze the complexity of features extracted from a deep learning model, we employed a detailed feature extraction process using dictionary learning, specifically utilizing an over-complete dictionary. This approach allows each activation fn(x) \u2208 Ae to be expressed as a linear combination of multiple basis elements v \u2208 Ae (atoms) from the dictionary D* = {v1, ..., vk} coupled with some sparse coefficient z \u2208 Rk associated to each atoms.\nThe over-completness of D* means that the dimension of the dictionnary (k) is larger than the dimension of the activations space k >> |Ae|. This property allow us to overcome the superposition problem [27] essentially stating that there be more feature than neurons.\nMathematically, given an activation function fn(x), it can be represented as a linear combination of atoms from the dictionary D, expressed as:\n$f_n(x) \\approx zD^* = \\sum_{i=1}^k z_iv_i$\nwhere zi are the coefficients indicating the contribution of each atom vi from the dictionary.\nImplementation. Our implementation was inspired by Craft [28], leveraging the properties of ReLU activations in ResNet50. Given that ReLUs induce non-negativity of the activation, we employed Non-Negative Matrix Factorization (NMF) [56, 109] for the reconstruction, as it naturally aligns with the sparsity and non-negativity constraints of ReLU activations. Unlike PCA, which cannot produce overcomplete dictionaries and may result in non-positive activations, NMF can create overcomplete dictionaries in this context.\nThe dictionary D* was trained to reconstruct the activations fn(x) using the entire ImageNet training dataset, comprising 1.2 million images. Formally, for the set of images X and their corresponding activations fn(X), the objective was to minimize the reconstruction error:\n$\\|f_n(X) \u2013 ZD^*\\|_F,$\nensuring that fn(X) can be closely approximated by ZD*. Additionally, the NMF framework enforces non-negativity constraints on the dictionary matrix D* \u2265 0 and the coefficients Z \u2265 0:\n$(Z, D^*) = \\arg \\min_{Z \\geq 0, D^* \\geq 0} \\| f_n(X) \u2013 ZD^*\\|_F.$\nThe dictionary D* was designed to encapsulate 10 concepts per class, resulting in a total of 10,000 concepts. To augment the training samples for NMF, we exploited the spatial dimensions of the last layer of ResNet50, which has 2048 channels with a spatial resolution of 7x7. By training the NMF independently on each of the 49 spatial dimensions, we effectively increased the number of training samples to approximately 58 million artificial samples (channel activations).\nWe utilized the block coordinate descent solver from Scikit-learn [83] to solve the NMF problem. This algorithm decomposes the problem into smaller subproblems, making it more tractable. The optimization process continued until convergence was achieved with a tolerance of \u025b = 10\u22124, ensuring the dictionary was sufficiently optimized for accurate feature extraction. Post-training, the reconstructed activations ZD* retained over 99% accuracy in common predictions compared to the original activations fn(X).\nExtracting Features for New Data Points. Once the dictionary D* was trained, it was fixed. For any new input x, the corresponding feature z was extracted by solving a Non-Negative Least Squares (NNLS) problem. This mapping of new input activations fn(x) to the learned feature space was performed by minimizing the following objective:\n$z = \\arg \\min_{z \\geq 0} \\| f_n(x) \u2013 zD^*\\|_F.$\nThis optimization problem is convex, ensuring computational feasibility and robust feature extraction for new data points."}, {"title": "C Complexity measure", "content": "In this section, we detail the closed-form expression of the V-information when the predictive family V consists of linear classifiers with Gaussian posteriors. Specifically, V is defined as follows:\n$\\mathcal{V} = \\{\\eta: x \\rightarrow \\mathcal{N}(\\psi(x), \\sigma^2), \\text{ with } x \\in \\mathcal{X} \\text{ and } \\psi \\in \\Psi; \\\\ [\u00f8 \\rightarrow \\mathcal{N}(\\mu, \\sigma^2), \\text{ with } \\mu\\in \\mathbb{R}, \\sigma^2 = 1;$\nwhere \u03a8 = {x \u2192 Mx | M \u2208 Rd} is a set of linear predictors. This setting corresponds to the linear decoding we apply during the computation of V-information. In this context, a closed-form solution is available (see [111]):\n$I_{\\mathcal{V}}(x \\rightarrow z) = H_{\\mathcal{V}}(z) \u2013 H_{\\mathcal{V}}(z | x)$\n$= \\inf_{\\mu\\in\\mathbb{R}} \\mathbb{E}_{z \\sim P_z} \\left[ -\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(z - \\mu)^2}{2\\sigma^2}} \\right] - \\inf_{\\psi\\in\\Psi} \\mathbb{E}_{x, z \\sim P} \\left[ -\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(z - \\psi(x))^2}{2\\sigma^2}} \\right]$\n$= \\frac{1}{2\\sigma^2} \\left[ \\inf_{\\mu\\in\\mathbb{R}} \\mathbb{E}_{z \\sim P_z} \\left[(z - \\mu)^2 \\right] - \\inf_{\\psi\\in\\Psi} \\mathbb{E}_{x, z \\sim P} \\left[(z - \\psi(x))^2 \\right] \\right]$\n$= \\frac{1}{2\\sigma^2} \\left( \\text{Var}(z) - \\frac{\\inf_{\\psi\\in\\Psi} \\mathbb{E}_{x, z \\sim P} \\left[(z \u2212 \\psi(x))^2\\right]}{\\text{Var}(z)}\\right)$\n$= \\frac{\\text{Var}(z)}{2\\sigma^2} R^2$\n$= \\text{Var}(z) R^2.$\nHere, R\u00b2 is the coefficient of determination. Therefore, the following inequalities hold:\n$0 \\leq I_{\\mathcal{V}}(x + z) \\leq \\text{Var}(z).$\nGiven that the input data are centered and scaled, we typically have Var(x) = 1 at the input layer. Furthermore, residual connections and batch normalization tend to preserve this scaling in deeper layers, implying Var(z) \u2248 1.\nWe define complexity as the opposite of the average V-information across layers: complex features are those that are harder to decode. We add a shift of 1 for the ease of plotting. Empirically, we observed that this adjustment yields K(z, x) in the range [0, 1], with 1 indicating a complex feature that is not available and 0 indicating a simple feature that is fully available."}, {"title": "D Feature Support Theory", "content": "While simplifying important features underscores a trend toward computational efficiency, the role of complex features within the model deserves a closer examination. Despite these features often being deemed less important directly, they contribute significantly to the model's overall performance, a paradox that can lead us to introduce the concept of \u201csupport features.\" These are a set of features that may not carry substantial importance individually, but that collectively play a crucial role in the model's decision-making process."}, {"title": "E Complexity and Redundancy", "content": "To further understand the link between feature complexity and redundancy, we utilized the redundancy measure from [73]. Our findings indicate that complex features tend to be less redundant, as depicted in Figure 10. This observation aligns with the strong correlation between our complexity measure and the redundancy-based complexity measure proposed by [23].\nTo quantify redundancy, [73] employed a modified version of Centered Kernel Alignment (CKA) [54], a measure of similarity between two sets of activation features. We briefly recall that CKA between two set of activations A, B in Rd is defined as follows:\n$CKA(A, B) = \\frac{\\|\\mathbf{K}_A \\mathbf{K}_B \\|_F^2}{\\|\\mathbf{K}_A \\mathbf{K}_A \\|_F \\|\\mathbf{K}_B \\mathbf{K}_B \\|_F}$\nwhere KA and KB are the Gram matrices of the feature activations A and B, respectively, and || || F denotes the Frobenius norm.\nIn our analysis, we calculated the CKA measure between a feature z and the activations for a set of 2,000 images from the validation set fn(X). Subsequently, we compared this with the CKA measure when a portion of the activation is masked using a binary mask m \u2208 {0, 1}|Ael, denoted as CKA(z, fn (X) m), where represents element-wise multiplication (Hadamard product). This comparison enabled us to assess whether masking a subset of neurons impacts the decoding of the features. Specifically, to evaluate redundancy, we employed a progressive masking strategy, successively masking 10%, 50%, and 90% of the activation. If the masked activations retain a high CKA with z, it indicates that the information remains largely intact, suggesting that the feature is redundantly stored across multiple neurons, sign of a redundant encoding mechanism within the network. Conversely, if masking results in a substantial decrease in CKA, it implies that the information was predominantly localized on a specific neuron. In this scenario, the feature is not redundantly encoded but rather concentrated in specific neurons. This concentration indicates a lower degree of redundancy, as the loss of these specific neurons (throught the masking) leads to a significant reduction in the CKA score."}, {"title": "F Complexity and Robustness", "content": "To measure robustness, we evaluate the stability of feature responses under perturbations. For each input point x, we add isotropic Gaussian noise with varying levels of standard deviation \u03c3. The robustness score is determined by measuring the variance in the feature response due to the noise. Formally, let z(x) represent the feature response for input x. We define the perturbed input as:\nx = x + N(0, \u03c32\u0399) where N(0, \u03c32I) represents Gaussian noise with mean 0 and variance 02. The sensitivity score Sensitivity(z) for a feature z is given by:\nSensitivity(z) = Var(z(x))\nSpecifically, we sample 100 random noise and repeat this for 3 levels of noise \u03c3\u2208 {0.01, 0.1, 0.5} to compute the variance in feature response for each input from 2,000 samples from the Validation set of ImageNet to get a distribution of feature value. We also consider other metrics such as the range (min-max) of the feature response, but all methods consistently indicate that more complex features are less robust.\nIn summary, our results, as shown in Figure 11, demonstrate that complex features exhibit lower robustness. This indicates that features with higher complexity are more sensitive to perturbations and noise, resulting in greater variability in their responses."}, {"title": "G Importance Measure", "content": "The problem of estimating feature importance is closely related to attribution methods [113, 31, 85, 77, 15, 98, 102, 22], which aim to identify the important pixels for a decision. A recent study has shown that all attribution methods can be extended in the space of concepts [30]. In our case, the features are extracted from the penultimate layer, where the relationship between feature values and logits is linear. We will elaborate on this and demonstrate that the notion of importance in the linear case is easier and optimal methods to estimate importance exist.\nSetup. Recall that for a point x, we can obtain its k feature values by solving an NNLS problem z = arg min || fn(x) \u2013 zD*||F. The vector z contains the k features in Rk. We can replace the activation of the penultimate layer fn(x) with its feature representation in the over-complete basis zD* \u2248 fn(x). Since we are in the penultimate layer, the model's decision, i.e., the logit y \u2208 R for the predicted class, is linearly related to each feature zi by the last weight matrix, denoted as W, as follows:\ny = fn(x)W (3)\n\u2248 zD*W with z = arg min || fn(x) \u2013 zD*||F (4)\n=zW' with W' = D*W \u2208 Rk (5)\nThus, the energy contributed to the logit by feature i can be directly measured by Wizi and y =\nWizi. Consequently, the contribution of a feature zi can be measured using gradient-input, (\u2207(ziy) zi. Several studies [5, 30] have detailed the linear case and shown the optimality of gradient-input with respect to fidelity metrics. They also demonstrated that many methods in the linear case boil down to gradient-input, including Occlusion [113], Rise[85], and Integrated Gradient[102].\nIn our case, we measured the importance by taking the absolute value of the importance vector, i.e., \u0393(zi) = EP2  (11). It is natural to question whether this approach might overlook important features due to their inhibitory effects. Indeed, as depicted in Figure 12, a large number of features may be important not because they add positive energy to the logits, but by inhibition,"}, {"title": "H Kolmogorov, Levin and V-information", "content": "In this section we recall some of the most important complexity measures like Kolmogorov complexity, its computationally tractable counterpart the Levin complexity, and finally we underline the epistemic similarity between these concepts in deep learning.\nKolmogorov complexity [52] is a measure of the complexity of an object. The objects (image, video, text, pdf, etc.) can be ecnoded as a sequence (un) \u2208 \u03a3\u039d of symbols over a finite alphabet \u03a3. \u0391 program is a finite sequence P \u2208 L written in language LC \u2211* (e.g a Python source file). Kolmogorov complexity KL K\u221e) (un) is the length of the shortest program P : N \u2192 \u03a3* that produces the n-th first terms of the sequence (un):\n$K_\\mathcal{L}^{(\\infty)}(u_n) \\stackrel{\\text{def}}{=} \\min_{P(n)=u_n} |P|.$\nIntuitively, if the sequence is highly compressible, the program will be short. For example, the sequences [1, 2, 3, 4, . . .] or [2, 4, 8, 16, 32, . . .] are few lines of code in most programming languages. Conversely, if the sequence is purely random, then no finite-length program exists. The digits of \u03c0, seemingly without structure, are not Kolmogorov random since there exist short programs computing them. The famous Cantor's diagonal argument [19] shows that most sequences are random, since no bijection exists between \u03a3* (countably infinite) and \u2211\u039d (cardinality of the continuum). The definition implicitly assumes a specific computation model (Python interpreter, C++ compiler, Turing machine) to describe the language. However, by definition Turing-complete models can simulate each other, which implies there exist a universal constant C(Python|C++) such that for all sequences un we have Kpython (Un) \u2264 KC) (Un) + C(Python|C++). This constant corresponds to the length of a Python interpreter written in C++ for example. In general, this holds for any other pair of languages.\nTherefore, if KL K\u221e) (un) \u2192 +\u221e as n \u2192 \u221e for some language L, then it is true in every other language: intrinsic randomness is universal in this sense [94].\nLevin complexity. Kolmogorov's complexity suffers from an important drawback: it is not Turing-computable. Put another way, there exists no algorithm that computes K(\u221e). Fortunately, by regularizing K(\u221e) appropriately it is possible to make it computable. Levin [61] proposed to regularize the cost with the runtime T(P, n) of program P on input n. This is the Levin complexity:\n$K_\\mathcal{L}^{(T)}(u_n) \\stackrel{\\text{def}}{=} \\min_{P(n)=u_n} |P|+log|x| T(P, n).$\nThis modification makes K(T) (un, L) computable with the Levin Universal Search algorithm (see Alg. 1). Informally, instead of looking for a shortest program, this algorithm seeks algorithms that run fast among those who are shorts. It is obtained by iterating over lengths i \u2208 N, and by running exactly one step of computation of all these programs in parallel. The first program P that halts on Un minimizes K(T). This is a central property of Levin's universal search: the first programs found are the simplest and the ones requiring the lesser compute [4, 104, 13].\nDeep learning and simplicity bias. The links between algorithmic information theory and deep learning have been a recurring although spurious interest throughout the years [90, 91, 72, 58, 65, 37]. Neural networks are a special kind of program, composed of the source files required for inference, and the weights embedded in the network. Therefore, results related to the complexity of sequences apply transparently. Program length (Kolmogorov) and program runtime (Levin) are tightly linked since deeper and wider networks also consume more FLOPS during inference. Smaller networks implement simpler programs. Similarly, features that can be decoded \"early\" in the network are simpler than those requiring all the layers. The idea is often coined as Minimum Description Length (MDL) principle [16], Occam's Razzor, or even simplicity bias [46]."}, {"title": "I Limitations", "content": "Task. Here, we have studied a specific CNN architecture, ResNet50. In future experiments, it will be useful to investigate whether other model families exhibit similar feature learning, including in domains beyond vision.\nArchitecture. The residual connections of the ResNet are shared by other architectures like Con-vNext [64", "14": ".", "105": "indicate that findings from convolutional models may transfer to ViTs. Furthermore, the work of [112", "87": "found significant quantitative differences between the layers of ResNets and ViTs, highlighting the need for further empirical testing.\nTraining Dynamics The observed dynamics of feature learning, including the emergence of complex features and the reduction in the complexity of important features later in training, are based on a specific training schedule and set of hyperparameters. To accurately attribute these findings, a more comprehensive study is required to evaluate the role of various factors such as the learning rate scheduler and weight decay. Future research should systematically investigate how these and other training parameters influence feature complexity and importance.\nNested predictive families. Our complexity metrics rely on the hypothesis that the different predictive families associated to the network up to depth fe(x) are nested, i.e. that stacking more layers strictly increases expressiveness. This is highlighted in the relevant assumption in section 2. If this hypothesis is violated, the true complexity of a feature may be overestimated in deeper layers. This is typically the case at the early stages of training.\nDictionary of features. Regarding the building of the dictionary using NMF, a previous study [30"}]}