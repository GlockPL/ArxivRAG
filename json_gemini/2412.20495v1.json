{"title": "A Multiparty Homomorphic Encryption Approach to Confidential Federated Kaplan-Meier Survival Analysis", "authors": ["Narasimha Raghavan Veeraragavan", "Svetlana Boudko", "Jan Franz Nyg\u00e5rd"], "abstract": "The proliferation of real-world healthcare data has substantially expanded opportunities for collaborative research, yet stringent privacy regulations hinder the pooling of sensitive patient records in a single location. To address this dilemma, we propose a multiparty homomorphic encryption-based framework for privacy-preserving federated Kaplan-Meier survival analysis, surpassing existing methods by offering native floating-point support, a detailed theoretical model, and explicit mitigation of reconstruction attacks.\nCompared to prior work, our framework provides a more comprehensive analysis of noise growth and convergence, guaranteeing that the encrypted federated survival estimates closely match centralized (unencrypted) outcomes. Formal utility-loss bounds demonstrate that as aggregation and decryption noise diminish, the encrypted estimator converges to its unencrypted counterpart. Extensive experiments on the NCCTG Lung Cancer and a synthetic Breast Cancer dataset confirm that the mean absolute error (MAE) and root mean squared error (RMSE) remain low, indicating only negligible deviations between encrypted and non-encrypted federated survival curves. Log-rank tests further reveal no significant difference between federated encrypted and non-encrypted analyses, thereby preserving statistical validity. Additionally, an in-depth reconstruction-attack evaluation shows that smaller federations (2-3 providers) with overlapping data are acutely vulnerable, a challenge our multiparty encryption effectively neutralizes. Larger federations (5-50 sites) inherently degrade reconstruction accuracy, yet encryption remains prudent for maximum confidentiality.\nDespite an overhead factor of 8-19\u00d7 compared to non-encrypted computation, our results show that threshold-based homomorphic encryption is feasible for moderate-scale deployments, balancing security needs with acceptable runtime. By furnishing robust privacy guarantees alongside high-fidelity survival estimates, this framework significantly advances the state of the art in secure, multi-institutional survival analysis.", "sections": [{"title": "1 Introduction", "content": "In today's data-driven world, the healthcare sector stands to benefit greatly from advanced data sharing, analytics, and artificial intelligence. Nevertheless, the sensitive nature of medical data demands robust privacy and security assurances. In response, federated learning and privacy-preserving cryptographic techniques such as homomorphic encryption have emerged as key enablers for multi-institutional collaboration, ensuring compliance with stringent regulations. Among these methods, federated Kaplan-Meier survival analysis has garnered attention for its effectiveness in estimating patient survival probabilities, a critical element in clinical research.\nRecent advances in Multiparty Homomorphic Encryption (MHE) [1] and Fully Homomorphic Encryption (FHE) [2] enable computations on encrypted data without intermediate decryption, significantly reducing privacy risks. The FAMHE framework [1] demonstrated scalability in federated survival analysis with integers, supporting up to 96 providers. In parallel, Geva et al. [2] showcased multiparty FHE for privacy-preserving analysis of oncological datasets, including Kaplan-Meier and log-rank tests.\nHowever, despite these milestones, several challenges persist. The FAMHE framework proposed in [1] lacks native floating-point arithmetic, limiting its use for real-world datasets requiring scaled survival times and advanced statistical workflows. While Geva et al. [2] provided empirical results, they did not offer a detailed theoretical analysis of noise growth, utility loss, and scalability across large federations. Moreover, neither baseline explicitly addresses vulnerabilities to reconstruction attacks in federated environments, particularly when data overlap occurs among providers.\nData overlap refers to scenarios where patient records appear either partially or fully at multiple institutions. In real-world healthcare, such overlap can arise when patients move between hospitals, receive referrals from specialists, or otherwise share treatment plans across sites. Whenever a substantial fraction of patient information is shared across sites, an adversarial party can exploit that knowledge to \"subtract out\" its own local data from the global aggregates, thereby inferring other sites' private counts or statistics. Consequently, greater overlap typically increases the risk of subtraction-based (reconstruction) attacks, as there are fewer truly unique patient records to obscure the contributions from other institutions.\nTo address these gaps, we propose a novel framework for federated Kaplan-Meier survival analysis leveraging multikey CKKS encryption and a star-based topology for distributed key sharing. Our approach provides a rigorous theoretical foundation for analyzing utility loss and noise growth, thereby ensuring both privacy and scalability. Further, we evaluate the method's robustness against reconstruction attacks, quantify privacy risks, and demonstrate its capacity to scale to large federations."}, {"title": "1.1 Summary of Contributions", "content": "In comparison with earlier baselines [1, 2], our work achieves several notable advancements in multiparty homomorphic encryption-based, privacy-preserving federated Kaplan-Meier analysis:\n1. Stronger Privacy Guarantees\nReconstruction Attack Mitigation: While prior studies largely demonstrated the feasibility of federated approaches, they did not deeply address vulnerabilities stemming from overlapping datasets. By incorporating multikey CKKS encryption and quantifying the risks of reconstruction attacks, this work explicitly tackles and mitigates a critical privacy gap that previous methods left underexplored.\n2. Robust Floating-Point Support\nCKKS Integration: Whereas prior frameworks [1] frequently depended on integer-based homomorphic encryption schemes (e.g., BFV or BGV), our approach leverages CKKS for native floating-point operations, enabling more precise survival times, log-rank tests, and confidence intervals. This obviates the need for coarse numeric approximations inherent in integer-only schemes.\n3. Comprehensive Theoretical Framework\nNoise Modeling and Convergence: Earlier approaches mostly relied on empirical evaluations or partial analyses of noise growth. Here, we develop formal noise-growth models, utility-loss bounds, and convergence guarantees for homomorphically encrypted survival estimates, thereby extending both the rigor and breadth of existing methods.\n4. Empirical Generalizability\nDiverse Datasets & Scenarios: Beyond single or toy datasets, this work applies its method to both a smaller real-world dataset (NCCTG Lung Cancer) [3] [4] and a larger synthetic breast cancer dataset [5] under various federation sizes and overlap conditions. Such extensive testing demonstrates generalizability and highlights performance trends previously under-documented.\n5. Measured Scalability\nRealistic Performance Assessments: While prior studies [1] have shown federations up to specific sizes (e.g., 96 providers), our work offers a detailed performance analysis across up to 50 clients, revealing how computational overhead scales. This enables practitioners to weigh the trade-offs of incorporating encryption in real-world multi-institutional collaborations."}, {"title": "2 Related Work", "content": "Federated survival analysis seeks methods for implementing survival models across distributed datasets while respecting local data ownership. Several studies have explored federated survival analysis under both horizontal and vertical data partitioning [6-9]. These works addressed challenges such as decentralized modeling and aggregation of survival estimates, yet they largely did not consider data security or potential data leakage threats. Consequently, although these approaches facilitate collaboration among healthcare institutions, they do not incorporate advanced cryptographic safeguards to prevent misuse or unauthorized inference of sensitive patient-level information. Furthermore, most existing solutions lack a rigorous theoretical foundation for modeling the impact of distributed computations on survival estimates, such as noise growth or utility-loss analyses under partial federated aggregations.\nIn contrast, research on confidential federated analytics has mostly focused on federated learning (FL), where homomorphic encryption is employed to protect gradients during model updates. Pan et al. [10] introduced FedSHE, an FL scheme using Adaptive Segmented CKKS Homomorphic Encryption to guard against gradient leakage at the aggregation server. However, they assume that all clients (and the Key Management Center) are trusted and do not collude, thus limiting its scope in adversarial scenarios. Similar assumptions arise in [11-13], where a single public-private key pair is shared by multiple clients, again presuming that the clients and/or a trusted key generator will not abuse or collude to reveal private data. While [11] combines homomorphic encryption with authenticated encryption to verify aggregation integrity, and [13] selectively encrypts privacy-sensitive parameters, both rely on a single-key setup. A more general multiparty approach is briefly mentioned in [13], yet the evaluation is confined to communication cost comparisons. Likewise, [14] discusses a single-key homomorphic solution for genomic analyses, noting only in passing how a multiparty extension might be realized. Other studies, such as [15] and [16], also employ single-key or somewhat-homomorphic encryption in FL but assume minimal adversarial behavior.\nThe framework introduced by Froelicher et al. [1] leverages a multiparty encryption library (Lattigo) for genomic data, enabling collaborative survival curve computation, yet it does not fully explore advanced threshold key-management schemes or nuanced reconstruction-attack scenarios. Additionally, while this line of work partly addresses the theoretical underpinnings of homomorphic encryption in distributed settings, deeper analyses of noise propagation, convergence guarantees, and privacy thresholds in survival models remain relatively limited or empirical in nature.\nOverall, although prior studies demonstrate the feasibility of federated survival models and basic integration of homomorphic encryption for FL, the challenge of strong adversarial models\u2014particularly those involving partial collusion, reconstruction attacks, and formal theoretical guarantees remains underexplored in existing approaches."}, {"title": "3 Background", "content": "Homomorphic encryption, as introduced by Rivest et al. (1978) [17], is a form of encryption that enables computations to be performed directly on ciphertexts. The resulting encrypted output, when decrypted, corresponds to the outcome of operations applied to the plaintext. This unique property of homomorphic encryption makes it highly valuable in the fields of data privacy and federated analytics, where sensitive data are processed.\nThe first practical Fully Homomorphic Encryption (FHE) scheme was proposed by Craig Gentry in 2009 [18]. Since then, various homomorphic encryption schemes have been introduced, each aiming to enhance computational efficiency [19-22]. These schemes were initially proposed as single-key homomorphic encryption methods. Although useful in several scenarios, single-key systems are not suitable for federated analytics. In federated analytics, different clients need their own unique secret keys to ensure that their data remains inaccessible to other parties.\nThe problem can be addressed by utilizing threshold homomorphic encryption [23, 24] that combines the principles of homomorphic encryption with threshold cryptography [25-27]. Threshold homomorphic encryption enables multiple parties to jointly perform computations on encrypted data without revealing the underlying plaintext to any individual party. The decryption of the resulting ciphertext requires a minimum number of parties, known as the threshold, to collaborate. Each party holds a share of the decryption key, and the data can be decrypted only when a sufficient number of key shares are combined. This mechanism enhances security by preventing any single party from accessing the complete decryption key.\nSeveral threshold homomorphic encryption schemes have been introduced in the literature and are available as open-source libraries [28, 29]. These schemes support arithmetic operations on both complex numbers and integers using the single-instruction, multiple-data (SIMD) approach. As a result, multiple data points can be encrypted within a single ciphertext. This enables homomorphic operations to be executed simultaneously in a component-wise manner across multiple datasets.\nThe Brakerski-Gentry-Vaikuntanathan (BGV) [19] and Brakerski/Fan-Vercauteren (BFV) [20, 30] schemes are based on the hardness of the Ring Learning with Errors (Ring-LWE) problem, which is a variant of the Learning with Errors problem tailored for rings. The BGV scheme was introduced to address some of the computational inefficiencies found in earlier homomorphic encryption systems. The innovations in managing noise and enabling modulus switching allow it to perform operations faster and with lower computational overhead compared to the first-generation FHE schemes. While the BGV scheme is not inherently equipped to handle unlimited operations due to noise growth, it supports levelled homomorphic encryption. This means that it can handle any number of operations up to a predefined depth. The depth, related to the number of layers in arithmetic circuits that can be evaluated, must be defined in advance during the setup of the encryption parameters. The BFV scheme, a scale-invariant construction, exhibits the same noise growth characteristics as the BGV scheme. Both schemes are specifically designed to perform complex arithmetic over integer arithmetic circuits."}, {"title": "3.1 Homomorphic Encryption", "content": "The Ducas-Micciancio (FHEW) [31] and the Chillotti-Gama-Georgieva-Izabachene (CGGI) [21] schemes are specifically designed to support the encryption of small bit-width integers and are optimized for Boolean circuit evaluation. In the FHEW scheme, the authors introduced an efficient bootstrapping technique that significantly reduces the noise level. In the CGGI scheme, the bootstrapping speed is improved by implementing programmable bootstrapping. This computational operation, performed on a ciphertext during the bootstrapping phase, also reduces noise in ciphertext processing.\nThe Cheon-Kim-Kim-Song (CKKS) scheme [32], is another threshold homomorphic encryption scheme that facilitates approximate homomorphic computations. One of the novel features of the CKKS scheme is its use of a rescaling operation. This operation is crucial for managing the growth of noise in ciphertexts during multiplicative operations. In homomorphic encryption, noise is introduced with each operation performed, and if it grows too large, it can prevent accurate decryption. The rescaling operation in CKKS helps mitigate this issue by scaling down the ciphertext and the noise, thereby enabling deeper computation circuits. Like the BGV scheme, this scheme supports levelled homomorphic encryption. Unlike previously mentioned homomorphic encryption schemes that were limited to integer arithmetic, CKKS allows for approximate calculations on encrypted real and complex numbers. This capability is particularly important for applications involving scientific computations, statistics, and machine learning algorithms that require handling of non-integer data.\nIt is also worth mentioning that these schemes fall under the umbrella of lattice-based cryptography. Currently, there are no known efficient quantum algorithms capable of solving hard lattice problems, which makes lattice-based methods robust against both classical and quantum attacks."}, {"title": "3.2 Survival Analysis: Kaplan Meier Estimation", "content": "Kaplan-Meier estimation is a widely used non-parametric method for analyzing time-to-event data, particularly in survival analysis. Originally introduced by Edward L. Kaplan and Paul Meier in 1958, the Kaplan-Meier estimator provides a way to estimate the survival function, which represents the probability that a given event (e.g., death, disease relapse, or equipment failure) occurs after a specified time. The estimator is especially useful in fields like medical research, engineering, and reliability analysis, where understanding the duration until an event occurs is critical."}, {"title": "3.2.1 Survival Function and Time-to-Event Analysis", "content": "The primary object of interest in Kaplan-Meier estimation is the survival function, denoted by $S(t)$. The survival function $S(t)$ gives the probability that an individual or object will survive beyond a certain time $t$:\n$S(t) = P(T > t),$\nwhere $T$ is the random variable representing the time-to-event. The survival function $S(t)$ typically decreases over time as the probability of an event occurring increases. In many cases, the survival curve, which plots $S(t)$ against $t$, is used to visualize the probability of surviving over time."}, {"title": "3.2.2 Key Concepts in Kaplan-Meier Estimation", "content": "Kaplan-Meier estimation is based on three main concepts:"}, {"title": "3.2.3 Methodology of the Kaplan-Meier Estimator", "content": "The Kaplan-Meier estimator calculates the survival function $\\hat{S}(t)$ by estimating the probability of survival at each observed event time. Let $t_i$ denote the ordered event times in the dataset, $d_i$ the number of events occurring at $t_i$, and $n_i$ the number of individuals at risk just before $t_i$. The probability of surviving beyond $t_i$ is given by $1 - \\frac{d_i}{n_i}$, representing the proportion of at-risk individuals who survive past $t_i$. The Kaplan-Meier estimator is defined as:\n$\\hat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right),$\nwhere the product is taken over all event times $t_i$ up to $t$."}, {"title": "3.2.4 Interpreting the Kaplan-Meier Curve", "content": "The Kaplan-Meier curve is a step function that decreases at each event time $t_i$, representing the cumulative survival probability. Between event times, the survival probability remains constant, resulting in a series of horizontal steps. This curve provides valuable insights into the probability of survival over time, making it possible to compare survival rates across different groups, treatments, or conditions. Additionally, the Kaplan-Meier estimator can be used to compute median survival times and generate confidence intervals around survival estimates."}, {"title": "3.3 Applications of Kaplan-Meier Estimation", "content": "The Kaplan-Meier estimator is extensively applied in medical research to evaluate the effectiveness of treatments, analyze patient survival, and compare different treatment groups. In engineering, Kaplan-Meier estimation is used in reliability testing to assess the durability of components or systems over time. In these applications, handling censored data is essential, as individuals or units may not experience the event by the end of the observation period, and Kaplan-Meier estimation effectively incorporates this aspect.\nDespite its simplicity and non-parametric nature, Kaplan-Meier estimation has proven to be a powerful tool for survival analysis. However, as data privacy regulations have become more stringent, especially in healthcare, federated implementations of Kaplan-Meier estimation are increasingly needed. Such implementations enable institutions to collaborate on survival analysis without sharing sensitive patient data directly, motivating the development of secure computation techniques to support privacy-preserving Kaplan-Meier estimation."}, {"title": "4 Problem Formulation", "content": "In a federated environment, multiple institutions {$I_1, I_2, ..., I_K$} aim to collaboratively estimate a global Kaplan-Meier (KM) survival function $\\hat{S}(t)$ without directly sharing their sensitive data (e.g., individual time-to-event records). Each institution $I_k$ (for $k = 1,..., K$) maintains a local dataset $D_k$ containing:\nEvent times: {t(k)i}, the distinct times at which an event (e.g., death or failure) occurs in $I_k$'s dataset.\nEvent counts: d(k)i, the number of events happening at each local event time t(k)i.\nAt-risk counts: n(k)i, the number of individuals at risk just before each event time t(k)i.\nTo form the global KM estimator, we first collect the union of all local event times into a set of unique times {$t_i$}$_{i=1}^{M}$, sorted in ascending order. For each $t_i$, let\n$d_i = \\sum_{k=1}^{K} d_i^{(k)}, \\qquad N_i = \\sum_{k=1}^{K} n_i^{(k)},$\nwhere $d_i^{(k)}$ and $n_i^{(k)}$ default to 0 if $t_i$ does not appear in institution $I_k$'s dataset. The global Kaplan-Meier survival function then follows the standard definition:\n$\\hat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{N_i}\\right).$\nThis yields the estimated probability that a randomly chosen subject from the combined population remains event-free up to time $t$.\nThe central challenge in this federated setting is computing the aggregated counts $d_i$ and $n_i$ securely across all institutions without revealing any institution's underlying records, thus satisfying privacy regulations and preserving local data autonomy. In the subsequent sections, we describe how threshold homomorphic encryption enables the necessary secure aggregation of event and at-risk counts, culminating in a joint survival estimate that closely matches what would be obtained in a standard, centralized KM analysis."}, {"title": "4.1 Challenges in Federated Kaplan-Meier Estimation", "content": "Although federated Kaplan-Meier analysis avoids centralizing raw patient-level data, several key obstacles arise when securely computing the global at-risk and event counts:\n1. Data Privacy Across Institutions: Simply sharing raw at-risk counts $n_i^{(k)}$ and event counts $d_i^{(k)}$ can lead to privacy breaches, particularly in institutions with small populations. Regulations such as GDPR and HIPAA mandate stringent data confidentiality practices, prohibiting patient-level detail exchange.\n2. Secure Aggregation Without Decryption: Traditional systems typically decrypt data before computation, reintroducing privacy risks. In federated KM estimation, we must aggregate encrypted counts so that neither the server nor other participants learn each institution's local values.\n3. Decentralized Key Management: Many federated environments require each institution to retain control over its own data and cryptographic keys, rather than relying on a single trusted authority. This motivates threshold or multiparty encryption schemes, wherein partial decryption shares are combined to recover the aggregated result without granting any one party full decryption power.\n4. Reconstruction Attacks with Overlapping Data: Even when institutions share only aggregated statistics, an adversarial party can attempt a reconstruction attack by subtracting its own local counts from the federated totals. Such inferences become especially potent when patient records overlap across multiple sites.\nIn Section 7.4, we further examine how data overlap and federation size impact reconstruction accuracy and present empirical evidence of this vulnerability.\nTo address these challenges, we propose a threshold homomorphic encryption approach that preserves data privacy during aggregation and mitigates the risks posed by reconstruction attacks, as detailed in the subsequent sections."}, {"title": "5 Proposed Solution", "content": "In this section, we detail a threshold homomorphic encryption (HE) framework designed to enable privacy-preserving, federated Kaplan-Meier estimation across multiple institutions. We begin by outlining the core assumptions, notations, and threat model, followed by a description of the distributed key generation process and, finally, the federated Kaplan Meier computation steps under the threshold HE scheme."}, {"title": "5.1 Assumptions, Definitions, and Notations", "content": "The proposed framework is specifically designed to facilitate collaborative analysis and encrypted sensitive data sharing among health care institutions, with only authorized personnel allowed to add and process data, and initiate and carry out the federated procedures. Given these circumstances, we anticipate that the inherent boundaries of the Honest-but-Curious threat model will apply. We consider the following assumptions."}, {"title": "5.1.1 Threat Model Assumptions", "content": "Semi-Honest (Honest-but-Curious) Participants: Institutions and the central server follow the protocol faithfully (no active data tampering or insertion of malicious operations), yet they may attempt to glean additional information from the data or partial decryptions they are legitimately allowed to see.\nLimited (Sub-Threshold) Collusion: We assume no coalition of institutions large enough to meet or exceed the threshold $T$ colludes to combine their secret key shares and decrypt data without authorization. If fewer than $T$ participants collude, they cannot decrypt the aggregated ciphertexts. Those holding enough shares to surpass the threshold are presumed to adhere to legitimate protocol steps rather than colluding maliciously.\nNo Malicious Tampering: While participants may be curious, we assume they do not intentionally alter data, generate invalid partial decryptions, or otherwise disrupt protocol integrity. The system functions in a semi-honest environment, not a fully malicious one.\nStable Federation Membership: The set of institutions taking part in the analysis remains fixed throughout the key generation and partial decryption phases. Institutions do not join or leave mid-protocol, avoiding complexities with dynamic key sharing and membership changes.\nUnder these assumptions, threshold homomorphic encryption and secure multi-party protocols prevent any single entity (including the central server) from decrypting sensitive data. This design thereby safeguards individual-level information, particularly in scenarios where curious participants might attempt to infer hidden details from aggregated counts or partial decryption results."}, {"title": "5.1.2 Notations and Key definitions", "content": "Let:\nK denote the number of participating institutions.\n$I_k$ be the k-th institution (for k = 1, ..., \u039a).\nEach institution $I_k$ (also called a local party, client, or data provider) holds its own local dataset $D_k$, consisting of:\n{t(k)i}: the event times for each observed event. We treat these as non-sensitive, meaning they do not require encryption. A global (federated) set of time points is then formed by aggregating all {t(k)i} in the clear (i.e., unencrypted) across participating institutions, consistent with the assumptions of the FAMHE framework [1].\nd(k)i: the number of events at each time t(k)i,\nn(k)i: the number of individuals at risk just prior to each event time t(k)i.\nT denotes the threshold specifying the minimum number of institutions required to perform decryption.\nAll institutions share a common public key pk for the threshold homomorphic encryption scheme, while each institution $I_k$ holds a unique decryption key share skk.\nOnly a subset of at least $T$ institutions can collectively decrypt data; fewer than $T$ key shares are insufficient for decryption."}, {"title": "5.2 Distributed Key Generation Process", "content": "To enable threshold-based encryption and decryption, we employ a distributed key generation process. While there are different possible network topologies (e.g., star-based vs. ring-based), in this work, we focus on a star-based approach.\nStar-Based Topology:\nA central server (coordinator) manages the key generation steps, instructing each institution in sequence to update a shared public key.\nAfter all designated institutions have contributed, the final public key is distributed to every participant.\nRing-Based Topology (Not Implemented Here):\nEach institution would be connected to exactly two others in a ring structure, passing updated keys in a loop."}, {"title": "5.3 Algorithm for Federated Kaplan-Meier Estimation", "content": "Once the distributed key generation is complete, each institution (local party/client/data provider) holds a local secret key share, and a shared public key is available to all parties."}, {"title": "5.3.1 Algorithm for Each Local Party (Institution Ik)", "content": "Each participating institution encrypts its own event and at-risk counts before sending them to the central server. By doing so, the raw data never leave the institution in plaintext form. Using the shared public key pk, each institution transforms its local counts {$d_i^{(k)}, n_i^{(k)}$} into ciphertexts. This ensures that even if messages are intercepted or the central server is compromised, individual patient-level information remains protected. The local parties thus retain control over their data while still contributing to the federated Kaplan-Meier computation."}, {"title": "5.3.2 Algorithm for the Central Server", "content": "The central server acts primarily as a secure aggregator in this threshold homomorphic encryption setup. It receives encrypted event and at-risk counts from each institution and performs the necessary homomorphic additions to obtain aggregate ciphertexts without ever learning the underlying plaintext values. Once these aggregated ciphertexts are computed, the server distributes them to the threshold group for partial decryption, thus coordinating the overall process without having access to raw patient data at any point. Throughout this aggregation, no weighting is applied, ensuring that all institutions' data is treated equally."}, {"title": "5.3.3 Collaborative Decryption and Kaplan-Meier Computation", "content": "Once decrypted, each aggregated count $d_i$ and $n_i$ can be used to compute the classical KM estimator. Because only the threshold group can recover these plaintext values, confidentiality is preserved under the assumption that no unauthorized coalition meets or exceeds $T$ key shares."}, {"title": "5.4 Theoretical analysis", "content": "Theorem 1 (Utility Loss Bound in Federated Homomorphic Encrypted Kaplan-Meier Estimators for Worst Case Scenario). Let $\\hat{S}_{centralized}(t)$ and $\\hat{S}_{federated}(t)$ denote the Kaplan-Meier survival probabilities estimated at time $t$ in centralized and federated homomorphic encrypted (HE) settings, respectively. Assume:\n1. The event counts $d_i^{(k)}$ and at-risk counts $n_i^{(k)}$ from client $k$ are encrypted and aggregated using CKKS homomorphic encryption.\n2. Noise is introduced during two distinct phases:\n(a) Aggregation Noise $\\epsilon_{aggregation}$: Noise from ciphertext operations, such as addition and multiplication, which depends on the total number of ciphertexts contributed by all clients.\n(b) Decryption Noise $\\epsilon_{decryption}$: Noise from partial decryption and multiparty decryption fusion, involving a subset of $T < K$ clients.\n3. Each client contributes a dataset of size $D_k$, resulting in $M_k$ ciphertexts proportional to $D_k$.\n4. A common ciphertext modulus $q$ is used across all clients.\n5. At-risk counts $n_i > 0$ for all $t_i$, as zero would render survival probabilities undefined.\nUnder these assumptions, the utility loss $\\Delta S(t)$ and error growth can be analyzed as follows:\nUtility Loss Bound:"}, {"title": "5.5 Theoretical analysis", "content": "$\\Delta S(t) = |\\hat{S}_{centralized}(t)-\\hat{S}_{federated}(t)| < \\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{N_i}\\right)\\frac{O\\left(log(q) \\cdot \\sum_{k=1}^K M_k\\right) + O(\\sigma \\cdot \\sqrt{T})}{N_i + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)},$\nwhere $M_k \\propto D_k$ reflects the relationship between dataset size and the number of ciphertexts.\nFor large $n_i$, the bound simplifies to:\n$\\Delta S(t) \\leq \\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{N_i}\\right)\\frac{O\\left(log(q) \\cdot \\sum_{k=1}^K M_k\\right) + O(\\sigma \\cdot \\sqrt{T})}{N_i}.$\nProof. 1. Centralized Kaplan-Meier Survival Probability:\nThe centralized Kaplan-Meier survival probability is calculated as:\n$\\hat{S}_{centralized}(t) = \\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{n_i}\\right),$\nwhere $d_i$ is the event count, and $n_i$ is the at-risk count at time $t_i$.\n2. Federated Kaplan-Meier Survival Probability:\nIn the federated setup, the survival probability is computed using encrypted event counts $d_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)$ and at-risk counts $n_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)$.\nThe federated survival probability is:\n$\\hat{S}_{federated}(t) = \\prod_{t_i \\leq t} \\left(1-\\frac{d_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}{n_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}\\right)$.\n3. Error in Survival Probability:\nThe utility loss $\\Delta S(t)$ is defined as the absolute difference:\n$\\Delta S(t) = |\\hat{S}_{centralized}(t) \u2013 \\hat{S}_{federated}(t)|.$"}, {"title": "5.6 Theoretical analysis", "content": "Expanding both terms:\n$\\Delta S(t) = |\\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{N_i}\\right) - \\prod_{t_i \\leq t} \\left(1-\\frac{d_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}{n_i^{(k)} + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}\\right)|.$\n4. Bounding Noise Growth:\nAggregation noise reflects the contribution from all clients:\n$\\epsilon_{aggregation}(t_i) = O \\left(log(q) \\cdot \\sum_{k=1}^{K}M_k\\right)$.\nwhere $M_k \\propto D_k$.\nDecryption noise depends on the subset $T$ of clients involved in decryption:\n$\\epsilon_{decryption}(t_i) = O(\\sigma \\cdot \\sqrt{T}),$\nwhere $\\sigma$ is the standard deviation of noise during decryption.\nTotal noise combines aggregation and decryption noise:\n$\\epsilon(t_i) = O \\left(log(q) \\cdot \\sum_{k=1}^{K}M_k\\right) + O(\\sigma \\cdot \\sqrt{T}).$\n5. Final Utility Loss Bound:\nSubstituting the noise terms into the cumulative error:\n$\\Delta S(t) \\leq \\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{N_i}\\right)\\frac{O\\left(log(q) \\cdot \\sum_{k=1}^K M_k\\right) + O(\\sigma \\cdot \\sqrt{T})}{n_i + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}.$\nFor large $n_i$:\n$\\Delta S(t) \\leq \\prod_{t_i \\leq t} \\left(1-\\frac{d_i}{N_i}\\right)\\frac{O\\left(log(q) \\cdot \\sum_{k=1}^K M_k\\right) + O(\\sigma \\cdot \\sqrt{T})}{n_i}.$\nThe utility loss bound explicitly accounts for both aggregation and decryption noise. Proper parameter tuning, including scaling factors, batch sizes, and noise distributions, is essential to minimizing this loss.\nTheorem 2 (Utility Loss Bound for Bounded At-Risk Counts with Noise Terms). If the at-risk counts $n_i$ are bounded below by a constant $c > 0$, and the noise terms from aggregation and decryption are included in the denominator, the utility loss bound can"}, {"title": "5.7 Theoretical analysis", "content": "be expressed as:\n$\\Delta S(t) \\leq \\frac{1"}, {"to": "n$\\Delta S(t) \\leq \\frac{1"}, {"Terms": "nFrom the utility loss theorem", "is": "n$\\Delta S(t) \\leq \\prod_{t_i \\leq t"}, "left(1-\\frac{d_i}{N_i}\\right) \\sum_{t_i \\leq t} \\frac{\\epsilon(t_i)}{N_i + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i)}.$\nSubstituting the expressions for $\\epsilon_{aggregation}(t_i)$ and $\\epsilon_{decryption}(t_i)$:\n$\\epsilon_{aggregation}(t_i) = O\\left(log(q) \\cdot \\sum_{k=1}^K M_k\\right), \\qquad \\epsilon_{decryption}(t_i) = O(\\sigma \\cdot \\sqrt{T}).$\nThe denominator becomes:\n$N_i + \\epsilon_{aggregation}(t_i) + \\epsilon_{decryption}(t_i) = n_i"]}