{"title": "Verbosity-Aware Rationale Reduction:\nEffective Reduction of Redundant Rationale via Principled Criteria", "authors": ["Joonwon Jang", "Jaehee Kim", "Wonbin Kweon", "Hwanjo Yu"], "abstract": "Large Language Models (LLMs) rely on gen-\nerating extensive intermediate reasoning units\n(e.g., tokens, sentences) to enhance final an-\nswer quality across a wide range of complex\ntasks. While generating multiple reasoning\npaths or iteratively refining rationales proves\neffective for improving performance, these\napproaches inevitably result in significantly\nhigher inference costs. In this work, we pro-\npose a novel sentence-level rationale reduction\ntraining framework that leverages likelihood-\nbased criteria, verbosity, to identify and remove\nredundant reasoning sentences. Unlike previ-\nous approaches that utilize token-level reduc-\ntion, our sentence-level reduction framework\nmaintains model performance while reducing\ngeneration length. This preserves the original\nreasoning abilities of LLMs and achieves an\naverage 17.15% reduction in generation costs\nacross various models and tasks.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models\n(LLMs) have demonstrated remarkable reasoning\ncapabilities comparable to human cognitive abili-\nties (Madaan et al., 2024; Shinn et al., 2024; Ku-\nmar et al., 2024). These models can solve com-\nplex reasoning tasks by explicitly generating ex-\ntended reasoning paths. The generation of such\npaths involves producing explicit reasoning units\n(e.g., tokens, steps) (Yu et al., 2024b), which ad-\nditionally enhances model performance through\niterative prompting (Wang et al., 2023; Yao et al.,\n2023). This iterative generation of explicit reason-\ning paths allows the model to refine and expand its\nthought processes, incorporating strategic planning\nand ongoing cognitive generation (Xi et al., 2023;\nYang et al., 2024).\nAlthough the extensive generation of explicit\nreasoning units leads to better peformance, it inher-\nently leads to higher inference costs and increased\nlatency (Yu et al., 2024b; Wang et al., 2024). More-\nover, fine-tuning LLMs using a complete reasoning\npath does not consistently guarantee stronger per-\nformance (Yu et al., 2024b; Deng et al., 2024b;\nLiu et al., 2024), suggesting the need for meth-\nods that can maintain reasoning performance while\nreducing the generation of reasoning units. De-\nspite this apparent requirement, it remains underex-\nplored how to maintain LLM reasoning capabilities\nwhile reducing intermediate reasoning paths across\ndiverse tasks.\nPrevious methods (Yu et al., 2024b) have drawn\ninspiration from human cognitive processes to ad-\ndress the aforementioned limitations. Some studies\nhave proposed training pipelines that utilize aug-\nmented datasets, iteratively generated by founda-\ntion LLMs, to fine-tune subsequent LLMs (Yu et al.,\n2024b; Liu et al., 2024). However, they are inher-\nently vulnerable as they significantly depend on the\ngenerative capabilities of LLMs. In response, other\nworks have directly trained LLMs without augment-\ning datasets that facilitate implicit reasoning before\nanswer generation. Deng et al. (2023) introduced a\nknowledge distillation method to distill explicit rea-\nsoning into implicit reasoning through token-level\nhidden states. Deng et al. (2024b) selected tokens\nas the reasoning unit for reduction and proposed a\nheuristic method to internalize explicit intermediate\nrationale tokens. They progressively eliminate to-\nkens from the beginning of the full rationale during\nthe Chain of Thought (CoT) fine-tuning. However,\nthese methods present a fundamental limitation as\nthey lack sufficient justification for selecting to-\nkens over more linguistically natural units (e.g.,\nsentences) for reduction (Table 1), and they fail to\nprovide principled criteria for the removal process.\nFurthermore, their evaluation has been predomi-\nnantly conducted on synthetic arithmetic reasoning\ntasks, which presents limitations for real-world ap-\nplications.\nTo address these limitations, we propose a novel"}, {"title": "2 Related Works", "content": "2.1 Performance-Cost Tradeoffs in Reasoning\nPath Generation\nRecent research has demonstrated the importance\nof generating iterative and refined reasoning paths\nto improve the reasoning capabilities of models, al-\nbeit at increased computational costs (Wang et al.,\n2023; Yao et al., 2023; Radha et al., 2024; Wang\net al., 2024; Madaan et al., 2024; Shinn et al., 2024;\nKumar et al., 2024). Self-Consistency (Wang et al.,\n2023) improves reasoning accuracy by generating\nmore than 20 distinct reasoning paths and selecting\nthe most consistent answer using ensemble meth-\nods, resulting in substantial computational over-\nhead. Similarly, Tree of Thoughts (ToT) (Yao\net al., 2023) expands the exploration of reasoning\npaths using a tree-structured method, while Strate-\ngic Chain of Thought (SCoT) (Wang et al., 2024)\nutilizes an Inner Dialogue Agent to adaptively ex-\nplore the diverse reasoning path.\nConcurrently, Self-Refine (Madaan et al., 2024)\nutilizes an iterative framework that incorporates\nfeedback to refine responses and enhance reason-\ning accuracy, necessitating multiple forward passes\nthrough the model. Additionally, the Reflexion-\nbased framework (Shinn et al., 2024; Kumar et al.,\n2024) guides models to iteratively generate rele-\nvant reasoning paths through reflection for accurate\nanswer generation. Given the necessity of iterative\ngeneration and refinement of reasoning paths to\nachieve optimal performance, they inherently in-\ncrease inference costs and latency. Therefore, it\nis essential to investigate methods for efficiently\ngenerating these paths.\n2.2 Reasoning Path Reduction\nTo address the computational costs associated with\nextensive reasoning paths generation, some lines\nof work (Yu et al., 2024b; Liu et al., 2024) have\nfocused on generating augmented datasets with\nvarying rationale lengths to shorten the generation\nof reasoning paths. Yu et al. (2024b) guide model\nto generate multiple reasoning paths using Self-\nConsistency to construct datasets, subsequently\nfine-tuning the model to generate direct answers\nwithout intermediate reasoning steps. Liu et al.\n(2024) developed a heuristic approach to merge\nreasoning steps and iteratively trained the model to\nproduce shorter reasoning paths, which are subse-\nquently incorporated into the progressive training\nphase. While these approaches demonstrate em-\npirical effectiveness, they present two fundamental\nlimitations: (1) their heavy dependence on LLM\ngeneration capabilities introduces inherent insta-\nbility, and (2) their objective of reducing reason-\ning path necessitates the paradoxical creation of\ndatasets requiring extensive reasoning path genera-\ntion.\nTo address these weaknesses, another line of\nwork (Deng et al., 2023, 2024b) has focused on\ndirectly training LLM without augmented datasets.\nImplicit-CoT (Deng et al., 2023) utilizes a multi-\nmodel framework where an emulator model is\ntrained to predict the teacher's token-level hidden\nstates, and a student model uses these predicted\nstates to generate answers. ICoT-SI (Deng et al.,\n2024b) identifies tokens as reduction units, propos-\ning a method to internalize explicit intermediate\nrationale tokens by progressively removing them\nfrom the beginning of the reasoning path within the\nCoT fine-tuning process. However, these methods\nhave shown limited generalization across diverse\ndatasets as they have only been validated on simple\narithmetic reasoning tasks, such as multiplication\nproblems. This limitation raises concerns about\ntheir applicability to real-world scenarios where"}, {"title": "3 Early Stage Rationales are Redundant", "content": "rationales are expressed in natural language. More-\nover, they do not justify their criteria for token-\nlevel removal nor consider whether more linguis-\ntically natural units (e.g., sentences) might serve\nas more effective reduction units. Specifically, the\ntoken-level removal approach may inadvertently\neliminate critical information necessary for answer\ngeneration or distort the semantic information of\nthe sentence, while requiring repeated removal of\nredundant reasoning steps. Motivated by the com-\nputational costs of the extensive reasoning path\ngeneration and the limitations of existing reduction\napproaches, we investigate the redundancy of var-\nious sentence positions for potential removal and\npropose a novel method with principled criteria\nthat can efficiently reduce reasoning paths while\nmaintaining their effectiveness."}, {"title": "3.1 Quanitifying the redundancy", "content": "Before delving into the method, we first investi-\ngate which positions within the rationale sentences\nshould be selected for reduction. When the like-\nlihood of the answer remains unchanged after a\nsentence is removed from the full rationales, it in-\ndicates that the sentence may be redundant for the\nreasoning process. To quantify the redundancy of a\nsentence, we compute the negative log-likelihood\n(NLL) for answer y after the sentence reduction as\nfollows:\n$NLL = - log p_\\theta(y|R',x)$,\nwhere $R' = R \\setminus \\{r_i\\}_{i \\in S}, S \\subseteq I$.\nLet R denote the complete set of rationale sen-\ntences, and I represent the full index set of these\nsentences. The subset of indices corresponding to\nsentences selected for reduction is denoted by S,\nand $R'$ represents the remaining rationale sentences\nafter their removal. $\\{r_i\\}_{i \\in S}$ denotes the sentences\nselected for reduction. For simplicity, we use $\\{r_i\\}$\nwithout the subset index notation throughout the\nrest of the paper."}, {"title": "3.2 Redundancy of leading sentences", "content": "We conduct a pilot study to empirically demon-\nstrate the redundancy of the leading sentences\nwithin the rationales. By varying the size of $\\{r_i\\}$\nfrom 1 to 4, we calculate the NLL of Mistral\n7B (Jiang et al., 2023) across different reasoning\ndatasets. To explore more general patterns of sen-\ntence removal, we employ a stochastic strategy in\nour experimental setup. We compare three con-\nfigurations for sampling $\\{r_i\\}$: front, random, and\nback. In the front configuration, initial sentences\nare assigned a higher probability when forming\n$R'$, whereas random and back configurations use\nuniform and progressively increasing probabilities,\nrespectively\u00b9. Additionally, we computed the NLL\nfor full rationale sentences (i.e., \u2013 log $p_\\theta (y|R, x)$)\nas a baseline to evaluate the impact of reduction. As\ndepicted in Figure 1, the NLL difference between\nthe full and reduced rationales shows a marginal\nlikelihood difference in the front (<) configuration.\nIn contrast, removing sentences randomly (\u25ca) or\nfrom the back (>) results in higher NLL as the\nremoved sentences increase, highlighting the im-\nportance of the selection of removal position in\nthe reasoning and answer prediction process (for\nadditional data, see Appendix A).\n\u00b9For generating $R'$, we assign probabilities $p_k = \\frac{N-k+1}{\\sum_{i=1}^{N}i}$ (front), $\\frac{1}{N}$ (random), and $\\frac{k}{\\sum_{i=1}^{N}i}$ (back) where k=1,...,N de-\nnotes sentence position."}, {"title": "4 Verbosity-Aware Rationale Reduction", "content": "Based on the above observation, we propose\nthe Verbosity-Aware Rationale Reduction (VARR)\nframework. In Section 4.1, we introduce the con-\ncept of 'verbosity' as a principled criterion for iden-"}, {"title": "4.1 Verbosity", "content": "To quantify the redundancy of the sentences for\nremoval, we first introduce a foundational concept\n\u2018verbosity'. Given an input x, full rationale R, and\na reduced rationale $R' = \\{r_j\\}_{j\\in I\\setminus\\{i\\}}$, we measure\nthe verbosity of a sentence $r_i$ on y by measuring\nthe difference in Kullback-Leibler divergence (KL-\ndivergence) as follows:\n$verbosity(y) = D_{KL}(q(y|x) || P_\\theta(y|R, x))$\n$- D_{KL}(q(y|x) || P_\\theta(y|R', x))$,\nwhere q(yx) is the ground truth distribution. The\nverbosity(y) highlights the informational contribu-\ntion or redundancy of a rationale sentence $r_i$ on\nanswer y. Since q(y|x) is the form of the one-hot\nvector (i.e., Dirac delta function), the verbosity(y)\ncan be computed as the log-likelihood ratio be-\ntween R and R' as follows:\n$verbosity(y_g) = [H_q(p_\\theta(y|R, x)) \u2013 H(q(y|x))]$\n$-[H_q(p_\\theta(y|R', x)) \u2013 H(q(y|x))]$\n$= E_q[-log P_\\theta(y|R, x)]$\n$+ E_q[log P_\\theta(y|R', x)]$\n$= log \\frac{(P(y_g|R', x))}{P(y_g|R, x)}$,\nwhere $y_g$ is the ground truth answer (i.e., $q(y_g|x) =$\n1). $H_q(\u00b7)$ and H(\u00b7) denote the cross-entropy and\nthe entropy, respectively. Intuitvely, a higher value\nof verbosity($y_g$) implies that the likelihood of the\nmodel generating the ground truth answer increases\nafter removing $r_i$, indicating that its removal is\nbeneficial."}, {"title": "4.2 Verbosity Identification in CoT Training", "content": "Given an input sequence, the CoT training (Nye\net al., 2021) aims to train LLMs to generate full\nrationale, followed by the ground truth answer with\nthe following loss:\n$- log p_\\theta (y_g, R|x)$.\nIn each training step t, each sentence $r_i$ within R\nis assessed with the following the equation:\n$verbosity(y_g) \u2265 0$."}, {"title": "4.3 Contrasting with Wrong Answer", "content": "Inspired by the miscalibrated log-likelihoods be-\ntween accepted and rejected responses during stan-\ndalone Supervised Fine-Tuning (SFT) in alignment\nlearning (Rafailov et al., 2024; Azar et al., 2024;\nHong et al., 2024), we also assess whether the re-\nduced rationales R' result in inaccuracy answer\ngeneration by introducing a wrong answer $y_w$. In-\nstead of employing the correct answer distribution\nq(yx) in Equation (2), we initiate our formula with\nthe wrong answer distribution q' (i.e., 1 \u2013 q(y|x),\nnormalized sum to 1) Through algebraic manipula-\ntion, we can obtain:\n$D_{KL}(q'(y|x) || p_\\theta(y|R, x))$\n$- D_{KL}(q'(y|x) || P_\\theta(y|R',x))$\n$= [H_w(p_\\theta(y|R,x)) \u2013 H(q'(y|x))]$\n$-[H_w(p_\\theta(y|R', x)) \u2013 H(q'(y|x))]$,\nwhere $H_w(\u00b7)$ denotes the cross-entropy calculated\nwith the wrong answer distribution $q'$. Due to the\nimpracticality of computing the expectation over\nthe entire space of V \u2013 1 wrong answers (where\nV is the vocabulary size), we sample K incorrect\nanswers for the following estimations:\n$E_w [-log p_\\theta (y|R, x)] + E_w[log p_\\theta(y|R', x)]$\n$=E_w [log \\frac{P_\\theta(y|R',x)]}{P_\\theta(y|R, x)}$.\n$\\approx\\frac{1}{K} \\sum_{k=1}^{K} log \\frac{(y_w^{(k)}|R', x)}{(y_w^{(k)}|R, x)}$,\nwhere $\\{Y_w^{(k)}\\}_{k \\in [K]}$ is sampled from the in-batch\nnegatives depending on the dataset. Consequently,\nthe verbosity($y_w$) is computed as:\n$verbosity(y_w) = \\frac{1}{K} \\sum_{k=1}^{K} log \\frac{(y_w^{(k)}|R', x)}{(y_w^{(k)}|R, x)}$.\nSince computational constraints necessitate sam-\npling incorrect answers to calculate verbosity($y_w$),"}, {"title": "4.4 CoT Training with Rationale Reduction", "content": "In our framework, the model is trained using Equa-\ntion (4) for predefined warm-up steps to inject its\nreasoning capabilities. After the warm-up steps, at\neach training step t, each sentence $r_i$, starting from\nthe first index sentence in R, is evaluated against\neither Equation (5) alone (denoted as VARR) or\nboth Equations (5) and (9) (denoted as VARR+).\nSentences satisfying these respective criteria are\nselected for removal and are not included in sub-\nsequent training steps. The maximum removable\nnumber of sentences at each training step t is deter-\nmined based on a linear schedule, adopting Deng\net al. (2024b)'s setting as follows:\n$r(t) = [N_t \u00b7 (t/T)]$,\nwhere T represents the total number of training\nsteps, $N_t$ is the total number of rationale sentences\nat step t, and r(t) indicates the maximum number\nof sentences that can be removed at that step."}, {"title": "5 Experiments", "content": "5.1 Training Configuration\nDatasets We conducted experiments across two\ncategories to provide a comprehensive evaluation\nof VARR's versatility and effectiveness, in con-\ntrast to prior research that predominantly focuses\non simple arithmetic tasks like multi-digit multi-\nplication (Deng et al., 2023, 2024b). Initially, we\nevaluate with arithmetic reasoning tasks, including\ndatasets like GSM8K (G8K; Cobbe et al. 2021a)\nand MathQA (MQA; Amini et al. 2019a). We\nalso examine the performance of our method on\ncommonsense reasoning tasks, employing datasets\nincluding CommonsenseQA (CQA; Talmor et al.\n2019), TrivaiQA (TQA; Joshi et al. 2017), and\nStrategyQA (SQA; Geva et al. 2021)."}, {"title": "5.2 Baselines", "content": "We compare our method to the following baselines:\nExplicit-CoT (Nye et al., 2021), where the model\nis finetuned with explicit chain-of-thought reason-\ning, and ICoT-SI (Deng et al., 2024b), where the\nmodel is fine-tuned using a linear token elimination\nschedule. It is important to note that after removing\nintermediate reasoning paths, the model's perfor-\nmance should be competitive with Explicit-CoT.\nConsequently, ICoT-SI serves as our primary base-\nline. We exclude Implicit-CoT (Deng et al., 2023)\nfrom our baselines due to its significant memory\nrequirements, requiring the training of three mod-\nels on a single GPU. Furthermore, according to\nDeng et al. (2024b), this method underperformed\ncompared to ICoT-SI. All baselines were trained"}, {"title": "5.3 Evaluation", "content": "We employ two evaluation metrics: First, we evalu-\nate the accuracy of each method in generating the\nfinal answer for the respective tasks. Second, we\ncount the generated tokens to evaluate reasoning\nefficiency while maintaining performance."}, {"title": "5.4 Main results", "content": "In Figure 3, we present the results for each\nreasoning task across different models. Our\nmethod demonstrates consistent performance im-\nprovements or maintains comparable results across\nmost benchmarks, while also achieving more effi-\ncient reasoning path generation. Notably, we ob-\nserved significant improvements in average accu-\nracy with increments of 2.59%, 6.27%, and 14.27%\nfor Mistral 7B, Llama3.2 1B, and Llama3.2 3B\n(VARR+), respectively, compared to Explicit CoT,\nwhile efficiency improved by 19.35%, 17.24%, and\n14.88% for these models. These findings indicate\nthat some reasoning sentences are redundant and\ncan even disturb answer generation, highlighting\nthat effective reasoning capabilities can be learned\nthrough selective utilization of essential reasoning\nunits.\nFor ICoT-SI, the model exhibited performance\ndegradation after token removal training. Specif-\nically, ICoT-SI shows an average performance\ndegradation of 18.93% compared to Explicit CoT."}, {"title": "5.5 Ablation Studies", "content": "In this section, we conduct ablation studies to em-\npirically validate our method. All experiments are\nimplemented using Mistral 7B due to its higher\nbase capacity compared to other models."}, {"title": "5.5.1 Identifying Appropriate Units for\nRemoval", "content": "In this section, we experimentally demonstrate\nwhy the sentences are suitable units for reduction\nreasoning units. Unlike ICoT-SI which lacks spe-\ncific criteria for token removal, we apply VARR+\nat the token level (denoted as VARR-Tok) to assess"}, {"title": "5.5.2 Analyzing the Impact of Sentence\nPosition on Removal Efficacy", "content": "In Section 3, we demonstrated that gradually re-\nmoving sentences from random and back positions\ncan degrade model performance. To further explore\nthis finding and assess the robustness of removing\nsentences from the front position, we conducted ex-\nperiments with unguided random sentence removal\n(denoted as No Rule) and applied VARR+ with ran-\ndom position and reverse sentence order (denoted\nas Random and Back, respectively). As shown\nin Table 2, unguided random sentence removal re-\nsulted in a 25.30% decrease in performance rela-\ntive to our method, highlighting the critical role of\nsystematic configuration even after selecting sen-\ntences as the units of reduction. Additionally, Ran-\ndom and Back strategies result in an average 7.50%\nperformance degradation compared to our method.\nThis finding again indicates that earlier sentences\nin the reasoning path tend to be more redundant,"}, {"title": "5.5.3 Reinitializing the Optimizer", "content": "We reinitialized the optimizer after each training\nepoch to stabilize training, inspired by Deng et al.\n(2024b). We employed the AdamW (Loshchilov\nand Hutter, 2019) optimizer, where the first and sec-\nond moments are gradually updated based on cur-\nrent gradients, which could lead to unstable train-\ning in our framework. Specifically, after applying\nVARR throughout the dataset, a reduction in the\nrationale for some data between epochs could lead\nto instability in the training process. With 19.32%\nperformance gain through reinitializing, we recon-\nfirmed that deng's findings can be robust across\ndiverse dataset, where reasoning paths are with\ncomplex semantic and syntactic structures. With\na 19.32% performance gain achieved by reinitial-\nizing the optimizer, we reconfirm that Deng et al.\n(2024b)'s findings are robust across diverse datasets\nwith complex semantic and syntactic structures in\nreasoning paths."}, {"title": "5.5.4 Varying the Warm-up Steps", "content": "We evaluated our method against various warm-\nup stages on the TriviaQA and CommonsenseQA\ndatasets. As the duration of the warm-up stages\nincreases, the model becomes more fitted to the\nnon-reduction dataset, which inhibits VARR's abil-\nity to eliminate redundant sentences from the rea-\nsoning path. Consequently, as illustrated in Figure\n4, longer warm-up periods result in an increase\nin generated tokens and a decrease in accuracy.\nThese results suggest that 0.1 training steps pro-\nvide sufficient time to inject reasoning abilities\nwhile enabling the systematic reduction of redun-\ndant reasoning sentences during the learning pro-\ncess. Therefore, we set 0.1 as the default setting\nfor the warm-up steps."}, {"title": "5.5.5 Removal Ratio Analysis", "content": "In Figure 5, we analyze the actual amount of\nrationale sentences removed during training. We\nexamine it by calculating the removal ratio, the\nproportion of actual removed sentences to the max-\nimum potential removal sentences r(t). Our anal-\nsis indicates that not all sentences designated for\nmaximum removal range are always eliminated\nduring the training process. Notably, a significant\nproportion of redundant sentences are removed in\nthe early stages of training, with fewer sentences\nbeing removed as the model progresses through\nthe middle to later stages, thereby stabilizing its\ntraining. A similar trend is observed across other\ndatasets, as detailed in Appendix E."}, {"title": "6 Conclusion", "content": "In this work, we propose sentence-level rationale\nreduction framework VARR and provide empirical\nevidence that models trained with non-redundant\nrationales achieve greater efficiency. We address\nthe lack of principled criteria for identifying redun-\ndant sentences during the training phase by devel-\noping a reduction method that not only preserves\nthe model's reasoning capabilities but also reduces\nthe likelihood of generating incorrect answers. Our\nexperiments show that VARR can efficiently han-\ndle a diverse range of tasks with fewer generated\ntokens, without sacrificing its accuracy. Further\nempirical findings confirm that sentences are ap-\npropriate reasoning units for rationale reduction,\nhighlighting the necessity of principled criteria for\nrobust application. This work contributes novel in-\nsights to rationale reduction research, contributing\nto the efficient elicitation of reasoning in language\nmodels."}, {"title": "Limitations", "content": "While our work provides novel insights into ratio-\nnale reduction research, our experiments were pri-\nmarily conducted using a relatively small large lan-\nguage model, constrained by computational costs.\nAdditionally, for the same reasons, it was not fea-\nsible to test the model with datasets featuring long\nsequences in both queries and rationales (Reddy\net al., 2024; Yu et al., 2024a). However, given that\nthe VARR/VARR+ frameworks are designed with a\nsystematically structured approach, we believe that\nthey could also be effective in scaled-up settings."}]}