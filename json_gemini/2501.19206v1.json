{"title": "An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents", "authors": ["Gregory Palmer", "Luke Swaby", "Daniel J.B. Harrold", "Matthew Stewart", "Alex Hiles", "Chris Willis", "Ian Miles", "Sara Farmer"], "abstract": "The recent rise in increasingly sophisticated cyber-attacks raises the need for robust and resilient autonomous cyber-defence (ACD) agents. Given the variety of cyber-attack tactics, techniques and procedures (TTPs) employed, learning approaches that can return generalisable policies are desirable. Meanwhile, the assurance of ACD agents remains an open challenge. We address both challenges via an empirical game-theoretic analysis of deep reinforcement learning (DRL) approaches for ACD using the principled double oracle (DO) algorithm. This algorithm relies on adversaries iteratively learning (approximate) best responses against each others' policies; a computationally expensive endeavour for autonomous cyber operations agents. In this work we introduce and evaluate a theoretically-sound, potential-based reward shaping approach to expedite this process. In addition, given the increasing number of open-source ACD-DRL approaches, we extend the DO formulation to allow for multiple response oracles (MRO), providing a framework for a holistic evaluation of ACD approaches.", "sections": [{"title": "1. Introduction", "content": "Deep reinforcement learning (DRL) has emerged as a promising approach for training autonomous cyber-defence (ACD) agents that are capable of continuously investigating and neutralising cyber-threats at machine speed. However, to learn robust and resilient ACD policies, DRL agents must overcome three open problem areas: i.) Vast, dynamic, high-dimensional state-spaces; ii.) Large, combinatorial action spaces, and; iii.) Adversarial learning against a non-stationary opponent (Palmer et al., 2023).\nWhile the autonomous cyber-operations (ACO) commu-"}, {"title": "2. Related Work", "content": "Despite ACD being, by definition, an adversarial game between defending and attacking agents, there exists limited work on the topic of adversarial learning. Evaluations typically feature a stationary set of opponents (Hicks et al., 2023; Nguyen et al., 2020; O'Driscoll et al., 2024; Tran et al., 2022). A possible explanation for this is that current cyber-defence gyms do not support adversarial learning. For example, while the CybORG CCs support both the training of ACD and ACA agents, modifications are necessary to enable learning against a DRL opponent. An exception here is work conducted by Shashkov et al. (2023), who adapt and compare DRL, evolutionary strategies, and Monte Carlo tree search methods within CyberBattleSim (Seifert et al., 2021). To the best of our knowledge, we are the first to conduct a holistic empirical game-theoretic analysis of ACD-DRL approaches within topical cyber-defence gyms.\nThe topic of reward shaping has been gathering attention from the ACD community (Lopes Antunes &\nLlopis Sanchez, 2023; Miles et al., 2024). For instance, to study the sensitivity of DRL to the magnitude of received penalties (Bates et al., 2023). In contrast to existing work,"}, {"title": "3. Background", "content": ""}, {"title": "3.1. Partially Observable Markov Games", "content": "Our ACD and ACA agents are situated within partially observable Markov games (POMGs). A POMG M is defined as a tuple (n, S, N, O, A, P, R, \u03b3), consisting of: a finite state space S; a joint action space $(A_1 \\times ... \\times A_n)$ for each state $s \\in S$, with $A_p$ being the set of actions available to player p; a state transition function $P : S_t \\times A_1 \\times ... X A_n \\times S_{t+1} \\rightarrow [0, 1]$, returning the probability of transitioning from a state $s_t$ to $s_{t+1}$ given a joint-action profile a;\na set of joint observations \u03a9; an observation probability function defined as $O_p : S \\times A_1 \\times ... \\times A_n \\times \u03a9 \\rightarrow [0, 1]$; a discount rate y, and; for each player p, a reward function\n$R_p : S_t \\times A_1 \\times ... \\times A_n \\times S_{t+1} \\rightarrow R$, returning a reward $r_p$."}, {"title": "3.2. The Adversarial Learning Challenge", "content": "Our focus is on adversarial learning scenarios that feature cyber-defence (Blue) and cyber-attacking (Red) agents. Formally, for each agent (player) p, the policy $\u03c0_p$ is a mapping from the state space to a probability distribution over actions, $\u03c0_p: S_p \\rightarrow \u0394(A_p)$. Transitions within POMGs are determined by a joint policy \u03c0. Joint policies excluding agent p are defined as $\u03c0_{-p}$. The notation $(\u03c0_p, \u03c0_{-p})$ refers to a joint policy with agent p following $\u03c0_p$ while the other agents follow $\u03c0_{-p}$. As noted above, the stationarity of an attacking agent's policy is a strong assumption. Therefore, selecting defending agents based on their performance against a pool of known attackers runs the risk of being blindsided by an unfamiliar attacker. Here, a more desirable solution concept commonly used in this class of games is the Nash equilibrium (Nash, 1951):\nDefinition 3.1 (Nash Equilibrium). A joint policy $\u03c0^*$ is a Nash equilibrium iff no player p can improve their gain through unilaterally deviating from $\u03c0^*$: \n$\u2200p, \u2200\u03c0'_p, G_p((\u03c0'_p, \u03c0^*_{-p})) \\geq G_p((\u03c0^*_p, \u03c0^*_{-p})).$ (1)\nOur focus is on finite two-player zero-sum games, where an equilibrium is referred to as a saddle point, representing the value of the game $v^*$. Given two policies $\u03c0_1, \u03c0_2$, the equilibrium of a finite zero-sum game is:\nTheorem 1 (Minmax Theorem). In a finite two-player"}, {"title": "3.3. Approximate Double Oracles", "content": "One of the long-term objectives of adversarial learning is to limit the exploitability of agents deployed in competitive environments (Lanctot et al., 2017). While a number of theoretically grounded methods exist for limiting exploitability, computing the value of a game remains challenging in practise, even for relatively simple games.\nHere, we provide a recap of a popular, principled adversarial learning frameworks for finding a minimax equilibrium and reducing exploitability: the DO algorithm (McMahan et al., 2003). This algorithm defines a two-player zero-sum normal-form game N, where actions correspond to policies available to the players within an underlying (PO)MG M.\nPayoff entries within N are determined through computing the gain G for each policy pair within M:\n$R((a_1, a_2)) = G_M((\u03c0_1, \u03c0_2)).$ (4)\nIn Equation 4, r and c refer to the respective rows and columns inside the normal-form (bimatrix) game. The normal-form game N is subjected to a game-theoretic analysis, to find an optimal mixture over actions for each player, representing a probability distribution over policies for the game M."}, {"title": "3.4. Potential-Based Reward Shaping", "content": "The iterative learning of ABRs presents a computationally costly endeavour, highlighting the need for methods that expedite the search for ABRs (Liu et al., 2022). We observe that learning ACO agents are confronted with the temporal credit assignment problem. For example, in numerous cyber-defence scenarios, large penalties are associated with ACA agents impacting high-value targets, such as operational hosts and servers (Standen et al., 2021). However, attacks on an operational subnet will typically be preceded by Blue/Red actions to protect/compromise more accessible user and enterprise subnets. Here, reward shaping offers a means of mitigating the temporal credit assignment problem via a modified reward function $R' = R + F$, where F represents the shaping reward (Grzes & Kudenko, 2009b).\nA shaping reward can be implemented using domain knowledge or learning approaches (Grzes & Kudenko, 2009a). However, unprincipled reward shaping can lead to policies"}, {"title": "4. Methods", "content": ""}, {"title": "4.1. Value-Function Potential-Based Reward Shaping", "content": "In their seminal work on policy invariance under PBRS transformations, Ng et al. (1999) state that one way for defining a good potential function \u03a6 is to approximate the optimal value function (VF) $V_M(s)$ for a given problem M. We observe that any ADO run featuring oracles using VFs can provide a plethora of approximate VFs, $V_M (s)$, enabling a VF driven PBRS (VF-PBRS); with $M_k$ representing the (PO)MG that an ABR k was trained on.\nThe number of VFs available after multiple ABR iterations raises the question of which $V_M$ to select. Here, the ADO algorithm's current mixture $\u03bc_i$ captures the best response for player i using available policies within our empirical matrix game against player j's mixture $\u03bc_j$. We hypothesise that $\u03bc_i$ can also help us select informative VFs.\nRather than sampling a single $V_M$ we propose using a weighted ensemble. However, normalization is required when ensembling VFs, as the magnitudes of approximated value estimates are often not directly comparable (Garcia &\nCaarls, 2024). We address this by applying Z-score normalization to each VF: $Z(V_{M_k}(s))$. The resulting potential function is the weighted sum of normalised value estimates:\n$\u03a6(s) = \\sum_{k=1}^{|\u03bc_i|} \u03bc_i^k \\times Z(V_{M_k}(s)).$ (6)\nTheoretical Analysis: Oracles using VF-PBRS do not impact the theoretical guarantees underpinning the DO algorithm. Given a two-player zero-sum game, a Nash equilib-rium has been found when neither player can compute a best"}, {"title": "4.2. Multiple Response Oracles", "content": "One of the disadvantages of the ADO formulation is that the function $O_i(\u03bc_j)$ only returns a single policy $\u03c0_i$ per ABR iteration. What if we have multiple approaches for computing responses against an opponent mixture, and do not know in advance which one is most likely to return the ABR? Here, we propose a novel ADO formulation to address this limitation. We call this approach the multiple response oracles algorithm (MRO). In contrast to the ADO algorithm, MRO replaces ABR oracles $O_i$ with functions $I_i \\leftarrow R_i(\u03bc_j)$, that return a set of responses. We note that this extension does not change the underlying theoretical properties of the"}, {"title": "4.3. Pre-trained Model Sampling", "content": "Training ACO agents requires lengthy wall-times to ensure an exhaustive exploration of semantically similar state-action pairs. To address this challenge, a general consensus has emerged within the AI community that PTMs should be utilised when possible, rather than training models from scratch (Han et al., 2021). Within the context of DO-based approaches, agents have the luxury of an iteratively expanding pool from which PTMs can be sampled. Here, once again, the mixtures $\u03bc_i$ can provide guidance; on this occasion for sampling from PTMs that represent the best available response against the current opponent mixture $\u03bc_j$. However, one of the strengths of the DO algorithm is the exploration of the strategy space (Wellman & Mayo, 2024).\nWe propose balancing the above trade-off by adding \u03b5-greedy exploration to our oracles. In each ABR iteration a mixture-guided PTM is sampled with a probability 1 \u2212 \u03b5. Initially \u03b5 is set to 1. A decay rate d \u2208 [0, 1) is subsequently applied after each ABR iteration. Exploratory iterations can either consist of training a freshly initialised agent, or using a designated PTM as a starting point, e.g., a policy that can be classed as a generalist. In contrast, in greedy iterations the mixture weights are used as sampling probabilities."}, {"title": "5. Evaluation Environments", "content": "Below we conduct an empirical game-theoretic evaluation of DRL approaches in two post-exploitation lateral movement scenarios: CybORG CAGE Challenges 2 and 4 (TTCP\nCAGE Working Group, 2022; 2023). Network diagrams and experiment settings can be found in Appendix B and C."}, {"title": "6. Cyber-Defence & Attacking Oracles", "content": "Cardiff University (Blue): Our first Blue oracle is a modified version of the CC2 winning submission from Cardiff\nUniversity (Hannay, 2022). This approach, based on the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017), benefits from a significantly reduced action space. It defines a single decoy action per host that greedily selects from available decoys. The submission also uses a \u201cfingerprinting\u201d function for identifying the two rules-based\nRed agents supplied by CC2 and countering with respective ABR policies. However, fingerprinting new Red policies is beyond our current scope. We therefore disable this method.\nCybermonic Inductive Graph-PPO (GPPO, Blue): Given that cyber-defence scenarios typically feature underlying dynamics unique to graph-based domains, ACD agents are increasingly benefitting from incorporating graph machine learning approaches (Symes Thompson et al., 2024). King\n& Bowman (2024) and King (2024) have implemented two OS Graph-PPO algorithms for CC2 and CC4. The approaches utilise graph observation wrappers for converting CybORG's observations into an augmented graph environment. An internal state is maintained that keeps track of changes in the graph structure. Graph convolutional net-works (Kipf & Welling, 2016) are used to extract features from the graph representation. An independent learner (IL) approach is used in CC4, with each agent updating independent actor-critic networks using local observations."}, {"title": "7. Empirical Game-Theoretic Analysis", "content": ""}, {"title": "7.1. CybORG CAGE Challenge 2", "content": "During lengthy preliminary runs using 5M time-steps per response, it was observed that the agents would often make significant improvements after 2.5M time-steps (See Appendix G). However, this corresponds to approximately one day per ABR iteration. To reduce wall-time, we instead initialise from \u201cgeneralist\u201d PTMs obtained from previous 5M\nstep runs. We allow 1.5M environment steps per response.\nResponses are evaluated using the mean episodic reward from 100 evaluation episodes.\nImpact of using PTMs and VF-PBRS: Upon completing around 20 ABR iterations, we find that Blue and Red oracles using PTMs struggle to improve on the Nash payoffs under\nthe joint mixture profile \u3008\u00b5Blue, \u00b5Red) (see Figure 1). At this point we confront the Blue mixture with a Red oracle that uses a random initialisation and a budget of 5M time-steps. This Red oracle initially finds an ABR, \u03c021 \u03c021 Red, that significantly improves upon the Nash payoff, from 27.30\nunder \u3008\u00b5Blue, \u00b5Red) to 93.05 for \u3008\u00b5Blue, Red). However, upon adding \u03c0Red Red to the empirical game, the Blue agent neutralises the new Red TTPs through adjusting its mixture, decreasing the Nash payoff to 29.75. The Red agent is subsequently unable to find a significantly better response."}, {"title": "8. Empirical Game Augmentation Complexity", "content": "While MRO provides a means through which to conduct a principled evaluation of cyber-defence and attacking approaches, it does come with an increase in complexity for augmenting the empirical payoff matrix. Given\nn = |RBlue(\u00b5Red)| approaches for computing responses for Blue, and m = |RRed(\u00b5Blue)| approaches for Red, in each iteration the MRO algorithm will need to compute nx |\u03a0Red| + m \u00d7 |\u03a0Blue| nx m new entries. Here, \u03a0Red and \u03a0Blue represent the sets of polices available at the start of an ABR iteration.\nA popular approach for dealing with large payoff matrices is to remove strategies that are found to be either strictly or weakly dominated (Conitzer & Sandholm, 2005; Kuzmics, 2011). Upon evaluating the payoff matrix from CC2 after 23 ABR iterations we are unable to identify any strictly\ndominated Blue policies. However, we do identify weakly dominated policies, including the majority of Cardiff ABRs"}, {"title": "9. Discussion & Conclusion", "content": "Our empirical game-theoretic evaluations show the risk posed by deploying autonomous cyber-defence agents that have only been confronted with a limited set of cyber-attack TTPs during training. Once deployed, cyber-defence agents will typically be unable to make online policy adjustments for damage limitation. Therefore, Blue's adversarial learning process must feature Red agents that pose a similar or worse threat to those that will be encountered once deployed. Put plainly, even the most principled and wall-time efficient adversarial learning framework will struggle to produce robust and resilient cyber-defence agents if the approach is trained against weak Red solutions.\nOur work shows that oracles using a combination of VF-PBRS and PTMs enable the computation of time-efficient, robust and resilient responses. Our multiple response oracles algorithm provides a principled solution, not only for evaluating the approaches against each other, but combining the best policies obtained from the learning approaches into a, potentially heterogeneous, mixture agent. We have shown the benefits of these approaches within the challenging context of ACD, and note their potential for any other application area where multiple private and public organisations are seeking to ensemble their custom built approaches."}, {"title": "Impact Statement", "content": "Through highlighting the perils of not viewing autonomous cyber-operations through the lens of adversarial learning, our work is intended to serve as a wake-up call for the ACD community. We show that a number of design principles need to be adhered-to for obtaining an idealised, robust, and resilient ACD agent. These principles include: i.) Not having to re-learn defence strategies that were previously successful against opponents; ii.) Using a principled method for determining how to re-use past knowledge against current threats; iii.) The ability to ensemble policies drawn from a heterogeneous set of policies; iv.) The ability to rapidly (re)train policies, and; v.) That learning truly robust and resilient ACD policies will only be possible if the learners are confronted with worst-case cyber-attacking policies.\nThe fifth principle naturally raises a number of ethical considerations. Researchers and developers have justifiable concerns that fully enabling adversarial learning could lead to the nefarious use of cyber-attacking agents trained within a high-fidelity gym environment (Seifert et al., 2021). However, our work highlights the weaknesses of agents trained against sub-optimal opponents. Therefore, we hope that our work will open up debates on how to confront this challenge head on, while simultaneously ensuring compliance with relevant guidelines and best practices."}, {"title": "A. Theoretical Analysis", "content": ""}, {"title": "A.1. Approximate Double Oracles", "content": "In games that suffer from the curse-of-dimensionality an oracle can at best hope to find an approximate best response\n(ABR) (Oliehoek et al., 2018):\nDefinition A.1 (Approximate Best Response). A policy $\u03c0_i \\in \u03a0_i$ of player i is an ABR, also referred to as a re-source bounded best response (RBBR) (Oliehoek et al., 2018), against a mixture of policies $\u03bc_j$, iff,\n$\u2203\u03c0'_i \u03b5 \u03a0_i, G_i ((\u03c0'_i, \u03bc_j)) \u2265 G_i(\u3008\u03c0'_i, \u03bc_j)).$ (10)\nABRs are also used to estimate the exploitability GE of the current mixtures:\n$G_E \\leftarrow G_i((O_i(\u03bc_j), \u03bc_j)) + G_j (\u3008\u03bc_\u017c, O_j (\u03bc_i))).$ (11)\nIf GE\u2264 0, then the oracles have failed to find ABRs, and a resource bounded Nash equilibrium (RBNE) has been found (Oliehoek et al., 2018). Resources in this context refers to the amount of computational power available for obtaining an ABR:\nDefinition A.2 (Resource Bounded Nash Equilibrium). Two mixtures of policies ($\u03bc_1, \u03bc_2$) are a resource-bounded Nash equilibrium iff,\n$\u2200iG_i(\u3008\u03bc'_i, \u03bc_j)) \u2265 G_i(\u3008O_i(\u03bc_j), \u03bc_j)).$ (12)\nExtending a proof from (Oliehoek et al., 2018) for generative adversarial networks (GANs) (Goodfellow et al., 2014), we can show that GE < 0 implies we have found a resource bounded Nash equilibrium for the ADO algorithm when applied to adversarial deep reinforcement learning:\nTheorem 3. If the termination condition from Equation 12 is met, then a resource bounded Nash equilibrium has been found.\nProof. It can be shown that GE \u2264 0 implies that the agents have converged upon a resource bounded Nash equilibrium:\n$G_E = G_i((O_i(\u03bc_j)), \u03bc_j)) + G_j (\u3008\u03bc_\u017c, O_j (\u03bc_i)))$\n$\u2264 0 = G_i(\u3008\u03bc'_i, \u03bc_j)) + G_j(\u3008\u03bc'_\u017c, \u03bc_j)).$ (13)"}, {"title": "A.2. Multiple Response Oracles", "content": ""}, {"title": "A.2.1. CONVERGENCE GUARANTEES", "content": "In this section we shall first theoretically show that our MRO algorithm does not disrupt the convergence guarantees of the double oracle formulation. Next, we shall show that, given a set of assumptions, MRO is guaranteed to produce stronger mixture agents than the ADO.\nWe begin with the termination condition, where we note that GE can now be computed with respect to the best performing approaches from each agent's respective sets of responses:\n$G_E \\leftarrow G_i((O_i(R_i(\u03bc_j)), \u03bc_j))+$\n$G_j((\u03bc_\u017c, O_j (R_j(\u03bc_i)))).$ (17)\nIn the above equation, $O_\u017c(R_i(\u03bc_j))$ and $O_j (R_j(\u03bc_i))$ return $\u03c0^*_i$ and $\u03c0^*_i$, i.e., the best response within their respective response sets $\u03c0 \\in R_i(\u03bc_j)$ and $\u03c0 \\in R_j (\u03bc_i)$, as defined in Equation 9. Therefore, it remains the case that if GE < 0, then the oracles have failed to improve on the current mixtures, and a resource bounded Nash equilibrium (RBNE) has been found."}, {"title": "A.3. VF-PBRS - Proof of Sufficiency", "content": "Ng et al. (1999) show that a reward shaping function F being a potential-based shaping function is a necessary and sufficient condition to guarantee consistency when learning an optimal policy on an MDP M' = (S, A,P, R + F, \u03b3) instead of M = (S, A, P, R, \u03b3).\nRegarding necessity, the authors note that if F is not a potential-based shaping function, then there exist transition and reward functions such that no optimal policy in M' is also optimal in M. With respect to sufficiency, if F is a potential-based shaping function, every optimal policy in M' will also be optimal in M.\nFor convenience we include Ng et al. (1999)'s proof of sufficiency below. As in the original PBRS paper the proof assumes scenario-optimal policies can be obtained, hence we relate it to VF-PBRS within the context of double oracles rather than the ADO algorithm, where at best we can hope to obtain approximate best responses. First, to recap:"}, {"title": "D. Strictly and Weakly Dominated Strategies", "content": "In this section we shall consider the different types of mixtures ($\u03bc_i, \u03bc_j$) that a Nash solver may yield for agents i and j given a normal-form game N, and the definitions for strictly and weakly dominated strategies. First we distinguish between a pure and a mixed strategy equilibrium. As the name indicates, for a pure strategy Nash equilibrium both players place a 100% probability on playing a single strategy respectively. A mixed strategy Nash equilibrium, meanwhile, involves at least one player using a weighted randomised strategy selection process. Von Neumann & Morgenstern (2007) showed that a mixed-strategy Nash equilibrium will exist for any zero-sum game with a finite set of strategies.\nHere, it is worth noting that, given a mixed strategy Nash equilibrium ($\u03bc'_i, \u03bc'_j$), for a mixed strategy $\u03bc_i$ each pure strat-egy included in the mix (i.e., with a weighting above zero) is itself a best response against $\u03bc'_j$, and will yield the same payoff as $G_i (\u3008\u03bc'_i, \u03bc'_j))$ (Fang et al., 2021).\nTheorem 7 (Mixed Strategy Theorem). If a mixed strategy $\u03bc_i$, belonging to player i, is a best response to the (mixed) strategy $\u03bc'_j$ of player j, then, for each pure strategy (action) $a_k$ with a weight $\u03bc_i(a_k) > 0$ it must be the case that $a_k$ is itself a best response to $\u03bc'_j$. Therefore, $G_i(\u3008a_k, \u03bc'_j))$ must be the same for all strategies (actions) included in the mix.\nWe note that a payoff matrix may feature strictly dominated strategies, weakly dominated strategies and strictly domi-nant strategies. Strictly dominated strategies (actions) are strategies that always deliver a worse outcome than at least"}, {"title": "E. Payoff Matrix Augmentation", "content": "We illustrate the computational impact of using our pro-posed MRO formulation, compared to the original ADO algorithm. First we highlight the exponential growth of the payoff table following each response iteration for MRO,compared to the quadratic growth for the ADO algorithm,in Figure 8a. For this plot we assume that both Blue andRed use four approaches for computing responses, i.e.,m = n = 4. Next, in Figure 8b, we consider the num-ber of additional cells that are added to the empirical payoffmatrix in each iteration, again assuming four response ap-proaches for Blue and Red. We observe that in iteration100 over 3,000 new cells would need to be added to thepayoff matrix. Finally, in Figure 9 we depict the exponen-tial growth in the size of the payoff matrix after 100 ABRiterations when varying the number of response approachesfor MRO."}, {"title": "F. Preliminary CAGE Challenge 2 Run", "content": "During one of our early preliminary runs within CAGE Challenge 2 we evaluated the impact of using: i.) VF-PBRS; ii.) initialising from PTMs; iii.) VF-PBRS & PTMs, and; Vanilla runs without PTMs and VF-PBRS. Results are shown in Figure 10."}, {"title": "G. Delayed Convergence Examples", "content": "During preliminary trial runs we found that when training Blue agents (without PTMs), they would often achievesignificant improvements after a large number of trainingsteps. This is illustrated in Figure 11, where GPPO andCardiff ABRs (in the third ABR iteration of a preliminaryrun) significantly improved their policies after 2.5M and1.5M time-steps respectively. Therefore, we turn to PTMs,to address the long wall-times associated with these \u201cfull\u201druns (approximately one day per run)."}, {"title": "H. Policy Characteristics \u2013 CAGE Challenge 2", "content": "In this section we expand upon the comparison between the starting point of our training process, \u03c0Blue (the parame-terisation available in the repository maintained by King& Bowman (2024)), and the final mixture \u00b5Blue. We notethat the final mixture agent consists of GPPO agents trainedusing King & Bowman (2024)'s approach. The differencebetween \u03c0Blue and the policies used by \u00b5Blue is the oppo-nents faced during training.\nTo recap, as our empirical game in Figure 3 illustrates, the majority of the Blue policies with an above zero samplingweighting within the final mixture generalise well againstall the responses computed by Red during the MRO run. Incontrast, for \u03c0\u03b2\u03b9ue there exist a number of Red responsescapable of forcing it into taking costly actions. The secondRed ABR, Red, has the biggest impact on \u03c0Blue, reducingthe Blue agent's payoff to -117.82. In contrast, \u00b5Blueachieves a mean payoff of -21.96 against Red \u03c0.\nWe find that Red's success against \u03c0\u03b2\u03b9ue comes from ini-tially targeting User2 during the first 5 time-steps, and sub-"}, {"title": "J. Why do we need mixed strategies?", "content": "Due to our empirical games for CC2&4 being finite zero-sum games, a Nash solver can be used to find the optimal weighting for each set of policies. This is enabled by us selecting a zero-sum reward for training, with Red receiving the negation of the Blue reward. Therefore, Red is in effect learning to degrade Blue's performance.\nAt this point it is worth noting, each pure strategy that is"}]}