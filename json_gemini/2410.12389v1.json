{"title": "A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetic", "authors": ["Lennert De Smet", "Pedro Zuidberg Dos Martires"], "abstract": "As illustrated by the success of integer linear programming, linear integer arithmetic is a powerful tool for modelling combinatorial problems. Furthermore, the probabilistic extension of linear programming has been used to formulate problems in neurosymbolic AI. However, two key problems persist that prevent the adoption of neurosymbolic techniques beyond toy problems. First, probabilistic inference is inherently hard, #P-hard to be precise. Second, the discrete nature of integers renders the construction of meaningful gradients challenging, which is problematic for learning. In order to mitigate these issues, we formulate linear arithmetic over integer-valued random variables as tensor manipulations that can be implemented in a straightforward fashion using modern deep learning libraries. At the core of our formulation lies the observation that the addition of two integer-valued random variables can be performed by adapting the fast Fourier transform to probabilities in the log-domain. By relying on tensor operations we obtain a differentiable data structure, which unlocks, virtually for free, gradient-based learning. In our experimental validation we show that tensorising probabilistic linear integer arithmetic and leveraging the fast Fourier transform allows us to push the state of the art by several orders of magnitude in terms of inference and learning times.", "sections": [{"title": "1 Introduction", "content": "Integer linear programming (ILP) [15, 32] uses linear arithmetic over integer variables to model intricate combinatorial problems and has successfully been applied to domains such as scheduling [31], telecommunications [34] and energy grid optimisation [26]. If one replaces deterministic integers with integer-valued random variables, the resulting probabilistic arithmetic expressions can be used to model probabilistic combinatorial problems. In particular, many problems studied in the field of neurosymbolic AI can be described using probabilistic linear integer arithmetic.\nUnfortunately, exact probabilistic inference for integer arithmetic is a #P-hard problem in general. Consequently, even state-of-the-art probabilistic programming languages with dedicated inference algorithms for discrete random variables, such as ProbLog [8] and Dice [13], fail to scale. The reason being that they resort to exact enumeration algorithms, as exemplified in Figure 1. Note that while approximate inference algorithms such as Monte Carlo methods and variational inference can be applied to probabilistic combinatorial problems, they come with their own set of limitations, as discussed by Cao et al. [5]. For instance, conditional inference with low-probability evidence.\nIn order to mitigate the computational hardness of probabilistic inference over integer-valued random variables, we make the simple yet powerful observation that the probability mass function (PMF) of the sum of two random variables is equal to the convolution of the PMFs of the summands. The key advantage of this perspective is that the exact convolution for finite domains can be implemented efficiently using the fast Fourier transform (FFT) in O(N log N), which avoids the traditionally quadratic behaviour of computing the PMF of a sum of two random variables (Figure 1). Moreover,\nOur main contributions are the following. 1) We propose a tensor representation of the distributions of bounded integer-valued random variables that allows for the computation of the distribution of a sum of two such variables in O(N log N) instead of O(N2) by exploiting the fast Fourier transform (Section 2). 2) We formulate common operations in linear integer arithmetic, such as multiplication by constants and the modulo operation, as tensor manipulations (Section 3). These tensorised operations give rise to PLIAt, a scalable and differentiable framework for Probabilistic Linear Integer Arithmetic.\u2020 PLIAt supports two exact probabilistic inference primitives; taking expected values and performing probabilistic branching (Section 4). 3) We provide experimental evidence that PLIAt outperforms the state of the art in exact probabilistic inference for integer arithmetic [5] in terms of inference time by multiple orders of magnitude (Section 5.1). Moreover, we deploy PLIAt in the context of challenging neurosymbolic combinatorial problems, where it is again orders of magnitude more efficient when compared to state-of-the-art exact and approximate methods (Section 5.2)."}, {"title": "2 Efficient Addition of Integer-Valued Random Variables", "content": "In what follows, we denote random variables by uppercase letters, while a specific realisation of a random variable is written in lowercase. That is, the value x is an element of the sample space (X) of X. We will also refer to \u03a9(X) as the domain of the random variable X. Furthermore, \u03a9(X) is assumed to be integer-valued, i.e. it is a finite subset of the integers Z with lower and upper bounds L(X) and U(X), respectively. In particular, we have that the cardinality |\u03a9(X)| = U(X)\u2212L(X)+1. We will call these integer-valued random variables probabilistic integers from here on.\nThe distribution of a probabilistic integer X is represented using its probability mass function (PMF) px : \u03a9(\u03a7) \u2192 [0, 1] with the conventional restrictions\n$\\forall x \\in \\Omega(X): p_X(x) \\geq 0 \\text{ and } \\sum_{x \\in \\Omega(X)} p_X(x) = 1.  \\tag{1}$"}, {"title": "2.1 Probabilistic Integers and the Convolution Theorem", "content": "At the core of PLIAt and linear arithmetic in general is the addition of two probabilistic integers X1 and X2. Let us assume for now that X\u2081 and X2 satisfy L(X1) = L(X2) = 0 and have upper bounds U(X1) = N\u2081 and U(X2) = N2. Just as in Figure 1, we would now like to find the PMF of the random variable X such that X = X1 + X2. However, contrary to Figure 1, we wish to avoid the\nThe subscript \"t\" in PLIAt stands for \u201ctensorised\".\n$P_X(x) = (p_{X_1} * p_{X_2})(x), \\quad X = X_1 + X_2. \\tag{2}$"}, {"title": "", "content": "Next, we apply the Fourier transform F to both sides of the equation and use the convolution theorem (CT) [23] that states that the Fourier transform of two convoluted functions is equal to the product of their transforms\n$\\mathcal{F}(p_X)(x) = \\mathcal{F}(p_{X_1} * p_{X_2})(x) \\rightarrow \\mathcal{F}(p_X)(x) = \\mathcal{F}(p_{X_1})(x) \\cdot \\mathcal{F}(p_{X_2})(x) = \\hat{p}_{X_1}(x) \\cdot \\hat{p}_{X_2}(x), \\tag{3}$"}, {"title": "", "content": "where we also introduce the hat notation $\\mathcal{F}(p_X) = \\hat{p}_X$ for a Fourier transformed PMF px. As X = X1 + X2, we know that \u03a9(X) = {0,..., N\u2081 + N2}. Consequently, the PMF px is non-zero for just these M = N\u2081 + N2 + 1 domain elements and can be represented using a vector of probabilities\n$\\pi_X[x] = p_X(x), \\forall x \\in \\{0,..., N_1 + N_2\\}. \\tag{4}$"}, {"title": "", "content": "All vectors of probabilities will be written using a boldface and their elements accessed using square brackets. Looking at Equation 3, we would now like to express the Fourier transformed probability vector \u03c0\u03c7 as the point-wise product of the transformed vectors\n$\\mathbf{F}_M \\mathbf{\\pi}_X = \\mathbf{\\hat{\\pi}}_{X_1} \\odot \\mathbf{\\hat{\\pi}}_{X_2}, \\tag{5}$"}, {"title": "", "content": "where $\\mathbf{F}_M \\in \\mathbb{C}^{M \\times M}$ is the M-point discrete Fourier transform (DFT) matrix and the symbol $\\odot$ denotes the Hadamard product. In order for Equation 5 to hold we need to have that\n$\\mathbf{\\hat{\\pi}}_{X_1} = \\mathbf{F}_M \\mathbf{\\pi}_{X_1} \\text{ and } \\mathbf{\\hat{\\pi}}_{X_2} = \\mathbf{F}_M \\mathbf{\\pi}_{X_2} \\tag{6}$"}, {"title": "", "content": "At first sight these equalities seem to cause a problem; each probabilistic integer X\u2081 has a domain of size N + 1 while its PMF should be represented with a probability vector \u03c0\u03c7\u2081 \u2208 RM for the multiplication with FM to make sense. Fortunately, this problem is easily resolved by observing that we can extend the domain $\\Omega(X_i)$ of Xi by simply assigning a probability of zero to newly added elements. In practice, we simply pad the probability vectors \u03c0\u03c7\u2081 and \u03c0\u03a7\u2082 with N2 and N\u2081 zeros at the end to obtain vectors of dimension M. With this issue resolved, we can finally obtain the probability vector \u3160\u2717 that represents the PMF px by using Equation 5 via\n$\\mathbf{F}_M \\mathbf{\\pi}_X = \\mathbf{\\hat{\\pi}}_{X_1} \\odot \\mathbf{\\hat{\\pi}}_{X_2} \\rightarrow \\mathbf{F}_M \\mathbf{\\pi}_X = \\mathbf{F}_M \\mathbf{\\pi}_{X_1} \\odot \\mathbf{F}_M \\mathbf{\\pi}_{X_2} \\tag{7}$"}, {"title": "", "content": "$\\mathbf{\\pi}_X = \\mathbf{F}_M^{-1} (\\mathbf{F}_M(\\mathbf{F}_M \\mathbf{\\pi}_{X_1} \\odot \\mathbf{F}_M \\mathbf{\\pi}_{X_2})). \\tag{8}$"}, {"title": "2.2 The Fast Log-Conv-Exp Trick", "content": "The attentive reader might have noticed that, even though we avoid the explicit construction of the joint probability distribution px1X2 (X1, X2), we have not gained much. The matrix-vector products in Equation 8 still take O(M\u00b2) to compute. Fortunately, matrix-vector products where the matrix is the DFT matrix or its inverse can be computed in time O(Mlog M) by using the fast Fourier transform (FFT), with M being the size of the vector. As a result, we can express Equation 8 as\n$\\mathbf{\\pi}_X = \\text{IFFT}(\\text{FFT}(\\mathbf{\\pi}_{X_1}) \\odot \\text{FFT}(\\mathbf{\\pi}_{X_2})). \\tag{9}$"}, {"title": "", "content": "Computing the values of the vector \u03c0\u03c7 can now be done in time O(M log M). First we apply the FFT on the vectors \u03c0\u03a7\u2081 and \u03c0\u03c7\u2082. Then we multiply the transformed vectors pointwise and apply the inverse FFT on the result of this Hadamard product. We note that Equation 9 is a well known result from the signal processing literature, where convolutions are always computed in this fashion [23].\nHowever, applying Equation 9 naively to the problem of probabilistic inference quickly results in numerical stability issues. The problem is that multiplying together small probabilities eventually results in numerical underflow. A well-known and widely used remedy to this problem is the log-sum-exp trick, which allows one to avoid underflow by performing computations in the log-domain instead of the linear domain. Inspired by the log-einsum-exp trick [29], we introduce the fast log-conv-exp trick, which allows us to perform the FFT on probabilities in the log-domain.\nWe first characterise a probability distribution px not by the vector of probabilities \u03c0x but by the vector of log-probabilities Ax = log \u03c0\u03c7. In terms of log-probabilities, Equation 9 can be written as\n$\\exp \\mathbf{\\lambda}_X = \\text{IFFT} (\\text{FFT}(\\exp \\mathbf{\\lambda}_{X_1}) \\odot \\text{FFT}(\\exp \\mathbf{\\lambda}_{X_2})). \\tag{10}$"}, {"title": "", "content": "Now define \u03bc\u2081 := $\\max_{x \\in \\Omega(X_1)} \\lambda_{X_1}[x]$ as the maximum value present in the vector Ax, which lets us write Equation 10 as\n$\\exp \\mathbf{\\lambda}_X = \\text{IFFT} (\\text{FFT}(\\exp (\\mathbf{\\lambda}_{X_1} - \\mu_{X_1}) + \\mu_{X_1})) \\odot \\text{FFT}(\\exp (\\mathbf{\\lambda}_{X_2} - \\mu_{X_2}) + \\mu_{X_2}))) \\tag{11}$"}, {"title": "", "content": "$= \\text{IFFT} (\\text{FFT}(\\exp (\\mathbf{\\lambda}_{X_1} - \\mu_{X_1})) \\odot \\text{FFT}(\\exp (\\mathbf{\\lambda}_{X_2} - \\mu_{X_2}))) \\exp(\\mu_{X_1}) \\exp(\\mu_{X_2}).$\nCrucially, we were able to pull out the scalars exp(\u03bc\u03c7\u2081) and exp(\u03bc\u03c7\u2082) due to the linearity of the FFT transform and its inverse. Taking the logarithm of both sides results in the fast log-conv-exp trick\n$\\mathbf{\\lambda}_X = \\log \\left[\\text{IFFT} (\\text{FFT}(\\exp(\\mathbf{\\lambda}_{X_1} - \\mu_{X_1})) \\odot \\text{FFT}(\\exp(\\mathbf{\\lambda}_{X_2} - \\mu_{X_2}))) \\right] + \\mu_{X_1} + \\mu_{X_2}, \\tag{12}$"}, {"title": "", "content": "which expresses the log-probabilities Ax in function of Ax\u2081 and Ax2. It can still be computed in time O(Mlog M) and avoids, at the same time, numerical stability issues by exponentiating Ax\u2081 \u03bc\u03b5 instead of Ax, directly.\nWhile using the fast log-conv-exp trick is necessary to scale computations in a numerically stable manner, describing operations on probability mass functions in the log-domain is rather cumbersome. Hence, we will, for the sake of clarity, describe PLIA\u2081 using probability vectors \u03c0 (cf.. Sections 2.3, 3 and 4). We refer the reader to our implementation for the log-domain versions.\nAnother solution to the numerical instability of applying the FFT on probabilities was given in an application of the FFT to open-population [6] N-mixture models [27]. However, it has a major drawback when compared to the fast log-conv-exp trick: it relies on repeated applications of the traditional log-sum-exp trick within each of the N log N iterations of the FFT. This drawback prevents the use of optimised, off-the-shelf FFT algorithms and adds computational overhead. In contrast, we utilise the linearity of the FFT transform to provide an implementation-agnostic solution that works with tensorised representations."}, {"title": "2.3 Translational Invariance", "content": "In the previous sections we assumed that the first non-zero probability event of all probabilistic integers X was the event X = 0, i.e. L(X) = 0. However, we can remove this assumption by characterizing a PMF px not only by a vector of probabilities \u03c0x, but also by an integer \u03b2x = L(X) encoding the non-zero lower bound of the domain \u03a9(\u03a7). Indeed, we can write any PMF px over an integer domain as the translation of a PMF px whose first non-zero probability event is X = 0\n$p_X(x) = (\\mathcal{T}_{\\beta_X} p_{\\tilde{X}})(x) = p_{\\tilde{X}}(x + \\beta_X). \\tag{13}$"}, {"title": "", "content": "Since the PMF px can be represented by a probability vector \u03c0\u03c7 as in the previous sections, it follows that the PMF of any probabilistic integer can be characterised by such a vector and an integer Bx that shifts the domain. The upper bound of the domain is not important for the characterisation of px as it can be obtained via U(X) = \u03b2x + dim(\u03c0\u03c7) \u2212 1, where dim(\u03c0\u03c7) is the dimension of the probability vector \u03c0\u03c7.\nThe inclusion of translations in our representation of PMFs is compatible with using convolutions to compute the PMF of the sum of two probabilistic integers because of the translational invariance of the convolution\n$\\mathcal{T}_k(f * g) = (\\mathcal{T}_k f * g) = (f * \\mathcal{T}_k g), \\tag{14}$"}, {"title": "", "content": "where Tk denotes the translations by a scalar k. In general, k can be real-valued, but for PLIAt we limit k to integers. Using Equation 13, we can write the PMF of the sum of two probabilistic integers X1 and X2 with non-zero lower bounds \u1e9ex\u2081 and Bx2 as\n$p_X = (p_{X_1} * p_{X_1}) = (\\mathcal{T}_{\\beta_{X_1}} p_{\\tilde{X_1}} * \\mathcal{T}_{\\beta_{X_2}} p_{\\tilde{X_2}}) = (\\mathcal{T}_{\\beta_{X_1}} \\odot \\mathcal{T}_{\\beta_{X_2}}) (p_{\\tilde{X_1}} * p_{\\tilde{X_1}}) \\tag{15}$"}, {"title": "", "content": "$= \\mathcal{T}_{\\beta_{X_1}+\\beta_{X_2}} (p_{\\tilde{X_1}} * p_{\\tilde{X_1}}). \\tag{16}$"}, {"title": "", "content": "This final equality shows that we can characterise the PMF px for X = X1 + X2 by the following lower bound and probability vector\n$\\beta_X = \\beta_{X_1} + \\beta_{X_2} \\text{ and } \\mathbf{\\pi}_X = \\mathbf{F}_M^{-1} (\\mathbf{F}_M \\mathbf{\\pi}_{X_1} \\odot \\mathbf{F}_M \\mathbf{\\pi}_{X_2}). \\tag{17}$"}, {"title": "2.4 Formalising PLIAt", "content": "PLIAt is concerned with computing the parametric form of the probability distribution of a linear integer arithmetic expression. It does so by representing random variables and linear combinations thereof as tensors whose entries are the log-probabilities of the individual events in the sample space of the random variable that is being represented. We define this formally as follows.\nDefinition 2.1 (Probabilistic linear arithmetic expression). Let {X1, . . ., XN } be a set of N independent probabilistic integers with bounded domains. A probabilistic linear integer arithmetic expression X is itself a bounded probabilistic integer of the form\n$X = \\sum_{i=1}^N f_i(X_i), \\tag{18}$"}, {"title": "", "content": "where each fi denotes an operation performed on the specified random variables that can be either one of the operations specified in Section 3 as well as compositions thereof.\nNote that operations within PLIA\u2081 are closed. That is, performing either of the operations delin-eated in Section 3 will again result in a bounded probabilistic integer representable as a tensor of log-probabilities and an off-set parameter indicating the value of the smallest possible event (cf. Section 2.3). In Section 4, PLIA\u2081 will also be provided with probabilistic inference primitives that allow it to compute certain expected values efficiently as well as to perform probabilistic branching.\nAssuming all fi (Xi) are computable in polytime, we can also compute PLIA\u0165 expressions (Equation 18) in polytime in N. However, when computing PLIAt expressions recursively, the domain size of the random variables might grow super-polynomially \u2013 manifesting the #P-hard character of probabilistic inference."}, {"title": "3 Arithmetic on Integer-Valued Random Variables", "content": "The previous section introduced how PLIA\u2081 deals with the addition of two probabilistic integers. We discuss now five further operations: 1) addition of a constant, 2) negation, 3) multiplications by a constant, 4) integer division by a constant and 5) the modulo.\nConstant Addition. The addition of a probabilistic integer X and constant scalar integer k forms a new probabilistic integer X' = X + k. Adding a scalar integer is equivalent to a translation of the distribution of X (Figure 2, left). In other words, the lower bound and probability vector of X' are given by\n$\\beta_{X'} = \\beta_X + k \\text{ and } \\mathbf{\\pi}_{X'} = \\mathbf{\\pi}_X. \\tag{19}$"}, {"title": "Negation", "content": "The negation X' = -X of a probabilistic integer X is equally straightforward to characterise. Taking a negation mirrors the probability distribution of X around zero (Figure 2, middle). In terms of lower bound and probability vector, we get $\\beta_{X'} = -(\\beta_X + \\text{dim}(\\mathbf{\\pi}_X) - 1)$ and\n$\\mathbf{\\pi}_{X'}[x] = \\begin{cases} \\mathbf{\\pi}_{X'}[x] = \\mathbf{\\pi}_X[\\text{dim}(\\mathbf{\\pi}_X) - x - 1], & \\text{ if } 0 < x < \\text{dim}(\\mathbf{\\pi}_X), \\\\ 0, & \\text{ otherwise}. \\end{cases} \\tag{20}$"}, {"title": "", "content": "respectively. That is, the lower bound of X' is equal to the negated upper bound of X while the probability vector is flipped, taking into account that probability vectors have to start at 0.\nConstant Multiplication. For the multiplication X' = X \u00b7k of a probabilistic integer X with a scalar integer k, we assume, without loss of generality, that k \u2265 0. Multiplication by a scalar is then characterised as\n$\\beta_{X'} = \\beta_X \\cdot k \\text{ and } \\mathbf{\\pi}_{X'}[x] = \\begin{cases} \\mathbf{\\pi}_X[\\frac{x}{k}], & \\text{ if } x \\text{ mod } k = 0 \\text{ and } 0 < \\frac{x}{k} < \\text{dim}(\\mathbf{\\pi}_X), \\\\ 0, & \\text{ otherwise}. \\end{cases} \\tag{21}$"}, {"title": "", "content": "Intuitively, only multiples of k get a non-zero probability equal to the probability of that multiple in \u03c0\u03c7. The lower bound of X' is also immediately given by multiplying the lower bound of X by k. In other words, we obtain \u03c0\u03c7, by inserting k \u2013 1 zeros between every two subsequent entries of \u03c0\u03c7 (Figure 2, right). The case k < 0 is obtained by first negating X.\nInteger Division and Modulo. For the case of integer division X' = X/k and the modulo operation X' = X mod k, the probability distribution of X' can be obtained by adequately accumulating probability mass from events in \u03a9(\u03a7). We demonstrate these operations by example in Figure 3 and refer the reader to Appendix A for the formal description."}, {"title": "4 Probabilistic Inference Primitives", "content": "PLIAt supports the exact computation of two different forms of expected values. The first is a straightforward expectation of a probabilistic integer X, given by weighing each element of \u03a9(X) with its probability\n$\\mathbb{E} [X] = \\sum_{x \\in \\Omega(X)} x \\cdot \\mathbf{\\pi}_X[x - \\beta_X]. \\tag{22}$"}, {"title": "4.1 Computing Expected Values", "content": "The second is computing the expectation of a linear comparative expression of probabilistic integers. Such a comparison can be an equality, inequality or negated equality. We only consider the equality and strictly larger inequality as the other cases follow from them. The strict inequality can itself always be reduced to an inequality with respect to zero and hence comprises a sum over all domain elements below zero\n$\\mathbb{E}[\\mathbf{1}_{X<0}] = \\sum_{x \\in \\Omega(X): x<0} \\mathbf{\\pi}_X[x - \\beta_X]. \\tag{23}$"}, {"title": "", "content": "Similarly, the computation of the expected value of an equality comparison can always be reduced to a comparison to zero. Hence, the expected value is computable by simple indexing\n$\\mathbb{E} [\\mathbf{1}_{X=0}] = \\mathbf{\\pi}_X[-\\beta_X]. \\tag{24}$"}, {"title": "4.2 Probabilistic Branching", "content": "Consider an if-then-else statement with condition c(x) = (f(x) < 0), where f is a composition of the functions introduced in Section 3 and < \u2208{<,<, =, >, >,\u2260}. Furthermore, x belongs to the domain (X) of a probabilistic integer X. In the case of c(x) being true, a function g\u315c is executed. If c(x) is false, another function g\u2081 is executed instead. We assume that both g\u315c and g\u2081 are again linear arithmetic functions expressible in PLIAt (Section 2.4).\nThe if-then-else statement defines a new probabilistic integer X' by combining both of its branches (Figure 4). These branches depend on X which itself influences a binary random variable C that represents the probabilistic condition of the if-then-else statement. To be precise, the PMF px' is given by the decomposition\n$p_{X'}(x') = p_{X'|c}(x' | \\top) \\cdot p_c(\\top) + p_{X'|c}(x' | \\bot) \\cdot p_c(\\bot), \\tag{25}$"}, {"title": "", "content": "where pxc is the conditional PMF of X' given C. The true branch gives rise to a probabilistic integer XT with probability distribution\n$p_{X_\\top}(x) = p_{X|c}(x | \\top) = \\frac{p_{C|X}(\\top|x) p_X(x)}{p_c(\\top)} = \\frac{\\mathbf{1}_c(x) \\mathbf{\\pi}_X[x - \\beta_X]}{p_c(\\top)} = \\frac{\\mathbf{1}_c(x) \\pi_X[x - \\beta_X]}{p_c(\\top)} \\tag{26}$"}, {"title": "", "content": "If C = T, then X' is given by an application of g\u315c on the instances x \u2208 \u03a9(\u03a7) that satisfy c(x). Consequently, by applying g\u315c, we find that\n$p_{X'|c}(x' | \\top) = p_{g_\\top(X_\\top)}(x'). \\tag{27}$"}, {"title": "", "content": "With the right-hand side of Equation 26, we now know how to obtain the probability vector for X\u315c. Using Equation 27 and the operations from Section 3, we can then compute the probability vector \u3160g\u315c(X\u315c), as well as the lower bound \u1e9eg\u315c(X+). Also note that pc(T) is nothing but an expected value as described by Equation 23 or Equation 24. Analogously, we obtain for the false branch that\n$\\mathbf{\\pi}_{X_\\bot}(x) = \\frac{(1 - \\mathbf{1}_c(x)) \\mathbf{\\pi}_X[x - \\beta_X]}{p_c(\\bot)} \\text{ and } p_{X'|c}(x' | \\bot) = p_{g_\\bot(X_\\bot)}(x'). \\tag{28}$"}, {"title": "", "content": "By plugging the expressions for px'|c(x' | T) and px'|c(x\u2032 | \u22a5) into Equation 25 we find that\n$p_{X'}(x') = p_{g_\\top(X_\\top)}(x') \\cdot p_c(\\top) + p_{g_\\bot(X_\\bot)}(x') \\cdot p_c(\\bot), \\tag{29}$"}, {"title": "", "content": "which are all quantities computable using either probabilistic linear arithmetic operations or expected values thereof."}, {"title": "5 Experiments", "content": "We first compare PLIA\u2081 to the state of the art in probabilistic integer arithmetic [5] in terms of inference speed (Section 5.1). These experiments were performed using an Intel Xeon Gold 6230R CPU @ 2.10GHz, 256GB RAM for CPU experiments and an Nvidia TITAN RTX (24GB) for GPU experiments. In Section 5.2, we then illustrate how PLIA\u2081 fares against the state of the art in neurosymbolic AI [14, 33]. These experiments were performed using an Nvidia RTX 3080 Ti (12GB). We implemented PLIA\u2081 in TensorFlow [1] using the Einops library [30]. This implementation is open-source and available at https://github.com/ML-KULeuven/probabilistic-arithmetic"}, {"title": "5.1 Exact Inference with Probabilistic Integers", "content": "The work of Cao et al. [5] exploits the structural properties and symmetries of integer arithmetic by proposing general encoding strategies for an arithmetic expression of probabilistic integers as logical circuits. That is, binary decision diagrams [4] obtained via knowledge compilation [7]. This strategy allows them to avoid redundant calculations and repetition, leading to improved scalability over more naive encodings [8, 13].\nWe compare PLIA\u2081 and Cao et al.'s inference algorithm on four of their benchmark problems. In the first three benchmarks, expected values of the sum of two random variables need to be computed. Concretely, the expectations E [X1 + X2] (cf. Equation 22), E [1X1+X2<0] (cf. Equation 23) and E [1X1+X2=0] (cf. Equation 24).\nAs a fourth benchmark we use a probabilistic version of the Luhn checksum algorithm [20], which necessitates summation of two probabilistic integers, negation, addition of a constant, multiplication by a constant, the modulo operations, as well as probabilistic branching. We provide further details on the Luhn algorithm in general and the encoding of its probabilistic variant in PLIAt in Appendix C.\nAs the probabilistic Luhn algorithm takes as input an identifier consisting of a sequence of probabilistic integers with domain {0, . . ., 9}, we can increase the problem size by increasing the length of this sequence. For the other three benchmarks we vary the problem size by varying the domain size of the probabilistic integers in terms of their bitwidth. That is, a bitwidth of i \u2208 N indicates that we consider probabilistic integers ranging from 0 up until 2\u00aa\u00b2 \u2013 1, increasing the problem size exponentially in terms of i. In our experimental evaluation (Figure 5), we measure the time it took for PLIA\u2081 and Cao et al.'s method to terminate for varying problem sizes. The measured time includes all computational overhead inherent to each method, such as the construction of computational graphs and compilation time. Each method is also profiled in terms of memory, which we discuss further in Appendix B. Note that Cao et al.'s method is denoted by \u201cDice\u201d, the probabilistic programming language in which it was implemented.\nFor the first three benchmarks, we observe that PLIA\u0165 easily scales to probabilistic integers with a domain size of 224 on both the CPU and GPU as the highest runtime reached is less than 100 seconds on the CPU and approximately 1 second on the GPU. The similarity of the curves is due to the fact"}, {"title": "", "content": "that the run time is dominated by computing the probability vector \u3160X1+X2 and not so much by computing the actual expected value.\nIn contrast, Dice, which only runs on CPU, already reaches a runtime of approximately 1000 seconds for integers with domain size 215, where PLIA\u2081 only takes around 10-1 and 10-2 seconds on the CPU and GPU, respectively. This is a rather considerable improvement in the order of 105. Dice can outperform PLIAt on the GPU (Figure 5, bitwidth smaller strictly below 5) due to the computational overhead of running on the GPU. However, much of this overhead can be avoided by running PLIAt on the CPU for smaller domain sizes, where it performs on par or better than Dice (Figure 5, bitwidth smaller strictly below 5).\nOn the probabilistic Luhn benchmark (Figure 5, extreme right) we observe that both methods exhibit similar linear scaling behaviors. However, the use of tensors as representations instead of logical circuits does result in a significant improvement in terms of run time in the order of 102 for the longest sequences of length 350."}, {"title": "5.2 Neurosymbolic Learning", "content": "For the comparison of PLIA\u0165 to neurosymbolic systems", "literature": "MNIST addition [21", "2": ".", "22": "and two approximate methods, Scallop [14", "33": "."}]}