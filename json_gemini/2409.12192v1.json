{"title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control", "authors": ["Zichen Jeff Cui", "Hengkai Pan", "Aadhithya Iyer", "Siddhant Haldar", "Lerrel Pinto"], "abstract": "Imitation learning has proven to be a powerful tool for training complex visuo-motor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io.", "sections": [{"title": "1 Introduction", "content": "Learning visuo-motor policies from human demonstrations is an exciting approach for training difficult control tasks in the real world [1-5]. However, a key challenge in such a learning paradigm is to efficiently learn a policy with fewer expert demonstrations. To address this, prior works have focused on learning better visual representations, often by pretraining on large Internet-scale video datasets [6-11]. However, as shown in Dasari et al. [12], these out-of-domain representations may not transfer to downstream tasks with very different embodiments and viewpoints from the pretraining dataset.\nAn alternative to using Internet-pretrained models is to train the visual representations 'in-domain' on the demonstration data collected to solve the task [13, 4]. However, in-domain datasets are much smaller than Internet-scale data. This has resulted in the use of domain-specific augmentations [13] to induce representational invariances with self-supervision or to collect larger amounts of demonstrations [2, 14]. The reliance of existing methods on large datasets might suggest that in-domain self-supervised pretraining is ineffective for visuo-motor control, and we might be better with simply training end-to-end. In this work, we argue the contrary \u2013 in-domain self-supervision can be effective with a better training objective that extracts more information from small datasets."}, {"title": "2 Background", "content": null}, {"title": "2.1 Visual imitation learning", "content": "Our work follows the general framework for visual imitation learning. Given demonstration data D = {(ot, at)}t, where ot are raw visual observations and at are the corresponding ground-truth actions, we first employ a visual encoder fo : Ot \u2192 St to map the raw visual inputs to lower-dimensional embeddings st. We then learn a policy \u03c0(at|st) to predict the appropriate actions. For rollouts, we model the environment as a Markov Decision Process (MDP), where each subsequent observation Ot+1 depends on the previous observation-action pair (ot, at). We assume the action-conditioned transition distribution p(Ot+1|Ot, at) to be unimodal for our manipulation tasks."}, {"title": "2.2 Visual pretraining for policy learning", "content": "Our goal is to pretrain the visual encoder fe using a dataset of sequential raw visual observations D = {ot}t to support downstream policy learning. During pretraining, we do not assume access to the ground-truth actions {at}t.\nPrior work has shown that pretraining encoders on large out-of-domain datasets can improve downstream policy performance [6\u201311]. However, such pretraining may not transfer well to tasks with different robot embodiments [12].\nAlternatively, we can directly pretrain the encoder in-domain using self-supervised methods. One approach is contrastive learning with data augmentation priors, randomly augmenting an image twice and pushing their embeddings closer. Another approach is denoising methods, predicting the original image from a noise-degraded sample (e.g. by masking [11, 8, 30]). A third approach is contrastive learning with temporal proximity as supervision, pushing temporally close frames to have similar embeddings [31, 32]."}, {"title": "3 DynaMo", "content": "Limitations of prior self-supervised techniques: Prior self-supervised techniques can learn to fixate on visually salient features and ignore fine-grained features important for control. We illustrate this limitation using the Block Pushing environment from Florence et al. [28]. In this task, the goal is to push a block into a target square. While the robot arm occupies much of the raw pixel space, the blocks are central to the task despite being smaller in the visual field. \nWe observe that prior self-supervised methods (details in \u00a74.2) focus on the visually dominant robot, matching the whole robot arm extremely accurately. However, they fail to capture the block positions, which are important to the task despite being much less salient visually.\nCan we learn a visual encoder that extracts task-specific features better? We know that the demonstrations are sequential: each observation is dependent on the previous observation, and an action (unobserved in this setting). Prior self-supervised methods ignore this sequential structure. Contrastive augmentations [16, 33] and autoencoding objectives [30, 8, 11] assume that the demonstration video is a bag of frames, discarding temporal information altogether. Temporal contrast [32, 31] uses temporal proximity but discards the sequential information in the observations: the contrastive objectives are usually symmetric in time, disregarding past/future order.\nInstead of a contrastive or denoising objective, we propose a dynamics prediction objective that explicitly exploits the sequential structure of demonstration observations.\nOverview of DynaMo: The key insight of our method is that we can learn a good visual representation for control by modeling the dynamics on demonstration observations, without requiring augmentations, contrastive sampling, or access to the ground truth actions. Given a sequence of raw"}, {"title": "3.1 Dynamics as a visual self-supervised learning objective", "content": "First, we sample an observation sequence Ot:t+h of length h and compute its representation st:t+h = fo(Ot:t+h). For convenience, we will write st:t+h as s:h, and St+1:t+h as 81:h below. At any given step, the distribution of possible actions can be multimodal [5]. Therefore, the forward dynamics transition p(81:h|S:h\u22121) can also have multiple modes. To address this, we first model the inverse dynamics q(z:h\u22121|8:h), where zt is the latent transition between frames. We assume zt to be well-determined and unimodal given consecutive frames {st, St+1}. We have z \u2208 Rm, s \u2208 Rd,m \u00abd such that the latent cannot trivially memorize the next frame embedding. Finally, we concatenate (st, 2t) and predict the one-step forward dynamics p($1:h|S:h\u22121, Z:h\u22121)."}, {"title": "4 Experiments", "content": "We evaluate our dynamics-pretrained visual representation on a suite of simulated and real benchmarks. We compare DynaMo representations with pretrained representations for vision and control, as well as other self-supervised learning methods. Our experiments are designed to answer the following questions: (a) Does DynaMo improve downstream policy performance? (b) Do representations trained with DynaMo work on real robotic tasks? (c) Is DynaMo compatible with different policy classes? (d) Can pretrained weights be fine-tuned in domain with DynaMo? (e) How important is each component in DynaMo?"}, {"title": "4.1 Environments and datasets", "content": "We evaluate DynaMo on four simulated benchmarks and two real robot environments (depicted in Figure 4). We provide a brief description below with more details included in Appendix A.\n(a) Franka Kitchen [27]: The Franka Kitchen environment consists of seven simulated kitchen appliance manipulation tasks with a 9-dimensional action space Franka arm and gripper. The dataset has 566 demonstration trajectories, each completing three or four tasks. The observation space is RGB images of size (224, 224) from a fixed viewpoint. We evaluate for 100 rollouts and report the mean number of completed tasks (maximum 4).\n(b) Block Pushing [28]: The simulated Block Pushing environment has two blocks, two target areas, and a robot pusher with 2-dimensional action space (end-effector translation). Both the blocks and targets are colored red and green. The task is to push the blocks into either same- or opposite-colored targets. The dataset has 1000 demonstration trajectories. The observation is RGB images of size (224, 224) from two fixed viewpoints. We evaluate for 100 rollouts and report the mean number of blocks in targets (maximum 2).\n(c) Push-T [3]: The environment consists of a pusher with 2-dimensional action space, a T-shaped rigid block, and a target area in green. The task is to push the block to cover the target area. The dataset has 206 demonstration trajectories. The observation space is a top-down view of the environment, rendered as RGB images of size (224, 224). We evaluate for 100 rollouts and report the final coverage of the target area (maximum 1).\n(d) LIBERO Goal [29]: The environment consists of 10 manipulation tasks with a 7-dimensional action space simulated Franka arm and gripper. The dataset has 500 demonstration trajectories in total, 50 per task goal. The observation space is RGB images of size (224, 224) from a fixed external camera, and a wrist-mounted camera. We evaluate a"}, {"title": "4.2 Does DynaMo improve downstream policy performance?", "content": "We evaluate each representation by training an imitation policy head on the frozen embeddings, and reporting the downstream task performance on the simulated environments. We use Vector-Quantized Behavior Transformer (VQ-BeT) [1] for the policy head. For xArm Kitchen, we use a goal-conditioned BAKU [38] with a VQ-BeT action head. MAE-style baselines (VC-1, MVP, MAE) use a ViT-B backbone. All other baselines and DynaMo use a ResNet18 backbone.\nFor environments with multiple views, we concatenate the embeddings from all views for the downstream policy. Further training details are in Appendix B. Table 1 provides comparisons of DynaMo pretrained representations with other self-supervised learning methods, and pretrained weights for vision and robotic manipulation:\n\u2022 Random, ImageNet, R3M: ResNet18 with random, ImageNet-1K, and R3M [9] weights.\n\u2022 VC-1: Pretrained weights from Majumdar et al. [11].\n\u2022 MVP: Pretrained weights from Xiao et al. [8].\n\u2022 BYOL: BYOL [16] pretraining on demonstration data.\n\u2022 BYOL-T: BYOL + temporal contrast [32]. Adjacent frames Ot, Ot+1 are sampled as positive pairs, in addition to augmentations.\n\u2022 MoCo-v3: MoCo [33] pretraining on demonstration data.\n\u2022 RPT: RPT [39] trained on observation tokens.\n\u2022 TCN: Time-contrastive network [31] pretraining on demonstrations. MV: multi-view objective; SV: single view objective.\n\u2022 MAE: Masked autoencoder [30] pretraining on demonstrations.\n\u2022 DynaMo: DynaMo pretraining on demonstrations.\nThe best pretrained representation is underlined and the best self-supervised representation is bolded. We find that our method matches prior state-of-the-art visual representations on Franka Kitchen, and outperforms all other visual representations on Block Pushing, Push-T, and LIBERO Goal."}, {"title": "4.3 Do representations trained with DynaMo work on real robotic tasks?", "content": "We evaluate the representations pre-trained with DynaMo on two real-world robot environments: the Allegro Manipulation environment, and the multi-task xArm Kitchen environment. For the Allegro environment, we use a k-nearest neighbors policy [40] and initialize with ImageNet-1K features for all pretraining methods, as the dataset is relatively small with around 1000 frames per task. In the xArm Kitchen environment, we use the BAKU [38] architecture for goal-conditioned rollouts across five tasks. For our real-robot evaluations, we compare DynaMo against the strongest performing baselines from our simulated experiments (see Table 1). The results are reported in Table 2. We observe that DynaMo outperforms the best baseline by 43% on the single-task Allegro hand and by 20% on the multi-task xArm Kitchen environment. Additionally, as shown in Table 3, DynaMo exceeds the performance of pretrained representations by 50% on the Allegro hand. These results demonstrate that DynaMo is capable of learning effective robot representations in both single-task and multi-task settings."}, {"title": "4.4 Is DynaMo compatible with different policy classes?", "content": "On the Push-T environment [3], we compare all pretrained representations across four policy classes: VQ-BeT [1], Diffusion Policy [3], MLP (with action chunking [2]), and k-nearest neighbors with locally weighted regression [40]. We present the results in Table 4. We find that DynaMo representa-"}, {"title": "4.5 Can pretrained weights be fine-tuned in domain with DynaMo?", "content": "We fine-tune an ImageNet-1K-pretrained ResNet18 with DynaMo for each simulated environment, and evaluate with downstream policy performance on the frozen representation as described in \u00a74.2. The results are shown in Table 5. We find that Dy-naMo is compatible with ImageNet initialization, and can be used to fine-tune out-of-domain pretrained weights to further improve in-domain task performance. We also note that our method works in the low-data regime with ImageNet initialization on the real Allegro hand in Table 2."}, {"title": "4.6 How important is each component in DynaMo?", "content": "In Table 6, we ablate each component in DynaMo and measure its impact on downstream policy performance on our simulated benchmarks.\nForward dynamics prediction: We replace the one-step forward prediction target sh with the same-step target sh\u22121. To prevent the model from trivially predicting st given st, we replace the forward dynamics input (s:h\u22121, 2:h\u22121) with only 2:h\u22121. The ablated objective is essentially a variant of autoencoding st. We observe that removing forward dynamics prediction degrades performance across environments."}, {"title": "4.7 Variants with access to ground truth actions", "content": "In Table 7, we compare with two variants of DynaMo where we assume access to ground truth action labels during visual encoder training.\nOnly inverse dynamics to ground truth actions: as proposed in Brandfonbrener et al. [26], we train the visual encoder by learning an inverse dynamics model to ground truth actions, with covariance regularization, and without forward dynamics.\nFull model + inverse dynamics to ground truth actions: we train the full DynaMo model plus an MLP head to predict the ground truth actions given the transition latents inferred by the inverse dynamics model.\nWe observe that in both cases, having access to ground truth actions during visual pretraining does not seem to improve downstream policy performance. We hypothesize that this is because the downstream policy already has access to the same actions for imitation learning."}, {"title": "5 Related works", "content": "This work builds on a large body of research on self-supervised visual representations, learning from human demonstrations, neuroscientific basis for learning dynamics for control, predictive models for decision making, learning from videos for control, and visual pretraining for control."}, {"title": "Self-supervised visual representations:", "content": "Self-supervised visual representations have been widely studied since the inception of deep learning. There are several common approaches to self-supervised visual representation learning. One approach is to recover the ground truth from noise-degraded samples using techniques like denoising autoencoders [42, 43] and masked modeling [44, 45, 30]. Another approach is contrastive learning, which leverages data augmentation priors [41, 16, 33, 36, 37] or temporal proximity [31, 46] to produce contrastive sample pairs. A third self-supervised method is generative modeling [47-49], which learns to sequentially generate the ground truth data. More recently, self-supervision in the latent space rather than the raw pixel space has proven effective, as seen in methods that predict representations in latent space [50, 51]."}, {"title": "Learning from demonstrations:", "content": "Learning from human demonstrations is a well-established idea in robotics [52-55]. With the advances in deep learning, recent works such as [3, 2, 5, 4, 1, 56] show that imitation learning from human demonstrations has become a viable approach for training robotic policies in simulated and real-world settings."}, {"title": "Neural basis for learning dynamics:", "content": "It is widely believed that animals possess internal dynamics models that facilitate motor control. These models learn representations that are predictive of sensory inputs for decision making and motor control [57-60]. Early works such as [17-20] propose that there exists an internal model of the motor apparatus in the cerebellum for motor control and planning. [21, 22] propose that the central nervous system uses forward models that predict motor command outcomes and model the environment. Learning forward and inverse dynamics models also helps with generalization to diverse task conditions [23, 24]."}, {"title": "Predictive models for decision making:", "content": "Predictive model learning for decision making is well-established in machine learning. Learning generative models that can predict sequential inputs has achieved success across many domains, such as natural language processing [61], reinforcement learning [62], and representation learning [46, 63]. Incorporating the prediction of future states as an intrinsic reward has also been shown to improve reinforcement learning performance [64-66]. Moreover, recent work demonstrates that world models trained to predict environment dynamics can enable planning in complex tasks and environments [67-70]."}, {"title": "Learning from video for control:", "content": "Videos provide rich spatiotemporal information that can be leveraged for self-supervised representation learning [71-76]. These methods have been extended to decision-making through effective downstream policy learning [7-11, 6]. Further, recent work also enables learning robotic policies directly from in-domain human demonstration videos by incorporating some additional priors [77-81]."}, {"title": "Visual representation for control:", "content": "Visual representation learning for control has been an active area of research. Prior work has shown that data augmentation improves the robustness of learned representations and policy performance in reinforcement learning domains [82, 83]. Additionally, pretraining visual representations on large out-of-domain datasets before fine-tuning for control tasks has been shown to outperform training policies from scratch [10, 12, 9, 11, 84, 8, 85]. More recent work has shown that in-domain self-supervised pretraining improves policy performance [86-88] and enables non-parametric downstream policies [40]."}, {"title": "6 Discussion and Limitations", "content": "In this work, we have presented DynaMo, a self-supervised algorithm for robot representation learning that leverages the sequential nature of demonstration data. DynaMo incorporates predictive dynamics modeling to learn visual features that capture the sequential structure of demonstration observations. During pretraining, DynaMo jointly optimizes the visual encoder with dynamics models to extract task-specific features. These learned representations can then be used for downstream control tasks, leading to more efficient policy learning compared to prior approaches. We believe that training DynaMo on larger unlabeled datasets could potentially improve generalization. Additionally, while promising for control tasks, more research is needed to evaluate DynaMo's effectiveness on robotic manipulation outside of lab settings."}, {"title": "A Environment and dataset details", "content": null}, {"title": "A.1 Franka Kitchen", "content": "The Franka Kitchen environment introdued by Gupta et al. [27] consists of a Franka arm with a 9-dimensional action space. This environment includes seven tasks and a dataset of 566 human-collected demonstrations. While the original environment is state-based, we created an image-based variant by rendering the states to 224 \u00d7 224 RGB images."}, {"title": "A.2 Block Pushing", "content": "In the Block Pushing environment introduced by Florence et al. [28], the objective is for the robot to push two colored blocks (red and green) into two target squares (also red and green). The training dataset consists of 1000 trajectories, evenly distributed among the four possible combinations of block target and push order. These trajectories were collected by a scripted expert controller."}, {"title": "A.3 Push-T", "content": "In the Push-T environment introduced by Chi et al. [3], the goal is to push a T-shaped block to a designated target position on a table. The dataset for this environment contains 206 demonstrations collected by human operators. The action space in this environment is a two-dimensional end-effector position control. Similar to the Franka Kitchen environment, we have created an image-based variant by rendering demonstrations to 224 \u00d7 224 RGB images."}, {"title": "A.4 LIBERO Goal", "content": "In the LIBERO Goal environment introduced by Liu et al. [29], there are 10 manipulation tasks, each with 50 teleoperated demonstrations for goal-conditioned policy benchmarking. The environment has a 7-dimensional action space and an observation space of 224 \u00d7 224 RGB images from two cameras (fixed external view, and wrist-mounted egocentric view)."}, {"title": "A.5 Allegro Manipulation", "content": "The environment consists of an Allegro hand attached to a Franka arm, and a fixed camera for image observations. The observation space is 224 \u00d7 224 RGB images. The action space is 23-dimensional, consisting of Cartesian position and orientation of the Franka robot arm (7 DoF), and 16 joint positions of the Allegro Robot Hand. The demonstrations are collected at 50Hz for Franka, and 60Hz for the Allegro hand. The learned policies are rolled out at 4Hz.\nWe evaluate on three contact-rich dexterous manipulation tasks that require precise multi-finger control and arm movement, described in detail below.\nSponge picking: This task requires the hand to reach to the position of the sponge, grasp the sponge, and lift the sponge from the table. We collect 6 demonstrations via OpenTeach [89] for the task, starting from different positions, with 543 frames in total. The task is considered successful if the robot hand can grasp the sponge from the table within 120 seconds.\nTeabag picking: This task is similar to the previous task, but more difficult with a smaller task object. We collect 7 demonstrations via OpenTeach with 1034 frames in total. In this task, the robot needs reach the teabag, grasp the teabag with two fingers, then pick it up. The task is considered successful if the robot hand can grasp the teabag from the table within 240 seconds.\nMicrowave opening: This task requires the hand to reach the microwave door handle, grasp the handle, and pull down the door. We collect 6 demonstrations via OpenTeach with 735 frames in total. The task is considered successful if the robot hand can open the door within 240 seconds."}, {"title": "A.6 xArm Kitchen", "content": "This is a real-world multi-task kitchen environment comprising a Ufactory xArm 7 robot with an xArm Gripper. The policies are trained on RGB images of size 128 \u00d7 128 obtained from four different camera views, including an egocentric camera attached to the robot gripper. The action space"}, {"title": "B Hyperparameters and implementation details", "content": null}, {"title": "B.1 Visual encoder training", "content": "We present the DynaMo hyperparameters below."}, {"title": "B.2 Downstream policy training", "content": "Table 13, 14 and 15 detail the downstream policy hyperparameters for VQ-BeT, Diffusion Policy and MLP training for the simulated environments."}]}