{"title": "Multi-Object Hallucination in Vision-Language Models", "authors": ["Xuweiyi Chen", "Ziqiao Ma", "Xuejun Zhang", "Sihan Xu", "Shengyi Qian", "Jianing Yang", "David F. Fouhey", "Joyce Chai"], "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1) LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2) The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3) Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have motivated increasing efforts in adapting them for understanding visual semantics, giving rise to a surge of large vision language models (LVLMs) (Alayrac et al., 2022; OpenAI, 2023; Reid et al., 2024). These models, whether explicitly trained with grounding data (Zhang et al., 2024b) or without (Liu et al., 2023b), demonstrate an impressive grounded understanding of visual entities. Despite their promising performances on various downstream applications (Nasiriany et al., 2024), LVLMs often suffer from object hallucination (Rohrbach et al., 2018; Dai et al., 2023b; Li et al., 2023a), where they produce objects not present in a given image.\nAlthough object hallucination was initially observed in image captioning describing multiple objects (Rohrbach et al., 2018), current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities. These benchmarks either verify if an object class mentioned in the caption can ground to an object in the image (Rohrbach et al., 2018; Jing et al., 2023), or probe the model about the existence of an object class, sometimes with additional attributes or relations to other objects (Li et al., 2023a; Lovenia et al., 2023). There are, however, two key limitations with these setups as shown by a case study in Figure 1. First, grounding is not simply one-to-one between objects and classes, but a many-to-many mapping between objects and phrases (Kamath et al., 2021; Ma et al., 2023). For instance, \"apple\" could potentially correspond to multiple referents in Figure 1(c), and the model doesn't necessarily need to recognize all of them to provide such a response. Therefore, being able to produce an object that exists in an image does not necessarily indicate that the model is free of hallucinations. Second, explicitly instructing the model to recognize multiple objects poses greater challenges compared to simple yes/no inquiries that contain explicit text descriptions for individual objects. For instance, while the model can correctly identify that a whisk is positioned to the left of a knife when \"a whisk\u201d is deliberately prompted, as shown in Figure 1(b-d), it may hallucinate a \u201cfork\u201d when directly prompted to recognize both the whisk and the knife (i.e., Figure 1a). This could be due to the common association between knives and forks, which leads to potential hallucinations when mod-"}, {"title": "2 Related Work", "content": "Large Vision-Language Models. There is a growing trend to harness and adapt the powerful"}, {"title": "3 Recognition-based Object Probing\nEvaluation (ROPE)", "content": "We introduce the Recognition Object Probing Eval-uation (ROPE), an automated protocol for assess-ing LVLMs in multi-object recognition."}, {"title": "3.1 Task Setup", "content": "Problem Definition. To avoid ambiguity frommultiple candidate referents when using textprompts, ROPE leverages visual prompts touniquely refer to objects. ROPE tasks LVLMs withselecting the best matching class for multiple ob-jects, as referred to by the visual prompt, from apredefined set of object classes. Specifically, eachsample in the ROPE protocol consists of a quadru-ple {I, L, (p1,\u2026\u2026, Pn), (o1,\u2026\u2026, on)}: (1) an im-age I consisting of at least n objects; (2) a natu-ral language instruction L that specifies the recog-nition task, including N candidate object classesC1,\uff65\uff65\uff65, CN; (3) n visual prompts p1,\u00b7\u00b7\u00b7, Pn, eachqueries an object in the image; and (4) n objectclasses o1,..., on as the answers. In this work, weconstruct a dataset with N = 50 and n = 5, i.e.,"}, {"title": "5 Analysis of Hallucinatory Behaviors", "content": "The task setup described above allows us to evaluate LVLMs in multi-object hallucinations and identify hallucinatory behaviors. Based on existing literature and our case studies, we further identify potential factors that correlate to and potentially explain these hallucinations."}, {"title": "4 Experiments and Results", "content": "4.1 LVLM Baselines\nThe proposed ROPE framework, in principle, applies to all LVLMs that can follow format instructions and understand multiple visual prompts. To cover a variety of LVLMs of different scales and training data (e.g., whether grounding data and conversational data are used), we selected the following LVLMs as baselines.\nScaling the base LLM: data and parameters.\nVisual instruction fine-tuning: chat and grounding."}, {"title": "4.2 Main Results and Findings", "content": "We summarize the average results across the splitsin Table 2 and present the most important findingsbelow. The full tables appear in Appendix A.2."}, {"title": "4.3 What May Help and What May Not?", "content": "Comparing the tested LVLMs, we discuss our observations regarding design considerations that may or may not help reduce multi-object hallucinations."}, {"title": "5.1 Potential Hallucinatory Factors", "content": "The task setup described above allows us to evaluate LVLMs in multi-object hallucinations and identify hallucinatory behaviors. Based on existing literature and our case studies, we further identify potential factors that correlate to and potentially explain these hallucinations."}, {"title": "5.2 When Do LVLMs Experience\nMulti-Object Hallucinations?", "content": "In Figure 6, we compare the distribution ofthese factors between hallucinatory and non-hallucinatory objects in the student forcing settingon the unseen split using LLaVA-13B. For continu-ous values, we use ridgeline plots, and for discretevalues with fewer bins, we use bar charts."}, {"title": "5.3 How Do LVLMs Experience Multi-Object\nHallucinations?", "content": "In Figure 7, we conducted a detailed comparisonof the distribution of actual versus predicted objectclasses within the context of hallucinatory objects,examining factors such as semantic salience, training salience, and input order. Although semanticsalience is a key factor in determining whether amodel hallucinates, it appears to have minimal impact on the prediction of hallucinated objects. Ouranalysis also shows that models are more likelyto hallucinate object classes that are prevalent inthe training data, but the reverse is not necessarilytrue. Additionally, there is a notable preference formodels to hallucinate objects that are listed earlyin the input prompt as candidate classes. Overall,our findings indicate that spurious correlations maylead to hallucinations involving multiple objects."}, {"title": "6 Discussions and Conclusion", "content": "Hallucinations in large vision-language models(LVLMs) can occur at different scales and granularities. In this study, we study the problem ofmulti-object hallucination, examining how LVLMSmay misperceive when tasked to focus on multipleobjects concurrently, and which factors cause thehallucinations. we introduce Recognition-basedObject Probing Evaluation (ROPE), an automatedevaluation protocol designed to account for the dis-tribution of object classes within a single image"}, {"title": "A Additional Experiments, Results, and Discussions", "content": "A.1 Reproducibility Data Curation Pipeline. Our data curation pipeline involves several essential steps designed to prepare and refine our dataset for evaluating multi-object hallucination. The pipeline begins by filtering images and candidate objects to query. We consider valid objects to be those belonging to the top 50 \"thing\u201d classes and exclude objects with a bounding box area less than 1% of the total image area. We discard images containing fewer than 5 valid objects, and allow an intersection-over-union between bounding boxes of no more than 0.1, which preserves data integrity while ensuring high image quality. We apply this pipeline to MSCOCO-Panoptic (Lin et al., 2014; Caesar et al., 2018) and ADE20K (Zhou et al., 2017).2 A.2 Additional Experiments and Results We provide the per-object performance in the following tables."}, {"title": "Task Setup", "content": "For a fair com-parison that accommodates both open-weight andAPI-based LVLMs, ROPE explicitly instructs mod-els to generate a formatted output of object classes,\ne.g., obj1:<class1>,\n..., obj5:<class5>.\nThis format enables automated evaluation throughsimple parsing, avoiding black-box neural modelsor human evaluators With different analytical pur-poses, we designed 3 types of task prompts forMulti-Object queries, as illustrated in Figure 2 anddescribed as follows."}, {"title": "Data Sources and Curation", "content": "Since our goal isto evaluate and analyze multi-object hallucination,the image data must contain multiple objects ofdiverse classes with instance-level semantic anno-tations. We build our dataset upon existing panop-tic segmentation datasets, including MSCOCO-"}]}