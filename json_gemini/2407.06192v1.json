{"title": "Multi-Object Hallucination in Vision-Language Models", "authors": ["Xuweiyi Chen", "Ziqiao Ma", "Xuejun Zhang", "Sihan Xu", "Shengyi Qian", "Jianing Yang", "David F. Fouhey", "Joyce Chai"], "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1) LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2) The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3) Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.", "sections": [{"title": "Introduction", "content": "Recent advances in large language models (LLMs) have motivated increasing efforts in adapting them for understanding visual semantics, giving rise to a surge of large vision language models (LVLMs) (Alayrac et al., 2022; OpenAI, 2023; Reid et al., 2024). These models, whether explicitly trained with grounding data (Zhang et al., 2024b) or without (Liu et al., 2023b), demonstrate an impressive grounded understanding of visual entities. Despite their promising performances on various downstream applications (Nasiriany et al., 2024), LVLMs often suffer from object hallucination (Rohrbach et al., 2018; Dai et al., 2023b; Li et al., 2023a), where they produce objects not present in a given image.\nAlthough object hallucination was initially observed in image captioning describing multiple objects (Rohrbach et al., 2018), current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities. These benchmarks either verify if an object class mentioned in the caption can ground to an object in the image (Rohrbach et al., 2018; Jing et al., 2023), or probe the model about the existence of an object class, sometimes with additional attributes or relations to other objects (Li et al., 2023a; Lovenia et al., 2023). There are, however, two key limitations with these setups as shown by a case study in Figure 1. First, grounding is not simply one-to-one between objects and classes, but a many-to-many mapping between objects and phrases (Kamath et al., 2021; Ma et al., 2023). For instance, \"apple\" could potentially correspond to multiple referents in Figure 1(c), and the model doesn't necessarily need to recognize all of them to provide such a response. Therefore, being able to produce an object that exists in an image does not necessarily indicate that the model is free of hallucinations. Second, explicitly instructing the model to recognize multiple objects poses greater challenges compared to simple yes/no inquiries that contain explicit text descriptions for individual objects. For instance, while the model can correctly identify that a whisk is positioned to the left of a knife when \"a whisk\u201d is deliberately prompted, as shown in Figure 1(b-d), it may hallucinate a \u201cfork\u201d when directly prompted to recognize both the whisk and the knife (i.e., Figure 1a). This could be due to the common association between knives and forks, which leads to potential hallucinations when models are tasked to focus on multiple objects at the same time. To enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes and to better quantify the complex phenomena we observed, this paper investigates multi-object hallucination, examining how models may misperceive (e.g., by inventing nonexistent objects or becoming distracted) when tasked to focus on multiple objects concurrently, and which factors cause the hallucinations.\nWe start by introducing Recognition-based Object Probing Evaluation (ROPE) for assessing multi-object hallucination with formatted output control. ROPE features an automated evaluation protocol without black-box neural models or humans as evaluators, and leverages visual prompts to uniquely refer to objects to avoid ambiguity and multiple referents caused by object class names. ROPE considers the distribution of object classes within each image at test time, dividing ROPE into 4 subsets: In-the-Wild, Homogeneous, Heterogeneous, and Adversarial. For instance, we investigate scenarios where all tested objects belong to the same class or where each tested object represents a different class. We conduct an in-depth analysis of the hallucination behaviors of LVLMs of different scales and training data (e.g., whether grounding data and conversational data are used), and provide a comprehensive analysis of potential factors that lead to multi-object hallucination. Our main findings are: (1) LVLMs suffer from more hallucinations when tasked to focus on multiple objects, compared to focusing on a single object; (2) The tested object class distribution affects the hallucination behaviors, revealing that LVLMs may be following shortcuts and spurious correlations; (3) The hallucinatory behaviors of LVLMs are affected by data-specific factors, salience and frequency, and model intrinsic behaviors. These findings provide key insights for the development and application of LVLMs, suggesting for more balanced object distributions, diverse annotations, and enhanced multi-object instructions in grounded LVLMs. We hope this work takes a step towards LVLMs that recognize and reason about multiple objects that often occur in realistic visual scenes."}, {"title": "Related Work", "content": "Large Vision-Language Models. There is a growing trend to harness and adapt the powerful large language models (LLMs) for multimodal understanding beyond text (Tsimpoukelli et al., 2021; Alayrac et al., 2022; Yu et al., 2024a; Qian et al., 2024). Especially, visual instruction tuning has gained prominence for its competitive performance with a comparatively moderate amount of data and computational resources, leading to a variety of Large Vision-Language Models (LVLMs) (Liu et al., 2023b, 2024; Dai et al., 2023a; Zhu et al., 2023; Gong et al., 2023; Wang et al., 2023d; Ye et al., 2023; Lauren\u00e7on et al., 2023a). Grounding datasets have been shown to benefit vision-language pre-training (Lu et al., 2022; Li et al., 2022; Ma et al., 2023). Researchers have developed a family of grounded LVLMs focusing on object grounding to bounding box (Pi et al., 2023; Chen et al., 2023; Zhao et al., 2023; Bai et al., 2023; You et al., 2023; Zhang et al., 2024a; Peng et al., 2024) and segmentation masks (Lai et al., 2023; Zhang et al., 2023; Xia et al., 2024; Rasheed et al., 2024; Zhang et al., 2024b). Of the large space of LVLMs, our work is most related to visual prompting (Yang et al., 2023a,b) and object hallucination (Rohrbach et al., 2018; Dai et al., 2023b). The paragraphs below describe the two lines of work in detail.\nVisual Prompting. LVLMs demonstrate their grounded understanding of user-provided visual cues, giving rise to a practical and user-friendly prompting paradigm known as visual prompting (Yang et al., 2023a,b). Early work on visual prompting in vision-language models can date back to tuning-based methods (Bahng et al., 2022; Yao et al., 2024). Recent studies show that LVLMs demonstrate zero-shot understanding of user-provided visual cues (e.g., a red circle) (Shtedritski et al., 2023; Yang et al., 2023b). This observation allows prompting LVLMs by editing images directly in the pixel space, e.g., by adding visual marks or visual text (Li et al., 2023b). Starting from Set-of-Marks (SoM) prompting (Yang et al., 2023a), several training-free methods have been introduced (Lei et al., 2024; Yang et al., 2024; Wan et al., 2024). Recent work further enhances visual prompting by additional visual instruction tuning with diverse visual prompts overlaid on the images (Cai et al., 2024), or explicit visual pointer tokens in the models (Lai et al., 2023; You et al., 2023; Zhang et al., 2024b). We leverage visual prompting to avoid potential ambiguity in textual descriptions, especially when evaluating multiple object hallucinations for objects of the same class.\nObject Hallucination. Despite their promising performance on benchmarks, these models frequently generate objects that do not exist in the provided images, a problem known as object hallucination (Rohrbach et al., 2018; Dai et al., 2023b). Several methods have been suggested to mitigate the object hallucination issue, such as integrating an external object detector (Zhai et al., 2023), applying visually grounded visual instruction tuning (You et al., 2023; Zhang et al., 2024b) or reinforcement learning (Sun et al., 2023; Gunjal et al., 2024), performing iterative refinement (Zhou et al., 2024), and adapting the decoding strategies (Huang et al., 2024). To quantify progress on mitigating them, various benchmarks have been developed and have revealed the prevalence of object hallucination, even in images that are seen during instruction tuning (Zhai et al., 2023; Liu et al., 2023a). We contrast our ROPE benchmark against existing benchmarks and setups in Table 1. ROPE, which is designed for evaluating multi-object hallucination, is distinguished in several ways. First, we deliberately consider the distribution of object classes within a single image at test time. Object hallucination is observed originally in image captioning applications, where multiple objects are described (Rohrbach et al., 2018). While existing research has demonstrated that the object class distribution in the instruction tuning dataset can influence hallucination patterns (Li et al., 2023a; Zhou et al., 2024; Wang et al., 2023a), the impact of object class distribution within an image at test time remains under-explored. Second, current benchmarks concentrate on the presence of an object class or distinguish instances using textual descriptions like attributes, which can still result in ambiguity and multiple referents. We instead leverage the visual referring prompting setups and use visual cues (i.e., marked bounding boxes) to uniquely refer to objects. Finally, our evaluation is automated, without black-box neural models or human evaluators."}, {"title": "Recognition-based Object Probing Evaluation (ROPE)", "content": "We introduce the Recognition Object Probing Evaluation (ROPE), an automated protocol for assessing LVLMs in multi-object recognition."}, {"title": "Task Setup", "content": "Problem Definition. To avoid ambiguity from multiple candidate referents when using text prompts, ROPE leverages visual prompts to uniquely refer to objects. ROPE tasks LVLMs with selecting the best matching class for multiple objects, as referred to by the visual prompt, from a predefined set of object classes. Specifically, each sample in the ROPE protocol consists of a quadruple {I, L, (p1,\u2026\u2026, pn), (o1,\u2026\u2026, on)}: (1) an image I consisting of at least n objects; (2) a natural language instruction L that specifies the recognition task, including N candidate object classes C1,\u00b7\u00b7\u00b7, CN; (3) n visual prompts p1,\u00b7\u00b7\u00b7, pn, each queries an object in the image; and (4) n object classes o1,..., on as the answers. In this work, we construct a dataset with N = 50 and n = 5, i.e., models are tasked with recognizing 5 objects out of 50 candidate object classes.\nLanguage Instruction Prompts. For a fair comparison that accommodates both open-weight and API-based LVLMs, ROPE explicitly instructs models to generate a formatted output of object classes, e.g., obj1:<class1>, ..., obj5:<class5>. This format enables automated evaluation through simple parsing, avoiding black-box neural models or human evaluators With different analytical purposes, we designed 3 types of task prompts for Multi-Object queries, as illustrated in Figure 2 and described as follows.\n\u2022 Default: We probe the model to recognize the 5 objects referred to by the visual prompts concurrently in a single turn of prompting. This setting tasks the model with focusing on and recognizing all 5 objects simultaneously, aiming to capture the complexity involved when the model generates language that includes multiple objects.\n\u2022 Student-Forcing: One potential confounder in the default setting is the model's ability to generate data in the specified format. To separate out errors due to following instructions, we force the model to follow the format template and decode only the object tokens for each of the five objects. Ideally, this setting allows the model to focus solely on object recognition.\n\u2022 Teacher-Forcing: This setting eliminates cumulative error, allowing the model to condition on the correct previous context when generating object classes, leading to upper bound performance in multi-object recognition. We similarly force the model to follow the provided template and decode only the object tokens for each of the five objects, but we replace the previously generated object tokens with the ground truth. This essentially follows the few-shot in-context learning setting. Teacher forcing benefits models especially when they take shortcuts by repeating the object class list as ordered in the prompt, e.g., LLaVA-7B (Liu et al., 2023b) and Gemini 1.0 Pro (Team et al., 2023) in Figure 3.\nFor comparison, we also designed task prompts for Single-Object query. We probe the model to recognize the object referred to by the visual prompts one at a time, repeating this as 5 independent and individual prompts. Unlike Default multi-object query, the model only needs to focus on one object, which can be seen as an extension of the POPE (Li et al., 2023a) setup from yes/no polling to classification. We refer to Appendix A.1 for the prompt templates for each type of task prompt."}, {"title": "Dataset Construction", "content": "Data Sources and Curation. Since our goal is to evaluate and analyze multi-object hallucination, the image data must contain multiple objects of diverse classes with instance-level semantic annotations. We build our dataset upon existing panoptic segmentation datasets, including MSCOCO-Panoptic (Lin et al., 2014; Caesar et al., 2018) and ADE20K (Zhou et al., 2017), to ensure access to all object instances and their semantic classes. We note that one can build a dataset using the ROPE protocol with any dataset containing multiple objects and their bounding boxes, such as Visual Genome (Krishna et al., 2017). We describe the data curation pipeline in Appendix A.1.\nSplits by Query Distributions. As shown in Figure 3and 4, our initial observations indicate that LVLMs are less likely to hallucinate objects when they recognize the same object class multiple times. However, they tend to make more mistakes when all tasked object classes are different or when a new object class is introduced after multiple repeated tasks. We thus vary the distribution of object classes within each image at test time, dividing ROPE into 4 subsets: Homogeneous, Heterogeneous, and Adversarial, In-the-Wild.\n\u2022 Homogeneous: All the 5 tested objects are of the same class, e.g., AAAAA.\n\u2022 Heterogeneous: All the 5 tested objects are of different classes, e.g., ABCDE.\n\u2022 In-the-Wild: A subset with mixed object class distribution, where the 5 tested objects are randomly chosen and ordered given a test image."}, {"title": "Experiments and Results", "content": "The proposed ROPE framework, in principle, applies to all LVLMs that can follow format instructions and understand multiple visual prompts. To cover a variety of LVLMs of different scales and training data (e.g., whether grounding data and conversational data are used), we selected the following LVLMs as baselines.\n\u2022 LVLMs with base LLMs at different scales: LLaVA v1.6 (7B/13B/34B) (Liu et al., 2023b, 2024) and Yi-VL (6B/34B) (Young et al., 2024).\n\u2022 LVLMs with conversational/grounded instruction tuning: QwenVL-Base/Chat (7B) (Bai et al., 2023) and CogVLM-Base/Chat/Grounding v1.1 (19B) (Wang et al., 2023c).\n\u2022 Mechanistically grounded LVLMs: GLaMM (7B) (Rasheed et al., 2024) and GroundHOG (7B) (Zhang et al., 2024b).\n\u2022 Other LVLMs: IDEFICS-instruct (9B) (Lauren\u00e7on et al., 2023b), MiniCPM-V v2.5 (8B) (Hu et al., 2024; Yu et al., 2024b), GPT-4V (OpenAI, 2023), and GPT-4O (OpenAI, 2024).\nFor mechanistically grounded LVLMs that take visual prompts through specially designed mechanisms, such as pointer tokens in GroundHOG (Zhang et al., 2024b), we additionally experiment with their default format and report whichever yields higher performance. For other LVLMs, we overlay the visual prompts on the images using a red bounding box with a width of 2 and visual text specifying the object index, presented with a white italic font on a black background with an alpha value of 0.75 for contrast and visibility."}, {"title": "Main Results and Findings", "content": "We summarize the average results across the splits in Table 2 and present the most important findings below. The full tables appear in Appendix A.2.\nMulti-object tasks introduce more hallucinations. Our immediate observation is that LVLMs suffer from more hallucinations when tasked with focusing on and recognizing multiple objects compared to a single object. Across most of the models and test splits, we find that the average accuracy of single-object queries (i.e., probing object classes one at a time) significantly outperforms that of all three types of multi-object queries. The first exceptions are GPT-4O, MiniCPM-V, and CogVLM-2, the latter two leverage LLaMA-3 (Meta, 2024). Another exception to this is when teacher-forcing is applied to homogeneous test splits, which demonstrates an unreasonably high accuracy. We discuss them later in this section.\nHeterogeneous queries introduce more hallucinations. We find that for all models and query methods, more heterogeneous queries lead to substantially more hallucinations, with performance decreasing from homogeneous to in-the-wild to heterogeneous test sets. The impact of heterogeneity applies to even start-of-the-art LVLMs like GPT-4O (Figure 3), although this performance gap is more significant in open-weight models.\nLanguage bias and shortcuts can lead to multi-object hallucinations. In the teacher-forcing setting, where there are no cumulative errors, LLaVA models score over 90% accuracy. There are three possible hypotheses for this abnormal observation: (1) LVLMs are smart enough to learn object recognition in general through few-shot in-context learning in the teacher-forcing setting; (2) LVLMs learn to recognize one specific object through few-shot in-context learning in the teacher-forcing setting; or (3) LVLMs simply exploit language biases and rule-based shortcuts (e.g., repeating previous answers). To reach a conclusion on this, we examine an Adversarial split, in which the first four tested objects are of the same class and we probe an object of a different class for the last one (e.g., AAAAB). We compare the single-object query performance with the teacher-forcing performance on the fifth object (object B). We anticipate the following outcomes: If hypothesis (1) is correct, the teacher-forcing performance should outperform the single-object query. If hypothesis (2) is correct, the teacher-forcing performance should perform on par with the single-object query. If hypothesis (3) is correct, the teacher-forcing performance should underperform compared to the single-object query. For a controlled comparison in the multi-object setting, we also reverse the order of queries (i.e., BAAAA) and repeat the experiments.\nWe present the results of LLaVA models on the unseen split in Figure 5, with the full results available in Appendix A.2. We find that the model's predictions on class A progressively improve, scoring nearly perfectly starting from the third repetition. However, the model's performance on the last object (with the different class label B) drops to nearly zero, with almost all hallucinations labeling it as A. This is in stark contrast to 23.35% if these objects are probed individually or 19.16% when these objects are placed as the first to query in multi-object settings. Our findings suggest that hypothesis (3) is true, indicating that the LVLMs\u2019 high performance on homogeneous queries could be an illusion resulting from textual shortcuts. We observe that models show lower performance in identifying object class B in single-object analysis, potentially due to the higher salience of object class A in these images.\nMulti-object hallucinations occur in both seen and unseen images. We finally investigate whether our observations and findings hold uniformly in both seen and unseen splits. We observe that the gap between multi-object hallucination and single-object hallucination, as well as the reliance on shortcuts, persists. Although most of the models perform slightly better on seen images, the trends remain consistent across both splits. While large-scale training is involved in developing these LVLMs, it appears they might not have fully exploited the fine-grained information in the data. Training on these images does not significantly reduce object hallucinations."}, {"title": "What May Help and What May Not?", "content": "Comparing the tested LVLMs, we discuss our observations regarding design considerations that may or may not help reduce multi-object hallucinations.\nScaling the base LLM: data and parameters. We find that using base LLMs with more parameters reduces single-object hallucinations, but may not have the same effect on multi-object hallucinations. We observe a consistent increase in performance with larger LLaVA models in the seen set and in single-object queries, but not in the unseen set with multi-object queries. One possible explanation for this finding is that LLMs with more parameters are better at memorizing seen images, as the performance gap between seen and unseen images is also more significant in larger models. We also notice that the performance gap between single-object probing and multi-object probing does not apply to MiniCPM-V and CogVLM-2, which adopt a LLaMA-3 (8B) (Meta, 2024) base LLM pre-trained with 15T tokens, as they fail to follow the instruction sometimes. Compared to LLaVA models developed upon LLaMA-2 (7/13B) (Touvron et al., 2023) and Yi (34B) (Young et al., 2024) with 2T and 3T pre-training tokens, these models underperform in quantitative measures due to instruction following error but exhibit greater robustness when multiple visual prompts are presented.\nVisual instruction fine-tuning: chat and grounding. While it's surprising that conversational tuning reduces multi-object hallucinations, we observe that models without conversational tuning struggle to follow instructions and are prone to shortcuts, such as repeating the list of all object class candidates in order or consistently repeating the first candidate. This might also explain why grounded tuning in CogVLM-G is of little help in reducing multi-object hallucinations thus far. These models typically lack conversational fine-tuning, and there is currently no available grounded dialogue data at scale. While mechanistically grounded LVLMS show strong performances in single-object probing, there remain a gap in multi-object probing with student forcing. This could be attributed to a significant portion of the grounded instruction tuning dataset consisting mainly of short captions or questions featuring one single or few objects."}, {"title": "Analysis of Hallucinatory Behaviors", "content": "The task setup described above allows us to evaluate LVLMs in multi-object hallucinations and identify hallucinatory behaviors. Based on existing literature and our case studies, we further identify potential factors that correlate to and potentially explain these hallucinations."}, {"title": "Potential Hallucinatory Factors", "content": "Data-specific Factors. We consider the following factors that are specific to the tested sample (e.g., object and token positions), and are not relevant to the frequency distribution.\n\u2022 Input Order: we consider the order in which the object classes are presented in the input prompt containing all candidates.\n\u2022 Query Homogeneity: We define query homogeneity as the total number of task objects of the same class, normalized by the total number of queried objects (five in this work).\n\u2022 Object Token Position: Zhou et al. (2024) has shown that more hallucinations occur in the latter part of captions. In this work, the object indices directly correspond to the object token positions.\n\u2022 Object Homogeneity: We define object homogeneity as the number of object types in the image, calculated upon panoptic annotations."}, {"title": "Query Homogeneity.", "content": "Previous research has identified a center bias in datasets and models, indicating that objects are disproportionately located at the center of images in detection models (Szab\u00f3 and Horv\u00e1th, 2022; Taesiri et al., 2023). We define object centrality as the distance d between the object's bounding box center and the image center, normalized by the diagonal distance D from the center to the corner."}, {"title": "Salience and Frequency.", "content": "We consider the following factors that are related to the saliency or frequency of the visual object or the object class.\n\u2022 Object Salience: Previous research has shown that smaller objects are harder to detect and ground to (Hoiem et al., 2012; Ma et al., 2023). We define object salience as the ratio of the number of pixels occupied by the object's instance segmentation mask to the total number of pixels in the image.\n\u2022 Semantic Salience: We observe and hypothesize that LVLMs are less likely to hallucinate objects when they co-occur with multiple copies of the same class (\"jar\" in Figure 3). We define semantic salience as the ratio of the total number of pixels in all instances of the same class, to the total number of pixels in the image.\n\u2022 Training Salience: Previous research has shown that spurious co-occurring patterns in the training data can lead to object hallucinations (Li et al., 2023a; Zhou et al., 2024). We use the log frequency of classes in MSCOCO as a proxy for training salience following previous work, and hypothesize that LVLMs tend to hallucinate more on less frequent objects in the training set."}, {"title": "Model Behaviors.", "content": "We consider the following factors relevant to the mechanistic behaviors.\n\u2022 Object Token Entropy: Zhou et al. (2024) have shown that object hallucinations are more likely when the decoded object tokens have a higher log perplexity. In our work, we define object token entropy as the entropy of the logits of the first token in the generated word. Given s as the softmax logits of the generated word's first token, we calculate the entropy using the following formula: H(s) = \u2212 \u2211i si log(si). Simply put, higher entropy indicates greater uncertainty in the model's prediction for the first token, which can lead to more frequent object hallucinations.\n\u2022 Visual Modality Contribution: We hypothesize that LVLMs pay less attention to the visual modality during object hallucinations. Motivated by the modality importance score (Cao et al., 2020), we define Visual Modality Contribution (VMC) as the proportion of attention allocated to visual tokens compared to textual tokens. To quantify this, we analyze the attention weights of the last generated token across all heads and layers. The VMC is computed as follows: VMC = \u03a3i\u2208V aij /(\u03a3i\u2208V aij + \u03a3k\u2208T akj), where aij represents the attention weight assigned to visual token i at head j, and akj represents the attention weight assigned to textual token k at head j. The sets V and T denote the visual and textual tokens, respectively. By examining the VMC, we can determine how much attention is given to visual inputs in comparison to textual inputs. A lower VMC may indicate a higher likelihood of object hallucinations due to insufficient attention to visual cues."}, {"title": "When Do LVLMs Experience Multi-Object Hallucinations?", "content": "In Figure 6, we compare the distribution of these factors between hallucinatory and non-hallucinatory objects in the student forcing setting on the unseen split using LLaVA-13B. For continuous values, we use ridgeline plots, and for discrete values with fewer bins, we use bar charts.\nData/Task-specific Factors. We observed that specific data factors, such as query and object homogeneity, significantly influence model performance, with increased hallucination occurring when models process images featuring multiple object classes or a variety of objects. For positional factors, the position of object tokens seems to have minimal impact and the object centrality has only a slight influence as LVLMs tend to hallucinate objects more frequently when they are positioned away from the center. This tendency may stem from a reporting bias, as objects mentioned in captions are typically foreground objects that distribute toward the centers of images.\nSalience and Frequency. We note that semantic salience significantly affects the model's performance, as it is more prone to hallucinate an object class that is less salient within the image. Conversely, the salience of individual objects does not statistically correlate with hallucination incidents. This implies that LVLMs may rely more on the presence of co-occurring objects of the same class to predict the labels of queried objects, rather than solely on the presence or salience of the objects themselves. Additionally, training salience plays a crucial role as models are less likely to hallucinate object classes that frequently appear in training."}, {"title": "How Do LVLMs Experience Multi-Object Hallucinations?", "content": "In Figure 7, we conducted a detailed comparison of the distribution of actual versus predicted object classes within the context of hallucinatory objects, examining factors such as semantic salience, training salience, and input order. Although semantic salience is a key factor in determining whether a model hallucinates, it appears to have minimal impact on the prediction of hallucinated objects. Our analysis also shows that models are more likely to hallucinate object classes that are prevalent in the training data, but the reverse is not necessarily true. Additionally, there is a notable preference for models to hallucinate objects that are listed early in the input prompt as candidate classes. Overall, our findings indicate that spurious correlations may lead to hallucinations involving multiple objects."}, {"title": "Discussions and Conclusion", "content": "Hallucinations in large vision-language models (LVLMs) can occur at different scales and granularities. In this study, we study the problem of multi-object hallucination, examining how LVLMS may misperceive when tasked to focus on multiple objects concurrently, and which factors cause the hallucinations. we introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol designed to account for the distribution of object classes within a single image during testing and to use visual referring prompts to reduce ambiguity. Our research provides key insights for the development and application of LVLMs. Since models tend to experience more hallucinations with multiple objects than with single ones, it may be advantageous to probe objects individually in visual prompts to enhance performance. The likelihood of a model's hallucinatory output is linked to various data factors and model behaviors. Particularly in situations involving heterogeneous data and low certainty from the model, there is an increased risk of hallucinations, and users should be vigilant. Moreover, our analysis indicates that merely adopting (grounded) instruction tuning and scaling the base language model may not be enough to fully address the issue of object hallucination. There is a need for more balanced object distributions, annotations of objects away from image centers, and an increase in diversity. Introducing instructions that require multiple visual pointers and complex multi-object reasoning is also crucial."}, {"title": "Acknowledgement", "content": "This work was supported in part by NSF IIS-1949634, NSF SES-2128623, and the DARPA Machine Common Sense Program. Our experiments have also benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program and the model access from the Amazon AGI team. The authors would like to thank Yichi Zhang for his valuable feedback."}, {"title": "Limitations", "content": "ROPE represents one of the pioneering efforts to publicly address the issue of multiple object hallucination. However, we acknowledge several limitations in our work: (1) The lack of transparency in the LVLMs makes it difficult to guarantee that our unseen dataset has not been previously exposed. (2) Our evaluation benchmark uses a fixed set of semantic objects, which may introduce bias and impose unnecessary constraints on the LVLMs' ability to follow instructions and reason effectively. (3) The evaluation process can be slow, as it involves performing five inferences per image for both student forcing and teacher forcing."}, {"title": "Ethical concerns and risks", "content": "This study does not require human annotators or participants for its interactive experiments. Instead, it utilizes publicly available datasets and content created by models for evaluation purposes. We are aware that these public data might introduce biases and sensitive elements, and it is essential for future research to address these concerns, possibly by creating datasets that incorporate fairness-based filtering and metrics."}, {"title": "Reproducibility", "content": "Our data curation pipeline involves several essential steps designed to prepare and refine our dataset for evaluating multi-object hallucination. The pipeline begins by filtering images and candidate objects to query. We consider valid objects to be those belonging to the top 50 \"thing\u201d classes and exclude objects with a bounding box area less than 1% of the total image area. We discard images containing fewer than 5 valid objects, and allow an intersection-over-union between bounding boxes of no more than 0.1, which preserves data integrity while ensuring high image quality. We apply this pipeline to MSCOCO-Panoptic (Lin et al., 2014; Caesar et al., 2018) and ADE20K (Zhou et al., 2017).2"}, {"title": "Language Instruction Prompt Templates.", "content": "We illustrate the 4 types of task prompts for Single-Object and Multi-Object queries in Figure 2", "Forcing": "nSelect one and the most appropriate class for each object located within red bounding boxes from the following list: [CLASS NAMES", "format": "obj1: <class1>, obj2: <class2>, obj3: <class3>, obj4: <class4>, obj5: <class5>", "Probing": "n(GroundHOG) Describe object 1 <PTR>"}]}