{"title": "\"ALL THAT GLITTERS\": APPROACHES TO EVALUATIONS WITH UNRELIABLE MODEL AND HUMAN ANNOTATIONS", "authors": ["Michael Hardy"], "abstract": "\"Gold\" and \"ground truth\" human-mediated labels have error. The effects of this error can escape\ncommonly reported metrics of label quality or obscure questions of accuracy, bias, fairness, and\nusefulness during model evaluation. This study demonstrates methods for answering such questions\neven in the context of very low reliabilities from expert humans. We analyze human labels, GPT model\nratings, and transformer encoder model annotations describing the quality of classroom teaching, an\nimportant, expensive, and currently only human task. We answer the question of whether such a task\ncan be automated using two Large Language Model (LLM) architecture families-encoders and GPT\ndecoders, using novel approaches to evaluating label quality across six dimensions: Concordance,\nConfidence, Validity, Bias, Fairness, and Helpfulness. First, we demonstrate that using standard\nmetrics in the presence of poor labels can mask both label and model quality: the encoder family of\nmodels achieve state-of-the-art, even \"super-human\", results across all classroom annotation tasks.\nBut not all these positive results remain after using more rigorous evaluation measures which reveal\nspurious correlations and nonrandom racial biases across models and humans. This study then expands\nthese methods to estimate how model use would change to human label quality if models were used\nin a human-in-the-loop context, finding that the variance captured in GPT model labels would worsen\nreliabilities for humans influenced by these models. We identify areas where some LLMs, within\nthe generalizability of the current data, could improve the quality of human ratings of classroom\ninstruction.", "sections": [{"title": "1 Introduction", "content": "Human mediated labels always have an unknown amount of error. In machine learning practice, this error is often\nquantified using inter-rater reliability metrics and correlations. However, this annotation uncertainty is often ignored\nduring standard supervised learning and model evaluation, leading to poorer models (Belz et al., 2023). Thus, imperfect\nlabels are treated as \"gold\" or \"ground truth\" (Belz et al., 2020; Hosking et al., 2024). This may be due in part to\nmeasures of accuracy being the most preferred methods of assessing and benchmarking model performance (Birhane\net al., 2022; Ribeiro et al., 2020; Kiela et al., 2021), but common practice might also arise from not using tools expressive\nenough to interpret labels in low reliability. To that end, this work demonstrates methods for working with low/unknown\nreliability annotations, often found in tasks requiring complex expert judgment.\nThe field of education has many complex tasks that often yield low reliabilities in labels (Jurenka et al., 2024; Kane and\nStaiger, 2012) which make edtech NLP models and research particularly vulnerable to the effects of inexpert annotations\nBelz et al. (2020); van der Lee et al. (2019); Zhou et al. (2023). The case study used to illustrate more expressive"}, {"title": "1.1 Study Task: Annotating Teaching Quality", "content": "The classification task of rating teaching may seem deceptively simple: using a rubric, provide a rating for the quality of\ninstruction of an elementary school math classroom. Such ratings are given to all US K12 public education teachers for\nboth formative educator development feedback and as high-stakes teacher evaluations. Despite their ubiquity, these\nratings, even when conducted by experts, are unreliable (Ho and Kane, 2013; Kane et al., 2015; Kane and Staiger, 2012;\nGlaese et al., 2022; Whitehill and LoCasale-Crouch, 2024), similar to the poor reliability of other K12 education labels\n(Jurenka et al., 2024; Tack et al., 2023) that have limited the rigor of education research (Slavin, 2002; Klahr, 2013;\nJurenka et al., 2024). Studies about ratings of instruction are also extremely expensive to conduct relative to other\nannotation tasks (Grissom et al., 2013; Liu and Cohen, 2021; Jurenka et al., 2024), with only two major studies across\nhundreds of public school teachers that use authentic instructional metrics to support development: the MET study\n(Kane et al., 2013; Kane and Staiger, 2012) and the NCTE Main Study (Kane et al., 2015), the latter of which is the\nsource of data for this study."}, {"title": "2 Related Work", "content": "2.1 Annotation Quality and Bias\nBetter understanding human label behaviors is key to training and evaluating models (Webson et al., 2023; Webson and\nPavlick, 2022; Gordon et al., 2022). Accuracy, based on \"gold\" or \"ground truth\" labels, is the primary and most valued\nperformance metric by which LLMs are evaluated (Birhane et al., 2022; Ribeiro et al., 2020; Kiela et al., 2021). For\nexpediency of development, data scientists often choose to assume data labels are reliable, accurate, and end-task aligned\nfor intended real-world use cases, (Hosking et al., 2024; Bejar et al., 2006; Messick, 1998), even in scenarios where\nthese assumptions could be detrimental (e.g., performing complex high-stakes tasks, reducing discriminatory biases\nfound in data (Field et al., 2021) that are immutably historical by definition of their creation, etc.), which is especially\ntrue of autoregressive models, whose labels are Internet text and which contain harmful biases (Hofmann et al., 2024a,b).\nAssessing the accuracy and reliability of idiosyncratically human annotated \"ground truth\" can be difficult (Eckes and\nJin; Wind and Guo, 2019; Wind, 2019; Abercrombie et al., 2023; Baan et al., 2024, 2022; Waseem, 2016; Kazai et al.,\n2013; Hosseiny Marani et al., 2022; Tack et al., 2023; Hosking et al., 2024), a challenge that is exacerbated when label\nuncertainty is underexamined or underreported. Limited transparency around label quality makes it more challenging to\nmeasure biases, interpret model findings, assess individual fairness, and establish real-world validity (Hill et al., 2012b;\nJurenka et al., 2024).\nPowerful and provocative research has begun to address the limitations of accuracy-only evaluations and propose more\nfair and responsible solutions under assumptions of uncertainty (Hardt et al., 2016; Dwork et al., 2012; Kasy and Abebe,\n2021; Song et al., 2020; Zhao and Ermon, 2021; Corbett-Davies et al., 2023; Pleiss et al., 2017; Zemel et al., 2013),\nincluding techniques for addressing when labels lead to undesirable model behaviors (Ding et al., 2022; Hebert-Johnson\net al., 2018; Qi et al., 2023). This paper offers several ways to quantify these issues and improve interpretability and\nexplainability (Adebayo et al., 2020; Lundberg and Lee, 2017; Rudin, 2019; Kim et al., 2018)."}, {"title": "2.2 Teacher Development and Evaluation", "content": "School leaders working with teachers to improve the quality of instruction typically evaluate the teacher's proficiency in\na range of competencies (typically measured during in-class observation and evaluation on a teaching rubric; Aguilar\n(2013); Bambrick-Santoyo (2016, 2018)), then determine which competencies are most important to improve first (i.e.,\nwhich change will have the biggest impact on student learning), and then provide supportive feedback and coaching.\nThis paper focuses on the first step of evaluating teacher proficiency, which is often time-consuming and produces\nratings (labels) that are unreliable (Kane and Staiger, 2012; Blazar, 2018; Kane et al., 2013; Casabianca et al., 2013).\nWithout accurate classifications, it is challenging for practitioners to prioritize instructional needs and aligned practices\nfrom among the many elements of good teaching (Saphier et al., 2008; Darling-Hammond, 2014; Hammond, 2015;\nLemov and Atkins, 2015; Lemov, 2021; Liljedahl et al., 2021; Darling-Hammond et al., 2020; Schwartz et al., 2016) and\nfor researchers to empirically quantify the impact of good teaching practices Pianta and Hamre (2009); Charalambous\nand Delaney (2019); Blazar and Pollard (2022); Jurenka et al. (2024)."}, {"title": "3 Data", "content": "The data used in this study and in Wang and Demszky (2023) are from the National Center for Teacher Effectiveness\n(NCTE) Main Study (Kane et al., 2015), which contains three years of data collection and observations of math\ninstruction in approximately fifty schools and three-hundred (4th and 5th grade) mathematics classrooms across four\nschool districts in the United States, including expert human ratings of individual video-captured classroom lessons\nacross two observation instruments (Bacher-Hicks et al., 2017, 2019): the CLASS framework (12 items) (Pianta et al.,\n2008) for general instructional practice and the content-specific Mathematical Quality of Instruction (MQI; 13 items)\n(Hill et al., 2008), together yielding over 400,000 distinct human rating labels assigned, the distributions of which are in\nFigure 6. Each instrument item is intended to measure a different aspect of teaching quality.\nLike all human mediated labels, an individual classroom observation rating requires at a minimum three facets: (1) a\ntask with rating criteria (Section 3.1), (2) raters/labelers (Section 3.2), and (3) observations to be classified (sections of\ntranscripts of classroom discourse, Section 3.3). As tasks increase in complexity, three facets contribute more error to\nestimates. This dataset has the additional real-world challenges of having very long and noisy transcripts and having\nlarge imbalances (Figure 4 panel (a), Figure 6) in human labels that have hindered previous research (Xu et al., 2024;\nWang and Demszky, 2023), but which provide extra opportunity to demonstrate the importance of robust methods of\nevaluation."}, {"title": "3.1 Rating Criteria: MQI Rubric", "content": "Just as all raters contribute uncertainty to a system, so too do the measurement instruments. Ambiguity uncertainty is\nintroduced when an instrument, instruction, or criteria for a task has language that could lead to two equally-expert\nraters to different results, ceteris paribus. The 13 MQI items within the dataset have at least two raters per classroom\nobservation. While both humans and Encoders evaluated all items, the this paper will focus on the 4 of the 13 MQI items\nevaluated in Wang and Demszky (2023) to support comparability across humans and models. These four ternary items\nare teacher explanations ( EXPL), remediation of student errors (REMED), student questioning and reasoning (SMQR), and\nimprecision in mathematical language (LANGIMP). Analyses for all other items are in the appendices. Prior studies\nhave explored the reliability of MQI instrument ratings generally (Kane and Staiger, 2012; Mantzicopoulos et al., 2018;"}, {"title": "3.2 Human Expert Raters", "content": "Human rater information for both the MQI and CLASS instruments can be found in the Appendix of the DSO Study-Level\nFiles from the NCTE Main study. MQI raters in particular were recruited from a separate pool of applicants based on\ntheir background in mathematics and through contacting colleagues in mathematics departments (Hill et al., 2012a;\nBlazar et al., 2017) and then passed certification exams to score the MQI, and attended biweekly calibration meetings to\nensure standardization of scoring procedures."}, {"title": "3.3 Classroom Observations", "content": "63 human raters watched videos and provided ratings at regular intervals across all items in the MQI. Transcripts of these\nsame videos (Demszky and Hill, 2022) are used by LLMs for the same task, where the class discourse is equipartitioned\nacross utterances (GPT family models) or words (Encoder family models) by the total number of classroom segments to\nalign the text to the human labels in the absence of timestamps. Data from the NCTE Main study (Kane et al., 2015) 5\nand for the associated transcripts (Demszky and Hill, 2023)6 are available online."}, {"title": "4 Model Families and Model Rater Data", "content": "GPT Models The GPT model family from Wang and Demszky (2023)7 have 7,660 ratings for 223 different teachers.\nThe family consists of three models differing in prompt engineering methods (herein called N, NR, and ND), and brief\nsummary of those differences is in Table 8. GPT models were evaluated on curated selections of classroom text with\nthe least transcriptorial noise (i.e., minimizing instances of [inaudible]), and were edited to indicate whether the\nspeakers were teachers or students.\nEncoder Models Encoder family models are custom transformer encoders trained on the NCTE classroom transcripts.\nThe five models (un1, un2, un3, gte, and e5) use fixed-parameter pretrained sentence embeddings, differing in these\nand in training hyperparamters, thereby exploiting LLM sensitivites to pretraining regimes (D'Amour et al., 2020;\nMcCoy et al., 2023). A summary of differences is in Table 7 and more training details can be found in Appendix D. In\ncontrast to the model experiments of Xu et al. who used different combinations of models by item, each encoder model\nproduces labels for all 13 MQI (and 12 CLASS) items. In contrast to the GPT models, the only text preprocessing\nused with the Encoders simply replaced all transcription notes with [inaudible] to mimic the uncertainty in live\naudio transcription, and no edits to indicate speakership were included. For the Encoder models, all model outputs in\nthis study were conducted with a lesson-level-stratified held-out test set (see Figure 8) that was not used during model\ndevelopment. Encoder models were trained a single GPU in Google Colab with training detailed in Appendix D.3."}, {"title": "5 Evaluation Methods", "content": "Typical reliability metrics (see Section 5.1) provide a backdrop of descriptives that can flag issues of low quality labels.\nMeasures of statistical dependability can be used for generalizing label conclusions and identifying spurious correlations\n(see Section 5.3), a part of improving accuracy. Methods for disentangling human and model label biases (see Section\n5.4) are first demonstrated and then extended to estimate fairness across racial lines in Section 5.5. Usefulness, as\nmeasured by the amount of rating reliability improvement a model can provide to a human rater in human-in-the-loop\ncontexts, including associated cost savings in human time (for encoder models) are in Section 5.6."}, {"title": "5.1 Concordance: Agreement and Reliability Metrics", "content": "RQ 1: How do automated models perform relative to humans in the presence of low label reliability? RQ 1: Case\nStudy Reframing: How well do automated models perform relative to humans when evaluating instruction?"}, {"title": "5.1.1 Baseline Human Metrics", "content": ""}, {"title": "5.1.2 Commonly Used Metrics", "content": "The results also include three additional correlation and reliability metrics:\nQuadratic Weighted Kappa (QWK) typically used in ordinal classification\ntasks to penalize distance quadratically (squared error) while accounting\nfor categorical agreement by chance (e.g., Shermis (2014); Hardy (2021);\nWang and Demszky (2023)), Pearson correlation r, (e.g., Whitehill and\nLoCasale-Crouch (2024)) Spearman correlation \u03c1 (e.g., Wang and Demszky\n(2023); Xu et al. (2024)), and Kendall correlation \u03c4 (e.g., Liu et al. (2023b)).\nFigure 2 shows Spearman correlations (\u03c1) and confidence intervals for all\nmodel families and for models from Xu et al. (2024). The table in Figure\n2 contains the \u03c1 estimates."}, {"title": "RQ1: Concordance", "content": "Metrics\nCorrelation: \u03c4 r, \u03c1, \nInter-rater Agreement:\n% Agree, % Agree \u00b1 1, Co-\nhen's K, QWK\nIntuition: QWK\nQWK is the extent to\nraters agree on ratings, not\nby chance. Bigger dif-\nferences in ratings show\nless agreement, scaled\nquadratically."}, {"title": "5.1.3 Results", "content": "Using nearly any standardized combination of metrics across all items from Section 5.1, Encoder models perform better\nthan the single highest performing expert human rater. The human labels assigned for the four focus MQI have very\nlow reliabilities, despite the significant training and calibration for human raters described in 3.2. Overall, the human\nlabels are highly unreliable, but if a researcher were trying to compare the model to human performance, they could be\ndisplayed as they are in Table 1. For metrics of agreement and reliability, each encoder model outperformed humans on\naverage, whilst each GPT model underperformed humans on every metric and every item. Table 1 has a summary of\nthe full panel of lesson segment-level inter-rater reliability metrics for each MQI item. Specific metrics for the four\nfocus MQI items in this study are in Panel (b) in Figure 4, and the full individual model-item comparisons for all MQI\nitems and metrics in this section are in Table 9. Additionally, the detailed full results for all models and metrics, MQI,\nand CLASS rubrics can be found in the supplementary materials online.\nUsing only these metrics and without further testing, one might assume that the encoder models are therefore ready to\nhelp with the task of automated annotations of teaching quality or that GPT models show improvement to ICC measures\nand could be helpful. Implications: Basic statistics in the presence of unreliable labels can mislead interpretations of\nmodel performance. Researchers should be wary of studies reporting few metrics in the presence of low reliabilities."}, {"title": "5.2 Confidence: Generalizable Reliability", "content": "RQ 2: How generalizable are findings from unreliable labels? RQ 2\nCase Study Reframing: To what extent would the ratings of a teacher's\ninstructional quality persist across lessons or contexts?"}, {"title": "RQ2: Confidence", "content": "Metrics\nGeneralizability: Ep2\nDependability: \u03a6\nIntuition: Ep2 and \u03a6\nBy accounting for the\ndifferent facets of varia-\ntion, we can estimate how\nmuch of the relative (Ep2)\nand absolute (4) label\nvariation associated with\nthe teacher is attributable\nto the teacher only."}, {"title": "5.2.1 Generalizability and Dependability", "content": "Generalizability Study (g-study) (Brennan, 2001a, 2013, 2001b; Hill et al.,\n2012b) designs utilize random effect estimates across possible configura-\ntions of different sources of variance to quantify how generalizable labels.\nThis is done by estimating the extent to which given labels would persist if\nsources of variation changed (e.g., same teacher, different day; same lesson,\ndifferent rater; human rater vs model rater; etc.). Ep\u00b2 is a measure of the\nrelative generalizability of a rating (i.e., is rating order preserved), and D,\naccounting for absolute error, is a measure of label dependability: how\nlikely specific ratings would be numerically the same with different sources\nof variation. These two reliability-like estimates can help quantify how\n\"golden\" labels are.\nThe multifaceted g-study design used to estimate the how much variation\n(v) in individual teachers' instructional quality, i, contributed to a rating\nlabel, X, annotated for a section of a lesson, s, during an observation, o, on\nrubric item j by rater r is known as a Item-by-Rater-by-Segment-within-\nObservation-within-Individual Teacher design: J\u00d7R\u00d7 (S : 0 : I).\nOverall estimates across all MQI items for a given rater family, F, are in Table 2. For item-level reliabilities, we simplify\nthe expression by holding the item fixed, resulting in a R \u00d7 (S : O : I) design. Using nested random effects notation,"}, {"title": "5.2.2 Results", "content": "Humans, on average, produce labels that are both more reliable and generalizable. The full results for human rater\nlabels, decomposed into variance components, can be found in H.311 and estimates for Ep\u00b2 and I can also be found in\npanel (c) of Figure 4. The Encoder models outperform humans on nearly every item in terms of inter-rater reliability\nmetrics (Table 1), but not in generalizable reliability metrics as seen in panel (c) tables of Figure 4. Importantly, the\nlarge difference between Ep2 and 4 for Humans and Encoders is due to properties of individual items, which accounted\nfor over 75% of the variation in those families. GPT models, on the other hand, did not change ratings very much on\ndifferent items, consistent with literature on these models not understanding such prompts (Liu et al., 2023a; Webson\nand Pavlick, 2022; Heo et al., 2024).\nTable 2 shows that Encoder model still performs better than humans on the majority of items, but it is no longer as clear.\nInterestingly, as mentioned in Section 4, the encoder models did not receive any annotations outside of the transcript,\nincluding speaker. This means that the model would struggle to identify teacher explanations (EXPL) from student\nexplanations (STEXPL). This shift in interpreting encoder family performance from superhuman to zero reliability adds\nvalidity to the argument that these metrics provide valuable insight, showing that the relationships found in some of the\nvariables could be explained by variance unrelated to the label construct. Implications: Measures of generalizability\nand dependability derived from structured variance decomposition can meaningfully quantify label quality."}, {"title": "5.3 Validity: Convergent and Spurious Correlations", "content": "RQ 3: To what extent can accuracy and validity be estimated with unreliable labels? RQ 3Case Study Reframing:\nTo what extent do models and humans rate the same underlying construct similarly?"}, {"title": "5.3.1 Disattenuating High Noise Correlations", "content": "Dependability and generalizability do not guarantee accuracy, but even at these very low levels, they can be used\nin indirect tests of convergent validity to see whether correlations between humans and models are low because of\nmeasurement error, such as poor rubric item construction, or because the two sets are really uncorrelated. If an individual\nteacher's latent instructional ability \u03b8; is about the same from lesson to lesson with the same students, we can correlate\nfor human (h) and model (m) family ratings for different lessons coming from the same teacher and correct for\nmeasurement error by disattenuating the correlations by each rater family's F label generalizability, E), for a given"}, {"title": "RQ3: Validity", "content": "Metric\nDisattenuated Conver-\ngent Correlation:\nIntuition: m\nTeaching abilities on item\nj do not change dramati-\ncally each lesson, so if hu-\nman rh and model rm ob-\nservers rate teacher i sim-\nilarly on different lessons\n(LandL), they are re-\nsponding to similar ob-\nservable indicators of the\nteacher."}, {"title": "Disattenuating High Noise Correlations", "content": "item j. The disattenuated correlation, \\(  \\), between humans and a family of models for item, j, can be estimated:\n\\(\begin{aligned}\n&\\operatorname{Corr}[X(i, L, j, r_h), X_m(i, \neg L, j, r_m)]\\sqrt{E_{p_{h}}^{(j)} E_{p_{m}}^{(j)}}\n\\end{aligned}\\) (4)\nwhere X is score retrieval function for individual teacher i on item j by\na random member r of rater family F in relation to some observed lesson\nL with family label generalizability, \\(E_{p_{F}}^{(j)}\\) defined in Equation 2. In other\nwords, the numerator (represented in red in Figure 3) is the correlation in\nscores whenever two different lessons from the same teacher were scored\nby raters from different families (human and model). The denominator then\nadjusts for based on the reliabilities of raters from each family to account\nfor the known tendency of low reliability to diminish observed correlations."}, {"title": "5.3.2 Results", "content": "Disattenuation analyses and Section 5.2 suggest that the Encoder model family's SOTA-level correlations on the EXPL\nand STEXPL item may have been spurious (likely identifying speech patterns associated with higher teacher performance,\nand not necessarily specific to explanations), a direct result of low generalizabilities found in Section 5.2. Additionally,\nwe see see very large confidence intervals for the encoders for items where item score distributions are most imbalanced\n(MGEN, MAJERR), suggesting that correlations found are not justified in the presence of low reliabilities. Items where\nthe disattenuated correlations are lower (e.g., LCP, MMETH) suggests that models and humans interpreted observational\nfeatures differently. Implications: when measurement error is high, disattenuating model and human correlations can\nhelp identify whether items with high or similar correlations have spuriousness or are responding to similar features.\nThis method only minimally provides evidence for investigating accuracy and validity, but, for the Encoder models,\nevidence can be built upon by comparing how the more continuous ratings of the models and humans change and\ncorrelate over the course of a given observation. While not explicitly part of this study, an example of how Encoders'\nand humans' ratings change from the start to the end of a class for a randomly chosen lesson observation is illustrated in\nFigure 15. Investigating the validity of a construct would require more robust qualitative review of the content."}, {"title": "5.4 Bias: Disentangling Individual Rater Behaviors", "content": "RQ 4: Can bias contributed by individual rater behaviors be identified and disentangled from labels? RQ 4:\nCase Study Reframe: How do individual rater effects contribute to ratings bias?"}, {"title": "5.4.1 Hierarchical Rater Models", "content": "Rater biases in complex tasks are usually not directly measur-\nable, but we can estimate latent constructs that quantify the\neffects of individual raters' behaviors using methods commonly\nused to estimate latent attributes of rubric items (e.g., item diffi-\nculty) and latent attributes individuals (e.g., ability) throughout\nItem Response Theory (IRT). If the data had no variation due\nto raters, various polytomous IRT methods could help estimate\n\"true scores\"/\"gold\" labels (\u00a7\u00a1\u00a1) during classroom observations,\nteacher instructional abilities (\\(\u03b8_i\\)), and the various individual\nitem effects. For tasks with human-mediated labels, human\nraters introduce additional sources of measurement error for\neach classification and the data may include multiple measures\nfrom multiple raters for a single observation (leading to an ac-\ncumulation of information at overlap observation points). To\naddress this, hierarchical rater modeling (HRM) (Patz et al.,\n2002; Decarlo, 2003; DeCarlo et al., 2011) combines an IRT\nmodel with a first stage estimation defined by a signal detection\ntheory (SDT) relationship. The latter asks the question, \"given\nthe presence of the 'true' score, can a rater detect it?\" as the\nformer asks, \"given the inputs, can we estimate the 'true' score\naccounting for differences in the tasks used to measure it?\".\nThe hierarchical structure addresses the problem of accumu-\nlation of information in the estimates. HRMs consist of three\ncomponents:"}, {"title": "RQ4: Annotation Bias", "content": "Method\nHierarchical Rater Model:\nThree layers of estimation,\nparameters solved simulta-\nneously (MCMC).\nTop Stage Intuition\nThe latent teacher abilities\nare assumed to be normally\ndistributed.\nIRT Stage Intuition\nEq. 6 estimates the probabil-\nity of a teacher i receiving\nan ideal rating \u03bei given the\nteacher's ability ; and item\ncharacteristics (\u03b1, \u03b3).\nSDT Stage Intuition\nEq. 7 estimates the probabil-\nity that a rater gave teacher\ni a rating X\u2081 given the ideal\nrating and rater tenden-\ncies (bias 4 and variability\n)."}, {"title": "Hierarchical Rater Models", "content": "where an IRT model estimates the \"gold\" label score \\(s_{oij}\\) for a\ngiven item for some time segment s in teacher i's o-th observed\nlesson for item j, which arises from i's M-dimensionally dis-\ntributed latent instructional ability/needs (\\(\u03b8_i\\)), and a Signal\nDetection Theory (SDT) model component disentangles indi-\nvidual rater biases from each recorded score, \\(X_{soijr}\\), by quanti-\nfying the latent attributes that mediate whether rater r correctly\ndetects the true score, i.e., \\(P_{\u20ackr} = P[X_{soijr} = k |\u00a7_{oij} = 5 ]\\)."}]}