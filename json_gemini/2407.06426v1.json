{"title": "DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations", "authors": ["Luke Yoffe", "Alfonso Amayuelas", "William Yang Wang"], "abstract": "To enhance Large Language Model (LLM) capabilities, multi-agent debates have been introduced, where multiple LLMs discuss solutions to a problem over several rounds of debate. However, LLMs often produce incorrect responses that appear deceptively confident, which can mislead other agents. This is partly because agents do not express their confidence levels during standard debates. To address this, we introduce DebUnc, a multi-agent debate framework that uses uncertainty metrics to assess agent confidence levels. We adapted the LLM attention mechanism to adjust token weights based on confidence levels and also explored using textual prompts to convey confidence. Our evaluations across various benchmarks show that attention-based methods are particularly effective, and that as uncertainty metrics evolve, performance will continue to increase. The code is available at https://github.com/lukeyoffe/debunc.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown impressive performance in various fields, including law, academia, and coding (OpenAI, 2024). \u03a4\u03bf handle more complex tasks, LLM-powered agents have been developed. These agents observe their environment and take actions, such as communicating with other agents, using tools, or performing reasoning (Wu et al., 2023). The potential of LLM agents increases when multiple agents collaborate, which can take different forms.\nOne approach involves agents assuming different roles, such as designer, programmer, tester, or manager, within a software development team of LLM agents (Qian et al., 2024). Alternatively, agents may all assume the same role, collaborating to solve problems through multi-agent debates. This can help address LLM's tendency to generate incorrect responses, known as hallucinations (Du et al., 2023), and is the focus of this paper."}, {"title": "2 Related Work", "content": "Large Language Models (LLMs) are characterized by their overconfidence and propensity to always provide a response to user queries. This often results in the generation of inaccurate content, a phenomenon known as hallucination, where the models produce information that is not supported by their training data or the given input (Liang et al., 2024; Yadkori et al., 2024; Duan et al., 2024; Yao et al., 2023; Aichberger et al., 2024). Addressing hallucinations is highly important for developing trust in LLM-based systems and facilitating their deployment in more real-world applications. As a result, there has been a significant increase in research aimed at understanding the mechanisms behind hallucinations and developing strategies to mitigate them (Ji et al., 2023; McDonald et al., 2024; Liu et al., 2023)."}, {"title": "2.1 Uncertainty in LLMs", "content": "Some current research efforts to mitigate hallucinations focus on measuring the model's uncertainty and enhancing their self-awareness (Kadavath et al., 2022; Amayuelas et al., 2023; Yin et al., 2023). By accurately evaluating their uncertainty, end-users can make more informed decisions about when to trust the model's generated output (Lin et al., 2022a; Xu et al., 2024). However, accurately measuring model uncertainty remains an unsolved problem, and ongoing research is exploring new uncertainty metrics. We identify three primary approaches to this problem:\nToken Probability-Based Uncertainty Metrics evaluate uncertainty based on the probabilities assigned to each token generated by the model. High token probabilities (close to 1) indicate strong model confidence, whereas lower token probabilities suggest a spread of probability across several tokens, signifying uncertainty about the correct choice. Prominent methods in this category include Mean Token Entropy, Perplexity (Fomicheva et al., 2020), SAR (Duan et al., 2023), RDE (Vazhentsev et al., 2023), and Claim-Conditioned Probability (Fadeeva et al., 2024).\nLLM-Generated Uncertainty Metrics involve the model explicitly expressing its uncertainty in its response. This can be achieved in multiple ways, including by training the model. This was the approach taken by Lin et al. (2022a), who fine-tuned GPT-3 (Brown et al., 2020) to provide its answer to a question along with its confidence level.\nAlternatively, the model can be prompted to express its uncertainty without prior training. Tian et al. (2023) found that this approach can can outperform token-probability-based methods when applied to LLMs fine-tuned with reinforcement learning from human feedback (Christiano et al., 2017). However, Xiong et al. (2023) report slightly inferior performance compared to token-probability-based methods on GPT-3.\nSampling-Based Uncertainty Metrics assess uncertainty by generating multiple outputs through sampling, analyzing the distribution of meanings across the outputs. A consistent meaning across multiple samples suggests high confidence, while variations indicate lower confidence. This approach can identify different sequences that convey the same meaning, which token-probability-based metrics do not account for. However, the need for multiple generations makes these methods more resource-intensive than the others described. Examples include Semantic Entropy (Kuhn et al., 2023), LUQ (Zhang et al., 2024), and other metrics that evaluate meaning diversity (Lin et al., 2023)."}, {"title": "2.2 Multi-Agent Debate", "content": "With the increased accessibility of LLMs and improvements in their inference times, LLMs are being integrated into more complex systems as autonomous agents (Wu et al., 2023; Li et al., 2023; Hong et al., 2023). A critical component of these agent-based systems is the collaboration mechanism, where models engage in debate with one another. These mechanisms are currently being studied (Zhang et al., 2023) and have been shown to foster more divergent thinking (Liang et al., 2023), enhance reasoning and factual accuracy (Du et al., 2023), and lead to more reliable evaluations (Chan et al., 2023). Through discussions, the LLMs can refine their outputs, ultimately achieving higher levels of agreement and producing more factually accurate text (Sun et al., 2024; Feng et al., 2024).\nReConcile (Chen et al., 2023) explored the integration of agent confidence in multi-agent debates, relying on LLM agents to self-report their confidence, which was communicated to other agents through prompts. This led to marginal performance improvements due to the confidence estimation. Building on their approach, we employ uncertainty metrics to estimate agent confidence and explore both prompting and attention scaling to convey this confidence. We found these enhancements to significantly improve multi-agent debate performance."}, {"title": "3 Method", "content": "In human debates, it is often possible to gauge someone's expertise on a subject by observing the fluency of their responses, their body language, and other cues. This helps in identifying whose arguments to consider more seriously when there are conflicting opinions. On the other hand, in multi-agent LLM debates, agents frequently generate inaccurate responses that sound confident, which can mislead other agents and result in a consensus on an incorrect response (Du et al., 2023). Our goal is to advise agents on which other agents' opinions to prioritize based on their confidence levels.\nOur modified debate pipeline, depicted in Figure 2, operates as follows: in each round of debate, every agent generates a response, and its uncertainty is estimated. In the next round, the responses and uncertainties from each agent are shared with every other agent. We test three uncertainty metrics and three approaches to communicate agent uncertainty."}, {"title": "3.1 Uncertainty Metrics", "content": "Uncertainty metrics gauge an LLM's confidence in its responses; high uncertainty signifies low confidence and potential unreliability, while low uncertainty indicates high confidence and greater reliability. These metrics typicaly fall into one or more of three categories: Token Probability-Based, LLM-Generated, and Sampling-Based methods. For more information about these categories, refer to Uncertainty in LLMs\nIn our experiments, we focus on token probability-based metrics due to their computational efficiency and flexibility. These metrics require only a single generation and are effective regardless of the model's ability to express its own uncertainty. We specifically chose Mean Token Entropy (Fomicheva et al., 2020) for its simplicity and TokenSAR (Duan et al., 2023), which incorporates a heuristic recognizing that certain tokens contribute more significantly to the sequence's meaning than others. We utilize the implementations from LM-Polygraph, a framework with implementations for many state of the art uncertainty metrics (Fadeeva et al., 2023). Additionally, to evaluate the potential of more advanced methods and to prepare for future improvements in uncertainty metrics, we have included a third \"oracle\" uncertainty metric in our analysis."}, {"title": "3.1.1 Mean Token Entropy", "content": "Mean Token Entropy (Fomicheva et al., 2020) is the average entropy across all tokens generated, with the entropy of a single token X defined as:\n\\(H(X) = \\sum_{x \\in V} -p(x) log p(x)\\)\nHere, V denotes the vocabulary of the model. Entropy is maximized when p(x) is uniform over all tokens in the vocabulary, indicating maximum uncertainty. Conversely, it is minimized when one token has a probability of 1 and all other tokens have a probability of 0, indicating complete certainty in the selected token. This method is highly computationally efficient, requiring minimal computation to determine the uncertainty once the text has been generated."}, {"title": "3.1.2 TokenSAR", "content": "TokenSAR (Duan et al., 2023) is defined as the weighted average of the negative log probabilities for each generated token, where the weights are the relevance scores of the tokens:\n\\(TokenSAR = \\sum_{i=1}^{N} - log p(t_i) R(t_i)\\)\nHere, N represents the number of tokens generated, \\(t_i\\) is the i-th token, and \\(R(t_i)\\) is the relevance of token \\(t_i\\).\nTo compute each token's relevance, RoBERTa-large (Liu et al., 2019) must be run N times in total. This is more computationally expensive than calculating mean token entropy, but still far less costly than metrics requiring multiple generations."}, {"title": "3.1.3 Oracle", "content": "While the aforementioned uncertainty metrics offer useful insights into agent uncertainty, they are not perfect. To mimic an ideal uncertainty metric, we also include an \"Oracle\" uncertainty metric in our evaluation. This metric yields low uncertainty when the agent is correct and high uncertainty when the agent is incorrect. It is defined as follows:\nuncertainty = \n\\begin{cases}\n0 & \\text{if the response is correct} \\\\\n\\infty & \\text{if the response is incorrect}\n\\end{cases}\n\nIn practice, using 0 and \u221e could cause issues with our uncertainty communication methods, so we detail exactly how this metric is applied in the following subsection. It is also important to note that this metric requires knowledge of the ground truth answer, making it impractical for real-world use. Instead, it serves to evaluate the effectiveness of our uncertainty communication methods independently of the performance of the uncertainty metrics themselves, and allows us to anticipate how improvements in uncertainty metrics could affect debate performance."}, {"title": "3.2 Incorporating Uncertainty into Debate", "content": "After computing the uncertainty of each agent, we explore multiple methods to incorporate these uncertainties into the following debate round."}, {"title": "3.2.1 Confidence in Prompt", "content": "One approach is to include the uncertainties directly in the text prompt for the next debate round. Mean Token Entropy and TokenSAR yield non-negative uncertainties. For Mean Token Entropy, the range of uncertainties depends on the model's vocabulary size, while for TokenSAR, the maximum uncertainty is unbounded. Therefore, the exact uncertainty values are less informative than the relative differences in uncertainty between agents. Rather than expressing their uncertainty, humans often express their confidence on 1 to 10 scale. Since LLMs are trained on human data, they may exhibit the same preference. As a result, we convert the uncertainties into confidence values. Given a list of uncertainties u for n agents, where ui is the uncertainty of agent i, we first invert them to obtain raw confidence values r. We then scale these values so that the average confidence si of all agents is 5. We then clamp the confidence levels to the range of 1 to 10. Finally, we round to the nearest integer:\n\\(r_i = \\frac{1}{u_i}\\)\n\\(s_i = \\frac{r_i}{\\frac{1}{n}\\sum_{i=1}^{n} r_i} (5n - 1)\\)\n\\(c_i = round(clamp(s_i, 1, 10))\\)\nWhen using the Oracle uncertainty metric, we set the confidence to 1 if the agent was incorrect and to 10 if the agent was correct. The computed confidence scores are then included in the text prompt to each agent, as illustrated in Figure 2."}, {"title": "3.2.2 Attention Scaling", "content": "As an alternative to including confidence levels in the prompt, we can modify the LLM's token-generation process to account for each agent's confidence. Many LLMs use Transformer decoder layers that generate an embedding for the last token and use this embedding to predict the next token (Radford et al., 2018). This embedding is determined by the attention mechanism, which creates \"query,\" \"key,\" and \"value\" vectors for each token.\nThe similarity between the \"query\" vector of the last token and the \"key\" vector of each token is used to compute a weight for every token. These weights are normalized with a softmax function to ensure they sum to 1, and are used to create the output embedding, which is the weighted sum of the value vectors of each token (Vaswani et al., 2017). The weight of each token determines its influence on the next token generated. By modifying these weights, we can adjust the model's focus on each token in the input.\nIn multi-agent debates, this allows us to shift the model's focus towards more confident agents. After each debate round, we will have responses from every agent. In the next round, each agent's prompt will include these responses. We also compute the uncertainty of each agent using an uncertainty metric.\nAfter the LLM computes the normalized attention weights, we multiply the weight of every token from agent j by the inverse of agent j's uncertainty uncertainty when using Mean Token Entropy or TokenSAR. When using the Oracle metric, to avoid divide-by-zero errors, we set the multiplier to \\(10^{-5}\\) if the agent was incorrect and 1 if the agent was correct.\nFormally, the attention weight for token i \\(a_i\\) is:\n\\(a_i = \\begin{cases}\n\\frac{W_i}{u_j} & \\text{if } i \\in t_j, \\text{ for any agent } j \\\\\nW_i & \\text{otherwise}\n\\end{cases}\\)\nHere, \\(t_j\\) is the set of token indices from agent j. We then normalize the scaled attention weights to ensure that the sum of all token weights equals 1, while leaving the weights of other tokens unchanged. The final weight \\(f_i\\) for every token i is calculated as follows:\n\\(f_i = \\begin{cases}\n\\frac{a_i}{\\sum_{j=1} \\sum_{k \\in t_j} \\frac{A_k}{u_j}} & \\text{if } i \\in t_j, \\text{ for any agent } j \\\\\nA_i & \\text{otherwise}\n\\end{cases}\\)\nWe only apply attention scaling to the responses from the previous round. For instance, in a three-round debate, attention would be rescaled for the responses from the first round during the second round, and for the responses from the second round during the third round. In the third round of debate, attention would not be rescaled to the first-round responses. Additionally, in order to prevent divide-by-zero errors during normalization, attention is not scaled when computing the embeddings for tokens within the prompt; it is only scaled when generating new tokens."}, {"title": "4 Experiment Design", "content": "To evaluate these methods, an open-source LLM is required, as implementing the attention scaling requires modifications to the model source code. Additionally, the uncertainty metrics used rely on token probabilities, which may not be readily available from closed-source models. We performed most of our experiments using Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), and add a small evaluation on Llama-3-8B-Instruct (AI@Meta, 2024) to verify that the results are applicable to other models. Tokens were sampled with a temperature of 1 to ensure variability in the responses. The methods were evaluated on multiple benchmarks:\n1. MMLU (Hendrycks et al., 2021): A dataset of multiple-choice questions across various subjects.\n2. GSM8k (Cobbe et al., 2021): A dataset of free-response grade school math problems.\n3. TruthfulQA (Lin et al., 2022b): A multiple-choice dataset testing the model's susceptibility to common misconceptions.\n4. Arithmetic: An randomly generated set of arithmetic problems in the form a + b \u00b7 c + d where 0 \u2264 a, b, c, d < 30.\nWe explore two variants of attention scaling:\n\u2022 Attention-Others, where agent i only rescales attention to other agents' response tokens \\(t_j \\vert j \\neq i\\)\n\u2022 Attention-All, where agent i rescales attention to other agents and itself, illustrated in Figure 3"}, {"title": "5 Results", "content": "In this section, we first analyze the effectiveness of each uncertainty incorporation method, and then analyze the effectiveness of the uncertainty metrics."}, {"title": "5.1 Uncertainty Incorporation Methods", "content": "Table 1 presents a comparison of the results obtained using different combinations of uncertainty metrics (Mean Token Entropy, TokenSAR, and Oracle) and methods (Confidence in Prompt, Attention-Others, and Attention-All). As a baseline, the performance of a standard 3-agent, 3-round debate without any uncertainty metrics is also shown. Overall, Attention-All was the top-performing method, achieving the highest average accuracy across all three uncertainty metrics. It was the only method that consistently matched or exceeded the performance of the standard multi-agent debate on all benchmarks.\nAs shown in Figure 4, Attention-All demonstrates the most significant accuracy improvements as the AUROC of the uncertainty metric increases, with a slope of 0.59 compared to 0.45 for Attention-Others and 0.17 for Confidence in Prompt. The accuracy improvement ratio compares the method's accuracy to the accuracy observed in a standard debate. AUROC, the area under the receiver operating characteristic curve, represents the probability that a correct answer is assigned a lower uncertainty than an incorrect one. A random uncertainty metric would have an AUROC of 0.5, while a perfect one would have an AUROC of 1."}, {"title": "5.2 Uncertainty Metrics", "content": "The best-performing uncertainty metric was the Oracle metric. Mean Token Entropy ranked next, with debates using it consistently achieving higher average accuracies than debates using TokenSAR, as shown in Table 1. Mean Token Entropy achieved an average AUROC across all experiments of 0.627, compared to 0.617 for TokenSAR. Full AUROC results are shown in Appendix B."}, {"title": "5.3 Llama Evaluation", "content": "Finally, Table 2 presents the results of a small-scale test using Llama-3-8B-Instruct on the MMLU benchmark with zero-shot prompting. The results demonstrate that Attention-All, paired with the Oracle uncertainty metric, consistently delivers the highest performance. However, performance drops when using Mean Token Entropy as the uncertainty estimator. While a more extensive evaluation is required to make definitive conclusions about DebUnc performance on Llama-3, the consistently high performance of Attention-All with the noise-free Oracle metric indicates its effectiveness in conveying agent uncertainty."}, {"title": "6 Conclusion", "content": "We present DebUnc, a framework that integrates confidence estimations into multi-agent debates. This addresses the challenge of determining who to trust when agents present different answers. Incorporating confidence levels into the debate provides agents with a method to evaluate the reliability of the other agents. We use uncertainty metrics to estimate the LLM's confidence based on the probability distribution generated for each token. To communicate confidence, we adjusted the LLM attention mechanism to modify token weights based on confidence levels, and we also explored using textual prompts for this purpose. Our findings indicate that adjusting attention is more effective, which shows that information can be conveyed to LLMs through methods beyond traditional text prompts. Attention scaling can be applied whenever parts of the input should be prioritized, and we hope future work explores this idea further."}, {"title": "7 Limitations", "content": "One key limitation of the methods presented is the reliance on open-source LLMs, as attention scaling requires modifications to the model's source code. Additionally, obtaining token probabilities can be challenging with proprietary LLMs, making it difficult to compute uncertainty. While open-source LLMs such as Mistral 7B (Jiang et al., 2023) and Llama 3 (AI@Meta, 2024) are quite powerful, they do not yet match the performance of leading proprietary models.\nIn addition, the attention scaling method is sensitive to the order of agent responses in the prompt. For example, if agent 2's response appears before agent 3's response, then the embeddings for tokens from agent 3 will have been influenced by agent 2's response, but not vice versa due to the unidirectional attention used. This could allow information from agent 2 to leak through, even if we focus entirely on agent 3.\nLastly, our experimental setup always uses the same LLM for each agent, with each agent being given the same role. Although this is effective for multi-agent debates, it may be worthwhile to explore combinations of different LLMs, such as pairing smaller LLMs fine-tuned on specific domains with larger LLMs that have broader knowledge."}, {"title": "Ethics Statement", "content": "Deploying large language models (LLMs) to the general public requires them to be trustworthy and reliable, particularly in high-risk scenarios. This concern gains more importance when LLMs interact within agent frameworks. We foresee a future where LLMs interact with other LLMs to achieve complex tasks. In this work, we take a step towards achieving reliable communication among agents by incorporating uncertainty estimations."}]}