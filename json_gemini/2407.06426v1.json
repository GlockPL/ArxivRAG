{"title": "DebUnc: Mitigating Hallucinations in Large Language Model Agent\nCommunication with Uncertainty Estimations", "authors": ["Luke Yoffe", "Alfonso Amayuelas", "William Yang Wang"], "abstract": "To enhance Large Language Model (LLM) ca-\npabilities, multi-agent debates have been in-\ntroduced, where multiple LLMs discuss solu-\ntions to a problem over several rounds of de-\nbate. However, LLMs often produce incor-\nrect responses that appear deceptively confi-\ndent, which can mislead other agents. This\nis partly because agents do not express their\nconfidence levels during standard debates. To\naddress this, we introduce DebUnc, a multi-\nagent debate framework that uses uncertainty\nmetrics to assess agent confidence levels. We\nadapted the LLM attention mechanism to ad-\njust token weights based on confidence lev-\nels and also explored using textual prompts\nto convey confidence. Our evaluations across\nvarious benchmarks show that attention-based\nmethods are particularly effective, and that as\nuncertainty metrics evolve, performance will\ncontinue to increase. The code is available at\nhttps://github.com/lukeyoffe/debunc.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown im-\npressive performance in various fields, including\nlaw, academia, and coding (OpenAI, 2024). \u03a4\u03bf\nhandle more complex tasks, LLM-powered agents\nhave been developed. These agents observe their\nenvironment and take actions, such as communicat-ing with other agents, using tools, or performing\nreasoning (Wu et al., 2023). The potential of LLM\nagents increases when multiple agents collaborate,\nwhich can take different forms.\nOne approach involves agents assuming different\nroles, such as designer, programmer, tester, or man-\nager, within a software development team of LLM\nagents (Qian et al., 2024). Alternatively, agents\nmay all assume the same role, collaborating to\nsolve problems through multi-agent debates. This\ncan help address LLM's tendency to generate incor-\nrect responses, known as hallucinations (Du et al.,\n2023), and is the focus of this paper.\nHallucinations undermine the trustworthiness\nand utility of LLMs in real-world applications\n(Rawte et al., 2023). For example, an LLM tu-\ntor providing incorrect information could harm a\nstudent's learning, and an LLM customer service\nagent giving incorrect advice could mislead a cus-\ntomer. In fields like healthcare or financial services,\ninaccurate information from an LLM could have\neven more severe consequences, ranging from fi-\nnancial losses to health risks. Multi-agent debates\naim to solve this issue by allowing agents to first\ngenerate diverse answers to a problem, then discuss\neach other's responses, and finally converge on a\nfinal answer (Liang et al., 2023)."}, {"title": "2 Related Work", "content": "Large Language Models (LLMs) are characterized\nby their overconfidence and propensity to always\nprovide a response to user queries. This often re-\nsults in the generation of inaccurate content, a phe-\nnomenon known as hallucination, where the mod-\nels produce information that is not supported by\ntheir training data or the given input (Liang et al.,\n2024; Yadkori et al., 2024; Duan et al., 2024; Yao\net al., 2023; Aichberger et al., 2024). Addressing\nhallucinations is highly important for developing\ntrust in LLM-based systems and facilitating their\ndeployment in more real-world applications. As\na result, there has been a significant increase in\nresearch aimed at understanding the mechanisms\nbehind hallucinations and developing strategies to\nmitigate them (Ji et al., 2023; McDonald et al.,\n2024; Liu et al., 2023)."}, {"title": "2.1 Uncertainty in LLMs", "content": "Some current research efforts to mitigate hallucina-\ntions focus on measuring the model's uncertainty\nand enhancing their self-awareness (Kadavath et al.,\n2022; Amayuelas et al., 2023; Yin et al., 2023).\nBy accurately evaluating their uncertainty, end-\nusers can make more informed decisions about\nwhen to trust the model's generated output (Lin\net al., 2022a; Xu et al., 2024). However, accurately\nmeasuring model uncertainty remains an unsolved\nproblem, and ongoing research is exploring new\nuncertainty metrics. We identify three primary ap-\nproaches to this problem:\nToken Probability-Based Uncertainty Met-\nrics evaluate uncertainty based on the probabili-\nties assigned to each token generated by the model.\nHigh token probabilities (close to 1) indicate strong\nmodel confidence, whereas lower token probabili-\nties suggest a spread of probability across several\ntokens, signifying uncertainty about the correct\nchoice. Prominent methods in this category include\nMean Token Entropy, Perplexity (Fomicheva et al.,\n2020), SAR (Duan et al., 2023), RDE (Vazhentsev\net al., 2023), and Claim-Conditioned Probability\n(Fadeeva et al., 2024).\nLLM-Generated Uncertainty Metrics involve\nthe model explicitly expressing its uncertainty in\nits response. This can be achieved in multiple ways,\nincluding by training the model. This was the ap-\nproach taken by Lin et al. (2022a), who fine-tuned\nGPT-3 (Brown et al., 2020) to provide its answer\nto a question along with its confidence level."}, {"title": "2.2 Multi-Agent Debate", "content": "With the increased accessibility of LLMs and im-\nprovements in their inference times, LLMs are be-\ning integrated into more complex systems as au-\ntonomous agents (Wu et al., 2023; Li et al., 2023;\nHong et al., 2023). A critical component of these\nagent-based systems is the collaboration mecha-\nnism, where models engage in debate with one\nanother. These mechanisms are currently being\nstudied (Zhang et al., 2023) and have been shown to\nfoster more divergent thinking (Liang et al., 2023),\nenhance reasoning and factual accuracy (Du et al.,\n2023), and lead to more reliable evaluations (Chan\net al., 2023). Through discussions, the LLMs can\nrefine their outputs, ultimately achieving higher\nlevels of agreement and producing more factually\naccurate text (Sun et al., 2024; Feng et al., 2024).\nReConcile (Chen et al., 2023) explored the inte-\ngration of agent confidence in multi-agent debates,\nrelying on LLM agents to self-report their confi-\ndence, which was communicated to other agents\nthrough prompts. This led to marginal performance\nimprovements due to the confidence estimation.\nBuilding on their approach, we employ uncertainty\nmetrics to estimate agent confidence and explore\nboth prompting and attention scaling to convey this\nconfidence. We found these enhancements to sig-\nnificantly improve multi-agent debate performance."}, {"title": "3 Method", "content": "In human debates, it is often possible to gauge\nsomeone's expertise on a subject by observing the\nfluency of their responses, their body language, and\nother cues. This helps in identifying whose argu-\nments to consider more seriously when there are\nconflicting opinions. On the other hand, in multi-\nagent LLM debates, agents frequently generate in-\naccurate responses that sound confident, which can\nmislead other agents and result in a consensus on\nan incorrect response (Du et al., 2023). Our goal\nis to advise agents on which other agents' opinions\nto prioritize based on their confidence levels.\nOur modified debate pipeline, depicted in Fig-\nure 2, operates as follows: in each round of debate,\nevery agent generates a response, and its uncer-\ntainty is estimated. In the next round, the responses\nand uncertainties from each agent are shared with\nevery other agent. We test three uncertainty met-\nrics and three approaches to communicate agent\nuncertainty."}, {"title": "3.1 Uncertainty Metrics", "content": "Uncertainty metrics gauge an LLM's confidence in\nits responses; high uncertainty signifies low con-\nfidence and potential unreliability, while low un-\ncertainty indicates high confidence and greater re-\nliability. These metrics typicaly fall into one or\nmore of three categories: Token Probability-Based,\nLLM-Generated, and Sampling-Based methods.\nFor more information about these categories, re-\nfer to Uncertainty in LLMs\nIn our experiments, we focus on token\nprobability-based metrics due to their computa-\ntional efficiency and flexibility. These metrics re-\nquire only a single generation and are effective\nregardless of the model's ability to express its own\nuncertainty. We specifically chose Mean Token\nEntropy (Fomicheva et al., 2020) for its simplicity\nand TokenSAR (Duan et al., 2023), which incorpo-\nrates a heuristic recognizing that certain tokens con-\ntribute more significantly to the sequence's mean-\ning than others. We utilize the implementations\nfrom LM-Polygraph, a framework with implemen-\ntations for many state of the art uncertainty metrics\n(Fadeeva et al., 2023). Additionally, to evaluate the\npotential of more advanced methods and to prepare\nfor future improvements in uncertainty metrics, we\nhave included a third \"oracle\" uncertainty metric in\nour analysis."}, {"title": "3.1.1 Mean Token Entropy", "content": "Mean Token Entropy (Fomicheva et al., 2020) is\nthe average entropy across all tokens generated,\nwith the entropy of a single token X defined as:\n$H(X) = \\sum_{x \\in V} -p(x) \\log p(x)$\nHere, V denotes the vocabulary of the model. En-\ntropy is maximized when p(x) is uniform over all\ntokens in the vocabulary, indicating maximum un-\ncertainty. Conversely, it is minimized when one\ntoken has a probability of 1 and all other tokens\nhave a probability of 0, indicating complete cer-\ntainty in the selected token. This method is highly\ncomputationally efficient, requiring minimal com-\nputation to determine the uncertainty once the text\nhas been generated."}, {"title": "3.1.2 TokenSAR", "content": "TokenSAR (Duan et al., 2023) is defined as the\nweighted average of the negative log probabilities\nfor each generated token, where the weights are the\nrelevance scores of the tokens:\n$TokenSAR = \\sum_{i=1}^N - \\log p(t_i) R(t_i)$\nHere, N represents the number of tokens generated,\n$t_i$ is the i-th token, and $R(t_i)$ is the relevance of\ntoken $t_i$."}, {"title": "3.1.3 Oracle", "content": "While the aforementioned uncertainty metrics of-\nfer useful insights into agent uncertainty, they are\nnot perfect. To mimic an ideal uncertainty metric,\nwe also include an \"Oracle\" uncertainty metric in\nour evaluation. This metric yields low uncertainty\nwhen the agent is correct and high uncertainty when\nthe agent is incorrect. It is defined as follows:\n$uncertainty = \\begin{cases} 0 & \\text{if the response is correct} \\\\ \\infty & \\text{if the response is incorrect} \\end{cases}$\nIn practice, using 0 and\u221e could cause issues with\nour uncertainty communication methods, so we\ndetail exactly how this metric is applied in the fol-\nlowing subsection. It is also important to note that\nthis metric requires knowledge of the ground truth\nanswer, making it impractical for real-world use.\nInstead, it serves to evaluate the effectiveness of\nour uncertainty communication methods indepen-\ndently of the performance of the uncertainty met-\nrics themselves, and allows us to anticipate how\nimprovements in uncertainty metrics could affect\ndebate performance."}, {"title": "3.2 Incorporating Uncertainty into Debate", "content": "After computing the uncertainty of each agent, we\nexplore multiple methods to incorporate these un-\ncertainties into the following debate round."}, {"title": "3.2.1 Confidence in Prompt", "content": "One approach is to include the uncertainties di-\nrectly in the text prompt for the next debate round.\nMean Token Entropy and TokenSAR yield non-\nnegative uncertainties. For Mean Token Entropy,\nthe range of uncertainties depends on the model's\nvocabulary size, while for TokenSAR, the maxi-\nmum uncertainty is unbounded. Therefore, the ex-\nact uncertainty values are less informative than the\nrelative differences in uncertainty between agents.\nRather than expressing their uncertainty, humans\noften express their confidence on 1 to 10 scale.\nSince LLMs are trained on human data, they may\nexhibit the same preference. As a result, we convert\nthe uncertainties into confidence values. Given a\nlist of uncertainties u for n agents, where $u_i$ is the\nuncertainty of agent i, we first invert them to obtain\nraw confidence values r. We then scale these values\nso that the average confidence $s_i$ of all agents is 5.\nWe then clamp the confidence levels to the range\nof 1 to 10. Finally, we round to the nearest integer:\n$r_i = \\frac{1}{u_i}$\n$s_i = \\frac{r_i}{\\sum_{i=1}^n r_i} (5n - 1) + \\frac{1}{n}$\n$c_i = round(clamp(s_i, 1, 10))$\nWhen using the Oracle uncertainty metric, we set\nthe confidence to 1 if the agent was incorrect and\nto 10 if the agent was correct. The computed confi-\ndence scores are then included in the text prompt\nto each agent, as illustrated in Figure 2."}, {"title": "3.2.2 Attention Scaling", "content": "As an alternative to including confidence levels\nin the prompt, we can modify the LLM's token-\ngeneration process to account for each agent's con-\nfidence. Many LLMs use Transformer decoder lay-\ners that generate an embedding for the last token\nand use this embedding to predict the next token\n(Radford et al., 2018). This embedding is deter-\nmined by the attention mechanism, which creates\n\"query,\" \"key,\" and \"value\" vectors for each token.\nThe similarity between the \"query\" vector of the\nlast token and the \"key\" vector of each token is used\nto compute a weight for every token. These weights\nare normalized with a softmax function to ensure\nthey sum to 1, and are used to create the output\nembedding, which is the weighted sum of the value\nvectors of each token (Vaswani et al., 2017). The\nweight of each token determines its influence on the\nnext token generated. By modifying these weights,\nwe can adjust the model's focus on each token in\nthe input.\nIn multi-agent debates, this allows us to shift the\nmodel's focus towards more confident agents. After\neach debate round, we will have responses from\nevery agent. In the next round, each agent's prompt\nwill include these responses. We also compute\nthe uncertainty of each agent using an uncertainty\nmetric."}, {"title": "4 Experiment Design", "content": "To evaluate these methods, an open-source LLM\nis required, as implementing the attention scaling\nrequires modifications to the model source code.\nAdditionally, the uncertainty metrics used rely on\ntoken probabilities, which may not be readily avail-\nable from closed-source models. We performed\nmost of our experiments using Mistral-7B-Instruct-\nv0.2 (Jiang et al., 2023), and add a small evaluation\non Llama-3-8B-Instruct (AI@Meta, 2024) to ver-\nify that the results are applicable to other models.\nTokens were sampled with a temperature of 1 to\nensure variability in the responses. The methods\nwere evaluated on multiple benchmarks:\n1. MMLU (Hendrycks et al., 2021): A dataset\nof multiple-choice questions across various\nsubjects.\n2. GSM8k (Cobbe et al., 2021): A dataset of\nfree-response grade school math problems.\n3. TruthfulQA (Lin et al., 2022b): A multiple-\nchoice dataset testing the model's susceptibil-\nity to common misconceptions.\n4. Arithmetic: An randomly generated set of\narithmetic problems in the form a + b \u00b7 c + d\nwhere 0 \u2264 a, b, c, d < 30."}, {"title": "5 Results", "content": "In this section, we first analyze the effectiveness of\neach uncertainty incorporation method, and then\nanalyze the effectiveness of the uncertainty metrics."}, {"title": "5.1 Uncertainty Incorporation Methods", "content": "Table 1 presents a comparison of the results ob-\ntained using different combinations of uncertainty\nmetrics (Mean Token Entropy, TokenSAR, and Ora-\ncle) and methods (Confidence in Prompt, Attention-\nOthers, and Attention-All). As a baseline, the\nperformance of a standard 3-agent, 3-round de-\nbate without any uncertainty metrics is also shown.\nOverall, Attention-All was the top-performing\nmethod, achieving the highest average accuracy\nacross all three uncertainty metrics. It was the only\nmethod that consistently matched or exceeded the\nperformance of the standard multi-agent debate on\nall benchmarks.\nAs shown in Figure 4, Attention-All demon-\nstrates the most significant accuracy improvements\nas the AUROC of the uncertainty metric increases,\nwith a slope of 0.59 compared to 0.45 for Attention-\nOthers and 0.17 for Confidence in Prompt. The ac-\ncuracy improvement ratio compares the method's\naccuracy to the accuracy observed in a standard de-\nbate. AUROC, the area under the receiver operating\ncharacteristic curve, represents the probability that\na correct answer is assigned a lower uncertainty\nthan an incorrect one. A random uncertainty metric\nwould have an AUROC of 0.5, while a perfect one\nwould have an AUROC of 1."}, {"title": "5.2 Uncertainty Metrics", "content": "The best-performing uncertainty metric was the\nOracle metric. Mean Token Entropy ranked next,\nwith debates using it consistently achieving higher\naverage accuracies than debates using TokenSAR,\nas shown in Table 1. Mean Token Entropy achieved\nan average AUROC across all experiments of 0.627,\ncompared to 0.617 for TokenSAR. Full AUROC\nresults are shown in Appendix B."}, {"title": "5.3 Llama Evaluation", "content": "Finally, Table 2 presents the results of a small-\nscale test using Llama-3-8B-Instruct on the MMLU\nbenchmark with zero-shot prompting. The results\ndemonstrate that Attention-All, paired with the Or-\nacle uncertainty metric, consistently delivers the\nhighest performance. However, performance drops\nwhen using Mean Token Entropy as the uncertainty\nestimator. While a more extensive evaluation is\nrequired to make definitive conclusions about De-\nBUnc performance on Llama-3, the consistently\nhigh performance of Attention-All with the noise-\nfree Oracle metric indicates its effectiveness in con-\nveying agent uncertainty."}, {"title": "6 Conclusion", "content": "We present DebUnc, a framework that integrates\nconfidence estimations into multi-agent debates.\nThis addresses the challenge of determining who to\ntrust when agents present different answers. Incor-\nporating confidence levels into the debate provides\nagents with a method to evaluate the reliability of\nthe other agents. We use uncertainty metrics to esti-\nmate the LLM's confidence based on the probabil-\nity distribution generated for each token. To com-\nmunicate confidence, we adjusted the LLM atten-\ntion mechanism to modify token weights based on\nconfidence levels, and we also explored using tex-\ntual prompts for this purpose. Our findings indicate\nthat adjusting attention is more effective, which\nshows that information can be conveyed to LLMs\nthrough methods beyond traditional text prompts.\nAttention scaling can be applied whenever parts of\nthe input should be prioritized, and we hope future\nwork explores this idea further."}, {"title": "7 Limitations", "content": "One key limitation of the methods presented is the\nreliance on open-source LLMs, as attention scaling\nrequires modifications to the model's source code.\nAdditionally, obtaining token probabilities can be\nchallenging with proprietary LLMs, making it dif-\nficult to compute uncertainty. While open-source\nLLMs such as Mistral 7B (Jiang et al., 2023) and\nLlama 3 (AI@Meta, 2024) are quite powerful, they\ndo not yet match the performance of leading pro-\nprietary models.\nIn addition, the attention scaling method is sen-\nsitive to the order of agent responses in the prompt.\nFor example, if agent 2's response appears before\nagent 3's response, then the embeddings for tokens\nfrom agent 3 will have been influenced by agent\n2's response, but not vice versa due to the unidirec-\ntional attention used. This could allow information\nfrom agent 2 to leak through, even if we focus\nentirely on agent 3.\nLastly, our experimental setup always uses the\nsame LLM for each agent, with each agent being\ngiven the same role. Although this is effective for\nmulti-agent debates, it may be worthwhile to ex-\nplore combinations of different LLMs, such as pair-\ning smaller LLMs fine-tuned on specific domains\nwith larger LLMs that have broader knowledge."}, {"title": "Ethics Statement", "content": "Deploying large language models (LLMs) to the\ngeneral public requires them to be trustworthy and\nreliable, particularly in high-risk scenarios. This\nconcern gains more importance when LLMs inter-\nact within agent frameworks. We foresee a future\nwhere LLMs interact with other LLMs to achieve\ncomplex tasks. In this work, we take a step towards\nachieving reliable communication among agents\nby incorporating uncertainty estimations."}]}