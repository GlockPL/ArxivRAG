{"title": "Easy, Interpretable, Effective: openSMILE for voice deepfake detection", "authors": ["Octavian Pascu", "Dan Onea\u021b\u0103", "Horia Cucu", "Nicolas M\u00fcller"], "abstract": "In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset\u2014a de facto standard in the field of voice authenticity and deepfake detection\u2014can be identified with surprising accuracy using a small subset of very simplistic features. These are derived from the openSMILE library, and are scalar-valued, easy to compute, and human interpretable. For example, attack A10's unvoiced segments have a mean length of 0.09\u00b10.02, while bona fide instances have a mean length of 0.18\u00b10.07. Using this feature alone, a threshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack A10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall EER of 15.7\u00b16.0%. We explore the generalization capabilities of these features and find that some of them transfer effectively between attacks, primarily when the attacks originate from similar Text-to-Speech (TTS) architectures. This finding may indicate that voice anti-spoofing is, in part, a problem of identifying and remembering signatures or fingerprints of individual TTS systems. This allows to better understand anti-spoofing models and their challenges in real-world application.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-speech (TTS) technology has advanced significantly in recent years, offering various beneficial applications, such as the restoration of voice capabilities for speech-impaired individuals [1]. Despite these positive developments, TTS also presents potential risks such as the compromise of voice biometric systems and the creation of deepfakes, which enable fraud, slander and misinformation. A key initiative in combating these threats is the ASVspoof Challenge [2]. Since its inception in 2015, this biennial event has released datasets crucial for the design of anti-spoofing systems. Notably, since the forth iteration of ASVspoof in 2021 [3], [4], the challenge has included a separate track specifically for detecting audio deepfakes. The latest iteration of the ASVspoof challenge dataset is called 'ASVspoof 5', and is set to succeed the ASVspoof 2021 dataset, the de-facto standard in evaluating of voice authenticity systems.\nWhile related work reports high performance on the ASVspoof datasets [5]\u2013[7], achieving true generalization that applies effectively in real-world scenarios remains a challenge. Existing studies suggest that anti-spoofing efforts often rely on shortcut artifacts, such as the length of silences [8] or bitrate information [9], and struggle with cross-dataset generalization [10]. Large self-supervised representations improve the generalization to some degree [11], but this approach comes at the cost of explainability, which remains an important desideratum given the decision-critical nature of deepfake detection.\nIn this work, we propose to use openSMILE [12], a software tool for the automatic extraction of audio features. This tool allows the computation of interpretable features, based on fundamental properties such as the length of voiced and unvoiced segments, spectral flux (a measure of how quickly the power spectrum of a signal is changing), or energy within specific frequency ranges. Our findings demonstrate that for the ASVspoof5 dataset, even single, scalar-valued openSMILE features allow for surprisingly accurate classification of attacks (i.e. voice deepfakes from a specific TTS system). For example, the 'MeanUnvoiceSegmentLength' feature allows for reliable identification of attack A10, c.f. Figure 1. We find that for each attack in ASVspoof5, such features can be found. Interestingly, the features extracted exhibit varying degrees of specificity: some are highly specific and fail to generalize across different types of attacks, while others show broader applicability. This variation suggests that the underlying text-to-speech (TTS) models each possess unique characteristics that, if recognized during training, facilitate identification. In summary:\n\u2022 We employ openSMILE to identify single, scalar-valued and human-interpretable features that effectively pinpoint attacks within the ASVspoof5 dataset.\n\u2022 We show that cross-domain generalization works best when attacks come from similar TTS architectures.\n\u2022 This observation suggests that TTS models exhibit unique characteristics, akin to fingerprints, which are readily identifiable if seen during training, but can pose significant challenges in cross-model generalization."}, {"title": "II. METHODOLOGY", "content": "A. Data Partitioning\nWe utilize the two currently available, labeled partitions of the ASVspoof5 [13] dataset: 'train' and 'dev'. Each partition a set of bona fide instances, denoted as BT and BD (bona fide 'train' and bona fide 'dev', respectively). Additionally, each partition includes spoofed audio files, categorized by the TTS system used to generate them, referred to as 'attacks'. The 'train' partition comprises attacks A01 through A08, while the 'dev' partition includes attacks A09 through A16; the types of attacks are listed in Table II. For our experiments, we divided BT and BD, along with each attack, into an 80% training pool and a 20% evaluation pool, c.f. Figure 2.\nB. Identifying Predictive openSMILE Features\nOur objective is to ascertain whether attacks in ASVspoof5 can be detected using a single, scalar-valued feature from openSMILE. We aim to evaluate the performance of this method in both in-domain (ID) and out-of-domain (OOD) scenarios. ID scenarios involve training and evaluation data from the same attack, whereas OOD scenarios involve training and evaluation data from different attacks. For instance, consider a classifier trained on bona fide instances from the training pool (BDtrain and BTtrain), as well as spoofed instances from attack A01train. Evaluating performance on BDeval, BTeval, and A01eval constitutes an ID challenge, while evaluating performance on BDeval, BTeval, and A02eval constitutes an OOD challenge. Prior research [10], [14], [15] highlights that OOD generalization is challenging but essential for real-world deepfake detection applications.\nWe aim to identify the most predictive features for each attack Aj. To this end, we utilize openSMILE's \u2018eGmapsV2' feature set [16], consisting of 88 scalar-valued features derived from an audio's loudness, spectral flux, voiced and unvoiced segment lengths, etc. This set is an extended version of the Geneva Minimalistic Acoustic Parameter (eGeMAPS) feature set, originally employed for emotion recognition and affective computing. While we have considered other feature sets, such as 'emobase', we found them to be less interpretable.\nTo illustrate our approach, consider Figure 1. The plot shows the distribution of the feature 'MeanUnvoicedSegmentLength' (shorthand F85, being the 86th feature from 'eGeMAPSv2') for bona fide instances and attack A10. The red line represents spoofed data from A10, while the blue line represents bona fide data from the 'train' and 'dev splits. It is evident that this feature exhibits distinct behavior between spoofed and real data. For spoofed data, this feature assumes values of 0.09 \u00b1 0.02 (mean and standard deviation, respectively), while for bona fide data, it assumes values of 0.18 \u00b10.07. Using this feature as a predictor, a threshold classifier achieves an EER of 10.3%, much better than random guessing (50% EER). Therefore, 'MeanUnvoicedSegmentLength' is a good predictor for attack A10. In subsequent experiments, we find the most predictive features using this approach: For each attack Aj and each \u2018eGeMAPSv2' feature, compute the EER on BTtrain, BDtrain, and Ajtrain. Then, we present the two features that individually yield the best EER."}, {"title": "III. EXPERIMENTS", "content": "A. Experiment A: openSMILE, In-domain\nIn this initial experiment, we evaluate the effectiveness of 'eGeMAPSv2' in an in-domain (ID) setting, where the training and testing data originate from the same distribution.\nFirst, we select the front-end: as outlined in Section II-B, we identify the two most predictive features per attack. Second, we train the back-end on each feature individually: A linear classification model \\(f(x) = wx + b \u2208 R\\) where \\(x, w, b \u2208 R\\). This model is trained on the 'train' portion of the attack data along with the bona fide training data (i.e. BDtrain, BTtrain and Ajtrain, c.f. Figure 2). We then compute the EER on the corresponding evaluation data, i.e. BDeval, BTeval and Ajeval.\nB. Experiment B: openSMILE, Out-of-domain\nIn our second experiment, we extend our approach to an out-of-domain (OOD) setting, where the training and testing data come from different distributions. This aims to evaluate the effectiveness of openSMILE features for cross-domain detection in the ASVspoof5 dataset. We follow the same training protocol as in Section III-A, but evaluate on OOD data. For instance, given a model trained on A01, we evaluate on A02 through A16 and present the EERs individually. Note that both the feature itself, as well as the parameters \\(w, b \u2208 R\\) are derived from the initial in-domain training. This approach results in a matrix of OOD performances, where the entries on the diagonal correspond to the in-domain (ID) values, as shown in Table I.\nThe results are shown in Table III, where the rows indicate the training attack, and the columns indicate the evaluation attack. Note that Table III can be partitioned into four sub-matrices based on the 'train' and 'dev' partitions of the ASVspoof 5 dataset. Systems trained on attacks A01 through A08 generalize relatively well to other 'train' attacks, as indicated by the green highlights and correspondingly lower EERS (20.0\u00b17.0%) in the top-left quadrant.\nWe observe that certain groups of attacks exhibit strong generalization between themselves, such as A09 and A10 (9.3\u00b11.7%EER). We hypothesize that this generalization is due to similarities in the underlying TTS algorithms (see Table II for details). For example, both A09 and A10 are derived from ToucanTTS [18], which explains their effective generalization. Similarly, A01, A02, and A03 are based on the GlowTTS architecture [17], which likely contributes to the strong generalization observed among them. Generalization across different TTS architectures often proves challenging; for example, systems trained on A16 do not generalize to any of the attacks from A01 through A08.\nIn summary, generalization from 'dev' to 'train' and vice-versa is challenging, as indicated by the predominantly red highlights and high EERs (52.9\u00b117.9%) in the bottom-left and top-right quadrants.\nC. Experiment C: Comparision with Neural Front-end\nWe compare the 'eGeMAPSv2' openSMILE front-end to the Wav2Vec2\u00b9 self-supervised neural front-end [27], used widely in related work [11], [28]\u2013[32]. For each of the 768 scalar outputs of Wav2Vec2, we assess how discriminative it is for each of the sixteen attacks in ASV5. This is done by using the scalar value as a score itself and directly computing the EER on them, similar to Figure 1. Thus, for each attack, we obtain 768 predictors with corresponding EERS. We perform the same process for openSMILE and compare via a (normalized) distribution plot, c.f. Figure 3. We observe that for both openSMILE and Wav2Vec2, many of the features are not predictive: the performance has its mode around 40-50% EER. However, for both feature sets there is a small subset of more predictive features"}, {"title": "IV. DISCUSSION AND CONCLUSION", "content": "In this work, we analyze the efficacy of openSMILE as a front-end for voice anti-spoofing, specifically on the ASVspoof5 dataset. We find that for all attacks, there exist scalar-valued, human interpretable features which allow for good in-domain detection performance. However, the application of these characteristics in out-of-domain settings remains limited, particularly across different TTS architectures.\nWe argue that TTS models display unique characteristics, akin to a fingerprint, which can be readily identified once encountered but can have limited use for generalization across different models. Judging by the good transferability between attacks originating from similar TTS architectures, c.f. Table II and Table III, these characteristics appear to include information about the underlying TTS architecture. Conversely, generalization across architecture boundaries is challenging. Success in the field of source tracing [39], [45], which involves assigning instances to their generating models, further demonstrates that individual models exhibit individual characteristics.\nThus, voice anti-spoofing may, in part, involve the challenge of identifying the unique signatures or fingerprints of individual TTS systems. While these distinctive characteristics can enable good identification of an attack, they require that the TTS model has been encountered during training. This may in part explain challenges in real-world application [10]."}]}