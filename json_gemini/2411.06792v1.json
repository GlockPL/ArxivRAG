{"title": "Evolving Efficient Genetic Encoding for Deep\nSpiking Neural Networks", "authors": ["Wenxuan Pan", "Feifei Zhao", "Bing Han", "Haibo Tong", "Yi Zeng"], "abstract": "By exploiting discrete signal processing and simu-\nlating brain neuron communication, Spiking Neural Networks\n(SNNs) offer a low-energy alternative to Artificial Neural\nNetworks (ANNs). However, existing SNN models, still face high\ncomputational costs due to the numerous time steps as well as\nnetwork depth and scale. The tens of billions of neurons and\ntrillions of synapses in the human brain are developed from only\n20,000 genes, which inspires us to design an efficient genetic\nencoding strategy that dynamic evolves to regulate large-scale\ndeep SNNs at low cost. Therefore, we first propose a genetically\nscaled SNN encoding scheme that incorporates globally shared\ngenetic interactions to indirectly optimize neuronal encoding\ninstead of weight, which obviously brings about reductions in\nparameters and energy consumption. Then, a spatio-temporal\nevolutionary framework is designed to optimize the inherently\ninitial wiring rules. Two dynamic regularization operators in\nthe fitness function evolve the neuronal encoding to a suitable\ndistribution and enhance information quality of the genetic\ninteraction respectively, substantially accelerating evolutionary\nspeed and improving efficiency. Experiments show that our\napproach compresses parameters by approximately 50% to\n80%, while outperforming models on the same architectures\nby 0.21% to 4.38% on CIFAR-10, CIFAR-100 and ImageNet.\nIn summary, the consistent trends of the proposed genetically\nencoded spatio-temporal evolution across different datasets and\narchitectures highlight its significant enhancements in terms of\nefficiency, broad scalability and robustness, demonstrating the\nadvantages of the brain-inspired evolutionary genetic coding for\nSNN optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial neural networks have provided important insights\ninto numerous application areas [1], [2], but the large number\nof matrix operations increases significantly with the size of\nthe network. Spiking Neural Networks [3], as the third gen-\neration of neural networks, achieve a low-energy computing\nparadigm by simulating the communication characteristics of\nbrain neurons and leveraging the inherent energy efficiency\nadvantages of discrete signal processing and are more biolog-\nically plausible. Most of the research on SNN optimization\nfocuses on the training mechanism, which can be roughly\ndivided into plasticity-based training [4], conversion-based\ntraining [5], and gradient-based training [6], which has be-\ncome the most mainstream method due to its high efficiency.\nHowever, a large number of time steps, proxy gradient calcu-\nlations, and increasingly deeper network architecture designs\nalso make SNNs increasingly computationally expensive.\nCommon techniques for optimizing computational overhead\ninclude network pruning and quantization [7], which greatly\nreduce storage and computational costs by pruning redundant\nconnections [8] and reducing operations [9]. Despite offering\nnumerous insights and successful methodologies for training\non complex tasks, current SNN models still lack effective\nintegration with brain-inspired mechanisms to achieve a\nbalance between cost and performance.\nGeneration after generation, environmental pressures drive\nbiological neural systems to evolve specific responses to\ncomplex tasks, with neural connections continually adapting\nto encode crucial information into the connectome [10].\nEvolution has endowed approximately 21,000 genes with the\nability to support the complex computing capabilities of the\nbrain's $10^{10}$ neurons and $10^{15}$ synapses. This compact and\nefficient encoding method not only saves biological energy,\nbut also facilitates genetic optimization during evolution,\nthereby supporting complex cognitive functions and highly\nflexible behavioral adaptability. This inspired us to design a\ngene-scaled neuronal coding paradigm for SNNs, controlling\nthe entire network with very few parameters, and simulate\nthe evolutionary process of the brain to optimize the genetic\nencoding.\nBased on this, this paper implements genetically encoded\nneural networks on a variety of common network archi-\ntectures, re-encoding weights with neuronal encoding each\nlayer and global shared gene interaction matrices, greatly\ncompressing parameters and energy consumption. Further-\nmore, to accelerate the evolution and training, we propose\na spatio-temporal evolutionary SNN framework. Spatially,\nthe initial wiring of neuronal encodings and gene interac-\ntion matrix are optimized through the Covariance Matrix\nAdaptation Evolution Strategy (CMA-ES). Temporally, the\ndynamic regularization scale helps solutions transition from\nfreely learning complex features to gradually establishing\nstable functional patterns, ultimately achieving a significant\nreduction in energy consumption without compromising per-\nformance. By integrating principles of neuroevolution, the"}, {"title": "II. RELATED WORK", "content": "This paper proposes a brain-inspired genetically encoded\nspiking neural network, supplemented by a spatio-temporal\nevolution framework to optimize the initial wiring. To this\nend, we first analyze energy-efficient network design tech-\nniques, and then discuss related work on tensor decomposi-\ntion methods and natural evolution strategies, respectively."}, {"title": "A. Efficient Network Designs", "content": "The discrete spiking communication of SNN simulates the\nbehavior of biological neurons, providing a more energy-\nefficient alternative to ANN. Optimization of computational\ncost has attracted research attention and has become an\nimportant trajectory in promoting the development of deep\nlearning. Structurally, the work of pruning SNN structures [8]\nnot only significantly reduces the computational and storage\ncosts by pruning redundant neurons, but also maintained\nthe performance comparable to the ANN models. On the\nother hand, quantization-based methods optimize weights\nby reducing floating-point multiplications [11], [9], reduc-\ning the amount of calculation consumption and balancing\nperformance [7]. From a brain-inspired perspective, this\nwork designs a gene-scaled neuronal coding paradigm that\nsimultaneously optimizes wiring and weights to significantly\nreduce computational cost without losing accuracy."}, {"title": "B. Tensor Decomposition Methods", "content": "Although not inspired by this, the genetic encoding\nadopted in this work is mathematically similar to a kind\nof tensor decomposition methods [12], which decomposes\na high-dimensional tensor into the product of several low-\ndimensional tensors to help extract features and simplify\ndata structures. According to the different decomposition\nmethods, it can be divided into CP decomposition, Tucker\ndecomposition and Tensor-Train (TT) decomposition, etc. Its\nsignificant advantage is that it changes the way to optimize\nand enhance the network by reparameterization, and has\nachieved significant success in many fields of deep learn-\ning [13], [14], [15], [16].\nThere is not much work on applying tensor decomposition\nto optimize SNNs. [17] applies parallel TT decomposition\nto decompose the SNN convolutional layer weights into\nsmaller tensors, reducing the number of parameters, FLOPS\nand training cost, but at the expense of some accuracy. [15]\nintroduces tensor decomposition into the attention module\nand uses CP decomposition to dynamically adjust the rank\nof the attention map in SNN according to the task, but\nit does not involve the structure or encoding of the entire\nnetwork. This paper is inspired by the genetic connection\nmechanism of the brain's neural circuits (rather than the\ntensor decomposition method) which is more consistent with\nthe genetic mechanism of the nervous system."}, {"title": "C. Natural Evolution Strategies", "content": "Natural Evolution Strategies (NES) [18] is a family of\noptimization algorithms that employ gradient-based updates\nto evolve a population of candidate solutions towards better\nperformance on a given task. NES optimizes the parameters\nof a probability distribution to minimize the expected eval-\nuation of solutions, rather than directly seeking the optimal\nsolution to an objective function [19], making it well suited\nfor continuous parameter optimization and fast convergence.\nThis inspired us to adopt NES to optimize the relevant param-\neters of the proposed genetic encoding network, promoting\nthe improvement of evolution efficiency and solution quality."}, {"title": "III. METHOD", "content": "The workings of neural systems in the human brain are\ninfluenced by neuronal connections, learning, and random\nnoise. Existing works have abstracted these mechanisms\nas functions of the interaction between neuronal identity\nand genetic factors [20], [10]. Based on this principle, we\nreconstruct deep spiking neural networks into genetically\nencoded efficient SNNs based on the Leaky Integrate and Fire\n(LIF) neuron model [21]. The framework is shown as Fig. 1.\nAssume the input channels are $C_{in}$, the output channels\nare $C_{out}$, and the kernel size is $k \\times k$. The weights of a\nconventional convolutional layer $W \\in R^{C_{out} \\times C_{in} \\times k \\times k}$. In\nour genetic convolutional layer, the weights are constructed\nbased on the indirectly genetic encoding of input neurons $E_1$\nand output neurons $E_2$ and can be expressed as:\n$W = E_1 G E_2$                                                                  (1)\nwhere $E_1 \\in R^{C_{in} \\times k \\times k \\times g}, E_2 \\in R^{C_{out} \\times k \\times k \\times g}$. G is the gene\ninteraction matrix, $G \\in R^{g \\times g}$. Such genetically encoding\nmethod greatly compresses the overall parameters of the\nnetwork, from the original weight size $C_{out} * C_{in} * k * k$,\nto re-encoded parameters determined by a small genes $g$:\n$Parameters = g * (C_{in}k^2 + g + C_{out}k^2)$                                                       (2)\nIn a deep network, except for the first and last layers, each\nlayer acts both as the output of the previous layer and the"}, {"title": "A. Indirectly Genetic Encoding for Deep SNNs", "content": "input of the next layer. Therefore, the weight of the $i^{th}$\nlayer is encoded by the neuronal encoding $E_i$ of this layer,\nthe neuronal encoding $E_{i+1}$ of the next layer, and the gene\ninteraction matrix G:\n$W_i = E_i G E_{i+1}$                                                                  (3)\nTo calculate the gradient of the loss $\\mathcal{L}$ with respect to $E_i$,\nwe need to apply the chain rule during backpropagation.\nAssume that the output of each layer is expressed as $X_{i+1} =$\n$f(W_iX_i)$, where f is the activation function. Let $\\delta_i$ represent\nthe gradient of the loss with respect to the output of $i^{th}$ layer.\nThe gradient of the weights with respect to the parameters\n$E_i$ is $GE_1$ and the gradient of the output of layer i + 1\nwith respect to the weights is $f'(W_iX_i)X_i$, where f is\nthe activation function and $X_i$ is the input to the $i^{th}$ layer.\nTherefore, for the last layer, the gradient of the weights with\nrespect to $E_{i+1}$ is $G^T E_i^T \\delta_i$. Therefore, the gradient is updated\nas:\n$E_i \\leftarrow E_i - \\eta * (\\delta_i f'(E_{i-1} G E_i X_{i-1}) X_i^T (G^T E_{i-1}^T))$                                                   (4)\nwhere $\\eta$ is the learning rate. For all previous layers, the\nparameters $E_i$ can be updated using gradient descent as\nfollows:\n$E_i \\leftarrow E_i - \\eta * (\\delta_{i+1} f'(E_i G E_{i+1} X_i) X_i^T (G E_{i+1}))$                                                (5)\nTheoretically, any $m \\times n$ matrix $W$ can be expressed\nthrough Singular Value Decomposition (SVD) as:\n$W = U \\Sigma V^T$                                                                      (6)\nwhere U is an $m \\times m$ orthogonal matrix, V is an $n \\times n$\northogonal matrix, and $\\Sigma$ is an $m \\times n$ diagonal matrix, with\nthe diagonal elements being the singular values. If the rank\nof W is $r_w$, then $\\Sigma$ contains only $r_w$ non-zero singular\nvalues. For a low-rank approximation, one can select the top\nr' largest singular values, where $r' < r_w$, and construct the\napproximate matrix:\n$W' = U' \\Sigma' V'^T$                                                                   (7)\nwhere $U'$ and $V'$ are the columns of U and V corresponding\nto the largest r' singular values, and $\\Sigma'$ is a r\u02b9 \u00d7 r' diagonal\nmatrix. If we define $E_1$ as $U' \\Sigma'^{1/2}$, $E_2$ as $\\Sigma'^{1/2} V'^T$, and\n$G$ as any $r' \\times r'$ matrix that encapsulates gene interactions,\nthen:\n$E_1 G E_2 = (U' \\Sigma'^{1/2}) G (\\Sigma'^{1/2} V'^T) = U' (\\Sigma'^{1/2} G \\Sigma'^{1/2}) V'^T$                               (8)\nThus, the gene encoding $E_1 G E_2$ provides a way to incor-\nporate gene interactions encoded in $G$ into the low-rank\napproximation of W. The weights approximated in this way\nare able to capture more complex behaviors or structures,\nsince G allows the approximate representation of W to be\nenriched and adjusted in a low-rank format."}, {"title": "B. Spatial-temporal Dynamic Evolution", "content": "As the network becomes deeper, multiple matrix multi-\nplications and indirect updates will lead to slow training\nprogress or even vanishing gradients. However, it is very\ndifficult to optimize all neuronal encodings because of the\nhigh dimensionality. Therefore, we choose to indirectly con-\ntrol the solution by evolving the distribution of the initial $E_i$\nand the global-shared gene interactions $G_i$, which signifi-\ncantly reduces the number of parameters and improves the\nefficiency of evolution. Each phenotype {E, E...E, Gi}\nrepresents an architecture, including the gene interactions\n$G_i$ and gene encoding $E_i$ of each layer, depending on the\ndepth l of the specific architecture type. We set the genotype\nto be evolved $s_j$ to {$\\beta_1, \\beta_2, G_i$}, where $\\beta_1, \\beta_2$ determine\na truncated normal distribution with mean 0 and standard\ndeviation $\\beta_1$, truncated to the interval $[-\\beta_2, \\beta_2]$:\n$E_{i} = \\begin{cases} -\\beta_2 & \\text{if } A < -\\beta_2 \\\\ A & \\text{if } -\\beta_2 \\leq A \\leq \\beta_2 \\\\ \\beta_2 & \\text{if } A > \\beta_2 \\end{cases}$                                                             (9)\nwhere $A \\sim N(0, \\beta_1^2)$."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we verify the effectiveness of the pro-\nposed method on CIFAR10 [30], CIFAR100 [31] and Im-\nageNet [32] datasets. To be fair, we apply GEE on mul-\ntiple commonly used network architectures VGGNet [24],\nResNet [33], CIFARNet [6], and conduct comprehensive\ncomparisons with various models to demonstrate the high\nefficiency, scalability, and robustness of GEE."}, {"title": "A. Comparative Results", "content": "Through genetic re-encoding and evolution, the proposed\nmodel can capture complex interactions between input and\noutput features more effectively. We compare the number of\nparameters and accuracy of the evolved candidate solutions\non ImageNet and CIFAR10/CIFAR100, as shown in Table I\nand Table II respectively. In ImageNet, in addition to pa-\nrameter size, timesteps and accuracy, we also calculate the"}, {"title": "C. Ablation Study", "content": "Gene Scale. In the proposed gene-encoded SNN, the\nnumber of genes affects the encoding ability of the network.\nIn theory, the more genes there are, the more complex infor-\nmation patterns can be captured. Therefore, we verify it on\nCIFAR10 and CIFAR100, as shown in Fig. 3. As the number\nof genes increases, the proposed genetically encoded method\n(in VGGNet, CIFARNet, and ResNet) has been empirically\nproven to enhance classification performance on CIFAR10\nand CIFAR100. As the intermediate representations become\nricher, allowing the model to express more intricate fea-\nture combinations and improve classification performance,\ndemonstrating excellent scalability and flexibility. By adjust-\ning g, the model can be extended to improve performance,\nas shown in Table II. Even if the number of genes has been\ncompressed to a very small number, only about 30% of the\nparameters of the model with the same architecture [25],\nGEE still achieves 2% and 1.6% improvement on the two\ndatasets. Different genetic scales show the same performance\nadvantages in the model, which shows that it is able to adapt\nto different complexity, thus highlighting the effectiveness\nand practicality.\nRegularization Term. In order to test the proposed reg-\nularization terms of STE, we fix the number of genes to\n150 and construct four ablation models: Random model (the\ninitial X, O follows $X_i, O \\sim N(0,1)$ without evolution),\nBaseline ($f_i = \\mathcal{L}$), Baseline + $r_1$ ($f_i = \\mathcal{L}+r_1$), Baseline +\n$r_2$ ($f_i = \\mathcal{L}+r_2$) and STE ($f_i = \\mathcal{L}+r_1+r_2$). The comparison"}, {"title": "D. Evolutionary Efficiency", "content": "To further evaluate the efficiency of the proposed STE,\nwe compared the time required for model Baseline (using\nonly the training loss as the optimization objective) and STE\nwithin 5 generations. We extend the training epochs of Base-\nline to 10, while STE remains unchanged. The experiments\nare conducted on a system equipped with seven NVIDIA\nA100 GPUs and an AMD EPYC 7H12 64-Core Processor,\nrunning on a Linux environment with Ubuntu 20.04. The\nresults show that after extending the training time, Baseline\nimproves the accuracy of the evolved solution by 1.2% on\nthe original basis, but still lags behind STE by 0.84%. In\naddition, the evolution time required for STE is about five gpu\nhours, while Baseline requires 25 gpu hours, which is five\ntimes that of STE. We visualize the evolution of $\\beta_1$ and $\\beta_2$"}, {"title": "V. CONCLUSION", "content": "This study introduces a genetically encoded evolutionary\nspiking neural network that leverages a compact, gene-scaled\nneuronal coding scheme to effectively reduce the parameter\ncount and computational demands typical of conventional\nSNNs. By embedding a spatio-temporal evolution framework,\nthe model not only improves the quality of the solution but\nalso significantly reduces energy consumption. The dynamic\nregularization mechanism improves the solution's ability to\neffectively evolve and stabilize functional neuronal patterns.\nEmpirical validation on CIFAR10, CIFAR100, and ImageNet\nshows that GEE not only reduces computational overhead,\nbut also maintains efficiency compared to traditional meth-\nods, demonstrating the potential of brain-inspired architec-\ntures to achieve high efficiency and low power consumption."}]}