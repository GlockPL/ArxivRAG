{"title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems", "authors": ["Maksim Zhdanov", "Max Welling", "Jan-Willem van de Meent"], "abstract": "Large-scale physical systems defined on irregular\ngrids pose significant scalability challenges for\ndeep learning methods, especially in the presence\nof long-range interactions and multi-scale cou-\npling. Traditional approaches that compute all\npairwise interactions, such as attention, become\ncomputationally prohibitive as they scale quadrati-\ncally with the number of nodes. We present Erwin,\na hierarchical transformer inspired by methods\nfrom computational many-body physics, which\ncombines the efficiency of tree-based algorithms\nwith the expressivity of attention mechanisms.\nErwin employs ball tree partitioning to organize\ncomputation, which enables linear-time attention\nby processing nodes in parallel within local neigh-\nborhoods of fixed size. Through progressive\ncoarsening and refinement of the ball tree struc-\nture, complemented by a novel cross-ball inter-\naction mechanism, it captures both fine-grained\nlocal details and global features. We demonstrate\nErwin's effectiveness across multiple domains,\nincluding cosmology, molecular dynamics, and\nparticle fluid dynamics, where it consistently out-\nperforms baseline methods both in accuracy and\ncomputational efficiency.", "sections": [{"title": "1. Introduction", "content": "Scientific deep learning is tackling increasingly computa-\ntionally intensive tasks, following the trajectory of computer\nvision and natural language processing. Applications range\nfrom molecular dynamics (MD) (Arts et al., 2023), compu-\ntational particle mechanics (Alkin et al., 2024b), to weather\nforecasting (Bodnar et al., 2024), where simulations often\ninvolve data defined on irregular grids with thousands to\nmillions of nodes, depending on the required resolution and\ncomplexity of the system.\nSuch large-scale systems pose a significant challenge to ex-\nisting methods that were developed and validated at smaller\nscales. For example, in computational chemistry, models are\ntypically trained on molecules with tens of atoms (Kov\u00e1cs\net al., 2023), while molecular dynamics simulations can\ninvolve well beyond thousands of atoms. This scale dispar-\nity might result in prohibitive runtimes that render models\ninapplicable in high-throughput scenarios such as protein\ndesign (Watson et al., 2023) or screening (Fu et al., 2022).\nA key challenge in scaling to larger system sizes is that\ncomputational methods which work well at small scales\nbreak down at larger scales. For small systems, all pairwise\ninteractions can be computed explicitly, allowing deep learn-\ning models to focus on properties like equivariance (Cohen\n& Welling, 2016). However, this brute-force approach be-\ncomes intractable as the system size grows. At larger scales,\napproximations are required to efficiently capture both long-\nrange effects from slowly decaying potentials or multi-scale\ncoupling (Majumdar et al., 2020). As a result, models val-\nidated only on small systems often lack the architectural\ncomponents necessary for efficient scaling."}, {"title": "2. Related Works: sub-quadratic attention", "content": "One way to avoid the quadratic cost of self-attention is to lin-\nearize attention by performing it on non-overlapping patches.\nFor data on regular grids, like images, the SwinTransformer\n(Liu et al., 2021) achieves this by limiting attention to local\nwindows with cross-window connection enabled by shift-\ning the windows. However, for irregular data such as point\nclouds or non-uniform meshes, one first needs to induce a\nstructure that will allow for patching. Several approaches\n(Liu et al., 2023; Sun et al., 2022) transform point clouds\ninto sequences, most notably, PointTransformer v3 (Wu\net al., 2024), which projects points into voxels and orders\nthem using space-filling curves (e.g., Hilbert curve). While\nscalable, these curves introduce artificial discontinuities that\ncan break local spatial relationships.\nParticularly relevant to our work are hierarchical attention\nmethods. In the context of 1D sequences, approaches like\nthe H-transformer (Zhu & Soricut, 2021) and Fast Multipole\nAttention (Kang et al., 2023) approximate self-attention\nthrough multi-level decomposition: tokens interact at full\nresolution locally while distant interactions are computed\nusing learned or fixed groupings at progressively coarser\nscales. For point clouds, OctFormer (Wang, 2023) converts\nspatial data into a sequence by traversing an octree, ensuring\nspatially adjacent points are consecutive in memory. While\nconceptually similar to our approach, OctFormer relies on\ncomputationally expensive octree convolutions, whereas our\nutilization of ball trees leads to significant efficiency gains.\nRather than using a hierarchical decomposition, another\nline of work proposes cluster attention (Janny et al., 2023;\nAlkin et al., 2024a). These methods first group points into\nclusters and aggregate their features at the cluster centroids\nthrough message passing or cross-attention. After comput-\ning attention between the centroids, the updated features\nare then distributed back to the original points. While these\napproaches achieve the quadratic cost only in the number\nof clusters, they introduce an information bottleneck at the\nclustering step that may sacrifice fine-grained details and\nfail to capture features at multiple scales - a limitation our\nhierarchical approach aims to overcome."}, {"title": "3. Background", "content": "Our work revolves around attention, which we aim to lin-\nearize by imposing structure onto point clouds using ball\ntrees. We formally introduce both concepts in this section."}, {"title": "3.1. Attention", "content": "The standard self-attention mechanism is based on the scaled\ndot-product attention (Vaswani et al., 2017). Given a set X\nof N input feature vectors of dimension C', self-attention is\ncomputed as\nQ, K, V = XWq, XWk, XWv\nAtt(Q, K, V) = softmax \\bigg( \\frac{QK^T}{\\sqrt{C}} + B \\bigg)V\nwhere Wq, Wk, Wv \u2208 RC\u00d7C' are learnable weights and\nBERNN is the bias term.\nMulti-head self-attention (MHSA) improves expressivity\nby computing attention H times with different weights and"}, {"title": "3.2. Ball tree", "content": "A ball tree is a hierarchical data structure that recursively par-\ntitions points into nested sets of equal size, where each set is\nrepresented by a ball that covers all the points in the set. As-\nsume we operate on the d-dim. Euclidean space (Rd, || \u00b7 ||2)\nwhere we have a point cloud (set) P = {p1, \u2026, pn}C Rd.\nDefinition 3.1 (Ball). A ball is a region bounded by a hy-\npersphere in Rd. Each ball is represented by the coordinates\nof its center c \u2208 Rd and radius r \u2208R+:\nB = B(c,r) = {z\u2208 Rd | ||z - C||2 \u2264 r}.\nWe will omit the parameters (c, r) for brevity from now on.\nDefinition 3.2 (Ball Tree). A ball tree T on point set P is a\nhierarchical sequence of partitions {L0, L1, ..., Lm}, where\neach level Li consists of disjoint balls that cover P. At the\nleaf level i = 0, the nodes are the original points:\nLo = {{Pj} | Pj \u2208 P}\nFor each subsequent level i > 0, each ball B \u2208 Li is formed\nby merging two balls at the previous level B1, B2 \u2208 Li\u22121:\nL\u2081 = {{B\u2081 U B2} | B1, B2 \u2208 Li-1}\nsuch that its center is computed as the center of mass:\nCB = \\frac{|B_1| C_1+|B_2| C_2}{|B_1|+|B_2|}\nand its radius is determined by the furthest point it contains:\nr\u0432 = \u0442\u0430\u0445{||p - CB ||2 | p \u2208 B\u2081 U B2}\nwhere |B| denotes the number of points contained in B.\nTo construct the ball tree, we recursively split the data points\ninto two sets starting from P. In each recursive step, we find\nthe dimension of the largest spread (i.e. the max-min value)\nand split at its median (Pedregosa et al., 2012), constructing\ncovering balls per Def.3.2. For details, see Appendix Alg.11."}, {"title": "3.2.1. BALL TREE PROPERTIES", "content": "In the context of our method, there are several properties of\nball trees that enable efficient hierarchical partitioning:\nProposition 3.3 (Ball Tree Properties). The ball tree T\nconstructed as described satisfies the following properties:\n1. The tree is a perfect binary tree.\n2. At each level i, each ball contains exactly 2\u00ba leaf nodes.\n3. Balls at each level cover the point set\n\\bigcup_{B\\in L_i} B = P\\quad\\forall i \\in \\{0, ..., m\\}.\nProposition 3.4 (Contiguous Storage). For a ball tree\nT = {L0, L1, ..., Lm} on point cloud P = {P1,..., Pn},\nthere exists a bijective mapping \u03c0 : {1, ..., n} \u2192 {1, ..., n}\nsuch that points belonging to the same ball B \u2208 Li have\ncontiguous indices under \u03c0.\nAs a corollary, the hierarchical structure at each level can\nbe represented by nested intervals of contiguous indices:"}, {"title": "4. Erwin Transformer", "content": "Following the notation from the background Section 3.2,\nwe consider a point cloud P = {P1,..., Pn} C Rd. Addi-\ntionally, each point is now endowed with a feature vector\nyielding a feature set X = {x1, ..., xn} CRC.\nOn top of the point cloud, we build a ball tree T =\n{Lo, ..., Lm}. We initialize Lleaf := Lo to denote the cur-\nrent finest level of the tree. As each leaf node contains a\nsingle point, it inherits its feature vector:\nXleaf = {XB = xi | B = {pi} \u2208 Lleaf}\n4.1. Ball tree attention\nBall attention For each ball attention operator, we specify\na level k of the ball tree where each ball B \u2208 Lk contains 2k\nleaf nodes. The choice of k presents a trade-off: larger balls\ncapture longer-range dependencies, while smaller balls are\nmore resource-efficient. For each ball B \u2208 Lk, we collect\nthe leaf nodes within B\nleaves = {B' \u2208 Lleaf | B' \u2286 B}\nalong with their features from Xleaf\nXB = {XB' \u2208 Xleaf | B' \u2208 leavesB}\nWe then compute self-attention independently on each ball2:\nXB = BAtt(XB) := Att(XBWq, XBWk,XBWv)\nwhere weights are shared between balls and the output XB\nmaintains row correspondence with XB.\nComputational cost As attention is computed indepen-\ndently for each ball B \u2208 Lk, the computational cost is\nreduced from quadratic to linear. Precisely, for ball atten-\ntion, the complexity is O(|B|2 \u00b7 B), i.e. quadratic in the\nball size and linear in the number of balls:\nPositional encoding We introduce positional information\nto the attention layer in two ways. First, we augment the fea-\ntures of leaf nodes with their relative positions with respect\nto the ball's center of mass (relative position embedding):\n\u0425\u0432 = \u0425\u0432 + (PB - cB)Wpos\nwhere PB contains positions of leaf nodes, CB is the center\nof mass, and Wpos is a learnable projection. This allows\nthe layer to incorporate geometric structure within each ball.\nSecond, we introduce a distance-based attention bias:\nBB = \u2212\\frac{1}{\\sigma^2}||\u0441\u0432\u02b9 \u2013 \u0421\u0412''||2, \u0412', \u0412'' \u2208 leaves\nwith a learnable parameter \u03c3\u2208 R (Wessels et al., 2024).\nThe term decays rapidly as the distance between two nodes\nincreases which enforces locality and helps to mitigate po-\ntential artifacts from the tree building, particularly in cases\nwhere distant points are grouped together.\nCross-ball connection To increase the receptive field of\nour attention operator, we implement cross-ball connections\ninspired by the shifted window approach in Swin Trans-\nformer (Liu et al., 2021). There, patches are displaced\ndiagonally by half their size to obtain two different im-\nage paTreertitioning configurations. This operation can be\nequivalently interpreted as keeping the patches fixed while\nsliding the image itself.\nFollowing this interpretation, we rotate the point cloud and\nconstruct the second ball tree Trot = {Lot, ..., Lot} which\ninduces a permutation wrot of leaf nodes (see Fig. 3, center).\nWe can then compute ball attention on the rotated config-\nuration by first permuting the features according to wrot,\napplying attention, and then permuting back:\n\u03a7\u03b2 = \u03c0rot (BAtt (\u03c0rot (XB)))\nBy alternating between the original and rotated configura-\ntions in consecutive layers, we ensure the interaction be-\ntween leaf nodes in otherwise separated balls.\nTree coarsening/refinement For larger systems, we are\ninterested in coarser representations to capture features at"}, {"title": "4.2. Model architecture", "content": "We are now ready to describe the details of the main model\nto which we refer as Erwin - a hierarchical\ntransformer operating on ball trees.\nEmbedding At the embedding phase, we first construct\na ball tree on top of the input point cloud and pad the leaf\nlayer to complete the tree, as described in Section 3.2. To\ncapture local geometric features, we employ a small-scale\nMPNN, which is conceptually similar to PointTransformer's\nembedding module using sparse convolution. When input\nconnectivity is not provided (e.g. mesh), we utilize the ball\ntree structure for a fast nearest neighbor search.\nErwinBlock The core building block of Erwin follows a\nstandard pre-norm transformer structure: LayerNorm fol-\nlowed by ball attention with a residual connection, and a\nSwiGLU feed-forward network (Shazeer, 2020). For the\nball attention, the size 2k of partitions is a hyperparameter.\nTo ensure cross-ball interaction, we alternate between the\noriginal and rotated ball tree configurations, using an even\nnumber of blocks per ErwinLayer in our experiments.\nOverall architecture Following a UNet structure (Ron-\nneberger et al., 2015; Wu et al., 2024), Erwin processes\nfeatures at multiple scales through encoder and decoder\npaths (Fig. 3, right). The encoder progressively coarsens the\nball tree while increasing feature dimensionality to maintain\nexpressivity. The coarsening factor is a hyperparameter that\ntakes values that are powers of 2. At the decoder stage,\nthe representation is refined back to the original resolution,\nwith skip connections from corresponding encoder levels\nenabling multi-scale feature integration."}, {"title": "5. Experiments", "content": "Implementation details for all experiments are given in Ap-\npendix C. The code is available at anonymized link.\nExtended experiments are given in Appendix B, including\nan additional experiment on airflow pressure modelling.\nComputational cost To experimentally evaluate Erwin's\nscaling, we learn the power-law form Runtime = C \u2022 n\u03b2\nby first applying the logarithm transform to both sides and\nthen using the least square method to evaluate B. The result\nis an approximately linear scaling with \u03b2 = 1.054 with\nR2 = 0.999, see Fig. 5, left. Ball tree construction accounts\nfor only a fraction of the overall time, proving the efficiency\nof our method for linearizing attention for point clouds.\nReceptive field One of the theoretical properties of our\nmodel is that with sufficiently many layers, its receptive\nfield is global. To verify this claim experimentally, for an\narbitrary target node, we run the forward pass of Erwin\nand MPNN and compute gradients of the node output with\nrespect to all input nodes' features. If the gradient is non-\nzero, the node is considered to be in the receptive field of\nthe target node. The visualization is provided in Fig. 5,\nright, where we compare the receptive field of our model\nwith that of MPNN. As expected, the MPNN has a limited\nreceptive field, as it cannot exceed N hops, where N is\nthe number of message-passing layers. Conversely, Erwin\nimplicitly computes all-to-all interactions, enabling it to\ncapture long-range interactions in data."}, {"title": "5.1. Cosmological simulations", "content": "To demonstrate our model's ability to capture long-range\ninteractions, we use the cosmology benchmark (Balla et al.,\n2024) which consists of large-scale point clouds represent-\ning potential galaxy distributions.\nDataset The dataset is derived from N-body simulations\nthat evolve dark matter particles from the early universe\nto the present time. After the simulation, gravitationally\nbound structures (halos) are indicated, from which the 5000\nheaviest ones are selected as potential galaxy locations. The\nhalos form local clusters through gravity while maintaining\nlong-range correlations that originated from interactions in\nthe early universe before cosmic expansion, reflecting the\ninitial conditions of simulations.\nTask The input is a point cloud X \u2208 R5000\u00d73, where\neach row corresponds to a galaxy and column to x, y, z\ncoordinate respectively. The task is a regression problem\nto predict the velocity of every galaxy Y \u2208 R5000\u00d73. We\nvary the size of the training dataset from 64 to 8192, while\nthe validation and test datasets have a fixed size of 512. The\nmodels are trained using mean squared error loss\nL = MSE(\u0176, Y)\nbetween predicted and ground truth velocities.\nResults The results are shown in Fig. 6. We compare\nagainst multiple equivariant (NequIP (Batzner et al., 2021),\nSEGNN (Brandstetter et al., 2022)) and non-equivariant\n(MPNN (Gilmer et al., 2017), PointTransformer v3 (Wu\net al., 2024)) baselines. In the small data regime, graph-\nbased equivariant models are preferable. However, as the\ntraining set size increases, their performance plateaus. We\nnote that this is also the case for non-equivariant MPNN,\nsuggesting that the issue might arise from failing to capture\nmedium to large-scale interactions, where increased local\nexpressivity of the model has minimal impact. Conversely,\ntransformer-based models scale favorably with the training\nset size and eventually surpass graph-based models, high-\nlighting their ability to capture both small and large-scale\ninteractions. Our model demonstrates particularly strong\nperformance and significantly outperforms other baselines\nfor larger training set sizes."}, {"title": "5.2. Molecular dynamics", "content": "Molecular dynamics (MD) is essential for understanding\nphysical and biological systems at the atomic level but re-\nmains computationally expensive even with neural network\npotentials due to all-atom force calculations and femtosec-\nond timesteps required to maintain stability and accuracy.\nFu et al. (2022) suggested accelerating MD simulation\nthrough coarse-grained dynamics with MPNN. In this ex-\nperiment, we take a different approach and instead operate\non the original representation but improve the runtime by\nemploying our hardware-efficient model. Therefore, the\nquestion we ask is how much we can accelerate a simulation\nw.r.t. an MPNN without compromising the performance.\nDataset The dataset consists of single-chain coarse-\ngrained polymers (Webb et al., 2020; Fu et al., 2022) sim-\nulated using MD. Each system includes 4 types of coarse-\ngrained beads interacting through bond, angle, dihedral, and\nnon-bonded potentials. The training set consists of polymers\nwith the repeated pattern of the beads while the polymers in\nthe test set are constructed by randomly sampling sequences\nof the beads thus introducing a challenging distribution\nshift. The training set contains 100 short trajectories (50k\nT), while the test set contains 40 trajectories that are 100\ntimes longer. Each polymer chain contains approximately\n890 beads on average.\nTask We follow the experimental setup from Fu et al.\n(2022). The model takes as input a polymer chain of N\ncoarse-grained beads. Each bead has a specific weight and\nis associated with the history {Xt\u221216\u2206t, ..., \u0130t-t} of (nor-\nmalized) velocities from 16 previous timesteps at intervals\nof \u2206t = 57. The model predicts the mean \u00b5\u03b5 \u2208 RN\u00d73\nand variance of \u2208 RY\u00d73 of (normalized) acceleration for\neach bead, assuming a normal distribution. We train using\nnegative log-likelihood loss\nL = - log N(xt|\u00b5\u03b5, \u03c3\u03b5)"}, {"title": "5.3. Turbulent fluid dynamics", "content": "In the last experiment, we demonstrate the expressivity of\nour model by simulating turbulent fluid dynamics. The prob-\nlem is notoriously challenging due to multiple factors: the\ninherently nonlinear behaviour of fluids, the multiscale and\nchaotic nature of turbulence, and the presence of long-range\ndependencies. Moreover, the geometry of the simulation do-\nmain and the presence of objects introduce complex bound-\nary conditions thus adding another layer of complexity.\nDataset We use EAGLE (Janny et al., 2023), a large-scale\nbenchmark of unsteady fluid dynamics. Each simulation\nincludes a flow source (drone) that moves in 2D environ-\nments with different boundary geometries producing air-\nflow. The time evolution of velocity and pressure fields\nis recorded along with dynamically adapting meshes. The\ndataset contains 600 different geometries of 3 types, with\napproximately 1.1 million 2D meshes averaging 3388 nodes\neach. The total dataset includes 1184 simulations with 990\ntime steps per simulation. The dataset is split with 80% for\ntraining and 10% each for validation and testing."}, {"title": "5.4. Ablation study", "content": "We also conducted an ablation study to examine the effect\nof increasing ball sizes on the model's performance in the\ncosmology experiment, see Table 1. Given the presence of\nlong-range interactions in the data, larger window sizes (and\nthus receptive fields) improve model performance, albeit at"}, {"title": "6. Conclusion", "content": "We present Erwin, a hierarchical transformer that uses ball\ntree partitioning to process large-scale physical systems with\nlinear complexity. Erwin achieves state-of-the-art perfor-\nmance on both the cosmology benchmark (Balla et al., 2024)\nand the EAGLE dataset (Janny et al., 2023), demonstrating\nits effectiveness across diverse physical domains. The effi-\nciency of Erwin makes it a suitable candidate for any tasks\nthat require modeling large particle systems, such as tasks in\ncomputational chemistry (Fu et al., 2024) or diffusion-based\nmolecular dynamics (Jing et al., 2024).\nLimitations and Future Work Because Erwin relies on\nperfect binary trees, we need to pad the input set with virtual\nnodes, which induces computational overhead for ball atten-\ntion computed over non-coarsened trees (first ErwinBlock).\nThis issue can be circumvented by employing learnable\npooling to the next level of the ball tree, which is always\nfull, ensuring the remaining tree is perfect. Whether we can\nperform such pooling without sacrificing expressivity is a\nquestion that we leave to future research.\nErwin was developed by jointly optimizing for expressivity\nand runtime. As a result, certain architectural decisions\nare not optimal with respect to memory usage. In partic-\nular, we use a distance-based attention bias (see Eq. 10),\nfor which both computational and memory requirements\ngrow quadratically with the ball size. Developing alterna-\ntive ways of introducing geometric information into atten-\ntion computation could reduce these requirements. Finally,\nErwin is neither permutation nor rotation equivariant, al-\nthough rotation equivariance can be incorporated without\ncompromising scalability. One possible approach is to use\ngeometric algebra transformers (Brehmer et al., 2023) and\nomit the proposed cross-ball connections, as they rely on\ninvariance-breaking tree building."}, {"title": "A. Implementation details", "content": "Ball tree construction The algorithm used for construct-\ning ball trees (Pedregosa et al., 2012) can be found in Alg. 1.\nNote that this implementation is not rotationally equivariant\nas it relies on choosing the dimension of the greatest spread\nwhich in turn depends on the original orientation. Examples\nof ball trees built in our experiments are shown in Fig. 9.\nMPNN in the embedding Erwin employs a small-scale\nMPNN in the embedding. More precisely, given a graph\nG = (V, E) with nodes vi \u2208 V and edges eij \u2208 E, we\ncompute multiple layers of message-passing as proposed in\n(Gilmer et al., 2017):\nmij = MLPe (hi, hj, pi - pj),\nmi = \\sum_{j\\in N(i)}mij,\nhi = MLPh(hi, mi),\nwhere h\u2081 \u2208 RH is a feature vector of vi, N(i) denotes the\nneighborhood of vi. The motivation for using an MPNN\nis to incorporate local neighborhood information into the\nmodel. Theoretically, attention should be able to capture it\nas well; however, this might require substantially increasing\nfeature dimension and the number of attention heads, which\nwould be prohibitively expensive for a large number of\nnodes in the original level of a ball tree."}, {"title": "B. Extended experiments", "content": "B.1. Turbulent fluid dynamics\nWe provide additional exemplary rollouts of Erwin for both\nvelocity (Fig. 10) and pressure (Fig. 11) fields.\nB.2. Airflow pressure modeling\nDataset We use the ShapeNet-Car dataset generated by\nUmetani & Bickel (2018) and preprocessed by Alkin et al.\n(2024a). It consists of 889 car models, each car being\nrepresented by 3586 surface points in 3D space. Airflow\nwas simulated around each car for 10s (Reynolds number\nRe = 5 \u00d7 10%) and averaged over the last 4s to obtain pres-\nsure values at each point. The dataset is randomly split into\n700 training and 189 test samples.\nTask Given surface points, the task is to predict the value\nof pressure P\u2208 RN\u00d71 at each point in XN\u00d73. The training\nis done by optimizing the mean squared error loss between\npredicted and ground truth pressures.\nResults The results are given in Table 4. We evaluate\nPointTransformer v3 and use the baseline results obtained\nby Alkin et al. (2024a) for U-Net (Ronneberger et al., 2015),\nFNO (Li et al., 2021), GINO (Li et al., 2023), and UPT\n(Alkin et al., 2024a). Both Erwin and PointTransformer\nv3 achieve significantly lower test MSE compared to other\nmodels, which can be attributed to their ability to capture\nfine geometric details by operating directly on the original\npoint cloud. In comparison, other approaches introduce\ninformation loss through compression - UPT encodes the\nmesh into a latent space representation, while the remain-\ning baselines interpolate the geometry onto regular grids\nand back. Moreover, in our experiments, ResNet-like con-\nfigurations that did not include any coarsening performed\ndramatically better than the ones following the U-Net struc-\nture. Overall, this result highlights the potential for Erwin\nto be used as a scalable neural operator (Wang et al., 2024)."}, {"title": "C. Experimental details", "content": "In this section, we provide experimental details regarding\nhyperparameter choice and optimization. All experiments\nwere conducted on a single NVIDIA RTX A6000. All mod-\nels were trained using the AdamW optimizer (Loshchilov\n& Hutter, 2019) with weight decay 0.01 and a cosine decay\nschedule. The learning rate was tuned in the range 10-4 to\n10-3 to minimize loss on the respective validation sets.\nCosmological simulations We follow the experimental\nsetup of the benchmark. The training was done for 5000\nepochs with batch size 16 for point transformers and batch\nsize 8 for message-passing-based models. The implementa-\ntion of SEGNN, NequIP and MPNN was done in JAX and\ntaken from the original benchmark repository (Balla et al.,\n2024). We maintained the hyperparameters of the baselines\nused in the benchmark. For Erwin and PointTransformer,\nthose are provided in Table 5. In Erwin's embedding, we\nconditioned messages on Bessel basis functions rather than\nthe relative position, which significantly improved overall\nperformance.\nMolecular dynamics All models were trained with batch\nsize 32 for 50000 training iterations with an initial learning\nrate of 5\u00b710-4. We finetuned the hyperparameters of every\nmodel on the validation dataset (reported in Table 7).\nTurbulent fluid dynamics Baseline results are taken from\n(Janny et al., 2023), except for runtime and peak memory\nusage, which we measured ourselves. Erwin was trained\nwith batch size 12 for 4000 epochs."}]}