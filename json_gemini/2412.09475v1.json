{"title": "New keypoint-based approach for recognising British Sign Language (BSL) from sequences", "authors": ["Oishi Deb", "KR Prajwal", "Andrew Zisserman"], "abstract": "In this paper, we present a novel keypoint-based classification model designed to recognise British Sign Language (BSL) words within continuous signing sequences. Our model's performance is assessed using the BOBSL dataset, revealing that the keypoint-based approach surpasses its RGB-based counterpart in computational efficiency and memory usage. Furthermore, it offers expedited training times and demands fewer computational resources. To the best of our knowledge, this is the inaugural application of a keypoint-based model for BSL word classification, rendering direct comparisons with existing works unavailable.", "sections": [{"title": "1. Introduction", "content": "Sign languages, having evolved within deaf communities, are unique visual languages with rich grammatical structures and lexicons, often displaying complexities similar to spoken languages. In our study, we introduce an innovative keypoint-based approach for recognizing words from British Sign Language (BSL), which is the primary language used within the British deaf community.\nOur research aims to leverage the power of keypoint representations, which provide valuable insights into the body gestures and expressions used in BSL. By utilizing this approach, we seek to enhance the accessibility and efficiency of BSL recognition, contributing to improved communication for the deaf community. The dataset used in our study encompasses a large-scale collection of diverse BSL expressions, enabling comprehensive training and evaluation of our proposed model. Through this work, we hope to make significant strides in promoting computationally efficient models for British sign language recognition and promoting inclusivity for individuals with hearing impairments.\nInstead of directly utilizing RGB videos, we explore the use of 2D keypoints, which represent specific points in the face, right hand, left hand and pose. There are three primary reasons for adopting this approach. Firstly, we can easily control the information used by including or excluding subsets of keypoints. For example, instead of using a facial mesh, we can easily reduce the number of facial key-points to only eyes, lips and the outer face circumference, and therefore reducing the computational cost. Secondly, keypoints eliminate many extraneous factors, such as lighting and clothing, providing a more compact representation compared to images. Consequently, training and exploring models become more manageable. Lastly, keypoints can be computed at frame rate, enabling real-time execution of models for the task.\nRecognising signing words from continuous signing clips is a very challenging task because of co-articulated signing, also known as \"signs in context,\" refers to signing that demonstrates variations in sign form influenced by the signs that come immediately before or after, or signs articulated simultaneously. To develop resilient models capable of comprehending sign language in real-world settings, it is essential to effectively recognise and account for co-articulated signs.\nIn this paper, our primary contribution is the development of a keypoint-based model designed to identify words from given BSL signing clips. Our evaluations indicate that this keypoint-centric approach substantially outperforms the RGB-based model in computational efficiency, memory usage, and training speed, thus demanding fewer overall computational resources."}, {"title": "2. Literature Review", "content": ""}, {"title": "2.1. Sign Recognition", "content": "There have been multiple works [6] on British Sign Language recognition. [2] presents an approach that aims to recognise coarticulated signs, which are signs that blend or flow into each other, using mouthing cues. Recognising that signs are often influenced or modified by the simultaneous mouthing of English words, the study incorporates these cues to scale up recognition systems. [1] explores the importance of lip patterns in understanding sign language. BSL, like many sign languages, incorporates facial expressions and mouth movements as integral components. This study aims to leverage machine learning techniques to accurately recognise and interpret these lip patterns.\nHowever, all the previous work uses RGB videos [16], this work takes a different approach using keypoint representations, as explained below."}, {"title": "2.2. Keypoint Representation", "content": "In recent years, there has been a shift in the methods used for estimating human body keypoints. Initially, manual feature engineering was the primary approach, but more recent advancements have embraced deep learning architectures such as Convolutional Neural Networks (CNNs) to estimate more accurate keypoints for human-body [7, 8, 11, 12, 15]. Various studies have explored the utilization of keypoints as inputs for different tasks, and one popular approach involves constructing Graph Neural Networks (GNNs) based on keypoint representations.\nGNNs have been applied in tasks like action recognition [20], gesture recognition [10], and sign language segmentation and recognition [6, 4, 13, 18], utilizing keypoints as inputs. These studies have demonstrated remarkable outcomes, highlighting the effectiveness of employing keypoint representations. In contrast, our research takes a slightly different approach by directly utilizing keypoint representations as inputs to a Transformer-based model.\nThe Transformer architecture [17], originally proposed for natural language processing tasks, has gained prominence in computer vision applications. By leveraging self-attention mechanisms, Transformers are capable of capturing long-range dependencies and modeling relationships between keypoints effectively. Our work capitalizes on this potential by employing keypoint representations as direct inputs to the Transformer model."}, {"title": "3. Procedure", "content": ""}, {"title": "3.1. Dataset", "content": "BOBSL dataset, which contains a total of 1,467 hours of BBC episodes, was used in this work. Out of various other available sign language datasets [3], we chose the BOBSL dataset due to the following advantages, unlike datasets with isolated signs, BOBSL comprises co-articulated signs, representing a more natural signing style (although distinct from conversational signing due to its use of interpreted content). Additionally, BOBSL is the largest dataset in terms of continuous signing, with a total duration of 1,467 hours. It covers a broad domain of discourse and benefits from automatic annotation for a substantial vocabulary of 8162 signs.\nEach episode is 30 to 60 minutes long, and the videos in the dataset have a resolution of 444 x 444 pixels and a frame rate of 25 fps.\nThe dataset also includes approximately 1.2 million sentences extracted from English subtitles, which cover a vocabulary size of 78,000 English words. It involves a total of 39 signers (interpreters) [3]. To facilitate signer-independent evaluation, we divide the data into three splits: train, validation, and test, ensuring that there is no overlap of signers between these splits.\nThis is a classification problem with 8162 word categories, we have used 8162 unique words for training which corresponds to a total of 3,555,141 frames, and we used 3348 words for the validation set corresponding to a total of 53,768 frames."}, {"title": "3.2. Keypoints Extraction", "content": "In order to obtain the keypoints, we employ Mediapipe [12], a publicly accessible library designed specifically for human pose estimation. The main benefit of utilizing Mediapipe, as opposed to alternative libraries like OpenPose [7], is its capability to extract keypoints in real time, even when running on a CPU. This feature significantly reduces the computational load associated with the process. We extracted 33 pose keypoints, 21 keypoints each for left and right fingers, and finally extracted 468 face keypoints.\nIn most cases, only one person (the BSL interpreter) is present in the frames. In the rare instances where additional people are visible in the background, we only extract the keypoints of the person with the highest confidence using the Mediapipe framework. This ensures that we process a maximum of one signer per frame. We extract keypoints for about 132 Millions frames in total."}, {"title": "3.3. Implementation Details", "content": "First, we extract keypoints from every frame, captured at a frame rate of 25 frames per second. We feed a sequence of 16 consecutive keypoint vectors corresponding to frames into the model. Prior research [19, 5, 14] has observed that co-articulated signs, often referred to as \"signs in context,\" tend to have a duration of approximately 13 - 20 frames.\nThe keypoints, comprising (x, y) coordinates, from 16 consecutive frames are stacked together to form a $[16 \\times K \\times 2]$ three-dimensional vector (where K represents the total number of keypoints per frame). This keypoint vector is then fed into the transformer for sign recognition. Further details of the Transformer architecture are in the next section."}, {"title": "3.3.1 Transformer Architecture", "content": "The Transformer model is composed of several integral components: the Tokenizer, Positional Encoding, Encoder, Multi-Headed Attention, Position-wise Feed Forward neural network, and a Generator, illustrated by Figure 1.\nInitially, the input data is tokenized by the Tokenizer and supplemented with positional information via the Positional Encoding module. This encoding allows the model to discern the order and position of tokens within the sequence.\nSubsequently, the transformed input navigates through the Encoder, which employs self-attention and feed-forward mechanisms to derive context-rich representations. Each Encoder comprises successive layers of self-attention and feed-forward neural network, facilitating the model's grasp of hierarchical and context-dependent aspects of the data. A total of eight attention heads have been used in each encoder module, which employs the scaled dot-product attention mechanism, discerning dependencies across varying sequence positions. Meanwhile, the Position-wise Feed Forward network applies a feed-forward neural network individually to each sequence position. Layer normalization is executed via the Layer-Norm functionality to ensure stable training.\nThe culmination of this process sees the Generator module transforming the learned representations into the specified class outputs."}, {"title": "3.3.2 Hyperparameters", "content": "Our Transformer model consists of six encoder layers, each equipped with eight attention heads. Our model uses 512-dimensional embeddings. To train our network, we utilize the Adam optimizer [9] with a learning rate set to 1e-4. We use a batch size of 128, and the training process is halted when the validation loss does not improve for 3 consecutive epochs."}, {"title": "4. Evaluation and Discussions", "content": ""}, {"title": "4.1. Experiments", "content": "We conducted training using various methodologies, including the Frame-wise Attention model and the Trajectory-wise Attention model, without employing any data augmentation. During our experiments, we explored models with different counts of total keypoints, specifically 543 and 203 keypoints (kp). In the 543 kp model, we used 468 keypoints in the face, whereas in the 203 kp model we used 128 face keypoints. Interestingly, we observed enhanced performance when increasing the number of keypoints associated with facial features.\nBuilding on these findings, our experimentation extended to data augmentation techniques, as elaborated in the subsequent section."}, {"title": "4.1.1 Augmentation", "content": "We explored several augmentation techniques including shifting, scaling, rotating, and horizontal flipping. Model performance improved with each of the augmentation methods when trained with each separately, and training with shift augmentation provided the maximum boost to the performance."}, {"title": "4.2. Evaluation", "content": ""}, {"title": "4.2.1 Accuracy", "content": "The top-5 percent accuracy has been calculated for all our models, which is 60%. Considering the inherent challenges of this task, this performance is commendable, particularly since we solely rely on keypoint vectors without incorporating RGB images. To the best of our knowledge, ours is the first work in only using keypoint vectors for this task, hence comparison with other models is infeasible. On the upside, we've notably reduced computational costs, training duration, and memory usage. There's potential for even greater accuracy by integrating keypoints with RGB images."}, {"title": "4.2.2 Computational Comparison", "content": "Our model employs 23.9 million parameters, compared to the 34.5 million parameters associated with RGB images. By exclusively utilizing keypoint vectors, we achieve a significant reduction in computational cost."}, {"title": "5. Conclusion and Further Work", "content": "Although our model demonstrates 60% accuracy on unseen data, but its computational efficiency is noteworthy. This is the pioneering effort to utilize a keypoint-based model for British Sign Language word classification, rendering direct comparisons with existing works unavailable. We acknowledge the potential for refining our model to enhance accuracy. Future extensions might incorporate keypoint-based imagery, specifically black-and-white skeleton images. Additionally, we plan to explore advanced sequence-level 3D pose estimation techniques in subsequent research."}]}