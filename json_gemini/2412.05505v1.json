{"title": "Trimming Down Large Spiking Vision Transformers via Heterogeneous Quantization Search", "authors": ["Boxun Xu", "Yufei Song", "Peng Li"], "abstract": "Spiking Neural Networks (SNNs) are amenable to deployment on edge devices and neuromorphic hardware due to their lower dissipation. Recently, SNN-based transformers have garnered significant interest, incorporating attention mechanisms akin to their counterparts in Artificial Neural Networks (ANNs) while demonstrating excellent performance. However, deploying large spiking transformer models on resource-constrained edge devices such as mobile phones, still poses significant challenges resulted from the high computational demands of large uncompressed high-precision models. In this work, we introduce a novel heterogeneous quantization method for compressing spiking transformers through layer-wise quantization. Our approach optimizes the quantization of each layer using one of two distinct quantization schemes, i.e., uniform or power-of-two quantification, with mixed bit resolutions. Our heterogeneous quantization demonstrates the feasibility of maintaining high performance for spiking transformers while utilizing an average effective resolution of 3.14-3.67 bits with less than a 1% accuracy drop on DVS Gesture and CIFAR10-DVS datasets. It attains a model compression rate of 8.71\u00d7-10.19\u00d7 for standard floating-point spiking transformers. Moreover, the proposed approach achieves a significant energy reduction of 5.69x, 8.72\u00d7, and 10.2\u00d7 while maintaining high accuracy levels of 85.3%, 97.57%, and 80.4% on the N-Caltech101, DVS-Gesture, and CIFAR10-DVS datasets, respectively.", "sections": [{"title": "1 Introduction", "content": "Vision transformers (ViTs) have emerged as a powerful vision backbone substitute to convolutional neural networks(CNNs) (Vaswani et al.; Dosovitskiy et al.). However, ViTs' state-of-the-art accuracy comes at the price of prohibitive hardware latency and energy consumption for both training on the cloud and inference on the edge, limiting their prevailing deployment on resource-constrained devices (Liu et al., b; Shu et al.). Spiking neural networks (SNNs), as the third generation of neural networks, are models of computation that more closely resemble biological neurons with real-time processing and low-power dissipation. SNNs can be seamlessly adapted to edge devices, enabled by great progress in neuromorphic applications (Davies et al.; Merolla et al., 2014) with high accuracy (Zhang & Li). More recently, several works have demonstrated the potential of realizing spiking transformers with low-power dissipation to address the aforementioned issues associated with ViTs. (Zhou et al.) propose a spiking-based self-attention mechanism and adapt the existing transformer architecture into spiking neural networks for vision tasks, reducing energy consumption while delivering improved performance over CNN-based SNNs. (Xu et al., 2023) further improve spiking neurons' spatiotemporal attention abilities and enhance attention map's representativeness by denoising functions or hashmaps(Xu et al., 2024a). In addition, as a variant of SNNs, the sparse and event-driven nature of spikes causes the spiking ViT's computation to leverage multiple neuromorphic hardware platforms (Davies et al., 2018; DeBole et al., 2019; Lee et al., 2022; Xu et al., 2024b) to support a real-time and low-power computation paradigm. A typical spiking transformer architecture is depicted in Fig. 1.\nDespite the encouraging progress on SNN-based ViTs, one pressing hurdle is that these approaches still suffer from large model sizes and expensive use of full-precision weights and multiplication operations. As such, these models are not amenable to deployment on resource-strained edge devices due to prohibitively high computational/memory access overheads, and energy consumption.\nQuantization is inherently hardware-friendly and offers a promising model compression solution, and has been adopted for DNN/SNN hardware accelerators. (Putra & Shafique) proposes a uniform and non-layer-wise quantization framework Spiking CNNs. (Tang & Han) binarizes the weight of all layers and replaces multiplication with XOR operations to in spiking CNNs. However, precision loss in uniform quantization and unsystematic aggressive quantization can lead to large performance drop and limit model compression ratio. Despite the promise of quantization for model compression, the use of quantization for SNNs and transformers in general has been limited. We introduce SpikeHQ, the first approach to heterogeneous quantization for spiking neural networks, with a specific focus on spiking transformers. With a neural architecture search formulation, SpikeHQ employs optimal layer-by-layer compression, utilizing either a uniform or power-of-two quantization scheme with mixed bit resolutions. While quantization has been employed for both artificial neural network (ANN) and spiking neural network (SNN) accelerators, heterogeneous quantization has not been explored for SNNs in general and ANN-based transformers."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Spiking Transformers", "content": "(Zhou et al.) adapts self-attention mechanisms (Vaswani et al.) and ANN-based transformer architectures for spiking neural network (SNN) based implementation to improve learning and efficiency. It takes a 2D image sequence as input and uses the Spiking tokenizer module to project the image into a spike-form feature vector, which is further divided into flattened spike-form patches. A spike-form relative position embedding is generated and added to the patches. The architecture proposed in (Zhou et al.) includes L-block Spikformer encoders, each consisting of a Spiking Self Attention (SSA) module and an MLP block. Residual connections are established within the SSA and MLP blocks. The SSA module models local-global information using spike-form Query, Key, and Value components without employing softmax. A global average-pooling operation is performed on the processed features, and the resulting feature vector is fed into a fully-connected classification head to obtain the final prediction."}, {"title": "2.2 Heterogeneous Quantization", "content": "Spiking transformers are advantageous in their improved biological plausibility. Equally importantly, they can lead to energy efficient processing due to the binary nature of spiking activities. However, the last key benefit has not been fully exploited in the existing spiking transformer architectures. In (Zhou et al.), even though the spiking neurons produce one-bit binary outputs, due to the residual connections across different blocks, the output of a block are multi-bit integers, which are inputs to the subsequent block. Furthermore, the parameters of each spiking transformer layer are floating point numbers to maintain accuracy, rendering expensive multi-bit additions and multiplications and high storage overheads.\nWe argue that the characteristics of spiking neural networks, when explored properly, would offer ample opportunities for weight quantization. Spiking activations are binary, signifying the robustness of spike-based computation with respect to parameter imprecision, and provides room for weight quantization either uniformly or in other means. Multipliers are much more costly in terms of area, power, and latency compared with shifters and adders. Multiplications can be efficiently performed with addition and shift operations(Xue & Liu, 1986) with simple hardware implementation at higher speeds (Marchesi et al., 1993; Sanchez-Romero et al., 2013). On present-day processors, bit-shift instructions are faster than multiply/divide instructions and can be leveraged to perform multiplications and divisions by powers of two. Multiplication (or division) by a constant can be implemented using a sequence of shifts and adds (or subtracts). This would particularly favor the power-of-two quantization, which we explore along with uniform quantization in SpikeHQ."}, {"title": "2.2.1 Uniform Quantizer", "content": "Uniform quantization is defined by three parameters: the scale factors, the zero-point z, and the bit-width b. s and z are both a floating-point number, and are used to map a floating-point value 0 to an integer grid,"}, {"title": "2.2.2 Power-of-Two Quantizer", "content": "Power-of-two quantization is symmetric with a power-of-two scale factor s. Scaling with s can be efficiently realized by performing bit-shifting k times. It is a suitable quantization choice for spiking transformers as the weight distributions are approximately symmetric. In spiking transformers such as (Zhou et al.), leveraging the residual connection results in a progressive enlargement of spike activation range accumulation as the network depth increases. Consequently, the adoption of power-of-two quantization enables efficient shift operations over low-precision multipliers, also enhancing the compression rate. The b-bit quantizer quantizes a floating point number @ into a power-of-two number as:\n$s = 2^{\\lfloor log(max(\\theta))\\rfloor}$ \n$\\theta_{scaled} = \\theta / s$\n$$\\hat{\\theta} = \\begin{cases}\n0 & \\text{if } -2^{-2^{b-1}+1} < \\theta_{scaled} < 2^{-2^{b-1}+1} \\\\\n1 & \\text{if } \\theta_{scaled} > 1 \\\\\n-1 & \\text{if } \\theta_{scaled} < -1 \\\\\n-2^{\\lfloor log_2(\\theta_{scaled}) \\rfloor} & \\text{if } -1 < \\theta_{scaled} < -2^{-2^{b-1}+1} \\\\\n2^{\\lfloor log_2(\\theta_{scaled}) \\rfloor} & \\text{if } 2^{-2^{b-1}+1} < \\theta_{scaled} < 1\n\\end{cases}$"}, {"title": "3 Heterogeneous Quantization by Neural Architecture Search", "content": ""}, {"title": "3.1 Modeling of Hardware Overhead", "content": "The proposed neural architecture search (NAS) finds the optimal heterogeneous quantizations for a given spiking transformer while jointly optimizing model accuracy and hardware overhead. We adopt the standard cross-entropy loss to evaluate model accuracy while proposing an analytic model to evaluate hardware cost.\nWhile an unquantized spiking transformer may involve float-point multiplications (Zhou et al.), integer multipliers and shifters are used for realizing multiplications in layers with uniform and power-of-two quantization, respectively. We consider computation and memory access overhead, and model the hardware overhead for each layer under two different quantizers as follows targeting the 45nm CMOS technology:\n$C_{HW} = \\begin{cases}\n\\#ops \\times (C_{mult} + C_{add}) + \\#bits \\times C_{dram} & \\text{if uniform} \\\\\n\\#ops \\times (C_{shift} + C_{add}) + \\#bits \\times C_{dram} & \\text{if power of two}\n\\end{cases}$\nwhere #ops is the number of multiplications performed, #bits the storage in terms of number of bits, and $C_{shift}$, $C_{add}$, $C_{mult}$, $C_{dram}$ the energy cost per shift/add/mult/DRAM operation based on Table 1. Specifically, #ops and #bits are given by:\n$\\#ops = \\begin{cases}\nf_i \\times f_o \\times d & \\text{for linear layer} \\\\\nk^2 \\times c_i \\times c_o \\times s & \\text{for conv layer}\n\\end{cases}$\n$\\#bits = \\begin{cases}\nf_i \\times f_o \\times b & \\text{for linear layer} \\\\\nk^2 \\times c_i \\times c_o \\times b & \\text{for conv layer}\n\\end{cases}$\nwhere $f_i$ and $f_o$ are the input feature and output feature dimensions of the linear layer, d the input tensor dimension, k the kernel size, $c_i$ and $c_o$ the input and output channel size of the convolution layer respectively, s the input image size, and b the bit width of the quantization scheme."}, {"title": "3.2 Problem Formulation", "content": "For a spiking transformer with N encoder blocks, L layers in each transformer block, and M candidate quantizers each with P different quantization precision, we aim to find the optimal quantization scheme for each layer while jointly optimizing model accuracy and hardware overhead. Formally, we formulate this as a bi-level neural architecture search (NAS) problem:\n$\\min_q L (q, \\theta^* (q))$\ns.t. $\\theta^* (q) = \\arg \\min_{\\theta} L (q, \\theta),$\nwhere $\\theta$ represents the weight parameter of the model, q is the architectural parameter and encodes the optimal quantization schemes including quantizer type and bit width for all layers, and L captures both model accuracy and hardware overhead. SpikeHQ is illustrated in Fig. 2."}, {"title": "3.3 Differential Architecture Search", "content": "The architectural (quantization) parameter q in our NAS problem is discrete, giving rise to a large discrete solution space of $O((MP)^{LN})$ complexity. We adapt an efficient differentiable neural architecture search approach (Liu et al., a) to solve the problem."}, {"title": "Continuous-valued relaxation", "content": "For each layer l in block j, we introduce a continuous-valued selection probability parameter $\\alpha_l^{<j>}$ for each i-th quantization scheme candidate in the set of all MP options. We construct a composite layer presentation by summing up all possible quantized realizations of the layer weighted by the corresponding selection probability parameter $\\alpha_l^{<i>}$:$\n\\psi_l^{<j>} = \\sum_{i=1}^{MP} g(\\alpha_l^{<i>}) \\cdot f_{\\theta_{i}^{<j>}}(y_{l-1}),$\nwhere $g(\\cdot)$ represents a suitable form of Gumbel softmax (Jang et al.), $\\theta_i^{<j>}$ is the weight parameter under quantization scheme i, $f_{\\theta_i^{<j>}}(\u2022)$ maps the output of layer l \u2013 1, i.e. the input to layer l, denoted by $y_{l\u22121}$, to the output of layer l's output $y_l$. We then construct a \"supernet\" for the entire transformer by cascading all composite layers.\nWe create a continuous-valued relaxation to the NAS problem of (10, 11), optimizing the supernet over the continuous-valued weight parameter $\\theta$ and all $\\alpha_l^{<i>}$. Once this relaxed bi-level problem is solved, the optimal quantization scheme for each layer i is chosen by the option that has the largest selection probability $\\alpha_l^{<i>}$.\nThe total hardware loss of the supernet is obtained by summing up all layer-wise hardware costs based on (7) weighted by the corresponding quantization selection probabilities:\n$L_{hw} = \\sum_{j=1}^{N} \\sum_{l=1}^{L} \\sum_{i=1}^{MP} g(\\alpha_l^{<i>})C_{HW,l_i}$\nWe define the total loss to be minimized based on a combination of cross-entropy model accuracy loss $L_{acc}$ and hardware loss $L_{hw}$ with a user-specified $\\beta$ trading off between the two:\n$L_{total} = L_{acc} \\times L_{hw}^{\\beta}$"}, {"title": "Solving the relaxed bi-level optimization problem", "content": "To solve the relaxed bi-level problem efficiently, we adopt an iterative two-step approach. In Step 1, we update all $\\alpha_l^{<i>}$, and then in Step 2, we update $\\theta$, both using backpropagation.\nThe standard Gumbel softmax is given by:\n$GS(a_i) = \\frac{\\exp((log a_i + G_i) / \\lambda)}{\\sum_{i=1}^{MP} \\exp((log a_i + G_i) / \\lambda)},$\nwhere the temperature hyperparameter $\\lambda \\in (0, \\infty)$, $G_i$ is an i.i.d random variable sampled from the Gumbel(0,1) distribution. $G_i$ adds random noise into the standard softmax. The smaller the $\\lambda$, the closer the Gumbel softmax GS(\u00b7) is to the typical softmax. For both updating steps, we initially set $\\lambda$ to be high to encourage exploration of diverse quantization choices at the beginning, and then gradually reduce $\\lambda$ to stabilize the optimization towards one specific choice per layer. While the standard Gumbel softmax GS(.) is used in Step 2 for the weight parameter, the one-hot version of it is adopted in Step 1 to update the quantization selection probabilities:\ng(a_k) = \\begin{cases}\nOne-hot(GS(a_k)) & \\text{when updating } \\theta \\\\\nGS(a_k) & \\text{when updating } \\alpha_l^{<i>}\n\\end{cases}$\nEssentially, the use of GS(.) in Step 2 allows the weight parameters under all quantization choices to be updated in one step. In Step 1, we turn the selection probabilities into a single one-hot quantization scheme and only update the selection probability weights under this quantization choice. This strategy helps speed up the convergence(Fu et al.)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets/models. We evaluated the proposed heterogeneous quantization on spiking transformer models trained on three widely adopted neuromorphic datasets: DVS128, N-Caltech101(Garrick et al.), and CIFAR10DVS (Li et al., 2017). We followed the same model architectures and used the full precision models with 32-bit floating point weights from (Zhou et al.) as reference. DVS128 contains 11 hand gesture categories from 29 individuals under 3 illumination conditions. The Neuromorphic Caltech101 (N-Caltech101) dataset is a spiking version of the frame-based Caltech101 dataset (Garrick et al.). It comprises a diverse collection of images categorized into 101 distinct object classes, ranging from animals and vehicles to household items and everyday objects. CIFAR10-DVS is an event-stream dataset for object classification (Li et al., 2017). It was created by converting 10,000 frame-based images from the CIFAR-10 dataset into 10,000 event streams with sophisticated spatio-temporal structures. We executed architecture search by training a supernet packing different quantized spiking transformers with a 4 CONV-layer spiking tokenizer, two transformer blocks with 256-embedding features and 16-head spiking self-attentions (SSA) over 16 time steps. We used the AdamW optimizer on 510 training epochs with the learning rate warmed up from 1e-4 initially to the maximum learning rate of 1e-3, then reduced with cosine decay down to the minimum learning rate of le-5.\nLayer-wise quantization. Our quantization neural architecture search space encompassed 5 configurations: 32-bit floating point representation, 2-bit power-of-two quantization, 4-bit power-of-two quantization, 2-bit uniform quantization, and 4-bit uniform quantization, applied to every convolution or linear layer."}, {"title": "4.2 Results on neuromorphic datasets", "content": "Accuracies, storage and energy costs. The effectiveness of our approach is clearly illustrated in Table 2, where $\\beta$ represents the weighting hyparameter for hardware cost in the overall optimization objective. A zero-valued $\\beta$ signifies the full-precision model. Across the range of $\\beta$, SpikeHQ results in only a marginal drop in accuracy, typically up to 1% on the DVS 128 and CIFAR10DVS datasets. SpikeHQ significantly reduces the energy and storage requirements of the full precision models to as low as 9.73% (10.42\u00d7) and 6.58% (15.20\u00d7), respectively. The average number of bits used for weight parameters is reduced from 32 to 2.16bits.\nEvolution of quantization architecture search. Figure 3 illustrates the architectural evolution in the quantization search for a query linear layer, designed for spiking self-attention, in the second block of each of the three transformers. At the initiation of the search, all five quantization configurations begin with an equal section probability of 20%. As the search progresses, one particular configuration is ultimately chosen. Notably, with an increasing dataset complexity from left to right (DVS 128, CIFAR10DVS, and N-Caltech101), the curves representing the selection probabilities become more intertwined, and the overall loss exhibits greater fluctuations. Each significant shift in the selection probabilities results in substantial changes in the loss either a distinct increase or decrease-emphasizing the significant impact of the chosen quantization scheme on the overall loss. As the oscillations in the search process diminish, an optimized quantization choice emerges, leading to the attainment of the lowest achievable loss.\nBreakdown of energy and storage overheads. In Figure 4, we break the total hardware overhead into three distinct components of the network architecture: tokenizer layers, spiking attention layers, and MLP layers. In the absence of quantization, full-precision models incur significant energy consumption and storage costs, particularly in the tokenizer and MLP layers. Taking the CIFAR10DVS dataset as an example, 41.4% of the total energy is expended in the tokenizer, 19.5% in self-attention, and 38.9% in the MLP. Similarly, 38% of the total storage is attributed to the tokenizer, 20% to self-attention, and 41% to the MLP. Following quantization, all layers experience substantial compression, leading to reduced storage costs. The tokenizer layer remains the predominant contributor to energy consumption. With a quantization parameter of $\\beta$ = 2.0, the total energy is only 9.7% of that of the full-precision model, distributed as 48%, 20%, and 30% across the tokenizer, spiking attention, and the MLP, respectively. The total storage reduces to only 6.6% of that of the full-precision model, with storage distribution among the tokenizer, spiking attention, and MLP at 36%, 24%, and 39%, respectively. This analysis reveals that the gradual conversion of images into embedding patches requires substantial resources. Additionally, the energy of the spiking attention and MLP layers can be significantly reduced with a small $\\beta$ value, such as 0.5, without compromising task performance. However, achieving a substantial energy reduction in the tokenizer necessitates a higher $\\beta$ value, potentially leading to some performance loss. This underscores the tokenizer layer as the key bottleneck in the energy/performance tradeoff, suggesting that it should be the focus of architectural and design innovations.\nDisparity in layer-wise quantization. Figure 5 shows the distributions of weight parameters within spiking transformers before and after applying the proposed quantization method, and the optimized layer-wise quantization choices. The application of quantization enables a discrete and compact parameter value range, and clearly improves the sparsity by creating more centralized distributions. But, do different layers favor a similar quantization choice? Within the spiking self-attention layers, computing the query, key, value, and output is based on the same operations with the equal amount of parameters. This symmetry seems to suggest an equal importance of parameter precision for these four types of computations. Intriguingly, we quantize the post-compression energy and storage share for the four parts in the pie charts of Figure 4, which consistently shows that computing the query and key requires a lower parameter precision, which is the case even though the neural activities associated with the query and key tend to fire at lower rates than those with value and outputs(Zhou et al.). Additionally, as shown in Figure 5, the first and the last layer, which are the first convolution layer and the classification head, respectively, consistently necessitate 4-bit uniform quantization, which is the most high-precision and costly choice, signifying the criticality of these layers in the overall task performance."}, {"title": "5 Conclusion", "content": "This paper introduces a heterogeneous quantization approach tailored for emerging spiking transformers. The proposed SpikeHQ strategically eliminates the heavy parameter challenge within spiking transformers and explores the optimal quantization scheme on a layer-by-layer basis, optimizing overall energy and storage overhead, and model accuracy through neural architectural search. Our experimental studies underscore the potential of SpikeHQ, demonstrating its capability to achieve up to an order of magnitude reduction in energy and storage requirements without significantly compromising model accuracy. Furthermore, our empirical findings highlight specific layers that require high-precision weights, signifying them as the focal points for future architectural and design innovations."}]}