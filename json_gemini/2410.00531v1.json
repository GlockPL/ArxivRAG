{"title": "TPI-LLM: SERVING 70B-SCALE LLMS EFFICIENTLY ON LOW-RESOURCE EDGE DEVICES", "authors": ["Zonghang Li", "Mohsen Guizani", "Wenjiao Feng", "Hongfang Yu"], "abstract": "Large model inference is shifting from cloud to edge due to concerns about the pri- vacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multi- ple devices to run and speed up LLM inference. Pipeline parallelism, the main- stream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor par- allelism can be more effective than pipeline on low-resource devices, and present a compute- and memory-efficient tensor parallel inference system, named TPI- LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allre- duce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Large Language Models (LLMs) have been widely deployed in the cloud for inference. User inputs are uploaded to the cloud, where high-performance GPUs are used to compute output sequences, and then sent back to user devices for display. This process poses privacy risks, as user prompts are exposed to network intermediaries and clouds. Therefore, there is an increasing need to shift LLM services to the network edge, such as on laptops, hand phones, tablets, and desktop computers. However, edge devices have very limited memory (4-16 GB) and computing power (often CPU-only). Even with quantization, running a Llama 3.1-70B model still requires at least 40 GB of memory, which far exceeds the capacity of most edge devices. Besides, running Bert-L on one Nano-M device results in a latency that is 120\u00d7 longer than on one A100 GPU. This gap requires the use of more edge devices to support and speed up LLM inference on the network edge.\nWhile advanced LLM serving systems (Shoeybi et al., 2019; Rasley et al., 2020; Li et al., 2023; Agrawal et al., 2024; Miao et al., 2024) have been designed for high-performance GPU clusters, recent efforts (Zhang et al., 2024; Mei et al., 2024; Borzunov et al., 2024) are adapting these sys- tems to edge environments, by adaptively partitioning model between edge devices and optimizing"}, {"title": "2 OBSERVATIONS AND MOTIVATIONS", "content": "Before presenting our TPI-LLM system, we address two questions that guide our design:\nQ1: On low-resource edge devices, which dominate inference time: computation or communication? Which is more efficient, tensor parallelism or model parallelism?\nOn the network edge, the balance between computation and communication differs from that in high-performance GPU clusters. To determine whether tensor or model parallelism offers more benefits, it is essential to identify which-computation or communication-takes up more time. For this purpose, we examine the Llama 3.1-8B model on a LAN network with 4 laptops of 8 cores. The network bandwidth between them is 178 Mbps, and the devices implement allreduce communications using a parameter server architecture (Li et al., 2014).\nFigures la and 1b show the timeline and computing-communication time ratio for model and tensor parallelism during inference. In model parallelism, communication accounts for only 2% of the time, with most spent on computation. However, when one device is computing, others are idle, creating pipeline bubbles and resource waste. In tensor parallelism, communication rises to 70%, but all devices compute simultaneously, and the speed boost outweighs the communication cost, leading to less overall inference time. This makes tensor parallelism the preferred choice.\nQ2: Is tensor parallelism enough for edge LLM serving?"}, {"title": "3 TPI-LLM FRAMEWORK WITH SLIDING WINDOW MEMORY SCHEDULING", "content": "In a typical inference workflow, many users send their prompts to a cloud-based service. These prompts are pooled and scheduled in batches, undergoing dozens of Transformer layers, and con- verted into probabilities to predict the next token. This process repeats until the generated sequence is finished. While the fundamental workflow on the cloud and edge are similar, key differences arise:\n(a) Keep prompts and generated sequences on users' device. In a cloud setup, user prompts are sent to remote servers for processing, which result in exposure of private data. Edge LLM serving systems are required to keep prompts and generated sequences in users' own devices to ensure raw data never get exposed to external unknown environments.\n(b) More single-prompt serving. Current LLM serving systems are typically optimized for batched prompts using pipeline scheduling. However, these optimizations lead to resource underutilization in edge scenarios like smart speakers, where only one prompt is processed at a time.\n(c) Low-resource devices without CUDA support. Edge devices, unlike cloud GPUs, have very limited memory and low computing power. Many of them lack CUDA support or do not have GPUs at all, and they often prioritize full precision to ensure faster computations."}, {"title": "3.1 THE PARALLEL FRAMEWORK DESIGN OF TPI-LLM SYSTEM", "content": "The proposed tensor parallel inference system (TPI-LLM) tackles these challenges by using a tensor parallel framework that distributes attention heads across multiple nodes. As depicted in Figure 2, it involves a master node, typically the user's device that initiates the prompt, and several worker nodes that share the computational load. Their pseudo codes are given in Algorithms 1 and 2.\nStep 1: The master node partitions and distributes model weights. Before inference begins, the master node partitions the pretrained model weights W, such as attention heads and FFN weights, among the worker nodes. Workers with greater computing power and larger memory are allocated more attention heads and FFN weights. This ensures no single device bears the full burden.\nStep 2: The master node initiates prompt and broadcast the input embedding to workers. The inference process starts at the master node, where a user prompt is tokenized into a list of token indices \u00e6 and then transformed into input embeddings $H_0 = xW_{emb}$. The embedding is then broadcast to all worker nodes $H^{out}_{0fn} = H^{0}$ to initiate the tensor parallel workflow."}, {"title": "Step 3: All nodes perform tensor parallel computing", "content": "The tensor parallel computing follows a cycle of four operations: attention computing, allreduce, FFN computing, and allreduce. These opera- tions together constitute a Transformer block. Devices compute attention and FFN with partitioned weights in parallel, reducing the computing delays on low-power devices.\nIn the attention computation phase of the l-th Transformer block, device h processes only a subset of attention heads $Q^{hl} = Norm(H^{l-1})W^{Q,l}_{h}$, $K^{hl} = Norm(H^{l-1})W^{K,l}_{h}$, $V^{hl} = Norm(H^{l-1})W^{V,l}_{h}$, where $Norm(H^{l-1})$ is the normed hidden state and weight partitions $W^{Q,l}_{h}$, $W^{K,l}_{h}$, $W^{V,l}_{h}$ are downloaded from the master node in Step 1. Once $Q^{h,l}$, $K^{h,l}$,$V^{h,l}$ are computed, we apply the scaled dot- product attention to calculate the attention score, and the result is then synchronized across devices:\n$H^{l}_{attn} = all reduce(softmax(\\frac{Q^{hl} (K^{hl})^T}{\\sqrt{d}}))V^{hl}) + H^{l-1}$,\nwhere d is the dimension for attention head. Here, attention is computed in parallel across devices, followed by an allreduce to aggregate their hidden states and a shortcut connection. The key-value pair ($K^{h,l}$, $V^{h,l}$) is cached locally on device h to reduce redundant computations. This distributed KVCache partitions the cache across devices, so memory cost is reduced on individual device.\nAfter the attention computation and allreduce, the process continues with the FFN computation:\n$H^{l}_{ffn} = all_reduce(W^{l}_{down} (\\sigma(W^{l}_{up} (Norm(H^{l}_{attn})))))+H^{l}_{attn}$,\nwhere FFN weights $W^{l}_{up}$, $W^{l}_{down}$, $W^{l}_{hidden}$ are also partitioned weights, $H^{l}_{norm} = norm(H^{l}_{attn})$, \u03c3 represents the activation function such as SiLU (Elfwing et al., 2018). Similar to the attention com- putation stage, the FFN is computed in parallel, followed by an allreduce and a shortcut connection."}, {"title": "Step 4: The master node reduces tensor parallel results and calculates the next token", "content": "After each node h completes its part of computation within the backbone network, the result is sent to the master node. The summed results $H^{out}_{ffn}$ are then passed through a task head $W^{head}$ and softmax to obtain the probability distribution of the next token $z = softmax(H^{out}_{ffn} W^{head})$, which is then sampled. Steps 2 to 4 repeat until an EOS token is generated or the length limit is reached.\nTPI-LLM provides three benefits: (i) The user prompt {$x_1,x_2,\u2026\u2026$} and generated sequence {$z_1 \\sim z_1, z_2 \\sim z_2,\u2026\u2026 $} are processed only on the master node, keeping them hidden from workers. Even if workers reverse-engineer input embeddings $H^{0}$, they cannot recover the raw prompt \u00e6 or next token z ~ z since the weights of input embedding $W_{emb}$ and task head $W^{head}$ reside solely on master. (ii) The inference speed is often limited by the computational latency, but in TPI-LLM, it is accelerated via parallel computing. (iii) Unlike other systems that use a mix of communi- cation primitives (reduce & broadcast (Shoeybi et al., 2019), reducescatter & allgather (Ye et al.,"}, {"title": "3.2 ALLREDUCE LATENCY ANALYSIS", "content": "Given the dynamic and heterogeneous nature of edge networks, we tested NetStorm (Li et al., 2024) as the communication backend, but unfortunately, it resulted in high token latency. After further validation, we confirmed that this latency was not due to network bandwidth, but due to link latency.\nTo analyze the impact of network bandwidth and link latency, we make the following assumption.\nAssumption 1. Assume that the edge network adopts a physical topology as shown in Appendix A.7, the network links have the same latency \u0442, the allreduce algorithm follows a tree-based structure of depth 2 for aggregation, and each device has the same computing power.\nThe allreduce latency can be expressed as $t_{all reduce} = 2L(t_{data} + t_{link} + t_{barrier} + t_{aggr})$, where L is the number of Transformer layers, $t_{data}$ is the cumulative data transfer latency, $t_{link}$ is the cumulative link latency, $t_{barrier}$ is the cumulative barrier latency during aggregation, and $t_{aggr}$ is the cumulative latency for aggregation calculation. Here we ignore $t_{aggr}$ as it takes only 0.1 ms and thus negligible compared to other factors.\nProposition 1. The bottleneck in allreduce is not network bandwidth, but link latency.\nProof. The data transfer latency $t_{data} = 2\\{ i, j\\} \\in P \\frac{32|H|}{B_{ij}}$ depends on the size 32|H| of the data being transmitted and the bandwidth $B_{ij}$ of the links in the path $P_h$, here $P_h$ is an index sequence from device h to the master device. For example, in the case of Llama 2-70B with a hidden size H = 8192 and a network bandwidth of 300 Mbps, the data transfer latency is only $t_{data}$ = 3.4 ms, which is negligible compared to other latencies. In addition, experiment results in Figure 5 show that increasing the network bandwidth does not significantly reduce token latency, further confirming that data transfer and network bandwidth is not the bottleneck.\nThe link latency $t_{link}$, which is often neglected, emerges as the main issue. For example, the path from device h\u2082 to h\u2081 via hs follows the route $h_2 \\rightarrow r_2 \\rightarrow r_9 \\rightarrow r_8 \\rightarrow h_8 \\rightarrow r_8 \\rightarrow r_9 \\rightarrow r_1 \\rightarrow h_1$, resulting in a total link latency of 16\u03c4, where \u03c4 is the per-hop link latency. To isolate the impact of"}, {"title": "3.3 SLIDING WINDOW MEMORY SCHEDULING", "content": "Quantizations like FP16 and INT8 are common for NVIDIA GPUs with CUDA support, but most edge devices lack CUDA and prefer full precision for faster computation due to their general-purpose CPU design. As a result, while tensor parallelism helps distribute memory costs across devices, the memory load remains high. Thus, memory scheduling is still required to manage these loads.\nWe introduce a memory scheduler, which manages memory by dynamically loading and unloading model weights during inference, ensuring that only the necessary parts are kept in memory (see Appendix A.2 for potential use). The memory scheduler operates on a daemon thread to asyn- chronously handle memory operations. To maintain the peak memory footprint, it uses a sliding window and preloads weights for upcoming layers while unloading those that have been processed.\nAs mentioned in Section 3.1, each Transformer layer is divided into attention computing, allreduce, FFN computing, and allreduce. For simplicity, in Figure 4, we assume the delays for these stages and weight loading to be equal. In each time slot, the memory scheduler asynchronously loads weights for either an attention or FFN block. By overlapping weight loading with ongoing computations and communications, it hides the I/O latency associated with loading weights from disk. For example, in Figure 4, the memory scheduler loads one more block during each allreduce until the sliding window reaches its size. As computations and communications proceed, we ensure weights are always ready when needed, allowing for seamless inference without computational stalls.\nNext, we provide the conditions for this mechanism to reach a steady state, under which all required weights are loaded before computation starts.\nProposition 3 (Loose Steady Condition). The memory scheduler reaches a steady state when the following condition is met:\n$t_{attn} + t_{ffn} + 2t_{all_reduce} \\geq T_{ffn} + T_{attn}$,\nand one of the following conditions is met:\n1. $t_{attn} + (l - 1) \\cdot t_{ffn} + (2l - 1) \\cdot t_{all_reduce} > l \\cdot T_{ffn} + (l - 1) \\cdot T_{attn}, \\forall l \\in \\{1,\u2026\u2026,L\\}$,\n(4)\n(5)\n(1-1) $t_{attn} + l \\cdot t_{ffn} + (2l - 1) \\cdot t_{all_reduce \\geq (l - 1) \\cdot T_{ffn} + l \\cdot T_{attn}, \\forall l \\in \\{1,\u2026\u2026,L\\}$,\n(6)"}, {"title": "4 EXPERIMENTS", "content": "Prototype and Testbed. We implemented the prototype of TPI-LLM\u00b9 with 3K LoC using PyTorch and Transformers to provide flexible support for various sizes and versions of pretrained LLMs. Our testbed, illustrated in Appendix A.7, was built upon Klonet (Ma et al., 2024) to create an edge network environment, emulating realistic conditions with configurable properties like network topol- ogy, bandwidth, and latency. By default, 8 edge devices were emulated on 2 Intel Xeon Gold 5220R CPUs, each limited to 8 logical cores, 8 GB of memory, and 4 GB of swap. Network bandwidth between devices was set to 300 Mbps with a 1 ms latency.\nModels. The inference speed of TPI-LLM is significantly affected by the model architecture. Deeper layers, more parameters, larger hidden sizes, and more attention heads increase the computational latency. Additionally, deeper layers result in more allreduce communications, and a larger hidden size leads to greater traffic. We tested with various models of different sizes, including Llama 2- 3B/7B/13B/70B, Llama 3-8B/70B, and Yi-34B. See Appendix A.8 for their configuration details."}, {"title": "4.1 OVERVIEW OF TPI-LLM PERFORMANCE", "content": "Fit 70B LLMs into edge devices and run in high efficiency. We tested the performance of TPI- LLM with a focus on 3 key metrics: time-to-first-token (TTFT), token latency, and peak memory footprint per device. The memory window size is set to 2 by default. As shown in Table 1, without the memory scheduler, the full weights are loaded into the memory at once. Despite that these weights have been distributed across multiple devices, the memory is still insufficient for larger models like Yi-34B and Llama 2/3/3.1-70B. Instead, enabling our memory scheduler significantly reduces the peak memory footprint, allowing larger models to run efficiently. For example, the Llama 2-70B model requires just 3.1 GB of memory per device, and the Llama 3.1-70B model fits within device limits. The results are summarized in Table 1."}, {"title": "4.2 SCALING OVER VARYING EDGE CONDITIONS", "content": "Computation remains the bottleneck, not network bandwidth. In this experiment, we examined the token latency of TPI-LLM under different edge conditions, the results are shown in Figure 5. As expected, increasing the number of devices reduces the computing load on each device, significantly lowering token latency, and more CPU cores also contribute to a reduced latency. Instead, a limited network bandwidth was no longer a bottleneck, boosting it from 300 Mbps to 1 Gbps had little effect on latency due to the tiny data size (only 256 KB) during each allreduce. Thus, the main bottleneck remains in the computation, which our future work should focus on."}, {"title": "4.3 COMPARISON WITH BENCHMARKS", "content": "We compared the TPI-LLM with 3 benchmarks: (a) Standalone: LLM inference is executed only on a single edge device using Transformers (Wolf et al., 2020). (b) Model Parallelism (MP): Since only one user is served at a time, the pipeline parallelism (Zhang et al., 2024; Mei et al., 2024; Borzunov et al., 2024) degrades to the model parallelism, where different layer sequences are distributed across multiple devices. Each device computes its layers and passes the result to the next device until the entire inference is complete. (c) Galaxy (Ye et al., 2024) combines tensor and sequence parallelism and overlaps communication and computation to accelerate inference. They all run in FP32 mode.\nRun larger models with lower latency and memory usage. As shown in Figure 6, a limited memory on a single device makes it challenging to run even 3B models in a standalone mode. MP addresses this by the collaboration of 8 devices, allowing models up to 13B, but suffers from high latency due to pipeline bubbles. Galaxy tries to reduce such latency by combining tensor and sequence parallelism. However, in Section 3.2, we concluded that the network bandwidth was no longer the issue, and the real problem is the link latency. Galaxy's use of a ring algorithm for reducescatter and allgather forces each link to be traversed at least 14 times. This causes high link"}, {"title": "4.4 REAL CASE STUDY", "content": "In this study, we used 4 laptops with different CPU architectures and memory capacities, connected via a local Wi-Fi router. The testbed and configurations are detailed in Appendix A.10. Macbook Pro was used by default. Due to the lack of CUDA, all computations were performed in full preci- sion. As shown in Table 3, Transformers loaded the entire model into the CPU memory, and when memory was insufficient, the operating system offloaded data to the swap. This frequent swap ex- change significantly increased TTFT and token latency, even for smaller 3B models. As the model size grows, the swap space overflowed, finally leading to OOM errors. As a more efficient alter- native, Accelerate (Gugger et al., 2022) instantly loads layer weights only when required for the computation and reduces unnecessary data I/O. While it speeds up inference, due to implementation flaws on disk offloading, it still requires loading full weights before splitting and offloading them to disk. This results in OOM errors when the model size reaches 13B.\nTPI-LLM stands out in TTFT, token latency, and model size. Our memory scheduler (Trans- formers+MS) outperforms Transformers and Accelerate in both TTFT and token latency across all model sizes. This is because our memory scheduler employs a sliding window mechanism, where a daemon thread asynchronously preloads the weights needed for upcoming computations. By over- lapping data I/O with computations and communications, the scheduler avoids delays caused by disk I/O blocks, ensuring smoother and faster inference. To further speed up inference, we integrate the computing power of 4 laptops to serve TPI-LLM. By distributing the computational load across 4 laptops, the reduction in computing time far exceeds communication delays, so both TTFT and token latency are further reduced. The results from using 3 laptops are shown in Appendix A.11, indicating a slightly higher latency due to reduced parallelism."}, {"title": "5 CONCLUSION", "content": "In this paper, we concluded that tensor parallelism can be more effective than pipeline parallelism on low-resource devices, and presented a compute- and memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale LLMs. TPI-LLM is designed with user prompt and generated sequence privacy in mind, by keeping sensitive raw data local in the users' devices. It leverages a sliding window memory scheduler to dynamically manage layer weights during infer- ence with disk I/O latency overlapped by onging computations and communications, allowing larger models to run smoothly on devices with very limited memory. Our analysis showed that link latency, not bandwidth, emerges as the main issue, so TPI-LLM implements a star-based allreduce algorithm, rather than the commonly used ring- and tree-based algorithms. Through extensive experiments on emulated and real testbeds, TPI-LLM demonstrated significantly lower TTFT, token latency, and peak memory footprint compared to Transformers, Accelerate, Galaxy, and enabled serving larger- scale LLMs such as Yi-34B and Llama 2/3/3.1-70B on low-memory devices."}, {"title": "A APPENDIX", "content": "A.1 PROOF OF PROPOSITION 2\nIn conventional data parallel systems, each device sends several gigabytes of data, putting signifi- cant pressure on network bandwidth. This makes data transfer latency a major concern, while link latency becomes negligible. Then, tree and ring-based algorithms are introduced to optimize the data transfer. However, they do not apply to our case. In TPI-LLM, each device only sends a small amount of data, usually just tens of kilobytes. This tiny data size does not strain the network, so data transfer latency is minimal. Instead, in edge networks where wireless communication causes higher transmission delays, link latency becomes more significant than data transfer latency. As a result, the commonly used tree and ring-based allreduce algorithms are less effective.\nLet us consider 1 master and 2 workers connected via a router. In Figure 7, we compare the traffic models of star, tree, and ring-based algorithms. In star-based allreduce, worker 1 sends data directly to the master via the router, and the allreduce latency (includes reduce and broadcast) is $t_{star} = 2(t_{data} + t_{link}) + t_{barrier} + t_{aggr}$. In this model, the router only forwards data packets."}, {"title": "In tree-based allreduce", "content": "data from worker 1 must first go through worker 2 before reaching the master, so there are 2 hops involved. In this process, worker 1 sends its data to worker 2, which aggregates it and forwards the result to the master. Once the global aggregation is complete, the final result is broadcast back to all workers. The total time for this process is $t_{tree} = 3t_{data} + 4t_{link} + 2t_{barrier} + 2t_{aggr}$.\nIn ring-based allreduce, each device communicates directly with its neighbors in a ring topology. Data is divided and sent in a sequence around the ring, with each device receiving, aggregating, and passing the data to the next device. Unlike star or tree-based methods, there is no central device, and data flows continuously between the devices. The total time for the ring-based allreduce is $t_{ring} = t_{data} + 4t_{link} + 3t_{barrier} + t_{aggr}$.\nAssume that all devices are homogeneous, i.e., $t_{barrier} \\approx 0$, and $t_{data} \\approx 0$, $t_{aggr} \\approx 0$ because the data size is very small. Then we have latencies simplified as follows:\n$t_{star} = 2t_{link} < t_{tree} = t_{ring} = 4t_{link}$.\nThus, the star-based allreduce is the most efficient method because it minimizes link latency."}, {"title": "A.2 A SIMPLE-TO-USE MEMORY SCHEDULER", "content": "In our implementation, a context manager is used to ensure that the required block weights are loaded correctly and unload the used weights to free up memory for subsequent blocks. This simplifies the deployment of large-scale LLMs on low-memory edge devices, requiring just one additional line of code:\n1 with memory_manager.wait_and_release(f\"self_attn.0\"):\n2 hidden_states = self_attn (hidden_states)"}, {"title": "A.3 PROOF OF PROPOSITION 3", "content": "We start with the first attention block and end with the final FFN block.\nTime slot 1 (attention computation): In this initialization step, $W^{V}_{attn}$ must be loaded before com- puting the first attention block, taking $T_{attn} + t_{attn}$. During the computation time $t_{attn}$, the next FFN weights, $W^{ffn}$, are loading in parallel.\nTime slot 2 (allreduce): The attention block is followed by allreduce communication, which takes $t_{all_reduce}$, with the next FFN weights, $W^{ffn}$, loading in parallel.\nTime slot 3 (FFN computation): By this time, the FFN weights $W^{ffn}$ should be fully loaded. If not, the computation must wait for loading to complete. Let $t' = t_{attn} + t_{all_reduce} - T_{ffn}$, if $t' \\geq 0$, no blocking occurs; otherwise, the computation is delayed by $t'$. Once loaded, compute the FFN block in $t_{ffn}$.\nDuring this time slot, the waiting, computation of the current FFN block and the weight loading of the next attention block occur simultaneously. By the time the current FFN block finishes, the next attention block's weights $W^{attn}$ have been loading for $max\\{0, t_{attn} + t_{all_reduce} - T_{ffn}\\} + t_{ffn}$.\nTime slot 4 (allreduce): The FFN block is followed by allreduce communication, which takes $t_{all_reduce}$, with the next attention weights, $W^{attn}$, loading in parallel.\nTime slot 5 (attention computation): Ensure that the attention weights $W^{attn}$ are fully loaded. Let $t' = max\\{0, t_{attn} + t_{all_reduce} - T_{ffn}\\} + t_{ffn} + t_{all_reduce} - T_{attn}$. If $t' \\geq 0$, the computation proceeds without blocking. Then, $W^{attn}$ is computed in $t_{attn}$, and the next FFN weights $W^{ffn}$ have been loading for $max\\{0, max\\{0, t_{attn} + t_{all_reduce} - T_{ffn}\\} + t_{ffn} + t_{all_reduce} - T_{attn}\\} + t_{attn}$.\nTime slot 6 (allreduce): The allreduce communication takes $t_{all reduce}$, while the next FFN weights $W^{ffn}$ are loading in parallel.\nTime slot 7 (FFN computation): Ensure that the FFN weights $W^{ffn}$ are fully loaded. Let $t' = max\\{0, max\\{0, t_{attn} + t_{all_reduce} - T_{ffn}\\} + t_{ffn} + t_{all_reduce} - T_{attn}\\} + t_{attn} + t_{all_reduce} - T_{ffn}$. If $t' \\geq 0$, the computation proceeds without blocking.\nThis process repeats, until the generation task is finished.\nFor the system to reach a steady state where computation is not blocked by weight loading at any time, the following conditions must hold.\nCase 1: $t_{attn} + t_{all_reduce} - T_{ffn} \\geq 0$.\nTime slot 3 (l = 1): $t_{attn} + t_{all_reduce} - T_{ffn} \\geq 0$,\nTime slot 5 (l = 1): $t_{attn} + t_{ffn} + 2t_{all_reduce} - T_{ffn} - T_{attn} \\geq 0$,\nTime slot 7 (l = 2): $2t_{attn} + t_{ffn} + 3t_{all_reduce} - 2T_{ffn} - T_{attn} \\geq 0$,\nTime slot 9 (l = 2): $2t_{attn} + 2t_{ffn} + 4t_{all_reduce} - 2T_{ffn} - 2T_{attn} \\geq 0$.\nWe repeat these conditions and derive the following patterns.\n$t_{attn} + t_{ffn} + 2t_{all_reduce} \\geq T_{ffn} + T_{attn}$,\n$l \\cdot t_{attn} + (l - 1) \\cdot t_{ffn} + (2l - 1) \\cdot t_{all_reduce} \\geq l \\cdot T_{ffn} + (l - 1) \\cdot T_{attn}$.\nCase 2: $t_{attn} + t_{all_reduce} - T_{ffn} < 0$.\nTime slot 3 (l = 1): $t_{attn} + t_{all_reduce} - T_{ffn} < 0$,\nTime slot 5 (l = 1): $t_{ffn} + t_{all_reduce} - T_{attn} \\geq 0$,\nTime slot 7 (l = 2): $t_{attn} + t_{ffn} + 2t_{all_reduce} - T_{attn} - T_{ffn} \\geq 0$,\nTime slot 9 (l = 2): $t_{attn} + 2t_{ffn} + 3t_{all_reduce} - T_{attn} - T_{ffn} \\geq 0$,\nTime slot 11 (l = 3): $2t_{attn} + 2t_{ffn} + 4t_{all_reduce} - T_{attn} - 2T_{ffn} \\geq 0$.\nSimilarly, repeat these conditions and derive the following patterns."}, {"title": "A.4 PROOF OF PROPOSITION 4", "content": "Let $a = l \\cdot t_{attn} + (l - 1) \\cdot t_{ffn} + (2l - 1) \\cdot t_{all_reduce} - l \\cdot T_{ffn} - (l - 1) \\cdot T_{attn} \\geq 0$, and we derive the following inequality from inequality (16):\n$l \\cdot t_{attn} + l \\cdot t_{ffn} + 2l t_{all_reduce} - l \\cdot T_{ffn} - l \\cdot T_{attn} > 0$.\nBy substituting a into this inequality, we have $a + t_{ffn} + t_{all_reduce} - T_{attn} > 0$. Let $a > 0 > T_{attn} - t_{ffn} - t_{all_reduce}$, we obtain the first condition:\n$t_{ffn} + t_{all_reduce} > T_{attn}$.\nLet $\\beta = t_{ffn} + t_{all_reduce} - T_{attn} > 0$, and substitute \u03b2 into inequality (16), then we have $\\beta + t_{attn} + t_{all_reduce} - T_{ffn} > 0$. Let $\\beta > 0 > T_{ffn} - t_{attn} - t_{all reduce}$, we obtain the second condition:\n$t_{attn} + t_{all_reduce} > T_{ffn}$.\nThus, the proposition is proved."}, {"title": "A.5 PROOF OF PROPOSITION 5", "content": "In this section, we analyze the peak memory footprint on both the master and worker nodes to estimate the largest model size that our memory scheduler can handle.\nLet us use the Llama model as an example, assume the vocabulary size is v, hidden size is h, number of attention heads is a, number of key-value heads is b, and intermediate size is s. Let p = [$p_1,p_2,\u2026,p_n$] be a vector representing the proportion of parameters handled by n devices, and w be the window size of the memory scheduler. Following the block definition in Figure 2, the parameter counts for each block are detailed in Table 4:\n\nThe memory footprint is affected by parameters, activation storage, temporary tensors, memory management, and caching, making precise quantification challenging. To estimate peak memory, we apply an empirical rule: multiply the parameter size by a scaling factor \u03b3."}, {"title": "From the memory window at the peak memory footprint shown in Figure 8, we can derive the following equations", "content": "$M_{master"}]}