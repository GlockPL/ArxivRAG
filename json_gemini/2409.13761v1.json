{"title": "Do Large Language Models Need a Content Delivery Network?", "authors": ["Yihua Cheng", "Kuntai Du", "Jiayi Yao", "Junchen Jiang"], "abstract": "As the use of large language models (LLMs) expands rapidly, so\ndoes the range of knowledge needed to supplement various LLM\nqueries. Thus, enabling flexible and efficient injection of new knowl-\nedge in LLM inference is critical. Three high-level options exist:\n(i) embedding the knowledge in LLM's weights (i.e., fine-tuning),\n(ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new\nknowledge to LLM during prefill. This paper argues that, although\nfine-tuning and in-context learning are popular, using KV caches as\nthe medium of knowledge could simultaneously enable more mod-\nular management of knowledge injection and more efficient LLM\nserving with low cost and fast response. To realize these benefits,\nwe envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the stor-\nage, transfer, and composition of KV cache across LLM engines\nand other compute and storage resources. We believe that, just\nlike content delivery networks (CDNs), such as Akamai, enabled\nthe success of the Internet ecosystem through their efficient data\ndelivery, KDNs will be critical to the success of LLM applications\nthrough their efficient knowledge delivery. We have open-sourced\na KDN prototype in https://github.com/LMCache/LMCache.", "sections": [{"title": "1 BACKGROUND AND MOTIVATION", "content": "Traditionally, machine learning models, such as computer vision [21,\n30, 31, 38] and image generation [19, 32, 37], learn all the knowledge\nfrom the training data. However, the rapidly growing usage of\nlarge language models (LLMs) requires the ability to inject new\nknowledge (unseen in training) into LLM inference. For instance,\nchatbots [1, 5, 15] use a user's chatting histories as the knowledge\nto generate personalized responses; enterprises use LLM agents to\nanswer queries based on their internal databases using retrieval-\naugmented generation (RAG) [13, 24, 27, 28, 39]; and searching\nengines use LLM to read fresh and relevant web data fetched from\nthe internet for each user query [2, 6, 9].\nRecent studies have suggested that providing more contextual\ndata is among the most efficient ways to boost the quality of LLMs\nin both daily and enterprise use cases [16, 28, 39]. Also, the require-\nments of knowledge injection grow more complex. For instance,\nin enterprise settings, Al engineers or application operators often\ndemand the flexibility to let them directly specify which documents\n(or which parts of a document) should or should not be used as the\ncontext to answer a given query [4, 8]. Moreover, as new data is\nbeing collected constantly [3, 7], the knowledge-injection scheme\nmust keep up with the rapidly evolving pool of knowledge.\nWhile incorporating more knowledge boosts the LLM's genera-\ntion quality, doing so can cause various system challenges. In-context\nlearning and fine-tuning are the two popular knowledge-injection\nmethods employed by knowledge-augmented LLM applications.\nFine-tuning retrains an LLM on a new dataset using a set of la-\nbeled knowledge to update all or parts of the model's parameters."}, {"title": "2 SYSTEM TRADEOFFS OF KNOWLEDGE\nINJECTION IN LLMS", "content": "The Al research has a long literature studying in-context learn-\ning and fine-tuning. In-context learning can enhance quality by\nallowing users to control the input context, thereby reducing hal-\nlucinations and increasing coherence in the model's responses.\nFine-tuning tends to yield better results when integrating new\nknowledge across a large corpus of data. Both methods can achieve\nsimilar text-generation quality if used in the right way [10, 20, 43].\nInstead of viewing these options only in terms of accuracy (i.e.,\nthrough the ML perspective), we compare the knowledge-injection\nmethods along the following two dimensions from a system per-\nspective: modularity and efficiency.\nThe modularity includes two aspects. First, a modular approach\nshould allow service providers to specify which knowledge to use\nand compose them easily. Second, the overhead (e.g., time, cost) of\ninjecting new knowledge into the LLM should be minimized.\nThe efficiency is measured by per-query cost and response delay\nduring LLM inference. Cost is the computation used for the LLM to\nhandle a request, and response delay is defined as the time between\nthe LLM receiving the request and the generation of the first token.\nIn-context learning puts the knowledge in the model's input,\nmaking it easier to incorporate new knowledge and compose dif-\nferent ones. The separation of knowledge and model ensures en-\nhanced modularity LLM service providers can clearly specify\nwhich knowledge to use and easily compose different pieces of\nknowledge, helping LLM avoid conflicting knowledge and improve\nthe generation quality. However, in-context learning has consid-\nerable runtime overhead that hurts efficiency. Specifically, when\nusing in-context learning, LLMs must prefill a long input text with\nthe knowledge before generating the first token. During the prefill,\nthe LLM takes in the input text and runs the attention operation,\nwhose complexity grows superlinearly with the input length in\nall mainstream LLMs. In many use cases, the prefill may block the\nGPUs up to tens of seconds [33, 46], leading to high GPU cost and\nlong response delay.\nOn the other hand, fine-tuning embeds the knowledge inside\nthe model, which enables it to take in a short question without the"}, {"title": "3 MANAGING KNOWLEDGE WITH\nKNOWLEDGE DELIVERY NETWORKS", "content": "Rather than fine-tuning and in-context learning, some recent re-\nsearch has started focusing on KV-cache learning, which uses KV\ncache as a medium of knowledge and puts it as part of the LLM\ninput. KV cache is a tensor generated during the prefill phase of\nthe in-context learning, and it contains the information from the\ninput knowledge for LLM to utilize later during text generation.\nOnce generated, the KV cache can be reused in all the subsequent\nrequests that require the corresponding knowledge.\nSimilar to in-context learning, KV-cache learning has good mod-\nularity since it also separates the knowledge and the model weights.\nIn the meantime, LLM can skip prefilling and start generating to-\nkens immediately if the KV cache is on the GPU, meaning that\nthe efficiency (runtime cost and response delay) will be similar\nto fine-tuning. Therefore, KV-cache learning has the potential to\nsurpass in-context learning and fine-tuning on both modularity\nand efficiency.\nThat said, it is impossible to put KV caches of all the knowl-\nedge in GPU or CPU memory due to their large memory foot-\nprint [25, 36, 40]. To address this issue, we propose a new system\narchitecture, knowledge delivery network (KDN), that stores, trans-\nmits, and composes the knowledge in the form of KV caches on\ndemand. Figure 2 shows the architecture of KDN, which consists\nof three main components:\n\u2022 KV cache store manages the compressed KV caches keyed by\nthe text of each piece of knowledge.\n\u2022 KV cache delivery transmits the compressed KV cache to\nGPUs that host the LLMs.\n\u2022 KV cache blender composes multiple KV caches correspond-\ning to different pieces of knowledge.\nKV caches, generated from LLMs, can be put into KDN online or\noffline. When an inference request comes, the LLM sends requests\nto the KDN for some specific knowledge. The KDN then retrieves"}, {"title": "4 MISSING PIECES?", "content": "Unifying interface with LLM serving engines: Currently, most\nLLM serving engines, such as vLLM [26], HuggingFace TGI [22],\nand SGLang [45], do not readily support the injection of externally\nprovided KV caches as part of the LLM input. Instead, their internal\nimplementation KV-cache management (e.g., paged memory in\nVLLM) is deeply cobbled with the model inference implementation.\nOne way to deploy KDN is to augment each LLM engine with a KDN\nas part of the LLM engine. However, as elaborated in \u00a73, developing\na performant KDN is a substantial understanding, so it will cause\nmuch redundant engineering effort if each LLM engine maintains\nand evolves its own KDNs. To prevent reinventing the wheel, the\nLLM serving engines could interface with a separate, shared KDN\nprovider, via two APIs: (i) the LLM stores the KV cache with the\nassociated text to KDN, and (ii) the LLM retrieves the KV cache\nfrom KDN using some text. Exposing these APIs is feasible, given\nmost popular LLM engines are open-source. Still, several design\nquestions remain. How to leverage heterogeneous links, such as\nNVLink, RDMA, or PCIe, to transfer KV caches? Can the APIs be\nshared with other LLM functions, like disaggregated prefilling?\nImproving inference quality by offline KV-cache editing: So\nfar, a KDN is seen as merely a passive storage facility of KV caches.\nHowever, by separating KV-cache management from the LLM-\nserving engines, KDNs open up new possibilities to improve infer-\nence quality. Recent works have shown that if the attention matrix\nof an LLM input is appropriately modified, the inference quality\ncan significantly improve [12, 41]. In other words, when a KV cache\nis \"deposited\" to KDN, KDN not only stores it but also can actively\ninfluence the LLM inference quality by offline editing the KV cache\nand returning the edited KV cache when it is retrieved next time,\nall of which is done without any change to the model itself or the\nprompt input. A possible technique is to use a larger LLM to tell the\nrelationship between tokens and adjust the KV cache generated by\na small model correspondingly.\nWe hope this paper can inspire more research to tackle the afore-\nmentioned problems."}, {"title": "5\nCONCLUSION", "content": "In short, this paper makes a case for (1) the separation between\nthe management of knowledge, in the form of KV caches, and LLM\nserving engines, and (2) a Knowledge-Delivery Network (KDN)\nas a new LLM system component that harnesses recent research\ndevelopment to optimize the efficiency (in speed and cost) of KV-\ncache-based knowledge injection."}]}