{"title": "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion", "authors": ["Weicai Ye", "Chenhao Ji", "Zheng Chen", "Junyao Gao", "Xiaoshui Huang", "Song-Hai Zhang", "Wanli Ouyang", "Tong He", "Cairong Zhao", "Guofeng Zhang"], "abstract": "Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even 360\u00b0 images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images.", "sections": [{"title": "1 Introduction", "content": "Generating scenarios from the text descriptions that meet one's expectations is an imaginative and marvelous journey, which has many potential applications, such as VR roaming [62] for metaverse [68, 70, 71, 74], physical world simulation [2, 6, 14], embodied agents in scene navigation and manipulation [77], etc. With the advent of the AIGC era, several works [8, 14, 20, 29, 42, 43, 48, 51, 52] on object generation even scene generation has emerged. However, they generally only generate a series of perspective images, making it impossible to comprehensively simulate the entire environment for scene understanding [27, 19, 67] and reconstruction [3, 5, 66, 64, 30, 35, 34, 26, 21, 65, 7, 12, 47, 49, 58, 69]. Given this, some methods [72, 9] try to generate panoramas to solve these problems by taking advantage of the inherent characteristics of panoramic images, which can capture the surrounding environment with a single shot.\nThese methods can be roughly divided into four categories: 1) Directly single-view equirectangular projection (ERP) panorama generation [72]. However, its camera is immovable, making it incapable of scene exploration; 2) Multiple perspective views generation methods [20, 43, 48, 40] without considering multi-view panorama generation. 3) The inpainting solutions are based on the infinite expansion of a single perspective view, which lacks 3D awareness. Single scene optimization methods [53] with inpainting have no generalization ability and cost too much time for optimization. 4) Directly extending the multiple perspective views generation methods [43] to the ERP panorama, which is difficult to converge and results in poor multi-view consistency (see Fig. 5).\nThis paper aims to generate scalable and multi-view consistent panoramic images from text descriptions and camera poses (see Fig. 1) with many potential applications such as immersive VR roaming with unlimited scapes and preview for interior home design. However, achieving this goal is not trivial. To the best of our knowledge, there is currently a lack of rich and diverse panoramic datasets to meet the task of text-to-multi-view ERP panorama generation. To this end, we propose a novel panoramic video-text dataset and a generation framework suitable for the text-to-multi-view panorama generation task, advancing the development of this field. Specifically, we first establish a large-scale panoramic video-text dataset using Habitat Simulator [41] (see Sec. 3), which contains millions of panoramic keyframes and corresponding panoramic depths, camera poses, and text descriptions. Next, built upon the proposed dataset, we propose a generation framework for consistent multi-view ERP panorama generation (see Sec. 4), termed DiffPano. The DiffPano framework consists of a single-view text-to-panorama diffusion model (see Sec. 4.1) and a spherical epipolar-aware multi-view diffusion model (see Sec. 4.2). The single-view text-to-panorama diffusion model is obtained by fine-tuning the stable diffusion model [38] of perspective images using LoRA [18]. Considering that the single-view pano-based diffusion model cannot guarantee the consistency of generated multi-view panoramas with different camera poses, we derive a spherical epipolar constraint applicable to panoramic images, inspired by the perspective epipolar constraint. We then incorporated it as a spherical epipolar-aware attention module (see Sec. 4.2) into the multi-view panoramic diffusion model to ensure the multi-view consistency of the generated ERP panoramic images.\nSince there are no related methods for comparison, we try to extend the MVDream method [43] to generate multi-view ERP panoramas. Trained and tested on the proposed panoramic video-text dataset, extensive experiments demonstrated that compared to the modified MVDream, our proposed multi-view panorama generation based on spherical epipolar-aware attention can generate more scalable and consistent panoramic images. Our method also demonstrates the generalization ability of the original diffusion model to generate satisfactory multi-view ERP panoramas with given unseen text descriptions and camera poses.\nOur contributions can be summarized as follows: 1) To the best of our knowledge, we are the first to propose a scalable and consistent multi-view panorama generation task from text descriptions and camera poses. 2) We established a large-scale diverse and rich panoramic video-text dataset, which fosters the research of text-to-panoramic video generation. 3) We propose a novel text-driven panoramic generation framework with a spherical epipolar attention module, allowing scalable and consistent panorama generation with unseen text descriptions and camera poses."}, {"title": "2 Related work", "content": "2.1 Single-View Panorama Generation\nRecently, latent diffusion model (LDM) methods have attracted widespread attention, and many single-view panorama generation works [72, 48, 76, 24, 60, 56, 33, 54, 62] have emerged, achieving remarkably impressive results. Among them, MVDiffusion [48] simultaneously generates eight fixed-viewpoint perspective images through a multi-view Correspondence-Aware diffusion model and stitches them together to produce a panorama. However, it cannot support the generation of top and bottom views, and the generated panorama resembles wide-angle images with an extensive field of view rather than true 360\u00b0 images. Some methods [55, 13] solve this problem using equirectangular projection (ERP) and try to facilitate the interaction between the left and right sides during the panorama generation process to enhance the left-right continuity property inherent in ERP images. To address the domain gap between panorama and perspective images, PanFusion [72] proposed a novel dual-branch diffusion model that mitigates the distortion of perspective images projected on panoramas while providing global layout guidance. However, its more complex model architecture incurs longer inference times for panorama generation. In addition, PanFusion cannot be expanded as an effective pre-trained model to the multi-view panorama generation task due to its excessive network parameters. To strike a balance between computational complexity and ensuring left-right continuity of panoramas, our proposed single-view panorama-based stable diffusion model only requires fine-tuning with LoRA [18] to learn panoramic styles and achieve good edge continuity while maintaining higher generation speed and simpler architecture.\nThe existing single-view panorama generation methods cannot achieve scalable panorama generation. The core of our paper lies in the generation of multi-view consistent panoramic images, which we will introduce in Section 2.2. More importantly, the single-view panoramic image generated by previous methods mainly supports 3DoF roaming, while our method can generate multi-view panoramic images for 6DoF roaming, which can serve as the inputs for 360\u00b0 Gaussian Splatting [23] or 360\u00b0 NeRF [10, 11]. Our method also has a great potential value in 360\u00b0 relightable novel view synthesis with the combination of 360\u00b0 multi-view inverse rendering method [25].\n2.2 Multi-View Image Generation\nTo the best of our knowledge, there is no work focusing on multi-view panorama generation. We review the existing works about multi-view generation for perspective images in this part.\nZero123 [29] laid the foundation for 3D object generation based on multi-view generation, while the pose-guided diffusion model [51, 40] explored consistent view synthesis of scenes. However, iteratively applying the diffusion model to generate individual views in the multi-view generation task may lead to poor multi-view consistency of generated images due to accumulated errors. To generate high-quality multi-view images simultaneously, some methods [31, 43, 57, 32] modify the UNets in the diffusion model into a multi-branch form and achieve the effect of generating consistent multi-view images through the interaction between different branch.\nCurrently, most multi-view generation tasks focus on generating multi-view perspective images of single objects [43, 61, 63, 28, 46, 45] or scenes [51, 15], while minimal research has been conducted on multi-view panorama generation. Narrow FoV (field-of-view) drawbacks of perspective images lead to the fact that the existing generation methods can only generate a very local region of the scene at a time. Our work focuses on the task of exploring the generation of 360\u00b0 images from multiple different viewpoints. Due to the camera projection difference between panoramic and perspective images, achieving consistency in multi-view panoramas is challenging. It is impossible to directly apply the existing epipolar attention module [51, 20] to multi-view panoramas. We strive to derive the spherical epipolar line formula for panoramic images and propose a spherical epipolar attention module to ensure the multi-view consistency of the generated panoramas.\n2.3 Panoramic Dataset\nGreat progress in text to single-view panorama generation has been witnessed. However, text-to-multi-view panorama generation is still a blank slate. One of the main limitations of this task is the lack of suitable datasets. The common panoramic datasets used in single-view panorama generation consist of indoor HDR dataset [16], outdoor HDR dataset [73], HDR360-UHD dataset [9], Structured3D [75],"}, {"title": "3 Panoramic Video-Text Dataset", "content": "Due to the lack of high-quality panorama-text datasets, most text-to-panorama generation tasks require researchers to construct their own datasets. The dataset constructed in PanFusion [72] suffers from blurriness at the top and bottom of the panoramic images, and the corresponding text descriptions are not precise enough. To address these issues, we utilize the Habitat Simulator [41] to randomly select positions within the scenes of the Habitat Matterport 3D (HM3D) [37] dataset and render the six-face cube maps. These cube maps are then interpolated and stitched together to form panoramas so we can obtain panoramas with clear tops and bottoms. To generate more precise text descriptions for the panoramas, we first use BLIP2 [22] to generate corresponding text descriptions for each obtained cube map, and then employ Llama2 [50] to summarize and receive accurate and complete text descriptions. Furthermore, the Habitat Simulator allows us to render images based on camera trajectories within the HM3D scenes, enabling the creation of a dataset that simultaneously includes camera poses, panoramas, and corresponding text descriptions. This dataset will be utilized in the multi-view panorama generation (see Sec. 4.2)."}, {"title": "4 Proposed Method: DiffPano", "content": "DiffPano is capable of generating multi-view consistent panoramas conditioned on camera viewpoints and textual descriptions, as illustrated in Fig. 1. In this section, we first introduce our single-view panorama stable diffusion in Sec. 4.1. We then elaborate on how to extend single-view panorama generation to multi-view consistent panorama generation by leveraging the spherical epipolar attention module in Sec. 4.2."}, {"title": "4.1 Single-View Panorama-Based Stable Diffusion", "content": "A straightforward way to generate a single-view panorama from text is to train a text-to-panorama diffusion model from scratch with a large number of text-panorama pairs, which is both time-consuming and computationally expensive. However, stable diffusion [38] leverages a vast amount of perspective images and their corresponding textual descriptions as training data, endowing it with excellent prior knowledge of perspective images and strong text understanding capabilities. An economical and effective way for panorama generation is to fine-tune the trained perspective diffusion model with a few text-panorama pairs. To this end, panorama generation from text can be regarded as a style transfer of images generated by stable diffusion, converting them from perspective style to panoramic style, and requiring them to satisfy the left-right continuity property of panoramas.\nLoRA-based fine-tuning Diffusion models for text-to-image generations possess excellent prior knowledge of 2D images and strong text comprehension capabilities. We aim to preserve these abilities of the model while fine-tuning it to generate images in the style of panoramas. We employ the Low-Rank Adaptation (LoRA) [18] fine-tuning method, which has been previously used in large language models. Compared to full fine-tuning, LoRA is faster and requires fewer computational resources. In our approach, we freeze all the parameters of the original Stable Diffusion model and add trainable layers to the UNet component using the LoRA fine-tuning method. We then train the model using our custom-created panorama-text dataset. To improve the left-right continuity of the generated images, we perform data augmentation on the panorama training dataset by randomly concatenating a portion of the right side of the panorama to the left side. Experiments demonstrate that the panoramas generated using this method exhibit satisfactory left-right continuity."}, {"title": "4.2 Spherical Epipolar-Aware Multi-View Diffusion", "content": "Built upon our proposed single-view panorama stable diffusion in Sec. 4.1, we extend the single-view diffusion model to a multi-view diffusion model with a spherical epipolar-aware attention module to generate multi-view scalable and consistent panoramas.\nSpherical Epipolar-Aware Attention Module Epipolar attention was proposed in [20, 51] to ensure consistency between generated multi-view perspective images. However, due to the differences in imaging methods between perspective and panoramic views, existing epipolar attention cannot be directly used for panoramic views. To overcome this challenge, we derived the epipolar line for panoramic images in the equirectangular projection (ERP), and the specific proof process is provided in Appendix A. Equation 14 shows the mathematical form of the spherical epipolar line in ERP images. The spherical epipolar line is visualized in the spherical epipolar-aware attention module of Fig. 3. We extend the principle of epipolar attention to panoramic images to implement the spherical epipolar-aware attention module. Given a pixel p in the target view, we calculate its corresponding spherical coordinates psphere based on the spherical projection process:\n$\\theta = (0.5 - \\frac{X_{pix}}{W}) \\cdot 2\\pi$\n$\\varphi = (0.5 - \\frac{Y_{pix}}{H}) \\cdot \\pi$, (1)\nwhere Xpix and Ypix are the pixel coordinates of p, \\theta and $\\phi$ are its corresponding spherical coordinates and W and H are the resolutions of panorama. We then transform the spherical coordinate system to the Cartesian coordinate system to obtain the camera coordinates Pcamera corresponding to p :\nXcam = cos($\\phi$) sin($\\theta$)\nYcam = sin($\\phi$)\nZcam = cos($\\phi$) cos($\\theta$). (2)\nThe camera coordinates of pcam are converted to world coordinates pworld through the camera's pose matrix. This allows us to compute the ray from the camera center to pworld in the world coordinate system and construct the spherical epipolar attention module.\nGiven N feature maps F = {Fi|1 \u2264 i \u2264 N} corresponding to N panoramic images and their respective camera pose matrices, we implement cross-attention between different views through the spherical epipolar-aware module. During the generation process, each feature map in F can be considered as the target view, and the K nearest views are selected from the remaining features as reference views.\nFor each feature point in the target view feature map, we uniformly sample S points on the ray between the feature point and the camera. All sampled points are reprojected onto the feature maps of the K reference views, and the corresponding feature values are obtained through interpolation. We denote the features in the target view as q, and the features of all sampled points in the reference views as k and v. The cross-attention is then constructed using these features.\nLet pi be a feature point in the target view feature map Ft, and pi,j|1 \u2264 j \u2264 S be the S uniformly sampled points on the ray between pi and the camera center. We reproject these points onto the K reference view feature maps Fre|1 \u2264 k \u2264 K to obtain the corresponding feature values fi,j,k|1 \u2264 j \u2264 S, 1 \u2264 k \u2264 K. The query qi, key ki, and value vi for the cross-attention mechanism are defined as follows:\nqi = Ft(pi), ki = fi,j,k|1 \u2264 j \u2264 S, 1 \u2264 k \u2264 K, Vi = fi,j,k|1 \u2264 j \u2264 S, 1 \u2264 k \u2264 K. (3)\nThe cross-attention output oi for the feature point pi is computed as:\noi =\n$\\sum_{k}^{K}Attention(qi, ki, vi) = softmax(\\frac{q_i^{T}k_i}{\\sqrt{d}})vi$, (4)\nwhere d is the dimension of the query and key vectors.\nPositional Encoding To enhance the model's understanding of 3D spatial relationships between different views, we follow EpiDiff [20] to employ the positional encoding method from Light Field Networks (LFN) [44]. In the world coordinate system, let pi be a pixel in the target view, and or and di be the origin and direction of the ray between pi and the camera center, respectively. The Pl\u00fccker coordinates ri of the ray are computed as:\nri = (oi \u00d7 di, di). (5)\nFor each sampled point pi,j on the ray, its corresponding spherical depth zij is transformed using a harmonic transformation to get yz(zi,j). Similarly, the Pl\u00fccker coordinates ri are transformed as yr (ri). The positionally encoded features yr(ri) and yz(zi,j) are then concatenated to obtain the combined positional encoding y(ri, zi,j):\ny(ri, zi,j) = [yr(ri), yz(zi,j)]. (6)\nThe combined positional encoding (ri, zi,j) is then concatenated with the feature maps Ft and Frk |1 \u2264 k \u2264 K to obtain the enhanced feature representations Ft and Frk|1 \u2264 k \u2264 K:\n$\\hat{F_t}(p_i) = [F_t(p_i), \\gamma(r_i, z_{i,j})], \\hat{F_{r_k}}(p_{i,j}) = [F_{r_k}(p_{i,j}), \\gamma(r_i, z_{i,j})]$, (7)\nwhere [,] denotes concatenation. These enhanced feature representations are then used in the cross-attention mechanism to improve the model's understanding of 3D spatial relationships between different views.\nTwo-Stage Training The main difference between panoramic images and perspective images is that panoramic images contain 360\u00b0 content of the surroundings, while perspective images only contain content from a given viewpoint. Therefore, when the camera only rotates or translates by a small amount, the corresponding content in the panoramic image hardly changes. To make the generated multi-view panoramic images better match each corresponding text, we divide the training into two stages. In the first stage, we use the selected dataset with almost no change in image content (small camera movement) for training, which enhances the effect of the spherical epipolar-aware attention module. In the second stage, we increase the camera movement distance between each viewpoint and train with images that generate new content, improving the model's ability to understand text based on changes in perceived spatial location while ensuring multi-view consistency and enhancing scalability."}, {"title": "5 Experiment", "content": "Dataset We leverage the Habitat Simulator [41] to render a panoramic video dataset based on the Habitat Matterport 3D (HM3D) dataset [37]. The pipeline of dataset rendering and captioning is shown in Sec. 3. After post-processing such as dataset filtering, we constructed 8,508 panorama-text pairs as training sets for single-view panorama generation. For multi-view panorama generation, we constructed 19,739 multi-view panorama-text pairs with nearly identical image content and 18,704 multi-view panorama-text pairs with different image contents as training sets. For specific details regarding the dataset, please refer to Appendix B.\nImplementation Details In the multi-view panorama generation, we simultaneously generate N = 4 panoramas from different viewpoints. Within the spherical epipolar-aware attention module, we consider the two nearest views to the target view as reference views, i.e., K = 3, and sample S = 10 points along each ray. We conducted separate training for 100 epochs on datasets with almost identical image content and datasets with varying image content. Please refer to Appendix C for further implementation details."}, {"title": "6 Conclusion", "content": "We have proposed the panoramic video-text dataset and panorama generation framework with spherical epipolar-aware attention for text-to-single-view or multi-view panorama generation. Extensive experiments demonstrate that our method can achieve scalable, consistent, and diverse multi-view panoramas.\nLimitation and Future Work Although our method demonstrates the ability to generate consistent multi-view panoramas under the same setting as the training phase, it is important to note that as the number of frames increases during inference, the model tends to hallucinate content.\nExploring the use of video diffusion models to improve the consistency of generated multi-view panoramas is a promising direction. Longer panoramic videos are expected to be realized based on the generated panoramas as conditions."}, {"title": "B.1 Applications", "content": "B.1.1 Text to Single-View Panorama\nB.1.2 Text to Multi-View Panoramas\nDiffPano can generate panoramic video frames with large camera pose spans and multi-view consistency based on diverse textual descriptions, thus achieving the effect of text-to-panoramic video, as shown in the Fig.11."}, {"title": "C Network Architecture and Training Details", "content": "Network Architecture Our LoRA-based single-view panorama generation model produces panoramas with a resolution of 512\u00d71024. In the multi-view panorama generation approach, we generate continuous frames of panoramas with a resolution of 256\u00d7512. Generating multiple frames of 512x1024 resolution panoramas simultaneously would consume a significant amount of computational resources. Moreover, our experiments reveal that generating multi-frame high-resolution panoramas requires an exceptionally long training time to improve the quality of the generated images. The network architecture of our multi-view panorama generation model is shown in Table 6 and Table7.\nTraining Details We fine-tuned the Stable Diffusion v1.5 model using the LoRA method for single-view pano-based synthesis. The training was conducted on 6 A100 GPUs with 80GB memory for 100 epochs (approximately 6.5 hours), with a learning rate of 1e-4 and a batch size of 6. For"}, {"title": "D Societal Impact", "content": "Since our method can achieve scalable, consistent, and diverse multi-view panoramas, it has many potential applications, such as unlimited room roaming in VR, interior design preview, embodied intelligent robot exploration, etc."}, {"title": "A Spherical Epipolar Line Computation", "content": "Given the pixel coordinates of a point in the panorama of the target view i, the corresponding epipolar line in the panorama of the source view j can be calculated. First, according to Eq. (1), the camera coordinates p in the target view i are computed. Based on the relative pose {Ri\u2192i, Ti\u21921}, p' in the source view j are calculated:\np' = Rip + Ti\u2192j. (8)\nSimultaneously, the coordinates o' of the camera origin o = (0, 0, 0)T in the target view i projected onto the source view j are computed:\no' = Rijo + Tij. (9)\nThen, we need to find the plane L containing the three points p', o, and o' in the camera coordinate system of the source view j. The intersection of this plane with the spherical surface is the desired epipolar line. The equation corresponding to plane L is:\nAX + BY + CZ + D = 0. (10)\nSubstituting the coordinates of the three points into the equation yields the coefficients A, B, C, and D:\n$\\begin{aligned}\nA &= \\frac{Y_{o'} Z_{p'} - Y_{p'} Z_{o'}}{Y_{p'} X_{o'} - Y_{o'} X_{p'}} C \\\\\nB &= \\frac{Z_{o'} X_{p'} - Z_{p'} X_{o'}}{Y_{p'} X_{o'} - Y_{o'} X_{p'}} C\\\\ D &= 0.\n\\end{aligned}$\n(11)\nTo simplify the representation of the equation for plane L, we introduce new coefficients a1 and 22:\na1 =\n$\\begin{aligned}\n  \\frac{Y_{o'} Z_{p'} - Y_{p'} Z_{o'}}{Y_{p'} X_{o'} - Y_{o'} X_{p'}} C \\\\\na2 &= \\frac{Z_{o'} X_{p'} - Z_{p'} X_{o'}}{Y_{p'} X_{o'} - Y_{o'} X_{p'}},\n  \\end{aligned}$\nthe equation of plane L can be simplified to:\na1X + a2Y + Z = 0. (13)\nAccording to Eq.1 and Eq.2, the epipolar line equation in the source view j can be obtained:\nYpix = H \u00b7 arctan$\\{\\frac{a_1sin(\\frac{2\\pi X_{pix}}{W})}{a_2} + cos(\\frac{2\\pi X_{pix}}{W})\\/\\pi + 0.5\\}$, (14)\nwhere Xpix and Ypix are the corresponding pixel coordinates."}, {"title": "B Experiment Details", "content": "Dataset Processing In single-view panorama generation, we select 100 scenes from HM3D [37] and render cube maps from random viewpoints within each scene, which are then stitched into panoramas through interpolation. However, due to the imperfect quality of the corresponding scene meshes, which may have missing parts, we filter out images with a high proportion of zero-depth values based on their corresponding depth maps, ultimately creating a dataset of 8,508 panoramas."}]}