{"title": "ARChef: An iOS-Based Augmented Reality Cooking\nAssistant Powered by Multimodal Gemini LLM", "authors": ["Rithik Vir", "Parsa Madinei"], "abstract": "Cooking meals can be difficult, causing many to use cookbooks and online recipes, which results in\nmissing ingredients, nutritional hazards, unsatisfactory meals. Using Augmented Reality (AR) can address\nthis issue, however, current AR cooking applications have poor user interfaces and limited accessibility.\nThis paper proposes a prototype of an iOS application that integrates AR and Computer Vision (CV) into\nthe cooking process. We leverage Google's Gemini Large Language Model (LLM) to identify ingredients\nbased on the camera's field of vision, and generate recipe choices with their nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user interface compatible with iOS\ndevices. Users can personalize their meal suggestions by inputting their dietary preferences and rating each\nmeal. The application's effectiveness is evaluated through user experience surveys. This application\ncontributes to the field of accessible cooking assistance technologies, aiming to reduce food wastage and\nimprove the meal planning experience.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the importance of cooking meals, using\ntraditional cooking methods can be a struggle for many,\nespecially those with limited cooking experience. In fact, a\n2013 study in the United Kingdom identified that 24.4\npercent of men, along with 7 percent of women, are not\nconfident in their abilities to cook [1]. Additionally, access\nto cooking aids such as robot cooking assistants or private\nchefs are extremely limited due to their cost, leading to\nnearly half of all 1.03 billion pounds of food waste being\ngenerated at the household level [2]. Due to the limited\naccessibility of cooking assistants, many resort to using\nconventional cooking methods such as cookbooks and\nonline recipes; however, doing so results in a struggle to\nfind a recipe that can be made using only the ingredients the\nuser has. The result of this inconvenience is unsatisfactory\nmeal choices, food wastage, and the skipping of meals\nentirely [3]. In addition, nutritional information, including\npotential allergens and intolerances, isn't always displayed\nin these conventional methods, which can cause chronic\nhealth problems and allergic reactions [4].\nBy creating an accessible cooking aid that can display\nmeal options using only the ingredients the user has, food\nwastage and hunger can be mitigated. Using Augmented\nReality (AR) technology, can serve as a more accessible\nsolution to this problem, especially when combined with\nComputer Vision (CV) to identify and display the\ningredients at hand. Additionally, Artificial Intelligence\n(AI) enables users to obtain high-quality meal options along\nwith detailed nutritional information. The result of this\ncombination, which also incorporates principles regarding\nthe user interface from Milgram and Kishino's\nReality-Virtuality (RV) continuum, is a hassle-free user\ninterface that provides detailed instructions for cooking\nspecific meals [5].\nThere have been several studies creating AR user\ninterfaces for cooking, including one that attempted to\ncreate a personalized application tailored towards users [6].\nIn this previous study, Iftene et al. developed an application\ncalled AREasyCooking which took in the user's dietary\nrestrictions and favorite cuisines to create a personalized\napplication for each user. However, when recommending\nrecipes, the AREasyCooking application would recommend\nrecipes from a pre-existing database, rather than using AI to\ngenerate them, not addressing the problem of users being\nable to find a recipe that uses all of their ingredients.\nAnother study attempted to build off of this by using the\nMagic Leap One AR headset and the YOLOv5 deep\nlearning model to scan ingredients and recommend recipes\n[7]. However, the accuracy of the YOLOv5 model when\ndetecting ingredients was relatively low, as it was\nprioritized for speed, so it had difficulty when detecting\ncertain ingredients. Additionally, the user would have to"}, {"title": "II. METHODS", "content": "To develop our application, we used Apple's ARKit\nprogramming interface via Xcode 15.4. We used the Google\nAI Software Development Kit to access the Google Flash\n1.5 Gemini model in our application and created the API\nkey through Google AI Studio.\nFor the first component of our application, the live-AR\nlabeling for ingredient detection, we utilized the ability of\nthe Gemini model to process multimodal inputs and\nattached a snapshot of the device's screen along with a\nprompt asking for the model to return the coordinates of\neach ingredient. Using the coordinates received, we added\nlabels to the screen above each ingredient and added the\ningredients detected to a list, which contained the total list\nof ingredients identified over time.\nWe used this list to attach to the prompt when prompting\nthe model to return the list of optimal recipes in order to\noptimize accuracy when suggesting meals. The Gemini\nmodel is also prompted to return a detailed recipe for each\nmeal suggestion that clearly outlines the ingredients and\namounts needed. An example of the layout of the scanned\ningredients and the suggested recipes is shown in Figure 1.\nIn addition, our application also features an AI assistant\navailable through the forms of a voice assistant and a\nchatbot. Both assistants stem from the same Gemini LLM\nand also use system instructions to tailor the behavior of\neach model. These assistants use the same system\ninstructions that form them to serve as the role of a\nknowledgeable sous chef who provides cooking advice,\nrecipe suggestions, and answers food-related questions.\nBoth these assistants have access to the ingredients the user\nhas scanned, the recipes they have been recommended, and\nthe selected recipe.\nTo optimize the user experience from the proposed\napplication, the Gemini model calculates the nutritional\ninformation that includes the amount of calories, fat,\ncarbohydrates, fiber, protein, and various vitamins. The\nGemini LLM then displays this information along with\npossible allergens next to each recipe in order to prioritize\nuser safety.\nIn order to help navigate users throughout the recipes, our\napplication includes the feature to check if a user has done a\nstep in the recipe correctly. This is done by having the\nmodel receive a snapshot of the user's workspace, along\nwith a prompt for it to provide feedback on the user's\nperformance. An example of the feedback provided is\ndisplayed in Figure 2.\nOur application is designed to be used either as a\nhandheld tool by having users interact with the app by\nusing buttons to capture snapshots of their environments\nwhile interacting with the heads-up displays located around\nthe screen. Users can also have back-to-back conversations\nwith the Al voice assistant to generate recipes and ask\nfollow-up questions on each recipe with the press of a\nbutton, to prompt it to listen to what the user is saying."}, {"title": "III. RESULTS & DISCUSSION", "content": "In this paper, we surveyed a total of 21 volunteers,\nranging from high schoolers to senior citizens. These\nvolunteers rated their knowledgeability and frequency of\ncooking to be an average of 2.714 and 2.857 on the 5-point\nLikert scale, respectively. Additionally, the participants\nnoted that they often struggle when deciding what to cook,\nas they chose an average score of 4.524 out of 5 when\nasked this question.\nWe evaluated the user experience of the volunteers by\ntaking their average score on each of the questions in the\nsecond section of the survey, which evaluated their\nexperience using the application. In the first round of\nsurveys, the 6 participants had an average usability score of\n3.75 out of 5, demonstrated in Figure 3, indicating that our\napplication somewhat helped their cooking experience.\nThese participants also provided feedback on the\napplication and what could be improved. The advice we\nimplemented into the next round of surveys was that they\n\"wished the windows self-organized themselves SO\n[they're] easy and clear to see.\" Another piece of feedback\nwe implemented into the next version of the application\nwas that we included \"a way to remove items from the\n[ingredient list] or manually add an ingredient that was\nmisclassified.\"\nIn the second round of user experience surveys, Figure 3\ndisplays that the average usability score of the next 7\nparticipants increased to 4.27 out of 5. The factors that\naffected this increase in the usability score the most were\nhow easy it was to learn how to use the application, as well\nas whether the participants were able to carry out system\nfunctions without difficulties or errors. The primary factor\nfor these improvements was the organization that was\nimplemented in between these surveys, as we gave the\napplication a more rigid structure by organizing the\nwindows for the user rather than leaving it up to them.\nAdditionally, we conducted a round of bug-fixing between\nthese rounds of surveys, which reduced the amount of\nerrors and inconveniences in the application.\nBy the third round of interviewing the participants, we\nimplemented the personalization feature of our application\nwhere recipes would be tailored to users dietary restrictions,\nfavorite cuisines, and cooking level. This resulted in an\naverage usability score of 4.36 out of 5, indicating that the\naverage user was very satisfied with our application.\nFurthermore, every user in this round reported that they\nwere satisfied with our application and would recommend it\nto others.\nOverall, throughout each section of the surveys, each\nparticipant was at least somewhat satisfied with our\napplication, and by the last round, every participant either\nagreed or strongly agreed that they would use our\napplication again."}, {"title": "IV. CONCLUSIONS", "content": "The research conducted in this paper, along with the\napplication developed, serves as a stepping stone for\nimplementing AR into the cooking experience through\naccessible mediums such as an iOS device. This application\nalso provides an example usage of the Gemini LLM into\naccessible cooking technology, as it can be used to scan\ningredients, generate recipes, check and provide feedback\non cooking steps, and highlight allergens and other\npotential food hazards.\nWe tested the impact of our application on the cooking\nexperience of a total of 21 volunteers through 3 separate\nrounds of surveys. The usability score of the application\nwent from 3.75 to 4.36 over the course of the 3 rounds,\nindicating that the volunteers were very satisfied by our\napplication in the cooking experience.\nBy using a similar framework, future studies can use the\nadvances in the depth perception of the cameras in iOS\ndevices to have users interact with AR elements in the AR\nscene. This would limit interaction with the screen itself,\nmaking the cooking experience less messy. Additionally,\nfuture works can use even faster and more powerful Al\nmodels to have live-labeling of ingredients while still using\nthe knowledge of the Gemini model to generate helpful,\ncreative recipes."}]}