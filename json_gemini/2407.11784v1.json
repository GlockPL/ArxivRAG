{"title": "DATA-JUICER SANDBOX: A COMPREHENSIVE SUITE\nFOR MULTIMODAL DATA-MODEL CO-DEVELOPMENT", "authors": ["Daoyuan Chen", "Haibin Wang", "Yilun Huang", "Ce Ge", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "abstract": "The emergence of large-scale multi-modal generative models has drastically ad-\nvanced artificial intelligence, introducing unprecedented levels of performance\nand functionality. However, optimizing these models remains challenging due to\nhistorically isolated paths of model-centric and data-centric developments, lead-\ning to suboptimal outcomes and inefficient resource utilization. In response, we\npresent a novel sandbox suite tailored for integrated data-model co-development.\nThis sandbox provides a comprehensive experimental platform, enabling rapid\niteration and insight-driven refinement of both data and models. Our proposed\n\"Probe-Analyze-Refine\u201d workflow, validated through applications on state-of-the-\nart LLaVA-like and DiT-based models, yields significant performance boosts, such\nas topping the VBench leaderboard. We also uncover fruitful insights gleaned\nfrom exhaustive benchmarks, shedding light on the critical interplay between data\nquality, diversity, and model behavior. With the hope of fostering deeper under-\nstanding and future progress in multi-modal data and generative modeling, our\ncodes, datasets, and models are maintained and accessible at https://github.\ncom/modelscope/data-juicer/blob/main/docs/Sandbox.md.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of multi-modal generative models has revolutionized artificial intelligence, pushing\nthe boundaries of functionality and creativity across various domains (OpenAI, 2024a;b; Wang\net al., 2024). Recognizing the pivotal role of training data in shaping model performance, there are\nfast-growing efforts to curate datasets of larger scales and higher quality (Jakubik et al., 2024).\nHowever, the development trajectories of these models and datasets have historically diverged, guided\nmore by intuition than by systematic co-development methodologies. Recent advances in enhancing\nmulti-modal generative models tend to be either model-centric or data-centric, rarely bridging the\ntwo aspects cohesively. For example, model-centric methods focus on algorithmic enhancements and\narchitectural innovations under fixed data priors, while data-centric strategies usually concentrate\non processing and cleaning datasets independently of specific model training contexts (Qin et al.,\n2024). Both approaches usually suffer from a lack of systematic guidance and cooperative synergy,\nrelying heavily on heuristic exploration and single-perspective expertise. This fragmented landscape\npresents a significant barrier to achieving optimal model performance, as the interplay between data\ncharacteristics and model capabilities remains largely underexploited.\nMoreover, the practical implementation of multi-modal generative models is further complicated by\ninfrastructure constraints, escalating computational costs, and the accelerating pace of development\ncycles (Xu et al., 2024b). In the age of large-scale models with rapidly growing model parameters\nand dataset sizes, the processes of data processing and model training become increasingly resource-\nintensive, demanding substantial time and computations. Due to the absence of cost-effective\nplatforms that simplify and speed up data-model co-development, researchers and developers often\nface the dilemma of prioritizing result-driven development at the expense of thorough, insight-led\nexploration. This deficiency hinders the iterative refinement for both domains, leading to sub-optimal\noutcomes as improvements in one domain are hard to inform, apply and enhance each other directly.\nTo fill this gap, we introduce the Data-Juicer Sandbox, a comprehensive suite for facilitating the\nco-development of multi-modal data and generative models. Building upon an open-source data\nprocessing system tailored for multi-modal generative models, Data-Juicer (Chen et al., 2024a), our\nsandbox suite further integrates a wealth of off-the-shelf components optimized for usability and\ncompatibility with existing model-centric infrastructures. Collectively, it offers flexibly customizable\norchestration from different levels including end-to-end workflows, specific development behaviors,\nand underlying data-model development capabilities. Within the sandbox laboratory, users are\nempowered to rapidly explore different data pipelines and model configurations on cost-controlled\ndatasets and models. This accelerates the discovery of insightful patterns and informed decision-\nmaking, ultimately paving the way for scalable, resource-efficient data-model development.\nTo exemplify the efficacy of our sandbox, we propose a \u201cProbe-Analyze-Refine\u201d workflow, meticu-\nlously crafted to explore the synergies between data processing operators, target model metrics, and\nthe scalability of these enhancements. We apply this workflow to two cutting-edge models: Mini-\nGemini (Li et al., 2023), an LLaVA-inspired model for image-to-text generation, and Easy Animate\n(Xu et al., 2024a), a Diffusion Transformer based model for text-to-video generation. Thanks to the\nsandbox's capabilities, we attain significant advancements in data quality and model performance,\nsuch as achieving top spot on the VBench (Huang et al., 2024) leaderboard, outperforming strong\ncompetitors such as VideoCrafter (Chen et al., 2024b) and AnimateDiff (Guo et al., 2024). These\nachievements are underpinned by a series of insights linking 31 data processing operators and 35\nmodel metrics, including analysis of the fine-grained impact of data processing for model training,\nthe delicate balance between data diversity and model performance, and the strategic optimization of\ndata scaling for enhanced model-data co-development.\nOur contributions can be summarized as follows:\n\u2022 Innovative Sandbox Suite: To the best of our knowledge, this is the first open-source sandbox\nsuite tailored for co-development between multi-modal data and generative models, rendering\nexperimental exploration in this field more insightful, systematic, convenient and reusable.\n\u2022 Effect-Proven Workflow: We present a new progressive workflow for data-model co-development\nand substantiate its impact through extensive empirical evidence such as achieving new top-tier\nperformance in image understanding and video generation tasks.\n\u2022 Practical Guidance: We conduct extensive experiments on benchmarking the effects of dozens\nof data processing operators and model metrics, providing fruitful and valuable insights toward\nfurther advancements in multi-modal generative models."}, {"title": "2 RELATED WORKS", "content": "Model-Centric Progress in Multimodal Generative Models. Multimodal generative models\nhave captivated researchers with their formidable capabilities (OpenAI, 2024a;b), leading to a surge\nin model-centric development efforts. These focuses mainly lie in refining training algorithms\n(Caffagni et al., 2024; Li et al., 2024; Zhang et al., 2024), advancing model architectures and\ncomponents (He et al., 2024a; Yin et al., 2024; Jiao et al., 2024), and harnessing the models' generative\npotential for various applications (Wang et al., 2024; Liu et al., 2024a; Zhou et al., 2024). There is a\ngrowing consensus that transformer-based scaling is predominant (Xu et al., 2023). However, the\nhigh computational requirements imposed by scaling laws (Xu et al., 2024b) and the optimization\nchallenges inherent to generative models (Manduchi et al., 2024) often confine insights to specific\ndatasets or vague data characteristics. This situation leaves a significant gap in comprehending the\nextent to which models' performance and behavior hinge upon implicit assumptions and inductive\nbiases embedded within the underlying data distributions.\nTrends in Data-Centric Development for Multimodal Generative Models. An emerging trend\nshifts the focal point from models to data (Jakubik et al., 2024; Bai et al., 2024), underscored by\nthe notion that large models function akin to data compressors (Del\u00e9tang et al., 2024). Echoing the\nprinciple of \"garbage in, garbage out\u201d, meticulous data processing is recognized as pivotal. Efforts\nnow isolate data manipulation as a primary experimental variable in multimodal generative modeling"}, {"title": "3 THE PROPOSED DATA-JUICER SANDBOX LABORATORY", "content": "3.1 MOTIVATION AND DESIGN\nWhy do we need data-model co-development? In the era of large models, the development of\nboth data and models necessitates a collaborative approach involving numerous algorithm researchers"}, {"title": "3.2 FLEXIBLE CAPABILITY FACTORY AND BEHAVIOR HOOKS", "content": "Components for Advanced Data Processing. Within the sandbox, we design numerous factory\nclasses and behavior hooks for data processing and evaluation, simplifying and unifying the interfaces\nprovided by the open-source Data-Juicer system (?). Users gain great flexibility to leverage over 100\nfeature-rich operators and dedicated tools for efficient data analysis, processing (including filtering,\nmodifying, generating, etc.), and evaluation:\n\u2022 From an actionable perspective, key components include DataProcessors and DataRecipeRefiners.\nDataProcessors invoke off-the-shelf data processing operators from Data-Juicer, such as Filters\nand Mappers, enabling accelerated and scalable data processing through system optimization and\nparallel support. DataRecipeRefiners offer tools to refine recipe configurations, such as adjusting\nfiltering thresholds based on k-sigma principle or percentile distribution to differentiate data subsets.\n\u2022 From a measurable perspective, DataAnalyzers and DataEvaluators are provided. DataAnalyzers\nencapsulate various data analysis tools from Data-Juicer, performing efficient computations of\ntarget metrics like text perplexity or video aesthetic value, and providing common statistical values"}, {"title": "3.3 A PROBE-ANALYZE-REFINE WORKFLOW FOR DATA-MODEL CO-DEVELOPMENT", "content": "To demonstrate the usage of the sandbox laboratory, here we elucidate a built-in systematic workflow.\nAs illustrated in Figure 2, we commence with a series of cost-effective contrastive explorations\naimed at addressing several key questions, before judiciously applying them to enhance large-scale\ndata-model co-development:\n1. Initially, we seek to ascertain which data processing operators (OPs) contribute most effectively\nto enhancing specific target model metrics? This involves creating equal-size data pools, each\nprocessed uniquely by a singular OP and subsequently ranked by data metrics. Models are trained\non these data pools, enabling us to perform an in-depth analysis of OP effectiveness and its\ncorrelation with model performance across various quantitative and qualitative indicators."}, {"title": "3.3.1 SINGLE-OPERATOR DATA POOLS", "content": "Data-Juicer provides a rich assortment of OPs tailored for multimodal data encompassing text,\nimages, videos, and audio. Given the variability in data sources, models, and downstream tasks, it\nis imperative to initially ascertain the impact of each OP to guide the optimization of data-model\nco-development effectively.\nTo this end, starting with an initial dataset D, we define a single-operator data pool $P_i$ as the dataset\nprocessed exclusively by the i-th OP ($OP_i$) available in Data-Juicer as follows:\n$P_i = D_J[OP_i(p_i)](D)$,\nwhere $D_J$ denotes the data-processing function implemented by Data-Juicer, and $p_i$ is the hyper-\nparameters governing the operation of $OP_i$. The OPs of Data-Juicer can compute specific statistics\nfor every data pools, and apply threshold criteria to selectively filter or modify the data based on\nthese statistics. Within this workflow, the initial dataset D into three equal-sized segments for filter\nOPS: $P_{high}, P_{middle}$ and $P_{low}$, representing data with high, moderate, and low statistical measures,\nrespectively. A randomly sampled $P_{random}$ serves as a control group. This stratification fosters\ndiscriminative insights across varying degrees of data processing intensity.\nSubsequently, models are trained independently on each data pool, undergoing comprehensive\nevaluation across multiple performance metrics. Throughout the training, we uphold consistent\nhyper-parameters and ensure $P_{random}$ is of substantial size to yield a reliable and robust average\nperformance across all downstream tasks. This setup enables the identification of best-performing\nOPs that universally excel or excel under specific evaluation criteria.\nImportantly, we adhere to a cost-conscious design, implementing strategies to curtail expenses, such\nas downsizing the data pools, limiting training iterations, or adopting efficiency-enhancing techniques\nlike LORA (Hu et al., 2021). It enables the entire experimental pipeline to conclude at an affordable\nlevel. Specifically, all of our experiments can be completed within a single day utilizing a single\nA100 GPU, remaining both feasible and scalable for broader adoption in data-model co-development\nendeavors."}, {"title": "3.3.2 MULTI-OPERATORS DATA POOLS", "content": "Having explored the individual impact of each OP, our next step logically progresses to understanding\nthe dynamics when multiple OPs are sequentially applied as in a data \"recipe\". Our aim is to discern\nwhether these OPs complement or counteract each other's effects. To achieve this, we extend the data\npool construction methodology from previous individual OP scenarios to a sequence of OPs:\n$P = D_J[OP_1(p_1), OP_2(p_2), ..., OP_i(p_i)](D)$\n$= D_J[OP_1(p_1)](D) \u2229 D_J[OP_2(p_2)](D) \u2229 \u2229 D_J[OP_i(p_i)](D)$.\nThe number of possible multi-OP data pools grows exponentially with the addition of each new OP,\nnecessitating a strategic selection of combinations to explore. Drawing on insights from single-OP\nexperiments, we propose a pragmatic strategy: combining \u201cTop\u201d OPs, those with progressively dimin-\nishing impacts on model performance, while incrementally increasing the number of combinators.\nFurthermore, we integrate the analysis of inter-OP relationships into our recipe formulation. Our\nworkflow accommodates two approaches:"}, {"title": "3.3.3 PYRAMID-SHAPED DATA POOLS", "content": "Incorporating a greater number of OPs in a recipe may lead to enhanced data quality; however,\nas per Equ. (2), the resultant data pool volume decreases exponentially with each additional OP.\nThis phenomenon prompts a critical investigation: should we prioritize reusing high-quality data or\nincorporate lower-quality yet more abundant data to escalate training dataset sizes?\nTo encapsulate this inherent trade-off between data scale and quality, we devise a hierarchical pyramid\narchitecture for data pools. Given k \u201ctop\u201d OPs, we can create $2^k 1$ combinations of these OPs,\nas depicted in the left-bottom area in Figure 2. For example, the combination of 3 OPs, $OP_{1,2,3}$,\nresides at the highest level of the hierarchy but results in the smallest data pool after Data-Juicer\ndata processing. The combinations of 2 operators, such as $OP_{1,2}$, are placed at a lower level, and\nthe resulting data pool encompasses that of $OP_{1,2,3}$ is several times larger in volume. Leveraging\nfindings from Section 3.3.2, we can pinpoint the optimal OP combinations within this pyramid\nstructure. Progressing downward through the pyramid, data pools exhibit a descending average OP\nranking (potentially indicative of reduced data quality) alongside an increase in volume.\nTo reconcile the desire for larger datasets without compromising quality, we devise two experimental\nstrategies built upon this data pyramid: (1) iterative model training with data repetition from the\ntop-layer (highest-quality) data pools, and (2) non-repetitive training incorporating progressively\nlower-quality, larger-volume data pools from the lower-layer pools. Specifically, we extract a\npredetermined quantity of data from the top-layer pool for iterative training with variable repetition\nrates. In parallel, as a baseline, we assemble a non-repeating dataset by consolidating pools from the\nupper to lower pyramid levels to match the size of the iterated dataset.\nThese comparative studies enable us to qualitatively assess the efficacy of data reuse versus the\ninclusion of suboptimal data within a fixed dataset size. Notably, all data pools are uniformly sampled\nfrom the original dataset D, ensuring that the relative proportions of pyramid data pools remain\nconsistent as D expands. Thus, insights garnered from these experiments can be extrapolated to\nlarger-scale data contexts, informing efficient scaling of data-model co-development practices."}, {"title": "4 PRACTICAL APPLICATIONS", "content": "In this section, we illustrate the application of the Data-Juicer sandbox in two distinct scenarios,\ndemonstrating its versatility and effectiveness in enhancing multimodal data-model co-development.\nThese examples instantiate the behavior hooks and capability classes detailed in Section 3.2, following\nthe workflow outlined in Section 3.3. Subsequently, Section 5 delves into the primary findings and\ninsights derived from these two practical scenarios."}, {"title": "4.1 IMAGE-TO-TEXT GENERATION", "content": "Our first task focuses on foundational image understanding ability, by experimenting on Mini-Gemini\n(MGM-2B), a state-of-the-art (SOTA) 2 billion parameter multimodal LLM (Li et al., 2023). The\ntraining protocol for MGM-2B involves two stages: pretraining and fine-tuning. Our experimental\nfocus lies in the pretraining phase, which seeks to harmonize visual and textual representations.\nWe utilize the original pretraining dataset, consisting of approximately 1,226k instances, as our\noriginal dataset D. The single-OP and multi-OP data pools are capped at a maximum of 200k"}, {"title": "4.2 TEXT-TO-VIDEO GENERATION", "content": "For the second task, text-to-video generation, we adopt the advanced DiT-based EasyAnimate (Xu\net al., 2024a) model, which integrates diverse datasets totaling 1,217k instances from InternVid\n(Wang et al., 2023b) (606k), Panda-70M (Chen et al., 2024c) (605k), and MSR-VTT (Xu et al., 2016)\n(6k). Baseline experiments are executed on a subset of 40k instances, employing LoRA (Hu et al.,\n2021) for efficiency. Model outputs are assessed using VBench (Huang et al., 2024) across 16 metrics\non video quality and video-text matchness.\nOur investigation covered 21 OPs, including 13 text-only OPs and 10 video-related multimodal\nOPs. Analogous to the image-to-text generation, we conduct single-OP and multi-OP combination\nexperiments. However, given the reduced relevance of data statistics in video-related OPs, our\nanalysis centers on the correlations among the 16 VBench evaluation metrics. These metrics are\nclustered into three groups, with the best-performing OP selected from each group.\nThrough OP combination experiments, we pinpoint the most effective set of OPs. We then sample\n40k instances from the filtered data pool and repeat the training process for up to 10 epochs. For\ncomparative analysis, we adhere to the method outlined in Section 3.3.3, selecting larger data volumes\n(80k, 120k, up to 400k instances) for single-epoch training. This exploration also leads to valuable\ninsights for optimizing text-to-video model training, especially on informed decisions of data filtering\nOPs and ideal iteration counts.\nAcross all experiments, results are reported as averages from 2 to 5 repetitions, inclusive of standard\ndeviations. Detailed training configurations, performance metrics, and OP descriptions are provided\nin the Appendix A."}, {"title": "5 MAIN RESULTS AND INSIGHTS", "content": "In this section, we show the results of the specific applications of the probe-analyze-refine workflow\nthat we proposed for data-model co-development in image-to-text and text-to-video generation\n(Section 4). Initially, by conducting experiments with individual operators, we investigate the impact\nof different data selection strategies on the model, summarizing fundamental insights (Section 5.1).\nSubsequently, building on this foundation, we select suitable candidate operators for combination\nexperiments to identify the appropriate recipe (Section 5.2). After discovering this recipe, we\nconstruct a data pyramid hierarchy to explore whether to reuse high-quality data or sacrifice quality\nfor increased data diversity (Section 5.3). Finally, leveraging the insights gained, we scale up the data\nand model to train high-quality models (Section 5.4).\nDue to the space limitation, we present main conclusions and defer complete numeric results and\ndetails in Appendix B. Specifically, for the image-to-text generation, we mainly report the increase in\naverage performance relative to the baseline on TextVQA (Singh et al., 2019), MMBench (Liu et al.,\n2023), and MME (Fu et al., 2023). For the text-to-video generation, we report the improvement in"}, {"title": "5.1 RANKING SINGLE-OPERATOR DATA POOLS", "content": "Table 1 illustrates the performance of some models trained on single-operator data pools, where the\ndata processing OPs alongside others of particular interest. For a comprehensive analysis and detailed\nexperimental results for each OP, please refer to Appendix B.1."}, {"title": "5.2 SHAPING DATA RECIPES OF TOP-3 OPERATOR COMBINATIONS", "content": "Based on the insights acquired from single OP experiments, we can combine OPs into recipes for\ndata filtering to obtain higher-quality data. Figure 3 illustrates the performance changes brought by\ndifferent recipe combinations compared to the baseline data pool."}, {"title": "5.3 HARNESSING MORE HIGH-QUALITY DATA", "content": "As discussed in Section 3.3.3 and illustrated in Figure 3, we can construct pyramid-shaped data pools\nby combining different OPs. Figure 3 provides specific performance metrics for each pool within this\npyramid structure. The best-quality data often resides at the higher levels of the pool; however, these\nlevels also contain less data. Therefore, during model training, it is crucial to determine when to\nreuse high-quality data and when to introduce suboptimal data to ensure diversity. The experiments\nin this section are designed to address this question.\nIn these experiments, we select the best-performing data pool from Figure 3 as the optimal data pool.\nSpecifically, for the image-to-text model, we select the data pool with a low text action number and\na high language score, and for the text-to-video model, we choose the data pool with a low video\nNSFW score and a high frames-text similarity. We also adhere to the setup in Figure 3, where the\nbaseline for image-to-text trains on 159k data instances, and the baseline for text-to-video trains on\n40k data instances."}, {"title": "5.4 APPLYING DATA-MODEL INSIGHTS IN LARGER-SCALE SCENARIOS", "content": "In this section, we apply the insights gained from the previous experiments to larger-scale models\nand datasets. Specifically, in line with the best data recipe from Figure 3 and Observation 7, we scale\nup data pool on the full-size candidate video datasets introduced in Section 4, with low video NSFW\nscores and high frame-text similarities, encompassing approximately 147k instances. Additionally,\nfollowing Observation 8, we set epochs to 6, running the data through the model for six passes\nof training. From the data development view, this transition allows us to probe the scalability of\nour methodologies, advancing from the 40k data pool used in Section 5 to a significantly more\nvoluminous dataset, approximately 22\u00d7 larger. From the model development view, we undertake a\nstringent challenge to assess the transferability of our findings across different model architectures, by\nreplacing the training model from previous EasyAnimate into another SOTA model, T2V-Turbo (Li\net al., 2024)."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduced the Data-Juicer Sandbox, a pioneering open-source suite designed to\nfacilitate the collaborative development of multi-modal data and generative models. By integrating a\nflexible and comprehensive array of customizable components at different levels, our sandbox enables\nsystematic, cost-effective exploration and optimization, effectively bridging the often-disjointed\ndomains of data and model development. Through the implementation of a \u201cProbe-Analyze-Refine\u201d\nworkflow, we showcased how our sandbox can yield not only significant improvements in both dataset\nand trained models, but also valuable insights into the complex interplay between data preprocessing\nand model performance.\nLooking ahead, we will continuously extend the sandbox's compatibility to encompass a broader\nrange of model-centric infrastructures and develop more off-the-shelf and effect-proven workflows, to\nfacilitate reuse and expedite innovation. Besides, recognizing the growing capabilities of generative\nmodels in synthesizing data, it is also a crucial direction to investigate the ethical implications of data\nprocessing choices and their downstream effects on model outputs."}, {"title": "A DETAILED EXPERIMENTAL SETUPS", "content": "A.1 TRAINING CONFIGURATIONS\nFor the image-to-text generation, we conducted experiments on one of the state-of-the-art MLLMs,\nMini-Gemini-2B (Li et al., 2023). We train the whole model from scratch with less training data\n(about 1/6 of the original training datasets) in baseline experiments to make sure each experiment\ncan be finished within one day. We keep every training setting (e.g. learning rate scheduler, global\nbatch size) the same as the original model except for training datasets and training devices. For\nsingle-OP and OP combination experiments are trained on only 1 A100 GPU for each experiment so\nwe increase the number of gradient accumulation steps from 4 to 32 to keep the same global batch\nsize. For experiments of duplicating high-quality datasets, 8 A100 GPUs are involved to train the\nmodel, and the number of gradient accumulation steps is restored to 4. Each experiment is repeated 3\ntimes with different random seeds to make the final results more reliable.\nFor text-to-video generation, we adopt the advanced DiT-based EasyAnimate (Xu et al., 2024a) model,\nwhich integrates diverse datasets totaling 1,217k instances from InternVid (Wang et al., 2023b) (606k),\nPanda-70M (Chen et al., 2024c) (605k), and MSR-VTT (Xu et al., 2016) (6k). Baseline experiments\nare executed on a subset of 40k instances, employing LoRA (Hu et al., 2021) for efficiency. During\ntraining, we maintain a video resolution of 256x256, sample every other frame, and randomly select\nsequences of 16 consecutive frames. The training process involves performing a backward pass for\nthe loss of every 8 samples, with single-OP and OP combination experiments trained on a single\nGPU with a batch size of 8 for 5k steps, amounting to approximately 16 GPU hours per training run.\nExperiments for duplicating high-quality data, as well as larger-scale training, are conducted with a\nbatch size of 1 across 8 GPUs. The models employ the Adam optimizer for training, with a learning\nrate set to 2 \u00d7 10\u20135, weight decay parameter at 3 \u00d7 10\u22122, and epsilon configured to 10-10. Each\nexperiment is repeated twice with random seeds of 42 and 45, respectively."}, {"title": "A.2 PERFORMANCE METRICS", "content": "In the paper, we mainly report overall performance as the relative changes over the baseline in terms\nof the average across all model metrics with normalization as follows:\n$\\frac{\u03a3s_i/N - \u03a3\\bar s_i/N}{\u03a3\\bar s_i/N}$=\\frac{\u03a3(s_i-\\bar s_i)}{\u03a3\\bar s_i}$ $(3)$\nwhere N is the number of involved metrics, $s_i$ is the score of i-th model measurement metric, $\\bar s_i$\nis the corresponding score gained by the baseline model. Below are the specific evaluation metrics\ninvolved in this study.\nTextVQA, MMBench, MME. These benchmarks serve as critical evaluators of MLLM's proficiency\nin understanding images. TextVQA (Singh et al., 2019) specifically targets the assessment of\nMLLMs' abilities to read and reason about textual content embedded within images. MMBench (Liu\net al., 2023), a vast multimodal benchmark, encompasses perception and reasoning skills through a\nplethora of multi-choice questions, numbering in the thousands. Additionally, a Chinese translation,\nMMBench-CN, is integrated for broader accessibility. MME (Fu et al., 2023) focuses on the\nperceptual and cognitive competencies of MLLMs, incorporating 14 finely categorized subtasks, each\naddressing Yes/No inquiries underpinned by meticulously crafted guidelines.\nVBench. We engage with VBench (Huang et al., 2024), a holistic benchmark suite tailored for the\nrigorous evaluation of video generative models. It facilitates granular and objective assessment across\na spectrum of dimensions, deconstructing the concept of \u201cvideo generation quality\u201d into 16 discrete\nmetrics. Each metric is assessed using a carefully curated suite of prompts, comprising 946 unique\nprompts, with the requirement to produce 5 videos per prompt.\nOwing to the disparity in evaluation criteria and the inherent variability across different modalities,\nwe discern that the magnitude of performance fluctuation in the image-to-text generation substantially\nexceeds that observed in the text-to-video generation. This discrepancy underscores again the need\nfor nuanced data-model co-development in addressing the complexities inherent in each modality."}, {"title": "A.3 OPERATOR DESCRIPTIONS", "content": "The study involves a total of 31 operators (OPs) from Data-Juicer (Chen et al., 2024a), with their\ncorresponding statistics and detailed descriptions provided in Table 3."}, {"title": "B ADDITIONAL EXPERIMENTAL RESULTS", "content": "B.1 COMPLETE OPERATOR RANKING\nIn Table 4, we present complete numeric results conducted on individual OP experiments (Section\n5.1), from which we can discern some more detailed observations.\nIn image-to-text generation, it is preferable for the input of training images to align as closely as\npossible with the original configuration of the vision tower, such as training dimensions (height,\nwidth, sizes). Additionally, CLIP similarity scores tends to be more reliable than BLIP similarity\nscores. The BLIP similarity does not show much distinction and paradoxically, a lower similarity\noften results in better performance, which defies common sense. Images with excessively high\naesthetic quality may offer limited assistance in feature alignment, while watermarks might have\ncertain impacts on the OCR performance of the model.\nIn text-to-video generation, having a consistent aspect ratio for the training data is better than having\nratios that are inconsistent but close to the 1:1 ratio used during training. For instance, a data pool\nwith a 'middle' video aspect ratio consistently at 16:9 performs optimally. Videos with high video\naesthetics scores and low video NSFW scores, as well as those with low video OCR-area ratios and\nhigh video motion scores, tend to be of higher quality. While single-text-related operators might not\nbe as critical in text-to-video generation, they can still effectively filter out some dirty data.\nB.2 CORRELATION ANALYSIS\nIn order to investigate the intrinsic relationships between OPs and to aid our recipe formulation, we\nobserve relevance from following two perspectives.\nFirstly, we adopt the most direct approach by examining the Pearson correlation coefficient between\nthe statistics of OPs, as shown in Figures 6 for the image-to-text generation and Figures 7 for\ntext-to-video generation. Intuitively, the associations between the statistics of OPs utilized in the\nimage-to-text generation appear to be much stronger compared to those in the text-to-video generation.\nFor example, in image-to-text generation, phrase grounding recall shows a strong positive correlation\nwith text perplexity and special character ratio, while exhibiting a strong negative correlation with\nalphanumeric ratio, language score, text action number, stopword ratio, and text length. Meanwhile,\nin text-to-video generation, we only observe relationships between some of the purely textual OPs,"}, {"title": "B.3 SETTING AND ABLATION STUDY BASED ON T2V-TURBO", "content": "The results shown in Table 2 represent the enhancements we achieved based on T2V-Turbo (Li\net al., 2024). T2V-Turbo applies LoRA (Hu et al., 2021) to VideoCrafter-2.0 (Chen et al., 2024b)\nand is trained on the WebVid (Bain et al., 2021) dataset, using VideoCrafter-2.0 as a teacher for\ndistillation and incorporating reinforcement learning with rewards for the generated videos. We make\nthe following cumulative modifications to T2V-Turbo:"}]}