{"title": "DATA-JUICER SANDBOX: A COMPREHENSIVE SUITE\nFOR MULTIMODAL DATA-MODEL CO-DEVELOPMENT", "authors": ["Daoyuan Chen", "Haibin Wang", "Yilun Huang", "Ce Ge", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "abstract": "The emergence of large-scale multi-modal generative models has drastically ad-\nvanced artificial intelligence, introducing unprecedented levels of performance\nand functionality. However, optimizing these models remains challenging due to\nhistorically isolated paths of model-centric and data-centric developments, lead-\ning to suboptimal outcomes and inefficient resource utilization. In response, we\npresent a novel sandbox suite tailored for integrated data-model co-development.\nThis sandbox provides a comprehensive experimental platform, enabling rapid\niteration and insight-driven refinement of both data and models. Our proposed\n\"Probe-Analyze-Refine\u201d workflow, validated through applications on state-of-the-\nart LLaVA-like and DiT-based models, yields significant performance boosts, such\nas topping the VBench leaderboard. We also uncover fruitful insights gleaned\nfrom exhaustive benchmarks, shedding light on the critical interplay between data\nquality, diversity, and model behavior. With the hope of fostering deeper under-\nstanding and future progress in multi-modal data and generative modeling, our\ncodes, datasets, and models are maintained and accessible at https://github.\ncom/modelscope/data-juicer/blob/main/docs/Sandbox.md.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of multi-modal generative models has revolutionized artificial intelligence, pushing\nthe boundaries of functionality and creativity across various domains (OpenAI, 2024a;b; Wang\net al., 2024). Recognizing the pivotal role of training data in shaping model performance, there are\nfast-growing efforts to curate datasets of larger scales and higher quality (Jakubik et al., 2024).\nHowever, the development trajectories of these models and datasets have historically diverged, guided\nmore by intuition than by systematic co-development methodologies. Recent advances in enhancing\nmulti-modal generative models tend to be either model-centric or data-centric, rarely bridging the\ntwo aspects cohesively. For example, model-centric methods focus on algorithmic enhancements and\narchitectural innovations under fixed data priors, while data-centric strategies usually concentrate\non processing and cleaning datasets independently of specific model training contexts (Qin et al.,\n2024). Both approaches usually suffer from a lack of systematic guidance and cooperative synergy,\nrelying heavily on heuristic exploration and single-perspective expertise. This fragmented landscape\npresents a significant barrier to achieving optimal model performance, as the interplay between data\ncharacteristics and model capabilities remains largely underexploited.\nMoreover, the practical implementation of multi-modal generative models is further complicated by\ninfrastructure constraints, escalating computational costs, and the accelerating pace of development\ncycles (Xu et al., 2024b). In the age of large-scale models with rapidly growing model parameters\nand dataset sizes, the processes of data processing and model training become increasingly resource-\nintensive, demanding substantial time and computations. Due to the absence of cost-effective\nplatforms that simplify and speed up data-model co-development, researchers and developers often\nface the dilemma of prioritizing result-driven development at the expense of thorough, insight-led\nexploration. This deficiency hinders the iterative refinement for both domains, leading to sub-optimal\noutcomes as improvements in one domain are hard to inform, apply and enhance each other directly.\nTo fill this gap, we introduce the Data-Juicer Sandbox, a comprehensive suite for facilitating the\nco-development of multi-modal data and generative models. Building upon an open-source data\nprocessing system tailored for multi-modal generative models, Data-Juicer (Chen et al., 2024a), our\nsandbox suite further integrates a wealth of off-the-shelf components optimized for usability and\ncompatibility with existing model-centric infrastructures. Collectively, it offers flexibly customizable\norchestration from different levels including end-to-end workflows, specific development behaviors,\nand underlying data-model development capabilities. Within the sandbox laboratory, users are\nempowered to rapidly explore different data pipelines and model configurations on cost-controlled\ndatasets and models. This accelerates the discovery of insightful patterns and informed decision-\nmaking, ultimately paving the way for scalable, resource-efficient data-model development.\nTo exemplify the efficacy of our sandbox, we propose a \u201cProbe-Analyze-Refine\u201d workflow, meticu-\nlously crafted to explore the synergies between data processing operators, target model metrics, and\nthe scalability of these enhancements. We apply this workflow to two cutting-edge models: Mini-\nGemini (Li et al., 2023), an LLaVA-inspired model for image-to-text generation, and Easy Animate\n(Xu et al., 2024a), a Diffusion Transformer based model for text-to-video generation. Thanks to the\nsandbox's capabilities, we attain significant advancements in data quality and model performance,\nsuch as achieving top spot on the VBench (Huang et al., 2024) leaderboard, outperforming strong\ncompetitors such as VideoCrafter (Chen et al., 2024b) and AnimateDiff (Guo et al., 2024). These\nachievements are underpinned by a series of insights linking 31 data processing operators and 35\nmodel metrics, including analysis of the fine-grained impact of data processing for model training,\nthe delicate balance between data diversity and model performance, and the strategic optimization of\ndata scaling for enhanced model-data co-development.\nOur contributions can be summarized as follows:\n\u2022 Innovative Sandbox Suite: To the best of our knowledge, this is the first open-source sandbox\nsuite tailored for co-development between multi-modal data and generative models, rendering\nexperimental exploration in this field more insightful, systematic, convenient and reusable.\n\u2022 Effect-Proven Workflow: We present a new progressive workflow for data-model co-development\nand substantiate its impact through extensive empirical evidence such as achieving new top-tier\nperformance in image understanding and video generation tasks.\n\u2022 Practical Guidance: We conduct extensive experiments on benchmarking the effects of dozens\nof data processing operators and model metrics, providing fruitful and valuable insights toward\nfurther advancements in multi-modal generative models."}, {"title": "2 RELATED WORKS", "content": "Model-Centric Progress in Multimodal Generative Models. Multimodal generative models\nhave captivated researchers with their formidable capabilities (OpenAI, 2024a;b), leading to a surge\nin model-centric development efforts. These focuses mainly lie in refining training algorithms\n(Caffagni et al., 2024; Li et al., 2024; Zhang et al., 2024), advancing model architectures and\ncomponents (He et al., 2024a; Yin et al., 2024; Jiao et al., 2024), and harnessing the models' generative\npotential for various applications (Wang et al., 2024; Liu et al., 2024a; Zhou et al., 2024). There is a\ngrowing consensus that transformer-based scaling is predominant (Xu et al., 2023). However, the\nhigh computational requirements imposed by scaling laws (Xu et al., 2024b) and the optimization\nchallenges inherent to generative models (Manduchi et al., 2024) often confine insights to specific\ndatasets or vague data characteristics. This situation leaves a significant gap in comprehending the\nextent to which models' performance and behavior hinge upon implicit assumptions and inductive\nbiases embedded within the underlying data distributions.\nTrends in Data-Centric Development for Multimodal Generative Models. An emerging trend\nshifts the focal point from models to data (Jakubik et al., 2024; Bai et al., 2024), underscored by\nthe notion that large models function akin to data compressors (Del\u00e9tang et al., 2024). Echoing the\nprinciple of \"garbage in, garbage out\u201d, meticulous data processing is recognized as pivotal. Efforts\nnow isolate data manipulation as a primary experimental variable in multimodal generative modeling"}, {"title": "3 THE PROPOSED DATA-JUICER SANDBOX LABORATORY", "content": ""}, {"title": "3.1 MOTIVATION AND DESIGN", "content": "Why do we need data-model co-development? In the era of large models, the development of\nboth data and models necessitates a collaborative approach involving numerous algorithm researchers"}, {"title": "3.2 FLEXIBLE CAPABILITY FACTORY AND BEHAVIOR HOOKS", "content": "Components for Advanced Data Processing. Within the sandbox, we design numerous factory\nclasses and behavior hooks for data processing and evaluation, simplifying and unifying the interfaces\nprovided by the open-source Data-Juicer system (?). Users gain great flexibility to leverage over 100\nfeature-rich operators and dedicated tools for efficient data analysis, processing (including filtering,\nmodifying, generating, etc.), and evaluation:\n\u2022 From an actionable perspective, key components include DataProcessors and DataRecipeRefiners.\nDataProcessors invoke off-the-shelf data processing operators from Data-Juicer, such as Filters\nand Mappers, enabling accelerated and scalable data processing through system optimization and\nparallel support. DataRecipeRefiners offer tools to refine recipe configurations, such as adjusting\nfiltering thresholds based on k-sigma principle or percentile distribution to differentiate data subsets.\n\u2022 From a measurable perspective, DataAnalyzers and DataEvaluators are provided. DataAnalyzers\nencapsulate various data analysis tools from Data-Juicer, performing efficient computations of\ntarget metrics like text perplexity or video aesthetic value, and providing common statistical values"}, {"title": "3.3 A PROBE-ANALYZE-REFINE WORKFLOW FOR DATA-MODEL CO-DEVELOPMENT", "content": "To demonstrate the usage of the sandbox laboratory, here we elucidate a built-in systematic workflow.\nAs illustrated in Figure 2, we commence with a series of cost-effective contrastive explorations\naimed at addressing several key questions, before judiciously applying them to enhance large-scale\ndata-model co-development:\n1. Initially, we seek to ascertain which data processing operators (OPs) contribute most effectively\nto enhancing specific target model metrics? This involves creating equal-size data pools, each\nprocessed uniquely by a singular OP and subsequently ranked by data metrics. Models are trained\non these data pools, enabling us to perform an in-depth analysis of OP effectiveness and its\ncorrelation with model performance across various quantitative and qualitative indicators.\n2. Guided by insights derived from the most impactful OPs that ranked highest by model metrics,\nwe proceed to study whether these OPs can be effectively combined into data recipes and scaled\nup. To facilitate this exploration, we establish a hierarchical data pyramid, wherein data pools are\ncategorized across different tiers based on their ranked model metric scores. This stratification\nalso elucidates the viability of OP combinations when scaled with increased data volumes.\n3. Finally, we delve into optimizing data utilization through a dual analysis focusing on duplication\nand diversity. We will assess whether the training process would benefit more from the repeated\nuse of high-quality data pools or from the inclusion of lower-quality data to expand the overall\ndata pool. Through a systematic examination of cost-controlled experiments, we can formulate\noptimized data recipes and datasets, which are then leveraged in more resource-intensive training\nsessions to cultivate models with superior performance."}, {"title": "3.3.1 SINGLE-OPERATOR DATA POOLS", "content": "Data-Juicer provides a rich assortment of OPs tailored for multimodal data encompassing text,\nimages, videos, and audio. Given the variability in data sources, models, and downstream tasks, it\nis imperative to initially ascertain the impact of each OP to guide the optimization of data-model\nco-development effectively.\nTo this end, starting with an initial dataset D, we define a single-operator data pool Pi as the dataset\nprocessed exclusively by the i-th OP (OPi) available in Data-Juicer as follows:\n$P_i = DJ[OP_i(p_i)](D),$\nwhere $DJ$ denotes the data-processing function implemented by Data-Juicer, and pi is the hyper-\nparameters governing the operation of OP\u017c. The OPs of Data-Juicer can compute specific statistics\nfor every data pools, and apply threshold criteria to selectively filter or modify the data based on\nthese statistics. Within this workflow, the initial dataset D into three equal-sized segments for filter\nOPS: Phigh, Pmiddle and Plow, representing data with high, moderate, and low statistical measures,\nrespectively. A randomly sampled Prandom serves as a control group. This stratification fosters\ndiscriminative insights across varying degrees of data processing intensity.\nSubsequently, models are trained independently on each data pool, undergoing comprehensive\nevaluation across multiple performance metrics. Throughout the training, we uphold consistent\nhyper-parameters and ensure Prandom is of substantial size to yield a reliable and robust average\nperformance across all downstream tasks. This setup enables the identification of best-performing\nOPs that universally excel or excel under specific evaluation criteria.\nImportantly, we adhere to a cost-conscious design, implementing strategies to curtail expenses, such\nas downsizing the data pools, limiting training iterations, or adopting efficiency-enhancing techniques\nlike LORA (Hu et al., 2021). It enables the entire experimental pipeline to conclude at an affordable\nlevel. Specifically, all of our experiments can be completed within a single day utilizing a single\nA100 GPU, remaining both feasible and scalable for broader adoption in data-model co-development\nendeavors."}, {"title": "3.3.2 MULTI-OPERATORS DATA POOLS", "content": "Having explored the individual impact of each OP, our next step logically progresses to understanding\nthe dynamics when multiple OPs are sequentially applied as in a data \"recipe\". Our aim is to discern\nwhether these OPs complement or counteract each other's effects. To achieve this, we extend the data\npool construction methodology from previous individual OP scenarios to a sequence of OPs:\n$P = DJ [OP_1(p_1), OP_2(p_2), ..., OP_i(p_i)](D)$\n$=DJ[OP_1(p_1)](D) \u2229DJ[OP_2(p_2)](D) \u2229\u2229DJ[OP_i(p_i)](D).$\nThe number of possible multi-OP data pools grows exponentially with the addition of each new OP,\nnecessitating a strategic selection of combinations to explore. Drawing on insights from single-OP\nexperiments, we propose a pragmatic strategy: combining \u201cTop\u201d OPs, those with progressively dimin-\nishing impacts on model performance, while incrementally increasing the number of combinators.\nFurthermore, we integrate the analysis of inter-OP relationships into our recipe formulation. Our\nworkflow accommodates two approaches:"}, {"title": "3.3.3 PYRAMID-SHAPED DATA POOLS", "content": "Incorporating a greater number of OPs in a recipe may lead to enhanced data quality; however,\nas per Equ. (2), the resultant data pool volume decreases exponentially with each additional OP.\nThis phenomenon prompts a critical investigation: should we prioritize reusing high-quality data or\nincorporate lower-quality yet more abundant data to escalate training dataset sizes?\nTo encapsulate this inherent trade-off between data scale and quality, we devise a hierarchical pyramid\narchitecture for data pools. Given k \u201ctop\u201d OPs, we can create 2k 1 combinations of these OPs,\nas depicted in the left-bottom area in Figure 2. For example, the combination of 3 OPs, OP1,2,3,\nresides at the highest level of the hierarchy but results in the smallest data pool after Data-Juicer\ndata processing. The combinations of 2 operators, such as OP1,2, are placed at a lower level, and\nthe resulting data pool encompasses that of OP1,2,3 is several times larger in volume. Leveraging\nfindings from Section 3.3.2, we can pinpoint the optimal OP combinations within this pyramid\nstructure. Progressing downward through the pyramid, data pools exhibit a descending average OP\nranking (potentially indicative of reduced data quality) alongside an increase in volume.\nTo reconcile the desire for larger datasets without compromising quality, we devise two experimental\nstrategies built upon this data pyramid: (1) iterative model training with data repetition from the\ntop-layer (highest-quality) data pools, and (2) non-repetitive training incorporating progressively\nlower-quality, larger-volume data pools from the lower-layer pools. Specifically, we extract a\npredetermined quantity of data from the top-layer pool for iterative training with variable repetition\nrates. In parallel, as a baseline, we assemble a non-repeating dataset by consolidating pools from the\nupper to lower pyramid levels to match the size of the iterated dataset.\nThese comparative studies enable us to qualitatively assess the efficacy of data reuse versus the\ninclusion of suboptimal data within a fixed dataset size. Notably, all data pools are uniformly sampled\nfrom the original dataset D, ensuring that the relative proportions of pyramid data pools remain\nconsistent as D expands. Thus, insights garnered from these experiments can be extrapolated to\nlarger-scale data contexts, informing efficient scaling of data-model co-development practices."}, {"title": "4 PRACTICAL APPLICATIONS", "content": "In this section, we illustrate the application of the Data-Juicer sandbox in two distinct scenarios,\ndemonstrating its versatility and effectiveness in enhancing multimodal data-model co-development.\nThese examples instantiate the behavior hooks and capability classes detailed in Section 3.2, following\nthe workflow outlined in Section 3.3. Subsequently, Section 5 delves into the primary findings and\ninsights derived from these two practical scenarios."}, {"title": "4.1 IMAGE-TO-TEXT GENERATION", "content": "Our first task focuses on foundational image understanding ability, by experimenting on Mini-Gemini\n(MGM-2B), a state-of-the-art (SOTA) 2 billion parameter multimodal LLM (Li et al., 2023). The\ntraining protocol for MGM-2B involves two stages: pretraining and fine-tuning. Our experimental\nfocus lies in the pretraining phase, which seeks to harmonize visual and textual representations.\nWe utilize the original pretraining dataset, consisting of approximately 1,226k instances, as our\noriginal dataset D. The single-OP and multi-OP data pools are capped at a maximum of 200k\ninstances, ensuring consistency in training samples. The fine-tuning dataset is sampled to match the\ndown-sampling rate used during pretraining, resulting in a 240k instance subset.\nAfter the two-stage training, model evaluation is conducted on established benchmarks such as\nTextVQA (Singh et al., 2019), MMBench (Liu et al., 2023), and MME (Fu et al., 2023). Our\nexperimentation encompasses 22 text-image relevant OPs from Data-Juicer, split evenly between text-\nonly and image-related multimodal OPs. For multi-OP data pools, we identify the top-3 performing\nOPs for all possible combinations. Additionally, we analyze the correlations among the 23 data\nstatistics produced by 22 OPs capable of generating instance-level stats. Employing a hierarchical\nclustering algorithm, these OPs are grouped into three clusters based on correlation coefficients, with\nthe highest-performing OP from each cluster selected for combination testing.\nFrom these experiments, we identify the optimal OP combination, noting that due to filtering, the\nfinal instance count reduces to approximately 159k. These data are then repeated in increments from\ndouble up to eightfold, mirroring the size of the original pretraining set. Concurrently, we adopt the\nmethodology from Section 3.3.3 for comparative experiments. The outcomes yield lots of profound\ninsights into image-to-text model training, data processing, and iteration strategies from a data-model\nco-development perspective."}, {"title": "4.2 TEXT-TO-VIDEO GENERATION", "content": "For the second task, text-to-video generation, we adopt the advanced DiT-based EasyAnimate (Xu\net al., 2024a) model, which integrates diverse datasets totaling 1,217k instances from InternVid\n(Wang et al., 2023b) (606k), Panda-70M (Chen et al., 2024c) (605k), and MSR-VTT (Xu et al., 2016)\n(6k). Baseline experiments are executed on a subset of 40k instances, employing LoRA (Hu et al.,\n2021) for efficiency. Model outputs are assessed using VBench (Huang et al., 2024) across 16 metrics\non video quality and video-text matchness.\nOur investigation covered 21 OPs, including 13 text-only OPs and 10 video-related multimodal\nOPs. Analogous to the image-to-text generation, we conduct single-OP and multi-OP combination\nexperiments. However, given the reduced relevance of data statistics in video-related OPs, our\nanalysis centers on the correlations among the 16 VBench evaluation metrics. These metrics are\nclustered into three groups, with the best-performing OP selected from each group.\nThrough OP combination experiments, we pinpoint the most effective set of OPs. We then sample\n40k instances from the filtered data pool and repeat the training process for up to 10 epochs. For\ncomparative analysis, we adhere to the method outlined in Section 3.3.3, selecting larger data volumes\n(80k, 120k, up to 400k instances) for single-epoch training. This exploration also leads to valuable\ninsights for optimizing text-to-video model training, especially on informed decisions of data filtering\nOPs and ideal iteration counts.\nAcross all experiments, results are reported as averages from 2 to 5 repetitions, inclusive of standard\ndeviations. Detailed training configurations, performance metrics, and OP descriptions are provided\nin the Appendix A."}, {"title": "5 MAIN RESULTS AND INSIGHTS", "content": "In this section, we show the results of the specific applications of the probe-analyze-refine workflow\nthat we proposed for data-model co-development in image-to-text and text-to-video generation\n(Section 4). Initially, by conducting experiments with individual operators, we investigate the impact\nof different data selection strategies on the model, summarizing fundamental insights (Section 5.1).\nSubsequently, building on this foundation, we select suitable candidate operators for combination\nexperiments to identify the appropriate recipe (Section 5.2). After discovering this recipe, we\nconstruct a data pyramid hierarchy to explore whether to reuse high-quality data or sacrifice quality\nfor increased data diversity (Section 5.3). Finally, leveraging the insights gained, we scale up the data\nand model to train high-quality models (Section 5.4).\nDue to the space limitation, we present main conclusions and defer complete numeric results and\ndetails in Appendix B. Specifically, for the image-to-text generation, we mainly report the increase in\naverage performance relative to the baseline on TextVQA (Singh et al., 2019), MMBench (Liu et al.,\n2023), and MME (Fu et al., 2023). For the text-to-video generation, we report the improvement in"}, {"title": "5.1 RANKING SINGLE-OPERATOR DATA POOLS", "content": "Table 1 illustrates the performance of some models trained on single-operator data pools, where the\ndata processing OPs alongside others of particular interest. For a comprehensive analysis and detailed\nexperimental results for each OP, please refer to Appendix B.1."}, {"title": "Observation 1", "content": "Generative models' efficacy is intimately tied to the fidelity of modalities they are trained\nto generate, which can be explicitly reflected in filtering processes on training data.\nUpon general observation, in the text-to-video generation, video-related OPs occupy the top per-\nformance ranks, with the first three OPs being video-related and presenting a notable gap over the\nfourth-ranked text-related OP. On the other hand, in the image-to-text generation, text-related OPs\nappear to be more influential."}, {"title": "Observation 2", "content": "Image-to-text models place greater emphasis on data diversity, whereas text-to-video\nmodels prioritize data quality."}, {"title": "Observation 3", "content": "Dynamic information in the data poses a heightened learning challenge for image-to-text\ngeneration compared to text-to-video generation.\nMoreover, considering the static nature of images, the image-to-text generation occasionally demands\nthe parsing of dynamic content, compelling models to engage in a degree of creative inference\nfor accurate interpretation and response. This challenge manifests in the difficulty image-to-text\nmodels encounter when confronted with data rich in dynamic information. As illustrated in Table 1,\nimage-to-text models exhibit commendable performance with fewer text actions, while text-to-video\nmodels display opposing trends regarding text action counts and video motion scores within the\nvideo-text data."}, {"title": "Observation 4", "content": "A high degree of match between different modalities within the data is crucial for model\nperformance in both image-to-text and text-to-video generation.\nFinally, both image-to-text and text-to-video models exhibit a common affinity for a higher degree of\nalignment between modalities. This is substantiated by their exemplary performances in scenarios\nwhere measures of image-text similarity, phrase grounding recall, and frames-text similarity are high."}, {"title": "5.2 SHAPING DATA RECIPES OF TOP-3 OPERATOR COMBINATIONS", "content": "Based on the insights acquired from single OP experiments, we can combine OPs into recipes for\ndata filtering to obtain higher-quality data. Figure 3 illustrates the performance changes brought by\ndifferent recipe combinations compared to the baseline data pool."}, {"title": "Observation 5", "content": "The optimal data recipe does not necessarily result from combining the best individual\nOPs, nor does incorporating more high-performing OPs always lead to superior outcomes.\nGiven the exponential increase in possible OP combinations as the number of OPs grows, we focus\non experimenting with combinations of three OPs at a time. An intuitive approach is to combine the\ntop three OPs with the best overall performance from Table 1. However, as shown in Figure 3(a)\nand Figure 3(c), combining higher-performing OPs does not always yield better results, nor does\nadding more OPs guarantee improvement. For instance, in the image-to-text experiment depicted\nin Figure 3(a), the data pool with a high image NSFW score, although performing best in single-\noperator experiments, generally diminishes performance when combined with other pools. Similarly,\nin Figure 3(c), while pairwise combinations of OPs all show positive gains, integrating the best-\nperforming OP into the combination of high frames-text similarity and low video NSFW score reduces\nthe relative improvement over the baseline from 2.48% to 1.88%.\nIn addition to selecting the overall best-performing OPs, we aim to identify operators with distinct\nadvantages to explore whether combining these operators can synergistically leverage their strengths\nfor improved outcomes. We selected operators with unique strengths based on correlation infor-\nmation obtained from single-operator experiments. Detailed correlation analyses can be found in\nAppendix B.2."}, {"title": "Observation 6", "content": "Combining OPs that excel in orthogonal dimensions on model or data does not guaran-\ntee complementary effects; rather, it is more likely that they will impede each other's\nperformance.\nAs depicted in Figure 3(b), regardless of how these top-performing OPs are combined, they ultimately\nreduce the model's performance in both image-to-text and text-to-video generation. This observation\nchallenges the naive assumption widely used in existing SOTA works, that various intuitively useful\ndata cleansing actions, when stacked serially, can synergistically enhance performance."}, {"title": "Observation 7", "content": "The performance of a single OP is positively correlated with the performance of the recipe\ncreated from its combination. Starting with high-performing OPs is a good initial step in\nexploring optimal higher-order data recipes.\nAlthough the Top-3 operator recipes exhibit suboptimal performance, we observe positive gains\nwhen combining the top-2 operators, outperforming both single top-1 and top-3 combinations. For\nexample, combining TextActionFilter and LanguageIDScoreFilter for the image-to-\ntext generation, as well as VideoNSFWFilter and VideoFramesTextSimilarityFilter\nfor the text-to-video generation, has proven to be highly effective."}, {"title": "5.3 HARNESSING MORE HIGH-QUALITY DATA", "content": "As discussed in Section 3.3.3 and illustrated in Figure 3, we can construct pyramid-shaped data pools\nby combining different OPs. Figure 3 provides specific performance metrics for each pool within this\npyramid structure. The best-quality data often resides at the higher levels of the pool; however, these\nlevels also contain less data. Therefore, during model training, it is crucial to determine when to\nreuse high-quality data and when to introduce suboptimal data to ensure diversity. The experiments\nin this section are designed to address this question.\nIn these experiments, we select the best-performing data pool from Figure 3 as the optimal data pool.\nSpecifically, for the image-to-text model, we select the data pool with a low text action number and\na high language score, and for the text-to-video model, we choose the data pool with a low video\nNSFW score and a high frames-text similarity. We also adhere to the setup in Figure 3, where the\nbaseline for image-to-text trains on 159k data instances, and the baseline for text-to-video trains on\n40k data instances."}, {"title": "Observation 8", "content": "Duplicating high-quality data is beneficial for both image-to-text and text-to-video models.\nFor the image-to-text model, optimal utilization of high-quality data may be achieved after\nfour repetitions, while for the text-to-video model, it is effective to reuse high-quality data\nextensively between six to ten times."}, {"title": "5.4 APPLYING DATA-MODEL INSIGHTS IN LARGER-SCALE SCENARIOS", "content": "In this section, we apply the insights gained from the previous experiments to larger-scale models\nand datasets. Specifically, in line with the best data recipe from Figure 3 and Observation 7, we scale\nup data pool on the full-size candidate video datasets introduced in Section 4, with low video NSFW\nscores and high frame-text similarities, encompassing approximately 147k instances. Additionally,\nfollowing Observation 8, we set epochs to 6, running the data through the model for six passes\nof training. From the data development view, this transition allows us to probe the scalability of\nour methodologies, advancing from the 40k data pool used in Section 5 to a significantly more\nvoluminous dataset, approximately 22\u00d7 larger. From the model development view, we undertake a\nstringent challenge to assess the transferability of our findings across different model architectures, by\nreplacing the training model from previous EasyAnimate into another SOTA model, T2V-Turbo (Li\net al., 2024)."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduced the Data-Juicer Sandbox, a pioneering open-source suite designed to\nfacilitate the collaborative development of multi-modal data and generative models. By integrating a\nflexible and comprehensive array of customizable components at different levels, our sandbox enables\nsystematic, cost-effective exploration and optimization, effectively bridging the often-disjointed\ndomains of data and model development. Through the implementation of a \u201cProbe-Analyze-Refine\u201d\nworkflow, we showcased how our sandbox can yield not only significant improvements in both dataset\nand trained models, but also valuable insights into the complex interplay between data preprocessing\nand model performance.\nLooking ahead, we will continuously extend the sandbox's compatibility to encompass a broader\nrange of model-centric infrastructures and develop more off-the-shelf and effect-proven workflows, to\nfacilitate reuse and expedite innovation. Besides, recognizing the growing capabilities of generative\nmodels in synthesizing data, it is also a crucial direction to investigate the ethical implications of data\nprocessing choices and their downstream effects on model outputs."}, {"title": "APPENDIX", "content": null}, {"title": "A DETAILED EXPERIMENTAL SETUPS", "content": null}, {"title": "A.1 TRAINING CONFIGURATIONS", "content": "For the image-to-text generation, we conducted experiments on one of the state-of-the-art MLLMs,\nMini-Gemini-2B (Li et al., 2023). We train the whole model from scratch with less training data\n(about 1/6 of the original training datasets) in baseline experiments to make sure each experiment\ncan be finished within one day. We keep every training setting (e.g. learning rate scheduler, global\nbatch size) the same as the original model except for training datasets and training devices. For\nsingle-OP and OP combination experiments are trained on only 1 A100 GPU for each experiment so\nwe increase the number of gradient accumulation steps from 4 to 32 to keep the same global batch\nsize. For experiments of duplicating high-quality datasets, 8 A100 GPUs are involved to train the\nmodel, and the number of gradient accumulation steps is restored to 4. Each experiment is repeated 3\ntimes with different random seeds to make the final results more reliable.\nFor text-to-video generation, we adopt the advanced DiT-based EasyAnimate (Xu et al., 2024a) model,\nwhich integrates diverse datasets totaling 1,217k instances from InternVid (Wang et al., 2023b) (606k),\nPanda-70M (Chen et al., 2024c) (605k), and MSR-VTT (Xu et al., 2016) (6k). Baseline experiments\nare executed on a subset of 40k instances, employing LoRA (Hu et al., 2021) for efficiency. During\ntraining, we maintain a video resolution of 256x256, sample every other frame, and randomly select\nsequences of 16 consecutive frames. The training process involves performing a backward pass for\nthe loss of every 8 samples, with single-OP and OP combination experiments trained on a single\nGPU with a batch size of 8 for 5k steps, amounting to approximately 16 GPU hours per training run.\nExperiments for duplicating"}]}