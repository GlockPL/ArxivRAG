{"title": "Symbolic Learning Enables Self-Evolving Agents", "authors": ["Wangchunshu Zhou", "Yixin Ou", "Shengwei Ding", "Long Li", "Jialong Wu", "Tiannan Wang", "Jiamin Chen", "Shuai Wang", "Xiaohua Xu", "Ningyu Zhang", "Huajun Chen", "Yuchen Eleanor Jiang"], "abstract": "The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing \u201clanguage agents\", which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That's to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI.\nIn this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in \u201cself-evolving agents\". We demonstrate the potential of the agent symbolic learning framework and open-source the entire framework to facilitate future research on data-centric agent learning.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models [Radford et al., 2018, 2019, Brown et al., 2020, Ouyang\net al., 2022, OpenAI, 2023, Touvron et al., 2023a,b] open the possibility of building language agents\nthat can autonomously solve complex tasks. The common practice for developing AI agents is to\ndecompose complex tasks into LLM pipelines where prompts and tools are stacked together [Park\net al., 2023, Hong et al., 2023, Zhou et al., 2023b, Chen et al., 2023b, Xie et al., 2023]. In a sense,\nlanguage agents can be viewed as AI systems that connect connectionism AI (i.e., the LLM backbone"}, {"title": "2 Related Work", "content": "2.1 Language Models, Prompts, and Language Agents\nLanguage model is a family of machine learning model that is trained to evaluate the probability\nof sequences of words or tokens. Large language models (LLMs) [Radford et al., 2018, 2019,\nBrown et al., 2020, Ouyang et al., 2022, OpenAI, 2023, Touvron et al., 2023a,b] often refer to\nlanguage models that adopt the autoregressive probability factorization scheme, parametrized by\nthe Transformer architecture [Vaswani et al., 2017], consists of a large amount of parameters, and\ntrained on large-scale corpus. With scaling of model size, training data, and computation, LLMs have\ndemonstrated remarkable capabilities in generating human-like texts and understanding context.\nPrompts, on the other hand, is the key for unleashing the capabilites of LLMs. Prompts are critical\ncomponents in controlling the behavior and output of LLMs and serve as the interface between human\nand LLMs. The design of prompts significantly impacts the performance of language models and a\nnumber of progress have been made on prompt engineering, including in-context learning [Brown\net al., 2020], chain-of-thought prompting [Nye et al., 2022, Wei et al., 2022], ReAct [Yao et al., 2022],\nself-refine [Madaan et al., 2023], self-consistency [Wang et al., 2023], recurrent prompting [Zhou\net al., 2023a], etc.\nLanguage agents further extend the functionality of language models beyond simple prompting by\nallowing LLMs to use tools [Schick et al., 2023] and integrating LLMs into broader systems capable\nof executing multi-step tasks [Park et al., 2023, Hong et al., 2023, Zhou et al., 2023b, Chen et al.,\n2023b, Xie et al., 2023]. By stacking prompts and tools into carefully designed pipeline, agents are\nversatile in various applications, from customer service automation to advanced data analysis."}, {"title": "2.2 From Automated Prompt Engineering to Agent Optimization", "content": "With the increasing popularity of prompt engineering in both academic and industry, a number of\nrecent work investigated methods to automate the prompt engineering process. For example, Pryzant\net al. [2020] and Yang et al. [2024] uses carefully designed prompts to unleash LLMs' ability to do"}, {"title": "3 Agent Symbolic Learning", "content": "Algorithm 1 Agent Symbolic Learning Framework\nRequire: I \u25b7 Input to the agent system\nRequire: A \u25b7 Agent pipeline with nodes\nRequire: G \u25b7 Prompt-based gradient propagation function\nRequire: L \u25b7 Prompt-based loss function\nEnsure: Updated symbolic components in the agent system\n1: \u03c4 \u2190 [] \u25b7 Initialize trajectory\n2: Forward Pass\n3: for each N\u2208 A do\n4: In \u2190 Get input for N \u25b7 Input to the node\n5: On \u2190 N(In, Pn, Tn) \u25b7 Output from the node\n6: Append (In, On, Pn, Tn) to \u03c4\n7: end for\n8: Loss Computation\n9: Llang \u2190 L(T) \u25b7 Compute language loss\n10: Back-propagation\n11: for each N \u2208 reverse(A) do\n12: lang \u2190 Gate, In, On, Pn, Tn, Llang) \u25b7 lang \u2190 \u00d8 for the last node\n13: Append lang to T\n14: end for\n15: Weight Update\n16: for each N\u2208 A do\n17: Update Pn, Tn using lang \u25b7 Update prompts and tools\n18: end for\n19: Update A using {lang} \u25b7 Update the agent pipeline\n20: return (A, P, T) \u25b7 Updated agent system"}, {"title": "3.1 Problem Formulation", "content": "We first formulate the agent symbolic learning framework by drawing analogies to the components\nand procedures used in neural network training. We define the key components of the framework and\nexplain the notations used throughout this section.\nThe agent symbolic learning framework, as illustrated in Figure 2, is inspired by the connectionist\nlearning procedures used for training neural nets [Hinton, 1990]. We first introduce the notations for\nkey concepts by making analogies to that in the connectionist learning framework:\n\u2022 Agent Pipeline A: Similar to the computational graph in neural nets that represents the\nstructure of layers and their connections, agent pipeline represents the sequence of nodes (or\nsteps) through which the agent processes input data. A sequence of nodes {N1, N2, ..., n}\nthat process the input data through various stages. Note that in some agent frameworks, the\nagent pipeline is input-dependent since the nodes are dynamically assigned during execution,\nwhich is similar to the case of dynamic neural nets.\n\u2022 Node N: An individual step within an agent pipeline. The role of a node in an agent is\nsimilar to a layer in a neural network. A node Nn receives Node Input In, which are\nalso in natural language form. In general, the input for a node consists of the output of the\nprevious node and (optionally) inputs from the environment (e.g., human input). The node\nNn processes the input In with an LLM using both prompts Pn and tools Tn. The output\nOn is in natural language and passed to the next node.\n\u2022 Trajectory T: Similar to the role of computational graph of neural nets, the trajectory stores\nall information during the forward pass, including the inputs, outputs, prompts, and tools\nusage for each node, and is responsible for gradient back-propagation.\n\u2022 Language Loss Llang: Language loss in the agent symbolic learning framework is similar to\nthe loss in neural networks since they both measure the discrepancy between the expected\nand actual outcomes. The main difference is that the language loss is in textual form and is\nproduced by a natural language loss function implemented by a carefully designed prompt\nwhile conventional losses are float numbers computed with loss functions that are numerical\nequations.\n\u2022 Language Gradient lang: Similar to the role of gradients in connectionist learning,\nlanguage gradients are textual analyses and reflections used for updating each component in\nthe agent with respect to the language loss."}, {"title": "3.2 Agent Symbolic Learning Procedure", "content": "After defining the key components, we can summarize the workflow of the agent symbolic learning\nframework in Algorithm 1. In this section, we describe each step in the agent symbolic learning\nframework in detail.\nForward Pass The forward pass is almost identical to standard agent execution. The main dif-\nference is that we store the input, prompts, tool usage, and the output to the trajectory, which is\nused for language gradient back-propagation. This is similar to deep learning frameworks such as\nPyTorch [Paszke et al., 2019] and TensorFlow [Abadi et al., 2016] that store the intermediate outputs\nand activation in the computation graph of the neural network.\nLanguage Loss Computation After the forward pass, we compute the language loss for a training\nexample by feeding the trajectory into an LLM using a carefully designed prompt template Ploss:\nLlang = LLM(Ploss(t)) (1)\nThe key is the design for the prompt template, which is expected to holistically evaluate how the agent\nperforms with respect to the input, environment, and task requirements. To this end, we carefully\ndesign a prompt template for language loss computation consisting of the following components: task\ndescription, input, trajectory, few-shot demonstrations, principles, and output format control. Among\nthem, task description, input, and trajectory are data-dependent while the few-shot demonstrations,\nprinciples, and output format control are fixed for all tasks and training examples. The language loss\nconsists of both natural language comments and a numerical score (also generated via prompting).\nWe can optionally feed the ground-truth label for the input when generating the language loss. We\ncall this scenario supervised agent learning. It can also generate language loss without ground-truth\nby evaluating the output and trajectory according to the task description. In this case, we can say that\nthe agent is doing unsupervised agent learning, which enables language agents to self-evolving. We\npresent the detailed implementation of this prompt template in the Appendix.\nBack-propagation of Language Gradients In standard connectionist learning, the goal of gradient\nback-propagation is to calculate the impact of the weights with respect to the overall loss so that\nthe optimizers can update the weights accordingly. Similarly, in our framework, we also design a\n\"back-propagation\" algorithm for language gradients. Specifically, we iterate from the last node to the\nfirst node and compute the gradient for each node with LLMs using a carefully designed prompt:\nlang = LLM(Pgradient(an, In, On, Pn, Tn, Llang)) (2)\nThe prompt template Pgradient is designed to instruct the LLM to generate language gradients that\nare analyses and reflections for the symbolic components within the node. Inspired by the idea of\nback-propagation, we give the language gradients of the node executed after the current node, as well\nas the information on the execution of the current node, which is stored in the trajectory. That's to\nsay, when doing analysis and reflection, the LLM not only needs to consider how the prompts and\ntools suit the subgoal of the current node but also has to consider how they affect the accomplishment\nof the subgoal of the next node. By chaining from top to bottom, the language gradients for all nodes\nare relevant and responsible for the overall success of the agent. This method effectively reduces the\nrisk of optimizing toward the local optimum for each isolated prompt and tool, leading to the overall\nperformance of agent systems.\nLanguage Gradient-based Update The final step in the framework is to update the prompts\nand tools in each node and optimize the overall agent pipeline with the help of language gradi-\nents. This is accomplished via \u201csymbolic optimizers\u201d. Symbolic optimizers are carefully designed\nprompt pipelines that can optimize the symbolic weights of an agent. We create three types of\nsymbolic optimizers: PromptOptimizer, ToolOptimizer, and PipelineOptimizer. We present detailed\nimplementation of these prompts in the Appendix.\nPromptOptimizer: To facilitate prompt optimization, we split prompts into different components,\nincluding task description, few-shot examples, principles, and output format control. We then design\nseparate prompts tailored for the optimization of each prompt component. All prompts share a\ndetailed explanation and demonstration of how the LLM should focus on the language gradients\nwhen reasoning about how to edit the original prompt components."}, {"title": "4 Experiments", "content": "4.1 Settings\n4.1.1 Tasks\nWe conduct experiments on both standard LLM benchmarks and more complex agentic tasks. We\ndescribe the tasks, datasets, and evaluation metrics as follows:"}, {"title": "4.3 Case Study & Analysis", "content": "We then show a case study for the optimization dynamics of the agent symbolic learning framework\nin Figure 3. We can see that our approach can effectively do prompt engineering and designing the\nagent pipeline in the way a human expert develops language agents.\nMoreover, we find that the initialization of the agent system has non-negligible impacts on the final\nperformance, just as the initialization of a neural nets is important for training. In general, we find\nthat it is generally helpful to initialize the agent in the simplest way and let the symbolic optimizers to\ndo the optimization. In contrast, the performance tends to become unstable if the initial agent system\nis over-engineered. A natural extension of this observation is that maybe we can do some kind of\npre-training on large-scale and diverse tasks as a versatile initialization for general-purpose agents\nand then adapt it to specialized tasks with agent symbolic learning. We also find that the success of\nour approach is more significant and stable on complex real-world tasks compared to that on standard\nbenchmarks where the performance is evaluated by accuracy or F1. This suggests that future research\non agent learning should focus more on real-world tasks, and the agent research community should\nwork on building a benchmark focusing on agent learning evaluation that consists of diverse complex\nagentic tasks and investigating robust approaches to measure progress."}, {"title": "5 Conclusion", "content": "This paper introduces agent symbolic learning, a framework for agent learning that jointly optimizes\nall symbolic components within an agent system. The agent symbolic learning framework draws\ninspiration from standard connectionist learning procedure to do symbolic learning. It uses language-\nbased loss, gradients, and optimizers to optimize prompts, tools, and the agent pipeline with respect\nto the overall performance of the agent system. The proposed framework is among the first attempts\nto optimize agents that can solve complex real-world tasks using sophisticated pipelines. Our\nframeworks enables language agents to \"learn from data\" and perform \u201cself-evolve\" after being\ncreated and deployed in the wild. We conduct several proof-of-concept experiments and show that the\nagent symbolic learning framework can effectively optimize agents across different task complexity.\nWe believe this transition from model-centric to data-centric agent research is a meaningful step\ntowards approaching artificial general intelligence and open-source the codes and prompts for the\nagent symbolic learning framework to accelerate this transition."}, {"title": "A Implementation Details", "content": "We adopt the agent programming language and framework introduced in Agents [Zhou et al., 2023b],\na language agent framework that enables developers to build language agents that stacks prompts\nand tools together into complex pipelines. The main advantage of the Agents framework is that\nit enables developers to use a config file to define the agent system, which makes it easier for the\nsymbolic optimizers in the agent symbolic learning framework to perform update operations on the\nagent system."}, {"title": "B Prompt Templates", "content": "Prompt Template for Language Loss Function\nLoss with ground truth:\nYou are a fine-tuner of a large model. I will provide you with some output results from the model and the\nexpected correct results. You need to evaluate these data and provide a score out of 10, please wrap the score\nusing <score></score>. Additionally, please provide some suggestions for modifying the model's output,\nusing <suggestion></suggestion> to wrap your suggestions.\nHere is the model's output:\n<result>result</result>;\nThe expected result is:\n<ground_truth>ground_truth</ground_truth>\nPlease note:\n1. Ensure that the output is wrapped with <score></score> and <suggestion></suggestion> respectively.\n2. The output should be as consistent as possible with the expected result while being correct. For example,\nif the expected result is \"BUST\", and the model's output is \"The women's lifestyle magazine is 'BUST'\nmagazine.\", even though the answer is correct, you should advise the model to be more concise.\n3. The standard for a score of 10 is that the model's output is exactly the same as the expected result in a\ncase-insensitive manner, and without any unnecessary content. Even if the model's output is semantically\ncorrect, if it includes superfluous content, points should be deducted.\nLoss with ground truth and score:\nYou are a large language model fine-tuner. I will provide you with a model's output and the expected\ncorrect result. You need to evaluate it and suggest modifications to the model's output. Please use '<sugges-\ntion></suggestion>' to enclose your feedback.\nBelow is the model's output:\n<result>result</result>\nThe expected result is:\n<ground_truth>ground_truth</ground_truth>\nHere is the evaluation score for the model. Your goal is to optimize this score:\n<score>score</score>\nThe relevant information about this score is as follows:\n<evaluation_info>score_info</evaluation_info>\nNote:\n1. Ensure that '<suggestion></suggestion>' exists and appears once.\n2. If the model's output is satisfactory, you can output <suggestion>The output is satisfactory, no additional\nrequirements</suggestion>.\n3. The output should be as close to the expected result as possible while ensuring correctness. For example,\nif the expected result is \"BUST\" and the model's output is \"The women's lifestyle magazine is 'BUST'\nmagazine.\", even though this answer is correct, you should remind the model to be concise."}, {"title": "Prompt Template for Gradient Back-propagation", "content": "Prompt-Level\nYou are now a prompt fine-tuner for a large language model. You are tasked with providing suggestions for\noptimizing the prompt template.\nPlease enclose your suggestions using <suggestion></suggestion>, for example, <suggestion>it could be\nmade shorter</suggestion>.\nThe task is divided into multiple steps; I will provide you with the output from the previous step, the\nrequirement proposed by the next step for the current output, the current output itself, and the prompt\ntemplate. You need to suggest improvements for the current step's prompt template.\nThe prompt template that needs optimization is: <prompt_template>prompt_template</prompt_template>\nThe output from the previous step is: <previous_output>previous_output</previous_output>\nThe current output is: <output>response</output>\nThe requirement proposed by the next step for the current output is: <require-\nment>suggestion</requirement>\nIn addition to suggesting modifications for the current prompt template, you also need to propose requirements\nfor the output of the previous step. Please wrap these using <suggestion></suggestion>, for example:\n<suggestion>the analysis should include a comparison of original data</suggestion>.\nNote:\n1. Ensure that the results are wrapped with <suggestion></suggestion> and <suggestion></suggestion>, and\neach tag appears only once.\n2. If you are the first node, you can state within <suggestion></suggestion> \u201cThis is the first node.\"\n3. Please note that during your analysis, remember that this prompt template will be applied to multiple\ndifferent datasets, so your suggestions should be general and not solely focused on the examples provided\nhere.\n4. Please analyze step by step.\nNode-Level\nYou are a large model fine-tuner. Now you need to try to optimize the information of a node. For a complex\ntask, it has been divided into multiple nodes, each of which contains multiple roles that work together\nto complete the task of this node. Each role is backed by an LLM Agent, and you need to optimize the\nconfiguration information of one of the nodes.\nHere are the relevant explanations for the Node configuration:\n- The fields in the \"controller\" indicate the scheduling method of the model. If there is only one role, this\nitem does not need to be optimized:\n\"route_type\" indicates the scheduling method, which has three values: \"random\" means random scheduling,\n\"order\" means sequential scheduling, and \"llm\" means scheduling determined by the LLM model.\n\"route_system_prompt\" and \"route_last_prompt\" are used when \"route_type\" is \"llm\" and are respectively\nthe system prompt and last prompt given to the LLM model responsible for scheduling.\n\"begin_role\" is a string indicating the name of the starting role of this node.\n\"roles\" is a dictionary where the key is the role name, and the value is the prompt used by this role.\nYou need to decide how to optimize the configuration of this node. Specifically, you need to try to provide\nsuggestions in the following aspects:\n1. Update the node description field. This field describes the function of the node and is also an important\nindicator to measure the performance of a node.\n2. Update the scheduling method of the role. Note that if there is only one role, no optimization is needed.\n3. Add a new role, and you need to clearly describe the function of this role.\n4. Delete a role, and you need to clearly describe the reason for deleting this role.\n5. Update a role, and you need to indicate how to update the description of this role.\nNext, I will give you a Node configuration, and you need to provide optimization suggestions based on the\ncurrent Node configuration. Please use <suggestion>[put your suggestion here]</suggestion> to enclose\nyour suggestions.\n## Current Node Config\n{node_config}\nYou need to first provide your analysis process, then give your optimized result. Please use <anal-\nyse></analyse> to enclose the analysis process. Please use <suggestion></suggestion> to enclose the\noptimization suggestions for the current node. Please use <suggestion></suggestion> to enclose the require-\nments for the previous node.\nNote: The suggestions provided need to be in one or more of the five aspects mentioned above."}]}