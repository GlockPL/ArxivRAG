{"title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems", "authors": ["Wenxiao Zhang", "Xiangrui Kong", "Conan Dewitt", "Thomas Braunl", "Jin B. Hong"], "abstract": "The integration of Large Language Models (LLMs) like GPT-40 into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent enhancements of Large Language Models (LLMs), such as the incorporation of vision features into LLMs like GPT-40, have enabled these generalist models to process and respond to multi-modal inputs\u2014including text and images-with greater contextual awareness and improved decision-making capabilities [1]. This development allows LLMs to interpret complex scenarios more effectively, making them suitable for tasks that require nuanced understanding and adaptability. Consequently, these advancements are paving the way for more sophisticated and capable robotic systems, demonstrating a promising trend in the integration of LLMs with robotics [2].\nHowever, this technological progression is accompanied by several critical challenges, particularly in the realm of security and practical application. LLMs possess advanced capabilities for reasoning and processing complex inputs but are highly susceptible to various input variations [3]. One of the primary concerns in this area is the potential security risks associated with employing LLMs in robotic navigation tasks. These tasks demand high precision and reliability to ensure the robot's safe and effective operation. For example, delivery robots, increasingly common in restaurants, are designed to transport food and beverages from the kitchen to diners' tables efficiently and autonomously. Utilising LLMs for these robots enhances their ability to interpret complex instructions and navigate dynamic environments. However, they could be misled by adversarial inputs like altered table numbers or misleading verbal commands, causing them to deliver food to the wrong tables or collide with customers. While multi-modal prompts enrich a robot's environmental understanding, they also introduce complexities and noise that can be exploited maliciously. For LLM-integrated mobile robotic systems, adversarial inputs designed to deceive the model can result in incorrect or hazardous navigational decisions, posing substantial risks to both the robot and its surroundings [4].\nDespite the advancements in LLMs, there has been insufficient"}, {"title": "II. RELATED WORKS", "content": "exploration of their integration with robotic systems, particularly concerning the security implications. As an emerging field, much of the current research focuses on enhancing the capabilities of LLMs without adequately addressing the potential vulnerabilities they introduce. Accordingly, in this study, we provide a practical approach to setting up an LLM-controlled mobile robot system in a simulation environment utilising structured prompts and explore the influence of prompt injection attacks on the security and reliability of the system. We investigate the resilience of GPT-40 against these attacks and how secure prompting can help them detect various adversarial inputs and mitigate their impact on the system. Our experiments measured various attack rates and the LLMs' ability to detect these attacks, revealing that LLMS with properly engineered prompts exhibited a higher detection rate of adversarial inputs and responded more effectively to mitigate their impact.\n\nA. LLM-based Navigation Tasks\nAccording to Xi et al. [5], an LLM-based agent comprises three modules: perception, brain, and action, with LLMs serving as the brain module that processes perception results and makes decisions on the next action. In robotic navigation tasks, several studies [6]\u2013[8] have shown that LLMs can effectively process and understand the surrounding environment through sensory data and human instructions, subsequently producing path planning based on the perception results.\nHowever, security and reliability concerns in such systems have also emerged as major issues. Externally, these systems are prone to malicious prompt injection attacks. Wen et al. [4] investigated the security vulnerabilities of LLM-based navigation systems and proposed a defence strategy called Navigational Prompt Engineering (NPE). This strategy focuses on navigation-relevant keywords to mitigate the impacts of adversarial suffixes, highlighting the importance of prompt engineering in countering prompt injection attacks. Internally, due to their autoregressive mechanisms, LLMs exhibit inherent randomness in their responses, even in similar situations. Consequently, this randomness can potentially result in the execution of erroneous movements [9]. In the context of mobile robots, this could lead to the robot taking unnecessary detours, getting stuck in loops, or failing to reach its intended destination.\n\nB. Prompt Engineering Techniques\nLLM prompting can be divided into zero-shot prompting [10], which relies on the model's extensive pre-trained knowledge to generate responses without any examples, and few-shot prompting [9], which involves providing the model with a few examples within the prompt to enhance its ability to produce accurate and relevant outputs. Few-shot prompting often leverages Retrieval Augmented Generation (RAG) [11], a technique that retrieves and appends the most relevant"}, {"title": "C. Prompt Injection and Counteracts", "content": "content from a database to the prompt, aiding in better context understanding and response generation.\nAdditionally, Wei et al. [12] introduced Chain-of-Thought (CoT), a method of breaking down the reasoning process into a sequence of logical steps to improve the quality and transparency of the generated responses. This process typically involves multi-round question-answering of user instructions with an LLM. Building on this, multi-agent collaboration technique [13] has emerged as an advanced approach that combines the strengths of multiple LLM agents working collaboratively with the structured reasoning process of CoT. Each agent can focus on different aspects of a problem, allowing for parallel processing, and they can communicate with each other for information sharing to improve the performance of the system.\n\na) Prompt Injection Attack Classification\nCyberattacks often aim to compromise one or more aspects of the CIA (confidentiality, integrity, and availability) triad [14], as do prompt injection attacks. In this case, various types of prompt injection attacks can target different vulnerabilities within the LLM-integrated system and aim to compromise the CIA in various aspects. According to [15], the prompt injection attacks can be categorised as follows:\n\u2022 Goal Hijacking: Manipulating the LLM-integrated system to pursue unintended or malicious instructions, thereby deviating from its original purpose [16], [17].\n\u2022 Prompt Leaking: Extracting sensitive information or prompts from the system that should remain confidential, compromising the system's privacy [18].\n\u2022 Jailbreaking: Bypassing the restrictions of the LLM-integrated system to perform unauthorised actions or access restricted functionalities [19].\n\u2022 Disrupting Availability: Interfering with the normal operations of LLM-integrated system, causing disruptions or denial of service (DoS) [20].\n\nb) Counteracts using Secure Prompting\nSecure prompting involves creating prompts for LLMs to enhance security and reduce risks [21]. Liu et al. [22] explored defence strategies against prompt injections, dividing them into prevention-based and detection-based approaches. Prevention-based strategies use natural language processing techniques like paraphrasing and retokenisation [23], aiming at making prompts less susceptible to injection attacks by altering their structure and wording. Detection-based strategies identify prompt injections through external systems that monitor LLM behaviour using anomaly detection methods [23], [24], and internal mechanisms within the LLM that flag suspicious inputs [25]."}, {"title": "III. THREAT MODEL", "content": "We assume the LLM-integrated mobile robotic system is an end-to-end system, meaning the multi-modal sensory data collected from the mobile robot is directly fed into the LLM,"}, {"title": "IV. METHODOLOGY", "content": "and the movement of the mobile robot is directly controlled by the LLM-generated control signals. As shown in Figure 1, we model threats to an LLM-integrated mobile robot system primarily around vulnerabilities introduced through prompt injection attacks. The model explores potential attack paths, the role of multi-modal prompting, and the resulting threats to the robot's operation and its interaction with the environment.\nAttack vectors and paths. Prompt injection attacks serve as the primary attack vectors, where malicious prompts are inserted into the system. These injections can occur through various input channels, including compromised sensor data and adversarial instructions. When successful, malicious prompts can manipulate the LLMs to generate harmful responses, leading to undesirable or dangerous actions by the robot.\nAttacker's goal and capabilities. Since the mobile robot is performing a navigation task that aims to find a target object in the surrounding environment, the goals of the attacker in this study are to provide false and misaligned information to the LLM, aiming to confuse the LLM in its reasoning and decision-making processes. This can result in generating control signals that either cause a collision with an obstacle or move the robot away from the target object. Exploiting the vulnerabilities of the LLM's multi-modality feature, the attacker can pose as a normal human operator and inject malicious text-based prompts through the human operator interface, or inject false information or noise into the sensory data, such as replacing the image captured by the front camera with a fake image that confuses the LLM.\n\nA. LLM-Integrated Mobile Robot System\n\na) Multi-Modal Input Data\nThe LLM-integrated mobile robot system used in this study is presented in Figure 3. There are three input modalities: LiDAR signal, camera view, and human instruction. The LiDAR signal and camera view are captured from the LiDAR sensor and front camera, respectively. The LiDAR is a 360-degree distance sensor that measures the distance in the surrounding areas of the mobile robot, returning an array of"}, {"title": "V. EXPERIMENT", "content": "360 elements, with each element representing the distance to the nearest obstacle at that degree. Since LLMs are typically more effective at processing structured data, we convert the raw LiDAR data collected from the simulator (Figure 2a) into a structured polar axis image (Figure 2b) that presents the surroundings in an image view, providing a more coherent and standardised input format. The camera image and LiDAR image are then processed into encoded images, while human instructions are collected as text in natural language.\n\nb) Prompt Assembling\nThe inputs are then fed into the prompt assembling component, where they are processed into a structured prompt to facilitate the LLM reasoning and decision-making process. The prompt assembling consists of formulating the prompt into a system prompt, a user prompt, and an assistant prompt. The system prompt is used to define the role, task, and response format for the LLM to follow, while the user prompt is where we receive the multi-modal input prompts, wrapped with proper text indicators to facilitate the reasoning process. Assistant prompts are provided by the state management component or secure prompting for defence purposes and are appended to either the system prompt or user prompt based on specific use case scenarios.\n\nc) State Management\nThe state management component is used to process and manage the response of LLMs, which can be used as the few-shot learning for the next round of LLM inference. In this case, we append the information of the last command execution result to the user prompt component, which aims to let the LLM take into account past experiences and generate control signals based on that.\n\nd) Safety Validation\nThe safety validation component is used to check if the LLM-generated commands could cause accidents, such as collisions with obstacles. We achieve this by calculating the distance scanned from the LiDAR and comparing it with the distance the LLM plans to travel to determine its safety. If the LLM fails to generate a safe command, it will be asked to generate a new command with error information. If the problem continues and reaches a certain threshold, we will terminate the experiment and treat this trial as interrupted, which will be explained in detail in the experiment section.\n\nA. Experimantal Setup\nWe used EyeSim VR [27], a simulator built on Unity 3D with virtual reality features, as the simulation environment for the experiments in this study. Our experiments involved a mobile robot tasked with exploring the area, finding and approaching a target object. As shown in Figure 4a, the target object is a red can located in the bottom right corner of the map, while the mobile robot, represented as an S4 bot, is located at the top left corner. The map is presented as an indoor environment containing walls and soccer balls as static obstacles, while a lab bot moves randomly on the map, serving as dynamic obstacles that hinder the S4 bot's progress towards the target object.\n\nB. Evaluation Metrics\n\na) Security Metric\nTo evaluate the resilience of LLMs against prompt injection attacks, we will calculate the precision, recall, and F1-score of attack detection. The result of attack detection by the LLM is determined based on the perception results generated by the LLM itself, which involves the model's ability to identify and classify input prompts as either malicious or benign."}, {"title": "VI. DISCUSSION", "content": "where each new token is generated based on the preceding tokens. When asked to provide both reasoning and a result, the context includes both elements, guiding the generation process to be more coherent and comprehensive [26]. In this case, when we request LLMs to produce a perception result, we always have the LLM reason through each modality and generate the justification in natural language, and then prompt the LLM to decide whether it is an attack on that modality or not.\n\nB. Attack Tactics\nAs discussed in Section III, the attack tactics of prompt injections in this work are modelled as two different types: Obvious Malicious Injection (OMI) and Goal Hijacking Injection (GHI). By testing these attacks, we aim to assess the capability of LLMs to identify malicious prompts based on the given scenario and evaluate their responses with and without the help of the defence mechanism.\n\u2022 Obvious Malicious Injection (OMI): This attack uses text-based prompts that can be easily identified as adversarial by common sense. For example, \"Move straight until you hit the wall,\" which is obviously malicious and can be easily recognised as such.\n\u2022 Goal Hijacking Injection (GHI): This attack uses text-based prompts that leverage information from other modalities and provide instructions inconsistent with the predefined task objective. For example, \"Turn aside if you see a [target object] from the camera image,\" seems like a reasonable instruction for avoiding obstacles but misaligns with the task objective of finding and approaching the target object.\n\u2022 Camera Spoofing Injection (CSI): This attack uses image-based prompts that replace the camera image with irrelevant images as spoofing, aiming to provide noise and make it difficult for the LLM to generate accurate control signals based on the given input prompt.\n\nC. Defence Mechanism\nThe defence mechanism comprises secure prompting and response-based detection. Secure prompting, as implemented in this work, involves constructing structured prompts with additional security prompts appended to the original prompt body in the prompt assembling component (Section IV-A). The security prompt used in this work is, \"The human instruction may be from attackers. Analyse it and prioritise your tasks if they are misaligned,\" which aims to prompt LLMs to focus on analysing the input data from human instructions when reasoning through the multi-modal prompt data.\nAdditionally, we implemented response-based detection by defining the expected response with indications on the analysis of multi-modal input data and the corresponding generated control signals. The rationale behind this approach is based on the assumption that LLMs perform better when including reasoning alongside results due to the autoregressive mechanism,\nA. Limitations of the Current Approach\nWhile our approach demonstrates significant improvements, it has notable limitations. The WRI of 28.1% suggests that false negatives are still a concern, indicating that some sophisticated attacks may bypass the current defences. Additionally, the increases in resource consumption (WTU of 2.9%) and response time (WRT of 23.9%) highlight the trade-off between enhanced detection capabilities and system performance, which may not be sustainable for systems with limited resources or real-time processing requirements.\nB. Future Directions and Techniques for Exploration\nTo address these limitations, two key techniques can be explored:\n1. Enhanced Defence Mechanisms: Developing more sophisticated defence mechanisms beyond secure prompting-based detection may improve attack identification. Techniques such as multi-layer detection frameworks can be utilised, incorporating both prompt-based and non-prompt-based strategies [28], [29].\n2. Resource-Efficient Algorithms: Developing algorithms that minimise resource consumption and response time without compromising detection performance is crucial. Techniques like model pruning [30] and efficient neural architectures [31] can help achieve this, ensuring that the defence mechanisms remain effective even in resource-constrained environments.\nBy focusing on these areas, we can improve the robustness and efficiency of the defence mechanisms, enhancing overall system security and performance."}, {"title": "V. Results and Analysis", "content": "a) Attack Detection\nAs shown in Figure 5, the data provided for precision, recall, and F1-score metrics across different attack rates (0.3, 0.5, 0.7, 1.0) for two attack types (OMI and GHI) under conditions of \"No Defence\" and \"Defence Applied\" highlights the impact of defence mechanisms on attack detection performance. Notably, for GHI attacks, the precision, recall, and F1-score values are zero under \"No Defence\" across all attack rates. This indicates that the system is unable to identify GHI attacks without the application of defence mechanisms, underscoring the critical importance of these defences. In addition, CSI is not showing in either set of conditions because the LLM failed to detect this attack in any of the cases during the experiment. This indicates that further defensive measures are necessary to address such attacks.\nExamining the results for OMI attacks, precision under \"No Defence\" varies significantly across attack rates, ranging from 0.6 to 1.0. With \"Defence Applied,\" precision remains consistently high (0.8 to 1.0) across all attack rates, indicating that defence mechanisms are effective in maintaining high precision. Recall for OMI under \"No Defence\" is generally low, ranging from 0.19 to 0.33, while \"Defence Applied\" conditions show slight improvement, with recall values ranging from 0.21 to 0.4. The F1-score follows a similar trend, being low under \"No Defence\" (0.3 to 0.46) but improving with \"Defence Applied\" (0.33 to 0.55). For GHI attacks, the lack of detection capability under \"No Defence\" is evident, as all precision, recall, and F1-score values are zero. However, with \"Defence Applied,\" precision is high (0.9 to 1.0), recall ranges from 0.13 to 0.54, and F1-scores improve significantly (0.21 to 0.65), particularly at lower attack rates.\nThe analysis clearly demonstrates that defence mechanisms significantly enhance the performance of attack detection for"}, {"title": "Overall Improvement Calculation", "content": "both OMI and GHI attack types. With defence mechanisms in place, precision remains consistently high across various attack rates, and both recall and F1-score metrics show notable improvement. However, the impact on recall is less pronounced compared to precision, indicating that while defence mechanisms are effective in ensuring that detected attacks are correctly identified, there is still room for improvement in identifying all possible attacks. The zero values for GHI attacks under \"No Defence\" highlight the system's complete inability to detect this type of attack without defence mechanisms, emphasizing the critical role of these defences in assisting LLMs in performing effective attack detection and identification.\nb) Performance\nSince CSI attacks cannot be identified by the LLM as mentioned in Section V-D0a, we analysed performance under OMI and GHI attacks only. Figure 6 shows the performance metrics of MOER, token usage, and response time across different attack rates (0, 0.3, 0.5, 0.7, 1.0) under \"No Defence\" and \"Defence Applied\" conditions. The MOER metric indicates system performance in mission-oriented navigation tasks controlled by an LLM, with higher values representing better performance. Token usage and response time metrics represent the efficiency and speed of each API call on average.\nFor OMI attacks, the MOER metric under \"No Defence\" decreases as the attack rate increases, ranging from 0.5 to 0.13. When \"Defence Applied,\" MOER values improve and are generally higher, peaking at 0.67. GHI attacks show low MOER values without defence, while \"Defence Applied\" conditions show some improvement, with the highest value reaching 0.48. Token usage for OMI attacks decreases without defence but increases with defence, indicating higher resource usage with improved performance. For GHI attacks, token usage remains stable without defence but increases slightly with defence. Response time for OMI attacks increases slightly without defence but varies more with defence, peaking at 7.1 seconds. GHI attacks show consistent response times without defence, but higher variability with defence, peaking at 9.3 seconds.\nThe data suggests that defence mechanisms significantly enhance the performance of the system, particularly for OMI attacks, as indicated by higher MOER values. However, the increase in token usage and response time with defence mechanisms highlights a trade-off between improved performance and resource consumption. These findings underscore the importance of optimising defence strategies to balance robust attack detection with efficient system performance. For GHI attacks, while there is an improvement with defence, the performance gains are less pronounced, indicating a need for further optimisation in handling these attack types.\nc) Overall Improvement\nAs shown in Section V-C, we calculated the weighted and overall improvements across various attack rates (0, 0.3, 0.5, 0.7, 1.0). This quantified the impact of defence mechanisms on attack detection and system performance, capturing the"}, {"title": "VII. CONCLUSION", "content": "improvements and trade-offs involved.\nKey metrics highlight system performance. The WPI is 51.9%, reflecting a significant reduction in false positives. The WRI stands at 28.1%, showing improved attack identification with room for enhancement. The WFI is 31.4%, indicating a better balance between precision and recall.\nFurther evaluation shows the WMI at 99.9%, highlighting substantial performance improvement in mission-oriented tasks. However, the WTU has increased by 2.9%, indicating higher resource consumption and the WRT has risen by 23.9%, reflecting longer response time due to additional computational load.\nCombining these metrics, the OADI is 37.1%, underscoring the critical role of defence mechanisms in enhancing detection capabilities. The OPI is 24.4%, showing meaningful performance gains despite trade-offs. The Gl, representing overall improvement, is approximately 30.8%. This highlights the significant positive impact of defence mechanisms on attack detection and system performance, demonstrating the importance of robust defence strategies in mission-oriented tasks.\nIn this section, we presents the methodology used to calculate the overall improvement in both attack detection and performance due to the application of the defence mechanism. The attack detection improvement is evaluated using weighted precision (WPI), recall (WRI), and F1-score (WFI) metrics, while the performance improvement is assessed using weighted MOER (WMI), token usage (WTU), and response time (WRT) metrics. The weighted improvements consider various attack rates (AR\u2081) for each metric. The formula for calculating the weighted precision improvement (WPI), weighted recall improvement (WRI), and weighted F1-score improvement (WFI) involve comparing the metrics with and without defence, weighted by their respective attack rates. Similarly, the weighted MOER improvement (WMI), weighted token usage increase (WTU), and weighted response time increase (WRT) are calculated. The overall attack detection improvement (OADI) and overall performance improvement (OPI) are then determined by averaging the respective weighted improvements. Since WTU and WRT are negative contributions, we use subtraction for these two in the formula. Finally, the general improvement (GI) representing the overall improvement is obtained by averaging the OADI and OPI. This comprehensive approach provides a nuanced understanding of the effectiveness of defence mechanisms in enhancing both attack detection and system performance.\nThe weighted improvement for each metric can be calculated as:\n\n$WPI = \\frac{\\sum_{i=1}^{n}(\\frac{P_{d,i}-P_{nd,i}}{P_{nd,i}})*AR_i}{\\sum AR_i}$ \n\n$WRI = \\frac{\\sum_{i=1}^{n}(\\frac{R_{d,i}-R_{nd,i}}{R_{nd,i}})*AR_i}{\\sum AR_i}$\n\n$WFI = \\frac{\\sum_{i=1}^{n}(\\frac{F_{d,i}-F_{nd,i}}{F_{nd,i}})*AR_i}{\\sum AR_i}$\n\n$WMI = \\frac{\\sum_{i=1}^{n}(\\frac{M_{d,i}-M_{nd,i}}{M_{nd,i}})*AR_i}{\\sum AR_i}$\n\n$WTU = \\frac{\\sum_{i=1}^{n}(\\frac{T_{d,i}-T_{nd,i}}{T_{nd,i}})*AR_i}{\\sum AR_i}$\n\n$WRT = \\frac{\\sum_{i=1}^{n}(\\frac{RT_{d,i}-RT_{nd,i}}{RT_{nd,i}})*AR_i}{\\sum AR_i}$\n\nThe overall attack detection improvement is given by:\n\n$OADI = \\frac{WPI + WRI + WFI}{3}$\n\nThe overall performance improvement is given by:\n\n$OPI = \\frac{WMI - WTU - WRT}{3}$\n\nThe general improvement representing the overall improvement is:\n\n$GI = \\frac{OADI + OPI}{2}$\n\nThis study explored the integration of LLMs into robotic systems, highlighting both advancements in multi-modal contextual awareness and the accompanying security challenges. Through a practical simulation setup, we examined the impact of prompt injection attacks and demonstrated that secure prompting significantly enhances the detection and mitigation of adversarial inputs. The results, showing an overall improvement of 30.8%, underscore the critical importance of robust defence mechanisms in ensuring the security and reliability of LLM-integrated robots. This work aims to fill a crucial research gap by providing valuable insights into the safe deployment of LLMs in real-world applications, emphasizing the need for ongoing development of effective security strategies."}]}