{"title": "CAUSAL DISCOVERY-DRIVEN CHANGE POINT DETECTION IN\nTIME SERIES", "authors": ["Shanyun Gao", "Raghavendra Addanki", "Tong Yu", "Ryan A. Rossi", "Murat Kocaoglu"], "abstract": "Change point detection in time series seeks to identify times when the probability distribution of time\nseries changes. It is widely applied in many areas, such as human-activity sensing and medical science.\nIn the context of multivariate time series, this typically involves examining the joint distribution\nof high-dimensional data: If any one variable changes, the whole time series is assumed to have\nchanged. However, in practical applications, we may be interested only in certain components of\nthe time series, exploring abrupt changes in their distributions in the presence of other time series.\nHere, assuming an underlying structural causal model that governs the time-series data generation,\nwe address this problem by proposing a two-stage non-parametric algorithm that first learns parts of\nthe causal structure through constraint-based discovery methods. The algorithm then uses conditional\nrelative Pearson divergence estimation to identify the change points. The conditional relative Pearson\ndivergence quantifies the distribution disparity between consecutive segments in the time series,\nwhile the causal discovery method enables a focus on the causal mechanism, facilitating access\nto independent and identically distributed (IID) samples. Theoretically, the typical assumption of\nsamples being IID in conventional change point detection methods can be relaxed based on the Causal\nMarkov Condition. Through experiments on both synthetic and real-world datasets, we validate the\ncorrectness and utility of our approach.", "sections": [{"title": "Introduction", "content": "Change point analysis aims to detect distribution shifts in observational time series data. This topic has been explored\nextensively and is popular in many areas, such as human activity analysis Brahim-Belhouari and Bermak [2004],\nCleland et al. [2014], image analysis Radke et al. [2005] and financial markets Talih and Hengartner [2005].\nVarious change point detection methods are chosen based on the compatibility between the statistical feature shifts they\ndetect and the characteristics of the data.\nLikelihood ratio methods compare the probability density of two consecutive intervals of the time series data and the\nsignificant difference in the probability density implies the change point. One representative estimator is the relative\nunconstrained least-squares importance fitting (RuLSIF) in Yamada et al. [2013] and Liu et al. [2013].\nKernel-based methods in Harchaoui et al. [2008] and Harchaoui et al. [2009] utilize kernel-based test statistics to test\nthe homogeneity of sliding windows in time series data. Other methods include Probabilistic methods, Graph-based\nmethods, clustering methods and Subspace modeling. See Chib [1998], Friedman and Rafsky [1979], Keogh et al.\n[2001] and Itoh and Kurths [2010], respectively. See Aminikhanghahi and Cook [2017] for a survey.\nHowever, these existing methods have two main limitations. Firstly, for multivariate time series, the target features are\ngenerated from the joint distribution, which is typically high-dimensional. Ignoring the impact of high-dimensional\ndata, this problem remains fundamental: if a change occurs in only one univariate time series, it will affect the joint\nfeature of the entire multivariate time series. With multiple change point estimators, which estimator corresponds to\nwhich univariate time series is unknown. In practice, considering a bivariate time series, such as daily temperature\nand ice cream sales, or monthly GDP and stock prices, the change points across the whole series is less interesting\nthan the change point of ice cream sales and stock prices, given the information from temperature and GDP. Applying\nthe detection method to each univariate time series individually will lose information, especially if there are strong\ncorrelations among the component time series.\nSecondly, many existing methods, including probability density ratio methods, some probabilistic methods, kernel-based\nmethods, and graph-based methods, require independent and identically distributed (IID) samples Aminikhanghahi and\nCook [2017], Harchaoui et al. [2008], Liu et al. [2013], Saggioro et al. [2020]. Although many of these methods are\nrobust to non-IID samples in simulated data Liu et al. [2013], they lack theoretical guarantees, which are needed for\ntrustworthy deployment of these algorithms in the real world, especially in safety-critical applications Liu et al. [2018].\nTo address these limitations, we propose a novel change point detection algorithm that integrates change point detection\nwith causal discovery. Our algorithm is motivated by the following two considerations.\nFirst, causal structure helps us obtain a more fine-grained look at the joint distribution by disentangling the causal\nmechanism of each component in the time series. Second, relying on the Causal Markov Condition assumption in the\nStructural Causal Model (SCM) framework of Pearl [2009], the correlated samples become independent and identically\ndistributed (IID) when conditioned on their causal parents. This observation has been successfully used earlier in causal\ndiscovery from stationary time-series data Runge [2018]. In this context, change point detection aims to identify shifts\nin the causal mechanism, specifically, the conditional distribution of the target variable given its parents.\nIn this paper, we summarize our main contributions as follows:\n\u2022 We propose a novel non-parametric algorithm aiming at detecting the change point in causal mechanisms for\ndiscrete-valued time series data assuming an underlying Causal-Shift SCM without imposing any constraints\non the causal mechanisms or the data distributions. Our algorithm Causal-RuLSIF, systematically uses the\nRuLSIF estimator for detecting change points, originally introduced in Yamada et al. [2013] and Liu et al.\n[2013], integrating the PCMCI algorithm for causal discovery in Runge et al. [2019].\n\u2022 We validate our method with synthetic simulations on both soft mechanism change and hard mechanism\nchange time series, showing that our method can correctly detect the change points with high probability and\nhence learn the causal mechanism of the synthetic time series. We also employ our method in an air pollution\napplication."}, {"title": "Related Work", "content": "Depending on the underlying goal, our algorithm can be used to solve either a change point detection problem or a\ncausal discovery problem. Hence, there are two different directions of the related work.\nRegarding the change point detection problems, one representative likelihood ratio estimator used to detect the\ndistribution shifts is RuLSIF, which was first proposed for distribution comparison with IID samples in Yamada et al.\n[2013] and then has been verified as a useful method for time series data in Liu et al. [2013]. As discussed in the\ninstruction section, such existing methods concentrate on joint features associated with all time series, with many\nassuming IID samples.\nAs for the causal discovery methods, numerous efforts have been made recently to extend constraint-based algorithms to\naccommodate stationary time series data. One representative method is PCMCI in Runge et al. [2019]. Non-stationary\ntemporal data is more challenging in causal discovery since data statistics are time-variant, and it is unreasonable\nto expect that the underlying causal structure will remain constant. Many of the causal discovery methods with\nnon-stationary time series assume a parametric model, such as the vector autoregressive model in Gong et al. [2015] and\nMalinsky and Spirtes [2019]; linear and non-linear state-space model in Huang et al. [2019]; linear causal relationship\nin Saggioro et al. [2020]. A on-parametric algorithm, named CD-NOD in Huang et al. [2020] assumes a smooth change\nin the causal mechanism across the time index instead of a sudden change. Fujiwara et al. [2023] proposed an algorithm\nJIT-LINGAM to obtain a local approximated linear causal model combining algorithm LiNGAM and JIT framework\nfor non-linear and non-stationary data. In Gao et al. [2023], the underlying causal mechanism of each time series is\nassumed to change sequentially and periodically."}, {"title": "Causal-RuLSIF: Detecting Change Point in a Causal Time Series", "content": "In this section, we describe the framework and problem formulation for change point detection in a causal time series."}, {"title": "Preliminaries", "content": "Let G(V, E) denote the underlying causal graph, and the set of all incoming neighbors for each variable $X \\in V$ is\ndefined as the parent set, denoted by PA(X). For any $X, Y \\in V$ and $S \\subset V$, we denote the conditional independence:\n$X \\perp Y | S$.\nFor simplicity, let's define sets: $[b] := \\{1,2,...,b\\}$ and $[a, b] := \\{a, a + 1, ...,b\\}$, where $a, b \\in \\mathbb{N}$. Let $X_t^j \\in \\mathbb{R}$\ndenote the variable of jth component time series at time t; $X^j = \\{X_t^j\\}_{t\\in[T]} \\in \\mathbb{R}^T$ denote a univariate time series\nwhich is a component in time series and $X_t = \\{X_t^j\\}_{j\\in[n]} \\in \\mathbb{R}^n$ denote a slice of all variables at time point t.\n$V = \\{X^j\\}_{j\\in[n]} = \\{X_t\\}_{t\\in[T]} \\in \\mathbb{R}^{n \\times T}$ denotes a n-variate time series. By default, we assume $n > 1$ and hence\n$X \\subset V$, and $p(V) \\neq 0$, where $p(.)$ denotes the probability. For discrete-valued time series V, we assume that the\ndomain set of each component $X^j \\subseteq V$, denoted by $D = \\{d_1,\\dots,d_s\\}$ is the same, i.e., $X_t^j \\in D \\forall j \\in [n], t \\in [T]$.\nAs PA(X) represents a set containing random variables, $pa(X_t^j)$ refers to one specific configuration. Consider\n$PA(X_t^j) = \\{X_{t_1}^{i_1}, X_{t_2}^{i_2},...\\}$ where $1 \\leq i_1 \\leq i_2 \\leq \\dots \\leq n$ and $1 \\leq t_1 \\leq t_2 \\leq \\dots \\leq T$. E.g., $pa(X_t^j) =$\n$\\{1,1,1,2,... \\}$ corresponds to a particular configuration (or instantiation). We assume that the configuration set is\nordered in the ascending order of the parent set $\\{X_{t_1}^{i_1}, X_{t_2}^{i_2},...\\}$, prioritizing the variable index over the time index. We\nbegin by defining the Structural Causal Models (SCM) that capture our setting.\nDefinition 3.1 (Non-Stationary SCM [Gao et al., 2023]). A Non-Stationary SCM is a tuple $M = (V,F,E,P)$ where\nthere exists a $\\tau_{max} \\in \\mathbb{N}^+$, defined as: $\\tau_{max} := \\text{max}_\\{T : X_{t-\\tau}^j \\in PA(X_t^i),i,j \\in [n]\\}$, such that each variable\n$X_t | t>\\tau_{max} \\in V$ is a deterministic function of its parent set $PA(X_t | t>\\tau_{max}) \\in V$ and an unobserved (exogenous) variable\n$\\varepsilon_t|t>\\tau_{max} \\in E$:\n$X_t^j = f_{j,t}(PA(X_t^j), \\varepsilon_t), j\\in [n], t \\in [\\tau_{max} + 1,T]$,\nand there exist at least two different time points $t_0, t_1 \\in [\\tau_{max} + 1,T]$ satisfying\n$f_{j,t_0} \\neq f_{j,t_1}, j\\in [n], \\{t_0, t_1\\} \\subset [\\tau_{max} + 1, T]$.\nwhere $f_{j,t}, f_{j,t_0}, f_{j,t_1} \\in F$ and $\\{\\varepsilon_t\\}_{t\\in[T]}$ are jointly independent with probability measure $P$. Observe that $\\tau_{max}$ is the\nfinite maximal lag in terms of the causal graph $G$.\nDefinition 3.2 (Mechanism-Shift SCM). A Mechanism-Shift SCM is a Non-Stationary SCM that additionally satisfies\nthe following conditions. For each $j \\in [n]$, there exists an ascending sequence of time points $\\{T_1^j, T_2^j,\\dots,T_{c^j}^j\\}$ with\n$c^j \\in \\mathbb{N}^+, \\tau_{max} < T_1^j < \\dots < T_{c^j}^j < T$ such that:\na) $f_{j,t_1} = f_{j,t_2}$, if $\\exists c \\in [c^j] \\text{ s.t. } t_1, t_2 \\in [T_{c-1}^j, T_c^j+1 -1]$; $ \\qquad \\qquad$(3)\nb) $f_{j,t_1} \\neq f_{j,t_2}$, if $\\exists c \\in [c^j] \\text{ s.t. } T_{c-1}^j < t_1 < T_c^j < t_2 < T_{c+1}^j$; $ \\qquad \\qquad$(4)\nc) $PA(X_{t_1}^j) = \\{X_{t_1+(l-t_2)}^{i}: X_{t_2}^i \\in PA(X_{t_2}^j), i \\in [n]\\}$, $ \\qquad \\qquad$(5)\nif $\\exists c \\in [c^j] \\text{ s.t. } t_1, t_2 \\in [T_{c-1}^j, T_c^j+1 - 1]$;\nd) $\\varepsilon_{t_1}^i \\perp \\varepsilon_{t_2}^i \\text{ } \\forall t \\in [T]$. $ \\qquad \\qquad$(6)\ne) $\\{\\varepsilon_t^j\\}_{t\\in[T]}$ are i.i.d. $ \\qquad \\qquad$(7)\nare satisfied for all $t_1, t_2 \\in [\\tau_{max} + 1,T]$. This indicates that within the univariate time series $X^j$ in V, there is a finite\nnumber of change points represented by $c^j$. The variables in $X^j$ before and after a specific change point should exhibit\ndistinct causal mechanisms without intersecting other change points. An instance of this model is depicted in Fig. 1a).\nOur goal is to detect the change points of the causal mechanisms, and if necessary any causal relations that might be\nhelpful.\nDefinition 3.3 (Illusory Parent Sets). For a univariate time series $X^j \\in V$ with Mechanism-Shift SCM having change\npoints set $\\{T_1^j, T_2^j,\\dots,T_{c^j}^j\\}$ with $c^j \\in \\mathbb{N}^+, \\tau_{max} < T_1^j < \\dots < T_{c^j}^j < T$, parent set index $pInd_k \\in [c^j+1]$ is defined as:\n$pInd_k := \\{(T_i, Y_i)\\}_{i\\in[n]}$, given $PA(X_t^j) = \\{X_{t_1}^{i_1},X_{t_2}^{i_2},\\dots,X_{t_n}^{i_n}\\}$, $\\forall t \\in [T_{k-1}^j, T_k^j]$\nwhere $n = |PA(X_t^j)|$, $T_i$ is the time lag and $Y_i$ is the variable index; set $T_0^j = \\tau_{max}$ and $T_{c^j +1}^j = T$. Given $pInd_k$,\nIllusory Parent Sets are defined as:\n$PA_k^*(X_t^j) = \\{X_{t-\\tau}^{i}: (T_i, Y_i) \\in pInd_k\\}, \\forall k \\in \\{k : t \\notin [T_{k-1}^j, T_k^j]\\}$\nIn essence, the illusory parent sets of $X^j$ are time-shifted versions of the parent sets of other variables in $X^j$ that have\ndifferent causal mechanisms than $X^j$ across change points. These sets are a generalization of a notion described in Gao\net al. [2023]. $PA_k^*(X_t^j)$ can also be extended to encompass the true parent set of $X_t^j$, as: $PA_k^*(X_t^j) := PA(X_t^j), \\forall t \\in\n[T_{k-1}^j, T_k^j]$. Further, we define the union parent set as:\n$SPA(X_t^j) := \\cup_{k\\in[c^j+1]}PA_k^*(X_t^j), t \\in [\\tau_{max} + 1,T]$\nE.g., in Fig. 1a), $PA(X_t^1) = \\{X_{t-1}^1, X_{t-2}^2\\}$ with $t < T_{cp}$ and $PA(X_t^1) = \\{X_{t-1}^1, X_t^2, X_{t-1}^3\\}$ with $t > T_{cp}$. As only\none change point of $X^1$ with $c^1 = 1$ exists, we have two illusory parent sets with index $k \\in [c^1 + 1]$. More specifically,\nthe two illusory parent sets of $X^1$ are $PA_1^*(X_t^1) = \\{X_{t-1}^1, X_{t-2}^2\\}$ and $PA_2^*(X_t^1) = \\{X_{t-1}^1, X_t^2, X_{t-1}^3\\}$."}, {"title": "Robust Distributional Distance Estimation", "content": "Definition 3.4 (Time Series Segments). For a univariate discrete-valued time series $X^j \\in V$ with Mechanism-\nShift SCM having change points set $\\{T_1^j,T_2^j,\\dots,T_{c^j}^j\\}$ with $c^j \\in \\mathbb{N}^+, \\tau_{max} < T_1^j < \\dots < T_{c^j}^j < T$, and finite\ndomain set $D = \\{d_1, d_2,\\cdots,d_s\\}$, the Time Series Segments are a collection of non-overlapping non-empty subsets\n$\\{X^j(\\Lambda)\\}_{{\\Lambda}\\in[s^{|SPA(X_t^j)|}]}$ such that:\n$X^j(\\Lambda) := \\{X_t^j : t \\in [\\tau_{max} + 1,T], pa(X_t^j) = \\Sigma_\\Lambda\\}$, where ${\\Lambda} \\in [s^{|SPA(X_t^j)|}]]$.\nHere, $\\Sigma$ represents the configuration matrix of $SPA(X_t^j)$, where each row corresponds to one specific configuration of\n$SPA(X_t^j)$. $\\Lambda$ denotes the row index of $\\Sigma$. As the domain size is $|D| = s$, the number of rows in $\\Sigma$ is $s^{|SPA(X_t^j)|}$. Based\non the above definition, we can observe that:\n$\\cup_\\{{\\Lambda}\\in[s^{|SPA(X_t^j)|}]]}\\{X^j(\\Lambda)\\} = X^j \\text{ and } \\{X^j({\\Lambda}_1)\\} \\cap \\{X^j({\\Lambda}_2)\\} = \\emptyset.$\nIn summary, the Time Series Segments $\\{X^j(\\Lambda)\\}$ partition $X^j$ into multiple non-overlapping, non-empty sub-time\nseries, conditioned on the configurations of the union parent set $SPA(X_t^j)$. Variables $X_t^j$ within the same Time Series\nSegments $X^j({\\Lambda})$ share identical configurations of the union parent set (See Appendix A.1.1 for an example).\nRuLSIF: Robust Distributional Distance Estimation. Given two distributions $p(\u00b7)$ and $p'(\u00b7)$ defined over the same\nsupport, there exists many metrics measuring the distance between them. In Yamada et al. [2013], the authors proposed\na method named RuLSIF, with an a-relative divergence estimation $r_\\alpha(x)$ and a corresponding metric, a-relative Pearson\nDivergence $PE_\\alpha$ defined as following:\n$r_\\alpha(x) := \\frac{p(x)}{(1 - \\alpha)p(x) + \\alpha p'(x)} := \\frac{p(x)}{q_\\alpha(x)} \\text{ and } PE_\\alpha := E_{x~q_\\alpha}[ (r_\\alpha(x) - 1)^2]$\nwhere $\\alpha$ is a parameter used to bound the value of $r_\\alpha(x)$.\nIn our work, we revised the above definition for better applicability in the time series, incorporating a sliding window\nindex i. Denote the sliding windows as $W_i$. Instead of having two fixed sets of IID samples, we now collect samples\ndynamically. In sliding windows where a change point occurs in the second half, the samples in that second half window\nare drawn from a mixture distribution represented by $(1 \u2013 \\beta_i)p(x) + \\beta_ip'(x)$, where $\\beta_i$ indicates the proportion of\nsamples in the second half derived from $p'(x)$. Based on this, the dynamic density ratio and the corresponding PE\ndivergence score are defined as follows:\n$\\gamma_{\\alpha\\beta_i}(x) := \\frac{p(x)}{(1 \u2013 \\alpha\\beta_i)p(x) + \\alpha\\beta_i p'(x)} := \\frac{p(x)}{q_{\\alpha\\beta_i}(x)} \\text{ and } PE_{\\alpha\\beta_i} := E_{x~q_{\\alpha\\beta_i}}[ (\\gamma_{\\alpha\\beta_i}(x) \u2013 1)^2]$\nAssumptions. We highlight the key assumptions that we make: (1) At most one change point for any univariate time\nseries, and a (2) lower bound on the Pearson Divergence. The first assumption ensures that multiple change points do\nnot have a cancellation effect making it difficult to identify the change point, while the second assumption is commonly\nused in many standard distributional distance estimation papers in the finite sample, discrete settings.\nThe formalization of these two assumptions is provided in Appendix A.1.3, where other assumptions that are known in\nprior works can also be found."}, {"title": "Causal-RuLSIF Algorithm", "content": "In this section, we first present an algorithm called Causal-RuLSIF. We also establish the correctness of Causal-RuLSIF\nto estimate the change point within a confidence interval accurately.\nOverview of Algorithm 1 Causal-RuLSIF: The values of the parameters $n_w$, $n_{st}$ and $\\alpha$ are set in the beginning. Each\ncomponent $X^j$ where $j \\in [n]$ is analyzed sequentially. Using PCMCI Runge et al. [2019], we obtain a superset of\nparents for each variable $X_t^i \\in X^j$ denoted by $SPA(X_t^i)$ (line 2). To have balanced samples required in Theorem A.1,\nwe apply PCMCI to non-overlapping consecutive intervals and take the union of the obtained edges. Based on the\nparents' configurations of $SPA(X_t^i)$, Time Series Segments $\\{X^j(\\Lambda)\\}$ are constructed. After using RuLSIF Yamada\net al. [2013] and Liu et al. [2013] on sliding windows over $\\{X^j(\\Lambda)\\}$, we obtain the divergence series denoted by\n$\\{PE_{\\alpha\\beta_i}^{j,\\Lambda}\\}$ (line 7-8). Note that the number of Time Series Segments should equal the number of divergence series,\nas each Time Series Segments will have a divergence series. Across all divergence series for $X^j$, the change point"}, {"title": "Theoretical Guarantees", "content": "Algorithm 1 Causal-RuLSIF\n1: Input: A n-variate time series $V = (X^1, X^2, \\dots, X^n)$ with domain set $D = \\{d_1, \\dots d_s\\}$. Set appropriate $\\tau_{ub}$, $n_w$,\n$n_{st}$ and $\\alpha$.\n2: A superset of the parent set is obtained using PCMCI with $\\tau_{ub}$ and denote it by $SPA(X_t^i)$ $\\forall j, t$.\n3: for $X^j$ where $j \\in [n]$ do\n4: $PA(X_t^i) \\leftarrow SPA(X_t^i)$\n5: Construct the Time Series Segments $\\{X^j(\\Lambda)\\}$ based on parents' configurations of $SPA(X_t^j)$.\n6: for $X^j (\\Lambda)$ where ${\\Lambda}\\in [s^{|SPA(X_t^i)|}]]$ do\n7: Store divergence score series $\\{PE_{\\alpha\\beta_i}^{j,\\Lambda}\\}$ with RuLSIF on sliding windows shifting over $X^j(\\Lambda)$ where\n$i\\in [[ \\frac{T_{sub}}{N_{st}}-2n_w] + 1]$ and $T_{sub} = |X^j(\\Lambda)|$.\n8: end for\n9: $\\hat{\\Lambda} \\leftarrow \\text{arg} \\text{max}_{\\Lambda} PE_{\\alpha\\beta_i}^{j,\\Lambda}$ D Pick the $\\hat{\\Lambda}$th Time Series Segments whose maximum value over i is maximum.\n10: $\\hat{T} \\leftarrow \\text{arg}_i \\text{max} PE_{\\alpha\\beta_i}^{j,\\hat{\\Lambda}}$ D Pick the ith window index with maximum PE score on $X^j (\\hat{\\Lambda})$.\n11: Project $\\hat{T}$, where $j \\in [n]$, back to the original time series $X^j$ with $T_{ci}^j = ((\\hat{t} + \\hat{t} + 1))$, where $\\hat{t}$ is the\ntime index in $X^j$ corresponding to the window index $\\hat{T}$ in $X^j (\\hat{\\Lambda})$.\n12: Consider $X_{t-\\tau}^{i} \\in PA(X_t^j)$. Remove $X_{t-\\tau}^{i}$ from $PA(X_t^j)$ using PCMCI if $X_{t-\\tau}^{i} \\perp X_t^j |$\n$(SPA(X_t^j) \\cup SPA(X_{t-1}^j)) \\setminus X_{t-\\tau}^{i}$ on samples $\\{X_{t-1}^j\\}_{t\\leq T_{ci}^j}$ and $\\{X_{t+1}^j\\}_{t>T_{ci}^j}$ respectively.\n13: end for\n14: return $T_{ci}^j$ and $PA(X_t^i)$ $\\forall j, t$.\nestimator $\\hat{T}$ is the window index that maximizes the PE divergence of $PE_{\\alpha\\beta_i}^{j,\\Lambda}$ based on all Time Series Segments\n(line 9-10). Since $\\hat{T}$ is the window index in Time Series Segments and not the original time index in $X^j$, it should\nbe projected back to $X^j$. The final change point estimator for $X^j$ is then obtained and denoted as $T_{ci}^j$ (line 11). After\nobtaining an accurate estimator $T_{ci}^j$ proved in Theorem 4.2, one may optionally run PCMCI on samples $X^j$ before and\nafter the change point $T_{ci}^j$, respectively, to fully learn the underlying causal graph.\nTheorem 4.1 establishes that if the window $W_i$ only contains samples from a single conditional distribution, meaning\nthere is no change point included in this window, the estimated relative Pearson Divergence $PE_{\\alpha\\beta_i}$ is close to zero\nwith high probability. Theorem 4.2 provides a confidence interval for the change point estimator $\\hat{T}$. All the missing\ndetails are provided in Appendix A.2.\nTheorem 4.1. Let $PE_{\\alpha\\beta_i}$ be the estimated PE series for one Time Series Segments $X^j(\\Lambda) \\subseteq X^j \\subseteq V$. Under certain\nassumptions, we have that $\\forall i \\in \\{i : i_{nst} + 2n_w - 1 < T_c\\}$\n$Pr(\\text{max} |PE_{\\alpha\\beta_i}| < o_p(1)) > 1 \u2013 \\frac{\\alpha_w}{\\frac{2}{\\frac{n_w}{n_{st}}}}\\frac{\\alpha_w}{\\log T_{sub}} \\frac{2}{T_{sub}}$,\nwhere $b_{st} = \\frac{[T_{sub}]}{[T_{sub}]}$ and $a_w = \\frac{[T_{sub}]}{n_{st}}$.\nThe window index i satisfying $i_{nst} + 2n_w - 1 < T_c$ guarantees that all the samples in $W_i$ are collected from the same\ndistribution. Theorem 4.1 states that the maximum estimated PE series obtained from such windows are bounded\nby any positive constant with probability $1 \u2013 \\frac{\\alpha_w}{b_{st} \\log T_{sub}} \\frac{2}{T_{sub}}$ if $n_w$ are larger than some threshold. In other words,\n$\\forall k > 0, \\exists N$ such that $\\forall n_w > N$:\n$Pr(\\text{max} PE_{\\alpha\\beta_i} < k) = 1 \u2013 \\frac{\\alpha_w}{\\frac{2}{\\frac{n_w}{n_{st}}}}\\frac{\\alpha_w}{\\log T_{sub}} \\frac{2}{T_{sub}}$.\nTheorem 4.2. Let $PE_{\\alpha\\beta_i}$ be the estimated PE series for one Time Series Segments $X^j(\\Lambda) \\subseteq X^j \\subseteq V$ and $T_c^j$ denote\nthe true change point in this Time Series Segments. $\\hat{T}$ denotes the estimator of $T_c^j$ obtained by:\n$\\hat{T} = \\text{arg}_i \\text{ max} PE_{\\alpha\\beta_i}$\nUnder certain assumptions, we have that given large enough $n_w$, $\\forall i \\in [\\tau_{max} + 1,T]$\n$Pr(\\{|\\hat{T} \u2013 T_c^j| < 2n_w\\}) > (1 \u2013 \\frac{\\alpha_w}{\\frac{2}{\\frac{n_w}{n_{st}}}}\\frac{\\alpha_w}{\\log T_{sub}} \\frac{2}{T_{sub}})(1-\\frac{1}{\\sqrt{n_w}})$,         (16)\nwhere $b_{st} = \\frac{[T_{sub}]}{[T_{sub}]}$, $\\alpha_w = \\frac{[T_{sub}]}{n_{st}}$, and $T_{sub}$ as defined in Algorithm 1."}, {"title": "Experiments", "content": "In this section, we provide an empirical evaluation of our approaches against existing approaches on synthetic and\nreal-world datasets."}, {"title": "Simulations on binary multivariate time series", "content": "To validate the correctness and effectiveness of our algorithm, we perform a series of experiments on binary-valued\ntime series dataset. In this section, we have five baseline algorithms, including RuLSIF algorithm in Liu et al. [2013", "1981": "changeforest algorithm (RF) in Londschien et al. [2023", "2013": "and kscp3o algorithm in Zhang et al. [2017", "Q": "."}, {"2013": "and Harchaoui et al. [2008"}, {"2013": "and Harchaoui et al. [2008"}]}