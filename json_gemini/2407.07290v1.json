{"title": "Causal Discovery-Driven Change Point Detection in Time Series", "authors": ["Shanyun Gao", "Raghavendra Addanki", "Tong Yu", "Ryan A. Rossi", "Murat Kocaoglu"], "abstract": "Change point detection in time series seeks to identify times when the probability distribution of time series changes. It is widely applied in many areas, such as human-activity sensing and medical science. In the context of multivariate time series, this typically involves examining the joint distribution of high-dimensional data: If any one variable changes, the whole time series is assumed to have changed. However, in practical applications, we may be interested only in certain components of the time series, exploring abrupt changes in their distributions in the presence of other time series. Here, assuming an underlying structural causal model that governs the time-series data generation, we address this problem by proposing a two-stage non-parametric algorithm that first learns parts of the causal structure through constraint-based discovery methods. The algorithm then uses conditional relative Pearson divergence estimation to identify the change points. The conditional relative Pearson divergence quantifies the distribution disparity between consecutive segments in the time series, while the causal discovery method enables a focus on the causal mechanism, facilitating access to independent and identically distributed (IID) samples. Theoretically, the typical assumption of samples being IID in conventional change point detection methods can be relaxed based on the Causal Markov Condition. Through experiments on both synthetic and real-world datasets, we validate the correctness and utility of our approach.", "sections": [{"title": "Introduction", "content": "Change point analysis aims to detect distribution shifts in observational time series data. This topic has been explored extensively and is popular in many areas, such as human activity analysis Brahim-Belhouari and Bermak [2004], Cleland et al. [2014], image analysis Radke et al. [2005] and financial markets Talih and Hengartner [2005].\nVarious change point detection methods are chosen based on the compatibility between the statistical feature shifts they detect and the characteristics of the data.\nLikelihood ratio methods compare the probability density of two consecutive intervals of the time series data and the significant difference in the probability density implies the change point. One representative estimator is the relative unconstrained least-squares importance fitting (RuLSIF) in Yamada et al. [2013] and Liu et al. [2013].\nKernel-based methods in Harchaoui et al. [2008] and Harchaoui et al. [2009] utilize kernel-based test statistics to test the homogeneity of sliding windows in time series data. Other methods include Probabilistic methods, Graph-based methods, clustering methods and Subspace modeling. See Chib [1998], Friedman and Rafsky [1979], Keogh et al. [2001] and Itoh and Kurths [2010], respectively. See Aminikhanghahi and Cook [2017] for a survey.\nHowever, these existing methods have two main limitations. Firstly, for multivariate time series, the target features are generated from the joint distribution, which is typically high-dimensional. Ignoring the impact of high-dimensional data, this problem remains fundamental: if a change occurs in only one univariate time series, it will affect the joint feature of the entire multivariate time series. With multiple change point estimators, which estimator corresponds to which univariate time series is unknown. In practice, considering a bivariate time series, such as daily temperature and ice cream sales, or monthly GDP and stock prices, the change points across the whole series is less interesting than the change point of ice cream sales and stock prices, given the information from temperature and GDP. Applying the detection method to each univariate time series individually will lose information, especially if there are strong correlations among the component time series.\nSecondly, many existing methods, including probability density ratio methods, some probabilistic methods, kernel-based methods, and graph-based methods, require independent and identically distributed (IID) samples Aminikhanghahi and Cook [2017], Harchaoui et al. [2008], Liu et al. [2013], Saggioro et al. [2020]. Although many of these methods are robust to non-IID samples in simulated data Liu et al. [2013], they lack theoretical guarantees, which are needed for trustworthy deployment of these algorithms in the real world, especially in safety-critical applications Liu et al. [2018].\nTo address these limitations, we propose a novel change point detection algorithm that integrates change point detection with causal discovery. Our algorithm is motivated by the following two considerations.\nFirst, causal structure helps us obtain a more fine-grained look at the joint distribution by disentangling the causal mechanism of each component in the time series. Second, relying on the Causal Markov Condition assumption in the Structural Causal Model (SCM) framework of Pearl [2009], the correlated samples become independent and identically distributed (IID) when conditioned on their causal parents. This observation has been successfully used earlier in causal discovery from stationary time-series data Runge [2018]. In this context, change point detection aims to identify shifts in the causal mechanism, specifically, the conditional distribution of the target variable given its parents.\nIn this paper, we summarize our main contributions as follows:\n\u2022 We propose a novel non-parametric algorithm aiming at detecting the change point in causal mechanisms for discrete-valued time series data assuming an underlying Causal-Shift SCM without imposing any constraints on the causal mechanisms or the data distributions. Our algorithm Causal-RuLSIF, systematically uses the RuLSIF estimator for detecting change points, originally introduced in Yamada et al. [2013] and Liu et al. [2013], integrating the PCMCI algorithm for causal discovery in Runge et al. [2019].\n\u2022 We validate our method with synthetic simulations on both soft mechanism change and hard mechanism change time series, showing that our method can correctly detect the change points with high probability and hence learn the causal mechanism of the synthetic time series. We also employ our method in an air pollution application."}, {"title": "Related Work", "content": "Depending on the underlying goal, our algorithm can be used to solve either a change point detection problem or a causal discovery problem. Hence, there are two different directions of the related work.\nRegarding the change point detection problems, one representative likelihood ratio estimator used to detect the distribution shifts is RuLSIF, which was first proposed for distribution comparison with IID samples in Yamada et al. [2013] and then has been verified as a useful method for time series data in Liu et al. [2013]. As discussed in the instruction section, such existing methods concentrate on joint features associated with all time series, with many assuming IID samples.\nAs for the causal discovery methods, numerous efforts have been made recently to extend constraint-based algorithms to accommodate stationary time series data. One representative method is PCMCI in Runge et al. [2019]. Non-stationary temporal data is more challenging in causal discovery since data statistics are time-variant, and it is unreasonable to expect that the underlying causal structure will remain constant. Many of the causal discovery methods with non-stationary time series assume a parametric model, such as the vector autoregressive model in Gong et al. [2015] and Malinsky and Spirtes [2019]; linear and non-linear state-space model in Huang et al. [2019]; linear causal relationship in Saggioro et al. [2020]. A on-parametric algorithm, named CD-NOD in Huang et al. [2020] assumes a smooth change in the causal mechanism across the time index instead of a sudden change. Fujiwara et al. [2023] proposed an algorithm JIT-LINGAM to obtain a local approximated linear causal model combining algorithm LiNGAM and JIT framework for non-linear and non-stationary data. In Gao et al. [2023], the underlying causal mechanism of each time series is assumed to change sequentially and periodically."}, {"title": "Causal-RuLSIF: Detecting Change Point in a Causal Time Series", "content": "In this section, we describe the framework and problem formulation for change point detection in a causal time series."}, {"title": "Preliminaries", "content": "Let G(V, E) denote the underlying causal graph, and the set of all incoming neighbors for each variable \\(X \\in V\\) is defined as the parent set, denoted by PA(X). For any \\(X, Y \\in V\\) and \\(S \\subset V\\), we denote the conditional independence: X is independent of Y conditioned on S, by \\(X \\perp Y | S\\).\nFor simplicity, let's define sets: \\([b] := {1,2,...,b}\\) and \\([a, b] := {a, a + 1, ...,b\\}\\), where a, b \u2208 N. Let \\(X_t^j \\in R\\) denote the variable of jth component time series at time t; \\(X^j = \\{X_t^j\\}_{t\\in[T]} \\in R^T\\) denote a univariate time series which is a component in time series and \\(X_t = \\{X_t^j\\}_{j\\in[n]} \\in R^n\\) denote a slice of all variables at time point t. \\(V = \\{X^j\\}_{j\\in[n]} = \\{X_t\\}_{t\\in[T]} \\in R^{n\\times T}\\) denotes a n-variate time series. By default, we assume n > 1 and hence \\(X \\subset V\\), and \\(p(V) \\neq 0\\), where \\(p(.)\\) denotes the probability. For discrete-valued time series V, we assume that the domain set of each component \\(X^j \\subset V\\), denoted by \\(D = \\{d_1, ..., d_s\\}\\) is the same, i.e., \\(X_t^j \\in D \\forall j \\in [n], t \\in [T]\\).\nAs PA(X) represents a set containing random variables, pa(\\(X_t^j\\)) refers to one specific configuration. Consider PA(\\(X_t^j\\)) = \\(\\{X_{t_1}^{i_1}, X_{t_2}^{i_2}, ...\\}\\) where \\(1 \\leq i_1 \\leq i_2 \\leq ... \\leq n\\) and \\(1 \\leq t_1 \\leq t_2 \\leq ... \\leq T\\). E.g., pa(\\(X_t^j\\)) = \\(\\{1,0,1,2,... \\}\\) corresponds to a particular configuration (or instantiation). We assume that the configuration set is ordered in the ascending order of the parent set \\(\\{X_{t_1}^{i_1}, X_{t_2}^{i_2}, ...\\}\\), prioritizing the variable index over the time index. We begin by defining the Structural Causal Models (SCM) that capture our setting.\nDefinition 3.1 (Non-Stationary SCM [Gao et al., 2023]). A Non-Stationary SCM is a tuple \\(M = (V,F,E,P)\\) where there exists a \\(\\tau_{max} \\in N^+\\), defined as: \\(\\tau_{max} := max\\{\\tau : X_{t-\\tau}^i \\in PA(X_t^j), i,j \\in [n]\\}\\), such that each variable \\(X_t, t > \\tau_{max} \\in V\\) is a deterministic function of its parent set PA(\\(X_t, t > \\tau_{max}\\)) \u2208 V and an unobserved (exogenous) variable \\(E_t, t > \\tau_{max} \\in E\\):\n\\(X_t^j = f_{j,t}(PA(X_t^j), e_t), j\\in [n], t \\in [\\tau_{max} + 1,T]\\),\t{(1)}\nand there exist at least two different time points \\(t_0, t_1 \\in [\\tau_{max} + 1,T]\\) satisfying\n\\(f_{j,t_0} \\neq f_{j,t_1}, j\\in [n], \\{t_0, t_1\\} \\subset [\\tau_{max} + 1, T]\\).\t{(2)}\nwhere \\(f_{j,t}, f_{j,t_0}, f_{j,t_1} \\in F\\) and \\(\\{e_t\\}_{t\\in[T]}\\) are jointly independent with probability measure P. Observe that \\(\\tau_{max}\\) is the finite maximal lag in terms of the causal graph G.\nDefinition 3.2 (Mechanism-Shift SCM). A Mechanism-Shift SCM is a Non-Stationary SCM that additionally satisfies the following conditions. For each j \u2208 [n], there exists an ascending sequence of time points \\(\\{\\tau_1^j, \\tau_2^j, ..., \\tau_{c^j}^j\\}\\) with \\(c^j \\in N^+, \\tau_{max} < \\tau_1^j < ... < \\tau_{c^j}^j < T\\) such that:\na) \\(f_{j,t_1} = f_{j,t_2}\\), if \\(\\exists c \\in [c^j] s.t. t_1, t_2 \\in [\\tau_{c-1}^j, \\tau_c^j + 1 -1]\\);\t{(3)}\nb) \\(f_{j,t_1} \\neq f_{j,t_2}\\), if \\(\\exists c \\in [c^j] s.t. \\tau_{c-1}^j < t_1 < \\tau_c^j < t_2 < \\tau_c^j + 1\\);\t{(4)}\nc) PA(\\(X_{t_1}^j\\)) = \\(\\{X_{t_1-\\tau}^{(i)}: X_{t_2-\\tau}^{(i)} \\in PA(X_{t_2}^j), i \\in [n]\\}\\), \t{(5)}\nif \\(\\exists c \\in [c^j] s.t. t_1, t_2 \\in [\\tau_{c-1}^j, \\tau_c^j + 1 -1]\\);\td) \\(e_{t_1}, e_{t_1+Nw}\\) are i.i.d. \\(\\forall t \\in [T]\\).\t{(6)}\t{(7)}\nare satisfied for all \\(t_1, t_2 \\in [\\tau_{max} + 1,T]\\). This indicates that within the univariate time series \\(X^j\\) in V, there is a finite number of change points represented by \\(c^j\\). The variables in \\(X^j\\) before and after a specific change point should exhibit distinct causal mechanisms without intersecting other change points. An instance of this model is depicted in Fig. 1a).\nOur goal is to detect the change points of the causal mechanisms, and if necessary any causal relations that might be helpful.\nDefinition 3.3 (Illusory Parent Sets). For a univariate time series \\(X^j \\in V\\) with Mechanism-Shift SCM having change points set \\(\\{\\tau_1^j, \\tau_2^j, ..., \\tau_{c^j}^j\\}\\) with \\(c^j \\in N^+, \\tau_{max} < \\tau_1^j < ... < \\tau_{c^j}^j < T\\), parent set index pInd\\(^j_k, k \\in [c^j+1]\\) is defined as:\npInd\\(^j_k := \\{(\\tau_i, Y_i)\\}i\\in[n]\\), given PA(\\(X_t^j\\)) = \\(\\{X_{t-\\tau}^{(i)},X_{t-\\tau}^{(i)}, ...\\}\\), \\(\\forall t \\in [\\tau_{k-1}^j, \\tau_k^j]\\)\t{(8)}\nwhere \\(\\eta = |PA(X^j)|\\), \\(\\tau_i\\) is the time lag and \\(Y_i\\) is the variable index; set \\(\\tau_0 = \\tau_{max}\\) and \\(\\tau_{c^j +1} = T\\). Given pInd\\(^j_k\\), Illusory Parent Sets are defined as:\nPA\\(_k^j(X_t^j) = \\{X_{t-\\tau}^{(i)}: (\\tau_i, Y_i) \\in pInd^j_k\\}\\), \\(\\forall k \\in \\{k : t \\notin [\\tau_{k-1}^j, \\tau_k^j]\\}\\)\t{(9)}\nIn essence, the illusory parent sets of \\(X^j\\) are time-shifted versions of the parent sets of other variables in \\(X^j\\) that have different causal mechanisms than \\(X^j\\) across change points. These sets are a generalization of a notion described in Gao et al. [2023]. PA\\(_k^j(X_t^j)\\) can also be extended to encompass the true parent set of \\(X^j\\), as: PA\\(_k^j(X_t^j) := PA(X_t^j)\\), \\(\\forall t \\in [\\tau_{k-1}^j, \\tau_k^j]\\). Further, we define the union parent set as:\nSPA(\\(X_t^j\\)) := \\(\bigcup_{k\\in[c^j+1]}PA_k^j(X_t^j)\\), t \u2208 \\([\\tau_{max} + 1,T]\\)\t{(10)}\nE.g., in Fig. 1a), PA(\\(X_t^1\\)) = \\(\\{X_{t-1}^1, X_{t-2}^2\\}\\) with \\(t < \\tau_{cp}\\) and PA(\\(X_t^1\\)) = \\(\\{X_{t-1}^1, X_{t}^2, X_{t-1}^3\\}\\) with \\(t > \\tau_{cp}\\). As only one change point of \\(X^1\\) with \\(c^1 = 1\\) exists, we have two illusory parent sets with index \\(k \\in [c^1 + 1]\\). More specifically, the two illusory parent sets of \\(X^1\\) are PA\\(_1(X_t^1) = \\{X_{t-1}^1, X_{t-2}^2\\}\\) and PA\\(_2(X_t^1) = \\{X_{t-1}^1, X_{t}^2, X_{t-1}^3\\}\\)."}, {"title": "Definition 3.4 (Time Series Segments)", "content": "For a univariate discrete-valued time series \\(X^j \\in V\\) with Mechanism-Shift SCM having change points set \\(\\{\\tau_1^j, \\tau_2^j, ..., \\tau_{c^j}^j\\}\\) with \\(c^j \\in N^+, \\tau_{max} < \\tau_1^j < ... < \\tau_{c^j}^j < T\\), and finite domain set \\(D = \\{d_1, d_2, ...,d_s\\}\\), the Time Series Segments are a collection of non-overlapping non-empty subsets \\(\\{X^j(\\Lambda)\\}_{\\Lambda\\in[s^{|SPA(X^j)|}]}\\) such that:\n\\(X^j(\\Lambda) := \\{X_t^j : t \\in [\\tau_{max} + 1,T], pa(X_t^j) = \\Sigma_{\\Lambda}\\}\\), where \\(\\Lambda \\in [s^{|SPA(X^j)|}]\\).\t{(11)}\nHere, \\(\\Sigma\\) represents the configuration matrix of SPA(\\(X^j\\)), where each row corresponds to one specific configuration of SPA(\\(X^j\\)). \\(\\Lambda\\) denotes the row index of \\(\\Sigma\\). As the domain size is \\(|D| = s\\), the number of rows in \\(\\Sigma\\) is \\(s^{|SPA(X^j)|}\\). Based on the above definition, we can observe that:\n\\(\bigcup_{\\Lambda\\in[s^{|SPA(X^j)|}]} \\{X^j(\\Lambda)\\} = X^j\\) and \\(\\{X^j(\\Lambda_1)\\} \\cap \\{X^j(\\Lambda_2)\\} = \\emptyset\\).\t{(12)}\nIn summary, the Time Series Segments \\(\\{X^j(\\Lambda)\\}\\) partition \\(X^j\\) into multiple non-overlapping, non-empty sub-time series, conditioned on the configurations of the union parent set SPA(\\(X^j\\)). Variables \\(X_t^j\\) within the same Time Series Segments \\(X^j(\\Lambda)\\) share identical configurations of the union parent set (See Appendix A.1.1 for an example).\n*RuLSIF: Robust Distributional Distance Estimation*. Given two distributions p(\u00b7) and p'(\u00b7) defined over the same support, there exists many metrics measuring the distance between them. In Yamada et al. [2013], the authors proposed a method named RuLSIF, with an a-relative divergence estimation \\(r_{\\alpha}(x)\\) and a corresponding metric, a-relative Pearson Divergence \\(PE_{\\alpha}\\) defined as following:\n\\(r_{\\alpha}(x) := \\frac{p(x)}{(1 - \\alpha)p(x) + \\alpha p'(x)} := \\frac{p(x)}{q_{\\alpha}(x)}\\) and \\(PE_{\\alpha} := E_{x~q_{\\alpha}}[ (r_{\\alpha}(x) - 1)^2 ]\\)\t{(13)}\nwhere \u03b1 is a parameter used to bound the value of \\(r_{\\alpha}(x)\\).\nIn our work, we revised the above definition for better applicability in the time series, incorporating a sliding window index i. Denote the sliding windows as \\(W_i\\). Instead of having two fixed sets of IID samples, we now collect samples dynamically. In sliding windows where a change point occurs in the second half, the samples in that second half window are drawn from a mixture distribution represented by \\((1 - \\beta_i)p(x) + \\beta_i p'(x)\\), where \\(\\beta_i\\) indicates the proportion of samples in the second half derived from p'(x). Based on this, the dynamic density ratio and the corresponding PE divergence score are defined as follows:\n\\(\\gamma_{\\alpha \\beta_i}(x) := \\frac{p(x)}{(1 - \\alpha \\beta_i)p(x) + \\alpha \\beta_i p'(x)} := \\frac{p(x)}{q_{\\alpha \\beta_i}(x)}\\) and \\(PE_{\\alpha \\beta_i} := \\frac{1}{2} E_{x~q_{\\alpha \\beta_i}}[ (\\gamma_{\\alpha \\beta_i}(x) - 1)^2 ]\\)\t{(14)}\nAssumptions. We highlight the key assumptions that we make: (1) At most one change point for any univariate time series, and a (2) lower bound on the Pearson Divergence. The first assumption ensures that multiple change points do not have a cancellation effect making it difficult to identify the change point, while the second assumption is commonly used in many standard distributional distance estimation papers in the finite sample, discrete settings.\nThe formalization of these two assumptions is provided in Appendix A.1.3, where other assumptions that are known in prior works can also be found."}, {"title": "Causal-RuLSIF Algorithm", "content": "In this section, we first present an algorithm called Causal-RuLSIF. We also establish the correctness of Causal-RuLSIF to estimate the change point within a confidence interval accurately.\nOverview of Algorithm 1 Causal-RuLSIF: The values of the parameters nw, nst and a are set in the beginning. Each component \\(X^j\\) where j \u2208 [n] is analyzed sequentially. Using PCMCI Runge et al. [2019], we obtain a superset of parents for each variable \\(X_t^j \\in X^j\\) denoted by SPA(\\(X^j\\)) (line 2). To have balanced samples required in Theorem A.1, we apply PCMCI to non-overlapping consecutive intervals and take the union of the obtained edges. Based on the parents' configurations of SPA(\\(X^j\\)), Time Series Segments \\(\\{X^j(\\Lambda)\\}\\) are constructed. After using RuLSIF Yamada et al. [2013] and Liu et al. [2013] on sliding windows over \\(\\{X^j(\\Lambda)\\}\\), we obtain the divergence series denoted by \\(\\{PE_{\\alpha \\beta_i}^{j,\\Lambda}\\}\\) (line 7-8). Note that the number of Time Series Segments should equal the number of divergence series, as each Time Series Segments will have a divergence series. Across all divergence series for \\(X^j\\), the change point estimator \\(\\hat{T}^j\\) is the window index that maximizes the PE divergence of \\(PE_{\\alpha \\beta_i}^{j,\\Lambda}\\) based on all Time Series Segments (line 9-10). Since \\(\\hat{T}^j\\) is the window index in Time Series Segments and not the original time index in \\(X^j\\), it should be projected back to \\(X^j\\). The final change point estimator for \\(X^j\\) is then obtained and denoted as \\(\\hat{\\tau}^j\\) (line 11). After obtaining an accurate estimator \\(\\hat{\\tau}^j\\) proved in Theorem 4.2, one may optionally run PCMCI on samples \\(X^j\\) before and after the change point \\(\\tau_{c^j}\\), respectively, to fully learn the underlying causal graph."}, {"title": "Theoretical Guarantees", "content": "Theorem 4.1 establishes that if the window \\(W_i\\) only contains samples from a single conditional distribution, meaning there is no change point included in this window, the estimated relative Pearson Divergence \\(PE_{\\alpha \\beta_i}\\) is close to zero with high probability. Theorem 4.2 provides a confidence interval for the change point estimator \\(\\hat{\\tau}_{c^j}\\). All the missing details are provided in Appendix A.2.\nTheorem 4.1. Let \\(PE_{\\alpha \\beta_i}\\) be the estimated PE series for one Time Series Segments \\(X^j(\\Lambda) \\subseteq X^j \\subseteq V\\). Under certain assumptions, we have that \\(\\forall i \\in \\{i : i_{nst} + 2n_w - 1 < \\tau_{c^j}\\}\n\\(Pr(\\max_i |PE_{\\alpha \\beta_i}| < o_p(1)) > 1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}}\t{(24)}\nwhere \\(b_{st} = [\\frac{T_{sub}}{n_w}]\\) and \\(a_w = [\\frac{T_{sub}}{n_{st}}]\\).\nThe window index i satisfying \\(i_{nst} + 2n_w - 1 < \\tau_{c^j}\\) guarantees that all the samples in \\(W_i\\) are collected from the same distribution. Theorem 4.1 states that the maximum estimated PE series obtained from such windows are bounded by any positive constant with probability \\(1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}}\\) if \\(n_w\\) are larger than some threshold.\nTheorem 4.2. Let \\(PE_{\\alpha \\beta_i}\\) be the estimated PE series for one Time Series Segments \\(X^j(\\Lambda) \\subseteq X^j \\subseteq V\\) and \\(\\tau_{c^j}\\) denote the true change point in this Time Series Segments. \\(\\hat{\\tau}^j\\) denotes the estimator of \\(\\tau_{c^j}\\) obtained by:\n\\(\\hat{\\tau}^j = arg_i max PE_{\\alpha \\beta_i}\t{(15)}\nUnder certain assumptions, we have that given large enough \\(n_w\\), \\(\\forall i \\in [\\tau_{max} + 1,T]\\)\n\\(Pr(\\{\\hat{\\tau}^j - \\tau_{c^j}| < 2n_w\\}) > (1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}})(1 - \\frac{1}{n_w})\t{(16)}\nwhere \\(b_{st} = [\\frac{T_{sub}}{n_w}]\\) , \\(a_w = [\\frac{T_{sub}}{n_{st}}]\\), and \\(T_{sub}\\) as defined in Algorithm 1"}, {"title": "Experiments", "content": "In this section, we provide an empirical evaluation of our approaches against existing approaches on synthetic and real-world datasets."}, {"title": "Simulations on binary multivariate time series", "content": "To validate the correctness and effectiveness of our algorithm, we perform a series of experiments on binary-valued time series dataset. In this section, we have five baseline algorithms, including RuLSIF algorithm in Liu et al. [2013], traditional method change in mean (CIM) in Vostrikova [1981], changeforest algorithm (RF) in Londschien et al. [2023], ecp algorithm in James and Matteson [2013] and kscp3o algorithm in Zhang et al. [2017]. The RuLSIF algorithm is executed using Matlab code provided by the authors. CIM and RF are implemented through the changeforest Python package. Both ecp and kscp3o are sourced from the ecp R package. All experimental code is included in the supplementary files.\nThe details of synthetic binary time series generation can be found in the Appendix B.\nWe have two methods to evaluate the performance of the algorithms. The first method is to calculate the estimation error using \\(\\frac{|\\hat{\\tau} - \\tau_c|}{T}\\). The second method is to construct an ROC curve by setting an interval length, denoted as 2Q. With the change point estimator \\(\\hat{\\tau}\\) and interval length Q, we increment a counter by 1 if the true change point \\(\\tau_{c^j}\\) falls within the interval \\([\\hat{\\tau} - Q, \\hat{\\tau} + Q]\\). This count is then averaged over the total number of univariate time series in the 100 random trials. This is a common metric for measuring the performance of the change point detection algorithm, as described in Liu et al. [2013] and Harchaoui et al. [2008].\nPlease note that in the RuLSIF method, the kernel width \u03c3 and the regularization parameter in the kernel function are typically chosen using cross-validation, as outlined in Liu et al. [2013] and Harchaoui et al. [2008]. This approach is justified, as high-dimensional data can render these parameters more sensitive. However, for binary time series, cross-validation may not be necessary when applying our method, Causal-RuLSIF. One possible reason is that our algorithm avoids the complexities associated with high-dimensional data V by focusing on the analysis of one-dimensional time series segments \\(X^j(\\Lambda) \\subseteq V\\). As shown in Fig.2(a) and Fig.2(b), the red line represents our algorithm. With or without the cross-validation technique does not influence its performance. For RuLSIF, the cross-validation (cv) and no cross-validation (non-cv) does not overlap."}, {"title": "Case Study", "content": "Here, we construct an experiment with a real-world air pollution dataset. This dataset monitors the amount of PM10 (coarse particles with a diameter between 2.5 and 10 micrometers) in the air. The 3-variate time series data records the hourly concentration of PM10 across three counties in California\u2014Fresno, Mono and Monterey\u2014from Jan to June 2023. There are a total of 4305 samples. Let \\(X^{Fr}\\), \\(X^{Mono}\\) and \\(X^{Mont}\\) denote the indicators of PM10 exceeding 10 across the three counties.\nUsing Causal-RuLSIF, the change point estimators \\(\\hat{\\tau}^j\\) for each \\(X^j\\) where \\(j \\in [4]\\) are shown in the Table 1, along with the parent sets before and after the estimated change point.\nBased on the results, assuming there is one change point in PM10 concentration in Fresno, Mono, and Monterey during the first half of 2023, the causal mechanism of PM10 in Fresno is likely to shift on May 8, 2023. Additionally, while the PM10 levels in Fresno and Monterey are not influenced by other counties, a causal link from Monterey to Mono has emerged after February 1, 2023."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel change point detection algorithm, Causal-RuLSIF, to identify significant changes in causal mechanisms for discrete-valued time series data. By integrating a post-processing causal discovery stage with divergence estimation dynamically, our algorithm accurately detects when causal mechanism shifts occur without imposing constraints on the form of the shift. We provide a theoretical uncertainty analysis of the change point estimator. Our empirical evaluation confirms the consistency and robustness of the proposed algorithm. It is possible that the causal mechanism shift happens slowly over a short time period. It would be interesting to extend our approach to handle such smooth changes."}, {"title": "Appendix", "content": ""}, {"title": "Preliminaries", "content": ""}, {"title": "Time Series Segments", "content": "In Fig. 1a), for \\(X^1\\), SPA(\\(X_t^j\\)) = \\(\\{X_{t-2}^1, X_{t-1}^1, X_{t-2}^2, X_{t-1}^3\\}\\). Assuming V is binary time series with D = [0, 1], we have:\n\\(   \\Lambda := \\begin{array}{c|cccc}    & X_{t-2}^1 & X_{t-1}^1 & X_{t-2}^2 & X_{t-1}^3\\\\     \\hline    \\Sigma_{1} & 0 & 0 & 0 & 0\\\\    \\Sigma_{2} & 1 & 0 & 0 & 0\\\\    \\Sigma_{3} & 0 & 1 & 0 & 0\\\\    \\Sigma_{S^{|SPA(X^j)|}} & 1 & 1 & 1 & 1\\\\  \\end{array}\\)\t{(17)}\nWith s = |D| = 2 and |SPA(\\(X^j\\))| = 4, there are 16 = \\(2^4\\) configurations of SPA(\\(X^j\\)). Hence there are 16 Time Series Segments of \\(X^1\\). More specifically, \\(X^1(1) := \\{X_t^1 : t \\in [\\tau_{max} + 1,T], pa(X_t^j) = [0, 0, 0, 0]\\}\\), \\(X^1(2) := \\{X_t^1 : t \\in [\\tau_{max} + 1,T], pa(X_t^j) = [0, 1, 0, 0]\\}\\) and so on. For \\(X^3, SPA(\\(X^j\\)) = \\(\\{X_{t-3}^1, X_{t-1}^2, X_{t-1}^3\\}\\) and hence there are total \\(2^3\\) = 8 configurations. Fig.1b) shows the construction of \\(X^3(1)\\) and Fig.1c) illustrates 8 total Time Series Segments of \\(X^3\\)."}, {"title": "RuLSIF: Robust Distribution Comparison", "content": "Given two distributions p(\u00b7) and p'(\u00b7) defined over the same support, there exists many metrics measuring the distance between them. In Yamada et al. [2013], proposed a method named RuLSIF, with an a-relative divergence estimation \\(r_{\\alpha}(x)\\) and a corresponding metric, a-relative Pearson Divergence \\(PE_{\\alpha}\\) defined as following:\n\\(r_{\\alpha}(x) := \\frac{p(x)}{(1 - \\alpha)p(x) + \\alpha p'(x)} := \\frac{p(x)}{q_{\\alpha}(x)}\\)\t{(18)}\n\\(PE_{\\alpha} := E_{x~q_{\\alpha}}[ (r_{\\alpha}(x) - 1)^2 ]\\)\t{(19)}\nwhere \u03b1 is a parameter used to bound the value of \\(r_{\\alpha}(x)\\). With IID samples, we can obtain a direct density-ratio estimator \\(r_{\\alpha}(x)\\) using a kernel function, by minimizing a squared loss function. The estimator has been proven to have a non-parametric convergence speed. In Liu et al. [2013], the method was applied to tackle the change point detection problem in time series using sliding windows. By dividing the time series into retrospective segments, a sequence of RuLSIF measurements is obtained by assessing the distribution divergence between samples from two consecutive segments. Peaks in the divergence score \\(PE_{\\alpha}\\) can indicate change points within the joint distribution. The retrospective segments in Liu et al. [2013] are high-dimensional, even for univariate time series V.\nIn our proposed method, we also utilize sliding windows. However, since the focus shifts from the joint distribution to the causal mechanism, we do not construct high-dimensional retrospective segments from V. Instead, we create one-dimensional consecutive segments on the Time Series Segments for each \\(X^j \\subseteq V\\). This approach avoids the Curse of dimensionality and enhances the method's robustness to the hyperparameters in the kernel functions, as will be verified in the experiment section.\nSince we apply the RuLSIF method to the Time Series Segments, the samples are IID. The distributions that need to be compared are formalized as follows:\np(\\(X_{t_1}^j | spa(X_t^j) = \\Sigma_{\\Lambda}\\)) vs p(\\(X_{t_2}^j | spa(X_t^j) = \\Sigma_{\\Lambda}\\))\t{(20)}\nwhere \\(\\Lambda \\in [s^{|SPA(X^j)|}]\\), \\(t_1 < \\tau_c\\) and \\(t_2 \\geq \\tau_c\\).\nTo simplify matters, we use p(x) to denote p(\\(X_{t_1}^j | spa(X_t^j) = \\Sigma_{\\Lambda}\\)) and p'(x) represent p(\\(X_{t_2}^j | spa(X_t^j) = \\Sigma_{\\Lambda}\\)) in the rest of the paper.\nFig.4 provides a toy example demonstrating how the sliding window operates on Time Series Segments. From each segment, a series of PE scores is obtained using two sets of samples from the two halves of the sliding windows. Denote \\(W_i^1\\) as the ith window, with \\(W_i^1\\) representing the first half and \\(W_i^2\\) representing the second half."}, {"title": "Assumptions", "content": "A1. Sufficiency: There are no unobserved confounders.\nA2. Causal Markov Condition: Each variable X is independent of all its non-descendants, given its parents PA(X) in G.\nA3. Faithfulness Condition Pearl [1980]: Let P be a probability distribution generated by G. (G, P) satisfies the Faithfulness Condition if and only if every conditional independence relation true in P is entailed by the Causal Markov Condition applied to G.\nA4. No Contemporaneous Causal Effects: Edges between variables at the same time are not allowed.\nA5. Temporal Priority: Causal relations always point from the past to the future.\nA6. Boundary Separation Assumption: There must be a minimum buffer period at both the beginning and the end of the time series where change points cannot be detected. More specifically, the change point for each time series X \u2208 V cannot occur within the specified window size nw.\nA7. One change point per component time series: \u2200j, \\(c^j = 1\\), that is, each time series X \u2208 V has only one change point.\nA8. Minimum Pearson Divergence: For each \\(X^j \\subseteq V\\), there should exist a window \\(W^i\\) satisfying \\(PE_{\\alpha \\beta_i} > \\frac{||r_{\\alpha \\beta_i}||_2 + (1-\\alpha \\beta_i)^2||r_{\\alpha \\beta_i}||_1 + \\alpha^2 \\beta_i^2||r_{\\alpha \\beta_i}||_0}{4} + \\frac{||r_{\\alpha \\beta_i}||_2 + (1-\\alpha \\beta_i)^2||r_{\\alpha \\beta_i}||_1 + \\alpha^2 \\beta_i^2||r_{\\alpha \\beta_i}||_0}{16}\\) in at least one Time Series Segments \\(X^j(\\Lambda)\\).\nAssumptions A1-A5 are conventional and commonly employed in causal discovery methods for time series data. Our approach requires specific Assumptions A6-A8 to be in place. To clarify, assumption A6 is essential because our algorithm utilizes a series of sliding windows to obtain the divergence score by measuring the first half and the second half of the samples within each window. If the change point is too close to the beginning or end of the time series, the divergence score will not be significant enough to be detected. Assumption A7 is required since the sliding windows are not directly applied on the original time series. Instead, multiple sub-time series are created and hence it is hard to impose the constraint on the minimum distance among multiple change points, such as in Harchaoui et al. [2009], Allen et al. [2018] and Chen and Chu [2023]. Assumption A8 is necessary for the proof, as it guarantees a significant difference between two distinct causal mechanisms. This assumption is crucial because detecting the change point successfully becomes highly unlikely if the two causal mechanisms are extremely similar."}, {"title": "Theoretical Guarantees", "content": "Theorem A.1 ensures that in the initial step of our algorithm, a superset of the true union parent set \\(SPA^j_{\\tau}\\) can be obtained for all j \u2208 [n]. This guarantees the correct construction of Time Series Segments \\(X^j(\\Lambda)\\), with IID samples. Theorem A.2 establishes that if the window \\(W_i\\) contains samples from a single conditional distribution, the estimated relative Pearson Divergence \\(PE_{\\alpha \\beta_i}\\) is close to zero with high probability. Lemma A.3 states that, as \\(n_w\\) increases, the estimated relative Pearson Divergence will be close to the true relative Pearson Divergence up to some constant with high probability. Lemma A.4 proves that the Pearson Divergence will achieve the maximum if all the samples in the first half window are from p and the samples in the second half window are from p'. Theorem A.5 establishes a confidence interval for the change point estimator \\(\\hat{\\tau}^j\\).\nTheorem A.1. Let \\(SPA(X_t^j)\\) denote the union parent set of \\(X_t^j\\) and \\(\\widehat{SPA(X_t^j)}\\) denote the estimated union parent set obtained from PCMCI algorithm on time series \\(X_t^j\\) with a Causal-shift causal mechanism, and the change point \\(\\hat{\\tau}\\) satisfies \\(\\hat{\\tau} = \\tau\\). Under assumptions A1-A5,A7 and with an oracle (infinite sample size limit), we have that:\n\\(SPA(X_t^j) \\subseteq \\widehat{SPA(X_t^j)}\t{(23)}\nTheorem A.1 asserts that when samples are balanced, with half originating from one causal mechanism and the remaining half from another, the estimated union parent set encompasses the true union parent set.\nProof. The proof follows the same logic as the Lemma 3.2 and 3.3 in Gao et al. [2023]. In the semi-stationary SCM, the samples are from multiple causal mechanisms and due to periodicity, heterogeneous samples from different causal mechanisms are balanced. However, in the causal-shift SCM, as per Assumption A7, there are only two causal mechanisms, and the samples are inherently unbalanced without specific clarification. With additional assumption \\(\\frac{\\tau_1}{T} = \\frac{\\tau_2}{T}\\), we can draw the same conclusion as in Gao et al. [2023].\nNote that \\(PE_{\\alpha \\beta_i}^{j,\\Lambda}\\), series with parameter \u03b1 and window index i is a function of \\(n_w\\), \\(n_{st}\\), time series index j and Time Series Segments index \\(\\Lambda\\). For simplicity, we use \\(PE_{\\alpha \\beta_i}\\) instead of \\(PE_{\\alpha \\beta_i}^{j,\\Lambda}(n_w, n_{st})\\) in the next section. Furthermore, we need to emphasize that \\(T_{sub}\\) is not the length of V but the length of the specific Time Series Segments.\nTheorem A.2. Let \\(PE_{\\alpha \\beta_i}\\) be the estimated PE series for one Time Series Segments \\(X^j(\\Lambda) \\subseteq X^j \\subseteq V\\). Under assumption A6-A7, we have that \\(\\forall i \\in \\{i : i_{nst} + 2n_w - 1 < \\tau_{c^j}\\}\n\\(P(\\max_i |PE_{\\alpha \\beta_i}| < o_p(1)) > 1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}}\t{(24)}\nwhere \\(b_{st} = [\\frac{T_{sub}}{n_w}]\\) and \\(a_w = [\\frac{T_{sub}}{n_{st}}]\\).\nThe window index i satisfying \\(i_{nst} + 2n_w - 1 < \\tau_{c^j}\\) guarantees that all the samples in \\(W_i\\) are collected from the same distribution. Theorem A.2 states that the maximum estimated PE series obtained from such windows are bounded by any positive constant with probability \\(1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}}\\) if \\(n_w\\) are larger than some threshold. In other words, \\(\\forall k > 0, \\exists N\\) such that \\(\\forall n_w > N\\):\n\\(P(\\max PE_{\\alpha \\beta_i} < k) = 1 - \\frac{a_w - 2}{b_{st} log T_{sub}} \\frac{a_w}{T_{sub}}\t{(25)}\nProof. \\(\\forall i \\in \\{i : i_{nst} + 2n_w - 1 < \\tau_{c^j}\\}\\), the asymptotic expectation and variance of \\(PE_{\\alpha \\beta_i}\\) is given by:\n\\(E(PE_{\\alpha \\beta_i}) = PE_{\\alpha \\beta_i} + o_p(\\frac{1}{\\sqrt{n_w}})=\t{(26)}\n\\( o_p(\\frac{1}{\\sqrt{n_w}})\nV(PE_{\\alpha \\beta_i}) = o_p(\\frac{1}{n_w})\\)\t{(27)}\t{(28)}"}, {"title": "Generate Binary-valued Time Series", "content": "The generation process have three steps.\n1. Determine time series length T, number of time series n of the multivariate time series, data domain D, maximum time lag \\(\\tau_{max}\\). Randomly generate \\(\\tau^j_{c_j}\\), j\u2208 [n], satisfying Definition 3.2 and Assumption A6-A7. Additionally, in order to guarantee enough samples in each time series segment, we also control the size of the union parent set, that is, |SPAX|.\nWith n, \\(\\tau_{max}\\) and the number of change points for each component time series c, one binary edge array with dimension [n, \\(c^j + 1\\), n, \\(\\tau_{max} + 1\\)] is randomly generated, where the first n denotes the index of parent variable, c denotes the number of change point, the second n represents the index of target variable (child) and \\(\\tau_{max} + 1\\) denotes the time lag. With Assumption A7, \\(c^j = 1\\) for all j \u2208 [n]. 1 in this binary edge array means that there is an edge from the parent variable to the child variable with the corresponding time lag; 0 means that there is no cause-effect between two corresponding variables.\nFor soft mechanism change, the parent set remains the same before and after the change point, hence the binary edge array satisfies [:, 0, :, :] = [:, 1, :, :]. For hard mechanism change, the binary edge array must satisfy [:, 0, :, :] \u2260 [:, 1, :, :].\n2. With the randomly generated binary edge array controlled by |SPAX|, a full causal graph for this n-variate time series is obtained. Given this edge array and data domain D, parent configuration matrices \\(\\{\\Sigma^j\\}_{j\\in[n]}\\) are obtained. Corresponding to \\(\\{\\Sigma^j\\}_{j\\in[n]}\\), conditional probability tables (CPTs) are randomly generated. For instance, in Fig. 1, X has a \u03a3 with dimension 16 \u00d7 4, resulting in 16 conditional distributions of \\(X_t^j\\) given its specific parent configuration.\n3. Fill the starting points \\(\\{X_t<\\tau_{max}\\}_{j\\in[n]}\\) with randomly generated binary data. Starting from time point t > \\(\\tau_{max}\\), generate vector \\(X_t\\) over time according to the CPTs, until t achieves T.\nPlease note that since our algorithm can be extended to handle multiple change points, the generation process can also be easily extended to accommodate such settings."}, {"title": "Additional Experiments", "content": "From Fig.5(a) and Fig.5(b), it's apparent that a large \\(n_w\\) may affect the performance of Causal-RuLSIF. This issue still stems from sample efficiency. The total number of windows for a specific time series segment \\(X^j(\\Lambda)\\) is given by \\(T_{sub} - 2n_w+1\\), where \\(T_{sub} = |X^j(\\Lambda)|\\). Increasing \\(n_w\\) decreases the total number of windows, resulting in less information collected, as smaller \\(n_w\\) is sufficient to estimate PE scores with kernel functions.\nAs demonstrated in Fig.6(a) and Fig.6(b), the performance of Causal-RuLSIF remains consistent regardless of the distance between the change points \\(\\tau^1\\) of \\(X^1\\) and \\(\\tau^2\\) of \\(X^2\\)."}]}