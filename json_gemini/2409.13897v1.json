{"title": "LLM for Everyone: Representing the Underrepresented in Large Language Models", "authors": ["Samuel Cahyawijaya"], "abstract": "Natural language processing (NLP) has witnessed a profound impact of large language models (LLMs) that excel in a multitude of tasks. However, the limitation of LLMs in multilingual settings, particularly in underrepresented languages, remains a significant hurdle. This thesis aims to bridge the gap in NLP research and development by focusing on underrepresented languages. A comprehensive evaluation of LLMs is conducted to assess their capabilities in these languages, revealing the challenges of multilingual and multicultural generalization. Addressing the multilingual generalization gap, this thesis proposes data-and-compute-efficient methods to mitigate the disparity in LLM ability in underrepresented languages, allowing better generalization on underrepresented languages without the loss of task generalization ability. The proposed solutions cover cross-lingual continual instruction tuning, retrieval-based cross-lingual in-context learning, and in-context query alignment. Furthermore, a novel method to measure cultural values alignment between LLMs operating in different languages is proposed, ensuring cultural sensitivity and inclusivity. These contributions aim to enhance the multilingual and multicultural alignment of LLMs in underrepresented languages, ultimately advancing the NLP field toward greater equality and inclusiveness.", "sections": [{"title": "Introduction", "content": "Natural Language Processing (NLP) is a burgeoning field of research and application that investigates how computers can be utilized to comprehend and manipulate natural language for practical purposes. The primary objective of NLP is to acquire a comprehensive understanding of how humans utilize language, thereby enabling the development of appropriate tools and techniques that facilitate the comprehension and manipulation of natural languages by computer systems to execute desired tasks. In its nascent stages, NLP research was primarily focused on the global lingua franca, English, despite the existence of over 7,000 languages worldwide. Other languages were often relegated to mere translation to English, while many others were neglected entirely. However, as NLP has advanced, it has become increasingly evident that restricting research to a single language is fraught with limitations, including translationese sentences, semantic ambiguity, transliteration issues, Anglocentricity, and monoculturalism. Over the past decade, deep learning has brought unprecedented progress to the field of natural language processing (NLP), resulting in the development of pre-trained language models (PLMs) that exhibit remarkable performance in various NLP tasks. However, despite their impressive capabilities, existing PLMs still face a significant challenge in terms of multilingualism, as they primarily focus on learning high-resource languages such as English. Consequently, the performance of PLMs in underrepresented languages remains fairly limited, leading to a significant disparity and inequality in access to state-of-the-art NLP technology. This issue highlights the urgent need to address the disparity and promote equality in NLP research and development.\nIn recent years, significant progress in Natural Language Processing (NLP) has facilitated the development of multilingual large language models (LLMs), an extraordinary technology that surpasses human capabilities, achieving professional-level proficiency in diverse domains. The remarkable capabilities of multilingual LLMs have created vast opportunities for NLP, leading to the emergence of open-source and commercial multilingual LLM solutions which hold tremendous potential to generate a significant impact on a global scale. However, despite their remarkable capabilities, a rigorous understanding of multilingual LLMs ability in languages other than English is still lacking, which raises questions about their generalization ability towards underrepresented languages, a challenge that has plagued NLP technology for decades.\nBuilding upon the limited understanding of the multilingual generalization of multilingual LLMs, this thesis presents a comprehensive evaluation that establishes a foundation for understanding the alignment capability of multilingual LLMs in underrepresented languages, specifically on Austronesian languages that are spoken in Indonesia. Alongside other large-scale multilingual  and regional evaluations on underrepresented languages, our thorough evaluations of LLMs on Austronesian languages, covering 18 underrepresented languages in language understanding, language generation, and cultural understanding capabilities, reveal the limitations of LLMs in generalizing toward multilingualism and multiculturalism. This underscores the urgent need for developing mitigation methods to address the multilingual and multicultural generalization gap, which is critical for advancing the field of NLP.\nTo overcome this problem, we propose two approaches for improving the language and cultural understanding of multilingual LLMs. The first method employs data-efficient instruction-tuning through cross-lingual objectives dubbed as InstructAlign. The second method is a training-free approach through in-context learning which is inspired by the traditional lexicon-based [] and example-based [] machine translation approaches dubbed as in-context query alignment. Our approaches signify the importance of acquiring capabilities novel underrepresented languages and cultures while at the same time preventing catastrophic forgetting and the loss of generalization ability . To this end, in this thesis, we formulate the following research questions and how we will approach each of the research questions:"}, {"title": "Thesis Outline", "content": "The contents of this thesis are focused on the language and cultural inclusivity and diversity of multilingual LLMs. This thesis covers comprehensive evaluations of multilingual LLMs on languages, underrepresented language adaptation methods for multilingual LLMs, and multicultural value alignment in multilingual LLMs. The rest of the thesis is divided into four chapters and organized as follows:\n\u2022 Chapter 2 (Preliminaries and Related Work) introduces the background and important preliminaries covering: 1) languages and cultures around the world, 2) transformer model and self-supervised language pre-training, 3) instruction-tuning and reinforcement learning with human feedback, 4) multilingual learning and cross-lingual alignment, and 5) zero-shot prompting and few-shot in-context learning.\n\u2022 Chapter 3 (Large Language Models Evaluation on Underrepresented Languages) presents extensive evaluations on multilingual LLMs in underrepresented languages on both language understanding and generation tasks. Additionally, we perform in-depth evaluations of the cultural understanding of multilingual LLMs to better understand the current state of multilingual LLMs on underrepresented language,"}, {"title": "Cross-lingual Alignment", "content": "With the introduction of word embedding methods such as word2vec , fast-text, and GloVe , various language-specific word embeddings trained using large amount of monolingual data have been released. A number of works  find that there are geometric similarities across different language embedding and a learnable linear map is sufficient to align the two embedding spaces. This process can be formulated as an minimization problem with the following objective:\n$min_W  \\sum_{i=1}^n ||Wxi - Yill$   (2.1)\nwith $xi \u2208 Rd$ and $yi \u2208 Rd$ denote the i-th word vector the word embedding model $X \u2208 Rm\u00d7d$ and $Y \u2208 Rm\u00d7d$, respectively, and $W \u2208 Rd\u00d7d$ denotes the linear transformation parameters. When the two embedding models are isometric (distance-preserving), this alignment becomes a Procrustes problem, that can be solve through a closed-form solution  defined as $W = V.U^T$ where $ULV = SVD(Y^TX)$. These method enable bilingual lexicon induction using only monolingual data from two languages.\nThis leads to the series of works in cross-lingual alignment in word embedding  which introduces similarity metrics for word embedding such as cross-domain similarity local scaling (CSLS) and relaxed cross-domain similarity local scaling (RCSLS). Despite its promise, these methods rely on the assumption of isomorphism between two embedding spaces, which is often violated especially when the two languages are distant. The depiction of cross-lingual alignment in word embedding is shown in Figure 2.2"}, {"title": "Cross-lingual Alignment in Contextualized Embedding", "content": "With the introduction of contextualized embedding models such as transformer-based pre-trained language models, there are a number of efforts exploring the possibility of contextualized embedding alignment especially in the multilingual pre-trained language models such as mBERT . These methods mostly incorporate another alignment term in the loss function that are heavily rely on the existence of parallel corpora Other line of works also analyze the cross-lingual capability of these models, and showcase that these models, despite mostly trained only on monolingual data from various languages, it has an inherent aligned representation across different languages and the alignment quality is significantly correlated with their cross-lingual transfer capability."}, {"title": "Transformer and Pre-trained Language Model", "content": "The Transformer  is a model architecture proposed for sequence modeling. Unlike, RNN-based models such as GRU and LSTM which retain only one single hidden state and incorporate a sequential operation to deal with long-term dependencies of a sequence, Transformer-based models process a sequence with a fully parallelizable operation based on a multi-head attention mechanism to model the long-term dependencies between input and output. This allows Transformer-based models to significantly speed up both training and inference processes showcasing their strong ability to model sequential data such as natural languages.\nThe illustration of the Transformer architecture is shown in Figure 2.3 The Transformer encoder and decoder are composed of a stack of Transformer layers. Each layer of the Transformer encoder and decoder is made up of two components: the self-attention layer and the feed-forward neural network, the latter of which consists of two linear layers with residual connections and layer normalization In the Transformer encoder-decoder architecture, an additional cross-attention layer is added between the self-attention and feed-forward layers on each of the decoder layer.\nMulti-Head Attention The depiction the scaled dot-product attention mechanism is shown in Figure 2.4. Unlike RNNs that summarize the whole natural language sequence into one single hidden state, the scaled dot-product attention allows the models to maintain the dimensionality of sequence length while extracting features for each token in the sequence. In a sequence of length L, we can obtain the hidden state Z \u2208 IRL\u00d7dm, where dm is the dimensionality of the hidden states. The dot-product attention mechanism computes as follows:\n$Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{Va}})V$,   (2.2)\nwhere Q, K, and V are projected from the input hidden states of the Transformer layer. In the scaled dot-product attention, Q represents the query vector, K represents the key vector, and V represents the value vector. In the self-attention layer, the entire sequence attends to itself, meaning all three vectors are projected from the input vector from either the encoder or the decoder side. However, in the cross-attention layer, the query vector Q is projected from the hidden states of the decoder, while key vector K and value vector V are from the final hidden states of the encoder.\nWhen the same dot-product attention function running for h times in parallel, this is known as multi-head attention with h heads. Multi-head attention improves the robustness of the model during training resulting in an improved performance. This is done by allowing the model to pay attention to different input sequence features simultaneously. The projection matrices are combined for different heads in practice. The projected hidden states are then divided into sub-matrices and used in multi-head attention, with each hidden state dimension denoted as dm."}, {"title": "Pre-trained Language Models", "content": "Pre-trained language models (PLMs), such as BERT  and GPT-2, have achieved great success across nearly all NLP tasks. This thesis focuses on large language models (LLMs) which employs PLMs with decoder-only architecture for solving generative tasks in natural languages. Such PLMs employ a Transformer-based architecture that can is easily scalable and can be pre-trained on enormous natural language corpora with self-supervised pre-training objectives to learn the representation of the natural language residing in the corpora. There are three widely-adopted architectures of PLMs, i.e., encoder-only, decoder-only, and encoder-decoder. Since encoder-only PLMs, such as BERT, ROBERTa, ELECTRA, and DeBERTa, can only be applied to classification tasks, only decoder-only and encoder-decoder PLMs will be introduced further. We showcase the decoder-only and encoder-decoder PLMs in Figure 2.5\nDecoder-Only PLMs Decoder-only PLMs learn to take inputs and generate outputs with a set of parameters. During pre-training, these models learn to predict successive tokens to model natural language autoregressively. In other words, given previous tokens, PLMs learn to predict the next token. Given a sequence of text $X = {x_1,...,x_n}$, decoder-only PLMs are pre-trained with an autoregressive causal language modeling objective:\n$L(0) = \\frac{1}{N} \\sum_{t=1}^N log p_\u03b8 (X_t|X<t)$ (2.5)\nwhere 0 denotes the parameters of the models. Decoder-only PLMs deal with inputs and outputs for practical use in downstream tasks by concatenating them as a single sequence. We denote input and output sequences as $X = {x_1, ..., x_m}$ and $Y = {Y_1, ..., Y_N}$, where M and N are lengths of the input and output sequences. As shown in Figure 2.5 a special token s separates the input and output sequences \u2013 in practice, most PLMs use either the BOS or the EOS tokens \u2013, and the model recursively generates the output sequence token-by-token given the input sequence and the special token s.\n$P(Y|X) =  \\Pi_{t=1}^N p_\u03b8 (Y_t|X_{1, ..., X_M, S, Y_1, ..., Y_{t-1}})$  (2.6)\nThe generation process stops whenever an EOS token is produced. Representative decoder-only PLMs include GPT series (GPT [303], GPT-2, and GPT-3 [57]), PanGu-\u03c3, BLOOM , LLaMA series (LLaMA, LLaMA-2, and LLaMA-3), etc. Following the scaling law of PLMs, these models have shown an even better zero-shot and few-shot in-context learning capabilities as the scale increases.\nEncoder-Decoder PLMs Encoder-decoder PLMs are typical Seq2Seq models that encode input sequences with the encoder and predict output sequences with the decoder. The pre-training methods of encoder-decoder PLMs vary from each other. One representative of encoder-decoder PLMs is T5 [306, 412]. T5 is pre-trained with self-supervised"}, {"title": "From Pre-trained Language Models to Large Language Models", "content": "PLMs have shown impressive performance on various tasks. Various works have displayed the positive correlation of scaling the size of PLMs to the language understanding and generation abilities of the PLMs. In addition, the humongous scale of these LLMs have demonstrated emerging capability on various downstream tasks. This quality scalability leads to the rapid development of larger PLMs starting from tenth-to-hundred million parameters up to hundred billion or even trillion parameters that is known as large language model (LLM). With the extreme scale of parameters, LLMs are able to perform inference on an unseen data through zero-shot and few-shot prompting. This ability is further enhanced with instruction-tuning that enable LLMs to better follow instructions even in the zero-shot setting which will be further elaborated in \u00a72.3.2. The ability of LLMs are further improved by aligning their responses to human feedback through reinforcement learning with human feedback (RLHF) [80, 284]. Aside from improving response quality, RLHF helps to align the value adopted by LLMs that will be further described in \u00a72.3.3"}, {"title": "Instruction Following in Large Language Models", "content": "Instruction following is an emergent ability that LLMs have which is useful for solving various tasks in zero-shot and few-shot manner through prompting. This ability is observed from LLM with >100 billion parameters in size . Instruction-tuning enables extending this capability to smaller LLMs through multitask fine-tuning using natural instructions. These smaller instruction-tuned LLMs have shown remarkable zero-shot generalization ability to unseen tasks starting from a few billion parameters in size, while distillation can even stretch the instruction following ability to LMs with scale of hundred millions to a billion parameters [407].\nMore formally, given fe as a model parameterized with 0, while X \u2208 R\u014b and Y \u2208 Rm respectively denote the input and the target text sequences, instruction-tuning reformulate the learning process of the original fine-tuning process from fe(X) \u2192 Y into fo (I(X)) \u2192 Y where I denotes a function for converting an input sequence X into a natural language instruction. For example, given an English-to-Indonesian machine translation task with the input X as \u201cHello world, good morning!\", one of the possible natural instruction format I(X) is \u201cTranslate the sentence \"Hello world, good morning!\" into Indonesian:\u201d. In order to generalize better over different instruction formats, in practice, multiple instruction formats will be used to represent a single task, and zero-shot task generalization emerge when scaling up this instruction-tuning process into a large number of tasks. The illustration of the instruction-tuning process is shown in Figure 2.6.\nInstruction-tuning offers improved generalization capabilities of LLMs, achieving remarkable zero-shot generalization quality on both unseen data and unseen tasks [392, 284]. While instruction-following abilities are observed starting from billion parameter-range LLMs [379, 81], This improved generalization is showcased to outperform the standard\""}, {"title": "Value Alignment in Large Language Models", "content": "Recent LLMs such as LLaMA , ChatGPT and GPT-4  are pre-trained with large-scale general natural language corpora that are converted to the dialogue style and then fine-tuned through reinforcement learning with human feedback (RLHF). These LLMs are aligned with humans to enhance their service and mitigate risks . The major goal of LLMs value alignment can be divided into three fold , i.e., 1) Teach LLMs to follow human instructions ; 2) Align LLMs with implicit human preferences [80]; and 3) Align LLMs to a set of pre-defined principles reflecting human values . Figure 2.7 showcases the overview of the LLMs value alignment that is commonly done in two phases, i.e., supervised fine-tuning (SFT) and reinforcement learning with human or AI feedback (RLHF/RLAIF). In SFT, the model is fine-tuned by consuming a set of curated conversation data complying with human desired attributes [210, 72, 273, 349]. The selection of high-quality, diverse data is substantial in SFT . The model can be fine-tuned using a standard language modeling loss or other training paradigms such as contrastive learning  and distillation . In the second step, RLHF is an essential alignment technique applied by the majority of recent LLMs .RLHF is achieved through reinforcement learning methods such as PPO where models receive feedback from a value-aligned reward model adjusting their policy. Recently, DPO is introduced to alleviate the need for a reward model. Unlike RLHF, RLAIF generates feedback based on the model itself, reducing reliance on manual annotation . In RLHF, preferences are implicit as they are elicited from ranking data pairs, making it difficult for LLMs to generalize to explicit principles. While RLHF implicitly elicit preferences from ranking data pairs, other approaches like Constitutional AI  establish explicit principles or 'constitutions' for AI, enhancing model alignment to explicitly-defined human values through self-critique and modification of responses."}, {"title": "Multilingual Language Model", "content": "Multilingual Pre-trained Language Model The development of pre-trained LMs has given rise to a new era of multilingual technology known as multilingual LMs. These models are trained on large-scale monolingual corpora in various languages, allowing them to learn language representations across different linguistic contexts. Multilingual LMs are capable of performing cross-lingual inference without the need for any explicit alignment, as discussed in \u00a72.1. This capability has significant implications for both the understanding and generation abilities of LMs across multiple languages.\nmBERT , a multilingual variant of BERT, can handle multiple languages simultaneously, demonstrating robust cross-lingual transfer capabilities despite having no explicit cross-lingual alignment. XLM-R  extend the monolingual data used during pre-training while keep using masked language modeling (MLM) objective similar to BERT while incorporating a larger pre-training corpus and more languages, achieving better performance on cross-lingual benchmarks including low-resource langyages. XLM-R highlights while increasing the number of languages generally improves performance on low-resource languages, it can eventually lead to the degradation of overall performance, a phenomenon known as the curse of multilinguality. To address this issue, Goyal et. al. (2023) [141] demonstrates that increasing the model capacity can mitigate this degradation, maintaining strong performance on both cross-lingual and high-resource language tasks. Similarly, Glot500  extends the language coverage of XLM-R from 100 to 500 languages while expanding the vocabulary size, thereby enhancing the inclusivity and applicability of multilingual LMs in diverse linguistic settings. Other line of work introduce language-adapter and its variants for extending the language coverage in PLMs.\nIn other line of work, various objectives for cross-lingual alignment in LMs have also been introduced. XLM  achieves explicit cross-lingual alignment during pretrain-ing through translation language model (TLM) objective which leverage parallel data to enhance cross-lingual understanding. While other models such as LASER  and LaBSE  focus on sentence-level cross-lingual alignment that results in multilingual sentence embeddings, which enable efficient cross-lingual tasks, including sentence retrieval and clustering. Another line of work  showcase a regularization approach for cross-lingual alignment through regularization between parallel samples.\nMultilingual Generative Pre-trained Language Model In addition to advancements in encoder-only PLMs, significant progress has been made in multilingual generative PLMs. XNLG  is a pioneering model that extends BERT and GPT architectures to support cross-lingual language generation. By leveraging cross-lingual pre-training, XNLG is capable of generating coherent text across multiple languages, making it suitable for tasks such as machine translation and cross-lingual text generation. mBART is designed as a sequence-to-sequence transformer model pre-trained for multilingual text generation. It excels in machine translation and text summarization by leveraging a denoising autoencoder pre-training objective. This allows mBART to generate high-quality translations and summaries across different languages, demonstrating its versatility and effectiveness in multilingual NLG tasks."}, {"title": "Multilingual Large Language Model", "content": "In recent years, various multilingual LLMs have been introduced, expanding the capa-bilities of multilingual technology with zero-shot and few-shot generalization capability through prompting and in-context learning. Several works have further showcased the effectiveness of cross-lingual in-context learning  and, even further, in-context alignment Several models have been developed to perform few-shot learning in a multilingual context. XGLM leverages extensive multilingual pre-training to excel in few-shot learning scenarios, demonstrating strong cross-lingual transfer capabilities. BLOOM is a large-scale multilingual language model that supports few-shot learning across 47 languages, promoting inclusivity in language technology by being able to handle underrepresented languages effectively. Similarly, Falcon and PolyLM  are LLMs that excels in few-shot learning by balancing the representations across various languages, ensuring high-quality generation and understanding across different linguistic contexts.\nTo improve the adaptability of LLMs to multilingual contexts, a few models have been tailored specifically for this purpose. MaLA-500 focuses on adapting existing LLM to a diverse set of languages, enhancing their cross-lingual understanding and generation capabilities. This model addresses the challenge of linguistic diversity by fine-tuning pre-trained models on a variety of languages, thus improving their applicability in multilingual settings. Furthermore, Bactrian-X  provides a model that adapts to low-resource languages, ensuring that even languages with limited training data are well-represented in multilingual applications.\nAdditionally, other prior works have focused on improving the LLMs' instruction-following capability in multiple languages, leveraging their remarkable capabilities in zero-shot learning scenarios. BLOOMZ and mT0  respectively extends the BLOOM and mT5 models with instruction tuning, enabling it to follow natural language instructions in multiple languages without requiring task-specific fine-tuning. This model demonstrates the potential of instruction tuning to enhance the usability of multilingual LLMs across diverse tasks. Aya-101 introduces a model specifically designed for zero-shot instruc-"}, {"title": "Underrepresented Language Evaluation in Large Language Model", "content": "Current LLMs perform on par or even better than state-of-the-art fine-tuned models on English , nonetheless their capability on underrepresented languages are under explored, most works in multilingual evaluation only showcase the performance in com-parison of other languages relative to English or to other LLMs . Recently, A number of recent have evaluated LLMs compared to the corresponding fine-tuned state-of-the-art models on the corresponding languages, nonetheless the language and task coverage are still limited. Adelani et. al. (2024a) evaluate LLMs on 200 languages covered in NLLB [378], Cahyawijaya et. al. and Lovenia et. al. evaluate LLMs on Austronesian languages spoken in South East Asia, Adelani et. al. evaluate LLMs on various African languages, Adelani et. al. evaluate LLMs on underrepresented Brazilian languages, Zhang et. al. evaluate LLMs on code-mixing across various languages including Spanish-English, Malayalam-English, Tamil-English, Hinglish, and Standard-Egyptian Arabic.\nCultural Evaluation of Underrepresented Languages The prevalence of Anglocentric training data in language models has raised concerns about potential cultural bias when generating texts in underrepresented languages . This bias can have far-reaching consequences, creating language and cultural barriers for individuals who do not speak the dominant language. Recent studies have shed light on this issue, revealing that the representations learned by large language models (LLMs) often fail to reflect local cultural values in other languages and contexts [107, 23, 214, 238]. The disparity in language and cultural representation poses significant challenges for individuals from minority groups who do not speak the dominant language. This discrepancy creates linguistic and cultural barriers to accessing technology and risks further marginalization of these communities.\nFor this reason, recent studies have offered valuable insights and dug deeper to inspect this problem. Various multilingual evaluations of linguistic nuances and/or cultural knowl-edge, such as MABL, M3Exam , and SeaEval , have evaluated multilingual models across a diverse set of tasks and languages and highlighted the performance gap between high-resource and low-resource languages. These evaluations also underscore the importance of considering the unique characteristics of each language, the potential pitfalls of relying solely on English-centric evaluation, as well as capturing the relevant cultural knowledge and cultural awareness. Further, the work by Naous et al. introduces CAM\u0112L, a framework for measuring cross-cultural biases in LLMs. They find that models exhibit a bias toward Western entities even when operating in Arabic, leading to concerning cases of stereotyping and cultural unfairness. This highlights the models' failure in cultural adaptation and the need for more inclusive representations. Similarly,"}, {"title": "Large Language Models Evaluation in Underrepresented Languages", "content": "The rapid release of open-source and commercial large language models (LLMs) including ChatGPT LLaMA-2 LLaMA-3 Command-R Aya etc has led to an increased need for a comprehensive evaluation framework to assess the quality, safety and usability of LLMs. While several evaluation frameworks currently exist , they are limited in their scope, especially their evaluation on underrepresented languages. As the number of LLMs and their coverage increases, so too does the necessity for robust multilingual evaluation frameworks, especially on underrepresented languages, to ensure the responsible development and deployment of these LLMs.\nBuilding upon the limited understanding of the underrepresented language gener-alization of LLMs, this chapter presents a comprehensive evaluation that establishes a foundation for understanding the alignment capability of LLMs in a varying degree of language underrepresentedness. The evaluation is focused on Austronesian languages that are spoken in Indonesia because of the humongous linguistic diversity in Indonesia that covers more than 700 languages [17]. Alongside other large-scale regional evaluations on underrepresented languages , our thorough evaluations of LLMs on Austronesian languages reveal the limitations of LLMs in generalizing toward multilingualism and multiculturalism in these underrepresented lan-guages. This underscores the urgent need for developing mitigation methods to address the multilingual and multicultural generalization gap in LLMs."}, {"title": "Introduction", "content": "LLMs have consistently pushed new frontiers in natural language processing (NLP) in terms of performance across a variety of benchmarks, such as MMLU, BIG-Bench  and HELM , achieving state-of-the-art results in both natural language understanding (NLU) and generation (NLG) tasks . Various applications of LLMs have also been adopted in the industry bringing a significant impact on society via technologies such as AI assistants, machine translations, search engines, etc. Despite its success, LLMs are only widely available for high-resource languages such as English and Mandarin Chinese , while their applicability to many languages \u2013 especially for underrepresented languages \u2013 remains obscure due to the unavailability of evaluation suites and benchmarks.\nIn this work, we focus on developing evaluation suites and benchmarks for languages in Indonesia, the second most linguistically-diverse country with 700+ languages equal to 10% of the languages in the world [17, 108]. We compare various LLMs with pre-trained language models (PLMs) and other baselines showcasing their limited proficiency in these languages. We compare the language capabilities of these LLMs in both NLU and NLG tasks for Indonesian (ind), the national language of Indonesia, and 17 other local languages spoken in Indonesia, i.e., Ambon (abs), Acehnese (ace), Mandailing (btm), Betawi (bew), Bima (bhp), Balinese (ban), Banjarese (bjn), Buginese (bug), Javanese (jav), Madurese (mad), Makassarese (mak), Minangkabau (min), Musi (mui), Ngaju (nij), Rejang (rej), Sundanese (sun), and Toba Batak (bbc). Our results suggest that, in Indonesian (ind), existing LLMs perform lower to almost on par with smaller fine-tuned PLMs across different tasks, and in some cases, slightly outperforming them. While in more underrepresented languages such as local languages spoken in Indonesia, LLMs still outcompeted by pre-trained language models and even to classical machine learning (ML) baselines such as Logistic Regression , Naive Bayes Support Vector Machine (SVM) for classification, while Bilingual Lexicon and Phrase-Based Statistical Machine Translation (PBSMT)  are incorporated for machine translation."}, {"title": "Indonesian: One Country, 700+ Languages", "content": "Indonesia is one of the richest countries globally in terms of linguistic diversity. More than 400 of its languages belong to the Austronesian language family, while the others are Papuan languages spoken in the eastern part of the country. As shown in Figure 3.1, the Austronesian languages in Indonesia belong to three main groups: Western-Malayo-Polynesian (WMP), Central-Malayo-Polynesian (CMP), and South-Halmahera-West-New-Guinea (SHWNG). WMP languages are Malay, Indonesian, Javanese, Sundanese, Balinese, and Minangkabau, among others. Languages belonging to CMP are languages of the Lesser Sunda Islands from East Sumbawa (with Bimanese) onwards to the east, and languages of the central and southern Moluccas (including the Aru Islands and the Sula Archipelago). The SHWNG group consists of languages of Halmahera and Cenderawasih Bay, and further-flung regions such as the Mamberamo River and the Raja Ampat Islands."}, {"title": "Landscape of Languages in Indonesia", "content": "At the time of the arrival of the first Europeans, Malay had become the major language (lingua franca) of interethnic communication in Southeast Asia and beyond . It functioned as the language of trade and the language of Islam because Muslim merchants from India and the Middle East were the first to introduce the religion into the harbor towns of Indonesia. After the arrival of Europeans, Malay was used by the Portuguese and Dutch to spread Catholicism and Protestantism. When the Dutch extended their rule over areas outside Java in the nineteenth century, the importance of Malay increased, and thus, the first standardization of the spelling and grammar occurred in 1901, based on Classical Malay [1, 362]. In 1928, the Second National Youth Congress participants proclaimed Malay (henceforth called Indonesian) as the unifying language of Indonesia. During World War II, the Japanese occupying forces forbade all use of Dutch in favor of Indonesian, which from then onward effectively became the new national language. From independence until the present, Indonesian has functioned as the primary language in education, mass media, and government. Many local language speakers are increasingly using Indonesian with their children because they believe it will aid them to attain a better education and career [207]."}, {"title": "Language Diversity in Indonesia", "content": "The diversity of languages spoken in Indonesia is not only reflected in the large number of local languages but also the large number of dialects of these languages. Speakers of local languages also often mix languages in conversation, which makes colloquial Indonesian more diverse. In addition, some local languages are more commonly used in conversational contexts, so they do not have consistent writing forms in written media.\nDialect Variation Indonesian local languages often have multiple dialects, depending on the geographical location. Local languages of Indonesian spoken in different locations might be different (have some lexical variation) to one another, despite still being categorized as the same language. Moreover, Indonesian and its local languages have multiple styles, even within the same dialect. One factor that affects style is the level of politeness and formality \u2013 similar to Japanese and other Asian languages . More polite language is used when speaking to a person with a higher social position, especially to elders, seniors, and sometimes strangers. Different politeness levels manifest in the use of different honorifics and even different lexical terms. The examples of different dialect variations in languages spoken in Indonesia are shown in Table 3.1 and Table 3.2.\nCode-Mixing Code-mixing is an occurrence where a person speaks alternately in two or more languages in a conversation . This phenomenon is common in Indonesian conversations . In a conversational context, people sometimes mix their local languages with standard Indonesian, resulting in colloquial Indonesian . This colloquial-style Indonesian is used daily in speech and conversation and is common on social media Some frequently used code-mixed words (especially on social media) are even intelligible to people that do not speak the original local languages. Interestingly, code-mixing can also occur in border areas where people are exposed to multiple languages, therefore mixing them together. Furthermore, code-mixing in Indonesia not only occurs at the word level but also at the morpheme level . The examples of code-mixing between languages spoken in Indonesia are shown in Table 3.3.\nOrthography Variation Many local indigenous languages spoken in Indonesia are mainly used in spoken settings and have no established standard orthography system. Some local languages do originally have their own archaic writing systems that derive from the Jawi alphabet or Kawi script, and even though standard transliteration into the Roman alphabet exists for some (e.g., Javanese and Sundanese), they are not widely known and practiced. Hence, some words have multiple romanized orthographies that are mutually intelligible, as they are pronounced the same. Some examples can be seen in Table 3.4. Such a variety of written forms is common in local languages in Indonesia. This variation leads to a significantly larger vocabulary size, especially for NLP systems that use word-based representations, and presents a challenge to constrain the representations for"}, {"title": "LLMs Capability in Languages Spoken in Indonesia", "content": "Despite of the large language coverage, most of the languages in Indonesia are underrepre-sented. In this thesis, we evaluate 18 languages, one of which is the national language of Indonesian, i.e., Indonesian (ind). The others 17 are local indigenous languages spoken in Indonesia each with \u2265500,000 speakers. Out of 17 languages, 14 come from Western Malayo-Polynesian (Balinese, Ngaju, Javanese, Madurese, Acehnese, Banjarese, Musi, Mi-nangkabau, Sundanese, Rejang Mandailing, Toba Batak, Buginese, and Makassarese), 2 are Malay-based creole (Betawi and Ambonese Malay), and the other is from Central Malayo-Polynesian (Bima). More detailed description of each language is shown in Table 3.5. It is important to note that, the number of speakers in languages under evaluation are generally similar or even higher than many higher-resource languages, e.g., Indonesian is spoken by 300M people similar to French and Portuguese, German is spoken by 80M speakers while there are 100M Javanese speakers, Swedish has 11M speakers while Sundanese is spoken by 32M speakers, Finnish is spoken by 5M speakers while Minangkabau is spoken by 8M, etc. However, these languages are underrepresented in the NLP community which"}]}