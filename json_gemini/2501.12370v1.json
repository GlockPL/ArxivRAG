{"title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models", "authors": ["Samira Abnar", "Harshay Shah", "Dan Busbridge", "Alaaeldin Mohamed Elnouby Ali", "Josh Susskind", "Vimal Thilak"], "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Expert models (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the ratio of non-active to total parameters, affects model performance in terms of both pretraining and downstream performance. We find that under different constraints (e.g. parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.", "sections": [{"title": "1 Introduction", "content": "Empirical scaling laws for language model pretraining (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI, 2023; OpenAI et al., 2024; Gemini Team et al., 2024; Henighan et al., 2020; Clark et al., 2022; Yun et al., 2024; Ludziejewski et al., 2024) have demonstrated that proportionally increasing model capacity, along with data and total compute budget, consistently decreases pretraining loss (i.e., perplexity), improves downstream task performance (Devlin et al., 2019; Brown et al., 2020; BIG-bench authors, 2023) and unlocks emergent capabilities (Wei et al., 2022a).\nA recurring notion in these studies is that model capacity is well quantified by the total number of model parameters. However, the number of parameters is not the only means to increase model capacity- compute per example (i.e., a fixed-sized input), measured in FLoating OPerations (FLOPs), also plays a significant role (Clark et al., 2022). In fact, several mechanisms (Shazeer et al., 2017; Dehghani et al., 2019; Wei et al., 2022b; Goyal et al., 2024; Csord'as et al., 2024) allow for independent variation of the number of parameters or FLOPs per example within a model. For instance, Sparse Mixture-of-Experts (MoE) models (Shazeer et al., 2017) introduce \u201cFLOP-free parameters\u201d by leveraging sparsity, where only a subset of expert modules is activated for each input.\nWhen studying scaling laws for specific classes of models, e.g., vanilla transformers, the total number of parameters can serve as a reasonable relative estimator of FLOPs per example. Therefore, we therefore ask\n\u201cCan we draw scaling laws for the optimal trade-off between\nparameter count and FLOPs per example?\"\nTo address this question, we study sparse Mixture-of-Expert Transformers (MoEs) (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022; Muennighoff et al., 2024) in the context of language modeling. Existing scaling law studies for MoEs, investigate the role of variables like number and granularity (Ludziejewski et al., 2024)of experts, underlying dense model size and inference compute in predicting the performance of the models under different conditions such as training or inference compute optimality (Du et al., 2021; Clark et al., 2022; Yun et al., 2024; Ludziejewski et al., 2024). In this paper, we focus on the interaction between FLOPs per example and total parameter count, and their impact on model performance in MoEs, through a large-scale empirical study.\nWe define sparsity as the ratio of inactive experts to the total number of experts, which controls the ratio of the total number of parameters to FLOPs per example in MoEs. We evaluate loss and downstream metrics for different sparsities, model sizes, and compute budgets. Through qualitative and quantitative analysis to derive scaling laws which disentangle total parameters vs FLOPs per example in MoEs, we can estimate the optimal sparsity level under the setting where both total training FLOPs and total number of parameters are given and fixed. Generally, we find that:\n\u2022 During pretraining, increasing a model's capacity by adding more parameters yields greater benefits than increasing FLOPs per example. We observe that the size of compute-optimal models increases as we increase the training budget (measured in terms of total FLOPs) while the active number of parameters, hence FLOPs per example, decrease for compute-optimal models.\n\u2022 During inference, FLOPs per example seem to play a more important role\u00b2. For many tasks, upstream performance is a good predictor of downstream performance and the relationship between upstream and downstream performance is not impacted by the sparsity level. However, we observe that for models with the same perplexity on the pretraining data distribution, sparser models, i.e., models with fewer number of active parameters, perform worse on specific types of downstream tasks that presumably require more \u201creasoning\u201d.\nOur results, inline with findings from previous relevant studies (Ludziejewski et al., 2024; He, 2024) on scaling laws for MoEs, show increasing sparsity level leads to better performance and efficiency during pretraining. Considering the various methods to increase compute per example during inference adaptively conditioned on task or example complexity, we conclude that approaches like Mixture-of-Experts (MoEs), which reduce the unit compute cost (i.e., FLOPs per token) by increasing the sparsity level, hold significant promise given their potential to enhance efficiency in both pretraining and inference."}, {"title": "2 The Interplay between Model Parameters and Sparsity in MoEs", "content": "Is there an optimal trade-off between parameter count and FLOPs per example in MoEs under the setting where the training compute budget (i.e., total training FLOPs) is fixed?\n\nIntuitively, under infinite data setting, scaling model capacity along with the training compute budget leads to performance improvements. Previous scaling law studies suggest that, conditioned on a training compute budget measured in FLOPs denoted by C, the optimal number of parameters, $N^* (C)$, exhibits a power-law relationship with C (Hoffmann et al., 2022):\n$N^*(C) = \\arg \\min_N L(N; C) \\propto C^\\alpha$\nOur goal is to study how to optimally trade-off FLOPs per example and total parameters in MoEs. In MoEs the balance between parameters and FLOPs can be expressed through the sparsity level, S. We define S as the ratio of non-active to total number of experts, i.e., $S = \\frac{E-K}{E}$; where E is the total number of experts and K is the number of selected experts per token. We can vary the sparsity level by either changing the number of active experts K or total number of experts E. 3 Essentially, for models with the same N, the model with a higher S will have fewer active parameters $N_a$, resulting in fewer FLOPs per example. For more details on the notations and experimental settings see Appendix A and Appendix B.\n$(N^*, S^*) = \\arg \\min_{N,S} L(N, S; C)$\nTo simplify the problem of understanding the joint role of N and S in predicting L, we break the problem, Equation 2, into two parts:\n1. \"How does the sparsity level impact the scaling laws of the relationship between N and C for training-compute optimal models?\" To address this question in \u00a72.1, we fix S and vary N, studying how optimal N and $N_a$ change for different values of S:\n$N^* = \\arg \\min_N L(N; C, S)$"}, {"title": "2.1 Optimal Model Size for Fixed Sparsity Level", "content": "Here we examine how sparsity influences scaling laws governing the relationship between N, $N_a$ and C for training-compute optimal models, i.e. how does $N^*$ and $N^*_a$, for a given C, S (Equation 3), change as we increase S? Looking at slices of the IsoFLOP surface along the model size dimension, we observe how the IsoFLOP curves shift along loss and"}, {"title": "2.2 Optimal Sparsity Level for Fixed Model Size", "content": "In this section we aim to understand the dynamics between the total number of parameters and FLOPs per example in MoEs. In Section 2.1 we are considering the case where there is no bound on the total number of parameters. In this case, we observe that under fixed training compute budget in terms of FLOPs, it is better to train sparser models with higher total number of parameters. However in practical scenarios it is reasonable to assume that there would be some bounds on the memory and hence the total number of parameters of a model. This leads us to a fundamental question: Is there an optimal balance between the total number of parameters and and FLOPs per example under a fixed training-compute budget? Thus, we investigate the optimal sparsity level when total number of parameters is fixed. Specifically, we ask: Given N and C, How does $S^*$ change as we vary N?\nTo address this, we look into slices of the IsoFLOP surface along the sparsity dimension. As we can see in Figure 2a, for a fixed training compute budget and fixed model size L(S; N, C) exhibits a parabolic profile, reaching its optimum value at the vertex where $S = S^*$. It is noteworthy that for a given total training compute, there is threshold value $N_{th}$ for the total number of parameters, where for larger models, models with $N > N_{th}$, increasing sparsity always has a positive impact, i.e., optimal sparsity level approaches 1.0. More accurately, for a fixed compute budget the optimal sparsity level increases with model size and converges to 1 as the model size grows (see Figure 4 in \u00a7D.2 in the Appendix for more details). Note that the optimal model, here is not the largest model, i.e., there is a compute optimal model size in terms of total parameters even after sparsity is introduced, and increasing total number of parameters would lead to under-training if training compute budget is fixed.\nThese results highlight the importance of balancing the number of parameters with FLOPs per example in MoEs. Intuitively, when the total number of parameters is small, higher sparsity results in fewer active parameters, and thus fewer FLOPs per example. This reduction in FLOPs per example may lead to inefficiencies during both training and inference. Conversely, when the total number of parameters is large, for a reasonable amount of FLOPs per example, a fixed compute budget may not allow sufficient training on enough tokens to make use of the model's additional capacity."}, {"title": "3 Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity", "content": "Does increasing compute budget impact the interaction between the parameters and compute per example in MoEs and how they contribute to model's capacity? In other words, does the recipe for optimally increasing model capacity, i.e., optimal sparsity level for MoEs change as we scale up the total training compute?\nTo answer this question. in Figure 3 we illustrate the trends for changing the total number of parameters, $N^*$, the number of active parameters, $N_a^*$, and the loss, $L^*$, with sparsity level across different compute budgets.\nFigure 3c shows that the optimal sparsity level approaches 1 across all compute budgets used in our experiments. There is no significant difference observed in the slope of the loss vs sparsity curves across different training compute budgets used in our experiments. This observation suggests that there is no diminishing effect of sparsity on the pretraining loss as we increase training compute budget, i.e., if there is no constraint on the model size, sparsity improves the performance of the model across all training budgets.\nIn Figures 3a and 3b,, we see a consistent trend of increasing N and decreasing $N_a$ for compute optimal models as sparsity level increases across all training compute budgets. Moreover, as can be seen in Figure 4, when model size in terms of total number of parameters is fixed, optimal sparsity level increases with training compute budget as well as model size as discussed in Section 2.2."}, {"title": "4 Effect of MoE Sparsity on Downstream Task Performance", "content": "In this section, we study how sparsity affects the relationship between upstream and downstream performance of MoEs. In other words, does sparsity impact the relative gains from improvements in pretraining tasks on downstream tasks?\nWe use downstream tasks from the evaluation suite in 11m-foundry for benchmarking our pretrained models. The downstream task are devided into four pre-defined categories namely: language understanding, world knowledge, reading comprehension, and symbolic reasoning to help us systematically test whether the downstream vs upstream performance trend remains the same or is different as we vary sparsity values.\nWe observe from Figure 5a (language understanding), Figure 5c (commonsense reasoning), and Figure 5d (world knowledge) that there is a strong correlation between upstream (pretraining) loss and downstream performance (error) across all these tasks. For these tasks, downstream performance is predictable based on upstream performance, regardless of the sparsity level. This indicates that, in the context of these tasks, the optimal sparsity level follows the same trend as the"}, {"title": "5 Incorporating Sparsity into Scaling Laws", "content": "The scaling laws proposed by Kaplan et al. (2020) provide a framework for predicting loss in dense models by establishing a power-law relationship between loss L, number of parameters N and dataset size D, where N and D interact linearly. Formally, the relationship is given by:\n$L(N, D) = \\frac{a}{N^\\alpha} + \\frac{b}{D^\\beta} + e$\nHere, the term $N^\\alpha$ captures the inverse relationship between model size and loss, where an increase in model size N leads to a reduction in loss. The exponent a quantifies the rate of this decrease; a larger a suggests a steeper reduction in loss with increasing model size. Similarly, the term $D^\\beta$ indicates the impact of dataset size D on loss, with larger datasets contributing to lower loss values. The exponent \u03b2 measures this relationship, where a larger \u03b2 implies a greater benefit from increased data. The constant e represents an asymptotic minimum for the loss, as both model size and dataset size approach infinity.\nFor dense models with a fixed total training FLOPs, C, the parameters N and D are interrelated through the equation for estimating FLOPs per example, given as C = 6ND for transformers."}, {"title": "7 Related Work", "content": "computation costs, these models have the potential to improve both efficiency and performance. The availability of GPUs with larger memory, for e.g., the recently introduced H200 GPU chip with 141 GB of memory as well as improving the efficiency of training and deployment pipelines (Authors, 2025) suggest that there is significant interest in developing efficient implementations for MoEs."}, {"title": "7.1 Scaling Laws for Language Models", "content": "Scaling laws have proven to be a powerful framework for understanding and predicting the performance of language models. Existing studies, such as Kaplan et al. (2020) and Hoffmann et al. (2022), reveal that power-law relationships govern model performance as a function of factors like model size, data size, and compute budget, offering predictable performance improvements with increased resources.\nHoffmann et al. (2022) emphasizes the critical balance between model size and the number of training tokens when the training compute budget is fixed, showing that scaling the model without corresponding data increases can lead to suboptimal performance. Additionally, DeepSeek-AI (2024) explores more nuanced scaling behaviors by incorporating data quality, demonstrating that higher-quality data allows for more efficient scaling, and thus, a larger portion of the compute budget should be allocated to increasing model size.\nRecent work extends scaling law analysis to specialized contexts, including over-training (Gadre et al., 2024), downstream task performance, and multilingual or multi-modal settings, where scaling laws provide valuable insights and can be adapted to address specific challenges."}, {"title": "7.2 Scaling Laws for MoEs", "content": "Mixture-of-Experts (MoE) models (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; DeepSeek-AI, 2025) have emerged as a powerful architecture for language modeling, primarily because they decouple computational cost from parameter count. This separation between parameters and FLOPs per token in MoE architectures calls for scaling laws that can accurately factor in the contributions of both.\nPrevious research on the scaling behavior of MoE models has established foundational scaling laws, incorporating factors such as total parameter count, the number of experts, and the granularity of these experts (Clark et al., 2022; Ludziejewski et al., 2024; Wang et al., 2024). However, these studies typically assume a fixed configuration for other critical variables influencing FLOPs per token, such as the number of active experts per input. In contrast, we propose a generalized scaling law that considers variables like active parameter count and sparsity level, thereby expanding the applicability of MoE scaling laws.\nA common theme in the literature suggests that training sparser models-achieved by increasing the number of smaller experts-offers significant gains in efficiency for both pretraining and inference phases. Through a comprehensive large-scale study, we provide empirical evidence for this, analyzing the impact of sparsity level on efficiency and defining optimal configurations.\nSupporting this, Du et al. (2021) demonstrates GLaM's superior efficiency and performance compared to GPT-3, showing that MoE architectures can achieve high performance with significantly lower computational and energy costs. Further insights are offered by Clark et al. (2022), who analyze scaling behaviors across various MoE routing techniques. While their study finds that MoEs generally outperform dense models, it also notes diminishing benefits as base model sizes grow. Ludziejewski et al. (2024) challenge this conclusion, attributing the diminished returns partly to the"}, {"title": "8 Conclusion", "content": "In this paper, we try to understand the optimal trade-off between parameters and compute per example to optimally increase the model capacity. Our experiments and analysis, underscores the role of sparsity, as a factor that controls the balance between total number of parameters and FLOPs per example, in the scaling laws for Mixture-of-Expert Transformers (MoEs), showing that the most efficient model configuration depends on balancing model size, training compute, and sparsity level. The optimal recipe for balancing FLOPs per example and parameter count in MoEs depends on the objective as well other resource constraints. Our findings indicate that sparsity, as a knob that controls FLOPs per example in MoEs, is a powerful mechanism for optimizing model performance under constrained training compute budgets. By balancing the total number of parameters, compute, and sparsity, MoEs can be scaled more effectively. These insights provide valuable guidance for scaling language models, especially for MoEs, where the trade-offs between parameters and FLOPs must be carefully managed.\nMoEs were originally introduced to allow increasing model capacity without a significant increase in inference cost. Our experiments show that under fixed total training compute budget increasing sparsity in MoEs leads to smaller FLOPs per example, higher number of parameters, and lower pretraining loss simultaneously. In other words, in the context of MoEs, if there are no constraints on the total number of parameters, increasing the capacity of the model through parameter count seem to be the optimal strategy if lower pretraining loss is the main goal. On the other hand, when comparing how well the pretraining performance transfers to various downstream tasks, denser models seem to be better on certain types of task that potentially rely on deeper processing of the input vs the knowledge stored in the parameters of the model. This potentially signals the importance of the role of FLOPs per example in increasing the capacity of the model during inference. Our experiments demonstrate that Mixture-of-Experts (MoE) models use Chain-of-Thought (CoT) prompting more effectively than dense models, achieving better performance when allocated additional computational resources during inference. This observation reveals an interesting direction to improve the performance efficiency of MoEs at inference time.\nFuture work will focus on determining the optimal balance between FLOPs per example and parameter count, with an emphasis on conducting in-depth analyses of model performance across diverse downstream tasks. A key direction will involve exploring strategies to balance parameter allocation and computational demands to minimize inference costs. Developing scaling law"}, {"title": "A Preliminaries", "content": "In this section, we provide a brief overview of Mixture-of-Expert (MoE) Transformers."}, {"title": "A.1 Mixture-of-Expert (MoE) Transformers", "content": "Mixture-of-Experts Transformers modify the standard transformer architecture by introducing in the MLP layer. In this design, the experts are MLP (Multi-Layer Perceptron) modules that follow the attention mechanism and are selectively activated for each token. A gating mechanism determines which MLP experts are most relevant for each token, ensuring that only a subset of experts (top-k) is active at any given time, while the rest remain inactive. Below, we provide the notations used throughout the paper for various terms related to training MoEs.\nTotal and Active Parameters: In MoEs, we distinguish between total and active parameters, denoted by N and $N_a$, respectively. The total parameter count, N, includes all parameters of the network, encompassing both the experts and the rest of the architecture. The active parameter count, $N_a$, refers to the parameters associated with the active portion of the experts, along with the rest of the network that is always utilized.\nTop-k Expert Selection: In MoEs, the gating mechanism assigns tokens to a subset of experts using a top-k selection process, where k denotes the number of experts activated for each token. The gate computes a relevance score for each expert, and the top k experts with the highest scores are selected and activated. This selective activation limits the computational overhead by ensuring that only a fraction of the experts are used per token.\nExpansion Factor and Granularity: The expansion factor, typically denoted by E, represents the increase in model capacity due to the inclusion of multiple experts, measured as a multiplicative factor relative to the base dense model. The granularity, G, determines the size of each expert relative to the size of the MLP module in the base dense model. The total number of experts in the model is given by E \u00d7 G, where E scales the capacity and G controls the level of granularity.\nSparsity (S): In general, sparsity is defined as the ratio of inactive to total parameters. However, in the context of MoEs, we focus on the sparsity of the MLP modules specifically. Therefore, we define the sparsity level as the ratio of inactive to total experts, given by:\n$S = \\frac{\\text{number of non-active experts}}{\\text{number of total experts}}$\nThis definition provides an interpretable measure of sparsity but cannot be directly used to calculate the active parameter count $N_a$ due to the contribution of other parameters in the model that remain unsparsified."}, {"title": "B Experimental Setup", "content": "We train and evaluate auto-regressive sparse Mixture-of-Experts (MoE) language models of varying sizes and configurations on subsets of the RedPajamaV1 dataset Computer (2023). The key variables we explore in our experiments are total model parameters N, training compute budget C, and the MoE sparsity S.\nPre-training data. Our models are pre-trained on subsets of the RedPajamaV1 dataset Computer (2023), which attempts to replicate the LLaMA pre-training data recipe and comprises 1.2 trillion tokens from sources such as Common Crawl, C4, GitHub, and Wikipedia. In all our experiments, the effective dataset size is adjusted based on the training compute budget C and the model size N. We tokenize the data using the GPT-NeoX tokenizer Black et al. (2022), which has a vocabulary size of 50, 432 tokens.\nModel and tokenizer. We use auto-regressive transformer-based MoE language models in order to study compute-parameter trade-offs by varying MoE sparsity. We use the Megablocks library Gale et al. (2023) to train dropless MoEs in which the routing mechanism ensures that all tokens are efficiently routed without being dropped due to routing capacity constraints.\nOptimizer and scheduler. We optimize our models using the scale-free Adam optimizer with variable learning rate, a weight decay of 1 \u00d7 10-5, and fixed Adam-specific parameters B = (0.9, 0.95) and \u025b = 1 \u00d7 10-8. We use a learning rate scheduler consisting of a linear warm-up phase followed by a cosine decay. The warm-up phase increases the learning rate from 0 to the base learning rate over a fraction of the total training steps (selected from {0.1, 0.05, 0.02}). After warm-up, the learning rate decays following a cosine schedule for the remaining training steps.\nFitting IsoFLOP surfaces. Recall that in Section 2, we fit isoFLOP surfaces to predict pretraining loss L as a polynomial function of model size N and MoE sparsity S for a fixed training budget C. The polynomial function takes the form\n$L(N, S) = \\sum_{i=1}^{\\alpha_1} a_i N^i + \\sum_{i=1}^{\\alpha_2} b_i S^i + \\sum_{i=1}^{\\alpha_3} c_i (N S)^i + d$\nwhere \u00d1 = log N and \u015c = \u2013 log(1 \u2013 S)-we find that applying log transformations improves the fit of the resulting IsoFLOP surface. Through a grid search over the polynomial coefficients $\\alpha_1, \\alpha_2, \\alpha_3 \\in {0, 1, 2, 3, 4}$, we found that the best fit was obtained for a = \u03b2 = \u03b3 = 2, i.e., a quadratic polynomial over \u00d1 and \u015c. We evaluate the fitted IsoFLOP surfaces by (a) re-running the fitting procedure k = 100 times on randomly subsampled data and (b) evaluating the Pearson correlation between the true and predicted pretraining loss values on a set of held-out data points.\nHyperparameters. We fix a subset of hyperparameters for which changing values in preliminary experiments (a) did not significantly improve pre-training loss, (b) the optimal value remained the same across several model configurations, or (c) in order to reduce the search space (i.e., limited compute resources). Specifically, we first opted to use z-router loss Zoph et al. (2022) and qk-normalization Wortsman et al. (2023) in order to stabilize training for large MoEs. Second, we fixed MoE router jitter noise to 0, as it did not improve performance. We also fixed our batch size to 1024 for all model sizes."}, {"title": "C Estimating Mixture-of-Expert (MoE) FLOPs", "content": "Similar to prior work on scaling laws (e.g., Kaplan et al. (2020); Hoffmann et al. (2022); Ludziejewski et al. (2024)), we use theoretical FLOP estimates as proxies for training and inference costs of language models. In this section, we (a) outline our methodology for estimating FLOPs for MoEs and (b) show that the proposed estimator closely approximates empirical FLOPs of large-scale MoEs.\nSetup and notation. Consider an MoE model with $n_{layers}$ MoE layers, each with an embedding dimension of $d_{model}$. We denote the number of total experts and active experts in each MoE layer by $E_{total}$ and $E_{active}$ respectively. Following Ludziejewski et al. (2024), we let G denote the MoE granularity, which defaults to 1 and controls the size of each expert relative to the size of a feed-forward layer in an equivalent dense transformer. In order to change sparsity in a more granular manner, we treat the number of active experts as an independent variable that does not scale with granularity G. In our experiments, we use a vocabulary size $n_{vocab}$ = 50, 432, a context length $n_{ctx}$ of 2048, and GLU modules (Gated Linear Units) (Shazeer et al., 2017) over feed-forward modules as the architecture of choice for MoE experts. We also set the (a) hidden dimension of each GLU expert $d_{ffn}$ to $4 \u00b7 d_{model}$ and (b) instantiate MoEs where the number of attention heads $n_{heads}$ times the dimensionality for each head $d_{head}$ equals $d_{model}$, i.e., $n_{heads}d_{head} = d_{model}$.\nEstimating module-specific FLOPs. To estimate the FLOPs of a given MoE model, we first individually estimate the FLOPs per token incurred by a forward and backward pass through every module in MoEs. Then, we aggregate these estimates to obtain the final estimator for the FLOPs per token incurred by a forward and backward pass through the model.\nLike in prior work on scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), we take a two-step approach to estimate module-specific FLOPs. Given a module, we first estimate the number of parameters in the module and then scale this with an appropriate constant corresponding to the number of add-multiply operations per parameter through a forward and backward pass of the given module. We also omit non-leading terms such as non-linearities, biases, and layer normalization in our estimation. We estimate the FLOPs per token for attention modules, MoE routers, MoE experts, and the final un-embedding layer as follows:\n1. Attention module. We estimate the FLOPs incurred via the QKV (and final) projections, attention logits, and attention values of all heads in a multi-head attention module as follows:\n\u2022 QKV (and final) projections. These projections involve $4 \u00b7 d_{model}n_{heads}d_{heads} = 4d_{model}^2$ parameters. Following Kaplan et al. (2020), we use the multiplicative constant C = 6 to account for the add-multiply operations per parameter in a forward and backward pass through linear modules, resulting in a FLOPs-per-token estimate of $4 \u00b7 C \u00b7 d_{model}^2$.\n\u2022 Attention logits. The FLOPs required to compute the attention logits for all $n_{ctx}$ tokens equals $C \u00b7 n_{ctx}^2 d_{model}$ FLOPs, making the FLOP-per-token estimate equal to $C \u00b7 n_{ctx} d_{model}$.\n\u2022 Attention values. The computation of attention values requires a per-token weighted sum over $n_{ctx}$ $d_{model}$-dimensional vectors, making the estimate $C \u00b7 n_{ctx} d_{model}$.\n2. MoE module. Given an MoE layer, we estimate the FLOPs incurred by its router and all experts separately.\n\u2022 Router. The MoE routing linearly maps a $d_{model}$-dimensional token embedding to a $E_{total}$-dimensional logit vector, which is subsequently used to map the token to $E_{active}$ active experts. Following Ludziejewski et al. (2024), we use a multiplicative constant R = 14 that accounts for the add-multiply-route operations per router parameter. The resulting FLOP estimate equals $R \u00b7 d_{model} E_{total}$\n\u2022 Experts. Each MoE experts corresponds to a GLU module (Shazeer et al., 2017) with $d_{ffn} = 4 \u00b7 d_{model}$. Since there are $E_{active}$ active experts with granularity G, each involving"}, {"title": "Un-embedding layer", "content": "The un-embedding linear layer maps the final $d_{model}$-dimensional embedding of a token to $n_{vocab}$-dimensional logits, making the FLOPs-per-token $C \u00b7 n_{vocab}d_{model}$.\nEstimating MoE FLOPs. We can aggregate the module-level FLOP estimates described above to estimate the FLOPs per token required for a single forward and backward pass through a given MoE model as follows:\n$Player (4Cd_{model}^2 + 2Cd_{model}N_{ctx} + \\frac{12C}{G}E_{active}d_{model}^2 + Rd_{model}E_{total}) + Cn_{vocab}d_{model}$\nWhen $E_{total}/d_{model}$ is small, which is typically the case in practice, the FLOPs induced by MoE routing can be ignored as they contribute negligibly to the estimator. This allows us to simplify the estimator to:\nMOE FLOPs per token := C \u00b7 $n_{layers} d_{model} (4 + \\frac{2n_{ctx}}{d_{model}} + \\frac{12 \\frac{E_{active}}{G}}{d_{model}} + \\frac{N_{vocab}}{n_{layers}})$\nEvaluating 6ND as a FLOPs-per-token estimator in MoE Models For standard dense transformers, the FLOPs are often estimated as 6ND (Kaplan et al., 2020; Hoffmann et al., 2022). Given that D is fixed and not adjusted dynamically, N can serve as a relative estimator of FLOPs per token for dense transformer models.\nTo adapt the 6ND estimator for MoE models, we replace N with $N_a$ (the active number of parameters)-the number of parameters used in every forward and backward pass."}, {"title": "D Additional Analysis", "content": "In this section, we show similar results with additional training compute budgets.\n In Figure 8, we first show that IsoFLOP surfaces mapping model size N and sparsity level S to pre-training loss L are predictive in a similar way for all training compute budgets that we consider, ranging from 3e19 to 1e21 FLOPs.\n In Figure 9, we analyze the fitted IsoFLOP surfaces (one for each training budget) and find that the (a) effect of model size N on optimal MoE sparsity $S^*$ and (b) the effect of MoE sparsity S on the optimal total and active parameters, $N^*$ and $N^*_a$, is similar for all training budgets."}, {"title": "D.2 Effect of training budget and model size on optimal MoE sparsity", "content": "In this section, we use the fitted isoFLOP surfaces to analyze how the optimal MoE sparsity $S^*$ changes as a function of total parameters N and training budget C: For a fixed model size (i.e., total parameters N), increasing the training budget C generally reduces the optimal sparsity level $S^*$. \n The relationship between model size N and optimal $S^*$ is not linear. For smaller models (up to about 500 \u00b7 10\u00ba parameters), the optimal sparsity remains at 0 (i.e., dense) for most compute budgets."}, {"title": "D.3 Effect of sparsity on downstream task performance", "content": "In Section 4, we analyzed the relationship between upstream pre-training loss and downstream task performance across different MoE sparsity levels. We found that language understanding and world knowledge tasks generally showed a strong correlation between upstream and downstream performance, while reading comprehension tasks seemed to favor denser models to some extent.\nIn this section, we provide additional plots for a broader"}]}