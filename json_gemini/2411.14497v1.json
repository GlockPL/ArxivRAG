{"title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning", "authors": ["Hang Zhou", "Yehui Tang", "Haochen Qin", "Yujie Yang", "Renren Jin", "Deyi Xiong", "Kai Han", "Yunhe Wang"], "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon\u00b9.", "sections": [{"title": "1 Introduction", "content": "The research and development of natural language understanding and generation have been dramatically accelerated with the emergence and prevalence of LLMs [39, 31, 30]. These models have been extensively applied in a wide range of scenarios, e.g., question answering and text generation, significantly enhancing downstream task performance due to their exceptional ability to follow instructions [3, 53, 49, 10, 28]. Such an instruction-following capability is primarily acquired through a process known as instruction tuning [40, 23, 5], where LLMs are fine-tuned on instruction data. It is hence widely acknowledged that the quality of instructions plays a pivotal role [5, 20, 48, 29].\nHistorically, the creation of instruction data for training LLMs has heavily relied on the expertise of human annotators, as evidenced by substantial scholarly contributions [14, 50, 41, 38, 9, 27, 21]. While expert-driven data generation assures the production of high-quality instructions, the enormous volume of data necessary for effective training renders this method economically untenable. In response, recent efforts have shifted towards the utilization of LLMs to automatically generate instructions, thereby mitigating the reliance on costly human annotation [37, 32, 44, 18]. Concurrently,"}, {"title": "2 Related Work", "content": "Our work is related to both instruction data generation and selection. We briefly review these topics within the constraint of space.\nInstruction Data Generation Datasets like Dolly [7] and OpenAssistant [15] are built from human-generated instruction data. The ShareGPT dataset, built from conversations between humans and ChatGPT, has been effectively used to improve the instruction-following performance of fine-tuned models [6]. Both Self-Instruct [36] and Alpaca [33] leverage the generation capabilities of GPT-3 to expand seed instructions. The generated instructions undergo filtering to eliminate low-quality instructions while the kept instructions are used to fine-tune the model to enhance the model's ability to respond to instructions. Baize [45] proposes a self-dialogue framework, using questions from popular Q&A websites as starting topics, then having LLMs converse with themselves. CAMEL [16] introduces a role-playing framework where LLMs discuss a given topic when playing a role as either \"user\" or \"assistant\". UltraChat [8] uses real-world named entities combined with various text-writing tasks to generate diverse and high-quality multi-turn dialogues for LLMs. Lion [13] introduces the concept of adversarial distillation, using the Imitation-Discrimination-Generation stages to iteratively generate data, refine existing instructions, and produces more complex and diverse instructions to expand the capabilities of the student model. Evol-Instruct [44] uses five manually designed prompts to explicitly guide LLM in rewriting existing simple instructions into more complex ones. The WizardLM model, trained with Evol-Instuct, ranks highly on MT-Bench [54], highlighting the importance of data quality in training effective LLMs.\nInstruction Data Selection With the aforementioned methods, it is not difficult to use LLMs to generate large instruction tuning datasets at low cost. However, for instruction-tuned language models, data quality is more crucial than quantity. In this aspect, ALPAGASUS [5] evaluates the effectiveness of instruction data by leveraging ChatGPT. INSTAG [24] automatically generates tags for instruction samples with ChatGPT and keeps diversity by selecting subsets with more tags. Cherry LLM [20] pioneers the self-guided approach, using the IFD metric to measure the difficulty for an LLM to learn"}, {"title": "3 Star-Agents", "content": "The aim of our research is to construct a high-quality dataset T of tailored complexity for the target LLM through the enhancement of an initial seed dataset S = (Ii, Ri)i=1N, consisting of instruction-response pairs (I, R).\nTo this end, we introduce the Star-Agents Framework, depicted in Figure 1, which is segmented into three steps. The first step leverages a spectrum of advanced LLMs, each trained independently. These models are engaged in a dynamic interaction to generate a diverse data candidate set D(Si) by sampling agent-pair derived from Si as detailed in Section 3.1. Following this, we apply a dual-"}, {"title": "3.1 Generating Diverse Data", "content": "To improve the instruction-tuned model, it is crucial to assemble a high-quality and diverse instruction dataset [22]. Traditional methods often use a single LLM, such as ChatGPT, for data enrichment. In contrast, our approach employs multiple LLMs to avoid monotonous data distribution. This multifaceted strategy also addresses the limitations and risks of quality degradation on domain-specific tasks associated with using a single model. To counter these challenges, we propose to use an Agent-Pair strategy.\nAgent-Pair. Utilizing a spectrum of LLMs, each trained with discrepant setting, facilitates the generation of varied responses to given instructions. This diversity is crucial for synthesizing a dataset characterized by high richness [24].\nThe Star-Agents framework strategically pairs different LLMs to rewrite the instructions in the seed dataset and generate new responses to increase the diversity. With agent-pair (Al, AR), a new instruction data can be generated as follows:\nfj,k (Ii, Ri) = (Al(Ii), AR(Ri)),\nwhere Al and AR represent the agents that rewrites the instruction and response to the instruction, respectively.\nGiven the high cost of deploying all agent-pairs, a feasible solution to balance cost and agent diversity is to sample a subset of agent-pairs from the Star-Agents for data generation. Equation 3 formulates this process, where D is collected dataset generated by performing f over all sampled pairs (Afj, ARk) of instruction agents Af and response agent ARk with sampling probabilities pjk:\nD (Si) = {fj1,k1 (Si),\u2026\u2026, fjm,km (Si) | (jm, km) ~ pjk, m = 1,2,\u2026\u2026\u2026, M},\nM is number of agent-pairs sampled for a single seed sample. The sampling probability pjk is initialized as a uniform distribution and will be updated using the method described in Subsection 3.3 during data generation. Meanwhile, an Instruction Memory Bank that stores high-quality instructions will be updated. To ensure the lower bound of data quality, each iteration will consistently call a fixed set of agent-pairs, referred to as base agent-pairs."}, {"title": "3.2 Evaluating Tailored Data via a Dual-model Strategy", "content": "Identifying and selecting tailored data from a diverse dataset is crucial for enhancing model performance, especially since the presence of low-quality data can impede model functionality. It is acknowledged that data samples that are lengthy, complex, and challenging significantly benefit the instruction tuning process [22].\nNevertheless, too complex instruction data may be not necessarily benefit model performance. We have observed that for models with 14M and 70M parameters as illustrated in Figure 2, the Evol-Instruct dataset, though more challenging than the Alpaca dataset, results in diminished model performance. This suggests that intricate examples may surpass the capabilities of small models and be harmful for model performance, despite the advantages of using complex data for large models."}, {"title": "3.3 Evolving Star Agents", "content": "As mentioned in Section 3.1, we use the joint probability of instruction agents and response agents to regulate the invocation of each agent-pair. Considering the abilities and specialities of each LLM vary, however, sampling each agent-pair with the same probability is not optimal. We hence use the score from Section 3.2 to dynamically evolve the sampling probability. Additionally, since the generation performance of agent-pairs is task-dependent, we also propose an Instruction Memory Bank to select the most suitable agent-pair for particular tasks.\nAgent-Pair Sampling Evolution. Section 3.2 has introduced the score \u03c0, which effectively estimates the quality of generated samples. During each iteration, if the generated samples are of high quality, we will increase the sampling probability of the selected agent-pair, which is updated as follows:\np'jk = pjk + \u03b2\u00b7 \u03c0(Ii, Ri),\npjk = \\frac{p'jk}{\\sum_{j,k}p'jk},\nThe updated sampling probability for the agent-pair of the j-th instruction agent and k-th response agent that successfully process the i-th data sample will be used in the next iteration, where \u03b2 denotes the evolution rate. This formula adjusts the sampling probabilities based on the effectiveness demonstrated by agent-pairs in generating relevant data. Iterative updates ensure that as the synthesis process advances, the probability of selecting more effective agent-pairs increases, while less effective pairs are gradually phased out.\nInstruction Memory Bank Evolution. We establish an Instruction Memory Bank storing high-quality instructions aiming to accelerate sampling and relate the evolution with task data. When processing a data sample (Ii, Ri), we perform a query in the Instruction Memory Bank for Ii, retrieving the top n closest matches according to embedding similarity. The associated agent-pairs, identified as highly proficient for tasks similar to I\u2081, are then sampled. We sample l agent-pairs from this pool using normalized probabilities to generate diverse data. Moreover, to foster the creation of a diverse dataset, additional M - l agent-pairs are sampled from the remaining pool using their respective probabilities to assist in data synthesis. As a result, M new samples are generated and then feed for data assessment. Subsequently, the Instruction Memory Bank will continuously evolve by incorporating tailored high-quality data, which get high socres as introduced in Section 3.2."}, {"title": "4 Experiments", "content": "We conducted extensive experiments to evaluate the proposed Star-Agents framework. A wide range of LLMs, benchmark datasets were used in our experiments to guarantee the robustness of our evaluation."}, {"title": "4.1 Setups", "content": "Datasets. In alignment with the WizardLM [44], we adopted the Supervised Fine-Tuning (SFT) dataset, designated as the Evol-Instruct dataset, which consists of 70,000 instruction-response pairs. The instructions in this dataset were refined using \"In-Depth Evolving\u201d and \u201cIn-Breadth Evolving\u201d methods, which were tailored to enhance the base instructions by adding intricate details or expanding the overall scope, respectively. To guarantee the fidelity of the data, ChatGPT was also integrated as generator into the refinement process. The quality of the instruction data from the Evol-Instruct dataset has been validated as superior [44, 25]; hence, our research continues to leverage these refined instructions. Employing the Star-Agents framework, our study invokes multiple LLMs to generate diverse and high-quality responses for these instructions. For further enriching our comparative analysis, we employed the Alpaca dataset [32], comprising 52,000 instruction-following samples. This dataset, developed under the self-instruct paradigm, utilizes the ChatGPT2 instead of text-davinci-003 for a fair comparison [44]."}, {"title": "4.2 Main Results", "content": "GPT-4 Automatic Evaluation Based on the findings summarized in Table 2, comprehensive training sessions were conducted for the Pythia-1B and Llama-2-7B models utilizing three distinct datasets: Alpaca, Evol-Instruct, and the optimally refined Star Instruct datasets. The latter was developed through the application of Star-Agents, which are derivatives of the Evol-Instruct datasets. Through comparative analyses with other contemporary state-of-the-art models, we observe that the SFT-aligned models employing the Star Instruct datasets consistently outperform nearly all aligned counterparts, across all evaluated model families."}, {"title": "4.3 Ablation Study", "content": "Main Components. As illustrated in Table 3, we conducted ablation experiments on the three principal components within the Star-Agents framework. Results indicate that models using solely diversified datasets with random sampling yield a bit lower performance than the baseline. This occurs because the baseline employs data generated by ChatGPT, which is of high quality. In contrast, the diversified datasets draw from a variety of sources, making it challenging to ensure uniformly high quality. Thus, random sampling may introduce low-quality data, leading to diminished model performance. The inclusion of a data selection module subsequently leads to a recovery in model performance, suggesting that this module effectively selects high-quality data suitable for the model. Integration of the evolution strategy also provides a significant improvement, demonstrating that the evolution module can effectively select the most appropriate data generation agent-pairs from a complex array of candidate agent-pairs.\nSelection Method. As demonstrated in Table 4, we evaluated a range of conventional selection methods, including both random selection and strategies informed by the IFD [20]. Our dual-model selection strategy significantly outperforms these approaches. Compared to random selection, our method achieves a significant improvement, registering an improvement exceeding 0.5 points on average across a variety of test sets. When compared with the IFD approach, our enhancement approaches a 0.9 point. These findings robustly validate the effectiveness of our dual-model selection strategy, illustrating its superior performance in refining model selection precision using diverse evaluation metrics.\nEvolution. As depicted in Figure 5, we analyzed the sampling probability curves of typical agent-pairs throughout an iterative evolutionary process. Initially, each agent-pair began with a sampling probability of approximately 10%. Due to its robust performance, the Mistral-ChatGPT receives consistent rewards, which leads to a gradual increase in its sampling probability. By the completion of about 70,000 iterations, this probability has escalated to 30%. In stark contrast, the Phi2-ChatGPT undergoes a steady decline over the same period, with its sampling probability ultimately plummeting to near zero as it is progressively phased out. Concurrently, the ChatGLM3-ChatGPT exhibits a relatively stable trajectory, albeit with a slight downward trend. Evolutionary trajectories present significant discrepancy indicating different generation suitability of different generators on different tasks, where all the differences are captured by our evolution mechanism."}, {"title": "5 Conclusion", "content": "In this paper, we have presented the Star-Agents framework, an automated system for optimizing data to be optimally challenging for target LLMs. This framework has been applied to the open-source SFT datasets, and we conduct training sessions on a variety of model families, adjusting the data to enhance its efficacy. Our empirical investigations include a series of instruction tuning experiments that utilize both multiple baselines and specially optimized datasets on well-known models such as Pythia and LLaMA. Extensive experiments confirm the substantial impact of our method: the optimized tailored datasets result in an average performance enhancement of approximately 12%, with certain metrics, especially those involved in Fermi problem tasks exhibiting increases exceeding 40%, as substantiated by results on benchmarks such as MT-bench, Vicuna bench, and the WizardLM testset. These findings underscore the premise that strategically diverse and tailored data can profoundly improve model alignment and performance. In conclusion, our research details a highly effective automated framework that significantly augments dataset functionality, thus fostering more efficient model alignment.\nLimitations. Our approach achieves remarkable performance improvements on single-turn instruction datasets. However, it has not yet been evaluated on multi-turn conversations. We hence leave the evaluation on multi-turn instruction datasets and validation on datasets with domain-specific instructions to our future work."}, {"title": "A Appendix", "content": "A.1 Prompt Examples\nFollowing the Fast-Chat [54], the prompts used in the data selection process are as listed in Table 5.\nA.2 Case Study\nExamples of the single-turn dialogue and multi-turn dialogue are presented in Table 6 and Table 7."}, {"title": "A.4 Performance on Open LLM Leaderboards", "content": "The performance on Open LLM Leaderboards is shown in Table 9."}, {"title": "A.5 Computational Cost.", "content": "The computational overhead of our proposed method primarily depends on the inference computational load of the various LLMs used:\n\u2022 Qwen-14B: During inference with a sequence length of 256 tokens, the computational load is approximately 4 \u00d7 1012 Multiply-Add cumulations (MACs).\n\u2022 Phi-2-2.7B: For the same sequence length, the inference computational load is around 7 \u00d7 1011 MACS.\n\u2022 ChatGPT: Given that ChatGPT is a proprietary model, we don't have details on its computational requirements.\nNonetheless, for estimation purpose, we can approximate the overall computational cost. Assuming an iterative process involving multiple LLMs (e.g., 10 LLMs) and a large dataset (e.g., 70,000 samples), the total computation without using our framework can be roughly estimated as:\n\u2022 4 \u00d7 1012 FLOPs (Qwen-14B) \u00d7 10 LLMs \u00d7 70,000 samples = 2.8 \u00d7 1018 MACs\nWhile, when the Agent-Pairs Sampling and Instruction Memory Bank are employed, 5 of 10 LLMs are used to generate data , therefore, total computation can be significantly reduced and roughly estimated as:\n\u2022 4 \u00d7 1012 FLOPs (Qwen-14B) \u00d7 5 LLMs \u00d7 70,000 samples = 1.4 \u00d7 1018 MACs"}]}