{"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "authors": ["Pavan Kumar Anasosalu Vasu", "Nate True", "Fartash Faghri", "Gokul Santhanam", "Albert Antony", "Chun-Liang Li", "James Gabriel", "Cem Koc", "Peter Grasch", "Oncel Tuzel", "Hadi Pouransari"], "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM\u2014a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2\u00d7 improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152\u00d71152), FastVLM obtains comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85\u00d7 faster TTFT and a vision encoder that is 3.4\u00d7 smaller.", "sections": [{"title": "1. Introduction", "content": "Vision Language Models (VLMs) enable visual understanding alongside textual inputs. VLMs are often built by passing visual tokens from a pretrained vision backbone to a pretrained Large Language Model (LLM) through a projection layer (also known as the connector module). Previous works have explored various training and fine-tuning strategies for these three components: the vision backbone, the adapter, and the LLM, which is typically a decoder-only model.\nSeveral studies highlight image resolution as a key factor in VLM performance, especially for text- and chart-rich data. However, increasing image resolution presents multiple challenges. First, pretrained vision encoders may not support high-resolution images, as this would make pretraining inefficient. To address this, one approach is to continuously pretrain the vision backbone to adapt it for high resolutions. Alternatively, tiling strategies, such as Sphinx, S2, and AnyRes, divide images into subregions, with each subregion processed independently by the backbone. This approach is particularly suitable for ViT-based backbones, which cannot accept varying input resolutions.\nA further challenge is the runtime computational cost associated with high-resolution inference. Both single high-resolution inference and multiple inferences at lower resolution (the tiling strategy) result in significant latency when generating visual tokens. Additionally, high-resolution images naturally produce more tokens, which increases the LLM prefilling time (the LLM forward pass time on all tokens in the context, including visual tokens), thereby further increasing the time-to-first-token (TTFT), which is the sum of the vision encoder latency and the LLM prefilling time.\nIn this work, we study VLM design and training from a runtime efficiency perspective motivated by their on-device deployment. We explore the optimization landscape as image resolution increases, aiming to improve accuracy-latency trade-off, where latency includes both the vision encoder inference time and the LLM prefilling time. Using extensive experiments with different LLM sizes and resolutions, we establish the Pareto optimal curve for a specific vision backbone, showing the best accuracy achievable within a given runtime budget (TTFT) based on different choices of resolution and LLM size.\nWe start by exploring the use of a hybrid convolutional-transformer architecture FastViT, pretrained with MobileCLIP, as a vision backbone for the VLM setup. We demonstrate the potential of this hybrid backbone, which generates visual tokens over 4\u00d7 faster than a ViT model while achieving higher overall VLM accuracy with multi-scale features. However, further architectural optimization is possible when the primary goal is a high-resolution VLM (rather than embedding generation as in MobileCLIP-pretrained FastViT). We introduce a new hybrid vision encoder, FastViTHD, specifically designed for efficient VLM performance on high-resolution images and use it as the vision backbone to obtain FastVLM through visual instruction tuning. FastVLM demonstrates a significantly improved accuracy-latency trade-off over VLMs based on ViTs, convolutional encoders, and our previously discussed hybrid FastViT for different input image resolutions and LLM sizes. In particular, FastVLM outperforms several prior works while being smaller, faster, and trained with less data. Compared to LLaVa-OneVision operating at the highest possible resolution (1152x1152), FastVLM obtains comparable performance with the same 0.5B LLM, but with 85\u00d7 faster TTFT and a 3.4x smaller vision encoder.\nThe following is a summary of our contributions:\n\u2022 We demonstrate the efficacy of hybrid vision backbones in VLMs compared to ViTs. We also introduce additional architectural interventions, such as multi-scale vision features, to further improve VLM performance while maintaining efficiency.\n\u2022 We design and pretrain a new hybrid architecture, FastViTHD, optimized for efficient VLM performance with high resolution input for FastVLM. In a controlled experimental setup, where only the vision backbone is changed, we show that FastViTHD outperforms its ViT-based and convolution-based counterparts when used in VLMs: achieving 3.2\u00d7 faster TTFT and 3.6\u00d7 smaller size than SigLIP-SO400M, and 2.3x faster TTFT and 1.7\u00d7 smaller size than ConvNeXT. We further demonstrate that FastVLM scales effectively as more visual instruction tuning data becomes available.\n\u2022 We systematically study the VLM accuracy-latency trade-off by considering both the vision backbone latency and the LLM prefilling time on actual hardware benchmarks. Our results demonstrate an improved resolution-latency-accuracy trade-off achieved by FastVLM, measured on-device rather than estimates."}, {"title": "2. Related Works", "content": "Large Multimodal Models. With the emergence of large language models and large pretrained vision models, such as CLIP, trained on web-scale image-text datasets, several multimodal architectures have been proposed to encode images aligned with a large language model (LLM) to enable the interpretation of visual signals. Earlier works like Frozen and Florence used a cross-attention mechanism where the image embeddings are fused with text embeddings in intermediate layers of the LLM. More recently, auto-regressive architectures have gained popularity where the image embedding is fed alongside text as input to an LLM. Some prominent works that use this architecture are LLaVA, mPLUG-Owl, InstructBLIP, BLIP-3, SPHINX, MiniGPT-4, VILA, MM1, Qwen-VL, InternVL and Cambrian-1. Recently, Fuyu and EVE introduced a simplified architecture that passes raw images directly to the LLM decoder."}, {"title": "3. Architecture", "content": "In this section, we first explore the adoption of the FastViT hybrid vision encoder for vision-language modeling. We then introduce architectural interventions to improve performance on VLM tasks. We present FastViTHD, a new hybrid vision encoder designed for efficient high-resolution VLM. We provide comprehensive ablations to demonstrate the optimality of FastViTHD over FastViT and prior works for different LLMs and input resolutions. Figure 2 illustrates the overall architecture of FastVLM and FastViTHD. The training setup for all results in this section follows the same configuration as LLaVA-1.5 with Vicuna-7B as the LLM decoder, unless mentioned otherwise. See Sec. 4 for more details."}, {"title": "3.1. FastViT as VLM Image Encoder", "content": "VLMs such as LLaVA have three main components: an image encoder, a vision-language projector, and a large language model (LLM). Both the performance and runtime efficiency of a VLM highly depend on its vision backbone. Encoding images at high resolution is essential for achieving strong performance across various VLM benchmarks, especially for text-rich tasks. Therefore, a vision encoder with scalable resolution is particularly beneficial for VLMs.\nWe identify hybrid vision encoders (convolutional layers followed by transformer blocks) as an ideal candidate for VLMs, as their convolutional component enables native resolution scaling, and their transformer blocks further refine high-quality visual tokens for consumption by the LLM.\nWe use a CLIP-pretrained hybrid vision encoder in our experiments. Specifically, we use the MCi2 image encoder from MobileCLIP, which has 35.7M parameters, is pretrained on DataCompDR, and is based on the FastViT architecture. For simplicity, we refer to this encoder as \"FastViT\u201d throughout the rest of the paper. As shown in Tab. 1, using FastViT at its CLIP-pretrained resolution (256x256) alone does not yield a strong VLM. The main advantage of a hybrid encoder like FastViT lies in its favorable image resolution scaling characteristics, meaning it generates 5.2\u00d7 fewer tokens than the ViT architecture with a patch size of 14. The considerable token reduction gives significant advantage to VLM, as it greatly reduces the prefilling time and time-to-first-token of the transformer decoders with quadratic-complexity. When the input resolution of FastViT is scaled to 768\u00d7768, it produces the same number of visual tokens as ViT-L/14 with an input resolution of 336\u00d7336 but achieves better performance on VLM benchmarks. This performance gap is even more pronounced on text-rich benchmarks like TextVQA and DocVQA, despite both architectures producing the same number of visual tokens. Moreover, even if it results in same number of tokens with higher resolution, it takes much less image encoding time, thanks to the efficient convolution layers."}, {"title": "3.1.1. Multi-Scale Features", "content": "Typical convolutional and hybrid architectures split up the computations into 4 distinct stages with a downsampling operation between them. While the VLM relies on features from the penultimate layer, features in earlier stages of the network extract information at different granularity. Aggregating information from multiple scales can complement high-level features from the penultimate layer. This design is commonly used in object detection models like [47]. The architecture for multiple scale feature extraction is shown in Fig. 2. We ablate between 2 designs to pool features from different stages, i.e. AvgPooling and 2D Depthwise convolutions. From Tab. 2, we find that using depthwise convolutions results in better performance. Along with multi-scale features, we also experimented with different connector designs for FastViT (more details provided in supplementary materials). Collectively, these model interventions benefit hierarchical backbones like ConvNeXt and FastViT."}, {"title": "3.2. FastViTHD: High Resolution Encoder for VLM", "content": "While FastViT with the introduced model interventions performs well as an image encoder that is 8.7\u00d7 smaller than ViT-L/14, previous studies [14, 42] have demonstrated that increasing the scale of the image encoder improves its generalization capabilities. Common practice in hybrid architectures is to scale the number of self-attention layers along with the width of each layer in a 4-stage architecture like [16, 85], but this approach has its drawbacks. From Fig. 3, simply scaling-up the number of self-attention layers in stage 3 and 4 by following [16, 85], in the existing FastViT architecture is not optimal. In fact, it is even slower than ConvNeXT-L. More details on the naively scaled version of FastViT are provided in supplementary materials. To reduce the impact of the added self-attention layers, we introduce an extra stage preceded by a downsampling layer, see Fig. 2. In this approach, the self-attention layers only handle tensors that have been downsampled by a factor of 32 on each side, compared to a factor of 16 in typical and more recent hybrid models like ViTamin [11]. The self-attention layers with the widest MLP layers process input tensors downsampled by a factor of 64 on each side. Our design reduces image encoding latency and generates 4x fewer tokens for the compute-intensive LLM decoder, thereby decreasing the time-to-first-token (TTFT). The architecture schematic is shown in Fig. 2, and we call this model FastViTHD.\nThe model architecture consists of 5 stages, as shown in Fig. 2, with the first three stages utilizing RepMixer blocks and the last two stages employing multi-headed self-attention blocks. The model depth at each stage is [2, 12, 24, 4, 2], and the embedding dimensions for each stage are [96, 192, 384, 768, 1536]. The MLP expansion ratio for the ConvFFN layers is set to 4.0. The model has 125.1M parameters, which is 3.5\u00d7 larger than the largest FastViT variant from MobileCLIP, but is still smaller than popular ViT alternatives.\nWe follow the CLIP pretraining setup of using the DataCompDR-1B dataset to pretrain FastViTHD before employing it for FastVLM training. Table 3 shows that FastViTHD, despite being 2.4\u00d7 smaller and 6.9\u00d7 faster than ViT-L/14, achieves comparable average performance across 38 multi-modal zero-shot tasks [24]. In comparison to ViTamin [11], a hybrid transformer architecture built for VLMs, FastViTHD delivers superior average retrieval performance while being 2.7\u00d7 smaller and 5.6\u00d7 faster. In Tab. 4, we compare FastViTHD with other CLIP-pretrained hierarchical backbones, i.e. ConvNeXT-L and ConvNeXT-XXL, for VLM tasks after LLaVa-1.5 training. FastViTHD performs as well as ConvNeXT-XXL while being 6.8\u00d7 smaller and 3.3\u00d7 faster."}, {"title": "3.2.1. Vision Encoder - Language Decoder Interplay", "content": "The accuracy-latency trade-off in a VLM is influenced by several factors. On one hand, the overall performance of the VLM depends on (1) the input image resolution, (2) the quantity and quality of visual tokens, and (3) the capability of the LLM. On the other hand, the total latency (time to first token generation) of a VLM is determined by (1) the latency of the vision encoder and (2) the prefilling time of the LLM. The latter is affected by both the number of tokens produced by the vision encoder and the size of the LLM.\nDue to the complex optimization landscape of VLMs, claims regarding the optimality of a vision encoder must be verified across various pairs of (Resolution, LLM). Here,"}, {"title": "3.2.2. Static vs. Dynamic Input Resolution", "content": "There are two approaches to input resolution scaling, the first approach is to change the input resolution of the model to the desired resolution. The second approach is to tile the input image and set the input resolution of the image encoder to the tile size. The tiled inference (AnyRes) was introduced in prior works to enable ViT models to process high resolution images. Since FastViTHD is designed to run inference efficiently on high input resolutions, we analyze the optimal operating point for various resolutions using the two strategies. From Fig. 6, we see that simply setting the input resolution of the model to the desired resolution results in VLMs with the best accuracy-latency tradeoff. Only at extremely high image resolutions like 1536\u00d71536 do we see the benefits of dynamic resolution, as model inference at this resolution is mostly affected by memory bandwidth available on-device. If dynamic resolution is desired, using a setting with fewer tiles exhibits better accuracy-latency tradeoff. With advancements in hardware and improvements in memory size and bandwidth, we expect that FastVLM can be efficiently scaled to even higher resolutions without the need for tiling strategies."}, {"title": "3.2.3. Comparison with Token Pruning & Downsampling", "content": "We further compare the performance of FastViTHD operating at different resolutions to popular token pruning methods in literature. From Tab. 5, we find that VLMs achieve better accuracy to latency trade-off using a hierarchical backbone as opposed to using token pruning methods on isotropic architectures like ViT. By simply training the VLMs at lower input resolution, FastViTHD achieves visual token counts as low as 16, while improving over recent token pruning methods. Interestingly, even the most effective token pruning methods, such as those proposed by perform worse than FastViTHD trained at a lower input resolution of 256x256."}, {"title": "4. Experiments", "content": "In this section, we present our training setup and results.\nTraining Setup. For all the ablations presented in Sec. 3, we follow the 2-stage setup described in LLaVA-1.5 with Vicuna-7B as the LLM decoder, unless mentioned otherwise. During the first stage, only the projector is trained using LLaVA-558K alignment dataset for one epoch, with a batch size of 256 and a learning rate of 10-3. At this stage, the input image resolution matches the backbone pretraining resolution (e.g., 256 for FastViT and 224 for FastViTHD). In the second stage, we use LLaVA-665K supervised finetuning dataset, training the models for one epoch and tuning all the modules, i.e., vision encoder, projector and the LLM. At this stage, the input image resolution is set to the target resolution.\nIn Sec. 4, we present results with different LLM decoders, primarily with Qwen2-0.5B/1.5B/7B model family (chat variant) and Vicuna-7B model [90]. We report results in two training setups, the first one is the 2-Stage setup introduced in LLaVA-1.5. We additionally scale the dataset used in Stage 2 from 665k samples to 1.1 million samples, which is a subset of the instruction tuning dataset used in InternVL (more details in Sec. D). For the second training setup, we follow the current trend in literature of training the VLMs in 3 stages, i.e. Stage 1 for training the connector, Stage 1.5 for resolution scaling and Stage 2 for visual instruction tuning. In Stage 1.5, we use the densely captioned CC3M and CC12M datasets. For the final stage we use 1.1 million instruction tuning dataset. In this setup, the input image resolution is set to the backbone pretraining resolution for Stage 1 and adjusted to the target resolution for the following two stages. In both setups, the vision encoder and LLM are frozen only in stage 1, while all modules are finetuned in the remaining stages.\nAll FastVLM models reported in the paper are trained on a single node with 8\u00d7 NVIDIA H100-80GB GPUs. Stage 1 training of VLM is quick, taking roughly 30 minutes to train with a Qwen2-7B decoder. Stage 1.5 and Stage 2 training runs are dependent on input resolution. For an input resolution of 1024\u00d71024, Stage 1.5 takes 77 hours and Stage 2 takes 8 hours. The reported wall clock times correspond to the following datasets used in these stages: 15 million samples in Stage 1.5 and 1.1 million samples in Stage 2. Evaluation. We evaluate the models on the mainstream benchmarks of GQA, ScienceQA, TextVQA, POPE, LLaVA-in-the-wild, VQAv2, MMVet, MMMU, DocVQA and SeedBench. For GQA, ScienceQA, TextVQA, POPE and LLaVA-in-the-wild benchmarks, we use the official evaluation from LLaVA. For the remaining evaluations we use lmms-eval library v0.2.2. We use the default settings for all the evaluations and lmms-eval defaults to 0613 version of GPT for evaluations that rely on GPT as a judge.\nFor ablations presented in Sec. 3, we report GQA, TextVQA, POPE, DocVQA and SeedBench. GQA and SeedBench are general knowledge benchmarks, DocVQA and TextVQA represent text-rich evaluations and POPE is a hallucination benchmark. Together these benchmarks provide diversity and are quick to evaluate for ablations. Most importantly, they exhibit lower variance to different initializations and under probabilistic decoding setting. We report the variance for all the evals for different initialization in Sec. D.3. The standard deviation across the 5 selected metrics is less than 0.5. We call the average of these 5 benchmarks Avg-5, and use it as a reliable signal for our analysis. Our empirical estimate of the standard deviation for Avg-5 is ~0.1.\nBenchmarking. We benchmark all the models on a MacBook Pro with the M1 Max chip and 32GB RAM. The image encoder is converted to a Core ML package file using coremltools v7.2 and benchmarked on the neural engine using XCode 15.4 (15F31d). The LLM is benchmarked on the MacBook Pro GPU using MLX [27]. The model is first converted using mlx_lm.convert tool, which converts the models on huggingface to the MLX format and casts the tensors to FP16. The prefilling latency is estimated using mlx_lm.cache_prompt tool [27]. Time-To-First-Token (TTFT) is estimated by adding the image encoding latency at a specific resolution to the LLM prefilling latency for the associated visual tokens."}, {"title": "4.1. Comparison with state-of-the-art", "content": "In Tab. 6, we compare FastVLM with recently published methods. The training setup can vary widely between works. For each, we report the LLM decoder and the sizes of the instruction tuning and pretraining datasets used to train the respective VLMs, to facilitate a fair comparison.\nHierarchical Backbones. When we compare FastVLM (R18) with ConvLLaVA, with the same LLM and similar training data size, our model obtains +8.4% better performance on TextVQA and +12.5% better performance on DocVQA while being 22% faster. The gap widens at higher resolution, where FastVLM (R26 and R27) achieves superior performance on wide range of benchmarks while being 2\u00d7 faster than ConvLLaVA (R24), with the same LLM decoder."}, {"title": "Dataset Scaling", "content": "When designing a new architecture, it is crucial to consider how effectively the model scales with training data. We demonstrate the performance of FastVLM when scaling the pretraining and instruction tuning datasets. By increasing the instruction tuning dataset from 0.6M to 1.1M samples (R18), FastVLM outperforms prior works like MobileVLMv2 (R15), which was trained on larger instruction tuning and pretraining datasets. Additionally, when scaling the pretraining dataset by incorporating an intermediate pretraining stage for resolution scaling with 15M samples, FastVLM (R19) matches or surpasses MM1 (R36) across a wide range of benchmarks, including MMMU, MMVet, SQA, POPE, and SeedBench. Remarkably, FastVLM achieves this performance while generating 5\u00d7 fewer visual tokens. With an input resolution of 1024\u00d71024 and a larger instruction tuning dataset of size 11.9M, FastVLM (R39) outperforms MM1 (R36) and LLaVA-NeXT (R37) across various benchmarks. Even on text-rich evaluations, like TextVQA and DocVQA, which are sensitive to input resolution and number of visual tokens, our model achieves better performance with 2.8x and 11.3 \u00d7 less visual tokens than MM1 and LLaVA-NeXT respectively. We provide details of the dataset splits in Sec. D.\nMultiple Vision Encoders. Recently, MiniGemini and Cambrian-1 introduced models that rely on multiple vision encoders. In Tab. 6, we compare FastVLM (R38), which uses a single vision encoder with methods that use multiple encoders and trained on similarly scaled visual instruction tuning dataset. In Cambrian-1 (R41), vision encoding contributes 3.2\u00d7 more than LLM prefilling to the total time-to-first-token of approximately 5 seconds (detailed breakdown is provided in Tab. 9). FastVLM (R38) outperforms Cambrian-1 (R41) when trained on a similar visual instruction tuning dataset, while being 7.9\u00d7 faster. By scaling the instruction tuning dataset to 11.9M, FastVLM (R39) achieves superior performance over Cambrian-1 (R41) with 2.3\u00d7 fewer visual tokens, even on text-rich evaluations (see Tab. 10) that are sensitive to the number of visual tokens.\nEffect of Decoder. VLM performance also depends on the quality of LLM, as demonstrated in prior studies, like . By switching from Vicuna-7B (R19, R27) to Qwen2 models (R20, R28), we see a good improvement in performance across all the benchmarks. The improvements are significant on MMVet, LLaVA-in-the-wild and MMMU benchmarks. With Qwen2-0.5B as the LLM decoder, FastVLM (R3) matches the performance of LLaVA-OneVision (R2) on key benchmarks such as SeedBench, MMMU, and MMVet, while being 85\u00d7 faster and trained on 2.9\u00d7 fewer instruction tuning samples. This result underscores the quality of our vision encoder, as both models use the same LLM decoder, while FastViTHD is 3.4\u00d7 smaller compared to SigLIP-SO400M."}, {"title": "5. Conclusion", "content": "In this work, we introduced FastVLM, which leverages FastViTHD image encoders designed for enhanced resolution scaling while maintaining efficiency. By strategically trading off the costly self-attention in ViTs with a purpose-built hybrid architecture, FastViTHD processes high-resolution images efficiently, and outputs a substantially reduced number of visual tokens. The design of FastVLM enables competitive performance with prior works across a wide range of VLM benchmarks, while improving efficiency in both time-to-first-token and the number of parameters in the vision backbone. Rigorous benchmarking on an M1 MacBook Pro demonstrates that FastVLM achieves a state-of-the-art resolution-latency-accuracy trade-off compared to existing works."}, {"title": "A. Training Setup", "content": "For experiments presented in Tab. 1, Tab. 2, Tab. 4, Tab. 5, we perform 2-stage training with the hyperparameters listed in Tab. 7. The model is trained for a single epoch in all the stages. Note, in Tab. 5, we do not re-train other token pruning works, we simply report the performance of the respective methods as they adhere to the 2-stage training setup described in Tab. 7, which was originally introduced in LLaVA-1.5 [49].\nTo showcase our model's performance in the presence of additional dataset, we scale both pretraining and instruction tuning datasets in Sec. 4. For results presented in R13, R17, R18, R25, R26 in Tab. 6, we still perform 2-stage training described in Tab. 7, for R18 and R26, we use instruction tuning dataset of size 1.1 million samples in Stage-2. For results presented in R3, R4, R6, R7, R10, R11, R14, R19, R20, R21, R27, R28, R38 and R39, we scale-up both instruction tuning dataset and pretraining dataset. We also introduce and additional stage of pretraining with the scaled-up dataset as described in Tab. 8. Details of 1.1 million, 6.5 million and 11.9 million instruction tuning dataset is presented in Sec. D."}, {"title": "B. Architecture Details", "content": "The patch embedding layers shown in Fig. 2, consists of 7\u00d77 depthwise convolutions with style train-time over-parameterization, followed by 1\u00d71 pointwise convolution. The stride for 7\u00d77 depthwise convolution is set to 2 in order to downsample the input tensor. In , squeeze-excite layers were incorporated into this block; however, we found them to negatively impact inference latency, especially for high image resolutions, so we opted not to include them in our model. We use the same ConvFFN layer defined in , i.e. 7\u00d77 depthwise convolutions preceding a typical FFN layer. The stem downsamples the input tensor by factor of 4 on each side, and each patch embedding layer downsamples the input tensor by a factor 2. Although recent architectures like ViTamin recommend an overall downsampling factor of only 16, FastViTHD incorporates an additional patch embedding layer compared to FastViT, resulting in an overall downsampling factor of 64x for the input tensor. In each stage, we increase the number of channels by a factor of 2 as done in FastViT and other convolutional and hybrid transformer architectures. This results in a Stage-5 with the widest MLP layers in the architecture, performing self-attention on an input tensor which is downsampled by a factor of 64."}, {"title": "B.1. Naive Scaling", "content": "In order to scale the model size of FastViT, we simply increased the embedding dimensions per stage to [128, 256, 512, 1024], and set the number of layers per stage to [2, 12, 16, 6]. Patch embedding layers in each stage use squeeze-excite layers and the MLP expansion ratio is set to 3.0, following the design in ."}, {"title": "C. Additional Results", "content": "We present the performance of FastVLM on text-rich benchmarks under various training settings in Tab. 10. FastVLM surpasses MM1 and Cambrian-1 across a wide range of benchmarks by scaling up pretraining and instruction tuning datasets. This result highlights the quality of visual tokens produced by FastViTHD, as FastVLM is able to achieve these improvements with 2.8\u00d7 less visual tokens than MM1 and with a vision encoder that is 5.1\u00d7 smaller."}, {"title": "D. Datasets", "content": "For Stage-1 training, we only use LLaVA-1.5 558K dataset. For Stage-1.5 training, we use densely captioned versions of CC3M and CC12M introduced in . The total size of this dataset is 15 million image-text pairs. We generated 300 generic questions, such as \"What is in this photo?\". For each (image, dense-caption) pair, we randomly selected a generic question to form a triplet of (question, image, dense-caption). With a 0.5 probability, we placed the image's special token <image> either before or after the question. From recent works like and our results in Tab. 6, scaling dataset in Scale-1.5 is beneficial to improve the performance of VLM across a wide range of evaluations. Even though FastViTHD is smaller than ViT-L/14 and ViT-H used in respectively, we see similar scaling trends."}, {"title": "D.2. Visual Instruction Tuning Datasets", "content": "We use 3 different version of instruction tuning datasets. The smallest scale is LLaVA-1.5 665K dataset . We further scale up this dataset by including training splits of the following datasets; AI2D, ScienceQA, ChartQA, COCO, DocVQA, DVQA, GeoQA+, OCRVQA, SegmentAnything, SynthDoG-EN, TextVQA and Visual Genome . The conversational data for the listed datasets is sourced from . The total number of samples in this dataset is 1.1 million and is referred to as \"1.1M\" in all the tables. We further scale-up instruction tuning dataset using image-based conversational data from Cambrian-7M , which amounts to 5.4 million samples. Filtered Cambrian-7M is merged with \"1.1M\u201d dataset to obtain \u201c6.5M\u201d instruction tuning dataset. We then append all available single-image instruction tuning data open-sourced by LLaVA-OneVision to \"6.5M\u201d to obtain \"11.9M\u201d instruction tuning dataset. From Tab. 6, we see further improvements in VLM benchmarks when in-"}, {"title": "D.3. Evaluations", "content": "In addition to evaluations listed in Sec. 4, we report performance of FastVLM on ChartQA, OCRBench and InfoVQA to compare FastVLM against recent methods on text-rich benchmarks. In Tab. 11, report performance of FastViT model (with architectural interventions) from multiple training runs and compute the standard deviation of metrics reported in Tab. 6. As described in Sec. 4, for ablations we are interested in benchmarks that are quick to evaluate and exhibit lower variance to different initializations. From Tab. 11, GQA, TextVQA, POPE, DocVQA and SeedBench fit the criteria. While VQAv2 also exhibits lower variance it is substantially larger and takes long time to evaluate. The standard deviation across the selected metrics is below 0.5, so we use the average of these metrics as a reliable indicator for our analysis in Sec. 3."}]}