{"title": "NaVIP: An Image-Centric Indoor Navigation Solution for Visually Impaired People", "authors": ["Jun Yu", "Yifan Zhang", "Badrinadh Aila", "Vinod Namboodiri"], "abstract": "Indoor navigation is challenging due to the absence of satellite positioning. This challenge is manifold greater for Visually Impaired People (VIPs) who lack the ability to get information from wayfinding signage. Other sensor signals (e.g., Bluetooth and LiDAR) can be used to create turn-by-turn navigation solutions with position updates for users. Unfortunately, these solutions require tags to be installed all around the environment or the use of fairly expensive hardware. Moreover, these solutions require a high degree of manual involvement that raises costs, thus hampering scalability. We propose an image dataset and associated image-centric solution called NaVIP towards visual intelligence that is infrastructure-free and task-scalable, and can assist VIPs in understanding their surroundings. Specifically, we start by curating large-scale phone camera data in a four-floor research building, with 300K images, to lay the foundation for creating an image-centric indoor navigation and exploration solution for inclusiveness. Every image is labelled with precise 6DoF camera poses, details of indoor PoIs, and descriptive captions to assist VIPs. We benchmark on two main aspects: 1) positioning system and 2) exploration support, prioritizing training scalability and real-time inference, to validate the prospect of image-based solution towards indoor navigation. The dataset, code, and model checkpoints are made publicly available at https://github.com/junfish/VIP_Navi.", "sections": [{"title": "Introduction", "content": "Indoor navigation [1, 2, 3] remains a complex challenge due to the unavailability of satellite signals, such as GPS, for indoor localization. The dynamic nature of indoor environment scenes further complicates the positioning. While Wi-Fi-based indoor positioning systems (WIPS) [4, 5] offer some promise by utilizing signal strength measurements from multiple access points (APs), their effectiveness can vary. To improve positioning accuracy and stability, deploying bluetooth low energy (BLE) beacons [6, 7, 8] within buildings has been explored. These beacons emit audio signals that, when received by mobile devices, can aid in determining the user locations. However, this solution necessitates extensive tag hardware deployment and ongoing maintenance and sometimes interferes with other signals, often making it impractical due to resistance from building managers. Similar technologies, such as radio frequency identification (RFID) [9] and ultra-wideband (UWB) [10], have been refined to mitigate the inconvenience and further enhance accuracy; however, these also come with the same disadvantages of requiring tag installations and maintenance. Other alternatives like magnetic and acoustic positioning offer infrastructure-free solutions but lack robustness against the spatial and temporal variations inherent in indoor settings. Dead reckoning [11, 12] and SLAM [13, 14] encounter challenges with error accumulation in positioning during navigation and also demonstrate limited robustness to environmental changes. Current marker-based methods offer cost-effective solutions but struggle with the accuracy of positioning when tracking visual landmarks (e.g., fiducial markers [15, 16]).\nAll the technologies above have their place in indoor navigation solutions, often complementing each other within hybrid systems [17, 18, 19] that aim to leverage the strengths of each approach while mitigating their individual disadvantages. While the above mentioned options may already serve sighted individuals with a strong sense of direction, the challenges of indoor navigation is much greater for visually impaired people (VIPs) [20, 21, 22] who cannot independently explore unfamiliar indoor environments due to lack of access to existing wayfinding signage and the inability to create a mental map of complex layouts. For VIPs, challenges of independent indoor navigation is more than just an inconvenience; it can mean the difference between venturing into unknown indoor spaces or just avoiding them altogether. In summary, current indoor navigation technologies struggle to provide effective solutions, a necessity for VIPs, for dynamic environments where (1) accurate and real-time positioning, (2) scalability across varying scene sizes, and (3) understanding of the surroundings, are all required simultaneously. To address these concerns, we are pioneering the application of visual intelligence towards indoor navigation. However, our initial and most significant challenge is the absence of a large real-world dataset derived directly from indoor navigation scenarios. These scenarios involve rapidly changing scenes as users move, and continuously evolving environments over days. Therefore, we curated a image-based indoor navigation dataset to facilitate the straightforward implementation of feasible applications. To further support this, we also ensured that the dataset collection process is scalable and adaptable to changes in building layouts and scene sizes.\nThe key task in such a purely image-based navigation system is positioning, simply put as determining your camera location from a single image. Contemporary methods for this camera localization task exhibit different trade-offs among hardware resources, prediction accuracy, inference time, and algorithm robustness. For example, state-of-the-art (SOTA) accuracy in camera localization is achieved by constructing a 3D model of the scene using either sparse feature points [23, 24] or dense reconstruction [25, 26]. The camera pose is then estimated through geometric calculations [27, 28] or 2D-3D matching [29]. However, these methods typically require significant memory overhead and have slow processing speeds, making them impractical for integration into a navigation system. On the other hand, absolute pose regression (APR) [30, 31, 32] can determine the camera pose with a single forward propagation in deep neural networks using query images. This line of research offers significant advantages in inference speed and can be easily deployed in thin client applications due to its minimal memory footprint. It achieves this while only incurring an acceptable level of accuracy loss in positioning, which is suitable for real-time navigation purposes. Additionally, the APR decision process is aligned with other well-established computer vision (CV) tasks in the era of deep learning, such as image recognition [33, 34], semantic segmentation [35, 36], and depth estimation [37, 38]. Considering task scalability is crucial for the development of an inclusive and accessible indoor navigation system. In this paper, we not only benchmark APR but also demonstrate how image captioning can assist VIPs in navigating indoor environments independently for their own needs [39, 40]. Without the burden of human-crafted annotations, this is achieved by leveraging"}, {"title": "Related Work", "content": "Indoor Navigation\nCurrent full-fledged indoor navigation systems [1, 2, 44, 45] employ a set of technologies including WiFi fingerprints, BLE beacons, magnetic fields, and IMUs, either individually or in combination. These sensor technologies are specifically engineered to address the most challenging aspect of indoor navigation-accurate and robust positioning. [46] proposed a hybrid multi-sensor fusion system for indoor localization using WiFi and LiDAR. ViNav [18], named vision-based navigation system, relies on a combination of WiFi fingerprints, dead reckoning, and SfM 3D point clouds to localize user positions. ASSIST [47] is a personalized system using multimodal sensors and high-level semantic information, with efficacy tested on a blind and visually impaired (BVI) group. [48] proposed data-driven methods similar to ours but trained their localization model using annotated Wi-Fi observations instead of images. [49] explored the combination of radio and visible light communication-based positioning technologies.\nRecent advancements in vision-and-language navigation (VLN) [50, 51, 52] has facilitated the translation of natural language instructions into practical actions for embodied agents, leveraging their visual perceptions. This development is instrumental in assisting users who can command such agents for indoor applications, such as search-and-rescue missions [53]. However, most VLN methods [54, 55, 56, 57, 58, 59, 60, 61, 62] are largely limited to controlled, simulated 3D environments [63], which significantly narrows the applicability in real-world settings. Interactive VLN [64, 65, 66], synchronizing human feedback to adapt to new environment, can explore unknown command feasibility, but struggles in scenarios where the oracle is disabled. While trajectory-instruction generation [67, 68, 69, 70, 52, 71, 72, 73, 74] that synthesizes new language instructions can alleviate data scarsity, their efficacy still lags behind precious oracle instructions [75].\nVisual Localization\nThe problem of visual localization, aka camera pose estimation, is fundamental in many CV applications except navigation, such as augmented reality, SLAM system, and autonomous driving.\nIndirect localization casts the camera pose as a query frame retrieval problem [76, 77, 24]. Traditional methods [78] are dependent on the quality of feature detection and matching, and often require manual tuning to effectively retrieve the most similar images stored in a database or to interpolate the camera pose from top retrieved images. In contrast, deep learning approaches [24, 79, 80, 81] utilize hierarchical pipelines that establish 2D-3D correspondences more efficiently and robustly. Despite achieving SOTA accuracy, these methods, which often incorporate PnP and RANSAC processes, tend to be slower by an order of magnitude ranging from 10 to 100 times compared to APR methods. Furthermore, although relative pose regression (RPR) [82, 83, 84, 85, 86, 87] also adopts camera pose regression, the inherent retrieval phase continues to hinder inference efficiency."}, {"title": "NaVIP", "content": "Data collection and annotation\nThis section presents the methods used for collecting and annotating our large-scale image dataset. To make it easily extend this process to other buildings, we prioritize reducing human labour. Within our workflow, human involvement is confined to recording videos, pinpointing 5-10 images from each video to the floor plan, and designing prompts for GPT-4. We also release the data preprocessing and annotating code to further alleviate the burdens associated with data collection and annotation."}, {"title": "Collection", "content": "To facilitate the adaptation of this data-driven, vision-based indoor navigation solution across various buildings in the future, data collection is streamlined to minimize human effort and allow for automation through a basic robot that does not require specialized design. We hereby consider the video recording by smartphone built-in cameras and subsequent extraction of image frames. In this study, we collected approximately 400 videos, each lasting 2\u20134 minutes, within the Health, Science, and Technology (HST) Building. By setting the frame extraction interval between 0.2 and 0.3 seconds, we obtained a big dataset comprising around 300K images. The simplicity of our data collection allows for its easy way of expansion, either through additional video recordings or by narrowing the time intervals for frame extraction.\nTo enhance the robustness of algorithms developed from this dataset, videos were captured using four distinct smartphone models, in both landscape and portrait orientations. These recordings were made in two different holding ways: by human hand and using a smartphone gimbal stabilizer. To"}, {"title": "Camera Pose", "content": "There are multiple ways to annotate a camera with its accurate shooting position. Most methods require professional equipment [88, 99, 26], e.g., the NavVis VLX 3D scanner backpack, or involve complex optimization pipelines that are either closed-source or difficult to replicate [128, 129, 130]. We conclude the summary of camera relocalization datasets, both indoors and outdoors, in Table 1. It is noteworthy that only the Cambridge [30] relied on ubiquitous device and open-source software to obtain the 6-DoF ground-truth of camera pose and achieved a 10 dm error level outdoors. Based on this, we emphasize the importance of low-cost and replicable methods for obtaining ground-truth data for training, especially when our users include low-income groups with health disparities. We employ the active open-source project COLMAP to reconstruct the 3D scene and determine the camera pose for each image in our collections. The algorithm for 6DoF annotation process is presented in Algorithm 1.\nAs discussed in \u00a7 3.1.1, to capture the dynamic variations in scene evolution over time, we independently collected approximately 400 videos. These videos were recorded by various collectors using different gestures and devices at diverse times. Consequently, the world coordinates for each video are isolated within the COLMAP 3D reconstructions; they are not geo-registered and cannot be integrated into a unified world coordinate system that would align with a floor plan for navigation"}, {"title": "Pols", "content": "By aligning all images to the floor plan, annotating nearby points-of-interest (Pols) becomes straightforward. As illustrated in Figure 3, we can mark pixel-level Pol labels for each public area. Predicted images at specific points can then provide feedback on these Pols to users, depending on their camera positions and orientations. We release the detailed Pols for each pixel of the HST floor plans to support the development of applications."}, {"title": "Captions for VIPs", "content": "We leverage recent MLLMs, e.g., OpenAI GPT-4 and Google Gemini, to generate tailored image descriptions that meet the specific needs of VIPs. Our prompt design considers both the capabilities of these models and the feedback from VIP volunteers. For more details on our explorations of prompting MLLMs and output examples of image descriptions, please refer to Appendix B. We offer three types of image descriptions for each image:\n\u2022 A concise description suitable for general use.\n\u2022 A detailed description specifically designed for individuals with vision impairments (including those with low vision or acquired blindness).\n\u2022 Another detailed description tailored for individuals who have been blind since birth."}, {"title": "Benchmarking", "content": "To evaluate the generalization ability across ever-changing indoor environments, we utilize images captured prior to April 15th as training dataset and images captured after May 1st as testing dataset throughout the benchmarking process."}, {"title": "Camera Relocalization", "content": "Preliminary. For clarity and consistency, we represent the camera pose in 3D space by a 2-element tuple $[x, q]$ according to [30], where $x \\in \\mathbb{R}^3$ defines the position of camera center in 3D Cartesian coordinates and $q \\in \\mathbb{R}^4$ is the unit quaternion encoding its orientation. Other variations focus on orientation representations, with BranchPoseNet [136] employing an Euler angle representation and MapNet [137] using a logarithm of the unit quaternion.\nOur purpose is to directly regress the camera pose $[\\hat{x}, \\hat{q}]$ from a single monocular image $I$ using the trained function $f$. The standard objective loss function is defined as follows:\n$L(I) = ||x - \\hat{x}||_2 + \\beta \\cdot ||\\frac{q}{||q||} - \\frac{\\hat{q}}{||\\hat{q}||}||_2$                                                                                                       (1)\nwhere $[\\hat{x}, \\hat{q}] = f(I)$ represents the values predicted by our models. Notably, $\\beta$ is a hyperparameter introduced to balance the learning scales between position and orientation. Learnable PoseNet [91] captures homoscedastic uncertainty [138] between two tasks and omit $\\beta$ as\n$L(I) = e^{-s_x} \\cdot ||x - \\hat{x}||_2^2 + e^{-s_q} \\cdot ||\\frac{q}{||q||} - \\frac{\\hat{q}}{||\\hat{q}||}||_2^2 + s_x + s_q$,                                                                                                                                                (2)\nwhere $s_x$ and $s_q$ are both learnable and only an approximate initial guess is required.\nSettings. To ensure a fair comparison in benchmarking camera relocalization, we employed identical CNN architectures\u2014ResNet-34 [34] and MobileNet-V3 [140]\u2014across 13 models ranging from the pioneering PoseNet [30] to its latest advancements [104, 105]. All the above models were trained for 200 epochs. Specifically, for the PoseNet-series models, we utilized the loss function defined in Eq.(1), setting $\\beta$ to $e^5$ across all four floors. In the Bayesian PoseNet, uncertainty is incorporated only before the layers with randomly initialized weights, following [98]. LSTM-PoseNet models have the hidden size of all LSTM units set at 256 to achieve optimal results [99]. Learnable PoseNet starts with initial guesses of $s_x$ and $s_q$ set to 0 and -5, respectively, for all scenes [91], and then Geometric PoseNet continues training these models using geometric reprojection data to balance positional and rotational errors per image. We employed feature map concatenation in ResNet-34 and element-wise summation in MobileNet-V3 to combine features from the front layers and achieve the optimal results in Hourglass PoseNet [100]. For BranchNet-Euler6 [136], the"}, {"title": "Image Captioning", "content": "We can train a captioning model using outputs from GPT-4 to meet various VIPs needs. Image captioning requires fluent descriptions to translate images into natural language [141, 142]. We mix image data from four floors and use concise descriptions as ground truth to distill [143] our own models. We validate the results using BLEU-4 [144], METEOR [145], CIDEr [146], and SPICE [147] metrics. Additionally, we report the model size in the number of parameters and training time in GPU hours to indicate the feasibility and practicality of this visual intelligence support. The results in Table 3 demonstrate accurate predictions with smaller-size distilled models."}, {"title": "Potential Applications", "content": "APR methods development. Popular datasets commonly used for benchmarking APR methods include 7-Scenes [88] and Cambridge Landmarks [30]. These datasets, however, are limited in scope and size, which can lead to the overfitting phenomenon already observed in previous research [30, 91, 99, 100]. This limitation complicates the performance assessment of new proposed models, particularly in an era dominated by deep learning and PFMs. For instance, Bayesian PoseNet [98] demonstrates inferior performance compared to the classic PoseNet using our dataset. This discrepancy arises because any dropout rate applied to PoseNet constrains its capacity rather than serving its intended purpose of regularization.\nVLN test under real-world environments. We plan to release this comprehensive dataset that includes not only the original videos but also the floor plans annotated with PoIs. Each video can be segmented into various clips representing a unique navigation path. These clips are associated with an automated point-to-point (PoI-to-PoI) oracle that can be used for VLN model training. Moreover, since descriptions for each image will be available, exploring a VLN model based on PoI-to-PoI instruction that operates without the need for detailed step-by-step guidance in natural language appears promising, particularly in unknown real-world environments. Our dataset has the potential to significantly facilitate more robust and flexible navigation solutions, essential for navigating dynamic or unfamiliar spaces effectively."}, {"title": "Limitations and Discussion", "content": "Inherent biases of human data collection. In NaVIP, we collected video data via sighted individuals, which inevitably introduced biases from collectors themselves. To mitigate this and reduce human labor in developing such data-driven navigation systems, one possible solution is the use of a simple robot equipped with a phone camera that navigates randomly to gather video data. Nonetheless, both sighted individuals and robotic agents fail to capture the user needs and data distribution pertinent to VIPs. Inspired by VizWiz [121], involving VIPs as participants in the data collection process is beneficial as our primary objective is to assist VIPs in navigating unfamiliar environments. Efforts to bridge this limitation will not only enhance the practicality of the solutions but also ensure their genuine benefit to the intended users.\nHow to merge 3D reconstruction losslessly? In our dataset, we utilize COLMAP to reconstruct the 3D models of indoor environments along each video path. Given that indoor environments can be ever-changing, there currently lacks a robust algorithm to effectively merge these 3D point clouds from different time. As a workaround, we utilize human supervision to pinpoint several anchor images directly onto the floor plan. These annotated points facilitate the geo-registration process built in COLMAP to transform the coordinates of all 3D point clouds. While this alignment method is not lossless-resulting in an inevitable system error in camera poses of approximately 0.5 meters-we will release both the sparse and dense models generated by COLMAP, before and after geo-registration, to facilitate further research in this area. This contribution is expected to further enhance the accuracy of positioning system in indoor navigation.\nHow robust is the image-centric positioning solution? We acknowledge the limitations in our experimental analysis regarding the robustness of this image-centric positioning system. Although our dataset was collected directly from real-world scenarios using common mobile devices, the performance of this system under extreme conditions, e.g., electronic failures leading to dark environments or post-construction changes within buildings, remains unexplored. To simulate a distribution shift due to changes in the building environment, our dataset incorporates a temporal gap between the training and testing datasets. Despite this, we observe no decline in performance over time.\nIs GPT-4 ready for assisting VIPs? Although we meticulously designed prompts tailored to the needs of VIPs (refer to Appendix B for our exploration), we encountered challenges in meeting their personalized requirements. Key challenges include: 1) optimizing GPT-4 as an image descriptor for VIPs to enhance accuracy and eliminate misinformation, and 2) developing prompts that guide GPT-4 to generate information that aligns with user expectations. To further explore these issues, we have released the outputs of GPT-4 on our dataset of 300K images."}, {"title": "Conclusion and Future Work", "content": "In this paper, we have redirected our research from conventional sensor-based navigation systems to purely vision-based solutions for indoor environments. To facilitate this shift, we created a large image-centric dataset, named NaVIP, within the largest building at Lehigh University, the HST, specifically for research purposes. Our comprehensive pilot experiments on benchmarking APR methods by leveraging real-time end-to-end inference in deep neural networks have validated its feasibility and accuracy. This solution not only streamlines the system architecture but also enhances its applicability and usability, thereby extending its utility in assisting VIPs. The integration of image captioning models distilled from GPT-4 further highlights the potential to independent exploration for VIPs. As we look to the future, our research will focus on developing a mobile application that incorporates our trained model. We plan to test within the Lehigh community, supported by approval from the Institutional Review Board (IRB) to ensure comprehensive human feedback integration into the study. Moreover, we intend to explore advanced unsupervised learning techniques and reinforcement learning from human feedback (RLHF) to further enhance the functionality and user experience of our mobile application. The next phase of our research will concentrate on incorporating real-world user insights to refine and optimize the navigational aid, aiming to create a more adaptive and effective tool for end-users."}, {"title": "Experimental Settings and Additional Results", "content": "Learning settings\nFor the learning settings, we employed the Adam optimizer. The configurations of optimizer and other hyper parameters in our experiments are shown in Table 4."}]}