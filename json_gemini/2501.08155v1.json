{"title": "FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification", "authors": ["NURIT COHEN-INGER", "LIOR ROKACH", "BRACHA SHAPIRA", "SEFFI COHEN"], "abstract": "Algorithmic decision-making has become deeply ingrained in many domains, yet biases in machine learning models can still produce discriminatory outcomes, often harming unprivileged groups. Achieving fair classification is inherently challenging, requiring a careful balance between predictive performance and ethical considerations. We present FairTTTS, a novel post-processing bias mitigation method inspired by the Tree Test Time Simulation (TTTS) method. Originally developed to enhance accuracy and robustness against adversarial inputs through probabilistic decision-path adjustments, TTTS serves as the foundation for FairTTTS. By building on this accuracy-enhancing technique, FairTTTS mitigates bias and improves predictive performance. FairTTTS uses a distance-based heuristic to adjust decisions at protected attribute nodes, ensuring fairness for unprivileged samples. This fairness-oriented adjustment occurs as a post-processing step, allowing FairTTTS to be applied to pre-trained models, diverse datasets, and various fairness metrics without retraining. Extensive evaluation on seven benchmark datasets shows that FairTTTS outperforms traditional methods in fairness improvement, achieving a 20.96% average increase over the baseline compared to 18.78% for related work, and further enhances accuracy by 0.55%. In contrast, competing methods typically reduce accuracy by 0.42%. These results confirm that FairTTTS effectively promotes more equitable decision-making while simultaneously improving predictive performance.", "sections": [{"title": "1 Introduction", "content": "The widespread adoption of machine learning (ML) algorithms in critical decision-making processes, from hiring practices to judicial sentencing [5] and credit scoring [9], has raised significant concerns about fairness and bias in automated systems. Despite their promise, ML models often reinforce biases in training data, leading to discriminatory outcomes for disadvantaged groups. For instance, predictive policing algorithms have been criticized for racial profiling, and automated loan approval systems have exhibited gender and racial disparities [4, 21]. These issues underscore the urgent need for fairness-aware ML algorithms that can provide equitable outcomes without compromising predictive performance. Regulatory frameworks, such as the EU's General Data Protection Regulation (GDPR\u00b9) and guidelines from the U.S. Equal Employment Opportunity Commission (EEOC\u00b2), emphasize the importance of preventing algorithmic discrimination. Consequently, there is a growing demand for ML models that can make accurate predictions while adhering to fairness constraints, ensuring that protected groups are not disadvantaged by automated decisions. Achieving fairness in production ML presents several challenges:\n\u2022 Trade-off Between Accuracy and Fairness: Improving fairness often leads to a decrease in predictive accuracy, as the model's ability to fit the data is constrained by fairness considerations [20, 29], specifically in post-processing bias mitigation methods.\n\u2022 Dependence on Sensitive Attributes: Many fairness-enhancing methods require access to sensitive attributes during training or inference, which may not be feasible due to privacy concerns or legal restrictions [24].\n\u2022 Metric Specificity: Existing approaches are frequently tailored to specific fairness definitions (e.g., demographic parity, equalized odds), limiting their applicability across different contexts [21].\n\u2022 Model and Data Constraints: In practical settings, retraining models may not be possible due to proprietary models or limited access to the training data, necessitating non-invasive interventions that can adjust pre-trained models [6, 31, 36].\nThese challenges highlight the need for a flexible, generalizable approach that can enhance fairness without extensive retraining or specific metric dependencies."}, {"title": "1.1 Our Solution: FairTTTS", "content": "To address these challenges, we propose FairTTTS, a novel post-processing bias mitigation method designed to enhance fairness in classification tasks, particularly within decision tree models. Inspired by the TTTS method [14], originally developed to improve robustness against adversarial attacks, FairTTTS adapts this approach to mitigate biases and promote equitable outcomes.\nFairTTTS operates by introducing controlled randomness in the traversal paths of decision trees during inference. Specifically, it employs a Monte Carlo simulation-based technique that probabilistically flips decision paths at nodes involving protected attributes when unprivileged samples are directed towards unfavorable outcomes. This probabilistic adjustment is guided by a distance-based heuristic that considers the feature value's proximity to the decision threshold, ensuring that samples near the threshold have a higher chance of flipping, thus reducing potential biases encoded in the tree structure.\nBy decoupling fairness adjustments from the training process, FairTTTS offers these contributions:\n\u2022 Fairness- and Accuracy-Enhancing Method: We introduce FairTTTS, a post-processing bias mitigation method for ML classification tasks. Building upon the TTTS technique, which is designed to enhance accuracy, FairTTTS incorporates fairness considerations by probabilistically adjusting decision paths at protected attribute nodes. This dual focus enables FairTTTS to improve both fairness and accuracy, addressing a key limitation of many post-processing methods that typically involve trade-offs between these objectives.\n\u2022 Flexible Integration: FairTTTS can be easily integrated into production ML systems, making it suitable for scenarios where retraining is impractical or when access to sensitive attributes is limited. This ensures compliance with privacy regulations and circumvents legal restrictions.\n\u2022 Broad Applicability: Extensive experiments on seven benchmark datasets from different domains, demonstrate that FairTTTS achieves consistent improvements in fairness metrics (21% reduction in Equalized Odds Difference) while maintaining or improving accuracy (average 0.55% gain)."}, {"title": "1.2 Bias Mitigation in Machine Learning", "content": "Bias in machine learning can be mitigated by pre-processing, in-processing, and post-processing techniques[6]. Pre- processing techniques mitigate biases in training data by transforming protected attributes [11, 17, 28], learning fair representations[32], or adjusting protected attribute distributions[16, 19]. In-processing methods incorporate fairness constraints directly into the model training process [1, 35] or impacting the model's architecture [3]. Post-processing methods, such as FairTTTS, adjust outputs from pre-trained models, providing flexibility when retraining is infeasible."}, {"title": "1.2.1 Post-Processing Bias Mitigation Methods", "content": "Post-processing methods address fairness after a model has been trained, making them particularly suitable for proprietary or pre-trained systems. Recent survey [22], categorizes these methods into input correction, classifier correction, and output correction.\nInput correction methods modify testing data before inference by applying perturbations on the test sample [2]. Output Correction methods directly alter predicted labels. Reject option-based strategies [23] flip predictions near decision boundaries, prioritizing fairness for unprivileged groups. Confidence-based approaches [18] use thresholds to modify predictions, often incorporating group-specific thresholds [29]. Techniques like causal model adjustments [13] ensure fairness by aligning predictions with counterfactual worlds. Changing the label by understanding the Shapely values impact of the protected attribute [25]. A recent study presented a method that balances predictions with synthetic data [15].\nClassifier Correction methods adjust a trained model to improve fairness. Notable works include Hardt et al.'s Equal-ized Odds Post-Processing [21], which optimizes predictions thresholds. Methods such as decision tree relabeling [23] aim to balance fairness for decision trees.\nOur work builds on classifier correction techniques, a Monte Carlo-based method that probabilistically adjusts decision paths during inference. Unlike previous methods relying on fixed thresholds or heuristics, FairTTTS dynamically incorporates fairness considerations at decision nodes, offering greater adaptability across diverse fairness metrics."}, {"title": "1.3 Inspiration from TTTS Method", "content": "The Tree Test Time Simulation (TTTS) method [14] introduces a probabilistic modification to decision paths in trees to enhance robustness against adversarial examples. TTTS employs Monte Carlo simulations to traverse alternative paths in the tree, guided by a distance-based heuristic that considers the feature value's proximity to the decision threshold. The flip probability is given by:\n$P_{flip} = max(P_{max\\_threshold}, e^{\\frac{-|v-t|}{\\sigma t_0}})$\nwhere $v$ is the feature value, $t$ is the threshold at the node, $\\sigma$ is the standard deviation of feature values at the node, and $P_{max\\_threshold}$ is the maximum flip probability.\nWhile TTTS focuses on robustness, we adapt its distance-based probabilistic traversal mechanism to address fairness concerns. By increasing the probability of flipping at nodes involving protected attributes when unprivileged groups"}, {"title": "1.4 Measuring Bias in ML", "content": "Fairness metrics are classified into two main categories [12]: those based on outcome probabilities, such as Statistical Parity [10] and Disparate Impact [17], and those derived from the confusion matrix, including Equalized Odds, Equal Opportunity [21] and Accuracy Rate Difference [8]. A single scenario may be labeled fair by one metric and unfair by another [34]. In this study, we focus on two widely used metrics, as identified in the survey [12]: Disparate Impact (DI) from the outcome-based category and Equalized Odds Difference (EOD) from the confusion-matrix-based category."}, {"title": "2 Method", "content": null}, {"title": "2.1 Problem Formulation", "content": "Let $D = \\{(X_i, y_i)\\}_{i=1}^n$ be a dataset where $X_i \\in R^d$ represents the feature vector of the i-th instance, and $y_i \\in \\{0,1\\}$ is the corresponding binary label. A sensitive attribute $Z \\subset X$ divides the dataset into privileged (Z = 1) and unprivileged (Z = 0) groups.\nOur objective is to enhance both predictive accuracy and fairness for a given pre-trained classifier $M : R^d \\rightarrow \\{0, 1\\}$. This involves improving the model's ability to make accurate predictions while reducing disparities in outcomes between privileged and unprivileged groups. Specifically, we aim to:\n1. Maximize predictive accuracy, denoted as $A(M)$, which measures the proportion of correct predictions made by M.\n2. Simultaneously enhance fairness, denoted as $F(M)$, where fairness metrics such as disparate impact and equalized odds difference quantify the model's adherence to equity principles.\nFormally, our goal is to adjust the decision paths during inference to achieve maximum $A(M)$ and maximum $F(M)$.\nwhere $F (M)$ measures fairness improvements beyond satisfying minimum fairness constraints. This dual objective ensures that the method achieves a balance between accuracy and fairness, surpassing traditional approaches that prioritize accuracy while meeting fairness constraints as secondary objectives.\nThe proposed FairTTTS framework operates in a post-processing manner, making it applicable to various pre-trained models without requiring retraining. By probabilistically adjusting decision paths based on a distance-based heuristic and fairness considerations, FairTTTS ensures more equitable outcomes while maintaining or improving predictive performance."}, {"title": "2.2 Overview of FairTTTS", "content": "FairTTTS adapts TTTS to address fairness by introducing controlled randomness during the decision-making process, specifically targeting nodes involving protected attributes.\nThe key idea is to increase the probability of flipping the traversal direction at these nodes when an unprivileged sample is being directed toward an unfavorable outcome. This probabilistic adjustment aims to mitigate bias encoded in the tree structure, providing unprivileged groups with increased opportunities for favorable classification without retraining the model.\nAs illustrated in Figure 1, when the decision tree encounters a protected attribute node that directs unprivileged samples towards an unfavorable class, FairTTTS increases the flipping probability by a factor a. This adjustment ensures that a higher proportion of these samples have the chance to be redirected to a more favorable class. By doing so, the algorithm dynamically reduces biases that may have been encoded in the original tree structure, allowing unprivileged groups more equitable access to favorable outcomes without retraining the underlying model."}, {"title": "2.3 Formal Description of the Method", "content": null}, {"title": "2.3.1 Decision Tree Structure", "content": "Consider a decision tree T trained on D using standard algorithms (e.g., CART). Each non-leaf node n in T splits on a feature fn with a threshold tn, directing samples to its left or right child nodes based on the feature value."}, {"title": "2.3.2 Monte Carlo Simulations for Fairness", "content": "For a given test sample X, we perform S Monte Carlo simulations to traverse T. In each simulation, we probabilistically decide whether to follow the original traversal path or to flip to the opposite child at each node, based on a calculated flip probability Pflip."}, {"title": "Flip Probability Calculation", "content": "At each node n, we compute the flip probability $P_{flip} (n, X)$ using a distance-based heuristic:\n$P_{flip} (n, X) = min(\\frac{(P_{max}  \\mid X_{f_n} - t_n \\mid)}{\\delta_{max}}, P_{max})$\nwhere:\n\u2022 $X_{f_n}$ is the value of feature $f_n$ in sample X.\n\u2022 $\\delta_{max}$ is the maximum possible distance between feature values and thresholds in the tree, ensuring $P_{flip} \\geq 0$.\n\u2022 $P_{max}$ is the maximum flip probability, set as a hyperparameter (e.g., 0.1).\nFairness Adjustment at Protected Nodes. If node n splits on the protected attribute Z, and the sample X belongs to the unprivileged group ($X_Z$ = 0), we adjust the flip probability when the traversal is directing X towards the unfavorable class y = 0. Specifically, we increase $P_{flip}$ by a factor a > 1:\n$P_{flip} (n, X) = min(\\alpha  P_{flip} (n, X), 0.5)$.\nThis adjustment ensures that unprivileged samples have a higher chance of being redirected towards a favorable outcome at critical decision points."}, {"title": "2.3.3 Traversal Algorithm", "content": "During each simulation, we traverse the tree from the root node according to Algorithm 1."}, {"title": "2.3.4 Aggregation of Predictions", "content": "After S simulations, we obtain a set of predicted classes {\u01771, \u01772,..., \u0177s} for sample X. The final predicted probability for class c is computed as:\n$P(\\hat{y} = c  X) = \\frac{1}{S} \\sum_{s=1}^S I[\\hat{y}_s = c]$\nwhere I[] is the indicator function."}, {"title": "2.4 Rationale Behind the Method", "content": null}, {"title": "2.4.1 Why We Use Probabilistic Traversal", "content": "Decision trees can encode biases present in the training data, particularly when splitting on protected attributes. By introducing randomness in the traversal paths, we can disrupt patterns that lead to unfair treatment of unprivileged groups."}, {"title": "2.4.2 Role of Distance-Based Heuristic", "content": "The distance-based heuristic ensures that the probability of flipping is higher when the feature value is close to the decision threshold, indicating uncertainty in the decision. This aligns with the intuition that near-threshold decisions are more susceptible to bias."}, {"title": "2.4.3 Fairness Enhancement through a Adjustment", "content": "Increasing the flip probability by a factor a at protected attribute nodes for unprivileged samples directed towards unfavorable outcomes actively counteracts the bias. It provides these samples with additional opportunities to reach favorable leaf nodes, improving fairness metrics."}, {"title": "2.5 Theoretical Considerations and Fairness Guarantees", "content": "Although decision trees are discrete structures and flipping decisions introduces stochasticity, we can provide intuition and partial theoretical grounding for why FairTTTS improves fairness. Consider a binary classification setting with a sensitive attribute Z. Suppose the original decision tree is fixed and let \u0176 be the random variable denoting the predicted label of a sample X. Without loss of generality, assume higher flips at protected nodes reduce the expected rate at which unprivileged samples are directed to Y = 0 leaves.\nKey Idea: The probability of flipping a decision at a protected node effectively shifts the local decision boundary around that node. By selectively increasing flip probabilities for unprivileged samples near the decision threshold, we create a locally \"more inclusive\u201d region where unprivileged individuals have a higher probability of receiving favorable outcomes. This translates to a reduction in group-level disparity.\nExample: Consider a single protected-attribute node with threshold t splitting an unprivileged group U. Let \u0176 be the prediction without flipping and \u0176' the prediction after flipping. If Xf is uniformly distributed near t for samples in U, increasing Pflip by a increases the probability that a sample close to t will be assigned to the branch leading to Y = 1 leaves. Hence, for these samples,\nP(\u0176' = 1  Z = 0) > P(\u0176 = 1  Z = 0) + \u0394,\nfor some \u25b3 > 0 dependent on a and the distribution of Xf near t. When combined over multiple nodes, these local shifts can globally improve fairness metrics like disparate impact or reduce the difference in TPR/FPR between groups.\nWhile this argument is heuristic, it suggests that under mild assumptions (e.g., local continuity of feature distributions, reasonable threshold assignments), increasing flip probabilities at protected nodes can raise the likelihood of favorable outcomes for the unprivileged group. In turn, this reduces disparities at the aggregate level."}, {"title": "3 Experimental Setup", "content": null}, {"title": "3.1 Datasets", "content": "We evaluate FairTTTS on seven distinct datasets commonly used in fairness research across eight total experiments (splitting the Adult dataset by race and sex) as covered in Table 1:"}, {"title": "3.2 Evaluation Metrics", "content": "To assess the performance of FairTTTS, we use the following metrics:\n\u2022 Equalized Odds Difference (EOD): Equalized Odds aims to ensure that both privileged (priv) and unprivileged (unpriv) groups have similar true positive rates (TPR) and false positive rates (FPR). Let $TPR_{priv}$, $TPR_{unpriv}$, $FPR_{priv}$, and $FPR_{unpriv}$ denote these rates for the privileged and unprivileged groups, respectively. We define EOD as:\n$EOD = \\frac{|TPR_{unpriv} \u2013 TPR_{priv} + FPR_{unpriv} - FPR_{priv}|}{2}$\nLower EOD values indicate better fairness.\n\u2022 Disparate Impact (DI): Disparate Impact measures the ratio of favorable outcome probabilities between unprivileged and privileged groups. Let P(Y = 1 | unpriv) and P(Y = 1 | priv) denote the probabilities of a positive (favorable) outcome for the unprivileged and privileged groups, respectively. Then:\n$DI = \\frac{P(Y= 1  unpriv)}{P(Y = 1  priv)}$\nA DI close to 1 indicates fairness, while values significantly below 1 indicate potential bias.\n\u2022 Accuracy: Accuracy is the proportion of correctly classified instances out of all instances. Let TP be the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives. Then:\n$Accuracy = \\frac{TP + TN}{TP+TN+FP + FN}$\nHigher accuracy values indicate better predictive performance."}, {"title": "3.3 Comparison Methods", "content": "We compare FairTTTS with the following methods:\n\u2022 Baseline: Standard random forest classifier without fairness interventions.\n\u2022 ThresholdOptimizer [21]: Post processing method that adjusts decision thresholds of the protected attribute to improve fairness."}, {"title": "3.4 Model", "content": "For the evaluation of FairTTTS, we tested the method on both Random Forest and standalone Decision Tree classifiers. Random Forests are widely used due to their strong predictive performance and interpretability, making them an ideal candidate for demonstrating the effectiveness of our approach. Similarly, Decision Trees offer a simpler yet highly interpretable architecture, further showcasing the versatility of FairTTTS across different decision tree-based models.\nSince FairTTTS operates directly on the decision tree structure, its application is straightforward: the method identifies splits involving protected attributes and probabilistically adjusts decision paths during inference. This ensures improved fairness without retraining or modifying the underlying model architecture. Importantly, FairTTTS is not restricted to these models. It is applicable to any decision tree-based architecture, including Gradient Boosted Trees and frameworks like LightGBM and XGBoost."}, {"title": "3.5 Implementation Details", "content": null}, {"title": "3.5.1 Hyperparameters", "content": "For FairTTTS, we set: Number of simulations S = 100. Maximum flip probability Pmax = 0.1. Fairness adjustment factor a = 9.0. These hyperparameters were chosen based on preliminary experiments to balance accuracy and fairness."}, {"title": "3.5.2 Additional Parameters", "content": "For each dataset, we define:\n\u2022 Protected Attribute: As specified in the dataset description.\n\u2022 Privileged Group: Samples where the protected attribute equals 1.\n\u2022 Unprivileged Group: Samples where the protected attribute equals 0.\n\u2022 Favorable Class: The positive class label (y = 1).\n\u2022 Unfavorable Class: The negative class label (y = 0)."}, {"title": "3.6 Experimental Process", "content": "(1) Data Preprocessing: We handle missing values, encode categorical variables, and split each dataset into 5 folds cross-validation.\n(2) Model Training: Train the Random_Forest_Baseline on the training set.\n(3) Fairness Enhancements:\n\u2022 Apply ThresholdOptimizer to the baseline model.\n\u2022 Implement FairTTTS by wrapping the trained decision tree with our Monte Carlo traversal method.\n(4) Evaluation: Predict on the test set using each method and compute the evaluation metrics and variances."}, {"title": "4 Results", "content": "We evaluated FairTTTS against the Baseline (RandomForest) and ThresholdOptimizer across eight experiments. These experiments were conducted on datasets with various sensitive attributes (e.g., race, sex, age): ADULT_RACE, ADULT_SEX,\nBANK_AGE, COMPAS_RACE, CREDIT_SEX, DIABETES_AGE, MIMIC_SEX, and RECRUIT_SEX. Our primary fairness metrics were Equalized Odds Difference (EOD) and Disparate Impact (DI), followed by Accuracy as a secondary metric.\nTable 2 displays the mean and standard deviation (Mean \u00b1 STD) for EOD, DI, and Accuracy across all experiments. For each experiment, the best-performing method in terms of EOD, DI, or Accuracy is shown in bold. Additionally, each cell associated with FairTTTS is color-coded based on its performance relative to both the baseline and ThresholdOptimizer:\nGreen cell: FairTTTS outperforms both the baseline and ThresholdOptimizer on that metric."}, {"title": "4.1 Key Observations", "content": "Fairness improvements, measured primarily by EOD and supported by DI, was our main focus. Across all eight experiments FairTTTS reduced EOD compared to the baseline in all eight cases. In seven of these, FairTTTS also outperformed ThresholdOptimizer. On average, FairTTTS improved fairness (indicated by reduced EOD) by 21% over the baseline, while ThresholdOptimizer improved it only by 18.77%. FairTTTS improved DI over the baseline in seven out of eight experiments. In three of these, it also surpassed the ThresholdOptimizer.\nEOD reductions confirm that FairTTTS is providing more equitable decision-making by ensuring that privileged and unprivileged groups face more similar error rates. DI improvements signal a more balanced distribution of positive outcomes. Together, these fairness metrics, underscore that FairTTTS is meeting its primary objective of enhancing fairness in decision-making.\nAccuracy improvements were also observed: FairTTTS increased Accuracy over both the baseline and Thresh-oldOptimizer in five of the eight experiments, on average improving Accuracy by 0.55% compared to the baseline. In contrast, ThresholdOptimizer often reduced Accuracy by 0.42%.\nRunning Time Results: We evaluated inference speed for the Random_Forest_Baseline, ThresholdOptimizer, and FairTTTS methods across eight tabular datasets and protected attributes. For the baseline and ThresholdOptimizer methods, total inference time across the entire dataset was typically around 0.2 seconds for the Adult dataset and even less for smaller datasets. FairTTTS, however, required around 52 seconds in total on Adult-roughly 200 to 250 times longer than the other methods. Although this may appear substantial at first glance, it is important to note that the datasets are tabular, which generally means that even these extended running times remain small per individual sample. For instance, the 52-second total on the Adult dataset translates to about 1.6 milliseconds per inference. In most real-world scenarios where tabular data processing does not require extensive computational resources, this overhead is still negligible. For use cases where fairness is a critical requirement, the slight latency increase per sample may be a reasonable trade-off."}, {"title": "4.2 Summary of Results", "content": "In summary, Table 2 clearly shows that FairTTTS delivers substantial improvements in fairness over both the baseline and ThresholdOptimizer in most experiments. It achieves these gains while often maintaining or increasing Accuracy. The consistent patterns confirm that FairTTTS is a practical, effective post-processing technique for achieving more equitable decision-making in decision tree models."}, {"title": "4.2.1 Consistency and Robustness", "content": "FairTTTS demonstrates consistent improvements in both fairness and accuracy across diverse datasets, sensitive attributes (race, sex, age), and varying data conditions such as class imbalance. It frequently outperforms the baseline and ThresholdOptimizer, and remains competitive even when not surpassing the latter. Overall, FairTTTS reduces bias (lower EOD), improves outcome balance (higher DI), and often boosts accuracy, confirming its versatility and robustness."}, {"title": "4.2.2 Comparative Analysis with Related Work", "content": "Most post-processing bias mitigation methods, such as ThresholdOptimizer, improve fairness metrics but often at the cost of accuracy. In our experiments, the ThresholdOptimizer method improved fairness metrics in some cases but resulted in an average accuracy drop of 0.42%. In contrast, FairTTTS achieved superior fairness in seven out of eight experiments while also maintaining or improving accuracy. In the one experiment where ThresholdOptimizer outperformed FairTTTS on the fairness metric, FairTTTS delivered better accuracy.\nFigure 2 illustrates the trade-offs between accuracy and Equalized Odds across experiments (combinations of datasets and protected attributes) for Random Forest. Methods are distinguished by color. Unlike traditional methods that often require sacrificing accuracy for fairness, FairTTTS leverages its foundation in the TTTS technique, which is inherently designed to optimize accuracy. As a result, FairTTTS effectively balances fairness and accuracy across a variety of datasets, demonstrating its robustness and generalizability in diverse scenarios.\nFairTTTS is applicable to any decision tree model architecture. The experimental results demonstrate the method's superiority in mitigating bias while also improving accuracy, particularly when applied to Decision Tree models, as illustrated in Figure 3."}, {"title": "4.3 Sensitivity Analysis", "content": "To evaluate the impact of the fairness adjustment factor a on FairTTTS, we performed a detailed sensitivity analysis across all datasets, presented in Figure 4. The parameter a controls the probability of flipping decisions at nodes associated with protected attributes, influencing both fairness and accuracy. By varying a from low to high values, we examined its effects comprehensively.\nThe results indicate that FairTTTS generally maintains stable accuracy performance across different a settings. Moderate a values consistently provide the best trade-off between fairness and accuracy, achieving significant reductions in Equalized Odds Difference without adversely affecting accuracy. At lower a values, the model exhibits modest but meaningful fairness improvements over the baseline. As a increases, disparities between groups diminish more significantly, leading to enhanced fairness as evidenced by lower EOD values. Importantly, these gains in fairness typically do not come at a steep cost to accuracy; in many cases, accuracy remains stable or improves slightly at moderate a levels.\nHowever, pushing a to extremely high values does not necessarily lead to continuous improvements. Overly aggressive flipping probabilities can introduce too much randomness. This suggests that, while FairTTTS offers a strong mechanism to enhance fairness, excessively large a values may lead to diminishing returns.\nOur analysis further reveals that the optimal a value depends on the dataset. While some datasets reach a favorable trade-off at lower a settings, others require fine-tuning to achieve similar outcomes. Practitioners are advised to begin with moderate a values and adjust based on the specific fairness requirements and accuracy thresholds of their application domain."}, {"title": "5 Conclusion", "content": null}, {"title": "5.1 Limitations and Future Work", "content": "While FairTTTS is effective for enhancing fairness, its current design requires direct traversal of the decision tree structure. Consequently, the model's architecture must be accessible, limiting its application to black-box models where the internal tree structure is not exposed. Future work may focus on adapting FairTTTS for other model classes, such as neural networks, or developing techniques that approximate decision paths in models lacking explicit tree structures. Additionally, exploring alternative fairness metrics, refining the probabilistic adjustment mechanisms, and studying the method's behavior under shifting data distributions could further improve performance and broaden its applicability."}, {"title": "5.2 Social Impact and Ethical Considerations", "content": "FairTTTS can help mitigate biases affecting marginalized communities by improving fairness metrics at inference time, potentially leading to more equitable outcomes in areas like credit, hiring, or healthcare. However, no single technique can eliminate all discrimination, and fairness definitions vary across contexts. Domain expertise, stakeholder input, and regulatory guidance remain crucial when selecting protected attributes, interpreting metric improvements,"}]}