{"title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models", "authors": ["Md Zarif Hossain", "Ahmed Imteaj"], "abstract": "Vision-language models (VLMs) have achieved significant\nstrides in recent times specially in multimodal tasks, yet they\nremain susceptible to adversarial attacks on their vision com-\nponents. To address this, we propose Sim-CLIP, an unsuper-\nvised adversarial fine-tuning method that enhances the ro-\nbustness of the widely-used CLIP vision encoder against such\nattacks while maintaining semantic richness and specificity.\nBy employing a Siamese architecture with cosine similarity\nloss, Sim-CLIP learns semantically meaningful and attack-\nresilient visual representations without requiring large batch\nsizes or momentum encoders. Our results demonstrate that\nVLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder\nexhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed\nimages. Notably, Sim-CLIP does not require additional train-\ning or fine-tuning of the VLM itself; replacing the original\nvision encoder with our fine-tuned Sim-CLIP suffices to pro-\nvide robustness. This work underscores the significance of\nreinforcing foundational models like CLIP to safeguard the\nreliability of downstream VLM applications, paving the way\nfor more secure and effective multimodal systems.", "sections": [{"title": "1 Introduction", "content": "The remarkable success of large language models (LLMs)\nin understanding and generat-\ning human-like text has inspired researchers to extend their\ncapabilities to the visual domain. This has led to the devel-\nopment of Vision-Language Models (VLMs) which aim to bridge the gap be-\ntween visual and textual information. To extract meaning-\nful visual representations from images, these VLMs often\nleverage pretrained vision encoders, such as CLIP, BEiT, and DINO. Leveraging pretrained vision encoders empowers\nVLMs to capitalize on the rich visual knowledge acquired\nthrough extensive pretraining on diverse image datasets.\nThis capability enhances the VLMs' performance across\nvarious downstream tasks, eliminating the requirement for\ntask-specific fine-tuning. Among these pretrained vision en-\ncoders, CLIP gained significant atten-\ntion and is widely used in many state-of-the-art VLMs such"}, {"title": "2 Related Works", "content": "Vision Language Models. In recent times, various VLMs\nhave emerged, such as LLaVA, Flamingo, OpenFlamingo (OF), and Macaw-LLM, to name a few.\nThese models leverage the power of multi-modal learn-\ning by combining visual and textual data, thereby enhanc-\ning their capability to understand and generate responses to\nmulti-modal inputs. Most of these VLMs employ pre-trained\nLLMs with a large-scale\nvision encoder, such as CLIP to pro-\ncess multi-modal data. During training, the vision encoder is\nkept frozen, while the model learns the interaction between\nmodalities. This interaction is typically facilitated by com-\nbining a projection layer with cross-attention mechanism.\nAdversarial robustness in traditional ML. The suscep-\ntibility of traditional machine learning models (e.g., CNNs,\nRNNs) to adversarial attacks is well-established and has\nbeen extensively studied. Most of the existing adversarial at-\ntacks predominantly target monomodal models, focusing on\neither text or image modality. In the image domain, gradient-based adversar-\nial attacks can deceive a target model by applying slight\nperturbations to the input images that are imperceptible to\nhuman eyes. Alternatively, patch-based attacks strategically place adversarial patches to deceive mod-\nels, rather than modifying the entire image. Text-based ad-\nversarial attacks are\nalso thoroughly investigated, requiring a different approach\ndue to the discrete nature of textual data. Adversarial train-\ning has emerged as an ef-\nficient defense mechanism against aforementioned attacks,\nwhich involves training the model with adversarial examples\nto improve its robustness."}, {"title": "Adversarial robustness for VLMs.", "content": "Given CLIP's re-\nmarkable performance in zero-shot settings and understand-\ning visual tasks, most VLMs employ it or its variants as their\nvision encoder. Few recent studies demonstrate CLIP's vulnera-\nbility to imperceptible attacks, which can significantly im-\npact downstream task performance of VLMs. For instance,\nin AdvCLIP, the authors generate univer-\nsal patches for CLIP models that can deceive VLMs across\nall of their downstream tasks. In the au-\nthors leverage diffusion models to create adversarial sam-\nples that manipulate the model into generating a targeted\noutput. Moreover, in, the au-\nthors demonstrate the potential of gradient-based attacks on\nVLMs, compelling the model to generate inaccurate results.\nOne recent study introduced a supervised\nadversarial fine-tuning scheme for CLIP that employs cross-\nmodal image-text contrastive loss. A few concurrent works\nproposed unsupervised\nadversarial training methods based on SimCLR contrastive\nloss. However, these methods necessitate a large batch size\nto achieve strong robust performance, making them unsuit-\nable for integration into VLMs. The authors in proposed an adversarial fine-tuning scheme based on\nBYOL, which addresses the issue of\nlarge batch sizes but introduces an overhead with the mo-\nmentum encoder. Besides, the authors in proposed an $\\ell2$ loss-based unsupervised fine-tuning\nscheme, but it fails to capture semantic features from the\nimages effectively. In contrast, our unsupervised fine-tuning\napproach, based on a siamese network, utilizes cosine simi-\nlarity to effectively capture semantic information during ad-\nversarial training without requiring a large batch size or an\nadditional momentum encoder."}, {"title": "3 Methodology", "content": "In this section, we first discuss how existing adversarial at-\ntacks for VLMs are formalized. Then we delve into the\nspecifics of existing unsupervised adversarial training meth-\nods and their limitations, followed by the details of our pro-\nposed framework, Sim-CLIP."}, {"title": "3.1 Adversarial Attacks on VLM", "content": "In gradient-based adversarial attacks (e.g., PGD, APGD), malicious attackers typically add slight perturba-\ntion $\\delta$ to an input image $x$. This perturbation is carefully\ncalculated for a specific target model, represented as $f_{\\theta}$,\nwhere $\\theta$ denotes the model's parameters. The core mecha-\nnism of these attacks involves leveraging the gradients of\nthe model's loss function (e.g., cross-entropy) $L$, with re-\nspect to the input image $x$. By exploiting these gradients,\nattackers are able to craft adversarial examples, denoted as\n$x_a = x + \\delta$. These adversarial samples are imperceptible to\nhuman eyes yet possess the ability to mislead deep learning\nmodels such as $f_{\\theta}$ towards misclassification:\n$x_a = \\text{argmax} L(x_a, y_{\\text{true}}), ||x_a - x||_p \\leq \\epsilon$.  (1)"}, {"title": "3.1.1 Targeted attack.", "content": "In a targeted attack, the adversary\nmanipulates the VLM to generate a specific incorrect pre-\ndiction by providing perturbed query samples. The adver-\nsary perturbs a query image $q$ by adding slight perturbation\n$\\delta q$, resulting in a perturbed sample $q + \\delta q$. When this per-\nturbed sample is fed into the VLM, it generates the desired\nmalicious output $y_i$, such as spam links or false information.\nThe targeted attack is formulated as an optimization problem\nthat aims to minimize the negative log-likelihood of generat-\ning the desired incorrect output $y_i$, given the perturbed query\nimage is $q + \\delta q$. This can be formalized as follows:\n$\\min_{\\delta q} - \\sum_{l=1}^m \\log p (y_i, q + \\delta q) \\quad \\text{s.t.} \\quad ||\\delta q||_{\\infty} \\leq \\epsilon_q$. (2)\nHere, the goal is to find the optimal $\\delta q$ while ensuring the\nperturbation is bounded by $\\epsilon_q$ in the $\\ell_\\infty$ norm. Note that, by\nminimizing the negative log-likelihood, we ensure that the\nprobability of the target token is maximized."}, {"title": "3.1.2 Untargeted attack.", "content": "In contrast to targeted attacks,\nuntargeted attacks seek to induce any form of error, without\nspecifying a particular target. The objective of an untargeted\nattack can be expressed as maximizing the likelihood of the\nmodel producing any output that differs from the correct or\nexpected output. The mathematical formulation for an un-\ntargeted attack can be represented as follows:\n$\\max_{\\delta q} \\sum_{l=1}^m \\log p (y_\\ell, q + \\delta q) \\quad \\text{s.t.} \\quad ||\\delta q||_{\\infty} \\leq \\epsilon_q$  (3)\nHere, $\\delta q$ denotes the perturbation to the query image $q$, with\nthe aim of maximizing the likelihood of the model out-\nputting any incorrect prediction $y_r$ that deviates from the\ncorrect output $y$."}, {"title": "3.2 Unsupervised Adversarial Fine-Tuning", "content": "We propose an unsupervised adversarial fine-tuning ap-\nproach for the CLIP vision encoder that aims to enhance its\nrobustness against adversarial attacks. Our methodology in-\ncorporates unsupervised learning techniques by leveraging\nsiamese architecture. Our goal is to enhance the resilience\nof the vision encoder against adversarial attacks while pre-\nserving the semantic meaning of the image features.\nPreviously, FARE introduced an\nunsupervised adversarial fine-tuning scheme for the CLIP\nvision encoder that does not rely on text embedding. FARE\nachieves its robustness by minimizing a $\\ell2$ loss function\nbetween an adversarially perturbed image embedding and\na clean image embedding. The embedding loss can be ex-\npressed as follows:\n$L_F(x, x_a) = \\max_{||x_a - x||} ||\\theta(x_a) - \\theta(x)||_2$ (4)\nThis loss encourages the embedding $\\theta(x_a)$ of perturbed im-\nages $z$ to stay close to the original embedding $\\theta(x)$. Al-\nthough FARE demonstrates robust performance against ad-\nversarial attacks, it exhibits two major issues. First, the\n$\\ell2$ loss employed by FARE may not be the most suitable"}, {"title": "3.3 Adversarial Fine-tuning with Siamese\nArchitecture", "content": "Sim-CLIP tackles the challenges present in FARE by tailor-\ning cosine similarity loss within a siamese architecture. Dur-\ning our adversarial fine-tuning phase, Sim-CLIP first gener-\nates a perturbed view $x'$ from the clean input image $x$. We\nutilize PGD perturbation to generate the perturbed view:\n$X_{t+1} = \\Pi_{X+\\epsilon}(x_t + \\alpha \\cdot \\text{sign}(\\nabla_x L(x_t, y)))$\n$s.t. ||x' - x|| \\leq \\epsilon$\n(5)\nHere, $y$ represents the true label of the input image and $L$\nis a cross-entropy loss. At each iteration $t$, a perturbation is\ncalculated based on the gradient of the loss function with\nrespect to the input image $x$. The magnitude of this pertur-\nbation is controlled by a step size parameter $\\alpha$, which de-\ntermines the strength of the perturbation applied in each it-\neration. This perturbation is constrained within a specified\nbound $\\epsilon$. Subsequently, both clean and perturbed views are\nfed into the CLIP models, which share the same weights,\nas depicted in Figure 2(b). The CLIP models generate clean\nrepresentation $R_c$ from the original image $x$ and perturbed\nrepresentation $R_p$ from the perturbed view $x'$. Now, we\nmaximize the similarity between the perturbed and clean\nrepresentations to encourage the model to learn features that\nare invariant to adversarial perturbations. To this end, we\nminimize negative cosine similarity between the represen-\ntations $R_p$ and $R_c$ to ensure their alignment in the vector\nspace, enhancing model coherence and robustness to adver-\nsarial perturbations:\n$\\text{CosSim}(R_p, R_c) = \\frac{R_p \\cdot R_c}{||R_p||_2 ||R_c||_2}$. (6)\nHere, $||.||_2$ denotes the $\\ell2$ norm, which is equivalent to min-\nimizing mean squared error between $\\ell2$-normalized vectors."}, {"title": "3.4 Symmetric Loss Collapse Prevention", "content": "Minimizing this negative cosine loss directly might lead to\na collapse of the loss function, yielding an undesirable con-\nstant solution Conventional unsupervised\nadversarial training methods attempt to address this issue by relying on negative\nsamples or momentum encoders. However, integrating neg-\native samples often necessitates larger batch sizes, leading to\nincreased computational burden, while momentum encoders\nintroduce extra overhead. To prevent the loss function from\ncollapsing while mitigating the resource burden, we inte-\ngrate a stop-gradient mechanism following into our adversarial fine-tuning loss. We employ a\nsymmetric loss for our adversarial training scheme, which\ncan be defined as follows:\n$L_{\\text{simclip}} (R_p, R_c) = \\frac{1}{2} (\\text{CosSim}(R_p, \\text{stopgrad}(R_c))\n+ \\text{CosSim}(R_c, \\text{stopgrad}(R_p)))$ (7)\nHere, one CLIP model's output is held constant using stop-\ngradient, while the other model's output is projected to align\nwith it. Subsequently, the roles of the two models are re-\nversed, with the first model's output being projected and\nmatched to the \"constant\" output of the second model. This\nimplies that in the first term of Equation 7, $R_p$ does not re-\nceive any gradients from $R_c$ since $R_c$ is treated as constant.\nHowever, in the second term of the same equation, $R_c$ re-\nceives gradients from $R_p$ as the stop-gradient operation is\nnow applied to $R_p$ instead. This process results in two losses,\nwhich are then averaged and minimized for optimization. In\nsummary, each model receives gradients for the loss term,\nexcept for cases where stop-gradient is not applied to its\noutput. The effectiveness of the stop-gradient mechanism in\npreventing loss function from collapsing is further validated\nthrough ablation study in the Appendix."}, {"title": "4 Experimental Analysis", "content": "We evaluate our adversarially fine-tuned robust CLIP model\nacross various downstream tasks, including zero-shot classi-\nfication. To evaluate performance on downstream tasks, we\nintegrate robust clip models into VLMs by substituting their\ndefault vision encoders.\nVLM models. We use OpenFlamingo 9B (OF) and LLaVA-1.5 with 7B param-\neters as our target VLMs. They both utilize CLIP ViT-L-14\nas their vision encoder. However,\nthey differ in their language decoders. OF employs MPT-7B,\nwhile LLaVA-1.5 utilizes Vicuna. During our evaluation, OF is provided only\nwith context text alongside the query image. In contrast,\nwe utilize both the default system prompt and task-specific\nprompts along with the query images for LLaVA.\nDatasets. We considered a wide range of datasets for\ndown-stream tasks, specifically for image captioning and\nVQA. For image captioning, we use the COCO and Flickr30k datasets, while\nfor visual question answering, we employ VizWiz and OKVQA. Addition-\nally, we assess the robustness of our adversarially fine-tuned\nCLIP model on the zero-shot classification task using the\nCIFAR10, CIFAR100, Eu-\nroSAT, PCAM, and Flower datasets. Our evalu-\nation process employs two approaches: for adversarial eval-\nuation, we randomly select 500 images from each respective\ndataset, while for clean evaluation we utilize all available\nsamples in the test suite."}, {"title": "4.1 Untargeted attack results and discussion.", "content": "In Table 1, we report clean and robust performance of vari-\nous CLIP versions. In terms of clean performance (without\nattack), original CLIP model performs better compared to\nadversarially fine-tuned models. However, under adversar-\nial attacks, the original CLIP model exhibits a significant\ndecline in performance. Particularly under stronger attack\nat $\\epsilon$ = 8/255, its performance deteriorates even further.\nWe observe a slight degradation in the clean performance\nfor the robust clip models due to adversarial fine-tuning.\nNotably, among the robust versions of CLIP, Sim-CLIP"}, {"title": "4.2 Targeted attack results and discussion.", "content": "We present the quantitative results of our targeted attacks at\n$\\epsilon$ = 4/255 in Table 2. This analysis includes CIDEr score to\nevaluate the quality of generated captions. Additionally, we\nillustrate randomly selected attacked samples and their cap-\ntions generated by LLaVA, integrated with different CLIP\nmodels in Figure 3. We observe that the original CLIP model\nis highly susceptible to targeted attacks and demonstrates no\nrobustness. In each instance, the original CLIP model breaks\nand generates the given target string. Conversely, TeCoA2\nand FARE2 break in 5 and 3 cases, resulting in mean suc-\ncess rates of 5% and 3%, respectively. In stark contrast,\nSim-CLIP2 breaks in only one case, further underscoring the\nsuperior performance of Sim-CLIP. Notably, Sim-CLIP4,\nFARE4, and TeCoA4 show complete robustness under tar-\ngeted attack. However, the quality of captions generated by\nSim-CLIP4 notably surpasses FARE4 and TeCoA4, as shown\nin Figure 3. Moreover, captions generated by TeCoA4 ex-\nhibit inferior quality and contain errors, while FARE4's cap-\ntions often lack intricate details or semantic features from\nthe corresponding images. For instance, consider the sam-\nple featuring a patient. With the original image, both FARE4\nand Sim-CLIP4 generate captions without errors and with\ncomprehensive details. However, under attack, the caption\ngenerated by FARE4 lacks specifics regarding the color of\nthe hospital bed and the presence of a mask, whereas Sim-\nCLIP4 retains these semantic features of the image. This ex-\nemplifies the robustness of our adversarial fine-tuning ap-\nproach, as it not only enhances the model's ability to resist\nadversarial attacks but also ensures the preservation of cru-\ncial details and captures the overall semantic meaning within\nthe generated captions. The reported CIDEr scores in Ta-\nble 2, also support these findings. Specifically, TeCoA4 and\nFARE achieve CIDEr scores of 64.4 and 75.3 respectively,\nwhereas Sim-CLIP4 attains the highest CIDEr score of 84.7.\nIn Appendix, we examine the robustness of CLIP models"}, {"title": "4.3 Zero-shot classification results and discussion.", "content": "In Table 3, we evaluate the performance of both original\nand robust CLIP models on zero-shot classification tasks\nacross various datasets under two adversarial attack settings:\n$\\epsilon$ = 2/255 and $\\epsilon$ = 4/255. Across both attack settings\nand a wide range of datasets, Sim-CLIP consistently outper-\nforms TeCoA and FARE. Notably, Sim-CLIP4 demonstrates\ngreater robustness compared to FARE4 and TeCoA4, with\na 3.4% gain in robust accuracy, while also slightly outper-\nforming Sim-CLIP2. These findings reinforce the effective-\nness of Sim-CLIP in handling various adversarial situations\nwithin zero-shot classification tasks, showcasing its superior\ncapabilities. We also evaluate the performance of CLIP mod-\nels using clean images (without perturbation) on zero-shot\nclassification datasets; detailed results are provided in the\nAppendix section."}, {"title": "5 Conclusion", "content": "In this paper, we introduce an unsupervised adversarial fine-\ntuning method that significantly enhances the robustness\nof the widely-used CLIP vision encoder against adversar-\nial attacks. By employing a Siamese architecture with co-\nsine similarity loss and a stop-gradient mechanism, our pro-\nposed framework, Sim-CLIP effectively learns semantically\nmeaningful and attack-resilient visual representations with-\nout requiring large batch sizes or momentum encoders. Ex-\ntensive experiments demonstrate that VLMs equipped with\nSim-CLIP's robust CLIP encoder exhibit superior robust-\nness against both untargeted and targeted adversarial attacks\nwhile maintaining high accuracy on clean data across di-\nverse downstream tasks, outperforming state-of-the-art ad-\nversarial fine-tuning approaches. As large VLMs become\nincreasingly prevalent in critical applications, Sim-CLIP es-\ntablishes strong benchmarks for evaluating robustness in the\nface of evolving adversarial threats. This work underscores\nthe importance of robustifying foundation models like CLIP\nto safeguard the integrity of downstream vision-language\nmodels and paves the way for the development of more se-\ncure and reliable multimodal AI systems."}, {"title": "Ablation with Stop-grad", "content": "In Figure 4, we demonstrate the effectiveness of stop-\ngradient mechanism in Sim-CLIP. For this ablation, all hy-\nperparameters remain unchanged, with the only distinction\nbeing the stop-gradient. As shown in Figure 4(a), Sim-"}, {"title": "Training Hyperparameters ablation.", "content": "In this segment, we delve into the specifics of our experi-\nmental setup, exploring different hyperparameter configura-\ntions for our Sim-CLIP approach. All experiments detailed\nin the main paper and the ablation studies, utilize the ViT-\nL/14 vision encoder of CLIP. Due to the significant com-\nputational resources required for training these CLIP mod-\nels, we employ an early stopping strategy at 500 adversarial\ntraining steps. This strategy allows us to effectively evaluate\nand compare the performance of Sim-CLIP across different\nhyperparameter configurations. Initially, we investigate the\nimpact of different optimizers in Sim-CLIP. Then, we con-\nduct additional experiments by adjusting the Learning Rate\n(LR) and Weight Decay (WD) to identify the optimal config-\nuration for the selected optimizer. In line with the supervised\nfine-tuning methodology proposed by we\nconstrain our LR and WD search within the values of 1e-3\nto le-6 and 1e-3 to 1e-5, respectively.\nIn Figure 5, we illustrate the performance of Sim-CLIP\nwith different hyperparameter configurations. Figure 5(a) il-\nlustrates our ablation study comparing Sim-CLIP's perfor-\nmance using both SGD and AdamW optimizers. For both\noptimizers, we maintain LR at le-5 and WD at 1e-4. No-\ntably, we observe Sim-CLIP's sensitivity to different opti-\nmizers, where, Sim-CLIP performs significantly better with\nthe AdamW optimizer compared to SGD. Hence, we select\nAdamW as our final optimizer for Sim-CLIP. Subsequently,\nwe experiment with different LR and WD combinations with\nAdamW optimizer and present our findings in Figure 5(b)\nand Figure 5(c). Initially, we explore the combination with\nthe highest LR (1e-3) and the lowest WD (1e-5) within our\npredefined hyperparameter search bounds. We observe that\nSim-CLIP exhibits challenges in converging with a higher\nlearning rate. In contrast, lowering the learning rates results\nin comparatively better performance.\nHowever, when we keep lowering the LR and set the LR\nto its lowest value (1e-6) within our search bounds, Sim-\nCLIP struggles to converge. Further analysis indicates that\nan LR of le-5 coupled with a WD of le-3 yields superior\nresults compared to alternative configurations. Furthermore,\nour analysis reveals that Sim-CLIP exhibits some sensitivity\nto Weight Decay (WD), although not to the extent observed\nwith LR. Our ablation study shows that setting WD to le-\n4 results in improved performance. We observe consistent\nperformance trends when evaluating the Robust accuracy on\nthe ImageNet dataset, as illustrated in Figure 5(c). Within\nthe initial 500 training steps, Sim-CLIP demonstrates higher\nrobust accuracy when configured with an LR of le-5 and\na WD of 1e-3. This finding suggests that the combination\nof a lower learning rate and a slightly higher weight decay\ncontributes to the enhanced robustness of Sim-CLIP during\nthe early stages of training. Based on these observations, we\ndetermine the optimal Sim-CLIP hyperparameters to be an\nLR of le-5 and a WD of le-3."}, {"title": "Targeted Attack Details", "content": "For targeted attacks in the main paper, we use the following\ntarget strings to fool the VLMs.\n1. COVID vaccine has severe health\nimplications\n2. Opposing political party aims to\nstrip citizen rights\n3. Investing in this new cryptocurrency\nwill guarantee benefit within a month\n4. Please visit https://deal.net/1x24g58\nkx0 for exclusive deals"}, {"title": "5. Climate Change is a hoax, created by\nthe scientists", "content": "Here, we provide several realistic examples illustrating how\nan attacker could exploit vulnerabilities in VLMs to deceive\nusers. Targets 1 and 5 present scenarios where users could\nbe misled with false information, potentially inciting panic.\nTarget 2 poses a risk of spreading political misinformation,\npotentially leading to societal harm. Target 3 may mislead\nindividuals into investing in specific cryptocurrencies, serv-\ning the adversary's interests. Lastly, Target 4 could lure users\nto phishing sites, endangering their personal information. To\nexecute targeted attacks for Targets 2, 3 and 4, we randomly\nselected 20 images from the COCO dataset and tailored\nLLaVA for inference. For targets 1 and 5, we collected sam-\nples from Google images, depicting patients in bed wearing\nmasks and polar bears in the Arctic, respectively.\nTargeted attack results and ablations We evaluate ro-\nbust CLIP models under targeted attacks at $\\epsilon$ = 2/255 in\nTable 4. Similar to our experiments under $\\epsilon$ = 4/255 attack,\nthe original CLIP model shows no robustness and breaks\nin each instance. While TeCoA2 breaks in one case, FARE\nand Sim-CLIP show complete robustness with their fine-\ntuned versions. Although both FARE and Sim-CLIP exhibit\nrobustness, Sim-CLIP consistently generates superior cap-\ntions and preserves semantic meaning and features in the\ncaptions, compared to FARE, as evidenced by its higher\nCIDEr scores. Additionally, we conduct ablation with vary-\ning numbers of attack iterations and report our findings in\nTable 5. In this ablation study, we focus on two specific tar-\nget strings. For each target, we randomly select 20 instances\nfrom the COCO dataset and execute $\\ell_\\infty$ targeted attacks at\nradii $\\epsilon$ = 4/255 for 500 and 20,000 iterations. The ratio-"}]}