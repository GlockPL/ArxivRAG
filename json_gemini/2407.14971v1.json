{"title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models", "authors": ["Md Zarif Hossain", "Ahmed Imteaj"], "abstract": "Vision-language models (VLMs) have achieved significant strides in recent times specially in multimodal tasks, yet they remain susceptible to adversarial attacks on their vision components. To address this, we propose Sim-CLIP, an unsupervised adversarial fine-tuning method that enhances the robustness of the widely-used CLIP vision encoder against such attacks while maintaining semantic richness and specificity. By employing a Siamese architecture with cosine similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder exhibit significantly enhanced robustness against adversarial attacks, while preserving semantic meaning of the perturbed images. Notably, Sim-CLIP does not require additional training or fine-tuning of the VLM itself; replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to provide robustness. This work underscores the significance of reinforcing foundational models like CLIP to safeguard the reliability of downstream VLM applications, paving the way for more secure and effective multimodal systems.", "sections": [{"title": "1 Introduction", "content": "The remarkable success of large language models (LLMs) (OpenAI 2023; Meta 2023) in understanding and generating human-like text has inspired researchers to extend their capabilities to the visual domain. This has led to the development of Vision-Language Models (VLMs) (Alayrac et al. 2022; Liu et al. 2024), which aim to bridge the gap between visual and textual information. To extract meaningful visual representations from images, these VLMs often leverage pretrained vision encoders, such as CLIP (Radford et al. 2021), BEiT (Bao et al. 2021), and DINO (Caron et al. 2021). Leveraging pretrained vision encoders empowers VLMs to capitalize on the rich visual knowledge acquired through extensive pretraining on diverse image datasets. This capability enhances the VLMs' performance across various downstream tasks, eliminating the requirement for task-specific fine-tuning. Among these pretrained vision encoders, CLIP (Radford et al. 2021) gained significant attention and is widely used in many state-of-the-art VLMs such as OpenFlamingo (Awadalla et al. 2023) and LLaVA (Liu et al. 2024). CLIP's popularity stems from its exceptional performance in zero-shot settings and its ability to align inputs from different modalities in a joint embedding space. Additionally, VLMs equipped with CLIP demonstrate remarkable zero-shot performance in down-stream tasks such as image captioning, and visual question answering (VQA). Due to the impressive down-stream task performance and contribution to the growing trend of multimodal learning, an increasing number of large VLMs are being made publicly available. While the availability of pre-trained weights and architectures for VLMs facilitated further research and application development, it also raised severe safety concerns that must be addressed. For instance, recent works (Zhao et al. 2024; Wei, Haghtalab, and Steinhardt 2024) have demonstrated the high susceptibility of VLMs to adversarial attacks targeting either text or image inputs. Notably, it has been argued that the vision modality is more susceptible to manipulation compared to text modalities (Goodfellow, Shlens, and Szegedy 2014; Carlini et al. 2024). Besides, the authors in (Schlarmann and Hein 2023) demonstrate that adversaries can employ human-imperceptible adversarial perturbations to create targeted attacks, effectively generating desired malicious outputs as shown in Figure 1.\nWhile existing methods (Mao et al. 2022; Schlarmann et al. 2024) have made strides in improving the robustness of CLIP models, significant challenges remain. Specifically, these adversarially fine-tuned CLIP models often lead to a noticeable degradation in performance on downstream tasks."}, {"title": "2 Related Works", "content": "Vision Language Models. In recent times, various VLMs have emerged, such as LLaVA (Liu et al. 2024), Flamingo (Alayrac et al. 2022), OpenFlamingo (OF) (Awadalla et al. 2023), and Macaw-LLM (Lyu et al. 2023), to name a few. These models leverage the power of multi-modal learning by combining visual and textual data, thereby enhancing their capability to understand and generate responses to multi-modal inputs. Most of these VLMs employ pre-trained LLMs (e.g., Vicuna (Chiang et al. 2023)) with a large-scale vision encoder, such as CLIP (Radford et al. 2021) to process multi-modal data. During training, the vision encoder is kept frozen, while the model learns the interaction between modalities. This interaction is typically facilitated by combining a projection layer with cross-attention mechanism.\nAdversarial robustness in traditional ML. The susceptibility of traditional machine learning models (e.g., CNNs, RNNs) to adversarial attacks is well-established and has been extensively studied. Most of the existing adversarial attacks predominantly target monomodal models, focusing on either text or image modality (Hossain et al. 2023; Shahid et al. 2023). In the image domain, gradient-based adversarial attacks (Goodfellow, Shlens, and Szegedy 2014; Madry et al. 2017) can deceive a target model by applying slight perturbations to the input images that are imperceptible to human eyes. Alternatively, patch-based attacks (Brown et al. 2017) strategically place adversarial patches to deceive models, rather than modifying the entire image. Text-based adversarial (Ebrahimi et al. 2017; Guo et al. 2021) attacks are also thoroughly investigated, requiring a different approach due to the discrete nature of textual data. Adversarial training (Madry et al. 2017; Pan et al. 2022) has emerged as an efficient defense mechanism against aforementioned attacks, which involves training the model with adversarial examples to improve its robustness.\nAdversarial robustness for VLMs. Given CLIP's remarkable performance in zero-shot settings and understanding visual tasks, most VLMs employ it or its variants as their vision encoder. Few recent studies (Jiang et al. 2020; Bansal et al. 2023; Zhou et al. 2023) demonstrate CLIP's vulnerability to imperceptible attacks, which can significantly impact downstream task performance of VLMs. For instance, in AdvCLIP (Zhou et al. 2023), the authors generate universal patches for CLIP models that can deceive VLMs across all of their downstream tasks. In (Zhao et al. 2024), the authors leverage diffusion models to create adversarial samples that manipulate the model into generating a targeted output. Moreover, in (Schlarmann and Hein 2023), the authors demonstrate the potential of gradient-based attacks on VLMs, compelling the model to generate inaccurate results. One recent study (Mao et al. 2022) introduced a supervised adversarial fine-tuning scheme for CLIP that employs cross-modal image-text contrastive loss. A few concurrent works (Jiang et al. 2020; Fan et al. 2021) proposed unsupervised adversarial training methods based on SimCLR contrastive loss. However, these methods necessitate a large batch size to achieve strong robust performance, making them unsuitable for integration into VLMs. The authors in (Gowal et al. 2020) proposed an adversarial fine-tuning scheme based on BYOL (Grill et al. 2020), which addresses the issue of large batch sizes but introduces an overhead with the momentum encoder. Besides, the authors in (Schlarmann et al. 2024) proposed an $l2$ loss-based unsupervised fine-tuning scheme, but it fails to capture semantic features from the images effectively. In contrast, our unsupervised fine-tuning approach, based on a siamese network, utilizes cosine similarity to effectively capture semantic information during adversarial training without requiring a large batch size or an additional momentum encoder."}, {"title": "3 Methodology", "content": "In this section, we first discuss how existing adversarial attacks for VLMs are formalized. Then we delve into the specifics of existing unsupervised adversarial training methods and their limitations, followed by the details of our proposed framework, Sim-CLIP."}, {"title": "3.1 Adversarial Attacks on VLM", "content": "In gradient-based adversarial attacks (e.g., PGD (Goodfellow, Shlens, and Szegedy 2014), APGD (Croce and Hein 2020)), malicious attackers typically add slight perturbation $\\delta$ to an input image $x$. This perturbation is carefully calculated for a specific target model, represented as $f_{\\theta}$, where $\\theta$ denotes the model's parameters. The core mechanism of these attacks involves leveraging the gradients of the model's loss function (e.g., cross-entropy) $L$, with respect to the input image $x$. By exploiting these gradients, attackers are able to craft adversarial examples, denoted as $x_a = x + \\delta$. These adversarial samples are imperceptible to human eyes yet possess the ability to mislead deep learning models such as $f_{\\theta}$ towards misclassification:\n$x_a = \\underset{x_a}{\\text{argmax}} \\; L(x_a, y_{\\text{true}}), \\; ||x_a - x||_p \\le \\epsilon$. (1)"}, {"title": "3.1.1 Targeted attack.", "content": "In a targeted attack, the adversary manipulates the VLM to generate a specific incorrect prediction by providing perturbed query samples. The adversary perturbs a query image $q$ by adding slight perturbation $\\delta_q$, resulting in a perturbed sample $q + \\delta_q$. When this perturbed sample is fed into the VLM, it generates the desired malicious output $y_i$, such as spam links or false information. The targeted attack is formulated as an optimization problem that aims to minimize the negative log-likelihood of generating the desired incorrect output $y_i$, given the perturbed query image is $q + \\delta_q$. This can be formalized as follows:\n$\\underset{\\delta_q}{\\text{min}} \\; - \\underset{l=1}{\\overset{m}{\\sum}} \\text{log} \\; p (y_i, q + \\delta_q) \\; \\text{s.t.} \\; ||\\delta_q||_{\\infty} \\le \\epsilon_q$ (2)\nHere, the goal is to find the optimal $\\delta_q$ while ensuring the perturbation is bounded by $\\epsilon_q$ in the $l_{\\infty}$ norm. Note that, by minimizing the negative log-likelihood, we ensure that the probability of the target token is maximized."}, {"title": "3.1.2 Untargeted attack.", "content": "In contrast to targeted attacks, untargeted attacks seek to induce any form of error, without specifying a particular target. The objective of an untargeted attack can be expressed as maximizing the likelihood of the model producing any output that differs from the correct or expected output. The mathematical formulation for an untargeted attack can be represented as follows:\n$\\underset{\\delta_q}{\\text{max}} \\; \\underset{l=1}{\\overset{m}{\\sum}} \\text{log} \\; p (y_i, q + \\delta_q) \\; \\text{s.t.} \\; ||\\delta_q||_{\\infty} \\le \\epsilon_q$ (3)\nHere, $\\delta_q$ denotes the perturbation to the query image $q$, with the aim of maximizing the likelihood of the model outputting any incorrect prediction $y_r$ that deviates from the correct output $y$."}, {"title": "3.2 Unsupervised Adversarial Fine-Tuning", "content": "We propose an unsupervised adversarial fine-tuning approach for the CLIP vision encoder that aims to enhance its robustness against adversarial attacks. Our methodology incorporates unsupervised learning techniques by leveraging siamese architecture. Our goal is to enhance the resilience of the vision encoder against adversarial attacks while preserving the semantic meaning of the image features.\nPreviously, FARE (Schlarmann et al. 2024) introduced an unsupervised adversarial fine-tuning scheme for the CLIP vision encoder that does not rely on text embedding. FARE achieves its robustness by minimizing a $l2$ loss function between an adversarially perturbed image embedding and a clean image embedding. The embedding loss can be expressed as follows:\n$L_F(x, x_a) = \\underset{||x_a-x||}{\\text{max}} \\; ||\\theta(x_a) - \\theta(x)||_2$ (4)\nThis loss encourages the embedding $\\theta(x_a)$ of perturbed images $z$ to stay close to the original embedding $\\theta(x)$. Although FARE demonstrates robust performance against adversarial attacks, it exhibits two major issues. First, the $l2$ loss employed by FARE may not be the most suitable"}, {"title": "3.3 Adversarial Fine-tuning with Siamese Architecture", "content": "Sim-CLIP tackles the challenges present in FARE by tailoring cosine similarity loss within a siamese architecture. During our adversarial fine-tuning phase, Sim-CLIP first generates a perturbed view x' from the clean input image x. We utilize PGD perturbation to generate the perturbed view:\n$X_{t+1} = \\Pi_{x+\\epsilon}(x_t + \\alpha \\cdot \\text{sign}(\\nabla_x L(x_t, y)))$\ns.t. $||x' - x|| \\leq \\epsilon$ (5)\nHere, y represents the true label of the input image and L is a cross-entropy loss. At each iteration t, a perturbation is calculated based on the gradient of the loss function with respect to the input image x. The magnitude of this perturbation is controlled by a step size parameter $\\alpha$, which determines the strength of the perturbation applied in each iteration. This perturbation is constrained within a specified bound $\\epsilon$. Subsequently, both clean and perturbed views are fed into the CLIP models, which share the same weights, as depicted in Figure 2(b). The CLIP models generate clean representation $R_c$ from the original image x and perturbed representation $R_p$ from the perturbed view x'. Now, we maximize the similarity between the perturbed and clean representations to encourage the model to learn features that are invariant to adversarial perturbations. To this end, we minimize negative cosine similarity between the representations $R_p$ and $R_c$ to ensure their alignment in the vector space, enhancing model coherence and robustness to adversarial perturbations:\n$\\text{CosSim}(R_p, R_c) = (\\frac{R_p \\cdot R_c}{||R_p||_2 ||R_c||_2})$ (6)\nHere, $|| \\cdot ||_2$ denotes the $l_2$ norm, which is equivalent to minimizing mean squared error between $l2$-normalized vectors."}, {"title": "3.4 Symmetric Loss Collapse Prevention", "content": "Minimizing this negative cosine loss directly might lead to a collapse of the loss function, yielding an undesirable constant solution (Chen et al. 2020). Conventional unsupervised adversarial training methods (Jiang et al. 2020; Fan et al. 2021) attempt to address this issue by relying on negative samples or momentum encoders. However, integrating negative samples often necessitates larger batch sizes, leading to increased computational burden, while momentum encoders introduce extra overhead. To prevent the loss function from collapsing while mitigating the resource burden, we integrate a stop-gradient mechanism following (Chen and He 2021) into our adversarial fine-tuning loss. We employ a symmetric loss for our adversarial training scheme, which can be defined as follows:\n$L_{simclip} (R_p, R_c) = \\frac{1}{2} (\\text{CosSim}(R_p, \\text{stopgrad}(R_c)) + \\text{CosSim}(R_c, \\text{stopgrad}(R_p)))$ (7)\nHere, one CLIP model's output is held constant using stop-gradient, while the other model's output is projected to align with it. Subsequently, the roles of the two models are reversed, with the first model's output being projected and matched to the \"constant\" output of the second model. This implies that in the first term of Equation 7, $R_p$ does not receive any gradients from $R_c$ since $R_c$ is treated as constant. However, in the second term of the same equation, $R_c$ receives gradients from $R_p$ as the stop-gradient operation is now applied to $R_p$ instead. This process results in two losses, which are then averaged and minimized for optimization. In summary, each model receives gradients for the loss term, except for cases where stop-gradient is not applied to its output. The effectiveness of the stop-gradient mechanism in preventing loss function from collapsing is further validated through ablation study in the Appendix."}, {"title": "4 Experimental Analysis", "content": "We evaluate our adversarially fine-tuned robust CLIP model across various downstream tasks, including zero-shot classification. To evaluate performance on downstream tasks, we integrate robust clip models into VLMs by substituting their default vision encoders.\nVLM models. We use OpenFlamingo 9B (OF) (Awadalla et al. 2023) and LLaVA-1.5 (Liu et al. 2024) with 7B parameters as our target VLMs. They both utilize CLIP ViT-L-14 (Dosovitskiy et al. 2020) as their vision encoder. However, they differ in their language decoders. OF employs MPT-7B (Team et al. 2023), while LLaVA-1.5 utilizes Vicuna (Chiang et al. 2023). During our evaluation, OF is provided only with context text alongside the query image. In contrast, we utilize both the default system prompt and task-specific prompts along with the query images for LLaVA.\nDatasets. We considered a wide range of datasets for down-stream tasks, specifically for image captioning and VQA. For image captioning, we use the COCO (Lin et al. 2014) and Flickr30k (Plummer et al. 2015) datasets, while for visual question answering, we employ VizWiz (Gurari et al. 2018) and OKVQA (Marino et al. 2019). Additionally, we assess the robustness of our adversarially fine-tuned CLIP model on the zero-shot classification task using the CIFAR10, CIFAR100 (Krizhevsky, Hinton et al. 2009), EuroSAT (Helber et al. 2019), PCAM (Veeling et al. 2018), and Flower (Nilsback and Zisserman 2008) datasets. Our evaluation process employs two approaches: for adversarial evaluation, we randomly select 500 images from each respective dataset, while for clean evaluation we utilize all available samples in the test suite."}, {"title": "5 Conclusion", "content": "In this paper, we introduce an unsupervised adversarial fine-tuning method that significantly enhances the robustness of the widely-used CLIP vision encoder against adversarial attacks. By employing a Siamese architecture with cosine similarity loss and a stop-gradient mechanism, our proposed framework, Sim-CLIP effectively learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Extensive experiments demonstrate that VLMs equipped with Sim-CLIP's robust CLIP encoder exhibit superior robustness against both untargeted and targeted adversarial attacks while maintaining high accuracy on clean data across diverse downstream tasks, outperforming state-of-the-art adversarial fine-tuning approaches. As large VLMs become increasingly prevalent in critical applications, Sim-CLIP establishes strong benchmarks for evaluating robustness in the face of evolving adversarial threats. This work underscores the importance of robustifying foundation models like CLIP to safeguard the integrity of downstream vision-language models and paves the way for the development of more secure and reliable multimodal AI systems."}, {"title": "Appendix", "content": "Ablation with Stop-grad\nIn Figure 4, we demonstrate the effectiveness of stop-gradient mechanism in Sim-CLIP. For this ablation, all hyperparameters remain unchanged, with the only distinction being the stop-gradient. As shown in Figure 4(a), Sim-CLIP with stop-gradient collapses within a few training epochs, reaching the minimum possible loss of -1. A similar collapse in performance is observed in terms of accuracy, as shown in Figure 4(b). Sim-CLIP with stop-gradient mechanism achieves robust accuracy over 60%, whereas without stop-gradient, the accuracy plummets after only a few training steps.\nPseudocode\nIn Algorithm 1, we present the pseudocode of our adversarial fine-tuning framework.\nTraining Hyperparameters ablation.\nIn this segment, we delve into the specifics of our experimental setup, exploring different hyperparameter configurations for our Sim-CLIP approach. All experiments detailed in the main paper and the ablation studies, utilize the ViT-L/14 vision encoder of CLIP. Due to the significant computational resources required for training these CLIP models, we employ an early stopping strategy at 500 adversarial training steps. This strategy allows us to effectively evaluate and compare the performance of Sim-CLIP across different hyperparameter configurations. Initially, we investigate the impact of different optimizers in Sim-CLIP. Then, we conduct additional experiments by adjusting the Learning Rate (LR) and Weight Decay (WD) to identify the optimal configuration for the selected optimizer. In line with the supervised fine-tuning methodology proposed by (Mao et al. 2022), we constrain our LR and WD search within the values of 1e-3 to le-6 and 1e-3 to 1e-5, respectively.\nIn Figure 5, we illustrate the performance of Sim-CLIP with different hyperparameter configurations. Figure 5(a) illustrates our ablation study comparing Sim-CLIP's performance using both SGD and AdamW optimizers. For both optimizers, we maintain LR at le-5 and WD at 1e-4. Notably, we observe Sim-CLIP's sensitivity to different optimizers, where, Sim-CLIP performs significantly better with the AdamW optimizer compared to SGD. Hence, we select AdamW as our final optimizer for Sim-CLIP. Subsequently, we experiment with different LR and WD combinations with AdamW optimizer and present our findings in Figure 5(b) and Figure 5(c). Initially, we explore the combination with the highest LR (1e-3) and the lowest WD (1e-5) within our predefined hyperparameter search bounds. We observe that Sim-CLIP exhibits challenges in converging with a higher learning rate. In contrast, lowering the learning rates results in comparatively better performance.\nHowever, when we keep lowering the LR and set the LR to its lowest value (1e-6) within our search bounds, Sim-CLIP struggles to converge. Further analysis indicates that an LR of le-5 coupled with a WD of le-3 yields superior results compared to alternative configurations. Furthermore, our analysis reveals that Sim-CLIP exhibits some sensitivity to Weight Decay (WD), although not to the extent observed with LR. Our ablation study shows that setting WD to le-4 results in improved performance. We observe consistent performance trends when evaluating the Robust accuracy on the ImageNet dataset, as illustrated in Figure 5(c). Within the initial 500 training steps, Sim-CLIP demonstrates higher robust accuracy when configured with an LR of le-5 and a WD of 1e-3. This finding suggests that the combination of a lower learning rate and a slightly higher weight decay contributes to the enhanced robustness of Sim-CLIP during the early stages of training. Based on these observations, we determine the optimal Sim-CLIP hyperparameters to be an LR of le-5 and a WD of le-3.\nTargeted Attack Details\nFor targeted attacks in the main paper, we use the following target strings to fool the VLMs:\nCOVID vaccine has severe health implications\nOpposing political party aims to strip citizen rights\nInvesting in this new cryptocurrency will guarantee benefit within a month\nPlease visit https://deal.net/1x24g58kx0 for exclusive deals"}]}