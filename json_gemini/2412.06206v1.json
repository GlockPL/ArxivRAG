{"title": "SIRERAG: INDEXING SIMILAR AND RELATED INFORMATION FOR MULTIHOP REASONING", "authors": ["Nan Zhang", "Prafulla Kumar Choubey", "Alexander Fabbri", "Gabriel Bernadett-Shapiro", "Rui Zhang", "Prasenjit Mitra", "Caiming Xiong", "Chien-Sheng Wu"], "abstract": "Indexing is an important step towards strong performance in retrieval-augmented generation (RAG) systems. However, existing methods organize data based on either semantic similarity (similarity) or related information (relatedness), but do not cover both perspectives comprehensively. Our analysis reveals that modeling only one perspective results in insufficient knowledge synthesis, leading to suboptimal performance on complex tasks requiring multihop reasoning. In this paper, we propose SIRERAG, a novel RAG indexing approach that explicitly considers both similar and related information. On the similarity side, we follow existing work and explore some variances to construct a similarity tree based on recursive summarization. On the relatedness side, SIRERAG extracts propositions and entities from texts, groups propositions via shared entities, and generates recursive summaries to construct a relatedness tree. We index and flatten both similarity and relatedness trees into a unified retrieval pool. Our experiments demonstrate that SIRERAG consistently outperforms state-of-the-art indexing methods on three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an average 1.9% improvement in F1 scores. As a reasonably efficient solution, SIRERAG enhances existing reranking methods significantly, with up to 7.8% improvement in average F1 scores.", "sections": [{"title": "INTRODUCTION", "content": "Retrieval-augmented generation (RAG) has shown strong potential in augmenting large language models (LLMs) with highly specialized and constantly updated knowledge (Lewis et al., 2020; Gao et al., 2024). Getting rid of fine-tuning LLMs, it is an efficient method for handling users' queries that require domain knowledge. A typical RAG pipeline may involve chunking, embedding, indexing, retrieval with queries, reranking, and LLM response generation (Wang et al., 2024).\nThe indexing step is a prerequisite. It focuses on organizing a large amount of data and serves as an upstream step of retrieval. For example, RAPTOR (Sarthi et al., 2024) shows a significant performance improvement by adding recursive summaries to text chunks of a dataset, which demonstrates the potential of adding synthesized information for retrieval. The added recursive summaries combine semantically similar information within a dataset. GraphRAG (Edge et al., 2024), on the other hand, indices an entire corpus via an entity-guided knowledge graph. It then constructs summaries from closely-related entities and their mentions, synthesizing the connections and relatedness among different pieces of information.\nHowever, none of the existing methods address the importance of indexing from both similarity and relatedness sides, which limits a holistic understanding of the provided dataset. We define similarity as the semantic distance of text pieces and relatedness as the degree of connection of texts based on signals such as entities and propositions. Indexing similar and related information facilitates more comprehensive knowledge integration than indexing individual kind of information. As shown in Figure 1, a complex question that involves two hops of reasoning requires the retrieval and synthesis"}, {"title": "RELATED WORK", "content": "RAG RAG is a framework that integrates retrieval mechanisms into generative models to enhance text generation by leveraging external knowledge. This concept has evolved from earlier retrieval-based methods such as DrQA (Chen et al., 2017) and DPR (Karpukhin et al., 2020). Instead of separating retrieval and generation phases, researchers also showed the potential of tightly coupling retrieval and generation into an end-to-end framework (Lewis et al., 2020)."}, {"title": "BOTTLENECK OF SOLELY MODELING SIMILARITY OR RELATEDNESS", "content": "To verify our hypothesis of insufficient knowledge integration when solely modeling similarity or relatedness, we perform different kinds of clustering philosophies on a retrieval corpus of MuSiQue (Trivedi et al., 2022). Using the same retrieval corpus as HippoRAG (Guti\u00e9rrez et al., 2024), we obtain 1000 questions from the validation set of MuSiQue along with their candidate passage clusters (each cluster includes supporting and distractor passages). We include distractor passages for a more realistic setting, since they are semantically close and/or related to the supporting candidates. Treating these clusters as the gold labels for different queries, we run our own clustering on all passages based on either similarity or relatedness.\nFollowing RAPTOR (Sarthi et al., 2024), we use Gaussian Mixture Models (GMMs) to perform soft clustering, assuming that a candidate passage can belong to multiple clusters. For similarity, we run GMMs on the deep representations of all passages to find semantically similar groups. For related-"}, {"title": "METHODOLOGY", "content": "We propose SIRERAG, a RAG indexing framework guided by similarity and relatedness. As shown in Figure 2, its left tree integrates information based on similarity while its right tree integrates information based on relatedness. As a first step, we study an alternative tree design to determine whether we can develop a generalized tree structure for similarity and relatedness trees, and beyond (Section 4.1). After the construction of the similarity tree, we extract propositions and their entities from our multihop reasoning dataset and perform clustering based on entities to synthesize related information (Section 4.2). Indexing separate similarity and relatedness trees (Section 4.3), SIRERAG explicitly models both kinds of information within our dataset."}, {"title": "EXPLORING A HIERARCHICAL STRUCTURE OF TREES", "content": "For efficiency, we stick to a tree structure to organize texts and explore whether a more structured tree design would offer performance improvement. RAPTOR placed all text chunks at the bottom level and recursive summaries at upper levels, but this design does not closely follow the commonsense"}, {"title": "SYNTHESIZING INFORMATION BASED ON RELATEDNESS", "content": "For relatedness, we need to synthesize information based on a different philosophy than RAPTOR. Because related information pieces always share some degree of connection (e.g., overlapping subjects), we assume that two text pieces are related if they mention the same entity (e.g., person, location, product, etc). For example, entity 2 and 3 chunks in Figure 1 are unlikely to be clustered in the same group based on similarity, but since they both mention Francis Bacon, we are able to connect them together.\nModeling Relatedness with Entity-Specific Propositions: To effectively use entities for organizing related content, we first need to determine the appropriate granularity for text pieces. There are three main limitations with directly connecting entities to standard text chunks. First, a chunk often contains information beyond the scope of a specific entity, making it challenging to localize information about one entity, potentially adding noise. Second, aggregating all chunks in an indexing corpus for each entity can result in hundreds of thousands of tokens for each entity, which may lead to long context performance issues, such as losing critical information in the middle Liu et al. (2024) or experiencing low coverage and citation performance Laban et al. (2024). Third, linking with chunks will introduce redundancy as each chunk may be a part of multiple entity clusters. Therefore, inspired by recent works on retrieval granularity (Liu et al., 2023; Chen et al., 2023), we propose to use short entity-specific \u201cpropositions\u201d to represent fine-grained knowledge about entities and build our relatedness tree."}, {"title": "INDEXING SIMILARITY AND RELATEDNESS TREES", "content": "We propose to construct similarity and relatedness trees independently. This approach ensures that summary nodes in one tree do not access the clusters of the other, leading to a simpler design. There is another slightly complex design in which we allow summary nodes from one tree to access clusters from the other tree. This interaction may enables summary nodes in both trees to inform and enhance each other and improving their informativeness and consequently performance. However, this approach sacrifices the distinction between similarity and relatedness. Additionally, allowing cross-tree interaction leads to more nodes to cluster at each level as well as requires summarization based on a greater number of nodes per cluster, all of which increases the overall complexity of the system. We experimented with both settings and did not observe performance improvement as shown in Appendix A. Therefore, we opted for the simpler first implementation in our evaluation.\nFlattening all tree nodes, we place them into a unified retrieval pool. In other words, regardless of a node's origin (e.g., bottom or upper levels, similarity or relatedness trees), it is added to a single list containing all nodes."}, {"title": "EXPERIMENT SETUP", "content": "To demonstrate the effectiveness of SIRERAG, we select three representative multihop QA datasets: MuSiQue (Trivedi et al., 2022), 2WikiMultiHopQA (Ho et al., 2020), and HotpotQA (Yang et al., 2018). Using the same corpus as HippoRAG (Guti\u00e9rrez et al., 2024), we obtain 1000 questions from each validation set of these three datasets."}, {"title": "DATASETS", "content": "To demonstrate the effectiveness of SIRERAG, we select three representative multihop QA datasets: MuSiQue (Trivedi et al., 2022), 2WikiMultiHopQA (Ho et al., 2020), and HotpotQA (Yang et al., 2018). Using the same corpus as HippoRAG (Guti\u00e9rrez et al., 2024), we obtain 1000 questions from each validation set of these three datasets."}, {"title": "BASELINES", "content": "We select RAPTOR, HippoRAG, and GraphRAG as state-of-the-art retrieval baselines. As discussed above, RAPTOR integrates knowledge based on similarity while the other two approaches focus on relatedness. Specifically, HippoRAG has both indexing and retrieval components, and we use Col-BERTv2 (Santhanam et al., 2022) as the retriever of HippoRAG due to its strongest QA performance reported. Although GraphRAG has a different goal (global questions directed at an entire dataset) than ours, we include it to show its performance on multihop QA datasets. Since the queries in our datasets ask fine-grained details, we use the local search function of GraphRAG instead of its global search."}, {"title": "EVALUATION METRICS", "content": "We use exact match (EM) and F1 scores to measure the QA performance of different models. Both metrics evaluate how accurate a generated answer is with respect to the ground truth. Like RAPTOR (Sarthi et al., 2024), we do not assess retrieval performance directly. The reason is that both SIRERAG and RAPTOR create new candidates (e.g., summary and proposition aggregate) in the retrieval pool, so it would be unfair to compare methods in terms of retrieval scores across different pools. Instead, QA performance is the best indicator of the overall capability of both RAG pipelines.\nWe use the average time per query (TPQ) and the time-pool efficiency ratio (TPER) to measure the efficiency of SIRERAG and RAPTOR, as both methods share a significant portion of their retrieval candidates. Average TPQ measures the average time (in seconds) taken to answer a query, and it represents the inference time of a method. For TPER, it computes the growth of total inference time with respect to the growth of the retrieval pool size between two methods:\nTPER =  Inference-Time A/Inference-Time B/Pool-Size A/Pool-Size B\nSetting SIRERAG as method A and a baseline as method B, we aim to ensure that the growth of inference time does not scale proportionally with the increase in the retrieval pool size. The reason behind is that there are many efficiency considerations (e.g., length and redundancy of retrieval candidates) beyond just the sheer number of retrieval candidates. Parallelization could also be designed"}, {"title": "IMPLEMENTATION DETAILS", "content": "To generate final answer, we use GPT-4o and the same prompt (\u201canswer this question in as fewer number of words as possible.\u201d) to answer queries for all methods, since we aim to control the instruction-following capabilities across all methods. We use either GPT-3.5-Turbo or GPT-4o as the choice of LLM if any methods involve LLM calls. We use OpenAI\u2019s text-embedding-3-small as the embedding model for all methods. During retrieval, we select top 20 candidates that match the provided query for all methods, because there is a large number of text chunks in our datasets and SIRERAG is expected to perform better when retrieving more due to the incorporation of proposition aggregates and their recursive summaries."}, {"title": "RESULTS AND ANALYSIS", "content": "Our results and analysis aim to answer the following research questions:\n\u2022 RQ 1: How does SIRERAG compare against other state-of-the-art baselines (sec 6.1)?\n\u2022 RQ 2: As an important contribution of SIRERAG, is considering both similarity and relatedness an effective method (sec 6.1 and 6.2)?\n\u2022 RQ 3: What is the effect of each component in SIRERAG(sec 6.2)?\n\u2022 RQ 4: What is the applicability of SIRERAG(sec 6.3)?\n\u2022 RQ 5: With the addition of relatedness tree, is SIRERAG an efficient method (sec 6.4)?"}, {"title": "OVERALL RESULTS", "content": "Our overall results are presented in Table 4. We show results on more datasets (single-hop QA, other multihop QA, and ambiguous questions) in Appendix E to show the generality of SIRERAG across various complex reasoning tasks. Besides quantitative scores, we also conduct our qualitative analysis in Appendix D.\nImprovement over baselines SIRERAG delivers consistent improvement over RAPTOR, Hip-poRAG, and GraphRAG. With an exception on 2Wiki when comparing against HippoRAG, SIR-ERAG achieves significantly higher performance than indexing baselines (e.g., approximately 5% higher than RAPTOR on average F1, up to 8.3% improvement of F1 on MuSiQue than HippoRAG, and more than 20% higher than GraphRAG on average EM and F1). This demonstrates the advantage of SIRERAG on multihop QA and modeling both similarity and relatedness. Specifically, SIRERAG outperforms RAPTOR due to the incorporation of a relatedness tree, and it has better overall performance than HippoRAG, because we explicitly model similarity while HippoRAG pri-oritizes relatedness signals such as nodes with the most edges. We see that HippoRAG is particularly strong on 2Wiki benchmark, which is also reported in its original paper. Thus, we believe 2Wiki is the best fit of HippoRAG, but it has lower performance scores than SIRERAG on other datasets."}, {"title": "ABLATION STUDY", "content": "To dissect SIRERAG, we perform a comprehensive ablation analysis as shown in Table 5. There are several variances, including (A) remove the recursive summary on the relatedness tree; (B) add all the propositions into the retrieval pool, and keep all aggregated propositions and recursive summary on the relatedness tree; (C) same as (B) but remove aggregated propositions; (D) same as (C) but further remove the recursive summary design on the relatedness tree.\nEntity clustering In (E), we do not maintain a separate relatedness tree and add an additional clus-tering philosophy to the similarity tree. Specifically, each text chunk in the similarity tree is simplified to, 'This chunk mentions entity 1 and entity 2', if both entities are extracted by our LLM. We then run GMMs (the same clustering method as RAPTOR) on these simplified chunks. Once the clustering decisions are obtained, we group the original chunks as additional clusters and append these clusters to the similarity tree, allowing higher levels of the tree to incorporate both clustering philosophies. Since entities primarily determine the outcome of this additional clustering approach, we apply entity clustering to model relatedness on the similarity tree. This allows us to eliminate proposition aggregates in order to examine their utility.\nFindings Overall, we observe performance drops across all variations, highlighting the effectiveness of our design for SIRERAG. First, the recursive summary on the relatedness tree proves beneficial, as seen in both (A) and (D). Interestingly, adding more propositions to retrieval negatively impacts performance, as shown in (B). This indicates adding redundant information into the retrieval pool hurts the QA performance, since we keep the aggregated propositions in SIRERAG. From (C), we also find that aggregated propositions are essential, with their removal resulting in a significant per-formance decline. This is an important indicator that adding grouped knowledge about relatedness to the similarity tree would offer improvements, which echoes the bottlenecks described in Section 3.\nBoth (A) and (B) have better performance than RAPTOR (GPT-40) from Table 4, which indicates the advantage of proposition aggregates. In contrast, although (E) also models both similarity and re-latedness, it exhibits a notable decline comparing against SIRERAG. This finding demonstrates the necessity of proposition aggregates of modeling relatedness. Because proposition aggregates reduce noise and information redundancy more effectively than text chunks as described in Section 4.2, they serve as an effective carrier of related dataset contents."}, {"title": "APPLICABILITY OF SIRERAG", "content": "We analyze how applicable SIRERAG is when a specific retrieval method is chosen. Therefore, we select BM25 (Robertson & Walker, 1994) and ColBERTv2 (Santhanam et al., 2022) as additional reranking-based options. We run them on the retrieval pool of SIRERAG to demonstrate its utility. We show how SIRERAG can complement these non-indexing options in Table 6. Results show that having SIRERAG benefits both BM25 and ColBERTv2 significantly, which demonstrates the advantage of our solution as the upstream step of these methods. On the other hand, RAPTOR only improves the QA performance on MiSuQue while showing performance degradation on other datasets. Thus, the utility of SIRERAG surpasses that of RAPTOR in the context of multihop reasoning. We also apply SIRERAG on an iterative retrieval method called self-ask (Press et al., 2023) and obtain significant performance improvement as shown in Appendix B. Our method showcases wide applicability on multihop QA across various retrieval methods."}, {"title": "EFFICIENCY OF SIRERAG", "content": "As shown in Table 7, we compare the efficiency of SIRERAG and RAPTOR using the metrics described in Section 5.3. All the values listed involve the time taken to retrieve the top 20 candidates and prompt GPT-40 to answer the query.\nRAPTOR requires less inference time than SIRERAG on average, which is expected due to SIR-ERAG's larger retrieval pool. However, with slightly longer inference time, SIRERAG has much better performance as discussed previously. To evaluate whether SIRERAG remains a reasonably efficient method, we compute its TPER values to measure its growth of total inference time relative to its growth of retrieval pool size. Since all its TPER values are well below 1, SIRERAG demonstrates reasonable efficiency without introducing many lengthy or redundant retrieval candidates."}, {"title": "CONCLUSION", "content": "In this paper, we identify the bottleneck of solely modeling similarity or relatedness when we need to index a multihop reasoning dataset for knowledge integration. To address it, we introduce SIR-ERAG, an innovative RAG indexing approach that considers both similarity and relatedness. SIR-ERAG delivers a consistent improvement over state-of-the-art indexing baselines across several multihop QA benchmarks."}, {"title": "AN ALTERNATIVE DESIGN OF ALLOWING CROSS-TREE INTERACTION", "content": "We discuss an alternative design of combining similarity and relatedness trees. Specifically, this design combines nodes from both sides in the same pool for finding additional clusters and performing summarization at every tree level. In other words, we find additional clusters by concatenating the nodes of both trees, which considers cross-tree interaction instead of keeping them separate.\nAs shown in Table 8, the performance of considering cross-tree interaction is slightly lower than SIRERAG. Therefore, it is more efficient to keep trees separate in order to reduce the overall complexity of the system as discussed in Section 4.3."}, {"title": "ADDITIONAL EXPERIMENT ON OTHER NON-INDEXING METHODS", "content": "Although there are many existing methods that work on multihop reasoning tasks, SIRERAG is about indexing corpus data under RAG setup. In other words, instead of being our baselines, other non-indexing works(Press et al., 2023; Islam et al., 2024) focus on other dimensions of improving performance on complex reasoning tasks."}]}