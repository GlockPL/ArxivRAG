{"title": "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set Annotation: An Energy-Based Approach", "authors": ["Chen-Chen Zong", "Sheng-Jun Huang"], "abstract": "Active learning (AL), which iteratively queries the most informative examples from a large pool of unlabeled candidates for model training, faces significant challenges in the presence of open-set classes. Existing methods either prioritize query examples likely to belong to known classes, indicating low epistemic uncertainty (EU), or focus on querying those with highly uncertain predictions, reflecting high aleatoric uncertainty (AU). However, they both yield suboptimal performance, as low EU corresponds to limited useful information, and closed-set AU metrics for unknown class examples are less meaningful. In this paper, we propose an Energy-based Active Open-set Annotation (EAOA) framework, which effectively integrates EU and AU to achieve superior performance. EAOA features a (C + 1)-class detector and a target classifier, incorporating an energy-based EU measure and a margin-based energy loss designed for the detector, alongside an energy-based AU measure for the target classifier. Another crucial component is the target-driven adaptive sampling strategy. It first forms a smaller candidate set with low EU scores to ensure closed-set properties, making AU metrics meaningful. Subsequently, examples with high AU scores are queried to form the final query set, with the candidate set size adjusted adaptively. Extensive experiments show that EAOA achieves state-of-the-art performance while maintaining high query precision and low training overhead. The code is available at this link.", "sections": [{"title": "1. Introduction", "content": "The success of deep neural networks (DNNs) is largely fueled by large-scale, accurately labeled datasets [16, 49]. However, acquiring such data is often expensive and time-consuming primarily due to the labor-intensive nature of manual labeling [30, 50]. To save labeling costs, recent research has focused on developing effective model training techniques that leverage limited and insufficiently labeled data [30, 37, 46]. Among them, active learning (AL) has emerged as a popular framework, which iteratively selects the most informative examples from the unlabeled data pool and queries their labels from the Oracle [12, 30, 34].\nExisting AL methods can be categorized into three types based on their sampling strategies: uncertainty-based [2, 10, 42, 50], diversity-based [25, 33, 40], and hybrid strategies [1, 11, 31]. Most of these methods operate under the closed set assumption, which posits that the classes in the unlabeled data match those in the labeled data. However, in real-world scenarios, it is challenging and often costly to ensure that no open-set classes are present in the unlabeled data. Meanwhile, several studies [6, 26, 28, 31] have shown that these methods perform poorly when open-set classes are involved, as such examples tend to receive uncertain model predictions and exhibit distinct features. Therefore, developing effective AL methods for open-world scenarios, where open-set classes exist, is of significant importance."}, {"title": "2. Related Work", "content": "Active learning (AL) has garnered great research interest as a primary framework for reducing labeling costs by querying the most informative examples for model training. AL's query methods can be categorized based on their data sources into three types: query-synthesizing [19, 20, 47], stream-based [7, 23], and pool-based approaches [2, 6, 10, 12, 50]. Among these, pool-based methods are currently the mainstream, operating under the assumption of a large pool of available unlabeled data, from which a subset of examples is selected for annotation in each AL round. These query methods can be further divided into three categories: 1) uncertainty-based strategies [2, 10, 17, 42], which select instances for which labeling is least certain; 2) diversity-based strategies [25, 33, 40], which query instances that are most representative or exhibit the greatest feature diversity; and 3) hybrid strategies [1, 11, 31], which combine both to achieve better performance.\nOpen-set recognition (OSR) refers to a system's ability to differentiate between data types it has encountered during training (in-distribution (ID) data) and those it has not previously seen (out-of-distribution (OOD) data). Earlier studies employed traditional machine learning techniques such as support vector machines [13, 32], extreme value theory (EVT) [43], nearest class mean classifier [3], and nearest neighbor [21]. Recently, there has been growing interest in using generative models to learn representation spaces focused exclusively on known examples [27, 29, 36, 44]. Other techniques often aim to simulate unknown examples, providing a more intuitive approach to OSR [4, 5, 8, 24, 45]. However, simply applying these methods in AL scenarios under the open-world assumption often leads to failure for two main reasons. First, the recognition performance of these methods heavily relies on the classifier's effective-"}, {"title": "3. Methodology", "content": "Recently, this emerging research problem has garnered considerable attention [6, 26, 28, 31, 50]. For instance, in LfOSA [26], the authors formulate this problem as open-set annotation (OSA) and utilize queried unknown class examples to train a (C + 1)-class detector for rejecting open-set examples and focusing more on selecting known class ones. Two recent methods, EOAL [31] and BUAL [50], adopt a structure similar to LfOSA, with EOAL aimed at improving the recognition of known class examples, while BUAL focuses more on sampling highly uncertain examples. Despite demonstrating high query precision, our findings reveal that these methods can not achieve satisfactory test accuracy and struggle to identify the most informative examples.\nTo analyze the reasons behind their failure, we review the concept of uncertainty quantification\u00b9. Epistemic uncertainty refers to a measure that remains high for instances not previously encountered and decreases when these instances are included in the training [14, 35]. In contrast, aleatoric uncertainty is characterized by elevated values in ambiguous examples [14, 35]. In closed-set settings, examples with high epistemic uncertainty tend to reside in low-density regions of the representation space, while those with high aleatoric uncertainty may appear around the decision boundary due to ambiguous features. Both types are potential targets for our queries. However, in open-set scenarios, examples with high epistemic uncertainty are likely to be open-set instances, and aleatoric uncertainty is meaningful only in closed-set contexts, as it reflects the ambiguity between different observable classes\u00b2 [14, 22]. Therefore, selecting examples with low epistemic uncertainty (to ensure a closed set) and high aleatoric uncertainty is a more reasonable choice. However, LfOSA and EOAL focus on querying examples with low epistemic uncertainty, while BUAL prioritizes querying those with high aleatoric uncertainty. This may be the potential reason for their failure.\nTo validate this, we present the results in Figure 1. As shown, focusing solely on either epistemic or aleatoric uncertainty in an open-world scenario results in suboptimal performance. However, effectively combining both leads to a significant improvement. Inspired by this, we propose Energy-based Active Open-set Annotation (EAOA), an approach that effectively queries the most informative examples by considering both types of uncertainty. EAOA maintains two networks: a (C + 1)-class detector and a C-class target classifier, with the following contributions:\n\u2022 An energy-based epistemic uncertainty measure is designed for the detector, expressed as the free energy score on known classes minus that on the unknown class. This measure integrates both learning-based and data-driven perspectives, enabling reliable uncertainty assessment in data-limited scenarios.\n\u2022 An energy-based aleatoric uncertainty measure is proposed for the target classifier, defined as the free energy score on all classes minus that on secondary classes.\n\u2022 A coarse-to-fine querying strategy is proposed. It first selects examples with low epistemic uncertainty to form a smaller candidate set, ensuring closed-set properties, which makes aleatoric uncertainty meaningful. Then, it queries examples with high aleatoric uncertainty within this set, with the candidate set size adaptively adjusted through a novel target-driven strategy.\n\u2022 A margin-based energy loss is introduced for the detector training, aimed at maximizing the free energy score on known classes while minimizing that on the unknown class, thereby enhancing the unknown class detection.\n\u2022 Extensive experiments show that EAOA outperforms current state-of-the-art methods in test accuracy, query precision, and training efficiency."}, {"title": "3.1. Preliminaries", "content": "Notations. Consider the problem of ordinary C-class classification. In active open-set annotation (AOSA) tasks, we start with a limited labeled dataset $D_{L}^{kno} = \\{(x,y)\\}$ containing $N_L$ examples from known classes for training, alongside a sufficiently large unlabeled data pool $D_U = \\{x\\}$ consisting of $N_U$ examples from both known and unknown classes for querying. The label $y^l$ of an instance $x$ belongs to $\\{1,\\ldots,C'\\}$, whereas the label $y$ of an instance $x$ is not provided prior to Oracle labeling and falls within $\\{1,...,C+1\\}$, with C + 1 representing all unknown classes. At each active learning (AL) cycle, a batch of b examples, denoted as $X_{query}^{kno} = \\{x_{i}^{query}\\}_{i=1}^{k_t}$ $\\bigcup$ $X_{query}^{unk} = \\{x_{i}^{query}\\}_{i=1}^{b}$, is selected according to a specified query strategy and sent to Oracle for labeling. Then, $D_{L}^{kno}$ and $D_U$ are updated accordingly, and $X_{query}$ forms the unknown class dataset $D_{unk}$ with $D_L = D_{kno} \\bigcup D_{unk}$.\nOverview. As outlined in the Introduction, addressing the AOSA problem requires first ensuring that the queried examples exhibit low epistemic uncertainty to approximate a closed set, enabling meaningful aleatoric uncertainty assessment, and then querying examples with high aleatoric uncertainty. To achieve this goal, we propose an Energy-based Active Open-set Annotation (EAOA) framework, as illustrated in Figure 2, which primarily consists of three key components: active sampling, detector training, and classifier training. Specifically, we begin by training a detector network to evaluate the epistemic uncertainty of examples from both learning-based and data-driven perspectives, leveraging the labeled data from both known and unknown classes. Next, we assess the aleatoric uncertainty of examples by utilizing the free energy discrepancy in predictions made by the target classifier. Finally, we query a smaller candidate set of examples with low epistemic uncertainty first, with the set dynamically adjusting by rounds, and then acquire the query set with high aleatoric uncertainty."}, {"title": "3.2. Energy-based Epistemic and Aleatoric Uncertainty for Active Sampling", "content": "Energy-based epistemic uncertainty. Energy-based models (EBMs) [18, 39] build a function $E (x) : \\mathbb{R}^D \\rightarrow \\mathbb{R}$ to map a D-dim data point to a scalar and defines the probability distribution in multi-class settings through logits as:\n$p (y|x) = \\frac{e^{-E(x,y)}}{\\sum_{c=1}^C e^{-E(x,c)}} = \\frac{e^{f_y(x)}}{\\sum_{c=1}^C e^{f_c(x)}} = \\frac{e^{-E(x,y)}}{e^{-E(x)}}$, (1)\nwhere $f_y(x)$ denotes the predicted logit of model f for instance x regarding label y, $E(x, y) = - f_y(x)$, and $E (x) = - \\log \\sum_{c=1}^C e^{-E(x,c)}$ is called free energy. The probability density of x in an EBM can be written as:\n$p(x) = \\frac{e^{-E(x)}}{\\int_x e^{-E(x)}} = \\frac{\\int_y e^{-E(x,y)}}{\\int_x \\int_y e^{-E(x,y)}} = \\frac{e^{-E(x)}}{Z}$. (2)\nThis implies that for two data points, x\u2081 and x2, if $E (x_1) > E (x_2)$, then x\u2081 lies in a sparser region compared to x2 w.r.t. the marginal distribution. This aligns with epistemic uncertainty: high-uncertainty examples are distributed in low-density regions due to their lower occurrence frequency.\nNevertheless, free energy is not ideal for directly measuring epistemic uncertainty in the AOSA task, as the underlying EBM does not fully utilize the information contained in the labeled unknown class examples. To counter this, we group all unknown classes into the C + 1 category, train a multi-class classifier (i.e., the detector), and extend the free energy theory by making the following Remark 1.\nRemark 1 For AOSA tasks, the epistemic uncertainty (EU) of x can be expressed through the free energy score on known classes minus that on the unknown class\u00b3,\n$EU (x) = E_{kno} (x) - E_{unk} (x)$\n$= - \\log \\sum_{c=1}^C e^{-E(x,c)} + \\log (1+e^{-E(x,C+1)})$. (3)"}, {"title": "Target-driven adaptive active sampling", "content": "As such, for two given data points x1 and x2, the inequality EU (x1) > EU (x2) implies that x\u2081 is occurring from a denser region w.r.t. the unknown class compared to x2.\nSince labeled data in AL is often quite limited, relying solely on the detector's predictions to assess the epistemic uncertainty of examples may not be sufficiently reliable. To obtain a more reliable measurement, we evaluate the epistemic uncertainty of examples in two ways: 1) a learning-based perspective that directly uses the detector's predictions, and 2) a data-driven perspective that relies on the similarity to labeled examples from different classes. To this end, we utilize the detector to extract features and emit K arrows from each instance in the labeled data pool D\u2081 to its K nearest neighbors in the unlabeled data pool Du based on cosine distance. For a data point x, its probability given class y can be approximated as:\n$p(x|y) = \\frac{\\# \\text{ of Arrows}(x,y)}{|X_y|}$ (4)\nwhere # of Arrows(x1,y) denotes the total number of arrows directed at x from examples with label y, and $|X_y|$ represents the total number of examples with label y in DL. If x is in a region where examples with label y densely exist, it is likely to receive more arrows, and vice versa.\nThen, by virtue of Bayes' theorem, we define the data-centric class probability distribution of $x$ as\n$p (y|x) = \\frac{p (x|y) p (y)}{\\sum_{c=1}^{C+1} p(x|c) p (c)}$ (5)\nHere, the prior p (y) is the probability of observing class y, and can be approximately determined by the sample count for each class in DL:\n$p (y) = \\frac{|X_Y|}{\\sum_{c=1}^{C+1} |X_c|}$ (6)\nBased on Eqs. (1), (4), (5), and (6), we can obtain:\n$p (y|x) = \\frac{\\# \\text{ of Arrows}(x,y)}{\\sum_{c=1}^{C+1} \\# \\text{ of Arrows}(x,c)} = \\frac{e^{-E(x,y)}}{e^{-E(x)}}$ (7)\nAs such, we can define the specific form of the energy function from a data-driven perspective to calculate the epistemic uncertainty of examples, as stated in Remark 1.\nHere, for a given data point x, the uncertainty scores calculated in two different ways are denoted as $EU_L(x)$ and $EU_D(x)$. We first gather the uncertainty scores for all examples in Du to form sets $\\{EU_L(x_1),..., EU_L(x_{N_U})\\}$ and $\\{EU_D(x_1),..., EU_D(x_{N_U})\\}$, and then fit two two-component Gaussian Mixture Models (GMMs) respectively to convert these scores into a probabilistic format. Suppose a tilde is added to denote the probabilistic format, we apply an element-wise product rule to obtain the final epistemic uncertainty score of x:\n$\\widetilde{EU(x)} = \\widetilde{EU_L(x)} \\odot \\widetilde{EU_D(x)}$. (8)"}, {"title": "Energy-based aleatoric uncertainty", "content": "Energy-based aleatoric uncertainty. Aleatoric uncertainty arises through inherent noise in the data, that is, for the same instance x, different labels might be observed if multiple annotators label it independently. This means that aleatoric uncertainty can be defined based on the confusion between classes. As such, we further extend the free energy theory by making the following Remark 2.\nRemark 2 For AL tasks, the aleatoric uncertainty (AU) of x can be expressed through the free energy scores on all classes minus that on secondary classes,\n$AU(x) = E(x) - E_{\\text{secondary classes}} (x)$\n$= - \\log \\sum_{c=1}^C e^{-E(x,c)} + \\log \\sum_{c=1, c \\neq \\text{ymax}}^C e^{-E(x,c)} - e^{-E(x, ymax)}$ (9)\nwhere ymax is the most probable label for x. This implies that for two data points x1 and x2, if inequality AU(x1) > AU(x2) holds, then x\u2081 occurs in a region closer to the decision boundary compared to x2.\nSimilarly, we gather the uncertainty scores for all examples in Du to form set $\\{AU(x_1),..., AU(x_{N_U})\\}$, and then fit a two-component GMM to convert these scores into a probabilistic format, i.e., $\\{\\widetilde{AU}(x_1),..., \\widetilde{AU}(x_{N_U})\\}$.\nTarget-driven adaptive active sampling. After having epistemic uncertainty scores and aleatoric uncertainty scores, we form the query set based on the strategy outlined in Figure 2. Specifically, in each active sampling round, we perform the querying in two rounds. In the first round, $k_t b$ examples with the lowest epistemic uncertainty scores are selected. Then, according to aleatoric uncertainty, we choose the top b examples with the highest scores from the candidates obtained in the first round to query their labels.\nObviously, the choice of the k value is critical. If k is too small, such as k = 1, it maximizes the likelihood that the queried examples belong to the closed-set classes. However, since the candidate set size matches the query set size, aleatoric uncertainty cannot effectively contribute. On the contrary, if the k value is too large, the closed-set condition of the candidate set cannot be ensured, rendering aleatoric uncertainty meaningless. Meanwhile, the optimal k value often varies for different datasets. To enhance the strategy's generalizability, we introduce the expected target precision for known class queries to drive the adaptive adjustment of the k value. The relation between the two is as follows:\n$k_{t+1} = \\begin{cases}\n  k_t + a & \\text{if } rP - tP > z, \\\\\n  k_t - a & \\text{if } tP - rP > z, \\\\\n  k_t & \\text{if } |tP - rP| < z,\n\\end{cases}$ (10)\nwhere $k_t$ is the value of k in the t-th AL round, tP is the expected target precision, rP is the real query precision calculated as $rP = \\frac{|X_{query}^{kno}|}{|X_{query}|}$ after Oracle labeling, a is the variation amplitude and z is the triggering threshold. Notably, although the number of hyper-parameters increases, setting them becomes significantly easier, and they are less sensitive to dataset variations."}, {"title": "3.3. Detector and Target Classifier Training", "content": "Detector training. All labeled examples from the known classes and unknown classes are jointly used to train a detector with C + 1 classes. For a given data point xi with label yi, let pi denote its one-hot label, i.e., pic is set to 1 and the others to 0, and qi denote its probability vector predicted by the detector. On the one hand, we use the following cross-entropy loss to train the detector:\n$L_{ce}^{x_i} = - \\sum_{c=1}^{C+1} p_{ic} \\log q_{ic}$ (11)\nOn the other hand, we propose a margin-based energy loss to ensure that, for examples from known classes, the free energy scores for the first C classes are high, while for examples from unknown classes, the free energy score for the (C + 1)-th class remains low, referring to Remark 1. The energy loss is calculated by:\n$L_{energy}^{x_i} = \\begin{cases}\n  (\\text{max} (0, E_{kno}(x_i) - m_{kno}))^2 & \\text{if } x_i \\in D_{kno}, \\\\\n  (\\text{max} (0, m_{unk} - E_{kno}(x_i)))^2 & \\text{if } x_i \\in D_{unk},\n\\end{cases}$ (12)\nwhere mkno and munk are the margins for known classes and unknown classes, respectively.\nThus, the total loss for training the detector is:\n$L_{detector}^{x_i} = L_{ce}^{x_i} + \\lambda_e L_{energy}^{x_i}$, (13)\nwhere \u03bbe is a hyper-parameter that balances the two losses.\nTarget classifier training. All labeled examples from the known classes are used to train the target classifier by minimizing the standard cross-entropy loss:\n$L_{classifier}^{x_i} = - \\sum_{c=1}^C p_{ic} \\log q_{ic}$ (14)"}, {"title": "4. Experiments", "content": "The pseudocode of EAOA is shown in Appendix C.\n4.1. Implementation Details\nDatasets. We validate the effectiveness of our method on three benchmark datasets: CIFAR-10 [15], CIFAR-100 [15], and Tiny-ImageNet [41], with category counts of 10, 100, and 200, respectively. To perform active open-set annotation (AOSA), we create their open-set versions by randomly selecting a subset of classes as known according to the specified mismatch ratio, while the remaining classes are treated as unknown. The mismatch ratio is defined as the proportion of known classes in the total number of classes. For CIFAR-10 and CIFAR-100, we set the mismatch ratios to 20%, 30%, and 40%. For Tiny-ImageNet, we set the ratios to 10%, 15%, and 20%, making it more challenging.\nTraining details. Initially, we randomly select 1%, 8%, and 8% of known class examples from CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, to construct the labeled dataset. The active learning (AL) process consists of 10 rounds, with 1,500 examples queried in each round. For all experiments, we choose ResNet-18 [9] as the base model and train it by SGD [48] optimizer with momentum 0.9, weight decay 5e-4, and batch size 128 for 200 epochs. The initial learning rate is set to 0.01 and is reduced by a factor of 10 every 60 epochs. We repeat all experiments three times on GeForce RTX 3090 GPUs and record the average results for three random seeds (seed = 1,2,3). We generally set the values of K, tP, k1, a, z, mkno, Munk, and de to 250, 0.6, 5, 1, 0.05, -25, -7, and 0.01, respectively, and these values generalize well across datasets.\nBaselines. We consider the following methods as baselines: Random, Uncertainty, Certainty, Coreset, BADGE, CCAL, MQNet, EOAL, and BUAL. Among them, EOAL and BUAL are currently state-of-the-art (SOTA). A detailed overview of these methods is provided in Appendix D."}, {"title": "4.2. Performance Comparison", "content": "Figure 3 displays the test accuracy of various methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet, varying with the number of AL rounds. Figure 4 presents scatter plots of the average query precision across all rounds and the final round test accuracy for each method on CIFAR-10, CIFAR-100, and Tiny-ImageNet. Here, the query precision refers to the proportion of queried known class examples to the total number of queried examples in each round.\nAs shown in Figure 3, our method achieves optimal test accuracy across all datasets and mismatch ratios, and in most AL rounds, the curve of our method completely overlaps with those of other methods, demonstrating its superiority. In Figure 4, our method achieves optimal final round test accuracy and mean query precision in most scenarios, particularly on the challenging Tiny-ImageNet dataset, demonstrating its strong recognition capabilities. Compared to the existing SOTA methods, BUAL and EOAL, our method ensures that queried examples exhibit low epistemic uncertainty while maintaining high aleatoric uncertainty. The significant test performance improvement over them validates the effectiveness of our proposed framework, sug-"}, {"title": "4.3. Ablation Studies", "content": "Effect of each component. Figure 5 (left) shows the ablation results to validate the effectiveness of each component in our method. Our proposed energy-based epistemic uncertainty (\"Part 1\") utilizes information from labeled unknown class examples, leading to significant improvements in both recognition and test performance compared to free energy alone. The proposed energy-based aleatoric uncertainty (\"Part 2\") performs poorly on its own, as aleatoric uncertainty is only meaningful in closed-set scenarios. However, when combined with \"Part 1\", it shows significant improvement, validating the superiority of the entire framework. Additionally, removing the data-driven epistemic uncertainty score or the margin-based energy loss leads to a decline in performance, confirming their necessity.\nHyper-parameter sensitivity. Figure 6 presents the ablation results for the hyperparameter target query precision tP on CIFAR-10 and CIFAR-100, with values set to [0.4, 0.5, 0.6, 0.7, 0.8]. Overall, the fluctuations in test performance are minimal, and as tP increases, the method's recognition performance improves. Figure 5 (middle) displays the ablation results for the energy loss weight de on CIFAR-100 with a 40% mismatch ratio, using values of [0.001, 0.005, 0.01, 0.05, 0.1]. A higher loss weight can impede model training, leading to reduced detector accuracy and reliability. Conversely, a lower loss weight fails to effectively separate the energy distributions of known and unknown class examples, adversely affecting the detector's recognition performance.\nThe ablation results for additional hyper-parameters are provided in Appendix E, which can demonstrate their ability to generalize effectively across different datasets.\nRuntime comparison. Figure 5 (right) presents the running times and mean query precision of various methods on CIFAR-100 with a mismatch ratio of 40%. A higher mean query precision indicates that a larger number of examples are involved in training, often resulting in longer training times. Traditional AL methods, characterized by lower mean query precision and the training of a single model, generally have shorter training times. Notably, our method has the shortest runtime among all AOSA methods, achieving the highest test accuracy and maintaining a very high recognition rate."}, {"title": "5. Conclusion", "content": "In this paper, we demonstrate that focusing solely on either epistemic uncertainty (EU) or aleatoric uncertainty (AU) in open-world active learning scenarios does not yield satisfactory performance. We argue that the most informative examples should primarily belong to closed-set classes, exhibiting low EU scores, ensuring that the derived AU metric is meaningful, and secondarily, should show high AU scores. To achieve this, we propose EAOA, a novel framework for addressing the challenging active open-set annotation problem. In EAOA, both types of uncertainty are defined in the form of free energy: EU is evaluated by the detector from both learning-based and data-driven perspectives, while AU is measured by the target classifier through class confusion. Additionally, we introduce a margin-based energy loss to enhance the detector's ability to distinguish known from unknown classes and a target-driven strategy to adaptively adjust the size of the candidate set obtained in the first query stage. Extensive experimental results across various tasks demonstrate EAOA's superiority."}]}