{"title": "DETERMINE-THEN-ENSEMBLE: NECESSITY OF TOP-K UNION FOR LARGE LANGUAGE MODEL ENSEMBLING", "authors": ["Yuxuan Yao", "Han Wu", "Mingyang Liu", "Sichun Luo", "Xiongwei Han", "Jie Liu", "Zhijiang Guo", "Linqi Song"], "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the UNIon Top-k Ensembling (UNITE), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that UNITE significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks and have shown promising results in real-world applications (OpenAI, 2023; Yang et al., 2024; Dubey et al., 2024). Given the diversity in data sources, model architectures, and training methods, LLMs exhibit varying strengths and weaknesses depending on the task at hand. Consequently, rather than relying solely on training an LLM from scratch, an alternative approach is to create an ensemble of LLMs. This method allows for leveraging the complementary advantages of different LLMs (Jiang et al., 2023b; Lu et al., 2024; Yu et al., 2024b).\nExisting model ensembling methods can be broadly categorized into three types: output-level, probability-level, and training-level approaches. Output-level methods (Jiang et al., 2023b; Lu et al., 2024; Shnitzer et al., 2023) aggregate the complete outputs of multiple candidate models. Probability-level methods (Huang et al., 2024; Yu et al., 2024b), integrate outputs based on probability distributions at each generation step through the intersection or union of the vocabulary. Training-level methods (Wan et al., 2024a; Xu et al., 2024) utilize output probability vectors as labels for richer information extraction during training. While output-level methods are constrained by the limitations of existing outputs, and training-level methods introduce additional computational overhead, probability-level methods have garnered particular attention.\nExisting methods for ensembling LLMs, such as DEEPEN (Huang et al., 2024) and GAC (Yu et al., 2024b), grapple with two significant challenges. First, these approaches concentrate solely on the ensembling technique, sidestepping the crucial discussion of which types of models can be effectively"}, {"title": "2 RELATED WORKS", "content": "Based on the sequence of inference and fusion processes, multiple model collaboration methods can be broadly classified into two categories: model ensembling (Jiang et al., 2023b; Yu et al., 2024b) and model merging (Yu et al., 2024a; Akiba et al., 2024). Model ensembling follows an inference-then-fusion approach, aiming to integrate the outputs of various models to achieve a more refined response. Conversely, model merging adopts a fusion-then-inference strategy, wherein different models are combined into a single model before inference. While model merging is typically applicable only to homologous models, our focus in this study is on the more general approach, namely model ensembling. We discuss different model ensembling methods as follows:\nOutput-Level Model Ensembling methods involve selecting multiple candidate models and utilizing their complete outputs for aggregation. For instance, Jiang et al. (2023b) developed PAIR-RANKER, an additional ranking model, to evaluate and select the best candidate output. Similarly, Lu et al. (2024) and Shnitzer et al. (2023) designed a router that determines the most appropriate candidate model based on the given question. However, these approaches are restricted by the existing outputs and become ineffective if all options are incorrect. Some studies have addressed this by training fusion models to combine outputs (Jiang et al., 2023b; Wang et al., 2024b), which alleviates the limitation of relying solely on available candidates and often yields improved results. Nevertheless, achieving generalization with fusion models remains a significant challenge, as they may not fully utilize the probability information generated at each step.\nProbability-Level Model Ensembling methods focus on integrating outputs from different models by utilizing the probability distribution at each generation step. COOL-FUSION (Liu et al., 2024) let each source LLM generate tokens until reaching common word boundaries, then jointly reranks the segments. However, this method relies on common word boundaries, limiting generation"}, {"title": "3 UNDERSTANDING MODEL ENSEMBLING FROM MODEL CAPACITY, VOCABULARY SIZE AND TASK", "content": "Existing model ensembling approaches (Huang et al., 2024; Yu et al., 2024b) only focus on designing the model ensembling method, offering limited discussion on the selection of base models for ensembling. However, insights from prior research (Dong et al., 2023; Lee et al., 2023), indicate that not all model pairs are compatible to be combined, particularly when there are significant differences in size, vocabulary, and performance. In our preliminary experiments, we explored various factors that might affect the performance of model ensembling, including model size (e.g., 3B/7B/8B/13B/70B), model architecture (dense/sparse), performance discrepancies, tokenization strategies (BPE/WordPiece), vocabulary size (e.g., 102K/64K/32K) and tasks variations (e.g., text generation/QA/multiple choices). Finally, we identified three representative factors for further analyses, including performance discrepancy, vocabulary size, and tasks variations."}, {"title": "3.1 IMPACT OF MODEL PERFORMANCE DISCREPANCY", "content": "To investigate the impact of performance disparity on model ensembling, we select LLaMA2-13B-Chat and Mistral-7B-Instruct-v0.3 as base models, which show a significant performance gap across various tasks. We evaluated the GSM8K (Cobbe et al., 2021), PIQA (Bisk et al., 2020), ARC-C (Clark et al., 2018), and NQ (Kwiatkowski et al., 2019) datasets with three comparative methods.\nAs shown in Fig. 1, we found that when there is a significant performance disparity between models, LLM-BLENDER, which selects the optimal response from model candidates, is not suitable for"}, {"title": "3.2 INFLUENCE OF VOCABULARY SIZE", "content": "Yu et al. (2024b) found that large models employing different tokenization methods, such as BPE and BBPE, exhibit over 90% overlap in tokenizing Oxford 5000 common words. This indicates that tokenization methods have a marginal impact on model ensembling, leading us to focus on the effect of vocabulary size on ensemble performance. To this end, we selected four models with varying vocabulary sizes that exhibit similar performance on certain datasets, namely LLaMA2-13B-Chat (vocabulary size: 32,000), Mistral-7B-Instruct-v0.3 (vocabulary size: 32,768), Yi-6B (vocabulary size: 64,000), and DeepSeek-LLM-7B-Chat (vocabulary size: 102,400)."}, {"title": "3.3 TASK-SPECIFIC CHALLENGES IN MODEL ENSEMBLING", "content": "In the aforementioned experiments, we identified an interesting phenomenon: Even when performance and vocabulary size are aligned across models, substantial differences in the response style can also hinder successful ensembling on specific tasks. For example, LLaMA3-8B and Qwen2-7B exhibit comparable performance across various tasks. Existing ensembling approaches such as"}, {"title": "4 METHODOLOGY", "content": ""}, {"title": "4.1 MODEL SELECTION STRATEGY", "content": "Given the aforementioned insights, we present a strategy to assess the compatibility of two models for ensembling and offer guidance for selecting base models from the candidate model pool.\nWhen ensembling two models, we begin by comparing their performance on the target tasks. An overall performance gap within 10% is desirable, as it indicates the potential compatibility for successful ensembling. If the performance gap falls within this range, it is crucial to compare the response styles of different models. This comparison is relatively subjective, with differences often identifiable through superficial features such as text length and BLEU score.\nIn this work, we use text length as a guiding metric by constraining the length of longer responses to not exceed twice the length of the shorter ones. When the response manner is also consistent across models, it is highly probable that these two models can be successfully ensembled to produce a superior response. We exclude discrepancies in vocabulary size from consideration, as prior analyses have demonstrated its insignificance in affecting ensembling outcomes.\nTo select base models from the pool, we recommend initially choosing the best-performing model for the target tasks. Subsequently, select the next best-performing model that satisfies the criteria for successful ensembling with the first chosen model, continuing this process iteratively until the maximum number of base models is reached or no further suitable models can be found."}, {"title": "4.2 UNION TOP-k ENSEMBLING", "content": "Apart from the model selection strategy, we also endeavor to improve the efficiency and performance of model ensembling. Existing probability-level model ensembling methods, e.g. DEEPEN and GAC, attempt to combine the models through full vocabulary alignment. We argue this approach is sub-optimal since the candidate next token typically resides within the top-k tokens. Conversely, incorporating tokens of lower probability may introduce unnecessary noise, thereby diminishing the overall accuracy and effectiveness of the ensemble. Motivated by this, we propose the UNION Top-k Ensembling (UNITE), a novel and efficient ensembling approach that only aligns the top-k tokens in each decoding step.\nGiven the base models for ensembling, denoted as $LLM_i$ with their respective tokenizer $T_i$ and vocabulary $V_i$, we feed the input text prompt into $LLM_i$ and obtain the probability distribution. Instead of aligning the probability distributions across different models, we only focus on the subset of top-k tokens since the candidate next token is highly likely to be within the subset. The selected top-k"}, {"title": "Algorithm 1 Union top-k ensembling", "content": "Require: LLM, Vocabulary Vi, Tokenizer Ti, Demonstration prompt, Stop condition stop(*)\n1: while not stop(*) do\n2:  TopKi, Pi \u2190 LLMi(prompt)\n3:  for each model do\n4:   if token w \u2208 Vu and w \u2208 TopKi then\n5:    Pi{w} and TopKi remains unchanged.\n6:   else if token w \u2208 Vu and w \u2208 Vi and w \u2209 TopKi then\n7:    TopKi \u2190 w, Pi \u2190 p\n8:   else if token w \u2208 Vu and w \u2209 Vi then\n9:    w1,..., wm \u2190 Ti(w)\n10:   TopKi \u2190 w1, Pi \u2190 p1\n11:   end if\n12:  end for\n13:  Phorm = softmax(Pi), Pavg = 1n \u2211in=1 Phorm\n14:  w = argmaxw\u2208Topki (Pavg)\n15:  prompt prompt + w\n16: end while\nNext, we aim to construct the union set Vu of TopKi across the base models. The most straightforward solution to obtain the union set is to directly remove the duplicate tokens and retain all other distinct tokens. For the token in Vu but not in Vi, the probability of this token in LLMi is assigned to 0. This strategy is adopted by GAC (Yu et al., 2024b). However, we contend that this strategy is not entirely reasonable since it is heavily influenced by the tokenizer and vocabulary. For example, the candidate token \u201cJames\u201d is within the top-k subset of LLM1 but not in V2 while the token \"Jam\" appears in TopK2. The word \u201cJames\u201d can be tokenized into \u201cJam\" and \"es\" by LLM2. If the token probabilities pi{Jam} and pi 02 James} are set to 0, the overall probability of generating the word \"James\" should be inherently reduced, even if both base models exhibit a preference for this word. Therefore, we present a new token probability alignment strategy on the union set. Firstly, we obtain the top-k union vocabulary Vu by directly merging the TopKi subsets. Then, the token distribution Pi and TopKi are updated according to following criteria:\n1. If the token w appears in both Vu and TopKi, Pi{w} and TopKi remains unchanged.\n2. If the token w in Vu is absent in TopKi but present in Vi, it will be appended to TopKi. Pi{w} also updates accordingly.\n3. If the token w in Vu does not exist in Vi, it should be tokenized by Ti. The first token of the result along with its token probability is then updated to TopKi and Pi.\nUp to now, we can obtain the aligned top tokens TopKi and the token distributions Pi of based models. Consequently, we normalize Pi as: Phorm = softmax(Pi).\nSince our method eliminates the need for full vocabulary alignment, it is essential to designate one model as the primary base model. As discussed in Section 3.2, the selection of primary base model is flexible when the candidate models exhibit comparable performance. To simplify the process, we directly employ the best-performing model as the primary base model. Then, the primary base model employs the average token probability Pavg Pavg to predict the next token, where Pavg = 1n \u2211in=1 Phorm.\nThe next token is determined using the maximization-based greedy strategy (Li et al., 2016). The chosen token will be appended to the input text. This process is iteratively repeated until the predetermined stopping criterion is met, such as generating an end-of-sentence token or reaching the maximum length."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 SETUP", "content": "Models All of our experiments are conducted with the following commonly used models, including LLaMA2-7B-Chat (Touvron et al., 2023), LLaMA2-13B-Chat (Touvron et al., 2023), LLaMA3-8B-Instruct (Dubey et al., 2024), LLaMA3.1-8B-Instruct (Dubey et al., 2024), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023a), DeepSeek-LLM-7B-Chat (Bi et al., 2024), Yi-6B (Young et al., 2024), OpenChat-3.5 (Wang et al., 2024a), Qwen2-7B-Instruct (Yang et al., 2024), Mixtral-8\u00d77B (Jiang et al., 2024) and Qwen1.5-72B (Bai et al., 2023).\nBaselines We selected three typical model ensembling methods for further analyses. 1) LLM-BLENDER (Jiang et al., 2023b) includes a reward model, PAIRRANKER, to evaluate LLM responses, and a fusion model, GENFUSER, to combine them. We only use PAIRRANKER due to significant over-generation issues with GENFUSER. 2) DEEPEN (Huang et al., 2024) employs relative representation theory to map each model's probability distribution to a universal space for aggregation, computed through the intersection of model vocabularies. 3) GAC (Yu et al., 2024b) projects multiple LLMs' probability vectors into a unified vocabulary dimension using a mapping matrix and aggregates outputs at each generation step to select the next token. Although GAC suggests the importance of identifying keywords to improve latency, we still exclude it owing to its hindrance on the ensembling performance.\nBenchmarks We evaluate six benchmarks, which can be categorized into three main groups. 1) Comprehensive Examination: MMLU (5-shot) (Hendrycks et al., 2021), covering 57 subjects that humans typically learn; ARC-C (0-shot) (Clark et al., 2018), collected from standardized natural science tests. 2) Reasoning Capabilities: GSM8K (Cobbe et al., 2021) (4-shot), a dataset of high-quality problems at the grade school math level; PIQA (Bisk et al., 2020) (0-shot), a commonsense reasoning dataset. 3) Knowledge Capacities: TriviaQA (5-shot) (Joshi et al., 2017), compiled by Trivia enthusiasts; NaturalQuestion (NQ) (5-shot) (Kwiatkowski et al., 2019), a question-answering corpus consisting of queries issued to the Google search engine."}, {"title": "5.2 MAIN RESULTS", "content": "As shown in Table 3 and Table 4, we have following observations:\n(1) UNITE enhances individual model performance when the base models exhibit similar performance levels. As demonstrated, the ensemble of models such as OpenChat achieved an average improvement of approximately 1.2% across five benchmark tasks, including ARC-C and MMLU. However, in the GSM8K task, a 15% performance gap between DeepSeek and OpenChat led to a slight decline in overall performance, reinforcing our observation in Section 3.1 that ensemble methods are most effective when model performances are closely aligned. Furthermore, experiments with LLaMA3 and Qwen2 validated our approach, resulting in performance increases of 3.39% and 4.82% on the GSM8K and MMLU tasks, respectively.\n(2) UNITE demonstrates greater robustness and generality. Ensembling experiments with LLaMA3 and Qwen2 indicate that while LLM-BLENDER improves performance on the GSM8K dataset, it significantly underperforms compared to baseline models in the PIQA, ARC-C, and MMLU benchmarks. Specifically, BLENDER demonstrates a decline of 2.89% in the MMLU task, and this issue is exacerbated in ensembles with the baseline models OpenChat and Mistral, which exhibit relatively low baseline performance, leading to an average performance drop of 3.32%. Additionally, GAC fails to improve the performance of individual models across most tasks and exhibits a significant decline of over 10% when combined with Mistral and OpenChat. Furthermore, due to its intersection limitations, DEEPEN is ill-suited for ensembling with the LLaMA3 model, which requires handling a large vocabulary. In contrast, across tasks with comparable performance levels, UNITE achieves the highest improvements and overall performance across the board, with the exception of a slight under-performance relative to DEEPEN on TriviaQA. This outcome underscores the effectiveness and robustness of UNITE.\n(3) Collaborating with comparable LLMs does not necessarily yield better results. The ensembling experiments conducted with three models, following the integration of LLaMA3.1, demonstrate improved performance on the GSM8K and ARC-C benchmarks compared to the ensemble of LLaMA3 and Qwen2. However, this approach results in suboptimal outcomes on PIQA and MMLU. This observation is justified, as the analysis in Section 3.2 indicates that while combining models with similar performance may enhance overall efficacy, such improvement is not guaranteed."}, {"title": "5.3 ABLATION STUDY", "content": "Effect of hyperparameter k selection To investigate the impact of the hyperparameter k, we conducted experiments using the Mistral and OpenChat models on the TriviaQA and ARC-C datasets. As illustrated in Fig. 4, increasing k from 5 to 10 enhances ensemble performance. However, further increasing k beyond 10 leads to either a slight decline or no change in performance. This finding supports our assertion that, in probability-level ensembling, aligning the entire vocabulary is unnecessary for predicting the next token.\nEffect of the next token selection strategy We also explored the impact of deterministic decoding and top-k sampling (Fan et al., 2018) on the next token emission. It is important to clarify that our UNITE focuses on the fact that, when predicting the next token, we only need to ensemble a subset of tokens rather than the entire vocabulary. In contrast, greedy decoding and top-k sampling emphasize how to determine the next token's ID after the ensemble process is complete. To investigate this, we conducted experiments using LLaMA3.1 and Mistral on the PIQA and ARC-C datasets correspondingly. As shown in Figure 6, for these deterministic tasks, the maximization-based greedy method outperforms the random sampling approach, which aligns with intuition."}, {"title": "5.4 FURTHER ANALYSIS", "content": "Latency analysis Following the settings detailed in GAC (Yu et al., 2024b), we recorded the latency (ms/token) of different methods. The data presented in Fig. 4 reveals significant differences in latency among the various methods. Notably, the latency of UNITE is 87.78 ms/token, which is substantially lower than that of DEEPEN and GAC, and only about 10 ms longer than that of the individual models.\nTokens manipulated each step Table 5 presents tokens manipulated at each step. Compared to DEEPEN and GAC, UNITE significantly reduces the number of tokens manipulated at each step, indicating its efficiency. As the vocabulary of the base model expands, the number of manipulated tokens of DEEPEN and GAC increases significantly, which not only increases the computational burden but may also lead to performance bottlenecks. Contrastly, UNITE is minimally affected, maintaining its effectiveness.\nEnsemble of the dense model and the sparse model We evaluate our method on the ensemble learning of the dense model and the sparse MoE model for reasoning and comprehensive examination tasks. Specifically, we utilize the widely-used large-scale dense model Qwen1.5-72B (Bai et al., 2023) alongside the popular sparse MoE model Mixtral-8\u00d77B (Jiang et al., 2024) as our base models. As shown in Tabel 6, our UNITE achieves +4.86% and +7.29% performance on the ARC-C and PIQA datasets respectively, despite the base models already exhibiting high levels of performance."}, {"title": "6 CONCLUSION", "content": "In conclusion, our research highlights the effectiveness of ensemble methods in enhancing the performance of LLMs. By examining existing techniques, we identified key factors influencing ensembling success, such as model performance and response processes, while finding that vocabulary size has a minimal impact. In addition, the issue of vocabulary redundancy exposed by existing methods during ensembling lead us to propose UNITE, which efficiently aggregates a bunch of tokens from multiple LLMs without the computational overhead. Through extensive experimentation, UNITE consistently outperformed state-of-the-art ensemble methods, demonstrating its effectiveness in leveraging the strengths of diverse LLMs. Our contributions not only advance the understanding of model ensembling but also provide a practical framework for selecting and integrating LLMs to achieve superior performance."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 TASK CHALLENGES ANALYSIS ON TRIVIAQA", "content": "Table 7 provides a detailed comparison of the differences in response styles between LLaMA3 and Qwen2 on the TriviaQA dataset. It is easily observable that, regardless of correctness, LLM-BLENDER consistently tends to choose longer responses as answers."}, {"title": "A.2 CASE STUDY OF UNITE", "content": "Table 8 presents the specific cases of UNITE and other ensembling approaches, showcasing the validity and stability of our method."}]}