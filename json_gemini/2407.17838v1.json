{"title": "UMono: Physical Model Informed Hybrid CNN-Transformer Framework for Underwater Monocular Depth Estimation", "authors": ["Jian Wang", "Jing Wang", "Shenghui Rong", "Bo He"], "abstract": "Underwater monocular depth estimation serves as the foundation for tasks such as 3D reconstruction of underwater scenes. However, due to the influence of light and medium, the underwater environment undergoes a distinctive imaging process, which presents challenges in accurately estimating depth from a single image. The existing methods fail to consider the unique characteristics of underwater environments, leading to inadequate estimation results and limited generalization performance. Furthermore, underwater depth estimation requires extracting and fusing both local and global features, which is not fully explored in existing methods. In this paper, an end-to-end learning framework for underwater monocular depth estimation called UMono is presented, which incorporates underwater image formation model characteristics into network architecture, and effectively utilize both local and global features of underwater image. Specifically, UMono consists of an encoder with a hybrid architecture of CNN and Transformer and a decoder guided by medium transmission map. Firstly, we developed an Under-water Deep Feature Extraction (UDFE) block, which leverages CNN and Transformer in parallel to achieve comprehensive extraction of both local and global features. These features are effectively integrated via the proposed Local Global Feature Fusion (LGFF) module. By stacking the UDFE block as the basic unit, we constructed a hybrid encoder that generates four-stage hierarchical features. Subsequently, the medium transmission map is incorporated into the network as underwater domain knowledge, together with encoded hierarchical features are fed into Underwater Depth Information Aggregation (UDIA) module, which aggregates depth information from physical model and neural network by a proposed cross attention mechanism. Then, the aggregated features serve as the guiding information for each decoding stage, facilitating the model in achieving comprehensive scene understanding and precise depth estimation. The final estimated depth map is obtained through consecutive upsampling processing. Experimental results demonstrate that the proposed method is effective for underwater monocular depth estimation and outperforms the existing methods in both quantitative and qualitative analyses.", "sections": [{"title": "I. INTRODUCTION", "content": "UNDERWATER monocular depth estimation aims to infer the three-dimensional information of the scene through sensors or algorithms, which is crucial for understanding un-derwater scenes and essential for marine tasks such as marine resource exploration [1], underwater robot navigation [2], [3] and 3D reconstruction [4] of underwater scenes. However, due to the influence of the underwater environment (such as water pressure, communication difficulties and energy sup-ply limitations), deploying depth sensors underwater requires exceedingly stringent conditions while entails a substantially high cost. Besides, underwater images often exhibit charac-teristics [5] such as blur distortion, aberration, and feature degradation due to the influence of light conditions, suspended particles, and water quality, making it extremely difficult to obtain accurate depth maps through algorithmic processing. Consequently, underwater monocular depth estimation is a difficult and challenging task.\nTraditional underwater monocular depth estimation ap-proaches can be categorized into active methods and passive methods. Active methods utilize cameras [6] or sensors [7] to acquire depth information of a scene based on the optical properties. For example, structured light sensors [8] capture the deformation of projected structured patterns to infer the depth information of object surfaces; time-of-flight cameras [9] calculate scene depth information based on the round-trip time of light. However, the wavelength-dependent light at-tenuation [10] in underwater environments causes significant changes in optical physics properties, which severely affects the effective operation of sensors and leads to inaccurate monocular depth estimation results. In addition, the deploy-ment of hardware devices underwater presents formidable challenges [11], notably the necessity for impeccably wa-terproof sealing, making these methods exceptionally hard to implement. Passive methods obtain the scene depth by inference underwater image formation model inversely, with some prior knowledge as known conditions. For example, Dark Channel Prior (DCP) [12] and its variations [13], [14], [15] tailored for underwater environments are widely applied to acquire scene depth to further restore underwater images. However, the prior is typically designed for specific situation"}, {"title": "II. RELATED WORK", "content": "Traditional approaches attempt to estimating the medium transmission map to acquire depth information of the scene, which is then applied to the underwater image formation model for image restoration [23], [24]. These approaches pre-dominantly depend on prior information or hand-crafted fea-tures as depth cues for scene depth estimation. Peng et al. [25] estimated scene depth based on the object blurriness, which employed the max filter and closing by morphological re-construction on pixel blurriness map to generate and refine the depth map. Drews et al. [26] proposed a method for underwater image restoration and depth estimation by applying Underwater DCP, which improved DCP to adapt to the under-water environment. Peng et al. [27] extended their previous work [25], which utilized image blurriness and light absorption and estimated depth map more accurately. Peng et al. [28] estimated depth based on the depth-dependent color change, which first estimated a rough depth scattering by considering the impact of scattering on smoothness and gradients, and the refined the depth map via regression analysis on image inten-sity and depth. Chang et al. [29] proposed Submerged DCP for their simplified optics-based underwater image formation"}, {"title": "III. PROPOSED METHOD", "content": "The proposed framework for underwater monocular depth estimation is described in this section. Fig. 2 presents the overview of the proposed UMono, comprising an encoder utilizing hybrid CNN and Transformer, along with a decoder guided by the medium transmission map."}, {"title": "A. Hybrid Encoder", "content": "Local information provides specific details and structure regarding the surfaces of nearby objects, which helps the recovery of continuous depth values within localized regions like object interiors and edges. Global information contributes to structure the entire scene comprehensively, which assists in enhancing the understanding of the scene and inferring the overall depth distribution [41]. Thus, integrating both local and global information can lead to the achievement of more accurate and robust underwater monocular depth estimation. CNN frequently employs 3 \u00d7 3 convolution kernels, restricting information aggregation to local regions and making it chal-lenging to model long-range correlations [42]. In contrast,, Vi-sion Transformer [43] leverages the self-attention mechanism to effectively extract global information. However, due to the patch embedding operation with relatively larger projection kernels, it may result in the loss of local details [44]. Thus, by considering the complementary properties of CNN and Transformer, we propose a hybrid encoder with the capability of extracting features with powerful representation.\nGiven an underwater image \\(I_{RGB} \\in R^{H\\times W\\times 3}\\), the pro-posed encoder can generate four hierarchical features with scales of [1/4, 1/8, 1/16, 1/32] and channels of [C1, C2, C3, C4], documented as E1, E2, E3, and E4 respectively. Specif-ically, in each encoding stage, the input (image or feature maps) is first divided into patches of size 4 \u00d7 4. Subsequently, these patches are fed into [N1, N2, N3, N4] Underwater Depth Feature Extract blocks according to encoding stages to comprehensively extracting Global Information and Local Information. Furthermore, the hierarchical features of the stage are obtained through patch merging processing.\nThe Underwater Depth Feature Extraction block serves as a key component of the proposed encoder, which consists of two parts: feature extraction utilizing CNN block and Transformer block, along with feature aggregation facilitated by Local-Global Feature Fusion Module.\n1) Feature Extraction: The CNN block is implemented by Depth-wise Separable Convolution [45], which employs depthwise convolution and pointwise convolution to capture spatial and cross-channel correlations, respectively. This fac-torized architecture achieves richer representations capacity while exhibits high efficiency, which can be formulated as\n\\[F_L= X + PWConv(BN(DWConv(X)))\\]"}, {"title": "B. Medium Transmission Guided Decoder", "content": "The widely used underwater image formation model [48] can be mathematically expressed as\n\\[I_c(x) = J(x)T(x) + A(1 - T(x))\\]\nwhere I is the observed underwater image, J is the restored underwater image, c\u2208 R, G, B and x is the pixel index, A is the ambient light, T is the medium transmission map, which is related to depth of underwater scene and can be formulated as\n\\[T = e^{-\\beta d}\\]\nwhere \\(\\beta\\) is the non-negative attenuation coefficient of water and d is the scene depth. It is obviously that the medium trans-mission rate decreases exponentially with increasing depth values. Generally, the medium transmission map is pivotal for un-derstanding underwater imaging process and is closely related to the depth, making beneficial to underwater monocular depth estimation task. Thus, we incorporate the medium transmission map as underwater domain knowledge into the decoding pro-cess, leading to a comprehensive understanding of underwater scenes and accurate depth estimation results. Specifically, we firstly estimated the medium transmission map via UDCP [13]. Then, we utilize the Underwater Depth Information Aggrega-tion module to integrate the medium transmission map with hierarchical encoding features. Ultimately, each decoding stage takes corresponding integrated features as guidance to generate the final depth map.\nThe medium transmission map is estimated via prior-based method UDCP, which can be expressed as\n\\[T=1 - \\min_{c \\in \\{R,G,B\\}} \\min_{\\Upsilon \\in \\Omega(x)} (\\frac{I_c(y)}{A})\\]\nwhere T is the estimated medium transmission map, \u03a9 is a local patch centered at pixel x.\nThe medium transmission map contains depth-related infor-mation derived from the physical model of underwater imag-ing, while the hierarchical encoded features encompass depth-related information extracted by the deep neural network. The complementary information between these is crucial for en-hancing the robustness of the model. Therefore, we propose an Underwater Depth Information Aggregation module designed to enhance depth information derived from the physical model and deep neural network.\nThe architecture of the proposed UDIA module is illustrated in Fig. 6. Given an encoded feature \\(E\\in R^{H\\times W\\times C}\\) and medium transmission map \\(T\\in R^{H\\times W\\times 1}\\), UDIA firstly apply the inverse operation to \\(1-T\\) to maintain a positive correlation with the depth value. To make full use of complementary information from T and E, a cross attention mechanism is designed to better guide the decoding process. Specifically, the learnable matrices \\(W_q\\), \\(W_k\\), and \\(W_v\\) are implemented to project T and E to query \\(Q_P\\), key \\(K_P\\), and value \\(V_D\\):\n\\[\\{Q_P,K_P,V_D\\} = \\{TiW_q, E_iW_k, E_iW_v\\}\\]\nwhere i denotes the i-th decoding stage. The complementary between information from physical models and neural net-works is calculated as\n\\[I = Softmax(-\\frac{(Q_P K_P)}{\\sqrt{d}})\\cdot V_D\\]\nwhere d is the dimension of the input feature, the application of Softmax activation function to negative values is utilized to calculate the complementarity between query and key. Then I is used to enhance the encoded feature by\n\\[F_i = E_i + Norm(I)\\]\nThe integrated features encompass rich depth information which serves as guidance for the decoding process. In each decoding stage, the integrated feature \\(F_i\\) is firstly concatenated with the decoding feature Di along the channel dimension. Subsequently, through conv-bn-relu and upsampling processes,"}, {"title": "C. Loss Function", "content": "We use the combination of the L2 loss and the Scale-Invariant Log loss LSILog [49] as the supervised loss function to train the proposed network, which can be defined as\n\\[L_{UMono} = \\lambda L_2 + \\mu L_{SILog}\\]\n\\[L_2=E[d_i-\\hat{d_i}^2]\\]\n\\[L_{SILog} = \\alpha\\frac{1}{N}\\sum g_i^2 - \\frac{\\beta}{N^2}(\\sum g_i)^2\\]\nwhere \\(g_i = log d_i - log \\hat{d_i}\\), \\(\\hat{d_i}\\) represents the ground truth depth, \\(d_i\\) represents the predicted depth, N is the total number of pixels in depth map. The balancing factor \\(\\lambda\\) and \\(\\mu\\) are set to 0.2 and 0.8 according to extensive experiments, \\(\\alpha\\) and \\(\\beta\\) are set to 10 and 0.85 same as [50]."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first introduce the implementation details, followed by a description of the experiment settings, including the Datasets, Evaluation Metrics, and Compared Methods. Then, we evaluate the performance of the proposed UMono through qualitative and quantitative analysis. Ultimately, ab-lation study is conducted to validate the effectiveness of the proposed architecture."}, {"title": "A. Implementation Details", "content": "The proposed method is implemented in PyTorch [51] and trained on a single NVIDIA GeForce RTX 3090 with a batch size of 8. Adam [52] is utilized as the optimizer with weight decay of le-2. The initial learning rate is set to le-4 and the one-cycle learning rate strategy [53] is adopted with a poly adjustment schedule. The model is supervised by RGB-D images pairs covering diverse underwater scenes and trained for 80 epochs, which takes about 24 hours.\nSpecifically, the value of stacked UDFE blocks and embed-ding channels at each encoding stages are set to [3, 4, 6, 3] and [64, 128, 256, 512] respectively, the medium transmission maps are downsampled via pooling operations to adapt to different decoding stages."}, {"title": "B. Experiment Settings", "content": "1) Datasets: The USOD10k dataset [54] contains 10255 underwater RGB images with corresponding depth maps from 12 different underwater scenes. To train and test the proposed UMono, we utilized the split 9229 training samples and 1026 testing samples in the experiments, each image has a resolution of 640 x 480.\nThe Atlantis dataset [55] is employed to evaluate the generalization capability of the UMono, which contains 3200 images with a resolution of 672 \u00d7 512 in various environment type.\n2) Evaluation Metrics: The commonly used evaluation metrics [49] are adopted to evaluate the performance of the UMono, which are defined as follows\n\u2022 Abs Rel: \\(\\frac{1}{N}\\sum \\frac{\\mid d_i-\\hat{d_i}\\mid}{\\hat{d_i}}\\)\n\u2022 Sq Rel: \\(\\frac{1}{N}\\sum \\frac{\\mid d_i-\\hat{d_i}\\mid ^2}{\\hat{d_i}}\\)\n\u2022 RMSE: \\(\\sqrt{\\frac{1}{N}\\sum \\mid d_i-\\hat{d_i} \\mid ^2}\\)\n\u2022 log10: \\(\\frac{1}{N}\\sum \\mid log_{10} d_i - log_{10} \\hat{d_i} \\mid\\)\n\u2022 Accuracy: \\(\\%\\) of \\(\\hat{d_i}\\), \\(d_i\\) s.t. max \\((\\frac{\\hat{d_i}}{d_i}, \\frac{d_i}{\\hat{d_i}})\\) = \\(\\delta < th\\)\nwhere \\(\\hat{d_i}\\) is the predicted depth, \\(d_i\\) is the ground truth, N denotes the total number of pixels in depth map.\n3) Compared Methods: We compared the proposed UMono with 7 methods, including GDCP [28], UW-Net [20], LapDepth [56], BTS [57], LiteMono [58], TransDepth [59], UDepth [37].\nGDCP is a traditional prior-based method which measures scene depth to restore underwater images through image formation model. LapDepth and BTS are classic CNN-based methods for monocular depth method. LiteMono and Trans-Depth are typical and exemplary methods which implement a hybrid architecture of CNN and Transformer. UW-Net and UDepth are specifically designed for underwater monocu-lar depth estimation, notably UW-Net is an unsupervised method based on CycleGAN [60] where UDepth employs a lightweight CNN and MiniViT for supervised learning."}, {"title": "C. Qualitative Comparison", "content": "Qualitative comparisons of UMono with the representative methods on USOD10k dataset are shown in Fig. 7. GDCP [28] estimates erroneous results due to the limitations imposed by prior information, while UW-Net [20] also produces poor depth map with no haze in input images as depth cue. BTS [57] and LapDepth [56] fail to accurately capture depth distribu-tions, resulting in deviations from the ground truth at both background and object. LiteMono [58] and TransDepth [59] estimate depth maps with better visual quality, while the results are not satisfactory on the detailed representation of objects, such as edge continuity and interior details. UDepth [37] performs well in depth layout but suffers from. In compar-ison, the proposed UMono effectively estimates precise depth maps exhibiting rich local details and accurate global layout, which suggests the satisfactory performance of the designed framework for underwater monocular depth estimation."}, {"title": "D. Quantitative Comparison", "content": "Quantitative comparisons on USOD10k dataset are con-ducted to quantify the performance of different compared methods, including GDCP [28], UW-Net [20], LapDepth [56], BTS [57], LiteMono [58], TransDepth [59], UDepth [37]. The results of commonly used evaluation metrics are demonstrated in Table I. Compared with other methods, UMono achieves the best performance for all metrics, with improvements of over 3.1% and 10.9 % in terms of \\(\\delta_3\\) and RMSE, respectively. Quantitative comparisons results on Atlantis dataset are pre-"}, {"title": "E. Ablation Study", "content": "Ablation experiments are designed to further analyze the effectiveness of the main components in UMono, including the Underwater Depth Feature Extraction (UDFE) block, Local-Global Feature Fusion (LGFF) module and Underwater Depth Information Aggregation (UDIA) module.\n1) The benefit of UDFE block: The UDFE module is designed for leveraging CNN and Transformer in parallel to extract both local and global information for monocular depth estimation, in which the LGFF module effectively integrate the local and global features to enhance the representation"}, {"title": "V. CONCLUSION", "content": "Generally, we propose a deep learning-based supervised framework for underwater monocular depth estimation. In the hybrid encoder, we combine the CNN and Transformer to fully extract the local and global information, where the following LGFF module effectively integrate these features to enhance the feature representation. In addition, we in-corporate the medium transmission map as the underwater domain knowledge into the network by a designed cross at-tention mechanism, and utilize the UDIA module to aggregate the depth-related information as the guidance for decoding process. Extensive experimental results demonstrate that the proposed UMono outperforms existing methods in both visual quality and quantitative metrics and the designed module is effective for underwater monocular depth estimation. In the future, we plan to joint underwater monocular depth estimation with other visual tasks, such as underwater image enhancement and underwater object detection, leveraging the relevance of these visual tasks to achieve better performance."}]}