{"title": "UMono: Physical Model Informed Hybrid CNN-Transformer Framework for Underwater Monocular Depth Estimation", "authors": ["Jian Wang", "Jing Wang", "Shenghui Rong", "Bo He"], "abstract": "Underwater monocular depth estimation serves as the foundation for tasks such as 3D reconstruction of underwater scenes. However, due to the influence of light and medium, the underwater environment undergoes a distinctive imaging process, which presents challenges in accurately estimating depth from a single image. The existing methods fail to consider the unique characteristics of underwater environments, leading to inadequate estimation results and limited generalization performance. Furthermore, underwater depth estimation requires extracting and fusing both local and global features, which is not fully explored in existing methods. In this paper, an end-to-end learning framework for underwater monocular depth estimation called UMono is presented, which incorporates underwater image formation model characteristics into network architecture, and effectively utilize both local and global features of underwater image. Specifically, UMono consists of an encoder with a hybrid architecture of CNN and Transformer and a decoder guided by medium transmission map. Firstly, we developed an Underwater Deep Feature Extraction (UDFE) block, which leverages CNN and Transformer in parallel to achieve comprehensive extraction of both local and global features. These features are effectively integrated via the proposed Local Global Feature Fusion (LGFF) module. By stacking the UDFE block as the basic unit, we constructed a hybrid encoder that generates four-stage hierarchical features. Subsequently, the medium transmission map is incorporated into the network as underwater domain knowledge, together with encoded hierarchical features are fed into Underwater Depth Information Aggregation (UDIA) module, which aggregates depth information from physical model and neural network by a proposed cross attention mechanism. Then, the aggregated features serve as the guiding information for each decoding stage, facilitating the model in achieving comprehensive scene understanding and precise depth estimation. The final estimated depth map is obtained through consecutive upsampling processing. Experimental results demonstrate that the proposed method is effective for underwater monocular depth estimation and outperforms the existing methods in both quantitative and qualitative analyses.", "sections": [{"title": "I. INTRODUCTION", "content": "UNDERWATER monocular depth estimation aims to infer the three-dimensional information of the scene through sensors or algorithms, which is crucial for understanding underwater scenes and essential for marine tasks such as marine resource exploration [1], underwater robot navigation [2], [3] and 3D reconstruction [4] of underwater scenes. However, due to the influence of the underwater environment (such as water pressure, communication difficulties and energy supply limitations), deploying depth sensors underwater requires exceedingly stringent conditions while entails a substantially high cost. Besides, underwater images often exhibit characteristics [5] such as blur distortion, aberration, and feature degradation due to the influence of light conditions, suspended particles, and water quality, making it extremely difficult to obtain accurate depth maps through algorithmic processing. Consequently, underwater monocular depth estimation is a difficult and challenging task.\nTraditional underwater monocular depth estimation ap- proaches can be categorized into active methods and passive methods. Active methods utilize cameras [6] or sensors [7] to acquire depth information of a scene based on the optical properties. For example, structured light sensors [8] capture the deformation of projected structured patterns to infer the depth information of object surfaces; time-of-flight cameras [9] calculate scene depth information based on the round-trip time of light. However, the wavelength-dependent light attenuation [10] in underwater environments causes significant changes in optical physics properties, which severely affects the effective operation of sensors and leads to inaccurate monocular depth estimation results. In addition, the deployment of hardware devices underwater presents formidable challenges [11], notably the necessity for impeccably waterproof sealing, making these methods exceptionally hard to implement. Passive methods obtain the scene depth by inference underwater image formation model inversely, with some prior knowledge as known conditions. For example, Dark Channel Prior (DCP) [12] and its variations [13], [14], [15] tailored for underwater environments are widely applied to acquire scene depth to further restore underwater images. However, the prior is typically designed for specific situation"}, {"title": null, "content": "and perform unsatisfactorily in diverse types of underwater images, thus resulting in poor generalization capability of these prior-based methods.\nIn recent years, deep learning-based methods have achieved significant advancements in various visual tasks, including monocular depth estimation [16], [17]. Researchers treat monocular depth estimation as a regression problem and exploit deep neural networks to extract depth-relevant features from RGB images in order to estimate the depth map from a single image. These methods benefit from the feature extrac- tion capability of encoders and require training on a large-scale dataset, such as KITTI [18] and NYUDepth [19], to achieve superior accuracy and generalization capability. In the field of underwater studies, due to the lack of large-scale underwater depth datasets, researchers resort to unsupervised learning methods or utilize synthetic datasets to train models. For in- stance, UW-Net [20] proposed an unsupervised framework for monocular depth estimation based on cycle-consistent learn- ing. UWGAN [21] trained generative adversarial networks (GAN) [22] using synthetic underwater datasets and presented a framework applicable for underwater monocular depth es- timation and image enhancement. However, these methods rely on stacked convolution blocks and do not fully exploit the global features of underwater images. Additionally, they fail to consider the characteristics of underwater environment within the model framework, which hinders a comprehensive understanding of underwater scenes, thus leading to poor accuracy and generalization capabilities.\nConsidering the aforementioned issues, this paper proposes a novel end-to-end framework for underwater monocular depth estimation, termed as UMono. The key idea of the proposed approach lies in enhancing the representation ability of en- coded features and integrating underwater domain knowledge into the model framework.\nThe proposed UMono consists of a hybrid encoder of CNN and Transformer and a decoder guided by medium transmission map. The encoder is composed of a stack of Underwater Depth Feature Extraction(UDFE) blocks, which leverages CNN and Transformer in parallel to extract features simultaneously. The former is used for extracting local features and spatial structural information, while the latter is utilized for modeling long-distance dependencies. The combination can ensure the continuity of depth values and the accuracy of overall distribution, which is crucial for underwater monocular depth estimation regression tasks. Based on the considera- tion of the varying dependence of depth values on local and global features. we propose a Local-Global Feature Fu- sion(LGFF) module for fusing local and global features, which efficiently combines extracted features by calculating weight maps for different features. Furthermore, through the stacking of Underwater Depth Feature Extraction(UDFE) block, an encoder capable of generating four-stage hierarchical features is constructed. Furthermore, underwater domain knowledge is integrated into the decoder network to achieve a more comprehensive understanding of underwater scenarios. The medium transmission map is a key parameter in the underwater image formation model, which represents the percentage of scene radiance reaching the camera and can reflect the depth"}, {"title": null, "content": "information of the scene. Therefore, we propose to guide the decoding process with the medium transmission map and hierarchical encoded features, both of which contain crucial depth-related information and are highly beneficial for recon- structing the depth map. Specifically, the Underwater Depth Information Aggregation (UDIA) module is designed based on a cross attention mechanism, which adaptively integrates medium transmission map and hierarchical encoded features to acquire enhanced depth-related features for progressively guiding the decoding process.\nThe contributions of the proposed method can be summa- rized as follows.\n1) A hybrid Underwater Depth Feature Extraction (UDFE) block that utilizes CNN and Transformer is proposed to extract and fuse local and global features required for underwater monocular depth estimation, ensuring the local details and global layout in the final depth map.\n2) The underwater domain knowledge is incorporated into the decoding network by utilizing medium transmission map, fusing with hierarchical encoded features through a cross attention based Underwater Depth Information Aggregation(UDIA) module, guiding the decoding pro- cess for better understanding of underwater scenes.\n3) The proposed UMono demonstrates effective perfor- mance in underwater depth estimation, achieving com- paratively favorable results on benchmark datasets in terms of qualitative analysis and quantitative metrics.\nThe remaining sections of this paper are organized as follows. Section II introduces a brief review of related works for underwater monocular depth estimation. Section III de- scribes the architecture of the proposed framework in detail. Section IV includes the qualitative, quantitative and ablation experiments results and analysis. Section V presents the con- clusion of this paper."}, {"title": "II. RELATED WORK", "content": "Traditional approaches attempt to estimating the medium transmission map to acquire depth information of the scene, which is then applied to the underwater image formation model for image restoration [23], [24]. These approaches pre- dominantly depend on prior information or hand-crafted fea- tures as depth cues for scene depth estimation. Peng et al. [25] estimated scene depth based on the object blurriness, which employed the max filter and closing by morphological re- construction on pixel blurriness map to generate and refine the depth map. Drews et al. [26] proposed a method for underwater image restoration and depth estimation by applying Underwater DCP, which improved DCP to adapt to the under- water environment. Peng et al. [27] extended their previous work [25], which utilized image blurriness and light absorption and estimated depth map more accurately. Peng et al. [28] estimated depth based on the depth-dependent color change, which first estimated a rough depth scattering by considering the impact of scattering on smoothness and gradients, and the refined the depth map via regression analysis on image inten- sity and depth. Chang et al. [29] proposed Submerged DCP for their simplified optics-based underwater image formation"}, {"title": null, "content": "model and straightforwardly estimated the scene depth map. Raihan et al. [30] utilized background neutralization to acquire blurriness and background light information from single view image, as the depth cues to estimate depth map.\nHowever, the inadequacy of these methods is apparent, as the prior information is only applicable to specific images, resulting in poor generalization capability of traditional ap- proaches. Furthermore, due to the unique optical properties underwater, estimating underwater imaging parameters poses significant challenges, and traditional methods are also unsat- isfactory in terms of accuracy.\nIn recent years, the rapid development of deep learning has shown tremendous potential in visual tasks, demon- strating great prowess and promise. Various methods have been proposed to promote the development of underwater monocular depth estimation through deep neural network. Gupta et al. [20] proposed an unsupervised method, they indirectly estimated depth map in the style transfer manner through cycle-consistent learning, which mapped the unpaired terrestrial hazy images and underwater images by using haze as the depth cue. Ye et al. [31] performed underwater color correction and depth estimation based on a joint learning architecture, achieving favorable results by exploiting the cor- relation between visual tasks. Hambarde et al. [21] proposed a GAN-Based method for underwater single image depth esti- mation and enhancement, the method employed the cascaded- stream architecture which estimated the fine-level depth by optimizing coarse-level depth. Zhao et al. [32] proposed a joint framework to synthesize underwater images and estimate depth based on adversarial learning, and designed a depth loss to mitigate texture leakage problems. Yang et al. [33] proposed a self-supervised framework for underwater monocular depth estimation, which considered underwater light attenuation as potential depth clues to estimate depth and utilized optical flow to refine the estimated depth map. Wang et al. [34] proposed a self-supervised model for underwater monocular sequences depth estimation which leveraged the correlation between consecutive frames to solve the scale ambiguity prob- lem, the predicted depth was further utilized for underwater image enhancement. Liu et al. [35] combined single-beam echosounder with monocular camera, representing the single range measurement as mask and integrating it as additional cues for depth estimation. Amitai et al. [36] utilized specific underwater data augmentation and incorporated photometric prior into the loss for self-supervised learning. Yu et al. [37] designed a new input space for depth estimation based on of underwater domain characteristics and proposed a lightweight pipeline which combined MobileNetV2 [38] and MiniViT [39] for fast monocular depth estimation. Ebner et al. [40] generated sparse depth prior by extracting feature points, and then fed it into the network via dense parameterization, as an extension of [37], this method improved the prediction accuracy on dense depth estimation.\nThese underwater monocular depth estimation models pri- marily focus on transferring terrestrial models, lacking thor- ough consideration for the characteristics of underwater scenes. Furthermore, owing to extracting features via stacked convolution layers, current methods have limited capability in"}, {"title": null, "content": "fully exploiting local and global information which are both essential for depth estimation. Thus, we are of the opinion that, a comprehensive extraction of features and integration of underwater domain knowledge are crucial for underwater monocular depth estimation."}, {"title": "III. PROPOSED METHOD", "content": "The proposed framework for underwater monocular depth estimation is described in this section. Fig. 2 presents the overview of the proposed UMono, comprising an encoder utilizing hybrid CNN and Transformer, along with a decoder guided by the medium transmission map."}, {"title": "A. Hybrid Encoder", "content": "Local information provides specific details and structure regarding the surfaces of nearby objects, which helps the recovery of continuous depth values within localized regions like object interiors and edges. Global information contributes to structure the entire scene comprehensively, which assists in enhancing the understanding of the scene and inferring the overall depth distribution [41]. Thus, integrating both local and global information can lead to the achievement of more accurate and robust underwater monocular depth estimation. CNN frequently employs 3 \u00d7 3 convolution kernels, restricting information aggregation to local regions and making it chal- lenging to model long-range correlations [42]. In contrast,, Vision Transformer [43] leverages the self-attention mechanism to effectively extract global information. However, due to the patch embedding operation with relatively larger projection kernels, it may result in the loss of local details [44]. Thus, by considering the complementary properties of CNN and Transformer, we propose a hybrid encoder with the capability of extracting features with powerful representation.\nGiven an underwater image \\(I_{RGB} \\in R^{H \\times W \\times 3}\\), the pro- posed encoder can generate four hierarchical features with scales of [1/4, 1/8, 1/16, 1/32] and channels of \\([C_1, C_2, C_3, C_4]\\), documented as \\(E_1\\), \\(E_2\\), \\(E_3\\), and \\(E_4\\) respectively. Specifically, in each encoding stage, the input (image or feature maps) is first divided into patches of size 4 \u00d7 4. Subsequently, these patches are fed into \\([N_1, N_2, N_3, N_4]\\) Underwater Depth Feature Extract blocks according to encoding stages to comprehensively extracting Global Information and Local Information. Furthermore, the hierarchical features of the stage are obtained through patch merging processing.\nThe Underwater Depth Feature Extraction block serves as a key component of the proposed encoder, which consists of two parts: feature extraction utilizing CNN block and Transformer block, along with feature aggregation facilitated by Local- Global Feature Fusion Module."}, {"title": null, "content": "1) Feature Extraction: The CNN block is implemented by Depth-wise Separable Convolution [45], which employs depthwise convolution and pointwise convolution to capture spatial and cross-channel correlations, respectively. This fac- torized architecture achieves richer representations capacity while exhibits high efficiency, which can be formulated as\n\\(F_L = X + PWConv(BN(DWConv(X)))\\)   (1)"}, {"title": null, "content": "where \\(PWConv\\) denotes the \\(1 \\times 1\\) pointwise convolution, \\(BN\\) represents batch normalization operation and \\(DWConv\\) stands for the \\(3 \\times 3\\) depth-wise convolution. The CNN block possesses the capability to effectively learn local features of an image, including edges, corners and clues related to object shapes, which are crucial for monocular depth estimation.\nThe Transformer block utilizes the spatial reduction scheme introduced in [46], [47] to efficiently compute the self- attention. Given an input feature \\(X \\in R^{H \\times W \\times C}\\), the queries \\(Q_E = XW_q\\), keys \\(K_E = XW_k\\), and values \\(V_E = XW_v\\) are obtained through linear projection, where \\(W_q, W_q\\), and \\(W_v\\) are linear projection matrices. \\(Q_E, K_E, V_E\\) have the same dimensions \\(R^{(HW) \\times C}\\), then the spatial reduction scheme is implemented to \\(K_E\\) and \\(V_E\\) for efficient calculation, which reshape \\(K_E\\) and \\(V_E\\) to dimensions \\(\\frac{HW}{R} \\times \\frac{C}{HW}\\) via reducing the spatial dimension, where R is the reduction ratio. The self- attention is calculated as\n\\(Attention(Q_E, K_E, V_E) = Softmax(\\frac{(Q_E K_E)^T}{\\sqrt{d_{head}}})V_E\\) (2)"}, {"title": null, "content": "where \\(d_{head}\\) is the channel dimension of each head. Ulti- mately, the final global feature \\(F_G\\) is achieved by concate- nating the Attention calculated by each attention head:\n\\(F_G = Concat(A_1,..., A_i,..., A_N)W_g\\)  (3)"}, {"title": null, "content": "where \\(Concat\\) denotes the concatenation operation, \\(A_i\\) repre- sents the attention of i-th head, N is the total head number, and \\(W_g\\) is the linear projection matrix. The Transformer block utilizes spatial reduction attention, reducing the computational complexity by \\(R^2\\) times. This approach enables the model to effectively understand the global structure and contextual information of the image, which is beneficial to achieving accurate global layout of depth in underwater scenes."}, {"title": "2) Feature Aggregation:", "content": "Considering the varying depen- dence of depth values across pixels on both local and global information, for instance, the depth of edges and interiors of objects is primarily influenced by local regions to maintain consistency. Conversely, Certain pixels play a crucial role in maintaining a coherent depth layout, necessitating a higher reliance on global information. Therefore, we propose a Local Global Feature Fusion (LGFF) module designed to adaptively fuse local and global features based on their respective depen- dency levels.\nThe architecture of the proposed LGFF module is depicted in Fig. 4. Given the local feature \\(F_L \\in R^{H \\times W \\times C}\\) and global feature \\(F_G \\in R^{H \\times W \\times C}\\) extracted by CNN and Transformer, the LGFF first concatenates them along the channel dimension. The concatenated feature \\(F \\in R^{H \\times W \\times 2C}\\) is then processed through multiple convolutional layers, including a sequence of"}, {"title": null, "content": "3\u00d73 Convolution, batch normalization, and ReLU function. Fi- nally, a single-channel weight map \\(W \\in R^{H \\times W \\times 1}\\) is obtained using the Sigmoid function. These weights are applied to \\(F_L\\) and \\(F_G\\) to construct the fused feature \\(F_E \\in R^{H \\times W \\times C}\\) with a powerful representation. To sum up, The proposed LGFF can be expressed as\n\\(F = Concat(F_L, F_G)\\)   (4)\n\\(W = Sigmoid(conv.b.r(F))\\)  (5)\n\\(F_E = F_L \\odot W + F_G \\odot (1 - W)\\)   (6)"}, {"title": null, "content": "where \\(\\odot\\) denotes Hadamard product. The LGFF effectively integrates local and global information, which is crucial for underwater monocular depth estimation, leading to more ac- curate results."}, {"title": "B. Medium Transmission Guided Decoder", "content": "The widely used underwater image formation model [48] can be mathematically expressed as\n\\(I_c(x) = J(x)T(x) + A(1 \u2013 T(x))\\)  (7)"}, {"title": null, "content": "where I is the observed underwater image, J is the restored underwater image, \\(c \\in {R, G, B}\\) and x is the pixel index, A is the ambient light, T is the medium transmission map, which is related to depth of underwater scene and can be formulated as\n\\(T = e^{-\\beta d}\\)   (8)"}, {"title": null, "content": "where \u03b2 is the non-negative attenuation coefficient of water and d is the scene depth. It is obviously that the medium trans- mission rate decreases exponentially with increasing depth values. Fig. 5 illustrates several RGB images, medium trans- mission maps, and depth maps, clearly demonstrating the inverse correlation between the transmission map and the depth map."}, {"title": null, "content": "Generally, the medium transmission map is pivotal for un- derstanding underwater imaging process and is closely related to the depth, making beneficial to underwater monocular depth estimation task. Thus, we incorporate the medium transmission map as underwater domain knowledge into the decoding pro- cess, leading to a comprehensive understanding of underwater scenes and accurate depth estimation results. Specifically, we firstly estimated the medium transmission map via UDCP [13]. Then, we utilize the Underwater Depth Information Aggrega- tion module to integrate the medium transmission map with hierarchical encoding features. Ultimately, each decoding stage takes corresponding integrated features as guidance to generate the final depth map.\nThe medium transmission map is estimated via prior-based method UDCP, which can be expressed as\n\\(T=1 - min_{\\Upsilon \\in \\Omega(x)} min_{c \\in {R,G,B}}( \\frac{I_c(y)}{\\mathcal{A}^c} )\\)  (9)"}, {"title": null, "content": "where T is the estimated medium transmission map, \u03a9 is a local patch centered at pixel x.\nThe medium transmission map contains depth-related infor- mation derived from the physical model of underwater imag- ing, while the hierarchical encoded features encompass depth- related information extracted by the deep neural network. The complementary information between these is crucial for en- hancing the robustness of the model. Therefore, we propose an Underwater Depth Information Aggregation module designed to enhance depth information derived from the physical model and deep neural network.\nThe architecture of the proposed UDIA module is illustrated in Fig. 6. Given an encoded feature \\(E_i \\in R^{H \\times W \\times C}\\) and medium transmission map \\(T \\in R^{H \\times W \\times 1}\\), UDIA firstly apply the inverse operation to \\(1-T\\) to maintain a positive correlation with the depth value. To make full use of complementary information from T and E, a cross attention mechanism is designed to better guide the decoding process. Specifically, the learnable matrices \\(W_q\\), \\(W_k\\), and \\(W_v\\) are implemented to project T and E to query \\(Q_i\\), key \\(K_i\\), and value \\(V_i\\):\n\\(\\{Q_i, K_i, V_i\\} = \\{T_i W_q, E_i W_k, E_i W_v\\}\\) (10)"}, {"title": null, "content": "where i denotes the i-th decoding stage. The complementary between information from physical models and neural net- works is calculated as\n\\(I = Softmax(\\frac{-(Q_i K_i)^T}{\\sqrt{d_{head}}})V_i\\) (11)"}, {"title": null, "content": "where d is the dimension of the input feature, the application of \\(Softmax\\) activation function to negative values is utilized to calculate the complementarity between query and key. Then \\(I_i\\) is used to enhance the encoded feature by\n\\(F_i = E_i + Norm(I_i)\\)  (12)"}, {"title": null, "content": "The integrated features encompass rich depth information which serves as guidance for the decoding process. In each decoding stage, the integrated feature \\(F_i\\) is firstly concatenated with the decoding feature \\(D_i\\) along the channel dimension. Subsequently, through conv-bn-relu and upsampling processes,"}, {"title": null, "content": "the input decoding feature for the next decoding stage is derived. This process can be summarized as\n\\(D_{i-1} = UP(conv.b.r(Concat(F_i, D_i)))\\) (13)"}, {"title": null, "content": "where UP represents the bilinear upsample operation, Concat denotes concatenation operation. After performing sigmoid and upsample processing on \\(D_1\\), we obtain the final predicted depth map \\(D \\in R^{H \\times W \\times 1}\\)."}, {"title": "C. Loss Function", "content": "We use the combination of the \\(L_2\\) loss and the Scale- Invariant Log loss \\(L_{SILog}\\) [49] as the supervised loss function to train the proposed network, which can be defined as\n\\(L_{UMono} = \\lambda L_2 + \\mu L_{SILog}\\) (14)\n\\(L_2 = E[d_i-d_i\\)  (15)\n\\(L_{SILog} = \\alpha \\frac{1}{N} \\sum_i g_i^2 - \\frac{\\beta}{N^2} (\\sum_i g_i)^2\\)]  (16)"}, {"title": null, "content": "where \\(g_i = log d_i - log d_i\\), \\(d_i\\) represents the ground truth depth, \\(d_i\\) represents the predicted depth, N is the total number of pixels in depth map. The balancing factor \u03bb and \u03bc are set to 0.2 and 0.8 according to extensive experiments, \u03b1 and \u03b2 are set to 10 and 0.85 same as [50]."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first introduce the implementation details, followed by a description of the experiment settings, including the Datasets, Evaluation Metrics, and Compared Methods. Then, we evaluate the performance of the proposed UMono through qualitative and quantitative analysis. Ultimately, ab- lation study is conducted to validate the effectiveness of the proposed architecture."}, {"title": "A. Implementation Details", "content": "The proposed method is implemented in PyTorch [51] and trained on a single NVIDIA GeForce RTX 3090 with a batch size of 8. Adam [52] is utilized as the optimizer with weight decay of le-2. The initial learning rate is set to le-4 and the one-cycle learning rate strategy [53] is adopted with a poly adjustment schedule. The model is supervised by RGB-D images pairs covering diverse underwater scenes and trained for 80 epochs, which takes about 24 hours.\nSpecifically, the value of stacked UDFE blocks and embed- ding channels at each encoding stages are set to [3, 4, 6, 3] and [64, 128, 256, 512] respectively, the medium transmission maps are downsampled via pooling operations to adapt to different decoding stages."}, {"title": "B. Experiment Settings", "content": "1) Datasets: The USOD10k dataset [54] contains 10255 underwater RGB images with corresponding depth maps from 12 different underwater scenes. To train and test the proposed UMono, we utilized the split 9229 training samples and 1026 testing samples in the experiments, each image has a resolution of 640 x 480.\nThe Atlantis dataset [55] is employed to evaluate the generalization capability of the UMono, which contains 3200 images with a resolution of 672 \u00d7 512 in various environment type."}, {"title": "2) Evaluation Metrics:", "content": "The commonly used evaluation metrics [49] are adopted to evaluate the performance of the UMono, which are defined as follows\n\u2022 Abs Rel: \\(\\frac{1}{N} \\sum \\frac{|d_i -d_i|}{d_i}\\)\n\u2022 Sq Rel: \\(\\frac{1}{N} \\sum \\frac{||d_i -d_i||^2}{d_i}\\)\n\u2022 RMSE: \\(\\sqrt{\\frac{1}{N} \\sum |d_i -d_i|^2}\\)\n\u2022 log10: \\(\\frac{1}{N} \\sum |log10 d_i - log10 d_i|\\)\n\u2022 Accuracy:% of \\(d_i\\) s.t. \\(max(\\frac{d_i}{d_i}, \\frac{d_i}{d_i} ) = \\delta < th\\)\nwhere di is the predicted depth, d\u2081 is the ground truth, N denotes the total number of pixels in depth map."}, {"title": "3) Compared Methods:", "content": "We compared the proposed UMono with 7 methods, including GDCP [28], UW-Net [20], LapDepth [56], BTS [57], LiteMono [58], TransDepth [59], UDepth [37].\nGDCP is a traditional prior-based method which measures scene depth to restore underwater images through image formation model. LapDepth and BTS are classic CNN-based methods for monocular depth method. LiteMono and Trans- Depth are typical and exemplary methods which implement a hybrid architecture of CNN and Transformer. UW-Net and UDepth are specifically designed for underwater monocu- lar depth estimation, notably UW-Net is an unsupervised method based on CycleGAN [60] where UDepth employs a lightweight CNN and MiniViT for supervised learning."}, {"title": "C. Qualitative Comparison", "content": "Qualitative comparisons of UMono with the representative methods on USOD10k dataset are shown in Fig. 7. GDCP [28] estimates erroneous results due to the limitations imposed by prior information, while UW-Net [20] also produces poor depth map with no haze in input images as depth cue. BTS [57] and LapDepth [56] fail to accurately capture depth distribu- tions, resulting in deviations from the ground truth at both background and object. LiteMono [58] and TransDepth [59] estimate depth maps with better visual quality, while the results are not satisfactory on the detailed representation of objects, such as edge continuity and interior details. UDepth [37] performs well in depth layout but suffers from. In compar- ison, the proposed UMono effectively estimates precise depth maps exhibiting rich local details and accurate global layout, which suggests the satisfactory performance of the designed framework for underwater monocular depth estimation."}, {"title": "D. Quantitative Comparison", "content": "Quantitative comparisons on USOD10k dataset are con- ducted to quantify the performance of different compared methods, including GDCP [28], UW-Net [20], LapDepth [56], BTS [57], LiteMono [58], TransDepth [59], UDepth [37]. The results of commonly used evaluation metrics are demonstrated in Table I. Compared with other methods, UMono achieves the best performance for all metrics, with improvements of over 3.1% and 10.9 % in terms of \u03b43 and RMSE, respectively. Quantitative comparisons results on Atlantis dataset are pre-"}, {"title": "E. Ablation Study", "content": "Ablation experiments are designed to further analyze the effectiveness of the main components in UMono, including the Underwater Depth Feature Extraction (UDFE) block, Local- Global Feature Fusion (LGFF) module and Underwater Depth Information Aggregation (UDIA) module."}, {"title": "1) The benefit of UDFE block:", "content": "The UDFE module is designed for leveraging CNN and Transformer in parallel to extract both local and global information for monocular depth estimation, in which the LGFF module effectively integrate the local and global features to enhance the representation"}, {"title": null, "content": "capacity. We verify the effectiveness of UDFE by adjusting the architecture, including a total of 3 designs\n\u2022 w/ CNN represents only utilizing CNN block for feature extraction.\n\u2022 w/ Transformer represents only utilizing Transformer block for feature extraction.\n\u2022 w/o LGFF represents replacing LGFF with the method of element-wise addition.\nThe ablation architecture are all train from scratch for 60 epochs with a batch size of 8. The qualitative and quantitative results are shown in Fig. 9 and Table III respectively.\nAs presented in Fig. 9, only utilizing CNN and Transformer block for feature extraction lead to relatively poor results, due to the lack or insufficiency of essential information necessary for monocular depth estimation. The results of 'w/o LGFF' also have an unsatisfactory performance on local details such"}, {"title": null, "content": "as edges and fail to achieve the effective integration of features, which is further corroborated by the quantitative results in Table III. While the proposed UDFE with LGFF module based on the features dependency levels, effectively aggregates local and global information, leading to better local details and global layout, and an improvement of over 13.3% in term of Sq Rel. The experiments results demonstrate that the hybrid architecture is effective for feature extraction and the LGFF module enhances the features representation via feature aggregation, indicating the UDFE is beneficial for underwater monocular depth estimation."}, {"title": "2) The benefit of UDIA module:", "content": "The UDIA module is designed for aggregating the medium transmission maps and hierarchical encoded features to guide the decoding process. To evaluate the importance of UDIA, we establish several vari- ants to substitute UDIA for providing guidance information, which are as follows\n\u2022 w/o UDIA denotes no information serve as guidance for the decoding process.\n\u2022 w/ HEF denotes the hierarchical encoded features serve as guidance for the decoding process.\n\u2022 w/ MTM denotes the medium transmission maps serve as guidance for the decoding process.\nThe experiments results are presented in Fig. 10 and Table IV. As shown in Fig. 10, the ablated designs 'w/o UDIA' produces poor results with discontinuous depth due to the lack of guidance information. 'w/ HEF' and 'w/ MTM' introduce single guiding information from the deep neural network and physical model, but it is not sufficient to produce satisfactory results. It should be noted that the aforementioned designs may introduce the gradient explosion problem during the training process, leading to difficulties for the model to converge. Compared to all the ablated designs, The benefit of introducing the UDIA module is to enhance depth information by computing the complementation between the hierarchical encoded features and medium transmission maps. The enhanced features, serving as the guidance with depth information from the physical model and deep neural network for the decoding process, is beneficial for achieving a comprehensive understanding of underwater scenes, thus leading to the optimal depth estimation results."}, {"title": "F. Complexity Analysis", "content": "Complexity comparisons experiments are conducted to ver- ify the efficiency of the proposed UMono, while models' pa- rameters, memory and FLOPs served as the evaluation metrics. As shown in Table V, LiteMono [58] and UDepth [37] achieve superior model size(8.78M) and FLOPs(77.16G) respectively, however the balance between complexity and precision is not satisfactory. LapDepth [56] and TransDepth [59] yield sub- optimal quantitative comparisons results, while have a large model size with over 2.6\u00d7 and 8.8\u00d7 parameters than UMono. Compared with these methods, UMono's performance in model```json\n complexity is less than excellent. However, in the aforementioned qualitative and quantitative analysis, UMono achieves a promising improvement in all evaluation metrics. In general, we hold the opinion that these gaps on complexity are acceptable under the premise of relatively high precision."}, {"title": "V. CONCLUSION", "content": "Generally, we propose a deep learning-based supervised framework for underwater monocular depth estimation. In the hybrid encoder, we combine the CNN and Transformer to fully extract the local and global information, where the following LGFF module effectively integrate these features to enhance the feature representation. In addition, we in- corporate the medium transmission map as the underwater domain knowledge into the network by a designed cross at- tention mechanism, and utilize the UDIA module to aggregate the depth-related information as the guidance for decoding process. Extensive experimental results demonstrate that the proposed UMono outperforms existing methods in both visual quality and quantitative metrics and the designed module is effective for underwater monocular depth estimation. In the future, we plan to joint underwater monocular depth estimation with other visual tasks, such as underwater image enhancement and underwater object detection, leveraging the relevance of these visual tasks to achieve better performance."}]}