{"title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states", "authors": ["Luis Ibanez-Lissen", "Lorena Gonzalez-Manzano", "Jose Maria de Fuentes", "Nicolas Anciaux", "Joaquin Garcia-Alfaro"], "abstract": "Large Language Models (LLMs) are increasingly used in a variety of applications, but concerns around membership inference have grown in parallel. Previous efforts focus on black-to-grey-box models, thus neglecting the potential benefit from internal LLM information. To address this, we propose the use of Linear Probes (LPs) as a method to detect Membership Inference Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner workings. We test this method across several model architectures, sizes and datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA achieves an average gain of 15.71% in Area Under the Curve (AUC) over previous techniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases an increment of 46.80% against the state of the art. Furthermore, our approach reveals key insights, such as the model layers where MIAs are most detectable. In multimodal models, LPs indicate that visual inputs can significantly contribute to detect MIAs - AUC>60% is reached in 85.90% of experiments.", "sections": [{"title": "1 INTRODUCTION", "content": "The proliferation of Large Language Models (LLMs) in recent years, along with their promises of scalability and enhanced generalization, has enabled them to effectively tackle previously unseen tasks by leveraging increasingly larger training corpora [34]. However, developers who \"open-source\" their models [5, 15, 53] are often reluctant to fully disclose the data and processing techniques used during training, which may conflict with transparency and accountability requirements outlined in regulations such as the recent EU AI Act\u00b9.\nTo ensure transparency, Membership Inference Attacks (MIAs) aim to determine whether specific data samples (such as sensitive or copyrighted items) were included in the training set of a model [52].While some researchers argue that it is impossible to prove that MIA are feasible on LLMs [55], others try to find methods that maximizes the Area Under the Curve (AUC) to get better performance. These efforts tackle the membership inference problem from a grey-box perspective, where the key idea is to establish a threshold on the model output that determines whether a sample was part of the training data. Yeom et al. [54] proposed computing the model's loss on the target samples, while Sablayrolles et al. [38] introduced the idea of using a separate model trained on a disjoint dataset to calibrate the threshold. Carlini et al. [8] defined the threshold by calculating the loss using the zlib compression algorithm. More recently, Shi et al. [40] proposed a new loss-based methodology built on the idea that unseen samples may include a few outlier words with low probabilities, which can potentially indicate membership. On the other hand, Kim et al. [24] propose a black-box algorithm that refines membership scores using an expectation-maximization algorithm. This approach leverages the concept that estimates of membership scores can be iteratively improved by cross-referencing them.\nAlthough these methods have proven effective in specific cases, they face challenges when dealing with large training corpora. Training reference models to calibrate thresholds may be impractical because (1) the training sets must be known in advance to create disjoint models, (2) the energy and computational costs required to train such models can be prohibitive [48], and (3) choosing the reference model is hard and it may alter the results [13].\nExisting techniques were analyzed by Duan et al. [14] and Das et al. [11]. While both showed promising results, the authors hypothesize that these outcomes may be influenced by inherent flaws in the benchmarks, potentially introducing biases between member and non-member samples."}, {"title": "Motivation", "content": "The need to create fair and transparent auditing processes for Al systems calls for adopting white-box approaches [9, 52]. With regards to MIA, we hypothesize that the internal model data from member and non-member samples may reveal distinguishing patterns. Specifically, we expect that data corresponding to previously seen (member) texts or images may behave differently from unseen (non-member) data. Only Liu et al. [29] have approached the membership inference problem from this perspective. They apply Linear Probes (LPs) [4] on model activations of a single layer to detect whether samples were part of the training set. However, their work is preliminary as there are a number of limitations which are tackled in our work. First, their approach involves fine-tuning the models to ensure that members have been seen. Therefore, results are biased since samples already used in the pretraining phase are seen twice. Second, such a fine-tuning is used to create proxy models for the experimentation, but there is no guarantee on the functional equivalence of the original and the proxy model. Thirdly, their experiments imply the use of a sample prompt which is input to the model. This simplifies the problem and hinders the ability to generalize, as the use of a prompt narrows the search space for distinguishing between members and non-members. Lastly, their scope is limited to text-based MIAs, thus excluding multimodal models.\nContribution. This paper offers the first insightful analysis of the effectiveness on using internal model data for MIA assessment. The approach, dubbed LUMIA\u00b2, uses internal activations of each model layer. LUMIA is directly applied on real-world models and datasets, thus characterizing the ability of LPs to succeed depending on the model, the dataset nature or its bias. As no sample prompts are used and LLMs are requested to perform a variety of tasks, our results are easily generalizable. Interestingly, experiments are not only text-based MIAs, but also multimodal. While the concurrent work by Li et al. [27] has proposed a benchmark for multimodal MIAs, they depend on the model output. Therefore, they limit themselves to tasks that generate long texts. On the contrary, LUMIA is not constrained by the LLM output.\nThe research question at stake is To what extent can internal activations of LLMs be used to improve and assess membership inference? In this vein, the list of contributions is as follows:\n\u2022 We provide a comprehensive study on the suitability of internal activations for assessing MIAs by using linear probes, showing their ability to outperform the state of the art.\n\u2022 We explore for the first time the impact of the LLM size, the dataset nature and bias and the impact of using deduplicated model versions.\n\u2022 We analyse the problem of MIAs in multimodal LLMs. We consider a variety of LLM tasks, which has never been tackled to the best of authors knowledge.\n\u2022 Our experimental results are based on 14 textual and 7 multimodal datasets and 3 model families, involving 15 LLM configurations. We release our experimental materials to foster further research in this direction\u00b3."}, {"title": "2 BACKGROUND", "content": "The background on Large Language Models (LLMs) is introduced in Section 2.1. Key issues related to LLM input data are introduced in Section 2.2. Lastly, the basics of linear probes are described in Section 2.3."}, {"title": "2.1 LLMs. Internal model data", "content": null}, {"title": "2.1.1 Large Language models (LLMs)", "content": "These models primarily refer to transformer-based neural networks [46], consisting of tens to hundreds of billions of parameters, and pretrained on vast amounts of data. Notable examples include models like LLaMA [15] and GPT-4 [3].\nThese models are significantly larger than traditional language models and exhibit far superior language understanding and generation capabilities. More importantly, LLMs have demonstrated emergent abilities, such as learning to generalize and perform novel, unseen tasks by being trained on a limited set of instructions. One of the most impactful emergent abilities is their capacity for multistep reasoning, where complex problems are broken down into simpler subproblems. This was effectively demonstrated through the chain-of-thought prompting technique, as shown by Wei et al. [49], which enhances the model's reasoning and problem-solving capabilities by encouraging stepwise, logical thinking."}, {"title": "2.1.2 Multimodal LLMs", "content": "Multimodal LLMs are an extension of classical LLMs that can process multiple types of input data, such as images, videos or audio in addition to text.\nTypically, multimodal LLMs consist of three key components:\n\u2022 Visual Model. This component is responsible for extracting relevant features from visual data, such as images or videos. It usually consists of a pretrained convolutional neural network (CNN) [21] or Vision Transformer (ViT) [12].\n\u2022 Projection Layer. It takes the high-dimensional visual features extracted by the visual model and maps them into a token space that the LLM can process [47].\n\u2022 LLM. It is the core of the model, performing the necessary reasoning to solve the instructed task.\nIn order to jointly use the visual model and the LLM, they must be pretrained together on large datasets that encompass a wide range of tasks and modalities as seen in [5, 17, 26]."}, {"title": "2.1.3 Internal model data", "content": "It refers to the information processed and stored by the neural network during training and inference. In the transformer model [46], internal model data specifically refers to the activations generated at the output of each transformer block during the feed-forward pass.\nThese activations represent the intermediate state of the model as it processes input data layer by layer. They play a key role in establishing and refining relationships between the input tokens, capturing both local and global dependencies at various stages of the forward pass.\nActivations are dynamic because they change with every new input and exist only during the execution of the model. Additionally, they are layer-specific, as each layer produces its own unique representation of the data, progressively transforming the input into more abstract and task-relevant features."}, {"title": "2.2 LLM input data. Deduplication and biases", "content": "A key factor in training these models is ensuring data quality, especially when scraping large corpora. Duplicated instances can affect the generalization capabilities of LLMs contributing to data memorization [28], which may affect MIA attacks [35]. To improve performance, it is essential to curate and deduplicate data by removing semantically similar samples using techniques like those proposed in [1, 45].\nOne way to measure data quality is by identifying biases. We hereby describe the two major types of bias [18]. One way is analyzing N-grams, which are sequences of n consecutive elements (e.g., words in natural language processing). Overlap, in this context, refers to the frequency of appearance of a particular N-gram within a corpus of texts [44]. In MIA, N-gram overlap indicates the percentage of N-grams in a non-member sample that also appear in at least one member sample. Thus, higher overlap would mean more similarity across samples [13]. In this proposal, we call this potential source of bias as N-gram bias (NGB).\nAnother form of bias in MIA arises from dynamic changes in data distribution over time. Thus, members are typically selected before a given date, and non-members are those after that deadline. Das et al. [11] identified this issue, which we refer to as temporal bias (TB)."}, {"title": "2.3 Linear classifier probes", "content": "Linear Classifier Probes, hereinafter Linear Probes (LP), are simple classifiers that contribute to deep learning models explainability efforts by providing insights into how the model processes information internally [4].\nLPs are used to make predictions over the hidden states of the models, trying to predict or identify if some specific information is correctly represented within them. For LLMs, a LP classifier is typically placed after each layer of the network and takes the hidden states as input $X$ and predicts a simple concept $Y$. For instance, as shown in [2], LPs can be used to predict the concept of deviation from the original prompted task. LPs are trained on probing datasets designed to predict an expected concept predefined and known in advance, and can be used to understand how different layers of the model encode and retain the expected concepts. They have been previously leveraged for explainability to catch deceptive behaviors [22] or understand model hallucinations or how general knowledge is represented internally [13, 23].\nLPs can be simple linear regressions, which assume linearity within the model's internal representations, or they can use more complex architectures, such as Multi-Layer Perceptrons (MLPs), to capture non-linear relationships [16]."}, {"title": "3 LUMIA", "content": "In this Section the foundations of the proposal are introduced. Section 3.1 covers the formulation of the problem and Section 3.2 describes the approach, including the process of extracting the LLM's internal activation data."}, {"title": "3.1 Problem formulation", "content": "To address the research question outlined in the introduction, we propose an alternative perspective on membership inference by analysing internal model activations, i.e. the hidden states generated at each layer during inference. These activations capture the interaction between the input data and the model, potentially providing deeper insights into membership information without relying on output-level signals such as probabilities or loss.\nOur approach, LUMIA (Linear probe-based Utilization of Model Internal Activations), leverages Linear Probes (LPs), lightweight classifiers trained directly on these activations. LPs offer an interpretable and efficient means to assess the distribution of membership information accross the model's layers. Specifically, we formalize the problem of membership inference using internal activations as follows:\n\u2022 Input: A pre-trained model $M$, and a set of labeled samples $S = \\{(x_i, y_i)\\}$. $x_i=\\{t_1,\u2026, t_k\\}$ is the input (text or multimodal text-image pair) formed by minimal data units called tokens $t_i$. $y_i \u2208 \\{0, 1\\}$ indicates membership status (1 if the sample is a member, 0 otherwise).\n\u2022 Objective: Train a linear probe MLP for each layer $l$ of the model $M$ to classify membership status based on the internal activation $A_l(x_i)$, where $A_l(x_i)$ represents the average activation vector at layer $l$ for all tokens $t_i$ within input $x_i$.\n\u2022 Metric: Evaluate $P_l$ using metrics such as Area Under the Curve (AUC) for each layer $l$, and identify the layer $l^\u2217$ where membership information is most detectable (i.e. where $P_{l^\u2217}$ achieves the highest AUC).\nThis formulation enables us to explore:\n(1) The distribution and concentration of membership information across different layers of LLMs.\n(2) The comparative effectiveness of LP-based MIAs versus traditional output-based methods.\n(3) The influence of factors such as model architecture, size, dataset characteristics, and multimodal inputs on membership inference success.\nBy rigorously applying this approach, LUMIA aims to advance the understanding of membership inference in LLMs, and establishes internal activations as versatile and powerful tool for MIA assessment."}, {"title": "3.2 Description", "content": "Depicted in Figure 1, once a LLM is trained with member and non-member samples, internal activations at each layer are input of a LP. In this work, LPs are implemented through MLPs (recall Section 2.3), whose output is AUC. LUMIA retrieves the AUC per layer as well as the layer $l^\u2217$ in which the maximum AUC is achieved.\nTo ensure the robustness and generalization of this process, unimodal (D) and multimodal (MD) datasets are applied over multiple LLM (e.g. $LLM_j$), where D provides answers to a general instruction prompt of any type, e.g. make a summary. More specifically, D are used to test the improvement of LUMIA over N-gram bias (NGB), thus studying the benefits of using LPs for MIA attacks with different levels of overlapping among inputs. D are also applied to study the effect of temporal bias (TB).\nGiven the large variety of LLMs, MD allows analysing MIA attacks once samples composed of image and text are input. A couple of ways to handle multimodality are devised - training a LLM just with images or with images and text, to calculate LP over the resulting activations $A_l(x_i)$ (see Section 3.2.1 for details on the process). Anyway, images and texts are used in the final testing process."}, {"title": "3.2.1 Extracting activation data", "content": "Activations $A_l(x_i)$, per layer, capture values for all tokens. First, samples $x_i$ from members and non-members are preprocessed by cropping the text to fit the maximum context length $n$ of the target LLM. Next, a forward pass [37] is performed for each sample, during which the hooks capture the activations $A_l(x_i)$ for each token $t_i$ at layer $l$:\n$A_l(x_i) = \\{a_l(t_1), a_l(t_2), ..., a_l(t_n)\\}$\nFor unimodal cases, hooks are placed after each transformer layer. In multimodal cases, hooks are positioned after the layers of both the text and visual models.\nThe activations $a_l(t_i)$ for each token $t_i$ in the sample (text, image, or combined text+image in multimodal cases) are extracted, and their average is computed as follows:\n$A_l(x_i) = \\frac{\\sum_{j=1}^{n} a_l(t_j)}{n}$       (2)\nThis results in a vector, the size of the hidden layers, that represents the average activation values for the sample.\nIt must be noted that using the average is different to the approach by Liu et al. [29], who only use the activation on the last text input token (i.e., $t_n$). Thus, our preliminary tests show that using the average leads to better results."}, {"title": "4 EXPERIMENT DESIGN", "content": "This section describes the design of the experiments to assess LUMIA. Models and datasets are explained on the Section 4.1. Section 4.2 and Section 4.3 which refers to the metrics and reference MIA values of the state of the art, used to assess the proposal. Finally, Section 4.4 introduces the experimental settings."}, {"title": "4.1 Models, datasets and tasks", "content": "This section provides information regarding applied models, datasets and aligned tasks."}, {"title": "4.1.1 Unimodal LLMs", "content": "Several models of different sizes are chosen in this study. On the one hand, the Pythia model family [6], trained on the Pile dataset [19], with 160M, 1.4B, 2.8B, and 12B of parameters was selected in both their non-deduplicated and deduplicated versions for comparison purposes. Additionally, the GPT-Neo family is also evaluated with 140M, 1.3B, and 2.7B parameters variants. These models are chosen (1) to compare them to other proposals, and (2) because data used for pre-training them is known, being essential to deal with MIA attacks."}, {"title": "4.1.2 Unimodal task and datasets", "content": "In line with the state of the art, the LLM processes text to carry out a text-masking causal modeling task. In this vein, datasets used to test the approach are the following. Note that they have been selected for the sake of comparability with previous works [14, 29, 33, 40].\n\u2022 WikiMIA [40]. It is composed of event pages from Wikipedia, created between 2017 and a specified cutoff date of January 1 2023. Positive samples (members) are pages within this time range, while negative samples (non-members) consist of pages created before 2016 and after the cutoff date.\n\u2022 ArXiv-MIA [29]. It consists of abstracts from ArXiv papers in the fields of computer science and mathematics, with a cutoff year of 2024. Members are considered to be any paper published before 2024 and non-members, anything published after.\n\u2022 Temporal ArXiv/wiki [14]. It corresponds to a selection of documents from ArXiv and Wikipedia using different cutoff dates. Member samples are documents before the cutoff date, while non-member samples are documents after the cutoff date.\n\u2022 ArXiv-1-month [33]. Members and non-members are extracted from ArXiv based on a cutoff date. However, the selection is constrained by a temporal range, where samples are chosen within a maximum distance of 1 month from the cutoff."}, {"title": "Gutenberg [33]", "content": "Member samples are extracted from the PG-19 dataset [36], which is a subset of the RedPajama pretraining dataset\u2074. By contrast, non-member samples are derived from new works added to Project Gutenberg\u2075 after 2019.\n\u2022 Mimir [14]. Members and non members are extracted from the training and test set of the Pile [20], an open source dataset extracted from 22 public sources. Samples are selected from the Pile creating 3 subsets of 'overlapping level' using the percentage of ngram overlap between the text samples. These datasets are divided into Wikipedia, Github, Pubmed, Pile CC, ArXiv, DM_math and Hackernews datasets.\nAll datasets, except for Mimir, have already been shown to suffer from TB [11]. This bias may arise due to evolving formats in pages, events, or documents over time, potentially leading to information that facilitates membership inference. Conversely, in Mimir, which is curated from a dataset with distinct training and testing sets, it is claimed that both the length of N-gram and the degree of overlapping between the N-grams of members and non-members samples could impact classification performance as a result of NGB. For instance, longer N-grams and higher overlapping should make membership inference more challenging, as there are more probabilities on findings syntactically similar patterns between members and non-member."}, {"title": "4.1.3 Multimodal LLMs", "content": "For the analysis of multimodality, the latest version of the LLava-OneVision model [26] is applied with 0.5B and 7.6B parameters. These models are chosen since (1) the data used during its pre-training and fine-tuning is known and, (2) due to available computational resources."}, {"title": "4.1.4 Multimodal tasks and datasets", "content": "Moreover, linked to this model, OneVision-Data dataset is applied. It is composed of a wide range of datasets used to train a multimodal model for multi-tasking. From this collection, we generate member and non-member samples from datasets that originally provided distinct training, validation, and testing splits. From all datasets, the following are selected considering that they encompass all the modalities and categories of tasks the model can accomplish: General resolution, Doc/Chart/Screen solving, Math/Reasoning, OCR and Language tasks.\n\u2022 Textcaps [43]. Challenges the model to recognise text and relate it to its visual context requiring spatial, semantic and visual reasoning. Pairs are formed by images-texts and prompt is the same across pairs. The task solved is a free text image description task.\n\u2022 MathV360k [41]. Synthetically augmented dataset of multi-choice image-texts questions of mathematical problems. Samples are formed by image-texts pairs and prompts follows a fixed template. The task solved is a mathematical task that returns the expected output as text.\n\u2022 AOK [39]. Crowd-sourced dataset composed of image-texts samples to train the model on questions that require \"common sense\" to answer. The task solved is a free text rationale generation task.\n\u2022 ChartQA [32]. Large-scale benchmark covering human-written questions as well as questions generated from human-written chart summaries that forces the model to answer over mathematical charts. The task solved is a description task returning single word responses.\n\u2022 ScienceQA [30]. Benchmark datasets consisting on multiple multimodal choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. The task solved is a reasoning task based on single word responses.\n\u2022 IconQA [31]. Dataset formed by image-texts pairs to train the model on comprehensive cognitive reasoning tasks on multi-image-choice, multi-text-choice, and filling-in-the-blank questions to respond a single word response task.\n\u2022 Magpie [51]. Dataset of high-quality text-only data generated with LLMs following a set of templates to respond general free text tasks. Being text-only, it enables the analysis of the multimodal LLM's behavior in both text+image and text-only scenarios based on the activations of the visual and LLM encoders (recall Section 2.1)."}, {"title": "Metrics. Performance and bias", "content": "In line with Duan et al. [14], Shi et al. [40] and Carlini et al. [7] and for the sake of comparison, the effectiveness of the detection method is measured with the following metric:\n\u2022 Area Under the ROC Curve (AUC). It measures the ability of a classifier to correctly determine a class, 0 or 1, by comparing the true positive rate (power) against the false positive rate (error) across various thresholds. A value closer to 1 means better performance. In line with [13], MIA will be considered successful when AUC is higher than 0.6.\nConcerning text-based bias, only NGB can be measured. In this regard, we use the n-gram length N and the percentage of overlap P, in line with [13]\n\u2022 Average Hash variation (HV). Images are converted to grey scale and the average value of the pixels is computed. Finally a hash is applied to compare similarity across samples.\n\u2022 Average Structural similarity index measure (SSIM). It refers to the perceptual similarity between images by considering their luminance, contrast, and structural content.\n\u2022 Pixel intensity histogram. The pixel intensity distributions are analyzed by generating histograms for each image."}, {"title": "4.3 Baseline MIAs", "content": "To compare the performance of LUMIA against the state of the art (see Section 6), the following MIAs are considered:\n\u2022 Loss. [54] Defines membership of a sample based on the loss of a target model.\n\u2022 Reference-based. [38] A model is calibrated with respect to another reference model to define membership based on the intrinsic complexity of the target sample.\n\u2022 Zlib Entropy. [8] Uses zlib compression size to calibrate the model. Then, a threshold is defined.\n\u2022 Min-k% Probability. [40] Defines membership looking at the k% of tokens with the lower likelihoods to compute a membership score."}, {"title": "4.4 Experimental settings", "content": "Training was conducted on two NVIDIA consumer GPUs, a RTX 4090 and a RTX 4080, using mix of the Pytorch, tensorflow frameworks and the Hugging Face library \u2077. For both training and validation, all datasets were randomly split in an 80%-20% balancing both classes (members and non-members) and repeating 3 times experiments with different samples. The average of all executions is then computed. MLPs models are trained with a learning rate of 1e-3, using the Adam optimizer [25] over 100 epochs with early stops and dropout regularization.\nIn what comes to the batch size, our preliminary tests show that small sizes do not affect the accuracy while harm performance. Thus, the batch size is set to the maximum capacity per GPU. Data balancing on each of the configurations is as follows:\n\u2022 Unimodal. For comparison with related work [14, 29, 33, 40], we extract 1,000 members and 1,000 non-members per dataset, except for WikiMIA and ArXiv-MIA. For these cases, we use the provided data: 250 members and 250 non-members for WikiMIA, and 400 samples per class for ArXiv-MIA.\n\u2022 Multimodal. For each experiment we draw 1,000 members and 1,000 non-members for each of the datasets, in line with most proposals on the unimodal configuration. To measure the effect of mixing samples across datasets, we create a joint subset extracting 100 sample members and 100 non-members from all the datasets, forming a total of 700 members and 700 non-members.\nFor multimodal configurations, we pick the members and non-members using the IDs provided on the original datasets to avoid contamination between training, validation and testing sets.\nLastly, since the original Magpie setup does not provide image inputs but the model requires both text and image modalities, we pair each text input with a black image to create the necessary input pairs."}, {"title": "5 RESULTS", "content": "This section presents the results of LUMIA. Firstly, how LP outperforms MIA attacks is analysed (Section 5.1), followed by a study of the impact of target model size (Section 5.2). The influence of potential bias is then explored (Section 5.3), along with the role of dataset nature (Section 5.4), the effects of data deduplication (Section 5.5), and the significance of layer depth (Section 5.6).\nResults are presented in the form of tables which are used across all sections. For the sake of clarity, Tables 1 and Table 2 highlight LUMIA values where AUC is higher than 60%, while Table 3 highlights the best values for each setting."}, {"title": "5.1 Overall effectiveness", "content": "This section reports the best results of previous proposals (hereinafter Best SOTA AUC) and those achieved in LUMIA.\n5.1.1 Unimodal. Table 1 and Table 2 summarize the results of our approach versus all the previous proposals. LUMIA overtakes previous results on all the cases except in two, which represents an improvement on 174 of the 176 cases (98.86%). Indeed, our approach provides an average AUC improvement of 15.75%. Considering AUC>0.6 as a threshold [13], previous approaches surpass that value on the 44.5% of the cases while LUMIA reaches that threshold on the 65.33% of the cases, that is an increment of 46.80%. All in all, LUMIA exhibits better performance across all AUC thresholds, as shown in Appendix A.1.\n5.1.2 Multimodal. Table 3 shows the results for the multimodal configurations. All of them, except Magpie, achieve AUC>0.6, suggesting that multimodality may be adding additional information useful for detecting MIA. Magpie reaches AUC=0.57, probably because it is the only text-only dataset. When making predictions over a joint dataset, the AUC remains above 0.60 which points out that even when mixing information and modalities, LPs find patterns across activations to define membership. Globally speaking, 85.9% of cases achieve AUC>0.6, demonstrating better performance as compared to unimodal setups, which meet this threshold in 65.33% of configurations."}, {"title": "5.2 Impact of model size", "content": "In this section, results considering the model size, that is the amount of parameters, are analyzed. Table 4 depicts results for the Pythia family and Table 5 for GPT-Neo family. Additionally, Appendix A.2 includes aggregated plots per model and dataset."}, {"title": "5.2.1 Unimodal", "content": "Table 4 shows a clear trend on the AUC as the model size grows. All datasets show better results in all configurations on the 12B version, excluding ArXiv-1 month. For this dataset, both deduplicated and non-deduplicated models show improved AUC scores when scaling from 70M to 2.8B parameters. Yet, a significant decline is observed in the 12B version of the model on this dataset, with AUC values dropping from 0.92 to 0.84 (deduped) and 0.86 (non-dedup). Despite this unexpected decrease, AUC values are still very high.\nFor the GPT-Neo family, the same trend is noticed in Table 5, as the model grows, better AUC is achieved. The only exception is arXiv-CS, were AUC of the largest model is 0.802, while the configuration 1.3B of parameters gets 0.842.\nBy analyzing the trends on the percentage of change of AUC of non-LP-based proposals and ours, while LPs shows an incremental trend, differences with other approaches are not significant."}, {"title": "5.2.2 Multimodal", "content": "From an architectural perspective, while there are no differences in the sizes of the visual encoders, having a larger LLM on the textual+visual part affects the results. In general, excluding again the Magpie dataset (since it only contains texts), the 7B model seems to reveal more information in both parts of the models, the visual only encoder and the textual+visual LLM, denoting higher memorization of the data than the 0.5B version."}, {"title": "5.3 Impact of bias", "content": "Table 1 shows the results for the TB datasets and Table 2 for NGB ones."}, {"title": "5.3.1 Unimodal", "content": "Table 1 presents a comparison of the best results from previous approaches and LUMIA for the datasets with TB on the model Pythia-dedup 12B. LUMIA significantly outperforms [29", "13": "Table 2 shows that LUMIA outperforms all reported configurations and baselines (except for Wikipedia Ref) across all models. They reported that no configuration reached an AUC>0.60 for the N = 13 overlap of P = 0.8 on the Pythia dedup model. Contrarily, in specific cases, such as Pile-CC, DM_math, we can reach this threshold. Additionally, for the 12B model, we achieve an AUC of 0.58 on PubMed and 0.584 on Hackernews, both approaching the 0.60 threshold more closely than previous approaches. All in all, results for N = 13 with P = 0.8 lead to an overall improvement of 13.10%.\nFor N = 7 with P = 0.2 configurations on the Pythia 12B family at Table 2, our approach consistently outperforms all results from the state of the art, with improvements ranging from a minimum of 2% on Wikipedia to a maximum of 64% on PubMed and an average improvement of 18.74%. Notably, on the DM_math dataset the ref method performs poorly under overlap configurations of P = 0.2 with N = 13 and N = 7, achieving AUC scores of 0.44 and 0.41, respectively. Consequently, our approach surpasses the ref method by 68% and 129%.\nFor the GPT-Neo 2.7B model, in line with the previous model, all configurations overtake results"}]}