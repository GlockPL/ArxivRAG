{"title": "Fair Best Arm Identification with Fixed Confidence", "authors": ["Alessio Russo", "Filippo Vannella"], "abstract": "In this work, we present a novel framework for Best Arm Identification (BAI) under fairness constraints, a setting that we refer to as F-BAI (fair BAI). Unlike traditional BAI, which solely focuses on identifying the optimal arm with minimal sample complexity, F-BAI also includes a set of fairness constraints. These constraints impose a lower limit on the selection rate of each arm and can be either model-agnostic or model-dependent. For this setting, we establish an instance-specific sample complexity lower bound and analyze the price of fairness, quantifying how fairness impacts sample complexity. Based on the sample complexity lower bound, we propose F-TaS, an algorithm provably matching the sample complexity lower bound, while ensuring that the fairness constraints are satisfied. Numerical results, conducted using both a synthetic model and a practical wireless scheduling application, show the efficiency of F-TaS in minimizing the sample complexity while achieving low fairness violations.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, a large body of work has focused on making machine learning systems more fair [1]. This effort reflects a broader societal shift towards more ethical algorithms, recognizing the impact these systems have across various sectors of society.\nNotably, the importance of fairness has been also recently acknowledged within the context of Multi-Armed Bandit (MABs) [2]. MABs (or simply bandits) [3] are sequential decision-making problems under uncertainty, in which a learner must strategically select arms to maximize a given objective over time.\nBandit algorithms have seen widespread adoption in various applications: online advertisement [4], recommender systems [5], [6], wireless network optimization [7], and others [8]. Due to the importance and impact of these applications, there has been an increasing understanding of the need to include fairness aspects in the decision-making process. For example, in wireless scheduling problems with multiple Quality of Service (QoS) classes (see Sec. VI), fairness is achieved by ensuring that users within each class receive appropriate performance according to their specific requirements, e.g., in terms of throughput [9].\nHowever, traditional bandit algorithms do not inherently address such fairness aspects. Indeed, while guaranteeing fairness has received attention in the setting of regret minimization [10], [9], [11], [12], this aspect remains largely unexplored within the problem of Best Arm Identification (BAI) [13], [14]. In BAI, the objective is to find an optimal arm with a prescribed level of confidence and with minimal sample complexity."}, {"title": "II. RELATED WORK", "content": "Different notions of fairness have been considered in MAB problems and, more generally, in sequential decision-making problems. For an extended literature review, refer to App. D. For comprehensive surveys on this topic, see [2], [12].\nIn MAB problems, fairness has been investigated in different settings including plain, combinatorial, contextual, and linear [10], [9], [15], [16] and with different notions of fairness. The majority of these notions, generally fall into the following categories: pre-specified fairness, individual fairness, counterfactual fairness and group fairness [12], [2]. Of these notions, the closest to our work are the first two, and we focus on these in the remainder of this section.\nSelection with pre-specified values of fairness [9], [17], [11], [18], [19] simply demands that the rate, or probability, at which an algorithm selects an arm stays within a pre-specified range. Related works in this category mostly target finite-time fairness constraints, which require that a given number of arms pull must be satisfied in each round [11], [19], [20]. These constraints are, in general, model-agnostic.\nOn the other hand, individual fairness [21], [22] requires a system to make comparable decisions for similar individuals, and the constraints could be based on similarity or merit [10], [20], [23], [24], [25], [26]. These constraints impose a minimal rate at which an algorithm must select arms, and they are generally model-dependent. Moreover, in this setting, algorithms often provide asymptotic guarantees as the time horizon grows large. In general, it is hard to avoid asymptotic guarantees when the constraints depend on the unknown model parameter, which needs to be estimated during the learning process.\nOther important works consider the a-fairness criterion [27], [28], [29] for fair resource allocation, which encompasses different fairness criteria when varying the value of the parameter a. This criterion includes different notions of fairness, such as max-min fairness, which allocates resources as equally as possible, or proportional fairness, which allocates resources in a proportional manner [26], [25], [30]. Notably, as explained in the next section, our fairness definition is very general and includes for example the case of individual, proportional, and pre-specified values of fairness (see Remark III.2 for details).\nLast, but not least, the totality of the above-mentioned works focuses on the regret minimization setting, where the aim is to maximize the expected cumulative rewards [3]. In contrast, our work focuses on the setting of pure exploration (a.k.a. BAI) with fixed confidence [14]. To the best of our knowledge, the only work investigating fairness in BAI is [31], where the authors consider fairness constraints on sub-populations (see App. D for details). However, our setting is inherently different and assumes fairness constraints on each arm rather than sub-populations."}, {"title": "III. PROBLEM SETTING", "content": "In this section, we outline the bandit model considered in this paper and we present our fair BAI setting."}, {"title": "A. Multi-Armed Bandit Model", "content": "We consider a stochastic bandit problem with a finite set of K arms, that we denote by $[K] := {1, ..., K}$. In each round $t > 1$, the learner selects an arm $a_t \\in [K]$, and observes a Gaussian reward $r_t \\sim \\mathcal{N}(\\theta_{a_t}, 1)$. The rewards are i.i.d. (over rounds) and $\\theta = (\\theta_a)_{a \\in [K]}$ is the unknown parameter vector. We indicate by $a^* = \\arg \\max_{a \\in [K]} \\theta_a$ the arm with highest average reward, that we assume to be unique, and indicate by $\\Theta = {\\theta \\in \\mathbb{R}^K : |\\arg \\max_{a \\in [K]} \\theta_a| = 1}$ the set of models satisfying this property. We define the sub-optimality gap for an arm $a \\neq a^*$ as $\\Delta_a = \\theta_{a^*} - \\theta_a$, and the maximal (resp. minimal) gap as $\\Delta_{max} = \\max_{a \\neq a^*} \\Delta_a$ (resp. $\\Delta_{min} = \\min_{a \\neq a^*} \\Delta_a$). We also indicate by $N_a(t) = \\sum_{s=1}^t \\mathbb{I}{a_s=a}$ the number of times an arm $a$ has been selected up to round $t$. The empirical average of $\\theta = (\\theta_a)_{a \\in [K]}$ at round $t$ is denoted as $\\hat{\\theta}(t) = (\\hat{\\theta}_a(t))_{a \\in [K]}$, where $\\theta_a(t) = \\frac{1}{N_a(t)} \\sum_{s \\in [t]} r_s \\mathbb{I}{a_s=a}$. We denote by $\\mathcal{F}_t$ the $\\sigma$-algebra generated by $(a_1, r_1, ..., a_t, r_t)$, the history of observations. We indicate by $kl(p, q)$ the Kullback\u2013Leibler divergence between two Bernoulli distributions of mean $p$ and $q$. For any two vectors we write $x, y \\in [0, 1]^K$, $x > y$ to denote $x_a \\in [y_a, 1], \\forall a \\in [K]$."}, {"title": "B. Fair Best Arm Identification", "content": "We now briefly introduce the BAI setting, then proceed to explain how to incorporate fairness constraints.\nBest Arm Identification: In BAI [32], the objective is to identify the best arm $a^*$ with probability at least $1 - \\delta$, where $\\delta \\in (0, 1/2)$, using the least number of samples. A BAI algorithm $\\mathcal{A}$ consists of a sampling rule $\\pi_t$, a stopping rule, and a decision rule. The sampling rule $\\pi_t$ decides which arm is selected in round $t$ based on past observations: $a_t$ is $\\mathcal{F}_{t-1}$-measurable. The stopping rule decides when to stop sampling, and is defined by $\\tau_{\\delta}$, a stopping time w.r.t. the filtration $(\\mathcal{F}_t)_{t \\ge 1}$. The sample complexity for an algorithm $\\mathcal{A}$ is denoted by $\\mathbb{E}_{\\theta, \\mathcal{A}}[\\tau_{\\delta}]$, and w.l.o.g. in the following we simply denote it by $\\mathbb{E}_{\\theta}[\\tau_{\\delta}]$. Lastly, the decision rule outputs a guess of the best arm $\\hat{a}_{\\tau_{\\delta}}$, based on observations collected up to round $\\tau_{\\delta}$.\nFairness Constraints: In fair BAI, we seek to identify the best arm as quickly as possible while satisfying a set of fairness constraints. We consider general types of constraints which can be either pre-specified or dependent on the problem parameter $\\theta$, as detailed in the following.\n1) Pre-specified constraints: the selection rate at the random stopping time $\\tau_{\\delta}$, needs be larger than some pre-specified value $p_a \\in [0, 1]$:\n$\\qquad \\qquad \\qquad \\mathbb{E}_{\\theta}[N_a(\\tau_{\\delta})] / \\mathbb{E}_{\\theta}[\\tau_{\\delta}] \\ge p_a, \\quad \\forall a \\in [K]$.\n2) $\\theta$-dependent constraints: asymptotically, as $\\delta \\rightarrow 0$, the selection rate at the stopping time $\\tau_{\\delta}$ needs to be larger than some $\\theta$-dependent function $p_a(\\theta) : \\mathbb{R}^K \\rightarrow [0, 1]$:\n$\\qquad \\qquad \\qquad \\qquad \\liminf_{\\delta \\rightarrow 0} \\mathbb{E}_{\\theta}[N_a(\\tau_{\\delta})] / \\mathbb{E}_{\\theta}[\\tau_{\\delta}] \\ge p_a(\\theta), \\quad \\forall a \\in [K]$.\nIn this case, we further assume $p_a(\\theta)$ to be continuous in $\\theta$, for every arm $a \\in [K]$.\nRemark III.1. An alternative definition of fairness could consider constraints of the type $\\mathbb{E}_{\\theta}[N_a(\\tau_{\\delta})/\\tau_{\\delta}] \\ge p_a(\\theta)$."}, {"title": "IV. SAMPLE COMPLEXITY LOWER BOUND AND THE PRICE OF FAIRNESS", "content": "In this section, we first provide an instance-specific sample complexity lower bound that is valid for any p-fair $\\delta$-PAC algorithm. Next, we analyse the price of fairness, quantifying how fairness constraints impact sample complexity."}, {"title": "A. Sample complexity lower bound", "content": "The following theorem states a lower bound on the sample complexity of any p-fair $\\delta$-PAC algorithm. Notably, the sample complexity is characterized by the following constant, that we refer to as characteristic time\n$\\qquad \\qquad \\qquad T^* = \\frac{2}{2} \\inf_{w \\in \\Xi_p} \\max_{a \\neq a^*} \\frac{w_{a^*} + w_a^{-1}}{\\Delta_a^2}$,\nwhere $\\Xi_p = {w > p : \\sum_{a \\in [K]} w_a = 1}$ is the clipped simplex."}, {"title": "B. The Price of Fairness", "content": "The next lemma states an upper bound on the ratio $T/T^*$. This ratio quantifies the price in sample complexity that the learner has to pay in order to guarantee fairness.\n$\\qquad \\qquad \\qquad T^*/T \\le \\mathcal{O}\\left(1 + \\min\\left(\\frac{1}{1 - p_{sum}}, \\frac{1}{Kp_{min}}\\right)\\right)$.\nThe lemma shows that the price typically scales either as $(1 - p_{sum})^{-1}$ or $(p_{min})^{-1}$.\nIn the remainder of this section, we discuss two exemplary cases that shed light on the nature of this scaling. We also refer the reader to App. B-D for further details and examples.\nCase 1, larger fairness rate for suboptimal arms:\nUnder the assumption that $p_{sum} < 1$, if $p_{a^*} < w_{a^*}$ and $p_a \\ge w^*_a$ for all $a \\neq a^*$ we can derive the following upper bound on the ratio $T/T^*$:\n$\\qquad \\qquad \\frac{T}{T^*} > \\mathcal{O}\\left(\\frac{\\Delta^2_{max}}{K \\Delta_{min}}\\left(\\frac{1}{1 - p_{sum}} + \\frac{1}{p_{min}}\\right)\\right).$"}, {"title": "V. THE F-TAS ALGORITHM", "content": "In this section, we propose F-TAS, an (asymptotically) p-fair and $\\delta$-PAC algorithm. The algorithm belongs to the family of Track-and-Stop (TaS) algorithms [14], which track the sampling allocations $w$ as suggested by the solution to the lower bound optimization problem (3). The algorithm mainly consists of (i) a sampling rule and (ii) a stopping rule. We detail these steps in the remainder of this section, and present the pseudo-code for F-TAS in Alg. 1."}, {"title": "A. Sampling rule", "content": "The main idea of the algorithm is that sampling the arms according to $w$ is automatically optimal in terms of sample complexity, and satisfies the fairness constraints. However, as the instance parameter $\\theta$ is unknown, we leverage a certainty-equivalence principle and use the current estimate $\\hat{\\theta}(t) = (\\hat{\\theta}_a(t))_{a \\in [K]}$ in place of the true parameter.\nIn the algorithm, we denote by $\\hat{w}(t)$ the solution to Eq. (3) with $\\hat{\\theta}(t)$ plugged into the expression, i.e.,\n$\\qquad \\qquad \\qquad \\hat{w}(t) = arg \\inf_{w \\in \\Xi_p} \\max_{a \\neq \\hat{a}_t} \\frac{w_{\\hat{a}_t}^{-1} + w_a^{-1}}{\\Delta_a(\\hat{\\theta}(t))^2}$,\nwhere $\\hat{a}_t = \\arg \\max_a \\hat{\\theta}_a(t)$ and $\\Delta_a(\\hat{\\theta}(t)) = \\hat{\\theta}_{a^*}(t) - \\hat{\\theta}_a(t)$.\nTo enforce that the parametric uncertainty asymptotically goes to 0 (i.e., $\\hat{\\theta}(t) \\rightarrow \\theta$ a.s.), we take a convex combination of $\\hat{w}(t)$ with a constant policy $\\pi_c = (\\pi_{c, a})_{a \\in [K]}$, using a parameter $\\epsilon_t$. This can be interpreted as a form of forced exploration [14] and guarantees that, asymptotically, each arm is sampled infinitely often.\nThe constant policy $\\pi_c$, and the value of $\\epsilon_t$ depend on the type of fairness constraint as follows:\n*   Pre-specified constraints: Let $K_0 = |{a \\in [K] : p_a = 0}|$ be the number of arms for which $p_a = 0$. In the simple case that $K_0 = 0$, we set $\\epsilon_a = p_a + (1 - p_{sum})/K$. Otherwise we set $\\epsilon_t = 1/(2\\sqrt{t})$, and define $\\pi_c$ as\n $\\qquad \\qquad \\pi_{c,a} = \\begin{cases}p_a & p_a > 0 \\\\\\frac{1 - p_{sum}}{K_0} & \\text{otherwise.} \\end{cases}$\n*   $\\theta$-dependent constraints: in this case, we select $\\pi_{c, a} = 1/K$, i.e., a uniform policy for all $a \\in [K]$, and we set $\\epsilon_t = 1/(2\\sqrt{t})$.\nNote that our tracking procedure is probabilistic (we sample an arm from $\\pi(t)$) and differs from the deterministic versions commonly employed in classical Track-and-Stop algorithms [14]. Therefore, our approach, inspired by best policy identification techniques [33], requires different arguments in order to prove its optimality and fairness guarantee. See also App. C for a detailed discussion."}, {"title": "B. Stopping rule", "content": "The stopping rule is defined through two components: (1) a generalized-likelihood ratio test (GLRT) $Z(t)$ and (2) a threshold function $\\beta(\\delta, t)$. Following [14], the GLRT can be expressed as $Z(t) := t/2\\hat{T}(t)$, where\n$\\qquad \\qquad \\hat{T}(t) := \\frac{2 \\max_a}{\\hat{\\theta}(t)} \\frac{w_{\\hat{a}_t}(t)^{-1} + w_a(t)^{-1}}{\\Delta_a(\\hat{\\theta}(t))^2}$,\nNext, we consider the following threshold function from [34]\n$\\qquad \\qquad \\beta(\\delta, t) = 3 \\sum_{a \\neq \\hat{a}_t} \\log(1 + \\log(N_a(t))) + K C_{exp}\\left(\\frac{t}{K} \\left(\\log(\\frac{t}{\\delta})\\right)^3\\right)$, where $C_{exp}$ is a function defined in Thm. 7 in [34]."}, {"title": "C. Sample Complexity and Fairness Guarantees", "content": "Fairness guarantees: We obtain the following guarantees on the fairness of F-TAS.\nProposition V.1. F-TAS is p-fair (resp. asymptotically p($\\theta$)-fair) and $\\delta$-PAC. Furthermore, for pre-specified constraints, F-TAS satisfies the fairness constraints for all rounds, i.e., $\\mathbb{E}[N_a(t)] / t > p_a, \\forall t \\ge 1, \\forall a \\in [K]$.\nCorollary V.1. F-TAS is sample-path p-fair (resp. p($\\theta$)-fair), i.e., it satisfies $\\mathbb{E}_{\\theta} [N_a(\\tau_{\\delta})/\\tau_{\\delta}] \\ge p_a(\\theta), \\forall a \\in [K]$."}, {"title": "VI. NUMERICAL RESULTS", "content": "In this section, we numerically evaluate the performance of F-TAS. We propose two sets of experiments: we apply F-TAS to a synthetic bandit instance (in Sec. VI-A), and to an industrial use-case from the radio communication domain: wireless scheduling (in Sec. VI-B). Additional results are reported in the appendix.\nFairness criteria: For both sets of experiments, we focus on two settings: agonistic fairness and antagonistic fairness. These terms relate to how the fairness parameter p impacts exploration. In the former setting, p promotes exploration (e.g., by aligning with the optimal allocation in the unconstrained setting $w^*$), while in the latter, it inhibits exploration. We clarify these concepts below in the context of pre-specified and $\\theta$-dependent constraints."}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we introduced Fair Best Arm Identification (Fair BAI), a novel setting that integrates the classical BAI framework with fairness constraints, which are either model-agnostic or model-dependent.\nFor both scenarios, we derived a sample complexity lower bound and quantified the price of fairness in terms of sample complexity. Leveraging this lower bound, we devised F-TAS, an algorithm that provably matches this bound while complying with the fairness constraints.\nOur experimental results, obtained from both synthetic and wireless scheduling scenarios, demonstrate that F-TAS effectively achieves low sample complexity while minimizing fairness violations.\nThe limitations of our work include: (i) the asymptotic nature of our fairness constraints in the $\\theta$-dependent constraint; (ii) the sample complexity analysis operates in the asymptotic regime; (iii) quantifying the variance of our method is technically challenging. Future research directions involve"}]}