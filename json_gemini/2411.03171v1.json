{"title": "Navigating Extremes: Dynamic Sparsity in Large Output Spaces", "authors": ["Nasib Ullah", "Erik Schultheis", "Mike Lasby", "Yani Ioannou", "Rohit Babbar"], "abstract": "In recent years, Dynamic Sparse Training (DST) has emerged as an alternative to post-training pruning for generating efficient models. In principle, DST allows for a more memory efficient training process, as it maintains sparsity throughout the entire training run. However, current DST implementations fail to capitalize on this in practice. Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most implementations simulate sparsity by masking weights. In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification with large output spaces, where memory-efficiency is paramount. With a label space of possibly millions of candidates, the classification layer alone will consume several gigabytes of memory. Switching from a dense to a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however, severely hampers training convergence, especially at the largest label spaces. We find that poor gradient flow from the sparse classifier to the dense text encoder make it difficult to learn good input representations. By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model. Overall, we demonstrate the applicability and practical benefits of DST in a challenging domain - characterized by a highly skewed label distribution that differs substantially from typical DST benchmark datasets \u2014 which enables end-to-end training with millions of labels on commodity hardware.", "sections": [{"title": "Introduction", "content": "Recent research [1, 2, 3, 4] has demonstrated that densely-connected neural networks contain sparse subnetworks - often dubbed \"winning lottery tickets\" that can deliver performance comparable to the full networks but with substantially reduced compute and memory demands. Unlike conventional techniques that start with a trained dense model and employ iterative pruning or one-shot pruning, Dynamic Sparse Training (DST) [5, 6, 7] initializes the a sparse architecture and dynamically explores subnetwork configurations through periodic pruning and regrowth, typically informed with heuristic saliency criteria such as weight and gradient magnitudes. This approach is particularly advantageous in scenarios constrained by a fixed memory budget during the training phase, making DST viable across various domains [4, 8, 9]. For instance, in reinforcement learning [10, 11], DST has been shown to significantly outperform traditional dense models. Additionally, models trained using DST often exhibit enhanced robustness [12, 13, 14, 15, 16]. However, the application of DST comes with challenges, notably prolonged training times; for example, RigL [6] and ITOP [7] require up to five and two times as many optimization steps during training, respectively, to match the generalisation\nPreprint. Under review."}, {"title": "Dynamic Sparse Training for Extreme Multi-label Classification", "content": null}, {"title": "Background", "content": "Problem setup Given a multi-label training dataset with N samples, $D = {(x_i, P_i)}_{i=1}^N$, where L represents the total number of labels, and $P_i \\subset [L]$ denotes a subset of relevant labels associated with the data point $x_i \\in X$. Typically, the instances are text based, such as the contents of a Wikipedia article [20] or the title of a product on Amazon [47] with labels corresponding to Wikipedia categories and frequently bought together products, respectively, for example. Traditional XMC methods used to handle labels the same way as is typically done in other fields, as featureless integers.\nHowever, the labels themselves usually carry some information, e.g., a textual representation, as the following examples, taken from (i) LF-AmazonTitles-131K (recommend related products given a product name) and (ii) LF-WikiTitles-500K (predict relevant categories, given the title of a Wikipedia page) illustrate:"}, {"title": "Memory-Efficient Training: Fixed Fan-In Sparse Layer", "content": "Unstructured sparsity is notoriously difficult to speed-up on GPUs [57], and consequently most DST studies simulate sparsity by means of a binary mask [19, 58]. On the other hand, highly structured sparsity, such as 2:4 sparsity [59], enjoys hardware acceleration and memory reduction [60], but results in deteriorated model accuracy compared to unstructured sparsity [61]. As a compromise, semi-structured sparsity [46, 34] imposes a fixed fan-in to each neuron. This eliminates work imbalances between different neurons, leading to an efficient and simple storage format for sparse weights, where each sparse weight needs only a single integer index. For 32-bit floating point weights with 16-bit indices (i.e., at most 65k features in the embedding layer), this leads to a 50% storage overhead for sparse weights; however, for training, gradient and two momentum terms are needed, which share the same indexing structure, reducing the effective overhead to just 12.5%.\nWhile fixed fan-in provides substantial speed-ups for the forward pass, due to the transposed matrix multiplication required for gradients, it does not give any direct benefits for the backwards pass. Fortunately, when used for the classification matrix, the backpropagated gradient is the loss derivative, which will exhibit high activation sparsity if the loss function is hinge-like [34]. In the enormous label space of XMC, for each instance only a small subset of labels will be hard negatives. The rest will be easily classified as true negatives, and not contribute to the backward pass.\nAs additional measures to keep the memory consumption low, we enable torch.amp automatic mixed-precision training [62] and activation checkpointing [63] for the BERT encoder."}, {"title": "Improved Gradient Flow: Auxiliary Objective", "content": "We find that, despite using a fully dense network, training the encoder using gradients backpropagated from a sparse classification layer requires more optimization steps to converge compared with to a dense classification layer. This compounds with the already-increased number of epochs required for DST [6, 7], further increasing the training duration of end-to-end XMC training, which requires longer training than comparable modularized or shortlisting-based methods [35]. Furthermore, the intermediate activations in the transformer-based encoder also take up a considerable amount of GPU memory, so to meet memory budgets, we may need to switch to smaller batch sizes or employ activation checkpointing, increasing the per-step time."}, {"title": "Experiments and discussion", "content": null}, {"title": "Datasets", "content": "In this study, we evaluate our proposed modifications of DST under the extreme classification setting across a diverse set of large-scale datasets, including Wiki10-31K [65], Wiki-500K [20], Amazon-670K [47], and Amazon-3M [66]. The datasets are publicly available at the Extreme Classification Repository. These were selected due to their inherent complexity and the challenges posed by their long-tailed label distributions, which are emblematic of real-world data scenarios and test the robustness of DST methodologies. Further validation of our approach is conducted using the"}, {"title": "Baselines and evaluation metrics", "content": "To ensure a comprehensive and fair evaluation of our proposed DST methodologies applied to XMC problems, we compare our proposed framework, SPARTEX, across three principal categories of baseline methods:\n1. Dense Models : Consistent with traditional DST evaluations, we compare the performance of our sparse models against their dense counterparts.\n2. Dense Models with bottleneck layer : This category (referred to as Dense BN in Table 2) includes dense models with the same number of parameters as our proposed DST method by having a bottleneck layer with the same dimensionality as the FFI size. This ensures that comparisons focus on the impact of sparsity rather than differences in model size or capacity.\n3. XMC Methods: For datasets devoid of label features, we benchmark against the latest transformer-based models such as CASCADEXML [25], LIGHTXML [23], and XR-TRANSFORMER. For datasets that incorporate label features, our comparison includes leading Siamese methods like SIAMESEXML [26] and NGAME[27], as well as other relevant transformer-based approaches.\nNotably, RENEE [35] qualifies as both a dense model and a state-of-the-art XMC method. However, in some instances, RENEE employs larger encoders (e.g., Roberta-Large [67]). To maintain consistency and fairness in our evaluations, we exclude configurations employing larger encoders from this analysis. For conceptual validation, we used RIGL[6] on datasets with label spaces up to 670K.\nAs is standard in XMC literature, we compare the methods on metrics which only consider prediction at top-k slots. This includes : Precision@kand its propensity-scored variant (which is more sensitive to performance on tail-labels). The details of these metrics are given in Appendix A."}, {"title": "Empirical performance", "content": "Table 2 presents our primary results on the datasets, compared with the aforementioned baselines. The performance metrics for XMC baselines are reported from their original papers. However, for peak memory consumption, we re-ran these baselines in half precision with the same batch size, as all baselines are also evaluated in half precision. Following DST protocols, we extended the training duration for RIGL, Dense Bottleneck, and our method to twice the number of steps used for dense models. Certain baselines (referred to as OOM) could not scale to the Amazon-3M dataset. Our results demonstrate that our method significantly reduces memory usage while maintaining competitive performance. On the Amazon-3M dataset, our approach delivers comparable performance to dense"}, {"title": "Adapting to increased sparsity and label size: the role of auxiliary objective", "content": "The DST approach is widely recognized to be problematic when dealing with high sparsity levels (\u226590%). This is also apparent in our experiment and can be observed in Figure 3 (right) when the label space size is constant. Our findings indicate that incorporating an auxiliary objective significantly aids"}, {"title": "Sensitivity to Auxiliary Loss cut-off epochs", "content": "We employ auxiliary loss with an initial scalar weight that decays until a specified cut-off epoch. Table 6 illustrates the model's final performance at various cut-off epochs for two sparsity levels. A value of 0 (No aux) indicates the absence of auxiliary loss, while 'No cut-off' signifies its application throughout training. Our analysis reveals that prolonging the auxiliary loss beyond optimal cut-off epochs adversely affects performance for both sparsity levels. Notably, maintaining the auxiliary loss throughout training leads to performance deterioration, resulting in scores lower than those achieved without its use."}, {"title": "DST with Fixed Embedding vs End-to-End Training", "content": "In Table 7, we compare the performance of models using fixed embeddings [34] with trained end-to-end using DST on the Wiki-500K and Amazon-670K datasets. End-to-end training yields consistent improvements over fixed embeddings across all metrics, with significant gains in P@1 (an increase of 3.1% on Wiki-500K and 4.5% on Amazon-670K). These highlight the need of enabling the model to adapt its representations while training for the best possible performance."}, {"title": "Conclusion and future work", "content": "In this paper, we demonstrated the feasibility of DST for end-to-end training of classifiers with hundreds of thousands of labels. When appropriately adapted for XMC problems with fixed fan-in sparsity and an auxiliary objective, DST offers a significant reduction in peak memory usage while delivering superior performance compared to bottleneck-based weight reduction. It is anticipated that the Python bindings of the CUDA kernels will be useful for the research community in making their existing and forthcoming deep XMC pipelines more memory efficient. We hope that our work will enable further research towards developing techniques which could be (i) combined with explicit negative mining strategies, and (ii) used in conjunction with larger transformer encoders such as Roberta-large."}, {"title": "Limitations and societal impact", "content": "For XMC tasks, this paper attempts to explore the landscape of sparse neural networks, which can be trained on affordable and easily accessible commodity GPUs. While the proposed scheme is able to achieve comparable results to state-of-the-art methods at a fraction of GPU memory consumption, it is unable to surpass the baseline with dense last layer on all occasions. The exact relative decline in prediction performance, when our proposed adaptations of Fixed Fan-In and auxiliary objective are employed, is shown in Figure 3.\nWhile we do not anticipate any negative societal impact of our work, it is expected that it will further enable the exploration of novel training methodologies for deep networks which are more affordable and easily accessible to a broader research community outside the big technology companies."}, {"title": "Effect of Intermediate Layer Size on Overall and Tail Label Performance", "content": "We investigated the impact of varying sizes of the intermediate layer on both overall performance and the performance of tail labels specifically as shown in Figure 5. It is important to highlight that although precision values peaked at certain layer sizes, the performance on tail labels continued to improve even"}, {"title": "The Role of Random Cluster based Meta Classifiers in XMC Problems", "content": "To understand the impact of random clusters on meta classifier-based methods, we selected the LightXML [23] approach and experimented with two large-scale datasets: Amazon-670K and Wiki-500K. For our experiments, we used the original code from the official LightXML repository and the original clusters provided by the authors. We randomized the original clusters by applying several iterations of random.shuffle(), repeating the process twice to generate two sets of random clusters.\nTo ensure randomness, we calculated the intersection of elements between each pair of clusters from the original and random sets. We then took the maximum overlap value among all pairs, which was less than 3.5% in both cases. Subsequently, we ran the LightXML code using the original clusters and the two sets of random clusters.\nOur observations revealed that the final performance remained largely unaffected, although the learning process slowed down initially, as shown in the upper row of Figure 6. The bottom row illustrates the precision of the meta classifier, which is lower for the random clusters as expected. We replicated the same experiment with the Wiki-500K dataset and observed similar results, which are also depicted in Figure 7."}, {"title": "Computational Resources", "content": "While we want to demonstrate the memory efficiency of our algorithms, in order to enable meaningful comparison with existing methods, we run all our experiments on a NVidia A100 GPU, and measure the memory consumption using torch.cuda.max_memory_allocated. On this GPU, the experiments"}, {"title": "Background", "content": "Problem setup Given a multi-label training dataset with N samples, D = {(xi, Pi){-1}, where L represents the total number of labels, and P\u2081 C [L] denotes a subset of relevant labels associated with the data point xi \u2208 X. Typically, the instances are text based, such as the contents of a Wikipedia article [20] or the title of a product on Amazon [47] with labels corresponding to Wikipedia categories and frequently bought together products, respectively, for example. Traditional XMC methods used to handle labels the same way as is typically done in other fields, as featureless integers.\nHowever, the labels themselves usually carry some information, e.g., a textual representation, as the following examples, taken from (i) LF-AmazonTitles-131K (recommend related products given a product name) and (ii) LF-WikiTitles-500K (predict relevant categories, given the title of a Wikipedia page) illustrate:"}, {"title": "Precision at k (P@k)", "content": "Precision at k (P@k): Precision at k is the fundamental metric for evaluating the top-k predictions\nin XMC applications such as e-commerce product recommendation and document tagging:\nP@k(yy) = 1 / k * \u03a3_i\u2208topk(y)  ye\nwhere y is the true label vector, \u0177 is the predicted score vector, and topk (\u0177) identifies the indices with\nthe top-k highest predicted scores."}, {"title": "Propensity-Scored Precision at k (PSP@k)", "content": "Propensity-Scored Precision at k (PSP@k): Given the long-tailed label distribution in many XMC\ndatasets, PSP@kincorporates a propensity score yi to weight the precision contribution of each label,\nthereby emphasizing the tail labels' performance:\nPSP@k(y,y) = 1 / k * \u03a3_i\u2208topk(y)  ye / Pe\nwhere pi corresponds to the propensity score for the label yt [38]."}, {"title": "Macro Precision at k (Macro P@k)", "content": "Macro Precision at k (Macro P@k): To capture the average precision across all labels and mitigate\nany label imbalance, Macro Precision at k is used:\nMacro P@k=1/L * \u03a3^L_i=1 |\u03a3(y\u2208topk(y) \u2229 Y_i)|  / min(k, |topk (y_i)|)"}]}