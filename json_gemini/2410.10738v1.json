{"title": "DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model", "authors": ["Yuqi Wang", "Ke Cheng", "Jiawei He", "Qitai Wang", "Hengchen Dai", "Yuntao Chen", "Fei Xia", "Zhaoxiang Zhang"], "abstract": "Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.", "sections": [{"title": "1 Introduction", "content": "World models [17, 20, 33, 21] have gained increasing attention due to their ability to model complex real-world physical dynamics. They also hold potential as general-purpose simulators, capable of predicting future states in response to diverse action instructions. Facilitated by advancements in video generation techniques [53, 24, 3, 2], models like Sora have achieved remarkable success in producing high-quality videos, thereby opening up a new avenue that treats video generation as real-world dynamics modeling problem [47, 19, 56]. Generative world models, in particular, hold significant promise as real-world simulators and have garnered extensive research in the field of autonomous driving [28, 48, 30, 49, 54, 60, 13].\nHowever, existing driving world models fall short of meeting the requirements of model-based planning in autonomous driving, which aims to improve driving safety in scenarios with diverse ego maneuvers and intricate interaction between the ego vehicle and other road users. These models perform well for non-interactive in-lane maneuvers but have shown limited capability in following more challenging action instructions like lane change. One significant roadblock to building next-generation driving world models lies in the datasets. Autonomous driving datasets commonly used in current world model literature like nuScenes [6], Waymo [45], and ONCE [37], are primarily designed and curated in a perception-oriented manner. As a result, it contains limited driving patterns and multi-agent interactions, which may not fully capture the complexities of real-world driving scenarios. The scarcity of interaction data limits the ability of models to accurately simulate and predict the complex dynamics of real-world driving environments.\nIn this paper, we propose DrivingDojo, a large-scale driving video dataset designed to simulate real-world visual interaction. As illustrated in Figure 1, DrivingDojo features action completeness, multi-agent interplay, and open-world driving knowledge. Our dataset aims to unleash the full potential of world models in action instruction following by including rich longitudinal maneuvers like acceleration, emergency braking and stop-and-go as well as lateral ones like U-turn, overtaking, and lane change. Besides, we explicitly curate the dataset to include a large volume of trajectories"}, {"title": "2.2 World Model", "content": "Learning world models. World models [17, 33] enable next-frame prediction based on action inputs, aiming to build general simulators of the physical world. However, learning dynamic modeling in pixel space is challenging, leading previous image-based world models to focus on simplistic gaming environments or simulations [18, 20, 9, 52, 44, 43, 21]. With advances in video generation, models like Sora can now produce high-definition videos up to one minute long with natural, coherent dynamics. This progress has encouraged researchers to explore world models in real-world scenarios. DayDreamer [51] applies the Dreamer algorithm to four robots, allowing them to learn online and directly in the real world without simulators, demonstrating that world models can facilitate faster learning on physical robots. Genie [5] demonstrates interactive generation capabilities using vast internet gaming videos and shows potential for robotics applications. UniSim [55] aims to create a universal simulator for real-world interactions using generative modeling, with applications extending to real-robot executions.\nWorld model for autonomous driving. World models serving as real-world simulators have garnered widespread attention [16, 61] and can be categorized into two main branches. The first branch explores agent policies in virtual simulators. MILE [27] employed imitation learning to jointly learn the dynamics model and driving behavior in CARLA [11]. Think2Drive [34] proposed a model-based RL method in CARLA v2, using a world model to learn environment transitions and acting as a neural simulator to train the planner. The second branch focuses on simulating and generating real-world driving scenarios. GAIA-1 [28] introduced a generative world model for autonomous driving, capable of simulating realistic driving videos from inputs like images, texts, and actions. DriveDreamer [48] emphasized scenario generation, leveraging HD maps and 3D boxes to enhance video quality. Drive-WM [49] was the first to propose a multiview world model for generating high-quality, controllable multiview videos, exploring applications in end-to-end planning. ADriver-I [30] constructed a general world model based on MLLM and diffusion models, using vision-action pairs to auto-regressively predict current frame control signals. DriveDreamer2 [60] leveraged LLMs and text prompts to generate diverse driving videos in a user-friendly manner. Unlike previous methods that focused on model design, OpenDV-2K [54] addressed the issue of training data by collecting over 2000 hours of driving videos from the internet. Previous research has predominantly addressed static scene generation, with limited emphasis on multi-agent interplays. Our dataset enables the exploration of world model predictions within dynamic, interactive driving scenarios."}, {"title": "3 The DrivingDojo Dataset", "content": "Our goal is to provide a large and diverse action-instructed driving video dataset DrivingDojo to support the development of driving world models. To accomplish this, we extract highly informative clips from a video pool collected through fleet data, spanning several years and comprising more than 500 operating vehicles across multiple major Chinese cities. As a result, our DrivingDojo features diverse ego actions, rich interactions with road users, and rare driving knowledge which are crucial for high-quality future forecasting as shown in Table 2."}, {"title": "3.1 Action Completeness", "content": "Using the driving world model as a real-world simulator requires it to follow action prompts accurately. Existing autonomous driving datasets, such as ONCE [37] and nuScenes [6], are generally curated for developing perception algorithms and thus lack diverse driving maneuvering.\nTo enable the world model to generate an infinite number of high-fidelity, action-controllable virtual driving environments, we create a subset called DrivingDojo-Action that features a balanced distribution of driving maneuvers. This subset includes a diverse range of both longitudinal maneuvers, such as acceleration, deceleration, emergency braking, and stop-and-go driving, as well as lateral maneuvers, including lane-changing and lane-keeping. As demonstrated in Figure 3a, our DrivingDojo-Action subset offers a significantly more balanced and complete set of ego actions compared to existing autonomous driving datasets."}, {"title": "3.2 Multi-agent Interplay", "content": "Besides navigating in a static road network environment, modeling the dynamics of multi-agent interplay like merge and yield is also a crucial task for world models. However, current datasets are either built without considering multi-agent interplays, such as nuScenes [6] and Waymo [45], or are constructed from large-scale internet videos that lack proper curation and balancing, like OpenDV-2K [54].\nTo address this issue, we design the DrivingDojo-Interplay subset focusing on interactions with dynamic agents as a core component of the dataset. As shown in Figure 1b, we curate this subset to include at least one of the following driving scenarios: cutting in/off, meeting, blocked, overtaking, and being overtaken. These scenarios encompass a variety of realistic situations, such as vehicles cutting into lanes, encounters with oncoming traffic, and the necessity for emergency braking. By incorporating these diverse scenarios, our dataset enables world models to better understand and anticipate complex interactions with dynamic agents, thereby improving their performance in real-world driving conditions."}, {"title": "3.3 Rich Open-world Knowledge", "content": "In contrast to perception and prediction models, which compress high-dimensional sensor input into low-dimensional vector representations, world models exhibit a superior modeling capacity by operating in the pixel space. This increased capacity enables world models to effectively capture the intricate dynamics of open-world driving scenarios, such as animals unexpectedly crossing the road or parcels falling off the trunks of vehicles.\nHowever, existing datasets, either perception-oriented ONCE [37] or planning-oriented ones like nuPlan [7], do not have adequate data for developing and assessing the long-tail knowledge modeling ability of world models. Therefore, we place a unique emphasis on including rich open-world knowledge video clips and construct the DrivingDojo-Open subset. As shown in Figure 1c, describing open-world driving knowledge like this is challenging due to its complexity and variability, but these scenarios are crucial for ensuring safe driving."}, {"title": "3.4 Data Curation and Statistics", "content": "Dataset statistics. The DrivingDojo dataset contains around 18k videos with resolution of 1920\u00d71080 and frame rate at 5 fps. Our video clips are collected from major Chinese cities including Beijing, Shenzhen, Xuzhou, etc., as shown in Figure 4. Furthermore, these videos are recorded in diverse weather conditions at different daylight conditions. All videos are paired with synced camera poses derived from the HD-Map powered high precision localization stack onboard. Videos in the DrivingDojo-Open subset are paired with text descriptions about the rare event happening in each video. More details are in the Appendix.\nData collection. We collected multi-modal fleet data using the platform of Meituan's autonomous delivery vehicles. Our dataset consists of video clips recorded by the front-view camera with a horizontal field of view of 120\u00b0 to capture comprehensive visual information. The raw data is collected from multiple Chinese cities between May 2022 and May 2024, amassing a total of 900,000 videos and approximately 7,500 hours of driving footage pre-filtered before recording.\nData curation. In order to ensure both the data diversity as well as balanced ego action and multi-agent interplay distribution, we include fleet data with different criteria. The data sources of DrivingDojo include 1) intervention data from safety inspectors during vehicle operation, 2) emergency brake data from automatic emergency braking, 3) randomly sampled 30-second general videos from collected videos, 4) selected distinct scenarios such as traffic light changes, barrier opening, left and right turns, straight crossings, vehicle encounters, lane changes, and pedestrian interactions, 5) manually sorted rare data containing moving and static foreign objects on the road, floating obstacles, falling and rolling objects. The curation details are in the Appendix.\nPersonal Identification Information (PII) removal. To avoid privacy infringement and obey the regulation laws, we employ a high precision license plate and face detectors [31] to detect and blur these PII for each frame of all videos. An in-house annotation team and the authors have manually double-checked that the PII removal procedure is correctly carried out for all the videos."}, {"title": "4 DrivingDojo for World Model", "content": "To facilitate the study of world models in autonomous driving, we define a novel action instruction following (AIF) task. We provide baseline methods (Section 4.2) and evaluation metrics (Section 4.3), enabling further investigations. More details are described in the Appendix."}, {"title": "4.1 Action Instruction Following", "content": "Action-controllable video forecasting is the core ability of world models [5]. Instead of solely focusing on predicting high-quality video frames, action instruction following requires world models to take both the initial video frame and ego action prompts into consideration for predicting corresponding world responses. Given the initial image $I_t$ and a sequence of actions ${A_t, ..., A_{t+k}}$, the model $f_\\theta$ predicts future states ${I_{t+1}, ..., I_{t+k}}$ as:\n${I_{t+1}, ..., I_{t+k}} = f_\\theta(I_t, {A_t, ..., A_{t+k}}).$ (1)\nHere, ${A_t, ..., A_{t+k}}$ refers to the action prompts for each frame, with trajectories $A_t = (\\Delta x_t, \\Delta y_t)$ in our experiment. $f_\\theta$ represents the world model, and ${I_{t+1}, \u2026, I_{t+k}}$ signifies the visual prediction for subsequent k frames."}, {"title": "4.2 Model Architecture", "content": "We propose DrivingDojo baseline, a video generation model based on Stable Video Diffusion (SVD) [2]. While SVD is a latent diffusion model for image-to-video generation, we extend its capability to generate videos conditioned on action. For the AIF task, we encode the value of each action sequence into a 1024-dimensional vector using a Multilayer Perceptron (MLP). Subsequently, the action feature is concatenated with the first-frame image feature and passed into the U-Net [40]."}, {"title": "4.3 Evaluation Metrics", "content": "Visual quality. To evaluate the quality of the generated video, we utilize FID (Frechet Inception Distance) [23] and FVD (Frechet Video Distance) [46] as the main metrics.\nAction instruction following. We propose the action instruction following (AIF) errors $E_{AIF}^x$ and $E_{AIF}^y$ to measure the consistency between the generated video and the input action conditions. Given the generated video sequences ${I_t, ..., I_{t+k}}$, we estimate vehicle trajectories in the generated videos with the offline visual structure-from-motion (SfM) implementation like COLMAP [41, 42]: ${A_t, ..., A_{t+k}} = SfM({I_t, ..., I_{t+k}})$, where ${\\hat A_t, ..., \\hat A_{t+k}}$ are estimated trajectories of unknown scale. We estimated the scale factor $\\hat S$ for the predicted trajectory by minimizing the error between estimated and input ego-motion in the first N frames. We compare the estimated actions with the ground-truth action instructions ${A_t, ..., A_{t+k}}$ and report the mean absolute error for both lateral ($E_{AIF}^x$) and longitudinal ($E_{AIF}^y$) actions:\n$\\left(E_{AIF}^x, E_{AIF}^y\\right) = \\frac{\\sum_{i=0}^{k}\\left|A_{t+i} - \\hat A_{t+i} * \\hat S\\right|}{\\hat k+1}$ (2)\nwhere the scale factor $\\hat S = arg\\min_S \\sum_{i=0}^{N} \\left|A_{t+i} - \\hat A_{t+i} * S.\\right|$"}, {"title": "5 Experiments", "content": "To illustrate the richness of behaviors and dynamics within our dataset, we compare video fine-tuning quality across various datasets."}, {"title": "5.1 Results of Visual Prediction", "content": "2K dataset [54] as our test set and evaluated fine-tuning performance of SVD [2] model across various datasets. The results indicate that models trained on our dataset exhibit better visual quality."}, {"title": "5.2 Results of Action Instruction Following", "content": "Diverse driving behaviors. Based on different sequences of actions, our model is able to generate multiple possible futures. As shown in Figure 5, we showcase the model's capability to execute forward, left turn, and right turn maneuvers at intersections, as well as lane-changing to the left or right, and maintaining on straight roads.\nAction instruction following. Although qualitative evaluations demonstrate the powerful generative ability of our model, we also endeavor to measure the accuracy of action instruction following quantitatively. We seek to evaluate whether the video trajectories generated by the model closely adhere to our expected route paths. This serves as a fundamental assurance for the future application of world model. As shown in Table 4, with the in-domain actions (original action sequences of the test video) as conditions, videos generated by the baseline world model trained on DrivingDojo exhibit strong loyalty towards the action instructions. The mean action error in each video frame is limited to only 10 cm in the lateral or longitudinal directions. In row 3, feeding the model with the same initial images and randomly sampled action instructions slightly increases the mean action errors. When the model is applied zero-shot to initial images from OpenDV-2K [54] and fed with randomly sampled action instructions, its generated videos still demonstrate considerable consistency to the action instructions. Note that the proposed action instruction following errors can sensitively reflect the impact of out-of-domain inputs on the performance of the model."}, {"title": "Zero-shot evaluation.", "content": "As shown in Table 5, we compared the performance of models trained on different datasets and their zero-shot generalization performance on new datasets. The results indicate that models trained on our dataset exhibit higher generation quality and significantly improved action-following ability. Especially, we noticed that richer driving actions in the autonomous driving datasets lead to significantly better AIF performance of models trained on them. According to Figure 3a, videos in DrivingDojo averagely contain far richer driving actions compared to ONCE or nuScenes. This leads to the far better AIF performance of model trained on DrivingDojo compared to those trained on ONCE or nuScenes. we observed that the model trained on the ONCE dataset will always generate videos in which the vehicle moves in a straight line, even with action instructions to turn left/right or change lanes. This leads to its especially poor AIF performance in the lateral direction ($E^x_{AIF}$). We speculate that this is because the driving action of making turns or changing lanes is very rare in the ONCE dataset, as shown in Figure 3a, which results in the lack of ability of the model trained on the ONCE dataset to follow the lateral motion instructions. Moreover, the even more lacking driving actions in the nuScenes dataset lead to a worse AIF performance of the world model."}, {"title": "AIF visualization.", "content": "We showcase examples of estimated trajectories from generated videos in Figure 6. In each frame, the red dot represents the current estimated camera pose and the black dots represent the camera poses in past frames."}, {"title": "5.3 Real-world Simulation", "content": "Action generalization. Our model demonstrates robust generalization capabilities in two key aspects. As illustrated in Figure 7a, firstly, it effectively generalizes to out-of-domain (OOD) actions, such as forcefully driving on pedestrian walkways, showcasing its adaptability to some unreasonable actions. Secondly, it successfully extends its capabilities to other datasets, executing tasks such as lane changes on the OpenDV-2K [54] dataset and backing-the-car maneuvers on the nuScenes [6] dataset without requiring further fine-tuning. This underscores the model's potential as a real-world simulator, capable of adapting to diverse driving scenarios."}, {"title": "Dynamic agents.", "content": "We showcase our model's ability to simulate interactions with dynamic agents in Figure 7b. The results indicate that the model can provide reasonable responses based on our actions. The first scenario depicts a pedestrian opting to yield as our vehicle continues forward, resulting in a change in trajectory. In the second scenario, a delivery person opts to stop and wait at a narrow road."}, {"title": "Open-world dynamics.", "content": "In Figure 7b, our model showcases the simulations of rare scenarios encountered on the road, including interactions with moving birds and parking lot barriers."}, {"title": "5.4 Limitations and Future Work", "content": "This dataset currently comprises only single-camera videos. Our primary focus is to maximize video diversity, which has led us to reduce the number of sensors used, enabling us to capture a wider range of scenes. Additionally, this paper primarily explores the value of the dataset, treating the model aspect as a baseline without any specialized design. Although the DrivingDojo dataset significantly improves model capabilities, there are still several limitations that require further investigation in future studies.\nHallucination. As shown in Figure 8, we observed that the model exhibits some hallucinations, such as the sudden disappearance of objects, and when an action is unrealistic given the scene, such as forcefully turning right, the model sometimes imagines a new road.\nLong-horizon visual prediction. Our baseline model is only capable of generating short videos, which can be used to simulate short-term interaction events. Longer predictions [4, 57, 22] and faster generation [38, 36] are left for future research.\nDriving policy. The long-tail cases in our dataset are valuable for driving policy research. While this work focuses on visual prediction in world models, future studies can investigate how this data improves driving policy."}, {"title": "6 Conclusion", "content": "In this work, we present DrivingDojo, a large-scale video dataset aimed at advancing the study of driving world models. DrivingDojo offers a testbed for studying diverse real-world interactions. Our findings indicate that simulating interactions and rare dynamics observed in open-world environments remains an unsolved challenge, highlighting significant opportunities for future research.\nSocietal impacts. By providing a comprehensive dataset covering diverse driving scenarios and behaviors, researchers can develop and refine algorithms that increase the safety, reliability, and efficiency of autonomous vehicles. However, the development of driving world model requires large and diverse driving videos, introducing privacy issues."}, {"title": "A Dataset", "content": "A.1 Overview\nWe will publish the DrivingDojo dataset, data format and annotation instructions, AIF benchmark, and code for the baseline method on our project page: https://drivingdojo.github.io.\nTerms of use and License. Our dataset is released under the CC BY-NC 4.0 license, allowing everyone to use it for non-commercial research purposes.\nData maintenance. The data is stored on Google Drive for global accessibility, and we will supply various links (e.g., Hugging Face) for researchers' convenience. We will maintain the data long-term and periodically verify its accessibility.\nIn the following, we showcase more video examples in our DrivingDojo dataset, the corresponding videos are better illustrated on our project page.\nAction completeness. We include more dataset visualizations depicting various ego-actions in Figure 9. From top to bottom, the images show the ego vehicle performing left turns, right turns, going straight, lane-changing, and making emergency brakes during the driving.\nMulti-agent interplay. Interaction plays a crucial role in driving scenarios. It usually means that the ego vehicle has engaged with other road users, leading to changes in the behavior of either the ego vehicle or the other road users. As shown in Figure 10, we present a series of interaction examples in our dataset. In the first scenario, the car suddenly encounters another vehicle crossing its path while moving forward, prompting an abrupt braking maneuver. The second scenario portrays the car encountering an electric scooter unexpectedly crossing its path. Illustrating the third scenario, the car comes across a vehicle in front opening its door, forcing an abrupt brake. In the fourth scenario, the challenge involves encountering a bicycle approaching from the opposite direction, while the fifth scenario involves navigating around a stroller. The sixth scenario showcases encountering road construction ahead, followed by encountering a street sweeper in the seventh scenario. The eighth scenario presents a situation where a car suddenly makes a U-turn from the opposite direction, prompting an urgent braking response from our vehicle. Subsequent scenarios involve interactions with pedestrians. These diverse interaction scenarios provide a crucial foundation for studying the interaction of real-world simulators.\nOpen-world knowledge. In complex driving environments, we often encounter a wide variety of open-world situations. These scenarios can include sudden appearances of unexpected obstacles such as fallen trees, construction barriers, or abandoned vehicles. Typically belonging to the tail end of a long-tail distribution, these scenarios are rare yet crucial for ensuring safe driving. In Figure 11, we showcase a series of examples from the dataset, which fully demonstrate the richness of our dataset in capturing long-tail scenarios. From top to bottom, the examples illustrate encounters with a crane, a towing rope, construction barriers, a fallen roadblock, a vehicle transporting iron pipes, a vehicle transporting tree branches, a herd of sheep, an excavator, a bonfire, and power lines."}, {"title": "A.2 Curation", "content": "In this section, we provide the details of the curation procedure of each subset of DrivingDojo dataset and the descriptions of curated actions and interactions. This section supplements the details for Section 3.4 in the main paper.\nAction Completeness Ego maneuvers for a car, particularly in the context of autonomous driving, refer to the actions and decisions the vehicle makes to navigate its environment safely and efficiently. Here is an exhaustive list of common ego maneuvers, and some examples in our datasets are shown in Figure la in the main paper:\n\u2022 Acceleration: Increasing speed to match traffic flow.\n\u2022 Deceleration: Gradual slowing down for stop signs, traffic lights, or traffic congestion."}, {"title": "\u2022 Lane Keeping", "content": "Maintaining the current lane.\n\u2022 Lane Changing: Changing lanes to overtake slower vehicles or merge into traffic.\n\u2022 Turning: Left/right or U-turns at intersections or roundabouts.\n\u2022 Stop and Move on: Stopping/proceeding at traffic signals or stop signs.\n\u2022 Emergency brake: Abrupt and sudden braking maneuver to avoid a collision or mitigate the impact of a potential hazard.\nSo, in the DrivingDojo-Action set, the videos follow different action commands, and the actions are mainly from the planning and control (PNC) signals, such as left and right turns, straight crossings, and lane changes. Each curated video clip begins with the PNC issuing a specific command and ends when the command is completed."}, {"title": "Multi-agent Interplay", "content": "The examples of multi-agent interplay are shown in Figure 1b in the main paper. Then we describe the detailed cases of the interactions with dynamic agents.\n\u2022 Cutting in/off: Another vehicle abruptly changes lanes and enters the path of the autonomous vehicle. Ego vehicle changes lanes and enters the path of the other vehicles.\n\u2022 Meeting: Ego vehicle encounters other vehicles traveling in the opposite direction.\n\u2022 Blocked: Ego vehicle is stopped by other agents, such as vehicles, motorcycles, and pedestrians.\n\u2022 Overtaking and being overtaken: Ego vehicle attempting to pass another vehicle and being passed by another vehicle.\nIn the DrivingDojo-Interplay set, the core data curation strategy is to find the interaction with other agents. The interaction is determined using PNC signals and manually defined rules. The main interaction videos are from PNC dangerous interaction data. PNC conducts a deduction between the ego vehicle and obstacles. When the ego vehicle cannot avoid collision by turning the steering wheel or slowing down slightly, it is a PNC interaction case."}, {"title": "Open-world Knowledge", "content": "Here, we select some representative and interesting examples from these rare cases and show them in Figure 1c in the main paper. Based on the provided image and the given descriptions, here are the detailed descriptions of each rare case in autonomous driving:\n(a) A worker's helmet rolls on the sidewalk next to the road. (b) A soccer ball is seen flying across the road. (c) A water bucket is depicted falling onto the road. (d) Parcel boxes have fallen onto the road. (e) A dog is crossing the road. (f) A rope is floating over the road. (g) The traffic light turns red. (h) A boom barrier blocks the vehicle from moving forward.\nAs mentioned above, we curated DrivingDojo-Open set in which the videos are more carefully categorized and labeled with text descriptions. The sources are unusual weather, foreign objects on the road surface, floating obstacles, falling objects, taking over cases, and interactions with traffic lights and boom barriers. For curating the foreign objects/obstacles, we manually check and label them by a large number of data annotators."}, {"title": "Dataset Format", "content": "DrivingDojo dataset provides a file named 'dataset_info.json' that stores information corresponding to each video segment, including the information shown in Table 6. The 'type' represents the major category, 'tag' represents the minor category, and 'remark' provides detailed descriptions of the reasons for hard braking and intervention."}, {"title": "Video info.", "content": "The video is stored as a sequence of individual image frames. The distribution of video duration is shown in Figure 12, with the majority of videos lasting around 20 seconds."}, {"title": "B Implementation Details", "content": "B.1 Experiment Setup\nDuring the experiment, we employed two settings for training the model. In the first setting, the focus is on visual prediction: the model predicts subsequent video content based solely on the initial frame image. In the second setting, we employ action-controlled video generation. Here, alongside the initial frame image, action information for the subsequent frames is provided to the model, enabling it to predict the ensuing video content.\nVisual prediction. In this setup, we trained a high-resolution version of the model, 1024\u00d7576 resolution for 14 frames, aimed at better capturing the generation of long-tail objects. Additionally, we developed a low-resolution version of the model, 576\u00d7320 resolution for 30 frames, to simulate various vehicle behaviors and interaction events. We fine-tune all parameters of the U-Net model.\nAction instruction following. In this setup, we trained model using 576\u00d7320 resolution for 30 frames. We fine-tune all U-Net parameters together with a new action encoder.\nB.2 Training\nWe initialize the model using the SVD-XT checkpoint. Following SVD, our model is trained with the EDM framework [32]. During training, we set the fps to 5 and the motion_bucket_id to 127. We utilize the AdamW optimizer [35] with a learning rate of 1 \u00d7 10-5. The training process is conducted on 16 NVIDIA A100 (80G) GPUs with 32 batch size for 50K iterations. To allow classifier-free guidance [25], we drop out action feature with a ratio of 20%.\nB.3 Evaluation\nDuring inference, we generate videos using the DDIM sampler for 25 steps.\nVisual Quality. To evaluate the quality of the generated video, we utilize FID (Frechet Inception Distance) [23] and FVD (Frechet Video Distance) [46] as the main metrics. For FID calculation on videos, we randomly select 5,000 frames for evaluation. Additionally, for FVD calculation, we generate 256 videos for evaluation. The results are the average of 10 calculations. We use the official UCF FVD evaluation code2."}, {"title": "Action instruction following (AIF).", "content": "For each generated video with action instructions, we estimate the camera poses for each frame in the video, align the scale of the estimated trajectory with the instruction trajectory, and compare the vehicle motion in each frame with the respective action instructions. We estimate the ego trajectories in generated videos using the offline visual structure-from-motion (SfM) implementation COLMAP [41, 42]. We found that moving objects significantly impact the quality of the reconstruction, so we used instance masks to occlude foreground moving objects during the reconstruction process. For videos generated based on initial images from DrivingDojo, we fix the camera intrinsic parameters as the ground truth values for videos from DrivingDojo. For videos generated from initial images with unknown camera intrinsics (e.g. images from OpenDV-2K), we estimate the camera intrinsics together with the camera extrinsics of images. We perform feature point extraction, feature point matching, and sparse scene reconstruction with the official implementation of COLMAP\u00b3 to estimate the poses of cameras. In our experiments, we generate videos in 30 frames and align the scale of estimated trajectories with the instruction trajectories based on the motions in the first N = 10 frames. We report the mean value of the absolute error between estimated motions and instruction motions in all video frames."}, {"title": "C Visualizations", "content": "In this section, we show the model generation demos trained on the DrivingDojo dataset. As shown in Figure 13, our model can generate high-resolution, complex driving scenarios."}, {"title": "C.1 Diverse Actions", "content": "As shown in Figure 14, we demonstrate how actions control the generation of different futures, such as moving forward, backward, and stopping."}, {"title": "C.2 Dynamic Interaction", "content": "As shown in Figure 15, we observe that choosing different actions can influence the behavior of other vehicles, resulting in different responses from the world model. For instance, in the first example, if we choose to proceed slowly, the vehicle on the left decides to stop and yield. Conversely, if our vehicle stops, the left vehicle perceives an obstruction and slightly reverses to make way. In the second example, when we choose to brake, the right vehicle quickly cuts in front of us, while if we choose to proceed straight, the right vehicle waits in place."}, {"title": "C.3 Open-world Knowledge", "content": "As illustrated in Figure 16, we demonstrate the model's ability to simulate various open-world objects, such as encountering construction zones, rare objects like ladders or balloons on the road, and simulating a puddle of water on the ground."}, {"title": "D License of Assets", "content": "We report licenses of all artifacts used in this work in this section.\nModel We use the pre-trained stable video diffusion [2] checkpoints from the huggingface platform. These checkpoints are released under the stable video diffusion non-commercial community license agreement for research purpose.\nOur Dataset Our dataset is collected and curated by the autonomous driving team of Meituan Inc. The road test and data collection procedures conform to privacy and security requirements of local authorities. The authors have obtained the permission for publicly releasing this dataset from both the management team and the company legal team. All personal identifiable information has been removed by both algorithm and subsequent manual inspection. We release the dataset under the CC BY-NC 4.0 license.\nOther Datasets We use other public datasets in this work including nuScenes [6], ONCE [37] and OpenDV-2k [54]. The nuScenes [6] dataset is released under the CC BY-NC-SA 4.0 license with Dataset Terms 5. The ONCE dataset is also released under the CC BY-NC-SA 4.0 license with Dataset Terms 6. The OpenDV-2K dataset is constructed from publicly licensed datasets and youtube videos that the authors claimed to support academic usage licenses."}, {"title": "E Datasheet", "content": "E.1 Motivation\n\u2022 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\nWe introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of\nE.2 Composition\n\u2022 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\nThe instances of our DrivingDojo dataset are videos with ego actions and DrivingDojo-Open subset is also with text descriptions for each scene.\n\u2022 How many instances are there in total (of each type, if appropriate)?\nThere are 17.8k videos for the whole Driving"}]}