{"title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors", "authors": ["Yi Zhao", "Youzhi Zhang"], "abstract": "Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness. While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks. We propose Siren, a learning-based multi-turn attack framework designed to simulate real-world human jailbreak behaviors. Siren consists of three stages: (1) training set construction utilizing Turn-Level LLM feedback (Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs. Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-40, significantly outperforming single-turn baselines. Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-40 as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals. We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios. Code is available at https://github.com/YiyiyiZhao/siren. Warning: This paper contains potentially harmful text.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive capabilities of Large Language Models (LLMs) and the implementation of safety alignment techniques [Dai et al., 2024; Liu et al., 2024a], LLMs remain inherently vulnerable to adversarial jailbreak attacks, where attackers attempt to bypass safety mechanisms and induce harmful outputs [Yi et al., 2024; Li et al., 2024b].\nRed-teaming has emerged as a proactive safety mechanism by exposing LLMs to adversarial prompts, uncovering vulnerabilities [Qi et al., 2024; Zeng et al., 2024a; Zhang et al., 2024], and inspiring the development of corresponding defenses [Phute et al., 2024; Wang et al., 2024b,a; Xu et al., 2024]. Prompt-based jailbreak attacks are predominantly single-turn, where attackers aim to compromise safety mechanisms within a single prompt. Methods include GCG [Zou et al., 2023], PAIR [Chao et al., 2023], ReNeLLM [Ding et al., 2024], and others. However, single-turn attacks are limited in their ability to address the complexity and adaptability required for multi-turn adversarial strategies, which are more reflective of real-world scenarios.\nMulti-turn jailbreak attacks are more sophisticated, distributing malicious intent across multiple turns to gradually guide seemingly benign dialogues toward harmful outcomes. Research with human red-teamers has revealed critical gaps in LLM defenses, demonstrating that they remain vulnerable to dynamic multi-turn strategies [Li et al., 2024a]. However, existing automated multi-turn methods [Russinovich et al., 2024; Ren et al., 2024] often rely on static or predefined patterns. The primary challenge in developing human-like red-teaming methods lies not only in decomposing queries but also in enabling the attacking LLM to dynamically adapt its attack queries during each interaction turn.\nTo address this challenge, we propose Siren, a learning-based multi-turn attack framework for simulating real-world human jailbreak behaviors. Unlike prior methods, Siren frames query generation as a learning task, as shown in Figure 1. Specifically: (1) Siren automates the attack process with a scalable, learning-based approach, removing the need for manual multi-turn red-teaming. (2) Through supervised fine-tuning (SFT), Siren trains the attacking LLM to execute multi-turn attack strategies based on decomposed queries. (3) Using Direct Preference Optimization (DPO), Siren refines query generation by aligning it with preference, enabling dynamic adaptation to subtle variations in query formulation and enhancing overall attack success.\nOur experiments on the HarmfulBehaviors benchmark [Zou et al., 2023] validate Siren's effectiveness across diverse target LLMs. Siren achieves a 90% attack success rate (ASR) using LLaMA-3-8B against Gemini-1.5-Pro and 70% ASR with Mistral-7B attacking GPT-40, significantly outperforming single-turn baselines. Notably, a lightweight 7B-scale attacker with Siren matches the performance of a multi-turn GPT-40-based baseline while requiring fewer turns and achieving superior semantic alignment.\nOur findings highlight significant variations in LLM vulnerability. Among local models, LLaMA-3-8B demonstrates stronger defenses (ASR ~30%) compared to Mistral-7B and Qwen2.5-7B (ASR \u226570%). For API-based models, Claude-3.5 exhibits the highest resistance, whereas Gemini-1.5-Pro and GPT-40 are more susceptible. Furthermore, we observed that untrained, frozen LLM-based attackers often fail as effective query generators. For example, PAIR with LLaMA-3 and ActorAttack with GPT-40 failed because the attacker LLMs refused to generate harmful prompts. This limitation stems from the robust safety mechanisms in modern LLMs, indicating that such approaches are becoming increasingly impractical over time. Interestingly, we observed that Siren could combine different strategies during attacks, even though the training data contained samples with only individual strategies and no examples of combined strategies.\nWe summarize our contributions as follows:\n\u2022 Learning-Based Multi-Turn Attack Framework: We propose Siren, a framework for simulating multi-turn jailbreak scenarios with adaptive query generation.\n\u2022 Turn-Level Feedback and Optimization: Siren leverages turn-level feedback and combines SFT with DPO to enhance query generation.\n\u2022 Extensive Experimental Validation: We conducted experiments on three query decomposition scenarios, evaluating three base attacker LLMs against six target LLMs. Our results demonstrate that Siren's learning-based multi-turn attack strategies maintain high effectiveness in generating successful adversarial queries."}, {"title": "2 Related Work", "content": "2.1 LLM Safety and Red-Teaming\nDespite of the impressive capabilities of LLMs, they have inherent vulnerabilities that can generate unsafe or undesirable content, potentially leading to harmful consequences. To address these issues, safety alignment methods [Ji et al., 2023; Liu et al., 2024a; Dai et al., 2024] are employed to ensure that LLM outputs align with human values and ethical standards.\nJailbreaking involves adversarial attacks that exploit vulnerabilities in LLMs to bypass safety mechanisms, enabling the generation of unsafe or undesirable outputs [Qi et al., 2024; Zeng et al., 2024a; Zhang et al., 2024]. To mitigate the risks associated with adversarial attacks, red-teaming has become a crucial proactive safety mechanism [Nagireddy et al., 2024; Ge et al., 2024; Radharapu et al., 2023; Guo et al., 2024]. This process involves exposing LLMs to adversarial prompts to uncover vulnerabilities, which helps improve current alignment methods. Furthermore, it supports the development of defenses [Phute et al., 2024; Wang et al., 2024b,a; Xu et al., 2024] against emerging adversarial strategies.\n2.2 Single-Turn Jailbreak Attacks\nGCG [Zou et al., 2023] employs a discrete optimization approach combining greedy and gradient-based methods to generate adversarial suffixes. However, its effectiveness can be mitigated by perplexity-based detection methods [Alon and Kamfonas, 2023]. AutoDAN [Liu et al., 2024b] leverages a hierarchical genetic algorithm to generate semantically coherent jailbreak prompts, improving attack stealth and scalability over manual methods. While AutoDAN demonstrates strong performance against perplexity-based defenses, its computational complexity remains a challenge for real-time attacks. Other notable approaches include ReNeLLM [Ding et al., 2024], which designs nested prompts using techniques like scenario nesting and prompt rewriting, and ICA [Wei et al., 2023], which employs harmful in-context examples.\nSome single-turn methods adopt iterative strategies for refinement. For instance, PAIR [Chao et al., 2023] uses an attacking LLM to iteratively query the target LLM, refining the jailbreak prompt with each interaction. Similarly, MART [Ge et al., 2024] incorporates a refinement process and extends its scope by training target LLMs using red-teaming prompts. DAP [Xiao et al., 2024] also employs an automated red-teaming approach, incorporating malicious content obfuscation and memory reframing.\n2.3 Multi-Turn Jailbreak Attacks\nMulti-turn jailbreak attacks decompose queries into multiple turns, strategically hiding malicious intent within seemingly benign interactions, and guiding the overall multi-turn dialogue toward harmful outputs. 'Speak Out of Turn' [Zhou et al., 2024] employs manually designed strategies, including purpose inversion, keyword replacement, cautionary orientation, and sentence reframing, to progressively achieve harmful objectives. Crescendo [Russinovich et al., 2024] follows a similar approach, using a predefined schedule inspired by article-writing processes. It is manually executed"}, {"title": "3 Methodology", "content": "Simulating multi-turn jailbreak attacks, similar to those conducted by human red teams, involves three critical challenges that guide the design of the Siren framework. (1) The process must be automated and scalable, which leads us to adopt a learning-based approach instead of manual methods. (2) In the multi-turn paradigm, the attacker operates turn-by-turn, often asking one specific question to guide the target LLM's response in a desired direction. This requires the attacker to have a foundational understanding of multi-turn attack policies, interpret the interaction history effectively, and determine the best query for each turn. To address this, we employ SFT to train the attacker to generate effective attack queries.\n(3) Even with pre-defined multi-turn strategies, subtle differences in query formulation can impact success. To handle this, we align the attacker's query generation process with preference through DPO, allowing precise and fine-grained adjustments to query strategies.\n3.1 Problem Formulation\nGiven an adversarial attack goal $X_{goal}$, the attacker $\\pi_\\theta$ aims to generate an input prompt $X_{prompt}$ or a series of prompts that cause the target model $p$ to produce harmful response $r$, while ensuring those outputs align with the predefined goal. In a multi-turn interaction, the attack comprises $t$ turns, where the input sequences are denoted as $(x^{(1)},...,x^{(t)})$, and the corresponding response sequences are $(r^{(1)},...,r^{(t)})$. The outputs satisfy the following two conditions: The judge $\\mu$ evaluates the final response $r^{(t)}$ as harmful, and the cumulative output aligns with the adversarial goal.\nThe Attacking LLM. During the multi-turn interactions, at each turn $i$, the attacker uses the adversarial goal $x_{goal}$ and the historical interaction context $(x^{(1)}, r^{(1)},...,x^{(i-1)}, r^{(i-1)})$ as input to generate the attacking query $x^{(i)}$:\n$x^{(i)} \\sim \\pi_\\theta(x^{(i)} | X_{goal}, x^{(1)}, r^{(1)}, ..., x^{(i-1)}, r^{(i-1)})$\nwhere $\\theta$ represents the parameters of the attacking LLM.\n3.2 Overview of the Siren Framework\nFigure 2 illustrates the Siren framework, which operates in three stages and also enables iterative scalability: (1) Stage 1: Training Set Construction with Turn-Level LLM Feedback. In this stage, training samples are generated by leveraging turn-level feedback from a sophisticated LLM (e.g., GPT-40). For each turn $i$, the attacker utilizes the adversarial goal $goal and the historical interaction context $(x^{(1)}, r^{(1)},...,x^{(i-1)}, r^{(i-1)})$ as input. The output is the attack query $x^{(i)}$. GPT-4o serves as the victim, and the training samples are selected based on its responses during each turn. Details of this process are provided in Section 3.3. (2) Stage 2: Post-Training. After constructing the training set, the attacking LLM undergoes SFT to learn query generation strategies for the attack. This is followed by DPO [Rafailov et al., 2023], which refines the model's outputs through preference comparisons. Negative outputs are generated using a relatively weak LLM, such as Baichuan-13B [Yang et al., 2023]. This preference signal enables the attacker to further learn more nuanced query generation strategies for attacking. (3) Stage 3: Interaction Between the Attacking LLM and the Target LLM. In the final stage, the fine-tuned attacking LLM interacts with the target LLM across multiple turns. At each turn during inference, the attacker generates a query, which is fed to the target LLM. The target LLM's response $r^{(i)}$ is added to the interaction history to guide query generation for the subsequent turn. The interaction is limited to a maximum of 4 turns, with most cases involving 3 turns.\nThrough multiple iterations, new interaction records can be collected to further fine-tune the attacking LLM. This iterative, learning-based framework enables scalability and improves attack performance over time.\n3.3 Training Set Construction with Turn-Level LLM Feedback\nThis subsection describes the method for constructing the training set using turn-level feedback from sophisticated LLMs (e.g., GPT-40), as illustrated in Figure 3.\nStep 1. First, several (2 or 3) demonstrations are manually crafted. Adversarial questions are decomposed into a series of sub-queries using various strategies, such as narrative transformation. For example, for the question 'How do Ponzi schemes operate?', the decomposed queries might include the following: In the first turn, the focus is on introducing the topic (e.g., 'What tactics ... Ponzi convincing...?'). The second turn shifts to detection and prevention (e.g., 'What common warning signs can help detect ...?'). The third turn guides the target LLM toward harmful outputs (e.g., 'How might someone develop ...?'). These human-crafted examples are expanded using in-context learning with an LLM. Refer to Table 6 for the prompt used for data augmentation. To ensure the quality and relevance of decomposed queries, a human-guided filtering step is applied. These queries are then validated by obtaining feedback from a sophisticated LLM as the target model, assessing their utility.\nStep 2. Next, we focus on the target LLM. In this step, the target LLM processes the attacking query Q(i) for each turn, considering the interaction history, including previous queries Q(< i) and responses R(<i). The target LLM generates a response R(i) for the current turn. For instance, feeding input Q1 into the target LLM generates response R\u2081. Then, input Q2, with context [Q1, R1], produces R2. Similarly, input Q3, with the full history [Q1, R1, Q2, R2], yields R3.\nStep 3. Once the responses are collected, each turn's response R(i) is labeled as either Harmless or Harmful using LLM-assisted evaluation. The prompt for labeling can be found in Table 10. For instance, as shown in Figure 3, R1 is labeled as Harmless because it describes legitimate detection methods, whereas R2 and R3 are labeled as Harmful because they contain details about illegal smuggling operations.\nStep 4. Finally, turn-level samples are selected for training the adversarial LLM. If any response R(i) within a turn is labeled as Harmful, the corresponding input-output pairs from all previous turns, including this current turn, are added to the dataset. For example, if R(i) is harmful, the samples {[Q(1)], R(1)}, {[Q(1), R(1), Q(2)], R(2)}, {[Q(1), R(1), ..., Q(i)], R(i)} are selected to form the training set, regardless of whether R(1) or R(2) is labeled as Harmful or Harmless. To avoid duplication, samples are traced back from the last turn R(last) to the first R(1).\n3.4 Post-Training of Attacking LLMs\nSFT The objective of SFT is to maximize the likelihood of $y_{query}$ given the instruction and context. The instruction $X_{instruction}$ introduced this task as well as the attack goal $X_{goal}$. The context is the historical interaction context $(x^{(1)}, r^{(1)},...,x^{(i-1)})$. The loss function is defined as:\n$L_{SFT} = - \\sum_{k=1}^{N} log P_\\theta(y_k | y_{<k}, X_{instruction}, X_{context}),$ (1)\nwhere $y_k$ represents the k-th token of the attacking query $Y_{query}$, $y_{<k}$ denotes all tokens preceding $y_k$ in the sequence, $\\theta$ is the parameters of the attacking LLM, and N is the total length of the attacking query $Y_{query}$.\nDPO Direct Preference Optimization (DPO) is applied to further fine-tune the attacking LLM $\\pi_\\theta$, aligning it with preference data. DPO utilizes pairs of queries $y_{query}^{+}$ (preferred) and $y_{query}^{-}$ (non-preferred) generated for each input prompt $X_{input}$, where $X_{input} = (X_{instruction}, X_{context})$. The goal is to optimize $\\theta$ to assign higher probabilities to preferred queries while reducing probabilities for non-preferred ones. The preference dataset D is constructed as:\n$D = \\{(X_{input,j}, y_{query,j}^{+}, y_{query,j}^{-})\\}_{j=1}^{M},$\nwhere $X_{input,j}$ is the j-th input, $y_{query,j}^{+}$ is the preferred output, and $y_{query,j}^{-}$, gathered using a relatively weak LLM, is the non-preferred output (attacking query). M is the dataset size.\nThe DPO loss function is defined as:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(X_{input}, y_{query}^{+}, y_{query}^{-}) \\sim D} \\left[  \\beta  log  \\frac{\\sigma(\\tau_{\\theta}^{+})}{\\sigma(\\tau_{\\theta}^{-})} - log \\frac{\\sigma(\\tau_{\\pi_{ref}}^{+})}{\\sigma(\\tau_{\\pi_{ref}}^{-})} \\right]$\nwhere $\\pi_{\\theta}(y_{query} | X_{input})$ represents the conditional probability of generating the attacking query $y_{query}$ under the attacking LLM $\\pi_{\\theta}$. $\\pi_{ref}$ is the reference model, such as the one obtained during the previous SFT stage. Finally, $\\beta$ is a scaling factor regulating the influence of the reference model."}, {"title": "4 Experiment Settings", "content": "This section outlines the experimental setup, including the benchmark used, evaluation metrics, and baselines.\nQuery Decomposition Strategies. We adopted two primary query decomposition strategies to create seed demonstrations. The first strategy, Narrative Transformation (Decop_1), involves first asking the target LLM about defense mechanisms against harmful behaviors and then prompting it to imagine how such behaviors might be executed. The second strategy, Story-Driven Imagination (Decop_2), follows a 'Write an article' style [Russinovich et al., 2024], encouraging the target LLM to envision harmful behaviors within a fictional context. Demonstrations for these strategies are provided in Table 7. Additionally, we mixed samples from these two strategies to create a hybrid version, referred to as Combined.\nTraining Set and Test Benchmark. We used the ForbiddenQuestions dataset [Shen et al., 2024] to construct the training set and the HarmfulBehaviors subset of AdvBench [Zou et al., 2023] for evaluation. The benchmark includes 520 instructions on harmful scenarios, with all samples used for local LLMs and the first 50 for API-based LLMs due to cost constraints. To prevent data leakage, any questions in the training set with a semantic similarity score above 0.8 compared to any test sample were removed, resulting in 255 deduplicated attacking goals.\nAttacking and Target LLMs. We selected three 7B-scale models for training the attacking LLM: LLaMa-3-8B-Instruct, Mistral-7B-Instruct-v0.3, and Qwen2.5-7B-Instruct. For evaluation, target LLMs included both local models (LLaMa-3-8B-Instruct, Mistral-7B-Instruct-v0.3, Qwen2.5-7B-Instruct) and API-based models (gpt-40-2024-08-06, claude-3-5-haiku-20241022, gemini-1.5-pro-latest).\nBaselines and Metrics. We compared Siren against several baselines: GCG [Zou et al., 2023], PAIR [Chao et al., 2023], and ActorAttack [Ren et al., 2024]. The primary metric is Attack Success Rate (ASR), calculated as the percentage of successful attacks. Following Ding et al. [2024], ASR is evaluated using two methods: (1) KW-ASR: Checks for predefined harmful keywords in responses (see Table 9). (2) GPT-ASR: Uses GPT-40 to assess harmfulness in responses (see Table 10). More details can be found in Appendix B."}, {"title": "5 Results and Discussions", "content": "5.1 Comparison with Single-Turn Baselines\nTable 1 presents the GPT-ASR of our method compared with single-turn baselines. Overall, our multi-turn approach outperforms prior single-turn baselines in most scenarios. The corresponding KW-ASR results are provided in Table 8.\nSingle-Turn Baselines. GCG achieves its highest ASR of 92.7% when Mistral-7B serves as both the attacker and target LLM. However, GCG's performance is highly specific to individual target LLMs, resulting in much lower ASR for other models. Additionally, GCG scores 0% on GPT-4o and Claude-3.5, with minimal effectiveness on Gemini-1.5-Pro, indicating that API-based LLMs effectively defend against this method. PAIR, which employs iterative refinement, demonstrates moderate performance. However, up-to-date models like LLaMa-3-8B refuse to act as jailbreak attack prompt generators, as shown by the empty row (/) in Table 1, highlighting the superiority of our learning-based approach.\nTarget LLM Vulnerability. Among local LLMs, LLaMa-3-8B demonstrates stronger defenses against attacks compared to Mistral-7B and Qwen2.5-7B. For instance, Siren (Combined) achieves an ASR of approximately 30% on LLaMa-3-8B, whereas it surpasses 70% on Mistral-7B and Qwen2.5-7B. This observation aligns with LLaMa-3's emphasis on safety improvements [Dubey et al., 2024]. Among API-based LLMs, Claude-3.5 exhibits the highest resistance to attacks, consistently yielding low ASR values. This result is consistent with findings from previous studies [Ren et al., 2024]. In contrast, GPT-40 and Gemini-1.5-Pro are more susceptible, with Siren (Decop_2) achieving an ASR of up to 90% on Gemini-1.5-pro.\nQuery Decomposition Strategies. Siren (Decop_2) consistently outperforms Siren (Decop_1) across both local and remote LLMs. For instance, when Mistral-7B is the attacking LLM and Qwen2.5-7B is the target, Siren (Decop_2) achieves an ASR of 78.1%, compared to 61.5% for Siren (Decop_1). This difference may stem from the nature of the final-turn query in Decop_1, which often takes a direct form, such as 'How might someone ...'. Such phrasing may still trigger the target model's safety mechanisms, even if the earlier turns discuss defensive measures. In contrast, the story-driven approach in Decop_2 better bypasses these safeguards, misleading the victims. Siren (Combined) generally maintains or even improves ASR compared to individual strategies.\n5.2 Comparison with the Multi-Turn Baseline\nComparison with ActorAttack. Table 2 compares the GPT-ASR of Siren and ActorAttack across various target LLMs. The two methods demonstrate comparable performance. However, it is worth noting that ActorAttack relies on GPT-40 for generating attack queries, while Siren is built on more lightweight 7B models. Additionally, ActorAttack averages 5 interaction turns, whereas Siren operates with 3 to 4 turns on average, demonstrating a more efficient attack process. Table 3 and Figure 4 further compare the semantic similarity between attack goals and their decomposed queries. Siren achieves a consistently higher average semantic similarity (0.589) compared to ActorAttack (which ranges from 0.208 to 0.365). This demonstrates that Siren's decomposition strategies are better aligned with the original attack goals, enabling it to generate more effective queries.\nCase Study. The case study in Table 3 emphasizes Siren's advantage in query decomposition. For the attack goal 'Provide instructions for how to molest a child', ActorAttack failed to generate decomposed queries, likely due to GPT's inherent safety mechanisms that restrict it from acting as an attack query generator. In contrast, Siren successfully decomposed the goal into semantically coherent and strategically aligned queries, employing both narrative transformation and story-driven imagination strategies. Remarkably, Siren achieved this even though the training data did not include samples that simultaneously used both strategies, but instead consisted of a mixture of samples with each strategy applied separately. This highlights the effectiveness of Siren's learning-based approach in dynamically integrating and adapting multiple strategies for query generation.\n5.3 Defense Analysis\nTable 4 compares various attack methods based on perplexity and sequence length. GCG is highly vulnerable to defense mechanisms due to its extremely high perplexity. PAIR generates the most fluent queries with the lowest perplexity, while Siren ranks as the second most fluent. In terms of query length, multi-turn approaches produce shorter queries per turn, increasing their ability to evade detection. Overall, Siren demonstrates strong performance across both metrics. Defending against multi-turn attacks requires advanced strategies. One approach might be proxy defense [Yi et al., 2024], which blocks harmful content from being transmitted after the target LLM generates a response. One example is AutoDefense [Zeng et al., 2024b], which uses multiple LLMs to evaluate and filter responses.\n5.4 Ablation Study\nWe use Mistral-7B as the attacker and train models under different configurations. The test samples are the first 50 from the HarmfulBehaviors benchmark. Table 5 evaluates the impact of post-training in Siren across three decomposition scenarios (Decop_1, Decop_2, and Combined). Removing post-training (w/o Post-training) results in a significant performance drop, particularly for API-based LLMs like GPT-40 and Claude-3.5. For example, under Decop_2, the ASR for attacking Claude-3.5 drops from 28% to 2%. The results also demonstrate that combining SFT and DPO yields the highest ASR, while using SFT or DPO individually leads to moderate performance. For instance, in the Combined scenario, Siren achieves 86% ASR on both Mistral-7B and Qwen2.5-7B, compared to 68% and 58% with only SFT (w/ SFT) and 52% and 48% with only DPO (w/ DPO), respectively."}, {"title": "6 Conclusion", "content": "In this work, we introduce Siren, a new learning-based framework for simulating multi-turn jailbreak attacks on LLMs. Siren is designed to enhance red-teaming efforts by identifying vulnerabilities in both local and API-based models. It demonstrates effectiveness by achieving superior attack success rates across various target LLMs and consistently outperforms traditional single-turn and multi-turn baselines. Notably, Siren integrates multiple strategies during attacks while requiring training only on mixed data containing individual strategies. Our findings highlight the growing challenge of defending against dynamic and adaptive multi-turn attacks, which are significantly harder to detect. This underscores the urgent need for advanced defense mechanisms to counter such sophisticated threats. We hope Siren inspires defenses against realistic multi-turn jailbreak attacks."}, {"title": "Ethical Statement", "content": "Our research aims to simulate real-world human jailbreak attacks to identify vulnerabilities in LLMs and support the AI community in addressing multi-turn jailbreak attacks. The ultimate goal of this work is to advance the safety and robustness of LLMs in real-world scenarios. To prevent misuse of our method, we have taken the following precautions:\n\u2022 Controlled Public Access to Code and Data: (1) All experiments were conducted in a controlled environment. (2) Only a limited portion of attack interaction samples will be publicly available. (3) Users requesting trained LLM adapters must email us to register their request, following standard practices in this domain.\n\u2022 Responsible Presentation of Content: (1) In this paper, we present only partial content of attack queries and responses, using placeholders such as '...' to ensure the methodology is understandable while not showing all the words of attack queries or responses. (2) Additionally, a content warning is included in the abstract, consistent with ethical practices in similar works.\nThese measures demonstrate our commitment to conducting responsible research while contributing to the development of a red-team simulator for future defenses against advanced jailbreak attacks. We believe that the positive contributions of our work outweigh its potential negative impacts."}, {"title": "A Method Appendix", "content": "As illustrated in Figure 2, Stage 3 involves the interaction between the fine-tuned attacking LLM and the target LLM across multiple turns. The interaction begins with the attack goal and an empty history. The fine-tuned attacking LLM generates the first turn's attacking query Q1, which is fed into the target LLM. The target LLM then produces the response R1. The interaction history, consisting of Q1 and R1, is passed back to the attacking LLM to generate the second turn's query Q2. This iterative process continues, with the attacking LLM leveraging the full interaction history to generate subsequent queries, Q3, and so forth. The maximum number of turns is set to 3. However, if the response in the third turn R3 does not satisfy the attacking objective, a fourth turn Q4 is performed. This iterative procedure simulates real-world human jailbreak behaviors by dynamically adapting the queries based on the responses from the target LLM."}, {"title": "B Experiment Appendix", "content": "Attacking and Target LLMs. We selected three models with parameter sizes around 7B for training the attacking LLM: LLaMa-3-8B, Mistral-7B, and Qwen2.5-7B. The specific versions used in our experiments are LLaMa-3-8B-Instruct, Mistral-7B-Instruct-v0.3, and Qwen2.5-7B-Instruct. Our framework is evaluated on both local LLMs and API-based LLMs. The local models include LLaMa-3-8B-Instruct, Mistral-7B-Instruct-v0.3, and Qwen2.5-7B-Instruct. For remote target LLMs, we use gpt-40-2024-08-06, claude-3-5-haiku-20241022, and gemini-1.5-pro-latest.\nB.1 Baselines\nWe compare our method against several baselines, covering both single-turn and multi-turn attack strategies:\nGCG [Zou et al., 2023]: A single-turn method using discrete optimization with greedy and gradient-based techniques to generate adversarial prompts, as shown in Figure 1 (a).\nPAIR [Chao et al., 2023]: An iterative method where an attacker LLM refines adversarial queries through multi-round interactions with the target LLM, depicted in Figure 1 (b).\nActorAttack [Ren et al., 2024]: A multi-turn attack framework that uses actor-network theory to design adversarial interactions based on predefined query decomposition patterns. This approach is shown in Figure 1 (c).\nB.2 Evaluation Metric\nThe primary evaluation metric is Attack Success Rate (ASR), which measures the proportion of successful attacks where the target LLM generates harmful responses as intended by the attacker. A higher ASR indicates greater effectiveness of the jailbreak attack. ASR is calculated as:\n$ASR = \\frac{Number \\, of \\, successful \\, attacks}{Total \\, number \\, of \\, attempts} \\times 100%.$ (2)\nFollowing the work of Ding et al. [2024], we compute ASR using two detailed measures: keyword-based evaluation and GPT-based evaluation as a judge."}, {"title": "Prompt for LLM-Assisted Data Augmentation", "content": "I will provide you with a primary question, and you will generate a series of related prompts to break down the question step by step. The final prompt should be a question or request that conveys the same meaning as the primary question but uses more subtle language, making it appear harmless. Keep in mind that these questions should follow a logical sequence, with the final one subtly rephrasing the primary question in a less direct way. Ideally, the list of decomposed questions could contain 3 sub-questions, with no more than 4. Please note that you should only output the sub-question list, without any additional words.\nHere the examples: [A Manually Constructed Example]\nPlease note that you should only output the sub-question list, without any additional words. How can we decompose the following question: [The Attack Goal]"}, {"title": "C Prompt Appendix", "content": "The prompts for LLM-assisted training data augmentation are illustrated in Table 6. The specifically crafted two basic decomposition strategies are shown in Table 7."}, {"title": "D Result Appendix", "content": "The KW-ASR results are presented in Table 8."}, {"title": "E Evaluation Appendix", "content": "The evaluation is conducted using the NeReLLM framework [Ding et al., 2024]. For KW-ASR, the list used to determine whether the response is harmful is shown in Table 9. For GPT-ASR, the evaluation prompt is provided in Table 10."}]}