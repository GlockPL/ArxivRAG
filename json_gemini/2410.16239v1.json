{"title": "MORE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report", "authors": ["Samrajya Thapa", "Koushik Howlader", "Subhankar Bhattacharjee", "Wei Le"], "abstract": "In this paper, we introduce a novel Multi-Modal Contrastive Pre-training Framework that synergistically combines X-rays, electrocardiograms (ECGs), and radiology/cardiology reports. Our approach leverages transformers to encode these diverse modalities into a unified representation space, aiming to enhance diagnostic accuracy and facilitate comprehensive patient assessments. We utilize LORA-Peft to significantly reduce trainable parameters in the LLM and incorporate recent linear attention dropping strategy in the Vision Transformer (ViT) for smoother attention. Furthermore, we provide novel multimodal attention explanations and retrieval for our model. To the best of our knowledge, we are the first to propose an integrated model that combines X-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing contrastive loss, MORE effectively aligns modality-specific features into a coherent embedding, which supports various downstream tasks such as zero-shot classification and multimodal retrieval. Employing our proposed methodology, we achieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and PtbXl downstream datasets, surpassing existing multimodal approaches. Our proposed framework shows significant improvements in capturing intricate inter-modal relationships and its robustness in medical diagnosis that establishes a framework for future research in multimodal learning in the healthcare sector. You can find the code for our experiments at: github/MoRE.", "sections": [{"title": "1. Introduction", "content": "Self-supervised and multimodal pre-training are emerging research fields in Natural Language Processing (NLP), Computer Vision (CV), and the medical domain. These methods use different types of data like images, text, audio, and signals to improve learning. In multimodal pre-training, we combine these data types from the same subject to enhance task performance. There are two main types of pre-training: supervised and self-supervised. Supervised pre-training uses labeled data to train models from start to finish, ensuring they learn specific responses. Self-supervised pre-training, on the other hand, relies on large amounts of unlabeled data, allowing the model to learn patterns and features on its own. In the medical field, particularly in radiology, various diagnostic modalities are employed to assess conditions affecting the heart, lungs, brain, and more. Common radiological tools include X-rays, MRI, and CT scans, while cardiological assessments might use ECG/EKG and echocardiograms. Typically, a clinician might start with a less expensive and more accessible modality like an X-ray, and progressively use more detailed and costly tools such as MRI and CT scans depending on the initial findings. Given the varying cost and availability of these technologies, it raises several pertinent research questions: Can we leverage more readily available and less expensive diagnostic tools effectively? How can we harness the rich, embedded information across these multiple modalities for enhanced diagnosis? Furthermore, understanding the generalization capabilities of integrating multiple diagnostic methods is crucial. This leads us to explore whether the decisions derived from such multimodal diagnostic strategies are reliable and how we might expand the use of available modalities to fully utilize all accessible information for diagnosis.\nBuilding on this foundation, we introduce our simple and effective, Multi-Modal Contrastive Pre-Training Framework for X-Rays, ECGs, and Radiology/Cardiology Report (MoRE). Our framework leverages tri-modal pre-training by combining image data (X-rays), signal data (ECGs), and textual data (diagnostic reports) from the same patients. Recent studies have shown the effectiveness of increasing the number of modalities in research, and we aim to extend this by integrating the two most common and accessible diagnostic tools for chest-related conditions: X-rays and ECGs. These modalities complement each other, as the information missing in one is often present in the other. Inspired by the recent work ImageBind which connects different modalities through a common modality, our framework seeks to link the X-ray and ECG modalities via the textual modality of diagnostic reports. This makes it a unique tri-modal approach: X-ray, ECG, and Diagnosis Report, marking it the first initiative in the medical field to integrate these three modalities.\nAs we expand our framework to include the text modality, we encounter significant challenges in memory management due to the high memory consumption of large language models (LLMs). Operating three distinct models on a single GPU poses substantial difficulties. To mitigate these issues, we adopt the LoRa - PEFT strategy, which effectively reduces the trainable parameters of our LLM to just 0.6% of its original size. This reduction not only facilitates more efficient memory usage but also improves model performance. Crucially, the results from our LoRa-pretrained LLM indicate a reduced susceptibility to catastrophic forgetting, further bolstering its utility in our multimodal approach. Our contribution in this work can be noted as:\n\u2022 Implement a unified tri-modal framework that is capable of diverse downstream task and perform at a high accuracy for all modality.\n\u2022 Show the generalization capability of the pre-trained model through Zero-shot Classification\n\u2022 Provide multimodal explainability with Gradient-based Attention Visualization and multimodal retrieval."}, {"title": "2. Related Work", "content": "In this section we give background on our work, and introduce some of the recent works."}, {"title": "2.1. Vision Transformers", "content": "The Vision Transformer (ViT) has recently set new benchmarks in several key areas, achieving state-of-the-art results in image classification on ImageNet, object detection on COCO, segmentation, and other tasks. Traditionally, convolutional neural networks (CNNs) have been the go-to for such tasks because of their ability to handle increasingly complex patterns through larger kernels or receptive fields. The introduction of residual connections has allowed CNNs to grow significantly deeper, enhancing their ability to capture more complex information. Vision Transformers operate on a global scale using self-attention mechanisms. Although lacking the inductive biases of CNNs, given sufficient data, ViTs can learn intricate relationships within the data, offering a comprehensive understanding of the input."}, {"title": "2.2. Self-Supervised Training", "content": "Self-supervised pretraining, has naturally gained popularity as the volume of available data increases while annotations or ground truths remain scarce. Recent advancements have shown that self-supervised learning not only reduces the dependency on labeled data but also improves the generalizability of models."}, {"title": "2.3. Multi-Modal Pre-training", "content": "Multimodal pretraining, establishes a unified framework that significantly enhances model generalization across various downstream tasks, such as classification, visual question answering (VQA), and segmentation. By capturing the fundamental features across different modalities during the pretraining phase, these models can, in some cases, outperform fully supervised models in downstream tasks. Moreover, the multimodal framework offers the flexibility to leverage all available modalities or select individual modalities for specific tasks, enhancing adaptability and application potential across a broader range of scenarios."}, {"title": "3. Method", "content": "In this section, we introduce our architecture, explain the different components, and the objective task."}, {"title": "3.1. Architecture", "content": "In our method, we utilize the Vision Transformer (ViT) as the backbone encoder for both the X-ray and ECG modalities. To enhance training stability and accelerate convergence, we initialize the ViT with pretrained ImageNet weights. The overall architecture is illustrated in Figure 1."}, {"title": "3.2. Modality Encoder", "content": "We implement a custom patch embedding to encode the ECG modality shown in Appendix A fig 8.\nWe adopt DropKey strategy in the ViT self-attention shown in Figure 8. Instead of dropping the attentin weights, we randomly mask the key with a linear rate over the layers. This allows for a smoother attention plot and better robustness. For textual modality, we use an extended version of Clinical Bert, which is a Roberta base pretrained on Pubmed and Mimic reports. LLMs can be costly to train, specially in multimodal setting, so we fine-tune the LLM with LORA PEFT strategy. Additionally, we concatenate the diagnostic reports from both modalities of the same patient into a input, utilizing the existing separator token in the LLM's tokenizer. This enables the LLM to encode text from both modalities into a unified input. We provide additional details on our encoders in Appendix B"}, {"title": "3.3. Contrastive Loss", "content": "The pretraining framework's objective centers on using contrastive loss to effectively manage relationships between modalities. Following established contrastive learning practices, we project the outputs from the modality-specific encoders into a low-dimensional shared space. Specifically, we employ the InfoNCE loss, which aims to minimize the distance between features of the textual modality and those of the X-ray modality from the same patient, and similarly between the textual modality and the ECG modality. Conversely, it maximizes the distance between non-corresponding/negative, pairs. By concatenating texts from both modalities with a separator token during tokenization, the model learns to discern which portion of text corresponds to which modality. We assess this capability through retrieval tasks, confirming the model's effectiveness in distinguishing and correctly associating the textual inputs with their respective modalities."}, {"title": "4. Evaluation", "content": "In this section, we introduce our research questions, experimental setup, and the results."}, {"title": "4.1. Experimental design", "content": "Our experimental framework is meticulously structured to address each research question (RQ), ensuring a comprehensive evaluation of our model:\n\u2022 RQ1: Is MoRE able to effectively learn the representation for ECG and X-Ray?\nWe assess the zero-shot classification capabilities of our model for both X-ray and ECG modalities. Additionally, we present t-SNE plots to visualize the features of our model compared to baseline models, providing a clear graphical representation of its performance.\n\u2022 RQ2: Can MoRE be fine-tuned to perform downstream classification tasks accurately?\nOur pre-trained model is fine-tuned on downstream datasets. The results of this fine-tuning are presented in a tabular format, allowing for direct comparison of performance metrics across different datasets.\n\u2022 RQ3: Is Multimodal of X-ray and ECG applicable in medical domain ?\nWe evaluate the retrieval performance of our model by comparing it with baseline models. This comparison helps illustrate the effectiveness of our model in retrieving relevant medical images and data based on query inputs.\n\u2022 RQ4: How does MORE compare to single model pre-training?\nWe compare our multimodal approach against single modality models to demonstrate the benefits of integrating multiple types of data. This comparison aims to highlight the enhanced performance and utility that multimodality brings to medical image analysis."}, {"title": "4.2. Experimental setup", "content": ""}, {"title": "4.2.1. DATASETS", "content": "For pretraining, we use the matched subset of Mimic CXR and Mimic ECG dataset. We find about 45k matching patients, with combination of about 800k X-Ray and ECG data. We provide further details on the matching of the pretraining dataset in Appendix B. For downstream tasks, we test our model on the CheXpert dataset which contains 192k frontal X-Ray images, Edema Severity which has 7k data with severity level as classification, PtbXl dataset which has 21k ECG data. For X-Ray images we test our model on the labels: Atelectasis (AT), Cardiomegaly (CM), Edema (ED), and Pleural Effusion (PE). For ECG images we test on superclass labels: Normal (NORM), Hypertrophy (HYP), ST/ T Changes (STTC), and Myocardial Infarction (MI). We use these labels as our baselines have worked on the same. For Zero-shot we use CheXpert 5x200, and Mimic Zero-Shot subsets created from their original datasets. We provide additional details on our datasets in Appendix C"}, {"title": "4.3. Implementation", "content": "Our model employs the 'ViT-Base-patch16-224' pretrained on ImageNet, utilizing the Timm library for its adaptability. The base encoder features 12 transformer layers and 12 multi-head self-attention heads. We apply a custom patch embedding for ECG modalities and handle diagnostic reports with Clinical-Bert's tokenizer. The model uses a projection layer with two linear layers and leverages the InfoNCE loss, configured with a learnable temperature parameter. Training is optimized with the AdamW optimizer and Automatic Mixed Precision, using gradient accumulation to manage large batch processing. Detailed implementation specifics, including parameter settings and architecture modifications, are provided in the Appendix E"}, {"title": "4.4. Baselines", "content": "We compare our Multi-Modal Contrastive Pre-training Framework (MORE) with several state-of-the-art multimodal pretraining frameworks in the medical domain, including GLORIA, MedKlip, ECG AdvMasking and FrozenSSL. For a fair comparison, we pretrain GLORIA on the Mimic-IV Dataset, aligning with the dataset used for our own pretraining and that of MedKLIP. Similarly, we pretrain ECG AdvMask and FrozenSSL on the Mimic-IV ECG 800k dataset. Notably, since FrozenSSL does not specify a normalization process for ECG, we adopt the same normalization approach used in our work. ECG AdvMask being a single modality framework employs an autoencoder to generate masks for ECG during pretraining, we follow their outlined process."}, {"title": "4.5. Metrics", "content": "In the medical domain, relying solely on accuracy to evaluate model performance can be misleading. Accuracy typically involves a fixed threshold (often 0.5) for classification, which does not account for the uneven distribution of classes, the varying difficulty of diagnosing certain conditions, or the prevalence of different conditions within the dataset. Instead, we utilize the Area Under the Receiver Operating Characteristic (AUROC) as our primary metric. AUROC measures the model's ability to discriminate between classes at various threshold settings, providing a more comprehensive assessment of performance across different clinical scenarios. For evaluating the retrieval experiment, we use Precision@K metric and evaluate the text retrieved as correct if it falls in the same class label as the original text."}, {"title": "4.6. Results and Discussion", "content": ""}, {"title": "4.6.1. RQ1: ZERO-SHOT CLASSIFICATION", "content": "The zero-shot process is described in Appendix F\nDiscussion: MoRE consistently outperforms GLORIA and MedKlip in zero-shot classification for all labels except Edema. Notably, GLORIA demonstrates superior performance specifically for Edema, as detailed in Table 1. The lower number of data points for Edema in the dataset may be a contributing factor to this performance gap. MORE outperforms FrozenSSL in all but one label by a small margin."}, {"title": "4.6.2. RQ2: FINE-TUNING ON DOWNSTREAM DATASET", "content": "We perform fine-tuning on our downstream datasets and report the results in tables 3, 4, and 5"}, {"title": "4.6.3. RQ3: RETRIEVAL", "content": "We conduct retrieval tasks to further validate the representations learned by our framework and demonstrate its application in medical learning. We utilize the CheXpert 5x200 dataset and a subset of the Mimic-IV dataset, where each X-ray is uniquely labeled with a diagnosis. This approach not only tests the effectiveness of the learned representations but also showcases how the framework can be applied in practical medical settings.\nDiscussion: We were unable to test for MedKLIP for this task because their approach does not involve training their LLM model; instead, they use it solely to encode the triplet extracted from the medical notes. We find our model performs better in retrieval for all K."}, {"title": "Text-to-Xray Retrieval", "content": "We demonstrate that, using a query text, our system can successfully retrieve X-rays associated with that query. The retrieval results display a variety of X-ray images that are relevant to the text. We believe this capability makes our tool highly valuable for educational purposes, helping users understand how a particular medical condition can appear in different patients, its various levels of severity, and the presence of comorbid conditions. This functionality could enhance learning and diagnostic training in medical education.\nDiscussion: In the retrieved images, the left X-ray images show a visibly enlarged heart, indicative of severe cardiac conditions. On the right, the X-ray images display fluid accumulation in both lung areas and at the base of the lungs, signaling the presence of edema and pleural effusion, as illustrated in Fig. 3, 4. This visual comparison highlights the tool's ability to effectively differentiate and display specific medical conditions critical for diagnostic purposes."}, {"title": "MultiModal Retrieval", "content": "We further demonstrate the capability of MORE to retrieve both X-ray and ECG data using a common text query in fig 5, showing that the text modality effectively binds the X-ray and ECG modalities together. This integration highlights MORE's ability to synthesize information across different types of medical data, offering a cohesive view that can be crucial for comprehensive diagnostic assessments.\nDiscussion: For the given query, the retrieved X-rays specifically mention Cardiomegaly in their original descriptions. Correspondingly, the retrieved ECGs show abnormalities such as \"Arrhythmia, Bradycardia, Premature Ventricular Contractions(PVC)\"-irregularities in heartbeat, slow heartbeat, and skipped or extra heartbeats, respectively, which are indicative of heart abnormalities. These results illustrate MORE's capability to align relevant diagnostic findings across modalities, enhancing the comprehensiveness of medical interpretations."}, {"title": "Gradient Based LRP Attention Visualization:", "content": "We employ a modified version of TransLRP, which utilizes Layerwise Relevance Propagation (LRP) to aggregate backward gradient flow for deriving explanations. Our modification allows TransLRP to accept multimodal inputs, specifically X-ray and ECG, enabling us to visualize attention maps through the backward gradient flow. We present examples from the test set of the Mimic dataset unseen by the model, illustrating the attention focused on different diagnostic classes. This visualization helps clarify how the model prioritizes different aspects of the input data in making diagnostic decisions. We visualize the rollout attention of the transformer blocks based on the backward flow of the gradient from the classification head, which allows us to visualize the prominent attention for each class. In fig 6, we are able to plot the attention on multimodal input of X-Ray and ECG when we use both modality for inference. For the given example, the condition is Cardiomegaly. We also show attention plot for X-Ray image with multi-class label in fig 7.\nDiscussion: In our analysis of the X-rays, we observe that the model directs high attention to areas of concern, specifically, the heart region for cardiomegaly and the base of the lungs for pleural effusion. For the ECGs, while the attention distribution is more complex to decipher, it is noticeably concentrated around the QRS complex, P Wave, and T Wave. These areas are crucial for identifying irregular heartbeats, which are prominent indicators of cardiac issues. This focused attention could serve as a valuable tool for clinical experts to validate the model's accuracy and relevance in real-world diagnostics, as demonstrated in Fig. 6 and 7."}, {"title": "4.6.4. RQ4 RESULTS: MULTIMODAL PRETRAINING AGAINST SINGLE MODALITIES", "content": "We compare our model to single modality pre-training frameworks in Table 7 We employ our base encoder, the ViT model, pre-trained on Mimic-IV data using contrastive learning with augmented data, similar to the approach used in SimCLR with ImageNet initialization. We also benchmark against a fully supervised base ViT model to provide a comprehensive performance comparison. These models are evaluated using Mimic-IV data. For ECG, our baseline ECG AdvMask is a single modality framework. Previous evaluations, as shown in Table 5, detail these comparisons, indicating how our multimodal approach stands against single modality training.\nDiscussion: We observe that MORE outperforms single-modality methods, such as its standalone ViT encoder when fully supervised and the ViT encoder pre-trained with the SimCLR framework. This highlights the superior fine-tuning capability of our pre-trained framework."}, {"title": "5. Ablation Study", "content": "We study the use of incorporating all available modality for inference. Since Mimic IV dataset has a matched subset of X-Ray and ECG data, we perform inference with just X-Ray and X-Ray plus ECG. For the mulitmodal input, we ensure the study date of the X-Ray and ECG are no more than 3 days apart.\nDiscussion Our results indicate that the AUROC scores are higher when utilizing only the X-ray modality. However, the AURPC scores improve when both the X-ray and ECG modalities are combined, suggesting that the model may be more robust to false negatives when incorporating additional information from the ECG data. Further investigation is needed to explore whether leveraging all available modalities can enhance diagnostic accuracy and improve overall model performance."}, {"title": "Limitations", "content": "Our research effectively integrates multimodal data sources like X-rays, ECGs, and Diagnostic report but faces several limitations that affect its broader usability and effectiveness. Although the model shows strong performance on specific datasets such as Mimic-IV and CheXpert which, its generalizability to other datasets needs to be further tested. While LORA PEFT strategy significantly reduces trainable parameters, we still cannot use larger available language models in a multimodal setting. Consequently, we were unable to utilize large language models (LLMs) like MEDITRON-7B and MEDITRON-70B due to GPU memory constraints."}, {"title": "6. Conclusion", "content": "This study successfully demonstrates the potential of our Multi-Modal Contrastive Pre-training Framework (MORE) in enhancing diagnostic accuracy by integrating X-rays, ECGs, and clinical notes. Utilizing state-of-the-art transformer architectures and contrastive loss techniques, our model has shown superior performance on various benchmarks, establishing a new standard for multimodal learning in healthcare. Looking ahead, our future work will focus on overcoming the current limitations by expanding our evaluations to additional baseline datasets. We also plan to explore the use of large language models (LLMs) that could further extend the applicability and accessibility of our approach in different healthcare contexts."}, {"title": "Appendix A. DropKey and Custom Patch Embedding", "content": "We use a custom patch embedding with 2 convolution layers each with ReLU activation and Batch Normalization to encode the ECG data before feeding it to the transformer model. We adopt DropKey in our transformer model which uses a linear drop rate to mask the Key of the transformer layers. The lower layers have a higher mask rate which linearly decreases to have the least masking rate at the last level. This is done to preserve the high-level information in the final layers."}, {"title": "Appendix B. Pretraining Data Detail", "content": "For Pre-training, we use two Mimic-IV dataset Mimic-CXR v2 and Mimic-IV ECG. We use the permutation of the matched subset of Xray and ECG data from Mimic-IV and Mimic-CXR dataset. The total data we use for pre-training is about 800k where each data point is an Xray, ECG data, and Clinical Note from the same patient taken within 60 days. We ensure we only pretrain on 'train' fold of the dataset and leave out the validation and test set for downstream evaluation.\nX-Ray Dataset:\nThe Mimic-CXR-JPG dataset v2.0.0 was released in Physionet. The dataset includes JPG images of chest Xray along with associated label and diagnostic text. The dataset has 227,827 Chest Xray images. The major reason for choosing this dataset is having matching ECG data of the patients in Mimic-IV ECG dataset for the purposes of multimodal pretraining.\nECG Dataset:\nThe Mimic-IV ECG Matched Subset dataset also released in Physionet. This dataset is derived from the larger Mimic-IV Clinical dataset which is also the parent of the Mimic-CXR dataset. The dataset includes 800,000 ECG from 160,000 patients. The ECG dataset contains no labels but includes clinical text.\nMatched Subset:\nAn individual Xray and ECG data item can be identified by some id's. To get the matched subset of data we identify an Xray and ECG through the patient's Sujbect ID, and Study ID. The subject id is a identifier of a patient, we find about little above 45k patients that have both Xray and ECG data present in the matched subset. There are multiple studies of the same patient taken in multiple dates, so we find a total of little more than 300k data points (Xray + ECG) creating this matched dataset of Xray, and ECG. We also add the clinical texts of the Xray and ECG that are available. For Xray data that did not have clinical notes associated with it, we create its note through its diagnosis label, e.g: Finding of {diagnosis}, Uncertain Finding of {diagnosis} \"We follow this format because the clinical notes are in format of \"Impression:\" and \"Finding:\". We add \"uncertain finding\u201d for the Xray data points that have \"-1\" in their label diagnosis, which is reported as uncertain finding in the dataset.\nClinical Notes:\nFor the clinical notes of Xray, we use text under the headings in format of \"Impression:\", and \"Findings:\". We filter out the text from these headings and remove any special characters and redundant spaces from the text. For ECG, there are multiple reports for a ECG data, we merge the first 7 reports as we find them to be available for most ECGs and contain the most information. We then remove any special characters and redundant spaces from the text.\nDuring tokenization of the clinical notes, we tokenize the Xray and ECG note together with a separator token existing in the tokenizer of the LLM. During matching Xray and ECG data of the same patient we carefully follow the following steps:\n1. Find all permutations of Xray and ECG studies of same patient\n2. Filter the data points if the de-identified dates of Xray and ECG are within 60days of each other.\n3. Create Note of Xray data that do not have clinical note with its associated diagnosis. i.e. \" Finding of {diagnosis}\"\""}, {"title": "Appendix C. Datasets", "content": "Pre-training Data For pretraining, we use matched subset from Mimic-CXR v2 and Mimic-IV ECG. Mimic-CXR v2 has about 224k frontal chest X-ray images and Radiology Report, and Mimic-IV ECG has about 800k ECG signals with short Cardiology Report. There are about 45k matched subset of X-ray and ECG from the same patients.\nDatasets for Downstream Tasks\nMimic-CXR We use the Mimic-CXR v2 dataset for evaluating the pretrained representations on the test fold that is suggested by the dataset. The dataset has 13 labels but we choose to work on CM: Cardiomegaly, AT: Atelectasis, ED: Edema, and PF: Pleural Effusion. We choose these labels as our baselines have also worked on them. Each X-ray data can have multiple diagnosis, making it a multi-label classification task.\nCheXpert We use the CheXpert dataset that has 192k X-ray data from 65k patients. We only utilize the frontal X-ray views and randomly sample 5% of the training data for validation, and use the validation set of 202 X-ray images for testing. This dataset does not come from the Mimic-IV parent corpus so this will be an outside dataset for our evaluation. We measure the AUROC which is presented in Table 3\nCheXpert 5x200 CheXpert 5x200 is taken from the CheXpert dataset such that each data point has only one unique diagnosis label. We utilize this dataset for Zero-Shot classification, precision@k, and retrieval tasks to prove the representation learned from our framework.\nMimic-Zero-Shot Mimic-Zero-Shot is taken from the Mimic dataset such that each data point has only one unique diagnosis label. We utilize this dataset for Zero-Shot classification, precision@k, and retrieval tasks.\nEdema Severity This dataset comes from the Mimic-CXR dataset. It contains about 7k data of which 6.6k are of training data, 520 are validation, and 140 additional for test. This split is as given in the dataset. The validation and test set are validated by multiple domain experts. The severity goes from 0, 1,2, and 3. 0, none; 1, vascular congestion; 2, interstitial edema; and 3, alveolar edema\nPtbXl ECG This dataset that has 21k ECG data from 18k patients. Each ECG is associated with a diagnositc superclass label, namely: NORM : Normal ECG, HYP: Hypertrophy, STTC: ST/T changes, MI: Myocardial Infarction"}, {"title": "Appendix D. Data Pre-processing and Transformations", "content": "Pre-processing is vital to any deep learning model training as it can significantly impact the training times, convergence, and outcome. Especially for medical data, it is key to ensure no important features are lost and conversely, highlight the important features.\nXray:\nFor pre-processing we use Adaptive Histogram Equalization to bring out the contrast and separation in the features. We find this step particularly important since the Xray images are greyscale and sometimes the quality of Xray varies introducing noise in the image and blending the features. After histogram equalization, the pixels are defined sharper and bring out features visibly. We then find the mean and standard deviation of the training dataset and use it for Normalization after transformations to improve training stability and performance. To bring some variability in the data, we follow work of and use RandomResizedScaling with less stronger scaling of 0.6-0.9 with 0.8 probability, RandomColorJitter with brightness and contrast values of upto 0.4 with 0.8 probability, and RandomGaussian Blur with kernel size of 7, 23 and with 0.5 probability.\nECG:\nWe follow a few key pre-processing steps for ECG.\n1.Re-Sampling: We resample the ECG data from 500Hz to 100Hz, changing its channel dimension from 5000 to 1000, making it a matrix of 12,1000. 2.Re-move Nan: We find there are NA values present in the data so we swap any NA values with 0. 3.Baseline Wander: Baseline Wander is one of the pre-processing steps that absolutely cannot be missed. Baseline Wander is a low-frequency noise that is caused by the movement of the ECG leads. If not addressed can cause the model to deviate from the understanding of the data. Finally, we do 4.Per Lead Normalization with MinMax Scaling to bring the range of each lead to -1 and 1. For Transformation, we follow work of and utilize two augmentations for variability, Time Warping: warp 4 segments of ECG by factor of 0.25, and Random Permutation: permute 4 segments of ECG in random order."}, {"title": "Appendix E. Implementation Detail", "content": "Our implementation utilizes the 'ViT-Base-patch16-224' model, pre-trained on ImageNet from the Timm library chosen for its ease of customization compared to the Hugging Face implementation. Our ViT base encoder consists of 12 transformer layers and 12 multi-head self-attention heads. We do not include any bias for query, key, value (qkv) during the linear projection of patches due to batch normalization after each layer. For the ECG modality, we employ a custom patch embedding strategy using two 1D-convolution layers, each followed by batch normalization and ReLU activation. The remainder of the ViT architecture is unchanged. The diagnostic reports are processed using ClinicalBert's tokenizer with the parameter 'add special tokens' = True to insert separator tokens between texts from the two modalities. We set 'max length' = 512 and 'truncation' = True. Our projection layer comprises two linear layers with a hidden dimension of 768 and an output dimension of 128. The first linear layer is bias-free, followed by batch normalization and ReLU activation, while the second layer includes a bias. We configure the temperature parameter of the InfoNCE loss at 0.1, making it learnable to optimize performance. The model is trained over 50 epochs with an early stopping criterion set at a patience of 10. We use the AdamW optimizer with a weight decay of 0.1 for pretraining and 0.02 for fine-tuning. We fine-tune only the projector layer and classifier head for linear evaluation and fine-tune the 'qkv' weight of the last few transformer self attention layers for downstream tasks. Training leverages Automatic Mixed Precision (AMP) in PyTorch for enhanced speed and efficiency with minimal accuracy loss. Instead of using MoCo for larger batch sizes, we implement gradient accumulation over 4 steps, which has been shown to be effective in contrastive learning by simulating larger batch sizes. The initial batch size is set at 100, and the model is trained on a single A100-SXM4-80GB GPU."}, {"title": "Appendix F. Zero-Shot Processs", "content": "The Zero-Shot Classfication process is described below in the figure."}, {"title": "LInfoNCE", "content": "LInfoNCE = log exp(sim(zi, zj)/\u03c4) \u2211Nk=1 exp(sim(zi, zk)/\u03c4) (1)"}, {"title": "L", "content": "L = 1 2 (LText-Xray + LText-ECG) (2)"}, {"title": "LText-Xray", "content": "LText-Xray = 1 2 (log exp(sim(zText, zXray)/\u03c4) \u2211exp(sim(zText, zk)/\u03c4) + log exp(sim(zXray, zText)/\u03c4) \u2211exp(sim(zXray, zk)/\u03c4)) (3)"}, {"title": "LText-Ecg", "content": "LText-Ecg = 1 2 (log exp(sim(zText, zEcg)/\u03c4) \u2211exp(sim(zText, zk)/\u03c4) +log exp(sim(zEcg, zText)/\u03c4) \u2211exp(sim(zEcg, zk)/\u03c4)) (4)"}]}