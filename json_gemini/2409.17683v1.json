{"title": "ZERO- AND FEW-SHOT NAMED ENTITY RECOGNITION AND TEXT EXPANSION IN MEDICATION PRESCRIPTIONS USING CHATGPT", "authors": ["Natthanaphop Isaradech", "Andrea Riedel", "Wachiranun Sirikul", "Markus Kreuzthaler", "Stefan Schulz"], "abstract": "Introduction: Medication prescriptions are often in free text and include a mix of two languages, local brand names, and a wide range of idiosyncratic formats and abbreviations. Large language models (LLMs) have shown promising ability to generate text in response to input prompts. We use ChatGPT 3.5 to automatically structure and expand medication statements in discharge summaries and thus make them easier to interpret for people and machines. Methods: Named-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and few-shot setting with different prompt strategies. 100 medication statements were manually annotated and curated. NER performance was measured by using strict and partial matching. For the task EX, two experts interpreted the results by assessing semantic equivalence between original and expanded statements. The model performance was measured by precision, recall, and F1 score. Results: For NER, the best-performing prompt reached an average F1 score of 0.94 in the test set. For EX, the few-shot prompt showed superior performance among other prompts, with an average F1 score of 0.87. Conclusion: Our study demonstrates good performance for NER and EX tasks in free-text medication statements using ChatGPT. Compared to a zero-shot baseline, a few-shot approach prevented the system from hallucinating, which would be unacceptable when processing safety-relevant medication data.", "sections": [{"title": "1 Introduction", "content": "Prescribing drugs has a high impact on patient safety. In many places, handwritten prescriptions are still common, and only when a discharge summary is written, medication information is registered in electronic health records (EHRs). Here, physicians tend to use compact language and particularly abbreviations. They mix up brands with ingredient names and skip units (\"ASA 100\") and dose form information (\u201ctablet\u201d, \u201csuspension\u201d, \u201ceyedrops\"). For route of administration, medication frequency, and time patterns, a broad range of different styles is used. Abbreviated Latin terms such as \"tds\" (three times a day) and \"qPM\" (once in the evening) are common in some jurisdictions, but completely unknown in others where \u201c1-1-1\u201d corresponds to \u201ctds\", and \"0-0-1\u201d to \u201cqPM\u201d. Wherever drug prescriptions are done in narrative form, their interpretation - particularly beyond their narrow context of use - remains a challenge. Ideally, a cryptic \"ASA 100 qPM\" should be automatically transformed into a clear \"Acetylsalicylic acid 100 milligrams oral tablet, to be taken orally every afternoon\".\nEnglish is the lingua franca in scholarly communications, but EHRs mostly use the official languages of the respective jurisdiction. Yet there are exceptions, such as in Arabic or Asian countries, which have not developed sophisticated medical terminology in their languages to an extent that would suffice for clinical documentation and therefore use English. International healthcare teams, especially in the Middle East, communicate in English. English is the interlingua of multilingual countries such as India and Malaysia. It is therefore unsurprising that many flavors of non-standard medical English have developed, which present challenges both for native and second-language speakers. Beyond affecting communication, we expect medical content in \"Asian Englishes\" to pose also difficulties for machine processing of medical language, because tools and resources have been trained with mainstream English EHR content and with publications polished and standardized by editors. In a similar vein, controlled vocabularies such as ICD-10 or SNOMED CT do not account for non-mainstream English term variants.\nThis paper focuses on healthcare documentation in Thailand, which has undertaken a concerted effort to stimulate computerized physician order entry (CPOE) to achieve high-quality prescriptions [1] Nevertheless, most hospitals still depend on written prescriptions, which subsequently require manual input into EHRs [2, 3]. Narrative medication statements are still used for drug reconciliation and communication [4]. Such expressions in \u201cThai English\u201d blend two languages and character sets, and use a variety of styles and formats depending on the physician's preferences, such as the merging of trade names and ingredients [2, 5], the use of abbreviated names and routes6, and the omission of dosage forms [6]. E.g., in \u201cThyrosit (50) 0.5x1 o ac \u0e08-\u0e28\u201d, \u201cThyrosit\u201d is a trade name for levothyroxine, and \u201c(50)\" indicates the strength. The unit (here micrograms) is omitted as there is no levothyroxine preparation at the milligram level. \"0.5x1\" indicates the action of consuming half (\u201c0.5\u201d) of the dosage once (\u201cx1\u201d) each day. Finally, \u201co ac \u0e08-\u0e28\u201d, comprises a mixture of Thai, English, and Latin abbreviations denoting routes and timings. \u201co\u201d represents \u201cper os (po)\" (by mouth) and \u201cac\u201d means \u201cante cibum\" (before meals). \u0e08-\u0e28 is a Thai abbreviation for \u201cMonday to Friday\". It is foreseeable that considerable problems arise whenever we aim to automatically extract information from such free-text instructions, although over the last decade machine learning-based approaches have gained popularity in clinical information extraction thanks to the ever-improving performance of human language technologies [7]. Large-language models (LLMs) have attracted immense interest after the release of ChatGPT in 2022 [8, 9, 10]. Their capacity for handling and analyzing large-scale text and generating content in response to input prompts makes them highly promising for a wide range of applications, from clinical name entity recognition (NER) [11, 12] to encoding clinical knowledge [13]. Moreover, LLMs have vastly benefited from transfer learning where their pre-existing knowledge is leveraged and fine-tuned with minimal labeled data or instructions, making them highly adaptable. This contributed to the popularity of zero to few-shot learning paradigms: for each new class, one-shot learning uses just one labeled example, few-shot learning uses a limited set thereof, whereas in zero-shot learning no labeled data are provided at all [14]. The capabilities of LLMs can be customized, improved, or refined by a set of instructions called a prompt [15]. To effectively communicate with LLMs, prompt engineering strategies have turned out to be fundamental [16].\nAgainst this background, we focus on the idiosyncratic and compact nature of medication statements in Thai medical records and formulate our research questions as follows:\n\u2022 To which extent is ChatGPT 3.5 able to restructure and normalize medication statements?\n\u2022 Does this depend on prompting patterns?\n\u2022 What are the weaknesses of this approach regarding patient safety?\""}, {"title": "2 Methods", "content": "2.1 Overview\nOur study consists of a named entity recognition (NER) and a text expansion (EX) phase, as shown in Figure 1. The initial step involves the human annotation of medication statements with the entity types Medication, Strength, Unit, Mode, and Instructions, as well as with manually expanded text, done by a domain expert. Prompts are then optimized by utilizing the NER-annotated training dataset (see below). Following this, prompt validation and selection occur on a separate dataset. The selected prompt is then evaluated on a designated test dataset. In the same way, leveraging the outcomes derived from NER prompts, we design and optimize another set of prompts for the task EX, using EX-annotated training data. Finally, prompts find the best-performing one for EX and assess its performance on the test dataset.\n2.2 Dataset\nNineteen discharge summaries were randomly selected from 6000 ones from the internal medicine department at Maharaj Nakorn Chiang Mai Hospital (2018\u20132022), authored by internists, residents, or 6th-year medical students. 100 medication prescriptions were manually extracted, annotated by a physician, and curated by another physician. Data was then randomly divided into a training set, a validation set, and a test set with a 25:25:50 split as shown in Table 2.2.\n2.3 NER Annotation\nThe annotations were divided into two separate sections, one for NER annotation and one for EX annotation. For NER, the following five entity types were distinguished:"}, {"title": "2.4 EX Annotation", "content": "For the EX annotation, the expansion types of the text were classified into eight categories. The latter five of them, viz. Quantity of Dose Form, Dose Form, Relation to Meal, Frequency, and Others extend the Instructions entity type:\nActive ingredients. Substituting the text in the Medication entity type by the corresponding active ingredient(s). E.g., converting \"paracet\" to \"paracetamol\u201d or \u201cThyrosit\u201d to \u201clevothyroxine\u201d.\nUnit. Expanding the short form Unit to a long-form, e.g. \u201cmilligram\u201d for \u201cm.g.\u201d and \u201cmg\u201d.\nMode. Expanding the Mode entity type into a non-abbreviated description of the route of administration, such as \u201cpo\u201d to \"oral\".\nInstructions Dose Form. Describing the form of drug dose, such as tablet, inhaler, or droplet.\nInstructions Quantity of Dose Form. Expanding information related to the quantity associated with the Dose Form expansion type, e.g., in \"Take Simvastatin 40 mg 0.5 tablet once daily\", \"0.5\u201d indicates Quantity of Dose Form, i.e., without using abbreviations or ambiguous terms.\nInstructions Relation to Meal. Describing the relation of the medication intake related to meals from the Instructions entity type, including \"before meals\u201d, \u201cafter meals\u201d, and \u201cbefore bed\".\nInstructions Frequency. Expanding details regarding how often the medication should be taken, e.g. \u201conce daily", "once weekly": "or", "daily": "nInstructions Other. Capturing any additional information in Instructions that do not fit into the EX categories, such as the duration of the prescription, or conditional instructions such as \u201ctake when experiencing palpitations\"."}, {"title": "2.5 Prompt engineering", "content": "Our approach to zero-shot learning prompt design was based on combinations of prompt patterns, as recently suggested [16]. We first created prompts using the training dataset and assessed the ChatGPT outcomes by applying these prompts to the validation set. Then we refined the prompt combinations to enhance the outcome performance, guided by the results from the validation set. Once we achieved satisfactory results, we selected the most effective prompt based on the validation set and evaluated it on the test dataset. For the NER task, we selected the following patterns for our prompt design:\nPersona. Assigning a role provides context and direction, enabling the LLM to adopt a specific identity and tailor its responses accordingly.\nTemplate. Assigning the LLM a precise template to adjust its output to a specific format.\nFew-shot. Assigning the LLM prescription examples from the annotated training dataset for the LLM to understand what the output looks like. In our case, we simply give the example of annotated NER in tabular format.\nThe prompts were structured based on combinations of patterns."}, {"title": "2.5.1 NER Prompt Patterns", "content": "For the Named Entity Recognition (NER) task, six different prompt combinations are considered:\nPrompt A. This prompt combines Persona and Template patterns."}, {"title": "2.5.2 EX Prompt Patterns", "content": "For the EX task, three different prompt combinations are considered:\nPrompt 1. This is a plain prompt command that does not follow any specific prompt pattern.\nPrompt 2. This prompt combines Persona and Template patterns.\nPrompt 3. This prompt combines Persona and Template patterns along with five examples of correct NER recognition.\nThe details of the prompts are in Appendices A and B."}, {"title": "2.6 Evaluation", "content": "Precision (P), recall (R), and averaged F1 scores were used to evaluate the model performance. For NER, scores were computed using strict and partial matching criteria. Strict matching required identical spans and a correct entity type. Partial matching only required overlapping of spans and correct entity types. The evaluation of the EX task was based on inspection by two domain experts. They carefully reviewed each result and assessed where semantic equivalence could be asserted, according to their interpretation of the intended meaning of the prescription statement in the context of their expertise."}, {"title": "3 Results", "content": "3.1 Zero- and few-shot named-entity recognition\nThe NER results are shown in Table 2 and 3. Prompts A and B showed good performance with an average F1 of 0.72 and 0.74, respectively, based on partial matching. Prompts C, D, E, and F demonstrated better overall performance than A or B. However, even though prompt C generally had a better overall F1 than prompt A, it exhibited the poorest Unit detection compared to all prompts, even with the same prompt pattern and more examples. Prompt D showed a similar issue, as it failed to recognize Instructions effectively. Prompts E and F showed superior performance compared to the others, with an average F1 of 0.94 and 0.90, respectively (partial matching). Both prompts revealed promising for medical NER tasks. Nevertheless, we decided to prioritize prompt E, as it had the highest average F1 and better NER performance on Unit in partial matching."}, {"title": "3.2 Zero- and few-shot text expansion and manual assessment", "content": "After obtaining metrics from NER prompts, we passed the ChatGPT output of the best-performing prompt E to the EX phase of our experiment. The NER output was converted to a tabular format to put the text into the ChatGPT chatbox. The EX performance results are shown in Table 5. In the validation dataset, Prompt 3 had the best overall performance on term expansion in all categories, excluding Unit, with an average F1 score of 0.87. Prompts 2 and 3 were generally better as they were given more examples of medication prescriptions. Prompt 1, being the simplest EX prompt, was not capable of expanding any terms in the Mode class at all. Prompt 3 was then selected for test dataset evaluation, and the results showed an average F1 of 0.77, which decreased by 0.10 compared to the validation set because the metrics for Unit and [Instructions] Relation to Meal dropped largely compared to the validation dataset."}, {"title": "4 Discussion", "content": "4.1 Comparison to previous work\nEarly attempts to use ChatGPT for clinical NLP were made by Hu et al., but its performance lagged behind that of the supervised BioClinicalBERT model, regarding the extraction of treatments mentioned in discharge summaries [17]. However, they did not distinguish drug treatments from other medical treatments. Therefore, it seems that our work is the first to use ChatGPT for specific and detailed medication statement analysis.\nComparing our NER results (mean F1 score 0.92) to an earlier study (mean F1 score 0.80) using CRF shows a clear superiority [18]. In that study, medication details were improperly delineated whenever punctuation patterns such as brackets did not follow the norm [19]. However, problems like this can mostly be seen as resolved by deep learning. So did the unusual bracketing of strength (e.g., \"ASA (500)\") not constitute any obstacle.\nMore interesting is the comparison with MT-NER-PMB (2021), which can be seen as a recent non-LLM baseline, which outperformed other BERT models. Comparing our results to their F1 score for partial results we could show slightly better detections for Medication (0.99 vs 0.96), Mode (1.00 vs. 0.95), and Strength (0.99 vs. 0.98). However, our study separates Strength and Unit, therefore the relatively low F1 score for Unit 0.51 is not integrated in the calculation for Strength [20].\nThat our medication statements often did not contain units of measurement (in the gold standard annotation constituting so-called zero-with annotations, cf. Fig. 2) led to a low precision in Prompt 1 because it repeated the NER task by adding the found abbreviation in the text. The results suggest that the addition of prompt patterns in prompts 2 and 3 solved this.\nCompared to traditional machine learning, it was astonishing to observe how few training examples produced a considerable benefit. This highlights the importance of good prompt engineering to optimally exploit the content of an LLM. However, we cannot explain the poor result for Unit in NER, whereas the even worse performance of Unit in EX (such as from \"mg\" to \"Milligram\") may result from the fact that unit symbols are hardly ever expanded, even in scholarly or popular publications or drug leaflets.\""}, {"title": "4.2 Error Analysis", "content": "An important result of our study was that incremental LLM prompting with repetitive human assessment improved the result and the NER task achieved an F1 of 0.79 and 0.92 on the test set for strict and partial matching, respectively, with the recognition of units of measurement (which were often omitted) showing the poorest results. The Unit expansion also performed poorly in the EX task, which achieved an F1 of 0.07 only, against 0.77 for all tasks. The expansion of abbreviated drug names and brand names to active ingredients achieved a 1.00 precision such as the expansion of mode of administration, which shows that the result has in no way deteriorated compared to the original text. It is important to highlight that from a patient safety point of view, precision is more important than recall. A low recall means that the system did not improve the quality of the medication statement, whereas any suboptimal precision value bears the risk that the content of the statement is distorted.\nUnclear short forms are a well-known problem in clinical texts [21]. Abbreviation of drugs often follows local jargon. Interestingly, only one (\"ASA\") was correctly resolved, whereas the other four were not resolved at all (\"MFM\" for metformin, \"MTV\" for multivitamins, and \"K\" for potassium). Regarding ChatGPT's risk of hallucination, it is remarkable that the system opted for non-resolution instead of for wrong resolution. While in prompt D the abbreviation \"ORS\" for \"oral rehydration solution\" was even not detected during the NER validation phase as Medication, in prompt E at least in NER it was detected in validation and test but not extended by test EX. Despite the fact it is a common abbreviation in our data set, the complexity increased due to a mixture of languages and documentation styles. Our results for EX show a drastic increase in F1 for the expansion type Mode from Prompt 1 without any prompt patterns (F1 score: 0.00) to Prompt 2 with adding Persona and Template (F1 score 0.94). In Prompt 2, ChatGPT detected all abbreviations of \"oral\" in all samples, but still tried to translate each different abbreviation literally like \"opc\" to \"oral, by consumption\" or \"po\" \"per mouth\". By giving 5 examples with the prompt pattern \"Few-shot\" in Prompt 3, ChatGPT detected, e.g., \"po\", \"o\", \"opc\", and \"po ac\" as \"oral\". Even \"oac\", which was not given as an example, was now correctly defined as \"oral\" and not as \"oral administration\". The changed definition of Mode leads to an increase of F1 by 0.11 for the expansion type Relation to Meal in Prompt 3 as well, because ChatGPT can now categorize the subsequent information. In contrast to the quality improvement of ChatGPT's output for Mode, its F1 slightly decreased by 0.03. Adding clearly defined case-specific prompt patterns for foreign language abbreviations is an essential step in creating the expected results."}, {"title": "4.3 Limitations", "content": "Hallucinations are a relevant issue in ChatGPT [22, 23]. Here, we obtained the remarkable result one instance of a hallucinated response could be identified in the test data (n=50). The daily dosage instruction of an antipsychotic drug was interpreted correctly, but ChatGPT added an \"as needed\" statement on top, however, regarding a small dose that is not expected to cause harm.\nGiven the original text prescription, \"Ridperidone (1) 0.5x1 po hs\", ChatGPT gave us \u201c0.5 Tablet oral at bedtime; 0.25 Tablet oral as needed for agitation\" for instructions EX task instead of \"0.5 Tablet Oral before bed Once daily\" which is the correct answer.\nThe size of the dataset must also be mentioned. Particularly, the assessment of rare but important outcomes such as the occurrence and the severity of hallucinations would have required much more data. Another limitation was the restriction to ChatGPT 3.5. We could have compared it to other LLMs, but our focus has been on the comparison of different prompting strategies rather than different models."}, {"title": "5 Conclusions and Outlook", "content": "Converting medication information into a standardized format is expected to improve patient safety by making it easier to interpret by both humans and machines. This study demonstrated good performance in the task of entity recognition (NER) and entity expansion (EX) in free-text medication statements using ChatGPT 3.5, in comparison to related work. The data, taken from Thai medical records, exhibited particularities in style and format and used a mixture of English and Thai, including local drug product names. We were able to demonstrate good performance in NER and EX. Compared to a zero-shot baseline, a 10-shot approach prevented the system from hallucinating, which would be unacceptable when processing highly safety-relevant medication data.\nOur study works on anonymized text snippets of real-world data. Further tests and analysis of huge real-world patient data for training are necessary to use ChatGPT on a variety of disciplines and text document types. In the future for the use of real-world patient data, especially the ethical and data safety issues need to be discussed [24, 25]. A good alternative would be an LLM deployment on-premise. Like Hu et al. with ChatGPT version 3, we experienced \"a significant degree of randomness\" while performing the equal prompts more often with version 3.5 as well. In our study, the input sequence length was even longer than the one presented in the publication of Hu et al [17].\nAs we are currently at the very beginning of the LLM era, we expect even better performance in the future. Medication-related statements are a particularly challenging use case because their overly compact and idiosyncratic style constitutes a risk for human and machine misinterpretation with unforeseeable impacts on patient safety on the one hand. On the other hand, a similar risk may occur from even minor hallucinations and imprecisions when expanded and normalized by AI methods. Finally, medication styles drastically vary across languages and jurisdictions. We therefore advocate investing particular efforts in the creation of multilingual, multicultural, and multi-specialty medication benchmarks."}]}