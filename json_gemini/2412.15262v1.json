{"title": "Advanced ingestion process powered by LLM parsing for RAG system", "authors": ["Arnau Perez Perez"], "abstract": "Retrieval Augmented Generation (RAG) systems struggle with processing multimodal documents of varying structural complexity. This paper introduces a novel multi-strategy parsing approach using LLM-powered OCR to extract content from diverse document types, including presentations and high text density files both scanned or not. The methodology employs a node-based extraction technique that creates relationships between different information types and generates context-aware metadata. By implementing a Multimodal Assembler Agent and a flexible embedding strategy, the system enhances document comprehension and retrieval capabilities. Experimental evaluations across multiple knowledge bases demonstrate the approach's effectiveness, showing improvements in answer relevancy and information faithfulness.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Natural Language Processing has witnessed remarkable progress with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented capabilities in processing multimodal data, including text and images. However, a significant challenge in LLM implementation remains the context window constraint, which limits the volume of information processed concurrently.\nLeading providers have made substantial strides in expanding context limitations, with some models now capable of handling up to 2.000.000 tokens simultaneously [1]. Nevertheless, this expansion comes at a cost: increased context length correlates with longer time to first token (TTFT), potentially impacting model performance and user experience [2]. Recent work by Anthropic has explored caching techniques to mitigate this issue and reduce TTFT [3].\nTo further enhance LLM efficiency and overcome these challenges, researchers introduced the Retrieval Augmented Generation (RAG) system. In this case for file data. A critical component of RAG is the ingestion phase, where the method of chunking plays a pivotal role in determining the quality of subsequently retrieved data. Common strategies include semantic, recursive, and hierarchical splitting. However, these approaches often fail to account for the diverse data structures present in source materials, such as images, tables, headers and pages. Moreover, they can introduce computational challenges [4].\nThis paper presents a complex ingestion process that not only considers the various data structures within files but also incorporates relevant metadata and establishes hierarchical relationships to improve retrieval."}, {"title": "II. INGESTION", "content": ""}, {"title": "A. Pre-processing", "content": "The pre-processing stage consists of three subprocesses: parsing, assembling and metadata extraction (see Figure 1).\nThe parsing phase extracts image and text content using three strategies:\n1. FAST: Utilizes Python libraries to extract text and images from each document page.\n2. LLM: Employs a multimodal LLM for OCR task, extracting text, table content, and image information. It describes images without text or extracts text from images. In this case it was used Sonnet 3.5 v2 model from Anthropic Claude family [5].\n3. OCR: Leverages external and dedicated machine learning models for OCR task. In this case it was used the AWS Textract service [6].\nBefore assembly, images are analyzed and described based on content type. For plots, the system extracts axis values, legends, labels, and provides a visualization description. Flowcharts are described in terms of process relationships. Page snapshots are then captured.\nIn the assembling process, a Multimodal Assembler Agent integrates the page snapshot, images with descriptions, and text extracted from all three strategies for each page. The agent produces a synthesized markdown file. The output includes individual page markdowns, snapshots, and described images, which are concatenated into a comprehensive document-level markdown.\nThe metadata extraction phase employs a Metadata Extractor Agent to analyze the consolidated markdown, extracting fields such as topic, keywords and summary. The system also extracts metadata directly from the document (title, author, creation date, last modified date, etc.), providing a comprehensive information set about the processed document."}, {"title": "B. Processing", "content": "Following the segmentation of document content into markdown files, including snapshots, images, and metadata, the processing phase extracts information nodes from this data. This approach aims to represent information in a vectorial space, with the primary challenge being the extraction of relevant information and determining its optimal embedding representation. Figure 2 illustrates four distinct node types extractable from a single page (Page node): Header, Text, Table and Image nodes. Text Nodes can encompass various forms of textual information, including paragraphs, words, sentences, ordered lists, and bullet point lists, each representing distinct information requiring specific consideration.\nThe processing phase comprises two primary subprocesses: Splitting and Contextualization. During Splitting, markdown files are parsed and segmented into the aforementioned node types. Four relationship types are established between nodes: next, previous, parent and child. Only Header nodes can possess child nodes, facilitating context window tracking and hierarchical structuring. For Text nodes exceeding a size threshold, a Recursive Splitter or Semantic Splitter is employed for chunking.\nContextualization focuses on Table and Header nodes. A Question Generator Agent creates a set of questions answerable using the information within the table or header node content. Subsequently, a Summary Generator Agent produces context-aware summaries for both table and header nodes.\nFollowing lower-level information node creation, the system generates Page and Document nodes, connecting previously established nodes to these higher-level nodes."}, {"title": "C. Embedding", "content": "At this juncture, all components are prepared for integration. Three critical decisions must be made: selecting a vector database, choosing an embedding model and determining the data to be embedded. The choice of vector database depends on various factors, with numerous options available in the industry.\nPinecone, specifically designed for embedding processes in RAG systems, offers optimized performance for this task. Its namespace system provides excellent scalability, and it is engineered for rapid query execution. However, it has a relatively low metadata storage capacity (40KB) [10], which may be a limiting factor depending on the volume of metadata generated.\nAlternatively, OpenSearch, an open-source solution reimagined as a vector database, offers a hybrid approach. It combines vector and ordinal database functionalities, enabling conventional queries unrelated to vector data. In contrast to Pinecone, OpenSearch does"}, {"title": "III. EVALUATION METRICS", "content": "Evaluation metrics are crucial for assessing the performance of Retrieval Augmented Generation (RAG) systems. These metrics help quantify various aspects of the system's effectiveness, including the relevance of retrieved information, the accuracy of generated answers, and the overall quality of the system's output. The following subsections describe key metrics used in the evaluation of the RAG systems."}, {"title": "A. Answer Relevancy", "content": "Answer Relevancy measures how well the generated answer addresses the user's query. For a set of S statements, the Answer Relevancy score can be calculated as:\n$MAR = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{1}{S} \\sum_{j=1}^{S} ra_{ij}^{a-q})$ (1)\nwhere $ra_{ij}^{a-q}$ scores how well the j-th answer statement addresses well to the i-th query."}, {"title": "B. Faithfulness", "content": "Faithfulness measures how accurately the generated answer reflects the information provided in the retrieved context. It ensures that the system isn't hallucinating or providing information not present in the context. Faithfulness can be calculated using techniques like entailment scores or fact-checking algorithms:\n$MF = \\frac{1}{N} (\\sum_{i=1}^{N} (\\frac{1}{S} \\sum_{j=1}^{S} ra_{ij}^{c-a})$ (2)\nwhere $ra_{ij}^{c-a}$ scores how well the j-th answer statement addresses well to the i-th set of retrieved context."}, {"title": "C. Contextual Relevancy", "content": "Contextual Relevancy evaluates how relevant the retrieved context is to the given query. It measures the system's ability to retrieve pertinent information from the knowledge base. This metric can be using the following metric:\n$MCR = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{1}{S} \\sum_{j=1}^{S} ra_{ij}^{c-q})$ (3)\nwhere $ra_{ij}^{c-q}$ scores how well the j-th retrieved context element addresses well to the i-th query."}, {"title": "D. Contextual Precision", "content": "Contextual Precision is a metric that quantifies the relevance ordering of retrieved information chunks. It is defined mathematically as follows:\nLet R(n) represent the cumulative sum of relevance scores for the first n retrieved items:\n$Ri(n) = \\sum_{k=1}^{n} Tik$\nThe mean Contextual Precision (MCP) is then calculated as:\n$MCP = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{1}{R_i (C_i)} \\sum_{k=1}^{C_i} \\frac{R_i (k)}{k} Tik ) $ (4)\nwhere Ci is the total number of retrieved items, and rk is the relevance score of the k-th item.\nThis metric encapsulates the trade-off between the quantity of relevant information chunks retrieved and the prioritization of relevant nodes in the initial positions of the ranking. A higher MCP value indicates a more effective balance between comprehensive retrieval and accurate ranking of relevant information."}, {"title": "E. Contextual Recall", "content": "Contextual Recall measures the relevancy of the retrieved context given the expected answer.\n$MCR = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{1}{S} \\sum_{j=1}^{S} ra_{ij}^{e-c})$ (5)\nwhere $ra_{ij}^{e-c}$ scores how well the j-th expected answer statement addresses to at least one of the retrieved context node in the i-th answer."}, {"title": "IV. EVALUATION AND RESULTS", "content": "To evaluate the system, three types of documentation were tested. First, article papers where the text density is greater than the image content. Second, corporate slides where the image content is much greater than the text content. Finally, a mix of these two content types and topics. By doing so, the system can be evaluated using two opposing data sources. To accomplish this, three knowledge bases were created.\nOne knowledge base contains 5 article papers extracted from arXiv, another includes over 10 corporate documentation files, and the third knowledge base consists of mixed files up to 10 files of different topics and content types. For each document, an LLM was used to generate a number of questions, expected answers, and ground truths equivalent to the number of pages in the document. For this purpose, the Claude Haiku 3.5 model was used for quick inference and dataset creation.\nFor the retrieval process, the maximum number of nodes retrieved was limited to 5. No filter was applied to the node type, so the resulting retrieval can include any kind of node. The answers were generated using the Claude Sonnet 3.5 v2 model.\nThe metrics scores mentioned in the last section range from 0 to 1, where an LLM evaluates the metric according to its description, assigning a quantified score from the set 0, 0.2, 0.4, 0.6, 0.8, 1 based on specific guidelines. The chosen LLM for this evaluation was Llama 3.1 405B Instruct. To ensure a more contrasting and potentially unbiased scoring, a different model was intentionally used for the evaluation process compared to the one used for answer generation."}, {"title": "A. Evaluating parsing", "content": "During the preprocessing phase, we observed the remarkable capacity of the three strategies to work together in extracting content from files, with each strategy taking on more responsibility depending on the file type.\nFor scanned PDF files, the FAST strategy has no content to extract, but OCR and LLM strategies take responsibility for parsing the document. In academic article-type files, which are mostly two-column documents, FAST and LLM strategies handle much of the task responsibility. This is because OCR will extract text horizontally, independent of the two-column structure. The assembling LLM then places the information as a single-column page, leveraging its processing power.\nFor presentation-like objects, all three strategies work together. FAST extracts the images, while LLM and OCR extract the text from both the images and each slide. It's worth noting that we have experienced better text extraction from images using the OCR strategy. However, while the LLM strategy is sometimes unable to extract the exact words, it proves useful in determining the correct text order. OCR, on the other hand, provides the exact text, even if the text organization is poor."}, {"title": "B. Evaluating node type influence", "content": "During the study, over 80% of retrieved nodes were primarily Page and Header nodes. This is a remarkable result, as the system found more relevance in summaries than in other low-hierarchy nodes. As seen in Figure 3, the context relevancy is quite low in all test cases. This result is influenced by both the number of retrieved nodes per query and the information within the nodes themselves. This indicates that most of the information is not relevant, as pages and sections contain multiple pieces of unused information."}, {"title": "C. Evaluating precision", "content": "Moreover, note that the Contextual precision is lower in the mixture of topics test case. This indicates that the system finds the first relevant context further down in the last retrieved nodes. This issue can be addressed by implementing a reranker, such as one of the rerank models from Cohere [? ), or by using an LLM as a reranker. It's worth noting that the context precision is remarkably high in the presentation (corporate documentation) test case. This could be due to the similarity of topics within this type of documentation."}, {"title": "V. CONCLUSIONS", "content": "\u2022 The system demonstrates an impressive capacity to extract content from both presentation-style documents and high text density documents, including those provided as scanned images. The ability of the three strategies to work together is remarkably enhanced by the capacity of the Claude Sonnet 3.5 family to perform OCR on documents.\n\u2022 This new node extraction and hierarchy approach opens up a novel way to chunk documents, moving beyond standard chunking strategies that do not distinguish among data types and their hierarchy in the document. This makes it easier to track the file structure.\n\u2022 The system excels at the Answer Relevancy and Faithfulness metrics, albeit at the cost of retrieving a larger amount of context. However, there is still work to be done with changing files and external references inside files. More concept linking must be implemented to overcome the relationships among entities and concepts."}, {"title": "VI. APPENDIX", "content": "This appendix presents a curated selection of markdown files generated during the pre-processing stage of the system. These examples serve to illustrate the system's capability in handling diverse document types and transforming them into structured markdown format. Each example is accompanied by a brief analysis highlighting the specific challenges addressed and the key features of the markdown output."}]}