{"title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation", "authors": ["Liqun Ma", "Mingjie Sun", "Zhiqiang Shen"], "abstract": "This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58 [1]) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regular LLM pretraining, while delivering competitive results in terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch. This research encourages a new computational framework and may facilitate the future design of specialized hardware tailored for fully 1-bit LLMs. We make all our models, code, and training dataset fully accessible and transparent to support further research\u00b9.", "sections": [{"title": "1 Introduction", "content": "Benefiting from the huge parameter scale and massive training corpora, transformer-based Large Language Models (LLMs), like ChatGPT [2] and LLaMA [3, 4], perform great in tasks requiring domain knowledge and complex reasoning. Moreover, the capabilities of LLMs tend to enhance as their parameter sizes expand. This substantial scale in parameters results in considerable storage and computational demands, which substantially restricts LLMs' broader application and development. Quantization efficiently mitigates these limitations by mapping 32-bit parameters to smaller bit sizes. It substantially cuts storage requirements and enhances computational speed and energy efficiency during inference.\nAs the most extreme case of quantization, binarization represents each parameter by just {-1, 1}. It maximizes compression and efficiency but at the cost of accuracy. Prior efforts to preserve the efficacy of binarized LLMs include retaining salient parameters [5] or using near-one-bit to represent each parameter [1]. While these approaches have shown promise, they still leave room for optimization in storage and efficiency, and additional full-precision parameters or parameter encodings expressed in non-powers of 2 can cause extra overhead when adapting to edge hardware."}, {"title": "2 Related Work", "content": "Neural Network Binarization. Binarization, the most extreme form of network quantization, converts model parameters into a 1-bit format. Many studies have focused on Binary Neural Networks (BNNs) to improve their accuracy despite inherent limitations. BinaryConnect [8] converts full-precision weights in neural networks to 1-bit binary weights using stochastic methods during training and simulates the effects of binary weights during inference. They also implement a clipping function in backward propagation to prevent excessive growth of real-valued weights. Expanding on this, they develop the Binarized Neural Network (BNN) [9], which includes detailed training and acceleration techniques, demonstrating the efficiency and practicality of BNNs through reduced storage and faster processing times in image classification.\nHowever, these methods typically suffer from accuracy loss, prompting numerous optimization-based solutions over recent years to mitigate this. Binary Weight Networks (BWN) and XNOR-Net [10] introduce a scaling factor that approximates floating-point parameters to reduce quantization errors. Further developments like DoReFa-Net [11], WRPN [12], and ABC-Net [13] introduce strategies to minimize information loss and quantization errors. Innovations such as XNOR-Net++ [10] and various mimic solutions like Distillation and Quantization (DQ) [14] continue to refine these approaches, emphasizing stability and high accuracy in training binary models. To address the non-differentiability of the binarization function, techniques like the straight-through estimator (STE) [15] are used for backpropagation. The ReActNet [16] improves BNNs with generalized activation functions, and the BNN+ [17] introduces an enhanced derivative approximation of the sign function along with a regularization strategy to optimize weight learning.\nLarge Language Model Binarization. PB-LLM [5] implements partial binarization of the LLMs, retaining the salient parameters at full precision, occupying only a small portion of all parameters, to maintain the linguistic reasoning capacity. BiLLM [18] also considers the distribution pattern of the weight scale. It uses a binary residual approximation strategy to binarize salient parameters, which consists of an original binary tensor and a residual binarized matrix to present the binarization result of salient parameters. BitNet b1.58 [1] quantizes all parameters to the set of {-1, 0, 1}, where they find that quantized LLMs achieve competitive performance to their full-precision counterparts. However, PB-LLM and BiLLM use the extra cost of storage to process salient weights, and BitNet b1.58 uses an average of 1.58 bits to represent weights. None of them have reached the limits of binary models. There is room for further improvement in model storage size and inference speed. Our work focuses on achieving fully binarized LLMs while preserving the model's capabilities as much as possible.\nBitNet [19] and OneBit [7] employ quantization-aware training (QAT) to binarize LLMs. BitNet utilizes group quantization by applying different scales to the parameters of various groups, which accelerates model training. Its training loss aligns with that of pretraining autoregressive language models. Conversely, OneBit preserves the full-precision model knowledge through quantization-aware knowledge distillation, using a full-precision model as the teacher and guiding the binarized model training with two distinct loss functions. Unlike these two methods, our approach achieves similar or better results through a more streamlined and efficient training process."}, {"title": "3 Methodology", "content": "In this section, we first provide an overview of the architecture of our FBI-LLM in Section 3.1. Then, in Section 3.2, we detail the FBI-linear module, the main component of FBI-LLM. Finally, we elaborate the FBI-LLM autoregressive distillation-based training procedure in Section 3.3."}, {"title": "3.1 Architecture of FBI-LLM", "content": "We illustrate the overall architecture of FBI-LLM in Fig. 2 (a). In transformer-based LLMs, the majority of parameters are found within the linear modules. FBI-LM replaces all linear modules, except for the causal head, with FBI-linear (Fully BInarized Linear). Since the causal head directly influences the output token distribution in each step, binarizing its parameters would significantly affect the accuracy of the model's output, so we retain its precision.\nAdditionally, the parameters in two other core modules of LLMs, embedding and layer norm, also need to be kept at full precision. This is because the embedding module contains semantic information about all tokens and, as the first layer of the model input, determines the text's initial representation. Layernorm, on the other hand, scales the activation values directly. Binarizing its parameters would significantly reduce the semantic expressiveness of the activation values at each layer. Similar settings are also adopted in other work [19] about the binarization of LLMs."}, {"title": "3.2 FBI-Linear", "content": "The main parameters in FBI-linear are a matrix $W^b \\in \\mathbb{R}^{m \\times n}$ consisting only of {1, -1}, which is binarized from the full-precision linear module $W^f \\in \\mathbb{R}^{m \\times n}$ of the LLMs. During the training, the binarization process can be formulated as:\n$W^b = sign(W^f)$ (1)\nwhere sign function is formulated as:\n$sign(W_j^f) = \\begin{cases} 1, & W_j^f > 0 \\\\ -1, & W_j^f \\le 0 \\end{cases}$ (2)\nWe follow the previous works [10, 19] to scale the binarized parameter with full-precision scale factors. Scale factors can effectively reduce the error between the binarized and original parameters, thereby preserving the more representational capacity of the corresponding module. They constitute a small fraction of the total parameters, making them a highly efficient enhancement to the model's performance without significantly increasing the parameter count and computational demand.\nSpecifically, in the FBI-linear, we apply scaling at the granularity of the matrix columns. The calculation process can be formulated as:\n$W_j^b = \\alpha_j W_j^b + \\beta_j$ (3)\nwhere $W_j$ donate the jth column of the scaled binarized weight matrix $W^b \\in \\mathbb{R}^{m \\times n}$. $\\alpha_j$ and $\\beta_j$ are the jth element in learnable scale vectors $\\alpha \\in \\mathbb{R}^{n}$ and $\\beta \\in \\mathbb{R}^{n}$ respectively.\nTo accelerate the model's convergence speed, we initialize $\\alpha$ and $\\beta$ before training as following:\n$\\alpha_j = a_j$ (4)\n$\\beta_j = \\frac{1}{m} \\sum_{i=1}^{m} W_{ij}^f - a_j$ (5)\nwhere $a_j = \\frac{1}{m} \\sum_{i=1}^{m} W_{ij}^f$, denotes the average of jth column in $W^f$."}, {"title": "3.3 Autoregressive Distillation", "content": "Given a training corpus of tokens $x = {x_1,..., x_n}$, a standard autoregressive language modeling objective [20] is to maximize the likelihood:\n$L(x) = \\sum_{i} logp (x_i | x_{i-k}, ..., x_{i-1}; \\theta)$ (6)\nwhere k represents the size of the context window and the conditional probability p is modeled through a neural network characterized by the parameters $\\theta$. Unlike conventional autoregressive language models, we train FBI-LLM using autoregressive distillation (AD). In the training, a full-precision pre-trained LLM is used as the teacher model, and the binarized target model acts as the student. Suppose each instance of training data consists of a sequence of input tokens $x_1,...,x_m$, the teacher prediction probability for the next token can be formulated as:\n$p^T (x_{m+1} | x_1,...,x_m) = softmax (hW_{m+1})$. (7)\nwhere $h$ represents the activation of the final transformer block, $W_{m+1}$ represents parameters of the added linear output layer to predict the next token's probability.\nThe cross-entropy between the outputs of the student model and the teacher model is calculated as the final loss function at each step of predicting the next token. It can be formulated as:\n$L = -\\frac{1}{n} \\sum_{i=1}^{n} p^T (x_{i+1}).logp^S (x_{i+1})$ (8)"}, {"title": "4 Experiments", "content": "In our experiment, we follow the W1A16 setup [18, 7], quantizing only the parameters to 1-bit while keeping the activation values at 16-bit. We train FBI-LLMs with sizes of 130M, 1.3B, and 7B, testing their performance across multiple tasks."}, {"title": "4.1 Setup", "content": "Dataset. We train FBI-LLMs with the Amber dataset [25]. Amber dataset is a mixture of Refined-Web [26], StarCoder [27], and RedPajama-v1 [28] and contains the total 1.26 trillion tokens. It divides the data into 360 chunks, with each chunk containing an average of 3.5 billion tokens.\nTraining details. Our models used for experiments adopt a similar structure as LLaMA2 [3]. For the specific hyper-parameters settings of FBI-LLMs of different sizes, refer to Table 1. The maximum sequence length is set to 2048. The optimizer is Adam with $\\beta_1 = 0.9, \\beta_2 = 0.98$. The initial learning rate for all model sizes is set at 3e - 4, following a cosine learning rate schedule that decreases to a final rate of 3e - 5 as it is warmed up over 2,000 steps. We use gradient clipping at 1.0. We use LLaMA2-7B as the teacher model for all size FBI-LLMs to calculate autoregressive distillation loss. We train models with 64 NVIDIA A100 GPUs in total and maintain BF16 precision while training. Please refer to Appendix C for more details.\nBaselines. We compare our work with prior binarized LLMs Bi-LLM [18], OneBit [7], and Bit-Net [19]. We also include the BitNet b1.58 [1], which is a ternary quantization LLM, as our baseline model for comparison\u00b3. We further include results from open-sourced full-precision models of various sizes, such as OPT [29], LLaMA [3, 4], and TinyLLaMA [30], as references.\nEvaluation Metrics. We evaluate the models based on their zero-shot performance in some downstream tasks, including BoolQ [31], PIQA [32], HellaSwag [33], Winogrande [34], ARC [35], and OpenbookQA [36]. We also use perplexity as the evaluation metric. Perplexity measures how well a probability model predicts a token, quantitatively measuring the model's generation power."}, {"title": "4.2 Main Results", "content": "Table 2 presents the main results comparing our FBI-LLMs to various state-of-the-art baseline models. We also report the average bit-width occupied by model parameters, excluding the embedding layer and the head, for different models. Details on the calculation process can be found in Appendix B. Our FBI-LLMs maintain the lowest average bit-width across different model sizes while demonstrating remarkable performance. We provide zero-shot accuracy, which is a foundation for understanding how well a model can perform without additional task-specific information. This metric is commonly used to assess the model's initial capabilities and aligns with certain benchmarking tasks aimed at measuring the pre-trained LLM's general comprehension and knowledge-reserving capabilities across diverse downstream tasks without additional few-shot information.\nSince there is no binary baseline for the 130M size, we compare our 130M model with the BitNet b1.58 at the 700M scale. Despite the fivefold difference in model size and significant variations in quantization degree, our model still outperforms BitNet b1.58 in BoolQA and OpenbookQA. For the 1.3B-scale binary models, our FBI-LLM achieves the best performance in most downstream tasks and perplexity, even matching or exceeding the capacity of some 7B-scale binary models like BiLLM-LLaMA2-7B. Compared to the full-precision models of a similar scale, the proposed FBI-LLM 1.3B can achieve up to 87% of their performance in downstream tasks. In the 7B scale, our model significantly outperformed nearly all baselines.\nFurthermore, limited by computational resources, the current results for FBI-LLM 7B are not final. We only use 8.6% (31 chunks) of the Amber dataset. Fig. 3 illustrates the changes in downstream task accuracy and perplexity during the training process of FBI-LLM-7B. It is clear that, as of the current training progress, the performance of FBI-LLM-7B will be improved consistently, and further training could yield better results."}, {"title": "4.3 Effectiveness of Autoregressive Distillation", "content": "To demonstrate the effectiveness of using only autoregressive distillation as the training objective, we train two models: one using solely the autoregressive distillation loss and the other using only the standard autoregressive loss. All other training procedures are identical to those used for FBI-LLM. We evaluate the performance of these models on downstream tasks and perplexity, as shown in Fig. 4. The evaluation tasks and datasets are the same as those listed in Table 2. For clarity, we present only the average accuracy across different tasks and the"}, {"title": "5 Analysis", "content": "In this section, we analyze 1) the better choice of training from scratch or continuing training from pretrained LLM for binarized LLMs. 2) training instability and our solution. 3) storage efficiency of our models. 4) generation case demonstrations."}, {"title": "5.1 Training from Scratch or Continue Training from Pretrained LLM?", "content": "Intuitively, continuing training from a pretrained LLM to obtain a binarized model can inherit knowledge from the original model, potentially achieving better results than training from scratch. To evaluate this hypothesis, we conduct analytical experiments to record and compare the behaviors of models under both training procedures."}, {"title": "5.2 Training Instability", "content": "Both binary and full-precision LLM training have been found to exhibit unstable training behaviors [19, 7, 38]. Our FBI-LLM exhibits similar issues, specifically manifesting as sudden spikes in training loss when training 1.3B and 7B FBI-LLMs, which sometimes fail to converge after that. We adopt the solution similar to PaLM [38]: if the loss no longer tends to converge, the model reverts to the previous checkpoint and skips the data chunk that triggered the unstable loss to continue training. The model no longer encounters issues at the same training steps using this approach. We observe that pretraining the 7B FBI model from scratch has approximately a 6% probability of causing loss spikes. For the 1.3B model, training is more unstable due to the lower capability, with about a 15% probability of loss spikes. This is consistent with the pretraining behavior seen in real-valued LLMs while the probability of spiking is significantly higher, which may be related to the limited expressive capability of binary parameters. To handle this, we skip any data blocks where loss spikes occur."}, {"title": "5.3 Storage Efficiency", "content": "Table 3 presents the theoretical storage space required by FBI-LLMs of various sizes compared to the full-precision LLaMA with the same structure. It also details the proportion of additional parameters (\u03b1 and \u03b2) introduced by FBI-LLM. The comparison in the table demonstrates that FBI-LLM can achieve a high compression ratio, significantly reducing the storage burden of LLMs. Although the extra parameters for scaling and shifting introduced by FBI-LLM need to be retained in full precision, their proportion is minimal, rendering their impact on storage negligible."}, {"title": "5.4 Generation Cases", "content": "As illustrated in Fig. 7, although the generation quality of FBI-LLMs does not fully match that of full-precision LLMs, FBI-LLMs can still generate fluent and meaningful content. Compared to BitNet b1.58, which has a higher parameter bit-width, FBI-LLMs demonstrate a better understanding of prompts and include more knowledge in some generated examples. This indicates that FBI-LLMs possess strong generative capabilities and contain sufficient knowledge. Furthermore, FBI-LLMS demonstrate the potential to scale up further, reaching new levels of intelligence while being more hardware-friendly for deployment."}, {"title": "6 Conclusion", "content": "We have proposed a learning framework using autoregressive distillation for 1-bit weight binarization of LLMs from scratch. Extensive experiments on models of various sizes of 130M, 1.3B, and 7B demonstrate that FBI-LLM outperforms strong baselines and strikes a good balance between model size and performance. We also analyze the capabilities, properties, and potential of these extremely low-bit models, providing guidance for future research.\nLimitations. Our proposed binarization framework significantly reduces the memory and computation consumptions of LLMs, providing potential for their efficient deployment. However, there are several limitations of our models. Firstly, our 1-bit binarization inevitably incurs a performance loss compared to the original full-precision model. Additionally, the training process, which includes knowledge distillation, brings additional computational costs. Moreover, due to the unique nature of binarization, current hardware makes it difficult to directly support binarized LLMs to achieve real speedup. We also have not yet considered intermediate activation binarization which is the same as previous studies. Finally, potential ethical issues of pretrained LLMs, such as harmful biases, privacy concerns, and the spread of disinformation, are likely to persist after binarization in our LLMs."}, {"title": "Appendix", "content": "A Broader Impacts\nOur proposed fully binarized large language models (FBI-LLM) require less computational power and memory in training and inference, making advanced AI technology accessible to organizations and individuals with limited resources. With reduced hardware requirements, smaller businesses, educational institutions, and non-profit organizations can implement LLMs, democratizing access to cutting-edge AI. Binarized models are more energy-efficient, which can significantly lower the carbon footprint associated with running large-scale AI applications. However, even binarized LLMs can still inherit and exist biases present in training data, leading to unfair outcomes in applications like hiring, law enforcement, and lending."}, {"title": "B Average Bit-width of Binarized LLM", "content": "This section explains how to calculate the average bit-width of a binarized LLM. Since the embedding layer and head have a large number of parameters and are not binarized, we do not consider them when calculating the average bit-width. Consider a module containing an RMSNorm and a linear layer with a parameter matrix $A \\in \\mathbb{R}^{n \\times n}$, which in total has n + $n^2$ parameters. Using the FBI-LLM binarization process as an example, after binarization, this module gains additional learnable scale parameters \u03b1 and \u03b2, totaling 2n parameters. During inference, the parameters of RMSNorm, \u03b1, and \u03b2 remain at 16 bits, while A is quantized to 1 bit. Therefore, the average bit-width of this module can calculated as follows:\nAverage Bit-width = $\\frac{1 \\times n^2 + 16 \\times 3n}{3n + n^2}$ (12)"}, {"title": "C Model Configuration and Training Details", "content": "In this section, we list the model configurations and training details for three scales of FBI-LLM models we trained in Table 4."}, {"title": "D Detail Experiment Results", "content": "We list the detailed experiment results about the effectiveness of autoregressive distillation in Section 4.3 in Table 5."}]}