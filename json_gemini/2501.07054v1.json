{"title": "PoAct: Policy and Action Dual-Control Agent for Generalized Applications", "authors": ["Guozhi Yuan", "Youfeng Liu", "Jingli Yang", "Wei Jia", "Kai Lin", "Yansong Gao", "Shan He", "Zilin Ding", "Haitao Li"], "abstract": "Based on their superior comprehension and reasoning capabilities, Large Language Model (LLM) driven agent frameworks have achieved significant success in numerous complex reasoning tasks. ReAct-like agents can solve various intricate problems step-by-step through progressive planning and tool calls, iteratively optimizing new steps based on environmental feedback. However, as the planning capabilities of LLMs improve, the actions invoked by tool calls in ReAct-like frameworks often misalign with complex planning and challenging data organization. Code Action addresses these issues while also introducing the challenges of a more complex action space and more difficult action organization. To leverage Code Action and tackle the challenges of its complexity, this paper proposes Policy and Action Dual-Control Agent (PoAct) for generalized applications. The aim is to achieve higher-quality code actions and more accurate reasoning paths by dynamically switching reasoning policies and modifying the action space. Experimental results on the Agent Benchmark for both legal and generic scenarios demonstrate the superior reasoning capabilities and reduced token consumption of our approach in complex tasks. On the LegalAgentBench, our method shows a 20 percent improvement over the baseline while requiring fewer tokens. We conducted experiments and analyses on the GPT-40 and GLM-4 series models, demonstrating the significant potential and scalability of our approach to solve complex problems.", "sections": [{"title": "Introduction", "content": "With the rise of Large Language Models (LLMs), knowledge-driven frameworks for agents have gradually become mainstream. LLMs are increasingly capable of handling complex tasks due to their powerful reasoning abilities and extensive knowledge bases. The ReAct framework (Yao et al., 2022), a significant advancement in this technology, offers substantial advantages. ReAct-like agents, by integrating reasoning and action, enable the decomposition of complex tasks into multiple manageable subtasks, facilitating a gradual resolution of the process(Wang et al., 2023a; Zhu et al., 2023). ReAct not only enhances the execution efficiency of agents in complex tasks but also ensures that they can effectively address intricate problems by receiving environmental feedback and continuously adjusting and refining their policies during task execution. Furthermore, ReAct provides flexible reasoning and action capabilities for agents, demonstrating the benefits of disassembly and step-by-step execution in solving complex tasks, thereby establishing itself as one of the cornerstones of agent solutions."}, {"title": "Related Works", "content": "Large Language Models (LLMs) demonstrate powerful reasoning and complex task-processing capabilities (Radford et al., 2019). LLMs such as GPT-3, GPT-4, and PALM perform well in multi-round dialogue comprehension and response generation. (Floridi and Chiriatti, 2020; Thoppilan et al., 2022;"}, {"title": "PoAct: Policy and Action Dual-Control Agent", "content": "In this section, we will provide a detailed introduction to the PoAct and its components. As illustrated in Figure 2, the PoAct framework is grounded in the reasoning paradigm of the ReAct Code Agent. To enable PoAct to concentrate on specific reasoning steps, the Policy Controller dynamically adjusts the step policies for different reasoning phases, allowing PoAct to focus solely on the context and reasoning intricacies of particular steps. Additionally, the Action Controller optimizes the action space and evaluates abnormal code actions using a RAG Selector and an Action Reviewer."}, {"title": "ReAct Code Agent", "content": "PoAct employs the ReAct Code paradigm, which integrates the ReAct Agent and Code Agent. This paradigm effectively combines the reasoning capabilities of LLMs with function-calling policies, similar to the ReAct Agent, while also enabling more complex actions than simple function calls through a code action space. As illustrated in Figure 3, after receiving a task, PoAct first executes the planning step to decompose the complex task into subtasks. It then engages in step-by-step planning and updates the global plan when the task cannot be resolved immediately. PoAct initiates a multi-step reasoning process based on this planning, generating local planning a thought for each step to guide the generation and execution of code actions. The process is informed by the results of these actions, referred to as observations. This iterative approach continues, generating subsequent rounds of reasoning based on the observations, until PoAct successfully deduces the answer to the user's task. By leveraging code actions, PoAct can navigate highly complex action spaces and manage all states flexibly using variables."}, {"title": "Policy Controller", "content": "The ReAct Code paradigm enhances the capability of agents to navigate complex environments and increases the challenge of organizing accurate and high-quality code actions. Existing frameworks for agents typically depend on a single policy to address the entire reasoning process, which can lead"}, {"title": "Agent Policy Prompt", "content": "The agent policy prompt primarily establishes a global policy for the PoAct, aiming to clarify the structure of the dialogue history. As illustrated in Figure 2, we define the meanings of query, thought, code and observation in agent policy prompt. This clarification helps PoAct maintain a comprehensive reasoning process and prevents confusion when switching between policies."}, {"title": "Step Policy Prompt", "content": "Step policy prompts primarily allow PoAct to transition into specific roles during various reasoning steps, allowing them to concentrate on the details and quality of the current step. Policies that emphasize particular reasoning steps can enhance the quality of each step while minimizing distractions from the demands of other steps. Furthermore, the division of a single policy into prompts that target specific reasoning steps reduces the time and effort required for prompt encoding. In the following, we will briefly outline the focus of the different policies.\nPlan Policy requires PoAct to focus on decomposing complex tasks into subtasks and providing a comprehensive plan for each subtask to guide subsequent reasoning steps. PoAct must adopt the perspective of a planning expert, who excels in global planning, to develop a complete and executable plan utilizing all available tools and packages during the execution of plan steps. The planning expert concentrates solely on breaking down queries that can be parallelized and does not require detailed knowledge of the specific code execution for the subsequent steps. The planning phase typically occurs before multiple rounds of reasoning and involves revising the plan based on the dialogue history as challenges arise during the reasoning process.\nThought Policy mandates that the PoAct to develop a local plan for the current round, drawing on the global plan and conversation history, which"}, {"title": "Action Controller", "content": "The code action can leverage these extensive tools to navigate a highly complex action space. However, such complexity may significantly hinder the agent's ability to select the appropriate action. To identify the correct action space and reasoning path, we employ the Action Controller, which determines the most suitable action space and evaluates the reasoning path. The Action Controller consists of two modules: the RAG Selector and the Path Reviewer. The RAG Selector primarily utilizes the Retrieval-"}, {"title": "RAG Selector", "content": "A large number of tools and few-shot examples can expand the action space of the agent, but they may also hinder the agent's decision-making capabilities and increase the coding time of the LLMs, ultimately reducing the agent's performance. To enable the PoAct to execute the most appropriate actions within a complex action space, we employ a RAG Selector to dynamically manage the visible tools and examples, thereby controlling the action space available to the PoAct. For more implementation details you can refer to Appendix A.\nTools and Few-Shot Examples In accordance with various task scenarios, we design different generic tools and few-shot examples for PoAct. LLMs will organize these tools to execute complex tasks based on different reasoning policies, using information about the tools, including their names, descriptions, input cases, and output cases. Few-shot examples primarily consist of instances of error-prone steps alongside exemplary uses of the tools. We generate these few-shot examples based on task types and manually review and modify them as necessary. The RAG Selector will retrieve and rearrange the most relevant tools and few-shot examples, dynamically updating them in the PoAct system prompt throughout the reasoning process."}, {"title": "Path Reviewer", "content": "The complex action space can lead to multiple anomalous reasoning paths, significantly impacting the performance and robustness of the agent's reasoning process. As illustrated in Table 4, the Path Reviewer alleviates some of these abnormal reasoning paths through three tasks: exception handling, query & answer rewrite, and code action reflection.\nException Handling In the process of solving PoAct tasks from the server side, the execution of code blocks may result in multiple exceptions. To address all potential exceptions, we append a message role error to the exception message and"}, {"title": "Results and Analysis", "content": "We comprehensively evaluate PoAct's capabilities using two agent datasets. 1) LegalA-"}, {"title": "Experiments on LegalAgentBench", "content": "Main Experimental Results Table 1 presents the primary evaluation results for the gpt-4o, glm-4 family of models, and qwen-72b-instruct on LegalAgentBench. Our observations indicate that PoAct consistently outperforms the baseline scores across all problem types. Furthermore, PoAct consumes far fewer tokens than the baseline approach in numerous tools setting. These results underscore the effectiveness of PoAct in enhancing the capabilities of agents in complex scenarios, thereby improving the performance of agent systems. In the glm-4-plus setting, PoAct's success rate is 25 percent higher than the ReAct, with the advantage of PoAct becoming increasingly pronounced as problem difficulty escalates. Notably, the success rate of PoAct in 5-hop problems is 28 percent superior to that of P-E, demonstrating the potential of our method in addressing multi-hop complex reasoning problems."}, {"title": "Ablation Study", "content": "To evaluate the capabilities of the modules, we conducted ablation experiments on glm-4-plus. As"}, {"title": "Case Study", "content": "We found that PoAct exhibits strong generalization capabilities. With a simple tool description customized to the data source, PoAct can solve most complex tasks with zero or few shots. In our experiments, we discovered that, thanks to the RAG Selector, the addition of few-shot examples and tools has little impact on the performance and accuracy of PoAct, demonstrating excellent scalability in specific domains. In practice, we have developed several versions of PoAct for various applications, including general assistance and data analysis. In real business scenarios, PoAct demonstrates remarkable scalability and versatility. More examples can be found in Appendix C."}, {"title": "Conclusion", "content": "We propose PoAct, a code agent that employs dual control over policy and action to execute high-quality code actions and optimize inference paths by dynamically switching between inference policies and action spaces. We evaluate our approach using two challenging agent benchmarks. The results indicate that PoAct significantly enhances the reasoning capabilities of tool-calling agents when addressing complex tasks, while also substantially reducing token consumption. Furthermore, we present various case studies of PoAct across different scenarios, demonstrating its strong scalability and generalizability."}, {"title": "Limitations", "content": "This study has two limitations. First, regarding the RAG Selector, it does not compare more complex and diverse embedded models to assess the impact of the RAG system on the experimental results. Second, we observed that the Code Agent appears to lose the language models' capability for complex reasoning. For instance, the Code Agent often resorts to using code to filter specific fields from structured data, even when the value of the field has already been presented in the context."}]}