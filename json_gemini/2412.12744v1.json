{"title": "Your Next State-of-the-Art Could Come from Another Domain: A Cross-Domain Analysis of Hierarchical Text Classification", "authors": ["Nan Li", "Bo Kang", "Tijl De Bie"], "abstract": "Text classification with hierarchical labels is a prevalent and challenging task in natural language processing. Examples include assigning ICD codes to patient records, tagging patents into IPC classes, assigning EUROVOC descriptors to European legal texts, and more. Despite its widespread applications, a comprehensive understanding of state-of-the-art methods across different domains has been lacking. In this paper, we provide the first comprehensive cross-domain overview with empirical analysis of state-of-the-art methods. We propose a unified framework that positions each method within a common structure to facilitate research. Our empirical analysis yields key insights and guidelines, confirming the necessity of learning across different research areas to design effective methods. Notably, under our unified evaluation pipeline, we achieved new state-of-the-art results by applying techniques beyond their original domains.", "sections": [{"title": "1 Introduction", "content": "Text classification with hierarchical labels is a fundamental challenge in natural language processing, where the goal is to assign one or more labels from a hierarchically-organized label set to each text input. This problem appears across diverse domains, such as medical coding that assigns International Classification of Diseases (ICD) codes to patient records (Edin et al., 2023), patent classification that predicts International Patent Classification (IPC) codes for patent documents (Kamateri, Salampasis, & Perez-Molina, 2024), and extreme multi-label classification tasks in legal, wikipedia, and e-commerce domains (Bhatia et al., 2016). While these applications may seem distinct, they share a common core: classifying text with labels that have inherent hierarchical relationships.\nDespite this commonality, current research is largely confined within individual domains. Methods are typically developed and evaluated only on domain-specific datasets, with minimal cross-domain analysis or comparison. This domain-centric approach has led to three significant knowledge gaps in the research literature: (1) how methods that excel in one domain compare to domain-specific approaches in others; (2) which architectural components (to which we refer as submodules, such as text encoders, label encoders, and prediction mechanisms) are truly domain-specific versus universally effective; and (3) how dataset characteristics, rather than domain origin, influence method performance.\nTo address these gaps, we present the first comprehensive cross-domain analysis of hierarchical text classification. Our work makes several key contributions:\n\u2022 Cross-Domain Survey: We present the first comprehensive cross-domain survey of hierarchical text classification methods, dissecting 32 representative methods and abstracting common elements that bridge different domains.\n\u2022 Unified Framework: We propose the first domain agnostic framework that decomposes hierarchical text classification methods into nine essential submodules, enabling systematic comparison of approaches across different domains. This framework particularly highlights how different methods utilize hierarchical label information.\n\u2022 Cross-Domain Analysis: We conduct the first large-scale cross-domain evaluation of hierarchical text classification methods (with necessary re-implementations and unified evaluation codebase), analyzing eight state-of-the-art methods across five domains on eight curated datasets.\n\u2022 Enhanced Datasets: We make several dataset contributions: (1) a cleaned version of EurLex with recovered taxonomy and restored original text; (2) a new dataset derived from EurLex, EurLex-DC-410, labeled with directory codes; and (3) a carefully curated selection of eight datasets across five domains, with necessary adaptations to enable fair cross-domain evaluation, publicly available at https://github.com/aida-ugent/cross-domain-HTC.\n\u2022 New State-of-the-Art Results: We achieved new state-of-the-art results\u00b9 on NYT-166, SciHTC-83, USPTO2M-632, and MIMIC3-3681 by applying methods from other domains or combining submodules across domains.\n\u2022 Empirical Insights: Our findings through extensive experiments reveal that dataset characteristics and architectural design choices, rather than domain specificity, primarily determine method effectiveness. We encourage researchers and practitioners to look beyond their specific domains, as state-of-the-art performance can often be achieved by leveraging methods and innovations from other fields."}, {"title": "2 Related Work", "content": "Section 2.1 presents the surveys and benchmarks for the general problem of multi-label text classification, while Section 2.2 focuses on domain-specific surveys and benchmarks."}, {"title": "2.1 General surveys/benchmarks", "content": "An overview of multi-label text classification is provided by X. Chen et al. (2022), which systematically categorizes methods by data efficiency, feature extraction, and label correlation modeling, but focuses on flat classification without exploring hierarchical structures or cross-domain analysis. An empirical comparison across benchmark datasets is presented in Bogatinovski, Todorovski, D\u017eeroski, and Kocev (2022), examining how model performance relates to dataset characteristics, but primarily examines non-textual domains with small label spaces. The work of Wei, Mao, Shi, Li, and Zhang (2022) focuses on extreme multi-label learning challenges and collects datasets and tools, but does not address hierarchical structures or provide empirical validation. Similarly, Bhatia et al. (2016) maintains a widely cited repository of extreme multi-label classification methods and datasets spanning e-commerce, wikipedia and legal domains, though many datasets only contain preprocessed feature vectors without original texts, limiting their applicability for pretrained language models.\nVarious aspects of multi-label classification are covered in recent surveys: multiple modalities (Han, Wu, Chen, Li, & Zhang, 2023), label imbalance (Tarekegn, Giacobini, & Michalak, 2021), and deep learning architectures (Tarekegn, Ullah, & Cheikh, 2024), but none comprehensively address text datasets, hierarchical structures, or cross-domain analysis. A review of hierarchical multi-label text classification methods is presented by Liu et al. (2023), categorizing them into tree-based, embedding-based and graph-based approaches, but it covers limited domains and lacks empirical validation. Most recently, Bertalis et al. (2024) compares hierarchical text classification (HTC) and extreme multi-label classification (XML), evaluating two methods on three datasets from each of these domains. While their observation about XML methods' adaptability to HTC tasks aligns with one aspect of our findings, our work provides the first comprehensive cross-domain analysis spanning five domains, evaluating eight state-of-the-art methods on eight diverse datasets, with a unified framework for analyzing method components and in-depth empirical analyses of their effectiveness."}, {"title": "2.2 Domain-specific surveys/benchmarks", "content": "For automatic medical coding, Edin et al. (2023) benchmarks state-of-the-art models on cleaned MIMIC datasets, providing an open-source evaluation pipeline. For patent classification, Kamateri et al. (2024) surveys classification complexities, reviews key datasets, and evaluates Deep Learning and large language models' potentials, highlighting domain-specific challenges. For legal documents, Chalkidis, Fergadiotis, Malakasiotis, Aletras, and Androutsopoulos (2019) establishes a benchmark by introducing EurLex-57K and conducting comparisons of several neural architectures, demonstrating that BiGRU-based models consistently outperform CNN-based approaches on legal text classification tasks.\nTo summarize, while these surveys provide excellent domain-specific insights, our work differs by offering the first cross-domain survey and benchmark evaluation, encompassing a comprehensive coverage of datasets and methods."}, {"title": "3 Across-domain overview of text classification with hierarchical labels", "content": "In this section, we provide the first cross-domain overview of text classification with hierarchical labels. We begin with a formal problem formulation in Section 3.1. Then, Section 3.2 presents recent methods from different domains. We analyze these methods within our proposed unified framework (Section 3.3), focusing on their common submodules (architectural components) and their utilization of label information."}, {"title": "3.1 Problem formulation", "content": "Conceptually, text classification with hierarchical labels is the task of assigning one or more relevant labels to a given text input, where these labels are organized in a hierarchical structure. The challenge lies both in understanding the text content and utilizing the relations between labels. The labeling scheme can vary across datasets: some use leaf-only labels (only the most specific labels), some require complete paths (including all ancestors), and some allow partial paths (labels at any level). The prediction task should match the labeling scheme of the dataset.\nFormally, the goal of text classification with hierarchical labels is to learn a function $f : X \\rightarrow Y^*$, where $X$ is the sample space of all possible text sequences and $Y^*$ is the power set of $L$ (the label set), so that for any given sample $x \\in X$, $f(x)$ outputs one or more labels most relevant to $x$. The label space can be organized in a hierarchy, defined as a tuple $(L, E, p)$ with $L$ being a set of labels, $E \\subseteq L \\times L$ being a set of edges forming a directed acyclic graph (though mostly a tree), and $p\\in L$ being a unique root node. The hierarchy $(L, E, p)$ satisfies: (1) there are no cycles, (2) each node except $p$ has at least one parent, and (3) $p$ has no parents.\nThis hierarchical structure distinguishes the task from flat multi-label classification, where labels have no structures to be exploited. The hierarchy can be either given as a"}, {"title": "3.2 Method selection", "content": "We analyzed 32 representative methods published between 2019 and 2024 across multiple domains. While an exhaustive review is beyond our scope, the selected methods encompass diverse approaches and variations, enabling us to identify common patterns and key differences, and to ultimately develop a unified framework.\nWe started with the recent surveys in Section 2 and collected the frequently cited papers, following their citations to find their most related predecessors. We further searched papers published after 2023 from Google Scholar using keywords \"hierarchical text classification\", \"multi-label text classification\", \"patent classification\", \"medical coding\" and \"extreme multi-label classification\". We also included papers that cited the benchmark datasets from Google Scholar and PapersWithCode. To limit the scope, we exclude methods published before 2019, unsupervised methods, and those primarily focused on non-textual datasets."}, {"title": "3.3 A unified framework of common submodules", "content": "From the surveyed methods, we abstract a framework constituted by commonly seen architectural components, shown as the nine submodules in Figure 1a. The framework begins with optional preprocessing steps: data augmentation (A) and long document handling (B). The core processing pipeline starts with parallel encoding of text and labels: a text encoder (C) transforms input text into embeddings using shallow, deep, or pre-trained models, while a label encoder (D) converts discrete labels into continuous representations, optionally incorporating hierarchical information. These representations are then integrated through text-label information fusion (E), which can occur at representational, architectural, or parameterization levels. For large label spaces, output space segmentation (F) may be employed for efficiency. The training process is guided by various training objectives (G), including classification losses and other custom objectives. Finally, the model makes inferences through score prediction (H), followed by optional prediction refinement (I) to enhance raw outputs using label correlations or ranking mechanisms.\nUtilizing our proposed framework, we locate each method\u00b3 by their specific instantiations of the submodules in the framework as shown in Figure 1b. We further emphasize that label hierarchy in the current problem setting can refer to either a given taxonomy or a learned structure, meaning that the usage of label hierarchy can be explicit (using the given taxonomy directly) or implicit (learning a label structure from the data via co-occurrence or other features). Since not all submodules in existing methods utilize label hierarchy information, we explicitly note such usage both in the following discussion and in Figure 1a."}, {"title": "A. Data Augmentation", "content": "Two methods address data imbalance through label information: REMEDIAL (Charte, Rivera, del Jesus, & Herrera, 2019) balances label distribution by decoupling frequent and rare label co-occurrences using the SCUMBLE metric (Charte, Rivera, del Jesus, & Herrera, 2014), while Gandalf (Kharbanda et al., 2024) leverages label co-occurrence graphs to generate soft targets as additional training data.\nLabel info utilization: Current methods only utilize label co-occurrence patterns."}, {"title": "B. Long document handling", "content": "While most methods simply truncate long documents, PLM-ICD (C.-W. Huang, Tsai, & Chen, 2022) employs a more sophisticated approach by segmenting text into smaller parts with pre-defined lengths for later information aggregation.\nLabel info utilization: Current methods do not explicitly use label information here."}, {"title": "C. Text encoder", "content": "This essential submodule transforms text into embeddings for input. It has evolved from shallow embeddings to deep learning models, and finally to pre-trained LLMs."}, {"title": "D. Label encoder", "content": "This submodule transforms discrete labels into embeddings. Methods vary from implicit relationship modeling (PIFA) to explicit structure encoding (GNNs), with LLM-based approaches offering a middle ground through semantic understanding."}, {"title": "E. Text-Label information fusion", "content": "This submodule integrates text and label information at three distinct levels. While simple concatenation is the most straightforward approach, learnable fusion mechanisms are more popular since they can adapt to different types of label hierarchies."}, {"title": "F. Output space segmentation", "content": "This submodule re-structures the prediction space, crucial for reducing huge label sets."}, {"title": "G. Training objectives", "content": "This submodule witnesses various loss functions and training strategies, from simple classification losses to sophisticated approaches that capture both semantic relationships and hierarchical structure."}, {"title": "H. Score Prediction", "content": "This submodule transforms model outputs into label predictions, showing different approaches to the task: basic linear layers treat each label as an independent binary decision, approaches with multiple classifiers each handle labels differently to capture their relationships, while similarity-based methods exploit learned embedding spaces."}, {"title": "I. Prediction refinement", "content": "This optional submodule enhances raw predictions through post-processing."}, {"title": "4 Cross-domain Evaluation", "content": "In this section, we conduct a comprehensive cross-domain evaluation by selecting representative datasets (Section 4.1) and methods (Section 4.2) from different domains. We evaluate each method's performance across all datasets using precision and recall metrics (Section 4.4), aiming to understand their generalization capabilities beyond their original domains. Section 4.5 briefly discusses the general trends and observations."}, {"title": "4.1 Datasets", "content": "We begin by introducing datasets from various domains that are widely used for hierarchical text classification. We present the key characteristics of the datasets and detail noteworthy data cleaning and preprocessing procedures to ensure fair comparisons."}, {"title": "4.1.1 Selection", "content": "We consider datasets from five domains for cross-domain evaluation: legal, scientific, news, medical, and patent classification, as these represent the most common and well-established applications of hierarchical text classification. Our selection criteria focus on datasets with gold-standard taxonomies, textual descriptions, and manageable label spaces (fewer than 4,000 labels), resulting in eight representative datasets: EurLex-3985, EurLex-DC-410, WOS-141, NYT-166, SciHTC-83, SciHTC-800, MIMIC3-3681, and USPTO2M-632. The naming format of the datasets is Dataset-N, where N is the number of labels."}, {"title": "4.1.2 Characteristics of datasets", "content": "To facilitate systematic comparison, we characterize each dataset along multiple dimensions: domain, document length statistics, label space properties (cardinality, hierarchy depth), and data distribution metrics (number of labels per sample, number of samples per label). Table 1 presents these detailed statistics."}, {"title": "4.1.3 Data preparation", "content": "While we maintain consistency with established preprocessing procedures where possible, several datasets required specific handling to ensure compatibility with our evaluation framework, and we also created new versions of some datasets for evaluation.\n1. EurLex-3956/3985: Introduced by Loza Menc\u00eda and F\u00fcrnkranz (2008) and known as EurLex-4k (Bhatia et al., 2016) in the literature, it contains European legal documents sourced from the EUR-Lex repository4.\n(a) Label processing: The original dataset is annotated with 3956 labels of EUROVOC descriptors, describing a wide range of EU-related topics. But the labels are raw text and not mapped to any existing taxonomy. We mapped them to the EUROVOC taxonomy using an ensemble of string similarity metrics6 and manual verification, resulting in 3890 mapped labels. We then enriched the labels by adding parent codes, yielding a final label set of size 3985. We maintain both versions: EurLex-3956 for literature comparison and EurLex-3985 as our cleaned version for evaluation.\n2. EurLex-DC-410: Using the original Eur Lex corpus, we created a new dataset annotated with the Directory Codes (DC). These codes represent classes used in the Directory of Community Legislation in Force. The hierarchy consists of 20 top-level chapter headings with up to four levels of subdivisions. After filtering documents to retain only those with valid DC annotations, the final dataset contains 410 labels.\n3. WOS-141: Introduced by Kowsari et al. (2017), it contains scientific article abstracts from the Web of Science. The labels represent a hierarchical taxonomy of scientific categories and subcategories, with a total number of 141. We obtained the data from the original source.\n4. NYT-166: Originating from the New York Times Annotated Corpus (Sandhaus, 2008), this dataset consists of news articles spanning diverse topics. The 166 labels form a hierarchical taxonomy representing thematic categories from general subjects down to more fine-grained topics.10\n5. SciHTC-83: Introduced by Sadat and Caragea (2022), this dataset comprises scientific abstracts from multiple research fields. The 83 labels represent a selection of the most frequently occurring subject categories, structured hierarchically to capture broad domains down to more specialized subfields. We use the same processed version provided by Sadat and Caragea (2022).\n6. SciHTC-800: We build SciHTC-800 from the original SciHTC dataset by adding the second-level codes and correcting the inconsistent codes, resulting in a dataset with 800 labels.\n7. MIMIC3-3681: The MIMIC-III clinical database (Johnson et al., 2016) consists of de-identified hospital discharge summaries annotated with International Classification of Diseases (ICD) codes. These codes span broad medical domains, covering both diagnoses and procedures. We use the MIMIC3-clean variant introduced by Edin et al. (2023), which provides a refined version of the dataset optimized for clinical classification tasks.\n8. USPTO2M-632: Introduced by S. Li, Hu, Cui, and Hu (2018), this dataset is derived from United States Patent and Trademark Office (USPTO) documents. It contains patent abstracts annotated with hierarchical patent classification codes, reflecting technological fields and subfields. We obtained the data from the original source11.\nData adaptation for methods across domains\nSince all datasets have given taxonomies, we handle them differently based on each method's requirements:"}, {"title": "4.2 Methods", "content": "We carefully select representative methods for cross-domain evaluation, focusing on those that effectively utilize or can be adapted to use label hierarchies. Our selection process balances comprehensive coverage with practical resource constraints."}, {"title": "4.2.1 Selection", "content": "Resource limitations prevent us from experimenting with all 32 methods surveyed above, therefore we select representative methods based on three criteria:\n1. Performance: We prioritize methods that achieved state-of-the-art results on their respective datasets, identified through recent surveys (Section 2) and 2024 publications.\n2. Label hierarchy utilization: We focus on methods that either integrate label hierarchical information into their main architecture or can be readily adapted to do so, excluding augmentation or post-processing components. Exception is made for PatentBERT/FlatBERT, which serves as a baseline model without label hierarchy information used anywhere.\n3. Implementation adaptability: For reproducibility and extensibility, we prioritize methods with clear architectures that are easy to implement and modify. We exclude ensemble methods to maintain architectural clarity, facilitate potential improvements, and manage computational resources."}, {"title": "4.2.2 Selected methods for benchmark", "content": "Based on these criteria, we selected eight representative methods covering 5 domains:\nLegal Domain: The current state-of-the-art on EurLex-3956, MatchXML (2024), extends XR-Transformer by incorporating advanced label encoding mechanisms. Its predecessor, XR-Transformer (2021), remains a widely-cited baseline for hierarchical text classification."}, {"title": "4.3 Evaluation Metrics", "content": "We evaluate the selected methods using precision@k and recall@k, the most widely used metrics for multi-label classification tasks. For a given instance, precision@k is the number of true positive labels in the top-k predictions divided by k, while recall@k is the number of true positive labels in the top-k predictions divided by the total number of true labels for that instance. These metrics are then averaged across all test instances. We also report the ranks of the methods on each dataset."}, {"title": "4.4 Results", "content": "We evaluate each method on each dataset. The precision@1 and recall@1 scores are presented in Tables 2 and 3, respectively, along with the ranks of each method on each dataset. Precision/Recall@3, 5, 8, 10 scores show similar patterns and are presented in Appendix B. General trends and high-level observations are discussed in the following Section 4.5, while in-depth analysis is provided later in Section 5.\nAll experiments were conducted using PyTorch (Paszke et al., 2019) on a single NVIDIA A40 GPU, including our re-implementations of FlatBERT and THMM. We maintained consistent dataset processing and train/test splits across all methods while adhering to each method's specific requirements. For each method-dataset combination, we conducted hyperparameter tuning using a validation set randomly sampled from the training data, with limited search ranges closely following the original papers. All reported results are averages across five random seeds."}, {"title": "4.5 General observations", "content": "Here, we report our high-level observations based on the main cross-domain results, focusing on patterns and insights that emerge from evaluating the methods in their original proposed forms. In the following Section 5, we will conduct more systematic analyses and ablation studies to examine the effects of specific design choices and further dissect the observed trends. Figure 2 visualizes three main trends:\nLabel space complexity strongly influences model performance, as shown in Fig. 2(a). Methods achieve higher scores on datasets with fewer labels (under 500), while performance declines as label count increases. Additionally, as seen from Tables 1 and 3, high per-sample label cardinality (MIMIC3-3681 with 15.6 labels/sample and EurLex-3985 with 12.8 labels/sample) presents challenges, with methods generally achieving lower recall scores on such datasets.\nModel architecture plays a crucial role, as illustrated in Fig. 2(b). Models using large language models as text encoders consistently outperform others, particularly when incorporating advanced learning strategies such as contrastive losses or regularization terms in their training objectives.\nText-label interaction mechanisms prove important, demonstrated in Fig. 2(c). Methods that effectively combine text and label information through sophisticated mechanisms, such as embedding alignment or label-aware attention, show marked improvements over simpler approaches."}, {"title": "5 Analysis & Lessons", "content": "Our cross-domain evaluation reveals several key insights about hierarchical text classification methods. To better understand these findings, we conduct in-depth analysis and extensive additional experiments investigating: the surprising effectiveness of methods outside their original domains (Section 5.1), the potential of combining submodules across domains (Section 5.2), and the impact of various design choices through controlled experiments, including domain-specific language models (Section 5.3), document length handling strategies (Section 5.4), training data size variations (Section"}, {"title": "5.1 State-of-the-art performance often comes from other domains", "content": "Our cross-domain evaluation reveals interesting patterns in how methods perform beyond their original domains as shown in Table 4:\nWOS-141: HILL maintains its leading position, but the top three performers now include PLM-ICD, originally designed for medical coding. NYT-166: Methods from the legal domain, XR-Transformer and MatchXML, have surpassed the previous leader, HILL. PLM-ICD from the medical domain also ranks among the top performers. EurLex-*: MatchXML and XR-Transformer continue to excel in their native legal domain. Notably, PLM-ICD, from the medical domain, consistently ranks third, indicating that document handling techniques from medical texts can enhance legal document classification. SciHTC-*: The original leader, HR-SciBERT-mt, has been overtaken by methods from other domains, including MatchXML and XR-Transformer from the legal domain, THMM from the patent domain. MIMIC3-3681: PLM-ICD retains its top status in the medical domain, while legal domain methods, XR-Transformer and MatchXML, achieve competitive performance. USPTO2M-632: MatchXML, a method from the legal domain, surpasses the previous leader, THMM."}, {"title": "Hybrid dominance in rankings", "content": "The top-3 performers for each dataset all span multiple domains, suggesting that effective hierarchical text classification strategies are often domain-agnostic. Methods originally designed for extreme multi-label classification (MatchXML, XR-Transformer) show particularly strong cross-domain generalization, while medical domain innovations in document handling (PLM-ICD) prove valuable across various technical domains."}, {"title": "Dataset characteristics matter more than domain specificity", "content": "To understand what truly drives model performance, we analyzed correlations between dataset features (e.g., document length, labels per sample, training size) and model performance metrics (mean, max, and min precision@1 across models). Figure 3 shows correlations with absolute values above 0.3.\nThe results show interesting patterns. Document length shows a strong divergent effect (r = +0.535 for max precision@1, r = -0.657 for min precision@1). Advanced models excel with longer documents, while baseline models perform poorly. Similarly, label size shows divergent effect (r = +0.329 for max precision@1, r = -0.423 for min precision@1). Per-sample label cardinality demonstrates consistent positive impact for above-average models. The average, maximum, and minimum number of labels per sample positively correlate with maximum precision@1 and mean precision@1. Label density significantly benefits weak models. The average, maximum and minimum number of samples per label positively correlate with minimum precision@1 (r = 0.549 between avg samples per label and min precision@1). Hierarchical depth negatively impacts top performers (r = -0.410 for max precision@1).\nThese findings challenge the common practice of developing and evaluating methods within single domains. It also highlights the need for thorough cross-domain testing and unified evaluation to improve hierarchical text classification."}, {"title": "5.2 Combining submodules across domains creates new and better models.", "content": "Both dataset characteristics and limitations of existing methods can guide the combination of submodules across domains to create new and better models. We showcase one such example here."}, {"title": "5.3 Domain specific LLMs are beneficial, especially for simpler models and low-resource settings", "content": "Among methods using language models as text encoders, most use bert-base-uncased (Devlin, 2018) by default, while PLM-ICD and THMM employ domain-specific models (ROBERTa-PM (Lewis et al., 2020) and SciBERT (Beltagy, Lo, & Cohan, 2019)) on their respective domains, following the original papers. Given PLM-ICD's superior performance on MIMIC3-3681, we investigate whether this advantage comes from its architecture or its domain-specific encoder, as well as the impact of domain-specific LLMs on other methods, with results shown in Figure 5.\nAll methods using domain-specific LLMs outperform the default bert-base-uncased encoder, and the performance gap is more significant for FlatBERT and MIMIC3-3681. Two conclusions can be drawn: (1) simpler architectures like FlatBERT benefit more from careful language model selection than complex ones, and (2) the impact of language model choice may diminish with larger training datasets, as evidenced by the smaller performance changes on USPTO2M-632.\nPLM-ICD's performance drops significantly when switching from ROBERTa-PM to BERT on MIMIC3-3681, losing its status as state of the art. This indicates that the domain-specific encoder plays a crucial role in its superior performance, while using SciBERT on USPTO2M-632 shows slight improvements.\nA surprising new state of the art with a 'mismatched' cross-domain LLM\nIn a surprising cross-domain application, using ROBERTa-PM (a medical domain LLM) for PLM-ICD on USPTO2M-632 (a patent dataset) improves performance (80.73/54.45 to 82.83/55.92) even more than using SciBERT (a scientific domain LLM), surpassing the original SOTA from MatchXML. This counter-intuitive success of using a medical LLM for patent classification suggests the improvement may stem from ROBERTa-PM's larger vocabulary rather than domain-specific knowledge. This finding hints at potential benefits of cross-domain LLM applications. Nonetheless, the confirmation of this hypothesis is left as future work."}, {"title": "5.4 Long document handling is crucial for medical text", "content": "Text encoders generally face challenges with long documents, whether due to memory constraints in RNNs or token length limitations in pretrained language models (e.g., 512 tokens for BERT-family models). While most methods simply truncate documents to handle these limitations, our experiments on MIMIC3-3681 reveal the significant impact of long document handling strategies on model performance, shown in Table 6.\nPLM-ICD's sophisticated handling of full documents proves beneficial, while our attempt to enhance XR-Transformer beyond its default truncation by averaging embeddings across 512-token segments actually hurts performance. This counter-intuitive result suggests that averaging across document segments may dilute important signals, and effective long document strategies need careful design, potentially incorporating domain knowledge and label-aware mechanisms."}, {"title": "5.5 Simple models are good enough sometimes, but more easily affected by the data size", "content": "Our cross-domain evaluation shows that FlatBERT performs poorly on most datasets but achieves competitive results on USPTO2M-632. We hypothesize this exception is due to USPTO2M's large training set (1.9M samples), which compensates for Flat-BERT's simple architecture. To test this hypothesis, we created two smaller versions of the dataset: USPTO100k-632 with 100k samples and USPTO10k-632 with 10k"}, {"title": "5.6 Prior knowledge of label hierarchy for Probabilistic Label Tree is somewhat beneficial", "content": "We investigate the impact of different initialization methods for the Probabilistic Label Tree (PLT) in XR-Transformer, which controls how the output space is initially segmented. We compare three approaches: PIFA (the original method using hierarchical clustering based on training samples), Gold (using the ground-truth taxonomy tree), and random initialization. Our experiments show minimal differences between PIFA and Gold, though random initialization leads to decreased performance (see precision and recall details in Tables B10 and B11). These results suggest that while prior knowledge of label hierarchies may provide some benefit, the advantage may be modest."}, {"title": "6 Conclusion", "content": "This paper proposes a unified framework for hierarchical multi-label text classification and conducts a comprehensive cross-domain analysis of state-of-the-art methods. Our evaluation reveals several important insights. First, top performance often comes from methods developed for other domains, challenging the common practice of domain-specific method development. This is evidenced by our achievement of new state-of-the-art results on NYT-166, SciHTC-83, and USPTO2M-632 using methods from other domains. Second, the effectiveness of methods depends more on dataset characteristics (e.g., label patterns, document length, training size) than on the domain it comes from. Third, combining architectural innovations from different domains can create more robust models and even new state-of-the-art results, as demonstrated by our experiments in Sec. 5.2.\nOther key findings include: domain-specific language models significantly improve performance, especially for simpler architectures and challenging datasets; sophisticated document handling is crucial for long texts like medical records; simple architectures can achieve competitive performance but require substantial training data; and prior knowledge of label hierarchies provides modest benefits.\nOur study has several limitations that highlight opportunities for future research. On the resource side, computational constraints limited our evaluation to a subset of available methods and datasets, potentially missing valuable insights from other approaches. This also affected our hyperparameter optimization, possibly underestimating some methods' full potential. Our unified evaluation framework, while enabling fair comparisons across methods, may yield results that differ from those reported in original papers due to standardized preprocessing and evaluation settings. Regarding methodological scope, our focus on end-to-end architectures excluded standalone components like data augmentation or post-processing methods that could provide complementary benefits. Additionally, methods for semi-supervised, unsupervised, or zero-shot approaches remain to be explored.\nSeveral promising research directions emerge from our findings. First, the success of combining PLM-ICD with Label2Vec suggests potential in systematically exploring other cross-domain architectural combinations. Second, the significant impact of document handling strategies on medical texts indicates a need for more sophisticated approaches that can effectively process long documents while maintaining computational efficiency. Third, developing methods that can maintain performance with limited training data or computational resources would increase practical applicability across domains. Fourth, recent advances in large language models suggest opportunities for effective zero-shot and few-shot classification with minimal domain-specific training data. Lastly, extending evaluation to emerging domains like skill tagging (N. Li, Kang, & De Bie, 2023a, 2023b) could reveal additional insights about method generalization and domain-specific challenges.\nLooking ahead, the key take-away of our study is that it is valuable for researchers and practitioners to consider possibilities beyond domain boundaries in hierarchical text classification. Rather than defaulting to domain-specific solutions, we should explore methods that have succeeded elsewhere. This shift in approach could accelerate progress and lead to more robust and effective classification systems across all domains."}]}