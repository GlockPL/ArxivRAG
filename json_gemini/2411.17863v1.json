{"title": "LongKey: Keyphrase Extraction for Long Documents", "authors": ["Jeovane Honorio Alves", "Radu State", "Cinthia Obladen de Almendra Freitas", "Jean Paul Barddal"], "abstract": "In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents. In this paper, we introduce LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKey's versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Efficient extraction of vital information from textual documents across diverse domains is essential for effective information retrieval, especially given the vast volume of data on the internet and within organizational datasets. In response to this need, Keyphrase Extraction (KPE) aims to identify representative keyphrases that enhance document comprehension, retrieval, and information management [1], [2].\nA keyword encapsulates the central theme or a distinct element of a document's subject matter. When multiple words are used, this term is referred to as a keyphrase. In practice, the terms keyword and keyphrase are often used interchangeably. This paper adopts this convention, treating keyword and keyphrase extraction as synonymous, applicable to terms of any length [3].\nKeyphrase extraction techniques are commonly categorized based on their underlying principles [3]. For example, unsupervised methods like TF-IDF [4] calculate term importance based on term frequency within a document and across the corpus. RAKE [5] assesses word relevance through co-occurrence ratios, while TextRank [6] uses a graph-based structure to measure word strength and similarity. KeyBERT [7], unlike unsupervised methods, employs supervised learning with pre-trained BERT embeddings [8] and cosine similarity to determine importance and relevance. PatternRank [9] is similar to KeyBERT, yet it uses a part-of-speech (POS) module to reduce the number of keyphrase candidates evaluated.\nA recent relevant work in keyphrase extraction is Joint-KPE [10], which finetunes a BERT model for keyphrase extraction based on two strategies: global informativeness and keyphrase chunking. Different algorithms served as baselines. ChunkKPE only uses keyphrase chunking as its strategy. Likewise, RankKPE uses only global informativeness as its strategy. TagKPE considers a five-tagging approach to facilitate n-grams extraction. And then SpanKPE, which employs a span self-attention mechanism.\nHyperMatch, a new hyperbolic matching model proposed in [11], advances keyphrase extraction beyond Euclidean space, evaluating the relevance of keyphrase candidates using the Poincar\u00e9 distance. The authors also combine intermediate layers of the RoBERTa [12] model through an adaptive mixing layer to enhance representation. Aimed in long-context documents, GELF [13] is based on graph-enhanced sequence tagging, using the Longformer [14] encoder. The authors constructed a text co-occurrence graph and utilized a graph convolutional network (GCN), focusing on edge prediction, to augment Longformer model embeddings.\nAlthough KPE is a powerful tool, most research has focused on short-context documents, such as abstracts and news articles. While many methods focus on short texts, challenges remain for longer documents. These challenges encompass diverse content structures, increased syntactic complexity, varying contexts within the same document, and limited compatibility with long-context language models. Addressing these intricacies demands developing advanced approaches explicitly tailored for the nuances of handling long-context data [2], [15].\nTo address these challenges, in this paper, we present LongKey\u00b9, a novel framework that extends keyphrase extraction to long documents through two key contributions. First, LongKey expands token support for encoder models like Longformer, capable of processing up to 96K tokens, ideal for inference on lengthy documents. Second, it introduces a new strategy for keyphrase candidate embedding that captures and"}, {"title": "II. PROPOSED APPROACH", "content": "Our proposed methodology, dubbed LongKey, is outlined in this section. LongKey operates considering three stages: initial word embedding, keyphrase candidate embedding, and candidate scoring, as shown in Figure 1. Each stage is designed to refine the selection and evaluation of keyphrases.\nTo generate embeddings for long-context documents, our proposal uses the Longformer model [14]. Longformer is an encoder-type language model that uniquely supports extended contexts through two innovative mechanisms: a sliding local windowed attention with a default span of 512 tokens and task-specific global attention mechanism.\nBy default, each of the model's twelve attention layers produces an output embedding size of 768. Furthermore, Longformer has a positional embedding size of 4,096. We extended it to 8,192 by duplicating the same weights to the next 4,096 elements. For global attention, preliminary experiments have demonstrated optimal results by designating the initial token ([CLS]) as the token for global attention, i.e., the token that attends to every document token and vice-versa.\nFirst, a tokenizer converts the input document to a numeric representation. Our approach uses the Longformer model as the encoder, with tokens defined by the RoBERTa [12] tokenizer. This token representation is then processed by Longformer to generate embeddings, capturing the contextual details of each token within the document.\nEven with Longformer, processing of large documents would not be possible with our current computational resources if they are not chunked. Therefore, documents larger than 8K tokens are split in equally sized chunks (with a maximum size of 8192 tokens). Each document is divided into chunks for processing by Longformer, and its embeddings are concatenated to create a unified representation of the entire text's tokens.\nGiven a document $D = \\{w_1,..., w_i,..., w_N \\}$ containing N words, we use an encoder-type model to generate the token embeddings $E^T$:\n$E^T = \\text{Encoder}(\\{w_1, ..., w_i, ..., w_N \\}).$ (1)\nThe resulted operation can be represented as follows:\n$E^T = \\{\\epsilon_{1,1}, \\epsilon_{1,2},..., \\epsilon_{1,M_1}, \\epsilon_{2,1} ..., \\epsilon_{i,j},...,\\epsilon_{N,M_N}\\},$ (2)\nwhere $\\epsilon_{i,j}$ represents the embeddings of the jth token from the ith word in document D. Each embedding $\\epsilon_{i,j}$ has a size of 768, which is omitted in the explanation for better clarity. If N > 8192, D is grouped in chunks which are processed separately and the resulting token embeddings are concatenated together.\nKeyphrase embeddings are context-sensitive, meaning the same keyphrase can yield different embeddings based on its surrounding textual environment. Once these embeddings are crafted, they are combined into unique embeddings for each keyphrase candidate, taking into account the document's overarching thematic and semantic landscape.\nSince a specific word may contain more than one token, it's necessary to create a single embedding for this word. Like JointKPE and other similar methods, we used only the first token embeddings to represent the word, since there was no significant difference between this strategy and other simple combinations evaluated, thus reducing computational calculation. These word embeddings are used as the input of our keyphrase embedding module.\nGiven the token embeddings $E^T$, the word embeddings are given by preserving only the first token embeddings for each word, given as follows:\n$E^W = \\{\\epsilon_{1,1}, \\epsilon_{2,1}..., \\epsilon_{i,1},..., \\epsilon_{N,1}\\},$ (3)\nwhich, for simplicity, we can omit the token index j. Then, we employ a convolutional network to construct embeddings for each potential n-gram keyphrase. For n-grams up to a predetermined maximum length, e.g., n = 5, we use n distinct 1-D convolutional layers, each with a kernel size k corresponding to its n-gram size (i.e., k = n), ranging from [1,n], and no padding, to generate the keyphrase embeddings from the pre-generated word embeddings. The n-gram representation of the keyphrase occurrence from words $w_i$ to $w_{i+k-1}$ is given by the convolutional module with kernel size k is calculated as follows:\n$h_{i:k} = \\text{CNN}_k(\\{\\epsilon_i,..., \\epsilon_{i+k-1}\\}),$ (4)\nwhere H, the set of keyphrase embeddings, can be represented as:\n$H = \\{h_{1:1}, h_{2:1}, ..., h_{1:2}, ..., h_{i:k},...,h_{N-k,n}\\}.$ (5)\nThe convolutional module generates embeddings for each keyphrase occurrence in the text. To capture the relevance of each keyphrase across the document, LongKey uses a keyphrase embedding pooler that combines all occurrences of a keyphrase candidate into a single, comprehensive representation. This approach helps emphasize the most contextually significant keyphrases. A computationally efficient max pooling operation aggregates the diverse embeddings of the keyphrase candidate's occurrences from various text locations into a singular, comprehensive representation. Given $KP_n$ as the set the unique possible keyphrases found in D with maximum size of n words"}, {"title": "B. Keyphrase Embedding", "content": "$KP^n = \\{kp_1,kp_2,..., kp_i,...,kp_M\\},$ (6)\nwhere M is the number of unique keyphrases found in D, from unigrams to n-grams, the embeddings of every occurrence of $kp_i$ are defined as follows:\n$H^{KP_i} = \\{h_{i:k} \\in H \\text{ where } W_{i:k} \\epsilon KP_i\\},$ (7)\nthus, for simplicity, $H^{KP_i}$ can be also represented as:\n$H^{KP_i} = \\{h_1,h_2,...,h_i,..., h_{S^i}\\},$ (8)\nwhere $S^i$ is the number of occurrences in the document for a specific KP. To generate the candidate embeddings $C^l$ for the unique keyphrase l, a max pooling is employed as follows:\n$C^l = \\text{max}(\\{h_1, h_2, ..., h_i,..., h_{S^l}\\}) .$ (9)\nAn overall presentation of the candidate embedding calculation is shown in the bottom-left part of Figure 1. For a clear explanation, we show an illustration with the embedding size of 1. This calculation is employed separately for each embedding position. In practice, for each keyphrase candidate, we select its occurrences and get the maximum value. In our example, the keyphrase candidate has three occurrences with values (3, -3,4) in position j = 0. After max pooling, the value for this candidate in the position j = 0 is $C'_{i=0} = 4$. Even though this illustration only has integers, floating-point numbers are employed."}, {"title": "C. Candidate Scoring", "content": "In the LongKey approach, candidate embeddings are each assigned a ranking score, with higher scores indicating keyphrases that more accurately represent the document's content. LongKey fine-tunes its performance during training by optimizing ranking and chunking losses, aligning closely with ground-truth keyphrases to ensure relevance. For both losses, ground-truth keyphrases are positive samples. Remaining instances are considered as negative samples.\nTo generate the scores for both ranking and chunking parts, we employ linear layers for different inputs. For the ranking score, we use the candidate embeddings as the input of a linear layer which converts the embedding to a single value (i.e., ranking score). Given $C^l$ as the embedding of candidate keyphrase l, we calculate the ranking score as follows:\n$S_{\\text{rank}}^l = \\text{Linear}_{\\text{rank}}(C^l)$ (10)\nUnlike JointKPE, which might assign multiple scores to a single keyphrase candidate based on its occurrences, LongKey assigns a singular score per candidate, facilitated by the efficient proposed keyphrase embedding pooler.\nEach candidate's score is then optimized through Margin Ranking loss, enhancing the distinction between positive y+ and negative y samples by elevating the scores of the true keyphrases. This loss is defined as follows:\n$MR_{\\text{loss}}(S_{\\text{rank}}^+, S_{\\text{rank}}^-) = \\text{max}(0, -s_{\\text{rank}}^+ + S_{\\text{rank}}^- + 1)$ (11)\nAs for the chunking score, we use the keyphrase embeddings as the input of a linear layer. Given $H^i$ as the embedding of a keyphrase i, we calculate the chunking score as follows:\n$S_{\\text{chunk}}^i = \\text{Linear}_{\\text{chunk}}(H^i)$ (12)\nOne thing to note is that LongKey maintains the same objective as JointKPE for keyphrase chunking, utilizing binary classification optimized with Cross-Entropy loss. Given a probability $p^+$\n$p^+ = \\text{Softmax}(s_{\\text{chunk}})^+,$ (13)\nrepresenting the likelihood of a sample belonging to the positive class, and z, the actual binary class label of the sample (where 1 indicates positive and 0 indicates negative), the BCE loss is calculated using the formula:\n$BCE_{\\text{loss}} = -[z \\text{log}(p^+) + (1 - z) \\text{ log}(1 - p^+)]$ (14)\nBoth losses are added together and jointly optimized across model training, similar to JointKPE. The formula is given as follows:\n$LongKey_{\\text{loss}} = MR_{\\text{loss}} + BCE_{\\text{loss}}$ (15)\nHowever, distinctively, LongKey diverges from JointKPE in the objectives of its loss functions. Regarding the ranking loss function, LongKey is specifically designed to refine the embeddings of keyphrase candidates, in contrast to JointKPE's focus on optimizing the embeddings of individual keyphrase instances, thereby enhancing the model's overall precision and contextual sensitivity in keyphrase extraction."}, {"title": "III. EXPERIMENTAL SETUP", "content": "This section outlines the empirical evaluation of the LongKey method, providing a comprehensive overview of the experimental datasets and the specific configurations underpinning our analysis.\nRobust and large datasets must be employed to train language models and evaluate the capability of an approach in extracting relevant keyphrases from an input document. Many large datasets typically only contain the title and abstract of scientific papers. They are sub-optimal in evaluating long context-based keyphrase extractors since they generally have samples with less than 512 tokens.\nDue to the scarcity of datasets containing a high volume of lengthy documents, the Long Document Keyphrase Identification Dataset (LDKP) was formulated specifically for extracting keyphrases from full-text papers (which generally surpass 512 tokens) [15]. LDKP has two datasets:\nLDKP3K: A variation of the KP20K dataset [16], which contains approximately 100 thousand samples and an average of 6027 words per document.\nLDKP10K: A variation of the OAGKx dataset [17], containing more than 1.3M documents, averaging 4384 words per sample.\nOther datasets are employed in a zero-shot fashion, i.e., inference only, to assess the capability of different methods trained on both datasets to adapt to different domains and patterns. These datasets are the following:\nKrapivin [18]: Features 2,304 full scientific papers from the computer science domain published by ACM.\nSemEval2010 [19]: Comprises 244 ACM scientific papers across four distinct sub-domains: distributed systems; information search and retrieval; distributed artificial intelligence multiagent systems; social and behavioral sciences economics.\nNUS [20]: This dataset contains 211 scientific conference papers with keyphrases annotated by student volunteers, offering a unique perspective on keyphrase relevance.\nFAO780 [21]: With 780 documents from the agricultural sector labeled by FAO staff using the AGROVOC thesaurus, this dataset tests the models' performance on domain-specific terminology.\nNLM500 [22]: This collection of 500 biomedical papers, annotated with terms from the MeSH thesaurus, assesses the methods' capability in the biomedical domain.\nTMC [23]: Including 281 chat logs related to child grooming from the Perverted Justice project, this dataset, with documents and keyphrases based on the formatting from [24], introduces the challenge of informal text and sensitive content.\nAlthough the focus is on long-context documents, it's possible to use the evaluated methods on short documents. To assess the effectiveness of the models trained on the LDKP datasets, we evaluate them on two of the most popular short-context datasets: KP20k and OpenKP:\nKP20k [16]: Highly correlated with the LDKP3K dataset, the KP20k is a dataset containing more than 500 thousand abstracts of scientific papers (20 thousand of abstracts for the validation and test subsets each).\nOpenKP [25]: The OpenKeyPhrase (OpenKP) is a popular short-context dataset containing more than 140 thousand of real-world web documents, where their keyphrases were human-annotated."}, {"title": "B. Experimental Settings", "content": "Our experiments utilized two NVIDIA RTX 3090 GPUs, with 24GB VRAM each. The training regimen was guided by the AdamW optimizer, combined with a cosine annealing learning rate scheduler, with a learning rate value of 5 \u00d7 10-5, and warm-up for the initial 10% training iterations. To circumvent VRAM constraints, we employed gradient accumulation, achieving an effective batch size of 16 in the training phase.\nTo maintain clarity and consistency in our reporting, we use the terms \u201citerations\u201d and \u201cgradient updates\u201d interchangeably.\nWe set a maximum token limit of 8,192 during training to accommodate the length of the documents within our available computational resources. The positional embedding was expanded to 8,192, duplicating the original size used by Longformer, which enhances support for longer chunks in inference mode (tested up to 96K in total). We limit keyphrases to a maximum of five words (k = [1,5]) to maintain computational efficiency and align with standard practices in keyphrase extraction. In the evaluation, longer ground-truth keyphrases are considered as false negatives. Moreover, models were trained on LDKP3K for 25 thousand iterations. Since LDKP10K had a substantially higher number of samples, we trained it for 78,125 iterations (i.e., almost an entire epoch). We also evaluated some methods with the BERT model, where we also used chunking to extend training to 8,192 tokens.\nTo maintain consistency in our analysis and ensure fair comparisons, we used the Longformer model for all supervised approaches that are encoder-based and fine-tuned on the LDKP datasets. Moreover, we employed the same global attention mask as used in LongKey.\nModel performance was quantitatively assessed using the F1-score, the harmonic mean between precision and recall, for the most significant K keyphrase candidates (F1@K), with K's value determined based on the overall average of keyphrases per document in each dataset, also following choices of related works, e.g., [26]. Given\n$\\hat{Y} = [\\hat{y}_1, \\hat{y}_2, \u2026\u2026\u2026, \\hat{y}_M]$ (16)\nas the predicted keyphrases sorted by their ranking scores in a decreasing order, and Y as the ground-truth keyphrases of a given document (with no specific order), we can calculate the F1-score and its intermediary metrics, i.e., precision and recall; using the top-K predicted keyphrases, given by\n$Y:k = [\\hat{Y}_1, \\hat{Y}_2,..., \\hat{Y}_{min(K,M)}].$ (17)\nWe calculate the intermediary metrics as follows:\n$\\text{Precision}@K = \\frac{|\\hat{Y}:k \\cap Y|}{|\\hat{Y}:k|}, \\text{Recall}@K = \\frac{|\\hat{Y}:k \\cap Y|}{|Y|},$ (18)\nthen, with these two metrics, we calculate the Fl-score at the top-K keyphrases as follows:\n$F1@K = 2 \\times \\frac{\\text{Precision}@K \\times \\text{Recall}@K}{\\text{Precision}@K + \\text{Recall}@K}$ (19)\nAnother relevant metric, proposed in [27], is a variation of the F1@K defined as F1@O. Here, O is the number of ground-truth keyphrases (i.e., oracle), thus K = |Y|, which is dynamically calculated depending on the document."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section, we delve into the performance outcomes on two primary datasets, extending our analysis to encompass zero-shot learning scenarios and domain-shift adaptability. Moreover, we unravel the contribution of the keyphrase embedding pooler, performance estimation, and inference on short-context documents."}, {"title": "A. LDKP Datasets", "content": "Table I presents the comparative results on the LDKP3K test subset, encompassing both unsupervised methods and models finetuned on the LDKP3K and LDKP10K training subsets. It's noteworthy that, aside from GELF, a standard benchmark model, all fine-tuned methods are tailored adaptations designed to handle extensive texts, utilizing the BERT (only when trained on LDKP3K) and Longformer architecture for enhanced context processing. Our approach was also evaluated without chunking, i.e., max of 8192 tokens, identified as LongKey8K.\nAmong the evaluated methods, LongKey8K emerged as the best, achieving an F1@5 of 39.55% and F1@O of 41.84%. Remarkably, even under a domain shift when trained on the broader LDKP10K dataset, which includes a more comprehensive array of topics beyond computer science, LongKey maintained its lead with an F1@5 of 31.94% and F1@ of 32.57%.\nPerformance metrics on the LDKP10K test subset are also provided in Table I, where LongKey emerges as the leading method, achieving an F1@5 of 41.81%.\nWhile LongKey trained on the LDKP3K dataset outperformed other models trained on the same dataset, it scored significantly lower when compared to its performance on the LDKP10K dataset, indicative of dataset-specific variations in effectiveness. This discrepancy, especially the reduced efficacy on the LDKP10K subset, could be attributed to the significant skew towards computer science papers within the LDKP3K dataset, as detailed in the LDKP study.\nGenerally, the evaluated methods had superior F1@O than F1s at specific Ks, suggesting that, for the LDKP datasets, ground-truth keyphrases were ranked higher in prediction."}, {"title": "B. Unseen Datasets", "content": "Without any finetuning on their respective data, LongKey and related methods were evaluated across six diverse domains. Remarkably, LongKey outperformed other methods in nearly all tested datasets, with the exception of SemEval2010 and TMC where its results were slightly below the top performers (HyperMatch and RankK\u03a1\u0395, respectively).\nThe choice of LDKP training dataset-LDKP3K or LDKP10K-significantly influenced performance across the unseen datasets, with LDKP3K-trained models excelling in every dataset with the exception of the NLM500 dataset. Although LDKP10K had broader areas of study, LDPK3K had overall longer samples, with an average of 6,027 words per document against an average of 4,384 words in the LDKP10K. Further studies are encouraged to assess the influence of study areas and sample size.\nAnother thing to note is that, for the unseen datasets, there was a balance dispute between BERT and Longformer-based methods as the best one, even for LongKey. Although access the robustness of BERT with a chunking approach, it also show room for improvements regarding long-context encoders.\nOverall, LongKey achieved consistently high scores across different encoder models, while JointKPE's performance was more variable. Notably, LongKey's Longformer model performed better on longer documents, while the BERT model maintained more balanced results across various lengths. Additionally, LongKey showed particularly strong results for documents between 512 and 1024 tokens, suggesting potential areas for optimization when handling even longer documents.\nOverall, LongKey's robustness was evident as it consistently outperformed other models in nearly all benchmarks, showcasing its broad applicability and strength in keyphrase extraction across varied domains."}, {"title": "C. Component Analysis", "content": "To assess the keyphrase embedding pooler (KEP) contribution, we undertook a component analysis using the LDKP3K validation subset. This analysis involved evaluating the LongKey approach with different aggregation functions, i.e., average, sum and maximum; but also the improvement obtained compared to the JointKPE approach.\nWe used the configuration outlined in the experimental settings, with each model configuration undergoing 12,500 iterations. Table IV shows each configuration's average and standard deviation results that were computed considering five runs per method.\nOverall JointKPE F1@5 score was around 36% with a high std dev. of 0.50%. Using the KEP proposed in LongKey, but with the average reduction, significantly impaired performance, resulting in an F1@5 score of 29.15%, but with a std dev. of 0.23%, lower than JointKPE. Using the summation aggregator improved F1@5 a little (32.76% \u00b1 0.20), but still inferior to JointKPE.\nThe best F1@5 score was obtained using max pooling,"}, {"title": "D. Performance Evaluation", "content": "We also evaluate the performance of each method in inference. We calculate the performance for each dataset based on the number of processed documents per second using a single RTX 3090. The overall results can be seen in Table V.\nAs we can see, LongKey performed slightly inferior to the supervised methods. This was basically caused by the keyphrase embedding pooler. However, this performance loss is minor compared with how much the overall F1 increased with the proposed module. Robust approaches with as little bottleneck as possible are encouraged. Also, though in some cases BERT-based methods had inferior results, they have a little boost in performance in comparison with Longformer-based."}, {"title": "E. Short Documents", "content": "In Table VI, we show the results of the evaluated methods in two short-context datasets, KP20k and OpenKP. Three methods were generally competitive: RankKPE, JointKPE, and LongKey. Overall, JointKPE was superior on the KP20k (which was originally developed using it). Since KP20k has a high correlation with LDKP3K, better results are expected in models trained with the latter.\nFor the OpenKP, models trained on the LDKP10K were generally better, especially RankKPE. Here, SpanKPE also had results similar to those of the other three. Overall, LongKey improvements on long-context datasets (except the TMC dataset, which has a quite different domain) are not seen in short-context documents. These improvements should be related to the proposed keyphrase embedding pooler. Still, LongKey may also be more biased toward long-context documents, which were not generally seen in the training datasets. Further experiments should be employed, increasing length and content variability in the training stage, to evaluate the capabilities of the keyphrase embedding pooler."}, {"title": "V. CONCLUSION", "content": "Automatic keyphrase extraction is crucial for summarizing and navigating the vast content within documents. Yet, prevalent methods fail to analyze long-context texts like books and technical reports comprehensively. To bridge this gap, we introduce LongKey, a novel keyphrase extraction framework specifically designed for the intricacies of extensive documents. LongKey's robustness stems from its innovative architecture, which is specifically designed for long-form content and rigorously validated on extensive datasets crafted for long-context documents.\nTo validate its efficacy, we conducted a simple component analysis and further assessments of the LDKP datasets, followed by testing across six diverse and previously unseen long-context datasets and two short-context datasets. The empirical results highlight LongKey's capability in long-context K\u03a1\u0395, setting a new benchmark for the field and broadening the horizon for its application across extensive textual domains.\nSelecting the appropriate LDKP training dataset was crucial for LongKey's performance on unseen data, highlighting the need for strategic modifications to improve generalization without sacrificing the effectiveness of keyphrase extraction. Slightly inferior results in the short-context datasets also indicate the necessity of improvements for a better generalization. Furthermore, the restriction on the maximum number of words per keyphrase inherently focuses the method on extracting keyphrases of specific lengths. Further adjustments to accommodate longer keyphrases should be explored, as simply increasing keyphrase length may not improve results without careful evaluation. Although this is a common pattern in K\u03a1\u0395 methods, future work must carefully consider the impact of different keyphrase lengths on overall performance.\nAdditionally, the context size limitation to 8K tokens \u2014 and similarly sized chunks during inference \u2014 may restrict LongKey's ability (through not restricted only to our approach) to fully capture and process extensive document content. However, any plans to expand this limit must carefully balance the increased computational demands with available resources.\nIn summary, LongKey sets a new benchmark in keyphrase extraction for long documents, combining adaptability with high accuracy across various domains. Its superior embedding strategy contributes to its effectiveness, suggesting significant potential for enhancing document indexing, summarization, and retrieval in diverse real-world contexts."}]}