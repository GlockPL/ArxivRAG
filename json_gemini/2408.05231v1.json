{"title": "Predictive maintenance solution for industrial systems - an unsupervised approach based on log periodic power law", "authors": ["Bogdan \u0141obodzi\u0144ski"], "abstract": "A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.", "sections": [{"title": "I. INTRODUCTION", "content": "Detecting the symptoms of a failure and predicting when it will occur, in multivariate or univariate time series collected via the Internet of Things (IOT), is central to the concept of predictive maintenance (PM), which is now used in almost every area of industry. PM allows a company to better prepare for a potential failure by redesigning the production process in advance or creating a workaround when shutdown is not possible, and minimizes costs associated with standard maintenance operations by reducing them through predictive engineering.\nPredicting failures, provided by PM can be very profitable for a company, under the condition that PM minimizes the number of false warnings (false positives) and maximizes the number of correctly predicted events (true positives). Creating a properly working PM process faces two main problems:\n1. related to the definition of what is a failure in the considered technological process or in the IoT data,\n2. the development or the application of the best algorithm (based on physical description, machine learning, or statistical methods) to the data being analyzed.\nWhile the theoretical definition of failure is well defined, in practice problems are encountered with its implementation. For various reasons (economic, production, etc.) not every failure requires corrective actions. Sometimes, a minor failure is not a good reason to stop a monitored machinery or a production process. This imprecise description of failure leads to substantial difficulties when attempting to use supervised methods to build a correct Predictive Maintenance process. This suggests that unsupervised approaches may prove to be a better suited tool for building PM processes.\nThis article describes an unsupervised failure prediction method used to monitor reciprocating compressor systems based on a concept used, among others, in finance, called Log-Periodic Power Law (LPPL) proposed by the [1] and [2]. However, due to the different nature of the data analyzed, the LPPL method cannot be applied in the same way as it was introduced for bubble (or anti-bubble) detection in economic time series. Due to different definition of PM failure in industrial applications, modifications need to be made to the LPPL method. In case of data describing the financial market, the variable that directly characterizes changes in the system under analysis is examined. Such a variable is, for example the index of the analyzed financial market, or directly the price of shares.\nIn case of a machine (e.g. compressor or other systems containing a number of subsystems), the analysis uses indirect data collected by sensors. It is not possible to measure the direct changes causing the failure (e.g. material degradation, cracks etc.) but it is possible to measure changes resulting from the influence of deteriorating machine components on the measured values.\nIn the description of the application of the presented method, which will be discussed in detail on the basis of monitoring data from reciprocating compressors, such indirect data are, for example, changes in the opening angle of suction or discharge valves inside the compression chamber as a function of the volume in the cylinder chamber expressed by the angle of rotation of the crankshaft. In other words, degradational (unmonitored) changes affect changes in measured variables in a less visible (more distorted) way than in the case of direct variable measurements. Every failure has a cause, an initiating event, let's call it an initial breakdown (IB). Using the IB concept, the failure analysis can be as follows:\n\u2022 until the IB occurs, the behavior of the machine is normal and shows no signs of failure.\n\u2022 After the IB occurs, noticeable changes begin to occur indicating future problems.\nThe idea of detecting the IB point in order to use it to predict failure is similar to the concept of detecting a point of trend change in a time series. Therefore, it is not necessary to predict the time of failure in the future. It is sufficient that it can be determined whether a given point in the time series is the initial (initiating) moment of IB or not. If it is known that the current point in the analyzed time series is an IB point, then on the basis of knowledge of the dynamics of the monitored device, it is possible to identify the time window in the future (in units of operating time of the device) in which the failure will occur. It is impossible to predict the exact time of failure of the monitored device using the presented method. This is mainly due to unpredictable interventions, such as: changes in the load of the device, periods of time when the device is switched off, etc.\nThe organization of the paper is as follows. Chapter II briefly describes the current status of unsupervised predictive maintenance in the industry. The chapter III answers the question of why the logarithmic-periodic oscillation detection method is suitable for detecting sudden failures in an industrial system. Chapter IV describes a numerical method for determining failure time points from the data. The part V introduces the data used for the analysis presented. Section VI shows the results obtained using the new method. Part VII discusses the results, advantages and disadvantages of the proposed method."}, {"title": "II. RELATED WORKS", "content": "Despite the appearance of clarity, the description of a method as unsupervised is sometimes used too hastily. As described in the section (I), it is not always possible to identify with sufficient precision the time of failure and what caused the failure. Even after repairs, trying to determine the actual time of failure and its cause is fraught with difficulties. Therefore, before describing the status of work on unsupervised methods, it should be stated, that in this work by \"unsupervised method\" will be understood those solutions that do not require knowledge of the labeled data in any sense, also without knowing whether the input data defines a good/healthy or anomalous condition.\nFollowing the work of [3] (which also includes references to other research works, as well as studies of specific solutions involving a broader understanding of the \"unsupervised method\"), the spectrum of available solutions can be generally divided into 2 categories.\n1. Solutions involving the use of prediction techniques to predict points in the future and then compare them with actual data to detect anomalies, for example [4]. Depending on the predictive solutions used, this category of methods requires a large amount of data. In this case, a separate problem is the accuracy of the prediction part, which is a supervised method. Hence the requirement to control it, which makes the whole solution complicated if one wants to use it in practical applications.\n2. Solutions based on distance measurements or similarity determination used to evaluate the degree of data anomaly. In this case, methods using clustering algorithms [5] or determining similarity between time series or data are used. In this case, there is a problem with new data that did not exist in the past. Sometimes this makes it impossible to use them for real-time data analysis.\nThe solution proposed in this paper is an attempt to solve the difficulties identified in both of the above categories: the problem of data demand and the problem associated with data behavior that is not present in the past."}, {"title": "III. THE OCCURRENCE OF LOG-PERIODIC OSCILLATIONS AS A PRELUDE TO FAILURE.", "content": "The purpose of the PM method is to provide predictions about future failures of the system described as a set of various components cooperating together. This section illustrates why the LPPL-based algorithm is applicable to failure prediction and describes the basics of the LPPL-based method.\nThe generalised relation describing the hazard rate h (t) (or hazard function) of a certain physical quantity at time t prior to material destruction by degeneration [6] is of the form\n$$h (t) = G \\dot{h} (t)$$\nwhere the $\\dot{h} (t)$ denotes the derivative of the function h (t) in time t. The $\\delta$ and $G$ are the parameters of model. The $G$ is a positive constant parameter. In the following, it is assumed that $\\dot{h} (t)$ corresponds to changes in the variable that is tracked and from which the failure is attempted to be predicted. The hazard function, based on conditional probability theory, measures the probability that the relevant variable will show signs of failure, given that the failure has not occurred before prior to time t.\nThe equation (1) has 3 classes of solutions depending on the value of $\\delta$. For $\\delta = 1$ the exponential function is obtained\n$$h(t) = h (t_0) e^{G(t-t_0)}$$\nwhere $t_0$ is a value of the initial time.\nFor $\\delta < 1$:\n$$h (t) = [(1 \u2013 \\delta) (t \u2212 t_0) G]^{\\frac{1}{1- \\delta}}$$\nand for $\\delta > 1$:\n$$h (t) = [(\\delta \u2013 1) (t_c - t) G ]^{\\frac{1}{1-\\delta}}$$\nwith $t_c$ as a constant corresponding to time in a future. As can be seen, the solutions found for $\\delta = 1$ and for $\\delta < 1$ does not converge for times t > 0. Therefore, the time of failure cannot be determined from their behavior. The most interesting case is $\\delta > 1$, where the solution has a converging point at a finite time $t_c$ in future. In our analysis, the time $t_c$ will denote a time of a potential failure and can be determined by the choice of $G = (\\frac{1}{\\delta - 1})$.\nThen $t_c = \\frac{1}{(\\delta-1)h(t_0)^{\\delta-1}}$.\nSuppose that the degradation of a working part, whose deterioration (failure) is predicted, can be treated as a discontinuous stochastic process associated with a given monitored variable. To simplify the analysis, the second assumption is to treat the degradation changes of physical quantity p (t) over time t as a non-homogeneous Poisson process in which the changes occur according to the hazard rate function h (t). The dynamics of such a process can be described by the equation\n$$dp (t) = -p (t) h (t) dt$$\nthe solution of which can be written as\n$$\\log {\\frac{p(t)}{p(t_0)}} = \\int_{t_0}^t h (u) du = P (t).$$\nIn our case, P (t) has an approximate form\n$$P (t) \\approx \\frac{h_0}{\\eta + 1} (t_c - t)^{\\eta+1}$$\nwhere $\u03b7 = \\frac{h_0}{(t_c-t_0)^{\\eta+1}}$ and the P (t) is shifted by the integration constant $h_0 \\frac{(t_c-t_0)^{\\eta+1}}{\\eta + 1}$. The result obtained (7) coincides with work [7] (compare with equation (3) in the reference 7)\nSolution (7) is invariant under continuous scale invariance (CSI), which manifests itself through the scaling property of the solution P (t) if the argument of the function P (t) is scaled to (tc-t). Rescaling by some factor v the argument tc - t \u2192 (tc - t) \u00d7 \u03bd changes the solution P (t) to the form P (t) \u00d7 \u03bc where \u03bc = \u03bd-1-\u03b7 [8]. The CSI feature, around the critical points t = tc, is common to systems demonstrating a continuous phase transition (second order phase transition).\nThe basic assumption of the LPPL method is that, the described process is near the critical point of the second-order phase transition. In our case, this is the point in time at which the failure of the described system occurs. With this assumption, the final equation is obtained, the use of which for fitting is known in the literature as the LPPL method [9], [10].\nLet W (t) = log (p (t)) and refer p (t) to the variable by which our industrial system is analyzed as a function of time t. Let tc be the time of event that defines our phase transition in physical framework (critical point of time). Then the argument x and the real function F (x) are defined as\n$$x = t_c-t  with t < t_c and F (x) = |W (t_c) \u2013 W (t)|$$\nAs a starting point for our derivation, the CSI is assumed to exist around critical points.\nThis allows us to use the renormalization group approach, which permits us to write a certain real function F (x) around a critical point x \u2248 0 through the rescaled argument x expressed by a scaling function f(x) in the form\n$$F(x) = \\frac{1}{\\mu} F [\\varphi (x)]$$\nwhere u is a constant and its argument x is invariant under arbitrary linear transformation of the x:\n$$\\varphi (x) = vx  and x > 0$$\nwith vas a constant.\nThe solution of equation (9) is the function\n$$F (x) = Cx^\\alpha$$\nwhere C and \u03b1 are constants to be determined.\nThe request of scaling invariance (9) with the general form of the solution postulated by eq. (11) can be rewritten as\n$$Cx^\\alpha = \\frac{C}{\\mu} (vx)^\\alpha$$\nwhich, after taking into account the identity,\nleads to the equality\n$$1 = e^{i2\\pi n} with n\u2208 N,$$\n$${\\frac{\\nu^{\\alpha}}{\\mu}} = e^{i2\\pi n}$$\nwhat allows to calculate the \u03b1 exponent in a most general form as\n$$\\alpha = \\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}} + i \\frac{2\\pi n}{\\log {(\\nu)}}$$\nTherefore, the solution of equation (9) can be expressed as [11]\n$$F (x) = \\frac{C}{(\\nu x)^{\\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}}} {\\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}}} } \\Pi {(\\frac{\\log {(\\nu)}}{\\log {(x)}})}$$\nwhere \u03a0(\u00b7) is a periodic function with period 1, i.e. \u03a0 (y) = \u03a0 (y + 1). The index n should be treated as one of the parameters characterizing the described physical system. Since n and other parameters appearing in the function (16) are unknown, it is necessary to reformulate the function (16) in such a way that it can be used to fit existing data and thus determine whether a given point tc is a critical point.\nAn additional necessary condition that must be satisfied by function (16), or more precisely by its real part, is the trend that is determined by the power law (n = 0), which is the leading order term and the oscillations associated with n\u2260 0 will contribute as a next-to-leading order corrections.\nThe periodic function \u03a0 (.) can be expressed by means of the Fourier series with respect to the variable y with period T\n$$\\Pi (y) = exp \\{i2\\pi n [y] \\} = \\sum_{k=-\\infty}^{+\\infty} C_ke^{i2\\pi k{()}}$$\nwith\n$$C_k = {\\frac{1}{T}} \\int_{{-T/2}}^{{T/2}} exp \\{iyn{-k)} \\} e^{{-iky}} dy={\\frac{1}{\\pi}} \\frac{sin [\\pi (k - n)]}{k-n} for n, k \u2208 N$$\nHowever, non-zero coefficients ck of the Fourier expansion (17) are obtained only for non-integer differences (k \u2013 n), which contradicts our claim for the existence of non-zero expressions associated with n \u2208 N. The problem of zero coefficients of the fourier expansion of (17) can be solved by rewriting the identity (13) to the form\n$$1 = e^{i2\\pi n} \u2192 1 = e^{i2(n\\frac{q}{q})}$$\nwhere, the variable q denotes additional parameter associated with the physical degradation mechanism of the described system.\nIn that formulation, \u03b1 (15) can be rewritten as\n$$\\alpha = \\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}} + i {\\frac{2\\pi n}{\\log {(\\nu)} \\frac{q}{q}}}$$\nwhat allows us to rewrite the equation (16) to the form\n$$F (x) = \\frac{C}{(\\nu x)^{\\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}}} + i {\\frac{2\\pi n}{\\log {(\\nu)} \\frac{q}{q}}}}}$$\nIn this case, the expansion of F (x) into Fourier series gives the following result.\n$$F(x) = \\frac{C}{(\\nu x)^{\\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}}} e^{{2\\pi n}}{\\frac{i}{\\log {(\\nu)} \\frac{q}{q}}}{(log(x))} }  = \\frac{C}{(\\nu x)^{\\frac{\\log {(\\frac{\\mu}{\\nu})}}{\\log {(\\nu)}}}}  \\sum_{k \\epsilon N} c_ke^{{\\frac{i}{\\log {(\\nu)} \\frac{q}{q}}}{(log(x)}k}$$\nwhere\n$$C_k = {\\frac{1}{T}} \\int_{{-T/2}}^{{T/2}} e^{-i y n } e^{{-iky}} dy = {\\frac{1}{\\pi}} \\frac{sin [\\pi (k - {\\frac{n}{q}})]}{k-\\frac{n}{q}}$$\nand with a redefined variable $y = \\frac{\\log(x)}{\\log(v){ \\frac{q}{q}}}$.\nGiven the denominator $(k - \\frac{n}{q})$ in the coefficients of ck (23), the main dominant terms of the series (22) are defined by the index k from the set ${-1+ [n/q], [n/q],1+ [n/q]}$ where $[n/q]$ is the integer part of division n/q. To simplify the notation, let us redefine the index values from {-1+ [n/q], [n/q], 1+ [n/q]} to {-1,0,+1}.\nThis allows us to approximate the final form of the function F (x) (22) by the first 3 largest components of the Fourier series (co and c-1 or c+1)\n$$F (x) \\approx {\\frac{C}{\\mu}} {\\frac{\\log({\\frac{\\mu}{\\nu}})}{\\log{(\\nu)}}} \\{C_0 + C_{\\pm 1} cos({\\frac{2\\pi}{\\log(\\nu)q}})log(x) + i C_{\\pm 1} sin({\\frac{2\\pi}{\\log(\\nu)q}})log(x)\\}$$\nwhere the notation $C_{\\pm 1}$ was used to denote ambiguity as to which coefficient is the second dominant one for k = -1 or k = +1. Since only the real part of the expression is of interest (our measurements are real values), using the previous definition of the variable F (x) (8) and generalizing our unknown parameters (\u03bc, \u03bd, C, q, n, k and W (tc)) by adding constant A and phase \u03a6 to the formula (24) to their new representations (A, B, m, C', \u03c9, \u03a6) one obtains the final formula which is referred to as a first-order model and used in the LPPL literature [9, 10]\n$$W (t) \\approx A + |t_c \u2013 t|^m [B + C_1 cos (\\omega log |t_c \u2013 t| + \\Phi)] .$$\nFor the purpose of numerical fitting the LPPL function to the data, a transformed version of the formula (25) is used to the form\n$$W (t) \\approx A + |t_c - t|^m [B + C_1 cos (\\omega log |t_c \u2013 t|) + C_2 sin (\\omega log |t_c - t|)]$$\nwhere $C_1 = C cos (\u03a6)$ and $C_2 = -C sin (\u0424)$. Both equations (25, 26) can be used to find critical time points in the time series of input data that indicate component failures in the input data."}, {"title": "IV. FITTING METHOD OF THE LPPL MODEL TO THE DATA", "content": "Due to the number of parameters (A,B,m,C1,C2,\u03c9) necessary to be determined during the fitting procedure of the LPPL function (26) and the presence of many local extremes, the procedure of obtaining the best fit is difficult and computationally expensive.\nInstead of trying to determine the critical time tc in the future, as is determined in the case of predictions of crashes in financial time series [7, 10, 12], it is assumed that the failure happens \"now\". According to this assumption, the calculations involved in fitting the function (26) to the data are performed for a range of time windows of different lengths from the past to \"now\". This corresponds to the hypothesis that the time point tc (time point of the phase transition) is \"now\" and corresponds to the last point in the input time series tinp. Therefore, it is necessary to add to the set of parameters, an additional parameter specifying the length of the subset of time series points tinp preceding the time point tc for which the best fit of the function W (t) (26) was found. The length of this subset is denoted as lmax.\nIn particular, our redefined set of arguments x = tc - t in the matching procedure is the set of points (1, xlmax) in time units characteristic to the tinp series. In particular, the definition of the argument set x = tc - t in the matching procedure is replaced by the set of points (1, xlmax), where 1 corresponds to the present time and lmax corresponds to the time of lmax from the past. The index lmax is the one of the parameters of the matching function (26).\nAs parameter fitting, the method described in the work of [12] is used, appropriately modified for our purposes, i.e., by excluding the parameter corresponding to the critical time tc and adding the parameter lmax to the fitting procedure.\nThe constraints imposed on the fitting parameters (lmax,A,B,M,C1,C2,\u03c9) are as follows:\n1. lmax: the number of past data used for the best fit. For a small number of data there may be too many good fits (with very small fitting error), which may correspond to random correlations of the data with the form of the fitted function (26).\n2. A > 0: since in our case there are always positive values. It is determined by the character of the input data.\n3. 0 < m < 1: to ensure that the fitting value for the critical time tc was greater than zero (m > 0) and changed faster than exponentially for times close to the critical time tc (m < 1).\n4. 2 < \u03c9 < 8: this condition avoids too fast log-period oscillations (otherwise they would fit the random component of the input data) and too slow log-period oscillations (otherwise they would contribute to the power law behaviour \u2248 A + B |tc - t|m)\n5. B, C1,C2: these parameters are fitted without additional constraints.\nDepending on the type of input data to be analyzed, the limits of variation of the parameters to be fitted require careful adjustment.\nHaving determined all parameters of the fitting LPPL function, in the next step, it is necessary to determine trends based on the identified local maxima and minima of the shape of the fitted function: Tmax for local maxima and Tmin for local minima. The procedure of finding trends is carried out separately for maxima and minima in the following way:\n1. all local extrema (N) are found,\n2. from this set, the N-1 extreme values closest to the current time point (i.e., the point for which an attempt is made to determine whether phase transition has occurred or not) are selected,\n3. To this set of points a straight line is fitted by linear regression. The slope of the line determines the trend for a given category of extremes (maxima or minima).\nThen, using the calculated trends of the extreme values of the best LPPL fit, it is determined whether a given point (defined as a pair: {datetime, value}) corresponds to a phase transition (is a critical point) or not. For this purpose, a theorem describing the critical point was formulated. The proof will be the aim of the next publication."}, {"title": "Theorem 1.", "content": "Assume that the function f (x) corresponds to the best found fit of the LPPL function (26) to the analyzed input time series ts = {(tn - tlmax, Yn\u2013lmax), ..., (tn - tn\u22121, Yn\u22121)} where lmax > 0 is one of the fit parameters of the function f (x) (26) specifying the length of the sequence preceding the actual values (tn - tn\u22121,Yn\u22121) defined by the index n. The function f (x), have Nmax > 2 local maxima and Nmin > 2 local minima. Let Tmax denote the slope of the linear fit determined by the last Nmax values of the local maxima and Tmin denote the slope of the linear fit determined by the last Nmin values of local minima.\nIf both trends Tmax and Tmin determined for the function f (x) have the same behavior (both increasing or decreasing) for the last point (tn - tn-1, Yn-1) of the series ts, then the point (tn, Yn) is the critical point for the series ts. The trend of the series ts will change to the opposite for next points (tn+k, Yn+k) (where k > 0) with respect to the trend for points preceding tn.\nIn the presented method, each point satisfying fulfilling the requirements of Theorem (1) is treated as a point initiating a future failure (IB point). Determining the time of failure requires knowledge of the dynamics of the monitored system and will be discussed in the next section (VI)."}, {"title": "V. DATA DESCRIPTION", "content": "Monitoring data from a reciprocating compressor, describing the PV diagram of one of the compression chambers, was used to demonstrate the operation of the method. Based on these, the values of the angle of opening of the suction valve (OSV) expressed by the angle of rotation of the crankshaft were determined.\nThis value, for a given cycle described by the PV diagram, is very sensitive to changes in the amount of gas in the chamber, for example, due to its leakage through a broken valve or piston rod seal system. OSV changes are directly dictated by the thermodynamics of the compression process in the compressor chamber. Monitoring the changes in OSV provides a basis for compressor diagnostics and allows to determine compressor efficiency, valve operation, or the condition of the piston seals or piston rod sealing elements [13].\nThe data has been averaged to daily values and covers a period of time between 2019-08-23 and 2022-01-19.\nIn order to compare the results of failure prediction, data identifying the dates of repair interventions with the specified reason for failure and the dates of observation of anomalous compressor behavior without interruption of operation were used."}, {"title": "VI. PREDICTION OF FAILURES: METHODOLOGY AND RESULTS", "content": "To test the effectiveness of our algorithm, a backtest of the detection method was conducted on historical data starting from the initial time 2019-10-09 (tstart) to 2022-01-01, calculating initial breakdown (IB) points for this time period. The range of the variable length of the time series Imax was assumed to be 30 < lmax < 101 in time units of days. Given the minimum number of observations (101 days) needed to perform calculations of the IB points, the backtest is started for time t = tstart + 101 (in daily units). Then, moving forward in time to the future, the best-fit LPPL function (26) is calculated for each subsequent time t by determining the goodness of fit of the LPPL function using the mean squared error (mse).\nIntuitively, one can expect that the accuracy of determining the critical points using theorem (1) will strongly depend on the error of fitting the LPPL curve (26) to the data. The smaller the fitting error, the greater the confidence that a given point of the input time series determined as an IB point according to the theorem (1) is really the IB critical point. In addition, it is expected that in the vicinity of the true IB breakpoint (before and after it), the method should find some good fits of the LPPL function with a small error, but this is not a necessary criterion for the existence of an IB point for days as time units.\nThe figure (1) shows 2 examples of the fit function (26) to data along with calculated criteria for trends determined from maxima and minima of the fitting function satisfying the criteria of Theorem (1).\nThe figure (2) shows the application of this procedure for the diagnosis of the critical points with additional information about the dates of failure repairs (see the description of the figure (2)).\nThe figure (2) confirms our intuition very well. It shows:\n1. groups of points with similar fitting errors at least 14 days before the time of failure identification (repair is usually performed with additional delay due to the compressor operating conditions)."}, {"title": "A. Determining the time window of predicted failures", "content": "By comparing the failure times predicted by the algorithm with their actual occurrence, a criterion for predicting the time window in which the failure will occur can be also determined. One of the parameters for fitting the LPPL function (26) to the data is the length of the chosen sequence of data preceding the analyzed time point Imax. For the data analyzed, the time window for the occurrence of a predicted failure was defined as:\nDefinition 2. The predicted time period of failure occurrence is defined as the interval (n + Imax, n +90), where n and Imax are the indices of the actual time point xn and the parameter defining the length of the input time series tinp used to find the best fit of the LPPL function (26) to a given value of tinp (Xn, Yn), respectively.\nThe definition (2) is based on knowledge of the dynamics of the device for which the algorithm parameters have been defined. For other devices, all parameters should be selected based on the dynamics of their behavior. The width of the time window with predicted failure is assumed to be valid for a certain period of time and is up to 90 days.\nIn the case of compressors, due to the different criticality of failures, some of them may be accepted for a longer time (even several months in the case of valve failures) and wait for a convenient moment of repair.\nConsidering:\n\u2022 classification of alerts specified in the definition (1),\n\u2022 selection of the representative of the groups of warnings (by selection of the initial signal for the common group of calculated IB points),\n\u2022 definition (2) specifying the expected time window in which the failure will occur,\nthe raw results shown in the figure (2) can be redrawn to a new form, as shown in the figure (3). The correlation between predicted failure times and actual repair times, and the time periods when experts detected abnormal compressor behavior is very good for the Critical event and Monitoring event categories. Predictions in the Irrelevant event category were not confirmed by any repair and diagnosis records."}, {"title": "B. Root cause of predicted failures", "content": "In the analysis presented here, the input data monitors the change in the opening angle of the suction valves in the compression chamber expressed by the angle of rotation of the crankshaft. The trend identified from the determination of the IB points can be used to guess the type of future failure in the compression chamber [13]. By predicting the trend for times after the IB point, it is possible to try to determine approximately which part will fail - the valve or the piston rod sealing rings. Thus, when the predicted trend of the suction valve opening angle is decreasing, it is likely that the suction valve or piston rod seal rings are failing. If the trend suggests an increase in angle, this behavior indicates a leak in the discharge valve.\nThus, based on the theorem (1) and a physical interpretation based on the behavior of the time series, the algorithm is able to predict not only the time window of failure, but also the group of parts that may fail. This provides an opportunity to verify the prediction not only on the basis of event times, but also on the basis of identifying the parts that can fail.\nTo better illustrate the additional information regarding the location of the future failure, the figure (4) shows the same data as in the Figure (3) with additional information about the parts that actually failed, the parts in which experts have observed problems and prognosis of failures predicted by the algorithm. Details are given in the description of the figure (4).\nFor the entire time period analyzed, 2 cases that deviate from the diagnosis are visible\n1. for category Monitoring event, for date 2021-07-15, there is a disagreement between the predicted failure type suction valve or sealing - leakage and the diagnosed one indiation of discharge valve leakage.\n2. the perturbation identified by experts, started in 2021 11 30 and identified as indiation of discharge valve leakage was not predicted by the algorithm at all."}, {"title": "VII. DISCUSSION", "content": "All the information resulting from the failure predictions and comparing it with the knowledge from maintenance logs and expert detection of periods of anomalous compressor operation", "by": "n\u2022 usually large variety of failures", "used": "Precision = TP/ (TP + FP) and Recall = TP/ (TP + FN), where TP - defines True Positive events, i.e. correctly predicted failures or inappropriate behavior in the operation of the compressor, FP - False Positives, i.e. events predicted by the algorithm that turned out to be false and FN - False Negatives, i.e. failures and instabilities of the compressor that were not predicted.\nIn the analysis of results (section VI), the limits of acceptance of errors of fitting the LPPL function (26) determining the criticality of the predicted events (definition 1) were defined on the basis of the results. Therefore, in order to calculate the Precision and Racall indicators, all results are taken into account without distinguishing them due to the defined criticality.\nComparing the predicted"}]}