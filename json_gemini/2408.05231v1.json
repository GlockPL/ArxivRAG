{"title": "Predictive maintenance solution for industrial systems - an unsupervised approach based on log periodic power law", "authors": ["Bogdan \u0141obodzi\u0144ski"], "abstract": "A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.", "sections": [{"title": "I. INTRODUCTION", "content": "Detecting the symptoms of a failure and predicting when it will occur, in multivariate or univariate time series collected via the Internet of Things (IOT), is central to the concept of predictive maintenance (PM), which is now used in almost every area of industry. PM allows a company to better prepare for a potential failure by redesigning the production process in advance or creating a workaround when shutdown is not possible, and minimizes costs associated with standard maintenance operations by reducing them through predictive engineering.\nPredicting failures, provided by PM can be very profitable for a company, under the condition that PM minimizes the number of false warnings (false positives) and maximizes the number of correctly predicted events (true positives). Creating a properly working PM process faces two main problems:\n1. related to the definition of what is a failure in the considered technological process or in the IoT data,\n2. the development or the application of the best algorithm (based on physical description, machine learning, or statistical methods) to the data being analyzed.\nWhile the theoretical definition of failure is well defined, in practice problems are encountered with its implementation. For various reasons (economic, production, etc.) not every failure requires corrective actions. Sometimes, a minor failure is not a good reason to stop a monitored machinery or a production process. This imprecise description of failure leads to substantial difficulties when attempting to use supervised methods to build a correct Predictive Maintenance process. This suggests that unsupervised approaches may prove to be a better suited tool for building PM processes.\nThis article describes an unsupervised failure prediction method used to monitor reciprocating compressor systems based on a concept used, among others, in finance, called Log-Periodic Power Law (LPPL) proposed by the [1] and [2]. However, due to the different nature of the data analyzed, the LPPL method cannot be applied in the same way as it was introduced for bubble (or anti-bubble) detection in economic time series. Due to different definition of PM failure in industrial applications, modifications need to be made to the LPPL method. In case of data describing the financial market, the variable that directly characterizes changes in the system under analysis is examined. Such a variable is, for example the index of the analyzed financial market, or directly the price of shares.\nIn case of a machine (e.g. compressor or other systems containing a number of subsystems), the analysis uses indirect data collected by sensors. It is not possible to measure the direct changes causing the failure (e.g. material degradation, cracks etc.) but it is possible to measure changes resulting from the influence of deteriorating machine components on the measured values.\nIn the description of the application of the presented method, which will be discussed in detail on the basis of monitoring data from reciprocating compressors, such indirect data are, for example, changes in the opening angle of suction or discharge valves inside the compression chamber as a function of the volume in the cylinder chamber expressed by the angle of rotation of the crankshaft. In other words, degradational (unmonitored) changes affect changes in measured variables in a less visible (more distorted) way than in the case of direct variable measurements. Every failure has a cause, an initiating event, let's call it an initial breakdown (IB). Using the IB concept, the failure analysis can be as follows:\n\u2022 until the IB occurs, the behavior of the machine is normal and shows no signs of failure.\n\u2022 After the IB occurs, noticeable changes begin to occur indicating future problems.\nThe idea of detecting the IB point in order to use it to predict failure is similar to the concept of detecting a point of trend change in a time series. Therefore, it is not necessary to predict the time of failure in the future. It is sufficient that it can be determined whether a given point in the time series is the initial (initiating) moment of IB or not. If it is known that the current point in the analyzed time series is an IB point, then on the basis of knowledge of the dynamics of the monitored device, it is possible to identify the time window in the future (in units of operating time of the device) in which the failure will occur. It is impossible to predict the exact time of failure of the monitored device using the presented method. This is mainly due to unpredictable interventions, such as: changes in the load of the device, periods of time when the device is switched off, etc.\nThe organization of the paper is as follows. Chapter II briefly describes the current status of unsupervised predictive maintenance in the industry. The chapter III answers the question of why the logarithmic-periodic oscillation detection method is suitable for detecting sudden failures in an industrial system. Chapter IV describes a numerical method for determining failure time points from the data. The part V introduces the data used for the analysis presented. Section VI shows the results obtained using the new method. Part VII discusses the results, advantages and disadvantages of the proposed method."}, {"title": "II. RELATED WORKS", "content": "Despite the appearance of clarity, the description of a method as unsupervised is sometimes used too hastily. As described in the section (I), it is not always possible to identify with sufficient precision the time of failure and what caused the failure. Even after repairs, trying to determine the actual time of failure and its cause is fraught with difficulties. Therefore, before describing the status of work on unsupervised methods, it should be stated, that in this work by \"unsupervised method\" will be understood those solutions that do not require knowledge of the labeled data in any sense, also without knowing whether the input data defines a good/healthy or anomalous condition.\nFollowing the work of [3] (which also includes references to other research works, as well as studies of specific solutions involving a broader understanding of the \"unsupervised method\"), the spectrum of available solutions can be generally divided into 2 categories:\n1. Solutions involving the use of prediction techniques to predict points in the future and then compare them with actual data to detect anomalies, for example [4]. Depending on the predictive solutions used, this category of methods requires a large amount of data. In this case, a separate problem is the accuracy of the prediction part, which is a supervised method. Hence the requirement to control it, which makes the whole solution complicated if one wants to use it in practical applications.\n2. Solutions based on distance measurements or similarity determination used to evaluate the degree of data anomaly. In this case, methods using clustering algorithms [5] or determining similarity between time series or data are used. In this case, there is a problem with new data that did not exist in the past. Sometimes this makes it impossible to use them for real-time data analysis.\nThe solution proposed in this paper is an attempt to solve the difficulties identified in both of the above categories: the problem of data demand and the problem associated with data behavior that is not present in the past."}, {"title": "III. THE OCCURRENCE OF LOG-PERIODIC OSCILLATIONS AS A PRELUDE TO FAILURE.", "content": "The purpose of the PM method is to provide predictions about future failures of the system described as a set of various components cooperating together. This section illustrates why the LPPL-based algorithm is applicable to failure prediction and describes the basics of the LPPL-based method.\nThe generalised relation describing the hazard rate $h (t)$ (or hazard function) of a certain physical quantity at time $t$ prior to material destruction by degeneration [6] is of the form\n$h (t) = Gh (t)$\t                                (1)\nwhere the $h (t)$ denotes the derivative of the function $h (t)$ in time $t$. The $\u03b4$ and $G$ are the parameters of model. The $G$ is a positive constant parameter. In the following, it is assumed that $h (t)$ corresponds to changes in the variable that is tracked and from which the failure is attempted to be predicted. The hazard function, based on conditional probability theory, measures the probability that the relevant variable will show signs of failure, given that the failure has not occurred before prior to time $t$.\nThe equation (1) has 3 classes of solutions depending on the value of $\u03b4$. For $\u03b4 = 1$ the exponential function is obtained\n$h(t) = h (t_0) e^{G(t-t_0)}$                                                                               (2)\nwhere $t_0$ is a value of the initial time.\nFor \u03b4 < 1:\n$h (t) = [(1 \u2013 \u03b4) (t \u2212 t_0) G]^{\\frac{\u03b4}{1\u2212\u03b4}}$                                                            (3)\nand for $\u03b4 > 1$:\n$h (t) = [(\u03b4 \u2013 1) (t_c - t) G]^{\\frac{\u03b4}{1\u2212\u03b4}}$                                                                 (4)\nwith $t_c$ as a constant corresponding to time in a future. As can be seen, the solutions found for $\u03b4 = 1$ (2) and for \u03b4 < 1 (3) does not converge for times t > 0. Therefore, the time of failure cannot be determined from their behavior. The most interesting case is \u03b4 > 1 (4), where the solution has a converging point at a finite time $t_c$ in future. In our analysis, the time $t_c$ will denote a time of a potential failure and can be determined by the choice of $G = (\\frac{\u03b4}{\u03b4-1})\\dot h(t_0) \u03b4\u22121$"}, {"title": "IV. FITTING METHOD OF THE LPPL MODEL TO THE DATA", "content": "Due to the number of parameters (A,B,m,C1,C2,w) necessary to be determined during the fitting procedure of the LPPL function (26) and the presence of many local extremes, the procedure of obtaining the best fit is difficult and computationally expensive.\nInstead of trying to determine the critical time $t_c$ in the future, as is determined in the case of predictions of crashes in financial time series [7, 10, 12], it is assumed that the failure happens \"now\". According to this assumption, the calculations involved in fitting the function (26) to the data are performed for a range of time windows of different lengths from the past to \"now\". This corresponds to the hypothesis that the time point $t_c$ (time point of the phase transition) is \"now\" and corresponds to the last point in the input time series $t_{inp}$. Therefore, it is necessary to add to the set of parameters, an additional parameter specifying the length of the subset of time series points $t_{inp}$ preceding the time point $t_c$ for which the best fit of the function $W (t)$ (26) was found. The length of this subset is denoted as $I_{max}$.\nIn particular, our redefined set of arguments $x = t_c - t$ in the matching procedure is the set of points $(1, X_{lmax})$ in time units characteristic to the $t_{inp}$ series. In particular, the definition of the argument set $x = t_c - t$ in the matching procedure is replaced by the set of points $(1, X_{lmax})$, where 1 corresponds to the present time and $lmax$ corresponds to the time of $lmax$ from the past. The index $I_{max}$ is the one of the parameters of the matching function (26).\nAs parameter fitting, the method described in the work of [12] is used, appropriately modified for our purposes, i.e., by excluding the parameter corresponding to the critical time $t_c$ and adding the parameter $I_{max}$ to the fitting procedure.\nThe constraints imposed on the fitting parameters ($lmax$,A,B,M,C1,C2,w) are as follows:\n1. $I_{max}$: the number of past data used for the best fit. For a small number of data there may be too many good fits (with very small fitting error), which may correspond to random correlations of the data with the form of the fitted function (26)."}, {"title": "V. DATA DESCRIPTION", "content": "Monitoring data from a reciprocating compressor, describing the PV diagram of one of the compression chambers, was used to demonstrate the operation of the method. Based on these, the values of the angle of opening of the suction valve (OSV) expressed by the angle of rotation of the crankshaft were determined.\nThis value, for a given cycle described by the PV diagram, is very sensitive to changes in the amount of gas in the chamber, for example, due to its leakage through a broken valve or piston rod seal system. OSV changes are directly dictated by the thermodynamics of the compression process in the compressor chamber. Monitoring the changes in OSV provides a basis for compressor diagnostics and allows to determine compressor efficiency, valve operation, or the condition of the piston seals or piston rod sealing elements [13].\nThe data has been averaged to daily values and covers a period of time between 2019-08-23 and 2022-01-19.\nIn order to compare the results of failure prediction, data identifying the dates of repair interventions with the specified reason for failure and the dates of observation of anomalous compressor behavior without interruption of operation were used."}, {"title": "VI. PREDICTION OF FAILURES: METHODOLOGY AND RESULTS", "content": "To test the effectiveness of our algorithm, a backtest of the detection method was conducted on historical data starting from the initial time 2019-10-09 (tstart) to 2022-01-01, calculating initial breakdown (IB) points for this time period. The range of the variable length of the time series $I_{max}$ was assumed to be $30 < lmax < 101$ in time units of days. Given the minimum number of observations (101 days) needed to perform calculations of the IB points, the backtest is started for time t = tstart + 101 (in daily units). Then, moving forward in time to the future, the best-fit LPPL function (26) is calculated for each subsequent time t by determining the goodness of fit of the LPPL function using the mean squared error (mse).\nIntuitively, one can expect that the accuracy of determining the critical points using theorem (1) will strongly depend on the error of fitting the LPPL curve (26) to the data. The smaller the fitting error, the greater the confidence that a given point of the input time series determined as an IB point according to the theorem (1) is really the IB critical point. In addition, it is expected that in the vicinity of the true IB breakpoint (before and after it), the method should find some good fits of the LPPL function with a small error, but this is not a necessary criterion for the existence of an IB point for days as time units.\n1. groups of points with similar fitting errors at least 14 days before the time of failure identification (repair is usually performed with additional delay due to the compressor operating conditions)."}, {"title": "B. Root cause of predicted failures", "content": "In the analysis presented here, the input data monitors the change in the opening angle of the suction valves in the compression chamber expressed by the angle of rotation of the crankshaft. The trend identified from the determination of the IB points can be used to guess the type of future failure in the compression chamber [13]. By predicting the trend for times after the IB point, it is possible to try to determine approximately which part will fail - the valve or the piston rod sealing rings. Thus, when the predicted trend of the suction valve opening angle is decreasing, it"}, {"title": "VII. DISCUSSION", "content": "All the information resulting from the failure predictions and comparing it with the knowledge from maintenance logs and expert detection of periods of anomalous compressor operation, including the prediction classification, is provided in Table I.\nMajor challenges in the field of industrial application of failure prediction, especially in the unsupervised version, is a number of problems related to the proper identification of failures and behaviors of monitored devices using precisely defined rules. These difficulties are mainly caused by:\n\u2022 usually large variety of failures,\n\u2022 a small number of failures compared to the amount of data,\n\u2022 a large variety of behaviors leading to the same type of failure,\n\u2022 usually large number of operating conditions. This causes ambiguity in labeling good data.\nThe problems are very difficult to solve if the methods used to predict failures are based on analysis of numerical values of input data (by calculating similarities, correlations, logic trees, building naural networks, etc.). Normalization and/or standardization procedures only introduce a common scale to the analyzed data.\nThe proposed method introduces a new type of procedure, which is based on the search for common functional behavior (26). From the point of view of the numerical values of the analyzed data, the course of the fitted function can be very different for different events - different patterns of the function for different values of the fitted parameters. Even when new data appears with values that did not exist in the past, it is possible to determine the critical point (IB) for a potential event, as the model fits functions to the data.\nThe IB points determined by this method, based on which the time window of failure occurrence is predicted in the next step, are the trend change points in the data. From this perspective, the described solution can be reduced only to the task of determining the trend change points.\nTo calculate the key performance indicators (KPIs) of the presented algorithm, the generally known indicators can be used: $Precision = TP/ (TP + FP)$ and $Recall = TP/ (TP + FN)$, where TP - defines True Positive events, i.e. correctly predicted failures or inappropriate behavior in the operation of the compressor, FP - False Positives, i.e. events predicted by the algorithm that turned out to be false and FN - False Negatives, i.e. failures and instabilities of the compressor that were not predicted.\nIn the analysis of results (section VI), the limits of acceptance of errors of fitting the LPPL function (26) determining the criticality of the predicted events (definition 1) were defined on the basis of the results. Therefore, in order to calculate the Precision and Racall indicators, all results are taken into account without distinguishing them due to the defined criticality.\nComparing the predicted failure periods, taking into account the dates and predicted types of failures with the dates and descriptions of Maintenance logs or recorded faults (Figs. 3 and 4 and the table I), the calculated values of TP, FP and FN are as follows: TP = 4, FP = 2, FN = 1. Hence Precision = 0.67, Recall = 0.8. Given that, this result, takes into account the dating of the predicted failure period along with the prediction of the cause of failure, such a result is considered very good.\nIn summary, the list of advantages and disadvantages of the presented method is a consequence of a paradigm change in data behavior classification, from the one based on numerical values to the one based on functional similarity.\nAdvantages of the method:\n\u2022 the proposed model can be applied to very short time series (in our case, the minimum length of the series is only 101 points).\n\u2022 There are no problems with data that appear for the first time. In the proposed solution, the part that qualifies certain data as IB is based solely on functional behavior. This universality is due to the renormalization group approach.\n\u2022 The simplicity of the final production solution. The most difficult part of the algorithm is fitting the function to the data. Since the method does not contain components based on supervised methods, so there is no need to monitor their quality.\nDisadvantages of the method:\n\u2022 data: the method is applicable to data that describe a physical process that degrades/changes due to pertur- bations introduced by interacting elements. This is because the method searches for behavior characteristic of phenomena in which phase transitions can be observed. Hence, not all data are appropriate for the described method.\n\u2022 Matching the function to the data is based on the proper determination of boundaries of the parameters to be matched (26). This requires individualized adjustment of the ranges of change of these parameters to the monitored device.\n\u2022 It is required to select the time step of the input time series in such a way that it is consistent with the dynamic characteristics of the monitored device."}, {"title": "VIII. CONCLUSIONS", "content": "This paper presents the application of a methodology for describing critical behavior in complex systems based on the renormalization group approach in unsupervised predictive maintenance. The proposed algorithm analyzes the behavior of a complex system based on a time series representing the physical behavior of the system. To demonstrate the effectiveness of the algorithm for industrial applications, predictive results are presented for time series describing the thermodynamics of the gas compression process in a monitored reciprocating compressor, in one of the compression chambers.\nIt was shown that failures in the analyzed industrial system can be treated as critical behavior in complex systems. Then the symptoms of future failure appear in the form of Log Periodic Power Law structures for phase transitions in the analyzed time series. Based on the most generalized scheme for describing the behavior of an analyzed system in the vicinity of phase transitions of the 2nd kind based on the Log Periodic Power Law, a new way of predicting failures in compressor systems is proposed.\nThe presented algorithm is based on 3 steps. In the first step, the algorithm determines the IB points in the analyzed time series by means of fitting the LPPL function (equation (26)) using the proposed theorem (1). The second step of the proposed method is based on the knowledge of the dynamics of the monitored system, and specifies the time window in which the predicted failure may occur. In the last step, a criticality classification of the predicted failure is carried out based on the goodness of fit of the LPPL curve to the data (critical event, monitoring event, insignificant event).\nTaking into account the specificity of problem detection in industrial systems (the demand to reduce the number of false alarms and to minimize the number of unpredicted events), it has been demonstrated that it is possible to experimentally determine such an error threshold of fitting the LPPL function to the data that all serious failures can be predicted if the fitting error is smaller than the threshold. In addition, it is also possible to define such thresholds for the LPPL curve-fit error to the data, for which the area of occurrence of less critical failures that do not require rapid intervention can be defined.\nThe method can also be applied to predictive IoT analysis of other industrial systems."}]}