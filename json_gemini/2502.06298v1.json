{"title": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia", "authors": ["Chaoqun Liu", "Wenxuan Zhang", "Jiahao Ying", "Mahani Aljunied", "Anh Tuan Luu", "Lidong Bing"], "abstract": "This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable performance across various English benchmarks, including both human exam datasets such as MMLU (Hendrycks et al., 2021), or instruction-following datasets such as MT-Bench (Zheng et al., 2023b), indicating their strong capabilities (OpenAI, 2023; Dubey et al., 2024; Team et al., 2024). As these LLMs are increasingly deployed globally, there is growing interest in their ability to handle multiple languages and adapt to a wide range of multilingual applications (Huang et al., 2023; Qin et al., 2024; Huang et al., 2024; Dou et al., 2024; Nguyen et al., 2023; Zhang et al., 2024).\nThis led to the development of multiple multilingual benchmarks to assess the multilingual capabilities of LLMs (Lai et al., 2023; Ahuja et al., 2023; Zhang et al., 2023). Among them, many datasets such as MGSM (Shi et al., 2022), XNLI (Conneau et al., 2018), and Multilingual MMLU (Hendrycks et al., 2021; OpenAI, 2023) are typically constructed by translating the English set into target languages. Considering that original English test sets are often carefully designed, such translations provide an effective way to leverage the task categorization, evaluation targets, and construction methods of the monolingual dataset into the multilingual context.\nHowever, such translated questions focus merely on evaluating the same contextual elements as their monolingual counterparts. In other words, they focus primarily on the application scenarios relevant to the original benchmarks rather than adapting to a wide range of multilingual applications in the real world. Instead, a truly effective multilingual benchmark must also consider the content typically used in the practical application of the target language (Liu et al., 2024). For example, as shown in Figure 1, we visualize the distribution of objects in questions collected from local usage queries versus those translated from English. Compared to local usage queries, translated questions based on English exhibit more of an American context, e.g., involving the place \u201cHawaii\u201d. It shows that translated questions cover only a small portion of the entities in local usage queries, indicating a significant divergence in the query context.\nConsidering the scarcity of such effective multilingual benchmarks, this paper introduces two new benchmarks, SeaExam and SeaBench. These benchmarks are specifically designed to address the unique application scenarios and cultural contexts of Southeast Asian (SEA) countries, which often differ significantly from western-centric datasets. Following the design principles of two widely used English-based datasets, MMLU and MT-bench, we do not simply translate the original English questions but incorporate real-world usage scenarios from SEA natives into the content allowing us to measure a model's adaptability in multilingual application scenarios. Specifically, SeaExam is a multitask exam dataset sourced from real exams in SEA countries that cover a wide range of subjects including local history, geography, and literature. SeaBench, following MT-Bench's approach, focuses on multi-turn instruction-following tasks spanning ten task categories. It incorporates scenarios and instructions that are commonly encountered in SEA cultures and daily life.\nOur experimental analysis quantitatively demonstrates that, 1) Compared to the translated benchmarks MMLU and MT-bench, our SeaExam and SeaBench benchmarks include questions that are more aligned with the daily usage of regional languages (Section 3.1). 2) Furthermore, using SeaExam and SeaBench, we are able to more effectively discern the capabilities of models in real-world multilingual applications (Section 3.2.1). Further analysis reveals that 3) While multiple-choice questions in exam datasets can objectively measure model capabilities, open-ended questions are more effective in highlighting differences in model performance across various languages (Section 3.2.2 and Section 3.2.3). Additionally, we find that 4) The nine models involved generally perform poorly in the \"safety\" category - evaluating whether the models generate harmful responses in the local context (Section 3.2.4). Therefore, we advocate for enhanced safety measures in multilingual applications to adapt to a broader range of scenarios.\nThe key contribution can be summarized as:\n\u2022 We introduce two new benchmarks, SeaExam and SeaBench, which extend the scope of the translated MMLU and MT-bench frameworks to better accommodate the unique linguistic features and practical content contexts of the Southeast Asian (SEA) region.\n\u2022 We compare these benchmarks with translated counterparts, such as MMLU and MT-Bench, and find that SeaExam and SeaBench have closer distribution to real-world queries. Utilizing these benchmarks allows for a better differentiation of model performance across different language uses."}, {"title": "2 Sea Exam and SeaBench", "content": "We aim to build multilingual benchmarks to comprehensively evaluate model adaptability to Southeast Asia applications, focusing on both linguistic style and content essence that cannot be fully measured with translated questions. Following the design principle of MMLU and MT-bench, two comprehensive datasets in measuring the English capabilities of large language models, we incorporate real local exams of each country for SeaExam and engage native speakers to craft instructions commonly used in the corresponding language communities for SeaBench. This approach ensures that our benchmarks reflect real-world usage in SEA contexts. We outline the detailed creation processes for SeaExam and SeaBench in Section 2.1 and Section 2.2, respectively."}, {"title": "2.1 SeaExam Construction", "content": "Evaluating LLMs using human exam questions can provide valuable insights into the model's performance, as these questions encompass a wide range of knowledge types. However, relying solely on translations of monolingual exam questions can introduce content biases into model evaluations. For example, the widely used MMLU benchmark includes categories such as \u201cUS History\u201d, which may be more relevant to American users.\nTo address this, we decide to manually collect exam questions from the SEA region (Indonesian (id), Thai (th), and Vietnamese (vi)). We follow the construction of M3Exam (Zhang et al., 2023),"}, {"title": "2.2 SeaBench Construction", "content": "Exam questions can objectively assess a model's knowledge and capabilities; however, many real-world user inquiries are inherently open-ended, challenging an LLM not only to demonstrate its knowledge retention but also to interpret instructions effectively and generate high-quality responses.\nCurrently, MT-bench (Zheng et al., 2023b), widely regarded as the most authoritative and sys-"}, {"title": "3 Experiment", "content": "Given the meticulously built SeaExam and SeaBench, we then conduct experiments to quantitatively demonstrate how our benchmarks could better evaluate models' abilities on multilingual applications from: 1) how our datasets align more closely with the daily usage of regional languages (Section 3.1), and 2) how it effectively distinguishing differences in model performance across various languages (Section 3.2.1) and distinguishing performance variations within the same model across different languages ((Section 3.2.2) and (Section 3.2.3)). Through our fine-grained analysis using SeaBench, we have uncovered significant deficiencies in LLMs' response safety across multilingual usage scenarios. Consequently, we advocate for enhanced safety measures in models for multilingual contexts to better adapt to actual usage realities (Section 3.2.4))."}, {"title": "3.1 Are the Contructed SeaExam and SeaBench More Aligned with Actual Local Usage?", "content": "Despite utilizing local exams and engaging native language experts specifically to tailor questions to the local context, the critical question remains unresolved: How do these questions more accurately reflect the actual local usage compared to those derived from translations? To evaluate the alignment of our benchmarks with actual local usage, we conduct a quantitative comparison between SeaExam and SeaBench and real-world user queries. As the first step, we construct the real-world user queries dataset \"Wild Queries\" as follows:\nWild Queries is constructed based on LMSYS-Chat-1M (Zheng et al., 2023a) and WildChat-1M (Zhao et al., 2024b; Deng et al., 2024), which are databases of real-world human queries with millions of conversations across various application scenarios. Using these conversation data, we conducted a meticulous post-filtering process to obtain high-quality queries in SEA languages. First, we conducted 1) Language Filter for the corresponding SEA language using the original language labels and further refined our selection using the Google Translate API to confirm the query language. Given corresponding SEA queries, we have 2) Data Balance Control \u2014 removing overly long conversations, limiting the data to extracting user inputs up to five rounds per conversation, to ensure data balance across different usage scenarios. Finally, we employ a capable multilingual model, GPT-40, to process 3) LLM-Based Heuristic Filter to further filter out questions that are not queries or instructions. After these three steps, we get a total of 4,658 queries real-world user queries in SEA languages.\nUsing these real-world user queries, we compare the similarity between them and our benchmarks, SeaExam and SeaBench, for each SEA language respectively. Specifically, we utilize the cluster distance (C-Dist) of sentence embeddings derived from the bge-multilingual-gemma2 model (Chen et al., 2024) to measure similarity. We also deploy translated MMLU (MMLU-SEA) and MT-bench (MT-bench-SEA) on SEA languages as baselines (more details on the datasets and the embedding"}, {"title": "3.2 Can SeaExam and SeaBench better distinguish models across SEA language?", "content": "We have quantitatively demonstrated that the constructed SeaExam and SeaBench benchmarks are more aligned with actual local usage questions (Section 3.1). However, does this greater alignment also improve our ability to distinguish between different models? This question is central to the purpose of building these benchmarks \u2014 aiming to better discern models' ability to handle multiple languages and adapt to a wide range of multilingual applications across SEA languages. To answer the question, we evaluate nine LLMs, a detailed experiment setting as follows:\nModels: We consider multiple factors when selecting nine models for evaluation. First, instruction-following capability is a key requirement, as SeaBench necessitates models that can effectively adhere to given instructions. Second, we select only those with parameters ranging from 7B to 9B, as they offer a good balance between performance and inference speed. Based on these criteria, we select models from three groups: (1) the most popular open-source models, including Meta-Llama-3.1-8B-Instruct (Llama-3.1-8B)(Dubey et al., 2024), Gemma-2-9b-it (Gemma-2-9B)(Team et al., 2024), Mistral-7B-Instruct-v0.3 (Mistral-7B)(Jiang et al., 2023), and Qwen2-7B-Instruct (Qwen2-7B)(Yang et al., 2024); (2) models optimized for multilingual capabilities, including glm-4-9b-chat (glm-4-9b)(GLM et al., 2024) and Aya-23-8B(Aryabumi et al., 2024); and (3) models specifically optimized for Southeast Asian languages, including SeaLLMs-v3-7B-Chat (SeaLLMs-v3-7B)(Zhang et al., 2024), llama3-8b-cpt-sealionv2-instruct (sealionv2)(Singapore, 2024), and Sailor-7B-Chat (Sailor-7B) (Dou et al., 2024).\nMetrics and Setups: For SeaExam, we conduct evaluation in 3-shot and use accuracy (%) as the evaluation metric. For SeaBench, we employ LLMs-as-a-Judge (Zheng et al., 2023b; Bai et al., 2023; Ying et al., 2024), setting GPT-40 as the judge model to evaluate LLM's responses based on the reference answers (construction details in Section 2.2). Considering that different categories of questions focus on assessing different aspects of model performance, we have designed a list of priority evaluation aspects for each category to facilitate a comprehensive judgment. We prompt GPT-40 to rate each response on a scale from 1 to 10.\nFollowing this experimental setup, we conduct tests using SeaExam and SeaBench, with results presented in Table 2. Upon analyzing these results, we identify several interesting findings as follows:"}, {"title": "3.2.1 Finding 1: SeaExam and SeaBench can better distinguish different models", "content": "We compare the performance of tested models between SeaExam and MMLU-SEA, examining the standard deviation of model performances across three SEA languages. Results, as shown in Figure 4, indicate that the variances in SeaExam are significantly higher than those in MMLU-SEA by 9.3%. A similar phenomenon was observed when comparing SeaBench with MT-bench-SEA by 8.7%. This consistency suggests that, compared to direct translations, our benchmarks more effectively discern the capabilities of models in real-world application scenarios.\nIn Figure 4, we find the abnormal phenomenon that SeaExam has no distinct advantage in differentiating among models for the Indonesian language. This may be due to the poor performance across the models on Indonesian, each showing a decline of more than 4.5% compared to MMLU-SEA, resulting in a lower standard deviation in differentiation. This observation prompts us to explore further whether the ability to effectively separate models extends to aiding in a more nuanced analysis across different languages."}, {"title": "3.2.2 Finding 2: SeaBench can better distinguish performance variations within the same model across different languages", "content": "We conduct a comparison of nine models' performance standard deviations on SeaExam across three SEA languages and compared these with performances on MMLU-SEA. As shown in Figure 5, SeaExam does not demonstrate a significant advantage in distinguishing language differences. In contrast, a notable distinction emerges when comparing SeaBench to MT-Bench. Specifically, the performance gaps across the three languages in SeaBench are significantly larger than those in the translated MT-bench-SEA, by 6.7% on average, indicating that SeaBench more effectively highlights the performance variations within the same model across different languages. Additionally, we identified a few models, such as Sailor-7B, SeaLLMs-v3-7B, and Sealionv2, that exhibited more balanced performances across SEA languages in SeaBench. This is because these models were specifically trained with a focus on SEA daily scenarios, which resulted in a more balanced performance on SEA language tests."}, {"title": "3.2.3 Finding 3: Open-Ended Question Formats More Effectively Distinguish Model Capabilities", "content": "We compare the performance of models across three languages in SeaExam and SeaBench. Since SeaExam employs accuracy (%) as its metric and SeaBench uses scores from a judge model, the scoring methods are not directly comparable. To standardize the evaluation, we converted the latter's scores to accuracy rates and full mark rates (where a response is considered correct only if it achieves full marks on all aspects). The results, depicted in Figure 6, reveal that the deviations among the nine models across the three languages are greater in SeaBench compared to SeaExam by 1.37 times. This observation supports our earlier hypothesis that open-ended question formats, requiring more extensive language use, better highlight differences in model capabilities."}, {"title": "3.2.4 Finding 4: LLMs Perform Poorly on Safety Questions", "content": "Through extensive experimental analysis, we have demonstrated that our benchmarks more effectively evaluate models' abilities in real-world multilingual applications. Building on this, we conduct a fine-grained analysis, with the results for SeaBench shown in Figure 7. We find that models perform significantly worse on the \u201csafety category\u201d of questions, with an average score of 5.02, which is 20% lower than the highest-performing \u201cSTEM category\". These questions assess the model's ability to avoid generating harmful responses. This finding highlights a notable deficiency in the models' safety performance in relevant usage scenarios. We speculate that most alignment efforts are conducted using data on the models' primary languages and overlooking other multilingual application contexts. Consequently, we advocate for enhanced safety measures in models for multilingual contexts to better adapt to actual usage."}, {"title": "4 Human Evaluation", "content": "For both constructed benchmarks, SeaExam and SeaBench, each question and its corresponding reference answer are meticulously crafted by engaged native linguists, ensuring high quality. To further validate the reliability of our experimental results\u2014particularly the evaluation scores assigned by GPT-40 for SeaBench\u2014we conduct a human agreement evaluation. For each question, we randomly sample three distinct model pairs, ensuring that no model combination is repeated. Since"}, {"title": "5 Related Work", "content": "SEA Benchmarks. Several benchmarks have been developed to evaluate LLMs on SEA languages. SeaEval (Wang et al., 2023) includes 28 datasets covering classic NLP tasks, reasoning, and cultural comprehension. For the newly created datasets, Cross-MMLU and Cross-LogiQA, the questions were translated from English using Google Translate and proofread by native speakers. SeaCrowd benchmarks (Lovenia et al., 2024) cover 4 NLU tasks with 131 data subsets and 7 NLG tasks with 100 subsets. BHASA (Leong et al.) offers a holistic evaluation suite for assessing linguistic and cultural aspects in LLMs tailored to SEA languages. These benchmarks aim to provide a comprehensive evaluation for SEA languages, with a focus on NLP tasks. However, none of the existing benchmarks evaluate open-ended questions or multi-turn conversations. In contrast, SeaExam focuses on real-world exam questions, and SeaBench offers the first SEA benchmark designed specifically for open-ended and multi-turn evaluations.\nLLM-as-a-Judge Strong LLMs have emerged as judges to evaluate model capabilities on open-ended questions. Zheng et al. (2023b) proposed MT-bench, with GPT-4 as the judge to test multi-turn conversation and instruction-following ability. Li et al. (2023) introduced AlpacaEval, a method for assessing a model's performance by determining the percentage of instances in which a powerful LLM favors the model's outputs compared to those from a reference model."}, {"title": "6 Conclusion", "content": "In this study, we introduced two benchmarks, SeaExam and SeaBench, specifically designed to evaluate LLMs within Southeast Asian (SEA) application scenarios. Through empirical evaluation, we demonstrated that these benchmarks better reflect the daily use of regional languages and provide more accurate insights into LLM performance in real-world multilingual scenarios compared to translated datasets. Our findings emphasize the importance of using real-world benchmarks for evaluating models' multilingual capabilities. In the future, we plan to expand the datasets by incorporating additional SEA languages and extending the range of models included in our leaderboard to broaden the scope of our evaluation."}, {"title": "Limitations", "content": "Like many existing benchmarks, SeaExam and SeaBench are static, which may lead to issues such as saturation and data contamination. To address these challenges, we are curating additional questions and keeping this dataset private. We also plan to implement dynamic updates to these benchmarks in the future to further mitigate these limitations. Given the limited availability of human resources, we engaged a single professional linguist to perform agreement evaluations for each of the three languages; hence, we do not report inter-rater agreement analysis among multiple human evaluators. However, the study by Zheng et al. (2023b) indicated that human agreement rates are approximately 80%, which provides a useful reference for our results."}, {"title": "A Benchmark Details", "content": "A.1 SeaExam\nFollowing the construction of M3Exam dataset (Zhang et al., 2023), we engage native speakers from the SEA region to collect official exam papers, along with their corresponding answers, typically taken at the end of each educational level\u2014primary school, middle school, and high school graduation exams.\nThe data cleaning process begins with using OCR to convert scanned exam papers into editable text. Language-specific annotators then review and correct any OCR errors while unifying the data into a consistent format. Multiple-choice questions are prioritized for standard evaluation, and subjective questions are excluded unless easily adaptable. Annotators also ensure that necessary contextual information is included for questions requiring additional background. Special formats, like equations, are converted into LaTeX, and multiple rounds of quality checks ensure the final dataset closely mirrors real exam conditions.\nAfter data cleaning, all questions were standardized to four answer options by removing those with fewer options and eliminating certain incorrect choices from those with more. The final SeaExam comprises a total of 5,451 test samples and the statistics of the SeaExam is shown in Table 4, following the original classification framework of M3Exam. We also map the subjects to MMLU categories, with the mapping shown in Table 5.\nA.2 SeaBench\nTable 6 shows the Distribution of subject categories by language for SeaBench and Table 7 the categories and their corresponding priority aspects in SeaBench.\nA.3 Translated Benchmarks\nWe compare SeaExam and SeaBench with the translated MMLU and the translated MT-bench. For an effective comparison with the two datasets, we process the datasets using the following procedures:\nMMLU We randomly select 50 questions from each subject, totaling 2850 questions. Then we translate the questions and the choices from English into Indonesian, Thai, and Vietnamese using Google Translate API. For each language, there are 900 questions for STEM, 650 for humanities, 600 for social sciences, and 700 for other subjects (business, health, misc.). We call the curated benchmark MMLU-SEA.\nMT-bench We translated MT-bench into Indonesian, Thai, and Vietnamese using the Google Translate API. Instead of the default model for MT-bench, GPT-4, we use GPT-40 (gpt-40-08-06) as the judge, as GPT-40 is more proficient in both English and other languages. In addition, we utilize GPT-40 to"}, {"title": "B Experiment Details", "content": "B.1 Evaluation Setup\nWe evaluate on SeaExam with 3-shot setting in the completion mode. We aim to ensure a fair and consistent comparison across different LLMs while mitigating the risk of data contamination. We have designed four instruction templates to provide a fair comparison and reduce LLMs' dependence on specific prompt templates. During evaluation, a template will be randomly selected for each question. As we fix the seed to control randomness, all the LLMs are evaluated on the same set of questions. Additionally, users have the option to change the seed value to generate a different set of questions for evaluation purposes.\nWe evaluate SeaBench with zero-shot setting to assess the model's instruction-following capabilities. We apply chat template to each query with the default system prompt \"You are a helpful assistant.\" If the model does not support the system prompt, we leave it empty. We run all the evaluations on Nvdia A100 GPUs.\nB.2 Additional Results\nC Human Evaluation\nC.1 SeaBench Evaluation\nThe prompt templates for reference-guided single-answer grading for SeaBench are shown in Figure 10 and 11. To compare the entity distributions between SeaExam, MMLU-SEA, and Wild Queries, we employ the prompt in Figure 12 to extract the entities from each query."}]}