{"title": "Harmonizing Attention: Training-free Texture-aware Geometry Transfer", "authors": ["Eito Ikuta", "Yohan Lee", "Yu Saito", "Akihiro Iohara", "Toshiyuki Tanaka"], "abstract": "Extracting geometry features from photographic images independently of surface texture and transferring them onto different materials remains a complex challenge. In this study, we introduce Harmonizing Attention, a novel training-free approach that leverages diffusion models for texture-aware geometry transfer. Our method employs a simple yet effective modification of self-attention layers, allowing the model to query information from multiple reference images within these layers. This mechanism is seamlessly integrated into the inversion process as Texture-aligning Attention and into the generation process as Geometry-aligning Attention. This dual-attention approach ensures the effective capture and transfer of material-independent geometry features while maintaining material-specific textural continuity, all without the need for model fine-tuning.", "sections": [{"title": "1. Introduction", "content": "In computer vision, image harmonization is the task of seamlessly integrating a foreground object from one image into the background of another to produce a cohesive composite. A significant challenge is achieving visual harmony by adjusting the foreground's appearance to align with the background. Traditional methods [1-10] have primarily addressed image harmonization by focusing on color and illumination adjustments.\nRecent advances in deep learning and diffusion models [11-21] have enabled sophisticated image composition and harmonization, as exemplified by ObjectStitch [20] and TF-ICON [21]. These methods primarily focus on style transfer, encompassing a broad range of attributes such as color schemes, brushstrokes, textures, and patterns that collectively determine an image's visual appearance. However, the selective transfer of geometrical features-which we term \"geometry\" in this paper such as holes, cracks, droplets, and dents from one material to another, independently of material-specific surface texture (e.g., wood grain or fabric weave), remains a complex challenge. To the best of our knowledge, existing techniques have not successfully addressed this issue. Such geometry transfer is crucial for creating realistic and aesthetically pleasing composite images, particularly in applications requiring the integration of complex textures from different materials, like transferring wood hole geometry onto metal surfaces. Traditional and recent deep-learning-based image harmonization methods struggle with geometry transfer due to their focus on color, illumination, and style adjustments of the object itself. This limitation highlights the need for a fundamentally different approach that can effectively decouple geometry from material-specific textures in image harmonization tasks.\nTo address this challenge, we propose Harmonizing Attention, a novel training-free diffusion-model-based approach for texture-aware geometry transfer. The key aspect of this method is a custom attention mechanism comprising Texture-aligning Attention and Geometry-preserving Attention, enabling reference to both source geometry and target texture information. By replacing the self-attention layers in the diffusion model with these custom attention layers during inversion and generation, we successfully incorporate geometry and texture from different images, as demonstrated in Fig. 1. Notably, since the Harmonizing Attention framework leverages pretrained diffusion models to perform geometry transfer, it requires neither model tuning nor prompt exploration for each dataset.\nOur contributions are summarized as follows:\n1. We introduce texture-aware geometry transfer, a novel approach for integrating geometry details from different materials into cohesive composite images.\n2. We develop Harmonizing Attention, a training-free mechanism enabling models to query information from multiple reference images within self-attention blocks, effectively capturing and transferring material-independent geometry while preserving material-specific textures.\n3. Our experiments demonstrate that Harmonizing Attention produces more harmonious and realistic composite images compared with existing techniques."}, {"title": "2. Related Work", "content": "Image Harmonization. Conventionally, image harmonization [1-10] has focused on color-to-color transformations to match visual appearances. These methods can be further divided into non-linear transformations [7, 9] and linear transformations [3, 6, 10]. Recently, deep-learning-enabled image harmonization emerged [11-19] and more recently diffusion-model-based harmonization techniques [20, 21] have been developed. One notable example is TF-ICON [21], which equips attention-based text-to-image diffusion models for image harmonization, enabling cross-domain image-guided composition.\nPaintely Image Harmonization. In the context of image harmonization, a similar task called painterly image harmonization [22-27] has been studied, and it might be another promising candidate technique for geometry transfer. The painterly image harmonization technique integrates a photographic foreground into an artistic background, resulting in a visually coherent painting. PHDiffusion [28] represents a significant pivot by adapting the GAN framework (PHDNet [22]) to a diffusion model with an adaptive encoder. Furthermore, TF-GPH [29], which utilizes an image-wise attention-sharing mechanism for general painterly harmonization, is proposed for training-free general painterly harmonization and provides flexible options for attention-based image editing methods."}, {"title": "3. Methods", "content": "Our objective is to synthesize an image $I_{out}$ that seamlessly integrates surface geometry information from a source image $I_{src}$ into a target background image $I_{tar}$. This integration is guided by a 0-1-valued foreground mask $M_{src}$, preserving the geometry of $M_{src} \\odot I_{src}$ while maintaining textural continuity with $I_{tar}$, independently of the source's original texture, where $\\odot$ denotes the Hadamard (elementwise) product. To achieve this, we propose a novel framework called Harmonizing Attention. As illustrated in Fig. 2, this framework leverages Stable Diffusion (SD) [30] and encompasses both inversion and generation processes. The key aspect of our approach is modifying self-attention computation during both inversion and generation to query additional information from $I_{src}$ and $I_{tar}$, enabling a more coherent and context-aware transfer process. Furthermore, to enhance textural continuity within the target region, we utilize the SD inpainting model rather than the text-to-image model.\nEditing. The initial stage of our framework involves preparation of a geometry image $I_{geo}$, which serves as a cornerstone for the subsequent image generation process.\nA geometry image is simply an image patch of a source image cropped by a 0-1 source mask $M_{src}$, with affine transformations $T$ (shifting, scaling, and rotation) according to the user's needs, as well as the color adjustment described below (see the uppermost panel in Fig. 2). The ultimate objective of our framework is to seamlessly synthesize this geometry image with the target image.\nThe color adjustment of the geometry image plays a crucial role in that it facilitates the inversion process to work properly. By aligning the color profile of the source region with that of the target image, we expect that it will be easier to bring the geometry image closer to the visual domain of the target through the inversion procedure. Because the geometry information is expressed through localized color differences rather than absolute values, we employ a simple uniform color shift, although more sophisticated color adjustment methods would also be feasible. Specifically, we compute this shift by calculating the difference between two color averages: (1) $c_{src} \\in \\mathbb{R}^3$, which is the mean color of the source image $I_{src}$ within the region formed by subtracting the source mask $M_{src}$ from its dilated version (i.e., the boundary of the complementary mask $M_{comp}$ in mathematical morphology, where the complementary mask $M_{comp}$ of a mask $M$ is defined by replacing 0 with 1 and vice versa in $M$); (2) $c_{tar} \\in \\mathbb{R}^3$, which is the mean color of the target image $I_{tar}$ within an analogous region, defined by subtracting the transplantation-area mask $M_{geo}:= T(M_{src})$ from its dilated version, where the transplantation-area mask $M_{geo}$ defines the transplantation region in the target-image coordinate. This process is represented by the following equation:\n$I_{geo}^{x,y} = I_{geo}^{x,y} + aM_{geo}(c_{tar} - c_{src}),$   (1)\nwhere $p_{geo}^{x,y}, p_{geo}^{x,y} \\in \\mathbb{R}^3$ represent the pixel values at coordinates $(x, y)$ in the image before and after color adjustment, respectively. $M_{geo} \\in \\{0, 1\\}$, value of $M_{geo}$ at coordinates $(x, y)$, becomes 1 only if the pixel $(x, y)$ in the target image is within the transplantation region. Equation (1)\nthen shows that the color adjustment is done only within the transplantation region. The scalar parameter $a \\in [0, 1]$ controls the strength of the color adjustment. It is anticipated that employing a small value of $a$ would result in inadequate integration of the transplantation region with the target image, while a large value of $a$ is expected to compromise the integrity of the source geometry.\nInversion. The geometry image $I_{geo}$ inherently retains substantial textural information from the source image. Consequently, a direct pasting of the geometry image onto the target image $I_{tar}$ (\"pasted image\" in Fig. 2) may result in textural discontinuities at the boundaries of $M_{geo}$ in the final output. To address this issue, our objective here is to derive a latent representation that effectively translates the geometry image into the visual domain of the target image through inversion. This domain-shifted latent representation facilitates seamless blending within the latent space, thereby enabling a more naturalistic geometry transplantation. It should be noted that since we leverage SD, the diffusion process operates in the latent space encoded by the variational autoencoder (VAE) instead of the pixel space.\nTo achieve this, we simultaneously invert the geometry and target images by Denoising Diffusion Implicit Models (DDIM) inversion [31], with replacing self-attention for the geometry image with a novel attention computation which we call Texture-aligning Attention, which additionally incorporates target image information. Let $z$ denote the latent representations in the VAE-encoded space, where specific instances will be distinguished by appropriate subscripts in the subsequent discussion. The standard self-attention computation is formulated as:\n$A(z) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})V,$   (2)\nwhere $A(\\cdot)$ denotes the functional representation of self-attention computation, where $d$ and $T$ denote the dimension of the latent space and the transposition of a matrix, respectively, and where $Q$, $K$, and $V$ represent the query, key, and value, respectively, all computed from the latents $z$ corresponding to the same image. Note that the diffusion step $t$ and the layer index $l$ are described in Fig. 2, but omitted here for readability. To align the source image domain with the target image domain, we replace the self-attention computation in the inversion of the geometry image with the following formula:\n$A_{TA}(z_{geo}; z_{tar}) = \\text{Softmax}(\\frac{Q_{geo} [K_{geo}]^T}{\\sqrt{d}} [K_{tar}] )\\begin{bmatrix}V_{geo}\\\\V_{tar}\\end{bmatrix},$   (3)\nwhere $A_{TA}(\\cdot)$ denotes Texture-aligning Attention, and where the superscripts $geo$ and $tar$ denote whether the respective $Q$, $K$, and $V$ are computed from the geometry\nimage feature map $z_{geo}$ or the target image feature map $z_{tar}$. As previously elucidated, the geometry image preserves the relative color variations associated with surface geometry through the color adjustment, while simultaneously approximating the visual characteristics of the target image. Consequently, by retaining both the conventional self-attention key ($K_{geo}$) and value ($V_{geo}$) alongside their counterparts derived from the target image ($K_{tar}$ and $V_{tar}$), we anticipate a synergistic effect. This dual-source attention mechanism is expected to efficiently facilitate both geometry preservation and material transformation during the inversion process. A noisy latent representation $z_{geo}$ of the geometry image $I_{geo}$ is then obtained as a result of $T$ iterations of the inversion step using this custom attention. Inversion of the source image $I_{src}$ and the target image $I_{tar}$ is performed independently, employing the standard self-attention mechanism without any modifications. The resulting noisy latent representations $z_{src}$ and $z_{tar}$ are then stored for subsequent use in the generation stage and in the blending stage, respectively.\nBlending. Utilizing the latent representations obtained in the inversion stage, in the blending stage (see the lower-middle panel in Fig. 2) we perform a latent-space blending between the geometry and target images. Let $m_{geo}$ be the target mask $M_{geo}$ resized to the latent space. The blended latent representation $z_{out}$ to be used for generating a synthesized image is computed from the latents $z_{geo}^{T}$ and $z_{tar}^{T}$ for the geometry and target images at diffusion step $T$ as follows:\n$z_{T}^{out} = z_{T}^{geo} \\odot m_{geo} + z_{T}^{tar} \\odot m_{geo}^{comp}.$   (4)\nwhere $m_{geo}^{comp}$ denotes the complementary mask of $m_{geo}$, defined by replacing 0 with 1 and vice versa in $m_{geo}$.\nGeneration. In the generation stage (see the lower-right panel in Fig. 2), we obtain the final output image by denoising the blended latent code $z_{out}^{T}$ synthesized in the blending stage. Although we initialize the generation process with the latents that explicitly incorporate a portion of the source image, it turns out that applying standard denoising procedures may potentially result in the undesirable removal of the transplanted content along with the noise, leading to the loss of the geometry.\nTo address this challenge, we introduce a novel attention computation named Geometry-preserving Attention. Specifically, we concurrently perform denoising on the source image latent representation $z_{src}^{T}$ obtained during the inversion stage and replace the self-attention computation for the blended latent representation $z_{out}^{T}$ with Geometry-preserving Attention by augmenting the key and value components with information derived from the source image.\nThe process is formally expressed by the following equation:\n$A_{GP}(z_{geo}; z_{tar}) = \\text{Softmax}(\\frac{Q_{geo} [K_{out}]^T}{\\sqrt{d}} [K_{src}] )\\begin{bmatrix}V_{out}\\\\V_{src}\\end{bmatrix},$   (5)\n$K^{l,src} = \\{K_{i}^{l,src} | m_{i}^{src} \\neq 0, i \\in \\{1, ..., H \\times W\\}\\},$   (6)\n$V^{l,src} = \\{V_{i}^{l,src} | m_{i}^{src} \\neq 0, i \\in \\{1, ..., H \\times W\\}\\}.$   (7)\nHere, $A_{GP}$ represents the functional form of the Geometry-preserving Attention. The superscripts $out$ and $src$ denote that the corresponding variables are associated with the $z_{out}$ or $z_{src}$, respectively. It is important to note that $K^{l,src}$ and $V^{l,src}$ are used instead of $K_{tar}^{l,src}$ and $V_{tar}^{l,src}$ to distinguish them from the keys and values used during the inversion process. $H$ and $W$ denote the height and width of the latent feature map $z_{src}$, $m^{src} \\in \\{0, 1\\}^{H \\times W}$ represents the source mask $M^{src}$ resized to the latent space, and $m_{i}^{src} \\in \\{0, 1\\}$ represents i-th element of it."}, {"title": "4. Experiments", "content": "4.1. Setup\nWe use the publicly available SD inpainting model checkpoint on HuggingFace\u00b9 as the backbone network. For our experiments, both source image $I_{src}$ and target image $I_{tar}$ are cropped to a uniform size of 512 \u00d7 512 pixels. When performing color adjustment as expressed in Eq. (1), we set the scalar parameter $a$ to 0.5. We set the number of diffusion steps $T$ to 25 and employ the DDIM sampler for both the inversion and generation processes, with no prompt input (i.e., an empty string is used) in either phase.\nDuring the inversion phase, we leverage the insights from Garibi et al. [32], which demonstrates that iterative inversion sampling at each diffusion step enhances inversion accuracy. Specifically, we perform 5 sampling iterations per diffusion step in the inversion procedure.\n4.2. Datasets and Metrics\nWe use images from MVTec AD [33,34] and Pixabay\u00b2 in our experiments. For the quantitative evaluation, we employ the following three metrics: LPIPS [35], CLIP [36], and DISTS [37], since metrics specifically tailored for geometry transfer do not currently exist. These established metrics allow us to assess our method's performance in terms of image harmonization, and compare ours with existing methods. Each metric is evaluated against both the composite image background (denoted LPIPS(bg), CLIP(bg), and DISTS(bg)) and the local region of the stretched and harmonized image (denoted LPIPS(fg), CLIP(fg), and DISTS(fg)).\nHere, (bg) and (fg) refer to background and foreground, respectively. It is noteworthy that LPIPS and CLIP have previously been used as evaluation metrics in the existing literature [21, 29]. A total of 150 generated images are used for the quantitative performance evaluation, consisting of 10 composite foreground images and 15 background images, resulting in 150 combinations. Scores are calculated for each method, and the averages of the scores are presented.\n4.3. Baselines\nFor comparison, we select four diffusion-model-based methods, including two image harmonization methods PHDiffusion [28] and TF-GPH [29], one image composition method TF-ICON [21], and one image editing method Paint by Example [38]. Since our Harmonizing Attention does not require prompts, we manually set suitable prompts for generating each sample with TF-ICON. For all methods, we use the default or recommended hyperparameters.\n4.4. Qualitative Comparison\nWe present a qualitative comparison of our proposed method, Harmonizing Attention, with four existing techniques: Paint by Example [38], TF-ICON [21], PHDiffusion [28], and TF-GPH [29]. Paint by Example and TF-ICON struggled with consistent texturing and maintaining original geometries. PHDiffusion showed better texture preservation but struggled with accurate geometry transfer. TF-GPH achieved a balance between texture and geometry alignment compared with previous methods, but still showed subtle texture inconsistencies and degradation of geometric properties upon closer inspection.\nIn contrast, our proposed method excelled in several critical aspects of image harmonization. It preserved the geometry and structural details of foreground objects with high fidelity. Simultaneously, it successfully adapted the texture and material properties to seamlessly match the background. This dual capability resulted in the most natural and realistic geometry integration across various scenarios, from textured surfaces such as wood to complex materials like metal parts.\nUser Study To further compare the apparent quality of each technique, we conducted a user study with 105 participants from CrowdWorks\u00b3. Participants rated 15 images, shown in Fig. 3, on a scale from 1 to 5 (1 being the worst and 5 being the best) on the basis of the following criteria: background preservation (referred to as Quality Of Background, QOB), foreground preservation (referred to as Quality Of Foreground, QOF), and seamless composition (referred to as Quality Of Composition, QOC). Each qualitative rating score was calculated by applying weights to\ntent across all three conditions.\nThe qualitative results under these conditions are shown in Fig. 4. For reference, we also include pasted images, which are geometry images obtained under each condition directly pasted onto the target image. Comparing (i) and (ii), (iii) in Fig. 4, we can see that without color adjustment, the texture remained unchanged, while with color shift, the geometry was reconstructed with a texture that was blended into the target image. Also, comparing (ii), (iii) and (iv), we find that when histogram matching was used, the relative color changes within the source image area to be transplanted were not preserved, resulting in loss of three-dimensionality. Finally, comparing (ii) and (iii), a stronger color shift tends to improve texture blending, but tends to reduce the three-dimensionality of the object, making it difficult to maintain the original geometry. These results suggest that a modest shift in the color of the source image to that of the target image is effective in transferring geometry.\n4.6.2 Effectiveness of Texture-aligning Attention\nWe examined the effectiveness of Texture-aligning Attention. As previously explained, the primary objective of the inversion phase is to align the geometry image, created from the source image, with the visual domain of the target image. Intuitively, one might expect that using only the keys $K^{tar}$ and values $V^{tar}$ computed from the target image in Eq. (3), while eliminating $K^{geo}$ and $V^{geo}$ derived from the geometry image, would yield results more harmoniously integrated with the target image. To evaluate the impact of each $K$, $V$ on this alignment, we conducted an ablation study comparing three inversion configurations: (i) employing only target-image-derived attention (i.e., eliminating $K^{geo}$ and $V^{geo}$), and (ii) utilizing only geometry-image-derived attention (i.e., removing $K^{tar}$ and $V^{tar}$). (iii) using both geometry-image-derived and target-image-derived attention components, It is noted that we used the same settings for editing, blending, and generation.\nFigure 5 presents a comparative analysis of generation results under the three conditions. Contrary to the initial intuition, the samples for condition (i) demonstrate that using only $K^{tar}$ and $V^{tar}$ does not lead to better integration with the target image. Instead, as shown in condition\n4.6.3 Effectiveness of Geometry-preserving Attention\nWe investigated the significance of Geometry-preserving Attention. Specifically, we compared generation results un-"}, {"title": "4.7. Limitation", "content": "The primary limitation of our work lies in its difficulty in transferring extremely large or small geometries. As evident from Eqs. (3) and (5), the ratio of texture-related to geometry-related information in our customized attention calculation is strongly dependent on the size of the geometry transfer area. In addition, the generated output occasionally exhibits significant geometric deviations from intended results or produces unrelated content. This limitation might\nbe attributed to our method's reliance on a pretrained SD model without additional training, potentially constraining its ability to fully capture texture and geometry. Future improvements could focus on dynamically adjusting attention based on transfer area size, developing more robust attention mechanisms for diverse image combinations, and expanding generatable geometries through advanced training or architectural improvements. These refinements aim to address the current limitations and further improve the versatility and reliability of our approach across a wider range of geometry and image combinations."}, {"title": "5. Conclusion", "content": "In this work, we introduce Harmonizing Attention, a novel approach that facilitates the effective capture and transfer of material-independent geometry while preserving material-specific textural continuity. Our method uses custom Texture-aligning and Geometry-preserving Attention during inversion and generation processes, respectively, enabling the simultaneous referencing of source geometry and target texture information. Our approach achieves effective geometry transfer without requiring additional training or prompt engineering. Our method presented herein not only improves the creation of photorealistic composites but also expands the horizons of computer vision applications, ranging from augmented reality to advanced image editing."}]}