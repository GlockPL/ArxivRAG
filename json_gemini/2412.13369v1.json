{"title": "Multiple Mean-Payoff Optimization under Local Stability Constraints", "authors": ["David Kla\u0161ka", "Anton\u00edn Ku\u010dera", "Vojt\u011bch K\u016fr", "V\u00edt Musil", "Vojt\u011bch \u0158eh\u00e1k"], "abstract": "The long-run average payoff per transition (mean payoff) is\nthe main tool for specifying the performance and dependabil-\nity properties of discrete systems. The problem of construct-\ning a controller (strategy) simultaneously optimizing several\nmean payoffs has been deeply studied for stochastic and\ngame-theoretic models. One common issue of the constructed\ncontrollers is the instability of the mean payoffs, measured by\nthe deviations of the average rewards per transition computed\nin a finite \"window\" sliding along a run. Unfortunately, the\nproblem of simultaneously optimizing the mean payoffs un-\nder local stability constraints is computationally hard, and the\nexisting works do not provide a practically usable algorithm\neven for non-stochastic models such as two-player games. In\nthis paper, we design and evaluate the first efficient and scal-\nable solution to this problem applicable to Markov decision\nprocesses.", "sections": [{"title": "Introduction", "content": "Mean payoff, i.e., the long-run average payoff per time unit,\nis the standard formalism for specifying and evaluating the\nlong-run performance of dynamic systems. The overall per-\nformance is typically characterized by a tuple of mean pay-\noffs computed for multiple payoff functions representing ex-\npenditures, income, resource consumption, and other rele-\nvant aspects. The basic task of multiple mean-payoff op-\ntimization is to design a controller (strategy) jointly opti-\nmizing these mean payoffs. Efficient strategy synthesis al-\ngorithms have been designed for various models, such as\nMarkov decision processes or two-player games (see Re-\nlated work).\n\nA fundamental problem of the standard mean payoff opti-\nmization is the lack of local stability guarantees. For exam-\nple, consider a payoff function modeling the expenditures\nof a company. Even if the long-run average expenditures\nper day (mean payoff) are $1000, there are no guarantees\non the maximal average expenditures per day in a bounded\ntime horizon of, say, one month. It is possible that the com-\npany pays between $800 and $1200 per day every month,\nwhich is fine and sustainable. However, it may also happen\nthat there are \"good\" and \"bad\" months with average daily\nexpenditures of $0 and $5000, respectively, where the sec-\nond case is not so frequent but represents a critical cashflow\nproblem that may ruin the company. The mean payoff does\nnot reflect this difference; hence, optimizing the mean pay-\noff (minimizing the overall long-run expenditures) may lead\nto disastrous outcomes in certain situations.\n\nThe lack of local stability guarantees has motivated the\nstudy of window mean payoff objectives, where the task is to\noptimize the average payoff in a \u201cwindow\u201d of finite length\nsliding along a run. The window size represents a bounded\ntime horizon (in the above example, the horizon of one\nmonth), and the task is to construct a strategy such that the\naverage payoff computed for the states in the window stays\nwithin given bounds. Thus, one can express both long-run\naverage performance and stability constraints. For a single\npayoff function, some technical variants of this problem are\nsolvable efficiently, while others are PSPACE-hard. For mul-\ntiple window mean payoffs, intractability results have been\nestablished for two-player games (see Related work).\n\nThe main concern of the previous works on window mean\npayoffs is classifying the computational complexity of con-\nstructing an optimal strategy. The obtained algorithms are\nbased on reductions to other problems, and their complex-\nity matches the established lower bounds. To the best of our\nknowledge, there have been no attempts to tackle the high\ncomplexity of strategy synthesis by designing scalable al-\ngorithms at the cost of some (unavoidable but acceptable)\ncompromises. This open challenge and the problem's high\npractical relevance are the primary motivations for our work.\n\nOur Contribution. We design the first efficient and scal-\nable strategy synthesis algorithm for optimizing multiple\nwindow mean payoffs in a given Markov decision process.\n\nWe start by establishing the principal limits of our efforts\nby showing that the problem is NP-hard even for simple in-\nstances where the underlying MDP is a graph. Consequently,\nevery efficient algorithm attempting to solve the problem\nmust inevitably suffer from some limitations. Our algorithm\ntrades efficiency for optimality guarantees, i.e., the com-\nputed solutions are not necessarily optimal. Nevertheless,\nour experiments show that the algorithm can construct high-\nquality (and sometimes quite sophisticated) strategies for\ncomplicated instances of non-trivial size. Thus, we obtain\nthe first practically usable solution to the problem of multi-\nple window mean payoff optimization.\n\nMore concretely, the optimization objective is specified\nby a function Eval measuring the \"badness\" of the achieved"}, {"title": "The Model", "content": "We assume familiarity with basic notions of probability\ntheory (probability distribution, expected value, conditional\nrandom variables, etc.) and Markov chain theory. The set\nof all probability distributions over a finite set A is denoted\nby Dist(A). We use \\(\\mathbb{N}\\) and \\(\\mathbb{Q_+}\\) do denote the set of non-\nnegative integers and non-negative rationals, respectively.\n\nMarkov chains. A Markov chain is a triple\n\\(M = (S, Prob, \\mu)\\) where S is a finite set of states,\n\\(Prob : S \\times S \\rightarrow [0,1]\\) is a stochastic matrix where\n\\(\\sum_{t \\in S} Prob(s,t) = 1\\) for all \\(s \\in S\\), and \\(\\mu \\in Dist(S)\\)\nis an initial distribution.\n\nA run in M is an infinite sequence of states. We use \\(\\mathbb{P}\\) to\ndenote the standard probability measure over the runs of M\n(see, e.g., (Norris 1998))\n\nA state t is reachable from a state s if \\(Prob^n(s,t) > 0\\)\nfor some \\(n \\geq 1\\). We say that M is strongly connected (or\nirreducible) if all states are mutually reachable from each\nother. For an irreducible M, we use \\(I \\in Dist(S)\\) to de-\nnote the unique invariant distribution satisfying \\(I = I Prob\\)\n(note that I is independent of \\(\\mu\\)). By ergodic theorem (Nor-\nris 1998), I is the limit frequency of visits to the states of S\nalong a run. More precisely, let \\(w = s_0, s_1,...\\) be a run\nof M. For every \\(s \\in S\\), let\n\\[\nFreq_s(w) = \\lim_{n \\rightarrow \\infty} \\frac{\\#_s(s_0,...,s_{n-1})}{n}\n\\]\nwhere \\(\\#_s(s_0,..., s_{n-1})\\) is the number of occurrences of s\nin \\(s_0,..., s_{n-1}\\). If the above limit does not exist, we put\n\\(Freq_s(w) = 0\\). Furthermore, let \\(Freq(w) : S \\rightarrow [0, 1]\\) be the\nvector of all \\(Freq_s(w)\\) where \\(s \\in S\\). The ergodic theorem\nsays that \\(\\mathbb{P}[Freq=I] = 1\\).\n\nA bottom strongly connected component (BSCC) of M is\na maximal BCS such that B is strongly connected and\nclosed under reachable states. Note that if M is irreducible,\nthen the set S is the only BSCC of M. Otherwise, M can\nhave multiple BSCCs, and each such B can be seen as an\nirreducible Markov chain where the set of states is B and\nthe probability matrix is the restriction of Prob to B\u00d7B.\n\nFor the rest of this section, we fix an irreducible Markov\nchain \\(M = (S, Prob, \\mu)\\).\n\nGlobal and Local Mean Payoff. Let \\(Pay: S \\rightarrow \\mathbb{N}\\) be a\npayoff function. For every run \\(w = s_0, s_1, . . .\\), let\n\\[\nGMP(w) = \\lim_{n \\rightarrow \\infty} \\frac{1}{n}\\sum_{i=0}^{n-1} Pay(s_i)\n\\]\nbe the limit-average payoff per transition along the run w.\nIf the limit does not exist, we put \\(GMP(w) = 0\\). An im-\nmediate consequence of the ergodic theorem (see above) is\nthat the defining limit of \\(GMP(w)\\) exists and takes the same\nvalue \\(gmp = \\sum_{s \\in S} I(s) \\cdot Pay(s)\\) for almost all runs, i.e.,\n\\(\\mathbb{P}[GMP=gmp] = 1\\). We refer to the (unique) value gmp of\n\\(GMP\\) as the global mean payoff.\n\nLet \\(d \\in \\mathbb{N}\\) be a time horizon. For every \\(j \\in \\mathbb{N}\\), let\n\\[\nWMP[d, j](w) = \\frac{1}{d} \\sum_{i=j}^{j+d-1} Pay(s_i)\n\\]\nbe the average payoff computed for the sequence of d con-\nsecutive states in w starting with \\(s_j\\). We refer to the value\n\\(WMP[d, j]\\) as the window mean payoff after j steps (assum-\ning some fixed d).\n\nNote that as \\(d \\rightarrow \\infty\\), the value of \\(WMP[d, j]\\) is arbitrar-\nily close to gmp with arbitrarily large probability, indepen-\ndently of j. However, for a given d, the value of \\(WMP[d, j]\\)\ndepends on j and can be rather different from gmp. Also ob-\nserve that \\(WMP[d, j]\\) is a discrete random variable, and the\nunderlying distribution depends only on the state \\(s_j\\). More\nprecisely, for all j, m \\(\\in \\mathbb{N}\\) and \\(s \\in S\\) such that \\(\\mathbb{P}[s_j=s] > 0\\)\nand \\(\\mathbb{P}[s_m=s] > 0\\), we have that the conditional random vari-\nables \\(WMP[d, j] \\mid s_j=s\\) and \\(WMP[d, m] \\mid s_m=s\\) are identi-\ncally distributed. In the following, we write just \\(WMP[d, s]\\)\ninstead of \\(WMP[d, j]| s_j=s\\) where \\(\\mathbb{P}[s_j=s] > 0\\).\n\nMean Payoff Objectives. Let \\(Pay_1,..., Pay_k : S \\rightarrow \\mathbb{N}\\)\nbe payoff functions. The standard multi-objective opti-\nmization problem for Markov Decision Processes (see be-\nlow) is to jointly maximize the global mean payoffs for\n\\(Pay_1,..., Pay_k\\). In this paper, we use a more general ap-\nproach where the \"badness\" of the achieved mean payoffs\nis measured by a dedicated function \\(Eval : \\mathbb{R}^k \\rightarrow \\mathbb{R}\\). A\nsmaller value of Eval indicates more appropriate payoffs.\n\nFor example, the joint maximization of the mean pay-\noffs can be encoded by \\(Eval_{max}(x) = - \\sum_{i=1}^k (Pay_{max}-x_i)\\),\nwhere \\(Pay_{max} = \\max_{s \\in S} Pay_i(s)\\). The defining sum can\nalso be weighted to reflect certain priorities among the\nPareto optimal solutions. In general, Eval can encode the\npreference of keeping the mean payoffs close to some val-\nues, within some interval, or even enforce some mutual de-\npendencies among the mean payoffs.\n\nAs we shall see, our strategy synthesis algorithm works\nfor an arbitrary Eval that is decomposable. Intuitively, the\ndecomposability condition enables more efficient evalua-\ntion/differentiation of Eval by dynamic programming with-\nout substantially restricting the expressive power."}, {"title": "The Optimization Problem", "content": "In this section, we define the multiple window mean-payoff\noptimization problem and examine the principal limits of its\ncomputational tractability.\n\nLet \\(D = (V, E, p)\\) be an MDP, \\(Pay_1,..., Pay_k: V \\rightarrow \\mathbb{N}\\)\npayoff functions, and \\(Eval : \\mathbb{R}^k \\rightarrow \\mathbb{R}\\) an evaluation func-\ntion. Furthermore, let \\(d \\in \\mathbb{N}\\) be a time horizon. The task is\nto construct an FR strategy \\(\\sigma\\) and an initial augmented vertex\nso that the value of wval is minimized.\n\nMore precisely, for a given FR strategy \\(\\sigma\\), the function\nwval is evaluated as follows. Every \\(Pay_i\\) is extended to\nthe augmented vertices of \\(D^\\sigma\\) by defining \\(Pay_i(\\bar{v}) = Pay_i(v)\\).\nLet \\(C_1,..., C_n\\) be the BSCCs of the Markov chain \\(D^\\sigma\\).\nRecall that every \\(C_i\\) can be seen as an irreducible Markov\nchain. We use \\(wval(C_i)\\) to denote the window mean payoff\nvalue computed for \\(C_i\\). The value of wval achieved by \\(\\sigma\\),\ndenoted by \\(wval^\\sigma\\), is defined as\n\\[\nwval^\\sigma = \\min\\{wval(C_i) \\mid 1 \\leq i \\leq n\\}\n\\]\nRecall that the initial augmented vertex can be chosen freely,\nwhich is reflected in the above definition (the strategy \\(\\sigma\\) can\nbe initiated directly in the \"best\" BSCC and thus achieve the\nvalue \\(wval^\\sigma\\)).\n\nA FR-strategy \\(\\sigma\\) is \\(\\epsilon\\)-optimal for a given \\(\\epsilon \\geq 0\\) if\n\\[\nwval^\\sigma - \\epsilon < \\inf_{\\pi} wval^\\pi\n\\]\nwhere \\(\\pi\\) ranges over all FR strategies. A 0-optimal strategy\nis called optimal.\n\nThe next theorem shows that computing an optimal strat-\negy is computationally hard, even for restricted subsets of\ninstances.\n\nTheorem 1 Let \\(D = (V, E, p)\\) be a graph, \\(Pay_1,..., Pay_k\\)\npayoff functions, \\(d \\leq |V|\\) a time horizon, and Eval an eval-\nuation function. The problem of whether there exists a FR\nstrategy \\(\\sigma\\) such that \\(wval^\\sigma < 0\\) is NP-hard.\n\nFurthermore, the problem is NP-hard even if the set of\neligible instances is restricted so that an optimal memory-\nless strategy is guaranteed to exist, and one of the following\nthree conditions is satisfied:\n\nA. k=1 (i.e., there is only one payoff function).\n\nB. k=2, and there are thresholds \\(c_1, c_2 \\geq 0\\) such that\n\\(Eval(K_1, K_2) = 0\\) iff \\(K_1 \\geq c_1\\) and \\(K_2 \\geq c_2\\).\n\nC. There is a constant \\(r \\in \\mathbb{N}\\) such that \\(k \\leq |V|= l_x\\). For\nevery \\(x \\in N^C\\), let \\(Occ_x\\) be an indicator assigning to every\nrun \\(w = v_0, v_1,. . .\\) of C either 1 or 0 so that \\(Occ_x(w) = 1\\)\niff \\(\\#_v(v_0,... v_{l_x-1}) = x(v)\\) for every \\(v \\in C\\).\n\nFor every \\(x \\in N^C\\), let\n\\[\nWMP_i(x) = \\frac{\\sum_{v \\in C} x(v) \\cdot Pay_i(v)}{l_x}\n\\]"}, {"title": "The Algorithm", "content": "For the rest of this section, we fix an MDP \\(D = (V, E,p)\\),\na time horizon d, payoff functions \\(Pay_1,..., Pay_k\\), and an\nevaluation function Eval.\n\nOur algorithm is based on optimizing wval by the meth-\nods of differentiable programming. The core ingredient is\na dynamic procedure for computing the expected value of\n\\(Eval(WMP_1[d, s], . . ., WMP_k[d, s])\\) (see (1)). The proce-\ndure is designed so that the gradient of wval can be com-\nputed using automatic differentiation. Then, we show how\nto incorporate this procedure into a strategy-improvement\nalgorithm for wval. For the rest of this section, we fix a mem-\nory allocation \\(\\alpha: V \\rightarrow 2^M\\).\n\nRepresenting FR Strategies. For every pair of augmented\nvertices \\((v, u)\\) such that \\((v, u) \\in E\\), we fix a real-valued pa-\nrameter representing \\(\\sigma(v)(u)\\). Note that if v is stochastic,\nthen the parameter actually represents the probability of se-\nlecting the memory state of \\(\\bar{u}\\). These parameters are initial-\nized to random values, and we use the standard \\(SOFTMAX\\)\nfunction to transform these parameters into probability dis-\ntributions. Thus, every function F depending on \\(\\sigma\\) becomes\na function of the parameters, and we use \\(\\nabla F\\) to denote\nthe corresponding gradient.\n\nComputing wval. Let \\(\\sigma\\) be an FR strategy where \\(\\alpha\\) is the\nmemory allocation. We show how to compute wval inter-\npreted as a function of the parameters representing \\(\\sigma\\).\n\nRecall that \\(wval^\\sigma = \\min\\{wval(C_i) \\mid 1 \\leq i \\leq n\\}\\). Hence,\nthe first step is to compute all BSCCs of \\(D^\\sigma\\) by the Tarjan's\nalgorithm (Tarjan 1972). Then, for each BSCC C, we com-\npute wval(C) in the following way.\n\nThe invariant distribution \\(I^C\\) is computed as the unique\nsolution of the following system of linear equations: For ev-\nery \\(v \\in C\\), we fix a fresh variable \\(x_v\\) and the corresponding\nequation \\(x_v = \\sum_{\\bar{u} \\in C} x_{\\bar{u}} \\cdot \\sigma(\\bar{u})(v)\\). Furthermore, we add\nthe equation \\(\\sum_{\\bar{u} \\in C} x_{\\bar{u}} = 1\\). Recall that \\(I^C\\) is the unique\ndistribution satisfying \\(I^C = I^C \\cdot Prob_C\\), where \\(Prob_C\\) is\nthe probability matrix of C. Hence, \\(I^C\\) is the unique solution of\nthe constructed system.\n\nThe main challenge is to compute the expected value\n\\[\n\\mathbb{E}[Eval(WMP_1[d, v^*], . . ., WMP_k[d, v^*])]\\quad\\quad (2)\n\\]\nfor each \\(v^* \\in C\\). This is achieved by Algorithm 1 described\nbelow. Then, wval(C) is calculated using (1).\n\nComputing the Expected Value of Eval by Dynamic Pro-\ngramming. In this section, we show how to compute (2)\nby dynamic programming for a given \\(v^* \\in C\\). We use \\(\\mathbb{P}\\) to\ndenote the probability measure over the runs in C, where the\ninitial distribution assigns 1 to \\(v^*\\).\n\nLet \\(N_C\\) be the set of vectors of non-negative integers in-\ndexed by the augmented vertices \\(v \\in C\\). For each \\(t \\in \\mathbb{N}\\), let\n\\(N_t^C = \\{x \\in N^C \\mid |x| = t\\}\\), where \\(|x| = \\sum_{u \\in C} x(u)\\). For\nevery \\(x \\in N_C\\), let \\(Occ_x\\) be an indicator assigning to every\nrun \\(w = v_0, v_1,. . .\\) of C either 1 or 0 so that \\(Occ_x(w) = 1\\)\niff \\(\\#_v(v_0,... v_{|x|-1}) = x(v)\\) for every \\(v \\in C\\).\n\nFor every \\(x \\in N_C\\), let\n\\[\nWMP_i(x) = \\frac{\\sum_{v \\in C} x(v) \\cdot Pay_i(v)}{|x|}\n\\]"}, {"title": "A Strategy Improvement Algorithm", "content": "In this section, we\ndescribe a strategy improvement algorithm WINMPSYNT\nthat inputs an MDP \\(D = (V, E, p)\\), payoff functions\n\\(Pay_1,..., Pay_k : V \\rightarrow \\mathbb{N}\\), a decomposable \\(Eval : \\mathbb{R}^k \\rightarrow \\mathbb{R}\\),\nand a time horizon \\(d \\in \\mathbb{N}\\), and computes an FR strategy \\(\\sigma\\)\nwith the aim of minimizing \\(wval^\\sigma\\).\n\nThe memory allocation function is a hyperparameter (by\ndefault, all memory states are assigned to every vertex).\nThe algorithm proceeds by randomly choosing the param-\neters representing a strategy. The values are sampled from\n\\(LOGUNIFORM\\) distribution so that no prior knowledge about\nthe solution is imposed. Then, the algorithm computes the\nBSCCs of \\(D^\\sigma\\) and identifies a BSCC C with the best\n\\(wval(C)\\). Subsequently, \\(wval(C)\\) is improved by gradient\ndescent. The crucial ingredient of WINMPSYNT is Algo-\nrithm 1, allowing to compute \\(wval(C)\\) and its gradient by\nautomatic differentiation. After that, the point represent-\ning the current strategy is updated in the direction of the\nsteepest descent. The intermediate solutions and the cor-\nresponding \\(wval(C)\\) values are stored, and the best solu-\ntion found within \\(STEPS\\) optimization steps is returned (the\nvalue of \\(STEPS\\) is a hyper-parameter). Our implementation\nuses \\(PYTORCH\\) framework (Paszke et al. 2019) and its au-\ntomatic differentiation with ADAM optimizer (Kingma and\nBa 2015). Observe that WINMPSYNT is equally efficient\nfor general MDPs and graphs. The only difference is that\nstochastic vertices generate fewer parameters."}, {"title": "Experiments", "content": "We perform our experiments on graphs to separate the\nprobabilistic choice introduced by the constructed strategies\nfrom the internal probabilistic choice performed in stochas-\ntic vertices. The graphs are structurally similar to the ones\nconstructed in the NP-hardness proof of Theorem 1. This\navoids bias towards simple instances. Recall that the prob-\nlem of constructing a (sub)optimal strategy for such graphs\nis NP-hard even if just one memory state is allocated to ev-\nery vertex, there are only two payoff functions, and we aim\nat pushing the window mean payoffs above certain thresh-\nolds (see item B. in Theorem 1).\n\nThe graphs \\(D_l\\). For every \\(l \\geq 2\\), we construct a directed\nring \\(D_l\\) with three \"layers\" where every vertex in the inner,\nmiddle, and outer layer is assigned a pair of payoffs (10, 0),\n(2, 2), and (0,10), respectively. The vertices are connected\nin the way shown in Fig. 2.\n\nThe Eval function. A scenario is a pair (l, d) where l, d\nare even integers in the interval [2, 20] representing \\(D_l\\) and\nthe window length d. For every scenario, we aim to push\nboth window mean payoffs simultaneously above a bound b,"}, {"title": "Conclusions", "content": "We have designed an efficient strategy synthesis algorithm\nfor optimizing multiple window mean payoffs capable of\nproducing high-quality solutions for instances of consider-\nable size. An interesting question is whether the proposed\napproach is applicable to a larger class of window-based op-\ntimization objectives such as the window parity objectives."}, {"title": "Supplementary Material", "content": "A Proof of Theorem 1\nLet us start by restating the theorem.\nTheorem 2 Let D = (V, E,p) be a graph, Pay1,..., Payk\npayoff functions, d \u2264 |V| a time horizon, and Eval an eval\nuation function. The problem of whether there exists a FR\nstrategy \u03c3 such that wval < 0 is NP-hard.\nFurthermore, the problem is NP-hard even if the set of\neligible instances is restricted so that an optimal memory\nless strategy is guaranteed to exist, and one of the following\nthree conditions is satisfied:\nA. k=1 (i.e., there is only one payoff function).\nB. k=2, and there are thresholds c1, c2 \u2265 0 such that\nEval(K1, K2) = 0 iff \u043a1 \u2265 c1 and \u043a2 \u2265 c2.\nC. There is a constant r \u2208 N such that k < |V|= lxi are consonants. For example if we have an input \"Lorem Ipsum\", then an out \"Lrm Ipsm\".\n```python\nimport re\ndef consonant_replace(text):\n    \n    return re.sub(r", "bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]": "", "print(consonant_replace(": "orem Ipsum"}]}