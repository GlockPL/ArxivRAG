{"title": "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "authors": ["Tanvir Mahmud", "Diana Marculescu"], "abstract": "Audio separation in real-world scenarios, where mixtures contain a variable number of sources, presents significant challenges due to limitations of existing models, such as over-separation, under-separation, and dependence on predefined training sources. We propose OpenSep, a novel framework that leverages large language models (LLMs) for automated audio separation, eliminating the need for manual intervention and overcoming source limitations. OpenSep uses textual inversion to generate captions from audio mixtures with off-the-shelf audio captioning models, effectively parsing the sound sources present. It then employs few-shot LLM prompting to extract detailed audio properties of each parsed source, facilitating separation in unseen mixtures. Additionally, we introduce a multi-level extension of the mix-and-separate training framework to enhance modality alignment by separating single source sounds and mixtures simultaneously. Extensive experiments demonstrate OpenSep's superiority in precisely separating new, unseen, and variable sources in challenging mixtures, outperforming SOTA baseline methods. Code is released at https://github.com/tanvir-utexas/OpenSep.git.", "sections": [{"title": "1 Introduction", "content": "Audio and music mostly appear in mixtures in real-world containing various audio sources and background noise (Kim et al., 2019). Separating clean sources from noisy mixtures have numerous applications in audio processing (Liu and Wang, 2018; St\u00f6ter et al., 2019). Precise separation of clean sound sources require their complete semantic understanding, as well as extensive training on natural mixtures. However, in open world scenarios, audio mixtures may contain a variable number of sources, as well as new, unseen, and possibly noisy sources. Hence, gathering clean sounds from a variety of sources, and modeling exhaustive mixture combi-\nations for training are often impractical, which limits the use of audio separators in practice.\nPrior work on audio separation mostly focused on two approaches: unconditional and conditional source separation. Unconditional separators (Yu et al., 2017; Wisdom et al., 2020) mostly attempt to disentangle the mixture into a fixed set of output predictions, and later rely on post-processing to select/process the separated sources. However, this approach is largely limited to training sources and mixture combinations, which, in practice, results in over-/under-separation (Karamatl\u0131 and K\u0131rb\u0131z, 2022). In addition, unconditional separators cannot provide the corresponding class entities of separated sources. Pishdadian et al. (2020) introduced source classifiers with audio separators, but their method is limited by training sources only.\nConditional source separation simplifies the"}, {"title": "2 Related Work", "content": "Unconditional Sound Separation Prior work on unconditional audio separation in speech and music mostly relies on post-processing methods to pick the target sound (St\u00f6ter et al., 2019; Sawata et al., 2021; Takahashi and Mitsufuji, 2021; Wang and Chen, 2018; Yu et al., 2017; Zhang et al., 2021; Luo and Mesgarani, 2018). Later, permutation in-variant training (PIT) (Yu et al., 2017; Kavalerov et al., 2019), followed by its variant mixture invariant training (MixIT) (Karamatl\u0131 and K\u0131rb\u0131z, 2022; Wisdom et al., 2020, 2021) relies on permutation alignment on source prediction for performance enhancement. However, these methods suffer from both over- and under-separation, due to training distribution misalignment in open world scenarios. Later, weakly supervised training with classifiers has been explored (Pishdadian et al., 2020; Tzinis et al., 2020), however, such methods are limited in their use on a fixed number of training sources. In contrast, OpenSep attempts to separate a variable number of sources in open world, without being limited to certain training sources.\nConditional Sound Separation Prior work on conditional sound separation used visual guid-ance (Gao and Grauman, 2019; Zhao et al., 2018; Tian et al., 2021; Chatterjee et al., 2021; Lu et al., 2018), text guidance (Dong et al., 2022; Liu et al., 2022; Kilgour et al., 2022; Tan et al., 2023; Liu et al., 2023a; Mahmud et al., 2024), and clean audio source guidance (Chen et al., 2022; Gfeller et al., 2021) for conditioning on noisy mixtures. Most of these methods mostly rely on a mix-and-separate framework (Zhao et al., 2018). However, the re-quirement for users to explicitly specify which sources to separate is often impractical in dynamic or complex audio scenes. Moreover, in general, these methods struggle with unseen, new sources for learning the conditional guidance with specific class prompts. In contrast, OpenSep attempts to fully automate the separation of all sources present in noisy, unseen source mixtures in open world, without using hand crafted prompts."}, {"title": "3 Methodology", "content": "OpenSep addresses two critical challenges of audio separation: handling a variable number of sources without manual intervention and enhancing performance on unseen sources during inference. As illustrated in Fig 2, the OpenSep architecture com-"}, {"title": "3.1 Source Parsing with Textual Inversion", "content": "The audio source separation task can be split into two key phases: (1) detecting the sound sources present in the mixture, and (2) separating each source correctly. Prior work on unconditional separators attempts to perform both tasks simultaneously. This approach is challenging since it attempts to align the data distribution on varying number of sources, often resulting in over/under separation. Conditional separators achieve superior performance by eliminating the source recognition phase, but they rely heavily on manual prompts, which limits their applicability to automatically parsing the sources present in the mixture. OpenSep solves this complex mixture disentanglement challenge in open world by using tex-tual inversion and leveraging the world knowledge in large language models.\nTextual Inversion: To simplify the source parsing in complex mixtures, we propose converting the audio mixture to a text representation by using an off-the-shelf audio captioning model. This model processes the audio input and generates a textual de-scription or caption, which encapsulates the salient features and sources present in the mixture. For example, a caption might describe an audio mix-ture as \"a person speaking with background music and occasional dog barks.\" This textual representa-tion enables the subsequent use of LLMs to further"}, {"title": "3.2 Knowledge Parsing for Each Source", "content": "To precisely separate each parsed source from complex mixtures, it is necessary to have more detailed knowledge of audio properties of the target source. Traditional conditional separators only rely on the class representation of each source. Despite their promising performance on seen classes used for training, these models underperform on unseen classes. To overcome this limitation, we propose the use of an instruction-tuned LLM as a knowledge parser to incorporate detailed audio proper-"}, {"title": "3.3 Text-Conditioned Audio Separator", "content": "OpenSep uses a text-conditioned audio separator as a core building block given its superior perfor-"}, {"title": "3.4 Proposed Training Pipeline", "content": "Most prior work relies on a mix-and-separate framework for conditional separator, which learns to separate single sources from synthetic audio mix-tures given a conditional prompt. OpenSep primar-ily focuses on separating the target source by using richer text conditioning rather than a simple class prompt. The overall performance improvement on unseen and noisy sources mostly comes from the deeper audio and textual feature grounding on di-verse audio properties. To achieve this, we propose a two-level separation training objective, extending the baseline mix-and-separate framework, such as mixture, and single-source separation.\nAs illustrated in Fig 3, we initially sample four single source audios (X\u2081,X\u2082, X\u2083, X\u2084) and prepare two synthetic mixtures (y\u2081, y\u2082) by mixing each pair of sources, given by y\u2081 = Mix(x\u2081,x\u2082), and\ny\u2082 = Mix(X\u2083, X\u2084). For mixing, we use amplitude re-scaling of each source, followed by simple addition on raw audio waveforms. After-wards, we generate a higher order mixture z of four sources using z = Mix(Y\u2081,Y\u2082). By leveraging the instruction tuned LLM on class enti-ties of each source, we extract single source text prompts (S\u2081, S\u2082, S\u2083, S\u2084), and two-source mixture prompts (M\u2081, M\u2082). The model generates a se-ries of predictions P of separated audios given the text prompt T and higher-order mixture z, where\nT\u2208 {S\u2081, S\u2082, S\u2083, S\u2084, M\u2081, M\u2082}. Finally, the L\u2081"}, {"title": "4 Results", "content": "Evaluation Setup\nDataset: For the experiments on synthetic mix-tures, we primarily use MUSIC (Zhao et al., 2018) and VGGSound (Chen et al., 2020) datasets. MU-SIC dataset contains 21 musical instruments, sep-arately played and recorded for 1 ~ 5 minutes duration. VGGSound contains 162, 433 audio sam-ples, mostly containing noisy single source sounds of 10s duration. For analyzing the performance on natural mixtures, we use AudioCaps (Kim et al., 2019) dataset containing 44,309 audio mixtures having around 1 ~ 6 sources with audio captions.\nImplementation Details: We use LLaMA-3-8b (Touvron et al., 2023) language model for source and knowledge parsing. We use RoBERTa-Base (Liu et al., 2019) text encoder with a context window of 512 for encoding parsed LLM knowl-edge. For audio captioning, we use the CLAP-based captioning model (Elizalde et al., 2023), which combines an audio encoder with a GPT-2 text decoder. We use a U-Net based audio separator with self and cross-attention conditioning.\nTraining: All models are trained for 80 epochs with initial learning rate of 0.001. The learning rate is decreased by a factor of 0.1 every 20 epochs. An Adam optimizer is used with \u03b2\u2081 = 0.9, \u03b2\u2082 =\n0.999 and \u20ac = 10\u207b\u2078. The training was carried out with 8 RTX-A6000 GPUs with 48GB memory. For training, we randomly sample different sources, then, mix and separate using our training method. Also, the class label of each source is used for knowledge parsing during training.\nEvaluation: We use signal-to-distortion ratio (SDR) and signal-to-interference ration (SIR) (Vincent et al., 2006) for evaluating different models. In general, SDR estimates overall separation quality"}, {"title": "4.2 Main Comparisons", "content": "Baseline Models: We used single-source permu-tation invariant training, PIT (Yu et al., 2017) and multi-source MixIT (Wisdom et al., 2020, 2021) for unconditional baselines. Note that we use synthetic mixtures for MixIT training. We also combine MixIT and PIT to train on both mixtures and single source sounds. For conditional baselines, we use LASSNet (Liu et al., 2022), AudioSep (Liu et al., 2023b), and CLIPSep (Dong et al., 2022) models with text conditions, which are built using the mix-and-separate (Zhao et al., 2018) framework. For a fair comparison, all baseline results are reproduced with similar architecture and data split. For the conditional models, we explicitly provide the class"}, {"title": "4.3 Ablation Study", "content": "We use VGGSound dataset in both seen and unseen cases for the ablation study."}, {"title": "5 Conclusion", "content": "In this paper, we introduced OpenSep, a novel framework for audio source separation in open-world scenarios. In particular, OpenSep leverages an off-the-shelf audio captioning model and the world knowledge embedded in large language mod-els (LLMs) to automatically parse and disentangle"}, {"title": "6 Limitations", "content": "OpenSep performance is limited by the precise detection of sound sources in noisy mixtures, which mostly stems from the challenge in having a suitable audio captioning model. Nevertheless, OpenSep framework can potentially integrate any superior audio captioning approach to scale-up on real world cases. Finally, given its use of multiple building blocks, OpenSep is computationally ex-pensive in general. However, by further optimizing the architecture, such as using mobile LLMs (e.g., Phi-3-mini, Gemma-2b), OpenSep computational cost can be largely reduced, which we leave for future study."}, {"title": "7 Ethics Statement", "content": "We only use publily available dataset for this study."}, {"title": "A Appendix", "content": "1 Implementation Details\nWe use audio segments of 10s duration with a sampling rate of 16000 Hz for all experiments. Each audio sample is processed with short-term Fourier transform (STFT) using the frame window of 1022, and the hop length of 256. Following prior work (Dong et al., 2022; Zhao et al., 2018), the supervision is provided on the mask prediction for each source, instead of final reconstruction. We use the similar conditional U-Net based encoder decoder architecture following prior work (Mahmud et al., 2024). The U-Net model contains a total of seven successive encoding and decoding stages with convolutional kernels having 43.2M parameters. We apply self-attention followed by cross-attention in the skip connection of four bot-tom feature levels. A multi-head attention (Vaswani et al., 2017) is used with 8 heads for each atten-tion operation. We use LLaMA-3-8b (Touvron et al., 2023) language model for parsing both sound sources and their corresponding details of audio properties, with 5-shot prompting for both parsing phases. These examples are manually curated and refined for guiding LLMs in diverse scenarios. We use ROBERTa-base language encoder to encode the extracted knowledge from LLM. A single sen-tence knowledge of audio properties is extracted for each parsed source targeting the maximum context length of 512. For the evaluation, we use torch-mir-eval (Montesinos, 2021) package for estimating both SDR and SIR in source separation from mix-tures."}, {"title": "A.2 Sample of Source Parsing with LLM", "content": "We present several samples of textual inversion with audio captioning and source parsing from real-world audio mixtures collected from Audio-Caps (Kim et al., 2019) in Table 8. We use ms-CLAP (Elizalde et al., 2023) model for audio cap-tioning, followed by instruction-tuned LLaMA-3-8b model for source parsing. In most cases, the generated captions contain all sound sources pre-sented in the audio mixtures. In general, the gen-erated captions simplify the source descriptions compared to the ground truth captions. Neverthe-less, by leveraging the detailed knowledge parsing from the LLM, we enrich details of each source. In a few cases of higher order mixtures, we observe the captioning model cannot detect muffled, short duration sounds. Nonetheless, we observe accurate source parsing with the LLM from the generated captions. Though source parsing is a simple ob-jective, however, it can be complicated in some scenarios with complex captions. For example, the \"cat meowing\" sound is detected twice with the generated captions for repeated sounds, though it represents a single source. However, LLM based source parsing effectively solves such challenges."}, {"title": "A.3 Sample of Knowledge Parsing with LLM", "content": "We provide samples of knowledge parsing from LLM in Table 9 for various sound sources. We use instruction-tuned LLaMA-3-8b for such parsing. Several key properties of audios, such as frequency range, amplitude, dynamic envelope characteristics, usual duration, and spectral contents are focused by guiding the LLM with manually curated 5-shot prompts. Such details of audio properties largely help the audio separator model to ground diverse audio features with text description for enhanced separation, particularly in noisy, unseen mixtures."}, {"title": "A.4 Additional Qualitative Results", "content": "We provide additional qualitative comparisons for natural audio mixture separation in Fig. 5 and Fig. 6. In general, OpenSep largely reduces the spectral overlap across multiple sources while pre-serving details of each source in separation from challenging mixtures, compared to state-of-the-art baseline methods, without accessing manual source prompts. We also provide audio samples from dif-ferent models in the supplementary for better un-derstanding of the separation performance."}, {"title": "A.5 Details of User Study", "content": "We conduct a user study to analyze the audio separation performance from real-world mixtures, where we don't have access to any ground truth sources. Each user is provided with 20 mixture samples, and their corresponding separated audios with competitive models. For each sample, user rates the superior separation quality between two choices. We collect the data of wins, loses, and ties across different models using the user study. The results of this study are reported in Table 3. We attach the screenshot of the human evaluation form with detailed guidelines in Fig. 7."}]}