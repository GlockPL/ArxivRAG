{"title": "Composable Strategy Framework with Integrated\nVideo-Text based Large Language Models for Heart\nFailure Assessment", "authors": ["Jianzhou Chen", "Xiumei Wang", "Jinyang Sun", "Xi Chen", "Heyu Chu", "Guo Song", "Yuji Luo", "Xingping Zhou", "Rong Gu"], "abstract": "Heart failure is one of the leading causes of death worldwide, with millons of deaths each year,\naccording to data from the World Health Organization (WHO) and other public health agencies.\nWhile significant progress has been made in the field of heart failure, leading to improved survival\nrates and improvement of ejection fraction, there remains substantial unmet needs, due to the\ncomplexity and multifactorial characteristics. Therefore, we propose a composable strategy\nframework for assessment and treatment optimization in heart failure. This framework simulates\nthe doctor-patient consultation process and leverages multi-modal algorithms to analyze a range of\ndata, including video, physical examination, text results as well as medical history. By integrating\nthese various data sources, our framework offers a more holistic evaluation and optimized\ntreatment plan for patients. Our results demonstrate that this multi-modal approach outperforms\nsingle-modal artificial intelligence (AI) algorithms in terms of accuracy in heart failure (HF)\nprognosis prediction. Through this method, we can further evaluate the impact of various\npathological indicators on HF prognosis,providing a more comprehensive evaluation.", "sections": [{"title": "Introduction", "content": "Heart failure (HF) is a complex and multifactorial disease, and its clinical\nmanifestations, treatment and prevention are widely challenging. With the aging of\nthe population and the increase in cardiovascular disease, the incidence rate of heart\nfailure continues to rise, resulting in a huge consumption of medical resources\nworldwide. In the treatment of heart failure, lifestyle intervention, pharmacological\nmanagement, the device-based management, and heart transplantation are the main\ntreatment methods. The addition of a neprilysin inhibitor to renin-angiotensin system\ninhibition (ARNI), and the advent of SGLT2 inhibition are the main supplemental\nsolutions of pharmacological management [1]. The incidence rate of newly diagnosed\nheart failure has reportedly stabilized or may even be declining in the general\npopulation. People with heart failure also live longer. However, the long-term trend of\nincreasing numbers of individuals at risk for heart failure and increasing survival rates\nfor individuals with heart failure has led to an overall increase in the prevalence of\nheart failure patients.\nOn the other hand, the vigorous development of deep learning has brought new\nparadigms to scientific research in various fields, and it has also been widely used in\nclinical studies [2]. Deep learning can discover patterns, predict diseases, assist in\ndiagnosis, and optimize treatment plans by analyzing large amounts of medical data,\nsignificantly improving medical efficiency. It has played a significant role in medical\nimage analysis [3], clinical decision support [4], clinical data analysis [5, 6], and other\nfields.\nRecent years, scientists employ the machine learning to mine the mapping\nrelationship connected with HF between various important factors such as subclinical\nhypothyroidism [7], clonal hematopoiesis of indeterminate potential (CHIP), extended\nsomatic variants (ExSV) [8]. This method was also used to predict the composite\noutcome of HF hospitalization and cardiovascular (CV) death [9, 10]. The machine\nlearning has achieved very good results in the study of HF. However, most of these\nworks focus on the influence of single factor on HF [7-16], which may lead to\ninaccurate and partial judgment.\nFortunately, multimodal artificial intelligence (AI) refers to AI systems that can\nsimultaneously process and understand data from multiple different modalities (such\nas text, images, audio, video, etc.) [17-27]. Compared with traditional single-modal\nAl systems, multi-modal AI can better simulate human perception and integrate\ndifferent types of information to provide more comprehensive, rich, and accurate\nunderstanding. This technology was quickly applied to the clinical studies, such as\nAlzheimer's disease dementia assessment [18, 21], neuroprognosis performance in\nclinically unresponsive critical-care patients with brain injury [19], identification of\nclinical disease trajectories in neurodegenerative disorders [6], etc.\nIn our work, we propose the Multimodal Post-Recovery Tracking Model\n(M-PRTM), a framework that integrates cinematic, numerical, and textual data to\nimprove HF prognosis prediction. M-PRTM combines clinical data such as patient\ndemographics, clinical metrics, and medication history with Cardiac Magnetic\nResonance (CMR) images to create a comprehensive model. Our framework employs\nspecialized models for each data modality, including the DAE-Former for cinematic\ndata, a fully connected network for numerical data, and a BERT-based model for\ntextual data. These features are then fused using an attention mechanism, which\ndynamically prioritizes the most important information for predictions, such as drug\nprescriptions and vital signs. Our results show that M-PRTM achieves 96.5%\naccuracy in predicting clinical outcomes, outperforming traditional single-modal\napproaches, and demonstrating its ability to handle complex, multimodal data for\nmore reliable patient predictions. The framework's ability to dynamically adjust the"}, {"title": "Results", "content": "The comprehensive workflow for M-PRTM's development and validation is\npresented in Fig. 1. Additionally, we have described the underlying structure that\nsupports the data modalities as well.In the Data column, patient data are categorized\ninto three main types: numerical indicators, textual prescriptions, and CMRcinematic\ndata. Numerical indicators include demographic information (e.g., gender and age)\nand key clinical metrics (e.g., blood pressure and BMI). Textual records primarily\nconsist of prescription information, such as drug type, dosage, and treatment stage.\nMeanwhile, CMR imaging data capture anatomical and functional features of the\nmyocardium, including regions of fibrosis.In particular, Late Gadolinium\nEnhancement (LGE) is a key imaging biomarker for detecting myocardial fibrosis. It\nprovides crucial insights into the structural integrity and scarring of the myocardium,\nwhich are closely linked to the progression of HF [28, 29]. To leverage these imaging\ninsights, we extract vector features from numerical, textual, and CMR cinematic data\nfor multimodal fusion-based predictions. Specifically, we provide an example in the\nMedical Prescription section. Considering that drug types and dosages often vary\nacross time or treatment stages during clinical practice, our data processing accounts\nfor these dynamic changes to enhance the M-PRTM's adaptability to patient\nmedication patterns.\nThe Model Development column outlines three modules designed to process\nimaging, numerical, and textual tasks. For imaging data, we employ the Dual\nAttention-guided Efficient Transformer model (DAE-former) [30]. The model\nintegrates a skip-connected CMR encoder and masked decoder, which enable precise\npredictions of myocardial fibrosis regions and can significantly improve segmentation\naccuracy. For the numerical type data, we construct a feature extraction model to map\nthe numerical inputs into classification and regression features. Those features are\nfurther refined using an Attention Model, which calculates the \"Query-Key-Value\"\nweights to highlight critical attributes impacting prediction outcomes. For the textual\ntype data, we apply a pre-trained Bidirectional Encoder Representations from\nTransformers (BERT) model [31, 32] to embed textual information. It is\ncomplemented by an Attention Model to dynamically adjust the importance of textual\nfeatures, effectively capturing the impact of phased drug adjustments on prediction\noutcomes.\nIn the Model Validation step, the Fibrosis Validation module evaluates the\npredicted fibrosis regions, showing high consistency between the prediction results"}, {"title": "Detailed M-PRTM's Performance and Results", "content": "In this section, we present key performance metrics of the proposed M-PRTM\nalongside its prediction results. By visualizing the training and testing processes, we\nprovide an intuitive demonstration of the model's efficiency and accuracy. The\ntraining process of the DAE-former is shown in Fig. 2(a), where the Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD) are plotted against the number of\nepochs. Additionally, we present examples of predicted Myocardial Fibrosis regions\nalongside their corresponding ground truth labels, as shown in Fig. 2(b). The results\nindicate that the fibrosis regions predicted by DAE-former closely align with manual\nannotations, achieving an accuracy of approximately 87%, demonstrating the model's\neffectiveness in processing imaging data. A key contribution of our work is the design\nand implementation of the M-PRTM, which is specifically developed to integrate\ntextual, numerical, and cinematic data for joint training. The training process of\nM-PRTM is illustrated in Fig. 2(c), where the test loss and accuracy curves are plotted\nagainst epochs. The results show that M-PRTM converges rapidly, with the test loss\ngradually decreasing and test accuracy steadily increasing, which demonstrates its\neffectiveness in processing multimodal data. The detailed algorithm flowchart for\nM-PRTM can be found in Fig. 4. To further validate the predictive capabilities of\nM-PRTM, we present examples of predictions for key clinical outcomes, including\nmortality, causes of death, days to rehospitalization, and MACCES-related events, as\nshown in Fig. 2(d). The results reveal strong alignment between predicted values and\nground truth, highlighting the ability to handle incomplete recovery data and\naccurately predict patient outcomes. Furthermore, M-PRTM provides early\nrecommendations for days to rehospitalization or follow-up visits, helping to prevent\nunnecessary mortality caused by deteriorating conditions during recovery. Fig. 2(e)\nhighlights the unique functionality of M-PRTM in dynamically assessing patient risk\nand predicting recovery prognosis.\nBy integrating multimodal data, the M-PRTM evaluates patient risk at different\nstages and forecasts critical events. For instance, during the treatment phase, it\nidentifies the recovery time point for HF (green low-risk region). As the patient\ntransitions to the follow-up stage post-discharge, potential risks such as declining\nejection fraction (blue medium-risk region) begin to emerge. The M-PRTM further\npredicts high-risk events, such as HF deterioration (orange high-risk region), and\nintelligently infers risk trends during data-absent follow-up windows based on\nhistorical data. This mechanism, depicted in Fig. 2(e), demonstrates the capability of\nM-PRTM to support personalized follow-up strategies. It provides tailored\nrecommendations for re-evaluation and follow-up visits, reducing unnecessary\nmedical burdens for low-risk patients while optimizing resources and offering\nscientific guidance for precise interventions."}, {"title": "M-PRTM's Evaluation and Analysis", "content": "In this section, we evaluate the performance of the proposed M-PRTM under\ndifferent modality combinations and analyze the impact of attention allocation\nstrategies. When integrating textual, cinematic, and numerical modalities, the\nM-PRTM achieves a maximum accuracy of 96.5%, indicating that combining\nmultimodal data significantly enhances predictive performance. When using only two\nmodalities, the combination of textual and cinematic modalities achieves an accuracy\nof 94.0%, outperforming the combinations of textual and numerical modalities\n(92.5%) and cinematic and numerical modalities (86.5%), as shown in Fig. 3(a). This\nhighlights the critical role of the textual modality, as it contains prescription\ninformation from physicians, which is pivotal for subsequent predictions.\nWe further investigate the impact of different attention allocation strategies on\nmodel performance, as depicted in Fig. 3(b). The Self-Reasoning Attention\nmechanism delivers the highest accuracy of 96.5%, demonstrating its effectiveness in\nbalancing the contributions of each modality to the predictions. Under fixed weight\nallocation strategies, the M-PRTM achieves an accuracy of 91.7% when assigning\n50% weight to the textual modality, 25% to the cinematic modality, and 25% to the\nnumerical modality. In contrast, assigning 25% weight to the textual modality, 50% to\nthe cinematic modality, and 25% to the numerical modality results in a lower accuracy\nof 87.7%. Assigning 50% weight to the numerical modality produces the lowest\naccuracy, at only 84.8%. These results demonstrate that adaptive attention allocation\nstrategies optimize the synergy among modalities and fully leverage the strengths of\ndifferent data types.\nThe analysis indicates that the textual data plays a dominant role in predictions,\nas it contains critical information such as phased prescription adjustments. In contrast,\nnumerical data such as personal and basic metrics tends to diminish the importance of\nfeatures like medical history. Consequently, its contribution to the M-PRTM's\nattention allocation is relatively lower. Ultimately, our Self-Reasoning Attention\nmechanism allocates approximately 55% to textual data, 30% to cinematic data, and\n15% to numerical data. These findings further highlight the significant variation in\ncontributions among modalities, underscoring the necessity of dynamic allocation\nstrategies to optimize balance and maximize the model performance.\nOur research establishes a multimodal feature extraction and fusion framework.\nThe proposed method integrates the characteristics of cinematic, textual, and\nnumerical data, introducing the targeted data processing workflows and the model\nstructures, as illustrated in Fig. 4. The methodology includes three main stages: data\ncollection, single-modal feature extraction (cinematic features, numerical features,\nand textual features), and multimodal feature fusion."}, {"title": "Data Collection and Preprocessing", "content": "Our research focuses on HF prognosis prediction and myocardial fibrosis\ndetection. It utilizes data from 136 and 688 patients, comprising CMR cinematic data\nand clinical information respectively. The clinical data encompass demographic\ndetails and key clinical indicators, with baseline characteristics detailed in Table 2. We\nhave also outlined the preprocessing methods for cinematic, textual, and numerical\ndata."}, {"title": "Cinematic Feature Extraction Module", "content": "In myocardial fibrosis detection tasks, we employ the DAE-Former model [30].\nDAE is a pure Transformer architecture designed specifically for medical image\nsegmentation. It addresses the limitations of traditional convolutional neural networks\n(CNNs) in capturing long-range dependencies and global contextual information\nwhile overcoming the high computational complexity of standard Transformers.\nAs illustrated in the imaging processing workflow in Fig. 4, DAE-Former\nconsists of an input encoder, output encoder, Efficient Dual Attention (Efficient\nAttention and Transpose Attention), Skip Connection Cross Attention (SCCA), and an\noutput layer. The core components of the model are Efficient Dual Attention and Skip\nConnection Cross Attention (SCCA), with their structural diagrams detailed in\nAppendix A.\nEfficient Dual Attention\nEfficient Dual Attention combines Efficient Attention (to capture spatial\ninformation) and Transpose Attention (to capture channel information), enhancing the\nmodel's ability to comprehend contextual features. These two mechanisms are applied\nsequentially, with normalization and residual connections incorporated to improve"}, {"title": "Skip Connection Cross Attention (SCCA)", "content": "SCCA introduces a cross-attention mechanism into the traditional U-Net\narchitecture. It adaptively fuses encoder features with decoder low-level features,\nenabling seamless integration of encoder and decoder representations. This process\ninvolves linear projection and efficient attention fusion methods, which enhance the\nrecovery of segmentation boundaries and fine details.\nFor myocardial fibrosis feature extraction from CMR cinematic images, the raw\ncoronal CMR images are first analyzed frame by frame. Myocardial regions, fibrosis\nshadow regions, and other areas are segmented into different masks. The CMR\nimages, matched with their corresponding masks, are then fed into the DAE-Former\nnetwork. The M-PRTM identifies potential lesion areas within individual frames and\ntracks their consistency across multiple slices, leveraging the attention mechanisms\ninherent in Transformers.Consequently, our M-PRTM can generate stable and\naccurate predictions of lesion regions, with the ability to capture changes in lesion\npositions across consecutive frames."}, {"title": "Numerical Feature Extraction Module", "content": "In the numerical tasks, a fully connected layer is designed to process patients'\nclinical numerical indicators, as illustrated in Fig. 4. the preprocessed numerical\nfeatures are input into a fully connected network for deep feature extraction. The first\nlayer maps the normalized input data into a 512-dimensional intermediate feature\nspace, incorporating a ReLU activation function to introduce non-linearity. To prevent\noverfitting, a Dropout layer with a dropout rate of 0.2 is added after the first layer.\nThe second layer further reduces the intermediate features to 256 dimensions to\nstandardize the feature size, facilitating convenient input for subsequent multimodal\nfeature fusion. The final output is a feature tensor with a size of Batchsize \u00d7 256,\nconsistent with features from other modalities."}, {"title": "Textual Feature Extraction Module", "content": "The textual feature extraction module plays a critical role in the HF prognosis\nprediction model. These textual data often contain detailed drug usage information\nand phased adjustments, which are key to clinical predictions. In this module, a\npre-trained BERT model is employed to extract deep semantic features from the\ntextual data. BERT can effectively capture the semantic relationships among words\nwithin a sentence, leveraging its strong contextual understanding capabilities. It\nconstructs a comprehensive and accurate representation of textual features, which also\nenriches the predictive model with semantic insights.\nAs shown in Fig. 4, textual input is first processed through the BERT tokenizer.\nPatients' medical histories and medication records are integrated into natural language\nsentence forms and input into the BERT model for embedding. During this process,\nBERT utilizes its multi-layer bidirectional Transformer structure to parse the input\ntext word by word, extracting dynamic semantic relationships for each word within its\ncontext. The model outputs a global feature vector that encapsulates the overall\nsemantic information of the text, including key clinical details such as drug types and\ndosage adjustments.\nTo further enhance the expressiveness of textual features and align them with\nfeatures from other modalities, a fully connected layer is added after the BERT\nmodel's output layer. This layer is designed to reduce dimensionality, mapping the\n768-dimensional semantic vector to a 256-dimensional feature space. This\ndimensionality reduction not only decreases computational complexity but also\nensures dimensional consistency with image and numerical features during the feature\nfusion stage. Besides, the step optimizes the representation of textual features,\nretaining original semantic information while improving computational efficiency."}, {"title": "Multimodal Feature Fusion", "content": "We adopt a multimodal fusion strategy incorporating an attention mechanism to\nintegrate features from cinematic, textual, and numerical modalities. To implement\nthis, we design a fusion process that consists of two stages: independent modality\nfeature extraction and multimodal feature fusion, as depicted in Fig. 4.\nIn the feature extraction stage, unique features are independently extracted from\neach modality through dedicated modules. Cinematic data are processed by the\nDAE-Former model, capturing spatial and temporal information at the frame level.\nTextual data are processed by the BERT model, extracting semantic features to\nanalyze medical histories and medication records. Numerical data are aggregated\nusing a multi-layer perceptron (MLP) to represent clinical indicators. These modules\nensure efficient learning of features within their respective spaces.\nIn the fusion stage, the model dynamically assigns weights to different modality\nfeatures using an attention mechanism. Specifically, the features of each modality\ngenerate Query, Key, and Value matrices, which are used to calculate attention scores\nacross modalities. These scores are scaled and transformed into probability\ndistributions through the Softmax function, which serves as weights for the feature\nfusion process. The weighted features are then combined into contextual features.\nThis dynamic process evaluates the contribution of each modality, emphasizing\nfeatures that are more critical for prediction tasks. Additionally, the fusion strategy\nincorporates cross-modality regulation, enabling the interaction of information\nbetween modalities to optimize feature representation collaboratively. For example,\ncinematic features can enhance the semantic representation of textual features, while\ntextual features can improve the interpretation of numerical features. This dynamic\ninteraction across modalities significantly boosts the expressiveness of fused features.\nFinally, the fused features are integrated through a linear layer to generate a unified\nfeature representation for disease and mortality cause prediction. This multimodal\nfusion strategy not only leverages complementary information from different\nmodalities but also achieves efficient predictive performance, providing essential\nsupport for personalized treatment strategies."}, {"title": "Discussion", "content": "In this study, we introduce the M-PRTM, a Composable Strategy Framework for\nHF prognosis that leverages multimodal clinical intelligence. Our model integrates\nthree primary data modalities: numerical indicators, textual prescriptions, and CMR\ncinematic data. Each modality is processed using specialized architectures, with the\nDAE-Former for cinematic data, a fully connected network for numerical data, and a\nBERT-based model for textual data. These representations are fused through an\nadaptive attention mechanism [33, 34] that dynamically prioritizes critical features,\nsuch as drug prescriptions and vital signs. The model achieves an accuracy of 96.5%\nin HF prognosis prediction, highlighting the benefits of multimodal integration for\nclinical decision support.\nOur findings reveal several key insights into the role of multimodal data in HF\nassessment. The fusion of textual, numerical, and imaging data provides a more\ncomprehensive evaluation of patient status, compensating for the limitations of\nsingle-modality approaches. Notably, textual prescription data proves essential in\npredicting clinical outcomes, particularly in forecasting rehospitalization risk and\ndetermining optimal follow-up strategies. The framework captures treatment trends\nthat significantly influence patient recovery by analyzing longitudinal drug usage\npatterns. Additionally, the DAE-Former model processes CMR data to enhance\nmyocardial fibrosis detection, which serves as a key prognostic factor for declining\nheart function. Multimodal data integration improves predictive accuracy and\nsupports personalized treatment, highlighting the value of cross-domain feature fusion\nin medical diagnostics.\nDespite its strong performance, certain limitations in the dataset may affect the\nM-PRTM's generalizability. The dataset used in this study originates from a single\nmedical institution, which may not fully represent the diversity of patient populations\nacross different geographical and clinical settings. Expanding the dataset to include\nmulti-center cohorts with broader demographic and pathological variations is essential\nto improve the robustness. Additionally, challenges related to data quality, such as"}, {"title": "Conclusion", "content": "missing values, inconsistent clinical records, and variations in imaging protocols,\nshould be addressed. Future work should focus on implementing advanced\npreprocessing techniques and data augmentation strategies to mitigate these issues.\nFurthermore, refining the M-PRTM's ability to handle incomplete data will be crucial\nfor its real-world applicability in heterogeneous clinical environments.\nWhile our framework demonstrates high predictive performance, its\ninterpretability remains a challenge, particularly given the complexity of deep\nlearning-based clinical models. The \"black box\" nature of neural networks poses\nobstacles for clinical adoption, as medical professionals require transparency in\ndecision-making. Although our attention mechanism dynamically adjusts modality\nimportance, further efforts are necessary to enhance the explainability. Future work\nshould explore post-hoc interpretability methods and integrate expert-driven\nvalidation, such as comparisons with cardiologist assessments, may further enhance\ntrust and acceptance in real-world clinical settings.\nLooking ahead, the M-PRTM framework shows great potential beyond HF\nprognosis. Its integration of video, textual, and numerical data allows adaptation to\nother medical fields, such as oncology, diabetes management, and neurodegenerative\ndisease monitoring. In resource-limited settings, where advanced imaging may be less\naccessible, the model's reliance on textual and numerical data still enables highly\naccurate predictions, expanding its applicability. As multimodal data collection and AI\nmethodologies continue to evolve, frameworks like M-PRTM are expected to\ntransform clinical decision support, driving the future of precision medicine and\npersonalized healthcare."}]}