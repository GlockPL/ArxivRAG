{"title": "Exploring Capabilities of Time Series Foundation Models in Building Analytics", "authors": ["XIACHONG LIN", "ARIAN PRABOWO", "IMRAN RAZZAK", "HAO XUE", "MATTHEW AMOS", "SAM BEHRENS", "FLORA D. SALIM"], "abstract": "The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytics face persistent challenges, primarily due to the inherent physical constraints of buildings and the diverse, heterogeneous nature of IoT-generated data. In this study, we conduct a comprehensive benchmarking of two publicly available IoT datasets, evaluating the performance of time series foundation models in the context of building energy analytics. Our analysis shows that single-modal models demonstrate significant promise in overcoming the complexities of data variability and physical limitations in buildings, with future work focusing on optimizing multi-modal models for sustainable energy management.", "sections": [{"title": "1 INTRODUCTION", "content": "The deployment of AI in digital infrastructure presents significant opportunities for enhancing energy efficiency and advancing net-zero strategies. Traditional building energy research relies on post-processed datasets, which typically include aggregated load data and weather station-monitored temperature-referred to as rough granularity data. While these higher-level data offer convenience for data scientists by providing a broad overview, they compromise the granularity necessary for a detailed description of activities. The introduction of the BLDG59[12] and BTS datasets[14] based on the centralized management tool, Brick Schema [1], addresses this limitation by offering comprehensive IoT sensing and metering point data, enabling the exploration of detailed usage patterns in digitalized buildings. However, the increased data texture complicates the building activity modeling due to the varied architecture preferences, energy efficiency measures, local climate conditions, usage patterns, and engineering practices, all contributing to heterogeneous sensing data among buildings[9]. Accurate cross-building forecasting using IoT point data necessitates models that can adapt to unseen physical constraints and variations in ontology usage.\nFoundation models, which utilize large, pre-trained models as the base for fine-tuning specific tasks, are receiving increasing attention in time series analysis due to their capabilities in generalization. Existing time series foundation models are typically summarized into two categories. Prompt-based methods, such as PromptCast[16] and LLMTime [5], typically convert time series to sentences using a manually defined template and feed to language models. Methods following this logic benefit from the easy implementation, but specific prompt templates are required based on targeted tasks and datasets. Tuning-based models typically employ fine-tuning techniques for LLMs to adapt the time series inputs. One-Fits-All (GPT4TS)[19] and LLM4TS[3] approach multivariate time series by treating them as univariate sequences, which are then segmented into patches. These patches are encoded using specific encoders, concatenated, and subsequently fed into the pre-trained LLMs. To enhance forecasting by incorporating domain knowledge, GPT4MTS[7] introduces a prompting template that combines time series data with textual prompts. TimeLLM[8] initially proposes the cross-modal attention mechanism, which aligns the time series with pre-train LLM's word embedding. Inspired by this work, LLaTA[10] designed a dual-branch architecture to process the textual and temporal modalities. Each branch fine-tunes a GPT-2 backbones with the Low-rank adaptation technique (LoRA)[6] to adapt to the task, aligning the distillation knowledge and temporal embedding by reducing the distribution discrepancy between the modalities.\nTS foundation models are typically evaluated on well-processed datasets, which differ significantly from real- world building IoT datasets. Building IoT contains branches of information about energy usage, comfort parameters, and environmental factors, but at the same time, the complex dependencies and data heterogeneity among the IoT network present challenges for machine learning and deep learning techniques. In the context of energy-efficient digital infrastructures, the application of TS models still needs extensive experimentation to validate their effectiveness. This comparative study aims to assess the performance of TS foundation models in the building energy domain, benchmarking them against traditional machine learning and deep learning approaches. Additionally, the study explores architectural ablations of TS foundation models to understand their strengths and limitations. The remainder of the paper is structured as follows: Section 2 evaluation methodology adopted by this benchmark study. Section 3 presents the results, followed by a discussion of the findings and future research directions in Section 4. Section 5 provides a concise conclusion."}, {"title": "2 METHODOLOGY", "content": "This study employs the following two public datasets:\n2.1.1 BTS-B[14]. The BTS-B contains 730 IoT points data, including 264 sensors. nUnique/nSample \u2265 o is used to filter the sensor data streams, 105 streams are chosen for further processing. Each time series is resampled to 10-min, where missing values are filled with second-order polynomial interpolation. Records from 00:00:00 01/06/2022 to 00:00:00 01/08/2022 are used for training, whereas records from 00:00:00 01/08/2022 to 00:00:00 01/09/2022 are used for testing.\n2.1.2 BLDG59[12]. Following the similar processing procedure as BTS-B, records for 49 selected sensors in BLDG59 from 00:00:00 01/06/2019 to 00:00:00 01/08/2019 are used for training. Records from 00:00:00 01/08/2019 to 00:00:00 01/09/2019 are used for testing. The original data is downsampled by mean values and interpolated upsampled by second-order polynomials for multi-timestep alignment, such that the granularity is set to 10-min. In both of the cases, \u03c3 = 0.1.\n2.2 Experimental Settings\nThe train/validation ratio is set to 7/3. Adam optimization with a 1e-3 learning rate guides the learning process. Training epochs are set to 100 while early stopping with 10-epoch patience is employed. All the models are respectively fed with 1-day historical observations and forecasts on multiple time steps, where S = 144 and H \u2208 {12, 48, 96, 144, 432, 1008}, indicating 2-hour, 8-hour, 16-hour, 1-day, 3-day, and 1-week ahead forecasting. The average scores across all the forecasting horizons are computed. The weights are updated using the Adam optimizer with a model-specific learning rate. EarlyStop with a patience equal to 10 is used for the experiments. The batch size of each model is adjusted to fit the GPU utilization rate. All experiments are completed on V100 or H100 GPUs.\n2.3 Baselines & Evaluation\nThe following models are employed by this comparative study: Machine Learning Methods RandomForest[2] and XGBoost[4]. Deep Learning Models DLinear [17], PatchTST[13], Informer[18] and iTransformer[11]. Time Series Foundation Models LLaTA[10], One-Fits-All[19] and TimeLLM[8], all TS foundation models share the same backbone GPT-2 model. Additionally, due to memory constraints, we applied offline PCA with n_component=500, as[10], to reduce the dimensions of the word embeddings for TimeLLM, rather than using the original embeddings outlined in the original paper. All baseline experiments are conducted using the zero-shot settings, where the model trained on one dataset with 2-month records is directly applied to forecast the following 1-month records on another dataset without any retraining. Metrics Mean absolute error (MAE), mean square error (MSE), and symmetric mean absolute percentage error (SMAPE) are employed to evaluate the model performance."}, {"title": "3 RESULTS", "content": "3.1 Results of Building IoT Forecasting\nTable1 indicates the result of the comparative study. Each model trained on a specific dataset is evaluated on the same and unseen datasets to assess its generalization performance across different data distributions. One-Fits-All achieves the lowest SMAPE across all train/test configurations, consistently delivering superior performance in percentage-based error, and secures the top position 6 times across all metrics, highlighting its robustness in both relative and absolute error handling. LLaTA demonstrates consistently weaker performance across most metrics, particularly higher errors in both MSE and SMAPE across the two datasets. While TimeLLM performs reasonably well in terms of cross-building forecasting, One-Fits-All outperforms it with a notable margin of 3.642%, 4.520%, 4.454%, and 4.627% in the BTS-B/BTS-B, BTS-B/BLDG59, BLDG59/BTS-B, and BLDG59/BLDG59 train/test configurations, respectively.\n3.2 Ablation Studies for TS Foundation Models\nTo assess the effectiveness of LLMs in foundation models for building IoT, we adopt an ablation framework designed by Tan et al.[15]. This framework introduces modifications to the original models by either removing the LLM (w/o"}, {"title": "4 DISCUSSION", "content": "The primary challenge in modeling building activity using IoT-generated data lies in addressing the heterogeneity of buildings, each of which has unique energy consumption patterns influenced by factors such as structural design, insulation quality, HVAC systems, and occupancy behavior. Furthermore, the diversity in energy systems and load management practices adds complexity, making accurate forecasting in heterogeneous building environments more difficult. The ideal forecasting tool for a Building Management System must be a flexible and robust Al solution capable of generalizing across various building types and conditions. Our study demonstrates that TS foundation models are among the most effective in tackling this challenge. Moreover, TS foundation models outperform traditional machine learning and deep learning methods in percentage-based error, particularly in SMAPE, offering a valuable metric for comparing forecasting performance across different sensor types.\nHowever, the proper design of the pipeline is significant for deploying foundation models for building energy control and unlocking the full capability of large language models. The temporal tokenizer is one of the significant factors, as LLM was not initially proposed for processing the time series modality. LLaTA introduces an additional parallel textual branch and processes temporal inputs using a linear tokenizer, where we argue that the linear layer may be suboptimal for encoding time-series data, potentially leading to information loss. Another important observation from this study is that the single-modal foundation model (OneFitsAll) outperforms the multi-modal foundation models (LLaTA and TimeLLM). This indicates there may exist information conflicts while providing the two different modalities of data to"}, {"title": "5 CONCLUSION", "content": "This study performs a benchmark study to evaluate the TS foundation models's capabilities in building IoT forecasting. Our research investigates TS foundation models with great potential in tackling the building heterogeneity challenges, indicating the significance of temporal tokenizer selection, domain description granularity, and modality alignment. This paper also recommends future research to enhance TS foundation models' performance on building analytics with domain-specific knowledge graphs, ensuring seamless integration into real-world BMS applications while maximizing the contribution to sustainable development."}]}