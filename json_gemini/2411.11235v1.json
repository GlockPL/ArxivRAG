{"title": "MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal\nLarge Language Models on Human Emotion Analysis", "authors": ["Yingjie Zhou", "Zicheng Zhang", "Jiezhang Cao", "Jun Jia", "Yanwei Jiang", "Farong Wen", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "abstract": "Artificial Intelligence (AI) has demonstrated significant\ncapabilities in various fields, and in areas such as human-\ncomputer interaction (HCI), embodied intelligence, and the\ndesign and animation of virtual digital humans, both prac-\ntitioners and users are increasingly concerned with Al's\nability to understand and express emotion. Consequently,\nthe question of whether AI can accurately interpret human\nemotions remains a critical challenge. To date, two pri-\nmary classes of AI models have been involved in human\nemotion analysis: generative models and Multimodal Large\nLanguage Models (MLLMs). To assess the emotional ca-\npabilities of these two classes of models, this study intro-\nduces MEMO-Bench, a comprehensive benchmark consist-\ning of 7,145 portraits, each depicting one of six different\nemotions, generated by 12 Text-to-Image (T2I) models. Un-\nlike previous works, MEMO-Bench provides a framework\nfor evaluating both T2I models and MLLMs in the context\nof sentiment analysis. Additionally, a progressive evalu-\nation approach is employed, moving from coarse-grained\nto fine-grained metrics, to offer a more detailed and com-\nprehensive assessment of the sentiment analysis capabili-\nties of MLLMs. The experimental results demonstrate that\nexisting T2I models are more effective at generating pos-\nitive emotions than negative ones. Meanwhile, although\nMLLMs show a certain degree of effectiveness in distin-\nguishing and recognizing human emotions, they fall short of\nhuman-level accuracy, particularly in fine-grained emotion\nanalysis. The MEMO-Bench will be made publicly avail-\nable to support further research in this area.", "sections": [{"title": "1. Introduction", "content": "Currently, artificial intelligence (AI) has achieved capabil-\nities comparable to, and in some cases surpassing, human\nperformance across a variety of domains, suggesting that\nit possesses a certain degree of rational thinking. How-\never, the presence of rationality alone does not equate to\ncomplete \"intelligence,\" as the question of whether AI truly\nexperiences emotions remains unresolved [5, 17, 28]. In\nreal-world applications, such as human-computer interac-\ntion (HCI) [8, 42, 59] and embodied intelligence [20, 50],\nas well as in the design of immersive media like virtual\ndigital humans [26, 64, 85, 87], user emotions play a criti-\ncal role in shaping AI decision-making processes. Unfor-\ntunately, most existing interactive systems rely primarily\non text-based interfaces [47], rather than leveraging mul-\ntimodal information, such as visual and auditory cues, to\neffectively capture and understand emotional fluctuations.\nThis limitation significantly restricts the emotional compre-\nhension capabilities of these systems. The advent of Mul-\ntimodal Large Language Models (MLLMs) introduces new\npossibilities for emotion-aware interactions, enabling AI to\npotentially understand human emotions. However, the ex-\ntent of this ability remains largely unexplored. Existing\nresearch [37] has focused predominantly on assessing the\ncapacity of MLLMs to recognize and categorize emotions,\noften overlooking their ability to understand emotions at a\nmore granular level. Moreover, the requirement for emotion\nunderstanding extends beyond MLLMs to include Text-to-\nImage (T2I) models, which are increasingly used to gener-\nate character portraits that convey specific emotional states.\nConsequently, evaluating AI's emotion analysis capabilities\nshould encompass both MLLMs and T2I models, as both\nare integral to advancing emotion-aware AI systems.\nIn this paper, we introduce a multiple emotion analysis\nbenchmark, MEMO-Bench, designed to evaluate the emo-\ntional analysis capabilities of both T2I models and MLLMs.\nMEMO-Bench consists of 7,145 AI-generated portrait im-\nages (AGPIs), representing six distinct emotional states.\nSpecifically, for each emotion, we employ 100 prompts,\nwhich are provided to T2I models for portrait generation.\nUpon generation, the resulting AGPIs are manually re-\nviewed to assess the alignment between the images and\nthe intended emotional expression in prompts, allowing us\nto evaluate the emotion generation abilities of T2I mod-\nels. For MLLM evaluation, we employ a progressive emo-\ntion assessment approach that ranges from coarse-grained\nto fine-grained analysis. Initially, the MLLM is tasked\nwith categorizing the emotions of the AGPIs. Subsequently,\nthe MLLM is presented with the correctly categorized por-\ntraits and asked to assess their emotional intensity at a finer\nlevel. The experimental results show that the existing T2I\nmodel has some sentiment generation capability, but is still\nlimited in negative sentiment generation. MLLM, on the\nother hand, performs relatively well in coarse-grained emo-\ntion categorization, but is completely incapable of accu-\nrately understanding the degree of fine-grained human emo-\ntions. These findings highlight the current limitations in\nAl's ability to fully comprehend human emotions, provid-\ning valuable insights for the development of more advanced\nemotion-aware Al systems. The main contributions of this\npaper are as follows:\n\u2022 A large-scale dataset, MEMO-Bench, is introduced to as-\nsess the sentiment analysis capabilities of AI. This dataset\nincludes 7,145 emotionally generated AGPIs across six\ndifferent emotions, produced by twelve T2I models.\n\u2022 Based on MEMO-Bench, a multiple benchmark frame-\nwork for Al's sentiment analysis capability is proposed.\nThe benchmark focuses on the sentiment generation ca-\npability of T2I models as well as tests the sentiment un-\nderstanding capability of MLLMs.\n\u2022 An asymptotic approach is applied for emotion com-\nprehension evaluation. MLLMs are queried with both\ncoarse-grained emotion categorization tasks and fine-\ngrained emotion level analysis tasks to provide a thorough\nassessment of their emotional understanding capabilities."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Affective Computing", "content": "Affective computing, also known as Emotion AI [31, 41], is\na key domain within AI that focuses on enabling machines\nto recognize, interpret, and simulate human emotions. With\nthe emergence of technologies such as virtual digital hu-\nmans, large language models (LLMs), and embodied in-\ntelligence, the integration of human-like emotions into AI\nhas become an area of significant interest [22, 25, 32, 53-\n55, 61, 62]. Affective computing can be broadly categorized\ninto two interrelated tasks: Affect Generation (AG) [46] and\nAffect Understanding (AU) [83], both of which are criti-\ncal for the continued advancement of AI systems [70]. AG\nrefers to the capability of AI to create content that conveys\nspecific or contextually appropriate emotions, either based\non human input or situational cues. In contrast, AU involves\nthe ability of AI to accurately interpret the emotions present\nin diverse forms of multimodal data. Traditionally, senti-\nment analysis and affective computing rely on pre-trained\nlanguage models [19, 40] that are trained on manually la-\nbeled datasets [2, 33, 56]; however, these models often ex-\nhibit limited generalization ability due to constraints in both\nmodel architecture and dataset size. In contrast, LLMs have\nintroduced a new paradigm for affective computation, lever-\naging extensive datasets and advanced model architectures\nto improve performance. Despite the progress made by\nLLMs in emotion-related tasks [3, 63, 69], the question of\nwhether AI is truly capable of achieving emotional aware-\nness remains an open and critical area of research."}, {"title": "2.2. Benchmarks for AI", "content": "As the influence of AI, particularly LLMs, on human life\ncontinues to deepen, there has been a growing interest\namong scholars to quantify the performance of these mod-\nels. This has led to the creation of reliable benchmarks that\ncan guide future research and model development. In re-\ncent years, a variety of benchmarks have been established to\nevaluate the capabilities of LLMs across different domains.\nProminent examples include C-Eval [30], AGI-Eval [84],\nMMLU [29], and CMMLU [36], which assess the profi-\nciency of LLMs in various academic and practical disci-\nplines. Besides, in the field of computer vision, benchmarks\nsuch as Q-Bench [77, 80] and A-Bench [79] have been de-\nveloped to evaluate LLMs' ability to perceive the quality of\ndigital media [81]. Although there have been benchmarks\n[37, 52] designed to evaluate the emotion-perception abili-\nties of LLMs, most existing frameworks still rely on coarse-\ngrained assessments that fail to account for the nuanced lev-\nels of emotional understanding. Furthermore, existing work\nprimarily focuses on the emotional comprehension abilities\nof LLMs, often neglecting the emotional capabilities of gen-\nerative models. In response to this gap, the MEMO-Bench\nintroduced in this paper aims to provide a more comprehen-\nsive evaluation by considering three key aspects, namely,\nthe emotional generation capabilities of generative models,\nthe quality of content generated by these models, and the\nemotional comprehension abilities of MLLMs. This holis-\ntic approach offers a more integrated and thorough assess-\nment of AI's capacity for emotion analysis."}, {"title": "3. Construction of MEMO-Bench", "content": ""}, {"title": "3.1. Emotions and Prompts", "content": "Building on prior research [37], six fundamental emotions\nare selected to construct the MEMO-Bench dataset: happi-"}, {"title": "3.2. Text-to-Image Models", "content": "To comprehensively evaluate the sentiment generation ca-\npabilities of existing T2I models and to provide suitable vi-\nsual data for assessing the sentiment comprehension abil-\nities of MLLMs, we select 12 representative T2I models.\nThe specific details of these models are outlined in Table 1.\nAdditionally, Fig. 3 presents some typical cases that illus-\ntrate the variation in performance across different models.\nAs shown in the figure, even with identical prompts, the im-\nages generated by different T2I models exhibit significant\ndifferences in both image quality and emotional expression.\nIn some instances, certain models produce images that are"}, {"title": "3.3. Subjective Annotation", "content": "In contrast to existing studies that primarily rely on GPT-\n40 responses for annotation [7], this paper ensures the re-\nliability and validity of the annotation process by subjec-\ntively annotating the 7,145 AGPIs. This is achieved through\nthe recruitment of volunteers who assess the images across\nthree dimensions: sentiment category, sentiment intensity,\nand image quality. Specifically, 15 male and 14 female\nparticipants are invited to participate in the subjective an-\nnotation, following the guidelines outlined in ITU-R B.T.\n500-13 [10]. The annotation process takes place in a well-\ncontrolled laboratory environment to maintain consistency.\nTo facilitate the presentation and annotation of the gener-\nated images, we design a subjective annotation platform us-\ning Gradio [1]. The platform includes modules for image\nquality assessment, sentiment recognition and level analy-\nsis and is displayed on an iMac monitor with a resolution\nof 4096x2304. The entire subjective annotation process is\ndivided into 15 phases, with no more than 500 AGPI anno-\ntation tasks per phase. To ensure the quality and reliability\nof the annotations, volunteers are instructed to take a break\nof at least 15 minutes between each phase, with a maximum\nof four phases completed per day. Prior to the start of the\nfirst phase, all volunteers underwent a 30-minute training\nsession, which included an introduction to the annotation\ntask and the platform interface.\nAt the end of the annotation process, a total of 207,205\nannotation sets are collected. Each annotation for the j-th\nAGPI by the i-th volunteer can be represented as a ternary\ntuple (et, ed, q), where et denotes the emotion category,\ned refers to the degree of emotion, and q represents image\nquality. For sentiment categorization, the emotion et with\nthe highest frequency of occurrence is selected as the final\nsentiment category Er of the generated image. In the case\nof sentiment intensity and image quality, z-scores [71-74,\n82, 90-92] are applied to standardize values of ed and q:\n\\begin{equation}\nz_{ij} = \\frac{u_{ij} - \\mu_i}{\\sigma_u}, \\quad \\mu_i = \\frac{1}{N_i}\\sum_{j=1}^{N_i}u_{ij}, \\quad \\sigma = \\sqrt{\\frac{1}{N_i}\\sum_{j=1}^{N_i}(u_{ij} - \\mu_i)^2}, u \\in [e_d, q],\n\\end{equation}\nwhere \u03bc = $\\frac{1}{N_i}\\sum_{j=1}^{N_i}U_{ij}$, \u03c3 = $\\sqrt{\\frac{1}{N_i}\\sum_{j=1}^{N_i}(U_{ij} - \\mu_i)^2}$, and\nNi denotes the total number of AGPIs evaluated by the i-th\nsubject. In accordance with the rejection procedure outlined\nin [10], ratings from unreliable subjects are excluded. The\nremaining z-scores zij are linearly rescaled to range [0, 5].\nFinally, the mean opinion scores (MOSs) for the j-th AGPI\nare computed by averaging the rescaled z-scores:\n\\begin{equation}\n\\operatorname{MOS}_j^u = \\frac{1}{M} \\sum_{i=1}^{M} u_{ij}', u \\in [e_d, q],\n\\end{equation}\nwhere M denotes the number of the valid subjects, and $z_{ij}'$\nrepresents the rescaled z-scores."}, {"title": "4. Benchmark for T2I Models", "content": ""}, {"title": "4.1. Experiment Setup", "content": "The evaluation of the emotional generation capability of T2I\nmodels encompasses two critical dimensions: the quality of\nthe AGPIs and the accuracy of emotion synthesis. To as-\nsess these aspects, a comprehensive evaluation framework\nis employed, which integrates subjective annotations and\ncustomized prompts. This approach allows for the assess-\nment of both the visual quality and the emotional fidelity of"}, {"title": "4.2. Generated Image Quality", "content": "To conduct a thorough evaluation of the quality of AGPIs\nwithin the MEMO-Bench, a detailed analysis of the distri-\nbution of Mean Opinion Scores (MOSs), derived from sub-\njective assessments, is presented in Fig. 4. The results in\nFig. 4 illustrate a predominant trend where the majority of\nAGIs exhibit a high standard of visual fidelity. This find-\ning not only highlights the outstanding performance of the"}, {"title": "4.3. Emotion Generation Capacity", "content": "Two metrics, GACC and GERR, are computed for all\nAGIs in the MEMO-Bench dataset, and the results are pre-\nsented in Table 2 and Fig. 6, from which some insights can\nbe drawn: 1) There are significant variations in the emo-\ntion generation capabilities of different T2I models. Specif-\nically, SGA outperforms all other models in terms of senti-\nment generation accuracy, whereas FCS exhibits the poor-\nest performance, with a gap of over 35% in GACC between"}, {"title": "5. Benchmark for MLLMS", "content": ""}, {"title": "5.1. Experiment Setup", "content": "To investigate the emotion comprehension capabilities of\nexisting MLLMs, a progressive testing methodology from\ncoarse-grained to fine-grained emotion analysis was de-\nsigned. Initially, 16 prominent and advanced MLLMs are\nselected for the evaluation, including two closed-source"}, {"title": "5.2. Coarse-grained Emotional Classification", "content": "The understanding accuracy, denoted as U ACC, and the er-\nror rate, UERR, for various MLLMs in the coarse-grained\nsentiment recognition phase are presented in Fig. 7 and Ta-\nble 3. A combined analysis of these results leads to several\nimportant insights: 1) mPLUG-Owl3 achieves the highest\nemotion classification performance on the MEMO-Bench,\nreaching an accuracy of 0.6759. However, this score indi-\ncates that the performance of current MLLMs in emotion"}, {"title": "5.3. Fine-grained Emotional Comprehension", "content": "All MLLMs are assessed on their ability to comprehend\nfine-grained emotion degrees using the set of correctly cat-\negorized AGPIs, and the results are presented in Table 3.\nThe data in Table 3 reveals that, although most MLLMS\nachieved an accuracy of 0.5 or higher in the emotion cat-\negorization task, their performance in fine-grained emotion\nunderstanding is notably unsatisfactory. This indicates that\nwhile MLLMs can categorize emotions with reasonable ac-\ncuracy, they fail to capture the more nuanced intensity or\ndegree of human emotions. In summary, MLLMs currently\nexhibit a limited capacity for understanding human emo-\ntions, lacking the ability to empathize with or perceive emo-\ntions at a level comparable to human emotional compre-\\hension. This significant finding highlights a critical gap\nin the emotional intelligence of existing MLLMs and pro-\nvides valuable insights for guiding the design and training\nof more advanced emotion-aware models in the future."}, {"title": "6. Conclusion", "content": "The question of whether AI possesses emotions is a crit-\nical and ongoing area of research. Addressing this ques-\ntion requires considering at least two key aspects: the emo-\ntion generation capabilities of Text-to-Image (T2I) mod-\nels and the emotion comprehension abilities of Multimodal\nLarge Language Models (MLLMs). To investigate these di-\nmensions, this study introduces the MEMO-Bench dataset,\nwhich comprises 7,145 AI-generated portrait images (AG-\nPIs) representing six distinct emotions, produced using 12\nstate-of-the-art T2I models. To ensure the accuracy and\nreliability of the labeling data, 29 volunteers are recruited\nto annotate the AGPIs on three dimensions: image quality,\nemotional content, and the intensity of the emotion, using\na multi-task subjective labeling interface developed on Gra-\ndio. The emotion generation capabilities of the T2I mod-\nels are assessed based on the quality and generation accu-\nracy of the AGPIs. The results indicate that the existing\nT2I models excel in generating high-quality portraits with\npositive emotions, but show some limitations in generating\nnegative emotions. Additionally, the MEMO-Bench dataset\nis utilized to evaluate the emotion comprehension capabil-\nities of MLLMs using a progressive testing approach. The\nfindings reveal that while MLLMs can recognize and cat-\negorize emotions to some extent, they are unable to assess\nthe intensity or degree of emotions. Overall, the results sug-\ngest that current AI models, both T2I and MLLMs, lack a\nfull understanding of or capacity for emotions, highlighting\na significant gap in their emotional intelligence."}]}