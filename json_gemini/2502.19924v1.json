{"title": "DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models", "authors": ["Weihao Wu", "Zhiwei Lin", "Yixuan Zhou", "Jingbei Li", "Rui Niu", "Qinghua Wu", "Songjun Cao", "Long Ma", "Zhiyong Wu"], "abstract": "Conversational speech synthesis (CSS) aims to synthesize both contextually appropriate and expressive speech, and considerable efforts have been made to enhance the understanding of conversational context. However, existing CSS systems are limited to deterministic prediction, overlooking the diversity of potential responses. Moreover, they rarely employ language model (LM)-based TTS backbones, limiting the naturalness and quality of synthesized speech. To address these issues, in this paper, we propose DiffCSS, an innovative CSS framework that leverages diffusion models and an LM-based TTS backbone to generate diverse, expressive, and contextually coherent speech. A diffusion-based context-aware prosody predictor is proposed to sample diverse prosody embeddings conditioned on multimodal conversational context. Then a prosody-controllable LM-based TTS backbone is developed to synthesize high-quality speech with sampled prosody embeddings. Experimental results demonstrate that the synthesized speech from DiffCSS is more diverse, contextually coherent, and expressive than existing CSS systems.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of deep learning, end-to-end text-to-speech (TTS) systems have made significant strides [1]\u2014[4]. However, in certain application scenarios such as chatbots and virtual assistants, these systems often underperform due to their limited ability to understand conversational context, highlighting the growing importance of conversational speech synthesis (CSS). CSS aims to generate speech that is not only appropriate for the current utterance but also coherent with the broader conversational context. Therefore, effective context modeling is crucial for CSS models. Guo et al. [5] are the first to propose a GRU-based conversational context encoder that sequentially processes the textual conversational context. Subsequently, graph-based networks [6], [7] are introduced to model the cross-sentence and cross-speaker relationships. To enhance the extraction of fine-grained details, multi-scale information has also been explored [7], [8]. Deng et al. [9] introduce contrastive learning to CSS, facilitating the learning of more stable and discriminative context representation. Most recently, Liu et al. [10] employ a GPT-based architecture to capture semantic and stylistic features within conversational sequences.\nHowever, current CSS methods are constrained by deterministic prosody predictions. Specifically, in a given conversational context, speech can be delivered in various ways, each conveying different emotions and intentions. Consequently, multiple prosody variations can correspond to the same conversational context. This one-to-many mapping problem has not been thoroughly explored in previous CSS systems, thus limiting their ability to generate diverse and expressive prosody.\nAdditionally, nearly all previous CSS approaches have relied on traditional TTS backbones, resulting in limited naturalness and quality. Recent advancements in text-modal language models [11]\u2013[13] have further driven the development of language model (LM)-based TTS systems [14]\u2013[17]. These systems employ pre-trained audio codec models [18]\u2013[20] to encode speech waveforms into discrete codes, which are then predicted by prompt-based language models. When trained on large-scale datasets, these models can effectively extract semantic information and synthesize speech that closely approximates human speech in terms of expressiveness and voice quality.\nIn this paper, we propose a novel CSS framework: DiffCSS. Inspired by the success of diffusion models in capturing complex data distributions and generating diverse outputs in the field of computer vision [21]\u2013[24], DiffCSS leverages diffusion models to generate diverse and expressive prosody conditioned on the conversational context. Additionally, DiffCSS integrates an LM-based TTS backbone to synthesize high-quality speech. In this framework, we first employ a pre-trained codec [25] model to extract prosody-related features from reference speech. Based on these features, we develop a prosody-enhanced ParlerTTS [17], which is capable of synthesizing expressive speech conditioned on provided prosody embeddings. To generate diverse and context-appropriate prosody embeddings from multimodal conversational context, we design a diffusion-based context-aware prosody predictor. Experimental results demonstrate that our proposed method significantly outperforms deterministic baselines in terms of expressiveness and contextual coherence. Furthermore, the"}, {"title": "II. METHODOLOGY", "content": "The architecture of our proposed model is illustrated in Fig. 1. It consists of two primary components: a TTS backbone based on ParlerTTS [17] and a diffusion-based context-aware prosody predictor. The TTS backbone synthesizes high-quality speech based on varying prosody inputs, while the prosody predictor generates diverse prosody embeddings conditioned on the conversational context.\nInspired by the success of LM-based TTS models, we develop a prosody-enhanced ParlerTTS [17] as our TTS backbone, which predicts pre-extracted acoustic tokens by decoder-only transformer blocks. By adopting the delayed pattern introduced in [26], the TTS backbone is capable of generating high-quality speech autoregressively in a short amount of time. Within the backbone, we designed a prosody extractor to learn prosody embeddings in an unsupervised manner from\nTo predict diverse and contextually appropriate prosody, we design a diffusion-based context-aware prosody predictor that generates the current prosody embedding conditioned on both the multimodal conversational context and the current text. The structure of the prosody predictor is illustrated in Fig. 2, where a set of Transformer encoder blocks are employed as the underlying denoiser network \u03b8.\nFor a conversation chunk of length $N+1$, we combine the multimodal context information as described in Equation 1, where $c$ denotes the multimodal context information, $s_i$ and $p_i$ denote the textual information extracted by a pre-trained sentence-level T5 [27] and the prosody embedding for the $i$-th turn, respectively.\n$c = [s_1, p_1, ..., s_N, p_N]$\nDuring the diffusion process, Gaussian noise is added to the current prosody embedding $p_{N+1}$ according to a fixed noise schedule \u03b11, ..., \u03b1\u03c4, where T is the total diffusion steps. This process can be described as Equation 2 and 3, where \u2208 ~"}, {"title": "A. Prosody-enhanced ParlerTTS", "content": "reference speech. To extract prosody features disentangled from other information, we employ the pre-trained FACodec [25] to extract frame-level prosody features {F1, F2, ..., Fn}, where n represents the number of audio frames. However, the computation cost of the TTS backbone increases linearly with the length of prosody features. To reduce resource consumption, we introduce a cross-attention layer with learnable query tokens {Q1, Q2, ..., Qm } to derive fixed-length prosody embeddings {P1, P2, ..., Pm}, where m < n.\nThese prosody embeddings are then combined with pre-extracted speaker embeddings and serve as the keys and values in the cross-attention layer of the TTS backbone, guiding the speech synthesis process."}, {"title": "B. Diffusion-based context-aware prosody predictor", "content": "$\\mathcal{N}(0, I)$ is the Gaussian noise, $z_0$ represents the ground truth prosody embedding $p_{N+1}$ and $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$.\n$q(z_t | z_{t-1}) = \\mathcal{N}(z_t; \\sqrt{\\alpha_t}z_{t-1}, (1 - \\alpha_t)I)$\n$z_t = \\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\nDuring training, we first uniformly sample a time step t from [1, T], based on which the ground truth prosody embedding $z_0$ is diffused into $z_t$ according to Formula 3. Then we concatenate textual information of the current sentence $s_{N+1}$ with $z_t$ as the input of the diffusion denoiser. Multimodal context information $c$ is incorporated through cross-attention. Given $z_t$, $s_{N+1}$, $t$ and $c$, the denoiser predicts the added Gaussian noise $\\epsilon_\\theta(z_t, s_{N+1}, t, c)$ and optimizes its parameters with the following denoising objective:\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{t,z_t,c}[||\\epsilon - \\epsilon_\\theta(z_t, s_{N+1}, t, c) ||^2]$\nIn the inference stage, we sample $z_T \\sim \\mathcal{N}(0, I)$ and iteratively perform the denoising process according to Equation 5 and 6 to compute $z_{t-1}$ from $z_t$ for $t = T, T - 1, ..., 1$ where $x \\sim \\mathcal{N}(0,I)$ except for $x = 0$ when $t = 1$. After completing T iterations, we obtain the generated prosody embedding $z_0$.\n$z_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(z_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(z_t, s_{N+1},t,c)) + \\sigma_t x$\n$\\sigma_t = \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}}(1 - \\alpha_t)$\nDuring inference, we first extract the multimodal conversational context information c and the current textual information $s_{N+1}$. We then iteratively compute the predicted prosody embedding $z_0$ from Gaussian noise, following Equation 5 and 6. Finally, we feed $z_0$ into the TTS backbone and synthesize speech according to it."}, {"title": "C. Training Strategy and Inference Procedure", "content": "To enhance the controllability of the TTS module and improve the prosody diversity of synthesized speech, we propose a two-stage training strategy, where prosody is explicitly modeled as an intermediate representation. In order to enhance the semantic understanding and speech synthesis capability of the TTS backbone, we first pre-train the TTS backbone on a large-scale dataset and subsequently fine-tune it on a conversational dataset. After completing the TTS backbone training, we freeze its parameters and use it to extract ground truth prosody embeddings from reference speech. We then divide the conversations into equal-length chunks, which are used to train the diffusion-based context-aware prosody predictor."}, {"title": "III. EXPERIMENTS", "content": "We conduct our experiments on two English datasets. For TTS backbone pre-training, we use an open-source multi-speaker English speech dataset LibriTTS-R [28], which contains 585 hours of high-quality speech from 2,456 speakers. For the conversational dataset, we select DailyTalk [29], which includes 2,541 conversations spanning about 20 hours, performed by two speakers. These conversations are divided into equal-length chunks, each containing 5 utterances, with the first 4 serving as the conversational context for the fifth. The first 2400 conversations are used for training while the remaining 141 are reserved for testing. A pre-trained audio codec model DAC [20] is employed to encode the raw waveform with 24kHz sampling rate and reconstruct the waveform based on the predicted acoustic tokens. Speaker embeddings are extracted by a pre-trained voiceprint model.\nIn our implementation, the TTS backbone comprises 12 transformer decoder blocks, and the diffusion-based context-aware prosody predictor consists of 6 transformer encoder blocks. We pre-train the TTS backbone on LibriTTS-R for 150000 iterations and finetune it on DailyTalk for 20000 iterations with a batch size of 64. The diffusion-based context-aware prosody predictor is trained on DailyTalk for 100000 iterations with a batch size of 32."}, {"title": "A. Training setup", "content": "We implemented the following three models as baselines.\nThis model uses a unidirectional GRU to process conversational context sequentially.\nThis model employs a relation-aware graph convolutional network to model context from both text and audio modalities.\nWe implement a contextual prosody predictor based on the Transformer encoder, which shares the same structure and parameters as our proposed method but without diffusion modeling."}, {"title": "B. Baseline Models", "content": "To evaluate the performance of our proposed model in comparison to baselines, we conduct two separate mean opinion score (MOS) tests: one for speech expressiveness and the other for contextual coherence. We randomly select 15 samples and their corresponding contexts from the test set for evaluation. A total of 20 listeners are invited to rate the synthesized speech on a scale of 1 to 5 with 1 point interval, based on both expressiveness and contextual coherence."}, {"title": "C. Subjective Evaluation", "content": "As shown in Table I, our proposed method achieves the best E-MOS of 3.602 and C-MOS of 3.574 compared to baselines. This indicates that our model can generate prosody that is both contextually appropriate and expressive. While the Transformer encoder-based method outperforms the GRU-based method, it lags behind the DialogueGCN-based method and significantly underperforms when compared to the proposed method. This suggests that although the Transformer encoder exhibits a moderate ability in modeling conversational context, the integration of diffusion models is essential for improving both prosody prediction and contextual comprehension."}, {"title": "D. Objective Evaluation", "content": "For objective evaluation, we employ Mel-Cepstral Distortion (MCD) to assess the overall quality of the synthesized speech, and Number of Statistically-Different Bins (NDB) along with Jensen-Shannon Divergence (JSD) [30] to evaluate the diversity of prosody, following [31]. NDB and JSD are computed through a clustering process. The procedure is as follows: 1) Cluster the ground-truth prosody into n bins to obtain the ground-truth prosody distribution across bins. 2) Generate prosody samples using the prosody predictor, and assign each generated sample to the closest bin. 3) Calculate the proportion of generated samples in each bin, resulting in a new distribution across the bins. 4) Evaluate the similarity between the generated and ground-truth distributions. JSD is the Jensen-Shannon divergence between the two distributions, and NDB counts the number of bins with statistically significant differences in sample proportions. In this evaluation, we set the number of prosody clustering bins to 20.\nAs presented in Table I, our method achieves an NDB of 4 and a JSD of 0.036, significantly outperforming all baselines. This demonstrates that the generated prosody from our method aligns much more closely with the ground truth prosody distribution. Furthermore, our proposed method also surpasses all baselines in MCD, indicating its ability to synthesize high-quality speech.\nWe further visualized the prosody distributions of different methods, as shown in Figure 3, where each unique color corresponds to a specific cluster. The Transformer encoder-based method predicts prosody embeddings concentrated in only a few clusters, with sparse representation in others. In contrast, prosody embeddings generated by our proposed method are more evenly distributed across clusters, closely resembling the ground truth distribution. This indicates that the deterministic baseline tends to predict similar, less diverse prosody, while our proposed method exhibits significantly improved prosody diversity. These results further highlight the importance of incorporating diffusion models in enhancing prosody diversity."}, {"title": "E. Ablation Study on multimodal context", "content": "Furthermore, we investigate the impact of multimodal context by evaluating three different settings: (1) Proposed model w/o textual context (2) Proposed model w/o acoustic context (3) Proposed model without full context. The modalities are excluded by setting the corresponding contextual information to zeros. We conducted a separate training session for each ablation setting, effectively preventing the model from receiving unwanted contextual information.\nAs shown in Table II, the absence of either modality reduces the overall quality and prosody diversity of the synthesized speech, thereby diminishing the effectiveness of context modeling. Notably, the exclusion of the acoustic context leads to a greater decline in performance compared to the textual context, underscoring the crucial role of acoustic information in modeling conversational context."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduce DiffCSS, a novel CSS framework designed to generate diverse and high-quality speech. We propose a diffusion-based context-aware prosody predictor to generate diverse prosody embeddings conditioned on the conversational context. Additionally, we develop an LM-based TTS backbone to synthesize high-quality speech based on sampled prosody embeddings. Experimental results demonstrate that our proposed model outperforms existing baselines in terms of expressiveness, contextual coherence, and prosody diversity."}]}