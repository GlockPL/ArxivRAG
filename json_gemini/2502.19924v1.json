{"title": "DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models", "authors": ["Weihao Wu", "Zhiwei Lin", "Yixuan Zhou", "Jingbei Li", "Rui Niu", "Qinghua Wu", "Songjun Cao", "Long Ma", "Zhiyong Wu"], "abstract": "Conversational speech synthesis (CSS) aims to synthesize both contextually appropriate and expressive speech, and considerable efforts have been made to enhance the understanding of conversational context. However, existing CSS systems are limited to deterministic prediction, overlooking the diversity of potential responses. Moreover, they rarely employ language model (LM)-based TTS backbones, limiting the naturalness and quality of synthesized speech. To address these issues, in this paper, we propose DiffCSS, an innovative CSS framework that leverages diffusion models and an LM-based TTS backbone to generate diverse, expressive, and contextually coherent speech. A diffusion-based context-aware prosody predictor is proposed to sample diverse prosody embeddings conditioned on multimodal conversational context. Then a prosody-controllable LM-based TTS backbone is developed to synthesize high-quality speech with sampled prosody embeddings. Experimental results demonstrate that the synthesized speech from DiffCSS is more diverse, contextually coherent, and expressive than existing CSS systems.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of deep learning, end-to-end text-to-speech (TTS) systems have made significant strides [1]\u2014[4]. However, in certain application scenarios such as chatbots and virtual assistants, these systems often underperform due to their limited ability to understand conversational context, highlighting the growing importance of conversational speech synthesis (CSS). CSS aims to generate speech that is not only appropriate for the current utterance but also coherent with the broader conversational context. Therefore, effective context modeling is crucial for CSS models. Guo et al. [5] are the first to propose a GRU-based conversational context encoder that sequentially processes the textual conversational context. Subsequently, graph-based networks [6], [7] are introduced to model the cross-sentence and cross-speaker relationships. To enhance the extraction of fine-grained details, multi-scale information has also been explored [7], [8]. Deng et al. [9] introduce contrastive learning to CSS, facilitating the learning of more stable and discriminative context representation. Most recently, Liu et al. [10] employ a GPT-based architecture to capture semantic and stylistic features within conversational sequences.\nHowever, current CSS methods are constrained by deterministic prosody predictions. Specifically, in a given conversational context, speech can be delivered in various ways, each conveying different emotions and intentions. Consequently, multiple prosody variations can correspond to the same conversational context. This one-to-many mapping problem has not been thoroughly explored in previous CSS systems, thus limiting their ability to generate diverse and expressive prosody. Additionally, nearly all previous CSS approaches have relied on traditional TTS backbones, resulting in limited naturalness and quality. Recent advancements in text-modal language models [11]\u2013[13] have further driven the development of language model (LM)-based TTS systems [14]\u2013[17]. These systems employ pre-trained audio codec models [18]\u2013[20] to encode speech waveforms into discrete codes, which are then predicted by prompt-based language models. When trained on large-scale datasets, these models can effectively extract semantic information and synthesize speech that closely approximates human speech in terms of expressiveness and voice quality.\nIn this paper, we propose a novel CSS framework: DiffCSS. Inspired by the success of diffusion models in capturing complex data distributions and generating diverse outputs in the field of computer vision [21]\u2013[24], DiffCSS leverages diffusion models to generate diverse and expressive prosody conditioned on the conversational context. Additionally, DiffCSS integrates an LM-based TTS backbone to synthesize high-quality speech. In this framework, we first employ a pre-trained codec [25] model to extract prosody-related features from reference speech. Based on these features, we develop a prosody-enhanced ParlerTTS [17], which is capable of synthesizing expressive speech conditioned on provided prosody embeddings. To generate diverse and context-appropriate prosody embeddings from multimodal conversational context, we design a diffusion-based context-aware prosody predictor. Experimental results demonstrate that our proposed method significantly outperforms deterministic baselines in terms of expressiveness and contextual coherence. Furthermore, the"}, {"title": "II. METHODOLOGY", "content": "The architecture of our proposed model is illustrated in Fig. 1. It consists of two primary components: a TTS backbone based on ParlerTTS [17] and a diffusion-based context-aware prosody predictor. The TTS backbone synthesizes high-quality speech based on varying prosody inputs, while the prosody predictor generates diverse prosody embeddings conditioned on the conversational context.\nInspired by the success of LM-based TTS models, we develop a prosody-enhanced ParlerTTS [17] as our TTS backbone, which predicts pre-extracted acoustic tokens by decoder-only transformer blocks. By adopting the delayed pattern introduced in [26], the TTS backbone is capable of generating high-quality speech autoregressively in a short amount of time. Within the backbone, we designed a prosody extractor to learn prosody embeddings in an unsupervised manner from\nreference speech. To extract prosody features disentangled from other information, we employ the pre-trained FACodec [25] to extract frame-level prosody features {$F_1, F_2, ..., F_n$}, where n represents the number of audio frames. However, the computation cost of the TTS backbone increases linearly with the length of prosody features. To reduce resource consumption, we introduce a cross-attention layer with learnable query tokens {$Q_1, Q_2, ..., Q_m$} to derive fixed-length prosody embeddings {$P_1, P_2, ..., P_m$}, where m < n.\nThese prosody embeddings are then combined with pre-extracted speaker embeddings and serve as the keys and values in the cross-attention layer of the TTS backbone, guiding the speech synthesis process.\nTo predict diverse and contextually appropriate prosody, we design a diffusion-based context-aware prosody predictor that generates the current prosody embedding conditioned on both the multimodal conversational context and the current text. The structure of the prosody predictor is illustrated in Fig. 2, where a set of Transformer encoder blocks are employed as the underlying denoiser network \u03b8.\nFor a conversation chunk of length N + 1, we combine the multimodal context information as described in Equation 1, where c denotes the multimodal context information, $s_i$ and $p_i$ denote the textual information extracted by a pre-trained sentence-level T5 [27] and the prosody embedding for the i-th turn, respectively.\n$c = [s_1, p_1, ..., s_N, p_N]$ (1)\nDuring the diffusion process, Gaussian noise is added to the current prosody embedding $p_{N+1}$ according to a fixed noise schedule $\u03b1_1, ..., \u03b1_T$, where T is the total diffusion steps. This process can be described as Equation 2 and 3, where \u2208 ~"}, {"title": "III. EXPERIMENTS", "content": "We conduct our experiments on two English datasets. For TTS backbone pre-training, we use an open-source multi-speaker English speech dataset LibriTTS-R [28], which contains 585 hours of high-quality speech from 2,456 speakers. For the conversational dataset, we select DailyTalk [29], which includes 2,541 conversations spanning about 20 hours, performed by two speakers. These conversations are divided into equal-length chunks, each containing 5 utterances, with the first 4 serving as the conversational context for the fifth. The first 2400 conversations are used for training while the remaining 141 are reserved for testing. A pre-trained audio codec model DAC [20] is employed to encode the raw waveform with 24kHz sampling rate and reconstruct the waveform based on the predicted acoustic tokens. Speaker embeddings are extracted by a pre-trained voiceprint model.\nIn our implementation, the TTS backbone comprises 12 transformer decoder blocks, and the diffusion-based context-aware prosody predictor consists of 6 transformer encoder blocks. We pre-train the TTS backbone on LibriTTS-R for 150000 iterations and finetune it on DailyTalk for 20000 iterations with a batch size of 64. The diffusion-based context-aware prosody predictor is trained on DailyTalk for 100000 iterations with a batch size of 32.\nWe implemented the following three models as baselines.\nThis model uses a unidirectional GRU to process conversational context sequentially.\nThis model employs a relation-aware graph convolutional network to model context from both text and audio modalities.\nWe implement a contextual prosody predictor based on the Transformer encoder, which shares the same structure and parameters as our proposed method but without diffusion modeling.\nTo evaluate the performance of our proposed model in comparison to baselines, we conduct two separate mean opinion score (MOS) tests: one for speech expressiveness and the other for contextual coherence. We randomly select 15 samples and their corresponding contexts from the test set for evaluation. A total of 20 listeners are invited to rate the synthesized speech on a scale of 1 to 5 with 1 point interval, based on both expressiveness and contextual coherence."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduce DiffCSS, a novel CSS framework designed to generate diverse and high-quality speech. We propose a diffusion-based context-aware prosody predictor to generate diverse prosody embeddings conditioned on the conversational context. Additionally, we develop an LM-based TTS backbone to synthesize high-quality speech based on sampled prosody embeddings. Experimental results demonstrate that our proposed model outperforms existing baselines in terms of expressiveness, contextual coherence, and prosody diversity."}]}