{"title": "FernUni LLM Experimental Infrastructure (FLEXI) \u2013\nEnabling Experimentation and Innovation in Higher Education\nThrough Access to Open Large Language Models", "authors": ["Torsten Zesch", "Michael Hanses", "Niels Seidel", "Piush Aggarwal", "Dirk Veiel", "Claudia de Witt"], "abstract": "Using the full potential of LLMs in higher edu-\ncation is hindered by challenges with access to\nLLMs. The two main access modes currently\ndiscussed are paying for a cloud-based LLM\nor providing a locally maintained open LLM.\nIn this paper, we describe the current state of\nestablishing an open LLM infrastructure at Fer-\nnUniversit\u00e4t in Hagen under the project name\nFLEXI (FernUni LLM Experimental Infrastruc-\nture). FLEXI enables experimentation within\nteaching and research with the goal of gener-\nating strongly needed evidence in favor (or\nagainst) the use of locally maintained open\nLLMs in higher education. The paper will pro-\nvide some practical guidance for everyone try-\ning to decide whether to run their own LLM\nserver.", "sections": [{"title": "1 Motivation", "content": "While the potential of Large Language Models\n(LLMs) for higher education has been identified\n(Kasneci et al., 2023), as long as access is not pro-\nvided by the university in some way, everybody\nis using the commercial service of their choice,\nleading to issues including potential data security\nproblems, decreased educational equity, potentially\nhigh costs, etc.\nThus, there is an ongoing discussion about\nwhether and how universities should provide LLM\naccess (Salden et al., 2024). Two main modes\nare generally being discussed: paying for a cloud-\nbased LLM or providing a locally maintained open\nLLM. Figure 1 gives a high-level overview of how\na closed, cloud-based LLM could be replaced with\nan open-source model. As it shows, replacing a\nclosed LLM with an open LLM can be as easy as"}, {"title": "2 FLEXI Concept & Realization", "content": "As we are aiming for an experimental proof-of-\nconcept realization, we aim for a single server that\nis able to run most open-source models. However,\nour concept is based on a bare-metal Kubernetes cluster, which could be extended by additional\nnodes and thus enable scalable and more robust\noperation. We currently ignore guaranteed uptime,\nredundancy, or other factors that would be central\nwhen moving from experimentation to central ser-\nvice delivery."}, {"title": "2.1 Hardware Setup", "content": "Our concept assumes that the university already op-\nerates a data center where the server can be housed.\nConsequently, the university can leverage exist-\ning processes for access control, hardware mainte-\nnance, network, security, or backup.\nThe bare-metal Kubernetes setup allows access\nto all server hardware settings without an additional\n(extra) virtualization layer. This is very helpful\nconsidering the configuration of GPU acceleration.\nWe first piloted the setup on one server (A) and\nthen replicated it on another server (B). Hardware\nand software specifications of the servers can be\nfound in Table 2. Both servers were purchased in\n2023 for approximately 40,000 \u20ac."}, {"title": "2.2 Software Setup", "content": "For serving the LLMs the open-source project Ol-\nlama is used. To ensure optimal performance,\nGPU acceleration is necessary, but Ollama can also\nrun the models without GPU support. NVIDIA and\nAMD graphic cards are supported. Our existing\nservers have NVIDIA GPU's build in. Thus, the\ncombination of operating system, kernel, drivers,\nand software must match the CUDA version com-\npatibility. The operating system is Ubuntu 22.04\nLTS on both of our servers with NVIDIA-535-\nServer and CUDA 12.4 on Server (A) and CUDA\n12.2 on Server (B). On Server (A), both Ollama\nand Open WebUI are deployed as Kubernetes\npods. On server (B), the docker-compose ser-\nvice is used. For routing and load balancing, the\nopen-source software traefik is used as ingress-\ncontroller (Sharma et al., 2021). The usage of con-\ntainerization allows us to quickly switch between\nOllama versions and custom configurations, which\nis very helpful in this experimental setting."}, {"title": "2.3 Model Selection", "content": "The setup described so far allows us to install and\nserve any model publicly hosted in the Ollama li-\nbrary. At the time of writing, there are over 90\nmodels available, from which we must select a suit-\nable subset. Additionally, the web interface enables\nexperimentation with 16,848 models available in\nGGUF format on Huggingface at the time of writ-\ning.\nAt the time of writing, we are testing the models"}, {"title": "2.3.1 Openness", "content": "Open-source large language models (LLMs) come\nin a variety of 'flavors' that significantly differ in\nhow open they actually are (Liesenfeld and Dinge-\nmanse, 2024). The minimum requirement for our\npurposes is that the weights are available so we can\nrun, modify, and improve models on our servers.\nLiesenfeld and Dingemanse (2024) lists several ad-\nditional dimensions, including the availability of\nbasic training data that is used for instruction tun-\ning as well as open documentation and a permissive\nlicense."}, {"title": "2.3.2 Language", "content": "Various open models are available that can han-\ndle unilingual and multilingual queries. For in-\nstance, models like STABLELM2 are trained on\nmultilingual data, including English, Spanish, Ger-"}, {"title": "2.3.3 Quality", "content": "A wide range of benchmarks are available on which\nLLM model quality can be evaluated. Benchmarks\ncan have different specializations, e.g. focusing on\nlanguage capabilities, world knowledge, common\nsense reasoning, or coding. We argue that some\ncapabilities are more important in our educational\nsettings than others. For example, medical knowl-\nedge might not be central at FernUniversit\u00e4t, while\nknowledge of German (Pfister and Hotho, 2024)\nor factual correctness seems more important. We\ndiscuss here our selection of benchmarks:\nARC (Clark et al., 2018) examine LLMs on 7,787\ngrade-school science questions. The test is\nchallenging and demands extensive general\nknowledge and strong reasoning skills. It in-\ncludes two sets: Easy and Challenge (with\nparticularly difficult tasks).\nGSM8K (Cobbe et al., 2021) is a set of 8,500\ngrade-school math problems, each requiring\ntwo to eight steps to solve using basic math\noperations. The questions are simple enough\nfor a smart middle schooler to solve and are\nuseful for testing LLMs' ability to handle mul-\ntistep math problems.\nHellaSwag (Zellers et al., 2019) This benchmark\nevaluates natural language inference (NLI) by"}, {"title": "2.3.4 Safety", "content": "Applications of LLMs within higher education\nraise concerns about their security and potential\nvulnerabilities. Ensuring LLM security involves\npreventing misuse by malicious actors or avoid-\ning unintentional errors, such as accidentally re-\nvealing email addresses. Unlike traditional cyber-\nsecurity, LLM security depends significantly on\nnatural language processing (NLP) techniques be-\ncause most attack strategies are language-based.\nAttacks can occur due to conflicts between appli-\ncation builders, end-users, and external tool out-\nputs, especially when there is explicit knowledge\nabout the builder's intentions or policies (Wei et al.,\n2024). Therefore, it is crucial for a secure model to\nundergo various vulnerability assessments before\nbeing deployed. LLM security evaluation frame-\nworks, such as the 'Generative AI Red-teaming\nand Assessment Kit' (garak), facilitate this pro-\ncess (Derczynski et al., 2024). Through systematic\nprobing, it helps users identify vulnerabilities in\nlanguage models or dialog systems. Checks such as"}, {"title": "2.3.5 Size", "content": "Finally, as FLEXI operates with the limited re-\nsources of a public university, model size is an\nissue, as bigger models might not run at all on our\nhardware, or throughput might be insufficient. As\nour main goal here is experimentation, we include\na few models from the fringes of the size distribu-\ntion but mainly focus on mid-distribution models.\nHowever, most currently available open models\nwould run on our servers, but throughput and maxi-\nmum concurrent queries might be insufficient (see\nsections 3 and 5)."}, {"title": "2.4 Maintenance & Monitoring", "content": "We use Checkmk to monitor the servers (A) and\n(B) and the resources they contain. The so-called\nCheckmk agent runs on our servers, which collects\ndata from the local system via plugins and transmits\nit to the backend. The backend receives and man-\nages the data and makes it available via dashboards.\nOur data center (Zentrum f\u00fcr Digitalisierung und\nIT, ZDI) operates the backend.\nTo monitor GPU utilization in particular, we use\na special script as a local plugin. The script cap-\ntures the GPU data (via nvidia_smi) and passes it\nto the backend so that the visualizations shown in\nFigure 2 can be viewed there. Using the dashboard,\nwe can see the current utilization of the servers, es-\npecially the GPUs, and the trend over the past days\nand weeks. This allows us to identify both peak\nloads and average server utilization. The knowl-\nedge gained in this way is used to better determine\nthe configuration of the servers for regular opera-\ntion."}, {"title": "2.5 Data Protection", "content": "When using a cloud-based LLM, all requests are\nsent to the cloud provider, enabling user tracking\nand possibly exposing sensitive data. Projects like\nHAWKI solve the tracking issue by bundling all\nrequests from one university so that chats cannot\nbe attributed to a specific person. Commercial"}, {"title": "3 Experiences", "content": "In this section, we describe the most important ex-\nperiences and takeaways from experimenting with\nFLEXI.\nLoad Test To analyze this, we attempted to test\nour server's load using a general-purpose laptop.\nWe sent multiple REST API POST requests to\nFLEXI. To fully utilize the available GPU space,\nmultiple models are initiated simultaneously. This\napproach will make efficient use of the GPUs and\nsignificantly reduce the server response time. In ad-\ndition, multiple instances of smaller models can be\ncreated, allowing it to handle multiple requests si-\nmultaneously. Figure 3 illustrates the load in terms\nof time taken by different models on SERVER (B)\nwhile handling concurrent requests. On the one\nhand, models such as PHI3 can handle multiple\nrequests with hardly an increase in latency time.\nOn the other hand, models such as MISTRAL and\nLLAMA3 exhibit higher latency with increased con-\ncurrent queries. For synchronous tasks like Chat-\nBots, the response times of larger models will prob-\nably be too high once this is scaled to many users.\nHowever, not all tasks require the largest models,\nespecially since benchmark quality improvements\nare often marginal (cf. Figure 4).\nOperating Costs Assuming that the data center\nitself already has fixed costs for the university, op-\nerating costs are dominated by energy demand. In contrast to closed LLM servers, where very lit-\ntle information about energy usage is available,\nwe can directly measure energy usage. For the 5-\nday period shown in Figure 2, approximately 26.7\nkilowatt-hours (kWh) were consumed by 8 GPUs,\ni.e. about 5 kWh per day. The theoretical maxi-\nmum, which we have not measured yet, would be\naround 44.16 kWh a day or 16 MWh a year. At\n0.30 \u20ac per kWh, this translates to a maximal annual\noperating cost for the 8 GPUs of about 5,000 \u20ac.\nThis energy usage translates into 6 tons of emit-"}, {"title": "4 Applications & Use Cases", "content": "While a setup as implemented through FLEXI can\nsupport a wide range of applications (Rashid et al.,\n2024), we list here some use cases that we believe\nto be of specific interest in higher education.\nChat Interface To assist the students and edu-\ncators, as an entry point familiar to everyone who\nhas used the web interface of, e.g., ChatGPT, we\nprovide a chat interface based on OpenWebUI.\nRAG We are experimenting with retrieval-\naugmented generation (RAG) applications, where,\ne.g., lecture notes are indexed, and students are\nprovided with access to a dedicated chatbot that\ncan answer questions regarding the study material.\nAPI Access Selected users who want to work\nwith the API are granted direct API access and may\nimplement their own applications. For example,\nthe https://what2study.de project uses direct\nAPI access to experiment with their system.\nLMS Integration We are developing middleware\nfor LLM access in Moodle, a widely used open-\nsource learning management system (LMS), under"}, {"title": "5 Future work: University-wide scaling", "content": "FLEXI is an experiment aimed at learning more\nabout the possible pitfalls of providing open LLM\naccess. Access is thus currently limited to selected\nearly adopters who know about models' possible\nshortcomings, who do not expect flawless opera-\ntion, and who are giving us valuable feedback on\nhow to improve the service.\nShould we eventually want to drop the 'exper-\nimental' status and provide the same service on\na university-wide level, we have some more chal-"}, {"title": "6 Summary", "content": "In this paper, we have argued that in an academic\ncontext, locally hosting open-source LLMs is cur-"}]}