{"title": "Natural language is not enough: Benchmarking multi-modal generative AI for Verilog generation", "authors": ["Kaiyan Chang", "Zhirong Chen", "Yunhao Zhou", "Wenlong Zhu", "Kun Wang", "Haobo Xu", "Cangyuan Li", "Mengdi Wang", "Shengwen Liang", "Huawei Li", "Yinhe Han", "Ying Wang"], "abstract": "Natural language interfaces have exhibited considerable potential in the automation of Verilog generation derived from high-level specifications through the utilization of large language models, garnering significant attention. Nevertheless, this paper elucidates that visual representations contribute essential contextual information critical to design intent for hardware architectures possessing spatial complexity, potentially surpassing the efficacy of natural-language-only inputs. Expanding upon this premise, our paper introduces an open-source benchmark\u00b9 for multi-modal generative models tailored for Verilog synthesis from visual-linguistic inputs, addressing both singular and complex modules. Additionally, we introduce an open-source visual and natural language Verilog query language framework to facilitate efficient and user-friendly multi-modal queries. To evaluate the performance of the proposed multi-modal hardware generative AI in Verilog generation tasks, we compare it with a popular method that relies solely on natural language. Our results demonstrate a significant accuracy improvement in the multi-modal generated Verilog compared to queries based solely on natural language. We hope to reveal a new approach to hardware design in the large-hardware-design-model era, thereby fostering a more diversified and productive approach to hardware design.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in the deployment of large language models for Verilog generation have garnered significant attention within the realm of electronic design automation (EDA) [6, 7, 15, 17]. These models are emerging as a pivotal methodology for the automated generation of both Verilog code and EDA scripts [4, 15, 18], which heralds a transformative shift in the area. The principal objective of this line of inquiry is to enable hardware developers to quickly design intricate hardware systems without requiring extensive expertise in the specific hardware [9].\nAlthough natural language interfaces have shown potential to perform fundamental code generation tasks, the domain of hardware design presents distinct and intricate challenges that transcend the capabilities of linear linguistic representations. Significant obstacles exist in the generation of intricate architectures involving state machines and the integration of multiple interacting modules, as illustrated in Fig. 3 and Fig. 5. One critical limitation is the challenge of effectively conveying the spatial interrelations and complex nested configurations inherent in hardware designs, attributed to the inherently sequential nature of language. Our empirical studies suggest that a multi-modal generative model, incorporating linguistic elements with visual block diagrams and structural data, holds promise in addressing these constraints.\nWhile our experiments substantiate the potential of multi-modal generative models in the domain of hardware design, three pivotal challenges remain to be addressed. A fundamental issue is the absence of standardized benchmarks, essential to assess and contrast various multi-modal architectures. In the absence of universally accepted benchmarks and metrics, it is arduous to perform systematic evaluations of model performance or to foster advancements in the field. Moreover, the current practice of benchmarking multi-modal models necessitates extensive labeled datasets and a protracted training process, hampering rapid iteration and the exploration of innovative model paradigms. There is a critical need for benchmarking frameworks that facilitate model evaluations with smaller data sets or restricted supervision. Lastly, the concept of multi-modality in the context of hardware tasks requires a precise definition. Important questions remain about which modalities are the most effective, how they should be integrated and processed, and how to quantitatively evaluate their performance on complex generative tasks. In response to these challenges, we introduce a Verilog multi-modal benchmark, a Verilog multi-modal model query language, and a comprehensive specification of the benchmark.\nIn this paper, we elucidate the limitations inherent in relying exclusively on natural language-based Verilog generation methodologies and address the aforementioned challenges. We introduce a novel co-generation query language to facilitate benchmarking, meticulously designed to formalize the automatic co-generation process intrinsic to our proposed method. Moreover, to aid in the comparison and evaluation of prospective works in this nascent field, we propose a benchmark with precise specifications for future investigations in the domain of vision-language Verilog models. The evaluation is conducted using GPT4V, GPT4, LLaVA, and LLaMA, which are representative foundational models for natural language Verilog generation[4]. The results demonstrate that multi-modal large language models for Verilog generation exhibit substantial improvements over conventional methodologies. In summary, we aspire to unveil a novel field in the era of large hardware design models, thereby promoting a more diversified and efficacious approach to hardware design. The contributions are listed below:\n\u2022 We disclose that for hardware structures with spatial complexity, the visual representations presented in this paper offer crucial additional context to clarify design intent, potentially surpassing natural-language-only inputs in AI-driven automated hardware generation.\n\u2022 We introduce an innovative multimodal model query language designed to formalize vision-language descriptions, effectively reducing token cost while enhancing the quality of the generated code.\n\u2022 We present a hierarchical benchmark, ranging from simple to complex, to assess the performance of multimodal large models in Verilog generation. This benchmark will be made open-source in conjunction with the proposed multimodal query language framework.\n\u2022 We conduct a systematic evaluation of multimodal large models using our benchmark, encompassing syntax, functionality, and next-token success rate. Incorporation of visual representations significantly improves the testbench passing rate from 46.88% to 71.81% for the GPT4 series, and from 13.41% to 25.88% for the LLAMA series."}, {"title": "2 Background & Motivation", "content": "Researchers have examined the utilization of large language models (LLMs) in the context of Verilog code generation, as depicted in Tab. 1. Benchmarking results presented in [13, 14, 17] elucidate the potential of these models to mitigate the challenges faced by hardware designers. Significant advancements have been achieved in the domain of fine-tuning for code completion [5, 18], general RTL generation [4], and the generation of EDA tool scripts [15]. Beyond single-sentence models such as GPT-3, conversational large language models (LLMs) have demonstrated proficiency in a variety of advanced applications, including RTL-level repair [4, 8, 19, 21], quantum computing [12], in-memory computing [20], testing [1, 11, 16], and the design of AI domain-specific processors [9]. Nonetheless, the generation of RTL utilizing LLMs remains constrained by several limitations:\nLimitation 1: The inherent limitations of linguistic representations render natural language insufficient for accurately conveying the nested spatial structures of hardware. The intricate spatial relationships among components in computer hardware cannot be fully articulated using solely natural language descriptions. For instance, the spatial accelerator depicted in Fig. 1 encompasses the IO module, PE, and controllers. The spatial interconnections and multi-tiered component structures rapidly exceed the representational capacity of natural language. Terms such as \"below,\" \"embedded,\" \"adjacent,\" and \"surrounding\" fall short in adequately expressing the embedded 2D spatial structures. Although natural language can depict basic hardware organization, its linear and imprecise representations lack the relational expressiveness requisite for modeling multi-level spatial information. Hence, formal diagrams are indispensable for comprehensively representing structural complexity.\nLimitation 2: Inefficiency of Linguistic Descriptions in Multi-Module Hardware Design Within the realm of advanced hardware design, particularly when analyzing systems composed of up to n submodules, traditional linguistic methodologies for articulating interconnections demonstrate notable inefficiencies, as illustrated for n = 12 in Fig. 1. Such hardware configurations can be represented as a multigraph G = (V, E), where each submodule is depicted as a node and the interconnections among these submodules are denoted as edges. The complexity of this system can escalate to the order of O(n\u00b2) in terms of interconnections, thus necessitating a description complexity of O(n\u00b2) tokens. This condition highlights a fundamental limitation inherent to single-modal language models, which are markedly less efficient than multi-modal models when managing the intricate details of interconnectivity within hardware designs.\nLimitation 3: Risk of Misalignment in Complex Hardware Designs Using Language Models Misalignment presents a significant obstacle in the deployment of large language models for intricate hardware design. This complication typically arises when the descriptions provided are ambiguous or incomplete. A particularly salient example of this issue is the port connection, as depicted in Fig. 1. Sole reliance on textual input to specify the functional elements of a design often leads to the model misrouting signals to incorrect ports or unintended submodules, thereby deviating from the expected configuration. This misalignment risk is substantially ameliorated through the incorporation of visual inputs. By adopting"}, {"title": "2.2 Visual and Natural Language Hardware Co-Design Case Study", "content": "This section employs two exemplar scenarios, specifically multi-module hardware and state machine generation, to demonstrate the superior efficacy of multi-modal generation techniques over those that depend exclusively on natural language in the domain of structural hardware.\nMulti-Module Hardware Generation Multi-module hardware architecting is prevalent in sophisticated hardware design paradigms. This case study investigates the potential superiority of multi-modal large-scale models over text-only language models in generating multi-module configurations. For instance, as illustrated in Fig. 2, a chain of Processing Elements (PEs) embodies a spatial architecture. We leverage OpenAI GPT4 Vision as the multi-modal large model to synthesize Verilog code. The findings depicted in Fig. 3 demonstrate that within a multi-module context, the multi-modal large model proficiently captures intricate hardware details, thereby surpassing methods reliant solely on textual input. Consequently, text-only models are prone to inaccuracies and erroneous outputs.\nState Machine Generation State machines are frequently depicted using diagrams. As illustrated in Fig. 4, we utilize an image to represent the state machine responsible for detecting the sequence 10011. When the input matches 10011, the circuit outputs a logic high (1). OpenAI GPT4 Vision was selected as the multi-modal large model for this task. The generated Verilog code was assessed using the pass@5 criterion. The results demonstrated that the multi-modal model (GPT4V) successfully produced a version of the code that passed the testbench evaluations, whereas the text-only model version exhibited deficiencies in certain test cases. Fig. 5 highlights the incorrect state transitions in red text. This example accentuates the superior capability of visual information in extracting structural details from images, thereby markedly enhancing the accuracy of code generation."}, {"title": "2.3 Motivation: Do we need a new benchmark?", "content": "Previous benchmarks such as RTLLLM and Verigen still have the following challenges in multi-modal generation environments."}, {"title": "3 Visual and Natural Language Hardware Co-Design Workflow", "content": "For an end-to-end Verilog generation flow, we split them as two parts as shown in Fig. 6. The frontend accepts natural language and image(i.e., hardware structure) for Verilog generation and outputs Verilog file. The backend accepts the Verilog file and outputs the PPA reports, GDSII layout and function analysis report. To do a democratic hardware design, we chose siliconcompiler and openlane as the backend, which are open-source EDA tools for ASIC synthesis."}, {"title": "3.2 The Formulation of the Circuit Structure in the Benchmark", "content": "To overcome the limitations of natural-language-only hardware generation and challenge I in Sec. 2, we illustrate the visual form and natural language form co-design knowledge as below. The visual form is used to generate top level connections and relations between each module. However, the detailed function can not be presented properly in the image/diagram only with visual information. The detailed information is collected in a natural language format. Several core concepts must be remembered in natural language designs below.\nWord Notations in the visual hardware graph: As shown in Fig. 2, Word Notations are the connections between visual and natural language hardware representation, which are the words on the image. Users declare the name on each block to illustrate it clearly in a natural language format. For example, for a five stage pipeline, the execution stage is annotated on the image and the natural language part can use \"In the execution stage, the processor [DO SOMETHING]\". Without word notations, large models only see the block, which can not detect the right description. Therefore, word notations are the interface between image design and natural language design.\n\u2022 Module name word notations: The diagram name provides a high-level name and description of the overall module or component represented visually. This allows connecting the diagram to natural language that references this module by name.\n\u2022 Wire width word notations: Lines with variable width can represent connections of differing bandwidth or bitwidth. Annotating the width numerically clarifies the intent.\n\u2022 Wire Function word notations: Text labels on wires indicate whether they carry data signals, control signals, clocks, etc. This clarifies the role of connections in the natural language.\n\u2022 Block Function word notations: Major functional blocks are annotated with descriptive labels indicating their roles (e.g., ALU, multiplexer, register file). This allows precise natural language references.\n\u2022 Ports word notations: Port labels designate the names and types of external interfaces. Natural language can then refer to interacting with specific ports.\nDefinition: Relations in the visual hardware graph. As shown in Fig. 2, relations are the arrows and connections on the image, which are the core parts of a hardware structure image. The relations in hardware structure connect the source and sink nodes, which represent inputs and outputs. According to relation theory in computer science, there are three possible relations on the image, one-to-one relations, one-to-many relations and many-to-many relations.\n\u2022 Single arrow relations: These represent singular connections between two components, like an output from one gate going to the input of another gate. They show a direct 1-to-1 relationship(e.g., wire).\n\u2022 1-to Many and Many-to-many arrow relations: These represent bus connections where multiple wires are bundled together. They model relationships where a group of outputs connects to a group of inputs, showing 1-to-many (e.g., bus) or many-to-many (e.g., crossbar) connectivity.\nDefinition: Module Function Description in natural language. Module function description is the function in natural language format. For example, the sentence \"a 3-8 decoder accepts a 3-bit number and outputs a 8-bit number where only one bit is one.\" is a typical module function description in natural language. The \"3-8 decoder\" is the word notation in the corresponding image.\nDefinition: Module Port Description in natural language. Module port description is the port width and function description in natural language format. For example, the sentence \"the input of 3-8 decoder is a 3-bit width wire named innum.\" is a typical port description in the natural language description."}, {"title": "3.3 The proposed Multi-modal Hardware Design Benchmark", "content": "To select a benchmark scientifically, we form the benchmark selection as an optimization problem as Equ. 1, where B denotes the benchmark and Data denotes world-wide data. The target of the benchmark is to be consistent with the worldwide data, which means that the difference between the distributions of the two datasets should be minimized.\n$$min D(B||Data)$$\nTo implement Sec. 3.2, we mitigate the presence of hardware image outliers lacking annotations through template coding, which transposes raw data into a standardized format as delineated in Sec. 3.2. Specifically, template coding processes an annotation-free hardware module image, upon which annotations are appended in accordance with textual specifications. Moreover, we employ four-element pairs as the foundational units of the benchmark, comprising a textual modality (i.e. Prompt_list.txt), a diagrammatic modality (i.e. Circuit_Structure.png), a testbench for pass rate assessment (i.e. TestBench. v), and a reference correct RTL Verilog program (i.e. reference.v).\nBenchmarking Output Complexity via Hierarchical Difficulty Workload We categorize the workloads into three distinct levels, ranging from low to high complexity, to more effectively benchmark the performance of LLMs across varied design complexities, as illustrated in Table 2. Specifically, the arithmetic level encompasses fundamental numerical operations such as addition, multiplication, and division. The logic level includes standard controllers in hardware design, such as edge_detect and pulse_detect. The advanced level pertains to intricate units in CPU design (e.g., a 3-stage pipeline) and core units in matrix multiplication (e.g., 4\u00d74 GEMM).\nBenchmarking Input Complexity via Multi-level Prompting To address the challenge delineated in Sec. 2, we integrate multi-level prompts into the proposed multi-modal benchmark for pre-trained multi-modal models, thereby facilitating the assessment of design comprehension capabilities. For instance, an adept large language model (LLM) can accurately derive the RTL program from both rudimentary and intricate prompts. Specifically, the complexity of the prompts ranges from low to high. The low-level prompt primarily consists of a diagram devoid of detailed natural language exposition. The middle-level prompt includes a diagram accompanied by succinct natural language that outlines the core functionality. The high-level prompt furnishes exhaustive hardware information, encompassing details such as register specifications and clock edge information.\nFine-grain Output Measurement Driven by challenge III, we introduce a fine-grain output segmentation method aimed at providing more thorough evaluations through the use of token-by-token metrics. Assuming the inference result output is {tok\u00b4\u2080, tok\u2081,..., tok\u0274} and the reference benchmark is {tok\u2080, tok\u2081,..., tok\u0274}, the fine-grain success metric is defined in Equation 2, where success is achieved when the tokens match.\n$$success = \\sum_{i=0}^{N} 1_{tok'_i=tok_i}$$"}, {"title": "3.4 Facilitate Flexible Benchmarking using Query Language", "content": "To execute the aforementioned visual and natural language co-design methodology, we introduce and realize a large model query language framework. By applying query language inference within a large language model, it is feasible to manage the LLM's output and input, thereby minimizing inference time and boosting accuracy through efficient network requests and template prompts[3]. We introduce a query language, named VLMQL (Verilog Large Model Query Language), specifically for generating Verilog using visual and natural language.\nVLMQL Framework The Visual and Language Model Query Language (VLMQL) is meticulously crafted to encapsulate the dual modalities of visual and textual hardware co-design, as delineated in Section 3.2. Functioning as an advanced form of controllable prompt engineering, the cornerstone of VLMQL resides in a Python function illustrated in Figure 7. This function's output serves as the input for high-capacity vision-language models. The VLMQL framework is systematically partitioned into three integral components: declarations for visual and natural language inputs, a detailed agent flow schema for Electronic Design Automation (EDA) tool operations, and constraint formulations for prompt scheduling.\nMode Declaration: Three-tier hardware representation The hardware visual description is divided into three tiers: gate-level, algorithm-level, and function block-level, ranging from concrete to abstract. Gate-level depicts the image as logic gates. Algorithm-level encompasses the basic blocks in the image, which are the familiar algorithms in large models, such as add and multiply blocks. For larger and customized designs, VLMQL employs the function block level to represent these elements, indicating that these blocks require further illustration. These levels are declared as the parameter as vlmql.set_mode(\"func_block\"), which is the prompt of Module function description. The declaration establishes a flexible guideline for users to create the diagram, which is then converted into a natural language prompt, like \"The basic block of the input image is a logic gate.\".\nLarge Model Declaration: Model Parameters and Their Inputs. For large models, declarations specify the task to be taken as input and then generate a prompt or select the function inputs.\n\u2022 Model Selection: To choose the target vision model and large language model, VLMQL uses vlmql.lvm(\"[vision model]\") and vlmql.llm(\"[language model]\") to choose the target model."}, {"title": "4 Evaluation", "content": "Our study systematically explores the efficacy of multi-modal language models in generating Verilog code by establishing a comprehensive benchmark that spans a spectrum of complexity across three categories: arithmetic, digital circuit, and advanced hardware designs. The benchmark is meticulously structured to assess the incremental benefits of multi-modal inputs as we progress from simple to more complex cases. To evaluate the performance, each model within the GPT-4 and LLaMA series is tasked with generating Verilog code for each category five times, facilitating a robust and iterative testing methodology.\nThe evaluation focuses on three critical aspects: syntax correctness, functional accuracy, and next-token accuracy, ensuring a holistic assessment of the Verilog code generated. The benchmark leverages a \"pass@5\" metric, which examines the best Verilog code within five attempts, providing insight into both the precision and reliability of the models."}, {"title": "4.2 Evaluation Result", "content": "The evaluation of multi-modal language models in the generation of Verilog code has yielded illuminating results, particularly in terms of syntax and functionality correctness.\nSyntax Correctness The introduction of vision representations has led to a notable enhancement in syntax correctness compared with natural-language-only representation. Specifically, as shown"}, {"title": "4.3 Sensitivity Study", "content": "Multi-Level Multi-Modal Prompt We benchmark the success rate on multi-level prompts from simple to complex as shown in Tab. 5. The results show that with the increase of the prompt information, the success rate has a further increase in most cases. Specifically, the prompts change from simple to detailed, while the success rate ranges from 40.63% to 71.81% in GPT4-V, from 9.38% to 25.88% in LLaVa. Therefore, the proposed multi-level prompt can distinguish the LLM-generating difference.\nFine-grain output Prompt In addition, we measure the output verilog program with the output metric as shown in Tab. 6, which compares the next token prediction success rate, reflecting the LLM's program completion ability. The results show that the natural-language-only mode is weaker than the natural language and image co-design mode. Specifically, compared to the natural-language-only mode, the average success rate of the co-design mode improves from 63.64% to 71.72% in GPT series, from 20.20% to 28.28% in LLaMa series. These results support our speculation that the co-design mode is better than the natural-language-only mode in the token prediction task.\nState Number Changes in FSM To further explore the LLM sensitivity to design complexity, we analyze several control modules with state number changing (i.e. push button LED) in Tab. 4 with GPT4-V as the base model, where the transition in the table denotes the state transition success rate, the state denotes the state register"}, {"title": "4.4 Ablation Study", "content": "To further reveal the difference between the natural-language model-based hardware generation and multi-modal model-based hardware generation, we compared the image-only mode, text-only mode, and mix input mode. As shown in Tab. 3, the results show that from the success rate perspective, the mix input mode is the best and the image-only mode is the worst. Specifically, the average success rate of image-only mode is 33.90%, and the average success rate of mix input mode is 71.81% in GPT4-V. Therefore, we recommend LLM for RTL generation with multi-modal model rather than the vision-only mode."}, {"title": "5 Conclusion", "content": "Our research underscores the significant potential of multi-modal large language models in Verilog generation. By integrating visual representations with natural language processing, we have achieved notable improvements in the generation of complex hardware designs. In addition, we propose a novel query language framework that enhances code quality and efficiency, and the comprehensive benchmark we established demonstrates a substantial increase in model performance. This approach not only advances hardware design methodologies but also provides a possible way for future research in generative AI applications within this field, marking a significant step towards more intuitive and efficient hardware design processes."}]}