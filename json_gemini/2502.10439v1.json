{"title": "Crypto Miner Attack: GPU Remote Code Execution Attacks", "authors": ["Ariel Szabo", "Uzy Hadad"], "abstract": "Remote Code Execution (RCE) exploits pose a significant threat to Al/ML systems, particularly in GPU-accelerated environments where the computational power of GPUs can be misused for malicious purposes. This paper focuses on RCE attacks leveraging deserialization vulnerabilities and custom layers, such as TensorFlow's Lambda layers, which are often overlooked due to the complexity of monitoring GPU workloads. These vulnerabilities enable attackers to execute arbitrary code, blending malicious activity seamlessly into expected model behavior and exploiting GPUs for unauthorized tasks such as cryptocurrency mining. Unlike traditional CPU-based attacks, the parallel processing nature of GPUs and their high resource utilization make runtime detection exceptionally challenging.\nIn this work, we provide a comprehensive examination of RCE exploits targeting GPUs, demonstrating an attack that utilizes these vulnerabilities to deploy a crypto miner on a GPU. We highlight the technical intricacies of such attacks, emphasize their potential for significant financial and computational costs, and propose strategies for mitigation. By shedding light on this underexplored attack vector, we aim to raise awareness and encourage the adoption of robust security measures in GPU-driven AI/ML systems, with emphasis on static and model scanning as an easier way to detect exploits.", "sections": [{"title": "1. Introduction", "content": "Security has always been a cornerstone of technology, protecting systems from unauthorized access, malicious actions, and unintentional damage. With the rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML), security has taken on new dimensions, becoming critical to ensure the safe and reliable operation of these systems. Al security matters because Al is not merely a set of algorithms; it is a transformative technology that influences decision-making in critical sectors such as healthcare, finance, transportation, and defense. Vulnerabilities in Al systems can lead to catastrophic consequences, from financial losses to compromised national security (Comiter., 2019).\nThe importance of Al security has grown in parallel with the rise of open-source Al frameworks and the commoditization of Al technologies. Open-source Al projects have democratized access to powerful tools and models, enabling innovation at an unprecedented scale. However, this openness has also introduced new risks. Publicly available Al models and tools can be exploited by malicious actors to embed vulnerabilities, manipulate behavior, or launch attacks. Additionally, the commoditization of Al - where Al systems are packaged as ready-to-use products\u2014has created a false sense of security, often overshadowing the need for robust safeguards.\nOne of the most pressing challenges in Al security is that it is frequently overlooked. Unlike traditional software systems, Al systems are inherently complex, relying on vast datasets and intricate models that are difficult to audit and secure. The non-deterministic nature of Al models further complicates the task, as small changes in input data or configurations can lead to unpredictable outcomes. Furthermore, the rapid pace of Al development often prioritizes performance and innovation over security, leaving vulnerabilities unaddressed. For example, companies racing to deploy Al solutions may lack the resources or expertise to thoroughly evaluate potential risks, resulting in systems that are both powerful and vulnerable.\nIn this new era of Al as a commodity, the consequences of overlooking security are far-reaching. Vulnerabilities in Al systems can undermine trust, compromise privacy, and even cause physical harm in safety-critical applications. For instance, adversarial attacks that deceive self-driving cars or tamper with medical diagnosis systems can have life-threatening implications. Additionally, as Al becomes integrated into global supply chains and critical infrastructure, its vulnerabilities can be exploited to disrupt entire industries or national economies.\nAddressing Al security requires a paradigm shift in how these systems are developed, deployed, and maintained. It calls for collaboration between researchers, industry practitioners, and policymakers to create a robust ecosystem that prioritizes security at every stage. By acknowledging the unique challenges of Al security and committing to proactive measures, we can harness the transformative power of Al while safeguarding against its potential risks.\nAmong the known attacks on Al models is the remote code execution attacks which provide a new set of challenges when it comes to GPU computation. Detecting GPU-based attacks on Al neural network models is difficult because GPUs execute tasks in parallel, making it harder to spot unusual activity. Malicious code can spread"}, {"title": "2. Related Work", "content": "Deserialization attacks, which became known in 2006 (Schoenefeld., 2006), exploit the process of reconstructing objects from serialized data, which, when mishandled, can lead to severe security exploits, including Remote Code Execution (RCE). Research on the topic gained popularity and in 2017 it climbed to eight place on OWASP Top 10 (Owasp, 2017; Schneider., 2020).\nA seminal study by Fingann (Fingann.,2020),provides an in-depth analysis of Java deserialization vulnerabilities. Building upon this, Gauthier and Bae (Gauthier et al., 2022) proposed a novel approach to prevent deserialization attacks at runtime. They introduced a lightweight method utilizing Markov chains to detect malicious deserialization behavior during execution.\nAdditionally, a study by Wanigathunga (Wanigathunga., 2021) on remote execution via insecure deserialization demonstrated how an intruder could execute arbitrary code on a remote machine by chaining techniques for uncontrolled file upload. This research underscores the critical nature of securing deserialization processes to prevent RCE attacks.\nThe integration of Al models into various applications has introduced new vectors for RCE attacks.\nNumerous studies have explored the security challenges associated with Al and ML. Previous works have highlighted vulnerabilities such as adversarial attacks (Goodfellow et al., 2015), poisoning attacks (Biggio et al., 2012 ), and backdoors in neural networks (Gu et al., 2017) and architectural backdoors (Bober-Irizar et al., 2022). Carlini et al. (2017) provided insights into adversarial examples and their impact on model robustness (Carlini et al., 2017), while Sayar (Sayar et al., 2022) shed light on deserialization vulnerabilities in large-scale software systems (Sayar, et al. 2022). A recent study, Liu (Liu et al., 2024), highlighted the potential for RCE vulnerabilities in applications that incorporate Large Language Models (LLMs). Frameworks like LangChain, which facilitate the development of LLM-integrated applications, offer code"}, {"title": "3. Background", "content": "3.1 Introduction to Al and ML\nArtificial Intelligence (AI) refers to the simulation of human intelligence by machines, while Machine Learning (ML) is a subset of Al that involves training algorithms to learn from data and make predictions or decisions. These systems rely on vast amounts of data and computational power to function effectively. For example, ML models are commonly used in applications like image recognition, language translation, and recommendation systems.\nAl and ML models function through intricate processes involving data ingestion, training, and inference. During training, models identify patterns in data, forming the basis for making predictions. For instance, a spam classifier might analyze millions of emails to differentiate between legitimate and spam messages based on learned features. However, the reliance on large-scale datasets introduces challenges, such as ensuring data integrity and securing the training environment from adversaries.\n3.2 Overview of Security in Al and ML\nSecurity in Al and ML involves protecting systems from unauthorized access, data manipulation, and malicious activities. The primary attack vectors include:\n\u2022 Architectural Backdoors: Malicious modifications introduced during model development and design.\n\u2022 Data Poisoning: Injecting harmful data into the training dataset.\n\u2022 Prediction Adversarial attacks: introducing maliciously designed data to deceive an already trained model into making errors.\n\u2022 Remote Code Execution (RCE): Exploiting software flaws to execute arbitrary code. RCE is mainly exploited using deserialization attacks - Leveraging unsafe serialization processes to execute malicious code.\n3.3 Remote Code Execution\nRemote code execution (RCE) occurs when an attacker exploits a vulnerability to run arbitrary code on a target system. In Al systems, RCE could target API endpoints or dependencies used by ML frameworks. For example, an outdated library with a known vulnerability could serve as an entry point for RCE attacks. In a cloud-based ML environment, RCE could allow attackers to access sensitive data, tamper with models, or disrupt operations.\n3.4 Vulnerabilities in models: Deserialization and Custom Layers\nThere are two main vulnerabilities that allow in RCE attacks in AI/ML models:\n1. Leveraging deserialization attacks, where malicious code is embedded within serialized model files or data, which is then executed upon deserialization.\n2. exploiting custom layers in deep learning frameworks, such as TensorFlow's Lambda layer, which allows arbitrary code to be executed as part of model computations.\nSerialized data is often used to store and transmit information efficiently. When deserialization is performed on untrusted input without proper validation, attackers can craft payloads that execute malicious code. This is particularly relevant in Al systems that rely on serialization for data exchange or model storage.\nFor example, a compromised Al application might include serialized models containing malicious commands. When the model is loaded, the application unknowingly executes the embedded payload. Modern AI/ML frameworks often include serialization tools like Pickle in Python, which are highly susceptible to such attacks if misused. Replacing unsafe serialization formats with secure alternatives such as \"safetensors\u201d or employing sandboxing techniques during deserialization are effective countermeasures.\nIn addition, many open-source machine learning (ML) and artificial intelligence (AI) frameworks, such as TensorFlow, provide flexible customization options to facilitate"}, {"title": "3.5 The Uniqueness of GPU Exploitation", "content": "Modern artificial intelligence (AI) models, particularly those based on deep learning, are designed to handle vast amounts of data and complex mathematical operations. These models often include architectures like neural networks and transformers, both of which rely on high computational power to function effectively. With the increasing scale and complexity of Al models, GPUs (Graphics Processing Units) have become a crucial part of Al workflows due to their parallel processing capabilities, making them well-suited for the demands of deep learning tasks.\nOne of the key challenges with securing GPU-based systems is the relative lack of monitoring compared to traditional CPU-based workloads. While CPUs are typically the focus of security tools and anomaly detection systems, GPUs are often overlooked or inadequately monitored. Most security tools and monitoring systems are designed to track CPU usage, looking for unusual activity such as spikes in resource consumption, unexpected code execution, or unauthorized access patterns. However, these tools are not always equipped to monitor the parallel processing power of GPUs.\nGPUs are a critical but costly resource for organizations relying on Al and high-performance computing. When GPU computations are compromised\u2014whether through unauthorized use, cryptojacking, or malicious attacks\u2014the financial impact can be substantial. Organizations may face increased costs due to wasted GPU cycles, reduced availability of resources for legitimate tasks, and the need for extensive remediation efforts, all of which can disrupt workflows and undermine operational efficiency.\nIn our work, we focus on remote code execution using deserialization and custom layers attacks on GPUs, which have not been as extensively studied. Specifically, the payload launches a cryptocurrency mining process, showcasing how attackers can exploit the computational power of GPUs for unauthorized resource utilization."}, {"title": "4. Analysis of Remote Code Execution on GPU Using Deserialization Attacks", "content": "This type of attack, known as cryptojacking(Lachtar et al., 2020), has been observed in cloud-based environments, where attackers gain unauthorized access to cloud instances and use the GPUs for their own profit to perform the hash calculations required for mining coins.\nThis section demonstrates an attack that utilizes deserialization vulnerabilities to run a crypto miner on GPU.\nThe exploit leverages Python's pickle.load (similar to PyTorch's torch.load) function, which is commonly used to deserialize saved model objects. The vulnerability arises when the function deserializes objects from an untrusted source without validating the input. Attackers can craft a serialized payload that includes malicious Python code, which is executed during deserialization.\nBelow is a Python code snippet illustrating how a pre-trained LLM model can be modified to include a malicious payload. When deserialized using pickle.load, the payload is executed:\n\u2022 Creating The Valid Model: Using Meta's Llama 3.3 multilingual large language model (LLM) pre-trained from HuggingFace as the valid base model.\n\u2022 Defining Malicious Behavior: The MaliciousCode class overrides the __reduce_ method, which controls how the object is serialized. This method is used to embed a system command (os.system) as the payload. This code specifically targeted Linux systems, downloading and executing XMRig (a high performance, open source, unified CPU/GPU miner) payload.\n\u2022 Crafting the Payload: Using a custom InjectablePickler object, the pickle.dump function is used to serialize the malicious object into a file (malicious_model.pickle).\n\u2022 Exploiting Deserialization: When pickle.load is called to load the serialized model, the_reduce_ method is triggered."}, {"title": "5. Analysis of Remote Code Execution on GPU Using Lambda Layer", "content": "This section, like the previous one, provides an example of how to create a crypto-miner GPU based remote code execution attack using vulnerabilities in the Tensoflow's Lambda layer.\nBelow is a Python code snippet illustrating how a Tensorflow's Keras model can be modified to include a malicious code inside a Lambda layer. When the model is used the code is executed:\n\u2022 Creating The Model With Malicious Behavior: Define a simple feedforward neural network using the Sequential API in Keras. In the Lambda layer we define a simple function ensuring the model continues to function properly by passing the input tensor through without modifications. This allows the malicious function to operate stealthily while still appearing as a valid layer in the model. The malicious code specifically targeted Linux systems, downloading and executing XMRig (a high performance, open source, unified CPU/GPU miner) payload.\n\u2022 Exploiting: When load_model is called to load the serialized model, the malicious code is triggered and also when the model is used to predict on new data the malicious method is triggered."}, {"title": "6. Challenges in Detecting GPU-based Attacks", "content": "In both examples, using Tensorflow's Lambda layer and using Pickle/Pytorch deserialization, the detection of GPU-based attacks on Al neural network models is particularly challenging due to the nature of GPU computations. When a neural network model is loaded from serialized data (e.g., a saved model file), it can be a vector for attackers to inject malicious code or manipulate model parameters without triggering obvious signs of intrusion. AI/ML Models are expected to use GPU-intensive computations, so a malicious code, offloading tasks from the CPU to the GPU would seem normal and expected making it challenging to distinguish malicious code from normal workloads. Since GPUs process tasks in parallel, the attack can spread across thousands of GPU cores, making it difficult to identify through traditional monitoring"}, {"title": "7. Remediation Strategies", "content": "Mitigating the risks of GPU-based Remote Code Execution (RCE) and deserialization attacks in AI/ML systems necessitates a comprehensive, layered approach that integrates prevention, monitoring, and response mechanisms. One of the most effective preventive measures is enforcing secure deserialization practices. By ensuring that all serialized models conform to strict validations and originate from trusted sources, organizations can reduce the risk of malicious payloads being injected into the system. The use of cryptographic signatures to verify the integrity and authenticity of serialized files further strengthens this approach, while automated scanning tools provide an additional safeguard by detecting detecting embedded malicious code or unexpected alterations.\nThe risks associated with custom computations, such as TensorFlow's Lambda layers, can be mitigated by minimizing their use in production environments and instead relying on pre-built, framework-supported operations. Where custom layers are unavoidable, thorough code audits are essential to identify and eliminate vulnerabilities. Additionally, organizations must adopt strict controls over object mapping during deserialization, explicitly defining safe mappings to avoid inadvertently executing unsafe code. Here, as well, automated scanning tools can provide an additional safety from malicious code.\nAnother critical component of remediation is the implementation of isolated execution environments to confine potential exploits. Containerization platforms, such as Docker, and GPU sandboxing techniques can effectively isolate GPU workloads, ensuring that malicious activities remain contained and do not compromise the broader system. Resource quotas should also be enforced to prevent exploitation of GPU resources for denial-of-service attacks or other malicious purposes.\nDependency and patch management play a crucial role in securing Al/ML frameworks. Regular updates to GPU drivers, libraries, and frameworks help address vulnerabilities present in older versions. Automated tools can assist in auditing dependencies, identifying outdated or insecure components, and streamlining remediation efforts. Additionally, reducing the overall attack surface by removing unnecessary libraries further enhances security."}, {"title": "8. Conclusion", "content": "Remote Code Execution (RCE) exploits leveraging deserialization vulnerabilities and custom Lambda layer attacks present significant challenges in the context of GPU-accelerated Al/ML workloads. The inherent complexity of GPU monitoring, coupled with the seamless integration of malicious code into expected computational workflows, makes these vulnerabilities particularly insidious. Unlike traditional CPU-centric attacks, where runtime monitoring can often detect anomalous behavior, the parallel and resource-intensive nature of GPU computations obscures signs of exploitation, allowing malicious operations to blend into legitimate workloads.\nThis underscores the critical importance of proactive security measures during the model development and deployment lifecycle. Static analysis techniques, such as"}]}