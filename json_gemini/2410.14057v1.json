{"title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs", "authors": ["Simone Conia", "Daniel Lee", "Min Li", "Umar Farooq Minhas", "Saloni Potdar", "Yunyao Li"], "abstract": "Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.", "sections": [{"title": "Introduction", "content": "The emergence of multilingual large language models (LLMs) and the wide availability of massive multilingual datasets have significantly advanced the field of Machine Translation (MT). These developments have led to MT systems that not only perform exceptionally well in high-resource languages but also support a growing number of low-resource languages (Fan et al., 2021; Tang et al., 2021; Costa-juss\u00e0 et al., 2022; Kudugunta et al., 2023, inter alia). Nevertheless, the research community still faces several unresolved challenges in MT. Among these, the translation of text that contains entities is still a hard task, especially with some categories of entities, e.g., movies, books, food, locations, and sometimes even people, to name a few. Indeed, word-for-word, or literal, translations of their names may not be suitable due to cultural-specific references, which can vary depending on social, geographical, historical, and political contexts, among other factors (Hershcovich et al., 2022). Therefore, the challenge lies in accurately identifying when and how to translate entities whose names are significantly different across languages. This step is crucial, as relying on literal translations may not convey the intended meaning, risking the effectiveness of the entire translation process (Gaballo, 2012; D\u00edaz-Mill\u00f3n and Olvera-Lobo, 2023). For example, if we were to translate word-for-word \u201cQual \u00e8 la trama de Il Giovane Holden?\" from Italian to English, we would obtain \"What is the plot of The Young Holden?\u201d, which is grammatically correct but semantically incorrect. The correct translation \"What is the plot of The Catcher in the Rye?\u201d requires not only fluency in both the source and target languages but also knowledge of the cultural contexts involved.\nIn this paper, we address the problem of cross-cultural translation on two fronts: resources and methods. More specifically, our contributions can be summarized as follows:\n\u2022 We introduce XC-Translate, the first large-scale, manually-created benchmark for cross-cultural translation across 10 language pairs of text containing entity names;"}, {"title": "Related Work", "content": "MT is a long-standing research topic in NLP. In this section, we briefly review the literature on recent advancements in MT, with a focus on studies that investigate entity names in relation to MT.\nMachine Translation. The field of MT has made a significant step forward with the emergence of multilingual language models, such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and massive multilingual corpora, such as OSCAR (Ortiz Su\u00e1rez et al., 2019) and MADLAD-400 (Kudugunta et al., 2023). Not only have these developments led to robust bilingual MT systems, such as OPUS-MT (Tiedemann et al., 2023), but also to multilingual MT systems that can translate to and from multiple languages with a single model, such as mBART-50 (Liu et al., 2020), M2M-100 (Fan et al., 2021), and NLLB-200 (Costa-juss\u00e0 et al., 2022). Therefore, we build KG-MT on top of these multilingual MT systems \u2013 which are openly available and widely used in the research community \u2013 while also comparing our results with state-of-the-art LLMs, such as GPT-3.5 and GPT-4, which have been shown to achieve competitive performance in general-purpose MT evaluations (Wang et al., 2023).\nExternal Knowledge in Machine Translation. Previous studies have already introduced methods to improve MT system via retrieval-augmentation or constrained-generation (Zhang et al., 2018; Bulte and Tezcan, 2019; Campolungo et al., 2022; Iyer et al., 2023). Notably, Zhang et al. (2018) proposed retrieval-augmentation to improve the translation of low-frequency words at inference time, while Bulte and Tezcan (2019) demonstrated the benefits of retrieving fuzzy matches to augment a dataset at training time. More recently, Campolungo et al. (2022) and Iyer et al. (2023) investigated the use of lexical constraints derived from external knowledge sources, e.g., dictionaries like WordNet, to improve the translation of senses in the long tail of the distribution. Although guiding or constraining the translation process has been shown to be an effective direction towards improving the translation quality of MT systems, the area at the intersection of retrieval-augmented generation and retrieval from large knowledge sources with millions of elements, such as Wikidata, is still understudied, to the best of our knowledge.\nEntity names in Machine Translation. Earlier investigations have long recognized and begun to address the challenges associated with translating texts that contain entity names (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002a,b). However, there are three important aspects that have yet to be fully explored in the literature. First, the focus has predominantly been on the transliteration of entity names, i.e., adapting an entity name from the script of one language to another (Sadamitsu et al., 2016; Ugawa et al., 2018; Zeng et al., 2023). Although transliteration is crucial for languages with different scripts, like English and Chinese, it does not necessarily account for the transcreation of entity names between languages using the same script, like English and Italian (Gaballo, 2012; D\u00edaz-Mill\u00f3n and Olvera-Lobo, 2023). Second, the depth of existing investigations has been constrained significantly by the absence of large-scale and high-quality benchmarks designed to highlight the challenges of cross-cultural translation (Zeng et al., 2023). Lastly, current approaches have mainly relied on training MT models by synthetically augmenting the training datasets to cover more entity names (Liu et al., 2021; Hu et al., 2022; S\u00e4lev\u00e4 and Lignos, 2022). However, data augmentation strategies, despite their effectiveness, often lead to a substantial increase of the training dataset size and the computational resources needed for training, especially when the entities to cover are in the millions. Furthermore, they also require fre-"}, {"title": "Enhancing Machine Translation using Multilingual Knowledge Graphs", "content": "In contrast with augmentation strategies based on synthetic data that aim to maximize entity coverage at model training time, our hypothesis is that MT systems do not need to memorize every possible entity name transliteration and transcreation for each source-target language pair to correctly translate a text that contains entities. Instead, the core idea that motivates our work is to leverage an external knowledge source to first retrieve the most relevant entities for an input text, and then generate the translation by incorporating the retrieved entity names in the target language. In fact, multilingual knowledge graphs, such as DBPedia (Auer et al., 2007), BabelNet (Navigli and Ponzetto, 2012; Navigli et al., 2021), and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), provide a wealth of lexical and factual knowledge about millions of entities in many languages, including their names, aliases, and descriptions (Kaffee et al., 2023; Conia et al., 2023). Not only that, but such knowledge is also easier to edit and is frequently updated to reflect the latest changes in the real world. By leveraging a multilingual knowledge graph, the focus of our approach shifts from memorizing entity names to learning when and how to retrieve the most relevant entities for a given input text and integrate their names in the target language in an end-to-end fashion.\nOur method, KG-MT, features two main components: (i) a knowledge retriever, which retrieves"}, {"title": "Retrieving Relevant Entities from Multilingual Knowledge Graphs", "content": "Given a source text $t = (w_1, ..., w_n)$ in a source language $l_s$, the objective of our knowledge retriever is to retrieve the top-k most relevant entities $E_t = \\{e_1, ..., e_k\\}$ from a knowledge graph $G = E \\times R \\times E$, where $E$ is the set of entities and $R$ is the set of relations in the multilingual knowledge graph.\u00b9 We represent each entity $e_i$ as a tuple $e_i = (n_i, d_i)$, where $n_i$ is the primary name of the entity and $d_i$ is its description. Including the description of an entity allows us to distinguish between homonyms, i.e., entities with the same name.\nWe define the relevance score $s(e_i, t)$ of an entity $e_i$ with respect to the source text $t$ as the cosine similarity between the entity and the source text:\ns(e_i, t) = \\frac{e_i \\cdot t}{||e_i||||t||}, (1)\nwhere $e_i$ is the embedding of the entity $e_i$. We then retrieve the top-k most relevant entities for the source text $t$ as follows:\nE_t = topk (\\{e_i \\in E | s(e_i, t)\\}). (2)\nThe embedding $e_i$ of an entity $e_i$ is obtained from an encoder (Izacard et al., 2021), which we train contrastively to maximize the likelihood of retrieving a relevant entity $e^+$ and minimize the likelihood of retrieving an irrelevant entity $e^\u2212$ given $t$ as the input query:\nL = log \\frac{exp(e^+\\cdot t)}{exp(e^+\\cdot t) + \\sum_{i=1}^n exp(e_i^-\\cdot t)} (3)\nwhere $e^+$ is the embedding of a relevant entity $e^+$ and $e^\u2212$ is the embedding of an irrelevant entity $e^\u2212$. Importantly, we also introduce a sampling strategy"}, {"title": "Integrating Explicit Knowledge into a Machine Translation Model", "content": "Given the source text $t = (w_1,..., w_n)$ in a source language $l_s$, and the entities $E_t$ retrieved by our knowledge retriever, the objective of our knowledge-enhanced translator is to generate the target text $t' = <w'_1, ..., w'_m>$ in a target language $l_t$ by incorporating the entity names of $E_t$ into the translation process. Therefore, instead of directly generating the target text $t'$ from the source text $t$, we first build a knowledge-enhanced source text $t^{+KG}$ as follows:\nt^{+KG} = (w_1, ..., w_n,\n ..., [KG], n_1 \\rightarrow n'_1, ..., n_k \\rightarrow n'_k), (4)\nwhere [KG] is a special token that indicates the start of the entity name translations, $n_i$ is the name of the entity $e_i$ in the source language $l_s$ and $n'_i$ is the name of the entity $e_i$ in the target language $l_t$, as provided by the multilingual knowledge graph. We then feed the knowledge-enhanced source text $t^{+KG}$ to a standard sequence-to-sequence MT model to generate the target text $t'$, in a similar vein to past work on guiding MT systems (Zhang et al., 2018; Bulte and Tezcan, 2019). Given the format of $t^{+KG}$, the MT model is fine-tuned to learn how to generate the target text $t'$ by also attending to the translation of the entity names $n'_i$. We refer to this method as explicit knowledge integration, as the translations of the relevant entities are explicitly provided in the input to the MT model."}, {"title": "Integrating Implicit Knowledge into a Machine Translation Model", "content": "Although the knowledge-enhanced translator can generate the target text $t'$ by incorporating the entity names of $E_t$, it does not take advantage of the representations of the retrieved entities $e_i$ learned by the knowledge retriever. To overcome this limitation, we also propose a method to fuse the latent representations of the knowledge retriever and the knowledge-enhanced translator, which allows KG-MT to better model the interconnections between the retrieved entities and the generated translation. Here, we assume that an MT model is structured as an encoder-decoder architecture, such as the"}, {"title": "Evaluating Cross-Cultural Translation of Texts Containing Entity Names", "content": "To evaluate the effectiveness of our method, we introduce Cross-Culture Translate (XC-Translate), the first large-scale, manually-curated benchmark for the task of cross-cultural translation of texts containing entity names. XC-Translate is composed of parallel texts in 10 English-to-X language pairs from a diverse set of languages, including both high-resource and low-resource languages, namely, Arabic, Chinese, French, German, Italian, Japanese, Korean, Spanish, Thai, and Turkish. We highlight that this design choice allows our benchmark to feature languages with diverse scripts, some of which are similar to English, such as French and Spanish, and others that are very different, such as Arabic, Chinese, and Thai. Importantly, our benchmark is:\n\u2022 Challenging: XC-Translate is the first benchmark to focus on cross-cultural translation of texts containing entity names, which is particularly challenging due to the cultural-specific references of entity names across languages;"}, {"title": "Design Principles", "content": "The creation process of XC-Translate is mainly driven by two design principles: (i) the texts should contain entity names that are likely to be affected not only by transliteration between languages, and (ii) the heart of the challenge should be the translation of the entity names, rather than the translation of the rest of the text, i.e., the text should not be too complex to translate if the entity names are translated correctly.\nTo satisfy the first design principle, we first identify for each language pair a set of entities from Wikidata that adhere to the following two main criteria: (a) the entity has at least one name in English and one name in the target language, (b) the English name of the entity is at least 50% different from the names in French, German, Italian, Spanish, and their word-for-word translation to English, as measured by the Levenshtein distance. The rationale behind the second criterion is that such a difference in the entity names across languages that mostly share the same script is likely an indicator of a name dissimilarity that goes beyond transliteration. For example, the Italian name of the entity \"The Catcher in the Rye\u201d is \u201cIl Giovane Holden\u201d, while its French name is \u201cL'Attrape-c\u0153urs\u201d.\nTo satisfy the second design principle, we ask a group of human annotators to curate a set of short knowledge-seeking questions \u2013 less than 25 words in English about the identified entities. Requiring"}, {"title": "Translation Process", "content": "Having identified the entities of interest for each language pair and having created English questions about them, we produce the translations in each target language via a two-step process. First, we ask a group of human translators to translate the questions from English to the target language. Then, we ask a second group of human annotators to verify the correctness of the translations.3\nThe entire process is guided by a set of instructions and guidelines that we provide to the annotators. Moreover, we require the annotators to be fluent in English, native speakers of the target language, and resident in a country where the target language is spoken. Before starting the translation process, we also require the annotators to pass an entrance test to further verify their language proficiency and their comprehension of the instructions and guidelines; otherwise, they are not allowed to participate in the annotation task. Finally, the annotators are periodically evaluated on a set of test questions: if they fail on them, they are excluded from the pool of annotators. Since each English question is formulated from a given entity, we can aid the translators by providing the entity name(s) from Wikidata in the target language as a hint (see Design Principle i.a in Section 4.1), the English and target language descriptions of the entity from Wikidata, and the English and target language Wikipedia pages of the entity, which are fundamental resources to understand the context and background of the entity of interest.\nAt the end of the process, each English question is translated into the target language by at least three different translators, and each translation is then verified by at least three different annotators, allowing us to retain only the translations that are"}, {"title": "Evaluation Metrics", "content": "It is well known that the evaluation of MT systems is challenging, as there is no single metric that can capture all the aspects of translation quality. For example, BLEU (Papineni et al., 2002) is a popular metric that measures the n-gram overlap between the generated translation and the reference translations, but it is long known not to correlate strongly with human judgments (Callison-Burch et al., 2006). More recently, the research community has proposed alternative metrics, such as BERTScore (Zhang et al., 2020) and COMET (Rei et al., 2020), that aim to capture more nuanced aspects of translation quality, such as semantic similarity and factual correctness. However, such learned metrics yield only a translation-level score, which is not easy to interpret (Perrella et al., 2024) and does not allow us to easily analyze the translation at the entity level.\nTo address the foregoing limitations, not only do we provide the translations of the questions in XC-Translate, but also the list of the valid translations of the entity names that are valid in the context of the considered text. Having a comprehensive list of manually-curated valid names allows us to introduce M-ETA (Manual Entity Translation Accuracy), a simple metric to easily measure the translation quality at the entity level. Differently from previuos metrics that rely on automatically identifying and aligning entities (Hu et al., 2022), M-ETA directly checks whether an automatic translation contains one of the manually-curated names. More formally, given a translation t' in a target language lt and a set of gold entities \u00cat, we define the entity-level translation quality score Q(t', \u00cat') as follows:\nQ(t', Et') = \\frac{1}{|Et|} \\sum_{e_i\\in E_t} \\sum_{n \\in N_{e_i}} q(t', e_i), (6)\nwhere q(t', ei) is the entity-level translation quality score of the entity ei in the target text t' and is defined as follows:\nq(t', e_i) = min\\{1, \\sum_{n\\in N_{e_i}} \u2161(n\\in t')\\}, (7)\nwhere $N_{e_i}^{l_t}$ is the set of manually-curated names of the entity ei in the target language $l_t$ and I(n \u2208 t') is an indicator function that is equal to 1 if the"}, {"title": "Experiments and Results", "content": "In this section, we first list the systems we consider in our main experiments, then describe the datasets used to train the MT systems, and finally report and discuss the results.\nSystems. We compare the following systems:\n\u2022 GPT-3, GPT-3.5, and GPT-4:4 among the most popular and best performing LLMs, which have shown strong translation performance in the literature;\n\u2022 mBART-50, M2M-100, and NLLB-200: recent multilingual MT models that support translation from and to about 50, 100, and 200 different languages using a single model, respectively;\n\u2022 KG-MT: our proposed approach, which leverages the information available in multilingual knowledge graphs for end-to-end retrieval-augmented translation.\nTo ensure a fair comparison, we fine-tune mBART-50, M2M-100, NLLB-200, and KG-MT using the same dataset. For more details about our experimental setup, please refer to the Appendix.\nDatasets. As mentioned in Sections 3.1 and 3.2, KG-MT requires two datasets for training: one for the knowledge retriever and one for the knowledge-enhanced translator. The training dataset for retrieval should contain instances of the form (t, e+, e\u2212), where t is a source text, e+ is a relevant entity for t, and e\u2212 is an irrelevant entity for t. The training dataset for translation should contain instances of the form (t, t', \u00cat), where t is a source text, t' is a target text, and \u00cat is the set of gold entities for t. To train the knowledge retriever, we use the training data from Mintaka (Sen et al., 2022), a recently proposed dataset for multilingual question answering in which each question is tagged with the entities that appear therein. Since the questions in Mintaka are manually translated, we can also use its gold translations to train the knowledge-enhanced translator."}, {"title": "Analysis and Discussion", "content": "In this section, we analyze KG-MT, and discuss where we believe it may be improved in future work. We expand on this analysis in the Appendix.\nExplicit or implicit knowledge? Sections 3.2 and 3.3 introduce two methods to integrate the knowledge retrieved by the knowledge retriever into the knowledge-enhanced translator: explicit"}, {"title": "Integrating Implicit Knowledge", "content": "of the translation, independently of the semantics represented within the entity embeddings.\nKnowledge retrieval. The knowledge retriever plays a fundamental role in KG-MT. If the retrieval step collects wrong or unrelated entities, the knowledge-enhanced translator has to (i) be robust against noisy or irrelevant knowledge, and (ii) fall back on its parametric memory, which is often unreliable as shown by the results of the vanilla MT systems on XC-Translate in Tables 1 and 2. However, our analysis shows that our knowledge retriever achieves 85.9% and 92.1% hits@1 and hits@3, respectively, on XC-Translate, which suggests that the knowledge retriever is effective at retrieving relevant entities. Part of this success can be attributed to our fine-tuning strategy with hard negative mining, which allows the knowledge retriever to improve the hits@1 and hits@3by 5.6% and 4.2%, respectively, compared to using mContriever (Izacard et al., 2021), a pretrained retriever. Given the good performance with hits@3, we set the knowledge retriever to retrieve at most three entities for each source text, which is a trade-off between retrieving more relevant entities and increasing the computational cost.\nGold knowledge. If knowledge retrieval is not a significant source of errors for KG-MT, then the knowledge integration step, in which the knowledge-enhanced translator has to learn how to effectively integrate the retrieved entities into the translation process, is likely to be the main bottleneck. To isolate the performance of the knowledge-enhanced translator from the performance of the knowledge retriever, we evaluate KG-MT when us-"}, {"title": "Conclusion and Future Work", "content": "In this paper, we addressed the problem of cross-cultural translation of texts containing entity names. Our contributions are threefold: (i) we introduced KG-MT, a novel approach for retrieval-augmented translation that leverages the information available in multilingual knowledge graphs to improve the translation of texts containing entity names; (ii) we introduced XC-Translate, the first large-scale, manually-curated benchmark for the task of cross-cultural translation of texts containing entity names; and (iii) we conducted extensive experiments on XC-Translate and other existing benchmarks for MT, showing that KG-MT significantly outperforms the state of the art on XC-Translate while maintaining comparable results on general-purpose MT benchmarks. We believe that our contribution will encourage further research on the problems that arise when translating texts containing cultural references beyond entity names, such as idioms and metaphors, and that retrieval-augmented translation will be a valuable tool to address these challenges."}, {"title": "Limitations", "content": "In this section, we discuss some of the main limitations of our work and how future research may be able to address them.\nLanguage coverage. XC-Translate contains a diverse set of languages, but it is clearly not exhaustive. Although the number of languages included in our benchmark is comparable with the number of languages studied every year in the WMT shared tasks, it is still a small fraction of the world's languages. While full coverage is likely infeasible, we still miss entire linguistic families, such as the Uralic, Dravidian, and Niger-Congo families, and many languages from the Indo-European family, such as Russian, Portuguese, and Hindi. Future work should consider expanding the coverage of XC-Translate to include more languages. Indeed, different languages may present different challenges for cross-cultural translation, and it is important to understand these differences to develop more robust and generalizable translation systems. Not only that, but another aspect that we do not consider in our work is the dialectal and regional variation within a language, which can also be a significant source of errors in translation. Our intuition is that, since current state-of-the-art MT systems and LLMs struggle in our setting, which mostly includes high- to medium-resource languages, they would struggle even more in low-resource languages and dialects.\nEntity coverage and selection. The entities in XC-Translate are selected from Wikidata, which is a large and diverse knowledge graph, but it is not complete. While our design principles (see Section 4.1) are aimed at selecting entities that are likely to be challenging for MT systems, our selection strategy can also be considere aggressive for several reasons: (i) we only consider entities that are linked to Wikipedia pages, which may exclude many entities that are relevant in a given culture; (ii) we only consider entities that have at least an English name, which may exclude many entities that are relevant in a given culture; and (iii) our selection strategy is based on our experience with using Wikidata and Wikipedia. Future work should consider more sophisticated strategies for selecting entities, such as considering language pairs that do not involve English and tuning the selection based on each language pair, rather than having a one-size-fits-all approach. Moreover, fu-"}, {"title": "Translations quality.", "content": "The evaluation of KG-MT is mostly based on the M-ETA metric, which is a simple metric that measures the translation quality at the entity level. While M-ETA is a useful metric to evaluate the performance of KG-MT, it is not a comprehensive metric to evaluate the translation quality of MT systems, i.e., it cannot be used alone to compare the performance different systems. This is the reason why we also report the BLEU and COMET scores of KG-MT on XC-Translate and the WMT benchmarks. However, we acknowledge that BLEU and COMET are also not comprehensive metrics to evaluate the translation quality of MT systems. Future work may consider fine-tuning learned metrics on our XC-Translate annotations, which also include a list of manually-curated valid translations of the entity names that are valid in the context of the considered text.\nKnowledge retrieval. Our knowledge retriever is based on a retrieval model that retrieves at most three entities for each source text. While this design choice is based on a trade-off between retrieving more relevant entities and increasing the computational cost, it is not clear whether this is the best design choice when using KG-MT on other types of texts, e.g., long documents where the number of entities may be significantly higher.\nComparison systems. Our comparison systems are based on the state of the art in MT and LLMs, but they are not necessarily the best systems for the task of cross-cultural translation. We use mBART-50, M2M-100, and NLLB-200 as the backbone for the knowledge-enhanced translator in KG-MT, as they are widely used, have shown strong performance in the literature, and are available for fine-tuning. Another advantage is that they are also multilingual, which allows us to use the same model for all the language pairs in XC-Translate. Moreover, we mainly considered the GPT family of LLMs, as they are among the most popular and best performing LLMs, having also shown strong translation performance in the literature. However, future work may consider using openly-available LLMs. In this work, we have focused on the retrieval-augmented translation approach for MT systems, but future work may consider similar approaches for openly available LLMs, such as LLama and Mistral."}, {"title": "Creating XC-Translate", "content": "In this section, we provide in-depth details on the creation process of XC-Translate, our novel dataset for evaluating cross-cultural translation of texts containing entity names."}, {"title": "Choice of languages", "content": "As mentioned in Section 4, 10 diverse languages are selected from a set of typologically-different linguistic families:\n\u2022 West Germanic: German;\n\u2022 Romance: Spanish, French, Italian;\n\u2022 Semitic: Arabic;\n\u2022 Sino-Tibetan: Chinese (simplified);\n\u2022 Altaic: Turkish;\n\u2022 Koreanic: Korean;\n\u2022 Japonic: Japanese.\n\u2022 Tai: Thai.\nThe architectural decision renders XC-Translate a complex challenge, given the variability or consistency in the symbol sets across different languages. For instance, the spelling of a person's name might remain unchanged between English and French, yet it's highly improbable for it to be identical in English and Chinese, necessitating at least a transliteration. Furthermore, the act of transliterating between English and Korean (as well as other languages, like Japanese) is fraught with unpredictability, complicating the reliance on rule-based methods for name translation between these linguistically divergent languages. Our investigation prioritized languages classified as high/medium-resource, following the quantitative evaluation in Conia et al. (2023) indicates that the representation of textual data is substantially lacking, even for the most recognized entities (top-10%) within those high/medium-resource languages. The extension of our benchmark to encompass lower-resource languages is earmarked for subsequent endeavors."}, {"title": "Translating text from English to the Target Language.", "content": "First, given the entity, the human annotators were asked to familiarize themselves with its information. Through the user interface (IU) the following details were provided: (i) entity names/aliases, (ii) short description for the given entity retrieved from Wikidata, and (iii) a built-in panel which displayed the Wikipedia article in the English and target locale, if available. By imbedding the relevant details in the UI, annotators were able to familiarize themselves with the entity without leaving the created tool. This is show in Figure 4.\nAfter learning about the entity, the annotators were tasked with understanding the task with a set of in-depth instructions in a separate guideline. The guidelines provided information about (i) task terminology, (2) detailed information on translating text, (3) tips and edge-cases on translating text, (4) positive and negative examples of the translation task. The guidelines will be provided in the supplementary material."}, {"title": "Verifying human-curated translations.", "content": "Using the data collected from the first task, a second group of human annoators were tasked with verifying the correctness of the translation.\nSimilar to the previous task, human annotators were provided details within the task UI and on a separate guidelines, information to familiarize themselves with the task entity, and task instructions as show in Figure 6)\nAfter, the human annotators were tasked with verifying the translation in the target language as show in Figure 7. To do so, they were provided the (i) English entity name and question and (ii) target language entity name and question). With this information, they answered two questions, with their corresponding options:\nPart A Is the Entity Name translated correctly?\n\u2022 Yes The Entity Name was translated correctly. Meaning, the translated entity name can be used to refer to the English entity. If you read the English and Translated Entity Name separately, you WOULD KNOW they refer to the same entity.\n\u2022 No Meaning, the translated entity name can"}, {"title": "Is the Entity Name translated correctly?", "content": "be used to refer to the English entity. If you read the English andTranslated Entity Name separately, you WOULD NOT KNOW they refer to the same entity.\nPart B Does the English Question and the Translated Question have the same meaning?\n\u2022 Yes The English Question and the Translated Question DO have the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD understand the same thing.\n\u2022 No The English Question and the Translated Question DO NOT have the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD NOT understand the samething.\n\u2022 Maybe The English Question and the Translated Question MAYBE HAVE the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD LIKELY understand thesame thing. But, it could be interpreted differently.\nThe phrasing of the question in Part B was fine-tuned, to ensure annotators did not index on translit-"}, {"title": "BX-Translate: Examples", "content": "In this section, we provide examples of the instances in XC-Translate and their translations.\n\u2022 English: \"Who is the author of the science fiction mystery-thriller novel called The Peripheral?\"\n\u2022 Italian: \"Chi \u00e8 l'autore del romanzo giallo-thriller di fantascienza chiamato Inverso?\"\n\u2022 English: \"How many seasons of Sweet Magnolias are available on Netflix?\""}, {"title": "Experimental Setup", "content": "In this section, we provide in-depth details on the experimental setup of our experiments, including the training of the knowledge retriever, the training of the knowledge-enhanced translator, and the evaluation metrics, as well as the training details of the baselines."}, {"title": "Hardware Infrastructure", "content": "The experiments were conducted on a server with a single NVIDIA V100 GPU, 32GB of RAM, and an 32-core CPU. The server runs Ubuntu 20.04 LTS and is equipped with CUDA 12.\nTraining times. The training of the knowledge retriever took approximately 4 hours to converge, while the training of the knowledge-enhanced translator took approximately 6 hours to converge, depending on the underlying MT model, with M2M-100 being the fastest and mBART-50 being the slowest. This makes our approach feasible for training on a single GPU and for short training times contrary to synthetic data augmentation approaches that usually require multiple GPUs and/or long training times."}, {"title": "Training of the Knowledge Retriever", "content": "The knowledge retriever is trained using the training data from Mintaka, a recently proposed dataset for multilingual question answering in which a subset of the questions are tagged with the entities that appear therein. The training data for the knowledge retriever contains instances of the form (t, e+, e\u2212), where t is a source text, e+ is a relevant entity for t, and e is an irrelevant entity for t, mined from Wikidata using the hard negative mining strategy outlined in Section 3.1.\\"}]}