{"title": "A New Approach to Solving SMAC Task: Generating Decision Tree Code from Large Language Models", "authors": ["Yue Deng", "Weiyu Ma", "Yuxin Fan", "Yin Zhang", "Haifeng Zhang", "Jian Zhao"], "abstract": "StarCraft Multi-Agent Challenge (SMAC) is one of the most commonly used experimental environments in multi-agent reinforcement learning (MARL), where the specific task is to control a set number of allied units to defeat enemy forces. Traditional MARL algorithms often require interacting with the environment for up to 1 million steps to train a model, and the resulting policies are typically non-interpretable with weak transferability. In this paper, we propose a novel approach to solving SMAC tasks called LLM-SMAC. In our framework, agents leverage large language models (LLMs) to generate decision tree code by providing task descriptions. The model is further self-reflection using feedback from the rewards provided by the environment. We conduct experiments in the SMAC and demonstrate that our method can produce high-quality, interpretable decision trees with minimal environmental exploration. Moreover, these models exhibit strong transferability, successfully applying to similar SMAC environments without modification. We believe this approach offers a new direction for solving decision-making tasks in the future.", "sections": [{"title": "Introduction", "content": "The StarCraft Multi-Agent Challenge (SMAC)(Samvelyan et al., 2019) has emerged as a widely adopted benchmark for the evaluation of multi-agent reinforcement learning (MARL) algorithms. Built upon the real-time strategy game StarCraft II, SMAC provides a highly challenging environment in which each agent controls a distinct unit on the battlefield. Agents must operate in partially observable and dynamic environments, where effective coordination is crucial to achieving common objectives. With access only to local information, agents are required to collaborate while contending with adversarial units governed by built-in AI opponents. The environment offers a diverse set of tasks, ranging from small-scale skirmishes to large-scale, complex battles, making it an ideal testbed for assessing agent cooperation, coordination, and competitive behavior in MARL settings. Consequently, SMAC has become a standard benchmark for evaluating the performance of cutting-edge algorithms, including VDN(Sunehag et al., 2017), QMIX(Rashid et al., 2018), QPLEX(Wang et al., 2021a), and MAPPO(Yu et al., 2022), which have shown their effectiveness in this domain.\nDespite the impressive performance of MARL in solving SMAC tasks, it has several notable drawbacks and challenges:\n\u2022 Low exploration efficiency: MARL requires extensive interaction with the environment to learn an optimal policy, which often demands significant training time and computational resources. In complex environments, an agent may need to perform millions of interactions to acquire an effective strategy.\n\u2022 Lack of model interpretability: Since deep learning relies on complex neural networks, MARL models are often considered black boxes, making it difficult to explain the decision-making process. In certain applications, such as autonomous driving or healthcare, this lack of interpretability can raise concerns about safety or regulatory compliance.\n\u2022 Poor policy transferability: MARL models are typically trained in specific environments, and they struggle to generalize or be directly applied to different environments. The transferability of policies is weak, requiring retraining or significant adjustments to adapt to new tasks or environments.\nTraditional rule-based decision tree models can indeed overcome the aforementioned issues, but building such models requires a significant amount of human expertise and prior knowledge, which leads to high labor costs. For a long time, there has been no effective method to integrate the strengths of both approaches.\nWith the advent of Large Language Models (LLMs)(Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023; Bai et al., 2023; Du et al., 2022), especially code-oriented LLMs(Guo et al., 2024; Hui et al., 2024), the ability of models to generate code has become increasingly robust, providing a novel approach for their integration. During pre-training, LLMs learn from vast amounts of human experience and knowledge, particularly in the domain of coding, laying a strong foundation for models to generate code that can solve complex tasks. Agents can take task descriptions as input, generate corresponding decision tree code, and iteratively improve the decision tree based on task feedback. In this paradigm, not only is the challenge of lacking a reward function during LLM training addressed, but it also overcomes the issue of the non-interpretability of policies generated by reinforcement learning.\nTaking the SMAC task, we propose a novel approach called LLM-SMAC. We are excited to report that LLM-SMAC, with minimal environmental exploration, produces a high-quality white-box decision tree model on the majority of SMAC maps. Moreover, this model exhibits strong transferability, as it can be applied to similar SMAC environments without any modifications. Through visualization, we observe that the agents trained with LLM-SMAC acquire the micro-skills required for SMAC, such as kite and focus fire. Moreover, these agents prioritize macro tactics and are less prone to exploiting specific loopholes in the game environment to achieve victory, resulting in higher strategy robustness.\nWe believe this presents a new perspective for addressing decision-making tasks. At the same time, we recognize that this is only the first step in this direction. Looking ahead, we plan to fine-tune the LLM using reward functions to further enhance the model's capability in generating decision tree code."}, {"title": "Related Work", "content": ""}, {"title": "StarCraft II Game AI", "content": "Research in artificial intelligence for StarCraft II has advanced significantly since DeepMind introduced the PySC2 learning environment in 2017 (Vinyals et al., 2017). This platform, together with Blizzard's release of anonymized game replays, provided a standardized interface for developing and testing Al agents. A landmark achievement occurred in 2019 with the introduction of AlphaStar (Vinyals et al., 2019), which reached Grandmaster status and defeated elite human competitors, showcasing the capabilities of reinforcement learning in intricate settings.\nFollowing the AlphaStar milestone, explorations diverged into multiple innovative pathways. Mini-AlphaStar (Liu et al., 2021b) was developed to streamline input variables while preserving performance. Concurrent studies, such as TG (Liu et al., 2021a) and HierNet-SC2 (Liu et al., 2022), investigated more efficient reinforcement learning strategies. AlphaStar Unplugged (Mathieu et al., 2021) marked a substantial progression in offline reinforcement learning utilizing human game replays. Further developments were made with TStarBotsX (Han et al., 2020) and SCC (Wang et al.,"}, {"title": "LLM Agents in Complex Game Environments", "content": "The advent of advanced language models like GPT-3.5 (Ouyang et al., 2022) has significantly propelled research on LLM agents forward, particularly in complex game environments. This research spans a wide range of game types, from classic board games to open-world video games, showcasing the versatility of LLM agents.\nIn board games, ChessGPT(Feng et al., 2023) demonstrated LLM agents' ability to understand and play strategic games. The social deduction game Werewolf presented unique challenges in strategic communication and decision-making, with recent studies (Jin et al., 2024; Xu et al., 2024) exploring LLM agents' capabilities in this complex, multi-agent environment.\nVideo game applications have been particularly diverse and impressive. The MineDojo environment (Fan et al., 2022) facilitated projects like GITM (Zhu et al., 2023) and Voyager (Wang et al., 2023) in Minecraft, demonstrating LLM agents' ability to navigate and perform tasks in complex 3D environments. Pok\u00e9LLMon (Hu et al., 2024) showcased their effectiveness in turn-based tactical games, emphasizing the importance of external knowledge retrieval and iterative policy refinement.\nThe Cradle framework (Tan et al., 2024) introduced a novel approach allowing LLM agents to interact with various software and games through a unified interface of screenshots and keyboard/mouse inputs. This demonstrated the potential for general-purpose game-playing agents across multiple commercial video games and software applications. CivRealm (Qi et al., 2024) presented a complex, imperfect-information general-sum game environment, challenging both learning and reasoning capabilities of AI agents."}, {"title": "Code as Action in LLM Agents", "content": "Recent research has explored the use of executable code as a unified action space for LLM agents, demonstrating significant potential in enhancing agent capabilities across various domains. This approach, often referred to as \"code as action\" or \"code as policies,\" has shown promising results in several key areas.\nThe CodeAct framework (Wang et al., 2024) proposed using executable Python code to consolidate LLM agents' actions. This approach outperformed traditional JSON or text-based action spaces, offering greater flexibility and the ability to compose multiple tools. The study demonstrated up to a 20% higher success rate compared to widely used alternatives.\nIn the realm of robotics, Chen et al. (Liang et al., 2023) introduced \"code as policies\" for robot control. In this approach, LLMs generate Python code to process perception outputs and parameterize control primitives. This method enabled robots to exhibit spatial-geometric reasoning, generalize to new instructions, and prescribe precise values based on context, demonstrating effectiveness across multiple robot platforms.\nThe Eureka algorithm (Ma et al., 2024) leveraged LLMs for human-level reward design in reinforcement learning tasks. By performing evolutionary optimization over reward code, Eureka generated reward functions that outperformed expert human-engineered rewards across a diverse suite of RL environments, including complex tasks like dexterous pen spinning.\nThe success of these \"code as action\" approaches suggests a promising direction for developing more capable and adaptable LLM agents, with potential applications in robotics, game AI, and other complex decision-making domains."}, {"title": "Method", "content": "This section introduces our LLM-SMAC framework for solving SMAC tasks via decision tree codes generated by LLMs. By leveraging the ability of reasoning and planning of LLM and the code generation ability of coder LLM, our framework successfully solves most of the SMAC micro-management tasks. Figure 1 illustrates the overview of our framework and we will now proceed to elaborate on its details."}, {"title": "Architecture", "content": "LLMs have acquired extensive knowledge about StarCraft II and learned python-sc2 and other SC2 scripts during their pretraining phase. Building upon this foundation, our framework addresses the code generation task for the python-sc2 package, which provides a comprehensive wrapper for StarCraft II gameplay. Unlike traditional MARL models on the pysc2 platform, the scripts from the python-sc2 package are open-loop control scripts that require the LLM to plan and anticipate game scenarios before execution. During the pretraining phase, LLMs have acquired extensive knowledge about StarCraft II and have learned python-sc2 and other SC2 scripts. The extensive workload of planning, code generation, bug identification, and bug fixing places significant strain on a single LLM. To address this, our framework divides the process into three critical modules: the strategy planning model, the decision-tree code generation model, and the summarization critic model.\nTo solve an SMAC task, unit and map information are analyzed and presented as an environment prompt for the planning LLM. Additionally, the current learned skill set and historical strategy traces are provided to the planning LLM to help generate a strategy skeleton. The planner then suggests skill options, describes each skill, and defines the conditions for using them. This decision-tree representation serves as the input for the LLM coder, which generates the corresponding Python scripts.\nGiven the environment prompt and strategy skeleton, the coder model generates a Python function that adheres to the requirements of python-sc2. Due to LLM hallucinations and the existence of multiple ways to implement a strategy skeleton, the coder iteratively generates and tests the code on SMAC maps. Based on the correctness and performance of the code, the SMAC simulation process provides stack traces of bugs or key results, such as win rates, scores, and damage statistics.\nThe critic module evaluates the generated code. It takes the rollout results and the script as input, with the critic LLM analyzing the reasons for high or low performance or identifying specific bugs. In addition, the critic provides suggestions for refining the strategy, as well as code corrections when"}, {"title": "Strategy planning", "content": "The planner provides the coder with an overall strategy and detailed descriptions of the corresponding skills. Due to the nature of open-loop control, the planner can only offer general, coarse-grained strategy suggestions without detailed information about the agent units or the maps. These coarse-grained strategies focus more on broad strategic considerations rather than micro-management. Therefore, the planner must refine these suggestions based on unit and map information. In our framework, unit details are pre-scraped from Liquipedia, where information about unit characteristics and usage can be automatically retrieved and incorporated into the environment prompt. Meanwhile, map information, including size, available operational areas, and terrain features, is also summarized and added to the prompt.\nTo leverage the in-context learning capability of LLMs, our framework stores historical skills and their corresponding rollout win rates as nodes and return values in an MCTS (Monte Carlo Tree Search) structure. The planner must not only plan based on the environmental prompt but also consider past planning decisions by either selecting a new strategy or adding a skill to an existing historical strategy.\n$ST = (s_0, s_1,...,s_n) = LLM_{planner}(I_{unit} + I_{map} + h(s_i, r_i)),$"}, {"title": "Decision-tree Generation", "content": "Given the strategy generated by the planner, the coder module is tasked with producing the corresponding Python code using the python-sc2 package for testing. However, the performance of the generated code depends on several factors: the strategy provided, the interpretability of the LLM itself, and the quality of the training data. To enhance the strategy's performance and facilitate the translation of the strategy into code, the planner must not only provide the skills but also specify the conditions for invoking each skill through detailed prompts:\n$C = LLM_{coder}(ST, LLM_{critic}(C', G)),$"}, {"title": "Perfomance Analyzer", "content": "When exceptions occur in the generated Python code, the critic model is tasked with identifying the bugs and potential fixes, reducing the burden on the coder model. The critic analyzes the generated code C along with the traceback information to pinpoint the cause of the exception. Due to LLM hallucinations and API changes in the python-sc2 package, the coder model may mistakenly invoke deprecated or incorrect APIs from older versions of python-sc2. While increasing model parameters and using a more capable coder LLM can significantly reduce the likelihood of such errors, the code still needs to be checked against the latest official python-sc2 API documentation.\nIf no exceptions occur during the rollout, the test results are provided to the critic model, which analyzes the reasons behind the current performance, identifying both the factors contributing to high performance and lessons from any underperformance. Even when exceptions are present, valuable insights can still be gained. Ultimately, the critic provides explanations, potential improvements, and recommendations on whether to adjust the strategy or refine the current approach $(LLM_{critic}(C', G))$ to guide the next steps in the pipeline.\nAfter re-entering the planning phase, the simulation results are backpropagated and incorporated into the planner's historical data by updating the average score for each skill:\n$r_i = \\frac{(r_i \\times \\Gamma_i + G)}{\\Gamma_i + 1},$"}, {"content": "When exceptions occur in the generated Python code, the critic model is tasked with identifying the bugs and potential fixes, reducing the burden on the coder model. The critic analyzes the generated code C along with the traceback information to pinpoint the cause of the exception. Due to LLM hallucinations and API changes in the python-sc2 package, the coder model may mistakenly invoke deprecated or incorrect APIs from older versions of python-sc2. While increasing model parameters and using a more capable coder LLM can significantly reduce the likelihood of such errors, the code still needs to be checked against the latest official python-sc2 API documentation.\nIf no exceptions occur during the rollout, the test results are provided to the critic model, which analyzes the reasons behind the current performance, identifying both the factors contributing to high performance and lessons from any underperformance. Even when exceptions are present, valuable insights can still be gained. Ultimately, the critic provides explanations, potential improvements, and recommendations on whether to adjust the strategy or refine the current approach $(LLM_{critic}(C', G))$ to guide the next steps in the pipeline.\nAfter re-entering the planning phase, the simulation results are backpropagated and incorporated into the planner's historical data by updating the average score for each skill:\n$r_i = \\frac{(r_i \\times \\Gamma_i + G)}{\\Gamma_i + 1},$"}, {"title": "Experiment", "content": "In this section, we evaluate our proposed framework on a variety of scenarios from the SMAC micro-management tasks, which require the use of various individual and combined skills. The results demonstrate that through the planning, coding, and critic phases, the LLMs are able to select appropriate skills, generate Python code that complies with python-sc2 requirements, and successfully win the combat scenarios. The code is available at https://github.com/devindeng94/LLM-SMAC."}, {"title": "Environmental Settings", "content": "From the 23 SMAC task scenarios, we selected 14 maps that represent a range of basic skill applications and complex skill combinations. These maps are:\n1. 3m, 2s3z, and 8m, which emphasize 'focus firing'.\n2. 2m_vs_1z, 3s_vs_3z, 3s_vs_4z, and 3s_vs_5z, which require 'hit-and-run' tactics.\n3. 2c_vs_64zg and corridor, where leveraging 'terrain' advantages is key.\n4. MMM, 2s_vs_1sc, and 3s5z, where 'health and shield management' is the primary focus.\n5. 1c3s5z and bane_vs_bane, which emphasize 'kiting' and maximizing damage output.\nWhile we describe the primary skills required for each map, auxiliary skills are also necessary to execute successful strategies. Additionally, we set the adversary difficulty to 'Very Hard', which corresponds to level 7 in the original SMAC MARL tasks. We validate the generated code within the StarCraft II client using 10 different random seeds. Both the time taken for planning and coding are recorded as part of the results.\nFor the sake of convenience, we use the open-source DeepSeek-V2.5 as the base model for the planner, coder, and critic modules. However, these models can be replaced with more advanced alternatives, such as GPT-4."}, {"title": "Main Results", "content": "We present the results of win rates, planning rounds, coding rounds, and the final strategies planned by the LLM for each map in Table 1. As shown in the table, the LLM-generated code achieves near-perfect win rates on easier tasks and competitive results on more difficult tasks. Leveraging the LLM's prior knowledge, the planner is able to select optimal strategies with relative ease, and thanks to the high coding capabilities of the DeepSeek model, most correct Python scripts are generated within 30 rounds of iteration.\nStrategy Planning The planner LLM outlines the strategy framework by combining skills and defining the conditions under which they should be used. According to the results, 'Focus Fire' emerges as one of the most common and effective skills across SMAC maps, as it quickly reduces the number of enemy units. For many of the easier tasks, once the planner identifies and selects the 'Focus Fire' skill, the likelihood of generating a winning code script significantly increases. As a result, planning rounds for maps requiring focus fire tend to be much shorter. Conversely, on maps where focus fire is less effective, such as the corridor map, the LLM requires more rounds to produce alternative skills after receiving feedback from the rollout results.\nAs illustrated in Figure 3, certain skills can conflict with one another. For example, the focus fire skill directs all units to attack the weakest enemy, while the kiting skill commands units to retreat to the backline. At the same time, the positioning skill might push some units forward to the front. As a result, a unit may receive conflicting commands, such as one attack order and two movement orders, which leads to inconsistent actions during the rollout. By refining the conditions for each skill, the planner makes the overall strategy more precise and provides the coder with a well-structured decision tree for further implementation.\nCode Generation Given the strategy planned by the planner and the refinement suggestions from the critic model, the coder module generates Python code in accordance with the python-sc2 package. Due to the coder's powerful yet still limited coding ability and the diversity of possible implementation methods, multiple rounds of code generation are required to fix any bugs in the decision tree script and to improve performance based on the rollout results.\nAs shown in Figure 4, starting from a strategy skeleton, we present the coding result from the first round and the refined code following the promotion prompt from the critic module. During the rollout process, the initial code (on the left) achieves 4 wins out of 10 simulations, while the refined code (on the right) achieves a perfect 10 out of 10 wins. Compared to the original code, the improved version refines retreat distances and adjusts the health and shield threshold values. Additionally, the refined code includes group coordination logic, further boosting the win rate."}, {"title": "Discussion", "content": "White-box Decision-Making via LLM Unlike traditional model-free reinforcement learning, which approximates state-action values, the decision-tree scripts generated by the coder explicitly represent skills and decision-making processes. As shown in Figure 6, utility networks from MARL can compute the maximum expected return for a specific action, but they fail to explain the relation-"}, {"title": "Policy Transferability", "content": "The decision-tree script can be viewed as a set of time-series regulations triggered by different conditions. As a result, strategies designed for more difficult tasks can be directly transferred to similar tasks that feature analogous unit formations. For example, in video demonstrations, the strategy developed for the 8m scenario can be applied without modification to the 3m scenario. Similarly, as shown in Figure 7, the policy generated for the 3s_vs_4z scenario (where three units form a triangle and one unit attracts enemy fire while the other two focus fire on the weakest target) can be directly reused in both the 3s_vs_3z and 3s_vs_5z scenarios. The strategy from 3s5z can also be effectively applied to the 2s3z scenario, offering a more precise solution than one learned directly from 2s3z."}, {"title": "Exploration Efficiency", "content": "Vanilla reinforcement learning models often suffer from low exploration efficiency, as they must train from scratch. Although some approaches in multi-agent reinforcement learning emphasize coordinated exploration techniques, these methods still explore at the action level, which limits their efficiency. In contrast, our LLM-SMAC framework leverages the prior knowledge of LLMs to explore at the skill level in the planner phase and at the decision-tree implementation level in the coder phase. This significantly enhances efficiency, allowing us to solve hard SMAC tasks in just tens of rounds of planning and code generation, whereas MARL algorithms typically require millions of timesteps over tens of thousands of iterations."}, {"title": "Map Information", "content": "Map descriptions provide crucial terrain advantages that enrich strategic planning. Some units, like the Colossus, can use the terrain to their advantage, such as climbing cliffs to kite enemies. Other units may use map edges to block choke points, reducing the risk of being surrounded by Zerglings. As illustrated in Figure 8, two Colossi can exploit the terrain by climbing up and down a cliff, while most enemy Zerglings remain on the other side. Without this map information, strategies might unintentionally rely on map edge advantages. Another example is the corridor scenario, where units should hold choke points to prevent being surrounded by Zerglings. Without the choke point, the generated code is hard to win the corridor combat."}, {"title": "Information on Units", "content": "While the training of LLMs already incorporates knowledge about units, explicitly providing comprehensive unit information for the specific map can enhance their effectiveness. The data crawled from Liquipedia includes essential attributes such as health, shield, and damage per second (DPS). This information enables the planner to perform statistical calculations, such as determining when to employ the kiting skill based on differing unit speeds. The retreat point is also influenced by the attacking range of units. Consequently, detailed unit information aids the planner module in generating more targeted strategies, allowing the coder to produce more precise decision-tree scripts."}, {"title": "Conclusion and Future Work", "content": "This paper proposes a novel approach to solving SMAC tasks, called LLM-SMAC. LLM-SMAC leverages LLM to generate decision tree code and refine the decision tree based on feedback from the environment's rewards. Our experiments demonstrate that LLM-SMAC can produce a high-quality white-box decision tree model with minimal environmental exploration in most SMAC tasks, and these policy models exhibit strong transferability.\nWe believe that generating decision tree code through LLM offers a new perspective for addressing decision-making tasks. In the future, we plan to fine-tune LLMs using reward functions to further enhance their capability to generate decision tree code. Additionally, we are highly interested in pre-training LLMs by crawling a significant amount of decision tree code. We see great potential for further exploration in this direction."}]}