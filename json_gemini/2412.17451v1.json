{"title": "DIVING INTO SELF-EVOLVING TRAINING FOR\nMULTIMODAL REASONING", "authors": ["Wei Liu", "Junlong Li", "Xiwen Zhang", "Fan Zhou", "Yu Cheng", "Junxian He"], "abstract": "Reasoning ability is essential for Large Multimodal Models (LMMs). In the ab-\nsence of multimodal chain-of-thought annotated data, self-evolving training, where\nthe model learns from its own outputs, has emerged as an effective and scalable\napproach for enhancing reasoning abilities. Despite its growing usage, a com-\nprehensive understanding of self-evolving training, particularly in the context of\nmultimodal reasoning, remains limited. In this paper, we delve into the intricacies\nof self-evolving training for multimodal reasoning, pinpointing three key factors:\nTraining Method, Reward Model, and Prompt Variation. We systematically ex-\namine each factor and explore how various configurations affect the training's\neffectiveness. Our analysis leads to a set of best practices for each factor, aimed\nat optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution\nDynamics during training and the impact of automatic balancing mechanisms\nin boosting performance. After all the investigations, we present a final recipe\nfor self-evolving training in multimodal reasoning, encapsulating these design\nchoices into a framework we call M-STAR (Multimodal Self-evolving Training\nfor Reasoning), which is universally effective for models with different sizes on\nvarious benchmarks, e.g., surpassing the pre-evolved model significantly on 5\nmultimodal reasoning benchmarks without using additional human annotations, as\ndemonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B).\nWe believe this study fills a significant gap in the understanding of self-evolving\ntraining for multimodal reasoning and offers a robust framework for future research.\nOur policy and reward models, as well as the collected data, is released to facilitate\nfurther investigation in multimodal reasoning.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid advancement of Large Language Models, their reasoning abilities have improved\nsignificantly (Shao et al., 2024; Xin et al., 2024; Yang et al., 2024). This progress has been ac-\ncompanied by a growing demand for more realistic and general reasoning capabilities. Multimodal\nreasoning, considered a fundamental skill in many real-world applications, such as intelligent agents\n(Liu et al., 2024c), robotics (Li et al., 2023; Liu et al., 2024b), and autonomous driving (Yang et al.,\n2023), exemplifies this trend. Multimodal reasoning requires Large Multimodal Models (LMMs) to\nunderstand various modalities beyond text. For example, visual mathematical reasoning (Lu et al.,\n2023) challenges models to analyze complex figures, diagrams, and charts, leveraging the provided\ninformation to perform reasoning tasks.\nDespite these advances, the availability of human-annotated thought processes in multimodal scenar-\nios remains limited, challenging the learning of multimodal reasoning. Consequently, self-evolving\ntraining, which utilizes model's own generation ability to iteratively tune and improve itself without\nexternal annotated data, has emerged as an appealing candidate to facilitate reasoning abilities. While\nresearch on self-evolving training has primarily focused on the text-only settings (Hosseini et al.,\n2024; Sun et al., 2024; Shao et al., 2024), its application in the multimodal domain, especially for"}, {"title": "2 OVERVIEW OF SELF-EVOLVING TRAINING FOR MULTIMODAL REASONING", "content": "Self-evolving training can be modeled as a general framework of reinforcement learning, where\nvarious algorithms can be formulated as a specific instantiation of RL, such as PPO (Schulman et al.,\n2017), STaR (Zelikman et al., 2022), ReST (Gulcehre et al., 2023) and ReSTEM (Singh et al., 2023).\nSpecifically, given a reward function R, the objective of self-evolving training is to train the policy\nmodel \\(\u03c0_\u03b8\\) to maximize expectation of reward R:\n\\[\u03c0_\u03b8^L = \\arg \\max_{\u03c0_\u03b8} \\sum_i E_{x,o \\sim D, \\hat{y}_i \\sim \u03c0_\u03b8[\\cdot|x,o]} [R(\\hat{y}_i)],\\]\nwhere x, o represent the query and image in the given training data D, while \\(\\hat{y}_i\\) is a response sampled\nfrom the current policy model \\(\u03c0_\u03b8\\). This standard RL objective, however, can be unstable to optimize\nand difficult to scale up, thus a popular algorithm adopted by recent works is to decouple the response\nrollout \\(\\hat{y}_i \\sim \u03c0_\u03b8[\\cdot|x, o]\\) and policy improvement into separate offline stages (Gulcehre et al., 2023;\nSingh et al., 2023): (1) Generate: the current policy model generates new responses \\(\\hat{y}_i \\sim \u03c0_\u03b8[\\cdot|x, 0]\\);\nand (2) Improve: using the rewards to selects certain responses from the Generate step, which are\nthen used to train the policy model with a standard supervised fine-tuning (SFT) loss. This way,\nthe algorithm resembles Rejection Fine-Tuning (RFT, Yuan et al. (2023)) as it filters out negative\nresponses in a hard manner. Both steps are performed iteratively to strike a tradeoff between offline\nand online training. In many tasks such as mathematical problem-solving, there exists a unique,\nground-truth answer a* which is utilized in the reward function, for example, Singh et al. (2023)\ndirectly adopts exact match to compute a binary reward by comparing \\(\\hat{y}\\) and a*. In such an iterative\ntraining procedure, the objective at iteration t is to obtain an improved policy model \\(\u03c0_\u03b8^{t+1}\\):\n\\[\u03c0_\u03b8^{t+1^L} = \\arg \\max_{\u03c0_\u03b8} E_{x,o,a^* \\sim D,\\hat{y}_i \\sim \u03c0_\u03b8^t[\\cdot|x,o]} [R(a^*, \\hat{y}_i)],\\]\nwhere the ground-truth answer input a* to the reward function R can be empty, for example, when\ndealing with unlabeled inputs, and then a reward model will be necessary to score \\(\\hat{y}_i\\).\nThe Design Spaces There are different design choices to model and implement Eq. 2, for example,\nthe design of reward function R and whether to incorporate additional unlabeled inputs without a*\ninto training. Additionally, the training algorithms to perform this iterative process vary as well. For\nexample, while Gulcehre et al. (2023); Xu et al. (2024b) initialize the model from the last checkpoint\nat each iteration, Zelikman et al. (2022); Singh et al. (2023) argue that initializing from the beginning\ncheckpoint reduces overfitting and gives better performance empirically. Next, we investigate these\nthree design spaces, training method, reward model, and prompt variation, aiming to summarize the\nbest practices for each factor to faciliate multimodal reasoning learning."}, {"title": "3 DIVING INTO SELF-EVOLVING DESIGN COMPONENTS", "content": "In this section, we explore the three key components of self-evolving training, examining various\nstrategies within each. We begin by outlining the general setup (\u00a73.1), followed by a comprehensive\nanalysis of each component to identify the best practices for multimodal self-evolution (\u00a73.2-\u00a73.4)."}, {"title": "3.1 GENERAL SETUP", "content": "Model We base our main exploration on MiniCPM-V-2.5 (8B) (Yao et al., 2024), and we also\nvalidate the final design choice for each component on two extra models with different sizes: Phi-\n3.5-Vision (4B) (Abdin et al., 2024) and InternVL-2 (2B) (Chen et al., 2024c). The details of these\nmodels can be found in Appendix A. To make the analysis process easier to understand, we mainly\npresent the results of MiniCPM-V-2.5 in this section, while we include the results of the other models\nin \u00a74.2.\nDatasets We utilize MathV360K (Shi et al., 2024), a high-quality and diverse multimodal reasoning\ndataset as our seed training dataset. Specifically, we downsample half of the examples (180K) from\nit to serve as our labeled training set, while setting aside the remaining half as a unlabeled training\nset by not using the answers in it. For evaluation, we split 750 samples from the unlabeled part\nof MathV360K as the in-domain (ID) testset. For our out-of-domain (OOD) testset we use the\ntestmini split of MathVista (Lu et al., 2023), a widely recognized comprehensive benchmark\nencompassing a wide range of multimodal reasoning tasks, including visual question answering,\nfigure-based question answering, science question answering, and more. We also keep an non-\noverlapping 250 samples from MathV360K as the global validation set in training.\nWarm-Up Phase to Unlock the Chain-of-Thought (CoT) Capability of LMMS In our prelim-\ninary experiments, we found that open-source LMMs would directly output the answer given the\nquery, while struggling to produce detailed chain-of-thought (CoT) reasoning processes. This may"}, {"title": "3.2 TRAINING METHODS", "content": "As described in \u00a72, there are multiple variants on how we would train to update the policy model.\nPrevious works mainly vary the model initialization factor, where at the \u201cImprove\" step, the model\ncan be initialized from either the last checkpoint (Xu et al., 2024b; Pang et al., 2024) or the beginning\ncheckpoint before the first iteration (Zelikman et al., 2022; Singh et al., 2023). Besides model\ninitialization, in this work, we introduce new variants of iterative self-evolving through delving into\nthe gap between iterative training and online RL \u2013 concretely, when the iteration interval is small,\nthe checkpoint at each iteration is initialized from one from the last iteration, and the optimizer as\nwell as the learning rate scheduler is inherited between iterations, then iterative training becomes an\nonline RL algorithm. Therefore, we propose Continuous Self-Evolving, a new iterative self-evolving\ntraining variant that represents a smoother interpolation between iterative training and online training.\nIn continuous self-evolving training, we inherit the optimizers as well as the learning rate schedulers\nfrom the last iteration besides inheriting the model checkpoint, so that the optimization is continuous\nand closer to purely online learning algorithms. This way, we only have a global optimizer and\nlearning rate scheduler essentially across the entire iterative training process. We also analyze the\niteration interval effect in continuous self-evolving, which is defined as the training queries passed\nfor one iteration we specifically study the effect of having a shorter iteration interval, which stands\nin contrast to the common practice that adopts a long iteration interval to process all the data queries\nfor one iteration.\nSetup We perform controlled experiments to study the effect of different training methods, thus\nin this experiment we use the labeled dataset only and simply adopt the binary exact-match reward\nbetween ground-truth answer a* and the generated answer. We compare with the most common\niterative self-evolving algorithms ReSTEM (Singh et al., 2023) and iterative RFT, which are specific\ninstantiations of our training methods design space. To study the effect of iteration interval in the\nproposed continuous self-evolving, we experiment with different percentage of all the queries per\niteration, varying from [6.25%, 12.5%, 25%, 50%, 100%]."}, {"title": "3.3 REWARD MODELS", "content": "In self-evolving training, the most common approach to reward function design uses a binary reward\nR(\\(\\hat{y}_i\\)) = 1(\\(\\hat{a}_i\\) = a*), where \\(\\hat{a}_i\\) is the predicted answer inside \\(\\hat{y}_i\\) and incorrect responses are filtered\nout to maximize rewards. While effective, this sparse binary reward has limitations. It overlooks the\nquality of the intermediate reasoning steps within a response. Additionally, reward models trained\nfrom equal or higher capacity models than the policy model (Fried et al., 2022; Wang et al., 2024;\nSun et al., 2024) can provide richer signals to improve the policy model's learning.\nIn this section, we introduce a Process Reward Model (PRM) (Lightman et al., 2023; Wang et al.,\n2024) for multimodal reasoning\u2014the first of its kind, to our knowledge\u2014and explore how integrating\nPRM can enhance reward design and whether it can improve policy model learning in self-evolving\ntraining for multimodal reasoning. To incorporate the reward scores into the objective of self-evolving\ntraining, the reward function is reformulated as:\n\\[R(\\hat{y}_i) = H(1(a^* = \\hat{a}_i) \u00d7 R_p(\\hat{y}_i))\\]\n\\[R_p(y_i) = \\min(f(s_1), f(s_2), ..., f(s_m))\\]\nHere, H is an operation that processes responses based on the final reward scores, where we ensure\nall responses are correct by matching the ground truths, and \\(R_p(\\hat{y}_i)\\) represents the process reward\nscore for each sampled response. The function f(\\(s^k\\)) denotes the reward score at each intermediate\nstep. Following Lightman et al. (2023), we use the min operation to aggregate stepwise rewards.\nSetup We conduct controlled experiments to assess the impact of incorporating the Process Reward\nModel (PRM) into self-evolving training and explore how best to utilize the reward signals provided\nby PRM. Notably, before applying PRM, responses are pre-filtered based on their final answers to\nensure consistency and quality during training. To train our PRM, we use Monte Carlo rollouts\nstarting from prefixes with partial reasoning steps (Wang et al., 2024) to generate the training data.\nSpecifically, we sample 16 responses per question and complete each step 8 times to obtain step-level\nannotations. For additional details on the training process of our PRM, please refer to Appendix D.\nWe evaluate two different H operations: (1) Top-K: Pick the top-K correct responses according to\ntheir reward scores, and (2) Filtering by a Threshold a: Filtering out sampled responses with lower\naggregated rewards than a. The optimal value of a is 0.2 which is determined by enumerating it with\nan interval of 0.1 on the validation set. Additionally, we investigate how varying the value of K in\nTop-K affects training, as it represents a trade-off between the quality and diversity of the samples.\nAccording to \u00a73.2, we fix training methods as continuous self-evolving with 45k interval and set\ncontinuous self-evolving, with or without randomly selected correct responses as our baselines."}, {"title": "3.4 PROMPT VARIATION", "content": "In this section, we explore how prompt variation affects self-evolving training. There are two primary\ntypes of prompts: labeled prompts and unlabeled prompts. Labeled prompts come with annotated\nground truth answers, which can be used to filter out incorrect responses during training. In contrast,\nutilizing unlabeled prompts in self-evolving training is more challenging due to the absence of ground\ntruth annotations. To maintain the quality of unlabeled prompts in training, surrogates like reward\nscores or pseudo labels must be employed. Meanwhile, unlike labeled prompts, unlabeled prompts\nare not be trained in SFT period, which increases the difficulty of learning for policy models.\nSkylines: Unlabeled Prompts with Oracle Reward Signals The coupling of these additional\nfactors introduces complexity, making the effective use of unlabeled prompts less predictable. To\ndissect these factors, we start by establishing a baseline with \u201cskyline\u201d experiments, where both the\nunlabeled prompts and their ground truth answers are available but not used during the SFT phase.\nThese unlabeled prompts with oracle reward signals serve as an intermediate difficulty between fully\nunlabeled and labeled prompts, providing insight into the challenges of training with unlabeled data.\nUnlabeled Prompts We incorporate unlabeled prompts into self-evolving training. To ensure the\nquality of sampled responses for these prompts, we use weighted voting to ensemble the predictions\nfrom different responses, treating the ensembled prediction as a pseudo label \\(\\tilde{a}\\). This pseudo label is\nthen used to filter out responses with conflicting predictions, ensuring consistency. Following the\nbest practices outlined in \u00a73.3, we apply PRM as a reranker to select the top-2 responses among\nthose with the predicted answer \\(\\tilde{a}\\). These unlabeled prompts are then mixed with labeled prompts for\nself-evolving training. Additionally, since learning from unlabeled prompts is more challenging for\npolicy models, we investigate the optimal stage to introduce them into training to better understand\ntheir impact on model performance. We maintain a training interval of 45k prompts and adjust when\nunlabeled prompts are introduced into the self-evolving training process. Specifically, we introduce\nunlabeled prompts after [0%, 25%, 50%, 75%] of the total training process."}, {"title": "4 DYNAMICS OF SELF-EVOLUTION AND THE FINAL RECIPE", "content": "So far, we have explored the impact of three pivotal factors within our design space, leading to\nestablished best practices for learning multimodal reasoning \u2013 we adopt continuous self-evolving\ntraining coupled with a reward model to help data selection as described in \u00a73.3, and we perform the\ntraining process on SFT datasets with final answer annotations. In this section, we delve even deeper\ninto the current self-evolution strategy to better understand the bottlenecks. Instead of analyzing from\na design space perspective as previously, we now fix the design parameters and focus exclusively on\nthe training dynamics during the model's self-evolution. This shift in focus allows us to examine the\nprocess from an orthogonal angle, providing further insights into the underlying mechanisms that\ndrive or impede progress in multimodal reasoning capabilities."}, {"title": "4.1 MONITORING THE TRAINING DYNAMICS", "content": "Intuitively, two critical conditions must be met for the success of self-evolving training: (1) the\npresence of high-quality candidate responses generated by the model, otherwise self-evolving will\nnot work no matter how strong the reward is; and (2) the reward function's ability to effectively\ndistinguish and prioritize these high-quality responses. These conditions align with the traditional\nreinforcement learning concepts of exploration and exploitation. Apparently, both exploration and\nexploitation capabilities are dynamic targets in self-evolving training, as the policy model evolves and\nthe distribution of rollout responses changes with each iteration. To better understand these training\ndynamics, we propose tracking and visualizing four metrics:\n\u2022 Greedy Accuracy: the model's accuracy with greedy decoding. We track this metric for reference\nto compare with other metrics.\n\u2022 Pass@K Accuracy: the percentage of samples for which the model produces at least one correct\nresponse when sampling K candidates. This metric measures the model's exploration ability.\n\u2022 (Pass@K - Greedy) Accuracy: the difference between Pass@K and Greedy accuracy. Typically,\nPass@K is an upper bound of Greedy Accuracy, and the gap roughly reflects the percentage of\nsamples where the model, while failing in greedy decoding, can generate a correct response when\nsampling more candidates. This gap is crucial for the success of self-evolving training\u2014a zero gap\nindicates that the model fails to explore correct responses for the current failure cases, suggesting\nthat further training is unlikely to yield significant improvement.\n\u2022 Reward-Pass@2: the percentage of samples for which there exist correct responses among the top\n2 responses ranked by the reward model. This metric directly reflects the exploitation efficacy of\nthe reward model for the current policy. We choose Pass@2 since our training strategy involves\nselecting the top 2 responses using the reward model (\u00a73.3)."}, {"title": "4.2 M-STAR- FINAL RECIPE WITH OPTIMAL DESIGN CHOICES & ADAPTIVE EXPLORATIONS", "content": "Reward-Pass@2 closely relates to the effectiveness of our self-evolving training strategy since our\nmethod selects top responses ranked by the reward model, and Reward-Pass@K directly reflects the\nquality of these 2 responses. While Reward-Pass@2 naturally measures exploitation when the policy\nis fixed, the absolute value of this metric actually encapsulates both exploration and exploitation \u2013 its\nvalue would be low if the model fails to explore high-quality candidates.\nTherefore, we hypothesize that enhancing the Reward-Pass@K scores for the current iteration through\nvaried configurations could potentially improve the efficacy of self-evolving training. We fix reward\nmodel as a control variable and focus on modifying the model's exploration capabilities to achieve\nthis objective. Analysis in \u00a74.1 suggests that the temperature, which is crucial for exploration,\nmay require dynamic adjustment. Thus we propose to adjust the temperature automatically at each\niteration based on the validation Reward-Pass@2 scores. This aims to optimize exploration so that\nthe selected responses are of higher quality, potentially enhancing overall training effectiveness.\nSpecifically, we adjust the temperature per two iterations, and pick the temperature from 0.3 to 1.6\nwith interal 0.1 automatically with maximum validation Reward-Pass@2 scores. The optimal design\nchoices outlined in \u00a73, combined with our adaptive exploration strategy, form our final recipe for\nmultimodal self-evolving training for reasoning, M-STAR. For experiments on Phi-3.5-vision and\nInternVL2, considering the limited capacity of these models and the computational cost, we utilized\nboth the warmup data and multimodal PRM based on MiniCPM-V-2.5.\nFull Results Table 4 presents the results of our final approach as well as the comparison with\nrepresentative baselines for all three models. We also demonstrate the performance on all sub-tasks\nof MathVista for a detailed and fine-grained analysis. We can see that by incorporating the dynamics\nof Reward-Pass@2, which balances both exploration and exploitation, our final recipe achieves the\nhighest results for all of the 3 backbone LMMs and the improvement is generally consistent across all\nsub-tasks. In addition to overall trend, we observe that self-evolving training based on larger models"}, {"title": "5 CONCLUSION", "content": "We dive into the self-evolving training for multimodal reasoning. Three static components are\nidentified at first, namely the training method, reward model and the prompt variation. Through\ncontrolled experiments, we conclude a set of optimal design choices. On the other direction, we go\ndeeper into the dynamics of self-evolving training to analyze the trade-off between exploitation and\nexploration. By monitoring the dynamics and adjusting key hyperparameters accordingly, we are\nable to further improve the model performance. We hope our work can provide insights and guidance\nfor future research on self-evolving training for multimodal reasoning."}, {"title": "A DETAILS OF SELECTED LMMS", "content": "MiniCPM-V-2.5 (Yao et al., 2024) is a powerful, openly released LMM. MiniCPM-V-2.5 leverages\nLLaMA-3-8B (Meta, 2024) for its language model and SigLIP (Zhai et al., 2023) as its vision\nencoder, resulting in strong multimodal capabilities. Its performance on a wide range of multimodal\nbenchmarks significantly surpasses previous openly released LMMs such as LLaVA (Liu et al., 2023;\n2024a) and Qwen-VL (Bai et al., 2023).\nPhi-3.5-Vision (Abdin et al., 2024) is a multimodal model combining a CLIP ViT-L/14 (Radford\net al., 2021) image encoder and a Phi-3.5-mini transformer decoder. It processes interleaved image-\ntext inputs using dynamic cropping for images and is pre-trained on 0.5T tokens from diverse datasets.\nPost-training via supervised fine-tuning (SFT) and direct preference optimization (DPO) enhances its\nmultimodal reasoning and language understanding capabilities.\nInternVL-2 (Chen et al., 2024c) InternVL 2.0 is a multimodal large language model series ranging\nfrom 1B to 108B parameters. The specific 2B version we use combines InternViT (300M) (Chen\net al., 2024c), an MLP projector, and InternLM-2-Chat (1.8B) (Cai et al., 2024), showcasing strong\nvision-language capabilities. Built with progressive alignment training, it efficiently aligns vision\nand language models while supporting diverse inputs (text, images, video, medical data) and outputs\n(images, bounding boxes, masks), performing competitively across various vision-language tasks."}, {"title": "B COLLECTING WARMUP TRAINING DATA WITH CHAIN-OF-THOUGHT", "content": "Since our base model typically outputs the answer directly when responding to multimodal reasoning\nquestions, during the warmup phase, we added additional instructions along with the input question,\nrequiring the model to output the rationale. The instructions used in this process are as follows:\nExtra instruction to guide CoT\nOffer a comprehensive breakdown of your analytical process, detailing each step, the reasoning behind\nyour decisions, and how you integrated various pieces of information, and put your answer at the end."}, {"title": "C HYPER PARAMETERS", "content": "We follow the training setup from Yao et al. (2024), using a learning rate of le-6 and a batch size\nof 128. A constant learning rate scheduler with a warmup ratio of 0.1 is applied. Input images are\nencoded using SigLIP SoViT-400m/14 (Zhai et al., 2023), and the visual tokens are compressed\nthrough a perceiver resampler structure with a single cross-attention layer. Additionally, each input\nimage is sliced into a maximum of 9 segments, with each segment compressed into 96 queries."}, {"title": "D TRAINING PROCESS REWARD MODEL (PRM)", "content": "To train our PRM, we first train another checkpoint (denoted as \\(\\hat{w}\\)) on our CoT-augmented training\ndata for a much longer period to make sure it fully converges.\nBased on this model, we leverage Monte Carlo Rollut method (Wang et al., 2024) to collect the\ntraining data for PRM. Specially, we randomly pick 50K questions from the full training set, and\nsample 16 responses for each of them with \\(\\hat{w}\\). We de-duplicate these responses, and only keep at\nmost 4 responses for each question. After that we randomly sample 50K question-response pairs\nfrom all the pairs, where we control the ratio of correct and wrong responses as 1:1, and the ratio of\nmulti-choice and free-form question as 1:1 as well, to keep a balanced distribution.\nTo construct the labels of each step, we use \\(\\hat{w}\\) as the completer to complete the solution\nfrom the end of each step in one response. For the kth step, the step label is annotated as\n11(C\\(j\\)(s\\(k\\))) = a*), where N(= 16) is the number of completion, C\\(j\\) is the j-th completion."}, {"title": "E MEASURING RESPONSE RELATIVITY", "content": "To get a comprehensive understanding of how our PRM works as a re-ranker, we conduct a quantitative\nanalysis using GPT4-0 (gpt-40-2024-08-06) to see how much a correct response is directly\nrelated to the query, e.g., does not contain irrelvant steps. The prompt we use is as follows:\nPrompt for GPT4-o to annotate the relativity score\nGiven the image and a related question, you need to judge how a candidate solution is directly related to\nthe question. You need to consider all its steps, and return a final value bewteen 1-10 as a overall score.\nConclude your judgement at the end as \"So the relativity score is X\" where X is the score you give.\n[Question]\n{question}\n[Solution]\n{solution}"}, {"title": "F MORE RESULTS FOR M-STAR", "content": "We plot the extra analysis results for M-STAR here. In Figure 6, we plot the changes of Pass @K\nand Reward-Pass@2 across different temperatures for M-STAR(Reward-Pass@2) as a compliment\nto the adapative adjustion mentioned in \u00a74.2. We can see that acroos all selected temperatures, the\nexploration ability reflected by Pass@K does not regress continuously, and the Reward-Pass@2\nreaches its peak more quickly, compared with training without the monitor of dynamics."}, {"title": "G MORE RELATED WORKS", "content": "In this section we will briefly introduce other works that are related to our study that cannot be\nelaborated in the main context due to page limit."}, {"title": "Self-Evolving Methods", "content": "The most straightforward and widely-used approach to enhance a model's\nreasoning ability is through supervised fine-tuning (SFT), where models mimic the outputs of highly\ncapable models (Yu et al., 2023; Yue et al., 2023). However, as the gap between open-source models\nand proprietary ones narrows, the performance improvements from SFT tend to plateau. This has\nled to increased attention on self-evolving methods, where models refine and improve themselves\nwithout external supervision, as a means to further boost their reasoning abilities.\nSome early self-evolving approaches primarily focus on single-round improvements. For instance,\nLMSI (Huang et al., 2022) leverages CoT prompting combined with self-consistency to generate\nhigh-confidence solutions from unlabeled data, which are then used to augment the training process.\nSimilarly, RFT (Yuan et al., 2023) enriches the training data by filtering solutions using existing\nlabels. Both methods apply this augmentation process in just one iteration.\nOn the other hand, several works have explored iterative approaches for self-improvement. Notably,\nSTaR (Zelikman et al., 2022), ReSTEM (Singh et al., 2023), and V-STaR (Hosseini et al., 2024) retrain\ntheir models from the original checkpoint after each iteration, while ReST (Gulcehre et al., 2023)\ncontinuously fine-tunes the model starting from the previous iteration's checkpoint. Reinforcement\nLearning (RL) techniques also fit into this iterative category, offering an online mechanism that tightly\ncouples exploration and exploitation. RL methods, such as PPO (Schulman et al., 2017) and GRPO\n(Shao et al., 2024), are frequently applied to unlabeled data, using an additional reward model to\nevaluate the quality of generated responses. GRPO, in particular, streamlines the process by removing\nthe value model from PPO and instead leverages in-batch comparison to estimate advantages across\ndifferent rollouts, providing a more stable alternative."}, {"title": "Multimodal Reasoning", "content": "Currently, the most common approach to improve multimodal reasoning\ncapabilities continues to be supervised fine-tuning (SFT). For example, G-LLaVA (Gao et al., 2023)\naugments existing geometry question-answering datasets to fine-tune the LLaVA-1.5 model (Liu\net al., 2024a). Math-LLaVA (Shi et al., 2024) selects and augments data from larger multimodal\nquestion-answer datasets, carefully balancing the difficulty of samples. Similarly, MAVIS (Zhang\net al., 2024) focuses on the geometry and function domains and generates instruction-based tuning\ndata through synthetic data engines.\nHowever, recent works have begun incorporating self-evolving mechanisms into multimodal reason-"}]}