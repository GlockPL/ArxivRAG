{"title": "Layerwise Change of Knowledge in Neural Networks", "authors": ["Xu Cheng", "Lei Cheng", "Zhaoran Peng", "Yang Xu", "Tian Han", "Quanshi Zhang"], "abstract": "This paper aims to explain how a deep neural network (DNN) gradually extracts new knowledge and forgets noisy features through layers in forward propagation. Up to now, although the definition of knowledge encoded by the DNN has not reached a consensus, Li & Zhang (2023b); Ren et al. (2023a; 2024) have derived a series of mathematical evidence to take interactions as symbolic primitive inference patterns encoded by a DNN. We extend the definition of interactions and, for the first time, extract interactions encoded by intermediate layers. We quantify and track the newly emerged interactions and the forgotten interactions in each layer during the forward propagation, which shed new light on the learning behavior of DNNs. The layer-wise change of interactions also reveals the change of the generalization capacity and instability of feature representations of a DNN.", "sections": [{"title": "1. Introduction", "content": "Recently, understanding the black-box representation of deep neural networks (DNNs) has received increasing attention. This paper investigates how a DNN gradually extracts knowledge from the input for inference during the layer-wise forward propagation, although the definition of knowledge encoded by an AI model is still an open problem. To this end, the information bottleneck theory (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018) uses mutual information between the input and the intermediate-layer feature to measure knowledge encoded in each layer. It finds that the DNN fits (learns) task-relevant information, and compresses task-irrelevant information. Liang et al. (2020) extract common feature components shared by different features as the shared knowledge.\nIn this paper, we aim to define and quantify the knowledge encoded in each layer. In this way, we can accurately decompose and track explicit changes of knowledge (i.e., the learning of new knowledge and the forgetting of old knowledge) through different layers.\nHowever, there is no a widely-accepted definition of knowledge, because we cannot mathematically define/formulate knowledge in human cognition. Instead of focusing on cognitive issues, Ren et al. (2023a); Li & Zhang (2023b) have discovered and Ren et al. (2024) have theoretically proven\u00b9 the sparsity property and universal-matching property of interactions, i.e., given an input sample x, a well-trained DNN usually only implicitly encodes a small number of interactions between the input variables, and the inference score can be explained as numerical effects of these interactions. Thus, these two properties mathematically make such interactions (also called interaction primitives or interaction concepts) be considered as the knowledge encoded by a DNN. As Fig. 1 shows, given a dog image x, each interaction implicitly encoded by the DNN represents a co-appearance relationship between input variables (image patches) in S = {eye, nose, mouth}. This is actually an AND relationship between image patches in image x. Only when all patches in S are present in the image, the interaction S is activated and makes a numerical effect I(Sx) on the classification score. Masking4 any patch will deactivate the interaction S and remove the effect.\nAlthough the above studies make it plausible to define and quantify interactions encoded by a DNN, our target of quantifying and tracking the interactions encoded by different layers presents the following three new challenges.\n(1) Alignment of interaction primitives. The fair comparison between any arbitrary pair of layers requires interaction primitives extracted from different layers to be aligned, although the physical feature dimensions in different layers do not have a clear correspondence/alignment.\n(2) Decomposability and countability of knowledge. In-"}, {"title": "learning behavior of a DNN.", "content": "teractions help us overcome the challenge of representing uncountable knowledge as countable primitive patterns. In this way, we can exactly quantify how many interaction primitives are newly emerged and forgotten in each layer.\n(3) Connection to the generalization capacity. We hope to provide deep insights into how newly merged interaction primitives and forgotten old interaction primitives are related to the generalization capacity of a DNN.\nTherefore, considering above challenges, we extend the definition of interactions to intermediate layers of a DNN. Specifically, given features of a certain layer, we train a linear classifier\u00b2 to use these features for classification, and extract a set of interactions from the classifier. We analyze the faithfulness of the newly proposed interaction towards the intermediate layers of a DNN, and we discover that the new interactions provide us with a more straightforward way to analyze how knowledge changes in the layerwise forward propagation. Instead of directly aligning features in different layers, we find that adjacent layers in a DNN usually encode similar sets of interactions. Thus, as illustrated in Fig. 1, we can clarify the emergence of new interactions and the forgetting of old interactions in each layer.\nFaithfulness of interactions. More crucially, the newly defined interaction primitives still belong to the typical paradigm of interactions, so that there are a series of theorems (Ren et al., 2023a; Li & Zhang, 2023b; Ren et al., 2024) as convincing evidence to take countable/symbolic interactions as primitive inference patterns to represent uncountable knowledge in a DNN. Please See Section 2 and Section 3.1 for details.\nIn this way, we can use interactions to explain the change of the representation capacity of features in different layers from the following two perspectives, which can help both theoreticians and practitioners gain new insights into the"}, {"title": "learning behavior of a DNN.", "content": "\u2022 The tracking of countable interactions in different layers reveals the change of representation complexity over different layers. The complexity of an interaction S is defined as the number of input variables in S, which is also termed the order of this interaction, i.e., order(S) = |S|. In experiments, we discover that in most DNNs, early and middle layers are usually trained to fit target interactions encoded by the entire network at the cost of encoding lots of redundant interactions, and later layers remove such redundant interactions.\n\u2022 Redefining the generalization capacity of DNNs and tracking generalizable interactions. The use of interaction primitives enables us to redefine the generalization power of a DNN from a new perspective. That is, given multiple DNNs trained for the same task, if these DNNs encode similar interactions, then we consider interactions shared by different DNNs generalizable. We discover that low-order interactions usually have stronger generalization capacity than high-order interactions. Besides, we also discover that low-order interactions encoded by the DNN usually exhibit more consistent effects I(S|x1 = x + \u20ac) when we add different small noises e to the input sample \u00e6. In comparison, high-order interactions often exhibit diverse effects I(S|x1) on inference scores w.r.t. different noises \u20ac. This indicates that low-order interactions often have higher stability.\nContributions of this study are summarized as follows.\n(1) We redefine the interaction on intermediate layers, and find that the new definition ensures adjacent layers to encode similar interactions.\n(2) Our study provides several theoretically verifiable metrics to quantify the newly emerged knowledge and forgotten knowledge in the forward propagation.\n(3) The change of interactions is also found to be related to the generalization power of a DNN."}, {"title": "2. Literature in Explaining Knowledge in DNNS", "content": "Explaining and quantifying the exact knowledge encoded by a DNN presents a significant challenge to explainable AI. So far, there has not existed a widely accepted definition of knowledge that enables us to accurately disentangle and quantify knowledge encoded by intermediate layers of a DNN, because it covers multiple disciplinary issues, such as cognitive science, neuroscience, etc. To explain and quantify the exact knowledge encoded by a DNN, previous studies have either associated units of DNN feature maps with manually annotated semantics/concepts (Bau et al., 2017; Kim et al., 2018) or automatically learned meaningful patterns from data (Chen et al., 2019; Shen et al., 2021; Zhang et al., 2020), but they failed to provide a mathematically guaranteed boundary for the scope of each concept/knowledge. Thus, previous studies could not accurately quantify the exact amount of newly emerged/forgotten/unexplainable knowledge in each layer. Appendix A provides further discussions of more methods (Kolchinsky et al., 2019; Liang et al., 2020; Saxe et al., 2018; Shwartz-Ziv & Tishby, 2017; Wang et al., 2022).\nFaithfulness of using interaction primitives to define knowledge in DNNs. Although there is no theory to guarantee that salient interactions can exactly fit the so-called knowledge in human cognition, a series of studies have empirically verified and theoretically ensured the faithfulness of interaction primitives from the following perspectives.\n(1) Li & Zhang (2023b) have observed and Ren et al. (2024) have partially proven\u00b9 that most DNNs encode a few interactions with salient effect I(S|x) on the network output.\n(2) Li & Zhang (2023b) have observed that interactions exhibited considerable generalization capacity across samples and across models. Besides, they have also discovered that salient interactions exhibited remarkable discrimination power in classification tasks.\n(3) Ren et al. (2023a) have proven seven desirable mathematical properties for interactions.\n(4) Interaction primitives can also be used to explain the representation capacity of DNNs. Deng et al. (2022) have proven a counter-intuitive bottleneck of a DNN in encoding interaction primitives of the intermediate complexity. Liu et al. (2023) have proven the learning difficulty of interaction primitives. Zhou et al. (2024) have discovered that low-order interactions have higher generalization power than high-order interactions.\nFurthermore, we compare the interaction-based explanation with attribution interpretability methods. Please Appendix B for detailed discussions."}, {"title": "3. Tracking Interactions through Layers", "content": "3.1. Preliminaries: using interactions to represent knowledge in DNNS\nSo far, there is not a widely accepted way to define knowledge encoded by a DNN, because the definition of knowledge is an interdisciplinary problem over cognitive science, neuroscience, and mathematics. Li & Zhang (2023b) has derived a series of properties as convincing evidence to define interactions as symbolic primitive inference patterns encoded by a DNN (please see Section 2 for details). Thus, in this paper, we extend the definition to quantify the change of interactions in the layer-wise forward propagation. Specifically, there are two types of interactions, including AND interactions and OR interactions.\nDefinition 3.1 (AND interactions). Given an input sample x = [x\u2081,x\u2082,..., x\u2099] comprising n input variables, let N = {1,2,..., n} denote the indices of all n input variables, and let v(x) \u2208 \u211d denote the scalar output of the DNN or a certain dimension of the DNN\u00b3. Then, the AND interaction Iand(Sx) is used to quantify the effect of the AND (co-appearance) relationship among a subset S \u2286 N of input variables, which is encoded by the DNN v to compute the inference scores of the label ytruth\n$$I_{and}(S|x) = \\sum_{T\\subseteq S}(-1)^{|S|-|T|} \\cdot v(x_T).$$\nHere, \u00e6r denotes the masked\u2074 sample obtained by masking variables in N \\ T, v(x\u012b) represents the output score\u00b3 for the target label ytruth on the masked sample \u00e6\u0442.\nEach AND interaction with non-zero effect Iand(Sx) \u2260 0 means that the DNN encodes the AND relationship between variables in S. The network output can be represented as the sum of interaction effects v(x) = \u2211s\u2286n Iand(S|x).\nOR interactions. Ren et al. (2023a); Zhou et al. (2023) have further extended the AND interaction to the OR interaction. To this end, the overall network output is decomposed into the component for AND interactions vand(x) and the component for OR interactions vor (xT), subject to vand(x) = 0.5\u00b7v(x)+\u04af\u0442 and vor(xT) = 0.5\u00b7v(x\u03c4)-\u03b3\u03c4. {\u04af\u0442} is a set of"}, {"title": "3.2. Tracking interactions through layers", "content": "Definition 3.2 (OR interactions). The OR interaction is used to quantify the effect of the OR relationship between a set SN of input variables encoded by the DNN..\n$$I_{or}(S|x) = - \\sum_{T\\subseteq S}(-1)^{|S|-|T|} \\cdot V_{or}(x_{N\\setminus T}).$$\nEq. (2) indicates that the presence of any input variable in S will activate the OR interaction and make an effect Ior(Sx) to the output score v(x). Ren et al. (2023a); Zhou et al. (2023) proposed to learn parameters {\u03b3\u03c4} to generate the sparsest AND-OR interactions. The AND-OR interactions is determined when {\u04af\u0442} are learned. Please see Zhou et al. (2023) for detailed technique of learning {\u04af\u0442} for the optimal decomposition of AND-OR interactions.\nFaithfulness. The sparsity property and universal-matching property mathematically guarantee the faithfulness of interaction-based explanation. Let us randomly mask4 an input sample x and generate a total of 2\u207f masked samples \u00e6\u0442. Then, Theorem 3.3 shows that output scores v(x) on all 2\u207f masked samples \u00e6r can always be well matched by AND-OR interactions.\nTheorem 3.3. (Proven in Appendix F) Given an input sample x \u2208 \u211d\u207f, the network output score v(x) on each masked input samples {x\u0442|T \u2286 N} can be decomposed into effects of AND interactions and OR interactions, subject to Iand(x) = vand(x) = v(x) and Ior(x) = vor(X) = 0.\n$$v(x_T) = V_{and}(x_T) + V_{or}(x_T)$$\n$$= \\sum_{S \\subset T} I_{and}(S/x_T) + \\sum_{S\\cap T \\neq \\emptyset} I_{or}(S/x_T)$$\nRen et al. (2024) have proven\u00b9 that most AND-OR interactions have negligible effects I(S|x) \u2248 0 on inference, which can be regarded as noisy patterns. Only a small number of interactions have considerable effects. Given an input sample x \u2208 \u211d\u207f, we can use a small set of salient AND interactions salient andent and OR interactions salient to universally match network outputs v(x) on all 2\u207f masked samples. This indicates that salient interactions can serve as primitive inference patterns encoded by the DNN.\nLemma 3.4. (Proving interactions as primitive inference patterns, c.f. Appendix G) Given an input sample x \u2208 \u211d\u207f, the network output on all 2\u207f masked input samples {\u0445\u0442|\u0422 \u0421 N} can be universally matched by a small set of salient interactions."}, {"title": "3.2.1. VERIFYING THE SPARSITY OF INTERACTIONS", "content": "Before we define interactions encoded by intermediate-layer features, we need to first examine whether the final layer of the DNN encodes a small number of interactions. Although the sparsity of interactions has been partially proven under three common conditions\u00b2, it is still a challenge to strictly examine whether the DNN fully satisfies these conditions in real applications. Besides, the sparsity of interactions has not been proven when we simultaneously use AND interactions and OR interactions to explain a DNN.\nThe interactions used by the final layer are directly extracted based on the network output score v(x)3, according to Eq. (1) and Eq. (2). Thus, we can consider interactions extracted from the final layer as the target interactions used for the inference. If these interactions are sparse, then the utility of all layers can be simplified as pushing features towards a specific small set of sparse interactions. This will significantly simplify feature analysis.\nExperiments. We conducted experiments to illustrate the sparsity of interactions. Given a well-trained DNN and an input sample x \u2208 \u211d\u207f, we calculated AND interactions Iand (Sx) and OR interactions Ior(Sx) of all 2\u207f possible subsets SCN. To this end, we trained VGG-11 (Simonyan & Zisserman, 2014), ResNet-20 (He et al., 2016) on the MNIST dataset (LeCun et al., 1998) and CIFAR-10 datasets (Krizhevsky et al., 2009), respectively. We also learned a seven-layer MLP (namely MLP-7) on the MNIST dataset and CIFAR-10 dataset, respectively, where each layer contained 1024 neurons. Please see Appendix M.4 for experimental details."}, {"title": "3.2.2. EXTRACTING INTERACTIONS FROM INTERMEDIATE LAYERS", "content": "In comparison with extracting interactions from the network output score v(x)3, defining and extracting interactions from intermediate layers present a new challenge. It is because the intermediate-layer features are usually high-dimensional vectors/tensors/matrices, rather than a scalar output. Thus, we need to define a new scalar metric v(1)(x), which faithfully identify signals in the high-dimensional feature directly related to the classification task, to compute interactions encoded by the l-th layer of the DNN.\nTo this end, given an input sample x, we propose to train a linear classifier p(\u00b9)(y|x) = softmax/sigmoid((w(l))T f(l) (x)+ b(1)) based on the cross-entropy loss, which uses the feature f(t) (x) of the l-th layer to conduct the same classification task as the DNN\u00ba. We can define the following v(1) (x) to represent signals encoded by the l-th layer of the DNN.\n$$v^{(l)}(x) = log \\frac{p^{(l)}(y = y_{truth}|x)}{1-p^{(l)}(y = y_{truth}|x)} - \\delta_T,$$\n$$v^{(l)}(x_T) = log \\frac{p^{(l)}(y = y_{truth}|x_T)}{1-p^{(l)}(y = y_{truth}|x_T)} - \\delta_T,$$\nwhere \u03b4\u03c4 is a learnable residual proposed to model and remove the tiny noise from the output v(1)(x\u012b), so as to extract relatively clean interactions. \u03b4\u03c4 is constrained to a small range \u043a = 0.04 \u00b7 |v(1) (xn) \u2013 v(1)(x)|. We discover that small noise in output function v(1) (x\u012b) may significantly change the interaction effect. In this way, parameters {\u03b3\u03c4, \u03b4\u03c4} are learned by minimizing \u03a3\u03c4\u2286N |Iand(T|x, v(1))|+ |Ior(T|x, v(\u00b9))|, s.t. \u2200T \u2286 \u039d,|\u0431\u0442| < \u03ba. An ablation study in Appendix J shows that the extraction of interactions is relatively robust to the K value.\nComparing interaction complexity over different layers."}, {"title": "3.3. Analyzing the representation capacity of a DNN", "content": "The new function v(1) (x) enables a fair comparison between interactions extracted from different layers. The classification score v(1) (x) potentially reflects a set of interactions, which are encoded by f(t) (x) and can be directly used for classification. Specifically, we conducted experiments to extract interactions from different layers of different DNNs5. We used the MLP-7, VGG-11, and ResNet-20 trained on the MNIST dataset and CIFAR-10 dataset, which were introduced in Section 3.2.1. We also fine-tuned pre-trained DistilBERT (Sanh et al., 2019) and BERTBASE (Devlin et al., 2019) models on the SST-2 dataset (Socher et al., 2013) for binary sentiment classification.\nWe used the order of an interaction to measure the complexity of the interaction. The order was defined as the number of input variables involved in this interaction, i.e., order(S) = |S|. As illustrated in Fig. 3, linear classifiers trained on features of early layers encode less high-order interactions than later layers. We can consider that features in early layers usually represent lots of local and simple non-linear patterns between a few input variables, but most of such patterns cannot be directly used by the classifier for the classification task. Besides, compared to linear classifiers trained on early-layer features, classifiers trained on features of later layers usually share more similar interactions with the final layer of the DNN.\nEmergence of new interactions and discarding of old interactions. In this experiment, we quantified how the DNN gradually learned new interactions and discarded useless interactions in the forward propagation and obtained the target interactions in the last layer. To this end, given all AND-OR interactions encoded by the l-th layer, let \u03a9(1), m = {S \u2286 N : |S| = m, |Iand(S|x,v))| > \u03c4}7 denote the set of salient AND interactions of the m-th order extracted from the l-th layer. Accordingly, \u03a9),m = {S \u2286 N : |S| = m, |Ior(S|x, v(t))| > \u03c47} represented the set of salient OR interactions of the m-th order extracted from the l-th layer. To this end, we used and all(L),m to"}, {"title": "3.3. Analyzing the representation capacity of a DNN", "content": ""}]}