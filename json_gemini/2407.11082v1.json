{"title": "Imbalanced Graph-Level Anomaly Detection via Counterfactual Augmentation and Feature Learning", "authors": ["Zitong Wang", "Xuexiong Luo", "Enfeng Song", "Qiuqing Bai", "Fu Lin"], "abstract": "Graph-level anomaly detection (GLAD) has already gained significant importance and has become a popular field of study, attracting considerable attention across numerous downstream works. The core focus of this domain is to capture and highlight the anomalous information within given graph datasets. In most existing studies, anomalies are often the instances of few. The stark imbalance misleads current GLAD methods to focus on learning the patterns of normal graphs more, further impacting anomaly detection performance. Moreover, existing methods predominantly utilize the inherent features of nodes to identify anomalous graph patterns which is approved suboptimal according to our experiments. In this work, we propose an imbalanced GLAD method via counterfactual augmentation and feature learning. Specifically, we first construct anomalous samples based on counterfactual learning, aiming to expand and balance the datasets. Additionally, we construct a module based on Graph Neural Networks (GNNs), which allows us to utilize degree attributes to complement the inherent attribute features of nodes. Then, we design an adaptive weight learning module to integrate features tailored to different datasets effectively to avoid indiscriminately treating all features as equivalent. Furthermore, extensive baseline experiments conducted on public datasets substantiate the robustness and effectiveness. Besides, we apply the model to brain disease datasets, which can prove the generalization capability of our work. The source code of our work is available online\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph-level anomaly detection (GLAD) has been increasingly applied across a diverse array of downstream tasks to pinpoint anomalous information in numerous graphs, these tasks focus on identifying graphs that exhibit anomalous patterns within a set of graphs. For example, identifying toxic targets from protein profiles with complex structures[23, 29], detecting fraudulent groups in a large number of financial transaction networks [11\u201313], filtering out malicious or fake messages from a large base of user messages on social media [3, 15, 37]. Besides, by processing and abstracting magnetic resonance imaging appropriately, GLAD can be applied to predicting disease [3, 28]. For instance, Li. et al. first construct brain graphs from neuroimaging data of subjects and then utilize GLAD methods to recognize anomalous brain graphs for disorder detection [20]. Thus, GLAD possesses immense value and can be used for various applications. By leveraging effective GLAD methods, large volumes of data with complex structures can be effectively resolved, and a basis for relevant decision-making can be provided for the user.\nThe past few years have witnessed that multiple methods on GLAD have already been proposed to capture anomalous features and improve detection performance. Existing GLAD works are summarized into two following classes: Firstly, traditional GLAD methods mainly rely on graph classification algorithms, such as isolation forest (IF) [25], local outlier factor (LOF) [1], and one-class support vector machine (OCSVM) [36]. For some relatively simple datasets, these methods can achieve an acceptable recognition accuracy by leveraging effective distance metrics, such as the family of graph spectral distances (FGSD) which leverages the multi-set of node pairwise distances and is able to capture the unique, stable properties of the graphs [41]. However, such traditional algorithms are often overly sensitive to the distribution of normal and abnormal samples due to their simple architecture. Furthermore, the methods mentioned are composed of two independent components: a feature extractor and a classification algorithm, collectively referred to as two-stage methods. The classification algorithm relies on the provided graph embedding features from the feature extractor which produces \"hand-crafted\" features [48]. Such features limit their potential for improvement.\nSecondly, methods based on deep learning mainly utilize graph neural networks (GNNs) [43] to learn abnormal graph representations to distinguish normal and abnormal graphs. The GNNs can effectively learn the interconnections of nodes and obtain the global structure of each graph from datasets with complex topologies and rich relational information. Multiple iterations allow for constant node representation updates, capturing complex graph relationships. Recently, the GNN-based methods have yielded more results. SIGNET [27] is an unsupervised learning method that leverages a multi-view subgraph information bottleneck framework to achieve interpretable and decent results. GLocalKD [30] adopts a knowledge distillation approach, distilling representations from graph-level and node-level. GLADST [23] utilizes a dual-students-teacher model to maximize the difference between normal and abnormal samples. And GLADD [22] combines multi-representations space separation to detect anomaly graphs. Although these deep learning methods can tackle GLAD tasks with acceptable efficiency and accuracy, limitations still remain. While these methods present their own unique solutions to the GLAD tasks, there are some flaws, they either fail to efficiently construct balanced and efficient datasets, or overlook fully leveraging the information from the graphs.\nAfter analyzing the existing solutions to GLAD tasks, it is evident that the imbalance in datasets is a prevalent issue. Previous methods tend to either overlook this severe imbalance or lack effective methods to construct reliable samples. In other words, since abnormal samples are typically in the minority in GLAD tasks, training models directly on imbalanced data may cause the models to predominantly learn patterns from normal graphs, thus increasing the likelihood of misclassifying targets as normal graphs. Furthermore, if the generated abnormal samples from improper methods are not reliable enough, they might even be counterproductive. Additionally, common deep learning approaches often only consider the node features themselves, neglecting the degree attributes. These methods fail to fully utilize all valuable information from the sample graphs, leading to suboptimal model performance. To resolve the aforementioned issues, several challenges arise. Firstly, the most straightforward approach for balancing the datasets is to employ anomaly sample augmentation methods [46] or direct random sampling for oversampling. However, samples generated by such methods may contain false samples which are incorrectly marked, or fail to enrich the sample features sufficiently, leading to unreliable samples. Secondly, maximizing the utilization of rich node information is the key to feature learning. Previous methods either focus solely on node features or substitute node features with degree attributes for training, resulting in suboptimal performance on feature-rich datasets. Thus exploring ways to enable node features and degree attributes to complement each other, thereby constructing more enriched and comprehensive information, is vital and meaningful.\nIn this work, we introduce an Imbalanced Graph-level Anomaly Detection via Counterfactual augmentation and Feature learning (IGAD-CF) to address the mentioned issues. Firstly, to solve the problem of scarce anomalous samples, we construct an anomaly sample generation module inspired by the counterfactual theory commonly employed in contrastive learning [44]. This module can generate abnormal samples by applying appropriate perturbations to normal samples, thereby creating a balanced dataset containing both original information and manually created abnormal information. Meanwhile, it enriches the patterns of anomalous graphs, enhancing the anomaly perception ability of our model. As is shown in Figure 1, the distribution result is obtained by employing a graph convolutional network to extract feature representations from the dataset, followed by normalizing the output through a linear function to map it onto the range from 0 to 1. The figure illustrates that the generated abnormal samples bear a close resemblance to the original abnormal samples. Specifically, their distributions are quite similar, clustering stably around a certain value and generally not exceeding 0.5. In contrast, the normal samples are mostly distributed above 0.5 with greater volatility. This demonstrates that our generated abnormal samples are highly consistent with the real anomalous samples, rendering them reliable and practical. This COX2 dataset is available online\u00b2. Additionally, to facilitate comprehensive feature utilization and accurate anomaly detection within graphs, we introduce an effective node feature learning module. In this module, we introduce degree attributes to supplement the node features, thereby enriching the information representation. This enabling enhances anomaly localization and improves anomaly pattern learning capabilities through the structural information provided by degree attributes. Furthermore, we incorporate an adaptive weight learning module into the framework. By assigning adaptive weights to different features across datasets, this module not only enhances model generalization but also enables the model to focus on features with greater impact on the results. Finally, to verify the effectiveness of the proposed framework, comprehensive experiments are designed and carried out on traditional public datasets against popular GLAD methods. Furthermore, we extend our model to practical application on brain diseases. Our model can solve problems caused by data imbalance, insufficient application of features, and individual differences in graphs. The contributions are outlined as follows:\n\u2022 Firstly, we apply the anomaly sample generation module to undertake balancing procedures on unbalanced datasets to mitigate the issue of poor model generalization. By using counterfactual mechanisms, the generated abnormal samples have high reliability, which allows the model to capture more anomalous information.\n\u2022 Secondly, we introduce a node feature learning module that specifically focuses on the intimate relationship between node features and degree features, utilizing the model to comprehend their inherent connection. By concatenating the convolution results, the potential interaction information between them is also taken into account while retaining their respective information.\n\u2022 Thirdly, according to the fact that there are always differences between graph individuals, we train an adaptive weight learning module to pinpoint the most favorable features for detection more effectively for our model.\n\u2022 Finally, we perform experiments on traditional public GAD datasets to confirm our model's ability and apply it to real brain-disease datasets."}, {"title": "2 RELATED WORKS", "content": "Based on the research topic and relevant techniques involved, here, we mainly introduce graph neural networks, graph anomaly detection, and counterfactual learning to highlight the contributions of our proposed method, IGAD-CF."}, {"title": "2.1 Graph Neural Networks", "content": "For the past few years, GNNs have been successfully employed in graph anomaly detection tasks, yielding fruitful results. By leveraging the message passing mechanisms, GNNs demonstrate an enhanced capability to capture complex representations embedded within the target graphs sensitively. For node-level tasks, each node's representation is encoded with its own information and the information from the neighbors [43]. Then the representation is updated according to the message vectors aggregated from neighboring nodes. A pivotal strength of this strategy is that the representation learned for each node encapsulates its own information and incorporates the information from its neighboring nodes [9, 43]. For example, Graph convolutional network (GCN) [16] can use convolution operators for information aggregation to update the representation of nodes through local information. And graph attention network (GAT) [40] uses attention mechanisms to assign different weights to neighboring nodes. For graph-level tasks, the node representations need to be further downsampled to obtain the overall graph representation. The common downsample methods are graph pooling operations [43] which aid GNNs in extracting high-level topological information. Some improvements start with optimizing pooling operations, DiffPool, for example, is a differentiable pooling that can improve graph classification benchmarks when applied to GNNs [45]. In our framework, we leverage GNNs to capture not only the node features but also the structural information of graphs, thereby enhancing the feature representations for anomalous graphs."}, {"title": "2.2 Graph Anomaly Detection", "content": "Graph anomaly detection (GAD) is commonly delineated into two genres: node-level anomaly detection (NLAD) and GLAD.\nNLAD targets identifying anomalies in the behavior of nodes within a graph by analyzing the patterns of node features or the connectivity between nodes. This method is useful in solving the problems of detecting fraudulent activities or supervising 'zombie' accounts on social networks. As an illustration, Liu. et al. propose a method based on attention mechanisms to find out fraud transactions on e-commerce platforms [24]. Such methods based on attention mechanisms are frequent on NLAD tasks [39, 42]. Tang. et al. accomplish GLAD tasks from the spectral domain and successfully achieve excellent results on datasets with a vast multitude of nodes and edges [39]. However, NLAD focuses on individual nodes but overlooks the global patterns from the entire graph.\nGLAD strives to pinpoint anomalous graphs in a series of graphs. The graph-level anomalies may arise due to irregularities in topological structures and node behaviors, or the relationships represented by the edges. The traditional methods for GLAD employ a two-stage approach to capture anomalies at the graph level. For instance, employing the FGSD [41] to generate spectral distance histogram vector representations for each graph, which are then fed into conventional binary classification detection techniques such as IF, LOF, and OCSVM [1, 25, 36]. These two-stage methods, with mutually independent phases, impose limitations on the model's ability to capture intricate features effectively. Consequently, these conventional methods exhibit suboptimal performance when confronted with datasets exhibiting complex features. Besides, Zhao."}, {"title": "2.3 Counterfactual Learning", "content": "The concept of counterfactual was initially introduced in the field of psychology [14, 18, 19, 34]. Counterfactual describes events that do not really exist and contradict factual world knowledge [17, 21]. In other words, counterfactual thinking involves considering a false premise about an event that has already occurred, leading to a conclusion that is contrary to the established facts [38]. For example, if the established fact is that a worker missed the subway to work, the counterfactual would be, \"If he had arrived earlier, he could have caught the subway.\" By varying certain preconditions, the result may be different from the actual facts. As a result, more and more scholars in artificial intelligence have considered applying this psychological pattern to their research. This approach can improve the models' interpretability to a certain extent [8, 10, 38]. The interpretability can help the researchers understand the reasons for the decision of their model [2]. Furthermore, Chen. et al. utilize counterfactual theory to process image datasets in the visual question answering domain, augmenting minority samples and capturing the rich inherent information of the majority classes [4]. Since counterfactual theory involves altering conditions to influence outcomes [31], Yang. et al. employ counterfactual theory to perturb the features of graphs, generating samples with features similar to the original samples but with opposite semantic labels, enabling contrastive learning [44]. This method not only preserves the original samples' information but also identifies the most discriminative features that largely determine the sample labels through feature perturbation. In our work, we employ counterfactual learning to generate anomalous samples to address the issue of imbalanced anomaly samples. Since counterfactual learning aims to generate samples that are similar to normal samples but with opposite labels, it can supplement the scarce anomaly samples."}, {"title": "3 PROBLEM DEFINITION", "content": "The given dataset G = {G1, G2 ..., GN} consists of two mutually disjoint subsets: the set of normal samples Gnor and the set of abnormal samples Gabn. Thus the relation of these sets is: G = GnorU Gabn. Furthermore, Gabn can be divided into two subsets based on whether the abnormal samples are original anomalies Gori or generated anomalies Ggen. Similarly, we have Gabn = Gori UGgen. For every graph Gi = {Vi, Ei, Di, Xi, Ai} \u2208 G, Vi = {v1, v2, ..., vn} represents the nodes in the corresponding graph, and Ei ={e1, e2, ..., em} is the set of edges connecting the nodes. Di ={d1, d2, ..., dn} \u2208 Rn\u00d71 donates degree attributes that describe the number of edges connected to each node, Xi = {x1, x2, ..., xn} \u2208 Rn\u00d7h corresponds to the matrix of node features. Ai \u2208 Rnxn denotes the adjacency matrix, which is typically binary, with the value of 1 indicating the presence of an edge. The samples generated by counterfactual learning are similar to normal samples but have opposite labels, i.e., generated abnormal samples Ggen. Based on the dataset G, we aim to complete GLAD tasks by constructing a model that can calculate each graph's final output score O. If O is closer to 1, the input graph is more likely to be abnormal, while it is closer to 0, the input could be normal."}, {"title": "4 METHOD", "content": "In this part, we will deliver a detailed delineation of the methodology for the IGAD-CF framework. The overall framework is shown in Figure 2. First, we will detail the process of utilizing an anomaly sample generation module to supplement the imbalanced dataset. Next, an introduction to how we extract more effective node features and assign them adaptive weights will be shown. Finally, we will present the delineation of anomalous graphs and introduce a complete loss function with hyperparameters."}, {"title": "4.1 Anomaly Sample Generation Module", "content": "In the task of GLAD, anomalous samples from the dataset typically represent the minority class. This implies that the majority of effective information in the dataset represents normal samples, leading to a lack of anomalous information. The absence of anomalous features in large datasets makes it challenging for machine learning methods to learn rich features, resulting in poor generalization and recognition capabilities. Therefore, generating anomalous samples with abundant anomalous information to supplement the original data is crucial. The CGC [44] method leverages counterfactual theory to perturb given samples, generating samples similar to the originals but with opposite labels. Inspired by the CGC method, we employ this approach to generate anomalous samples. Since our problem is essentially a binary classification task, generating samples with opposite labels from the majority class can effectively supplement the minority class. Moreover, this method can construct samples that retain rich information, enabling the generation of efficient and near-realistic abnormal samples. The core of this generation method is to obtain two perturbation matrices for modifying the structures and node features of normal graphs. Specifically, we apply structural perturbation matrix Ma and feature masking matrix M\u2081 to every single graph in the selected set Gsel \u2286 Gnor, the number of Gsel is equal to the gap between Gnor and Gori. Gj represents the j-th graph in Gsel. We initialize a trainable adjacency matrix Ma, which is a perturbation matrix trained under the guidance of every adjacency matrix Aj of Gj. During training, we multiply Ma with the Aj and then apply a threshold function to binary the result into a new 0/1 adjacency matrix A'.\n\\(A'_{j} = \\mathbb{I}(\\text{sigmoid}(M_a \\times A_j) \\geq \\sigma).\\)\n(1)\nThe function \\(\\mathbb{I}(\\cdot)\\) here donates an indicating function that returns 0 or 1 depending on the threshold \\(\\sigma\\). Next, we initialize another trainable feature masking matrix M_b, which is also produced via a threshold function depending on the threshold \\(\\tau\\) to obtain a 0/1 mask matrix M\u2081. We applied an element-wise multiplication of the binary mask M\u2081 and the feature matrix Xj from Gj to obtain the masked feature matrix X'. In other words, X' is obtained by taking the Hadamard product of M\u2081 and Xj. The process can be formalized as:\n\\(M_l = \\mathbb{I}(\\text{sigmoid}(M_b) \\geq \\tau),\\)\n(2)\n\\(X'_{j} = M_l \\bigodot X_j.\\)\n(3)\nThen, the overall loss comprises two principal subparts based on the counterfactual reasoning mechanism: maximizing similarity to the original samples and enforcing dissimilarity in labels. We first calculate the Frobenius norm between the original adjacency matrix Aj and the perturbed A'j and that of the mask matrix Ml. Then we combine them as the first part of the overall loss during the generation process LG1:\n\\(L_{G1} = ||A_j - A'_{j}||_F - ||M_l||_F,\\)\n(4)\nwhere the function calculates the Frobenius norm inside. Subsequently, we evaluated the probability distributions of the original and perturbed graphs using the KL-Divergence function \\(D_{KL}(\\cdot)\\). This builds the second part of the overall loss LG2.\n\\(L_{G2} = D_{KL}(p, p_o) + D_{KL}(p, p_b).\\)\n(5)\nFinally, the two loss terms were combined to satisfy the counterfactual reasoning mechanism, thereby we get the final loss of this generation module Lcoun.\n\\(L_{coun} = L_{G1} - L_{G2}.\\)\n(6)\nAfter training, we applied the learned perturbation matrix Ma and mask matrix M\u2081 simultaneously to the sample Gj, perturbing both node features and edges, to supply the minority to the same size as the majority.\nThus, the two classes can have the same number of training samples, which can help avoid model bias when classifying positive and negative samples in training. So that our model can better capture the features from normal and anomalous samples, and improve the overall performance on detecting anomaly, so as not to bias the class that is dominant in data volume. In short, this module contributes to enhancing the overall generalization and performance of our model."}, {"title": "4.2 Node Feature Learning Module", "content": "Existing GCN methods may focus more on the impact of node features themselves on node anomalies [23] while overlooking certain structural factors. In our experiments, we find that when using node features for anomalous graph learning, incorporating degree attributes as corresponding structural information into the model can complement the information learned by the model, enabling it to learn more comprehensive and effective anomalous information. This approach can effectively pinpoint the feature information of anomalies. When constructing the Node Feature Learning Module, we use the method of combining the node feature and the degree feature, making the features complementary.\nTo fully leverage node features and degree features, as well as uncover potential relationships between them, we capture these features separately after data balance. Thereafter, we concatenate them in a principled manner to capture their intrinsic connections, thereby enhancing the accuracy of the GAD task. To be exact, during the data preprocessing stage, we constructed two sets of input features: Node attribute feature matrices X and degree attributes D. To utilize X and D better, we designed an encoder model based on GCNs. This model first encodes the node features X and degree attributes D separately to get corresponding representations from node features Zx and that from degree attributes ZD:\n\\(Z_X = GCN_X(X, A),\\)\n(7)\n\\(Z_D = GCN_D(D, A),\\)\n(8)\nwhere A is the graph adjacency matrices, encoding the aggregated attribute and degree features of neighboring nodes, respectively. Next, Zx and ZD are concatenated in the last dimension as:\n\\(Z = Concat(Z_X, Z_D, dim = -1),\\)\n(9)\nwhere Concat() represents the operation of concatenating two matrices on the dimth dimension, and Z denotes the concatenated feature that fuses the node attribute and structural topology information. Thus Z is capable of capturing the intrinsic associations between them."}, {"title": "4.3 Adaptive Weight Learning Module", "content": "Given that, each graph has multiple feature information, and different feature information holds varying degrees of importance for the model's training. We introduce an Adaptive Weight Learning Module W, which can adaptively focus on the information with higher assigned weights. W can aid in determining which features can help the model make predictions to a greater extent. Such appropriate weight allocation becomes particularly important after combining the multiple node features and degree features. Specifically, we first sort the obtained feature Z based on the L1 distance di of each node to the origin:\n\\(d_i = ||x_i||_1, \\forall i \\in {1, 2, ..., n},\\)\n(10)\nwhere the function calculates the L1 norm of each node xi. Then, we sort Z in descending order based on di, obtaining the sorted feature Zsorted.\n\\(Z^{sorted} = Z[:, sort(-d), :], d = [d_1, d_2, ..., d_n].\\)\n(11)\nThis is to prioritize the processing of node vectors that are farther away from the origin, as these node features are more prominent, making it more likely for them to be assigned higher weights in the subsequent weight allocation. Then we map Zsorted with a linear layer to reduce its dimension and get Z'. Finally, we multiply Z' by the weight matrix W, obtaining a feature Zweighted that is adaptively weighted.\n\\(Z^{weighted} = Z' \\times W.\\)\n(12)"}, {"title": "4.4 Loss Function", "content": "After obtaining the weighted feature, we utilize an effective loss function for our model. Before feeding the balanced samples to the model, we divide the normal and abnormal samples, with the first half being normal samples and the others being abnormal samples. Based on the division, we split the loss after each forward passes into the loss Lnor from normal samples and the loss Labn from all abnormal samples, including the original abnormal samples and the generated abnormal samples. Lnor,i and Labni represent the i-th loss of normal or abnormal samples, Nnor and Nabn donate the number of normal samples and the number of all abnormal samples. To this end, we first construct an initial loss Linitial formula:\n\\(L_{nor} = \\frac{1}{N_{nor}}\\sum_{i=1}^{N_{nor}} log(1 - L_{nor,i}),\\)\n(13)\n\\(L_{abn} = \\frac{1}{N_{abn}}\\sum_{i=1}^{N_{abn}} log(L_{abn,i}),\\)\n(14)\n\\(L_{initial} = L_{nor} + L_{abn}\\)\n(15)\nBenefiting from the anomaly sample generation module, these two-class samples are well balanced. However, considering that the generated abnormal samples used are not from the original data, we introduce a hyperparameter \\(\\beta\\) to limit the influence of these generated abnormal samples during training. Furthermore, the weights assigned to generated abnormal samples in the final loss calculation should be determined by their proportion within all abnormal samples. Thus, we first calculate the ratio a of generated abnormal samples Ngen to the number of all anomaly samples Nabn. After that, we divide the loss of all anomaly samples into two parts according to a.\n\\(a = \\frac{N_{gen}}{N_{abn}}\\)\n(16)\nWe multiply the loss from generated abnormal samples by the hyperparameter \\(\\beta\\). This way, we achieve manual control over the degree of influence of generated abnormal samples on the model. The L{ori} represents the loss from the original abnormal samples and Lgen represents the loss from the generated abnormal samples, Nori and Ngen donate the number of original samples and the number of generated abnormal samples, and Lori,i and Lgen,i represent the i-th loss of original or generated abnormal samples.\n\\(L_{ori} = \\frac{1}{N_{ori}}\\sum_{i=1}^{N_{ori}} log(L_{ori,i}),\\)\n(17)\n\\(L_{gen} = \\frac{1}{N_{gen}}\\sum_{i=1}^{N_{gen}} log(L_{gen,i}).\\)\n(18)\nThus, Labn is decomposed into two components, Lgen and Lori, such that Labn = Lori \u00d7 (1 - a) + Lgen \u00d7 \u03b2 \u00d7 a. The initial formula is upgraded to calculate the final loss Lfinal:\n\\(L_{final} = L_{nor} + L_{ori} \\times (1 - a) + L_{gen} \\times \\beta \\times a.\\)\n(19)"}, {"title": "4.5 Graph Anomaly Detection Module", "content": "After completing the model's training, the test datasets are applied to evaluate the model's accuracy. In our graph anomaly detection module, we first apply the node feature learning module to the received test data to handle the adjacency matrix, node features, and degree attributes. Subsequently, the Adaptive Weight Learning Module is employed to the results from the previous step, obtaining the Zweighted. Then, a linear transformation maps the results to a single value, which is normalized to the range of 0 to 1. Specifically, we first applied a non-linear transformation to the obtained ZR using the ReLU() function.\n\\(Z_R = ReLU(Z^{weighted}).\\)\n(20)\nSubsequently, we use a fully connected layer for processing a result U, mapping the features to a scalar value.\n\\(U = W_a Z_R + b_d.\\)\n(21)\nIn this process, Wa represents the weight and ba is the bias of the fully connected layer. Finally, we utilized the Sigmoid function \u03c3(\u00b7) to constrain the output O, ensuring the result falls within the range of 0 to 1, facilitating the interpretation of whether the input graph is normal or not.\n\\(O = \\sigma(U).\\)\n(22)\nThen we apply a heaviside step function H(.) to the output O, when the result exceeds the certain threshold t, it indicates a higher likelihood of the graph being anomalous, and vice versa. In our approach, we set the default threshold to 0.5.\n\\(\\mathbb{H}(O - t) =\\begin{cases}1, & \\text { if } Ot>0; \\\\0, & \\text { otherwise. }\\end{cases}\\)\n(23)"}, {"title": "5 EXPERIMENTS", "content": "In order to assess the efficacy of our framework on datasets with higher recognition, we selected 8 real-world anomaly detection datasets from various fields [32, 33]. Initially, we chose commonly used datasets for graph anomaly detection, such as AIDS, BZR, COX2, DHFR, and NCI1. These are typical datasets of small molecules with relatively smaller amounts of graphs, usually constructed based on abstract atoms of each molecule as nodes, and abstract chemical bonds between atoms as edges. Additionally, we selected the ENZYMES dataset from bioinformatics, where the graphs represent proteins, with amino acids as nodes and edges constructed based on their spatial distances and states. Furthermore, to verify the situation in actual social networks, we used the IMDB-BINARY dataset for experiments, where each independent individual is abstracted as a node, and edges are added to connect any two individuals with a collaborative relationship. Finally, we included the hERG dataset from biological joint molecules to test our model's performance. The number of graphs, the average number of edges, and the average number of nodes in these datasets are shown in Table 1. When preprocessing the datasets, we select these data samples which are labeled 1 as the anomalous instances, and the rest as normal instances. During the data balancing process, we perform sample generation on the minority class samples within the dataset. For some datasets (e.g., AIDS, ENZYMES, HERG) where abnormal samples outnumber normal samples, we similarly employ the counterfactual approach to construct normal samples, thereby balancing the datasets."}, {"title": "5.2 Baselines", "content": "To showcase that our proposed method is not only an improvement over traditional detection methods but also shows advantages over newer methods, we first choose the traditional methods based on conventional binary classification detection techniques:\n\u2022 FGSD-LOF is a traditional anomaly detection approach that utilizes a two-stage method, which is composed of two parts, the FGSD [41] algorithm to calculate the distances between node vectors and the LOF [1] to classify the dataset basing on the distances. LOF detects the abnormal by comparing each instance's local density to the neighbors.\n\u2022 FGSD-IF has a similar structure, while the detector is replaced with IF [25], which makes isolating observation by random feature selection and value splitting.\n\u2022 FGSD-OCSVM is driven by the FGSD distance algorithm as well. Its detection core OCSVM [36] identifies the anomalies by their distance from the hyperplane.\nThen we select some deep learning methods, to exhibit the superiority of IGAD-CF:\n\u2022 OCGTL [27] is a method that has demonstrated excellence in graph anomaly detection within the domains of bioinformatics, small molecules, and social networks. OCGTL absorbs the strengths of one-class classification and transfer learning, achieving graph embeddings from multiple perspectives by integrating various GNNs, thereby effectively utilizing deep learning for GAD tasks.\n\u2022 GLocalKD [30] is a GCN-based method which employs the concept of stochastic knowledge distillation, adeptly capturing both local and global information in graphs.\n\u2022 GOOD-D [26] is a representative of unsupervised contrastive learning, which captures the latent patterns of in-distribution (ID) graphs by contrastive learning.\n\u2022 SIGNET [27] is an explainable unsupervised learning method based on a Multi-View Subgraph Information Bottleneck. It can capture the substructures to predict the agreement between two views.\n\u2022 GLADST [23] is constructed of dual-students-teacher model which can differentiate between normal and abnormal samples to a greater extent."}, {"title": "5.3 Parameter Settings", "content": "In our IGAD-CF, we apply two GCN layers with d = 256 \u2192 128 dimensions, where d represents the dimension size of the features. The hyperparameter \u03b2 is 1.2 on all the datasets, except for BZR (\u03b2 = 0.6) and DHFR (\u03b2 = 1.4). The learning rate for most datasets is 0.001, while it is 0.0001 for AIDS and NCI1. We perform the experiments"}, {"title": "5.4 Anomaly Detection Performance", "content": "In order to show the robust performance of IGAD-CF on GLAD tasks across most scenarios, we conduct comparative analyses with all the mentioned baselines on the mentioned datasets. During the experimental phase, we employ a five-fold cross-validation approach to ensure the credibility of our results. Subsequently, we record the average value of AUC and standard deviation results post-testing to reflect the IGAD-CF's accuracy and stability.\nThe top-performing approach for each dataset is highlighted in bold font within Table 2, while the second-best method is underlined. On the AIDS dataset, the standard deviation of IGAD-CF is slightly lower than GLADST, but the mean AUC of IGAD-CF and GLADST are both the optimal cases on this dataset. Apart from the AIDS dataset, IGAD-CF demonstrates superior performance on the remaining seven datasets, and in most of these datasets, IGAD-CF achieves an AUC improvement of at least 4% compared to the second-best method. Notably, on the IMDB-BINARY dataset, the improvement is as high as 14.85%, indicating that our method can capture anomalous information more effectively and classify the datasets better. Overall, it can be observed that IGAD-CF not only achieves a higher mean AUC value but also exhibits relatively low standard deviations across all datasets, with all standard deviations being less than 5%. The largest standard deviation occurs on the hERG dataset, at only 4.31%. This suggests that our model can more consistently achieve superior performance and stronger generalization capabilities when faced with different data groupings."}, {"title": "5.5 Ablation Study", "content": "Apart from carrying out baseline comparison experiments on the public datasets, we systematically deconstruct the model and conduct a series of ablation studies on diverse datasets, including AIDS, BZR, COX2, DHFR, and NCI1, to elucidate the impact of each component. First, to showcase the effectiveness of our generated abnormal samples, we delete the Anomaly Sample Generation Module (w/o ASGM) and directly train the entire model with the imbalanced datasets. Second, we omit the Adaptive Weight Learning Module (w/o AWLM) to show the improvement from proper weight assignment. Then, to demonstrate the performance optimization brought about by the complementarity of node features and degree attributes, we remove the GCN that processes node features (w/o GCNX) and the GCN that processes degree attributes (w/o GCND) respectively. Finally, our losses are obtained by combining losses from abnormal samples Labn with losses from normal samples Lnor, so we eliminate each part of them one by one (w/o Labn and w/o Lnor) to ascertain the significance of each loss. For each ablation model, we remove the mentioned parts only and leave the remaining parts unchanged for targeted analysis. The result is exhibited in Table 3.\nThe results indicate that the entire IGAD-CF consistently exceeds the ablated models on these selected datasets except for AIDS in which every method produces a similar score. The suboptimal performance of our entire model on the AIDS could be attributed to the relatively small number of edges and nodes, resulting in simpler structures. In such cases, the node features or degree attributes alone may be sufficient to capture the necessary information, leading to generally good performance across ablation models. Moreover, the highest average AUC achieved by the ablation model without GCND exceeded that of the entire model by only 0.01%, which is within an acceptable fluctuation range. For the"}, {"title": "5.6 Available Feature Analysis", "content": "Due to incomplete node features in some datasets, when we apply the methods to the datasets mentioned, we disregard the node attributes provided in the original datasets and instead derive node features solely from the graph adjacency matrices. For each adjacency matrix A\u00a1 from a graph, we explore three different approaches to construct the corresponding node feature matrix [7]. Identity Encoding: Each node is transferred by a one-hot encoding vector. Formally, a node feature matrix X\u00a1 is constructed by repeating the identity matrix I\u00a1 N times along the diagonal, where N represents the nodes' number in the graph. Degree Binning (DB): After calculating each node's degree, these degrees D are discretized into predefined bins. During the processing,a histogram where each bin corresponds to a range of degrees is created. Then we encode the nodes' degrees to put them into the bins respectively.\nLocal Degree Profile (LDP): This approach uses both the individual node's degree and statistical properties of the degrees of the neighbors to construct node features.\n5.6.1 Performance Comparison. By exploring these three distinct methods for constructing node features from the adjacency matrices, our aim is to examine the impact of varying node features and their impact on the performance of IGAD-CF. The results are displayed in Table 5. For most cases, the identity method outperforms others, with the LDP method generally yielding the least favorable results. This suggests that the identity method is the most effective in highlighting the distinctive attributes of nodes.\n5.6.2 Validity Verification. To provide validation for the efficacy of our feature construction approach across various datasets, we also conduct comparative experiments with the baseline methods GLocalKD, GOOD-D, and GLADST on six datasets. Specifically, we train the baseline models on the selected datasets using the initial node features from the datasets or the node features constructed via Identity Encoding separately. Subsequently, we evaluated the trained models and recorded the corresponding results. As shown in Table 4, except for the ENZYMES dataset where our constructed node features performed slightly weaker than the original node features, our method exhibited significantly superior performance on the other datasets."}, {"title": "5.7 Visualization Analysis", "content": "To visually reflect the classification performance of our model on normal and abnormal samples from a more intuitive perspective, we visualized the anomaly scores of the node identification test set samples through bar charts. Here we choose AIDS, DHFR, and NCI1 for verification. GLADST and GLocalKD are selected to compare with our model. The results are shown in Figure 3.\nThe abscissa of these histograms represents the anomaly scores, and the ordinate is the number of corresponding samples with anomaly scores falling within that range. An anomaly score that is closer to 1 means a higher likelihood that this object is an abnormal sample, while a score closer to 0 suggests that the sample is normal. These histograms reveal that, for the majority of cases, the abnormal and normal samples tend to cluster on opposite ends of the"}, {"title": "5.8 Parameter Analysis", "content": "As outlined in the method section, we introduced a hyperparameter, \u03b2, to manually control the impact of our generated abnormal samples in the framework's final decision-making process. In this section, we will focus on investigating its impact on model performance, and observe whether the optimal \u03b2 changes in different datasets. Thus, we have selected four datasets (AIDS, BZR, COX2, DHFR, and ENZYMES) for our experimental evaluation.\nIn Figure 4, the abscissa represents the values of \u03b2, while the ordinate corresponds to the AUC values. The graph illustrates that variations in beta between 0.2 and 2.2 have a slight impact on the model's performance. Notably, when \u03b2 is set between 1.2 and 1.4 or between 0.6 and 0.8, the overall effect is more favorable."}, {"title": "5.9 Case Study for Disorder Detection", "content": "In addition to achieving satisfactory results on usual graph anomaly detection datasets, we extend the downstream applications of IGAD-CF by applying it to the identification of abnormal brain"}, {"title": "5.9.1 Datasets from Brain Graphs", "content": "We select two brain disorder datasets, human immunodeficiency virus infection (HIV) and bipolar disorder (BP), each encompassing two neuroimaging modalities functional diffusion tensor imaging (DTI) and magnetic resonance imaging (fMRI) [6]. Consequently, four datasets are formed: HIV-DTI, HIV-fMRI, BP-DTI, and BP-fMRI. We construct the brain graphs by employing previously established techniques from brain network studies [5, 6]. Specifically, for each subject in the datasets, the neuroimaging data is used to construct a brain graph. The nodes from the constructed graph represent distinct brain regions, and the edges represent the structural or functional relationships between these regions. Each HIV dataset comprises a total of 70 samples, evenly distributed between normal and anomalous cases. For every graph in the HIV dataset, there are 90 valid brain regions, leading to the construction of 90 nodes. Each BP dataset contains 97 samples in total, with 52 representing anomalous brain graphs and the remaining 45 being normal cases. Each sample in the BP dataset consists of 82 nodes."}, {"title": "5.9.3 Performance Analysis", "content": "We conducted experiments on the brain graph datasets. Similar to previous studies on classic anomaly detection datasets, we record the mean AUC and standard deviation for evaluation. The final results are presented in Table 6, which showcases that IGAD-CF significantly outstrips the two baselines across all datasets. Moreover, our method demonstrates markedly superior stability compared to the other methods in the HIV datasets. The satisfying result further improves the feasibility of applying our approach to the detection of brain diseases."}, {"title": "6 CONCLUSION", "content": "In our proposed work, to address the key challenges in the GLAD task, we design the IGAD-CF framework. To resolve the performance degradation caused by sample imbalance, we construct an anomaly sample generation module that leverages counterfactual reasoning mechanisms to generate anomalous samples, thereby enriching the datasets. Furthermore, to address the issue of existing methods not fully utilizing degree attributes and node features, our node feature learning module innovatively integrates the two, allowing for complementary information exchange and presenting more comprehensive information. Subsequently, we employ an adaptive weight matrix to emphasize the extracted graph features at different levels of importance. Finally, we carry out a comprehensive set of experiments that not only demonstrate our model's outstanding performance on traditional public datasets but also exhibit its significant potential in brain graph anomaly detection."}]}