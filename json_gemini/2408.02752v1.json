{"title": "Diffusion Models as Data Mining Tools", "authors": ["Ioannis Siglidis", "Aleksander Holynski", "Alexei A. Efros", "Mathieu Aubry", "Shiry Ginosar"], "abstract": "This paper demonstrates how to use generative models trained for image synthesis as tools for visual data mining. Our insight is that since contemporary generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns. Concretely, we show that after finetuning conditional diffusion models to synthesize images from a specific dataset, we can use these models to define a typicality measure on that dataset. This measure assesses how typical visual elements are for different data labels, such as geographic location, time stamps, semantic labels, or even the presence of a disease. This analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches since it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focus on a single dataset, our approach works on diverse datasets in terms of content and scale, including a historical car dataset, a historical face dataset, a large worldwide street-view dataset, and an even larger scene dataset. Furthermore, our approach allows for translating visual elements across class labels and analyzing consistent changes. Project page: https://diff-mining.github.io/.", "sections": [{"title": "1 Introduction", "content": "Visual data mining aims to discover patterns within large visual corpora such as collections of street view panoramas [12,23], historical images of faces [9,13] or photographs of cars [10,24]. This paper proposes a novel idea: to turn generative models trained for image synthesis into a scalable method for visually mining image datasets. Generative models digest massive amounts of data, which they implicitly store in their weights. Our central insight is that we can use this learned summary of the visual input to identify the most typical image regions. This unconventional use of a diffusion model for studying its training data demonstrates that generative models are potent tools beyond synthesis for data mining, summary, and understanding.\nOur target task, mining for informative visual patterns, is challenging. Unlike text, where words act as discrete tokens that we can directly compare, the visual world seldom contains exactly repeating elements. Even common simple visual elements, such as windows, can have different colors and different numbers of panes; they may be seen from various viewpoints, and they may be located at multiple positions as part of different facades. The standard approach to visual data mining [12, 24, 40] involves learning data-specific similarities with relevant invariances (e.g., such that different-looking windows will be similar) and using them to search for discriminative patterns. However, these techniques are not easily scalable since one must apply them across all pairs of visual elements within all pairs of images in the dataset. The similarity graph between visual elements scales quadratically with the size of the dataset. In contrast, our proposed analysis-by-synthesis approach does not require pairwise comparisons between different visual elements and thus scales to very large datasets.\nThe approach we propose takes as input a dataset with image-level tags, such as time [9,24], geography [28], or scene labels [49]. Our goal is to provide a visual summary of the elements typical of the different tags, such as the common elements that enable us to determine the location of a streetview panorama. To arrive at this summary, we first finetune a conditional diffusion model on the target dataset. We then use the finetuned model to define a pixel-wise typicality measure by assessing the degree to which the label conditioning impacts the model's reconstruction of an image. We mine visual elements by aggregating typicality on patches, selecting the most typical ones, and clustering them using features extracted from the finetuned model [44]."}, {"title": "2 Related Work", "content": "Visual data mining. Visual data mining turned the manual and subjective process of comparing photographs (e.g., [22]) into algorithmic methods for summarizing image data, such as architectural details [12, 23], fashion [9, 13, 30], industrial design [17], and art [19,39,41] by locating visual patterns. This has mainly been achieved using discriminative techniques such as clustering or contrastive learning. For example, [24] demonstrated how correspondence based mining across time can be achieved in a dataset of objects of similar parts, namely cars, and [12] showed that geographically representative image elements can be automatically discovered from Google Street View imagery in a discriminative manner. However, such traditional data mining approaches do not scale to large modern datasets. Indeed, they require pairwise comparisons between all the visual elements of each image to the entire dataset in order to locate nearest neighbors and establish clusters. Notably, the discriminative clustering algorithm of [12] requires training a separate linear SVM detector for each visual element- a computationally prohibitive approach when considering multiple possible visual elements for the purposes of analysis. In contrast, our approach is scalable to very large datasets. Closer to our work, generative model have been trained to analyze the evolution of faces [9] and cars [10] across time. However, these two works essentially perform image translation, and do not enable actual mining of typical elements in the datasets.\nDiffusion models. Diffusion models have gained popularity in recent years due to their stability in training and effectiveness in modeling complex multimodal distributions [11, 15, 16, 20, 42]. These models are capable of generating high-quality imagery conditioned on input signals beyond categorical labels, like text [35,36,38], and can further incorporate additional modalities [26, 48]. In addition to generating images from scratch, diffusion models have been used extensively for instruction-driven image-to-image translation [7, 14, 32, 33, 45]. It has also been shown that pre-trained text-to-image diffusion models encode strong priors for natural scenes, allowing their internal features to be used for secondary tasks [29,44,47]. They can easily be adapted for new tasks or to new data distributions through minimal finetuning [7,37,48].\nBeyond mere image synthesis, generative image models, and in particular diffusion models, have been used as data augmentation engines. While most"}, {"title": "3 Data Mining via Diffusion Models", "content": "Our approach turns generative models into data mining tools. It relies on finetuning a conditional stable diffusion model trained for image synthesis, using it to extract a summary of the visual world. We start by reviewing diffusion models and the techniques we leverage in section 3.1. In section 3.2, we introduce our measure of typicality, which allows us to measure how the class label conditioning affects the synthesis of an image by the diffusion model. In section 3.3, we describe how we aggregate typicality on patches to mine typical visual elements and cluster them to summarize the training data."}, {"title": "3.1 Preliminary", "content": "Diffusion models. Diffusion models are generative models trained to transform random noise $e \\sim N(0, 1)$ into a target distribution $X$. The denoising process is iterative, indexed by a step index $t$. A diffusion model $e_e(z, t)$, with parameters $0$, takes as input an image $z$ to be denoised at the fractional timestep $t$.\nDuring training, a training sample $x$ is artificially noised at a strength associated to a uniformly sampled step $t$, by mixing the image with a randomly sampled Gaussian noise image $e$ in what is known as the forward process:\n$\\text{noise}(x, e,t) = \\sqrt{\\bar{a}_t} x + (1 - \\sqrt{\\bar{a}_t})e$,\nwhere $\\sqrt{\\bar{a}_t}$ defines the noise mixing coefficient which varies over the denoising process. The learnable denoising model $e_{\\theta}$ takes as input both a noised image and the corresponding noising step and is trained to predict the noise image (or equivalently the denoised image) using a loss:\n$L_t(x, e) = ||e_{\\theta} (\\text{noise}(x, e,t), t) - e||^2$.\nAt test time, the target distribution can be sampled by transforming the noise distribution through an iterative denoising process [15,43], in which a randomly sampled image of noise $z_T$ is gradually denoised according to the model's predictions.\nConditional Diffusion Models. Diffusion models can be extended to take a conditioning $c$ associated with the image content as an additional input. This leads to a model $e_e(z,t,c)$ that depends on the noisy image $z$, the time step $t$,"}, {"title": "3.2 Typicality", "content": "We design our measure of typicality based on the following intuition: a visual element is typical of a conditioning class label (e.g., country name or date) if the diffusion model is better at denoising the input image in the presence of the label than in its absence. We, therefore, design typicality as a ranking measure across pixels between the ground-truth conditioning $c$ and the null conditioning $\\O$. We define the typicality of an image $x$ given the class label conditioning $c$ as:\n$T(x|c) = E_{e,t}[L_t(x, e, \\O) - L_t(x, e, c)]$,\nwhere $t$ is sampled uniformly from $[0, 1]$, and $e$ is sampled according to the noise distribution $N(0, 1)$. This typicality measure enables us to sort visual elements from a specific class by how typical they are of that class (see supplementary material for additional formal motivation of this measure). Our typicality measure is related to the image-level classification approach of Li et al. [25], but it is built for pixel-based analysis and data mining. Unlike Li et al., we find that reducing the sampled range of $t$ to $[0.1, 0.7]$ improves the quality of our results, as the tails can contribute uninformative yet typical samples, as we show in the supplementary material."}, {"title": "3.3 Mining for Typical Visual Elements", "content": "Conditioning and finetuning. To mine typical visual elements for a given class, we use the text class label conditioning $c$ in the form of its CLIP text features [34]. We convert the tags associated with the datasets to text using the embeddings of the following sentences: \u201cA car/portrait from the {decade}s.\" for faces and cars (\"A car/portrait.\" for the null conditioning $\\O$), \u201cA Google streetview image of {country}.\u201d for streetview data (\"A Google streetview image.\u201d for the null conditioning $\\O$), and \"An image of {scene}.\u201d for images of the Places dataset [49] (empty string for the null conditioning $\\O$). We finetune a latent diffusion model [36] on the target dataset by optimizing the reconstruction loss (Equation 2) given the conditioning. We use Stable Diffusion V1.5 [36] as a base model in all our experiments.\nPatch-based analysis. To find condition-specific visual elements, we compute our typicality scores over patches of images by averaging typicality in the area of a patch\u00b3. To identify the set of most typical visual elements for a dataset we pick the 5 most typical non-overlapping patches in each image according to the patch typicality, and select the 1000 most typical patches over all the dataset.\nClustering visual elements. We cluster the most typical patches using k-means [27] with 32 clusters. To cluster elements, we embed them with DIFT [44] features, computed at timestep $t = 0.161$ using our finetuned models. For visualization, we rank clusters by the median typicality of their elements in decreasing order and the elements within a cluster by the distance to the centroid in increasing order."}, {"title": "4 Experiments", "content": "We showcase the effectiveness of our approach in summarizing visual data for a wide variety of datasets. First, in Section 4.1, we introduce the datasets used in our experiments. Second, in section 4.2, we evaluate the ranking given by our typicality measure. Third, in Section 4.3, we discuss our main result, the mined visual summaries of the analyzed datasets, and compare with Doersch et al. [12]. Finally, we discuss the limitations of our approach in Section 4.4."}, {"title": "4.1 Datasets", "content": "We experiment with four diverse datasets. CarDB [24] and FTT [9] have already been used for visual mining and include a few tens of thousands of images. G^3 [28] and Places [49] are much larger with 344K and 1.8M images respectively, and, to our knowledge, have never been used for visual mining.\nCars. The CarDB dataset [24] contains 10,130 photos of cars from 1920 to 1999, collected from cardatabase.net. They are labeled with creation years, which we bin into decades for our analysis. This dataset contains cars seen from various viewpoints and in diverse environments. As a result, extracting time-informative elements is challenging. We rescale all images to a height of 256 pixels while preserving their original aspect ratio.\nFaces. The Faces Through Time (FTT) Dataset [9] contains 24,874 images of notable people from the 19th to 21st century, with roughly 1,900 images per decade, sourced from Wikimedia Commons. All photos are of size 256x256 pixels.\nGeo. The G^3 [28] dataset contains images obtained from crops of street-view panoramas, diversely sampled worldwide, of which we selected 344,224 images, which we rescaled to 512x756 pixels. This dataset is challenging because of the small details that characterize a scene's appearance and scale. We focus on the 8 countries with the largest number of panoramas (United States, Japan, France, Italy, United Kingdom, Brazil, Russia, and Thailand) and two countries with"}, {"title": "4.2 Typicality Measure Evaluation", "content": "Typicality score for patches. Fig. 2 shows the most and least typical patches according to our typicality measure and random patches for the four datasets. We note that the most typical patches are unique to each class and more discriminative than random patches, while the least typical patches are uninformative of the conditioning label.\nEffect of finetuning. Unsurprisingly, we found that finetuning the diffusion model on the dataset of interest was critical to the quality of our results. First, on a given image, finetuning changes the spatial distribution of typicality, prioritizing elements more correlated with the training labels (see Fig. 3a). Second, in Fig. 3b, we show the most typical clusters identified before and after finetuning. The patches selected after finetuning avoid the biases in the training data of the base model and are more specific to the $G^3$ dataset, identifying elements such as post-boxes. We also demonstrate this quantitatively in Section 5.2 for our application to X-ray images. Third, finetuning enables better translation between labels (see Sec. 5.1), as can be seen in Figure 3c, allowing vegetation, roads, road tracks, and utility poles to be translated consistently across the class labels in the parallel dataset (which can be found in the supplementary material)."}, {"title": "4.3 Clusters of Typical Visual Elements", "content": "In this section, we analyze our visual summary of each dataset, obtained by clustering the typical visual elements for the different class labels. We show our"}, {"title": "4.4 Limitations", "content": "Although our method makes the first step towards utilizing generative models for data mining, it comes with limitations. We visualize our two main failure modes in Fig. 9. First, clustering elements using k-means can lead to mixed clusters containing different categories of samples (Fig. 9a) or produce repetitively similar"}, {"title": "5 Applications", "content": "Our typicality score allows us to explore two different applications. First, in Section 5.1, we translate geographical elements across locations and mine typical translations. Then, in Section 5.2, we show how disease localization emerges from typicality when training to generate frontal X-rays of patients, of various diseases."}, {"title": "5.1 Analyzing Trends of Visual Elements", "content": "Having a diffusion model finetuned on a dataset of interest enables further applications that were not possible with previous visual mining approaches [9, 12, 13, 24]. One new application is the summary of variation of typical visual elements across different class labels. As a case study, we use the $G^3$ dataset to discover and summarize how co-typical elements, such as windows, roofs, or license plates, vary across locations. We start by using our finetuned diffusion model to create a \"parallel dataset\", by translating all the images in our mining dataset to all locations, then define a co-typicality measure.\nGenerating a parallel dataset. We first use Plug and Play [45] to translate input images from one location to another, which we denote by $x^{c_o \\rightarrow c}$, where $c_o$ is the initial country and $c$ is the target country. We translate 1000 images for each of the 10 selected countries to all others, resulting in 100K images, which we refer to as our parallel dataset. Performing translation using our finetuned model is critical for keeping scene elements consistent, as seen in Fig. 3c.\nWe show in supplementary material how performing semantic segmentation for each image and its translations to different countries enables measuring statistical trends. For example, we can measure that translations to Thailand or Brazil add many potted plants, and translations to Nigeria add dirt roads and people. We can visually confirm those trends on our parallel dataset.\nMining typical transformations across location. To further analyze our parallel dataset, we define a cross-location typicality measure to mine parallel translation of patches across locations. We define the co-typicality $T$ as the median typicality across location:\n$T(x) = \\text{med}_c [T(x^{c_o \\rightarrow c}, c)]$,"}, {"title": "5.2 Analysis of Medical Images", "content": "In Section 4.2, we discussed how typicality helps find relevant patches for an input label. In this section, we test this idea on completely different images: X-rays of patients who may suffer from a combination of various thorax diseases. We finetune Stable Diffusion on the ChestX-ray8 dataset [46] containing 108,948 frontal-view X-ray images annotated with 14 single-word disease-name labels. Experts annotated a test set of 879 images with 7 diseases with rectangular regions of interest (ROI) for each disease. For each image, we compute typicality per latent feature, interpolate the resulting typicality to the input dimension, and blur the resulting typicality map for visualization. In Fig. 11, we show the resulting typicality maps together with the ROI annotation before and after finetuning. Finetuning clearly improves the localization. We quantify this effect by computing the area under the precision recall-curve [5] (AUC-PR) associated"}, {"title": "6 Conclusion", "content": "We presented a novel use of diffusion models as visual mining tools. We defined a typicality measure using a pretrained stable diffusion model finetuned for conditional image synthesis. We used typicality to mine visual summaries of four datasets, tagged by year or location. We further showed that we can use our typicality measure to localize abnormalities in medical data and extend it to discover trends in the variations of translated visual elements within a generated parallel dataset. In summary, our work presents a novel approach to visual data mining, enabling scaling to datasets significantly more extensive and diverse than those showcased in prior works, as demonstrated by our experiments."}]}