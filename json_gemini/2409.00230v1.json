{"title": "Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations", "authors": ["Yilin Zhuang", "Sibo Cheng", "Karthik Duraisamy"], "abstract": "Diffusion models have gained attention for their ability to represent complex distributions and incorporate uncertainty, making them ideal for robust predictions in the presence of noisy or incomplete data. In this study, we develop and enhance score-based diffusion models in field reconstruction tasks, where the goal is to estimate complete spatial fields from partial observations. We introduce a condition encoding approach to construct a tractable mapping mapping between observed and unobserved regions using a learnable integration of sparse observations and interpolated fields as an inductive bias. With refined sensing representations and an unraveled temporal dimension, our method can handle arbitrary moving sensors and effectively reconstruct fields. Furthermore, we conduct a comprehensive benchmark of our approach against a deterministic interpolation-based method across various static and time-dependent PDEs. Our study attempts to addresses the gap in strong baselines for evaluating performance across varying sampling hyperparameters, noise levels, and conditioning methods. Our results show that diffusion models with cross-attention and the proposed conditional encoding generally outperform other methods under noisy conditions, although the deterministic method excels with noiseless data. Additionally, both the diffusion models and the deterministic method surpass the numerical approach in accuracy and computational cost for the steady problem. We also demonstrate the ability of the model to capture possible reconstructions and improve the accuracy of fused results in covariance-based correction tasks using ensemble sampling.", "sections": [{"title": "1. Introduction", "content": "The global field reconstruction problem, which involves reconstructing a full field from partial observations, is underdetermined and has long been a challenge across various science and engineering domains [1, 2, 3]. Various numerical and deep-learning methods have been proposed to address this challenge, including Kriging [4], iterative Kalman filtering-based methods [5, 6], Voronoi-tessellation Convolutional Neural Networks (VCNNs) [1], and Physics-Informed Neural Networks [7].\nAmong classical numerical methods for solving the field reconstruction tasks, Gaussian process [8] and its variants, such as the ensemble Kalman filter [9, 10, 11] and extended Kalman filter [12, 5], are commonly used statistical methods for approximating fields using Gaussian kernels. For optimization-based approaches, they are often combined with model reduction techniques to manage high-dimensional fields and perform field reconstruction [5, 13, 14, 15]. However, these methods remain computationally expensive, and the optimization formulation can become intractable for time-dependent PDEs.\nVarious deep learning frameworks have been developed for field reconstruction tasks. The VCNN [1, 16] is a convolutional neural network that uses Voronoi tessellation to map interpolated fields to reconstructed fields. Voronoi tessellation describes a class of interpolation methods that maps point data to a field. Neural operators function by simultaneously learning differential operators and field solutions. Variants such as Physics-Informed Neural Operators (PINOs) [17] and Latent Neural Operators (LNOs) [18] are also capable of solving inverse problems. Another commonly used method is the Physics-Informed Neural Network (PINN) [7], which leverages automatic differentiation to solve PDEs. Several variants of PINNs and automatic differentiation-based methods have been proposed to address inverse problems [19, 20, 21]. These methods are typically deterministic, and uncertainty quantification is often performed by injecting noise into the observations. For a fixed set of observations, fields reconstructed by deterministic methods are fixed and do not support uncertainty quantification.\nGenerative models, derived from probabilistic learning and variational inference, have emerged as a powerful class of methods for generating new samples from data distribution. In the context of field reconstruction, generative models map an initial distribution, typically Gaussian, to the target data distribution [22], conditioned on the observed fields. Previous work has demonstrated that Generative Adversarial Networks (GANs) can reconstruct patches of turbulence data based on observations"}, {"title": "2. Methods", "content": "Consider a two dimensional squared domain $\\Omega\\in \\mathbb{R}^{N_a\\times N_a}$, where $N_a$ is the grid size. Let $x \\in \\mathbb{R}^{N_c\\times N_a\\times N_a}$ denote the fields on $\\Omega$, where $N_c$ is the number of fields.\nWe denote $M\\in \\mathbb{R}^{N_c\\times N_a\\times N_a}$ as the observation matrix with the one-hot encoding:\n\nMi,j,k = \\{\n1 & \\text{if } (j, k) \\in \\text{Observed points}\\\\\n0 & \\text{otherwise}\n\n(1)\n\nHere, we assume that the observed points across all fields have the same coordinates.\nWe also define the unobserved matrix $M^c$ such that $x = (M \\odot x) \\oplus (M^c \\odot x)$, where $\\odot$ and $\\oplus$ denote the element-wise multiplication and addition, respectively. Let $H: \\mathbb{R}^{N_c\\times N_a\\times N_a} \\rightarrow \\mathbb{R}^{N_c\\times N_{obs}}$ denote the observation operator, and we have the observed data as $y = H(x)$.\nLef $\\{S_{c,1}, S_{c,2},..., S_{c,N_{obs}}\\}\\subseteq \\Omega$ denote the set of Voronoi-tessellated field of for the variable $x_c \\in \\mathbb{R}^{N_a\\times N_a}$. Each sub-region $s_{c,i}$ is defined as:\n\ns_{c,i} = \\{x \\in \\Omega \\ | \\ ||x - Pos(y_{c,i})|| \\le ||x - Pos(y_{c,j}) ||, \\forall j \\ne i\\},\nwith $s_{c,i}(x) = y_{c,i}, \\ \\forall x \\in s_{c,i}$.\n\nwhere $Pos$ denotes the position of the observed point for field $x_c$. Let $q$ denote the reconstructed field, and the reconstructions using VT-UNet, unconditional diffusion and conditional diffusion models can be obtained as: $q = F_{VT}(\\{s_{c,i}\\}) $, $q = F_{Diff}(\\epsilon, y)$, and $q = F_{CondDiff}(\\epsilon, \\{s_{c,i}\\}, M\\odot x)$, respectively. Here, we slightly abuse the notation for diffusion models because $q$ is generated through iterative calls to the diffusion model, and $\\epsilon$ denotes the randomized field initialization. The VT-UNet model is trained to minimize the mean squared error, $\\mathbb{E}_{x,y} [||q - x||^2]$\n\nThe forward map of diffusion models is a tractable transformation where noise is gradually added, and the reverse map is approximated by neural networks to generate the reconstructed fields [50]. We denote the data distribution as $\\pi_0$ and the random noise as $\\pi_1 \\sim \\mathcal{N}(0, I)$. Let $x_0$ be the initial data sample. Its intermediate representations $x_t$ at timesteps $t \\in [0,1]$ can be obtained through the following transformation:\n\nxt = a_tx_0 + b_t\\epsilon, \\text{ where } \\epsilon \\sim \\mathcal{N}(0, I),\n\nwhere $a_t$ and $b_t$ are the parameters of the transformation. Here, the timestep $t$ is an artificial notation for describing the mapping between the data distribution"}, {"title": "2.1. Problem formulation", "content": "Consider a two dimensional squared domain $\\Omega\\in \\mathbb{R}^{N_a\\times N_a}$, where $N_a$ is the grid size. Let $x \\in \\mathbb{R}^{N_c\\times N_a\\times N_a}$ denote the fields on $\\Omega$, where $N_c$ is the number of fields.\nWe denote $M\\in \\mathbb{R}^{N_c\\times N_a\\times N_a}$ as the observation matrix with the one-hot encoding:\n\nMi,j,k = \\{\n1 & \\text{if } (j, k) \\in \\text{Observed points}\\\\\n0 & \\text{otherwise}\n\n(1)\n\nHere, we assume that the observed points across all fields have the same coordinates.\nWe also define the unobserved matrix $M^c$ such that $x = (M \\odot x) \\oplus (M^c \\odot x)$, where $\\odot$ and $\\oplus$ denote the element-wise multiplication and addition, respectively. Let $H: \\mathbb{R}^{N_c\\times N_a\\times N_a} \\rightarrow \\mathbb{R}^{N_c\\times N_{obs}}$ denote the observation operator, and we have the observed data as $y = H(x)$.\nLef $\\{S_{c,1}, S_{c,2},..., S_{c,N_{obs}}\\}\\subseteq \\Omega$ denote the set of Voronoi-tessellated field of for the variable $x_c \\in \\mathbb{R}^{N_a\\times N_a}$. Each sub-region $s_{c,i}$ is defined as:\n\ns_{c,i} = \\{x \\in \\Omega \\ | \\ ||x - Pos(y_{c,i})|| \\le ||x - Pos(y_{c,j}) ||, \\forall j \\ne i\\},\nwith $s_{c,i}(x) = y_{c,i}, \\ \\forall x \\in s_{c,i}$.\n\n(2)\n\nwhere $Pos$ denotes the position of the observed point for field $x_c$. Let $q$ denote the reconstructed field, and the reconstructions using VT-UNet, unconditional diffusion and conditional diffusion models can be obtained as: $q = F_{VT}(\\{s_{c,i}\\}) $, $q = F_{Diff}(\\epsilon, y)$, and $q = F_{CondDiff}(\\epsilon, \\{s_{c,i}\\}, M\\odot x)$, respectively. Here, we slightly abuse the notation for diffusion models because $q$ is generated through iterative calls to the diffusion model, and $\\epsilon$ denotes the randomized field initialization. The VT-UNet model is trained to minimize the mean squared error, $\\mathbb{E}_{x,y} [||q - x||^2]$"}, {"title": "2.2. Diffusion model with spatial feature cross attention", "content": "The forward map of diffusion models is a tractable transformation where noise is gradually added, and the reverse map is approximated by neural networks to generate the reconstructed fields [50]. We denote the data distribution as $\\pi_0$ and the random noise as $\\pi_1 \\sim \\mathcal{N}(0, I)$. Let $x_0$ be the initial data sample. Its intermediate representations $x_t$ at timesteps $t \\in [0,1]$ can be obtained through the following transformation:\n\nxt = a_tx_0 + b_t\\epsilon, \\text{ where } \\epsilon \\sim \\mathcal{N}(0, I),\n\n(3)\n\nwhere $a_t$ and $b_t$ are the parameters of the transformation. Here, the timestep $t$ is an artificial notation for describing the mapping between the data distribution"}, {"title": "", "content": "and the Gaussian prior, rather than physical time. Various choices exist for these transformation parameters [51, 26, 52, 22]. The Elucidating Diffusion Model (EDM) framework [53] can be regarded as a special case of variance-exploding (VE) formulation [54] and it can be expressed as:\n\nx = x_0 + \\sigma \\epsilon,\n\nwhere $\\sigma_{\\epsilon}$ denotes the noise level, sampled from a log-normal distribution during training. For simplicity, we will drop the subscript of $x_t$ and $\\sigma_t$. One advantage of the VE formulation is its capability to handle unevenly distributed data, which is common in physical fields. Even after normalizing with the mean and standard deviation of the training dataset, physical fields can exhibit significant variability, with some regions being highly positive and others highly negative, despite having a mean close to zero. The variance-exploding formulation is well-suited to address this issue, as it can accommodate large noise scales.\nWe also tested a diffusion model with noise prediction using the variance-preserving (VP) [52] formulation on the Darcy flow problem. We found the model trained with VP formulation struggled to generate the unevenly distributed fields. One possible reason is the sampled Gaussian noise typically has a smaller magnitude than the variability in the uneven regions. In this case, the noise level may not be large enough to capture the variability in the data distribution, leading to poor generation performance starting from the Gaussian prior.\nFor the reverse sampling process, instead of solving the stochastic differential equation (SDE), Song et al. [52] proposed solving the following probability flow (PF) ordinary differential equation (ODE):\n\ndx = \\left[f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x log\\ p_t(x; t)\\right] dt,\n\n(5)\n\nwhere $f$ and $g$ are the drift and diffusion functions, respectively. $\\nabla_x log\\ p_t(x;t)$ is the score function, which is the gradient of the log-likelihood of the data distribution at time $t$ with respect to the data sample $x$ [55]. For generating physical fields, the PF ODE is preferred over the SDE due to its deterministic nature, which ensures a more tractable generation process [29].\nLet $D(x; \\sigma)$ denote the denoiser function that is optimized by the following training objective [53] to minimize the $L_2$ denoising error:\n\n\\mathbb{E}_{x_0 \\sim P_{data}} \\mathbb{E}_{n \\sim \\mathcal{N}(0,\\sigma^2I)} || D(x_0 + n; \\sigma) - x_0||^2, \\text{ with } \\nabla_x log\\ p_t(x; \\sigma) = \\frac{D(x; \\sigma)-x}{\\sigma^2}\n\n(6)"}, {"title": "", "content": "where $n$ denotes the added noise. Instead of approximating the denoiser function directly with neural network, it has been shown that scaling the output of denoising estimator with respect to the noise level, $\\sigma$, improves overall performance. The following scaling scheme is utilized in the loss function [53]:\n\nD_{\\theta}(x; \\sigma) = c_{skip}(\\sigma)x + c_{out}(\\sigma) F_{\\theta}(c_{in}(\\sigma)x; c_{noise}(\\sigma))\n\n(7)\n\n\\mathbb{E}_{x_0,n,\\sigma} \\left[\\lambda(\\sigma) c_{out}(\\sigma)^2 \\left|\\left| F_{\\theta}(c_{in}(\\sigma) \\cdot (x_0 + n); c_{noise}(\\sigma)) - \\frac{1}{c_{out}(\\sigma)} \\cdot (x) - c_{skip}(\\sigma) \\cdot (x_0 + n))\\right|\\right|^2\\right]\n\n(8)\n\nwhere $\\lambda(\\sigma)$ is a positive weighting function, $c_{out}(\\sigma)$, $c_{noise}(\\sigma)$, and $c_{in}(\\sigma)$ are scaling factors. The function $F_{\\theta}$ represents the neural network parameterized by $\\theta$. To generate the full-field solution, we solve the following deterministic ODE, derived by substituting $\\sigma(t) = t$ as the noise schedule in Equation (6)\n\ndx = -t\\nabla_x log\\ p_t(x; \\sigma)dt = \\frac{x \u2013 D_{\\theta}(x; \\sigma)}{\\sigma} dt\n\n(9)\n\nWe utilize the multi-step and predictor-corrector methods to solve the ODE.\nWith access to partial measurements of the field, Song et al. [52] proved that the score function can be approximated as:\n\n\\nabla_z log\\ p_t(z_t|y) \\approx \\nabla_z log\\ p_t(z_t|M^c \\odot \\hat{x}_t) = \\nabla_z log\\ p_t \\left(z_t \\oplus (M \\odot \\hat{x}_t)\\right)\n\n(10)\n\nwhere $z_t = M\\odot x_t$ defines a new diffusion process of the unknown fields, and $M^c \\odot \\hat{x}_t$ denotes a random sample from $p_t(M^c \\odot x_t|y)$.\nUsing partial observations as conditioning information, we tested three conditioning methods: guided sampling, classifier-free guidance, and cross-attention. A schematic of the latter two methods, along with the proposed condition encoding block, is shown in Figure 1. The guided sampling method is based on the inpainting approach, where the full fields are initially filled with noise, and an unconditional model is trained to denoise the fields. For the guided reverse sampling process, the unobserved field is updated by Equation (9), $M^c \\odot dx_t$, and the observed field is updated by [56]:\n\nMx_{t-1} = M \\odot x_0 + \\sigma_{t-1}\\epsilon\n\n(11)\n\nFor CFG [45], the pooled embedding of the conditioning information is combined with the noise scale embedding using the Feature-wise Linear Modulation (FiLM) [57] to generate the denoised fields. FiLM performs learnable modulations on the hidden state using the conditional information, offering an effective and flexible way of"}, {"title": "", "content": "modulating the hidden state. In the cross-attention approach [46], cross-attention is applied between the embedding of the conditioning information and the hidden states, $h$, of the diffusion model. Let $E_\\theta$ denote the condition encoding block. The cross-attention has the same formulation as self-attention but with different matrix assignments:\n\nAttention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\n(12)\n\nwhere $Q = W_qh$, $K = W_kE_{\\theta}(x)$, and $V = W_vE_{\\theta}(x)$. Here, the query ($Q$) is derived from the hidden states of the diffusion model, while the key ($K$) and value ($V$) are derived from the condition encoding block $E_{\\theta}(x)$. $W_q$, $W_k$, and $W_v$ are learned projection matrices, and $d_k$ is the dimensionality of the key.\nMathematically, conditioning via cross-attention can be regarded as a form of CFG, the new score function of the unobserved field can be expressed as:\n\n\\nabla_z log\\ p_t(z_t|E_{\\theta}(y)) = \\nabla_z log\\ p_t(z_t|E_{\\theta}(y_{null}))+\n\\gamma \\cdot (\\nabla_z log\\ p_t(z_t|E_{\\theta}(y)) \u2013 \\nabla_z log\\ p_t(z_t|E_{\\theta}(y_{null})))\n\n(13)\n\nwhere $\\gamma$ is the guidance scale, set to 1, and $E_{\\theta}(y_{null})$ denotes the unconditional encoded state. Compared to Equation (10), we rely on the condition encoding block to capture the encoded representation and establish a tractable mapping between observed and unobserved regions. We set the CFG and cross-attention diffusion models to capture $p(z(t)|E_{\\theta}(y))$, while retaining Equation (8) as the training objective for the encoder block to extract the observations. During the reverse sampling process, we update only the unobserved regions of the field."}, {"title": "2.3. Data assimilation as posterior fine-tuning", "content": "The prediction of the physical field can be further enhanced using data assimilation (DA) algorithms based on Bayesian methods. Let $x_t^{b}$ denote the predicted control vector (also known as the background state) and $y_t^{o}$ denote the sparse observation at time t [58]. In this section, t denotes the physical time in simulations. Variational DA aims to find the optimal compromise between $x_t^{b}$ and $y_t^{o}$ by minimizing the cost function $J_t$, defined as:\n\nJ_t(x) = \\frac{1}{2}(x - x_t^{b})^T B_t^{-1}(x - x_t^{b}) + \\frac{1}{2}(y_t^{o} - H(x_t^{b}))^T R_t^{-1}(y_t^{o} - H(x_t^{b}))\n\n=\\frac{1}{2} || x - x_t^{b} ||_{B_t^{-1}}^2 +  \\frac{1}{2} ||y_t^{o} - H(x) ||_{R_t^{-1}}^2\n\n(14)\n\nwhere the operator $(\\cdot)^T$ in Equation (14) indicates the transpose. The error covariance matrices associated with $x_t^{b}$ and $y_t^{o}$ are denoted by $B_t$ and $R_t$, respectively:\n\nB_t = Cov(x_t^{b} - X_{true,t}, x_t^{b} - X_{true,t}), R_t = Cov (H(X_{true,t}) - Y_t, H(X_{true,t}) - Y_t),\n\n(15)\n\nwhere $X_{true,t}$ represents the ground truth. Equation (14) represents the three-dimensional variational (3D-Var) approach. The analysis state $x_t^a$ corresponds to the point at which the cost function in Equation (14) reaches its minimum, that is,\n\nx_t^a = argmin_{x} (J_t(x)).\n\n(16)\n\nTypically, DA assumes that the background error (i.e., prior estimation error) and the observation error are uncorrelated. Since the diffusion model prediction is generated using the observation points, we adopt the DA framework here only as a posterior fine-tuning tool. In this context, the background error covariance $B_t$ can be empirically estimated from the ensemble output of the diffusion model, a task that is challenging for deterministic machine learning approaches [59]. Therefore, the approach proposed in this paper also addresses the bottleneck of prior and posterior error estimation in inverse modeling [48]. To improve efficiency and effectively capture the spatial correlation of physical fields, DA is conducted within the reduced-order space of Principal Component Analysis (PCA). The details of this reduced order DA algorithm are provided in Appendix A.3."}, {"title": "2.4. Benchmark Problems", "content": "We benchmark the performance of the diffusion model with different conditioning methods against the adapted VT-UNet on three fluid-like systems and one static system. Below, we provide a brief overview of the benchmark problem setups, with more detailed information on the data sources and generation procedures available in Appendix B. A summary of the benchmark problems is provided in Table 1. The three time-dependent PDEs selected here cover advection and diffusion dynamics for fluid-like systems, as well as non-linear reaction dynamics. The static problem is chosen because it is a common benchmark for reconstructing fields from correlated observations."}, {"title": "2.4.1. Darcy Flow", "content": "The Darcy flow equations describe the relationship between fluid pressure, $p(x)$, and the permeability, $a(x, \u03b8)$, of a porous medium through which the fluid moves. The pressure and the permeability field are governed by the following relationships:\n\n-\\nabla \\cdot (a(x, \u03b8)\\nabla p(x)) = f(x), x \\in D,\n\n(17)\n\np(x) = 0, x \\in \\partial D\n\n(18)\n\nThe permeability field is generated using a Karhunen-Lo\u00e8ve Expansion (KLE) of a Gaussian random field. The dataset is generated with 128 modes, and the corresponding pressure field is computed. In this problem, only partial observations of the pressure field are available. The numerical iterative Kalman filtering method [5] optimizes the coefficients of 64 modes to minimize the observation error. We use the code provided in [5] to generate 10,000 samples, with observation points evenly spaced across the pressure field. The boundary conditions are set to Dirichlet."}, {"title": "2.4.2. Shallow water", "content": "The shallow water equations describe a non-linear wave propagation problem defined over a spatial domain with three variables: water height, $h(x)$ (in mm), x-velocity, $u$, and y-velocity, $v$. The equations are given by:\n\n\\frac{\\partial h}{\\partial t} + \\nabla \\cdot (hu) = 0,\n\n(19)\n\n\\frac{\\partial u}{\\partial t} + \\frac{\\partial h}{\\partial x} + bu = 0,\n\n(20)\n\n\\frac{\\partial v}{\\partial t} + \\frac{\\partial h}{\\partial y} + bv = 0,\n\n(21)\n\nu_{t=0} = 0,\n\n(22)\n\nv_{t=0} = 0\n\n(23)\n\nThe simulations represent a dam break scenario, where a column of water is released at a random location within the domain. The boundary conditions are periodic, and We use the data simulated in [49], with partial observations of all three fields available."}, {"title": "2.4.3. 2D Diffusion-reaction", "content": "The 2D diffusion-reaction system consists of two fields: the concentrations of an activator and an inhibitor. The equations for this system are given by:\n\n\\frac{\\partial u}{\\partial t} = D_u \\frac{\\partial^2 u}{\\partial x^2} + D_u \\frac{\\partial^2 u}{\\partial y^2} + R_u,\n\n(24)\n\n\\frac{\\partial v}{\\partial t} = D_v \\frac{\\partial^2 v}{\\partial x^2} + D_v \\frac{\\partial^2 v}{\\partial y^2} + R_v,\n\n(25)\n\nwhere $u$ and $v$ are the activator and inhibitor fields, respectively, with diffusion coefficients $D_u = 1 \\times 10^{-3}$ and $D_v = 5 \\times 10^{-3}$. The reaction terms $R_u$ and $R_v$ are defined by the FitzHugh-Nagumo equations:\n\nR_u(u, v) = u - u^3 - k - v\n\n(26)\n\nR_v(u, v) = u - v\n\n(27)\n\nwhere $k = 5 \\times 10^{-3}$. The initial concentration at each point in both fields follows a Gaussian distribution. We use the data simulated in [60], with partial observations of both fields available. The boundary conditions are set to Neumann."}, {"title": "2.4.4. 2D Compressible Navier-Stokes (CFD)", "content": "The compressible Navier-Stokes equations describe the motion of a compressible fluid. The equations for the 2D compressible Navier-Stokes system are given by:\n\n\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho v) = 0,\n\n(28)\n\n\\rho \\left(\\frac{\\partial v}{\\partial t} + v \\cdot \\nabla v\\right) = - \\nabla p + \\nabla \\cdot \\eta \\nabla v + (\\zeta + \\frac{\\eta}{3}) \\nabla (\\nabla \\cdot v),\n\n(29)\n\n\\frac{\\partial}{\\partial t} \\left(\\rho \\frac{v^2}{2} + \\epsilon\\right) + \\nabla \\cdot \\left[\\left(\\epsilon + p + \\frac{\\rho v^2}{2}\\right) v - v \\cdot \\sigma'\\right] = 0,\n\n(30)\n\nwhere $\u03c1$ is the density, $v$ is the velocity, $p$ is the pressure, $\u03c3'$ is the viscous stress tensor, $\u03b7$ and $\u03b6$ are the shear and bulk viscosities, respectively, and $\u03f5$ is the internal energy. The initial conditions are constructed by a randomly initialized superposition of sinusoidal waves. We use the data simulated in [60], with partial observations of all four fields available. The selected dataset has $\u03b7 = \u03b6 = M = 0.1$. The boundary conditions are set to periodic."}, {"title": "3. Numerical Results", "content": "For performing the field reconstruction tasks, we select the ratio of observed data points to be 0.3% and 1.37% for the Darcy flow problem, corresponding to 49 and 225 observed points on a 128 \u00d7 128 grid, respectively. To align with the original numerical approach, these observed points are evenly spaced across the domain, and the loss metrics are computed only on the permeability field, for which we do not have any direct information.\nFor the remaining three time-dependent PDEs, we select the ratio of observed data points to be 0.3%, 1%, and 3%, with locations randomly sampled. The three loss metrics we use, selected from [60], are the root mean squared error (RMSE), the normalized root mean squared error (nRMSE), and the RMSE of the conserved quantity (cRMSE). These metrics are computed in the unknown regions of the fields. Due to the size of the dataset, we select 1000 samples for each problem for benchmarking. We abbreviate the 2D compressible Navier-Stokes equations as CFD in the following sections.\nThe reconstructed fields from noiseless observations using different methods are compared in Table 2. The results for the diffusion models are generated from an ensemble of 25 trajectories using the predictor-corrector scheme with 20 steps. We"}, {"title": "4. Conclusion", "content": "We enhance and evaluate diffusion models for field reconstruction tasks, with the goal of estimating complete spatio-temporal fields from sparse observations. By introducing a novel condition encoding block that integrates Voronoi-tessellated fields and sensing positions as an inductive bias, we constructed a tractable mapping between observed and unobserved regions. This approach leverages Feature-wise Linear Modulation (FiLM) and self-attention mechanisms to effectively capture the conditioning representation and support probabilistic reconstruction. We benchmark the effectiveness of conditioning using two commonly employed methods: hidden state augmentation, which we refer to as classifier-guidance free (CFG), and the cross-attention mechanism, against the adapted deterministic method, VT-UNet, with the same number of training steps. In addition, we include guided sampling in our comparison, a commonly used method that operates in the reverse sampling process without requiring explicit conditioning.\nThe proposed conditional encoding is shown to enable the diffusion model to generate high-quality fields from sparse observations. It offers a flexible approach to"}, {"title": "Appendix A. Additional Information", "content": "The hyperparameters of the EDM framework are designed to handle normally distributed data with a standard deviation of 0.5. Therefore, we scale our data accordingly to achieve a similar distribution based on the mean and standard deviation of the training data.\nThe U-Net architecture is utilized for the denoiser function, De, following the same design as in [53]. The implementation is based on the PyTorch 2.3.1 and diffusers 0.29.2 libraries. The network consists of Nblock down-sampling and up-sampling blocks, where Nblock is the number of blocks required to achieve a bottleneck size of 8 \u00d7 8. Specifically, Nblock equals 3 for the shallow water equations and 4 for the other problems. The first down-sampling block has 128 channels, while the remaining blocks have 256 channels. The up-sampling blocks are symmetric to the down-sampling blocks. The network uses a 3x3 convolutional layer with a stride of 2 for down-sampling and a nearest interpolation followed by a 3x3 convolutional layer for up-sampling. FiLM [57] is applied for both the noise level embedding and the classifier-free guidance (CFG).\nThe network is trained using the AdamW optimizer with a learning rate of 10-4 and a weight decay of 0.01 for 100,000 steps, with a batch size of 128 samples per step, on 8 Nvidia H100 GPUs. The loss is calculated using the MSE between the noiseless fields and the denoised fields, with weighting as proposed in [53]. The model weights are updated using an EMA with a decay rate of 0.999, inv_gamma = 1.0,"}, {"title": "Appendix A.1. Implementation and Training details", "content": "The hyperparameters of the EDM framework are designed to handle normally distributed data with a standard deviation of 0.5. Therefore, we scale our data accordingly to achieve a similar distribution based on the mean and standard deviation of the training data.\nThe U-Net architecture is utilized for the denoiser function, De, following the same design as in [53]. The implementation is based on the PyTorch 2.3.1 and diffusers 0.29.2 libraries. The network consists of Nblock down-sampling and up-sampling blocks, where Nblock is the number of blocks required to achieve a bottleneck size of 8 \u00d7 8. Specifically, Nblock equals 3 for the shallow water equations and 4 for the other problems. The first down-sampling block has 128 channels, while the remaining blocks have 256 channels. The up-sampling blocks are symmetric to the down-sampling blocks. The network uses a 3x3 convolutional layer with a stride of 2 for down-sampling and a nearest interpolation followed by a 3x3 convolutional layer for up-sampling. FiLM [57] is applied for both the noise level embedding and the classifier-free guidance (CFG).\nThe network is trained using the AdamW optimizer with a learning rate of 10-4 and a weight decay of 0.01 for 100,000 steps, with a batch size of 128 samples per step, on 8 Nvidia H100 GPUs. The loss is calculated using the MSE between the noiseless fields and the denoised fields, with weighting as proposed in [53]. The model weights are updated using an EMA with a decay rate of 0.999, inv_gamma = 1.0,"}, {"title": "Appendix A.2. Noise Schedule", "content": "Various noise schedulers exist for diffusion models [50", "53": "using parameters Pmean = -1.2 and Pstd = 1.2. It has been shown that the log-normal noise schedule needs to be tuned for optimal performance [61"}]}