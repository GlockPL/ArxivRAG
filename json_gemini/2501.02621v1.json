{"title": "LLMs Help Alleviate the Cross-Subject Variability in\nBrain Signal and Language Alignment", "authors": ["Yifei Liu", "Shuhang Li", "Yubo Liu", "Shuqi Gu", "Peilin Li"], "abstract": "Decoding human activity from EEG signals has long been a popular research\ntopic. While recent studies have increasingly shifted focus from single-subject to\ncross-subject analysis, few have explored the model's ability to perform zero-shot\npredictions on EEG signals from previously unseen subjects. This research aims\nto investigate whether deep learning methods can capture subject-independent\nsemantic information inherent in human EEG signals. Such insights are crucial\nfor Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the\nmodel's robustness against subject-specific temporal biases, and on the other, they\nsignificantly enhance the generalizability of downstream tasks. We employ Large\nLanguage Models (LLMs) as denoising agents to extract subject-independent se-\nmantic features from noisy EEG signals. Experimental results, including ablation\nstudies, highlight the pivotal role of LLMs in decoding subject-independent se-\nmantic information from noisy EEG data. We hope our findings will contribute to\nadvancing BCI research and assist both academia and industry in applying EEG\nsignals to a broader range of applications.", "sections": [{"title": "Introduction", "content": "Decoding human cognitive and motor activities from Electroencephalography (EEG) signals has\nlong been a cornerstone of Brain-Computer Interface (BCI) research. EEG offers a non-invasive\nmeans to capture the brain's electrical activity, providing valuable insights into neural processes\nassociated with various cognitive states and intentions. Despite significant advancements, one of the\npersistent challenges in EEG-based applications is the substantial variability across different subjects.\nThis cross-subject variability arises from individual differences in brain anatomy, neural dynamics,\nand signal acquisition conditions, making it arduous to develop generalized models that perform\nconsistently across diverse populations.\nTraditional approaches to EEG signal decoding have predominantly focused on within-subject\nanalyses, where models are trained and tested on data from the same individual. While effective to\na certain extent, these methods often fail to generalize to new, unseen subjects due to the inherent\nsubject-specific characteristics embedded in the EEG signals. This limitation not only hampers\nthe scalability of BCI systems but also restricts their applicability in real-world scenarios where\naccommodating every new user individually is impractical.\nRecent strides in deep learning have introduced more sophisticated models capable of capturing\ncomplex patterns within EEG data. However, the challenge of cross-subject generalization remains\nlargely unaddressed. In this context, Large Language Models (LLMs) emerge as a promising tool to\nbridge the semantic gap between noisy EEG signals and meaningful linguistic representations. By\nleveraging the powerful denoising and semantic extraction capabilities of LLMs, it becomes feasible\nto distill subject-independent semantic information from EEG data, thereby mitigating the adverse\neffects of cross-subject variability.\nIn this study, we explore the integration of LLMs into the EEG decoding pipeline to enhance the\nalignment between brain signals and language representations. Our approach employs an autoencoder\nto generate low-dimensional dense representations of EEG signals, which are then aligned with\nthe text embedding space of an LLM. By incorporating prompt tuning techniques, we enable the\nLLM to effectively interpret and generate coherent language outputs from the encoded EEG features.\nThis methodology not only facilitates zero-shot predictions on previously unseen subjects but also\nsignificantly improves the robustness and generalizability of EEG-based BCI systems.\nThe primary contributions of this paper are threefold:\n1. Cross-Subject Generalization: We introduce a novel framework that leverages LLMs to\nextract subject-independent semantic information from EEG signals, thereby addressing the\nchallenge of cross-subject variability.\n2. Zero-Shot Prediction Capability: Our model demonstrates the ability to perform accurate\nzero-shot predictions on EEG data from subjects not encountered during training, showcasing\nits potential for scalable BCI applications.\n3. Comprehensive Evaluation: Through extensive experiments and ablation studies, we\nvalidate the efficacy of our approach, highlighting the pivotal role of LLMs in enhancing the\ndecoding performance and generalization capabilities of EEG-based models.\nBy advancing the integration of LLMs into EEG signal processing, this research paves the way for\nmore adaptable and universally applicable BCI systems, ultimately broadening the horizons for both\nacademic research and industrial applications."}, {"title": "Related works", "content": "The study of brain signal decoding and its applications has seen significant advancements in recent\nyears, driven by the integration of neuroscience, machine learning, and emerging technologies.\nResearchers have explored innovative neural network architectures, multimodal synthesis techniques,\nand advanced brain-computer interfaces (BCIs) to decode and interpret neural signals, shedding light\non the intricate mechanisms of perception and cognition. This progress has not only enhanced our\nunderstanding of brain functions but also opened new avenues for practical applications, including\nneuroprosthetics, adaptive neurostimulation, and human-computer interaction. This section provides\nan overview of the key areas of related work, focusing on advancements in brain signal decoding, BCIs,\nmultimodal synthesis, machine learning techniques, and their diverse applications in neuroscience\nand beyond.\nBrain Signal Decoding Recent advancements in brain signal decoding have been made through\nthe development of innovative neural network architectures. [19]introduced a hybrid gated recurrent\nnetwork (HGRN) for inter-subject visual MEG decoding, providing a new tool for analyzing brain"}, {"title": "Method", "content": "A pipeline view of our method can be found in Fig1. The pipeline consists of the following key\ncomponents: Data Preprocessing, which standardizes the sample format; EEG Autoencoder, which\ngenerates a low-dimensional dense representation of the EEG signal; EEG Latent and Text Embedding\nAlignment, where the mapping between the EEG latent space and the LLM text embedding space is\nlearned while preserving semantic consistency; and EEG Prompt Tuning, where the EEG information\nis incorporated into the LLM's prompt for inference and result generation."}, {"title": "Dataset", "content": "We use the ChineseEEG dataset [26], which provides extensive EEG recordings from 10 subjects\nreading approximately 11 hours of Chinese text. The dataset includes raw EEG sensor-level data and\ncorresponding semantic embeddings generated by BERT-base-chinese [4]. To adapt the data for\ndownstream LLM applications, we discarded the original BERT embeddings and instead utilized the\ntokenizer and embedding layer of our chosen LLM to generate EEG-aligned text embeddings."}, {"title": "EEG autoencoder", "content": "The raw EEG signals are represented as high-dimensional sparse matrices, which pose challenges for\ndirect processing in machine learning models. To obtain a low-dimensional dense representation, we\nemploy an autoencoder architecture. Specifically, we train the autoencoder using the raw EEG data,\nwhere the encoder learns to map the input EEG signal into a lower-dimensional latent space, while\nthe decoder reconstructs the original signal from this latent representation. After training, we freeze\nthe encoder and discard the decoder, retaining only the latent representations for further processing.\nThis latent representation captures the essential features of the EEG signals in a compressed form,\nreducing the dimensionality and improving the efficiency of downstream tasks."}, {"title": "Alignment between EEG latent and text embedding", "content": "Aligning EEG latent vectors with text embeddings involves mapping the low-dimensional neural\nrepresentations to the high-dimensional semantic space. We achieve this alignment through a multi-\nlayer perceptron (MLP) architecture, specifically designed to handle the dimensional disparity and\ncapture the complex relationships between EEG features and textual semantics."}, {"title": "Prompt tuning", "content": "After learning the mapping from EEG latent vectors to text embeddings (referred to as the EEG\nembeddings), we inspired by the concept of prompt tuning [17]. We construct a task-specific prompt\ndesigned to extract signal information from noisy input. This task prompt is processed through the\nLLM's tokenizer and embedding layer to produce a prompt embedding. The EEG embeddings are\nthen concatenated with the prompt embedding, creating a combined input. Finally, the concatenated\nembeddings are fed into the LLM, which performs autoregressive decoding to produce the final\noutput. This approach enables the model to leverage the rich semantic representations of the EEG\nsignals in a natural language processing framework, enhancing the ability to decode EEG signals and\ninterpret them in a human-understandable form."}, {"title": "Experiment", "content": "To reduce the complexity of model training, we used the preprocessed data from the ChineseEEG\ndataset, which follows a standard processing pipeline. Each sample consists of an EEG time series and\na corresponding text pair of length 1. The EEG data for each sample is represented as $X \\in R^{128 \\times T}$,\nwhere 128 denotes the number of channels and T represents the time duration. The corresponding\ncharacter embedding is represented as $E \\in R^{3584}$, where 3584 is the hidden state size of the\nQwen-2.5 model.\nSince the time duration T of each EEG sample varies (typically around 300 time points), we first use\nan adaptive average pooling operation to normalize the length of each channel to a fixed size of 256.\nThis results in a tensor $X_{pool} \\in R^{128 \\times 256}$, where 128 represents the number of channels and 256 is\nthe fixed time length. This pooled version of the EEG signal is then fed into the EEG autoencoder for\ndimensionality reduction. For each channel, the adaptive average pooling operation computes the\naverage over contiguous time windows, thereby generating a fixed-length representation for each\nchannel:\n$X_{pool} [i, j] = \\frac{1}{\\frac{T}{256}} \\Sigma X [i, t], (1)$\nWhere $S_{j}$ is the set of time indices corresponding to the j-th output window of size $\\frac{T}{256}, X_{pool} [i, j]$\nrepresents the pooled value at channel i and pooled time index j.\nIn order to achieve effective compression and reconstruction of input data, we designed a deep\nconvolutional autoencoder (CAE). The autoencoder consists of two parts: the encoder and the\ndecoder, which are responsible for mapping the input data to a low-dimensional latent space and\nreconstructing the original input from the latent space.\nThe Encoder is constructed with three one-dimensional convolutional layers (1D Convolutional\nLayers), each followed by a ReLU activation function to introduce non-linearity. After the convolutional operations, a Flatten layer\nconverts the multi-dimensional tensor into a one-dimensional vector. This vector is then mapped to\nthe latent space via a fully connected layer (FC). Assuming the input length is downsampled to 32\nafter the convolutional layers, the FC layer takes a 16 \u00d7 32 dimensional vector as input and outputs a\nlatent_dim dimensional latent variable."}, {"title": "Ablation Study", "content": "Firstly, we carried out dimensionality reduction on the 64-dimensional latent space and used the PCA\nalgorithm to reduce it to two dimensions. The result of the dimensionality reduction is shown in the\nfigure3. However, the outcome is poor, as different classes, represented by various colors, are heavily\noverlapping with no clear boundaries, making it difficult to distinguish between them. Furthermore,\nthe data points form a dense, elliptical cluster with no apparent grouping or structure, indicating that\nthe principal components fail to capture meaningful features or the inherent separability of the original\ndata. Therefore, we considered using some simple machine learning tasks to perform classification\ntasks to obtain a direct mapping from latent to tokenid. We randomly selected 8 subjects from the\noriginal data set as the training set and the remaining subject as the test set. As an experiment, we\nrepeated the experiment three times. The average prediction accuracy is taken as the experimental\nresult. We chose the classical KNN algorithm and decision tree algorithm to try, and the specific\nexperimental results are shown in Table 4 and Table 5."}, {"title": "Finetune regression model", "content": "To adapt the existing neural network for the regression task, we incorporate an additional classifier\nlayer. Given the superior performance of the trained EEG embedding, we opt to freeze the parameters\nof the LatentToEmbedModel. This strategy leverages the robust latent representations while focusing\nthe training process on the newly introduced classifier layer.\nThe classifier layer is a single fully connected (FC) layer that maps the high-dimensional embedding\nproduced by the LatentToEmbedModel to the desired regression output. Formally, the classifier is\ndefined as:\n$Y_{pred} = W_{cls}d_{3} + b_{cls}, (10)$\nSince the LatentToEmbedModel has demonstrated effective embedding capabilities, we freeze its\nparameters to prevent them from being updated during the fine-tuning process. This is achieved\nby setting the 'requires_grad' attribute of all parameters in the LatentToEmbedModel to 'False'.\nConsequently, only the weights $W_{cls}$ and bias $b_{els}$ of the classifier layer are trainable.\nThe training process involves minimizing the regression loss, typically using Mean Squared Error\n(MSE) loss, with respect to the classifier layer's parameters. The loss function is defined as:\n$L = \\frac{1}{N} \\Sigma \\frac{1}{2}  (Y_{pred} ^(i) - Y_{true}^(i) )^2 , (11)$\nDuring optimization, only $W_{cls}$ and $b_{els}$ are updated using gradient descent-based algorithms, while\nthe parameters of the LatentToEmbedModel remain unchanged. This approach ensures that the\nhigh-quality embeddings are retained, and the classifier effectively learns to map these embeddings\nto the regression targets.\nThis fine-tuning strategy thus effectively balances the utilization of pre-trained embeddings with the\nadaptability required for the specific regression objectives."}, {"title": "Conclusion", "content": "In this paper, we present a novel approach that harnesses the capabilities of Large Language Models\n(LLMs) to mitigate the challenges posed by cross-subject variability in EEG signal decoding. Our\nproposed framework effectively aligns low-dimensional EEG latent representations with the semantic\nembedding space of an LLM, enabling accurate and robust zero-shot predictions on data from\npreviously unseen subjects. The integration of LLMs as denoising agents plays a crucial role in\nextracting subject-independent semantic information, thereby enhancing the generalizability and\nscalability of EEG-based Brain-Computer Interfaces (BCI).\nOur experimental results demonstrate the superior performance of the proposed method, achieving\nconsistently high accuracy across various subject masks, including scenarios where up to 30% of\nsubjects are unseen during training. The ablation studies further underscore the indispensability of\nLLMs in our pipeline, revealing that traditional machine learning models falter in the presence of\nnoisy EEG data, whereas our LLM-enhanced approach maintains exceptional decoding capabilities.\nThis stark contrast highlights the profound impact of leveraging advanced language models to interpret\ncomplex neural signals.\nThe findings of this study have significant implications for the future of BCI research and applications.\nBy overcoming the limitations of subject-specific variability, our method facilitates the development\nof more versatile and user-independent BCI systems. This advancement opens avenues for deploying\nEEG-based technologies in a wider array of settings, ranging from assistive devices for individuals\nwith motor impairments to interactive gaming and cognitive monitoring in healthy populations.\nLooking ahead, there are several promising directions for future research. One potential avenue is\nthe exploration of more sophisticated alignment techniques between EEG latent spaces and LLM\nembeddings, which could further enhance the fidelity and interpretability of the decoded signals. Ad-\nditionally, extending this framework to incorporate multimodal data sources, such as combining EEG\nwith other physiological signals, could yield richer and more nuanced representations of cognitive\nstates. Finally, conducting longitudinal studies to assess the performance of the proposed method\nover extended periods and diverse populations would provide deeper insights into its robustness and\npractical viability.\nIn conclusion, this study marks a significant step towards the realization of more adaptive and\ngeneralized BCI systems. By leveraging the strengths of LLMs in semantic understanding and\ndenoising, we have demonstrated a robust method for decoding EEG signals that transcends individual\nsubject differences. This innovative approach not only advances the technical capabilities of EEG-\nbased BCIs but also contributes to the broader goal of creating more inclusive and universally\napplicable neurotechnology solutions."}]}