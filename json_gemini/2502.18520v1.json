{"title": "Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features", "authors": ["Mingli Zhu", "Shaokui Wei", "Hongyuan Zha", "Baoyuan Wu"], "abstract": "Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign performance. In this work, inspired by the concept of optical polarizer-which allows light waves of specific polarizations to pass while filtering others we propose a lightweight backdoor defense approach, NPD. This method integrates a neural polarizer (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (CNPD). The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD. We propose three implementations of CNPD: the first is r-CNPD, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: e-CNPD, which embeds class information as additional features, and a-CNPD, which directs network attention using class information. Additionally, we provide a theoretical guarantee for the feasibility of the proposed CNPD method, and demonstrate that CNPD establishes an upper bound on backdoor risk. Extensive experiments show that our lightweight and effective methods outperform existing methods across various neural network architectures and datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "EEP Neural Networks (DNNs) have achieved remark-able performance in lots of critical domains, such as facial recognition [46] and autonomous driving [66]. However, the susceptibility of DNNs to deliberate threats [10], [14], [60], [65] poses a serious challenge, as it imperils the integrity of these systems and undermines their reliability. Notably, the risk posed by backdoor attacks is garnering increased attention due to their inherent sophistication and stealthiness [59], [72]. Backdoor attacks entail the malicious manipulation of the training dataset [15] or training process [31]. This results in a backdoored model which classifies poisoned samples with triggers to a predefined target label and behaves normally on benign samples [15]. Backdoor attacks may originate from a multitude of avenues, such as the use of contaminated training datasets, employing third-party platforms for model training, or procuring pre-trained models from untrusted sources. These circumstances considerably heighten the risk posed by backdoor attacks on DNN's applications. Concurrently, they underscore the cruciality of devising backdoor defenses against such attacks. This work focuses on post-training defense, which aims to mitigate the backdoor effect in a backdoored model, using a small subset of benign data, while preserving the model's performance on benign inputs. Current approaches in this category mainly involve global fine-tuning [67], [73] or network pruning [32], [71]. However, these methods face two challenges. First, the large parameter space involved in fine-tuning or pruning makes it difficult for a limited amount of benign data to effectively remove backdoors without compromising model performance. Second, the optimization of this parameter space is computationally expensive. To address the above constraints, we introduce a lightweight yet effective countermeasure that operates without requiring model retraining. Our approach is inspired by optical polarizers [63], which allow only light waves with specific polarizations to pass while blocking others in a mixed light wave. Similarly, our method involves inserting one learnable layer while keeping all layers of the original backdoored model fixed. By applying a similar principle, if we consider a poisoned sample as a combination of both trigger and benign features, we can design a neural polarizer (NP) to filter the trigger features while conserving the benign ones. Consequently, contaminated samples could be cleansed thereby diminishing the impact of the backdoor,"}, {"title": "2 RELATED WORK", "content": "Deep neural networks (DNNs) have been found to be vulnerable to backdoor attacks, where the backdoored model performs normally on benign inputs while predicting a specified target label when encounters input with the pre-defined trigger. Backdoor attacks are generated by poisoning training dataset [5], [15], [30], [68] or injecting backdoors in the training process [11], [31], [40], [41]. In dataset poisoning attacks, the adversary maliciously modifies the dataset by introducing carefully crafted triggers onto images and altering the corresponding labels to a target label. As a result, the backdoored model learns incorrect associations between the triggers and the intended target label. The seminal work by Gu et al. [15] introduced the \"BadNets\", demonstrating that subtle alterations to a fraction of training samples could lead to a targeted misclassification during inference. This foundational study spurred extensive exploration into various mechanisms of backdoor insertion, particularly focusing on invisible triggers design [30], [68], dataset poisoning with low poisoning ratio [48], [75], or clean-label attacks [2], [12], [45] that crafts poisoned samples without modifying labels. These works increase the stealthiness of backdoor attacks, and highlight the necessity for improved detection and mitigation techniques. In training controllable backdoor attacks, the adversary strategically optimizes the triggers and backdoored models meanwhile to enhance the stealthiness and impact of the triggers, ensuring they can be effectively activated by the model. Additionally, the adversary seeks to make the model more resilient to backdoor detection and mitigation efforts. For instance, WaNet [41] proposes a novel \"noise\" training mode to increase the difficulty of detection. He et al. [19] proposes sharpness-aware data poisoning attack that maintains the poisoning effect under various re-training process. Such advancements underscore the increasing sophistication of backdoor strategies, emphasizing the growing need for"}, {"title": "2.2 Backdoor defense", "content": "Numerous studies [42], [61] have been conducted to develop defenses against the threats posed by backdoor attacks. According to defense stages, existing studies can be roughly partitioned into four categories: pre-processing [4], [76], in-training [21], [28], post-training [29], [52], and inference stage defenses [6], [17]. Pre-processing defenses aim to remove poisoned samples from a compromised dataset by input anomaly detection. Training-stage strategies assume that a defender is given the poisoned training dataset and the defender needs to train a secure model and inhibit backdoor injection based on the poisoned dataset. These strategies exploit differences between poisoned and clean samples to filter out potentially harmful instances [4]. Post-training defenses assume that a defender has access to a backdoored model and a limited number of clean samples. These defenses aim to eliminate backdoor effects from compromised models without requiring knowledge of the trigger or target labels. Techniques include pruning potentially compromised neurons [32], [71] and performing strategic fine-tuning [67], [73]. For fine-tuning based methods, I-BAU [67] establishes a minimax optimization framework for training the network using samples with universal adversarial perturbations. Similarly, PBE [39] leverages untargeted adversarial attacks to purify backdoor models. SAU [57] firstly searches for shared adversarial example between the backdoored model and the purified model and unlearns the generated adversarial example for backdoor purification. These studies leverage adversarial examples in backdoor models to mitigate backdoor threats; however, none have discussed the impact of different adversarial examples on the effectiveness of backdoor removal. Inference stage defenses seek to prevent backdoor activation through sample detection and destroying trigger's feature. In this work, we mainly consider post-training backdoor defense, and follow their default settings."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the details of our methodology. First, in Sec. 3.1, we define the basic settings, including the notations and the threat model. Next, in Sec. 3.2, we introduce our Neural Polarizer-based Defense (NPD) method, outlining its objective, optimization, and algorithm. In Sec. 3.3, we analyze the limitation of NPD and present our Class-conditional NPD method, followed by the introduction of three distinct implementations in Sec. 3.4. Finally, in Sec. 3.5, we provide a theoretical analysis to support the feasibility and effectiveness of our approach."}, {"title": "3.1 Basic Settings", "content": "We consider a classification problem with C classes (C \u2265 2). Assume \\(x \u2208 X \u2282 \\mathbb{R}^d\\) to be a d-dimensional input sample, its corresponding ground truth label is denoted as \\(y \u2208 Y = {1, . . ., C'}\\). A deep neural network comprising of L layers \\(f : X \u00d7 W \u2192 \\mathbb{R}^C\\), parameterized by \\(w \u2208 W\\), is defined as follows:\n\\[f(x; w) = f^{(L)} \\circ ...\\circ f^{(l+1)} \\circ f^{(l)} \\circ ...\\circ f^{(1)}(x),\\]\nwhere \\(f^{(l)}\\) embodies the function (e.g., convolution layer or batch normalization layer) with parameter \\(w_l\\) located in the \\(l\\)th layer, where \\(1 \u2264 l \u2264 L\\). To maintain simplicity, \\(f(x; w)\\) is presented as \\(f_w(x)\\) or \\(f_w\\). Given an input \\(x\\), the predicted label of \\(x\\) is determined by \\(arg max f_c(x; w), c = 1, ...,C\\), where \\(f_c(x; w)\\) represents the logit of the \\(c\\)th class."}, {"title": "3.1.2 Threat model", "content": "Attacker's goal. In this work, we consider the scenario that the attacker is an unauthenticated third-party model publisher or dataset publisher. Thus the backdoored model \\(f_w\\) can be generated through the manipulation of either the training process or the training data, ensuring that \\(f_w\\) functions correctly on benign sample \\(x\\) (i.e., \\(f_w(x) = y\\)), and predicts the poisoned sample \\(x_\\triangle = r(x, \\triangle)\\) to the targeted label T. Here, \\(\u25b3\\) indicates the pre-defined trigger and \\(r(\u00b7, \u00b7)\\) is the fusion function that integrates \\(x\\) and \\(\u25b3\\). Considering the possibility of the attacker establishing multiple target labels, we assign \\(T_i\\) to represent the target label for sample \\(x_i\\). This threat is practical in the real world applications that heavily rely on AI-driven decisions, particularly in scenarios where organizations use third-party models or datasets without thoroughly scrutinizing their integrity due to resources limits. The stealthiness of backdoor attacks makes them particularly hard to immediately recognize, thus posing substantial potential risks for individuals or entities. Defender's capabilities and objectives. Assume that the defender is given the backdoored model \\(f_w\\) and has access to a limited number of benign training samples, denoted as \\(D_{bn} = {(x_i, y_i)}_{i=1}^N\\). The defender's goal is to obtain a new model \\(f\\) based on \\(f_w\\) and \\(D_{bn}\\), such that the backdoor effect is removed and the benign performance is maintained in \\(f\\\\), i.e., \\(f(x) \u2260 y\\) and \\(f(x) = y\\). Note that the defender is unaware of the attacker's target labels and specified triggers, and can only leverage a limited number of samples to modify the backdoored model. This presents a stringent yet highly practical scenario; for instance, a user may lack the resources to train a model from scratch and instead obtain a model from a third-party unauthenticated organization. However, fine-tuning the model is affordable to achieve a secure model."}, {"title": "3.2 Neural Polarizer based Backdoor Defense", "content": "We propose the neural polarizer (NP) to purify poisoned samples in the feature space. As shown in Fig. 2 (a), the neural polarizer \\(g_\u03b8\\) is inserted into the backdoored model \\(f_w\\) at a specific immediate layer, to obtain a combined network \\(f_{w,\u03b8}\\), i.e.,\n\\[f_{w,\u03b8} = f^{(L)} \\circ ... \\circ f^{(l+1)} \\circ g_\u03b8 \\circ f^{(l)} \\circ ... \\circ f^{(1)}(x).\\]\nFor clarity, we denote \\(f_{w,\u03b8}\\) as \\(f_\u03b8\\), since \\(w\\) is fixed."}, {"title": "3.2.1 Objective of the neural polarizer", "content": "Informally, a desired neural polarizer should have the following three properties:"}, {"title": "3.2.2 Learning neural polarizer", "content": "Loss functions in oracle setting. To learn a desired neural polarizer \\(g_\u03b8\\), we begin with an oracle scenario in which the trigger \\(\u25b3\\) and the target label T are provided. We introduce several loss functions designed to encourage the NP \\(g_\u03b8\\) to fulfill the two properties previously mentioned, as outlined below:\nLoss for filtering trigger features in poisoned samples. Given trigger \\(\u25b3\\) and target label T, trigger features can be filtered by weakening the connection between \\(\u25b3\\) and T using the following loss function:\n\\[L_{asr} (x, \u0394, \u03a4; \u03b8) = log(1 - s_T(x_\u0394; \u03b8)),\\]\nwhere \\(s_T(x_\u0394; \u03b8)\\) represents the softmax probability of the model \\(f_\u03b8\\) predicting \\(x_\u0394\\) as label T.\nLoss for maintaining benign features in poisoned samples. To ensure that a poisoned sample can be correctly classified to its true label, we utilize the boosted cross-"}, {"title": "3.3 Class-conditional NPD", "content": "One limitation of NPD lies in its reliance on the accuracy of the estimated target label (see Eq. (5)). An inaccurate estimation may either fail to eliminate backdoors or result in oscillations in the attack success rate during training. To further investigate this, we conduct experiments to compare the defense performance using different AEs. In detail, we use the same model architecture and bi-level optimization framework as in NPD, and substitute the AEs generation method with four different AEs generation methods. As shown in Fig. 3, \"Target\" refers to unlearning targeted AEs (using the same target label as attacker's target label T), while \"Wrong\" involves unlearning AEs assigned a label different from the attacker's target. The \u201cRandom\u201d consists of unlearning AEs with randomly assigned labels, and \"Untarget\" represents unlearning AEs with an untargeted objective. The experiment is conducted using the Trojan attack [33] on the CIFAR-10 dataset with PreAct-ResNet18 network. As the figure illustrates, the \u201cWrong\u201d group fails in backdoor mitigation, and the ASR curves in the backdoor removal processes of the \"Random\u201d and \"Untarget\u201d groups exhibit oscillations. In contrast, the \u201cTarget\u201d group removes the backdoor effectively and steadily with minimal impact on accuracy. This experiment highlights that different types of AEs result in significantly varying backdoor removal performance, with targeted AEs proving to be the most effective. However, the target label remains unavailable to the defender in post-training stage. Fortunately, the target label is revealed when the attacker launches an attack during inference. Therefore, we propose utilizing the output of the backdoored model as a proxy for the target label, taking advantage of the defenders' \"home field\" advantage."}, {"title": "3.3.2 Class-conditional NPD for DNNS", "content": "Unlike NPD, which unlearns targeted AEs based on esti-mated target labels, Class-conditional NPD (CNPD) designs a fusion module that is trained evenly across all classes, using class information as a condition for feature purification. As dipicted in Fig. 2 (b), the fusion module combines the class information y and the feature map \\(m^l(x)\\) as input, and the output of the module is:\n\\[\u011d_\u03b8(m^l(x), y) = S(g_\u03b8(m^l(x)), y).\\]\nHere, S represents fusion operation, and \\(\u0398\\) is the parameters set of NP. The combined network is denoted as \\(f_{w,\u03b8}\\) or \\(f_\u03b8\\),"}, {"title": "3.4 Three Implementations of CNPD", "content": "To implement our CNPD approach, we design three dis-tinct fusion module architectures, each accompanied by its corresponding training algorithm."}, {"title": "3.4.1 Replicated CNPD", "content": "As dipicted in Fig. 4 (a), the replicated CNPD (r-CNPD) defines a separate NP for each class c, parameterized by \\(\u03b8_c\\), i.e., \\(\u0398 = {\u03b8_c, c = 1,\u2026\u2026,C'}\\). Given an input feature map \\(m^l(x)\\) with label y, the output of the NP layer is:\n\\[\u011d_\u03b8(m^l(x), y) = g_{\u03b8_y}(m^l(x)).\\]\nIn the training phase, the parameters \\(\u03b8_y\\) are sequentially trained across all classes for backdoor mitigation. The training algorithm is shown in Alg. 2."}, {"title": "3.4.2 Embedding-based CNPD", "content": "Although our r-CNPD method performs effectively in prac-tice, the training cost and network capacity increase as the number of categories in the classification task grows, which may hinder its practical application. To address this issue, we design a fusion module that incorporates a fixed NP structure and a label embedding set, as illustrated in Fig. 4 (b). Unlike r-CNPD, the e-CNPD method directly incorporates category information into the network as embedded features to influence the effect of different categories on feature filtering. Let the input feature be \\(m^l(x) \u2208 \\mathbb{R}^{C_l \u00d7 H_l \u00d7 W_l}\\). We predefine specific embedded features for each category, denoted as \\(e(c) \u2208 \\mathbb{R}^{1\u00d7 H_l \u00d7 W_l}\\), where \\(c \u2208 {1, ...,C'}\\). The concatenated feature \\(cat[m^l(x), e(y)]\\) is then passed through the NP for filtering, defined as:\n\\[\u011d_\u03b8(m^l(x), y) = g_\u03b8(cat[m^l(x), e(y)]).\\]\nDuring each batch update in the training phase, we first randomly generate target labels for all batch samples and then create targeted adversarial samples to train the NP layer."}, {"title": "3.4.3 Attention-based CNPD", "content": "Although the e-CNPD method is effective, careful considera-tion is required when designing the scale of feature fusion to avoid excessive distortion on the original features. To address this, we propose a more refined feature filtering method called Attention-based CNPD (a-CNPD). As shown in Fig. 4 (c), we design an attention-like [51] mechanism, enabling different labels to modulate the network's attention to features across channels. We denote the embedding of class c as \\(e(c)\\), where \\(c \u2208 {1, . . ., C'}\\), and the parameters of the two linear layers as \\(\u03b8_Q\\), \\(\u03b8_K\\), one convolutional layer as \\(\u03b8_V\\). The subsequent convolutional and batch normalization layers are denoted as \\(g_\u03b8\\). Given an input feature map \\(m^l(x)\\) with label y, the output of the network \\(g_\u03b8\\) is defined as:\n\\[\u011d_\u03b8(m^l(x), y) = g_\u03b8(softmax (\\frac{\u03b8_Q \\cdot e(y)}{\\sqrt{d_k}})(\\frac{\u03b8_K \\cdot e(y)}{\\sqrt{d_k}}), (\u03b8_V (m^l(x)))),\\]\nwhere \\(d_k\\) is the dimension of \\(\u03b8_K\\)."}, {"title": "3.5 Theoretical Analysis", "content": "In this section, we analyze the effectiveness of the CNPD method theoretically."}, {"title": "3.5.1 Existence of Class-conditional Neural Polarizer", "content": "We first provide theoretical foundation for the existence of Class-conditional Neural Polarizer. To simplify our analysis, we focus on classification task equipped with a squared loss [37] in a reproducing kernel Hilbert space Hx [44] generated by kernel \\(K_x\\), feature mapping \\(\u03c6_x\\) and inner product (\u00b7,\u00b7)Hx. Then, for a function \\(h \u2208 H_x\\), we have \\(h(x) = (\u03c6_x(x), h)_{Hx}\\). We introduce M(x) as a function indicating whether a data point (x, y) is poisoned (M(x) = 1) or not (M(x) = 0). Given a poisoned dataset \\(D_{bd}\\), we consider a poisoned classifier that minimizes the expected squared loss over this poisoned dataset:\n\\[h_{bd} = arg \\underset{h\u2208H_x}{min} E_{(X,Y)\u223cD_{bd}}(h(X) \u2013 Y)^2 .\\]\nThen, we provide the following Theorem:"}, {"title": "3.5.2 Mechanism of CNPD", "content": "Building upon the backdoor risk introduced in [57], we demonstrate that the CNPD learning process is equivalent to minimizing an upper bound on backdoor risk, providing a theoretical guarantee of CNPD's effectiveness."}, {"title": "4 EXPERIMENTS", "content": "To evaluate the effectiveness of the proposed method, we perform extensive experiments across different benchmark datasets and network architectures, including CIFAR-10 [23], Tiny ImageNet [25], and GTSRB [49] datasets, on PreAct-ResNet18 [18] and VGG19-BN [47] networks. We evaluate the proposed method against ten state-of-the-art (SOTA) backdoor attack methods, and compare the performance with nine SOTA backdoor defense methods. Additionally, we provide some analyses in Sec. 4.4."}, {"title": "4.1 Implementation Details", "content": "Three benchmark datasets [58] are used to conduct comparison experiments including CIFAR-10 [23], Tiny ImageNet [25], and GTSRB [49]. In details, CIFAR-10 contains a total of 60,000 images divided into ten categories. Each category has 5,000 and 1000 images for training and testing, respectively. The size of each image is 32 \u00d7 32. Tiny ImageNet, which is a smaller version of the ImageNet dataset [9], includes 100,000 images for training and an additional 10,000 for testing, spread across 200 different classes. The images in this dataset have 64 \u00d7 64 pixels in size. GTSRB contains a total of 39,209 images for training and 12,630 for testing, grouped into 43 categories. Each image in GTSRB is 32 \u00d7 32 pixels in dimensions. To demonstrate the applicability of our method across different network architectures, we conduct experiments on both PreAct-ResNet18 [18] and VGG19-BN [47], and compare their performance with SOTA methods."}, {"title": "4.1.2 Attack settings", "content": "We evaluate the proposed three defense methods against ten well-known backdoor attacks, including BadNets [15] (both BadNets-A2O and BadNets-A2A, representing attacking one target label and multiple target labels, respectively), Blended attack (Blended [5]), Input-aware dynamic backdoor attack (Input-aware [40]), Low frequency attack (LF [68]), Sample-specific backdoor attack (SSBA [30]), Trojan backdoor attack (Trojannn [33]), Warping-based poisoned networks (WaNet [41]), Frequency domain trojan attack (FTrojan [53]), and Adaptive Backdoor Attack (Adap-Blend [43]). To ensure a fair comparison, we follow the default attack configurations outlined in BackdoorBench [58]. The poisoning ratio is set to 10% and the targeted label is set to the 0th label when evaluating against state-of-the-art defenses. To demonstrate the effectiveness of our method under multiple target labels setting, we employ an all-to-all label attack strategy, BadNets-A2A, where the target labels for original labels y are set to \\(y_t = (y + 1) mod C\\). Detailed introductions about these attacks are provided in Appendix B."}, {"title": "4.1.3 Defense settings", "content": "We compare the proposed three defense methods against nine SOTA backdoor defense methods, i.e., FP [32], NAD [29], NC [52], ANP [62], i-BAU [67], EP [71], FT-SAM [73], SAU [57], and FST [38]. All these defenses have access to 5% benign training samples for training. All the training hyperparameters are aligned with BackdoorBench [58]. We evaluate the proposed three defense methods with these SOTA defenses on different datasets and networks."}, {"title": "4.1.4 Details of our methods", "content": "For our methods, we apply an \\(l_2\\) norm constraint to the adversarial perturbations, with a perturbation bound of 3 for the CIFAR-10 and GTSRB datasets, and 6 for the Tiny ImageNet dataset. The loss hyperparameters \\(\u03bb_1\\), \\(\u03bb_2\\), and \\(\u03bb_3\\) are set to 1.0, 0.4, and 0.4 for the CIFAR-10 in our NPD, e-CNPD and a-CNPD, and set to 1.0, 0.5, and 0.5 for in our r-CNPD. We train the neural polarizer for 50, 10, 100, and 200 epochs for the NPD, a-CNPD, e-CNPD, and r-CNPD methods, respectively, with a learning rate of 0.01 on the CIFAR-10 dataset. The neural polarizer block is inserted before the final convolutional layer of the PreAct-ResNet18 architecture for CNPD and before the penultimate convolutional layer for NPD. Additional implementation details regarding state-of-the-art attacks, defenses, and our methods can be found in Appendix B."}, {"title": "4.1.5 Evaluation metrics", "content": "In this work, we utilize three primary evaluation metrics to assess the effectiveness of various defense methods: clean Accuracy (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER).\nACC reflects the accuracy of benign samples.\nASR evaluates the percentage of poisoned samples that are successfully classified as the target label.\nDER [73]) is a comprehensive metric that considers both ACC and ASR:\nA higher ACC, a lower ASR, and a higher DER indicate superior defense performance. In the main tables, the effectiveness of defense methods against various attack methods is color-coded for clarity: the most effective defense is highlighted in red, the second most effective in yellow, and the third in blue."}, {"title": "4.2 Main Results", "content": "We first evaluate the effectiveness of our NPD and CNPD defense methods against ten SOTA backdoor attacks, using three different datasets and two network architectures. As shown in Tab. 1 and 2, we observe that our defense methods demonstrate superior performance across all tested attacks. Specifically, our r-CNPD, e-CNPD, and a-CNPD methods achieved the first, second, and third highest average DER scores, respectively, and our a-CNPD achieves average DERs of 95.88%, 97.90%, and 93.32% on CIFAR-10, GTSRB, and Tiny ImageNet datasets, respectively. Additionaly, our NPD also shows superior performance in terms of DER for almost all attacks compared to SOTA defenses. These results highlight that our methods outperform other defenses, with the a-CNPD method yielding the best overall average DER."}, {"title": "4.3 Ablation Study", "content": "Low poisoning-ratio attacks have long posed a significant challenge to the effectiveness of defense mechanisms [34], [43], [69]. Tab. 3 presents the results of our e-CNPD defense against attacks with varying poisoning ratios on CIFAR-10 dataset. As shown, our method remains robust even under low poisoning ratios, likely because it effectively identifies backdoor-related signals and blocks them efficiently."}, {"title": "4.3.4 Effectiveness of each loss term", "content": "We conduct an ablation study to assess the contribution of each component of the loss function to the overall perfor-mance on CIFAR-10 dataset. Specifically, we examine the first and second terms of the loss function, Lbce (see Eq. (3)), which we denote as Lbce1 and Lbce2, respectively. As shown in Tab. 6, both Lbce and Lasr play a crucial role in enhancing overall performance, and removing any component results in a considerable drop in defense effectiveness."}, {"title": "4.4 Analysis", "content": "We demonstrate that the CNPD framework can be effectively extended to detect poisoned samples during the inference phase. Specifically, we introduce a poisoned sample detection mechanism that compares the network's output with and without the NP layer. By evaluating any discrepancies between the two outputs, we can identify whether a query sample is poisoned. Notably, this detection method is highly efficient and incurs only a negligible increase in inference cost compared to a single forward pass. Our approach leverages the unique properties of the NP layer. When a backdoor attack is successful, the attacked sample is misclassified as the target label. However, the NP layer suppresses the backdoor feature, altering the network's output label for the poisoned sample, while the output for clean samples remains unchanged. This distinction allows us to design a simple yet effective poisoned sample detection method, as described by the following rule:\n\\[R(x; f_w, f_\u03b8) = \\begin{cases}\n1, & \\text{if } f_w(x) \u2260 f_\u03b8(x) \\\\\n0, & \\text{if } f_w(x) = f_\u03b8(x)'\n\\end{cases}\\]\nwhere x is input, 1 denotes that x is detected as a backdoor sample. Unlike existing backdoor detection methods, this approach eliminates the need for repeated model queries, additional computational overhead, or extra training for optimization or estimation. In essence, it performs backdoor detection with only minimal computational cost beyond the initial inference. We denote this detection method as NPDT"}, {"title": "4.4.2 Visualization on purified features", "content": "In this work, we mitigate backdoors by filtering the interme-diate poisoned features of the network through a learnable neural polarizer (NP) layer. To further understand the impact of this mechanism, we conduct a detailed analysis of feature changes before and after passing through the NP layer. We perform two visualization experiments as follows. Visualization of feature map changes before and after the NP layer. An effective NP layer should suppress features that exhibit high activation values on backdoor-related neurons. Here, we use the TAC metric [70] to measure the correlation between neurons and the backdoor effect. We then rank these neurons according to their correlation with the backdoor and visualize the feature maps of poisoned samples on these neurons. We compare the feature maps between backdoored models with the defense models, as shown in Fig. 6. Note that we use the output of the final convolutional layer of the network for visualization. In each subplot in the figure, the neurons are arranged from top to bottom according to their correlation with the backdoor, from high to low. As observed, in the backdoored model (first row), poisoned samples exhibit highlighted activations on the backdoor-related neurons, whereas in the defense model (second row), poisoned samples no longer show highlights on the backdoor-related neurons. This indicates that the backdoor-related features have successfully been suppressed. Visualization of feature space changes between poi-soned and clean samples. To further compare NP's effects on clean and poisoned samples, we analyze it from the perspective of feature subspaces. Specifically, we examine the following four types of features: 1) Features of clean samples under backdoored models (Clean before); 2) Fea-tures of clean samples under defense models (Clean after); 3) Features of poisoned samples under backdoored models (BD before); 4) Features of poisoned samples under defense models (BD after). We perform Singular Value Decomposition (SVD) [50] on the \"Clean before\u201d and \u201cBD before\" groups, extracting two principal component vectors, \\(v_{clean}\\) and \\(v_{bd}\\). For each of the four sample groups, we compute the normalized similarity for the two vectors. The results are visualized as the components along \\(v_{clean}\\), as shown in Fig. 7. It can be observed that:\nAfter passing through the NP layer (\"Clean after,\" orange bars), the feature activations remain largely aligned with \\(v_{clean}\\), suggesting that the NP layer preserves clean sample features effectively while applying minimal perturbations.\nThe \"BD after\" group (green bars) exhibits strong ac-tivations on clean, and suppressed activations on bd, different from the \"BD before\" group (red bars)."}, {"title": "5 CONCLUSION", "content": "In conclusion, we have proposed a novel and lightweight backdoor defense approach, Neural Polarizer Defense (NPD), which effectively filters out malicious triggers from poisoned samples while preserving benign features. Drawing inspira-tion from the concept of optical polarizers, NPD integrates a learnable neural polarizer (NP) as an intermediate layer within compromised models, effectively mitigating backdoor attacks while maintaining benign performance. Furthermore, to enhance the defense capabilities, we introduce class-conditional neural polarizer-based defense (CNPD), which emulates the targeted adversarial approach by combining class information predicted by the backdoored model with the features to be purified. Through three distinct instantia-tions of CNPD\u2014r-CNPD, e-CNPD, and a-CNPD-we show how to balance backdoor mitigation with computational efficiency, especially for large class numbers. Moreover, we provide a theoretical guarantee for the class-conditional neural polarizer, establishing an upper bound on backdoor risk. Our method also includes an effective poisoned sample detection mechanism during inference, requiring minimal additional computational cost while achieving high detec-tion accuracy. Extensive experiments across diverse neural network architectures and datasets show that our approach outperforms existing backdoor defense techniques, offering a robust and efficient solution with minimal reliance on clean training data. Overall, the proposed neural polarizer offers a promising direction for enhancing backdoor defense in deep learning models, balancing attack mitigation with performance preservation. Limitations and future work. Although minimal, the method still relies on some clean training data for effective NP layer training. In scenarios with no clean data, defense performance may be compromised. Therefore, a promising direction for future work is to explore data-free training or training the neural polarizer with out-of-distribution samples. Applying our method to multimodal models or large-scale neural networks could be a promising direction, as full finetuning these models is often computationally expensive. By integrating our defense mechanism with such models, we could achieve more robust and scalable backdoor defenses."}, {"title": "A MORE ALGORITHMIC DETAILS AND THEORETICAL RESULT", "content": "In this section, we introduce the PGD attack algorithm [36"}]}