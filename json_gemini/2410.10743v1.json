{"title": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models", "authors": ["Yanbiao Ji", "Chang Liu", "Xin Chen", "Yue Ding", "Dan Luo", "Mei Li", "Wenqing Lin", "Hongtao Lu"], "abstract": "Graphs are a fundamental data structure for representing relationships in real-world scenarios. With the success of Large Language Models (LLMs) across various natural language processing (NLP) tasks, there has been growing interest in integrating LLMs for graph learning. However, applying LLMs to graph-related tasks poses significant challenges, as these models are not inherently designed to capture the complex structural information present in graphs. Existing approaches address this challenge through two strategies: the chain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the graph structure so that LLMs are relieved from understanding spatial positions; and Graph-to-Text Conversion, which translates graph structures into semantic text representations that LLMs can process. Despite their progress, these methods often struggle to fully preserve the topological information of graphs or require extensive computational resources, limiting their practical applicability.\nIn this work, we introduce Node Tokenizer for Large Language Models (NT-LLM), a novel framework that efficiently encodes graph structures by selecting key nodes as anchors and representing each node based on its relative distance to these anchors. This position-anchored encoding effectively captures the graph topology, enabling enhanced reasoning capabilities in LLMs over graph data. Additionally, we implement a task-specific tuning procedure to further improve structural understanding within LLMs. Through extensive empirical evaluations, NT-LLM demonstrates significant performance improvements across a variety of graph-related tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as LLaMA [53] and GPT [39] have revolutionized artificial intelligence. They have demonstrated powerful capabilities in solving various natural language processing (NLP) tasks, including question answering [33, 47], text generation [2, 44], and document understanding [25, 62]. While LLMs have primarily been applied to text data, an increasing number of applications now involve text data intertwined with structured information represented as graphs. For instance, in social networks, nodes represent entities, while edges capture the relationships between them. Both nodes and edges can also be associated with textual descriptions that detail their attributes. Since LLMs are primarily designed to model text in a sequential format, applying them to graph-related tasks aforementioned presents new challenges, particularly in encoding the structural information of graphs.\nTo leverage the strength of LLMs for graph learning, existing efforts can be categorized into two groups [22, 46]: (1) Chain of Tasks, which decomposes graph learning into a chain of tasks, each handled by a distinct model specialized for the corresponding task. For example, LLMs as Prefix first leverages LLMs to process textual information embedded within graph data, and incorporate it as enhanced features into GNNs for downstream tasks [23, 57]. Similarly, GNNs as Prefix uses GNNs to first generate structure-aware embeddings from graph data, which are then fed into LLMs for downstream tasks [51, 52]. Stacking LLMs with GNNs is straightforward yet effective, and they have made promising progress for various applications, such as subgraph matching [7] and 3D point clouds [30]. However, the overall effectiveness of the chain-of-tasks strategy is often constrained by its weakest component, limiting performance. Achieving scalability comparable to LLMs requires an expressive GNN (e.g., with elaborate graph convolution paradigms) of similar scale, which introduces additional computational overhead and complexity, making the models more difficult to train and maintain. (2) Graph-to-text conversion, which translates a graph's structure into a descriptive textual representation. The LLMs-Graphs Integration explores advanced methods of incorporating graph data into LLMs [24, 37], while LLMs-Only focuses on developing effective prompting and tuning techniques to encode graph data into sequential text formats [9, 50]. The underlying assumption is that the powerful capabilities of LLMs can generalize to interpret and manage graph-structured knowledge through textual descriptions. However, such descriptions typically provide only local graph structures [15], which limits the model's ability to capture long-range dependencies in a broader global view. In summary, existing methods fall short in enabling LLMs to effectively understand graph structures, as they rely on intermediate processing through either GNNs or structure conversion, as illustrated in Figure 1. Therefore, a new position encoding, which enables LLMs understand the graph structure, is needed.\nTo address this challenge, we propose the Node Tokenizer for Large Language Models (NT-LLM), a novel framework that efficiently encodes graph structures for seamless integration with LLMs. The core of our method is the strategic selection of key nodes, referred to as anchors, which serve as reference points for encoding the graph topology. Each node is then represented based on its relative distance to these anchors, effectively capturing the structural information of the graph. To further enhance the interpretability of the encoded representations and preserve their partial ordering relation, i.e., nodes close in the graph having similar embeddings, we introduce a transformation function during pretraining that maps them into a continuous Euclidean space. Furthermore, we apply task-specific tuning procedures using prompt tuning and LoRA techniques to facilitate better structural understanding for LLMs.\nExtensive empirical studies demonstrate that NT-LLM substantially improves LLM performance across a diverse range of graph-related tasks, from basic graph analysis to complex reasoning. The main contributions of this paper are as follows:\n\u2022 We introduce a position-anchored graph encoding approach for LLMs that efficiently preserves crucial structural information while reducing the computational complexity associated with traditional graph encoding methods.\n\u2022 We develop a novel graph position embedding pretraining scheme, which seamlessly integrates our graph encoding method with state-of-the-art LLMs. Further task-specific tuning based on the"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Graph Positional Encoding", "content": "Graph Neural Networks (GNNs) have significantly advanced graph representation learning by enabling the extraction of meaningful embeddings from graph-structured data through message-passing mechanisms [3, 4, 27, 38, 55]. However, standard GNN architectures often struggle to differentiate between nodes with similar local structures but different positions within the global graph topology. Graph positional encoding addresses this limitation by enhancing node representations with positional information, allowing the capture of important structural features.\nSeveral approaches have been developed to encode positional information in graphs. Laplacian eigenmaps [5, 6] utilize the eigenvectors of the graph Laplacian matrix for this purpose. In contrast, random walk encodings [8, 42, 63] capture structural information by simulating random walks on the graph. This method encodes the co-occurrence probabilities of nodes during these walks, thereby embedding nodes with similar neighborhoods closer together in the embedding space. More recently, researchers have introduced Distance Encoding [12, 31, 43], which incorporates structural information by encoding the shortest path distances between nodes. Furthermore, Random Feature methods [1, 13] have been developed to approximate positional encodings using learnable or predefined random feature maps. To provide a comprehensive overview of these approaches, Table 1 presents a detailed comparison of various graph positional encoding methods."}, {"title": "2.2 LLMs in Graph-Related Tasks", "content": "The rapid advancements in LLMs have led to their successful application across various domains, leveraging their powerful sequence modeling capabilities [34, 35, 56]. In recent years, there has been a growing interest in applying LLMs to graph-related tasks, aiming to harness their ability to capture long-range dependencies and perform complex reasoning.\nInitial efforts focused on directly inputting textual descriptions of graphs into LLMs to tackle tasks such as node classification and link prediction [15, 21]. While these methods demonstrated the potential of LLMs in understanding graph data, they faced significant scalability challenges due to the complexity of constructing comprehensive prompts and the loss of crucial structural information during the graph-to-text conversion process. To address these limitations, subsequent research has explored the integration of Graph Neural Networks (GNNs) with LLMs to better leverage the strengths of both paradigms [17, 20, 51]. One common approach involves using GNNs to generate structure-aware embeddings, which are then fed into LLMs for downstream tasks [51, 52]. More advanced techniques have delved into model fusion training [64],"}, {"title": "3 PRELIMINARY", "content": ""}, {"title": "Textual Graphs", "content": "A textual graph is a graph in which nodes and edges are associated with textual attributes. Formally, it is defined as G = (V, &, {Tv}v\u2208V, {Te}e\u2208&), where V and & represent the sets of nodes and edges, respectively. Here, Tv and Te denote the textual attributes corresponding to each node and edge, which are usually represented by language descriptions."}, {"title": "Text Encoding via Language Models", "content": "Language Models (LMs) have proven to be highly effective in encoding textual attributes in graphs, producing embeddings that capture rich semantic information. For a given textual attribute Ti associated with a node or edge i, a LM encodes this attribute into an embedding vector as follows:\n$x_i = LM(T_i) \\in \\mathbb{R}^k$ \n(1)"}, {"title": "Large Language Models (LLMs)", "content": "LLMs are trained on vast corpora of textual data, demonstrating emergent capabilities that facilitate advanced semantic understanding and exceptional task generalization. Formally, an LLM parameterized by \\( \\theta \\) takes as input a sequence of tokens \\( X = \\{X_1, X_2, ..., X_n\\} \\) along with a prompt \\( P \\), and generates an output sequence \\( Y = \\{y_1, y_2,..., y_r\\} \\). The probability distribution of the output sequence, conditioned on the concatenated input sequence and prompt \\( [P; X] \\), is expressed as:\n$P_{\\theta} (Y|[P; X]) = \\prod_{i=1}^{r} P_{\\theta}(y_i|y_{<i}, [P; X]),$ \n(2)\nwhere \\( y_{<i} \\) represents the prefix of sequence \\( y \\) up to position \\( i - 1 \\), and \\( P_{\\theta} (y_i|y_{<i}, [P; X]) \\) denotes the probability of generating token \\( y_i \\) given the preceding tokens \\( y_{<i} \\) and the input \\( [P; X] \\)."}, {"title": "Prompt Tuning for LLMs", "content": "Prompt tuning [29] is an efficient technique for adapting LLMs to specific tasks without modifying the model's parameters. This technique keeps the pre-trained LLM frozen, and optimizes a small set of continuous prompt embeddings \\( \\{e_i\\}_{i=1}^n \\), where \\( n \\) is the number of prompt tokens. These prompts are generally initialized either randomly or using the embeddings of specific tokens, and are subsequently optimized throughout the"}, {"title": "4 METHODOLOGY", "content": "We propose NT-LLM, which can seamlessly integrate graph-structure knowledge with LLMs through two key components: Node Position Encoding and Task-Specific LLM Tuning. Our proposal first transforms graph nodes into tokens rich in structural information through pretraining. These tokens are then incorporated into the LLM's input. Subsequently, the LLM undergoes task-specific fine-tuning, enabling it to effectively leverage both textual and graph-based information. Figure 2 illustrates the overall framework of NT-LLM."}, {"title": "4.1 Node Position Encoding", "content": "In large language models, it is straightforward to inject information about the relative or absolute position of tokens in a sequence via their index. However, this approach is not feasible for graphs due to two key differences. First, graphs do not have an inherent linear ordering of nodes, unlike sequences, where tokens follow a clear order. Nodes in a graph are interconnected in a complex, multidimensional structure, where relationships are defined by edges, and there is no natural start or end. Second, the neighborhood of each node can vary significantly in size and shape, which makes the concept of a relative or absolute \"position\" less meaningful. To address this challenge, we propose node position encoding, which consists of three key steps: anchor node identification, relative distance encoding, and positional embedding pretraining."}, {"title": "4.1.1 Anchor Node Identification", "content": "Prior works [11, 60] have demonstrated that using anchor nodes can well capture the position of a given node with respect to all other nodes in a graph. In particular, the position of a node can be described in terms of its relative distance (e.g., shortest path distance) to these anchor nodes. For efficient identification of anchor nodes, we implement a greedy anchor selection algorithm with coverage ratio threshold. The details of this greedy selection procedure are shown in Algorithm 1. Given a coverage ratio CR and coverage radius c, we start with an empty set A of anchor nodes and an empty set Ncover of covered nodes (Line 1). Here, we define that a node u is covered by a node v only if u is in the c-hop subgraph of node v; otherwise, u is considered uncovered by v. Then, we iteratively select a new anchor node that contains the maximum set of uncovered nodes in its c-hop subgraph (Line 3) and add these covered nodes into Ncover (Line 8) until the size of Ncover is no less than CR * |V| (Line 2).\nThe time complexity of this selection process is O(|V|\u00b2 \u00b7 |A| + |V||&|), where |A| is typically much smaller than the number of nodes VI. A comprehensive analysis of the time complexity is provided in Appendix B. To this end, the identified anchor nodes enable us to provide a unique node description for other nodes in terms of their relative distance, capturing both global and local structures within the graph. Given the identified anchor nodes A = {a1, a2, ..., ak}, we encode the position of each node v with respect to these anchors:\n$d_v = (d_1, d_2,..., d_k),$\n(6)\n$d_i = dist(v, a_i), \\forall i \\in \\{1,..., K\\},$\n(7)\nwhere dist(v, ai) denotes the number of hops in the shortest path between node v and anchor node ai.\nUtilizing relative distance, we approximate the shortest distance between any two nodes u and v in the graph defined as:\n$d(u, v) := \\min_{k \\in \\{1,...,K\\}} (d_u[k] + d_v[k]),$\n(8)\nwhere du[k] means the k-th element of du. This approximation estimates the distance by identifying the anchor node that provides the minimal combined distance from u and v.\nCareful readers may notice that our approximated shortest path distance may not necessarily be the actual shortest path distance. However, d(u, v) actually serves as an upper bound for the true shortest path distance between u and v. More formally, the error between the estimated distance and real distance is bounded by parameters c and CR:\nLEMMA 4.1. Given any two nodes u, v from a graph, the error of the estimated shortest path distance can be bounded by 2c with a probability no smaller than 1 \u2013 (1 \u2013 CR)\u00b2, where c is the coverage radius and CR is the coverage ratio."}, {"title": "4.1.2 Positional Embedding Pretraining", "content": "While anchor-based encoding enables the representation of spatial positions for nodes in a graph, it is not directly applicable for positional embeddings in large language models (LLMs). This is because shortest path distances in graph space do not correspond to distances in Euclidean space, potentially distorting actual spatial relationships. Next, we first elaborate this argument, and then present our solution.\nMismatch between Shortest Path Distance and Euclidean Distance. In LLMs, positional embeddings reflect the linear order of tokens, where proximity in the sequence corresponds to closeness in the embedding space, adhering to Euclidean-like assumptions. This enables the model to capture local relationships: tokens near each other in the input sequence are also close in the learned embedding space, preserving context and meaning. However, as demonstrated in Figure 3, when nodes 2 and 4 are set as anchor nodes, the shortest path distances between nodes 2 and 3, as well as between nodes 1 and 2, are both 1 in the graph's non-Euclidean space. In contrast, the corresponding Euclidean distances would be 1 and \u221a2, respectively. This discrepancy in relative distances between node pairs leads to a mismatch between shortest path and Euclidean distances.\nTo address this issue, we propose a pretraining approach that maps the distance encoding from non-Euclidean to Euclidean space, aiming to preserve geometric relationships between nodes. The necessity of this mapping is further justified through ablation studies in Section 5.4. The pretraining process involves a learnable function \u03c6 : RK \u2192 Rn that projects the anchor-based encoding into Euclidean space:\n$e_v = \\varphi(d_v) \\in \\mathbb{R}^n$ \n(9)\nwhere e represents the transformed node embedding for node v.\nTo preserve geometric relationships between nodes in the embedding space, we propose a rank-preserving training objective based on maximum likelihood estimation. The objective is to maximize the posterior probability p(\u03a6|>), where \u0398 denotes the parameters of the mapping function \u03c6, and > represents the desired order of distances. Assuming independence for the ordering of each pair of distances, we formulate the likelihood function as:\n$p(> |\\Phi) = \\prod_{(u,v), (i,j) \\in E} p \\left(d_{\\Phi}(u, v) > d_{\\Phi}(i, j)\\right)^{I(d(u,v)>d(i,j))} \\cdot \\left(1 - p \\left(d_{\\Phi}(u, v) > d_{\\Phi}(i, j)\\right)\\right)^{(d(u,v) \\leq d(i,j)}$\n(10)\nwhere d(u, v) denotes the estimated distance between nodes u and v, and d\u00f8 (u, v) represents the Euclidean distance between their corresponding mapped embeddings eu and ev. We can model the probability of one distance being greater than another using the logistic function \u03c3:\n$p \\left(d_{\\Phi}(u, v) > d_{\\Phi}(i, j)|\\Phi\\right) := \\sigma(\\hat{d}_{u,v,i,j} (\\Phi)),$ \n(11)\nwhere \u00c2u,v,i,j (\u03a6) denotes the difference between the Euclidean distances of the two pairs of mapped embeddings.\nBy maximizing the log-posterior, which is equivalent to minimizing the negative log-likelihood function, we derive the rank-preserving training objective:\n$\\min_{\\Phi} L = - \\sum_{(u,v), (i,j) \\in E} \\left(I(d(u, v) > d(i, j)) \\ln \\sigma(\\hat{d}_{u,v,i,j} (\\Phi)) +I(d(u, v) \\leq d(i, j)) \\ln(1 - \\sigma(\\hat{d}_{u,v,i,j} (\\Phi)))\\right)$\n(12)\nThis objective function encourages the ranking of distances between nodes in the embedding space to align with the ranking of their corresponding shortest path distances in the graph.\nTo facilitate practical implementation, we reformulate the objective as a binary cross-entropy (BCE) loss:\n$min \\mathcal{L} = BCE \\left(\\sigma \\left(\\|e_u - e_v \\|_2 - \\|e_i - e_j \\|_2\\right), y\\right),$\n(13)\nwhere \\(y\\) captures the relative ordering of distances:\n$y = I(d(u, v) > d(i, j)) = \\begin{cases} 1, & \\text{if } d(u, v) > d(i, j), \\\\ 0, & \\text{otherwise}. \\end{cases}$\n(14)\nThis pretraining approach ensures that the positional embeddings derived from graph structures are compatible with the Euclidean assumptions of LLM architectures while preserving the essential spatial relationships between nodes."}, {"title": "4.2 Task-Specific LLM Tuning", "content": "We now focus on adapting LLMs to leverage graph-based knowledge for specific downstream tasks. Our approach integrates prompt tuning with Low-Rank Adaptation (LoRA) for efficient and effective task-specific fine-tuning."}, {"title": "4.2.1 Prompt Tuning", "content": "We employ prompt tuning to incorporate pretrained graph-based knowledge into the LLM. This technique introduces a small, trainable adapter layer that transforms our pretrained anchor-based node embeddings to soft prompts. These soft prompts serve as a learned prefix to the input, guiding the model's attention and output generation.\nThe generation process, including our prompt tuning adapter, can be formally expressed as:\n$P_{\\theta,\\phi}(Y|G, q) = \\prod_{i=1}^r P(\\mathcal{Y}_i|\\mathcal{Y}_{<i}, [e_G; e_r; e_q]),$\n(15)\nwhere e denotes the frozen LLM parameters, I represents the trainable parameters of the prompt tuning adapters, eG is the pretrained positional encoding derived from the graph structure, er signifies the textual embeddings, and eq represents the question designed for corresponding graph tasks. The prompt tuning adapter is implemented as a shallow neural network that maps the input embeddings to a sequence of continuous prompt tokens. These tokens"}, {"title": "4.2.2 Low-Rank Adaptation (LoRA)", "content": "To further enhance the LLMs' adaptability to graph-structure data, we implement Low-Rank Adaptation (LoRA) in conjunction with prompt tuning. LoRA modifies the weight update mechanism of the LLM by introducing low-rank decomposition, allowing for efficient fine-tuning of the model. For each weight matrix W \u2208 Rdim\u2081\u00d7dim2 in the LLM, we introduce a low-rank update:\nW' = W + BA,\n(16)\nwhere B \u2208 Rdim\u2081\u00d7r and A \u2208 Rrxdim2 are low-rank matrices with rank r < min (dim1, dim2). This decomposition significantly reduces the number of trainable parameters, as r is typically much smaller than dim\u2081 and dim2.\nDuring the training process, only A and B are updated while the original weights W remain frozen. The update rule for the LORA parameters can be expressed as:\n$A_{t+1} = A_t - \\eta \\nabla_A \\mathcal{L}(\\Theta, A_t, B_t),$\n(17)\n$B_{t+1} = B_t - \\eta \\nabla_B \\mathcal{L}(\\Theta, A_t, B_t),$\n(18)\nwhere \u03b7 is the learning rate, L is the task-specific loss function, and t denotes the training iteration.\nThe combination of prompt tuning and LoRA in our approach enables the model to effectively incorporate graph-structural knowledge while adapting to various downstream tasks."}, {"title": "5 EXPERIMENTS", "content": "We conduct extensive experiments to demonstrate the effectiveness of our NT-LLM by investigating the following research questions:\n\u2022 RQ1: Can NT-LLM outperform state-of-the-art methods in various graph-related tasks?\n\u2022 RQ2: What does node position encoding learn? Does it capture the spatial information as designed?\n\u2022 RQ3: What influence do variant designs have on NT-LLM?"}, {"title": "5.1 Experimental Settings", "content": "We evaluate our approach on diverse graph-based tasks using benchmark datasets from Cora [49], the Open Graph Benchmark (OGB) [19], and ExplaGraphs [48]. Our experiments cover node classification with Cora and OGBN-arxiv, edge prediction using OGBL-ddi, and graph property prediction employing OGBG-molhiv. Additionally, we assess knowledge graph question answering tasks using the ExplaGraphs dataset. These datasets encompass a wide range of graph structures and task complexities, allowing for a comprehensive evaluation of our method. Table 2 presents key statistics for each dataset, while Figure 4 illustrates their characteristics in detail.\nWe evaluate our proposed method against various baselines, including both traditional graph learning approaches and Large Language Model (LLM)-based methods:\n\u2022 GNN-based methods: We incorporate widely-adopted GNN architectures, including Graph Convolutional Networks (GCN) [28], Graph Attention Networks (GAT) [54], and GraphSAGE [16].\n\u2022 LLM-only methods: We consider approaches that process graph information directly as textual sequences using LLMs. This category includes implementations utilizing zero-shot inference, prompt tuning [29], and Low-Rank Adaptation (LoRA) [18] techniques."}, {"title": "5.2 Main Results (RQ1)", "content": "Table 3 compares the performance of our proposed NT-LLM method against baselines on five benchmark datasets on the corresponding task, respectively. We observe the following key findings:\n\u2022 NT-LLM consistently outperforms all baseline methods across various tasks and datasets. This observation justifies the superiority of NT-LLM and demonstrates its effectiveness and broad applicability in graph learning.\n\u2022 NT-LLM effectively addresses the challenge of enabling LLMs to understand graph structures. In other words, NT-LLM leverages the strengths of LLMs in understanding textual attributes while benefiting from our proposed node position encoding to capture the graph topology. First, NT-LLM outperforms pure LLM and GNN baselines on all datasets. This observation demonstrates that understanding textual attributes and topology are equally important forp graph learning tasks. Second, when fine-tuning NT-LLM with LoRA (fine-tuned NT-LLM), it performance surpasses LLM-GNN hybrid approaches. This suggests that NT-LLM is more effective at enabling LLMs to understand graph structures compared to intermediate solutions, i.e., LLM-GNN hybrid approaches.\n\u2022 The superiority of NT-LLM in graph understanding comes from our proposed node position encoding. In particular, the OGBL-ddi dataset lacks textual attributes. As we can see, LLM methods perform worse than GNN baseline methods, which highlights their limitations in capturing topological information from graph data. Unlike LLM methods, our proposed NT-LLM, despite not using GNNs, outperforms all baselines with over 60% improvement compared to LLM methods, demonstrating its ability to effectively encode graph structure.\nIn conclusion, NT-LLM shows superior performance and adaptability across various graph-related tasks and datasets. The improvements over state-of-the-art baselines, even in the absence of textual attributes, highlight the effectiveness of our proposed method in capturing both textual and structural information."}, {"title": "5.3 Understanding Node Position Encoding (RQ2)", "content": "To understand what node position encoding learns, in this section, we provide visualization for the learned node position embedding on the Cora dataset to gain further insights. We select this dataset because, in Cora, nodes from the same class tend to be naturally closer in the graph structure. This property allows us to directly evaluate the quality of the node position embeddings by observing how well they align with the class labels.\nFigure 5 illustrates the embeddings before and after the transformation in positional embedding pretraining, shown against class labels. As we can see, prior to the transformation, nodes belonging to the same class can be separated distantly in the embedding space. However, after applying the transformation, these nodes are effectively projected into the same region, highlighting the efficacy of our pretraining approach in capturing the underlying semantic relationships among nodes. For instance, the green dots, which are dispersed before the transformation, become densely clustered afterward."}, {"title": "5.4 Ablation Studies (RQ3)", "content": "In this section, we conduct extensive ablation studies to investigate the effectiveness of each component in NT-LLM, and justify our model design choices.\nNT-LLM has specific design features, including the node position encoding, its corresponding pretraining task, and two different strategies for LLMs to leverage node position encoding, i.e., prompt tuning and low-rank adaptation. We evaluate the performance of the each variants of our model on five datasets as follows:\n\u2022 w/o PE: The NT-LLM without positional encoding, using raw node features as input to the LLM.\n\u2022 w/o Pre: The NT-LLM without the distance transformation pre-training module, using concrete anchor-based distances as node position embeddings.\n\u2022 w/o PT: The NT-LLM without the prompt tuning module, directly inputting all embeddings into the LLM.\nTable 4 presents the results of the ablation study, which evaluates the impact of removing individual components from the proposed method. The observed performance drop across all datasets confirms the importance and complementary nature of each component within the method. In particular, we observe that node position encoding pretraining is critical for NT-LLM. The variant without pretraining (w/o Pre) experiences a significant performance drop when the pretraining module is removed, supporting our argument in Section 4.1.2. This is due to the mismatch between shortest path and Euclidean distances, which distorts actual spatial relationships. Therefore, positional embedding pretraining is an indispensable component of NT-LLM."}, {"title": "5.4.2 Effectiveness of Anchor Selection Strategies", "content": "Since anchor nodes offer a comprehensive view of the graph structure, different strategies for identifying anchor nodes may impact NT-LLM's ability to comprehend the graph. In this section, we conduct an extensive evaluation of various anchor selection strategies, using a fixed seed and the NT-LLM architecture. Experiments are conducted on three datasets: Cora, OGBN-arxiv and OGBL-ddi. The inputs are positional embeddings derived from different anchor sets, all subjected to identical pretraining procedures. Table 5 presents the experimental results. Our method achieves best performance among all evaluated strategies, outperforming both traditional centrality-based approaches (such as Degree and PageRank [40]), random selection and landmark-based HPLC [26]."}, {"title": "5.4.3 Impact of Hyperparameters", "content": "We investigate the impact of two key hyperparameters in NT-LLM: the coverage radius c and the coverage ratio CR. Figure 7 presents the relationships between these hyperparameters, model accuracy and the number of anchor nodes. The results demonstrate that smaller values of c and larger values of CR generally lead to a better performance. This trend aligns with the error bound established in Lemma 4.1. Notably, we observed that the number of anchor nodes increases exponentially as c decreases and CR increases. This relationship underscores the importance of carefully selecting these hyperparameters to balance computational complexity and model performance."}, {"title": "6 CONCLUSION", "content": "This paper presents NT-LLM, a novel approach that enables Large Language Models (LLMs) to understand graph-structured data. We introduce node position encoding, which allows NT-LLM to efficiently encode graphs while preserving critical structural information. We also demonstrate that node position encoding can easily integrating into popular LLM tuning strategies. We conducted extensive experiments on various graph benchmarks, which demonstrated that NT-LLM consistently outperforms existing techniques. These results highlight the potential of NT-LLM for further research at the intersection of Graph Neural Networks (GNNs) and LLMs."}, {"title": "A PROOF OF LE\u039c\u039c\u0391 4.1", "content": "Lemma. Given any two nodes u, v from a graph, the error of the estimated shortest path distance can be bounded by 2c with a probability no smaller than 1 \u2013 (1 \u2013 CR)\u00b2, where c is the coverage radius and CR is the coverage ratio.\nPROOF. Given node pair u, v from graph and a set of anchor nodes A = {a1, a2, . . ., ak }, assume u is covered by an anchor node, denoted as a*, then the shortest path distance between them d(u, a*) \u2264 c. Without loss of generality, we assume d(u, a*) < d(a*, v). Note that the following error bound still holds if d(u, a*) > d(a*, v). The error of the estimated shortest path distance between u, v is bounded by\nerr(u, v) = d(u, v) \u2013 d(u, v)\n= mina\u2208 A(d(u, a) + d(a, v)) \u2013 d(u, v)\n\u2264 d(u, a*) + d(a*, v) \u2013 d(u, v)\n\u2264 d(u, a*) + d(a*, v) \u2013 |d(u, a*) \u2013 d(a*, v)|\n= 2d(u, a*)\n< 2c\nThe error bound holds when either u or v are covered by some anchor nodes. When neither u nor v is covered, this error is unbounded. The probability for this case is (1 \u2013 CR)\u00b2. Therefore, the probability that the error of an estimated distance is bounded is 1 (1 - CR)\u00b2."}, {"title": "B TIME COMPLEXITY ANALYSIS", "content": "The time complexity of the greedy algorithm for anchor node selection can be analyzed in two parts:\nInitialization. Each node performs a BFS to construct its c-hop neighborhood, requiring O(|V|\u00b7|8|) time, where |V| is the number of nodes and |&| is the number of edges in the graph. The c-hop neighborhoods are stored for each node.\nAnchor Selection. In each iteration, the algorithm selects an anchor and updates the coverage for remaining nodes. The worst-case time complexity for this part is O(|V|\u00b2 \u00b7 |A|), where A is the number of selected anchors. This is because:\n1. Selecting an anchor requires examining all uncovered nodes in each candidate's c-hop neighborhood (O(|V|) in the worst case).\n2. After selecting an anchor, the algorithm must update the uncovered node counts for all other nodes' c-hop neighborhoods that overlap with the newly covered area (O(|V|) nodes to update, each potentially affecting O(|V|) other neighborhoods).\nThe total time complexity is thus O(|V| \u00b7 |E| + |V|\u00b2 \u00b7 A). In the worst case, where A approaches |V|, this becomes O(|V|\u00b7 |E| + |V|\u00b3). However, in practice, |A| is often much smaller than |V|, leading to better average-case performance."}, {"title": "C ADDITIONAL IMPLEMENTATION DETAILS", "content": "GNN-based methods utilize a 4-layer architecture with a hidden dimension of 256, ReLU activation, and a dropout rate of 0.5. These models are trained for 500 epochs using the AdamW optimizer [36] with a learning rate of 1e-3 and a weight decay of 5e-4. For LLM-only methods, we use the pre-trained LLaMA3-8B model without fine-tuning for zero-shot inference, and fine-tune using the same hyperparameters as our proposed method for prompt tuning and LoRA. GNN-LLM hybrid baselines employ a 4-layer GAT and LLaMA3-8B, following their original architectures and recommended settings. In LLM fine-tuning, we configure LoRA with a low-rank matrix dimension of 8 and a scaling factor of 16, using the AdamW optimizer with a learning rate of 1e-4 and a weight decay of 0.05. Experiments are conducted for a maximum of 10 epochs with early stopping based on a patience of 3 epochs. Batch sizes are set to 32 for OGBN-arxiv and OGBL-ddi, and 2 for OGBG-molhiv and ExplaGraphs, accommodating their varying sizes and complexities."}]}