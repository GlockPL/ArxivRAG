{"title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with Pythonic Syntax", "authors": ["Augusto Seben da Rosa", "Marlon Daniel Angeli", "Jorge Aikes Junior", "Alef Iury Ferreira", "Lucas Rafael Gris", "Anderson da Silva Soares", "Arnaldo Candido Junior", "Frederico Santos de Oliveira", "Gabriel Trevisan Damke", "Rafael Teixeira Sousa"], "abstract": "We developed a jitted compiler for training Artificial Neural Networks using C++, LLVM and Cuda. It features object-oriented characteristics, strong typing, parallel workers for data pre-processing, pythonic syntax for expressions, PyTorch like model declaration and Automatic Differentiation. We implement the mechanisms of cache and pooling in order to manage VRAM, cuBLAS for high performance matrix multiplication and cuDNN for convolutional layers. Our experiments with Residual Convolutional Neural Networks on ImageNet, we reach similar speed but degraded performance. Also, the GRU network experiments show similar accuracy, but our compiler have degraded speed in that task. However, our compiler demonstrates promising results at the CIFAR-10 benchmark, in which we reach the same performance and about the same speed as PyTorch. We make the code publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence Neural Networks allowed the creation of models for label classification [16], text generation [7, 32], image generation [30], robotics [6, 34] and beyond. The foundations of these models relies on coding languages such as Python and Julia\u00b9, and on frameworks such as PyTorch [28], Tensorflow\u00b2 and"}, {"title": "Related Work", "content": "Other neural network academic coding languages are RADENN [27], which focuses on simplifying neural network and the data pipeline declaration; Deep-Scratch [2] that created a visual coding language with focus on facilitating the learning of deep learning; DeepDSL [37] which is a domain specific language embedded in Scala, and it was an early implementation of portable neural network code by compiling them into Java source code.\nThere are also other LLVM [18] based coding languages with the purpose of facilitating to write compute efficient mathematical code, like Julia. This language also supports neural networks training. Currently, there are 139 thousand github PyTorch repositories and 21 thousand Julia repositories\u2074.\nIn terms of compiler related methods, we take fully inspiration from Python. It is an object-oriented, high-level language and that disposes the use of brackets in conditional and control structures by using indentation only. Its simplicity accelerates the development time when compared to other coding languages like C++ or Java [20]. This characteristic made the coding language popular, being currently the most used coding language for training neural networks."}, {"title": "Compiler Methods", "content": "Kaleidoscope [21] is the basis of our compiler. It successfully generates code for condition and control structures and recursion calls. However, it does not contain support for multiple instructions inside these structures and it is not object-oriented. Also, its scope control is incompatible with multiple threads, since it creates only one scope per function, but we need to have multiple scopes for the same function if it is called by different threads.\nWe allow the use of multiple instructions by assigning a list of instructions for condition and control structures. This assignment is organized during the parsing phase [1], and we parse it by indentation, like Python. Furthermore, in order to make the language object-oriented, we reverse engineer the Python self argument of class methods. That is, when an object calls a method we send the object name as the first argument of the function. Then, when there is a variable using the self expression, we concatenate the object name contained on the function first argument with the variable name.\nFollowing this line of thought, we also generate a new scope at function calls and send it as the function second argument. Every variable that does not use the self expression is concatenated with the scope. We also pass the previous scope as the third argument, so we can send variables into the scope from which they were called when using return expressions. We hide these three arguments at the high-level language for simplicity and they are only present at user declared functions (non-native functions).\nIn order to implement parallelism, we followed the guide from the Bolt compiler [29]. That is, C++ has a function for thread that allows to create threads for functions by passing the pointer of these functions. Bolt uses LLVM to call this C++ thread function and it passes the pointers of its high level functions to create concurrency.\nAlso, we realized that everything in Kaleidoscope is executed as a function, and therefore we were able to send the function pointer of any code section. We implement parallelism using finish/async expressions, i.e, finish is a structure that contains a list of normal/serial instructions and async (parallel) instructions. It waits for the async and the serial instructions to finish before exiting the structure. Beyond that, we use scope control and add mutex locks at cpu variable attributions in order to avoid data races [5] \u2013 otherwise they would lead to segmentation faults.\nBesides, we had to use C++ char pointers instead of LLVM variable Allocas for representing strings, because with LLVM Allocas the code would crash when"}, {"title": "Neural Network Methods", "content": "In this Section, we present the mechanisms of Automatic Differentiation, Memory Pooling and Operation Overlapping."}, {"title": "Automatic Differentiation", "content": "We leverage the pre-existing structures of our compiler to build the Backpropagation algorithm [31] using Automatic Differentiation [15].\nCompilers are restricted by hardware to use the form of three address code during the intermediate-code generation phase [1] for numerical expressions. Also, coding languages traditionally organize parsed instructions into Abstract Syntax Trees (AST) [1]. Thus, it is straightforward to represent this expressions using a binary tree [9], like in Kaleidoscope.\nThis process is observed at Figure 1a, in which the parsing algorithm mounts a binary tree from the expression $y = x@w+x$, i.e, the assignment of y from the result of adding x with the matrix multiplication between x and w, expressed by the operator @ which transposes w. The Kaleidoscope algorithm for this parsing organizes nodes according to their precedence: high precedence operators like @ should be solved first and thus they are close to leaf nodes, whereas low precedence operators like + and = should be solved later."}, {"title": "Caching and Pooling", "content": "Beyond the Backpropagation, it is also important to apply efficient memory management algorithms. PyTorch applies the mechanisms of Caching and Pooling to maximize this efficiency, and we derive from these mechanisms for NSK.\nWe implement caching for memory information that should be kept across multiple forward and backward iterations. For example, the memory of weight and biases gradients: they consist of a global variable dictionary that maps a parameter's name to its float pointer. This pointer is accumulated every time it is seen during the Backpropagation and it is overwritten with zeros after the optimizer finishes its iteration.\nFurthermore, the naive approach to deal with tensors that are allocated at each iteration - intermediate operation results and non-weight gradients - is to malloc and free their pointers at every iteration, as illustrated in Figure 2. However, when tensors (gpu float pointers) have the size of millions of floats, this repetitive operation adds a heavy computational overhead.\nHence, memory pooling is a technique applied to increase the efficiency of frequent allocation and deallocation of memory, like on the Linux Kernel [24]. Besides, PyTorch also uses memory pooling for the memory management of tensors. Thus, we also apply memory pooling for tensors that should be allocated and deallocated at each iteration.\nAs demonstrated by Figure 3, we create a dictionary that maps the size of a required tensor into a list of gpu float pointers containing that size. The list of pointers for a given size will be empty the first time a tensor is required, we thus malloc a new pointer with that demanded size on the gpu on this case and send it to the operation it was required from. When the tensor is no longer needed, we append its pointer to the list of pointers containing that size on the dictionary instead of freeing it. Now that the pool is not empty anymore, we sample a float pointer from the pool whenever required instead of allocating it."}, {"title": "Operation Overlapping", "content": "Besides low-level Cuda kernel optimizations \u2013 like kernel fusing [3, 11], operation overlapping is the last PyTorch optimization we are aware of.\nTo design Cuda efficient kernels it is desired to maximize GPU occupancy [3, 10]. In other words, when we launch kernels that are cheap to compute, we want to launch as many parallel kernels as possible. One of the ways PyTorch does this is by overlapping data loading and model training using different Cuda streams. This means that while a network is processing its batch, PyTorch will already start to load the next batch on the GPU because these operations belong to different streams.\nWe apply this overlapping technique at our compiler. We also launch a separate stream for each parameter group of the optimizer."}, {"title": "Experiments and Discussion", "content": "The Python experiments were run using Python 3.10.14, PyTorch 2.2.2 and cuDNN10 8.9.2. NSK used cuDNN 8.9.7. We used Cuda 12.1 for both PyTorch and NSK. The hardware comprises a RTX 4090, a i9-13900KF and 32 GB of RAM.\nWe execute CNN experiments on MNIST [13], CIFAR-10 [17] and ImageNet [12]. And the Recurrent Neural Networks (RNN) experiments at the IDMB Sentiment Analysis benchmark [25]. We use 3 parallel workers for all experiments except ImageNet, in which we use 12. Also, we implemented the image augmentations at the GPU instead of at the CPU workers.\nCurrently, NSK does not support dictionaries. Thus, we had to change the folder name of each dataset class to a class number. Then, we splitted the file path string and associated the instance class to that number.\nWe follow the ResNet recipe of [16] with 20 layers for the CIFAR-10 benchmark. The only differences we are aware of is the weight initialization \u2013 we use xavier uniform [36], and the data augmentations: we use random crop with padding 4 and random horizontal flip. This benchmark results are demonstrated in Table 1."}, {"title": "Conclusion", "content": "We have implemented a jitted coding language that supports concurrency, object orientation and neural network training. Despite our Backpropagation implementation successfully trained residual and recurrent neural networks, there is still need of a better memory pooling, speed improvements and higher architectures support.\nFor future work, we need to improve the efficiency of the memory pooling mechanism. Furthermore, we noticed that on the PyTorch repository there are several operations which have a fused version with the Relu [26] activation function. We pretend to apply this type of operation fusion and to optimize recurrent neural networks as well, similar to [3].\nOn the long term, we pretend to add support of Generative Adversarial Networks [14], Transformers [33], Diffusion Models [30], Reinforcement Learning and Audio Neural Networks. We will also implement class inheritance on the future."}]}