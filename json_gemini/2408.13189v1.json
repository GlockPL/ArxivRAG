{"title": "Accelerating the k-means++ Algorithm by Using Geometric Information", "authors": ["Guillem Rodr\u00edguez Corominas", "Maria J. Blesa", "Christian Blum"], "abstract": "In this paper, we propose an acceleration of the exact k-means++ algorithm using geometric information, specifically the Triangle Inequality and additional norm filters, along with a two-step sampling procedure. Our experiments demonstrate that the ac- celerated version outperforms the standard k-means++ version in terms of the number of visited points and distance calculations, achieving greater speedup as the number of clusters increases. The version utilizing the Triangle Inequality is particularly effective for low-dimensional data, while the additional norm-based filter enhances performance in high-dimensional instances with greater norm variance among points. Additional exper- iments show the behavior of our algorithms when executed concurrently across multiple jobs and examine how memory performance impacts practical speedup.", "sections": [{"title": "Introduction", "content": "The k-means clustering is a widely used method in data clustering and unsupervised machine learning, aiming to divide a given dataset into k distinct, non-overlapping clusters. This division seeks to minimize the within-cluster variance.\nThe k-means clustering problem becomes NP-hard when extended beyond a single di- mension [3]. Despite this complexity, there are algorithms designed to find sufficiently good solutions within a reasonable amount of time. Among these, Lloyd's algorithm, also referred to as the standard algorithm or batch k-means, is the most renowned [42].\nThe k-means algorithm is one of the most popular algorithms in data mining [58, 32], mainly due to its simplicity, scalability, and guaranteed termination. However, its perfor- mance is highly sensible to the initial placement of the centers [5]. In fact, there is no general approximation expectation for Lloyd's algorithm that applies to all scenarios, i.e., an arbi- trary initialization may lead to an arbitrarily bad clustering. Therefore, it is crucial to employ effective initialization methods [24]."}, {"title": "Related Work", "content": "As previously stated, the computational cost of the k-means++ algorithm can become im- practical for very large instances. Thus, many methods have been proposed to accelerate the algorithm.\nThe k-means++ algorithm goes through two phases at each iteration. First, when a new center is added to determine whether or not the new one is the closest one the distances from all points to the newly added center have to be recalculated. The computational cost of this phase comes from calculating the distance between them, which can be huge in high-dimensional spaces. Second, the $D^2$ sampling phase requires scanning the whole data, and due to its sequential nature, it becomes difficult to parallelize. Thus, the methods usually split in two: approximate methods, which modify some of the aspects of the algorithm to be more easily computable while trying to obtain provably similar results; and exact accelerations, which maintain the same base idea but try to optimize some of its aspects, thus yielding the same results as the standard approach.\nIn [10], a parallel implementation of the k-means++ algorithm was proposed, named k- means||, which obtains the same guarantee in expectation as the standard approach. The main idea is to sample more points than necessary. The algorithm first oversamples the number of centers to $k \\log n$, by performing $O(\\log n)$ rounds of sampling in which $k$ points are sampled in parallel. Then, the $O(\\log n)$ sampled points are clustered again using the standard k-means++ algorithm to select the final $k$ centers. Although the total complexity of the algorithm rises to $O(ndk \\log n)$, the speedup comes from the effective distribution of the work in a parallel process. Some authors have proposed enhancements to this method."}, {"title": "Preliminaries", "content": "This section introduces concepts and methods to aid us in subsequent sections. The main in- tention behind these concepts is to reduce the computational time employed by the algorithm by avoiding unnecessary or more complex calculations."}, {"title": "Distances and Metrics", "content": "The Euclidean Distance (ED) represents the shortest path between two points in the Euclidean space. More specifically, given two points $\\vec{x}$ and $\\vec{y}$ in a d-dimensional Euclidean space $\\mathbb{R}^d$, the ED is calculated as follows:\n$ED(\\vec{x}, \\vec{y}) = ||\\vec{x} - \\vec{y}||_2 = \\sqrt{\\sum_{j=1}^d (x_j - y_j)^2}$         (1)\nwhere $| |\\vec{x} - \\vec{y}||_2$ denotes the $l_2$ norm of the vector difference between $\\vec{x}$ and $\\vec{y}$. As ED is a metric, it satisfies the Triangle Inequality (TIE), which states that the direct path between two points is the shortest. More specifically,\n$d(\\vec{x}, \\vec{y}) \\leq d(\\vec{x}, \\vec{z}) + d(\\vec{z}, \\vec{y})$       (2)\nAs its name indicates, the Squared Euclidean Distance (SED) modifies the ED by squaring its value. This modification, while seemingly straightforward, significantly impacts various"}, {"title": "The Triangle Inequality", "content": "The Triangle Inequality (TIE) is a fundamental property extensively utilized in enhancing the efficiency of clustering algorithms [26] as well as in related fields such as speeding up the closest codework search process in Vector Quantization (VQ) [30].\nIn algorithms like the k-means, assigning a point to its nearest center is a major time- consuming task. For a given set of centers $\\mathcal{C}$ and a point $\\vec{p}$ in a d-dimensional space, the objective is to assign $\\vec{p}$ to the closest center $\\vec{c}_{best} \\in \\mathcal{C}$, i.e., ensuring that $d_{min} = d(\\vec{p}, \\vec{c}_{best}) < d(\\vec{p}, \\vec{c})$ for all $\\vec{c} \\in \\mathcal{C} \\setminus {\\vec{c}_{best}}$, where $d_{min}$ is the distance from point $\\vec{p}$ to its closest center. When $d$ is a metric, as per the TIE, the following holds true: $d(\\vec{p}, \\vec{c}_{best}) \\leq d(\\vec{p}, \\vec{c}) + d(\\vec{c}, \\vec{c}_{best})$. Given that $d(\\vec{p}, \\vec{c}_{best}) \\leq d(\\vec{p}, \\vec{c})$ by definition, we can substitute into the previous equation and obtain\n$d(\\vec{c}, \\vec{c}_{best}) \\leq 2 \\cdot d(\\vec{p}, \\vec{c})$ .\nTherefore, any center $\\vec{c}$ can be disregarded if\n$d(\\vec{c}, \\vec{c}_{best}) > 2 \\cdot d(\\vec{p}, \\vec{c}) \\geq 2 \\cdot d(\\vec{p}, \\vec{c}_{best}) = 2 \\cdot d_{min}$,        (4)\ni.e., any center $\\vec{c}$ meeting the condition $d(\\vec{c}, \\vec{c}_{best}) > 2 \\cdot d_{min}$ can be safely rejected. This implies that finding a tighter $d_{min}$ in the early stages allows for the dismissal of more potential points.\nSince the ED is a metric, the above equation applies, meaning any center $\\vec{c}$ satisfying $ED(\\vec{c}, \\vec{c}_{best}) > 2 \\cdot ED_{min}$ can be discarded as nearest center. However, as the SED is not a metric, the TIE cannot be directly applied. Nonetheless, by squaring the aforementioned equation, given the non-negativity property of the ED, the following can be derived:\n$SED(\\vec{c}, \\vec{c}_{best}) > 4 \\cdot SED_{min}$               (5)\nThis adjustment allows the use of the SED in a similar context."}, {"title": "Norm-based filters", "content": "Given a point $\\vec{p}$ and a center $\\vec{c}$ in a d-dimensional space, the following two inequalities can be derived from the TIE:\n1. $d(\\vec{O}, \\vec{p}) \\leq d(\\vec{O}, \\vec{c}) + d(\\vec{c}, \\vec{p})$\n2. $d(\\vec{O}, \\vec{c}) \\leq d(\\vec{O}, \\vec{p}) + d(\\vec{p}, \\vec{c})$\nHereby, $\\vec{O}$ is the origin in d dimensions. Given that $d$ must be a metric to satisfy the TIE and that the ED between the origin $\\vec{O}$ and any given point is equal to the norm of the point, we can express the previous equations as follows:\n1. $| |\\vec{p}||_2 \\leq | |\\vec{c}||_2 + ED(\\vec{c}, \\vec{p})$\n2. $| |\\vec{c}||_2 \\leq | |\\vec{p}||_2 + ED(\\vec{p}, \\vec{c})$\nThus, knowing that $| |\\vec{p}||_2 - | |\\vec{c}||_2 \\leq ED(\\vec{p}, \\vec{c})$ and $| |\\vec{c}||_2 - | |\\vec{p}||_2 \\leq ED(\\vec{p}, \\vec{c})$, we obtain the following equation by combining them:\n$| | ||\\vec{c}||_2 - | |\\vec{p}||_2 | \\leq ED(\\vec{p}, \\vec{c}),$                 (6)\ni.e., the difference in norm between point $\\vec{p}$ and center $\\vec{c}$ is constrained by their ED. Suppose we start with an initial center $\\vec{c}_{best}$ which we suppose is the closest one to point $\\vec{p}$, and let $d_{min}$ be their distance, i.e., $d_{min} = ED(\\vec{p}, \\vec{c}_{best})$. Then, any potential best center $\\vec{c}$ must be closer to $\\vec{p}$ than $\\vec{c}_{best}$, i.e., $ED(\\vec{p}, \\vec{c}) \\leq d_{min}$. Then, by Equation 6, we obtain:\n$| | ||\\vec{c}||_2 - | |\\vec{p}||_2 | \\leq ED(\\vec{p}, \\vec{c}) < d_{min}$\nwhich leads us to exclude any center $\\vec{c}$ that fulfills:\n$| | ||\\vec{c}||_2 - | |\\vec{p}||_2 | \\geq d_{min},$                  (7)\nThis filter can also be applied using the SED by squaring both sides of Equation 6, given that they are both positive, thus obtaining:\n$(| | ||\\vec{c}||_2 - | |\\vec{p}||_2 )^2 \\leq SED(\\vec{p}, \\vec{c})$                (8)\nThis norm-based filtering approach, as well as other similar methods, have been employed in the k-means literature [27, 44, 59], and have also found applications in closely related fields such as Vector Quantization [57] and Color Quantization [31]."}, {"title": "Accelerating k-means++", "content": "In this section, we first introduce a detailed description of the standard k-means++ algorithm. Then, we introduce an exact acceleration of the algorithm using the TIE along with a two-step sampling procedure, and further refine it using the previously introduced norm filters."}, {"title": "Standard k-means++", "content": "Algorithm 1 outlines the pseudo-code for the standard k-means++ algorithm. The algorithm starts by selecting a random point as the first center. Then, subsequent centers are selected using a \"roulette wheel selection\" mechanism (line 6). Let $w_i$ be the weight assigned to each point $x_i \\in X$, where $w_i$ is equal to the SED of the corresponding point to its closest center. Then, roulette wheel selection operates by assigning each point $x_i$ a selection probability $p_i$ proportional to its weight $w_i$. More specifically, it selects point $\\vec{x}$ with probability $p_i = \\frac{w_i}{\\sum_{j=1}^n w_j}$, i.e., it performs a weighed probability selection. Thus, points with a higher weight are more likely to be selected as centers. This is called $D^2$ sampling.\nTo this end, a random number $r$ is drawn from a uniform distribution between 0 and the total sum of weights. We then iterate through the weighted points cumulatively, selecting the point $\\vec{x}_i$ where the cumulative sum of weights just exceeds $r$. While this process typically requires linear time, O(n), due to the need to traverse through the points until the appropriate one is selected, it could potentially be optimized by pre-calculating cumulative weights and applying binary search. However, this optimization only yields time savings in scenarios where multiple points are chosen without altering the probability distributions, as it initially requires O(n) time to compute the cumulative weights but allows us to get the subsequent points in logarithmic O(logn) time. However, in this scenario, given that the weights are updated after each center selection, and only one point is chosen per iteration, the potential gains from employing binary search are to be neglected.\nUpon selecting a new center, the distance between each point and its closest center must be recalculated since the new center might become the closest to a subset of points (line 5). Therefore, the weight of a point is updated as $w_i = min_{c \\in C} SED(\\vec{x}_i, c)$. Although the straightforward procedure would entail checking all centers for each point-leading to O(kn) complexity per iteration and an overall complexity of O($k^2n$)\u2014one can optimize by comparing each point only with the newly selected center and adjusting the closest center accordingly, using the fact that the closest center prior to the introduction of the new one remains the nearest among all predecessors. This optimization maintains the algorithm's runtime at O(kn). Then, this process is repeated until all of the centers have been selected. Note that the space complexity of this algorithm is O(n)."}, {"title": "Using the TIE", "content": "Although the time complexity of the k-means++ algorithm is linear with respect to both the number of data points and clusters, it may become impractical for handling large datasets. As previously stated, the high computational cost comes from both phases: the calculation of the distances between each point and its closest center and the sampling procedure, as both need to iterate over the whole dataset in the worst-case scenario.\nAs previously stated, the TIE has been effectively employed to accelerate both the k- means [21, 26] and k-means++ [60, 47] algorithms. In this work, we detail the application of TIE to the exact k-means++ algorithm. This is done in a similar way as in the recently introduced Ball k-means [59]. Our application uses the SED instead of the ED and adapts the sampling procedure to this framework using a two-step procedure. The pseudo-code of the procedure is presented in Algorithm 2."}, {"title": "First acceleration action", "content": "The first acceleration action (named as Filter 1 at line 15 of Algorithm 2) involves applying the TIE to bypass the calculation of distances between each point and a newly introduced center. Let a(i) denote the center to which the point $x_i$ is assigned, i.e., a(i) = $j \\leftrightarrow arg min_{c \\in C} d(x_i, c) = c_j$. Then, we denote cluster $j$ as $P_j \\subseteq X$, where $P_j$ denotes the set of points assigned to it, i.e., $P_j = {x_i \\in X | a(i) = j}$, with $\\vec{c}_j$ representing the respective cluster center. Hence, points are categorized based on their current assigned cluster.\nFor every cluster $j$, we maintain the maximum distance from its center $c_j$ to any of its assigned points, denoted by $r_j$. This maximum distance indicates that all points belonging to cluster $j$ are enclosed within a hyper-sphere of radius $r_j$ centered at $c_j$. By the TIE rule previously introduced in Equation 4, if the distance between a newly introduced center $c_{new}$ and the existing cluster center $c_j$ exceeds twice the radius of that cluster's hyper-sphere, then it is guaranteed that no point within cluster $j$ is closer to $c_{new}$ than to $c_j$. This is because, by definition, the distances from all points in the cluster to its center are less than or equal to $r_j$. As previously stated, the SED can be used instead, which is less computationally expensive. Hence, we can discard any cluster $j$ for which the following condition holds true:\n$SED(\\vec{c}_j, \\vec{c}_{new}) \\geq 4 \\cdot r_j,$          (9)\nwhere, in this case, $r_j = max{SED(x_i, c_j) | x_i \\in P_j}$.\nAs previously outlined, the first center $c_1$ is selected uniformly at random among the set of points. Consequently, all of the points initially belong to the first cluster ($P_1$), with a(i) = 1 for all $i \\in X$. Moreover, the radius $r_1$ is set to the maximum distance between any point and $c_1$, i.e., $r_1 = max{SED(x_i, c_1) | x_i \\in X}$. This initialization can be found at lines 1 to 7. Upon the selection of a new center $c_{new}$, we can bypass the comparison with all points of a cluster $j$ by using the previous equation, which implies that the new center $c_{new}$ is too distant from $\\vec{c}_j$ to be nearer to its assigned points, thereby saving significant computation time.\nOn the other hand, if this initial filter fails, we must iterate over all points within the cluster. Yet, the TIE can further be used to avoid the direct distance computations between a point $\\vec{x}_i$ and the new center $c_{new}$ by using Equation 5 to forego the explicit distance computation. This is indicated as Filter 2 in the algorithm (line 17). If this second filter fails, then the respective distance must be calculated."}, {"title": "Second acceleration action", "content": "The second enhancement concerns the roulette wheel selection procedure. In the worst case, performing roulette wheel selection requires iterating over the entire dataset. However, by grouping the points by the cluster to which they belong, we can streamline this process using the following two-step procedure. First, we can determine the cluster from which the next center will be selected by applying roulette wheel selection based on the sum of the weights of the points in each cluster. In the second phase, we only need to apply roulette wheel selection to the set of points belonging to the selected cluster.\nAs previously stated, the probability of each point being selected is proportional to the SED with respect to its assigned center. Rather than executing roulette wheel selection on the set of all points, the following is done. Let $s_j$ be the sum of the weights of all of the points in cluster $j$, i.e., $s_j = \\sum_{x_i \\in P_j} w_i$. Recall that $w_i = SED(x_i, j)$. Then, we proceed as follows. First, the cluster from which the next center will be drawn is selected by roulette wheel selection with respect to the cluster weights $s_j$ (line 10). Thus, the probability of selecting a cluster $j$ is $p_j = \\frac{s_j}{\\sum_{l \\in k} s_l}$. Then, once the cluster is selected, we select a point from that cluster also by roulette wheel selection over the points of the selected cluster, knowing that their sum is equal to $s_j$ (line 11). Note that by doing this in two steps, we do not change the probabilities of each point being selected, and the procedure is equivalent to the standard sampling procedure. As the weights of the points of the clusters that we do not update"}, {"title": "Using additional geometric information", "content": "The TIE filter proves to be particularly effective at the later stages of the process. As additional centers are incorporated, the radius associated with each cluster tends to diminish, thus increasing the likelihood of excluding other clusters from consideration. Nonetheless, this procedure can be further refined, especially in the algorithm's early stages, where clusters are larger, which can limit the computational savings achievable through this filter.\nFigure 1 shows an example of a clustering in a two-dimensional space. The data points are depicted in purple and the centers in red, with the currently assigned center marked by a star shape. The area highlighted in yellow represents twice the radius of the cluster formed by these points, assuming we are using the ED for simplicity. Thus, any center falling within this yellow area would not pass the TIE filter, and therefore, a review of all points in the cluster to determine if they should be reassigned to the newly introduced center would be needed. In this case, it is apparent that centers 2 and 3, although lying within this area, cannot be the closest center to any of the points due to the points' spatial distribution. This observation suggests that additional geometric insights could further exclude potential centers from consideration. To this end, we integrate the norm filters mentioned earlier (see Section 3.3) to refine the search space further. Recall that the norm difference between a point and a center is constrained by their Euclidean Distance. Therefore, if the norm difference between a point and the newly added center exceeds the distance between the point and its assigned cluster, the point cannot be part of the newly added cluster.\nTo leverage this property, we further divide each cluster $P_j$ into two partitions: the lower partition $L_j$ and the upper partition $U_j$. The lower partition of a cluster is formed by the points whose norm is less than or equal to the center norm, i.e., $L_j = {x_i \\in P_j | ||x_i||_2 \\leq ||c_j||_2}$."}, {"title": "Experimental Evaluation", "content": "To test the efficiency of our proposed approach, we use real-world instances. Table 1 shows the list of instances, along with (1) their size n in terms of the number of points and (2) the dimension d of the points. Instances have been pre-processed by removing data points with missing values. They are categorized into low-dimensional instances (first 12 table rows) and high-dimensional instances (last 9 table rows), defined as those with dimensions less than or greater than 16, respectively. Within these groups, instances are ordered by increasing size. Moreover, all available data points were merged into one set for those test instances with data points separated into training, validation, and testing sets. Note that in those cases in which the instance dimension (d) shown in Table 1 does not coincide with the dimension information given in the original source, we have reduced the original dimensionality by removing features irrelevant to our objectives, such as identifiers, dates, times, labels, classes or tags. Some specific features that showed weak correlations or relevance to the main features under consideration were removed from certain instances. All test instances can be obtained from the corresponding author on request."}, {"title": "Performance evaluation", "content": "The following three algorithm variants are considered in the experiments: (1) standard k- means++, (2) accelerated k-means++ without norm filter (that is, only using the TIE filter), and (3) full accelerated k-means++. Each algorithm variant was applied 10 times to each combination of a problem instance and a cluster number $k \\in {2^0 = 1,..., 2^{12} = 4096}$. All results are shown in terms of mean values over the 10 algorithm applications.\nFor the evaluation, we focused on the following key metrics, which we aim to optimize: (1) the portion of the dataset examined during the identification of the new closest center, (2) the portion of the dataset examined during the $D^2$ sampling phase, (3) the total number of distance computations performed between points and centers and (4) the total running time.\nThe first three metrics serve as indicators of the algorithm's intrinsic efficiency, high- lighting improvements brought by our optimized approach. These measures are particularly valuable as they remain unaffected by variables external to the algorithm, such as the comput- ing environment or implementation specifics. While the primary objective of our approach is"}, {"title": "Results of the accelerated k-means++ variant only using the TIE filter", "content": "The following observations can be made:\n\u2022 The algorithm generally outperforms the standard k-means++, achieving speedups of up to $2^5$ - $2^6$ in some instances. It obtains better results in low-dimensional settings, which was to be expected given the curse of dimensionality. Overall, the efficacy of TIE tends to grow with an increasing number of clusters. As the k-means++ algorithm inherently produces well-separated clusters, the filtering mechanism of TIE becomes progressively more effective. Conversely, when a low number of clusters is considered, TIE loses efficacy due to the lower number of point visit savings identified by the TIE filter, combined with the additional overhead from computing extra pairwise distances.\n\u2022 The spatial distribution of points within each dataset significantly influences the algo- rithm's performance. For instance, in some low-dimensional instances such as CIF-C, CIF-T, RNA, HAR, and YAH, the accelerated k-means++ hardly causes any improvement for low numbers of clusters.\nVisualization of these datasets in two dimensions using PCA reveals a lack of distinct separation between points. This is more noticeable in instances like CIF-C and HAR, where points are densely distributed around a central mass, compared to slightly more dispersed structures in the other datasets. In contrast, instances like YAH exhibit a more uniform distribution across the visible cluster. Such uniformity generally enhances performance as the algorithm progresses and more centers are selected. Initially, the algorithm struggles due to the apparent absence of separation between points due to"}, {"title": "Results of the full accelerated k-means++ variant", "content": "In general, the full accelerated k-means++ algorithm exhibits performance trends similar to those of the accelerated version using only the TIE, with some notable differences:\n\u2022 In lower-dimensional settings, particularly at smaller values of $k$, this version performs slightly worse than the TIE-only version. Concerning the RNA and CIF-C instances, for example, it initially underperforms but achieves comparable results to the TIE-only variant as the number of clusters increases. This is expected because\u2014even though the norm filter effectively filters additional points not identified by the TIE filter at lower values of $k$\u2014it introduces an additional overhead by initially having to calculate all the points' norms. Thus, this added complexity can be counterproductive in scenarios where the computational savings are not as high.\n\u2022 Despite the norm filter reducing the number of calculations in almost all cases, this does not always translate into increased speedup for two primary reasons. Firstly, in some instances, the marginal savings do not significantly impact performance in lower dimensions as much as in higher dimensions, where reducing distance calculations typically yield more substantial gains. Secondly, this version demonstrates poorer data locality, an aspect that will be further explored in upcoming experiments."}, {"title": "Hardware-Related Performance Issues", "content": "As previously stated, the theoretical gains expected from algorithm optimizations do not al- ways go along with the performance improvements measured in practice. For example, while the additional norm filter generally reduces the number of computations and visited points, this does not consistently translate into speedups. Initially, one hypothesis was that both accelerated versions, especially the one with the norm filter, suffer from poor data locality compared to the standard k-means++ variant, which sequentially processes points. This de- viation might lead to increased cache misses, adversely affecting the algorithm's performance.\nAdditionally, as mentioned earlier, the implementation and the computing environment can significantly influence the practical performance (time) of the algorithms. In this context, note that the previously reported experiments were conducted in parallel on a computing cluster, whereby all applications for a single instance were sent simultaneously to the clusters' queue. This setup raises questions about whether such concurrency might skew the expected results.\nTo investigate these factors further, we conducted the following experiments: An algo- rithm is first run in isolation for a specific instance, meaning that only that specific algo- rithm/instance combination is executed on the system. Then, the same combination is run in j concurrent jobs, with j scaling up to 10. Each algorithm application is repeated 10 times. These tests are performed on a computer cluster equipped with two processors, each having 12 cores.\nWe decided to use the 3DR instance for our analysis. First, low-dimensional data helps us better identify memory issues related to data access, as high-dimensional data could occupy almost all of the cache, causing cache misses due to fewer points being stored there rather than access patterns. Additionally, we observed that although the fully accelerated version visits slightly fewer points and performs fewer calculations than the TIE-only version, this does not result in an actual speedup. This discrepancy is what we aim to investigate.\nDuring each run, we measure several performance metrics, including execution time, the percentage of level 1 cache misses, the percentage of last-level cache misses, and the number of"}, {"title": "L1 Cache Misses", "content": "In the heatmaps of the second row of Figure 6 we analyze the percentage of L1 cache misses, calculated as the number of load misses from the L1 data cache divided by the total loads."}, {"title": "Last-level cache misses", "content": "In the third row, which analyzes last-level cache (LLC) misses, we observe that running mul- tiple processes simultaneously affects the LLC miss rate. This outcome is expected, because the LLC is shared among all cores within the same CPU; thus, the actions of one core can impact the memory usage of another. As a result, the number of LLC misses increases with the number of concurrent jobs. This increase is more pronounced with a higher number of clusters.\nFocusing on results from running a single job, where there is no external influence from other cores, we see a similar pattern as observed with L1 cache misses, especially when the number of clusters is low (e.g., k = 32). The standard k-means++ variant initially exhibits more misses than the accelerated algorithm variants. However, the miss rate in the standard version decreases rapidly, reaching a low of about 4%. In contrast, the accelerated k-means++ variants result in a less pronounced decrease. This holds in particular for the full accelerated k-means++ variant, which only shows a total reduction of 4%.\nThe decrease in miss rates is expected as most of the data is likely loaded in the initial iterations, and as less of that data is used in subsequent iterations, it becomes more probable that the data can be stored in the cache or is already present. This scenario is optimal when accessing data sequentially, as in the standard k-means++ variant, because L2 and L3 caches, being larger than L1, can store more data and are more likely to have subsequent data pre- loaded. Conversely, non-sequential access is less efficient because it often necessitates accessing RAM, which is slower. In such cases, prefetching becomes more challenging. Particularly for the full accelerated k-means++ variant, which has a miss rate double as high as the one of the"}, {"title": "Number of instructions per cycle", "content": "The number of instructions per cycle (IPC) measures the number of instructions executed per CPU cycle, with higher values indicating a more efficient algorithm for memory access. In this analysis, the standard k-means++ consistently achieves a higher IPC than the accelerated k-means++ variants. Hereby, the difference grows as the number of clusters increases, with the standard algorithm variants' IPC more than doubling that of the accelerated algorithm variants at k = 4096. Additionally, the IPC decreases in all cases as the number of concurrent jobs increases.\nOne significant factor influencing IPC is memory latency. When the processor awaits data retrieval, it generally cannot execute further instructions until the data is fetched. Therefore, high latency, often due to cache misses, can significantly reduce IPC. As observed, the LLC experiences more misses as the number of concurrent jobs increases, adversely affecting the IPC.\nHowever, the IPC measure of the standard k-means++ variant typically increases with the number of clusters. This increase can be attributed to both the sequential access of data and the reduction in calculations since points chosen as centers do not need to be revisited, resulting in lower memory usage. At k = 4096, the number of clusters accounts for approximately 1% of the total number of points in this instance, which likely impacts data fetching during the final phases.\nConversely, IPC in both accelerated versions decreases as the number of clusters increases. This decrease is logical because although many calculations are saved (thus reducing mem- ory access), the non-sequential nature of memory access in these versions introduces greater latency due to cache misses, causing the CPU to wait longer for data retrieval. The situa- tion worsens with an increase in the number of concurrently running jobs on the same CPU. Hereby, the full accelerated k-means++ variant exhibits a lower IPC measure than the TIE- only variant across most scenarios, but these differences diminish with a higher number of concurrent jobs and clusters.\nAfter evaluating the memory performance of the algorithms, it becomes apparent that the full accelerated k-means++, despite reducing the number of calculations, achieves less speedup than expected, likely due to its poor data locality. A similar issue affects both accel- erated k-means++ variants when compared to the standard one. Consequently, developing a more cache-friendly algorithm could potentially enhance the speedup of the accelerated algorithm variants even further."}, {"title": "Conclusions and Future Work", "content": "In this paper, we proposed an accelerated version of the exact k-means++ algorithm, lever- aging geometric information, specifically the Triangle Inequality and additional norm filters, along with a two-step sampling procedure. Our experiments showed that the accelerated algo- rithm variants outperform the standard k-means++ in terms of the number of visited points and distance calculations, achieving greater speedup as the number of clusters increases. The Triangle Inequality-based acceleration is particularly effective for low-dimensional data, while the norm-based filter enhances performance in high-dimensional instances with a sig- nificant norm variance among points. Additional experiments demonstrated the behavior of our algorithms when executed concurrently across multiple jobs and examined how memory performance impacts the speedup measured in practice.\nFor future work, we plan to enhance the method by avoiding the calculation of all center- center distances at each iteration and improving the norm methods by using reference points other than the origin. Additionally, we aim to improve the algorithm by accessing data in a more ordered manner to enhance data locality and reduce cache memory failures, thereby aligning speedup more closely with the number of saved calculations. Some of these ideas are briefly introduced in appendices A and B."}, {"title": "Avoiding center-center distance computations", "content": "As explained, both accelerated algorithm versions require calculating the distances between the newly selected center and the existing ones. Although this step does introduce additional computational overhead, it is crucial for reducing the far greater number of distance calcu- lations that would otherwise be necessary between each point and the new center, especially given that, in practice, the number of points usually greatly exceeds the number of clusters. However, particularly when the number of clusters is large, calculating the distance be- tween the new center and previous ones can become unnecessary. As new clusters are formed, they tend to decrease in size. Furthermore, when two clusters are significantly separated, it becomes apparent that points within one cluster are unlikely to be closer to a point from another cluster, even if that point becomes the newly selected center, than they are to their existing center. In such scenarios, we can, again, make use if"}]}