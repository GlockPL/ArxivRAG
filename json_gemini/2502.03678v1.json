{"title": "Reflection-Window Decoding: Text Generation with Selective Refinement", "authors": ["Zeyu Tang", "Zhenhao Chen", "Loka Li", "Xiangchen Song", "Yunlong Deng", "Yifan Shen", "Guangyi Chen", "Peter Spirtes", "Kun Zhang"], "abstract": "The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.", "sections": [{"title": "1. Introduction", "content": "It is a well-known insight in optimization theory that coordinate-wise optimization, conditioned on the previously optimized arguments along each axis sequentially, generally does not guarantee finding the global optimum (T\u00f6rn & \u017dilinskas, 1989; Nocedal & Wright, 1999). Similarly, in the context of decoding in large language models (LLMs), expecting to achieve a globally optimal response by sequentially accumulating per-token optimal decisions, as done in purely autoregressive decoding, may be overly optimistic.\nDespite significant recent progress (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Gemini Team, 2023; Llama Team, 2024; Gemma Team, 2024; Abdin et al., 2024a; DeepSeek-AI, 2025), how to approach the optimal text that one can possibly sample from a model still remains an open question.\nPrevious works have demonstrated challenges faced by autoregressive decoding in terms of the handling of long sequences (Wu et al., 2021) and the inefficient inference (Lin et al., 2021; Li et al., 2022). At the level of decoding, other than the traditional greedy search, Holtzman et al. (2020) proposed Top-p sampling (also known as Nucleus Sampling), a stochastic method that adjusts the next-token set based on the shape of conditional probability mass function. Alternatively, different from the cumulated probability mass, Top-k sampling limits the number of available options when sampling for the next token (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019). Another empirical technique involves adjusting the shape of probability distribution with the temperature hyperparameter (Fan et al., 2018; Holtzman et al., 2018; Peeperkorn et al., 2024).\nIn order to improve the sampling efficiency, speculative decoding approaches have been proposed, where multiple tokens are predicted in parallel as if one were sampling from the model (or its lighter counterpart) repetitively (Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2023; Kim et al., 2024; Sun et al., 2024). Efficient inference with beam search (Xie et al., 2024; Zhu et al., 2024; Wei et al., 2024; Yang et al., 2024b) and probabilistic programs (Lew et al., 2023) have been explored in the recent literature. Improving generated content through high-level behaviors, e.g., through instructing self-correction or conducting self-improvement with external or internal feedback (Yao et al., 2022; Bai et al., 2022; Pan et al., 2023; Shinn et al., 2023; Ganguli et al., 2023; Chen et al., 2023b; Kim et al., 2023; Tyen et al., 2023; Madaan et al., 2024), has also been studied.\nWhile previous literature has explored various methods to enhance generated content, the fundamental limitation of autoregressive decoding for text generation remains under-explored. This gap represents a distinct perspective, different from high-level model behaviors (e.g., self-refinement studied in Madaan et al. 2024) or inference"}, {"title": "2. Motivation and High-Level Illustration", "content": "In this section, we first present our motivations behind addressing the inherent shortcoming of purely autoregressive decoding for text generation (Section 2.1). Then in Section 2.2, we present a high-level summary of our proposed approach involving interchangeably switching between refinement (upon reflection on previously generated content) and generation (of the additional new content)."}, {"title": "2.1. Inherent Shortcoming of Autoregressive Decoding", "content": "Insight from the optimization theory highlights the gap between coordinate-wise accumulation of optimum and the global optimum (T\u00f6rn & \u017dilinskas, 1989; Nocedal & Wright, 1999). Recent research advances in cognitive linguistics have also argued that language is primarily a tool for communication (for humans) rather than thought (Fedorenko et al., 2024). While language naturally unfolds in a one-dimensional sequence, its underlying dependence pattern extends beyond a purely autoregressive structure (where the current content is conditioned solely on what has been generated so far).\nLet us consider an example of writing a novel. For a long-format writing like novels, outlining (also referred to as plotting) is essential for structuring ideas, planning narratives, and crafting engaging drafts (King, 2000; Serravallo, 2017). Sub-goals refer to relatively small and achievable tasks that guide the author through each stage of the story, for instance, the setting of the circumstance, the element of tension and emotion, the sensory imagination of the scene.\nAs we illustrate in Figure 1(a), X's represent words or phrases in the novel, and Si's represent sub-goals, which may be related in a hierarchical way. For instance, sub-goals within a single scene altogether serve the purpose of furthering the development of the story. We model sub-goals in terms of selection variables Si since they represent constraints or objectives to achieve, which involve certain criteria to be satisfied over the variables that they operate upon. As we can see from Figure 1(a), the variables in optimal sequence (the novel in this example) X's jointly satisfy criteria, or optimize objectives, specified by sub-goals Si's. This indicates that the best X in the optimal sequence depends on best values of all other X's. However, with an autoregressive way of text generation, as illustrated in Figure 1(b), we only allow Xi to depend on Xj's if j < i, which is clearly sub-optimal."}, {"title": "2.2. Selective Refinement Through Reflection Window", "content": "As illustrated in Section 2.1, one outcome of limiting the dependence pattern to the autoregressive structure is the lack of a built-in mechanism to correct or refine previously generated content at the decoding level, since what was generated is not subject to further edit (if without further processing). In this subsection, we present a high-level summary of our approach that attempts to address this issue. Specifically, we propose a selective refinement framework that facilitates an interchangeable process of refinement and generation, so that the overall response satisfies requirements or objectives that operate jointly over all involved tokens.\nAs illustrated in Figures 2 and 3, we can use fast and slow pointers on the generated content to form segments of a certain length, namely, the sliding reflection window, and perform potential refinements within this sliding window as the text generation proceeds. Our reflection-window decoding framework allows for revision before the entire output is completed, which offers several advantages.\nFirstly, we can improve the generated content in a timely manner. If there are multiple potential issues in the generation history, the revision after finishing the generation can be inefficient since we allow errors to accumulate. In other words, without a built-in mechanism for refinement or correction at the decoding level, we are forced to rely on high-level model behaviors and operate at a coarser granularity. This often involves regenerating entire sentences (rather than refining words or phrases), and/or editing through multiple iterations (rather than interchangeably generate and refine in a single run), as in self-correction and self-improvement approaches (Yao et al., 2022; Bai et al., 2022; Pan et al., 2023; Shinn et al., 2023; Ganguli et al., 2023; Chen et al., 2023b; Kim et al., 2023; Tyen et al., 2023; Madaan et al., 2024).\nSecondly, our focus on selective refinement during decoding is not solely driven by inference efficiency considerations. The primary goal of previous approaches, e.g., speculative decoding (Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2023; Kim et al., 2024; Sun et al., 2024; Xia et al., 2024), is to accelerate sampling from (a lighter version of) the original model, while the underlying decoding mechanism remains purely autoregressive.\nThirdly, due to the one-dimensional progression of text generation, our sliding reflection window mechanism, given a pausing criterion, enables timely and assured detection of issues in the generated text. Our framework complements previous approaches, and furthermore, offers versatility. One can incorporate pausing criteria and refinement/correction methods at the decoding level, while preserving the ability to further leverage strategies that rely on high-level behaviors.\nThe empirical pausing criteria we use (detailed in Section 5) are guided by our theoretical characterization of the sub-optimality of autoregressive text generation, and to this theoretical analysis we now turn."}, {"title": "3. Theoretical Characterization of the Sub-Optimality of Autoregressive Decoding", "content": "We theoretically characterize the sub-optimality of autoregressive text generation. We show that even if an LLM is sufficiently trained and can perfectly capture any autoregressive decomposition of the joint density, the autoregressive way of text generation can still deviate from the globally optimal response, even for the well-defined objective of maximizing output probability given a fixed length (setting aside whether this objective fully aligns with the ultimate goal).\nLet us denote a token from the vocabulary V as $w_\\upsilon \\in V$, whose index in the vocabulary is $\\upsilon \\in |V|$. We use \u201ci : j\" to denote the increasing integer sequence from i to j if i < j, e.g., 1: t := 1, 2, ..., t if t > 1, otherwise, i : j := \u00d8.\nDefinition 3.1 (Globally Optimal Length-T Response). We say a sequence $w_{\\upsilon^*[1]}w_{\\upsilon^*[2]} \\cdots w_{\\upsilon^*[T]}$ is globally optimal among all possible length-T responses following the prompt $X_{\\leq 0}$, if it has the highest ground-truth conditional probability, denoted by $f(X_{1:t} | X_{\\leq 0})$ where $t \\in [1,T]$:\n$\\overrightarrow{v^*} = (v^*[1], v^*[2], ..., v^*[T]) $\n$:= \\underset{V_i\\in[V],i=1,2,...,T}{argmax} f(X_{1:T} = w_{v_1}w_{v_2} ... w_{v_T} | X_{\\leq 0}).$  (1)\nDefinition 3.2 (Stepwise Optimal Length-T Response). We say a sequence $w_{\\hat{\\upsilon}_T[1]}w_{\\hat{\\upsilon}_T[2]} \\cdots w_{\\hat{\\upsilon}_T[T]}$ is stepwise optimal for prompt $X_{\\leq 0}$, if the sequence consists of tokens that correspond to highest token-by-token conditional probabilities,"}, {"title": "4. Sanity Check: Semi-Synthetic Settings", "content": "The implication of our theoretical analysis is straightforward. However, it is natural to ask whether the phenomenon actually occurs in real-world LLM decoding scenarios. To provide clear empirical evidence accompanying our theoretical analysis, in this section, we present semi-synthetic experiments that serves as a sanity check. In particular, in moderately realistic settings, we show that greedy decoding for text generation with stepwise optimization results in suboptimal responses. We first outline the semi-synthetic setting, and then present the empirical findings.\nIllustrative Approximation For any modern LLM with a vocabulary size |V| (typically on the order of 104 to 105), identifying the globally optimal sequence across multiple steps becomes computationally intractable, even for relatively short sequence lengths (< 100). To ensure the validity of our claim while providing a clear and accessible illustration, we adopt beam search as an approximation strategy of obtaining globally optimal sequence. Since we measure the chance that greedy decoding can attain the global optimum with the stepwise optimal response, this approximation serves as an upper bound on achievable performance, indicating the discrepancy between greedy decoding and the true globally optimal response.\nApproximating Natural Language Scenarios Since the prompt or context of the generation influence model behavior, we align our experimental setting with common human-LLM interactions. Specifically, we utilize MT-Bench (Zheng et al., 2023) questions as curated prompts, which are designed to evaluate conversational chat models. These samples serve as an approximation of real-world natural language context distributions, ensuring that our findings are grounded in practical scenarios.\nFindings For each prompt, together with a certain length of generation history (0 means only the prompt is given), we evaluate whether the joint probability of the sequence generated with greedy decoding is greater than or equal to that produced by beam search (the proxy of the global optimum). This comparison indicates the extent to which greedy decoding deviates from the globally optimal response. As illustrated in Figure 4(a), greedy decoding consistently results in suboptimal sequences, and the phenomenon can be observed with a small number of newly generated tokens.\nIn addition, the potential deviation may behave differently across various positions in the generated text. For instance, when openings of response diverge, it is hard for greedy decoding to achieve optimal results afterwards. To reduce potential inductive bias resulting from the diversity at early stages of generation, we evaluate generations starting/continuing from various positions throughout generation history, as presented in Figures 4(b)-4(d). We can observe that the deviation persists across different positions, which empirically demonstrate the common existence of sub-optimality in autoregressive decoding for text generation."}, {"title": "5. Empirical Approach and Experiments", "content": "In Sections 5.1\u20135.2, we provide technical details about our empirical approach and settings of our experiments. Then in Sections 5.3-5.4, we present experimental results to demonstrate both the effectiveness and efficiency of our method."}, {"title": "5.1. Reflection-Window Decoding: Technical Details", "content": "Our findings through both the theoretical characterization of sub-optimality in autoregressive decoding for text generation (Section 3), and the sanity check with empirical verifications in semi-synthetic settings (Section 4), suggest the necessity of a built-in reflection-and-refine mechanism at the decoding level. To empirically address this issue, we propose a selective refinement framework that interchangeably refine and generate as the response unfolds.\nText typically unfolds in a single direction, i.e., from the start to the end, with words, phrases, and sentences. This differentiates text from other forms of objects that occupy multiple dimensional spaces, e.g., images or videos. Taking advantage of this one-dimensional nature, our decoding framework integrates a sliding reflection window along with two additional modules: (1) a pausing criterion that specifies whether we should pause the generation upon reflecting on generated content, and (2) a refinement/correction method that facilitates revision at the decoding level (if the pausing criterion is triggered). We present the pseudocode of our reflection-window decoding approach in Algorithm 1.\nPausing Criterion Guided by our theoretical characterization (Theorem 3.6), the reflection at the decoding level needs to capture the (increasing trend of) uncertainty as text generation proceeds. For an empirical pausing criterion, we use the conditional entropy H(\u00b7) based on the next-token logits across the vocabulary. Specifically, given an LLM which models the conditional distribution $g(X_t | X_{1:t-1})$ of the token at t-th step given all the observed history $X_{1:t-1} = \\textbf{X}_{1:t-1}$, we use the pausing criterion $h(t; \\sigma, d)$:\n$h(t; \\sigma, d) =\\begin{cases}  \\text{True} & \\text{if } H (X_{t-i} | X_{1:(t-i-1)}) > \\sigma, \\forall i \\in [0, d- 1], \\\\  \\text{False} & \\text{Else},  \\end{cases}$ (4)\nwhere \u03c3 denotes the hyperparameter for the threshold of conditional entropy, and d denotes that for the window size (how far we look back in history, in terms of the token counts). When $h(t_{(fast)}; \\sigma, d)$ is True, the pausing criterion (denoted by IfPause() in Algorithm 1, with hyperparameter enclosed) is triggered upon reflecting on the most recent d generated tokens, i.e., the length-d reflection window when the fast pointer is at $t_{(fast)}$. The two parameters, \u03c3 and d, jointly decide the sensitivity and effective region of the pausing criterion, and we present more discussions in Section 5.4.\nRefinement/Correction Method When the pausing criterion is triggered, tokens within the current sliding reflection window need to be refined or corrected. Since beam search can approximate the global optimum relatively well, empirically we introduce a fixed-length beam search to generate a new segment with a length d (denoted by the hyperparameter-enclosed function ReGenerate(\u00b7) in Algorithm 1), to replace the content within the reflection window. After the refinement, the slow pointer $t_{(slow)}$ catches up with the fast one $t_{(fast)}$ and the model proceeds with generation while maintaining the sliding reflection window.\nRemark: Versatile Decoding Framework We would like to note that our reflection-window decoding approach is highly versatile. While our empirical approach employs a specific pausing criterion and refinement/correction method, practitioners can customize these components by incorporating different functions, namely, IfPause() and ReGenerate() in Algorithm 1, to meet their needs. Our selective refinement framework integrates the sliding reflection window mechanism with these components, enabling simultaneous refinement and generation at the decoding level while retaining the flexibility to incorporate additional strategies, such as those based on high-level model behaviors and/or speculative decoding (Section 2.2)."}, {"title": "5.2. Experiment Settings", "content": "We provide technical details about settings of our experiments, including models, benchmarks, evaluation metrics, and baseline methods.\nLLM Models We conduct experiments using multiple models across different families/herds. Specifically, we use Llama-3.1-8B-Instruct (denoted as Llama3.1-8B), which belongs to the Llama 3.1 herds (Llama Team, 2024), Phi-3-Medium-128K-Instruct (Abdin et al., 2024b) (denoted as Phi3-Medium) with 14 billion parameters, Qwen2.5-14B-Instruct (Yang et al., 2024a) (denoted as Qwen2.5-14B) with 14 billion parameters, Qwen2.5-7B-Instruct (denoted as Qwen2.5-7B) with 7 billion parameters, and Mistral-Nemo-Instruct-2407 (MistralAI, 2024) (denoted as Mistral-Nemo) with 12 billion parameters.\nBenchmarks and Evaluation Metrics Our experiments are conducted on MMLU (Hendrycks et al., 2020) and MT-"}, {"title": "6. Conclusion", "content": "In this paper, we theoretically characterize one inherent shortcoming, among others, of the autoregressive decoding for text generation in LLMs. In particular, we show that even when the optimality is defined in terms of the joint probability over all generated tokens, an oracle LLM can still potentially deviate from the globally optimal response of the same length. To mitigate the sub-optimality of the autoregressive way of text generation, we propose an empirical approach guided by our theoretical characterization. We incorporate a sliding reflection window and a pausing criterion so that refinement and generation can be performed interchangeably.\nOur experimental results demonstrate that our reflection-window decoding strategy achieves significant improvement over regular decoding strategies in inference-intensive settings and maintains performance that is comparable, or even superior to, beam search while being more efficient."}, {"title": "Broader Impact", "content": "In this paper, we theoretically characterize and empirically address the sub-optimality of the autoregressive decoding for text generation. In particular, we propose a selective refinement framework and implement it with a sliding reflection window mechanism, enabling interchangeable refinement and generation as the decoding proceeds. Our approach strikes a balance between efficiency and optimality. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. The Proof of Theoretical Result", "content": "Theorem A.1 (Indication of Deviation from the Globally Optimal Length-T Response). Given the prompt X\u22640, when an oracle LLM (Assumption 3.3) generates a stepwise optimal length-T response which is not the globally optimal response with the same length, let L < T denote the minimum length of prefix-sequence needed in order for such deviation to manifest itself (Assumptions 3.4 and 3.5). Then, the deviation from the globally optimal response happens at some step K < L. Furthermore, the conditional probability when generating the token $W_{v_L} \\in V$ is strictly smaller than a positive number, which itself is strictly smaller than 1, i.e.,\n$1 > \\epsilon_L > \\underset{\\Psi \\in V}{max}g(X_L = w | X_{1:L-1} = w_{\\hat{v}_T[1]}w_{\\hat{v}_T[2]} \\cdots w_{\\hat{v}_T[L-1]}, X_{\\leq 0}),$\nwhere $\\epsilon_L = \\frac{f(X_{1:L} = W_{v^*[1]}W_{v^*[2]} \\cdots W_{v^*[L-1]}W_{v^*[L]} | X_{\\leq 0})}{f(X_{1:L-1} = W_{\\hat{v}_T[1]}W_{\\hat{v}_T[2]} \\cdots W_{\\hat{v}_T[L-1]} | X_{\\leq 0})}$(5)\nProof. We first show that the deviation from the globally optimal response happens before step L. Then, we show that the conditional probability when generating the token $W_{v_L}$ is bounded away from 1.\nBy definition of oracle LLM (Assumption 3.3), the advantage of the globally optimal response cannot manifest itself at L = 1 (even if the deviation happens at step 1), i.e., L > 1. Since the minimum length of prefix-sequence needed in order for the deviation of stepwise optimal response from the same-length globally optimal response to manifest is L, then the advantage of the globally optimal response is not manifested until step L. Until step L - 1, in terms of the ground-truth conditional probability following the prompt X\u22640, prefix-sequences of the globally optimal response is not strictly preferred compared to their same-length counterparts of the stepwise optimal response:\n$f(X_1 = W_{\\hat{v}_T[1]} | X_{\\leq 0}) \\geq f(X_1 = W_{v^*[1]} | X_{\\leq 0}),$\n$f(X_{1:2} = W_{\\hat{v}_T[1]}W_{\\hat{v}_T[2]} | X_{\\leq 0}) \\geq f(X_{1:2} = W_{v^*[1]}W_{v^*[2]} | X_{\\leq 0}),$\n$f(X_{1:L-1} = W_{\\hat{v}_T[1]} W_{\\hat{v}_T[2]} \\cdots W_{\\hat{v}_T[L-1]} | X_{\\leq 0}) \\geq f(X_{1:L-1} = W_{v^*[1]}W_{v^*[2]} \\cdots W_{v^*[L-1]} | X_{\\leq 0}).$ (6)\nStarting from step L and onwards (Assumption 3.5), prefix-sequences of the globally optimal response are strictly preferred compared to their counterparts of the stepwise optimal response:\n$f(X_{1:L} = W_{\\hat{v}_T[1]} \\cdots W_{\\hat{v}_T[L-1]}W_{\\hat{v}_T[L]} | X<0) < f(X_{1:L} = W_{v^*[1]} \\cdots W_{v^*[L-1]}W_{v^*[L]} | X_{\\leq 0}),$\n$f(X_{1:T} = w_{\\hat{v}_T[1]} W_{\\hat{v}_T[2]} \\cdots W_{\\hat{v}_T[T]} | X<0) < f(X_{1:T} = W_{v^*[1]}W_{v^*[2]} \\cdots W_{v^*[T]} | X_{\\leq 0}).$ (7)\nAssumption 3.4 specifies that for any two same-length but different sequences following the prompt X<0, there is a strict ordering between them. Then, in order for the advantage of the globally optimal length-T response to manifest, in terms of strict preferences staring from the length-L prefix-sequence (Equation (7)), there is at least one strict preference of the prefix-sequence of stepwise optimal response over its globally optimal counterpart before step L. In other words, there is at least one step K \u2208 [1, L \u2212 1] such that a strict preference (\u201c>\u201d instead of \u201c\u2265\u201d) is present in Equation (6):\n$f(X_{1:K} = W_{\\hat{v}_T[1]}W_{\\hat{v}_T[2]} \\cdots W_{\\hat{v}_T[K]} | X<0) > f(X_{1:K} = W_{v^*[1]} W_{v^*[2]} \\cdots W_{v^*[K]} | X_{\\leq 0}).$(8)\nIn order to see why this is the case, consider the opposite scenario where there is no strict preference in Equation (6). Under Assumption 3.4, the comparison between prefix-sequences is either strict preference (they are different) or exactly the same (identical sequences). If there is no strict preference in Equation (6), then for all $t \\in [1, L \u2212 1], W_{\\hat{v}_T[t]} = W_{v^*[t]}$, i.e., the first L 1 tokens in the stepwise optimal response are the length-(L \u2013 1) prefix of the globally optimal response. If this is the case, the token generated at step L has to deviate from the globally optimal response (since L is the minimum length for"}, {"title": "B. Additional Results and Analyses", "content": "In this section, we present additional results and further discussions on influences from hyperparameters. We also provide concrete examples demonstrating the process and overall performance of our reflection-window decoding.\nB.1. Analysis on Window Size d\nWe conduct extensive experiments on MT-Bench to analyze the impact of window size using both Mistral-Nemo (Table 6) and Llama3.1-8B (Table 7), with a fixed pausing criterion with \u03c3 = 0.5. These GPT-4o evaluator scores on MT-Bench provide additional evidence that our approach consistently outperforms traditional decoding methods.\nFor Mistral-Nemo, the optimal performance is achieved at d = 3 with an overall score of 7.93, surpassing both greedy decoding and beam search. For Llama3.1-8B, our method consistently outperforms both greedy decoding and beam search across different window sizes, with d = 5 achieving the best overall performance. While d = 4 may not always yield the best result, it demonstrates robust performance across both models and serves as a reliable default setting.\nWe further evaluate how different window sizes affect the performance on MMLU social science tasks using Qwen2.5-7B. The results are presented in Table 8.\nB.2. Analysis on Threshold \u03c3\nWe investigate the impact of threshold o on MMLU social science subjects using Qwen2.5-7B with a fixed window size d = 4. The detailed results are presented in Table ??.\nB.3. Analysis on Regeneration Ratio\nTo further understand the computational efficiency of our method, we analyze the regeneration ratio under different window size settings. We select six college-level subject categories from the MMLU test set (including biology, chemistry, computer science, mathematics, medicine, and physics) for analysis, and conduct experiments with the Llama3.1-8B model with a threshold of \u03c3 = 0.5. We consider the window size d as the key hyperparameter, because it directly influences the regeneration ratio, which is calculate by the product of the times criterion get triggered (the regeneration call) and the window size (d), divided by the total length of final response.\nAs shown in Table 10 and Figure 6, as the window size increases from 2 to 4, the average regeneration ratio shows a clear downward trend, decreasing from 9.60% to 3.70%. The trend indicates that larger window sizes lead to a faster decrease in the number of modifications needed. Notably, across all settings, the regeneration ratio remains below 15%, suggesting that our method maintains comparable computational workload as greedy decoding for the majority of the time. These results demonstrate the efficiency of our approach, since it only invoke beam search to find optimal approximations for sub-sequences when necessary, while maintaining the overall efficiency comparable to greedy decoding.\nB.4. Demonstrative Examples on MT-Bench\nIn this subsection, we present three examples selected from MT-Bench to illustrate the generation process in our reflection-window decoding. The first example is the 28th test sample from MT-Bench. As is shown in Figure 7, the pausing criterion gets triggered when \"part of the car\" was generated. The part gets refined and replaced by \"specific part, whereas\" which leads to a more flexible and natural answer. Figure 8 and Figure 9 are selected from the 29th and 7th test sample of MT-Bench, respectively."}, {"title": "C. Performance Across All MMLU Subjects", "content": "Due to the size of the table, the material is arranged in the one-table-per-page manner (starting from the next page)."}]}