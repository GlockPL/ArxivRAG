{"title": "TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional Networks to Predict Transcription Factor Binding Sites", "authors": ["Nimisha Ghosh", "Pratik Dutta", "Daniele Santoni"], "abstract": "Transcription factors are proteins that regulate the expression of genes by binding to specific genomic regions known as Transcription Factor Binding Sites (TFBSs), typically located in the promoter regions of those genes. Accurate prediction of these binding sites is essential for understanding the complex gene regulatory networks underlying various cellular functions. In this regard, many deep learning models have been developed for such prediction, but there is still scope of improvement. In this work, we have developed a deep learning model which uses pre-trained DNABERT, a Convolutional Neural Network (CNN) module, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale Convolutions with Attention (MSCA) module and an output module. The pre-trained DNABERT is used for sequence embedding, thereby capturing the long-term dependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are useful in extracting higher-order local features. TFBS-Finder is trained and tested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies as well as cross-cell line validations and comparisons with other models. The experimental results show the superiority of the proposed method in predicting TFBSs compared to the existing methodologies. The codes and the relevant datasets are publicly available at https://github.com/NimishaGhosh/TFBS-Finder/.", "sections": [{"title": "I. INTRODUCTION", "content": "Transcription factors (TFs) are proteins that play an important role in the regulation of genetic transcriptions by binding to DNA regulatory sequences known as Transcription Factor Binding Sites (TFBSs) (typically of size 4-30 bp) [1]\u2013[3]. Accurate prediction of TFBSs provides crucial information on regulatory networks that control cellular functions [4], [5], as well as identifying disease-associated variants and regulatory regions, enabling early detection and targeted therapies [6]. Chromatin immunoprecipitation sequencing (ChIP-seq) [7] can be used to identify DNA fragments that interact with TFs. However, such method is costly, paving the way for more inexpensive but effective computational approaches to predict TFBSS.\nIn recent years, many researchers have proposed several machine learning and deep learning approaches to identify TFBSs which include Hidden Markov Models (HMM) [8], Support Vector Machine (SVM) [9], Random Forest (RF) [10], [11] models etc. Although traditional machine learning algorithms provide competitive results, they are dependent on other methods for feature extraction and they also do not work well with large-scale datasets. To address these challenges, recently many deep learning algorithms have been proposed, which show immense potential in predicting TFBSs when working with huge amount of experimental data such as ChIP-seq. In this regard, DeepSEA [12] and DeepBind [13] which use convolutional neural networks (CNNs) showcase good performance for the prediction of TFBSs. However, CNNs fail to capture long term dependencies between different positions in a DNA sequence. To overcome this, DanQ [14] proposed by Quang et al. and DeepSite [15] proposed by Zhang et al., combine CNN and bidirectional long-short term memory (BiLSTM) to predict TFBSs. Other models such as DSAC [16], D-SSCA [17], DeepSTF [18] and SAResNet [19] use attention mechanisms for TFBS prediction.\nThe evolutionary deep learning model Bidirectional Encoder Representations from Transformers (BERT) [20] effectively learns contextual information in natural language processing. Since its proposal in [20], it has also been used for pre-training human DNA and protein sequences giving birth to DNABERT [21] and ProteinBERT [22] respectively.\nDNABERT is an effective pre-trained model for embedding DNA sequences and subsequently other deep learning models may be used for classifying TFBSs. In this regard, Ghosh et al. [23] have used DNABERT for embedding the DNA sequences, while CNN, BiLSTM and capsule network have been used as the subsequent prediction layers. On the other hand, BERT-TFBS [6] has DNABERT-2, a CNN module, a convolution block attention module (CBAM) and an output module for TFBS prediction on 165 ChIP-seq datasets. Both works have shown promising results in predicting TFBSs.\nTaking cue from the literature, in this work we propose TFBS-Finder that utilises DNABERT, a CNN module, a modified convolution block attention module (MCBAM), a multi-scale convolutions with attention (MSCA) module and an output module to predict TFBSs. Similarly to [6], we have trained and tested TFBS-Finder on 165 ChIP-Seq datasets, encompassing different cell-lines and TFs. The results show the superiority of TFBS-Finder when compared to the state-of-the-art predictors.\nTo summarise, the main contributions of the work are as follows:\n\u2022 We propose a novel DNABERT-based deep learning model (TFBS-Finder) for predicting TFBSs. The model utilises DNABERT for embedding while CNN, MCBAM, MSCA and output modules are used for subsequent prediction.\n\u2022 Ablation study is conducted to showcase the importance of each module of TFBS-Finder.\n\u2022 To show the generalisability and robustness of TFBS-Finder, cross-cell line validations are conducted to predict TFBSS.\n\u2022 All codes along with a sample dataset and a trained model are publicly available at https://github.com/NimishaGhosh/TFBS-Finder/."}, {"title": "II. MATERIALS AND METHODS", "content": "In this section, data preparation is provided followed by a discussion on the pipeline of the work."}, {"title": "A. Data Preparation", "content": "In this work, we have considered the benchmark 165 ChIP-seq datasets from Enclyopedia of DNA elements (EN-CODE) [24], consisting of 29 different TFs from 32 cell lines. The dataset is taken from Zeng et al. [25] where each positive sequence in the dataset is a 101 bp DNA sequence length containing TFBSs while the negative samples are obtained by shuffling the positive sequences conserving dinucleotide frequencies. Detailed descriptions of the datasets used in this work are given in Supplementary Table S1."}, {"title": "B. Pipeline of the Work", "content": "The pipeline for the proposed work is given in Figure 1. The proposed model consists of five modules encompassing a DNABERT module, a Convolutional Neural (CNN) module, a modified CB\u0410\u041c (MCBAM) module, a multi-scale convolutions with attention module (MSCA) and an output module.\n\u2022 DNABERT: DNABERT is pre-trained on DNA sequences to extract long-term dependencies within such sequences. Here, the sequences are divided into tokens of size k.\n\u2022 CNN: DNABERT generates embeddings for k-mer tokens in DNA sequences, which capture context-aware representations of the input. The CNN module is used for extracting high-order local features from such embeddings by utilising a convolutional layer.\n\u2022 MCBAM: This module applies spatial and channel attention mechanisms to selectively enhance important local features extracted by the previous module.\n\u2022 MSCA: This module further enhances the local features captured by the feature extraction module. This is achieved by introducing multi-scale convolutions with an attention mechanism. Here, the attention is a form of feature-wise attention which is applied in the context of convolutional feature extraction.\n\u2022 Output module: Using the relevant features captured by MCBAM and MSCA, this module uses such features and applies a series of convolutions, pooling and multi-layer perceptron to provide the final prediction of the presence or absence of TFBSs.\n1) DNABERT: DNABERT is a pre-trained bidirectional encoder to enocode DNA sequences using k-mer technique. Sequences are divided into tokens of size k and provided as initial input to DNABERT. Token and positional embeddings are applied to such tokens to form the input matrix M. This input matrix is then passed through N sequential encoders to get the embedded matrix $M^1$. Each such encoder has multi-head self-attention, two layer of normalisations and a feed-forward network. The processing of the multi-head self attention mechanism on the input matrix for each encoder can be given as:\n$Multihead(M^n) = Concatenation(head^1, . . ., head^h)W^{O,n}$ (1)\nwhere\n$head_i = Softmax(\\frac{M^{Qi,n}(M^nW^{Ki,n})^T}{\\sqrt{d_k}})M^{Vi,n}$ (2)\nHere, all Ws are parameters learned during training of the model. Next, the residue connection M and the multihead attention Multihead($M^n$) is passed through layer normalisation (LN) and feed forward network (FFN) to obtain:\n$M^{n+1} = LN(LN(M^n + Multihead(M^n))+FFN(LN(M^n + Multihead(M^n))))$ (3)\nFinally, after passing through the N sequential encoder layers, the embedding matrix $M_1$ of size d \u00d7 D is obtained where d is the number of tokens while D is the embedding dimension.\n2) CNN Module: A single convolutional layer is used in this module to extract high-order local features from the embedded matrix $M_1$. This module consists of one convolutional block with convolutional operation $Conv_1$, batch normalisation (BN), Gaussian error linear unit (GELU) activation function and dropout operation. This operations can be given as:\n$M_2 = Dropout(GELU(BN(Conv_1(M_1))))$ (4)\n3) Modified Convolutional Block Attention Module: Spatial and channel attention blocks [26] are used in this module to enhance important local features obtained from the previous module. As given in [26], the order of the two submodules affects the overall performance and in their work they have considered Channel-Spatial module. However, in our work (as reported later in Table I), spatial attention before channel attention provides a slightly better result. This placement of attention blocks has also shown better results in [27]. The detailed operations of MCBAM module is given in Figure 2.\nAs shown in Figures 1 and 2, spatial attention is applied to feature matrix $M_2$. In this regard, initially $M_2$ is subjected to global maxpooling and global average pooling in order to extract the spatial features. Convolution operation $Conv_2$ is then applied on such features and finally a sigmoid function is applied, resulting in spatial attention feature $M_s$. Subsequently, element-wise matrix multiplication is carried out between $M_2$ and $M_s$ to generate $M'_2$. The aforementioned operations can be expressed as:\n$M_s = Sigmoid(Conv_2(Maxpool(M_2), Avgpool(M_2)))$ (5)\n$M'_2 = M_s \\otimes M_2$ (6)\nOnce $M'_2$ is constructed, channel attention submodule is applied on $M'_2$ where each of its channel is subjected to global maxpooling and global average pooling separately. The resultant features are then individually passed through two convolutional layers where the first layer has convolution operation $Conv_{3,1}$ and a GELU activation function while the second layer consists of only a convolution operation $Conv_{3,2}$. The resultant feature channels are then added and passed through a sigmoid function to acquire the channel attention feature $M_c$. Element-wise matrix multiplication is then performed between $M_c$ and $M'_2$ resulting in the feature matrix $M_3$ which is the final output of MCBAM module. These operations can be given as:\n$M_c = Sigmoid(Conv_{3,2}(GELU(Conv_{3,1}(Maxpool(M'_2)))+ Conv_{3,2}(GELU(Conv_{3,1}(Avgpool(M'_2)))))$ (7)\n$M_3 = M_c \\otimes M'_2$ (8)\n4) Multi-Scale Convolution with Attention Module: This module utilises multi-scale convolutions with attention to further enhance the local features captured by the feature extraction module. As shown in Figure 1, three separate convolutions $Conv_{4,1}$, $Conv_{4,2}$ and $Conv_{4,3}$ are applied to each input channel of the feature matrix $M_2$. The results of such convolutions are concatenated along the channel dimension producing $M''_{2}$, thus retaining all multi-scale information across channels. Next, a point-wise convolution $Conv_5$ is applied on $M''_{2}$, thereby combining and compressing the concatenated features into a unified representation, enabling the model to learn meaningful relationships between features from different scales. Subsequently, a batch normalisation and a sigmoid activation function are applied to capture attention weights. Finally, element-wise multiplication is performed between such attention weight matrix $M'''_{2}$ and $M_2$ to get the final output. In this way, features that are deemed more relevant (with higher weights) are enhanced, while less relevant features are suppressed. Such operations can be summarised as:\n$M''_{2} = Concat(Conv_{4,i}(M_2)), i \\in \\{1,2,3\\}$ (9)\n$M'''_{2} = Sigmoid(BN(Conv_5(M''_{2})))$ (10)\n$M_4 = M'''_{2} \\otimes M_2$ (11)\nThus, $M_4$ is the final matrix of the MSCA module.\n5) Output Module: The output block considers the feature matrices $M_3$ and $M_4$ respectively, obtained from MCBAM and MSCA modules. This has the effect of parallel attention [27] in the output module, thereby exploiting both MCBAM and MSCA modules. The matrices obtained from these modules are passed separately through convolutional operations $Conv_6$ and the resultant matrices are combined via addition which are then passed in parallel through global maxpooling and average pooling separately. The resultant matrices $M_5$ and $M_6$ are then concatenated and passed through yet another convolution operation with GELU as the activation function along with batch normalisation and dropout operations, leading to the feature matrix $M_7$. Finally, the prediction is obtained by flattening $M_7$ and subsequently applying multilayer perceptron which consists of a fully connected layer with dropout as well as another fully connected layer with softmax activation function. The output of this module is the prediction probability \u0177, which predicts if a given DNA sequence consists of TFBSs. All the aforementioned operations can be depicted as:\n$M_5, M_6 = Conv_7(Maxpool(Conv_6(M_3) + Conv_6(M_4)), Avgpool(Conv_6(M_3) + Conv_6(M_4)))$ (12)\n$M_7 = Dropout(GELU(BN(Conv_8(M_5 + M_6))))$ (13)\n$\\hat{y} = Softmax(Linear(Dropout(Linear(Flatten(M_7)))))$ (14)"}, {"title": "C. Model Training", "content": "For training purpose, cross-entropy loss function is used in this work [6]:\n$Loss(y, \\hat{y}) = \\frac{1}{\\eta} \\sum_{i=1}^{\\eta} (y_i(log\\hat{y}) + (1 - y_i)log(1 - \\hat{y}_i))$ (15)\nHere, y are the actual values of the DNA sequences while \u0177 are the predicted values of TFBS-Finder and \u03b7 represents the batch size of DNA sequences which is considered as 64 in this work. AdamW optimisation method is used in this work while ReduceLROnPlateau is used as the scheduler which automatically aids in reduction of learning rate when the value of area under the precision-recall curve plateaus. Moreover, to prevent overfitting, early stopping and dropout are also used in this work where the dropout values are set to 0.2 and 0.3. The training is performed for 15 epochs. All the hyperparameters are selected based on experiments. All such hyperparameter values are provided in Supplementary Tables S2 and S3."}, {"title": "III. RESULTS AND DISCUSSIONS", "content": "This section discusses the different results as obtained in this work. Initially, the analysis is performed to check if TFBS-Finder is able to differentiate sequences containing TFBS from shuffled sequences. As TFBS-Finder model is made up of a combination of 5 modules, we have conducted ablation studies to evaluate the contribution of such modules. We have also investigated the ability of TFBS-Finder when trained on sequences coming from a given cell-line to recognise sequences of different cell-lines, observing substantial cross- cell line consistency. Subsequently, we have performed comparison studies with existing state-of-the-art predictors to show the superiority of TFBS-Finder."}, {"title": "A. Performance Metrics", "content": "The different metrics used in this work to showcase the prediction performance of TFBS-Finder are accuracy, PR-AUC and ROC-AUC. Accuracy represents the proportion of correctly predicted samples. However, accuracy can turn out to be a biased metric when the data is imbalanced. In this regard, PR-AUC and ROC-AUC are some other parameters to judge prediction performance. PR-AUC is the area under the precision-recall curve and represents the model's overall performance. Its value lies between 0 and 1 where a higher value of PR-AUC depicts better prediction performance. For imbalance class distributions, PR-AUC is a better metric than Accuracy. Another metric which is equally suitable for evaluating predictor performance is ROC-AUC or Area Under the Receiver Operating Characteristic curve, which is a graphical representation of the performance of a binary classification model at various classification thresholds. In order to evaluate the performance of TFBS-Finder, all the three metrics have been used in this work. However, since early stopping is used in this work to avoid overfitting, PR-AUC is used as the early stopping criteria. If PR-AUC does not improve for 2 consecutive runs during validation, the model stops training. We have used different values of k (3, 4, 5 and 6) and observed that with k=5, we get the best results. Thus, all the following experiments are conducted with k = 5."}, {"title": "B. Ablation Study", "content": "TFBS-Finder uses different important modules such as DNABERT, CNN, MCBAM and MSCA for predicting TFBSS. In order to showcase their individual contributions, experiments and ablation studies are performed on 165 ChIP-seq datasets with different variants to compare such variants.\n\u2022 DNABERT+Output (Variant 1): In this variant, only DNABERT+Output is considered. All other modules are removed.\n\u2022 DNABERT+CNN+Output (Variant 2): Only DNABERT with CNN and output modules are used in this variant.\n\u2022 DNABERT+CNN+MSCA+Output (Variant 3): In this variant, the MCBAM module is removed.\n\u2022 DNABERT+CNN+MCBAM+Output (Variant 4): In this variant, the MSCA module is removed.\n\u2022 DNABERT+CNN+CBAM+MSCA+Output (Variant 5): Here, the positions of spatial and channel attention modules are reversed (such position is similar to CBAM [26]) that is, channel attention comes before spatial attention module in order to show the importance of their individual position in the MCBAM module."}, {"title": "C. Cross-cell line validation", "content": "Cross-cell line validation is performed in order to show the generalisability and robustness of TFBS-Finder to identify the binding sites of a particular TF, in this case CTCF, occurring in different cell lines. In this regard, we have considered four cell lines Gm12878, Helas3, Hepg2 and K562 which have many TFs in common (as shown in Figure 4) and also have a large number of CTCF sequences.\nTo prepare the training dataset for cross-cell validation, the training sequences bounded by CTCF are considered as positive while the rest of the sequences bounded by other TFs are considered as negative. This resulted in 71761 positive and 147870 negative sequences for Gm12878. Similarly, for Helas3, these statistics are 84901 and 166311 while for Hepg2 such numbers of positive and negative sequences are 35857 and 381333 and for K562, they are 82297 and 406752. As can be observed from the statistics, the number of negative sequences is quite high as compared to the positive sequences which is quite obvious given the fact that only the sequences bounded by CTCF forms the positive sequences while those bounded by the other TFs together constitute the negative sequences. As the motivation is to identify the binding sites of CTCF, to balance the dataset we have randomly selected a number of negative sequences equal to the number of positive ones while ensuring that the negative sequences have candidates from all the other TFs (71761, 84901, 35857 and 82297 for Gm12878, Helas3, Hepg2 and K562 respectively). The test dataset is also prepared in a similar manner, where the sequences bounded by CTCF are considered as positive and those bounded by other TFs are negative. For example, for CTCF pertaining to Gm12878 (for wgEncodeAwgTfbsBroadGm12878CtcfUniPk), the number of positive sequences in test set is 8644 indicating that there are 8644 such sequences in the dataset wgEncodeAwgTfbsBroadGm12878CtcfUniPk with TFBSs for CTCF while 37098 negative sequences constitute the other 15 TFs. For the other TFs, the number of positive sequences includes the binding sequences of CTCF in wgEncodeAwgTfbsBroadGm12878CtcfUniPk and wgEn-"}, {"title": "D. Comparison with existing models", "content": "In this subsection, we present the results of the comparison of the prediction performance of TFBS-Finder with other existing models. In this regard, we have considered seven benchmark models which include BERT-TFBS [6], DeepBind [13], DanQ [14], DLBSS [28], CRPTS [29], D-SSCA [17] and DSAC [16]. BERT-TFBS uses DNABERT-2 [30] as the encoding technique along with CNN and CB\u0410\u041c modules while DeepBind considers one-hot encoding along with CNN architecture. On the other hand, DanQ utilises CNN as well as Bi-LSTM architectures, DLBSS and CRPTS use a shared CNN and CNN with RNN architectures, respectively, while D-SSCA and DSAC use CNNs and attention mechanisms. The results of this comparison are based on 165 ChIP-seq datasets where the dataset is the same as considered in [6]."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "In this work, we propose TFBS-Finder which utilises DNABERT for global context while CNN, MCBAM and MSCA performs feature extraction and capture local contexts to predict TFBSs for DNA sequences. TFBS-Finder is trained and tested on 165 ChIP-seq datasets where the experimental studies show that TFBS-Finder outperforms the existing predictors in terms of average accuracy, PR-AUC and ROC-AUC. We have performed ablation studies to understand the importance of local and global contexts to identify TFBSs. Furthermore, cross-cell line validations are also performed to show the generalisability and robustness of TFBS-Finder. Although, the proposed model shows very inspiring results, as a future work, we would additionally like to incorporate DNA structure information along with DNA sequences to see the effect on the prediction capacity of TFBS-Finder."}]}