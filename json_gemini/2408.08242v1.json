[{"title": "A Conflicts-free, Speed-lossless KAN-based Reinforcement Learning Decision System for Interactive Driving in Roundabouts", "authors": ["Zhihao Lin", "Zhen Tian", "Qi Zhang", "Ziyang Ye", "Hanyang Zhuang", "Jianglin Lan"], "abstract": "Safety and efficiency are crucial for autonomous driving in roundabouts, especially in the context of mixed traffic where autonomous vehicles (AVs) and human-driven vehicles coexist. This paper introduces a learning-based algorithm tailored to foster safe and efficient driving behaviors across varying levels of traffic flows in roundabouts. The proposed algorithm employs a deep Q-learning network to effectively learn safe and efficient driving strategies in complex multi-vehicle roundabouts. Additionally, a KAN (Kolmogorov-Arnold network) enhances the AVs' ability to learn their surroundings robustly and precisely. An action inspector is integrated to replace dangerous actions to avoid collisions when the AV interacts with the environment, and a route planner is proposed to enhance the driving efficiency and safety of the AVs. Moreover, a model predictive control is adopted to ensure stability and precision of the driving actions. The results show that our proposed system consistently achieves safe and efficient driving whilst maintaining a stable training process, as evidenced by the smooth convergence of the reward function and the low variance in the training curves across various traffic flows. Compared to state-of-the-art benchmarks, the proposed algorithm achieves a lower number of collisions and reduced travel time to destination.", "sections": [{"title": "I. INTRODUCTION", "content": "As urban roadways evolve, roundabouts have significantly increased the vehicle distribution and road capacity [1]. Compared to other crucial traffic scenarios, roundabouts have been verified to offer fewer conflicts [2]. However, the safety issue becomes more pronounced in high-traffic roundabouts, which has a higher potential for crashes [3]. The design of roundabouts varies for cities of different scales [4]. Typically, a roundabout is a type of circular intersection created to enable a smooth flow of traffic, characterized by a central island that is not meant for vehicular access [5]. In standard roundabouts, the direction of travel is set either clockwise or counterclockwise\u2014simplifying the interaction complexities. The essential driving maneuvers in a roundabout include entering, lane selection, circulating, lane-changing, and exiting. Both merging into and exiting from the roundabout require interaction between the host vehicles and surrounding human-driven vehicles (HDVs), with host vehicles making self-optimized decisions by interpreting the intentions of other drivers. Choosing the correct lane allows host vehicles to maneuver with greater flexibility and efficiency. The lane-changing process obliges host vehicles to carefully monitor the state of surrounding HDVs to avoid possible collisions. Precise control is essential to adhere to the generated trajectories throughout the driving process. Thus, during navigation in roundabouts, host vehicles must have the ability to select the appropriate lane, monitor nearby entities and the environment, avoid collisions, and precisely control their movement.\nAutonomous vehicles (AVs) can significantly reduce safety incidents that stem from human errors such as fatigue, distraction, and delayed reactions [6]. AVs are also capable of computing optimal decision-making solutions more swiftly than human drivers, thereby enhancing traffic efficiency [7]. Notably, AVs can enhance the capacity of roundabouts [8]. With the advent of AVs, whose amount is expected to exceed 50 million by 2024 [9], modern roundabout designs are increasingly accommodating AVs to address the challenges of safe control and interaction with HDVs [10]. Therefore, it is essential to ensure the safe operation of AVs in high-traffic roundabouts, particularly when they interact with HDVs [11]. Control strategies for connected autonomous vehicles (CAVs) are meticulously crafted to prioritize safety and efficiency in interactive driving situations [12]. Contemporary roundabout designs increasingly accommodate the operational needs of AVs, optimizing traffic flow and improving safety [13].\nExisting studies on the interaction between AVs, CAVs, and HDVs in roundabouts can be broadly categorized into model-based and learning-based approaches. Model-based methods, such as game theory, have been widely used to model the decision-making process in interactive driving scenarios. Different driving behaviors in game processes can be modeled by adjusting the ratios of safety, efficiency, and driving comfort [14]. However, works using game process often rely on simplified environment and may not be able to handle the complex and dynamic nature of real-world roundabout scenarios [15], [16]. Other model-based frameworks face limitations due to the simplistic design of roundabouts, the low number"}, {"title": "", "content": "of vehicles involved, and the limited generalization as only an abnormal roundabout is focused [17], [18].\nLearning-based approaches, such as machine learning and deep reinforcement learning (DRL), have shown great potential in complex interactive driving in roundabouts. Machine learning techniques have been used to explore safe and efficient interaction between AVs and HDVs [19]. However, these machine learning-based methods often require a large amount of labeled data and struggle to generalize to unseen scenarios. DRL, on the other hand, allows for extensive exploration of strategies in complex interactive environments. Therefore, DRL is well-suited for the interactive driving in roundabouts. By learning from experience, DRL-based controllers can balance safety and efficiency in dense traffic scenarios.\nSeveral DRL algorithms have been applied to autonomous driving, including Deep Deterministic Policy Gradient (DDPG) [20], Proximal Policy Optimization (PPO) [21], and deep Q-learning network (DQN) [22]. DDPG is well-suited for continuous action spaces, but its advantages may not be fully exploited in complex tasks with discrete decisions, such as driving in the roundabout [23]. PPO has been used to develop intelligent driving strategies that balance safety and efficiency in dense highway traffic [24]. However, PPO's on-policy learning mode may be inefficient in utilizing historical data, which is crucial for approximating strategies in complex environments like roundabouts.\nDQN has been effectively applied in various traffic simulations and is adept at tasks similar to roundabout navigation, such as intersection management [25]. DQN's discrete action framework directly applies to tasks like proper lane selection in roundabouts without the need for action discretization. Additionally, DQN applies experience replay to relearn from past experiences, achieving more efficient use of historical data. Furthermore, DQN tends to be less computationally demanding [26]. The recently proposed Kolmogorov-Arnold Network (KAN) has improved performance compared to traditional multi-layer perceptrons [27]. KAN uses the B-spline function to replace the linear layer in the multi-layer perceptron, which can adaptively adjust the feature extraction method of each neuron. It can flexibly change the shape of the kernel function to learn key feature patterns related to the task, which improves the adaptability and generalization performance in different environments.\nTo solve the complex driving in roundabouts, this paper proposes to integrate KAN with DQN (K-DQN) to enhance the decision-making and learning capabilities of AVs in complex roundabout scenarios. The K-DQN leverages the advantages of both DQN and KAN, enabling AVs to learn robust and efficient driving strategies through interaction with the environment. For conflict-free driving, we introduce an action inspector applied to time to collision (TTC) [28] to assess the relative collision risks between the AV and other HDVs. By replacing dangerous actions that may cause collisions with safe actions, our proposed method can decrease the ego vehicle collision rates with neighboring vehicles (NVs) during training. For proper lane selection, we introduce a route planner that considers the number of HDVs and the available free-driving space in each lane. For precise control of planned trajectories, we"}, {"title": "", "content": "implement model predictive control (MPC) to allow the AV to navigate with precision and robustness.\nThe main contributions of this paper are summarized as follows:\n\u2022 We introduce a novel KAN-based DQN (K-DQN) to improve AVs' decision-making in complex roundabouts. By integrating KAN with spline-based activation functions, the K-DQN boosts observation and decision-making capabilities, leading to enhanced training convergence, reduced collisions, and increased average speeds.\n\u2022 We propose an action inspector and a route planner to reduce collisions and enhance driving efficiency. The action inspector mitigates collision risks with HDVs, and the route planner optimizes lane selection for safety and space, significantly reducing collisions across various traffic flows compared to benchmarks.\n\u2022 We integrate model predictive control (MPC) with K-DQN to convert planned actions into safe, smooth control commands, thereby enhancing journey safety and robustness. Our integrated solution adeptly manages diverse roundabout traffic flows, showing improved speed stability and efficiency over current benchmarks.\n\u2022 We present mathematical analysis and experimental demonstrations to substantiate the superior performance of our K-DQN over traditional DQN methods. Extensive simulations confirm robustness and efficiency of our approach and its advantages over benchmarks.\nThe rest of this paper is organized as follows: Section II presents the problem statement and system structure; Section III describes the enhanced KAN-based DQN; Section IV introduces the action inspector and route planner; Section V presents the MPC design; Section VI provides the simulation results with analysis; Section VII draws the conclusion."}, {"title": "II. PROBLEM STATEMENT AND SYSTEM STRUCTURE", "content": "The previous section underscored the significant attention given to studies on the interactive driving of AVs with HDVs in roundabouts. However, it is challenging in the integrated process of decision-making, path planning, and control for AVs to navigate roundabouts amid the varied complexity presented by HDVs. Interactions with HDVs, which can be randomly and densely distributed along both the inner and outer boundaries of roundabouts, frequently result in unexpected outcomes, such as conflicts and inefficient driving. Unlike straight or other curvy roads, roundabouts present unique challenges in making safe and efficient decisions due to their complex network of entrances and outlets. As illustrated in Fig. 1, there are four ports, each divided by a center white line into two half-ports-an entrance and an outlet. In this study, the right half-port of each is considered an entrance, while the left half-port serves as an outlet. The unpredictability of HDVs' intended outlets and their various maneuvers exacerbates the challenges for AVs in ensuring safe passage. Moreover, satisfactory driving considerations encompass not only safety but also efficiency, which involves minimizing the total driving time of an AV to a designated"}, {"title": "A. Problem Statement", "content": "A. Problem Statement"}, {"title": "B. System Structure", "content": "This paper primarily addresses the safety and efficiency of AVs from their entry to exit in roundabouts, taking into account the variable distribution and number of HDVs present. We consider varying traffic flows in the roundabout, from low to high density, and evaluate the system's adaptability to these changes. To adapt to varying traffic flows, the proposed system requires a combination of techniques. First, the agent is required to adapt to various driving scenarios with different traffic densities. Second, the real-time changes in the environment is required to make safe and efficient decisions even in highly dynamic scenarios. Finally, the smooth and safe navigation is needed through the driving in roundabout.\nTo solve the above problems, the proposed KAN-based, conflict-avoidance, and proper-lane-detection DRL system is designed, as depicted in Fig. 3. This system comprises four key components: the environment, decision network, safety and efficiency mechanism, and robust control. The environment module updates the state of AVs based on the generated control commands and also generates a reward at each step to assess the value of the selected action. The decision network makes safe and efficient decisions for each step. The safety and efficiency mechanism comprises two components: the route planner and the action inspector. The former is activated during the initial merge into the roundabout, guiding AVs to select a lane that ensures sufficient driving space and safety. The latter assesses and mitigates collision risks with HDVs. For robust control, MPC is implemented to translate planned actions into safe and smooth control commands, thus enhancing safety and robustness throughout the journey.\nBesides the normal driving, the system uses action inspector to detect their presence and take related response to emergency situations. The AV will choose actions that allow the emergency vehicle to pass safely and quickly, such as moving to the outer lane or slowing down. The route planner module will also adjust its path to minimize interference with the emergency vehicle's trajectory."}, {"title": "III. KAN-ENHANCED DQN METHOD", "content": "The K-DQN network includes a replay memory, a KAN-Q-network, and a target Q-network. The KAN-Q-network, featuring a robust and precise learning process and a Q-network, processes data from the environment to generate gradient-based updates. This network is tasked with observing the surrounding HDVs and calculating the Q-value for making safe and efficient decisions. The target Q-network has the"}, {"title": "A. Basic DQN", "content": "DQN combines deep learning with Q-learning, a reinforcement learning algorithm to address problems with high-dimensional state spaces. The core idea behind DQN is using a deep neural network to approximate the optimal action-value function $Q^*(s, a)$, which represents the maximum expected return for taking action $a$ in state $s$.\nThe two key components of DQN are experience replay and target networks. Experience replay stores the agent's past experiences $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer, allowing the agent to learn from past experiences multiple times, which stabilizes the learning process. The target network is a separate neural network used to compute the target Q-values during training. The parameters of the target network are periodically synchronized with the main Q-network. This helps to stabilize the learning process by reducing the correlation between the target Q-values and the current Q-values.\nIn Q-learning, the Q-value update rule is given by\n$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)\\right)$\nwhere $Q(s_t, a_t)$ is the Q-value of executing action $a_t$ in state $s_t$ at time step $t$, $\\alpha$ is the learning rate, $r_t$ is the immediate reward obtained after executing $a_t$ at time step $t$, $\\gamma$ is the discount factor to balance the importance of immediate and future rewards, and $\\max_a Q(s_{t+1}, a)$ is the maximum Q-value of all possible actions in the next state $s_{t+1}$ at time step $t+1$."}, {"title": "B. Structure of KAN", "content": "The core of the KAN architecture consists in using spline-based activation functions of the form:\n$f(x_i; \\theta_i, \\beta_i, a_i) = a_i \\cdot spline(x_i; \\theta_i) + \\beta_i b(x_i)$\nwhere $x_i$ is the input to the $i$-th neuron, $spline(x_i; \\theta_i)$ represents the spline function parameterized by coefficients $\\theta_i$, $b(x_i) = SiLU(x_i) = x_i/(1+e^{-x_i})$ is an activation function, and $a_i$ and $\\beta_i$ are learnable coefficients. Spline functions are piecewise polynomial functions with high expressiveness and can approximate any continuous function. By adjusting the parameters and coefficients of the spline function, KAN can approximate various complex nonlinear functions."}, {"title": "", "content": "The coefficients of the spline, $\\theta_i$, are updated through gradient descent methods using the loss function $L(\\theta)$ in (3). The update law is\n$\\theta_i^{(t+1)} = \\theta_i^{(t)} - \\eta \\frac{\\partial L}{\\partial \\theta_i}$\nwhere $\\eta$ is the learning rate. KAN also employs regularization strategies to mitigate the risk of overfitting:\n$R(\\theta) = \\lambda_1 \\sum_i |\\theta_i| + \\lambda_2 \\sum_{i} \\sum_{j \\neq i} |\\theta_i - \\theta_j|$\nwhere $R(\\theta)$ is the regularization term added to $L(\\theta)$, which can control the complexity of the model whilst fitting the data and improve the generalization ability of the model. $\\lambda_1$ and $\\lambda_2$ are regularization coefficients. The $L_1$ regularization term, $\\lambda_1 \\sum_i |\\theta_i|$, promotes sparsity in the parameter matrix. Another $L_1$ regularization term, $\\lambda_2 \\sum_{i} \\sum_{j \\neq i} |\\theta_i - \\theta_j|$, ensures smoothness in the parameter values across different neurons, facilitating model stability and preventing drastic changes in the output for minor fluctuations in the input data.\nKAN also adopts parameter sharing among neurons defined as follows:\n$\\theta_{shared} = \\frac{1}{N_{group}} \\sum_{i \\in group} \\theta_i$\nwhere $group$ includes indices of neurons sharing parameters, and $N_{group}$ is the number of neurons in this group. The shared parameters, $\\theta_{shared}$, reduce the overall model complexity by averaging the parameters of neurons within the same group, enhancing computational efficiency and potentially improving the model's generalization capabilities over similar features.\nThese elements of the KAN architecture collectively enhance the flexibility and efficiency of the learning process, whilst ensuring robustness against overfitting and maintaining high performance across reinforcement learning tasks."}, {"title": "C. KAN Enhanced DQN", "content": "The integration of KAN within DQN, termed K-DQN, enables the network to better approximate the Q-function, especially in scenarios with complex action dynamics and reward functions. This capability translates into more robust learning and improved policy development in DRL tasks. To demonstrate why using the KAN network combined with DQN is superior to combining it with other reinforcement learning frameworks, it is essential to analyze the core differences in how these algorithms handle environments and tasks, as well as the characteristics of KAN activation functions.\nTo formulate the roundabout driving problem as a Markov Decision Process, we define the following key components:\nState Space: The state space $S$ consists of the ego vehicle's position, velocity, and heading, as well as the relative positions, velocities, and headings of the surrounding vehicles within a certain range. The state at time $t$ is represented as\n$s_t = [p_{ev}(t), v_{ev}(t), h_{ev}(t), p_{Nv}(t), v_{Nv}(t), h_{Nv}(t)]$\nwhere $p_{ev}(t)$, $v_{ev}(t)$, and $h_{ev}(t)$ denote the position, velocity, and heading of the ego vehicle at time $t$, and $p_{nv}(t)$, $v_{nv}(t)$,"}, {"title": "", "content": "and $h_{nv}(t)$ represent the position, velocity, and heading of the $i$-th neighboring vehicle at time $t$.\nAction Space: The action space $A$ is discrete and consists of five high-level actions: faster, slower, idle, turn right, and turn left.\nReward Function: The reward function $r: S \\times A \\rightarrow R$ is designed to encourage the ego vehicle to drive safely and efficiently through the roundabout. The reward function is defined as a weighted sum of several components:\n$r(s_t, a_t) = w_1 r_c + w_2 r_s + w_3 r_{l_c} + w_4 r_h + w_5 r_a$\nwhere $r_c$ penalizes collisions, $r_s$ rewards driving at high speeds, $r_{l_c}$ penalizes excessive lane changes, $r_h$ encourages maintaining a safe distance from surrounding vehicles, and $r_a$ rewards reaching the target exit. The weights $w_1, ..., w_5$ balance the importance of each component.\nBy using (6), the goal is to directly approximate the optimal action-value function\n$Q(s, a; \\theta) = \\sum_j f(x) + b$\n$= \\sum_j \\sum_i (\\alpha_i \\cdot spline(x_i; \\theta_i) + \\beta_i \\cdot b(x_i)) + b$\n$= \\sum_j \\sum_i (\\alpha_i \\cdot spline((s, a)_i; \\theta_i) + \\beta_i \\cdot b((s,a)_i)) + b$\nwhere $W$ and $b$ are the weights and bias of the network, $f(x)$ is the output from the KAN layer, $j$ is the index of the output layer neuron, and $n$ is the total number of output layer neurons.\nTheorem 1: (Approximation theory [27]) Suppose that a function $f(x)$ admits a representation $f = (\\Phi_{L-1} \\circ \\theta_{L-2} \\circ ... \\circ \\theta_1 \\circ \\Phi_0)x$, where each part $\\Phi_i$ is ($k + 1$)-times continuously differentiable. Then there exist k-th order B-spline functions such that for any $0 \\leq m \\leq k$,\n$\\| f - (\\Phi_{L-1} \\circ \\theta_{L-2} \\circ ... \\circ F_1 \\circ F_0) x \\|_Cm \\leq CG^{-k-1+m}$\nwhere $C$ is a constant and $G$ is the grid size. The magnitude of derivatives up to order $m$ is measured by the $C^m$-norm as\n$\\|g\\|_Cm = \\max_{\\|\\beta\\| \\leq m} \\sup_{x \\in [0,1]^n} |D^{\\beta} g(x)|$\nWe aim to prove that under the conditions of Theorem 1, DQN with KAN as the backbone network can effectively approximate the optimal action-value function $Q^*(s, a)$. Assume the true action-value function for taking action $a$ in state $s$ is $Q^*(s, a)$. Our goal is to find an approximation function $Q(s, a; \\theta)$ that is as close as possible to $Q^*(s, a)$.\nConsidering the mean squared error properties of DQN and $y$ given in (2), we have\n$E[(Q(s_t, a_t; \\theta) - Q^*(s_t, a_t))^2] = E[(Q(s_t, a_t; \\theta) - y)^2] + C$\nwhere $C = E[(r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta') - Q^*(s_t, a_t))^2]$ is a constant independent of $\\theta$. Therefore, minimizing the loss function $L(\\theta)$ is equivalent to minimizing the mean squared error between the approximate value function $Q(s_t, a_t; \\theta)$ and the target value $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta')$."}, {"title": "", "content": "When we use KAN as the backbone network in DQN, the optimization objective can be rewritten as\n$\\min_{\\theta} E[(Q(s_t, a_t; \\theta) - (r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta')))^2]$.\nBy using (12), $Q(s_t, a_t; \\theta)$ can be defined as:\n$Q(s_t, a_t; \\theta) = \\sum_j \\sum_i (\\alpha_i \\cdot spline((s_t, a_t)_i; \\theta_i) + \\beta_i \\cdot b((s_t, a_t)_i)) + b$.\nSince the spline functions and $SiLU(x)$ in (6) used in KAN are continuously differentiable, the conditions of Theorem 1 are satisfied. By applying Theorem 1, we can conclude that for any state-action pair (s,a), there exists an optimal set of parameters $\\theta^*$ such that $Q(s, a; \\theta^*)$ in (17) can arbitrarily approximate the optimal action-value function $Q^*(s, a)$.\nTheorem 2: Let $Q(s, a; \\theta)$ be the approximate action-value function defined by (12), where the spline functions $spline(x; \\theta)$ and the activation function $b(x)$ are Lipschitz continuous with Lipschitz constants $L_{spline}$ and $L_b$, respectively. Assume that the optimal action-value function $Q^*(s, a)$ is also Lipschitz continuous with Lipschitz constant $L_{Q^*}$. Then, for any $\\epsilon > 0$, there exists a set of parameters $\\theta^*$ such that\n$\\| Q(s, a; \\theta^*) - Q^*(s, a) \\|_\\infty \\leq \\epsilon$,\nand for any $\\theta \\in \\Theta$,\n$\\| Q(s, a; \\theta) - Q^*(s, a) \\|_\\infty \\leq \\epsilon + C \\| \\theta - \\theta^* \\|_2$\nwhere $C = \\sqrt{m} (a_{max}L_{spline} + \\beta_{max}L_b)$, $m$ is the number of basis functions used in the spline approximation, $a_{max} = \\max_i a_i$, and $\\beta_{max} = \\max_i \\beta_i$.\nProof: By the universal approximation theorem for spline functions [29], $\\forall \\epsilon > 0$, there is a $\\theta^*$ such that\n$\\| Q(s, a; \\theta^*) - Q^*(s, a) \\|_\\infty \\leq \\epsilon$.\nFor any $\\theta \\in \\Theta$, we have\n$\\| Q(s, a; \\theta) - Q^*(s, a) \\|_\\infty \\leq \\| Q(s, a; \\theta) - Q(s, a; \\theta^*) \\|_\\infty + \\| Q(s, a; \\theta^*) - Q^*(s, a) \\|_\\infty \\leq \\| Q(s, a; \\theta) - Q(s, a; \\theta^*) \\|_\\infty + \\epsilon$.\nBy the Lipschitz continuity of $spline(x; \\theta)$ and $b(x)$, we obtain\n$\\| Q(s, a; \\theta) - Q(s, a; \\theta^*) \\|_\\infty \\leq \\sum_j \\sum_i (a_i L_{spline} \\| \\theta_i - \\theta_i^* \\|_2 + \\beta_i L_b \\| \\theta_i - \\theta_i^* \\|_2)$\n$< \\sqrt{m} (a_{max}L_{spline} + \\beta_{max}L_b) \\| \\theta - \\theta^* \\|_2$.\nCombining (21) and (22) gives (19).\nTheorem 2 provides a quantitative bound on the approximation error between the learned action-value function $Q(s, a; \\theta)$ and the optimal one $Q^*(s, a)$. The bound consists of two terms: (i) The universal approximation error $\\epsilon$, which can be made arbitrarily small by choosing a suitable $\\theta^*$. (ii) The term $C \\| \\theta - \\theta^* \\|_2$, which depends on the distance between the learned parameters $\\theta$ and the optimal parameters $\\theta^*$. The Lipschitz continuity of the spline functions and the activation"}, {"title": "", "content": "function, as well as the bound on the coefficients $a_i$ and $\\beta_i$, ensure the stability and generalization of the learned action-value function. As the training progresses and $\\theta$ approaches $\\theta^*$, the approximation error decreases, indicating the convergence of the learned action-value function to the optimal one.\nUnder Theorem 2, by minimizing the loss function (3), DQN combined with KAN can effectively approximate the optimal action-value function $Q^*(s, a)$, as demonstrated by:\n$\\lim_{\\theta \\rightarrow \\theta^*} L(\\theta) \\rightarrow 0 \\rightarrow \\lim_{\\theta \\rightarrow \\theta^*} Q(s, a; \\theta) \\rightarrow Q^*(s, a)$.\nThe optimal policy $\\pi^*$ is derived from the optimal action-value function $Q^*$ by selecting the action that maximizes $Q^*$ for each state:\n$\\pi^*(a | s) = arg \\max_a Q^*(s, a)$.\nTherefore, K-DQN can approximate the optimal action-value function $Q^*(s, a)$ by optimizing over the loss function (3), and thus learn the optimal policy $\\pi^*$. This demonstrates the effectiveness and superiority of KAN in enhancing DQN. The direct optimization approach of K-DQN, supported by the approximation theory and the quantitative error bound, enables K-DQN to better learn and optimize policies."}, {"title": "IV. ROUTES PLANNER AND ACTIONS INSPECTOR", "content": "This section introduces the mechanisms to ensure the safety and efficiency during the interactive driving in the roundabout. Three subsections are included: the driving rules of HDVs, the action inspector, and the route planner."}, {"title": "A. Driving Rules of HDVs", "content": "This subsection outlines the priority rules for HDVs operating within roundabouts, designed to maintain traffic flow and enhance safety. The rules are formulated to address typical scenarios encountered in roundabouts.\n1) Entry Rule: When an HDV approaches a roundabout entrance, it must yield to any vehicle already passing through the entrance it intends to use. This rule ensures that vehicles inside the roundabout maintain a smooth flow and reduces potential entry conflicts.\n$HDV_{entering} \\Leftrightarrow \\nexists HDV_{passing}$\nOnce inside the roundabout, ego HDVs (EHDV) are required to adjust their speeds according to the Intelligent Driver Model (IDM) to maintain a safe distance from their immediate front HDV (FHDV) until they reach their intended exit. This adjustment is crucial for preventing rear-end collisions and ensuring steady traffic flow within the roundabout. The IDM following rule is formulated as\n$a_{IDM} = a_{max}[1 - (v_{FHDV}/v_e)^4 - (h^*/h)^2]$\nwhere $a_{max}$ is the maximum acceleration of EHDV, $v_{FHDV}$ is the velocity of FHDV, $v_e$ is the expected velocity of EHDV, and $h$ is the real gap between EHDV and FHDV. $h^*$ is the desired gap between EHDV and FHDV with the formula\n$h^* = h_c + \\frac{v_{AV}T_e}{\\sqrt{a_{max} c}}$\nwhere $h_c$ is the expected space with FHDV, $v_{AV}$ is the speed of the AV, $T_e$ is the expected time gap, $\\Delta v$ is the velocity difference between EHDV and FHDV, and $c$ is the comfortable deceleration.\n2) Inner Lane Following Rule: HDVs in the inner lane of the roundabout must align their speeds with the nearest vehicle ahead, even if that vehicle is in the outer lane. This rule is intended to synchronize speeds across lanes and enhance the cohesive flow of traffic, particularly in multi-lane roundabouts."}, {"title": "B. Route Planner", "content": "The integrated route planner for the EV comprises initial-lane selection decisions, a path-planning algorithm, and a lane-change selection mechanism. The initial-lane selection is guided by the TTC metric for each lane, ensuring safety and efficiency from the start. The path planning algorithm employs a node-based shortest path calculation to determine the most optimal route. The lane-change selection mechanism is driven by a proposed lane change cost formula, facilitating effective and strategic lane changes.\n1) Initial-Lane Selection: By computing the TTC between the ego vehicle and surrounding vehicles, the safety levels can be ensured and unsafe actions can be avoided. In this scenario, the more potential space for driving and safety are the major considerations, thus we calculate the TTC for the inner and outer lanes as follows:\n$TTC_{inner} = \\frac{Distance \\, to \\, HDV_{inner}}{Speed \\, of \\, EV - Speed \\, of \\, HDV_{inner}}$\n$TTC_{outer} = \\frac{Distance \\, to \\, HDV_{outer}}{Speed \\, of \\, EV - Speed \\, of \\, HDV_{outer}}$\nThe obtained TTC of both lanes can then be used to make the initial-lane selection rules. This paper considers several situations: No HDVs present, One HDV in outer lane, One HDV in both lanes, and Multiple HDVs in both lanes. These scenarios are described as follows:\n\u2022 No HDVs present: The lane selection rule is\n$Laneselected = Inner \\, lane \\, if \\, HDVs == 0$.\nWhen no HDVs are present in the roundabout, the AV will directly select the inner lane due to its shorter radius, which reduces the distance from the entrance to the outlet.\n\u2022 One HDV in outer lane: Fig. 4(a) illustrates this scenario. In this scenario, the EV will compute the TTC to maintain a safe distance from surrounding vehicles and merge into the inner lane.\n\u2022 One HDV in both lanes: As illustrated in Fig. 4(b), by evaluating the TTC of both lanes, the lane with the higher TTC is selected for safety and more driving space. If having equal TTC values, the inner lane is chosen to enhance efficiency. The rule is summarized as\n$Laneselected = \\begin{cases} Inner \\, lane \\, if \\, TTC_{inner} > TTC_{outer} \\\\ Outer \\, lane \\, otherwise \\end{cases}$\nIf two HDVs have the same velocity but the inner-lane HDV is farther from the EV, the inner lane is selected.\n\u2022 Multiple HDVs in both lanes: This scenario is most complex, as both inner-lane and outer-lane have two HDVs."}, {"title": "", "content": "When multiple vehicles (in this paper more than 2 vehicles are defined as multiple case) are present in both lanes", "used": "n$TTC_{weighted"}, "w_1 \\cdot TTC_{nearest} + w_2 \\cdot TDT, \\\\ TDT = \\sum Driving \\, time \\, of \\, each \\, HDV \\, to \\, EV's \\, outlet.$\nThe weights $w_1$ and $w_2$ are predetermined based on the traffic model preferences. The lane with the lower weighted score is selected to maximize safety and avoid the lane with longer total travel time of HDVs, which prevents the forward driving of AV. The entire initial-lane selection process is summarized in the Algorithm 1.\n2) In-Roundabout Lane Selection: After entering the roundabout and selecting an initial lane, the next stage is path planning. We adopt a modified Breadth-First Search (BFS) [30"], "density": "n$C(e) = w_1D(e) + w_2 \\bar{D"}, {"min\\{p": "s \\rightarrow ... \\rightarrow g | p \\"}]