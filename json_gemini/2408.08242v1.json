{"title": "A Conflicts-free, Speed-lossless KAN-based Reinforcement Learning Decision System for Interactive Driving in Roundabouts", "authors": ["Zhihao Lin", "Zhen Tian", "Qi Zhang", "Ziyang Ye", "Hanyang Zhuang", "Jianglin Lan"], "abstract": "Safety and efficiency are crucial for autonomous driving in roundabouts, especially in the context of mixed traffic where autonomous vehicles (AVs) and human-driven vehicles coexist. This paper introduces a learning-based algorithm tailored to foster safe and efficient driving behaviors across varying levels of traffic flows in roundabouts. The proposed algorithm employs a deep Q-learning network to effectively learn safe and efficient driving strategies in complex multi-vehicle roundabouts. Additionally, a KAN (Kolmogorov-Arnold network) enhances the AVs' ability to learn their surroundings robustly and precisely. An action inspector is integrated to replace dangerous actions to avoid collisions when the AV interacts with the environment, and a route planner is proposed to enhance the driving efficiency and safety of the AVs. Moreover, a model predictive control is adopted to ensure stability and precision of the driving actions. The results show that our proposed system consistently achieves safe and efficient driving whilst maintaining a stable training process, as evidenced by the smooth convergence of the reward function and the low variance in the training curves across various traffic flows. Compared to state-of-the-art benchmarks, the proposed algorithm achieves a lower number of collisions and reduced travel time to destination.", "sections": [{"title": "I. INTRODUCTION", "content": "As urban roadways evolve, roundabouts have significantly increased the vehicle distribution and road capacity [1]. Compared to other crucial traffic scenarios, roundabouts have been verified to offer fewer conflicts [2]. However, the safety issue becomes more pronounced in high-traffic roundabouts, which has a higher potential for crashes [3]. The design of roundabouts varies for cities of different scales [4]. Typically, a roundabout is a type of circular intersection created to enable a smooth flow of traffic, characterized by a central island that is not meant for vehicular access [5]. In standard roundabouts, the direction of travel is set either clockwise or counterclockwise\u2014simplifying the interaction complexities. The essential driving maneuvers in a roundabout include entering, lane selection, circulating, lane-changing, and exiting. Both merging into and exiting from the roundabout require interaction between the host vehicles and surrounding human-driven vehicles (HDVs), with host vehicles making self-optimized decisions by interpreting the intentions of other drivers. Choosing the correct lane allows host vehicles to maneuver with greater flexibility and efficiency. The lane-changing process obliges host vehicles to carefully monitor the state of surrounding HDVs to avoid possible collisions. Precise control is essential to adhere to the generated trajectories throughout the driving process. Thus, during navigation in roundabouts, host vehicles must have the ability to select the appropriate lane, monitor nearby entities and the environment, avoid collisions, and precisely control their movement.\nAutonomous vehicles (AVs) can significantly reduce safety incidents that stem from human errors such as fatigue, distraction, and delayed reactions [6]. AVs are also capable of computing optimal decision-making solutions more swiftly than human drivers, thereby enhancing traffic efficiency [7]. Notably, AVs can enhance the capacity of roundabouts [8]. With the advent of AVs, whose amount is expected to exceed 50 million by 2024 [9], modern roundabout designs are increasingly accommodating AVs to address the challenges of safe control and interaction with HDVs [10]. Therefore, it is essential to ensure the safe operation of AVs in high-traffic roundabouts, particularly when they interact with HDVs [11]. Control strategies for connected autonomous vehicles (CAVs) are meticulously crafted to prioritize safety and efficiency in interactive driving situations [12]. Contemporary roundabout designs increasingly accommodate the operational needs of AVs, optimizing traffic flow and improving safety [13].\nExisting studies on the interaction between AVs, CAVs, and HDVs in roundabouts can be broadly categorized into model-based and learning-based approaches. Model-based methods, such as game theory, have been widely used to model the decision-making process in interactive driving scenarios. Different driving behaviors in game processes can be modeled by adjusting the ratios of safety, efficiency, and driving comfort [14]. However, works using game process often rely on simplified environment and may not be able to handle the complex and dynamic nature of real-world roundabout scenarios [15], [16]. Other model-based frameworks face limitations due to the simplistic design of roundabouts, the low number"}, {"title": "II. PROBLEM STATEMENT AND SYSTEM STRUCTURE", "content": "The previous section underscored the significant attention given to studies on the interactive driving of AVs with HDVs in roundabouts. However, it is challenging in the integrated process of decision-making, path planning, and control for AVs to navigate roundabouts amid the varied complexity presented by HDVs. Interactions with HDVs, which can be randomly and densely distributed along both the inner and outer boundaries of roundabouts, frequently result in unexpected outcomes, such as conflicts and inefficient driving. Unlike straight or other curvy roads, roundabouts present unique challenges in making safe and efficient decisions due to their complex network of entrances and outlets. As illustrated in Fig. 1, there are four ports, each divided by a center white line into two half-ports-an entrance and an outlet. In this study, the right half-port of each is considered an entrance, while the left half-port serves as an outlet. The unpredictability of HDVs' intended outlets and their various maneuvers exacerbates the challenges for AVs in ensuring safe passage. Moreover, satisfactory driving considerations encompass not only safety but also efficiency, which involves minimizing the total driving time of an AV to a designated"}, {"title": "B. System Structure", "content": "This paper primarily addresses the safety and efficiency of AVs from their entry to exit in roundabouts, taking into account the variable distribution and number of HDVs present. We consider varying traffic flows in the roundabout, from low to high density, and evaluate the system's adaptability to these changes. To adapt to varying traffic flows, the proposed system requires a combination of techniques. First, the agent is required to adapt to various driving scenarios with different traffic densities. Second, the real-time changes in the environment is required to make safe and efficient decisions even in highly dynamic scenarios. Finally, the smooth and safe navigation is needed through the driving in roundabout.\nTo solve the above problems, the proposed KAN-based, conflict-avoidance, and proper-lane-detection DRL system is designed, as depicted in Fig. 3. This system comprises four key components: the environment, decision network, safety and efficiency mechanism, and robust control. The environment module updates the state of AVs based on the generated control commands and also generates a reward at each step to assess the value of the selected action. The decision network makes safe and efficient decisions for each step. The safety and efficiency mechanism comprises two components: the route planner and the action inspector. The former is activated during the initial merge into the roundabout, guiding AVs to select a lane that ensures sufficient driving space and safety. The latter assesses and mitigates collision risks with HDVs. For robust control, MPC is implemented to translate planned actions into safe and smooth control commands, thus enhancing safety and robustness throughout the journey.\nBesides the normal driving, the system uses action inspector to detect their presence and take related response to emergency situations. The AV will choose actions that allow the emergency vehicle to pass safely and quickly, such as moving to the outer lane or slowing down. The route planner module will also adjust its path to minimize interference with the emergency vehicle's trajectory."}, {"title": "III. KAN-ENHANCED DQN METHOD", "content": "The K-DQN network includes a replay memory, a KAN-Q-network, and a target Q-network. The KAN-Q-network, featuring a robust and precise learning process and a Q-network, processes data from the environment to generate gradient-based updates. This network is tasked with observing the surrounding HDVs and calculating the Q-value for making safe and efficient decisions. The target Q-network has the same structure as the KAN-Q-network, but its parameters are updated less frequently. The target Q-network helps to mitigate the issue of oscillations or divergence in the learning process. The parameters of the target Q-network are periodically synchronized with those of the KAN-Q-network to ensure that the target values remain consistent with the learned Q-values. To provide a clear understanding of the variables used in the mathematical derivations of K-DQN, we summarize their descriptions in Table I. This table includes the variables related to the KAN-Q-network and the approximation theory used."}, {"title": "A. Basic DQN", "content": "DQN combines deep learning with Q-learning, a reinforcement learning algorithm to address problems with high-dimensional state spaces. The core idea behind DQN is using a deep neural network to approximate the optimal action-value function $Q^*(s, a)$, which represents the maximum expected return for taking action a in state s.\nThe two key components of DQN are experience replay and target networks. Experience replay stores the agent's past experiences $(s_t, a_t, r_t, s_{t+1})$ in a replay buffer, allowing the agent to learn from past experiences multiple times, which stabilizes the learning process. The target network is a separate neural network used to compute the target Q-values during training. The parameters of the target network are periodically synchronized with the main Q-network. This helps to stabilize the learning process by reducing the correlation between the target Q-values and the current Q-values.\nIn Q-learning, the Q-value update rule is given by\n$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))$                                  (1)"}, {"title": "B. Structure of KAN", "content": "The core of the KAN architecture consists in using spline-based activation functions of the form:\n$f(x_i; \\theta_i, \\beta_i, \\alpha_i) = \\alpha_i \\cdot spline(x_i; \\theta_i) + \\beta_i b(x_i)$ (6)\nwhere $x_i$ is the input to the i-th neuron, $spline(x_i; \\theta_i)$ represents the spline function parameterized by coefficients $\\theta_i$, $b(x_i) = SiLU(x_i) = x_i/(1+e^{-x_i})$ is an activation function, and $\\alpha_i$ and $\\beta_i$ are learnable coefficients. Spline functions are piecewise polynomial functions with high expressiveness and can approximate any continuous function. By adjusting the parameters and coefficients of the spline function, KAN can approximate various complex nonlinear functions.\nThe coefficients of the spline, $\\theta_i$, are updated through gradient descent methods using the loss function $L(\\theta)$ in (3). The update law is\n$\\theta_i^{(t+1)} = \\theta_i^{(t)} - \\eta \\frac{\\partial L}{\\partial \\theta_i}$                              (7)\nwhere $\\eta$ is the learning rate. KAN also employs regularization strategies to mitigate the risk of overfitting:\n$R(\\theta) = \\lambda_1 \\sum_i |\\theta_i| + \\lambda_2 \\sum_i \\sum_{j\\neq i} |\\theta_i - \\theta_j|$                        (8)\nwhere $R(\\theta)$ is the regularization term added to $L(\\theta)$, which can control the complexity of the model whilst fitting the data and improve the generalization ability of the model. $\\lambda_1$ and $\\lambda_2$ are regularization coefficients. The $L_1$ regularization term, $\\lambda_1 \\sum_i |\\theta_i|$, promotes sparsity in the parameter matrix. Another $L_1$ regularization term, $\\lambda_2 \\sum_i \\sum_{j\\neq i} |\\theta_i - \\theta_j|$, ensures smoothness in the parameter values across different neurons, facilitating model stability and preventing drastic changes in the output for minor fluctuations in the input data.\nKAN also adopts parameter sharing among neurons defined as follows:\n$\\theta_{shared} = \\frac{1}{N_{group}} \\sum_{i\\in group} \\theta_i$                              (9)\nwhere $group$ includes indices of neurons sharing parameters, and $N_{group}$ is the number of neurons in this group. The shared parameters, $\\theta_{shared}$, reduce the overall model complexity by averaging the parameters of neurons within the same group, enhancing computational efficiency and potentially improving the model's generalization capabilities over similar features.\nThese elements of the KAN architecture collectively enhance the flexibility and efficiency of the learning process, whilst ensuring robustness against overfitting and maintaining high performance across reinforcement learning tasks."}, {"title": "C. KAN Enhanced DQN", "content": "The integration of KAN within DQN, termed K-DQN, enables the network to better approximate the Q-function, especially in scenarios with complex action dynamics and reward functions. This capability translates into more robust learning and improved policy development in DRL tasks. To demonstrate why using the KAN network combined with DQN is superior to combining it with other reinforcement learning frameworks, it is essential to analyze the core differences in how these algorithms handle environments and tasks, as well as the characteristics of KAN activation functions.\nTo formulate the roundabout driving problem as a Markov Decision Process, we define the following key components:\nState Space: The state space S consists of the ego vehicle's position, velocity, and heading, as well as the relative positions, velocities, and headings of the surrounding vehicles within a certain range. The state at time t is represented as\n$s_t = [p_{ev}(t), v_{ev}(t), h_{ev}(t), p_{Nv}(t), v_{v}(t), h_{v}(t)]$                        (10)\nwhere $p_{ev}(t)$, $v_{ev}(t)$, and $h_{ev}(t)$ denote the position, velocity, and heading of the ego vehicle at time t, and $p_{ony}(t)$, $v_{v}(t)$,"}, {"title": "IV. ROUTES PLANNER AND ACTIONS INSPECTOR", "content": "This section introduces the mechanisms to ensure the safety and efficiency during the interactive driving in the roundabout. Three subsections are included: the driving rules of HDVs, the action inspector, and the route planner."}, {"title": "A. Driving Rules of HDVs", "content": "This subsection outlines the priority rules for HDVs operating within roundabouts, designed to maintain traffic flow and enhance safety. The rules are formulated to address typical scenarios encountered in roundabouts.\n1) Entry Rule: When an HDV approaches a roundabout entrance, it must yield to any vehicle already passing through the entrance it intends to use. This rule ensures that vehicles inside the roundabout maintain a smooth flow and reduces potential entry conflicts.\n$HDV_{entering} \\text{ if } \\exists HDV_{passing}$                            (25)\nOnce inside the roundabout, ego HDVs (EHDV) are required to adjust their speeds according to the Intelligent Driver Model (IDM) to maintain a safe distance from their immediate front HDV (FHDV) until they reach their intended exit. This adjustment is crucial for preventing rear-end collisions and ensuring steady traffic flow within the roundabout. The IDM following rule is formulated as\n$a_{IDM} = a_{max}[1 - (v_{FHDV}/v_e)^4 - (h^*/h)^2]$                        (26)\nwhere $a_{max}$ is the maximum acceleration of EHDV, $v_{FHDV}$ is the velocity of FHDV, $v_e$ is the expected velocity of EHDV, and h is the real gap between EHDV and FHDV. $h^*$ is the desired gap between EHDV and FHDV with the formula\n$h^* = h_c + \\frac{v_{Av} T_e}{v_c} + \\frac{v_{Av} \\Delta v}{2 \\sqrt{a_{max} C}}$                       (27)\nwhere $h_c$ is the expected space with FHDV, $v_{AV}$ is the speed of the AV, $T_e$ is the expected time gap, $\\Delta v$ is the velocity difference between EHDV and FHDV, and c is the comfortable deceleration.\n2) Inner Lane Following Rule: HDVs in the inner lane of the roundabout must align their speeds with the nearest vehicle ahead, even if that vehicle is in the outer lane. This rule is intended to synchronize speeds across lanes and enhance the cohesive flow of traffic, particularly in multi-lane roundabouts."}, {"title": "B. Route Planner", "content": "The integrated route planner for the EV comprises initial-lane selection decisions, a path-planning algorithm, and a lane-change selection mechanism. The initial-lane selection is guided by the TTC metric for each lane, ensuring safety and efficiency from the start. The path planning algorithm employs a node-based shortest path calculation to determine the most optimal route. The lane-change selection mechanism is driven by a proposed lane change cost formula, facilitating effective and strategic lane changes.\n1) Initial-Lane Selection: By computing the TTC between the ego vehicle and surrounding vehicles, the safety levels can be ensured and unsafe actions can be avoided. In this scenario, the more potential space for driving and safety are the major considerations, thus we calculate the TTC for the inner and outer lanes as follows:\n$TTC_{inner} = \\frac{\\text{Distance to HDV}_{inner}}{\\text{Speed of EV} - \\text{Speed of HDV}_{inner}}$\n$TTC_{outer} = \\frac{\\text{Distance to HDV}_{outer}}{\\text{Speed of EV} - \\text{Speed of HDV}_{outer}}$                (28)\nThe obtained TTC of both lanes can then be used to make the initial-lane selection rules. This paper considers several situations: No HDVs present, One HDV in outer lane, One HDV in both lanes, and Multiple HDVs in both lanes. These scenarios are described as follows:\n*   No HDVs present: The lane selection rule is\n$Laneselected = \\text{ Inner lane if HDVs } \\neq 0$.                        (29)\nWhen no HDVs are present in the roundabout, the AV will directly select the inner lane due to its shorter radius, which reduces the distance from the entrance to the outlet.\nIf two HDVs have the same velocity but the inner-lane HDV is farther from the EV, the inner lane is selected.\n  2) In-Roundabout Lane Selection: After entering the roundabout and selecting an initial lane, the next stage is path planning. We adopt a modified Breadth-First Search (BFS) [30] method that considers both distance and traffic conditions to compute the optimal path from a start point to a target within a graph structure, where nodes represent intersections in the road network, and edges represent drivable roads. The modified BFS algorithm uses a cost function that takes into account both the distance and traffic density:\n$C(e) = w_1D(e) + w_2D(e)$                      (32)\nwhere C(e) is the cost of edge e, $D(e)$ is the distance of edge e, $D(e)$ is the traffic density of edge e, and $w_1$ and $w_2$ are weight factors that determine the relative importance of distance and traffic density. $D(e)$ is calculated by\n$D(e) = N_e/L_e$                       (33)\nwhere $N_e$ is the number of vehicles on edge e, and $L_e$ is the length of edge e. The modified BFS is formulated as\n$BFS(s, g) = \\min \\{p : s \\rightarrow ... \\rightarrow g | p \\in Paths(s,g)\\}$                     (34)\nwhere s is the start node, g is the goal node, and Paths(s, g) is the set of all possible paths. The optimal path is given as\n$p^* = \\arg \\min_{p \\in Paths(s,g)} \\sum_{e \\in p} C(e)$                           (35)\nwhere $p^*$ is the optimal path.\n*   Traffic Density $D$ is calculated by iterating over all vehicles to count the number on a specified node and lane, and adjusting the density value based on vehicles' relative positions, with closer vehicles having a higher weight. When the EV is at lane node n, the density is\n$D(n,1) = \\sum_{N_v \\in N} 1(N_{V.n = n} / N_{V.l = l}) - 1(N_{V.n = n} / N_{V.l \\neq l})$       (36)\nwhere n indicates the node, I indicates the lane, $N_V$ is the set of neighbor vehicles, and 1 is the indicator function:\n$\\text{1}_{\\text{condition}} = \\begin{cases} 1 & \\text{if the condition is true} \\\\ 0 & \\text{otherwise} \\end{cases}$ (37)\n  2) Action execution for EV in a Roundabout\n\n*   Lane Change Cost C is obtained by computing the distance between the controlled vehicle and other vehicles. The costs increase sharply if the distance is less than a threshold safety distance $D_{safe}$. The cost formula is\n$D(EV, NV) = ||p_{ev}(t) - p_{nv}(t)||$                   (38)\n$C(n,1) = \\frac{D(EV,NV)}{D_{safe}}  \\sum_{N_{VENVD}}1(D(EV,NV)<D_{safe})$\nwhere D(EV, NV) is the distance between EV and NV, $p_{ev}(t)$ is EV's position at time step t, and $p_{nv}(t)$ is NV's position at time step t.\nBased on the above parameters, the lane is selected using\n$lane\\_choice = arg \\min_{le{0,1}} (D(n,l) + C(n,l))$                  (39)\nIf the costs for both lanes are equal, the decision is further optimized based on the vehicle's current position. This approach enhances the efficiency and safety of vehicle control around a roundabout by integrating real-time traffic conditions with the potential risks of lane changes. By utilizing the modified BFS algorithm, the algorithm optimizes the selection of paths.\n  2) Safety Margin Calculation: This margin is used to guide decision-making when selecting driving actions. As vehicles maintain a wider angle relative to each other while in proximity, the likelihood of their paths intersecting decreases. Therefore, the safety margin for each vehicle's maneuver is defined as the minimum difference in relative angle, $D_{a,k}$, or the shortest time until a potential collision could occur.\n$Safety Margin = \\min_{a \\in A_{feasible}} D_{a,a,k}$                    (40)\nwhere $A_{feasible}$ is the set of feasible actions and $D_{a,a,k}$ is the safety margin angle under action a at prediction time k."}, {"title": "V. MPC FOR ENHANCING DRL PERFORMANCE", "content": "This section introduces the robust control for AVs including the vehicle dynamic model and MPC. The MPC controller considers the vehicle dynamics, collision avoidance, and other constraints in its optimization process. It predicts the future states of the EV and surrounding vehicles using the vehicle dynamic model and the actions of neighboring vehicles predicted by the DRL agent. The combination of DRL and MPC in the proposed framework brings several benefits: it allows the DRL agent to focus on high-level decisions while the MPC manages low-level controls; the MPC can correct any imperfections in the DRL agent's decisions to ensure safe and feasible actions; and MPC provides a reliable, interpretable control strategy based on clear vehicle dynamics and constraints.\nThe state of the EV at time step t is updated using\n$p_{ev} (t + 1) = p_{ev}(t) + v_{ev}(t) \\cdot cos(h_{ev}(t)) \\cdot \\Delta t$                \n$VEV(t + 1) = v_{ev}(t) + u(t) \\cdot \\Delta t$  \n$h_{ev}(t + 1) = h_{ev}(t) + \\frac{VEV (t)}{L} tan(\\delta(t)) \\cdot \\Delta t$           (41)\nwhere $\\Delta t$ is the sampling time, $v_{av}(t)$ is the speed, $h_{av}(t)$ is the heading angle, L is the wheelbase length, u(t) is the acceleration, and $\\delta(t)$ is the steering angle.\nThe following control input and collision avoidance constraints are applied to ensure safety and feasibility:\n$U_{min} \\leq VEV(t) \\leq U_{max}, amin \\leq u(t) \\leq amax,$\\\n$\\delta_{min} \\leq \\delta(t) \\leq \\delta max, ||p_{ev}(t) - p_{nv}(t)|| \\geq Dsafe.$   (42)\nAt time step t, the optimal solutions u*(t) and $\\delta*(t)$ are obtained by solving the optimization problem:\n$\\min J_c$ \ns.t. (41), 0 < vav(k) \u2264 Umax, Amin \u2264 u(k) \u2264 amax,\n$\\delta_{min} \\leq \\delta(k) \\leq \\delta max, NV \\in N_v,k \\in [0, N_p \u2212 1]$     (43)\nNp-1N\nwith the cost function $J_c$  \u03a3ko (WAV (k) \u2013 AV) + N (Ps (k)-Pay(k) || - Dsafe)*+ \u03a3ku\u00b2(k).\nNp and Nc represent the prediction horizon and control horizon, respectively. Np is the prediction horizon and Ne is the control horizon. vy(k) is the target speed, N\u2082 is the number of NVs, and is a weighting factor for the acceleration penalty. In our experiments, we set Np as 10 and the Ne as 5. The entire control process is summarized in Algorithm 3."}, {"title": "VI. SIMULATION RESULTS", "content": "In this section, we assess the performance of the proposed K-DQN algorithm by evaluating its training efficiency and collision rate within the roundabout driving scenario, within the roundabout driving scenario depicted in Section II. The inner roundabout's road radius is 40 meters, and the outer roundabout's road radius is 48 meters, with both lanes having a width of 4 meters. Vehicles that exit the roundabout will be removed from the AV's view, although their kinematics will continue to be updated. Specifically, we examine three situations with different sectional configurations and varying levels of traffic density, characterized by distinct safety and control mechanisms, and different numbers of initial vehicles. The situations are defined as follows:\n*   Functional mechanism validation on hard mode: The proposed K-DQN is compared with K-DQN without a safety inspector and without the MPC control module.\n*   Normal mode validation: The proposed K-DQN is compared with benchmark algorithms with seven initial vehicles in the roundabout as depicted in Fig. 5(a).\n*   Hard mode validation: The proposed K-DQN is compared with benchmark algorithms with eleven initial vehicles in the roundabout as depicted in Fig. 5(b).\nThe benchmarks used for comparison in the normal and hard mode validations include PPO [21], A2C [31], ACKTR [32], and DQN [33]. The performance metrics used for evaluation include training convergence rate, collision rate, average speed, and reward values during training and evaluation. Considering the inherent risks associated with real-world vehicles and the constraints imposed by legal regulations, scenario-based virtual testing offers significant benefits, such as, precise environmental replication and enhanced testing efficiency. Therefore, this study employs scenarios developed on the Highway virtual simulation platform [34]. At the end of each episode, the vehicles and their velocities are slightly randomized at their spawn points to enhance the generalization capability of our proposed model. The computer and environment setup for this study include Python 3.6, PyTorch 1.10.0, Ubuntu 20.04.6 LTS OS, a 12th generation 16-thread Intel\u00ae Core\u2122 i5-12600KF CPU, an NVIDIA GeForce RTX 3090 GPU, and 64GB of RAM."}, {"title": "A. Functional Mechanism Validation", "content": "This section describes experiments to evaluate the crucial functions of the safety inspector and MPC of the proposed system in hard mode. We divide the experiments into four validations: Validation 1 evaluates training convergence of K-DQN, K-DQN without safety inspector, and K-DQN without MPC. Validation 2 tests the stability of the speed variations. Validation 3 compares the reward across the evaluation. Validation 4 analyzes the number of collisions.\nValidation 1: Training convergence. To better evaluate performance in the case where surrounding HDVs exhibit random driving behavior, we conducted tests using three random seeds and varied the generated scenarios. Fig. 6 illustrates the comparison of training curves between the proposed K-DQN, the K-DQN without a safety inspector, and the K-DQN without MPC. As expected, the proposed K-DQN design outperforms both the K-DQN without a safety inspector and the K-DQN without MPC, delivering higher peak rewards and faster convergence. The training curves of the proposed K-DQN exhibit smooth convergence and low variance across different random seeds, indicating a stable training process. In contrast, the K-DQN without a safety inspector and the K-DQN without MPC show more fluctuations in their training curves and slower convergence rates. It is evident that the absence of a safety inspector leads to the least satisfactory training outcomes. The safety inspector effectively reduces collisions, aligning with the observation that the loss due to collisions is greater than the loss of efficiency during training. The proposed K-DQN surpasses the peak reward of the K-DQN without MPC in approximately 4000 epochs, indicating its superior training performance.\nValidation 2: Stability of the speeds variation. Figure 7 illustrates the comparison of speed variation between the proposed K-DQN, the K-DQN without a safety inspector, and the K-DQN without MPC. As expected, the proposed K-DQN design demonstrates more stable speed control compared to the other two networks. It is evident that the absence of a safety inspector leads to the least stable speed variation, resulting in unstable driving. The safety inspector effectively reduces collisions, which aligns with the observation that collisions result in zero-speed events, significantly decreasing the average speed. The K-DQN without MPC exhibits a speed variation around 5 m/s. In contrast, the proposed K-DQN limits the variation to within 2 m/s, enhancing stability.\nValidation 3: Reward values among evaluation. Figure 8 illustrates the comparison of rewards between the proposed"}, {"title": "B. Normal Mode Validation", "content": "In this section, we conduct experiments to evaluate the safety and efficiency of the proposed system in normal mode compared to other benchmark DRL algorithms, PPO [21], A2C [31], ACKTR [32], and DQN [33]. We divide the experiments into two validations: Validation 1 evaluates fast-convergence training, stable and high-speed variation, and collision rate during training. Validation 2 compares the reward levels during evaluation.\nValidation 1: Better training performance. To evaluate the training performance of the proposed K-DQN compared to other benchmark algorithms, we conducted tests using three random seeds and varied the generated scenarios. Fig. 9 illustrates the comparison of training curves between the proposed K-DQN and other benchmark algorithms. Fig. 9(a) shows the comparison of rewards during training, where the proposed K-DQN design outperforms other benchmark algorithms, achieving much higher peak rewards and faster convergence. The DQN has the poorest performance, with rewards around 80. The A2C and ACKTR exhibit similar reward levels, while the PPO reaches approximately 125, the highest reward among the other benchmark algorithms. The proposed K-DQN surpasses all, achieving rewards over 200. Fig. 9(b) depicts the comparison of speed variation during training. The proposed K-DQN design excels, maintaining higher speeds and stable speed variation at the end of training. The PPO demonstrates both the lowest speed and the most unstable speed variation. The A2C, ACKTR, and DQN have similar curves, with the A2C reaching a higher speed of around 18 m/s by the end of 10,000 training episodes. The proposed K-DQN maintains a speed level of approximately 22 m/s at the end, significantly faster than the others. Fig. 9(c) shows the collision rate during training, where the proposed K-DQN design again outperforms the benchmark algorithms by achieving a much lower collision rate. The DQN has the highest collision rate at around 0.55. The ACKTR reduces the collision rate to around 0.35. The A2C and PPO have similar collision rates at around 0.2, with the PPO exhibiting greater variability. The proposed K-DQN maintains a collision rate below 0.05, significantly lower than the others.\nValidation 2: Reward values among evaluation. Figure 11 illustrates the reward comparison between the proposed K-DQN and other benchmark algorithms. The proposed K-DQN design demonstrates significantly higher rewards compared to all other benchmark algorithms. The DQN has the lowest reward during the evaluation, falling below 75. The A2C and ACKTR are similar, both increasing the reward to around 75. The PPO has a relatively higher reward of around 100, peaking at 125. However, the reward of the proposed K-DQN fluctuates around 175, significantly surpassing other benchmark algorithms."}, {"title": "C. Hard Mode Validation", "content": "In this validation", "1": "Better training performance. We employed three distinct random seeds to generate a variety of scenarios", "2": "Reward values among evaluation. In the conducted evaluation, the proposed K-DQN algorithm's reward efficacy was benchmarked against conventional algorithms using a triad of random seeds to foster diverse scenario generation, as illustrated in Fig. 12. The empirical data unequivocally demonstrates the K-DQN's superior reward acquisition, consistently outperforming its counterparts. The traditional DQN algorithm trailed with suboptimal rewards, not exceeding 70."}]}