{"title": "A theory of understanding for artificial intelligence: composability, catalysts, and\nlearning", "authors": ["Zijian Zhang", "Sara Aronowitz", "Al\u00e1n Aspuru-Guzik"], "abstract": "Understanding is a crucial yet elusive concept in artificial\nintelligence (AI). This work proposes a framework for ana-\nlyzing understanding based on the notion of composability.\nGiven any subject (e.g., a person or an AI), we suggest char-\nacterizing its understanding of an object in terms of its abil-\nity to process (compose) relevant inputs into satisfactory out-\nputs from the perspective of a verifier. This highly universal\nframework can readily apply to non-human subjects, such as\nAls, non-human animals, and institutions. Further, we pro-\npose methods for analyzing the inputs that enhance output\nquality in compositions, which we call catalysts. We show\nhow the structure of a subject can be revealed by analyzing\nits components that act as catalysts and argue that a subject's\nlearning ability can be regarded as its ability to compose in-\nputs into its inner catalysts. Finally we examine the impor-\ntance of learning ability for Als to attain general intelligence.\nOur analysis indicates that models capable of generating out-\nputs that can function as their own catalysts, such as language\nmodels, establish a foundation for potentially overcoming ex-\nisting limitations in AI understanding.", "sections": [{"title": "1 Introduction", "content": "Understanding is both a technical term in philosophy and\na topic of interest in science and everyday life. In philoso-\nphy, discussions of understanding often center around its re-\nlationship with other epistemological concepts, as well as its\nconnection to epistemic and practical norms. From this perspec-\ntive, a successful definition of understanding matches pre-\ntheoretic intuitions, draws links with concepts like knowl-\nedge and belief, and explains why understanding is desir-\nable. With a satisfying definition in hand, we can then ask\nwhether machines understand.\nHowever, in the pursuit of machine intelligence, the ques-\ntion of understanding has arisen in a different, and urgent\nform: in a passive sense, we want to know whether current\nAl systems can understand certain objects, such as moral-\nity, psychology, or Newtonian physics. For example, Krenn\net al. (2022) proposed potential tests to verify whether a\nsystem has scientific understanding and highlighted the im-\nportance of whether AI can transfer their understanding to\nhumans. In an active sense, creating AI systems that can\nunderstand is also an important aim of many designers.\nAmong various efforts in building AI systems, the advent\nof large language models (LLMs) (Brown et al. 2020; Ope-\nAI 2023a; Team et al. 2023; Anthropic 2024) has dramat-\nically advanced this field by providing a universal language\nprocessor that is powerful yet not limited to specific tasks.\nMany of these models have been reported to outperform humans in prede-\nfined natural language understanding challenges while maintaining generalizability to many\nother tasks. This generalizability has enabled the construc-\ntion of LLM-based agents that show unprecedented univer-\nsal ability in observing, planning, and acting.\nDespite the advancement, controversy still surrounds\nwhether LLMs truly understand, which is closely related to some of the practical problems\nthat LLMs currently face. One of the well-known issues is\nhallucination, where LLMs generate nonsensical informa-\ntion. Although humans\nalso generate incorrect information, LLMs additionally suf-\nfer from the hardness of model editing, mak-\ning it difficult to remove hallucinations by updating knowl-\nedge. In addition, LLMs constantly have much lower perfor-\nmance on counterfactual tasks that rarely appear in the train-\ning set, implying that LLMs are far from\nthe kind of ideal thinker who can generate and use general-\npurpose concepts for out-of-distribution problem solving.\nLLMs also face the problem of prompt brittleness, in which\neven trivial changes in input (prompt) can drastically alter\nthe output produced by LLMs. Further, a more general problem lies in challenges\nfor statistical learning as a path to understanding: the train-\ning regime of language models may not be able to ground\nunderstanding, even for an arbitrarily sophisticated machine.\nAll these problems imply that LLMs are still far from\nachieving understanding at the human level. This under-\nstanding would enable them to operate their memory ro-\nbustly, link observations to concepts, and produce unusual\nyet correct outputs. However, it has been argued that LLMs\nhave some significant ability to understand and produce\nmeaningful utterances according to some standards. Further, many higher-level con-"}, {"title": "2 What is understanding?", "content": "We start by noticing the challenges in developing a general\naccount of understanding. Consider the following uses of\n\"understanding\":\n\u2022 Alice understands Newtonian mechanics. (a theory)\n\u2022 An LLM understands black holes. (an object)\n\u2022 Charlie understands that the sky is blue. (a proposition)\nThese examples show that, at least intuitively, understanding\ncan apply to objects and subjects of many types. This im-\nplies that a general theory of understanding must apply uni-\nversally to these objects and subjects. However, achieving\nthis desired universality introduces two complexities arising\nfrom both the diversity of objects and subjects.\nTo illustrate the complexity introduced by the diversity of\nobjects, let us explore the concept of \"knowing\" in relation\nto \"understanding\u201d. While \u201cknowing\u201d can be predicated of\nany true proposition, \u201cunderstanding\u201d carries a more com-\nplex implication about its object. To see this, consider\nthe following examples:\n\u2022 ChatGPT knows that the sky is blue.\n\u2022 ChatGPT understands that the sky is blue.\n\u2022 Bob knows his phone number.\n\u2022 Bob understands his phone number.\nWhen the verb \"knows\" is replaced with \"understands\",\nthe sentences seem to imply a substantially different mean-\ning rather than merely increased confidence or strength.\nWhen we say ChatGPT understands that the sky is blue,\nwithout further context, it implies that ChatGPT possesses\nmore background knowledge about this fact. For example, it\nknows that the color of the sky is caused by the scattering\nof photons by particles in the atmosphere. Similarly, when\nBob understands his phone number, it sounds odd unless we\ncan see in context that he does not merely know his phone\nnumber but also grasps some additional features of it, such\nas how to help others remember it easily or how to dial it\nfrom abroad. Understanding seems to always imply some\nadditional ability related to the understood object. While it\nmay be easy to infer what \"understanding\" implies for a spe-\ncific object, it is challenging to define a universal rule that\nspecifies which additional abilities should contribute to the\nunderstanding of an object.\nMost contemporary philosophers sidestep the range of ob-\njects of understanding by focusing on propositional under-\nstanding, where the object of understanding is limited to\npropositions (for instance Khalifa (2012) or Sosa (2010),\nsee Hannon (2021) for a review). While this approach might\nbe suitable for debates about the connection between under-\nstanding and other epistemological concepts, such as knowl-\nedge, it is insufficient in our context: in the case of AI, we\nhave a direct interest in whether they can understand certain\ndomains or subject areas and are only indirectly concerned\nwith whether they understand particular propositions. For\ninstance, Bender and Koller (2020) survey discourse about\nwhether LLMs can understand \"how human beings com-\nmunicate\", \"common sense\", \"running prose\", and so on\n(p5186).\nAnother complexity of understanding is rooted in the di-\nversity of its subjects. Consider the following example:\n(Understanding with assistance) Bob is not good at\nexplaining or identifying safe driving. However, when\nhe drives a car equipped with a safety system, he ad-\nheres to the system's prompts and has an excellent\ndriving record. Though it might contain many rules\nfor safe driving, the system itself cannot drive with-\nout Bob.\nDoes Bob understand safe driving? He surely does not\nunderstand it in the same way that someone who can drive\nsafely without the system does. One way to deal with this\ncase would be treating Bob and the safety system as a com-\nposite subject. But this approach ignores the useful structure\ninside the composite subject: Bob's way of understanding\nis in relation to a particular aid, the safety system. In fact,\nwe find it an underappreciated feature that understanding is\nvery often significantly dependent on an aid: understanding\na multiplication table from memory, by using an algorithm,\nor by relying on notes are importantly different. This second\ncomplexity is in the subject of understanding.\nWhat kind of account of understanding considers the\nbroadness and complexity of the objects and subjects? We\ncontend that the answer is one that is both practical and\nminimal. A practical account of understanding links un-"}, {"title": "3 Understanding as composability", "content": "To design our practical, minimal theory of understanding,\nwe need a theoretical tool to describe the subject's be-\nhaviour. These two requirements suggest the following defi-\nnition:\nDefinition 1 (Composition) Composition is a process in\nwhich a subject creates the output $I_{out}$ given a list of inputs\n$I = [I_0, I_1, ...]$.\nIn the definition, the \"inputs\" and \"output\" of a composi-\ntion can be anything the theory user believes to be reason-\nable in the context. For example, consider syllogistic rea-\nsoning. Imagine a person facing statements: \"Deduce using\nsyllogism\", \"All men are mortal\" and \"Socrates is a man\".\nA natural answer from the person would be, \"Socrates is\nmortal\". From the notion of composition, we regard the per-\nson as composing the first two statements into the conclu-\nsion. The inputs of compositions are not limited to a cer-\ntain category. For example, the inputs of compositions can\nalso include tools and instructions, with the outputs being\nthe results of using these tools. Consider a search engine\nand an instruction to write a report about apple. The process\nof using the search engine to gather information and pro-\nduce the report can be viewed as a composition, where the\ninputs are composed into the finished report. This broad def-\ninition of inputs and outputs makes the concept of composi-\ntion flexible enough to describe broad behaviours related to\nunderstanding, which helps make our theory of understand-\ning practical.\nFurther, composition is also a general concept that can\napply universally to subjects. For example, Turing machines\ncan be treated as subjects that compose inputs from the tape,\nand the processed state of the tape can be regarded as the\noutput of composition (Turing 2009). More abstractly, we\ncan treat the universe as a huge machine that composes its\ncurrent state, including the information of all the elementary\nparticles, into its next state. This universal applicability of\ncomposition helps to make the theory subject-general and\ntherefore minimal.\nWe continue by discussing how to use composition to\nbuild an operationalization of understanding. Here, we in-\ntroduce the concept of a verifier of understanding. A veri-\nfier of understanding is a subject that can assess instances\nof understanding (which might be their own understanding),\nincluding comparing understanding across subjects or con-\ntexts, and defining which inputs to composition are related\nto which objects of understanding. A reader who prefers a\nmore objective theory of understanding that is not relativized\nto a verifier could consider the verifier as a placeholder for\nthe role of a further epistemological theory \u2013 in this case,\nwe are defending the skeleton of an account of understand-\ning which requires further specification to be a final, objec-\ntive theory. With this construction, we formally give the core\ndefinition introduced by this work:\nDefinition 2 (Understanding as composability) A sub-\nject's understanding of an object O, in the view of a verifier\nV, can be fully characterized by the set S, which contains\nall the tuples $(\\vec{I}, I_{out})$, where $\\vec{I}$ is from the set $S_\\ast$ of all the\nlists of inputs that V deems to be related to O and $I_{out}$ is\nthe output of the subject composing $\\vec{I}$.\nWe use an example to illustrate how the definition can be\nused for ascribing understanding in context.\n(Understanding phenomena) A verifier can evalu-\nate Charlie's understanding of the phenomenon \"the\nsky of the earth is blue\" by his output $I_{out}$ of com-\nposing inputs that the verifier deems related (i.e., in\n$S_\\ast$), such as \"what causes the sky to be blue?\". Some\nsophisticated verifiers may include \u201cI want to see the\nblue sky of the moon\u201d in $S_\\ast$ because it tests whether\nCharlie can realize this is impossible.\nOur definition does not give criteria for understanding. In-\nstead, the definition aims to provide a framework for ana-\nlyzing instances of understanding by the set S that includes\nall behaviours of the subject about the understood object O."}, {"title": "4 Characterizing understanding", "content": "If you have read the book Dr. Jekyll and Mr. Hyde, could you\ndraw the layout of Dr. Jekyll's house? The writer Vladimir\nNabokov used to have his students produce drawings like\nthis as part of a literature course (see Figure 2). These draw-\nings are tests of understanding: Nabokov used both to test\nhis students and to make them aware of the limits of their\nown understanding (Nabokov 2017). You could also ask an\nartificial agent to draw the house: would it succeed, and if\nso, what would that tell us?\nIn Section 2, we have shown how to use the set S, which\ndescribes all the related behaviours of the subject, to char-\nacterize its understanding of an object. Commonly, the pref-\nerence of understanding varies from verifier to verifier. It\nis hard to give a verifier-independent method to character-"}, {"title": "4.1 Universality", "content": "Humans can input and output many different types of in-\nformation. One dimension of variation is modality: verbal,\nauditory, visual, tactile, etc. In the context of verbal inputs,\nhumans can also process multiple types of natural or pro-\ngramming languages. This ability allows humans to com-\npose multi-type inputs that are related to the object when\nthey understand it, as well as to produce multi-type outputs.\nFor example, humans can read a novel and draw the scenar-\nios in it, as in the example shown in Figure 2. It is also pos-\nsible to use the drawing as an input to help human readers\nunderstand the novel better. As we have demonstrated how\na verifier's understanding can influence their assessment of\nunderstanding, it is easy to see how the generality of hu-\nman processing is a significant factor in our perception of a\nparticular instance of understanding. This generality is not\nfully universal, but we can infer that approaching universal-\nity would similarly increase and alter understanding.\nIt is straightforward to observe that current AI systems,\nas of 2024, have not reached the same level of universality\nas humans. This implies that these Als' understanding (1)\nis different from humans, though they might outperform hu-\nmans in some way; (2) falls short in composing many inputs\nthat human verifiers might find critical. Machine learning\nmodels are not new to outperform humans in their tasks,\nsuch as playing games (Silver et al. 2016; Vinyals et al.\n2019), image recognition and some natu-\nral language processing tasks . Although these models acquired an S that hu-\nmans cannot achieve, they are still accused of not under-"}, {"title": "4.2 Scale", "content": "Another aspect we consider central to the subject's under-\nstanding of a subject is the scale of inputs and outputs it\ncan handle. For example, suppose Charlie could only com-\npose information from articles from his English textbook up\nto 200 words, and, after one week of study, Charlie could\ncompose with inputs of much longer English articles. In this\ncase, it is safe to say Charlie understands English better af-\nter one week. Similar cases appear when it is expected that\nsubjects who can handle large-scale bodies of information\ncan also handle small ones with similar complexity. For ex-"}, {"title": "5 Structure of understanding", "content": "In the preceding sections, we proposed a framework of un-\nderstanding that is minimal and independent of the prop-\nerties of the subject. This independence makes it possible\nto define subjects with flexibility when describing a situa-\ntion involving understanding. This allows us to handle cases,\nsuch as the case of assisted safe driving of Bob in Section 2,\nwhere the subject of understanding could be composite. Our\nframework, focusing on the operational implication of un-\nderstanding, has no difficulty in admitting the composite\nsystem as the subject of understanding. Further, by empha-\nsizing \u201ccomposing\u201d, our framework also provides other in-\nterpretations of Bob's safe driving:\n\u2022 Bob makes proper outputs by composing the safety sys-\ntem, the car, and other inputs.\n\u2022 The safety system makes proper outputs by composing\nBob, the car, and other inputs.\n\u2022 The car makes proper outputs by composing Bob, the\nsafety system, and other inputs.\nWe allow all three interpretations and hold that each pro-\nvides insights into different situations and make different\nemphases. The first interpretation might be the most natural\none in terms of its human-centric description, which empha-\nsizes that it is Bob who drives the car. From this point of\nview, we notice that Bob has to have the ability to compose\nprompts from the safety system and operate the car. How-\never, it is also possible to claim it is the safety system that is\nusing Bob as a tool to control the car. This view emphasizes\nthe system's ability to interact with humans and the car. The\nthird interpretation considers both Bob and the safety system\nas inputs. This interpretation moves our attention to the car's\nability to compose the driver and the safety system, which is\nrelated to its cockpit and interfaces to hold the safety system\n1.\nThe above discussions show how choice of subject pro-\nvides different insights into understanding. Our next ques-\ntion is how to leverage this flexibility in analyzing under-\nstanding in general. Though our framework is independent\nof the properties of the subjects, we will show how the struc-\nture of a subject can be revealed by choosing parts of the\nsubjects as assistance (catalyst) of understanding. We will\nalso demonstrate how the catalyst of understanding is re-\nlated to concepts such as explanation and knowledge, which\nare regarded as important to understanding in previous theo-\nries. Finally, we will show how to use the notion of catalysts\nto clarify the acquisition of understanding, where a better\nunderstanding is formed."}, {"title": "5.1 Catalyst", "content": "Humans' understanding can be boosted with assistance. The\nprevious example of understanding safe driving is not un-\ncommon. Diagrams, such as that in Figure 2, can also be re-\ngarded as assistance as they help readers understand a novel\nmore deeply. More theoretically, consider the following ex-\nample:\n(Proofs as catalysts) The Traveling Salesman Prob-\nlem (TSP) (Biggs, Lloyd, and Wilson 1986) is an NP\nproblem that asks whether there exists a path shorter\nthan $n$ that traverses all nodes in an input graph.\nThe problem is generally hard to solve, as the afore-\nmentioned TSP problem is an NP-complete problem\nfor which people do not have an efficient algorithm.\nHowever, for a certain graph, if one can show an\ninstance of a path shorter than $n$ (i.e. proof of the\nfact), answering the question can be done by verify-\ning the instance (composing with the instance), which\nrequires much fewer computational steps and could\nhelp the subject produce better outputs.\nThe path in the example can be regarded as a catalyst that\nboosts the subject's understanding in solving the TSP prob-\nlem in the graph. 2\nBased on the universal existence of assistance in all the\nabove examples, we formally define a catalyst as follows:\nDefinition 3 (Catalyst) Suppose a subject understands O\nwith the set S, in the view of verifier V (with S). In the\nview of V, C is a catalyst of understanding if it satisfies"}, {"title": "5.2 Subject decomposition", "content": "When we assess a subject's understanding by mere com-\nposability, we treat subjects as black boxes, not consider-\ning their internal structures. However, most subjects have an\ninternal structure, which matters for assessing understand-\ning. For example, in the example of Bob driving the car, the\ncomposite subject can be naturally decomposed into the car,\nBob, and the safety system. Given a subject and a catalyst,\nwe can always treat them as a whole, and this composite sub-\nject's internal structure would include the original subject\nand the catalyst. The examples above show how we make a\ncomposite subject and decompose it to identify its internal\nstructure.\nA similar idea can also be applied when we want to de-\ncompose a subject whose structure is unclear. For example,\nwhen we say Charlie understands the sky is blue. It is com-\nmon to say that Charlie should be able to use his knowledge\nof the sky's colour. Theoretically, Charlie's knowledge is a\npart of Charlie's body, which all functions in a black-box\nway to produce the output. However, in some contexts, we\nfind it helpful to decompose Charlie into a subject that can\nprocess knowledge and then some knowledge about the sky,\noptics, etc. This decomposition allows us to reason about the\nrole of each decomposed part in forming the understanding\nby acting as a \u201ccatalyst\u201d for understanding. It helps us to ask\nand answer \u201cif he did not have this part, what would be his\nunderstanding?\u201d, \u201cwhat caused him to get this part?\u201d, \u201chow\nwell can he use this part?\u201d, etc. With the above heuristics,\nwe define the inner catalyst below as a central concept to\ndecompose and find the structure of a subject.\nDefinition 4 (Subject decomposition) Regarding a sub-"}, {"title": "5.3 Acquisition of understanding", "content": "An important reason we propose subject decomposition is\nthat it allows us to reason about the elements constituting\nthe subject concerning understanding and, therefore, helps\nanalyze the acquisition of the subject's understanding. For\nexample, it makes it possible to ask questions such as \u201chow\ndo changes in inner catalysts cause the improvement of un-\nderstanding?\u201d and conversely, \u201chow is the improvement of\nunderstanding related to the changes of inner catalysts?\u201d. If\nthe car with Bob appears to drive more safely, the decompo-\nsition helps us conjecture that it might be either the safety\nsystem, Bob, or the car itself is improved. It also lets us con-\nsider how improving Charlie's knowledge can enhance his\nresponse when answering questions posed by others.\nHowever, is it always possible to reason about the acquisi-\ntion of understanding by the notion of catalysts? Our answer\nis yes. We note that there are only two cases when the un-\nderstanding of a subject is enhanced when the understood\nobject O is unchanged. The first is when a new catalyst is\nadded to the composition, and the second is when the sub-\nject is improved. The first case is trivially related to catalysts.\nFor the second, note that whenever we attribute the acquisi-"}, {"title": "6 General intelligence and learning", "content": "We have developed theories to characterize a subject's un-\nderstanding and analyze the structure behind the understand-\ning by catalysts. An immediate question that follows is:\n\"how could we apply this framework to the current LLM-\nbased systems and measure their distance towards the 'gen-\neral intelligence' of humans?\" It might be tempting to re-\ngard problems, such as hallucinations, failures on counter-\nfactual tasks, and prompt brittleness, as the gap between\nLLM-based systems and humans. However, it is obvious that\nhumans also suffer from similar problems. It is also com-\nmon for humans to be overconfident about their knowledge,\nunable to generalize their ability to new tasks, and be too\nsensitive to inputs.\nInstead of noticing the obvious gaps between the current\nAI systems and humans, we compare them at a higher level\nby their learning ability. Humans have strong learning abil-\nities, which might make learning ability an indispensable\npart of general intelligence in a human verifier's view. More\nimportantly, no matter how the AI system understands at a\npoint, a high-level learning ability enables the system to im-\nprove its understanding to the level that satisfies the verifier.\nWhen evaluating whether the AI system has achieved gen-\neral intelligence, humans usually only expect that the sys-\ntem has acquired a level of understanding that is similar to\nhumans, and humans usually know how this level of under-\nstanding can be acquired by human-style learning, such as\nreading a specific textbook and remembering some key facts.\nIn all the problems we mentioned for LLMs, though humans\nalso suffer from similar problems, we usually do not think\nthey are severe drawbacks. We claim that a crucial reason is\nthat we know how humans can overcome these problems by\nlearning. This point is revealed by the following analysis.\n(Baby intelligence) We usually regard human babies\nas having general intelligence, though their under-\nstanding of many objects is low when born. This is\nbecause we know that they have human-level learning\nabilities and will acquire human-level understanding\nwith their learning abilities (either innate or acquired\nover development).\nThough current AI systems have demonstrated learning\nability to some extent, it is still unclear how human-level ar-\ntificial learners can be built, and it is beneficial to investigate\nthe major gaps between the learning ability of current AI\nsystems and humans."}, {"title": "6.1 Gaps in learning ability", "content": "We have formulated learning ability as the ability to com-\npose inputs into the subject's inner catalysts in Claim 2,\nwhich allows us to analyze learning ability by composabil-\nity. In fact, we can also view learning ability as an instance of\nunderstanding and use the same framework to characterize\nit. This makes salient the universality and scale of learning\nability.\nHumans have the ability to handle universal types of in-\nputs for learning. For example, students would build their\nknowledge during classes through texts, diagrams, and the\nteacher's voices. In contrast, neural network-based learning\nonly accepts a narrow format of inputs in general. For ex-\nample, neural networks for image classification usually only\ntake pairs of images and categories as inputs. It is impos-"}, {"title": "6.2 Autocatalysis and learning", "content": "All these gaps between human and machine learners indicate\nthere is still huge space for AI to improve further. Though\nLLMs are not a complete solution to it, we still argue that\nthe advent of LLMs brings us closer to the goal of general\nintelligence. Our argument is based on the following spe-\ncial property of LLMs that is rare in other machine-learning\nmodels\u00b3.\nObservation 1 LLMs can use their outputs as catalysts.\nWe highlight that this autocatalysis property of LLMs\ncannot be found in the machine-learning models whose in-\nputs and outputs are of different types, such as models for\nimage recognition and image generation from texts. For\nmodels whose inputs and outputs are of the same type, such\nas natural language processing models for text summariza-\ntion and translation (Khurana et al. 2023), it is also not usual\nthat their outputs can serve as catalysts to boost their perfor-\nmance.\nHowever, it is very common that the outputs of a run of\nLLM are treated as part of the inputs of another run, and\nthis multi-stage processing of inputs has been leveraged in\nmany studies . The outputs that are input to LLMs again can be\ntreated as catalysts, as they usually carry critical information\nthat helps produce better outputs and, therefore, demonstrate\nbetter understanding.\nIt is not hard to notice that the autocatalytic property of\nLLMs is closely related to the learning ability described. The"}, {"title": "7 Conclusion", "content": "We have proposed a practical and minimal framework to\ncharacterize understanding by the subject's behaviour in\ncomposing inputs into outputs. We demonstrated how to use\nthis idea to analyze the universality and scale of the subject's\nunderstanding. Based on the framework, we established how\nto use the notion of catalysts to formulate assistance for un-\nderstanding and how to reveal the structure of a subject by\nthe notion of primitive subject and inner catalysts. Further,\nwe argue that the learning ability of a subject is determined\nby its ability to compose inputs into its inner catalysts. We\npointed out that a higher level of learning ability is criti-\ncal for Al systems to reach general intelligence. Finally, we\nshow how LLMs pave the way for implementing human-\nlevel artificial learners by their ability of autocatalysis.\nFrom the view of this work, our frameworks in this pa-\nper can be regarded as catalysts that help understand AIs'\nunderstanding. Moreover, the readers demonstrate the uni-\nversality and scale of their learning ability by reading and\nbuilding knowledge about our theory.\nFor future work, we note that we only demonstrated a\nsmall number of examples to which our framework can\nbe applied. Besides universality and scale, more important\ncharacteristics might be formulated using the notion of com-\nposability. For example, studying creativity as a characteris-\ntic of understanding might be interesting. Regarding cata-\nlysts and subject decomposition, we mainly focused on how\nto analyze the structure of existing subjects. However, de-\nscribing how to compose existing subjects into a larger sub-\nject, such as human or AI community, is also important.\nMoreover, we only discussed LLMs as an example of au-\ntocatalytic models. Nonetheless, studying other useful auto-"}]}