{"title": "A theory of understanding for artificial intelligence: composability, catalysts, and learning", "authors": ["Zijian Zhang", "Sara Aronowitz", "Al\u00e1n Aspuru-Guzik"], "abstract": "Understanding is a crucial yet elusive concept in artificial intelligence (AI). This work proposes a framework for analyzing understanding based on the notion of composability. Given any subject (e.g., a person or an AI), we suggest characterizing its understanding of an object in terms of its ability to process (compose) relevant inputs into satisfactory outputs from the perspective of a verifier. This highly universal framework can readily apply to non-human subjects, such as Als, non-human animals, and institutions. Further, we propose methods for analyzing the inputs that enhance output quality in compositions, which we call catalysts. We show how the structure of a subject can be revealed by analyzing its components that act as catalysts and argue that a subject's learning ability can be regarded as its ability to compose inputs into its inner catalysts. Finally we examine the importance of learning ability for Als to attain general intelligence. Our analysis indicates that models capable of generating outputs that can function as their own catalysts, such as language models, establish a foundation for potentially overcoming existing limitations in AI understanding.", "sections": [{"title": "Introduction", "content": "Understanding is both a technical term in philosophy and a topic of interest in science and everyday life. In philosophy, discussions of understanding often center around its relationship with other epistemological concepts, as well as its connection to epistemic and practical norms. From this perspective, a successful definition of understanding matches pre-theoretic intuitions, draws links with concepts like knowledge and belief, and explains why understanding is desirable. With a satisfying definition in hand, we can then ask whether machines understand.\nHowever, in the pursuit of machine intelligence, the question of understanding has arisen in a different, and urgent form: in a passive sense, we want to know whether current Al systems can understand certain objects, such as morality, psychology, or Newtonian physics. For example, Krenn et al. (2022) proposed potential tests to verify whether a system has scientific understanding and highlighted the importance of whether AI can transfer their understanding to humans. In an active sense, creating AI systems that can understand is also an important aim of many designers. Among various efforts in building AI systems, the advent of large language models (LLMs) has dramatically advanced this field by providing a universal language processor that is powerful yet not limited to specific tasks. Many of these models have been reported to outperform humans in predefined natural language understanding challenges while maintaining generalizability to many other tasks. This generalizability has enabled the construction of LLM-based agents that show unprecedented universal ability in observing, planning, and acting.\nDespite the advancement, controversy still surrounds whether LLMs truly understand, which is closely related to some of the practical problems that LLMs currently face. One of the well-known issues is hallucination, where LLMs generate nonsensical information. Although humans also generate incorrect information, LLMs additionally suffer from the hardness of model editing, making it difficult to remove hallucinations by updating knowledge. In addition, LLMs constantly have much lower performance on counterfactual tasks that rarely appear in the training set, implying that LLMs are far from the kind of ideal thinker who can generate and use general-purpose concepts for out-of-distribution problem solving. LLMs also face the problem of prompt brittleness, in which even trivial changes in input (prompt) can drastically alter the output produced by LLMs. Further, a more general problem lies in challenges for statistical learning as a path to understanding: the training regime of language models may not be able to ground understanding, even for an arbitrarily sophisticated machine.\nAll these problems imply that LLMs are still far from achieving understanding at the human level. This understanding would enable them to operate their memory robustly, link observations to concepts, and produce unusual yet correct outputs. However, it has been argued that LLMs have some significant ability to understand and produce meaningful utterances according to some standards. Further, many higher-level constructions, such as retrieval augmented generation (RAG) and LLM-based agents, have been built on LLMs and may be closer to understanding. Analyzing the understanding achieved by these composite systems presents another layer of complexity. More generally, pessimism about LLM understanding can raise the bar for understanding so high that some humans and animals may not meet it.\nHere, we introduce a framework to analyze understanding with the notion of composition, which involves synthesizing inputs into an output in the most general sense. We propose to characterize the understanding of an object by the subject's ability to properly compose inputs that a verifier believes to be appropriate. This framework allows for accessing understanding according to a broad range of behaviours beyond traditional approaches dependent on concepts such as beliefs, concepts, and assertions. With this method of characterizing understanding, we analyze the understanding of current AI systems from the aspects of universality and scale.\nIn order to go beyond a black-box behaviourism, we propose using the concept of \"catalyst\" to capture assistance in understanding, whether physically internal or external to the subject. The ability to produce catalysts can be used to characterize the subject's learning ability. Finally, we use the view of learning ability to view the problems of LLMs we mentioned above. We will provide arguments that imply that LLMs are still far from general intelligence, and yet how future LLM-based systems may offer promising solutions to fill the gaps."}, {"title": "What is understanding?", "content": "We start by noticing the challenges in developing a general account of understanding. Consider the following uses of \"understanding\":\n\u2022 Alice understands Newtonian mechanics. (a theory)\n\u2022 An LLM understands black holes. (an object)\n\u2022 Charlie understands that the sky is blue. (a proposition)\nThese examples show that, at least intuitively, understanding can apply to objects and subjects of many types. This implies that a general theory of understanding must apply universally to these objects and subjects. However, achieving this desired universality introduces two complexities arising from both the diversity of objects and subjects.\nTo illustrate the complexity introduced by the diversity of objects, let us explore the concept of \"knowing\" in relation to \"understanding\u201d. While \u201cknowing\" can be predicated of any true proposition, \u201cunderstanding\u201d carries a more complex implication about its object. To see this, consider the following examples:\n\u2022 ChatGPT knows that the sky is blue.\n\u2022 ChatGPT understands that the sky is blue.\n\u2022 Bob knows his phone number.\n\u2022 Bob understands his phone number.\nWhen the verb \"knows\" is replaced with \"understands\", the sentences seem to imply a substantially different meaning rather than merely increased confidence or strength. When we say ChatGPT understands that the sky is blue, without further context, it implies that ChatGPT possesses more background knowledge about this fact. For example, it knows that the color of the sky is caused by the scattering of photons by particles in the atmosphere. Similarly, when Bob understands his phone number, it sounds odd unless we can see in context that he does not merely know his phone number but also grasps some additional features of it, such as how to help others remember it easily or how to dial it from abroad. Understanding seems to always imply some additional ability related to the understood object. While it may be easy to infer what \"understanding\" implies for a specific object, it is challenging to define a universal rule that specifies which additional abilities should contribute to the understanding of an object.\nMost contemporary philosophers sidestep the range of objects of understanding by focusing on propositional understanding, where the object of understanding is limited to propositions. While this approach might be suitable for debates about the connection between understanding and other epistemological concepts, such as knowledge, it is insufficient in our context: in the case of AI, we have a direct interest in whether they can understand certain domains or subject areas and are only indirectly concerned with whether they understand particular propositions. For instance, Bender and Koller (2020) survey discourse about whether LLMs can understand \"how human beings communicate\", \"common sense\", \"running prose\", and so on (p5186).\nAnother complexity of understanding is rooted in the diversity of its subjects. Consider the following example:\n(Understanding with assistance) Bob is not good at explaining or identifying safe driving. However, when he drives a car equipped with a safety system, he adheres to the system's prompts and has an excellent driving record. Though it might contain many rules for safe driving, the system itself cannot drive without Bob.\nDoes Bob understand safe driving? He surely does not understand it in the same way that someone who can drive safely without the system does. One way to deal with this case would be treating Bob and the safety system as a composite subject. But this approach ignores the useful structure inside the composite subject: Bob's way of understanding is in relation to a particular aid, the safety system. In fact, we find it an underappreciated feature that understanding is very often significantly dependent on an aid: understanding a multiplication table from memory, by using an algorithm, or by relying on notes are importantly different. This second complexity is in the subject of understanding.\nWhat kind of account of understanding considers the broadness and complexity of the objects and subjects? We contend that the answer is one that is both practical and minimal. A practical account of understanding links un-"}, {"title": "Understanding as composability", "content": "To design our practical, minimal theory of understanding, we need a theoretical tool to describe the subject's behaviour. These two requirements suggest the following definition:\nDefinition 1 (Composition) Composition is a process in which a subject creates the output $I_{out}$ given a list of inputs $I = [I_0, I_1, ...]$.\nIn the definition, the \"inputs\" and \"output\" of a composition can be anything the theory user believes to be reasonable in the context. For example, consider syllogistic reasoning. Imagine a person facing statements: \"Deduce using syllogism\", \"All men are mortal\" and \"Socrates is a man\". A natural answer from the person would be, \"Socrates is mortal\". From the notion of composition, we regard the person as composing the first two statements into the conclusion. The inputs of compositions are not limited to a certain category. For example, the inputs of compositions can also include tools and instructions, with the outputs being the results of using these tools. Consider a search engine and an instruction to write a report about apple. The process of using the search engine to gather information and produce the report can be viewed as a composition, where the inputs are composed into the finished report. This broad definition of inputs and outputs makes the concept of composition flexible enough to describe broad behaviours related to understanding, which helps make our theory of understanding practical.\nFurther, composition is also a general concept that can apply universally to subjects. For example, Turing machines can be treated as subjects that compose inputs from the tape, and the processed state of the tape can be regarded as the output of composition (Turing 2009). More abstractly, we can treat the universe as a huge machine that composes its current state, including the information of all the elementary particles, into its next state. This universal applicability of composition helps to make the theory subject-general and therefore minimal.\nWe continue by discussing how to use composition to build an operationalization of understanding. Here, we introduce the concept of a verifier of understanding. A verifier of understanding is a subject that can assess instances of understanding (which might be their own understanding), including comparing understanding across subjects or contexts, and defining which inputs to composition are related to which objects of understanding. A reader who prefers a more objective theory of understanding that is not relativized to a verifier could consider the verifier as a placeholder for the role of a further epistemological theory \u2013 in this case, we are defending the skeleton of an account of understanding which requires further specification to be a final, objective theory. With this construction, we formally give the core definition introduced by this work:\nDefinition 2 (Understanding as composability) A subject\u2019s understanding of an object O, in the view of a verifier V, can be fully characterized by the set S, which contains all the tuples $(\\\u1ec8, I_{out})$, where $\\\u1ec8$ is from the set $S_{*}$ of all the lists of inputs that V deems to be related to O and $I_{out}$ is the output of the subject composing $\\\u1ec8$.\nWe use an example to illustrate how the definition can be used for ascribing understanding in context.\n(Understanding phenomena) A verifier can evaluate Charlie's understanding of the phenomenon \"the sky of the earth is blue\" by his output $I_{out}$ of composing inputs that the verifier deems related (i.e., in $S_{*}$), such as \"what causes the sky to be blue?\". Some sophisticated verifiers may include \u201cI want to see the blue sky of the moon\u201d in St because it tests whether Charlie can realize this is impossible.\nOur definition does not give criteria for understanding. Instead, the definition aims to provide a framework for analyzing instances of understanding by the set S that includes all behaviours of the subject about the understood object O."}, {"title": "Characterizing understanding", "content": "If you have read the book Dr. Jekyll and Mr. Hyde, could you draw the layout of Dr. Jekyll's house? The writer Vladimir Nabokov used to have his students produce drawings like this as part of a literature course. These drawings are tests of understanding: Nabokov used both to test his students and to make them aware of the limits of their own understanding. You could also ask an artificial agent to draw the house: would it succeed, and if so, what would that tell us?\nIn Section 2, we have shown how to use the set S, which describes all the related behaviours of the subject, to characterize its understanding of an object. Commonly, the preference of understanding varies from verifier to verifier. It is hard to give a verifier-independent method to characterize instances of understanding. However, we can show that there are still useful heuristics for analyzing a set S and the corresponding understanding. In the following, as examples, we demonstrate how the notion of composability can help us find two useful and general characteristics of instances of understanding."}, {"title": "Universality", "content": "Humans can input and output many different types of information. One dimension of variation is modality: verbal, auditory, visual, tactile, etc. In the context of verbal inputs, humans can also process multiple types of natural or programming languages. This ability allows humans to compose multi-type inputs that are related to the object when they understand it, as well as to produce multi-type outputs. For example, humans can read a novel and draw the scenarios in it, as in the example shown in Figure 2. It is also possible to use the drawing as an input to help human readers understand the novel better. As we have demonstrated how a verifier's understanding can influence their assessment of understanding, it is easy to see how the generality of human processing is a significant factor in our perception of a particular instance of understanding. This generality is not fully universal, but we can infer that approaching universality would similarly increase and alter understanding.\nIt is straightforward to observe that current AI systems, as of 2024, have not reached the same level of universality as humans. This implies that these Als' understanding (1) is different from humans, though they might outperform humans in some way; (2) falls short in composing many inputs that human verifiers might find critical. Machine learning models are not new to outperform humans in their tasks, such as playing games , image recognition , and some natural language processing tasks. Although these models acquired an S that humans cannot achieve, they are still accused of not understanding the tasks . To see why these criticisms are reasonable, consider AlphaGo and related models , which are intrinsically unable to process natural languages, whether as inputs or outputs. This makes them fundamentally unable to explain why a particular move is better than another in a way that makes the understanding transferable to humans, though they might possibly produce some numerical evidence showing their move is reasonable. We note that this analysis also applies to most computer vision models and even models for specific natural language processing tasks.\nThe above observations make it easier to see why the increased universality of LLMs can be seen as an important advancement. LLMs are designed to accept input texts and even images with different types and purposes. It is possible to specify instructions, examples, and background knowledge to help LLMs finish complicated tasks and produce human-friendly outputs. From the view of human verifiers, this makes LLMs able to compose information similarly to humans and thus produce more satisfactory outputs to the elements in S. Therefore, we see that LLMs, though probably not strictly outperforming some specific models, offer a way to produce agents with more favourable instances of understanding for human verifiers. We also note that the importance of the transferability of AI understanding, as discussed in , is also driven by the preferences of human verifiers.\nBesides being human-friendly, LLMs' increased universality probably brings a more critical advantage: their ability to use tools. Tool use has been already regarded as an important demonstration of human intelligence. LLMs, as being universal in processing languages, are shown to be able to use various tools , such as RESTful APIs , domain expert systems , and other machine learning models . It is not hard to imagine that an LLM-based agent can prepare inputs in a structured language (e.g., JSON) to machine learning models (e.g., AlphaGO) and interpret the outputs to humans in English text. We can also equip LLMs with a text-to-speech model that accepts JSON inputs to make their outputs more human-like. In contrast, models that handle a specific task generally cannot use these tools, though they might perform better than more universal models. This makes it difficult for them to improve their understanding without undergoing laborious model training."}, {"title": "Scale", "content": "Another aspect we consider central to the subject's understanding of a subject is the scale of inputs and outputs it can handle. For example, suppose Charlie could only compose information from articles from his English textbook up to 200 words, and, after one week of study, Charlie could compose with inputs of much longer English articles. In this case, it is safe to say Charlie understands English better after one week. Similar cases appear when it is expected that subjects who can handle large-scale bodies of information can also handle small ones with similar complexity. For example, to evaluate an LLM's understanding of reading comprehension, it is reasonable to test whether it can produce satisfactory outputs on longer texts. Though larger bodies of information are not always harder to compose than small ones, it is usually useful to check the scale of the inputs in S.\nIncluding scale as a special dimension seems redundant because one may regard large-scale input as just another type of inputs. We admit this ambiguity but suggest that the scale is of special importance due to its inherent difficulty. We point out that handling large-scale information is especially challenging for both humans and AI. Though being universal in input types, most LLMs are constrained by a limited input size. GPT-3.5 is capable of handling only up to 4,096 tokens in the initial version (each word takes one or more tokens). Upon its release, the more advanced GPT-4 is restricted to a mere 8,192 tokens . This limitation is due, in part, to inherent structural limitations within the transformer neural network. Some LLMs claim to be proficient with longer inputs, but empirical studies indicate a consistent decline in performance as input lengths increase . In contrast, humans can process long inputs, including long articles or videos, though they may process them much more slowly than LLMs. From this perspective, humans demonstrate a greater understanding in processing and handling articles and watching videos."}, {"title": "Structure of understanding", "content": "In the preceding sections, we proposed a framework of understanding that is minimal and independent of the properties of the subject. This independence makes it possible to define subjects with flexibility when describing a situation involving understanding. This allows us to handle cases, such as the case of assisted safe driving of Bob in Section 2, where the subject of understanding could be composite. Our framework, focusing on the operational implication of understanding, has no difficulty in admitting the composite system as the subject of understanding. Further, by emphasizing \"composing\u201d, our framework also provides other interpretations of Bob's safe driving:\n\u2022 Bob makes proper outputs by composing the safety system, the car, and other inputs.\n\u2022 The safety system makes proper outputs by composing Bob, the car, and other inputs.\n\u2022 The car makes proper outputs by composing Bob, the safety system, and other inputs.\nWe allow all three interpretations and hold that each provides insights into different situations and make different emphases. The first interpretation might be the most natural one in terms of its human-centric description, which emphasizes that it is Bob who drives the car. From this point of view, we notice that Bob has to have the ability to compose prompts from the safety system and operate the car. However, it is also possible to claim it is the safety system that is using Bob as a tool to control the car. This view emphasizes the system's ability to interact with humans and the car. The third interpretation considers both Bob and the safety system as inputs. This interpretation moves our attention to the car's ability to compose the driver and the safety system, which is related to its cockpit and interfaces to hold the safety system 1.\nThe above discussions show how choice of subject provides different insights into understanding. Our next question is how to leverage this flexibility in analyzing understanding in general. Though our framework is independent of the properties of the subjects, we will show how the structure of a subject can be revealed by choosing parts of the subjects as assistance (catalyst) of understanding. We will also demonstrate how the catalyst of understanding is related to concepts such as explanation and knowledge, which are regarded as important to understanding in previous theories. Finally, we will show how to use the notion of catalysts to clarify the acquisition of understanding, where a better understanding is formed."}, {"title": "Catalyst", "content": "Humans' understanding can be boosted with assistance. The previous example of understanding safe driving is not uncommon. Diagrams, such as that in Figure 2, can also be regarded as assistance as they help readers understand a novel more deeply. More theoretically, consider the following example:\n(Proofs as catalysts) The Traveling Salesman Problem (TSP) is an NP problem that asks whether there exists a path shorter than n that traverses all nodes in an input graph. The problem is generally hard to solve, as the aforementioned TSP problem is an NP-complete problem for which people do not have an efficient algorithm. However, for a certain graph, if one can show an instance of a path shorter than n (i.e. proof of the fact), answering the question can be done by verifying the instance (composing with the instance), which requires much fewer computational steps and could help the subject produce better outputs.\nThe path in the example can be regarded as a catalyst that boosts the subject's understanding in solving the TSP problem in the graph. 2\nBased on the universal existence of assistance in all the above examples, we formally define a catalyst as follows:\nDefinition 3 (Catalyst) Suppose a subject understands O with the set S, in the view of verifier V (with S). In the view of V, C is a catalyst of understanding if it satisfies that, for each element $\\overrightarrow{I} = [I_1, I_2, ...]$ of $Sv$, composing $\\overrightarrow{I^\\prime} = [C, I_1, I_2, ...]$ and use the output $I_{out}$ to replace the original $I_{out}$ in each $(\\overrightarrow{I}, I_{out}) \\in S$, makes a new set $S^\\prime$ that is more favourable in the view of V.\nPeople might find the definition of catalyst too loose as it can include nearly everything that helps to understand. For example, a neat workplace can be considered a catalyst because it helps the worker work better and produce more favourable S'. Though the workplace environment seems non-intellectual, we still regard it as a catalyst due to the minimal nature of our framework - it ignores the properties of the object involved besides its effect in changing understanding. We note that the purpose of defining catalysts is to help the user of our framework find important catalysts that matter in analyzing a situation of understanding. In the workplace example, gravity can also be regarded as a catalyst because gravity is necessary for the workers to carry out their work. However, noticing gravity as a catalyst, in this case, is less useful than noticing the effect of tidiness. Similarly, tidiness might be less important than intellectual catalysts such as well-written work instructions. The usefulness of a catalyst could depend on factors such as reusability, transferability, and associated cognitive costs such as storage size and demands of use.\nThe notion of catalyst can readily apply to the design of LLM-based AI systems. Designing catalysts for LLMs to use is an area of active research. To enhance the performance of LLMs, various methods for designing the prompts that are input to them have been proposed. For example, in chain-of-thought prompting , LLMs are prompted to carry out multi-round reasoning, enabling them to address complex inquiries without necessitating alterations to the LLMs themselves.\nRetrieval augmented generation (RAG) is another example. To help LLMs use knowledge from a long document, a common approach is to break it into fragments and retrieve the relevant fragments based on the context. The retrieved fragments will be input to the LLM along with other inputs to produce better outputs. The fragments retrieved can be regarded as direct catalysts. From another view, one can also regard the program that retrieves the fragments as a catalyst, producing outputs similar to those of composing the entire long document when it is composed.\nFrom the perspective of philosophy, many of the conditions of understanding proposed in previous work can be considered as requirements of catalysts. For example, many theorists believe that explanation is central to understanding . In our framework, explanations can be viewed as catalysts that boost understanding because we can expect composing inputs related to the phenomenon with the explanation of the phenomenon would produce better outputs. Extending this idea, we treat explanation-seeking curiosity as a kind of desire for catalysts.\nOn a larger scale, scientific theories can also be seen as catalysts when treating them as tools that generate explanations or solve problems. Kuhn considers scientific paradigms as instruments that help scientists solve scientific puzzles and regards outstanding puzzle-solving ability as the core reason new paradigms are accepted . In our framework, this is equivalent to saying that scientific paradigms are catalysts for scientific understanding, and paradigms are accepted if they help compose important inputs that have not been properly composed by their predecessors. In the context of scientific understanding, De Regt proposes that understanding a phenomenon requires fitting it into a broader theory . We claim that the requirement of a theory can be seen as a requirement of a catalyst, which proves the subject has the ability to properly compose a broader scope of inputs besides the standalone phenomenon.\nFinally, we note that catalysts do not have to be directly related to the object of understanding though we defined so, and catalysts could be applied across domains. For example, in the case of analogical reasoning, having a theory of motion in space can improve understanding of the analogically linked case of \"motion\" of the solution moving in the parameter space when using gradient descent methods based on momentum and friction. More generally, any bits of information, such as a vivid across-domain metaphor, that makes the explanation more accessible will also count as catalysts."}, {"title": "Subject decomposition", "content": "When we assess a subject's understanding by mere composability, we treat subjects as black boxes, not considering their internal structures. However, most subjects have an internal structure, which matters for assessing understanding. For example, in the example of Bob driving the car, the composite subject can be naturally decomposed into the car, Bob, and the safety system. Given a subject and a catalyst, we can always treat them as a whole, and this composite subject's internal structure would include the original subject and the catalyst. The examples above show how we make a composite subject and decompose it to identify its internal structure.\nA similar idea can also be applied when we want to decompose a subject whose structure is unclear. For example, when we say Charlie understands the sky is blue. It is common to say that Charlie should be able to use his knowledge of the sky's colour. Theoretically, Charlie's knowledge is a part of Charlie's body, which all functions in a black-box way to produce the output. However, in some contexts, we find it helpful to decompose Charlie into a subject that can process knowledge and then some knowledge about the sky, optics, etc. This decomposition allows us to reason about the role of each decomposed part in forming the understanding by acting as a \"catalyst\" for understanding. It helps us to ask and answer \"if he did not have this part, what would be his understanding?\u201d, \u201cwhat caused him to get this part?", "how well can he use this part?": "etc. With the above heuristics, we define the inner catalyst below as a central concept to decompose and find the structure of a subject.\nDefinition 4 (Subject decomposition) Regarding a subject's understanding of the object O, C is an inner catalyst if 1. C is a part of the subject; 2. C can be regarded as a catalyst that is composed by a primitive subject who is a part of the subject.\nThe following example shows an application of subject decomposition.\n(Optimizing neural networks) To see how the understanding of a neural network can be improved, we can first decompose it into two inner catalysts held by the backbone computer: the network architecture and the corresponding network parameters. Based on the decomposition, it is natural to see that we can improve its understanding by 1. optimizing the parameters, 2. finding better network architecture, and 3. fabricating better computers that support a better network architecture.\nSubject decomposition encourages us to find a proper set of inner catalysts and study their interaction with the primitive subject. At the beginning of Section 5, we discussed how we can describe the understanding of safe driving in the composite subject in different ways. With subject decomposition, we see that Bob and the safety system can be seen as two straightforward examples of the inner catalysts when treating the car as a primitive subject. This decomposition, thus, reminds us to think about the different roles that Bob and the safety system take when composing inputs, as well as the primitive subject's (the car's) ability to use them as catalysts. Subject decomposition can also be a tool for extracting cognitive architecture in biological subjects. As shown in the example of Charlie, explanations, knowledge, and theories that are stored in a person's mind are also useful inner catalysts to discuss when analyzing the subject's understanding. Though these catalysts might be totally imagined by us as they are usually stored in an undifferentiated way, treating them as inner catalysts still helps to clarify the basis of the instance of understanding and make predictions about the improvement and decay of understanding in other circumstances.\nAs an application of subject decomposition, ablation study is a widely adopted strategy for investigating the structure of AI systems . This technique involves systematically disabling or removing individual components of the Al system, such as a structure in the neural network, to evaluate their contribution to the overall performance. We note that it is necessary to find a proper decomposition of the system to disable its components in this process. From the view of subject decomposition, we regard this process as decomposing the system into inner catalysts and testing how each catalyst contributes to the AI system's understanding by observing the change of understanding when altering the catalysts."}, {"title": "Acquisition of understanding", "content": "An important reason we propose subject decomposition is that it allows us to reason about the elements constituting the subject concerning understanding and, therefore, helps analyze the acquisition of the subject's understanding. For example, it makes it possible to ask questions such as \u201chow do changes in inner catalysts cause the improvement of understanding?", "how is the improvement of understanding related to the changes of inner catalysts?\". If the car with Bob appears to drive more safely, the decomposition helps us conjecture that it might be either the safety system, Bob, or the car itself is improved. It also lets us consider how improving Charlie's knowledge can enhance his response when answering questions posed by others.\nHowever, is it always possible to reason about the acquisition of understanding by the notion of catalysts? Our answer is yes. We note that there are only two cases when the understanding of a subject is enhanced when the understood object O is unchanged. The first is when a new catalyst is added to the composition, and the second is when the subject is improved. The first case is trivially related to catalysts. For the second, note that whenever we attribute the acquisition of understanding to the improvement of the subject, we can always attribute to the improvement of a part of the subject. As the improved part improves understanding, we can treat it as a catalyst that is inside the subject. We can also use an example to demonstrate the relationship between the acquisition of understanding and catalysts.\n(Improving understanding) To better understand physics, Alice can use a better catalyst, such as a textbook or search engine, when she composes physics-related inputs. Besides, she must update herself by updating her inner catalysts, such as any knowledge that is related to physics, as well as her physiological conditions, such as blood sugar level, that affect her overall thinking ability.\nWe summarize the above arguments as the following claim.\nClaim 1 (Acquisition of understanding) According to verifier V, the subject must obtain and employ a new catalyst (inner or external) to make a subject's understanding more favourable to V.\nThe above claims show how the acquisition of understanding is related to catalysts. However, it is worth pointing out that the acquisition of understanding is not equivalent to learning, as learning seems to entail the subject improving her understanding at least in part by herself \u2013 learning is a success attributable to a subject, as opposed to mere change which can happen accidentally or by outside pressure. For example, when the search engine provides better results, though Alice might show better understanding by using it, it sounds unnatural that Alice demonstrated her learning ability through the search engine updates. Nonetheless, by updating her knowledge by reading textbooks, Alice demonstrates her ability to make new inner catalysts, and it is intuitive that Alice learned in this process. With the notion of catalysts, we can define the learning ability of a subject as below.\nClaim 2 (Learning ability) A subject's learning ability is determined by its ability to compose related inputs into an update of the subject's inner catalysts.\nWe note that the learning ability of a subject is of special importance. If a subject's learning ability is absent, it must wait for an external process to improve. For example, imagine there is a computer vision model that gives categories to pictures that wrongly recognize a panda as a gibbon. The model can be improved by some external forces, such as fine-tuning by humans. However, the model is far less intelligent than humans as their understanding acquisition depends heavily on other subjects. In contrast, humans can accept inputs (such as \u201cthis is a panda, not a gibbon\", along with an image of a panda) and update their inner catalysts by themselves, allowing them to learn to achieve higher understanding gradually.\"\n    },\n    {\n      \"title\"": "General intelligence and learning"}, {"content": "We have developed theories to characterize a subject's understanding and analyze the structure behind the understanding by catalysts. An immediate question that follows is: \"how could we apply this framework to the current LLM-based systems and measure their distance towards the 'general intelligence' of humans?\" It might be tempting to regard problems, such as hallucinations, failures on counterfactual tasks, and prompt brittleness, as the gap between LLM-based systems and humans. However, it is obvious that humans also suffer from similar problems. It is also common for humans to be overconfident about their knowledge, unable to generalize their ability to new tasks, and be too sensitive to inputs.\nInstead of noticing the obvious gaps between the current AI systems and humans, we compare them at a higher level by their learning ability. Humans have strong learning abilities, which might make learning ability an indispensable part of general intelligence in a human verifier's view. More importantly, no matter how the AI system understands at a point, a high-level learning ability enables the system to improve its understanding to the level that satisfies the verifier. When evaluating whether the AI system has achieved general intelligence, humans usually only expect that the system has acquired a level of understanding that is similar to humans, and humans usually know how this level of understanding can be acquired by human-style learning, such as reading a specific textbook and remembering some key facts. In all the problems we mentioned for LLMs, though humans also suffer from similar problems, we usually do not think they are severe drawbacks. We claim that a crucial reason is that we know how humans can overcome these problems by learning. This point is revealed by the following analysis.\n(Baby intelligence) We usually regard human babies as having general intelligence, though their understanding of many objects is low when born. This is because we know that they have human-level learning abilities and will acquire human-level understanding with their learning abilities (either innate or acquired over development).\nThough current AI systems have demonstrated learning ability to some extent, it is still unclear how human-level artificial learners can be built, and it is beneficial to investigate the major gaps between the learning ability of current AI systems and humans."}, {"title": "Gaps in learning ability", "content": "We have formulated learning ability as the ability to compose inputs into the subject's inner catalysts in Claim 2, which allows us to analyze learning ability by composability. In fact, we can also view learning ability as an instance of understanding and use the same framework to characterize it. This makes salient the universality and scale of learning ability.\nHumans have the ability to handle universal types of inputs for learning. For example, students would build their knowledge during classes through texts, diagrams, and the teacher's voices. In contrast, neural network-based learning only accepts a narrow format of inputs in general. For example, neural networks for image classification usually only take pairs of images and categories as inputs. It is impossible for them to learn from natural language instructions such as \"Pandas are bear-like animals with black and white hair\". A similar problem also happens in the training of LLMs. Though LL"}]}