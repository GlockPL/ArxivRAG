{"title": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation", "authors": ["Quanting Xie", "So Yeon Min", "Tianyi Zhang", "Aarav Bajaj", "Ruslan Salakhutdinov", "Matthew Johnson-Roberson", "Yonatan Bisk"], "abstract": "There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction. To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans excel as generalist embodied agents in part due to our ability to build, abstract, and reason over rich memories. We seamlessly log experiences at appropriate levels of detail and retrieve information ranging from specific facts to holistic impressions, allowing us to respond to diverse requests across different contexts. In contrast, current embodied agents [1]\u2013[4] lack such versatile memory capabilities, limiting their ability to operate effectively in unbounded and complex real-world environments. While existing methods such as semantic mapping [1], [2] and scene graphs [5], [6] attempt to capture spatial and contextual relationships, they largely fall short of the dynamic and flexible memory, retrieval, and generative abilities exhibited by humans.\nIn the language domain, foundation models combined with non-parametric memory mechanisms have achieved near human-level performance across various tasks. Retrieval-Augmented Generation (RAG) [7]-[9] has been widely adopted in the field of Natural Language Processing (NLP) as a non-parametric memory mechanism over large document corpora, enhancing the accuracy and relevance of responses generated by Large Language Models (LLMs). Similarly, the continuous stream of experiences gathered by embodied agents forms vast databases that exceed the context window limitations of LLMs. To address this, approaches like RAG are essential for enabling human-like embodied agents to operate effectively in large, dynamic environments. By integrating non-parametric memory, foundation models within robots can store and retrieve a diverse range of experiences, enhancing their ability to navigate and respond in real-world scenarios.\nHowever, applying RAG to embodied scenarios presents unique challenges due to key differences between textual data and embodied experiences. First, while RAG relies on existing documents, building memory from embodied experiences is itself a core research challenge. Current methods, such as dense point clouds or scene graphs, fail to capture the full range of experiences beyond object-level attributes, without relying on human-engineered schemas or exceeding memory budgets. Second, unlike documents, embodied experiences have inherent correlated structure semantically similar objects are often spatially correlated and hierarchically organized so embodied experiences should not be treated as independent samples. Finally, embodied observations vary in granularity and structure: outdoor scenes might be sparse, while indoor environments are cluttered, and repeated objects across frames can confuse LLMs, complicating retrieval.\nTo bridge this gap, we present Embodied-RAG. Embodied-RAG has two components, Memory Construction (Fig. 2(a)) and Retrieval and Generation (Fig. 2(b c)). During Memory Construction, the system autonomously builds a topological map for low-level navigation and a hierarchical semantic forest without relying on hand-crafted constraints or features. This forest is organized based on spatial correlations between hierarchical nodes, each containing language descriptions of observations, and can be expanded to handle temporal or multi-modal inputs. Root nodes represent global explanations, leaf nodes capture specific object arrangements, and intermediate nodes reflect various mid-level scales. Embodied-RAG allows retrieval at various levels of abstraction in the language query (explicit, implicit, global), matching it with the spatial/semantic resolution (local, intermediate, global) of the memory (Fig. 2(b)/(c), Fig. 3). In the Retrieval and Generation process, to mitigate perceptual hallucinations from semantic similarity searches, Embodied-RAG incorporates a"}, {"title": "II. TASK: EMBODIED-RAG BENCHMARK", "content": "The Embodied-RAG benchmark contains queries from the cross-product of {explicit, implicit, global} questions with potential {navigational action, language} generation outputs. A task consists of:\n\u2022 Query: The content can be explicit (e.g. a particular object instance), implicit (e.g. looking for adequacy, instruction with more pragmatic understanding required), or global. The request might pertain to a location or general vibe.\n\u2022 Experience: The experience is a sequence of egocentric visual perception and odometry, occurring in indoor, outdoor, or mixed environments.\n\u2022 Output: The expected output can be both navigation actions with language descriptions (Fig 4 top, Fig. 2 c-1), or language explanations (Fig 4 bottom, Fig. 2 c-2). Example tasks are shown in Fig. 4, with instances of explicit, implicit, and global queries in Fig. 1. Spatially, the queries range from specific regions small enough to contain certain objects to global regions encompassing the entire scene. Linguistically, global queries are closer to retrieval-augmented generation tasks, while explicit/implicit ones are more retrieval-focused. Explicit and implicit queries are navigational tasks that expect navigation actions and text descriptions of the retrieved location. Global queries are explanation"}, {"title": "III. RELATED WORKS", "content": "a) Nonparametric Methods Outside the Embodied Domain: In the text and multimodal domain, RAG [8], [9], [20] augments LLMs by incorporating a retrieval component from a vector database, enhancing the accuracy and relevance of generated content. However, these methods assume a pre-existing memory and focus on retrieval and generation rather than the construction of the memory. Active agents operating in space require a dynamic approach where memory construction and retrieval/generation are coupled and simultaneous. Furthermore, embodiment introduces the challenge of connecting spatial resolution with language abstraction. GraphRAG [21] addresses the level of abstraction between language queries and document scope. In contrast, our method, Embodied-RAG, extends this resolution problem to the multimodal and spatial domains, simultaneously tackling all of memory construction, retrieval, and generation of language and navigational actions.\nb) Parametric Use of Foundation Models in the Embodied Domain: Current approaches in embodied AI often rely on the parametric use of foundation models to perceive environments and plan [22]\u2013[24]. Systems like PIVOT [25] and NavGPT [26] employ large language models (LLMs) in a Markovian manner, where decisions are made based on the current state without incorporating external memory or past experiences. This reliance on pretrained LLMs or vision-language models (VLMs) can lead to hallucinations, as these models depend on internal knowledge for action generation, often misinterpreting the world state [27]. Furthermore, agents may suffer from extrinsic visual hallucinations, which negatively affect decision-making [28]. In the textual and multimodal domain, it is known that RAG [7]-[9], [29] mitigates hallucinations. In our work, we show that Embodied-RAG reduces hallucinations with the use of nonparametric components. While embodied foundation models such as RT-x models [30] and OpenVLA [31] have been introduced, their nonparametric components have not been explored.\nc) Existing Methods of Semantic Memory and Retrieval: Several methods have been proposed for storing and querying semantic memory in spatial environments, but they remain limited and task-specific compared to the potential of foundation models. Approaches like [2], [32], [33] associate voxels with predefined object categories, enabling fixed vocabulary retrieval, while methods such as [34], [35] map voxels to image embeddings, allowing for open vocabulary queries. Systems like [36] store images per voxel, supporting queries about people, language/image inputs, and object categories. However, a common challenge across these approaches is aligning the semantic abstraction with the spatial resolution. Queries such as \u201ccup,\u201d \u201cred cup,\u201d or \u201cI want to heat my lunch\" are object-level, but methods like [37], [38] focus primarily on local retrieval during exploration, using structured frontiers based on object layouts. Scene graphs [5], [6], while free from dense memory issues, rely on human-engineered schemas (e.g. floor \u2192 room \u2192 object \u2192 asset), making them unsuitable for novel or outdoor environments.\nOther approaches, such as OCTREE maps [39] and their semantic versions [40]\u2013[42], organize occupancy data efficiently but still limit semantics to the object level. Methods like Semantic OCTREE [40], [42] and GENMos [41] use fixed object categories, lacking support for free-form language queries or varying levels of spatial and semantic resolution needed for holistic understanding.\nd) Semantic Navigation and Question Answering: Tasks like ObjectNav [2], [10], [38], ImageNav [3], [11], [43], and Visual Language Navigation [13] assess a robot's ability to navigate towards semantic targets based on object categories, images, or language descriptions. While recent efforts like GOATBench [1] combine multiple input types, these tasks still focus on object-level queries and lack the flexibility to handle broader, more abstract user requests. Embodied Question Answering (EQA) [16], [44]\u2013[46] and Video Question Answering (VideoQA) [47]\u2013[50] extend navigation by requiring text-based answers within actionable or video environments, though EQA is limited to indoor settings and VideoQA lacks active navigation. Our approach expands these paradigms by integrating action-based and question-answering capabilities across a wider range of environments and user queries.\""}, {"title": "IV. METHOD: EMBODIED RETRIEVAL AND GENERATION", "content": "A. Memory Construction\nThe memory construction process of Embodied-RAG consists of two parts: a topological map and a semantic forest.\nTopological map We employ a topological graph composed of nodes with the following attributes:\n\u2022 Position Information: The allocentric coordinates (x, y, z) and the yaw angle 0.\n\u2022 Image Path: Each node contains a path to an associated ego-centric image.\n\u2022 Captions: Generated by a vision-language model, these captions provide object-level natural language textual descriptions of the image.\nThe nodes form a topological map (blue nodes in Fig. 2), eliminating the need for specific control parameters like velocity and yaw, which often vary across different drive systems. This abstraction enables compatibility with any local planner, regardless of the robot's embodiment. Furthermore, the topological structure is far more memory-efficient than traditional metric maps [2], [32], [34], allowing for efficient scaling in both large outdoor and complex indoor environments. Our experiments show that this approach successfully navigates kilometer-scale simulated environments.\nSemantic Forest We use a separate tree structure, referred to as a semantic forest, to capture meaning at various spatial resolutions. The nodes of this forest are those of the topological map, with the non-leaf nodes capturing larger spaces at a thinner density of semantic specificity. First, we create the forest through hierarchical clustering. Since spatially approximate leaf nodes (blue nodes in Fig. 3(a)) exhibit semantic correlations, we employ an agglomerative clustering mechanism [51] to group nodes based on their physical positions assigning the mean position of the leaves. This iterative process continues until a root node is formed, stopping when no further relevance is found based on a threshold set by the algorithm. Once we have a complete forest with one or more root nodes, each non-leaf node receives a language description. We achieve this by prompting a large language model (LLM, e.g., GPT-4) to generate a abstraction that encompasses the descriptions of its direct child nodes (see website for the prompting). This process is conducted bottom-up, starting from the leaf nodes and moving up to the parent nodes. We parallelized this process across all nodes at the same hierarchical level.\nB. Retrieval\nTo address perception hallucinations and improve reasoning capabilities over hierarchies of abstraction constructed for a given environment, we modified RAG's relevancy scoring mechanism from semantic similarity to LLM selections at each level, following a strategy similar to Tree-of-Thoughts [52]. The input to this retrieval process consists of N semantic trees, and the output is the top k chains, which represent node paths from selected leaf nodes to the root (e.g., the concatenation of green, yellow, and red nodes in Fig. 2(c)).\nWe run the following process, which takes a single tree as input and outputs a single leaf node. Starting by visiting the root node, we run BFS with LLM selection; we ask LLM_Selector to choose the best child node of the currently visited node based on compatibility with the given query. For example, if the query is \"find me a place that is bright and quiet but has some presence of people,\u201d we prompt the LLM to select the best description among the children of the currently visited node. We then visit the selected best child node and iterate this process until we reach a leaf node. Once we obtain k leaf nodes (nodes from each tree) by running this process N times for each of the N trees, we obtain the \u201cchain\u201d from the selected node to the root node. The $\\frac{N}{k}$ processes are parallelized across the N trees. The set of these best k chains is the retrieval output, containing semantics at all scales"}, {"title": "C. Generation", "content": "We pass the retrieved k best chains as part of a context, for the LLM to generate navigation and text description (Fig. 4 top) or global explanations (Fig. 4 bottom). Given the query and the k chains, we prompt the LLM to \"select\" a waypoint with a reasoning, or to \"explain\" (prompt in our project website).\nNavigation We select a waypoint (a leaf node of the semantic forest) and use a planner to generate navigational actions-sequences of (torque, velocity) pairs- to reach the waypoint. To select this waypoint, we ask the LLM to choose the best single leaf node, togehter with textual reasoning, using the query and the chain as input. Again, including the entire chain as input ensures that a waypoint can be generated for implicit navigation tasks as well.\nText Answers As depicted in Figure 2 (c), we concatenate the k chains as part of the prompt to the LLM. We ask it to generate an answer to the query based on the k retrieved chains. The spatial scale of attention in each node of the chain facilitate the LLM to generate responses at any semantic scale (explicit, implicit, general) based on the retrieved result."}, {"title": "V. EXPERIMENTS", "content": "Task To assess the efficacy of our approach and ensure statistical robustness, we collected data across 19 diverse environments, including both indoor and outdoor settings. These environments span simulated settings (AirSim [53], Habitat Matterport [54]) and real-world locations, comprising 7 small and 12 large environments. The dataset contains 250 distinct queries, categorized by their nature and complexity.\nEmbodiment For the real-world robotic configuration, we utilized a Unitree Go2, equipped with three Realsense cameras to capture a 180-degree field of view. Positional data was acquired using the Go2's integrated lidar and SLAM algorithms. For simulations, we use the default drone setup with a 210-degree panoramic view for and APIs for drone manipulation and positional data acquisition for AirSim. For Habitat, we use the default locobot setup. To construct the experience, human annotators teleoperated and mapped each environment. However, our methodology is adaptable to any frontier-based exploration with minimal modifications [37], [55], [56], since the problem distills into \"retrieving\" a frontier (potential leaf nodes of the topological map) under Embodied-RAG.\nEvaluation Before evaluation, users familiarized themselves with the collected dataset to understand the environment. The four human annotators who generated the queries cross-evaluated, with each query receiving three evaluations, excluding the one from its author. For navigation output, participants chose binary success or fail, and we calculated the Success Rate (SR) from the average across evaluators and tasks. For text output, participants rated the relevance and correctness of the response on a Likert scale of 1 to 5.\na) Baselines: To establish the comparative performance of our Embodied-RAG approach, we benchmarked it against two baseline methodologies. The first baseline, Semantic Match, follows existing methods by computing cosine similarities between the query and the semantic embeddings of captions from nodes in the topological map, which are also leaf nodes of the semantic forest [35], [57], [58]. The second baseline, we employs an naive RAG [20] in embodied setting, identifying the k = 10 semantically closest nodes in the topological map and using their scene captions to augment a LLM's prompt, enhancing retrieval accuracy. We use the same k = 10 for the retrieval of chains in Embodied-RAG."}, {"title": "A. Quantitative Result", "content": "We first present quantitative results that demonstrate the effectiveness of our approach in Table II. As outlined in Section II, we categorize the Embodied-RAG benchmark queries into three major types: explicit retrieval, implicit retrieval, and global retrieval. Additionally, we classify environments as either small or large based on the number of topological nodes mapped. Our results indicate that Embodied-RAG consistently outperforms RAG and Semantic Match across all tasks and environments. Crucially, all approaches yield expected strong results for explicit queries where a single node is being"}, {"title": "B. Qualitative Result", "content": "We further conduct a qualitative comparison on the reasoning generated by Embodied-RAG and the baseline models before they select a retrieval goal. Embodied-RAG consistently demonstrated superior reasoning, especially in handling implicit requests and global queries. This is likely because relying solely on retrieving semantically similar objects, as the baselines do, is insufficient for addressing queries that require global context and understanding relationships between different parts of the environment (Fig. 4).\nImplicit Query: Find where I can buy some drinks? From the figure, we see that Embodied-RAG correctly identifies a food service area, while the baselines provide incorrect answers. For RAG and direct semantic match, the most relevant results retrieved are those with direct semantic associations, such as a refrigerator or water fountain. However, there is a clear mismatch between the user's intention and the retrieved objects. The goal is to 'buy' water, which typically requires a counter or vending machine for the transaction, rather than simply grabbing it from a refrigerator or drinking from a water fountain. Embodied-RAG performs multi-step reasoning from the top of the tree to the bottom, and retrieves more diverse and plausible locations. It successfully identifies counters as the most appropriate locations for the user's intention.\nFor the second query, the goal is to find somewhere to take a nap outside. RAG and semantic match both focus on green, grassy, shady areas, but without global context, they chose someone's backyard instead of a public facility. Embodied-RAG, on the other hand, utilizes the hierarchical chain and compares Neighborhood \u2192 Grassy area and Park \u2192 Grassy area, then chooses the park area as it is more appropriate.\nGlobal Query: As illustrated in Figure 4, Embodied-RAG demonstrates a comprehensive understanding of the environment by accurately describing it as a suburban neighborhood intertwined with a park. This holistic perception is attributed to Embodied-RAG's hierarchical organization of information, Figure 3, enables it to pseudo-attend to every node in the map. In contrast, RAG retrieves only the most similar nodes, resulting in a fragmented view characterized by redundant items"}, {"title": "C. Computational Efficiency", "content": "Both memory construction and retrieval have a computational complexity of O(log N), where N represents the number of nodes in the environment. This choice allows us to efficiently scale to larger environments, as the time complexity only increases logarithmically with the number of nodes. Additionally, when performing the k retrievals, we execute them in parallel to minimize the overall time cost. In our real-life experiments, the time costs are demonstrated in the supplementary video, which is 8x fast-forwarded. On average, a single retrieval takes around 20 seconds in most of our environments, and the travel time depends on the speed of the specific embodiment in use."}, {"title": "D. Ablation", "content": "We investigate the impact of k \u2208 {1, GPT4 Token Limit} on Embodied-RAG and RAG in Figure 5. A total of 15 experiments were conducted for each k in each environment. We observe that with larger k, both RAG and Embodied-RAG show improved performance, but this improvement plateaus at higher values. RAG still fails to capture the larger holistic resolution with just more object-level nodes and cannot adequately solve the implicit/general queries, further justifying our hierarchy and tree selection approach."}, {"title": "VII. LIMITATIONS AND FUTURE WORK", "content": "We primarily focused on semantic forests rather than a topological map. Therefore, we may not be robust in obstacle avoidance involving dynamic objects and people. Furthermore, Embodied-RAG currently struggles with requests that require precise counting of objects at a small scale (e.g., \"How many chairs are there around the red table?\"). This limitation arises because the agglomerative clustering of the semantic forest does not consider multi-view consistency. Future work could incorporate multi-view consistency in the hierarchies of the semantic forest with a learned or pre-trained mechanism to cluster with positional information (e.g. utilizing a LLM)."}, {"title": "VIII. CONCLUSIONS", "content": "We present Embodied-RAG, a system capable of capturing spatial memory at any spatial and semantic resolution in both indoor and outdoor environments, and retrieving and generating responses for navigation and explanation requests. Additionally, we introduce the task of Embodied-RAG benchmark, unifying semantic navigation and question answering. Our findings demonstrate that Embodied-RAG can robustly handle implicit and global queries, as well as ambiguously phrased requests from human annotators. Our results indicate that Embodied-RAG shows potential as the basis for incorporating large non-parameteric memories into robotics foundation models. We are excited for future extensions to manipulation and dynamic environments that enable robotics tasks out of reach for current memory/context constrained approaches."}]}