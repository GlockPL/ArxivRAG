{"title": "WEARABLE ACCELEROMETER FOUNDATION MODELS\nFOR HEALTH VIA KNOWLEDGE DISTILLATION", "authors": ["Salar Abbaspourazad", "Anshuman Mishra", "Joseph Futoma", "Andrew C. Miller", "Ian Shapiro"], "abstract": "Modern wearable devices can conveniently and continuously record various biosig-\nnals in the many different environments of daily living, ultimately enabling a rich\nview of individual health. However, not all biosignals are the same: high-fidelity\nmeasurements, such as photoplethysmography (PPG), contain more physiolog-\nical information, but require optical sensors with a high power footprint. In a\nresource-constrained setting, such biosignals may be unavailable. Alternatively, a\nlower-fidelity biosignal, such as accelerometry that captures minute cardiovascular\ninformation during low-motion periods, has a significantly smaller power footprint\nand is available in almost any wearable device. Here, we demonstrate that we can\ndistill representational knowledge across biosignals, i.e., from PPG to accelerome-\ntry, using 20 million minutes of unlabeled data, collected from ~172K participants\nin the Apple Heart and Movement Study under informed consent. We first pre-train\nPPG encoders via self-supervised learning, and then distill their representational\nknowledge to accelerometry encoders. We demonstrate strong cross-modal align-\nment on unseen data, e.g., 99.2% top-1 accuracy for retrieving PPG embeddings\nfrom accelerometry embeddings. We show that distilled accelerometry encoders\nhave significantly more informative representations compared to self-supervised or\nsupervised encoders trained directly on accelerometry data, observed by at least\n23%-49% improved performance for predicting heart rate and heart rate variability.\nWe also show that distilled accelerometry encoders are readily predictive of a wide\narray of downstream health targets, i.e., they are generalist foundation models. We\nbelieve accelerometry foundation models for health may unlock new opportunities\nfor developing digital biomarkers from any wearable device, and help individuals\ntrack their health more frequently and conveniently.", "sections": [{"title": "1 INTRODUCTION", "content": "The recent growth of wearable devices has empowered individuals to track their health more con-\nveniently and frequently. Wearable devices can record various biosignals and provide individuals\nwith a continuous view of their health metrics in daily environments, a view which is hard to achieve\nin clinical settings. While this unlocks new exciting opportunities for wearable devices, challenges\nremain. High-fidelity biosignals that have substantial physiological information content tend to\nrequire specific hardware and sensors for recording, therefore making them unavailable in some\nwearable devices. Even when available, they may be difficult to collect frequently due to power\nconstraints of small wearable devices, or they may require active user engagement. An example of\nsuch is photoplethysmography (PPG), which measures volumetric changes in arterial blood flow\nand contains diverse physiological information but requires power-consuming optical sensors for\nrecording. On the other hand, lower-fidelity biosignals, such as accelerometry, are available on\nalmost all wearable devices and has a significantly small power footprint, which makes them ideal for\nenabling digital biomarkers efficiently and on a variety of wearable devices. While accelerometry\nduring gross motion is widely used for activity recognition and fitness, our focus is on accelerometry\nduring low motion. During low-motion periods, accelerometry is capable of detecting minute phys-\niological movements on the body as a result of heart function, known as ballistocardiogram"}, {"title": "2 RELATED WORK", "content": "Uni-modal and multi-modal representation learning: Unsupervised representation learning tech-\nniques, e.g., self-supervised learning, have been proven successful in training generalist models,\nalso known as foundation models, without requiring any explicit labels during training in various\ndomains of deep learning such as natural language processing, computer vision, speech recognition\nand health. While most of these works\nhave been primarily on training uni-modal foundation models, there has been a recent shift in training\nmulti-modal foundation models to allow for leveraging information from multiple modalities, either\nto train a model that simultaneously processes multiple modalities, or to train and bind multiple modality-specific foundation models, particularly with a contrastive objective. Similarly for health applications,\nthere has been a growing interest in cross-modal reconstruction of biosignals, or simultaneously pre-training multiple biosignal modalities or wearable metrics.\nKnowledge distillation: Knowledge distillation has been extensively used for transferring knowledge\nfrom a neural network (teacher) to another neural network (student) in other domains, traditionally often in supervised settings to transfer knowledge from a large\nneural network to a small neural network or from a high-fidelity\nmodality to a low-fidelity modality, or from\nan ensemble network into a single one, by using the teacher's output logits as\nsoft labels of the student model. Alternatively, knowledge distillation can also be performed using\nintermediate representations. With the recent emergence of foundation models, there has been several\nworks for combining self-supervised learning and knowledge distillation via self-distillation, or distilling one or several existing foundation\nmodels to a single foundation model to improve and agglomerate their representations."}, {"title": "3 METHODS AND IMPLEMENTATION DETAILS", "content": "Our representational knowledge distillation framework is fully unsupervised and consists of two\nsteps: teacher pre-training and cross-modal representational knowledge distillation as depicted in\nFigure 1. Below, we go over the details of each of these steps as well as their implementation details."}, {"title": "3.1 TEACHER PRE-TRAINING", "content": "The first step of our representational knowledge distillation framework is to pre-train the PPG teacher\nencoder via self-supervised learning without any labels. To show that our knowledge distillation\nframework is agnostic to the pre-training of the PPG teacher encoder and study its efficacy, we\ninvestigated two popular pre-training strategies: masked autoencoding (MAE) as the main framework\nand contrastive learning (CL) as an ablation, which we explain in detail below.\nMasked autoencoding: Our masked autoencoding pre-training framework is adopted from the prior\nwork on images , for time-series. We turn the multi-channel PPG input (4-channels\nand 60s-long, as explained in Section 4.1) into patches using non-overlapping fixed-length windows,\nand then project the patches with a learnable linear tokenizer into tokens, which results in 192 256-D\ntokens. Sinusoidal positional embeddings are added to the input tokens, then 80% of input tokens\nare randomly dropped and the 38 kept tokens are passed through the encoder Transformer to get\nencoder output tokens. Learnable mask tokens, initialized with a 256-D token drawn from a uniform\ndistribution ~ U(0, 1), are added back to the encoder output tokens at positions where input tokens\nwhere dropped, followed by adding sinusoidal positional embeddings. These 192 new tokens are\nthen processed by the decoder Transformer to generate the decoder output tokens. Finally, with a\nlinear projection, the decoder output tokens are projected to the multi-channel PPG output, with the\nobjective of reconstructing PPG \u201cpixel\u201d values in those indices whose patches/tokens were dropped\nin the encoder's input. For maximum learning rate, we used 2e-4 and batch size was set to 512.\nA complete list of other architectural and training hyperparameters that are shared across methods\nusing Transformers can be found in Appendix Table 4. As a baseline for our distilled accelerometry\nencoders, we also train masked autoencoders for accelerometry in the same way explained above.\nContrastive learning: While our main pre-training framework for the teacher encoder is masked\nautoencoding, we perform ablation in regards to the teacher pre-training method and architecture via\ncontrastive learning with EfficientNets. We would like to emphasize that contrastive learning can also\nbe done with Transformer models without any meaningful difference (Appendix Table 13), but we\nchoose EfficientNets for the main results to simultaneously demonstrate an ablation on changing the\nteacher pre-training strategy and student/teacher architecture. Our contrastive pre-training framework\nclosely follows a prior work for PPG signals , with the major difference\nthat the positive pairs are selected as two augmented views of the same sample. This choice is\nmade to enforce the encoders to contain more segment-level information necessary for the main\ndownstream targets used in this study, as opposed to participant-level positive pair selection which\nwas investigated in the prior work (Appendix Table 14). During pre-training, we augment each\nsample twice with our stochastic augmentation module that consists of a stochastic cascade of several\nstandard individual augmentations including {cut out: 0.4, magnitude warp:\n0.25, add Gaussian noise: 0.25, channel permute: 0.25, time warp: 0.15}, where the values are\nthe assigned probabilities of whether the augmentation is applied for each segment or not. The\ntwo augmented views are then passed through a joint-embedding architecture, where one encoder\nis an exponential moving average (m = 0.99 for momentum updates) of the other encoder that is\nupdated via backpropagation. The 256-D embeddings of each encoder are then projected to a lower\ndimensional subspace (128-D) via multi-layer perceptron projection heads. The training objective\nis maximizing the mutual information of the down-projected embeddings of the two views of the\nsame PPG segment, and minimizing that for different PPG segments, implemented by a regularized\nInfoNCE loss. We use Kozachenko-Leonenko (KoLeo) differential entropy estimator with the weight of 0.1 for regularization, and temperature of 0.04 for\nscaling similarity scores in the InfoNCE loss. For the maximum learning rate, we used le-3, while\nbatch size was set to 256. Other common hyperparameters for training EfficientNets is available in\nAppendix Table 5. As a baseline for our distilled accelerometry encoders, we also train contrastive\nlearned encoders for accelerometry in the same way explained above."}, {"title": "3.2 CROSS-MODAL REPRESENTATIONAL KNOWLEDGE DISTILLATION", "content": "After pre-training the PPG encoder in Section 3.1, we distill its representational knowledge to an\naccelerometry encoder in a dataset of paired PPG-accelerometry segments (Section 4.1). This second\nstage is also fully unsupervised without requiring any explicit labels. We perform representational\nknowledge distillation via multi-modal contrastive learning similar to a technique used previously\nto weakly supervise an image encoder with text (CLIP) , but here we use\nit to transfer knowledge. To do this, unlike standard approaches , we first\nperform augmentations on both modalities, PPG and accelerometry, using our stochastic augmentation\n(see Section 3.1), where the augmentations are independently drawn for each modality in a PPG-\naccelerometry pair. We found that augmentations were crucial for the quality of the embeddings (see\nAblation 5.5). The augmented PPG and accelerometry signals are then processed by the PPG teacher\nencoder (frozen) and accelerometry student encoder, respectively, to get 256-D output embeddings.\nTo calculate the objective, we first down-project the embeddings to 128-D with trainable multi-layer\nperceptron projection heads (one 1024-D hidden layer) for both the student and teacher encoders. We\nfound separate learnable projection heads to a smaller subspace was necessary to avoid representation\ncollapse. The student encoder is trained to generate embeddings similar to the teacher encoder, where\nthe objective is contrastive and maximizes the mutual information of paired PPG and accelerometry\nembeddings, while minimizing the mutual information of an accelerometry embedding with other\nPPG embeddings. For each batch of embeddings h from N positive pairs for student and teacher (ht,\nhs), we define multi-modal InfoNCE, where the teacher embeddings are selected as anchors:\n\nL^{(ts)}_{contrastive} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(sim(h_s^i, h_t^i) / \\tau)}{\\sum_{j=1}^N \\exp(sim(h_s^i, h_t^j) / \\tau)}   \\qquad (1)\n\nHere, sim(\u00b7, \u00b7) is the cosine similarity function and L^{(ts)}_{contrastive} can be viewed as a N-way classifi-\ncation problem (N = batch size), such that h_t^i is the correct pair to h_i^s compared to all other potential\npairs in the batch {h_t^j|1 \u2264 j \u2264 N, j \u2260 i}. The final objective is computed as the weighted sum of\nInfoNCE, from teacher to student and from student to teacher:\n\nL = \\lambda L_{contrastive}^{(ts)} + (1-\\lambda) L_{contrastive}^{(st)},  \\qquad (2)\n\nwhere X is the scalar weight between 0 and 1. In our experiments, the temperature parameter is\nset to 0.04, and unless otherwise specified we set X to 1, to emphasize more on alignment when\nPPG embeddings are anchors (see Ablation on X in Section 5.5). For the maximum learning rate,\nwe used le-3, while batch size was set to 256. As mentioned above, our main encoders are with\nTransformers whose common hyperparameters are in Appendix Table 4, and we perform ablations\nwith EfficientNets whose common hyperparameters are in Appendix Table 5."}, {"title": "4 EXPERIMENTS", "content": "4.1 DATASETS\nWe used the PPG and accelerometry signals recorded on Apple Watch from participants in the Apple\nHeart and Movement Study (AHMS). AHMS is an ongoing\nresearch study designed to explore the links between physical activity and cardiovascular health,\nwhich is sponsored by Apple and conducted in partnership with the American Heart Association and\nBrigham and Women's Hospital. To be eligible for the Study, participants must be of legal age to\nprovide informed consent (18, 19, or 21 based on location), reside in the United States, have access\nto an iPhone with the Research app, have an Apple Watch, and provide informed consent within the\nResearch app to participate .\nWe created a paired PPG-accelerometry pre-training dataset that we used for pre-training uni-modal\nand distilled models. Apple Watch intermittently and passively records simultaneous green PPG and\naccelerometry signals during low-motion periods multiple times per day, to reliably make predictions\nabout an individual's health. PPG and accelerometry signals are recorded simultaneously at a 256Hz\nor 64Hz sampling rate and are 60 seconds in duration. PPG signals consist of four optical channels"}, {"title": "4.2 EVALUATION METRICS", "content": "PPG-accelerometry pair embeddings retrieval: To assess the quality of the PPG-distilled ac-\ncelerometry embeddings (256-D representations after the encoder), we perform a retrieval experiment\nto see how well the accelerometry embeddings can retrieve their corresponding matched PPG seg-\nment on unseen test data. For a batch of paired PPG and accelerometry segments on held-out test\nparticipants, we compute the cosine similarity between each distilled accelerometry embedding and\nPPG embedding, which generates a ranked list that can be used for retrieval. We evaluate retrieval\nquality using top-K accuracy for K = 1, 3, 5 (i.e., for a given query accelerometry embedding, how\noften is its paired PPG embedding in the top K most similar embeddings). We also report the mean\nrank, i.e., the mean position of the true paired PPG segment in the rankings; smaller values are better,\nwith 1 indicating perfect retrieval as the correct segment is always ranked first. As an ablation, we\nrepeated the same retrieval analysis from PPG to accelerometry embeddings.\nLinear probing for downstream targets: As our main targets, we perform linear probing for\npredicting heart rate (HR), and two popular measures of heart rate variability: standard deviation\nof normal-to-normal intervals (SDNN) and root mean square of successive differences (RMSSD)\n. In addition, we perform linear probing for\npredicting self-reported age, body mass index (BMI), biological sex, and 46 health targets including\nhealth conditions, medication use, and lifestyle habits, for participants in AHMS. Detailed explanation\nof our linear probing evaluation metrics is explained in Appendix A.2."}, {"title": "5 RESULTS", "content": "5.1 REPRESENTATIONAL KNOWLEDGE DISTILLATION UNLOCKS STRONG\nACCELEROMETRY-PPG EMBEDDING ALIGNMENT\nWe first visually inspected the embeddings of 200 random PPG-accelerometry pairs (20 from 10\nparticipants) from our test split, embedded via PPG teacher encoder trained with MAE (\"PPG-MAE\")\nand 2 accelerometry encoders: 1) accelerometry encoder trained with MAE (\u201cAccel-MAE\u201d), 2)\ndistilled accelerometry encoder (\u201cAccel-KD via PPG-MAE\u201d) from \u201cPPG-MAE\u201d, by projecting them\ninto 2D t-sne representation subspace as shown in Appendix Figure 4.\nWe observed a marked difference in the alignment of PPG-accelerometry embeddings for PPG\nteacher encoder and distilled accelerometry encoder (Appendix Figure 4-right) compared to the"}, {"title": "5.2 REPRESENTATIONAL KNOWLEDGE DISTILLATION FROM PPG IMPROVES THE QUALITY OF\nACCELEROMETRY EMBEDDINGS", "content": "To gain additional insight about the quality of accelerometry embeddings after knowledge distillation,\nwe compared several encoders in terms of their downstream performance: 1) \u201cAccel-MAE\u201d, 2)\n\"Accel-KD via PPG-MAE\", and 3) supervised encoder trained directly on each target from scratch\n(\"Accel-supervised\"). In the meantime, to study the label efficiency of these models, we sweep\nthe proportion of available training segments/labels for linear probing and supervised training from\n0.1% to 100% while keeping the number of test segments/labels the same. Figure 2a represents the"}, {"title": "5.3 DISTILLED ACCELEROMETRY ENCODERS ARE SIGNIFICANTLY MORE PREDICTIVE OF\nDEMOGRAPHIC VARIABLES AND HEALTH TARGETS", "content": "We next questioned whether the distilled accelerometry encoders are predictive of other health-related\ntargets that require capturing waveform information as opposed to pulse timing information that may\nbe sufficient for heart rate and heart rate variability. To this end, we evaluated their downstream\nprediction performance of a wide array of targets including age, biological sex, BMI and 46 binary\nhealth targets derived from AHMS self-reported questionnaires (see Appendix Section A.3). We\nobserved that distilled accelerometry encoders are better predictive of the demographic variables\n(Table 2) and health targets (Appendix Table 12), compared to the baseline uni-modal accelerometry\nencoders. This indicates the generalizability of the distilled accelerometry encoders to a wide range of\ntasks and being a foundation model. To the best of our knowledge, this is the first work demonstrating\nthat a single accelerometry encoder is predictive of demographic variables with remarkably high\naccuracy (Table 2), and is readily predictive of variety of health targets (Appendix Table 12)."}, {"title": "5.4 CROSS-MODAL REPRESENTATIONAL KNOWLEDGE DISTILLATION CAN ALSO ENABLE\nMODEL COMPRESSION", "content": "One important aspect of models for wearable devices is that they should be compact in size for\non-device inference with a minimal power footprint. This allows more frequent estimation of digital\nbiomarkers throughout the day, and makes them available from resource-constrained wearable devices.\nTherefore, we questioned whether we can perform model compression during the representational\nknowledge distillation from PPG to accelerometry. To show this, we reduced the size of accelerometry\nstudent encoder to 4 new smaller sizes by shrinking the depth and width of the Transformer backbone\nduring the distillation (Figure 3, and see Table 7 for model sizes). We observed that our distillation\nframework robustly maintains the information quantified by downstream performance, even in"}, {"title": "5.5 ADDITIONAL ABLATION STUDIES", "content": "Unless otherwise specified, we performed the following ablation studies with \u201cAccel-KD via PPG-\nMAE\" as it is our main method (Figure 1), without loss of generalization given similar qualitative\nconclusions to \u201cAccel-KD via PPG-CL\" (Section 5.2 and Appendix Table 13). Additionally, we also\nperformed several other ablations presented in Appendix A.3.\nAugmentations: During our cross-modal knowledge distillation, both pairs of signals, PPG and\naccelerometry, are augmented with our stochastic cascade augmentation module (Section 3.2). While\nprior works have investigated augmentations for uni-modal contrastive learning of biosignals, the\neffect of augmentations for multi-modal knowledge distillation of biosignals remains unknown.\nTherefore, we investigated the importance of augmentations and observed that they were critical for\nour cross-modal knowledge distillation as shown in Appendix Table 17. For instance, we observed\n45% higher mean absolute error for heart rate prediction when the augmentations were absent during\nthe distillation stage. In addition, we investigated the importance of individual augmentation functions\nduring knowledge distillation (Appendix A.3 and Appendix Table 17).\nMulti-modal pre-training of both PPG and accelerometry encoders simultaneously results in\nsignificantly reduced performance: As discussed in Section 2, there are prior works for multi-\nmodal pre-training of biosignals where they bind different modality embeddings via multi-modal\ncontrastive learning , which does not involve pre-training\na uni-modal teacher encoder (stage 1 in Figure 1). While, our motivation here is different for we\nuse multi-modal contrastive learning to enable unsupervised representational knowledge distillation,\nwe questioned whether we get the same improvement for the accelerometry encoders when binding\nPPG and accelerometry embeddings in a multi-modal pre-training setup. Therefore, we did an\nablation of multi-modal contrastive learning where both PPG/accelerometry encoders are trainable\nduring training, and to do a fair comparison, we experimented with different A values. For all\ndifferent A values, we observed that the learned accelerometry encoder had significantly lower quality\nembeddings (see Table 3) as quantified by a significant drop of performance in all downstream targets\n(95%, 47% and 35% higher mean absolute error for heart rate, SDNN, and RMSSD, respectively).\nIn addition, to emphasize the importance of freezing the teacher encoder in knowledge distillation,\nwe did the multi-modal training where we initialized the PPG encoder using the pre-trained weights\nof \"PPG-MAE\" encoder, and still observed significant degrade in performance (Appendix Table\n19). We believe that this is due to the asymmetric amount of information present in the PPG and\naccelerometry, such that allowing the PPG encoder to update results in more trivial embeddings and\ndegraded performance for the accelerometry encoder. Overall, this observation further demonstrates\nthe importance of two-stage representational knowledge distillation and freezing the teacher encoder\nto allow maximal knowledge transfer."}, {"title": "6 DISCUSSION, LIMITATIONS AND FUTURE WORK", "content": "Here, we demonstrated that a single accelerometry encoder can predict heart rate, heart rate variability,\ndemographic variables and a wide array of downstream health targets, i.e., as a foundation model.\nThis was done with a fully unsupervised representational knowledge distillation framework to transfer\nknowledge from high-fidelity to low-fidelity biosignals. We showed that our knowledge distillation\nframework is not unique to the teacher/student architecture, or teacher pre-training method, and it\nresults in near-perfect alignment of PPG and accelerometry embeddings as quantified by our retrieval\nanalysis. In addition, it improves the quality of the representations in the low-fidelity biosignal com-\npared to self-supervised and supervised baselines, while maintaining label efficiency for downstream\ntargets. We also showed that cross-modal distillation can simultaneously enable compressing the\nstudent model while transferring knowledge across modalities. We also performed ablations with\nrespect to several training choices. Our work primarily focuses on accelerometry signals during\nlow-motion and sedentary periods (Section 4.1), where accelerometer captures ballistocardiogram and\ntherefore minute cardiovascular-related information useful for health targets . Future work can investigate models that take slower-scale activity metrics on wearable\ndevices such as steps, speed, sleep, and slow changes in accelerometry, as well as minute changes in\naccelerometry at sedentary settings to improve the performance for downstream targets.\nIn addition, another interesting area of investigation\nfor future work could be experimenting with modality specific augmentations .\nOne caveat of our work is that it currently supports two modalities, while\nfuture work can consider statistical objectives for mutual information maximization, when there are\nmore than one teacher or student modalities, or by binding all student modalities\nto a single teacher modality . Another caveat is that while this work can be\nused for knowledge transfer or retrieval of the high-fidelity modality embeddings, it does not provide\na generative model across modalities ; future work can consider recent\ntechniques to incorporate generative capabilities using unified encoder and decoder Transformers.\nIdeally, future work can also consider modeling other modalities\nsuch as text, images and videos to leverage information from these other input sources and weakly\nsupervise biosignal representations, similar to prior work for modeling accelerometry during motion\nand other modalities such as video and text"}, {"title": "A APPENDIX", "content": "A.1 IMPLEMENTATION DETAILS\nThe common architectural and training hyperparameters for training Transformer and EfficientNet\nmodels, can be found in Tables 4 and 5, respectively. The changes in Transformer model architecture\nfor ablation on compressing the Transformer model size (Section 5.4) can be found in Table 7.\nA.2 DATASET AND EVALUATIONS\nDataset statistics: Brief statistics for our curated PPG-accelerometry dataset from AHMS is available\nin Table 6.\nLinear probing for heart rate and heart rate variability: We perform linear probing for predicting\nheart rate (HR), and two popular measures of heart rate variability: standard deviation of normal-to-\nnormal intervals (SDNN) and root mean square of successive differences (RMSSD). These targets\nare from Apple Watch's generated values during low-motion periods where PPG peaks are reliably\ndetected, resulting in accurate prediction of heart rate and heart rate variability. These targets are\nchosen because they are widely used in wearable devices and are indicative\nof health status , training load in athletes and stress\nlevels . Being able to predict them via low-fidelity biosignals will enable a more\nfrequent prediction of such targets, giving the users a broader view of their health-related changes in\ndifferent scenarios of their daily life. We formulate this problem as a regression task, where we use\nridge regression to predict the continuous value of these targets and we use mean absolute error to\nquantify performance. For heart rate, we report error in beats per minute (BPM), and for SDNN and\nRMSSD, we report error in milliseconds (ms). For these targets given that they change from segment\nto segment, we perform the linear probing at segment granularity: each segment contributes one and\nonly one sample in the downstream training/evaluation. However, the downstream training/evaluation\nsplits are stratified based on participants such that the evaluation split's participants have no overlap\nwith those in the training split of the linear probing or pre-training.\nLinear probing for age, BMI, biological sex, and health targets from AHMS survey questions:\nWe perform linear probing for predicting self-reported age, body mass index (BMI), biological sex\n(sex assigned at birth), and health targets from survey questions for participants in AHMS. During\nAHMS, participants fill out multiple questionnaires containing various questions regarding their\nhistorical health record and demographics . The\nresponse to these questions are usually in form of 'yes' or 'no' for whether the participant has had\na history of a health condition (e.g., asthma), or whether they take specific medications (e.g., anti-\ndepressants), or regarding their lifestyle habits (e.g., smoking). For the classification tasks, we use\nridge regression to predict scores for binarized targets (0/1) and we quantify the performance with area\nunder curve of receiver's operating curve (AUC). For biological sex, we classify male versus female,\nand for health targets we classify 'yes' versus 'no'. For regression tasks (age and BMI), we use ridge\nregression to predict continuous targets and we use mean absolute error to quantify performance.\nAge is reported in years and BMI is reported in kg/m\u00b2. In all these subject-related targets that do\nnot vary from segment to segment, we perform the linear probing at participant granularity: we\nmean-aggregate all the embeddings associated to each participant so that each participant contributes\none and only one sample in the downstream training/evaluation. Similar to the heart rate and heart\nrate variability linear probing, the downstream training/evaluation splits for these targets are stratified\nbased on participants such that the evaluation split's participants have no overlap with those in the\ntraining split of the linear probing or pre-training.\nAHMS survey is formed of multiple questionnaires which participants fill out over the course of their\nparticipation in the study. Tables 20 and 21 contain AHMS survey questions about medical conditions\nand medications, respectively, in addition to the corresponding target labels used in Appendix Table\n12. Table 22 includes AHMS survey questions about drinking and smoking habits, and Table 23\ndefines our logic to summarize these questions into binary labels for the related targets used in\nAppendix Table 12."}, {"title": "A.3 RESULTS AND ABLATION STUDIES", "content": "Visual inspection of the T-SNE representations: The visual inspection of the T-SNE embeddings\nfor the random, uni-modal and distilled accelerometry encoder as well as the PPG teacher encoder is\nshown in Figure 4 as explained in Section 5.1.\nRetrieval analysis: Retrieval analysis for accelerometry embeddings from PPG embeddings is shown\nin Table 8.\nExtended numbers for linear probing of heart rate, SDNN and RMSSD: Linear probing and\nsupervised evaluation performance numbers at 0.1% and 100% data availability in Figure 2, as well\nas root mean squared error and Pearson's R metrics, can be found in Appendix Tables 9, 10 and 11.\nAblation on choice of positive pairs in teacher pre-training with contrastive learning: We\nselected the positive pairs as two augmented views of the same sample, as opposed to two different\nsegments of the same participants proposed in . This choice was made to\nenforce the encoders to contain more segment-level information necessary for the main downstream\ntargets used in this study (heart rate and heart rate variability). Appendix Table 14 demonstrates the\nperformance of heart rate, SDNN and RMSSD for the \u201cPPG-CL\u201d trained with two different positive\npair selection strategies.\nAblation on number of PPG channels in teacher pre-training: We performed an ablation about\nthe effect of the number of PPG channels in teacher pre-training on downstream evaluations in Table\n15, where we only kept one of the PPG channels for modeling and observed a drop in performance.\nAblation on larger model sizes for teacher pre-training: We made several optimizations to keep\nour model sizes small for feasibility on running wearable devices with power and resource constraints.\nInterestingly, we observed signs of overfitting as we increased our encoder size, which is why our\nencoder sizes are not larger. This could be due to the fact that one needs to scale the model and data\nsize simultaneously to gain benefits of scaling laws . As an\nexample, when we increased the encoder size in \u201cPPG-MAE\" (from 6.3M to 12.7M) by increasing\nthe number of layers from 8 to 16, we observed initial signs of overfitting as shown in Table 16. We\nbelieve future work can investigate the scaling laws for encoder models of wearable biosignals by\ngrowing the encoder and data size simultaneously .\nAblation on augmentations: In addition to comparing knowledge distillation with and without aug-\nmentations (Section 5.5), we studied the importance of individual augmentations during knowledge\ndistillation. To this end, we performed the knowledge distillation from PPG to accelerometry, where\nwe only kept one of the augmentation functions during distillation (applied in every forward pass),\none at a time. This was done while maintaining all other training choices the same to control for\nthe effect of augmentations. We observed that: 1) our stochastic augmentation module achieved the\nhighest accuracy (Appendix Table 17), likely because it captures more diverse range of distortions\nduring training, 2) among the individual augmentations, \"add Gaussian noise\" and \"Cut out\u201d had the\nhighest importance, while \"Time warp\u201d had the least importance.\nIn general, our hypothesis for why augmentations are important for knowledge distillation across\nPPG and accelerometry is that given the relationship between arterial blood flow present in PPG and\nballistocardiogram in accelerometry , particularly for aligned\nPPG-accelerometry segments during low-motion periods which is the focus of our work, knowl-\nedge distillation without augmentations is a relatively easier pre-training task compared to that with\naugmentations. Therefore, we think distillation without augmentations, and even very simple augmen-\ntations as shown by individual augmentations results in Appendix Table 17, leads to capturing less\nminute information, which is relatively similar to why and how the amount and type of augmentations\nin uni-modal contrastive learning is critical as demonstrated in prior work .\nAblation on impact of \u5165 in the multi-modal contrastive objective \u5165: We studied the impact\nof A in Equation 2 for our representational knowledge distillation. We observed that while the\nimprovements of accelerometry embeddings via distillation were robust to \u5165, higher values of \u5165\n(X = {0.75, 1}) were the most optimal (Appendix Table 18), indicating that keeping PPG embeddings\nas anchor embeddings provided the most knowledge transfer, perhaps due to the fact that PPG is the\nhigher-fidelity modality."}, {"title": "A.4 DISCUSSION", "content": "Discussion for why in uni-modal pre-training with contrastive learning was better for accelerom-\netry, and with masked autoencoding was better for PPG: Our hypothesis for this is that given that\naccelerometry is noisier than PPG (see Figure 1 for examples), reconstructing accelerometry in the\noutput space via MAE is a rather difficult pre-training method that bottlenecks the quality of extracted\nrepresentations, as opposed to that for a signal such as PPG which is more structured and less noisier\nto reconstruct. Therefore, we think MAE pre-training may be more suitable for less noisy biosignals,\nand CL pre-training is more suitable for noisier biosignals. This, in fact, is a major motivation and\ndifference for pre-training strategies that reconstruct in the representation space versus output space\n. All in all, for both pre-training frameworks and Transformer/EfficientNet\narchitectures, our representational knowledge distillation framework robustly distills the information\nfrom PPG to accelerometry, and improves the information in the accelerometry embeddings."}]}