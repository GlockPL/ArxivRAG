{"title": "Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder", "authors": ["Xianjun Yang", "Shaoliang Nie", "Lijuan Liu", "Suchin Gururangan", "Ujjwal Karn", "Rui Hou", "Madian Khabsa", "Yuning Mao"], "abstract": "Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset data selection important but underexplored. On the other hand, existing quality-driven data selection methods such as LIMA (NeurIPS 2023 (Zhou et al., 2024)) and AlpaGasus (ICLR 2024 (Chen et al.)) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders to tackle the challenge of data diversity measure. In addition, sparse autoencoders can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 (Zhao et al.)). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors.", "sections": [{"title": "1 Introduction", "content": "Scaling large language models (LLMs) has been shown to significantly enhance performance (Kaplan et al., 2020; Achiam et al., 2023), augmented by alignment (Bai et al., 2022; Rafailov et al., 2024; Schulman et al., 2017) to make LLMs follow human instructions. Instruction fine-tuning (IFT) (Wei et al.; Longpre et al., 2023; Sanh et al.; Ouyang et al., 2022) has become an essential step in adapting LLMs to perform effectively across diverse tasks, and it is believed that the data quality and diversity are the most important factors during IFT. Recently, there is a growing interest on data-centric AI. For example, innovations in data engineering enable scaling to vast contexts, extending the model's capacity to handle extensive data sequences (Fu et al.). Alpaca (Taori et al., 2023b) can elicit Llama's instruction-following by distilling 52k instruction conversations from ChatGPT (OpenAI, 2023). Techniques like importance resampling for data selection have been developed to optimize training data relevance (Xie et al., 2023).\nAlthough it is well acknowledged that the critical to effective instruction tuning is the training data's quantity (Ding et al., 2023), quality (Chen et al.), diversity (Bukharin and Zhao, 2023), and complexity (Wang et al., 2023), as also highlighted by recent industrial technical reports, such as Llama-3 (Dubey et al., 2024) and QWen-2 (Yang et al., 2024), it is still a mystery of how to accurately measure those features in data. Generally, ensuring data quantity and quality are easier to achieve with either human feedback (Ouyang et al., 2022) or automated AI feedback (Lee et al.). However, evaluating data diversity and complexity are challenging. For instance, a recent call (Zhao et al., 2024) urge a quantifiable approach to dataset diversity to ensure that claims of diversity are substantiated and meaningful: \"Measure Dataset Diversity, Don't Just Claim It\".\nPrevious work has proved the effectiveness of small data quantity for achieving either general instruction following (Chen et al., 2023; Zhou et al., 2024), or task-oriented abilities (Xia et al.). Zhang et al. (2024a) shows some preliminarily results to claim that instruction diversity drives generalization to unseen scenarios through symbolic tasks. And Liu et al. perform controlled studies to measure data across three dimensions: complexity, quality, and diversity (measured by cosine similarity of sentence representation). Besides, data pruning based on concept complexity has been shown effective in both pre-training (Abbas et al.) and post-training (Lu et al.). However, many previous text diversity metrics (Shaib et al., 2024) are not adopted due to their intrinsic limitations. Therefore, there is still a lack of reliable diversity measure toward more efficient, effective, and targeted data selection in the instruction tuning landscape. And it is also"}, {"title": "2 Related Work", "content": "Sparse Autoencoders have been crucial in advancing our understanding of neural representations (Gao et al., 2024; Paulo et al., 2024; Braun et al., 2024), as well as in scaling and enhancing the interpretability of LLMs (Cunningham et al., 2023; Foote et al., 2023). The foundational work in sparse coding, also known as dictionary learning, dates back to an overcomplete basis set proposed around 30 years ago (Olshausen and Field, 1997). Building on this, K-SVD (Aharon et al., 2006) was developed as an algorithm for designing overcomplete dictionaries, facilitating sparse representations in signal processing. Furthermore, K-sparse autoencoders (Makhzani and Frey, 2013) introduced a K-sparse constraint to enforce sparsity, leading to more accurate data representations. Bau et al. (2020) highlight the importance of understanding unit-specific functions for model interpretability. Additionally, Tonolini et al. (2020) propose that the visual cortex may employ such strategies to efficiently represent natural images. Recently, sparse dictionary learning has been applied to visualize transformer models (Elhage et al., 2022; Henighan et al., 2023), revealing that contextualized embeddings can be expressed as linear combinations of transformer factors (Yun et al., 2021). Research has also investigated polysemanticity and capacity in neural networks, finding that neurons often encode multiple concepts, which poses challenges for interpretability (Scherlis et al., 2022; Lieberum et al., 2024). Moreover, engineering monosemanticity has been explored to design neurons that represent single concepts, thereby enhancing interpretability (Jermyn et al., 2022). A structured mathematical framework has been proposed for analyzing transformer circuits (Elhage et al., 2021). It has been demonstrated that LLMs can elucidate the functions of neurons within themselves, offering a novel perspective on model introspection (Bills et al., 2023). Sparse probing techniques have identified significant neurons, providing case studies that underscore the utility of sparsity in interpretability (Gurnee et al.). Improvements in dictionary learning with Gated-SAE (Rajamanoharan et al., 2024a) and JumpReLU-SAE (Rajamanoharan et al., 2024b) have further enhanced the quality of learned representations.\nData Selection (Albalak et al., 2024; Wang et al., 2024), the process of selecting a representative subset of data to achieve efficient training without compromising model performance, is important in both pre-training (Brandfonbrener et al., 2024; Tirumala et al., 2023) and post-training (Chen et al.; Li et al., 2024b). Previous studies (Mindermann et al., 2022; Paul et al., 2021) have focused on optimizing this selection for various training objectives, specifically targeting model performance constraints (Xia et al., 2024), the value of individual data points (Covert et al.), and addressing bias (Jain et al., 2024). Recently, the emphasis on data curation (Taori et al., 2023b; Chiang et al., 2023; Cui et al., 2024; Wang et al., 2023) and selection (Zhou et al., 2024) for LLMs suggests that the main capabilities of LLMs come from pre-training, and a small amount of well-crafted instruction data can enable excellent instruction following. As a result, various data selection methods have been proposed. For example, AlpaGasus (Chen et al.) uses ChatGPT to score data quality and selects only the top 1000 highest-scoring data points. Zhao et al. propose a simple yet effective baseline of selecting the longest responses, while Xia et al.; Zhang et al. (2024b); Pan et al. (2024) employ gradient-based clustering for task-agnostic coreset selection. Further research (Liu et al.) indicates that data quality (Li et al., 2024c; Ding et al., 2023; Li et al., 2024a, 2023), diversity (Ge et al., 2024), and complexity (Xu et al., 2023; Sun et al., 2024; Ivison et al., 2023) are all crucial to the success of IFT, especially under complex constraints. However, accurately measuring these dimensions is a nontrivial task. In this work, we focus on data diversity. Data diversity in instruction-tuning is increasingly recognized as crucial for building robust models (Bukharin and Zhao, 2023). #InsTag (Lu et al.) measures"}, {"title": "3 Sparse Autoencoder", "content": "Formulation. Let $x_i$ denotes the d-dimension residual stream of the jth-layer of the transformer layer before the final layer of logits (we discard j hereafter and only use x for simplicity). A typical SAE can be decomposed into two parts: the encoder and decoder, where the encoder is used for representation decomposition and the decoder is only used during training for loss reconstruction and discarded during inference. A ReLU-SAE can be written as\nEncoder: $z = ReLU(W_{encoder}(x - b_{norm}) + b_{encoder})$\nDecoder: $x = W_{decoder}z + b_{norm}$\n(1)\n, where $b_{norm}$ means all inputs are normalized to the unit norm before passing to the autoencoder and computing the reconstruction errors. Notice that $W_{encoder} \\in \\mathbb{R}^{n \\times d}, b_{encoder} \\in \\mathbb{R}^{n}$, where n is number of pre-defined latents in SAE.\nSimilarly, a k-sparse autoencoder (Makhzani and Frey, 2013) regulates the count of active latent units by employing the TopK activation function, which retains only the k largest latent values, setting all others to zero. Thus,\nEncoder: $z = TopK(W_{encoder}(x - b_{norm}))$\n(2)\nand the decoder is the same. The model is trained by gradient descent through training loss $L = ||x - x||^2$. The TopK-SAE has already been verified by OpenAI (Gao et al., 2024) for its superior performance in explaining GPT-4, thus we also follow their training tricks, such as forcing dead neuron activation if it has not activated in 10 million tokens (with an auxiliary loss coefficient of 1/32), normalizing the decoder weights to have unit norm, and setting $W_{decoder}$ to be the transpose of $W_{encoder}$ for improving the training process.\nSAE Training. We pick the last residual stream from the final layer of Llama-3.1-8b-instruct \u00b9 for training our TopK-SAE. And we set the number of latents n to 131,072, and K to {16, 32, 64, 128, 256}. We use Top-128-SAE for our main experiments for its moderate size. We use the 10B tokens from RedPajama-Data-V2 (Computer, 2023) for SAE training considering its high quality and diverse source. We tried various batch sizes from {4,096, 8, 192, 12, 288} and found 4, 096 to be the optimal, and more can be found in Appendix D. For a total batch size of 4,096, the batch size is 32 per device, and grad_acc_steps and micro_acc_steps are 4 and 2, respectively. For all training, we use 4 nodes with 8 Nvidia A100 80G GPUs per node through model parallel. The lr_warmup_ratio is 0.5 and the learning rate is 7e - 5. We set epoch to 4 and we do not find additional benefits with more epochs. Since the Llama-3 tokenizer contains a BOS token which is useless and even detrimental to SAE training, we discard all BOS tokens for SAE training. We preprocess all the data to the same length through concatenating and chunking passages.\nSAE Inference After training the SAEs, inspired by the significant performance boost achieved with JumpReLU-SAE (Rajamanoharan et al., 2024b), we further use JumpReLU during inference to rectify the activations and only treat the activation value larger than the threshold as true activation.\nJumpReLU(x) =\n\\begin{cases}\nx, & \\text{if } x > \\theta,\n\\\\ 0, & \\text{otherwise}.\n\\end{cases}\n(3)\n, where @ is the jump threshold."}, {"title": "4 Methods and Experiments", "content": "4.1 Diversity-driven Data Selection through Sparse Autoencoder\nBased on the extracted features from our trained SAEs, we design two sets of data selection methods: 1) when we only want to select a limited number of data as presented in (Chen et al.; Zhao et al.), we propose greedy sampling using features from SAE for limited data (SAE-GreedSelect) to maximize the utilization of features, and 2) when we want to scale up the selected data rather than only picking a fixed number of data, we propose similarity-based sampling using features from SAE for scaling up the data selection (SAE-SimScale). For example, the original #InsTag (Lu et al.) method uses a greedy search but can not scale to 5k data since no data can bring new intention tags. The details of the two methods can be found in Algorithm 1. As we mentioned above that we actually use JumpRelu during inference, and we empirically set the threshold to 10 across all experiments in our work. For method SAE-SimScale, there is one additional parameter of similarity ratio, and we set it to 0.8 for all experiments. Thus, there are only 1 and 2 tunable parameters for SAE-GreedSelect and SAE-SimScale, respectively. This way, we can maximize the simplicity. Additional experiments regarding the threshold can be found in Section 5.7, and some even witness better results.\n4.2 Experimental Settings\nModels. To validate our method for data selection for supervised instruction fine-tuning (SFT), we use Llama-2-13b-base for our main experiments as the foundation base model. In addition, we also use Gemma-2-9b and Llama-2-7b-base to verify the method at different model scales.\nDatasets. We use Alpaca-52k and WizardLM_evol_instruct_70k as the target instruction tuning datasets because they are widely used and contain large enough data points for data selection.\nBaselines. We choose Longest-response (Zhao et al.), #Instag (Lu et al.) and Repr Filter (Liu et al.) as the baselines. #Instag (Lu et al.) and Repr Filter (Liu et al.) share similar ideas of gradually selecting dadapoints that add additional diversity, measured by intention tags and Cos similarity (we follow the similarity threshold of 0.9 in their paper but"}, {"title": "5 Results and Analysis", "content": "5.1 Why 1,000 Longest Responses (Zhao et al.) Lead to Strong Performance?\nPicking the 1,000 longest responses (Zhao et al.) serves as a simple but tough-to-beat baseline for SFT and they hypothesize that this is because the longest responses intuitively contain more learnable information, but do not provide quantitative evidence to this assumption. Thus, we provide a quantitative explanation of the activated features through SAE by examining the correlation between activated features and text length. Figure 2 shows a strong positive correlation (r = 0.92) between text length and feature richness in an SAE extracted from text in the Alpaca dataset, supporting their hypothesis that longer responses encode more learnable information. This simple strategy provides a competitive baseline for fine-tuning LLMs efficiently but is not always better, as shown later."}, {"title": "5.2 Comparison with Baselines", "content": "Table 1 presents a comparative analysis of different data selection methods applied to Alpaca-52k and WizardLM_evol_-instruct-70k datasets, evaluated under \"Strict prompt-level,\" \"Strict instruction-level,\" \"Loose prompt-level,\" and \"Loose instruction-level\" on the IFEval. As the data size increases from 1k to 5k, the performance of all methods improves across both datasets. Our proposed methods SAE-GreedSelect and SAE-SimScale exhibit more significant gains than baseline approaches, indicating their robustness and scalability. For instance, in the WizardLM_evol_instruct-70k dataset at the 3k data scale, SAE-SimScale achieves a \"Loose instruction-level\" score of 50.96, significantly surpassing the #instag baseline's 46.16 and other baselines. And it even matches the upper bound of using the entire 70k data. SAE-SimScale achieves the best results overall, particularly with higher data sizes (3k and 5k), with significant performance improvements in \"Strict instruction-level\" and \"Loose instruction-level\" evaluations. Performance differences are more pronounced in the WizardLM_evol_instruct-70k dataset, highlighting the challenges and opportunities in leveraging larger and more complex instruction-tuning datasets.\nThe results suggest that the SAE-SimScale method is the most effective approach for data selection through diversity-driven frameworks. By outperforming baselines across all metrics and datasets, SAE-SimScale highlights its potential for optimizing scalable data selection."}, {"title": "5.3 Preference Evaluation", "content": "In addition to the above evaluation, we also focus on assessing human or AI preference evaluation. The head-to-head comparison is shown in Figure 5, showing that our methods again consistently outperform the baselines. More details can be found in the Appendix E."}, {"title": "5.4 Different SAE as Feature Extractor", "content": "In Table 2, we show the result using different SAE as the feature extractor. This table compares the performance of the same algorithm SAE-GreedSelect using two SAEs as the backbone, Gemma-SAE and Ours-SAE, across four evaluation metrics on subsets of the Alpaca-52k dataset with 1k and 3k data points. For the 1k subset, Ours-SAE achieves higher accuracy scores than Gemma-SAE, such as 24.21% vs. 23.66% on strict prompt-level and 37.53% vs. 35.25% on loose instruction-level metrics. For the 3k subset, Gemma-SAE slightly outperforms Ours-SAE in strict prompt-level (31.61% vs. 29.76%) and instruction-level (41.96% vs. 40.89%), though both methods are comparable in loose-level evaluations. This confirms the universal effectiveness of our proposed data selection method when using different SAEs as the backbone, showing the flexibility and compatibility of our data selection strategy."}, {"title": "5.5 Results on Base Model with Different Size", "content": "In Table 4, we show the results using base models at different sizes to explore the impact of model parameters. The results demonstrate that our proposed data selection method consistently outperforms baselines across all data scales (1k, 3k, and 5k) for both Gemma-2-9b and Llama-2-7b models. And SAE-SimScale achieves the highest scores across almost all metrics, showcasing its effectiveness in improving performance over other methods. The improvements with the proposed method are more evident in the Gemma-2-9b model compared to the Llama-2-7b, and the same trend is also observed in previous results on Llama-2-13b. The significance of the improvements achieved by the proposed data selection method diminishes when evaluated on smaller models, highlighting the dependence on the model scale for pronounced benefits. But this demonstrates significant importance of our method since it meets the scaling trend of current LLMs."}, {"title": "5.6 Results of Using SAEs on Different Layers.", "content": "The previous results come from picking the data using the SAE trained on the final (31st) layer. However, it is also possible to use other layers. Thus, we also pick the SAE trained on the 29th layer as the backbone and perform a similar pipeline of selecting the data and training the model. shows the radar plot to compare the four IFEval scores on models trained on 1k, 3k, and 5k data selected using different SAE layers. For the 1k dataset, Layer-31-SAE scores ranged from 24.21 to 37.53, while Layer-29-SAE ranged from 21.44 to 35.25. With larger datasets (3k and 5k), scores improved, with Layer-31-SAE reaching up to 43.65 (3k) and 42.33 (5k), consistently outperforming Layer-29-SAE, which peaked at 41.72 (3k) and 41.01 (5k)."}, {"title": "5.7 Impact of Inference Threshold of SAE", "content": "In previous experiments, we always set the threshold to 10.0 during the SAE inference. Here we show the results of using different threshold. compares accuracy levels across varying thresholds for two versions of our methods (\"SAE-GreedSelect \" and \"SAE-SimScale \") under different data selection conditions (1k, 3k, 5k). Both methods exhibit an upward trend in accuracy as thresholds increase, although the rate of improvement varies. At lower thresholds,"}, {"title": "5.8 Case Study", "content": "Here we show some examples in as a case study, and the instructions are picked from the IFEval dataset. Our model outperforms the baseline in instruction adherence and generation correctness, with additional cases in App. C."}, {"title": "6 Factual Knowledge benchmarks", "content": "Despite the above instruction-following abilities, we look at the model performance on knowledge-intensive benchmarks, as shown in . This figure presents a comparative analysis of model performance across five standard benchmarks and the average performance. It highlights that SAE-SimScale generally outperforms other models across most datasets, particularly in Winogrande and the overall score. GSM8k witnesses the lowest results among all datasets for all models, while Winogrande shows the highest accuracy, showcasing the variability in task difficulty. However, there are marginal differences between different selection methods, suggesting that factual knowledge is almost maintained across five models, which is also proven in (Zhao et al.) that small scale SFT does not impact abilities on knowledge benchmarks. More can be found in Appendix 6."}, {"title": "7 Conclusion", "content": "Our findings unveil a novel measure of data diversity through the learned monosemanticity in sparse autoencoders. Based on the activated features in the SAEs, we proposed a new data selection algorithm for instruction tuning corset selection. The models fine-tuned on our selected data consistently outperform other selection methods in instruction-following abilities across different models and datasets. Also, when we change the ratio of selected data, our approach consistently achieves better results. Besides, we can use our data diversity measure to explain why longer instruction-response data"}, {"title": "C Additional Case Study", "content": "Here we show additional examples in as case studies, and the instructions are picked from the IFEval dataset. Our model outperforms both baselines in instruction adherence and generation correctness."}, {"title": "D Additional SAE Training Loss VS. Batch Size", "content": "Here in figure 11 we the SAE training loss for layer 29 when we change the batch size from 4,096, 8, 192 to 16, 384. We can see that setting batch size to 4, 096 makes the SAE training convergence faster and leads to smaller final loss."}, {"title": "E Instruction Following Evaluation", "content": "The head-to-head comparison was conducted on the 805 instructions in the AlpacaEval dataset. We utilize the same evaluation prompt template for GPT-40, as employed by AlpaGasus (Chen et al.), Longest (Zhao et al.) and originally used in Vicuna work (Chiang et al., 2023). In Figure 12, we show the evaluation prompt for GPT-40. In our preliminary experiments, we find that the the response length has a significant correlation with model judge, aligning with the findings in (Dubois et al., 2024). Thus, we first truncate both responses to the same length and then continue the LLM-as-a-judge for head-to-head comparison. For human evaluators, we hire PhD volunteers to only evaulate 200 of the 805 instructions and only ask for a score, with no explanation for simplicity. On the other hand, for the AlpacaEval 2.0 leadboard evaluation, we keep the original response length since they also offer the length controlled win rate."}]}