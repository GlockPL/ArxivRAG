{"title": "QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning", "authors": ["Md Jueal Mia", "M. Hadi Amini"], "abstract": "Federated Learning has emerged as a leading approach for decentralized machine learning, enabling multiple clients to collaboratively train a shared model without exchanging private data. While FL enhances data privacy, it remains vulnerable to inference attacks, such as gradient inversion and membership inference, during both training and inference phases. Homomorphic Encryption provides a promising solution by encrypting model updates to protect against such attacks, but it introduces substantial communication overhead, slowing down training and increasing computational costs. To address these challenges, we propose QuanCrypt-FL, a novel algorithm that combines low-bit quantization and pruning techniques to enhance protection against attacks while significantly reducing computational costs during training. Further, we propose and implement mean-based clipping to mitigate quantization overflow or errors. By integrating these methods, QuanCrypt-FL creates a communication-efficient FL framework that ensures privacy protection with minimal impact on model accuracy, thereby improving both computational efficiency and attack resilience. We validate our approach on MNIST, CIFAR-10, and CIFAR-100 datasets, demonstrating superior performance compared to state-of-the-art methods. QuanCrypt-FL consistently outperforms existing method and matches Vanilla-FL in terms of accuracy across varying client. Further, QuanCrypt-FL achieves up to 9x faster encryption, 16x faster decryption, and 1.5x faster inference compared to BatchCrypt, with training time reduced by up to 3x.", "sections": [{"title": "I. INTRODUCTION", "content": "Data is essential for research, driving innovation and discoveries. However, privacy laws like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) impose strict regulations that complicate how data is collected, used, and shared [1] [2]. Although these laws provide exceptions for research, compliance with requirements such as consent, anonymization, and data minimization is mandatory, thereby making large-scale data collection both challenging and time-consuming [3]. This creates significant hurdles for deep learning model training, which relies heavily on large datasets, as centralized data collection is often restricted by these regulations. As a result, researchers face difficulties navigating legal frameworks, which can substantially slow down the research process [3].\nFederated Learning (FL) [4] offers a promising solution by enabling edge users or organizations to collaboratively train global models by sharing local parameters or gradients without sharing raw data. FL is categorized into cross-device and cross-silo settings. Cross-device FL involves many mobile or IoT devices with limited resources, while cross-silo FL includes fewer organizations with reliable communications and robust computing power [5]. During FL training, clients share gradients with an honest server to protect data. However, numerous studies have highlighted challenges of indirect data theft through membership inference attacks (MIA) [6], [7] and gradient inversion attacks (GIA) [8]\u2013[14]. These attacks demonstrate that even without direct access to raw data, an honest-but-curious (or semi-honest) [15] server, while following the protocol correctly during FL training, can still infer sensitive information by analyzing the exchanged gradients from the communication channel. Such attacks or vulnerabilities raise questions regarding the real-time applicability of FL in cross-device and cross-silo applications.\nHomomorphic Encryption (HE) [16]\u2013[19] is a widely adopted technique in FL to address security vulnerabilities. HE schemes are categorized based on the number of operations allowed on encrypted data: Partial Homomorphic Encryption (PHE) [20], [21], Somewhat Homomorphic Encryption (SWHE) [22], [23], and Fully Homomorphic Encryption (FHE) [23], [24]. These schemes significantly enhance security during the upload and download of model parameters between the server and edge users or organizations in the training phases [25], [26]. HE has therefore been increasingly adopted in various sectors, including healthcare [27], finance [28], and autonomous systems [29]. During training, edge users or organizations upload encrypted gradients or parameters using a public key, allowing the server to perform aggregation on the encrypted data without decryption. The updated global model is then sent back to the users for the next round, ensuring that gradients remain secure from the server. In cross-silo FL settings, the Paillier cryptosystem [21] is particularly well-suited, as it provides strong privacy guarantees without compromising learning accuracy [30], making it a popular choice for secure FL deployments [31]. However, Paillier is vulnerable to quantum attacks, as its security relies on the hardness of integer factorization, which can be efficiently solved using"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In this section, we highlight the privacy requirements in FL and survey existing techniques, including Differential Privacy (DP), Secure Multiparty Computation (SMC), and HE. We explore the challenges of applying machine learning to homomorphically encrypted models and how efficiently HE enhances security against an honest-but-curious server and external adversaries.\nThe concept of DP [40], [41] has emerged as a crucial framework for providing quantifiable guarantees against data leakage. In FL, DP can be classified into two categories: Central Differential Privacy (CDP) and Local Differential Privacy (LDP). CDP ensures that the aggregate model does not reveal a client's participation or any information about individual training samples [42]\u2013[44]. LDP overcomes CDP limitations by ensuring privacy without trusting the data curator. In LDP, clients individually perturb or encode their data before submitting it to the central server [45]-[47]. This approach helps obscure sensitive information by introducing noise to the model parameters. However, the primary drawback of DP is that this noise can degrade the performance of the global model [48]. The trade-off between utility and noise is a key challenge of DP, making it impractical for real-time applications. This is especially problematic in scenarios where high precision and responsiveness are critical, such as in healthcare or autonomous driving applications, where even small accuracy losses caused by noise can lead to dangerous outcomes.\nSMC is an effective method to protect model parameters from potential attacks in FL. It allows participants to contribute their data for computation without disclosing it to others [49]. SMC has been implemented in FL to securely aggregate model parameters using privacy-preserving protocols [50], [51]. However, SMC introduces high communication overhead and remains vulnerable to inference attacks, as the final output can still leak information about individual inputs [52]. Additionally, client dropouts in SMC lead to delays, reduced system robustness, and incomplete model updates. In FL, SMC protocols can be adapted to handle client dropouts by using redundancy and failover mechanisms, ensuring continued training by redistributing tasks to active clients. This improves model robustness, as adaptive client scheduling and reliability selection further mitigate the negative effects of dropouts [53], [54].\nHE has emerged as an important technique in FL for preserving data privacy by enabling computations on encrypted data without revealing sensitive information. Despite its security advantages, HE often leads to increased computational and communication overhead, necessitating optimization strategies to maintain efficiency [55]-[57]. FPGA hardware accelerators technique, as demonstrated by Yang et al. [58], offer significant improvements in HE efficiency within FL systems. However, they did not provide an in-depth empirical analysis considering large-scale datasets and models. Other methods, such as the PFMLP framework [26] and the FastECDLP algorithm [59] are significant advancements in homomorphic encryption and elliptic curve-based systems, respectively. However, both present certain limitations. The PFMLP framework faces performance overhead due to homomorphic encryption operations, including communication overhead during training, encryption and decryption key length, and key replacement frequency, which can affect training efficiency. Conversely, while the FastECDLP algorithm significantly improves decryption efficiency in EC-based AHE schemes, it provides limited insights into its effect on encryption performance and scalability for longer plaintexts. Furthermore, there is minimal discussion on potential security implications for encryption within this context. The FedML-HE [16] system offers selective parameter encryption to reduce overheads, though it still faces challenges when scaling to more extensive models.\nTo further mitigate the overhead of HE in FL, quantization and sparsification techniques have been employed. BatchCrypt [30] and similar methods encode quantized gradients into compact representations, reducing encryption and communication costs while maintaining accuracy in cross-silo FL settings. However, these approaches can struggle to scale efficiently with larger deep learning models due to increased overhead. Zhu et al.'s [60] distributed additive encryption combined with quantization provides a notable reduction in computational demands by focusing on key parameters rather than encrypting the entire model. Techniques like DAEQ-FL enhance privacy in FL using additive ElGamal encryption and ternary quantization, but they face challenges in scaling to larger models due to the limited precision of ternary quantization, which can result in reduced accuracy for complex models [60]. Sparsification techniques, such as those used in the FLASHE [36] scheme and Ma et al.'s [25] xMK-CKKS protocol, further enhance efficiency by reducing data volume and supporting modular operations, which involve performing computations within a fixed numerical range, thereby ensuring encryption consistency. This approach not only leads to lower communication costs but also improves computational performance. Despite these advancements, challenges remain in the deployment of HE in FL, primarily due to vulnerabilities to honest-but-curious clients and servers, as well as the inherent computational and communication bottlenecks during model training. While HE provides strong post-quantum security and preserves model performance by avoiding artificial noise and trusted environments, it still requires significant computational resources, especially in large-scale scenarios [59], [61].\nTo summarize, each privacy-preserving technique in FL has its own strengths and limitations. SMC provides strong privacy by enabling secure data aggregation without revealing individual contributions, but it incurs high communication overhead and is vulnerable to inference attacks, especially with client dropouts. DP is straightforward to implement, but its addition of noise can degrade model accuracy, limiting its use in precision-critical tasks. HE offers robust privacy by allowing computations on encrypted data without compromising accuracy or requiring major algorithm changes. Although its computational and communication overheads have traditionally hindered its scalability for large-scale deployments, ongoing advancements in optimization techniques and hardware acceleration are addressing these challenges, making HE increasingly viable for real-time applications. One notable advancement is the use of quantization techniques, which reduce the precision of encrypted data to minimize communication costs while maintaining accuracy. Another key improvement is the introduction of hardware acceleration with GPUs and FPGAs, significantly boosting the performance of HE computations by offloading intensive operations, such as modular arithmetic, onto specialized hardware. In conclusion, the choice of privacy-preserving technique in FL must consider the specific use case, as trade-offs between privacy, performance, and scalability are critical. Techniques like quantization combined with model pruning or sparsification in the FHE process can significantly reduce communication, storage, and training costs, all while maintaining performance integrity in both cross-silo and cross-device applications."}, {"title": "III. THREAT MODEL", "content": "In our mechanism, we assess the security of our method under two distinct adversarial scenarios. In the first scenario, the adversary has no knowledge of the model architecture, training parameters, or the underlying FL protocol. HE ensures that all communications between clients and the server remain fully encrypted, preventing the extraction of meaningful information even if encrypted model weights or parameters are intercepted, making this the strongest security assumption. In the second scenario, HE is applied, but the adversary gains access to decrypted model parameters or weights by compromising one or more participants, including honest but curious servers attempting to infer information. This simulates a more realistic threat model where adversaries leverage decrypted model parameters to infer sensitive data. To mitigate this, our framework employs pruning during FL, which significantly limits the adversary's ability to reconstruct raw client data or infer sensitive information, even when partial or full access to decrypted parameters or weights."}, {"title": "A. Overview of Threat Model", "content": "FL has gained significant attention in both industry and academia due to its ability to train a global model across localized training data from multiple participants. However, this collaborative approach to machine learning is not without its vulnerabilities. MIA has been identified as a serious privacy concern in FL [6]. These attacks involve adversaries training classification models to determine if a data record is part of the model's training dataset, potentially leading to privacy breaches [6]. Unlike MIA, which aim to infer whether a specific data point was included in the training set, GIA [62] go a step further by reconstructing the actual training data from the gradients exchanged during model updates. While MIA threatens privacy by revealing membership information, GIA poses a more direct risk by recovering sensitive input data, such as images or personal information. One specific type of inference attack is the user-level inference attack, which targets individual users participating in FL [63]. Additionally, local model reconstruction attacks have been studied, where adversaries eavesdrop on messages exchanged between clients and servers to reconstruct personalized models [10], [64]."}, {"title": "B. Mathematical Formulation", "content": "In this section, we focus on the security threats to FL, particularly on GIA [10]. In a GIA, an adversary intercepts the gradients shared by clients with the central server and attempts to reconstruct the original input data by optimizing the difference in gradient directions. This optimization helps the adversary because the direction of the gradients provides information on how the model's parameters need to change to minimize the loss for specific input data. By iteratively adjusting a guessed input to align its gradient with that of the actual input, the adversary can gradually reconstruct the original data.\nThe adversary's goal can be formulated as an optimization problem. The equation below represents the objective for reconstructing client data by minimizing the cosine similarity between gradients. Cosine similarity is used because it measures the alignment of gradient directions, making it an ideal metric for determining how closely the reconstructed gradients match the actual ones, which helps refine the reconstruction process.\n$\\min\\limits_{x \\in [0,1]^n} \\left( 1 - \\frac{\\langle \\nabla_\\theta L_\\theta(x,y), \\nabla_\\theta L_\\theta(x^*,y) \\rangle}{\\| \\nabla_\\theta L_\\theta(x,y) \\| \\cdot \\| \\nabla_\\theta L_\\theta(x^*,y) \\|} \\right) + \\alpha \\cdot TV(x)$                                                                                                                                                  (1)\nIn Equation 1, the gradient of the loss function with respect to the model parameters for input x, denoted as $\\nabla_\\theta L_\\theta(x, y)$, is compared to the gradient for the ground truth input x*, $\\nabla_\\theta L_\\theta(x^*, y)$. The objective focuses on aligning the gradient directions rather than their magnitudes, as the direction of the gradient provides crucial information about how the model's predictions change in response to specific input data. By aligning the gradient directions, the adversary ensures that the"}, {"title": "C. Attack Practicality", "content": "Inference attacks, particularly gradient inversion attacks, have been demonstrated to pose significant and practical threats to FL systems. In these attacks, adversaries exploit the gradients or model updates exchanged between clients and the central server to reconstruct sensitive training data. Gradients are used to optimize model parameters by representing how much and in what direction the model's predictions should change with respect to the input data. However, this same process can unintentionally expose information about the underlying data because the gradients encode key features of the input, such as patterns or specific attributes that the model learns from. By analyzing these gradients, attackers can reverse-engineer and reconstruct the original input data, especially in models where the gradient directions reveal detailed information about the features that influence the model's predictions. This makes gradient inversion attacks both feasible and dangerous in real-world settings, as the gradients provide a rich source of data that adversaries can exploit. Connected autonomous vehicles (CAVs) can benefit from FL to allow for decentralized decision-making. CAV applications can be vulnerable to adversarial attacks during both the FL training and inference phases [65]. FL allows AVs to improve models collaboratively by sharing aggregated model without exposing personal information. However, attacks like GIA and MIA still pose serious risks, as shared data may contain sensitive information about drivers, vehicle routes, and surroundings. For example, GIA could reconstruct images or AV sensor data collected during training, potentially revealing private information about pedestrians and drivers. These privacy threats are significant, as exposed data could disclose personal information or allow attackers to track specific vehicles by analyzing patterns.\nStudies, such as MiBench, provide substantial evidence of this vulnerability by showing that even the partial extraction of gradient information can lead to the revelation of highly sensitive personal data, such as medical or financial records, in real-time environments [66]. Gradients used for model optimization encode important features of the input data that are critical to the model's predictions. As a result, even small portions of gradient data can reveal patterns or attributes from the original dataset, which can be exploited by attackers to reconstruct sensitive information. This illustrates that these attacks are not merely theoretical but are actively being used to extract private information from models during deployment.\nIn another real-world example, adversaries used generative AI models to extract sensitive data from shared model outputs. In FL, the attack occurs when an adversary exploits the gradients shared by a victim during the training process to train a GAN (Generative Adversarial Network). The GAN's generator produces synthetic data that mimics the victim's private dataset, while the discriminator evaluates the quality based on the gradients. Over several iterations, the GAN learns to generate data closely resembling the victim's original data, allowing the attacker to reconstruct sensitive information and reverse-engineer the data distribution [67]. This further underscores the practicality and danger of inference attacks, especially in large-scale FL deployments [68]. These instances highlight the need for enhanced defenses like HE to protect FL systems from evolving threats. HE encrypts gradients during training, preventing adversaries and even the honest but curious server from accessing raw gradient data, thus preventing gradient inversion attacks. Additionally, gradient pruning limits the amount of gradient information exchanged, further reducing vulnerabilities. Together, these techniques significantly mitigate the risks of inference attacks during FL training and inference phase."}, {"title": "IV. METHODOLOGY", "content": "In our proposed method, we implemented Homomorphic Encryption (HE) in FL with quantization and pruning to enhance training efficiency, reduce communication costs, and make the training process more resilient against inference attacks. The process begins by sharing the initial global model parameters wo with all clients. Each client performs local training on its own dataset, followed by the aggregation of model updates at the server.\nFor each communication round t, the server first sends the current global model parameters w\u00b9 to each client Ci. The clients then perform local training using their respective datasets Di. During local training, each client computes the model update by minimizing the loss on its local dataset. The model update is calculated using the equation 2.\n$\\triangle w_i^{t+1} = w^t - \\eta \\nabla L(w^t, D_i)$  (2)\nwhere w\u00b9 are the global model parameters shared by the server at round t, $\\triangle w_i^{t+1}$ represents the model update for client i after local training, \u03b7 is the learning rate used by the optimizer, and $L(w^t, D_i)$ is the loss function evaluated on the local dataset Di.\nDuring local model training, we employed a pruning technique to iteratively remove less important weights or gradients from the model updates. Specifically, clients perform soft, unstructured pruning based on the L1 norm, which creates a sparse model and makes the FL training process more efficient. The pruning process is guided by a dynamically updated pruning rate pt, which increases over the communication rounds, allowing for more aggressive pruning as training progresses. After pruning, clients send their pruned updates to the server,\nwhich aggregates them using FedAvg to generate the global model. This pruning technique not only reduces the model size and computational costs but also makes the training process more resistant to inference attacks.\nBy progressively increasing the pruning rate, the communication efficiency improves throughout the rounds. As clients share a sparsified model with the server, the transmitted model is no longer the full model, limiting the information available to potential attackers. The sparsity introduced by pruning constrains the parameter space, significantly reducing the chances of reverse engineering or inferring sensitive data. This reduction in exposed parameters inherently enhances privacy protection, making it more difficult for adversaries to extract meaningful insights about the underlying data. Weight pruning or sparsification process is visualized in Figure 2.\nThe pruning rate pt is updated iteratively using the equation 3.\n$p_t = max \\Big( 0, \\frac{t - t_{eff}}{t_{target} - t_{eff}} \\times (p_{target}-p_o)+p_o \\Big)$         (3)\nwhere pt is the pruning rate at round t, teff is the effective round when pruning starts, ttarget is the target round when the target pruning rate is reached, po is the initial pruning rate, and Ptarget is the target pruning rate. This pruning rate increases gradually from the initial value to the target value, ensuring that pruning is progressively applied more aggressively as training advances.\nOnce pruning is applied to the model updates at each client, the pruned local model update $\\bigtriangleup w_{p,i}^{t+1}$ is computed as in equation 4.\n$w_{p,i}^{t+1} = \\triangle w_i^{t+1} \\odot m_i$                (4)\nwhere $\\odot$ represents the element-wise product, and $m_i$ is the local pruning mask generated to identify which weights to prune at communication round t. This pruned update $\\triangle w_{p,i}^{t+1}$ is then quantized and sent to the server for aggregation.\nWe also employed a dynamic mean-based layer-wise clipping technique in Algorithm 1 to help reduce inconsistencies during the training process. The clipping factor controls the clipping parameter, dynamically adjusting the clipping based on layer-wise updates, rather than using a static clipping method. This approach ensures that each layer's updates are clipped according to their specific dynamics, leading to more stable and efficient training. After the local model updates are computed, each client clips its own model update $\\triangle w_i^{t+1}$ to avoid instability before sending it to the server. The clipping for client i's model update is applied using the following equation 5.\n$w_i^{t+1} = clip(\\triangle w_i^{t+1}, -\\alpha \\cdot \\mu_i, \\alpha \\cdot \\mu_i)$     (5)\nwhere \u00b5i is the mean of the absolute values of the elements of client i's model update, calculated as in equation 6.\n$\\mu_i = \\frac{1}{n} \\sum\\limits_{j=1}^n |w_{i,j}^{t+1}|$   (6)\nThe clipping function clip(\u2206w+1,a,b) ensures that the values of \u2206w+1 are constrained within the range [-\u03b1\u00b7 \u03bc\u03af, \u03b1\u00b7 \u03bc\u1f76], limiting the impact of extreme values.\nNext, each client performs quantization on the pruned and clipped model updates to reduce communication costs. The quantization process involves calculating the scaling factor s and determining the quantized values qx to ensure the updates are compressed before transmission. The scaling factor s is calculated using equation 7.\n$s = \\begin{cases}  1.0 / (q_{max} - q_{min}), & \\text{if } x_{max} = x_{min} = 0 \\\\ x_{min} / (q_{max} - q_{min}), & \\text{if } x_{max} = x_{min} \\neq 0 \\\\ (x_{max}-x_{min})/(q_{max}-q_{min}), & \\text{otherwise} \\end{cases}$   (7)\nUsing this scaling factor s, the quantized values qx are then computed as in equation 8.\n$q_x = round(\\frac{x - x_{min}}{s} + z_0)$ (8)\nwhere zo represents the zero-point, calculated as:\n$z_0 = clip (q_{min}, q_{max}, q_{min} - \\frac{x_{min}}{s})$.\nThe quantized values qx are then clamped to the range [qmin, qmax] and converted to the appropriate data type based on the bit width (e.g., 8-bit, 16-bit, or 32-bit) to minimize communication overhead.\nAfter completing quantization, each client encrypts the quantized model updates using the CKKS homomorphic encryption scheme. The server then aggregates the encrypted updates from all clients using the following equation:\n$AggEnc(w_q) = \\frac{1}{N} \\sum\\limits_{i=1}^N Enc(\\triangle w_i)$.\nLeveraging the homomorphic capabilities of CKKS, the server performs this aggregation directly on the encrypted model updates, without requiring decryption of individual updates. Since the server operates solely on ciphertexts, it can compute the sum of the encrypted updates element-wise while keeping each client's data private. This process ensures that even during aggregation, the server has no access to the underlying data. The resulting aggregated model remains encrypted and can be decrypted only by a trusted party or the clients, thus preserving data privacy throughout the FL training process. This approach aligns with the security and efficiency objectives of FL by enabling secure computations while safeguarding client data.\nAfter aggregation, the server decrypts the global model update to evaluate its performance using the following equation:\n$w_q = Dec (Enc(w_q))$. While the server can assess the overall model, individual client updates remain encrypted, ensuring privacy is preserved. This approach balances the need for performance evaluation with the protection of client data in the FL process.\nThe server then performs dequantization to recover the floating-point values using the following equation:\n$x' = s \\cdot (q_x-z_0)$.\nAfter the global model is updated and dequantized, pruning is applied to eliminate certain weights based on pruning rate. The pruned model weights $w_{dq}^{t+1}$ are obtained by applying the pruning mask mt to the global model weights $w_{dq}^{t+1}$ as stated in here:\n$w_p^{t+1} = w_{dq}^{t+1}m_t$,\nwhere $\\odot$ represents the element-wise (Hadamard) product, and m\u2081 is the pruning mask applied to eliminate certain weights in the global model. Once pruning is applied to the global model, the pruned global model $w_p^{t+1}$ is sent to the clients for the next communication round. The clients will apply the same process again: local training, pruning, clipping, quantization, and secure aggregation. The pruning rate pt is updated iteratively for each round, and the process continues until the model converges."}, {"title": "V. EXPERIMENTS AND RESULT ANALYSIS", "content": "Our experiments were conducted on an Ubuntu server equipped with two NVIDIA RTX A6000 GPUs, an Intel Core i9 processor, and 128 GB of RAM. We explored FL across 10 to 50 clients using both IID and non-IID data distribution strategies. Each client independently trained their local models on a subset of data, using the Adam optimizer with a learning rate of 0.001 and a weight decay of 0.0001. Batch sizes were set at 64, or 128 depending on the number of clients, and each client trained for one local epoch per communication round.\nTo ensure data privacy and security, we implemented HE with TenSEAL [69], based on Microsoft SEAL, adopting the CKKS scheme with a polynomial modulus degree of 16384 and coefficient modulus sizes [60, 40, 40, 40, 60]. The encryption context contained a public and private key pair shared among all clients, with the server using only the public key to securely aggregate model updates without revealing individual client data. For evaluation, model weights were decrypted with the private key at the server when necessary to assess testing accuracy."}, {"title": "B. Datasets and Models", "content": "In our FL experiments, we used three datasets: CIFAR10 [70], CIFAR100 [70], and MNIST [71]. These datasets were partitioned among clients using both IID and non-IID strategies to simulate different real-world data distribution scenarios. In the IID setting, data was evenly and randomly distributed among clients, whereas in the non-IID setting, each client received data containing only a subset of classes. CIFAR10 and CIFAR100 images were normalized using their respective mean and standard deviation values, while MNIST's grayscale images were normalized accordingly. The training datasets were split into 80% for training and 20% for validation, with each client receiving a portion for local training. For evaluation purposes, the full test dataset from each respective dataset (CIFAR10, CIFAR100, and MNIST) was used, ensuring that the model performance was assessed on a standardized and unchanged test set after each communication round.\nSeveral models were utilized in the experiments, including CNN, AlexNet, and ResNet18, with each model trained locally on client data to evaluate performance under IID data distribution. The is a CNN designed for MNIST dataset, consisting of two convolutional layers with ReLU activation, max-pooling, a fully connected layer, dropout, and an output layer. We modified AlexNet, adapted from the original AlexNet, includes four convolutional layers with 3x3 kernels, ReLU activations, max-pooling, dropout, two fully connected layers, and a softmax activation for classification. The ResNet18 model follows the standard ResNet-18 architecture, using BasicBlock to create four stages with increasing feature map sizes and downsampling, ending with average pooling and a fully connected output layer."}, {"title": "C. Quantization and Pruning", "content": "In our FL experiments, we employed quantization to compress model updates, with the option to use 8-bit, 16-bit, or 32-bit quantization, controlled by the quantization bit-length qbit, to reduce communication overhead. A clipping factor a = 3.0 was applied to manage extreme values in the model updates, ensuring stable training. For pruning, the initial pruning rate po was set to 20%, and pruning began at the effective round teff = 40. The pruning rate increased progressively until it reached the target pruning rate Ptarget = 50% by round target = 300. A pruning mask m\u2081 was applied to generate pruned weights Wp, reducing the model size and computational cost while maintaining accuracy."}, {"title": "D. Clipping, Smoothing, and Checkpoints", "content": "To control the magnitude of model updates, we applied a clipping mechanism with a clip factor a, set by default to 3.0 after conducting a grid search within the range [1.0,5.0]. This ensured that extreme values in model updates were controlled, ensuring stability during training and preventing large deviations."}, {"title": "E. Result Analysis", "content": "We will provide a comparative analysis of our proposed method with BatchCrypt and Vanilla FL. The results will be evaluated across multiple metrics, including model accuracy, training time, encryption time, decryption time, inference time, and storage efficiency. This analysis will demonstrate the effectiveness of our approach in enhancing both computational performance and security in FL.\nFigure 4 shows the test accuracy comparison of BatchCrypt, QuanCrypt-FL, and Vanilla-FL on the MNIST dataset using a CNN model with 10 clients over 300 communication rounds. The results indicate that BatchCrypt starts with reasonably high accuracy but quickly stabilizes at around 99.04%, consistently underperforming compared to both QuanCrypt-FL and Vanilla-FL. QuanCrypt-FL, however, closely tracks Vanilla-FL throughout the training, reaching an accuracy of 99.40% by the final round, while Vanilla-FL achieves a similar result of 99.32%. This minimal difference between QuanCrypt-FL and Vanilla-FL demonstrates that QuanCrypt-FL achieves nearly identical performance to Vanilla-FL, while offering additional privacy-preserving features. BatchCrypt, although competitive, lags behind both methods across all 300 rounds, showing the trade-offs involved in using a heavier encryption mechanism. Ultimately, QuanCrypt-FL maintains strong accuracy comparable to Vanilla-FL and clearly outperforms BatchCrypt, making it a more effective choice when both privacy and accuracy are essential.\nFigure 5 illustrates the maximum accuracy achieved by BatchCrypt, QuanCrypt-FL, and Vanilla-FL across different client counts (10, 20, 30, 40, 50) on the MNIST dataset using a CNN model. QuanCrypt-FL consistently performs at the highest level, achieving 99.44% for 10 clients, slightly outperforming Vanilla-FL at 99.40%. As the number of clients increases, QuanCrypt-FL maintains strong accuracy with 99.29% for 20 clients, 99.22% for 30 clients, 99.10% for 40 clients, and 98.98% for 50 clients. Vanilla-FL closely follows with accuracies of 99.33%, 99.20%, 99.06%, and 98.97% for the same client counts, respectively. In contrast, BatchCrypt consistently underperforms compared to both QuanCrypt-FL and Vanilla-FL, reaching 99.14% for 10 clients and showing slightly lower values for the remaining client counts, with accuracies ranging between 99.00% and 99.13%. While BatchCrypt provides competitive results, it consistently lags behind the higher performance of QuanCrypt-FL and Vanilla-FL, demonstrating the superior ability of QuanCrypt-FL to maintain high accuracy while incorporating privacy-preserving features.\nA comparison of BatchCrypt, QuanCrypt-FL, and Vanilla-FL on the CIFAR-10 dataset with 10 clients reveals key differences in performance, as shown in Figure 6. While BatchCrypt initially achieves slightly higher accuracy in the early rounds, it quickly falls behind as training progresses. By the final epochs, QuanCrypt-FL and Vanilla-FL reach similar accuracy levels of approximately 80.40%, whereas BatchCrypt lags significantly with a final accuracy of 69.96%. This substantial performance differ of over 10.00% underscores the trade-offs inherent in BatchCrypt, where the pursuit of privacy leads to a notable compromise in accuracy. On the other hand, QuanCrypt-FL closely matches the performance of Vanilla-FL, demonstrating its ability to maintain accuracy while offering including privacy features.\nIn the case of 50 clients, the differences in performance between BatchCrypt, QuanCrypt-FL, and Vanilla-FL become even more pronounced, as shown in Figure 7. While BatchCrypt initially shows some promise in the early rounds, it fails to keep up as training progresses. By the final epochs, QuanCrypt-FL achieves an accuracy of 73.12%, closely aligning with Vanilla-FL's 75.90%, while BatchCrypt trails behind with 66.85%. This (7-9)% difference further highlights BatchCrypt's limitations in scaling effectively. In contrast, QuanCrypt-FL consistently maintains competitive accuracy, making it a reliable and scalable choice for FL scenarios with larger client bases.\nFigure 8 compares the performance of BatchCrypt, QuanCrypt-FL, and Vanilla-FL on the CIFAR-10 dataset using the AlexNet model, with the x-axis representing the number of clients (10, 20, 30, 40, 50) and the y-axis showing the maximum accuracy achieved over 300 communication rounds. QuanCrypt-FL consistently achieves accuracy close to Vanilla-FL across all client counts. For example, at 10 clients, QuanCrypt-FL achieves an accuracy of 81.45%, which is almost identical to Vanilla-FL's 81.64%. As the number of clients increases, QuanCrypt-FL continues to perform strongly, reaching accuracies of 79.62%, 78.96%, 77.10%, and 75.62% for 20, 30, 40, and 50 clients, respectively. In contrast, BatchCrypt shows a noticeable drop in performance, with accuracies starting at 70.84% for 10 clients and steadily declining to 68.18% at 50 clients. While Vanilla-FL remains the top performer with accuracies of 81.64%, 80.39%, 80.29%, 78.92%, and 78.05% as the client count increases, the minimal difference between QuanCrypt-FL and Vanilla-FL indicates that QuanCrypt-FL closely approximates Vanilla-FL's performance while consistently outperforming BatchCrypt across all client counts.\nDue to space limitations, we do not provide the result visualization of the CIFAR-100 dataset. Table I presents the comparative analysis results for all methods across different datasets and models, including CIFAR-10 using AlexNet and CIFAR-100 using AlexNet and ResNet18. The results demonstrate that QuanCrypt-FL outperforms BatchCrypt. For CIFAR-10, we achieve accuracy similar to Vanilla FL, with less than a 1% accuracy loss, while BatchCrypt shows a greater than 10% accuracy loss compared to Vanilla FL across different client counts (1"}]}