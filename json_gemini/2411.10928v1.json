{"title": "Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning", "authors": ["Wenke Huang", "Jian Liang", "Zekun Shi", "Didi Zhu", "Guancheng Wan", "He Li", "Bo Du", "Dacheng Tao", "Mang Ye"], "abstract": "Multimodal Large Language Model (MLLM) have demonstrated strong generalization capabilities across diverse distributions and tasks, largely due to extensive pre-training datasets. Fine-tuning MLLM has become a common practice to improve performance on specific downstream tasks. However, during fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during pre-training, which can result in a decline in generalization abilities. To balance the trade-off between generalization and specialization, we propose measuring the parameter importance for both pre-trained and fine-tuning distributions, based on frozen pre-trained weight magnitude and accumulated fine-tuning gradient values. We further apply an importance-aware weight allocation strategy, selectively updating relatively important parameters for downstream tasks. We conduct empirical evaluations on both image captioning and visual question-answering tasks using various MLLM architectures. The comprehensive experimental analysis demonstrates the effectiveness of the proposed solution, highlighting the efficiency of the crucial modules in enhancing downstream specialization performance while mitigating generalization degradation in MLLM Fine-Tuning.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed remarkable progress in Multimodal Large Language Model (MLLM), which have demonstrated impressive competency in various vision-understanding tasks [14, 48, 52, 53]. MLLM generally follows the paradigm to fuse the pre-trained vision encoder [16, 71] into the representation space of the Large Language Models (LLM), e.g., LLaMA [84] and Vicuna [12], via the connector module [14, 53, 60]. Considering that Multimodal Large Language Model is optimized on huge-scale and various-type multimodality instruction-following datasets [49, 66, 77], it brings powerful generalization ability on different related tasks. Despite this, MLLM still performs poorly on downstream datasets [50, 59, 61, 83, 97].\nThe common practice is to fine-tune foundation models on specific tasks to enhance task performance or align the model behavior with human expectations [26, 106]. Specifically, existing solutions normally freeze the visual encoder, focusing solely on connector layers and the LLM component [8, 51, 79]. Thus, during the fine-tuning stage, the MLLM gains specialization ability to achieve exceptional performance on the fine-tuning task. However, since the small fine-tuning dataset does not have sufficient coverage of the distribution as well as tasks, the fine-tuned model can potentially lose its generality which is acquired through pre-training stage. The effect of deteriorating the model"}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Multimodal Large Language Models", "content": "With the impressive success of Large Language Models (LLM), such as GPT [6, 67, 70], LLaMA [84], Vicuna [12], PaLM [2, 13], growing interest has been aroused in building end-to-end Multimodal Large Language Model (MLLM), e.g., Flamingo [1], BLIP-2 [42, 43], InstructBLIP [14], QWen-VL [4], LLaVA [52, 53, 111], VILA [48]. Existing MLLM solutions normally follow to utilize the visual extractor [16, 71] to encode visual features and utilize the connector module to project visual tokens into word embedding space of the LLM, i.e., treating visual input as the foreign language [87]. Then, the visual and textual tokens are concatenated and fed into the LLM. The LLM is used to accomplish various vision-language tasks in an auto-regressive manner. For example, the famous MLLM work, LLaVA [53] adopts a linear projection layer to connect the visual encoder and the LLM [12, 84]. Despite their effectiveness, existing works primarily emphasize the generalization ability across various tasks, resulting in the constrained performance on specific downstream target tasks. Therefore, it is an intuitive solution to fine-tune the MLLM in order to enhance the performance on the particular task."}, {"title": "2.2. Catastrophic Forgetting in Multimodal Large Language Model Fine-Tuning", "content": "Commonly optimized on downstream tasks [15], deep neural network is empirically proved to suffer from the catastrophic forgetting problem [20, 37, 61, 64, 73, 97], a significant issue where models forget previously learned information when exposed to new data. In the context of MLLM, this results in catastrophic forgetting of generic knowledge, which severely impairs the model transferability across previously learned datasets. Therefore, balancing the ability to fit downstream tasks while maintaining generalization becomes a crucial challenge for Multimodal Large Language Model. Existing methods could be roughly divided into four categories [26, 50, 89]. i) Additive Parameter Learning [30, 40, 55, 91, 100, 104, 105] primarily focus on strategically incorporating additional trainable parameters within the architecture. For example, adapter [21, 30, 45, 80, 100] typically consist of multi-layer perceptions and residual connections [28] that combine pre-trained features with updated ones. Additionally, prompt [40, 76, 88, 95, 104, 105] directly appends adjustable vectors to the input sequence. ii) Reparameterization Tuning [27, 31, 54, 85, 99] also introduce new learnable parameters during the training stage, which are then integrated into the original MLLM through reparameterization during inference. For instance, LoRA [31] assumes that the changes in linear model weights follow a low-rank behavior. Despite the certain advantages, these two research streams introduce additional parameters into the pre-trained model and disrupt the original architecture, leading to increased computational costs and presents restricted architecture compatibility. iii) Regularization-based Optimization [7, 37, 44, 68, 74, 90, 96] introduce the loss constraints to preserve the previously learnt knowledge. Several studies add regularization terms to the loss functions to penalize parameter changes and mitigate catastrophic forgetting. However, aforementioned solutions require to modify the loss function and thus conflict with personalized fine-tuning loss design. iv) Partial-based Updating [3, 46, 47, 59, 94, 101, 103, 109] focuses on modifying a subset of downstream-relevant parameters, making it architecture-agnostic and orthogonal to the downstream loss objective. For instance, GPS [103] and SPU [101] perform sparse updates based on gradient signals, while DARE [94] and Tailor [109] operate on delta parameters. However, previous methods struggle to retain generic knowledge and their performance is highly sensitive to predefined selection thresholds. In our research, recognizing the distinct characteristics of deep neural networks, we argue that parameters exhibit differing importance distributions between pre-training and fine-tuning phases. Therefore, we measure parameter importance in a self-driven manner, selectively updating those with relatively higher importance for downstream tasks while preserving the generalization capability."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Preliminary", "content": "Given the Multimodal Large Language Model (MLLM) architecture, the MLLM model ($\\theta$) typically includes three parts: visual encoder $f$, e.g., ViT [16], LLM ($g$), e.g., Vicuna [12] and LLaMA [84], and the connector module $\\varphi$ [14, 48, 52, 53]. For a query instance, the input consists of both a visual image $x^v$ and a textual instruction $x^t$. The corresponding label is a language response $y$. First, we extract the visual features $z^v = f(x)$, and then apply the trainable projection $\\varphi$ to convert $z^v$ into language embedding tokens, $h^v = \\varphi \\cdot z^v$. And textual token as $h^t = \\text{Tokenize}(x^t)$. Next, we combine both visual and textual tokens and pass them into the LLM module $g$ to generate the language output $\\hat{y} = g([h^v, h^t])$. In our work, following previous MLLM fine-tuning works and benchmarks [106, 109], we select and fine-tune partial trainable parameter module $w$ from the MLLM model to adapt to the downstream task $T$ with distribution ($DT$). Normally, learnable modules are the connector module ($\\varphi$) and candidate LLM ($g$) block layers as $w = {\\varphi, g}$. This default MLLM optimization follows:\n$\\arg \\min_w E_{(x^v,x^t,y) \\in DT} L (g([\\varphi (h^v), h^t]), y)$ . (1)"}, {"title": "3.2. Specialization via Importance Discrepancy Evaluation for Refinement", "content": "To enhance downstream efficiency while preserving generic knowledge in MLLM, we assess parameter importance across pre-training and fine-tuning distributions, selectively updating downstream critical elements, including two components: Importance Discrepancy Measurement (IDM Sec. 3.2.1) for ranking parameter importance, and Importance Selection Mask (ISM Sec. 3.2.2) for selective updates."}, {"title": "3.2.1. Importance Discrepancy Measurement", "content": "Importance for Generalization Knowledge. Generic knowledge embedded in MLLM provides bases for strong performance in various domains and quick transfer to different tasks; when directly fine-tuning on newly received tasks with no regard to preserving its pre-existing, MLLM faces the catastrophic forgetting on the generalization ability. Thus, with respect to the generalization knowledge, we take inspiration from the magnitude pruning [24] and weight magnitude represents how much the parameter contributes to the model prediction [18]. Thus, in our work, we directly utilize the weight magnitude [18, 24, 81] for pre-trained parameters $w^*$ to rank the generalization parameter"}, {"title": "3.3. Discussion and Limitation", "content": "Related Parameter Signal Investigation. Generally speaking, parameter signals could be revealed in two aspects: magnitude [18, 24, 81] and gradient [17, 37, 39, 65, 69, 75]. The weight magnitude represents how much the parameter contributes to the prediction. The gradient reveals the information intensity during optimization. Thus, magnitude and gradient acts as parameter importance metrics to select target elements, which has incurred huge research interest in broad fields, such as network pruning [18, 24, 46, 102], domain generalization [72, 86, 107], federated learning [63, 82], and malicious defense [25, 32, 33, 108, 110]. Existing explorations focus on training a network from scratch and face no requirement to preserve the previously learned knowledge, thus entangling the magnitude and gradient information to select the crucial elements for the target task. However, pre-trained Multimodal Large Language Model (MLLM) models have inherent generalization knowledge, as evidenced by the capacity to execute diverse tasks without fine-tuning [52, 53, 98]. Thus, maintaining the generalization ability and enhancing the downstream specialization ability during the fine-tuning stage acts as a crucial task for MLLM. In our work, we utilize the pre-trained parameter magnitude (I in Eq. (2a)) and optimizing parameter gradient ($\\delta$ in Eq. (3)) to respectively reveal the parameter importance metrics for the generalization and specialization abilities. We further select relative downstream-kernel elements to balance the generalization and specialization ability during the fine-tuning process.\nConcept Difference. Existing methods to mitigate MLLM forgetting, such as DARE [94] and Tailor [109], primarily focus on selectively updating and rescaling optimized parameters using random selection and the Hessian matrix [17, 19, 82, 83]. However, these post-combination operations can conflict with optimization strategies that aim to adjust all trainable elements for downstream performance and is sensitive with the changing scale [36, 94]. Our approach evaluates parameter importance for both generalization and specialization objectives during the tuning stage. This enables us to selectively update parameters relevant to downstream tasks while preserving others, effectively performing an information extrusion [5, 9, 18, 102] to reduce conflicts between pre-training and fine-tuning knowledge. We illustrate the concept difference in Fig. 2.\nLimitation. SPIDER leverages both previously pre-trained and current fine-tuning knowledge to select relevant downstream important elements and re-weight the candidate parameters, thereby ensuring generalization while pressing the fine-tuning optimization pathway. However, ours fails in certain circumstances. (i) We rank parameter importance based on the pre-trained weights and the current gradient matrix, which incurs additional memory usage. However, this increase is linear relative to the scale of learnable"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental Setup", "content": "Architecture and Datasets. Adhering to the Multimodal Large Language Model paradigm, we evaluate the effectiveness of our methods using two popular models as the foundations for our experiments: LLaVA [53] and VILA [48]. We categorize the datasets into two groups: pre-training (seen) and fine-tuning (unseen) datasets to respectively measure the generalization and specialization ability. The pre-training datasets consist of those used in the training process; accordingly, we assess the learned generalization ability on OKVQA [62], TextVQA [77], GQA [34], and OCRVQA [66]. For fine-tuning tasks, we consider four downstream datasets: Flickr30k [93], COCO-Capation [49], IconQA [57], ScienceQA [58]\u00b9, which respectively associate with image caption and visual reasoning views. We follow [106?] resource setting, and randomly sample 10k samples from the training set of each dataset.\nCounterparts. We focus on exploring model-agnostic MLLM fine-tuning methods and mainly compare with the Regularization-based Optimization and Partial-based Updating solutions as follows:\n\u2022 Full Fine-Tuning (Full FT) [arXiv'05] [15]: Default optimize full parameters towards the downstream task.\n\u2022 L2-Regularization (L2-Reg) [PNAS'17] [37]: Add an L2 regularization term with the regularization hyper-"}, {"title": "4.2. Diagnostic Analysis", "content": "We ablation on Flickr30k and IconQA for in-depth analysis. Candidate Parameter Selection Metrics. Selecting candidate parameters plays a crucial role in mitigating catas-"}, {"title": "4.3. Comparison to State-of-the-Arts", "content": "Quantitative Results. We compare our SPIDER against related approaches on image-captioning (CAP) and visual question-answering (VQA) tasks. Due to the architectural complexity and task differences, we limit the VQA evaluation to VILA1.5-3B. As shown in Tabs. 4 and 5, several key observations can be made: The larger the task gap between the fine-tuning and pre-training distributions, the more severe the generalization-specialization trade-offs in the MLLM fine-tuning process. For example, we denote the $\\alpha^{OCAP}_{H_{ZS,T}} = \\sum_T H_{Full FT} - H_{ZS, (T \\in {Flickr30k, COCO-Capation})}$. $\\alpha^{AVQA}_{H_{ZS,T}} = \\sum_T H_{Full FT} - H_{ZS, (T \\in {IconQA, ScienceQA})}$. Thus, $\\alpha^{OCAP} = -21.58\\% < \\alpha^{AVQA} = +81.11\\%$ for the VILA architecture. Notably, regularization-based optimization approaches typically offer limited performance improvements, as controlling parameter stiffness to regulate the extent of LLM updates remains a challenging task. Moreover, in partial-update methodologies, directly combining updated parameters with pre-trained ones often leads to performance fluctuation, largely due to the influence of delta parameters scale. In contrast, both Half FT and our proposed method guide the LLM to fine-tune on selected parameters, demonstrating competitive performance across various experiments. Additionally, our approach selectively targets relatively important parameters for downstream tasks, yielding better performance compared to the random selection strategy employed in Half FT. We further plot the radar visualization in Fig. 4 to highlight the performance advantages of ours compared to other approaches.\nPerformance on More Tuning Epochs E. We investigate the impact of extending fine-tuning epochs E from 5 to 10 rounds, as shown in Fig. 5. The results highlight several key findings: (i) Extending fine-tuning epochs intensifies the pre-training knowledge forgetting phenomenon across different architecture scales. (ii) Smaller architectures, such as VILA-1.5-3B, encounter more severe pa-"}, {"title": "5. Conclusion", "content": "In conclusion, we address the catastrophic forgetting in fine-tuning Multimodal Large Language Model (MLLM). We introduce Specialization via Importance Discrepancy Evaluation for Refinement (SPIDER), a novel approach to assess parameter importance for both generalization and specialization, focusing on identifying downstream-important elements and performing critical-aware updates on selected parameters. Our method enjoys third advantages: First, No Architecture Dependency: SPIDER functions without specific model architecture, which presents high transferability across different architectures. Second, No Fine-tuning Pattern Conflict: we conduct partial parameter updates, maintaining compatibility with various optimization functions. Third, No Hyper-Parameter Configuration: leveraging parameter importance discrepancies requires no additional hyper-parameters, enhancing fine-tuning effectiveness. SPIDER has been validated on fruitful scenarios, highlighting the potential for broader applications."}]}