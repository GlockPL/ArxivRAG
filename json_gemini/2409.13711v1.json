{"title": "WebQuest: A Benchmark for Multimodal QA on Web Page Sequences", "authors": ["Maria Wang", "Srinivas Sunkara", "Gilles Baechler", "Jason Lin", "Yun Zhu", "Fedir Zubach", "Lei Shu", "Jindong Chen"], "abstract": "The rise of multimodal LLMs and web agents calls for the creation of challenging benchmarks to evaluate neural architectures. Unlike existing benchmarks that focus on multi-step web navigation, we present WebQuest, a multi-page question-answering dataset that requires simultaneous retrieval and reasoning across web interaction sequences grounded in real-world usage. WebQuest includes three question categories: single-screen reasoning, multi-screen reasoning, and questions based on navigation traces. We evaluate some of the leading multimodal models like GPT-4V, Gemini Flash, and Claude 3 on our dataset, revealing a significant gap between single-screen and multi-screen reasoning. Finally, we investigate inference time techniques like Chain-of-Thought prompting to improve model capabilities on multi-screen reasoning.", "sections": [{"title": "Introduction", "content": "As the de facto I/O for Human-Computer Interaction, Graphical User-Interfaces (UIs) allow users to acquire information and complete tasks similar to APIs, as the native interface between computer software. A predominant type of data we consume are multimodal web pages rich in semantic knowledge and structure. The advent of smart devices has promised humans the ease of use, motivating research for multimodal, assistive, predictive, automatic and personalized technology for smart UIs. However, designers of such systems do not always consider that the Internet is modeled after our cognitive patterns, such as a limited attention span and working memory. Bounded by single-page interfaces and the ability to traverse back and forward in a sequence, machine learning techniques have been proposed for UI automation but success on multi-step, multi-screen settings are limited, making them unable to fulfill sophisticated tasks for users. Additionally, there are very few datasets focused on information extraction across navigation traces versus task completion.\nRecently, Multimodal Large Language Models (MLLMs) like Gemini (Team et al., 2023), GPT-4V (OpenAI et al., 2024) have been trained on text and image data crawled from the web to demonstrate powerful zero shot capabilities. However, the data collection process of visual-language datasets (Schuhmann et al., 2021, 2022) reveals a discrepancy with how a human user would interact with the web: navigating multiple pages searching for information using the various UI elements present on the UI. By simply extracting text and images from web pages, these datasets not only remove the functional congruence shared between desktop and mobile form factors, but also fail to capture the actions taken and the implicit retrieval requirement that is common to a information-seeking or goal-oriented workflow, i.e. flight booking or shopping. As a result, these models are effective in general reasoning, but perform poorly when used as end-to-end autonomous agents.\nA key challenge in developing UI agents is that they not only have to interpret spatial relations between structures, they also have to ground open ended instructions within an expanding set of semantically correlated pages across diverse applications. Traditional NLP benchmarks like question answering (QA) and information retrieval have been adapted for visual grounding, but they contain either single-page or multi-page questions that contain the answer on one of the pages. We postulate the lack of a temporal dimension might stem from human's limited working memory and the limitations of existing models to effectively predict such complex interactions. When provided a sequence of pages,"}, {"title": "Related Work", "content": "WebQuest is a dataset for Visual Question Answering across multiple images on web screens. In addition, it also includes navigation traces composed of the sequence of steps taken to arrive at the relevant screens to answer the questions. We present prior research along the different aspects covered by our dataset below.\nVisual Question Answering The problem of answering questions based on the contents of an image has been studied extensively by the research community from natural image VQA (Antol et al., 2015), to TextVQA (Singh et al., 2019) of text in natural images and DocVQA (Mathew et al., 2021) containing documents with text, tabular structures and figures. Focusing more on documents and UIs, datasets like ChartQA (Masry et al., 2022) and InfographicVQA (Mathew et al., 2022) emphasize more complex numerical and compositional reasoning. For web and UI understanding, QA datasets like WebSRC (Chen et al., 2021) focused on desktop screens and ScreenQA (Hsiao et al., 2022) focused on mobile app screens contain questions which require extracting relevant information from a single screen. These two datasets are representative of related work. However, our dataset contains questions that necessitate more complex reasoning and also span multiple screens. Similar to our dataset in having multiple images as input, Multi-page DocVQA (Tito et al., 2023) requires visual extraction from up to 20 pages to answer questions. However, Multi-page DocVQA requires identifying a single page that contains the answer whereas in our dataset, the answer spans multiple screens. To the best of our understanding, there are no QA benchmarks spanning multiple websites or images in a task-oriented sequence, where a subset of pages are necessary for joint reasoning.\nVisually-situated Grounding Computer control and workflow automation involve decision-making grounded in visual inputs. Agents have to plan across diverse APIs, interfaces to execute the correct action at the OS and application level. Datasets either captures grounding in a multi-task setting via natural language expressions (Liu et al., 2024) or end-to-end (Rawles et al., 2024). Previous modeling focused on text-based observations, while pixel-inputs are increasingly explored in recent work like (Shaw et al., 2023), (Yang et al., 2023), and (Zheng et al., 2024). Despite the increase in datasets related to single-page action grounding and multi-step navigation as discussed in the section below, our dataset is the first to focus on question answering in a single and multi page setting.\nWeb Navigation and Agent Benchmarks Recently datasets that focus on performing various tasks in web and mobile scenarios have garnered widespread attention. Earlier efforts introduced simulated web and mobile environments, such as MiniWob++ (Liu et al., 2018), MoTIF (Burns et al., 2022), Mind2Web (Deng et al., 2023) and Web-Shop (Yao et al., 2023). To facilitate autonomous web agents, WebArena (Zhou et al., 2024), VisualWebArena (Koh et al., 2024) and WebLinx (L\u00f9 et al., 2024) pair interactive instructions with DOM trees, HTML and pixel-based environments. A majority of these datasets do not have step-by-step supervision for each action taken as part of completing the task. Recently released MMInA (Zhang et al., 2024) is the first to collect multi-hop, sequential navigation across websites. While they focus on long-range reasoning with only one website per task category (e.g. shopping, travel), WebQuest has multiple websites for each task which is closer to real world workflows."}, {"title": "WebQuest Benchmark", "content": "WebQuest is a multimodal benchmark for question answering in a web navigation setting. It is targeted to replicate the scenarios where users navigate to websites to retrieve information across these different websites. To better study the capabilities and failure cases of various modeling approaches, we split these scenarios into three categories:\n1. Single screen QA: Questions about the web page the user is currently on. (see Figure 2 for an example);\n2. Multi screen QA: Questions across multiple web pages simultaneously displayed to the user in their browser, where the web pages are shortlisted such that all of them are required to answer the question. (see Figure 1 and Figure 3 for examples);\n3. Trace QA: Questions about a user's browsing session containing the entire sequence of"}, {"title": "Single screen Question Answering", "content": "The single screen QA benchmark consists of 542 web screenshots with a question-answer pair associated with each screen. To collect the screenshots and associated QA pairs, we provide the raters a list of most popular websites. Then we ask the raters to browse these popular websites, consider the kind of questions a user navigating this website might be interested in, and take a screenshot and generate the relevant QA pair for each such page. The QA pair is required to involve reasoning, in-"}, {"title": "Multi screen Question Answering", "content": "For multi screen question answering benchmark, we provide 171 QA pairs each containing multiple screens. In the multi screen setting, we aim to capture the case where a user has navigated to several relevant pages simultaneously, most commonly in different tabs, then want to get answers to questions about these different web pages, such as calculating prices of products across websites and comparing different product attributes. To ensure the screens in the same example are relevant, we construct a list of popular websites and group them into different categories, such as shopping, flights, etc. We provide these websites lists to human raters and they are required to navigate to 3 to 5 websites listed in the same category. For example, these sets can be menu pages of several restaurants or flight search pages across different booking websites. After navigating to the web pages, the raters generate the associated question-answer pair with the example. Similar to the single screen questions, the questions are focused on logical reasoning, such as comparisons, counting, filtering, arithmetic, etc."}, {"title": "Trace Question Answering", "content": "The trace question answering benchmark consists of 292 examples. The screens in this category represents a sequence of web pages the user navigates to while obtaining the answer to a question. In all cases, only a subset of the screens are relevant to answering the question. As the number of screens increases in the navigation trace, to answer questions based on the screens seen, a representation of the relevant information encountered needs to be maintained. The main difference between this category and the Multi screen QA category is that, in Multi Screen QA each QA pair contains only the screens with information relevant to answer the question, whereas in Trace QA each QA pair also contains the different screens encountered in the same browsing session. We consider Multi screen QA as a simplified version of the Trace QA task which enables us to measure the capabilities of different modeling approaches on answering questions across multiple screens.\nFor the data collection, similar to the multi-screen screen QA data collection, we provide top websites and their associated categories to the human raters. The raters browse through websites and record screen traces. On average, each trace contains 16 screens. After constructing the screen navigation history, the rater reviews the screens and generates QA pairs. The raters are instructed to create questions involving multiple screens in the screen sequence and requiring logical or arithmetic reasoning."}, {"title": "Experiments", "content": "To evaluate the performance of various models on the WebQuest benchmark, we utilize a variant of the Relaxed Accuracy metric used for the ChartQA (Masry et al., 2022) dataset. It is noteworthy that over half of the answers in this benchmark are numerical values. For these numerical values answers, we use heuristics to extract the numbers from predictions and ground truths. For floating-point answers, similar to Relaxed Accuracy, we"}, {"title": "Baselines", "content": "We evaluate the performance of a number of state-of-the-art MLLMs, along with the adapted forms of web-oriented models on the WebQuest benchmark. We use pretrained parameters and default settings of API-based models for evaluation. The model input includes the screen image(s) and question. Although image ordering should not be relevant to the questions, we use the same ordering of images for Multi Screen QA and the original navigation sequence orders for Trace QA. We craft the model"}, {"title": "Main Results", "content": "In this section, we present a comprehensive comparison of different MLLMs on our WebQuest benchmark. To better understand the capabilities and"}, {"title": "Single Screen QA", "content": "As described in section 3, the single screen question answering split involves answering a question based on the content of a single web screen. On this task, we notice that all the MLLMs perform relatively well compared to Multi Screen QA and Trace QA. Chain-of-thought (Wei et al., 2022) prompting improves the scores significantly. We also note that the performance difference for large MLLMs is very close on this split when compared to a model"}, {"title": "Multi Screen QA", "content": "To recap briefly, the multi screen question answering split involves question answering across various relevant screens from different websites. As we saw in section 3, the images are from related websites and on average, each QA pair contains 3.84 screens with 3, 4 or 5 screens. For this split, we notice that all the MLLMs perform worse than the single screen split. With some MLLMs, we notice considerable performance improvement with Chain-of-thought (Wei et al., 2022) prompting and few shot evaluations. However, the performance for all the MLLMs drops to around 45% on this split, highlighting the difficulty some of these models have in reasoning across multiple images. We further analyze the performance gap in section 4.4."}, {"title": "Trace QA", "content": "The main difference between this split and the Multi Screen QA 4.3.2 is that this split also contains the screens seen during navigation to the relevant screens for answering the questions. As stated in section 3, the Multi Screen QA split is a simplified version of this split as it already contains only the relevant screens. This split can also be used for evaluating different agent systems since it contains action information for each step enabling step-by-step supervision compared to existing datasets. On average, each question has 15.8 screens, out of which relevant information is present on 3.2 screens.\nWe notice that the performance of all the models is significantly worse on this split compared to both the Single Screen and Multi Screen QA splits. With Chain-of-thought prompting (Wei et al., 2022) and prompt engineering, MLLMs on average achieve"}, {"title": "Analysis", "content": "In this section, we compare the performance of different MLLMs on WebQuest in Table 1. We highlight some of our significant findings below.\nSingle screen vs Multi screen\nWe observe that all the models including the largest ones achieve much higher performance on single screen tasks compared to multi screen tasks. However, as we can see in figure 8, for the MultiScreen QA split, there is no clear evidence of model performance decreasing as the number of relevant screens increase. This observation needs further investigation. From figure 9 we observe a significant performance gap in questions involving counting across screens compared to arithmetic or comparison operations.\nImpact of prompting techniques on results\nWe observe that prompting techniques like Chain-of-thought (Wei et al., 2022), asking the models to split the question into its constituent parts and describing the contents of each screen results in significant improvements across different kinds of models, even with the largest models across the GPT-4 (OpenAI et al., 2024), Gemini (Team, 2024) and Claude 3 (Anthropic, 2024) model families. On average, we see improvements of 26.5 percentage points on Multi screen QA split and 6.7 percentage points on the Trace QA split. When we manually observe the CoT responses generated, we do note that the model is able to answer sub-questions on various screens correctly and this ability results in better overall performance.\nVarying performance on different question categories\nFor each of the questions in the dataset, we also provide information on the kind of reasoning needed to answer the question. Inspired by similar annotations in the InfographicVQA (Mathew et al., 2022) dataset, we use the categories: arithmetic, comparison, counting and filtering where each question can have more than one category."}, {"title": "Conclusion", "content": "In this work, we introduce WebQuest, the first multimodal question answering benchmark designed to evaluate reasoning abilities across multiple screens. Our benchmark encompasses three distinct settings: single-screen, multi-screen, and trace-based, enabling a comprehensive assessment of model capabilities. Evaluations of state-of-the-art MLLMs, including Gemini Flash (Team, 2024), Claude 3 (Anthropic, 2024), and GPT-4V (OpenAI et al., 2024), reveal a significant performance gap between single-screen and multi-screen reasoning tasks. Notably, Chain-of-thought prompting (Wei et al., 2022) proves effective for navigating complex, multi-screen scenarios by facilitating information extraction and synthesis across multiple screens. Future research directions include leveraging richer screen information, such as OCR, DOM, and screen annotations from tools like ScreenAI (Baechler et al., 2024), to potentially enhance model performance and even replace raw screen images in cases of long screen sequences. Furthermore, extending WebQuest to support personalized, multi-turn dialogue scenarios presents a compelling avenue for future exploration, enabling the development of agents capable of assisting users in dynamic, context-aware conversations."}, {"title": "Dataset Examples", "content": "We show a few examples of the WebQuest dataset, in Figure 10 for Single Screen QA, in Figures 11 and 12 for Multi Screen QA, and in Figure 13 for Trace QA."}, {"title": "Statistics of the Dataset", "content": "We show detailed statistics of the dataset in Table 2, Table 3, and Table 4."}, {"title": "Zero-shot prompt", "content": "You are given a sequence of screens and a question. Answer the question using only the information on the screens. You should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents. Question:"}, {"title": "Chain-of-thought prompt", "content": "You are given <num_screen> screenshots and a question. Your goal is to answer the question according to the screen information only. Please follow the below steps to answer the question.\nQuestion:\n<question>\n(Screenshots Analysis)\nFirst, analyze the contents of each screenshot and list them here.\n(Question Analysis)\nWhat kind of information is needed from each screenshot to"}, {"title": "Data Collection Instructions", "content": "The data collection process was performed with human raters who were responsible for capturing the screenshots and generating the corresponding questions and answers. In each task", "below": "n* Consider use cases where a user is researching a product to buy or comparing options from different web-sites.\n* Capture 3-5 web screenshots taken from Google Chrome ( desktop) spanning websites related to each other.\n* Most of these screenshots should be from different websites.\n* Generate a question-answer pair\n*\nfocusing on arithmetic or logical reasoning.\n* Ensure that answering the question needs to use information from all the screenshots.\nFor the Trace QA split"}, {"below": "n*\nConsider use cases where a user is researching a product to buy or comparing options from different web-sites.\n* Go to the first web page and start the trace recording.\n* Navigate through the different webpages as part of the trace. Once the navigation is complete, click \"stop\" on the recording and review the screenshots captured.\n* Delete any screenshots containing any personal information.\n* Generate a question-answer pair\n*"}]}