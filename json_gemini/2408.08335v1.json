{"title": "Plan with Code: Comparing approaches for robust NL to DSL generation", "authors": ["Nastaran Bassamzadeh", "Chhaya Methani"], "abstract": "Planning in code is considered a more reliable approach for many orchestration tasks. This is because code is more tractable than steps generated via Natural Language and make it easy to support more complex sequences by abstracting deterministic logic into functions. It also allows spotting issues with incorrect function names with the help of parsing checks that can be run on code. Progress in Code Generation methodologies, however, remains limited to general-purpose languages like C, C++, and Python. LLMs continue to face challenges with custom function names in Domain Specific Languages or DSLs, leading to higher hallucination rates and syntax errors. This is more common for custom function names, that are typically part of the plan. Moreover, keeping LLMs up-to-date with newer function names is an issue. This poses a challenge for scenarios like task planning over a large number of APIs, since the plan is represented as a DSL having custom API names. In this paper, we focus on workflow automation in RPA (Robotic Process Automation) domain as a special case of task planning. We present optimizations for using Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with an ablation study comparing these strategies with a fine-tuned model. Our results showed that the fine-tuned model scored the best on code similarity metric. However, with our optimizations, RAG approach is able to match the quality for in-domain API names in the test set. Additionally, it offers significant advantage for out-of-domain or unseen API names, outperforming Fine-Tuned model on similarity metric by 7 pts.", "sections": [{"title": "1 Introduction", "content": "There has been significant progress made in improving and quantifying the quality of Natural Language to Code Generation or NL2Code (Chen et al., 2021, Nguyen and Nadi, 2022). Recent improvements in models for general-purpose languages like Python, C++ and Java can be attributed to larger LLMs (ChatGPT, GPT-4) and the availability of Pre-trained open-source models (Code Llama, Abdin et al., 2024, Codestral) advancing the state-of-the-art. However, there hasn't been a focus on improving quality of Natural Language to Domain Specific Languages or NL2DSL, which a lot of enterprise applications rely on.\nDomain Specific Languages (or DSLs) are custom computer languages designed and optimized for specific applications. Examples of DSLs include SQL and industry-specific languages for formalizing API calls, often using formats like JSON or YAML to represent API sequences. In this paper, we focus on the task of generating a DSL used for authoring high-level automation workflows across thousands of web-scale APIs. These workflows support a variety of customer scenarios like invoice processing, sales lead integration with forms/emails etc. The automation DSL represents API names as functions and codifies a sequence of API calls (or a plan) along with conditional logic over the invocation of APIs. The syntax itself borrows from known languages like Javascript, however, the logic resembling the workflow along with the custom function names, make it unique.\nExisting code generation methods are hard to adapt for this scenario due to the frequent hallucinations and syntax errors. This is largely due to the custom API names, high cardinality and diversity of APIs in public as well private domain along with the ever-changing API landscape. A typical workflow can choose among thousands of publicly available APIs (eg. Database connectors, Emails, Planner, Notifications etc.) as well as private APIs in tenant subscriptions and string them together to automate business processes.\nIn this paper, we outline an end to end system architecture for NL2DSL generation with high response rate using selective improvements to RAG techniques (Liu et al., 2023, Poesia et al., 2022) using OpenAI models. We focus on bench-marking the impact of different contexts used for grounding. We fine-tuned a Codex model for NL2DSL and show a comparative analysis of the impact of the approaches used to optimize RAG.\nThe remainder of this study is structured as follows. In Section 2, we present the NL2DSL problem formulation along with literature review. Section 3 lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section 4 discusses Data Generation, Metric definition and Section 5 shares our results and discussion followed by Conclusion in Section 6."}, {"title": "2 Related Work", "content": "2.1 Code Generation or Program Synthesis\nProgram Synthesis is a hard research problem (Jain et al., 2021, Li et al., 2022, Xu et al., 2021). It has gained significant interest with many open-source models (Code Llama, Li et al., 2023, Codestral, Abdin et al., 2024, Chen et al., 2021) focusing on general programming languages. Many of these advancements have been achieved through pre-training language models for code generation with a focus on improving datasets (Code Llama, Abdin et al., 2024). However, for domain adaptation, instruction fine-tuning on top of a base model remains a popular approach (Chen et al., 2021, Gao et al., 2023, Lewkowycz et al., 2022, Patil et al., 2023).\nPrompting LLMs is an alternative technique for code generation (Liu et al., 2023, White et al., 2023, Wei et al., 2023, Kojima et al., 2023). Most papers focus on metaprompt optimization and learning, while Poesia et al., 2022 focused on improving response quality by dynamically selecting few-shots for grounding the model.\n2.2 Reasoning and Tool Integration\nFor modeling the problem of selecting a sequence of API calls, we need to consider formulating it as a planning or reasoning task. LLMs show remarkable reasoning capability, however, they also have limitations when it comes to staying up-to-date with recent knowledge, performing mathematical calculations etc. A popular way to overcome this has been granting the LLMs access to external tools. This framework gained significant popularity with OpenAI Code Interpreter's success (OpenAI Code Interpretor).\nExternal Tool Integration has been studied since with a focus on including specific tools such as web search (Schick et al., 2023), python code interpreters (Gao et al., 2023, OpenAI Code Interpretor), adding calculators (Parisi et al., 2022 Gao et al., 2023) and so on. Expanding the tool set to a generic list of tools has been explored (Schick et al., 2023, Patil et al., 2023), but it remains limited and often predicts single tool instead of sequences needed for most enterprise scenarios. Tool Use has mostly been explored in the context of generating more accurate text outputs for Q&A tasks with the help of external tools(Schick et al., 2023, Parisi et al., 2022).\nThere is an increase in focus on incorporating LLM's code generation capabilities to reasoning and task orchestration making this an area of active research (Gao et al., 2023, Liang et al., 2023, Patil et al., 2023). However, similar to Q&A scenarios mentioned above, most of the research either limits the tools to a set of small well-documented APIs (Gao et al., 2023, Liang et al., 2023), or limited their scope to predicting a single output API (Patil et al., 2023, Schick et al., 2023).\nPosing the reasoning or orchestration task as a code generation problem is similar to the API sequence generation scenario highlighted in this paper. Improving the quality of Natural Language to DSL generation, is thus beneficial for both reasoning and plan generation.\n2.3 Contributions\nNL2DSL generation suffers from the hallucination and quality issues we discussed in Section 1. Few studies address the challenges of end-to-end DSL generation, specifically over a large set of custom APIs. In this paper, we present an end-to-end system architecture with improved strategies to add grounding context with known RAG techniques. We also present an ablation study showing improvements for DSL generation quality for enterprise settings. DSL samples in our test set consider API or tool selection sequences of 5-6 API calls, also referred to as chain of tools, which is a first to the best of our knowledge. We also consider the real-world scenarios of adding conditional logic with API calls as shown with an example in Figure 1."}, {"title": "3 Methodology", "content": "3.1 Fine-Tuned NL2DSL Generation Model\nWe took the Codex base model from OpenAI due to it's pre-training with code samples and used LoRA-based fine-tuning approach. The training set consists of NL-DSL pairs, NL refers to the user query and the DSL represents the workflow that the user is looking to automate. The training set consists of a pool of 67k samples in the form of (prompt, flow) tuples with the NL generated synthetically (details in Section 4.1, examples of NL-DSL are shared in Figure 1 and Appendix 8).\nWe ran many iterations on this model to improve performance on the test set, specifically for the body and tail connectors, and went through multiple rounds of data augmentation. We found that predicting the parameter keys was very challenging with the fine-tuned model due to limitation of high-quality data generation.\n3.2 Grounding with dynamically selected few-shots\nWe tried two types of grounding information for RAG based DSL generation as described below. For each technique, we selected 5 and 20 few-shots dynamically, and compared performance impact driven by the approach used for sample selection.\n3.2.1 Pre-trained Model\nThe first approach is using a vanilla Pre-trained model for determining the semantic similarity of NL-DSL samples based on the NL query. We computed the embeddings of NL queries using a DistilROBERTa Pre-trained model. We created a Faiss Index (Douze et al., 2024) for these embeddings to help with search over the dense embedding space.\n3.2.2 TST based BERT Fine-tuning\nIn this approach, we fine-tuned the Pre-trained model to improve retrieval accuracy of few-shots similar to the work done by Poesia et al., 2022.\nTo get positive and negative samples for fine-tuning, we generated embeddings for all NL queries in our dataset using a Pre-trained Tansformer model. A pair of tuples is considered a positive sample if the cosine similarity between their corresponding NL prompts is greater than 0.7 and negative otherwise. We generated 100k pairs this way and leveraged them as training data for our fine-tuned model.\nThe loss function used by TST (Equation 1 from Poesia et al., 2022) is minimizing the Mean-Squared Error between the vanilla loss functions comparing the utterances (ui, uj) and the target programs (pi, pj). Program similarity is denoted by S. We used a Jaccard score over lists of API function names as the similarity metric between programs.\n\\(L_{TST}(\theta) := E_{i,j} D[f_{\theta}(u_i, u_j) \u2013 S(P_i, p_j)]^2\\) (1)\n3.3 Grounding with API Metadata\nIn addition to few-shots, we appended the API metadata in the metaprompt. This metadata includes Function Description along with the parameter keys and their description (Example API Function Definition shared in Appendix 8). We followed the below two approaches for selecting the metadata to be added.\n3.3.1 API Function Definitions for Few-Shots\nFor few-shot samples selected using the methods described above, we extracted the metadata for each of the functions present in those samples. This means that for the n few-shot samples dynamically added to the metaprompt, we iterated over all the API function names in each of these flows and added their function definitions to the metaprompt.\n3.3.2 Semantic Function Definitions\nAnother approach for selecting function definitions is to retrieve semantically similar functions from a vector database created with API metadata. This approach is similar to the one followed by (LlamaIndex). We created an index of all API definitions and used the input NL query for search. Please note that this is different from the faiss index created for few-shot samples in Section 3.2.\nWe call this approach Semantic Function Definition (SFD) and will compare it with the Regular FDs described above. This approach can be specifically useful for tail-ish prompts where no few-shots might be retrieved."}, {"title": "4 Experiment Design and Metrics Definition", "content": "In this section, we outline the process of Dataset Generation and introduce the metrics we used for estimating the code quality. We then describe our experiments. We have used Azure AML pipelines and GPT-4 (16k token limit) for our experiments.\n4.1 Dataset Generation\nOur train and test set consists of a total of 67k and 1k samples, respectively. These samples are (prompt, flow) pairs with the workflows being created by users across a large set of APIs. We scrubbed PII from these automations and sampled workflows containing 700 publicly available APIs. We synthetically generated the corresponding Natural Language prompts using GPT-4.\n4.2 DSL Generation Quality Metrics\nWe defined 3 key metrics to focus on code generation quality as well as syntactic accuracy and hallucination rate.\n4.2.1 Average Similarity\nAverage Similarity measures the similarity between predicted flow and the ground truth flow and is defined using the Longest Common Subsequence match (LCSS) metric. Each flow is reduced to a list of API sequences and then the LCSS is computed. The final metric is reported as an average over all test samples. Hallucination and Parser failures lead to the sample being discarded and is assigned a similarity score of 0.\n\\(Similarity = \\frac{LCSS(A, B)}{max(|Actions_A|, |Actions_B|)}\\) (2)\nwhere Actions\u0104 | is the number of actions in flow A and ActionsB| is the number of actions in flow B.\n4.2.2 Unparsed rate\nThis metric captures the rate of syntactic errors. A flow that cannot be parsed by the parser is considered not usable for the purpose of similarity metric computation. Unparsed rate is computed as follow:\n\\(%unparsed flows = \\frac{|Flows_{unparsed}|}{|Flows_{total}|}\\) * 100 (3)\nwhere, FlowSunparsed| is the number of flows that were not parsed and FlowStotal is the total number of flows in the sample set.\n4.2.3 Hallucination rate\nThis metric captures the rate of made-up APIs and made-up parameter keys in the generated code. Predicting a flow with a hallucinated API name is counted as a failure and leads to the code being considered invalid. However, hallucinated parameter keys would only lead to run-time errors which can be fixed down the line. Fixing these run-time errors is beyond the scope of this paper.\n\\(%made-up APIs = \\frac{|Flows_{h}|}{|Flows_{parsed}|}* 100\\) (4)\n\\(%made up parameters = \\frac{|Flows_{hp}|}{|Flows_{parsed}|} * 100\\) (5)\nwhere Flowsh is the number of flows with hallucinated API names, |Flowshp| is the number of flows with hallucinated parameter key names and |FlowSparsed| is the number of flows that were parsed correctly."}, {"title": "5 Results", "content": "In this section, we present the results of the above approaches on a test set of 1000 NL-DSL pairs. The test set is split in 864 in-domain samples and 136 out-of-domain samples. The NL component in these samples, while generated synthetically, has been evaluated by human judges for quality. They were also sampled to represent the distribution of APIs in actual product usage.\nWe compare the impact of each ablation in sections below. Please note that in the following sections all the results are presented as A change compared to a baseline scenario where the higher A is better for Avg. similarity and lower \u25b2 is better for the rest of metrics capturing failures.\n5.1 Impact of number of few-shots\nWe compare the impact of number of code samples added to the meta prompt with two different settings i.e. 5 few-shots vs 20 few-shots. We measured the results for both Pre-trained model as well as TST model. Results are shared in Table 1.\nLooking at rows 2 and 4 having 20 few-shot samples, we can see that adding more few-shots improves the performance of both the Pre-trained as well as the TST model on all metrics. The gain is particularly pronounced for reducing the number of made-up API names as well as reducing the number of made-up API parameter keys.\n5.2 TST vs Pre-trained Model\nTo compare the impact of selecting samples using TST vs Pre-trained model, we look at the impact with and without the presence of API Function Definitions (see Table 2 and Table 3). Here, we have used GPT4 model with 5 and 20 few-shots, respectively. TST with FD setting performs overall better than all other options with values close to the best in every metric.\nThis leads us to conclude that the presence of few-shot examples is supported by adding their API functions definitions (as described in Section 3). The addition predominantly helps reducing the hallucination rate for API names and parameters, which improves the overall response rate of NL2DSL generation. This supports our initial premise: adding tool descriptions (like it is done in planning tasks) along with few-shot code samples helps improve reliability of plan generation.\n5.3 Regular Function Definition vs Semantic Function Definitions\nWe used a Fine-Tuned model as baseline for this experiment (Table 4). Based on the insights from the previous step, we used 20 few-shots for TST along with including FDs.\nLooking at metrics in columns for % made-up API names and % made-up parameter keys, we see that the hallucination rate is, in general, increasing for RAG based approach. However, we need to keep in mind that a fine-tuned model on the function names is hard to beat as it has been trained on 67,000 samples compared to only 20 few-shots that have been added to the RAG model.\nWithin the RAG approaches, comparing rows 1 and 2 (\"TST + FD\" vs \"TST + SFD\"), SFD results in a slight drop in average similarity and an increase in the unparsed rate and hallucination rate for parameters. This indicates that the approach to simply add semantically similar API metadata for a query is not useful for DSL generation. We get better similarity, and reduced hallucination rate, when we include the API Function Definitions for the samples selected by TST (as shown in Row 1).\n5.4 Out of Domain APIs\nTo compare the impact of RAG on unseen APIs, not available for fine-tuning, we created an out of domain test set. We selected 10 APIs, and discarded the flows containing these APIs from the train set. The test set contains 136 (NL, flow) pairs having these APIs.\nWe share the results in Table 5. The baseline is a fine-tuned Codex model with the updated training data. The RAG-based approach notably enhances average similarity (by 7 pts) and reduces API hallucinations (by 1.5 pts) for out of domain APIs. This indicates that when samples are not present in the train set, grounding with RAG context can provide the LLM support for improving code quality.\nHowever, fine-tuned model outperforms RAG model in terms of syntactic errors and parameter key hallucinations. The role of few-shots in informing the syntax of the output code cannot be substituted with just adding function definitions. Since, it is hard to obtain the examples for unseen APIs, we need to find alternate ways to improve syntactic errors. We will look into improving this as future work."}, {"title": "6 Conclusion", "content": "Based on the presented results, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity (Table 4). This could be due to the fact that few-shot examples have been successfully teaching the correct syntax to the LLM model. The positive role of relevant few-shot samples in improving RAG's syntactic accuracy is further confirmed by the drop seen for out of domain data. In absence of relevant few-shots for unseen APIs, we chose examples with low similarity, directly impacting the syntactic accuracy (Table 5).\nCounter intuitively, with the exception of out-of-domain data, this benefit does not transfer to hallucinated API names and their parameter keys where the fine-tuned model holds the advantage (Table 4). Among RAG approaches (Tables 2 and 3), TST + Regular Function Definitions reduced hallucinations the most. Adding Semantic Function Definitions to TST + FD did not confer any advantage for in-domain APIs, but greatly improved code similarity for out-of domain APIs.\nOverall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts. and by 20 pts for parameter keys (Table 2). The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table 4), with better performance for unseen APIs. This reduces the need to fine-tune the model frequently for new APIs saving compute and resources."}, {"title": "7 Ethical Considerations", "content": "We used instructions in meta prompt to not respond to harmful queries. This is supplemented with a harms classifier on the input prompt. The Fine-tuned model was shown examples where it should not generate an output and consequently learnt not to respond to queries considered harmful."}, {"title": "8.1 Sample with computed Average similarity", "content": "Sample showing how flow similarity is computed for two flows Flow A and Flow B.\nQuery = \"Post a message in the channel of teams, when a new form is created in the forms\"\nGround Truth = \"triggerOutputs =\nawait shared_microsoftforms.\nCreateFormWebhook({});\noutputs_shared_teams_PostMessageToConversation\n= shared_teams.PostMessageToConversation(\n{ \\\"poster\\\": \\\"User\\\" });\"\nprediction: \"triggerOutputs =\nawait shared_microsoftforms.\nCreateFormWebhook({});\noutputs_Get_my_profile_V2 =\nshared_office365users.MyProfile_V2({});\noutputs_shared_teams_PostMessage\n= shared_teams.PostMessageToConversation(\n{\\\"poster\\\": \\\"User\\\",\\\"location\\\":\n\\\"Channel\\\"});\"\nAPI Functions list in ground_truth =\n[shared_microsoftforms.CreateFormWebhook,\nshared_teams.PostMessageToConversation]"}, {"title": "8.2 An example of API metdata", "content": "We share a sample of API metadata to highlight the details included in the API description provided to the metaprompt.\n\"shared_outlook.SendEmailV2\": {\n\"FunctionName\": \"shared_outlook.\nSendEmailV2\",\n\"Description\": \"This operation sends\nan email message.\",\n\"IsInTrainingSet\": false,\n\"DisplayName\": \"Send an email (V2)\",\n\"ParametersInfo\": [\n{\n\"Key\": \"emailMessage/To\",\n\"Type\": \"String\",\n\"Summary\": \"To\",\n\"Format\": \"email\",\n\"Description\": \"Specify\nemail addresses\nseparated by semicolons\nlike someone@contoso.com\"\n},\n],\n\"ResponseSchema\": [],\n\"IsTrigger\": false\n}"}]}