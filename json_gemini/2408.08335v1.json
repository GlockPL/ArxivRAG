{"title": "Plan with Code: Comparing approaches for robust NL to DSL generation", "authors": ["Nastaran Bassamzadeh", "Chhaya Methani"], "abstract": "Planning in code is considered a more reliable\napproach for many orchestration tasks. This\nis because code is more tractable than steps\ngenerated via Natural Language and make it\neasy to support more complex sequences by ab-\nstracting deterministic logic into functions. It\nalso allows spotting issues with incorrect func-\ntion names with the help of parsing checks that\ncan be run on code. Progress in Code Gen-\neration methodologies, however, remains lim-\nited to general-purpose languages like C, C++,\nand Python. LLMs continue to face challenges\nwith custom function names in Domain Spe-\ncific Languages or DSLs, leading to higher hal-\nlucination rates and syntax errors. This is more\ncommon for custom function names, that are\ntypically part of the plan. Moreover, keeping\nLLMs up-to-date with newer function names is\nan issue. This poses a challenge for scenarios\nlike task planning over a large number of APIs,\nsince the plan is represented as a DSL having\ncustom API names. In this paper, we focus\non workflow automation in RPA (Robotic Pro-\ncess Automation) domain as a special case of\ntask planning. We present optimizations for us-\ning Retrieval Augmented Generation (or RAG)\nwith LLMs for DSL generation along with an\nablation study comparing these strategies with\na fine-tuned model. Our results showed that the\nfine-tuned model scored the best on code simi-\nlarity metric. However, with our optimizations,\nRAG approach is able to match the quality for\nin-domain API names in the test set. Addition-\nally, it offers significant advantage for out-of-\ndomain or unseen API names, outperforming\nFine-Tuned model on similarity metric by 7 pts.", "sections": [{"title": "1 Introduction", "content": "There has been significant progress made in im-\nproving and quantifying the quality of Natural Lan-\nguage to Code Generation or NL2Code (Chen et al.,\n2021, Nguyen and Nadi, 2022). Recent improve-\nments in models for general-purpose languages like\nPython, C++ and Java can be attributed to larger\nLLMs (ChatGPT, GPT-4) and the availability of\nPre-trained open-source models (Code Llama, Ab-\ndin et al., 2024, Codestral) advancing the state-\nof-the-art. However, there hasn't been a focus on\nimproving quality of Natural Language to Domain\nSpecific Languages or NL2DSL, which a lot of\nenterprise applications rely on.\nDomain Specific Languages (or DSLs) are cus-\ntom computer languages designed and optimized\nfor specific applications. Examples of DSLs in-\nclude SQL and industry-specific languages for for-\nmalizing API calls, often using formats like JSON\nor YAML to represent API sequences. In this pa-\nper, we focus on the task of generating a DSL\nused for authoring high-level automation work-\nflows across thousands of web-scale APIs. These\nworkflows support a variety of customer scenarios\nlike invoice processing, sales lead integration with\nforms/emails etc. The automation DSL represents\nAPI names as functions and codifies a sequence of\nAPI calls (or a plan) along with conditional logic\nover the invocation of APIs. The syntax itself bor-\nrows from known languages like Javascript, how-\never, the logic resembling the workflow along with\nthe custom function names, make it unique. An\nexample of the DSL is shown in Figure 1.\nExisting code generation methods are hard to\nadapt for this scenario due to the frequent halluci-\nnations and syntax errors. This is largely due to\nthe custom API names, high cardinality and diver-\nsity of APIs in public as well private domain along\nwith the ever-changing API landscape. A typical\nworkflow can choose among thousands of publicly\navailable APIs (eg. Database connectors, Emails,\nPlanner, Notifications etc.) as well as private APIs\nin tenant subscriptions and string them together to\nautomate business processes.\nIn this paper, we outline an end to end system\narchitecture for NL2DSL generation with high re-\nsponse rate using selective improvements to RAG"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Code Generation or Program Synthesis", "content": "Program Synthesis is a hard research problem (Jain\net al., 2021, Li et al., 2022, Xu et al., 2021). It\nhas gained significant interest with many open-\nsource models (Code Llama, Li et al., 2023, Code-\nstral, Abdin et al., 2024, Chen et al., 2021) focus-\ning on general programming languages. Many of\nthese advancements have been achieved through\npre-training language models for code generation\nwith a focus on improving datasets (Code Llama,\nAbdin et al., 2024). However, for domain adap-\ntation, instruction fine-tuning on top of a base\nmodel remains a popular approach (Chen et al.,\n2021, Gao et al., 2023, Lewkowycz et al., 2022,\nPatil et al., 2023).\nPrompting LLMs is an alternative technique for\ncode generation (Liu et al., 2023, White et al., 2023,\nWei et al., 2023, Kojima et al., 2023). Most papers\nfocus on metaprompt optimization and learning,\nwhile Poesia et al., 2022 focused on improving re-\nsponse quality by dynamically selecting few-shots\nfor grounding the model."}, {"title": "2.2 Reasoning and Tool Integration", "content": "For modeling the problem of selecting a sequence\nof API calls, we need to consider formulating it as\na planning or reasoning task. LLMs show remark-\nable reasoning capability, however, they also have\nlimitations when it comes to staying up-to-date\nwith recent knowledge, performing mathematical\ncalculations etc. A popular way to overcome this\nhas been granting the LLMs access to external tools.\nThis framework gained significant popularity with\nOpenAI Code Interpreter's success (OpenAI Code\nInterpretor).\nExternal Tool Integration has been studied since\nwith a focus on including specific tools such as\nweb search (Schick et al., 2023), python code inter-\npreters (Gao et al., 2023, OpenAI Code Interpretor),\nadding calculators (Parisi et al., 2022 Gao et al.,\n2023) and so on. Expanding the tool set to a generic\nlist of tools has been explored (Schick et al., 2023,\nPatil et al., 2023), but it remains limited and often\npredicts single tool instead of sequences needed\nfor most enterprise scenarios. Tool Use has mostly\nbeen explored in the context of generating more\naccurate text outputs for Q&A tasks with the help\nof external tools(Schick et al., 2023, Parisi et al.,\n2022).\nThere is an increase in focus on incorporating\nLLM's code generation capabilities to reasoning\nand task orchestration making this an area of active\nresearch (Gao et al., 2023, Liang et al., 2023, Patil\net al., 2023). However, similar to Q&A scenarios\nmentioned above, most of the research either limits\nthe tools to a set of small well-documented APIs\n(Gao et al., 2023, Liang et al., 2023), or limited\ntheir scope to predicting a single output API (Patil\net al., 2023, Schick et al., 2023).\nPosing the reasoning or orchestration task as\na code generation problem is similar to the API\nsequence generation scenario highlighted in this\npaper. Improving the quality of Natural Language\nto DSL generation, is thus beneficial for both rea-\nsoning and plan generation."}, {"title": "2.3 Contributions", "content": "NL2DSL generation suffers from the hallucination\nand quality issues we discussed in Section 1. Few\nstudies address the challenges of end-to-end DSL\ngeneration, specifically over a large set of custom\nAPIs. In this paper, we present an end-to-end sys-\ntem architecture with improved strategies to add\ngrounding context with known RAG techniques.\nWe also present an ablation study showing improve-\nments for DSL generation quality for enterprise\nsettings. DSL samples in our test set consider API\nor tool selection sequences of 5-6 API calls, also\nreferred to as chain of tools, which is a first to the\nbest of our knowledge. We also consider the real-\nworld scenarios of adding conditional logic with\nAPI calls as shown with an example in Figure 1."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Fine-Tuned NL2DSL Generation Model", "content": "We took the Codex base model from OpenAI due\nto it's pre-training with code samples and used\nLoRA-based fine-tuning approach. The training\nset consists of NL-DSL pairs, NL refers to the\nuser query and the DSL represents the workflow\nthat the user is looking to automate. The training\nset consists of a pool of 67k samples in the form\nof (prompt, flow) tuples with the NL generated\nsynthetically (details in Section 4.1, examples of\nNL-DSL are shared in Figure 1 and Appendix 8).\nWe ran many iterations on this model to improve\nperformance on the test set, specifically for the\nbody and tail connectors, and went through mul-\ntiple rounds of data augmentation. We found that\npredicting the parameter keys was very challeng-\ning with the fine-tuned model due to limitation of\nhigh-quality data generation."}, {"title": "3.2 Grounding with dynamically selected\nfew-shots", "content": "We tried two types of grounding information for\nRAG based DSL generation as described below.\nFor each technique, we selected 5 and 20 few-shots\ndynamically, and compared performance impact\ndriven by the approach used for sample selection."}, {"title": "3.2.1 Pre-trained Model", "content": "The first approach is using a vanilla Pre-trained\nmodel for determining the semantic similarity of\nNL-DSL samples based on the NL query. We com-\nputed the embeddings of NL queries using a Distil-"}, {"title": "3.2.2 TST based BERT Fine-tuning", "content": "In this approach, we fine-tuned the Pre-trained\nmodel to improve retrieval accuracy of few-shots\nsimilar to the work done by Poesia et al., 2022.\nTo get positive and negative samples for fine-\ntuning, we generated embeddings for all NL\nqueries in our dataset using a Pre-trained Tans-\nformer model. A pair of tuples is considered a\npositive sample if the cosine similarity between\ntheir corresponding NL prompts is greater than 0.7\nand negative otherwise. We generated 100k pairs\nthis way and leveraged them as training data for\nour fine-tuned model.\nThe loss function used by TST (Equation 1\nfrom Poesia et al., 2022) is minimizing the Mean-\nSquared Error between the vanilla loss functions\ncomparing the utterances (ui, uj) and the target\nprograms (pi, pj). Program similarity is denoted\nby S. We used a Jaccard score over lists of API\nfunction names as the similarity metric between\nprograms.\n$L_{TST}(\\theta) := E_{i,j} D[f_{\\theta}(u_i, u_j) - S(P_i, p_j)]^2$ (1)"}, {"title": "3.3 Grounding with API Metadata", "content": "In addition to few-shots, we appended the API\nmetadata in the metaprompt. This metadata in-\ncludes Function Description along with the param-\neter keys and their description (Example API Func-\ntion Definition shared in Appendix 8). We followed"}, {"title": "3.3.1 API Function Definitions for Few-Shots", "content": "For few-shot samples selected using the methods\ndescribed above, we extracted the metadata for\neach of the functions present in those samples. This\nmeans that for the n few-shot samples dynamically\nadded to the metaprompt, we iterated over all the\nAPI function names in each of these flows and\nadded their function definitions to the metaprompt."}, {"title": "3.3.2 Semantic Function Definitions", "content": "Another approach for selecting function definitions\nis to retrieve semantically similar functions from a\nvector database created with API metadata. This ap-\nproach is similar to the one followed by (LlamaIn-\ndex). We created an index of all API definitions\nand used the input NL query for search. Please note\nthat this is different from the faiss index created for\nfew-shot samples in Section 3.2.\nWe call this approach Semantic Function Defi-\nnition (SFD) and will compare it with the Regular\nFDs described above. This approach can be specifi-\ncally useful for tail-ish prompts where no few-shots\nmight be retrieved."}, {"title": "4 Experiment Design and Metrics\nDefinition", "content": "In this section, we outline the process of Dataset\nGeneration and introduce the metrics we used for\nestimating the code quality. We then describe our\nexperiments. We have used Azure AML pipelines\nand GPT-4 (16k token limit) for our experiments."}, {"title": "4.1 Dataset Generation", "content": "Our train and test set consists of a total of 67k\nand 1k samples, respectively. These samples are\n(prompt, flow) pairs with the workflows being\ncreated by users across a large set of APIs. We\nscrubbed PII from these automations and sampled\nworkflows containing 700 publicly available APIs.\nWe synthetically generated the corresponding Nat-\nural Language prompts using GPT-4."}, {"title": "4.2 DSL Generation Quality Metrics", "content": "We defined 3 key metrics to focus on code gen-\neration quality as well as syntactic accuracy and\nhallucination rate."}, {"title": "4.2.1 Average Similarity", "content": "Average Similarity measures the similarity between\npredicted flow and the ground truth flow and is\ndefined using the Longest Common Subsequence\nmatch (LCSS) metric. Each flow is reduced to a list\nof API sequences and then the LCSS is computed.\nThe final metric is reported as an average over all\ntest samples. Hallucination and Parser failures lead\nto the sample being discarded and is assigned a\nsimilarity score of 0.\nSimilarity = $\\frac{LCSS(A, B)}{max(|Actions_A|, |Actions_B|)}$ (2)\nwhere Actions\u0104 | is the number of actions in flow\nA and ActionsB| is the number of actions in flow\nB."}, {"title": "4.2.2 Unparsed rate", "content": "This metric captures the rate of syntactic errors. A\nflow that cannot be parsed by the parser is consid-\nered not usable for the purpose of similarity metric\ncomputation. Unparsed rate is computed as follow:\n%unparsed flows = $\\frac{|Flows_{unparsed}|}{|Flows_{total}|}$ (3)\nwhere, FlowSunparsed| is the number of flows that\nwere not parsed and FlowStotal is the total number\nof flows in the sample set."}, {"title": "4.2.3 Hallucination rate", "content": "This metric captures the rate of made-up APIs and\nmade-up parameter keys in the generated code. Pre-\ndicting a flow with a hallucinated API name is\ncounted as a failure and leads to the code being\nconsidered invalid. However, hallucinated parame-\nter keys would only lead to run-time errors which\ncan be fixed down the line. Fixing these run-time\nerrors is beyond the scope of this paper.\n%made - up APIs = $\\frac{|Flows_h|}{|Flows_{parsed}|}$ * 100 (4)\n%made up parameters = $\\frac{|Flows_{hp}|}{|Flows_{parsed}|}$ * 100 (5)\nwhere Flowsh is the number of flows with hal-\nlucinated API names, |Flowshp| is the number of\nflows with hallucinated parameter key names and\n|FlowSparsed| is the number of flows that were\nparsed correctly."}, {"title": "5 Results", "content": "In this section, we present the results of the above\napproaches on a test set of 1000 NL-DSL pairs.\nThe test set is split in 864 in-domain samples and\n136 out-of-domain samples. The NL component in\nthese samples, while generated synthetically, has\nbeen evaluated by human judges for quality. They\nwere also sampled to represent the distribution of\nAPIs in actual product usage.\nWe compare the impact of each ablation in sec-\ntions below. Please note that in the following sec-\ntions all the results are presented as A change com-\npared to a baseline scenario where the higher A is\nbetter for Avg. similarity and lower \u25b2 is better for\nthe rest of metrics capturing failures."}, {"title": "5.1 Impact of number of few-shots", "content": "We compare the impact of number of code sam-\nples added to the meta prompt with two different\nsettings i.e. 5 few-shots vs 20 few-shots. We mea-\nsured the results for both Pre-trained model as well\nas TST model. Results are shared in Table 1.\nLooking at rows 2 and 4 having 20 few-shot\nsamples, we can see that adding more few-shots\nimproves the performance of both the Pre-trained\nas well as the TST model on all metrics. The gain\nis particularly pronounced for reducing the number\nof made-up API names as well as reducing the\nnumber of made-up API parameter keys."}, {"title": "5.2 TST vs Pre-trained Model", "content": "To compare the impact of selecting samples using\nTST vs Pre-trained model, we look at the impact\nwith and without the presence of API Function\nDefinitions (see Table 2 and Table 3). Here, we\nhave used GPT4 model with 5 and 20 few-shots,\nrespectively. TST with FD setting performs overall\nbetter than all other options with values close to the\nbest in every metric.\nThis leads us to conclude that the presence of\nfew-shot examples is supported by adding their\nAPI functions definitions (as described in Section\n3). The addition predominantly helps reducing\nthe hallucination rate for API names and param-\neters, which improves the overall response rate\nof NL2DSL generation. This supports our initial\npremise: adding tool descriptions (like it is done in\nplanning tasks) along with few-shot code samples\nhelps improve reliability of plan generation."}, {"title": "5.3 Regular Function Definition vs Semantic\nFunction Definitions", "content": "We used a Fine-Tuned model as baseline for this\nexperiment (Table 4). Based on the insights from\nthe previous step, we used 20 few-shots for TST\nalong with including FDs.\nLooking at metrics in columns for % made-up\nAPI names and % made-up parameter keys, we see\nthat the hallucination rate is, in general, increasing\nfor RAG based approach. However, we need to\nkeep in mind that a fine-tuned model on the func-\ntion names is hard to beat as it has been trained on\n67,000 samples compared to only 20 few-shots that\nhave been added to the RAG model.\nWithin the RAG approaches, comparing rows\n1 and 2 (\"TST + FD\" vs \"TST + SFD\"), SFD re-\nsults in a slight drop in average similarity and an\nincrease in the unparsed rate and hallucination rate\nfor parameters. This indicates that the approach"}, {"title": "5.4 Out of Domain APIs", "content": "To compare the impact of RAG on unseen APIs,\nnot available for fine-tuning, we created an out of\ndomain test set. We selected 10 APIs, and dis-\ncarded the flows containing these APIs from the\ntrain set. The test set contains 136 (NL, flow) pairs\nhaving these APIs.\nWe share the results in Table 5. The baseline is a\nfine-tuned Codex model with the updated training\ndata. The RAG-based approach notably enhances\naverage similarity (by 7 pts) and reduces API hallu-\ncinations (by 1.5 pts) for out of domain APIs. This\nindicates that when samples are not present in the\ntrain set, grounding with RAG context can provide\nthe LLM support for improving code quality.\nHowever, fine-tuned model outperforms RAG\nmodel in terms of syntactic errors and parameter\nkey hallucinations. The role of few-shots in in-\nforming the syntax of the output code cannot be\nsubstituted with just adding function definitions.\nSince, it is hard to obtain the examples for unseen\nAPIs, we need to find alternate ways to improve\nsyntactic errors. We will look into improving this\nas future work."}, {"title": "6 Conclusion", "content": "Based on the presented results, we see that the role\nof dynamically selected few-shot samples is very\nimportant in making RAG useful for syntactically\ncorrect generation of DSL as well as improving\ncode similarity (Table 4). This could be due to\nthe fact that few-shot examples have been success-\nfully teaching the correct syntax to the LLM model.\nThe positive role of relevant few-shot samples in\nimproving RAG's syntactic accuracy is further con-\nfirmed by the drop seen for out of domain data.\nIn absence of relevant few-shots for unseen APIs,\nwe chose examples with low similarity, directly\nimpacting the syntactic accuracy (Table 5).\nCounter intuitively, with the exception of out-\nof-domain data, this benefit does not transfer to\nhallucinated API names and their parameter keys\nwhere the fine-tuned model holds the advantage\n(Table 4). Among RAG approaches (Tables 2 and\n3), TST + Regular Function Definitions reduced\nhallucinations the most. Adding Semantic Function\nDefinitions to TST + FD did not confer any advan-\ntage for in-domain APIs, but greatly improved code\nsimilarity for out-of domain APIs.\nOverall, we were able to significantly improve\nthe performance of RAG for DSL generation, with\nhallucination rate for API names dropping by 6.29\npts. and by 20 pts for parameter keys (Table 2). The\nperformance of RAG is now comparable to that of\nfine-tuned model (see Avg. Similarity in Table 4),\nwith better performance for unseen APIs. This\nreduces the need to fine-tune the model frequently\nfor new APIs saving compute and resources."}, {"title": "7 Ethical Considerations", "content": "We used instructions in meta prompt to not respond\nto harmful queries. This is supplemented with a\nharms classifier on the input prompt. The Fine-\ntuned model was shown examples where it should\nnot generate an output and consequently learnt not\nto respond to queries considered harmful."}, {"title": "8 Example Appendix", "content": ""}, {"title": "8.1 Sample with computed Average similarity", "content": "Sample showing how flow similarity is computed\nfor two flows Flow A and Flow B.\nQuery = \"Post a message in the channel of teams,\nwhen a new form is created in the forms\"\nGround Truth = \"triggerOutputs =\nawait shared_microsoftforms.\nCreateFormWebhook({});\noutputs_shared_teams_PostMessageToConversation\n= shared_teams.PostMessageToConversation(\n{ \\\"poster\\\": \\\"User\\\" });\"\nprediction: \"triggerOutputs =\nawait shared_microsoftforms.\nCreateFormWebhook({});\noutputs_Get_my_profile_V2 =\nshared_office365users.MyProfile_V2({});\noutputs_shared_teams_PostMessage\n= shared_teams.PostMessageToConversation(\n{\\\"poster\\\": \\\"User\\\",\\\"location\\\":\n\\\"Channel\\\"});\"\nAPI Functions list in ground_truth\n=\n[shared_microsoftforms.CreateFormWebhook,\nshared_teams.PostMessageToConversation]\nAPI function list in model generation =\n[shared_microsoftforms.CreateFormWebhook,\nshared_office365users.MyProfile_V2,\nshared_teams.PostMessageToConversation]\nSimilarity Score = 2/3 = 0.666\nSince the functions shared_microsoftforms.\nCreateFormWebhook and shared_teams.\nPostMessageToConversation are found\nin the ground truth."}, {"title": "8.2 An example of API metdata", "content": "We share a sample of API metadata to highlight the\ndetails included in the API description provided to\nthe metaprompt.\n\"shared_outlook.SendEmailV2\": {\n\"FunctionName\": \"shared_outlook.\nSendEmailV2\",\n\"Description\": \"This operation sends\nan email message.\",\n\"IsInTrainingSet\": false,\n\"DisplayName\": \"Send an email (V2)\",\n\"ParametersInfo\": [\n{\n\"Key\": \"emailMessage/To\",\n\"Type\": \"String\",\n\"Summary\": \"To\",\n\"Format\": \"email\",\n\"Description\": \"Specify\nemail addresses\nseparated by semicolons\nlike someone@contoso.com\"\n},\n],\n\"ResponseSchema\": [],\n\"IsTrigger\": false\n}"}]}