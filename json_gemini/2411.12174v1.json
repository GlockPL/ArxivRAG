{"title": "Just KIDDIN' : Knowledge Infusion and Distillation for Detection of INdecent Memes", "authors": ["Rahul Garg", "Trilok Padhi", "Hemang Jain", "Ugur Kursuncu", "Ponnurangam Kumaraguru"], "abstract": "Toxicity identification in online multimodal environments remains a challenging task due to the complexity of contextual connections across modalities (e.g., textual and visual). In this paper, we propose a novel framework that integrates Knowledge Distillation (KD) from Large Visual Language Models (LVLMs) and knowledge infusion to enhance the perfor-mance of toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused within a compact VLM framework. The relational context between toxic phrases in captions and memes, as well as visual concepts in memes enhance the model's reasoning capabilities. Experimental results from our study on two hate speech benchmark datasets demonstrate superior performance over the state-of-the-art base-lines across AU-ROC, F1, and Recall with im-provements of 1.1%, 7%, and 35%, respec-tively. Given the contextual complexity of the toxicity detection task, our approach showcases the significance of learning from both explicit (i.e. KG) as well as implicit (i.e. LVLMs) con-textual cues incorporated through a hybrid neu-rosymbolic approach. This is crucial for real-world applications where accurate and scalable recognition of toxic content is critical for creat-ing safer online environments.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of online platforms has led to an unprecedented increase in harmful and toxic con-tent, posing significant challenges to maintaining safe and inclusive digital environments (Alatawi et al., 2021; Kursuncu et al., 2019b). According to"}, {"title": "2 Related Work", "content": "2.1 Online Toxicity Detection\nEarly approaches to toxicity detection primarily focused on textual content, employing traditional machine learning (Leo et al., 2023; Saha et al., 2023) and, later, advanced deep learning tech-niques were introduced (Jonathan and Setiawan, 2023; Karim et al., 2022) as per the nuanced and context-dependent nature of the online toxicity problem. Kursuncu et al. (2019a) incorporated multiple dimensions of online content through domain-specific corpora in detecting malicious ac-tors related to Islamist extremism. Multimodal approaches have emerged in recent years that lever-age both textual and visual content. Specifically, HateCLIPper (Kumar and Nandakumar, 2022a) and PromptHate (Cao et al., 2022) have shown promising results by utilizing pre-trained models, fusion methods, prompt tuning, and large datasets. Researchers introduced another multimodal frame-work for detecting toxicity in code-mixed videos through cross-modal synchronization (Maity et al., 2024). Large multimodal models such as Flamingo (Alayrac et al., 2022) and LENS (Berrios et al., 2023) have further improved the detection perfor-mance while their computational complexity poses challenges for deployment in resource-constrained environments, limiting their practical applicability.\n2.2 Knowledge Infusion and Distillation\nKnowledge Infusion (KI) is a paradigm that in-tegrates external knowledge sources (e.g., KGs) into machine learning models to enhance the rep-resentation and improve reasoning abilities by cap-turing complex relationships in multimodal con-tent, such as images, text, and videos. Incorpo-rating structured knowledge provides models with access to the explicit context that is often miss-ing from the raw training data (Zhang et al., 2022; Kursuncu et al., 2020). Liang et al. (2023) pro-posed dual Knowledge Distillation (KD) to en-hance both multimodal monolingual and cross-lingual summarization while filtering irrelevant vi-sual features. Dai et al. (2022) introduced vision-language KD (VLKD) to improve dual-stream mod-els for multimodal generation, boosting zero-shot performance while preserving text generation abil-ities. In health informatics, Sharma et al. (2019) integrated domain-specific knowledge to enhance the performance of predictive models for health-related tasks. Similarly, Mitra and et al. (2019) and Gaur et al. (2021) illustrated the utility of external knowledge in dialogue systems, showing improved contextual understanding. Xu et al. (2024) used KGs and LLMs to enhance synthetic clinical data generation, improving performance while address-ing privacy and fairness concerns. (Lymperaiou et al., 2022) reviewed knowledge-enhanced mul-timodal learning, highlighting the role of KGs in improving commonsense, and temporal reasoning in VL models.\nKnowledge distillation (KD) is a widely used technique where a smaller student model mimics a larger teacher model to improve efficiency while retaining performance. In multimodal learning, Wang et al. (2020) demonstrated KD's effective-ness in handling incomplete modalities by distilling knowledge from multimodal models. KD has also been applied in graph-based tasks, such as Hong and Zhen (2023), which improved GNN perfor-mance by capturing community structures.\nIn contrast to prior work, our approach com-bines KD with KI by leveraging KGs, as recent past work on online multimodal toxicity detection is limited by their reliance on training data without incorporating external knowledge sources, poten-tially constraining their ability to capture complex, context-dependent toxicity."}, {"title": "3 Methodology", "content": "Our approach, KID-VLM as illustrated in Figure 1, starts by generating multimodal feature represen-tations, which are learned from a large pre-trained teacher model (e.g., LLaVA). These representations are then enriched by integrating knowledge from ConceptNet, resulting in a knowledge-enhanced representation. The subsequent sections outline the key processes of KD and joint reasoning in greater"}, {"title": "3.1 Multimodal Representation: Aligned & Distilled from LVLM", "content": "We use CLIP as the backbone to extract visual and textual features from meme data. The vision en-coder processes meme images, while two separate text encoders handle captions and OCR text. The Caption Text Encoder processes captions generated by the teacher model (LLaVA), with its weights frozen during training. For each image-text pair D = {Ii, Ti}, captions C\u2081 are generated using the LLaVA model to capture the implicit meme context (prompts are provided in Appendix Table 8). The Caption Text Encoder, Meme Text Encoder, and Vision Encoder generate embeddings for Ci, Ti, and Ii. Embeddings Ti, and Ii are then fused using the Align Fusion method (Kumar and Nandakumar, 2022b), which computes a Feature Interaction Ma-trix (FIM), modeling interactions between image and text feature spaces:\nR = Pi Pt\n(1)\nDiagonal elements of R represent the alignment between image and text embeddings:\nPi Pt = \\frac{\\sum_{j=1}^n R_{jj}}{n}\n(2)\nThis aligned representation, wi, is refined through KD, using a consistency loss defined as the Euclidean distance between the aligned embed-ding wi and the embedding from the teacher model wclip_llava.\nLKD = ||wi - wclip_llava ||2\n(3)\nThis aligns the internal feature representations between the student and teacher models, enabling the student model to capture rich semantic knowl-edge and implicit contextual cues from the larger teacher model."}, {"title": "3.2 Graph-Based Reasoning", "content": "To enrich the distilled multimodal representation with explicit relational knowledge, we use a joint reasoning framework, which extracts working graphs for each data point using concepts from the meme caption and the LVLM-generated caption Ci, and encodes through graph neural networks.\n3.2.1 Construction of the Working Graph\nWe first extract a subgraph Gsub (refer Figure 1) from ConceptNet based on the concepts from the meme caption and the LVLM caption. The nodes in this subgraph are ranked by relevance scoring pr to reduce noise and focus on the most relevant KG entities. pr for each node v \u2208 Vsub (nodes of Gsub) is calculated based on its relationship to the meme context. We create a context node z encapsulating the overall context of the data point. The score is calculated by passing the concatenated text representations of z and v through the Roberta (Liu et al., 2019) model, where the relevance score is determined using the perplexity score:\nPv = fhead(fenc([text(z); text(v)]))\n(4)\nAdditionally, for experimentation, we com-pute the cosine similarity between the concate-nated text representation of z and KG entities v using MiniLM (Wang et al., 2021) (Sentence-Transformer) to obtain the Relevancy Score pv = similarity(z, v).\nWe use these two scores separately for the ex-periments to assess the relevance, retaining the top k (i.e., 750) most relevant entities for inclusion in the working graph Gw (Gworking in Figure 1). Af-ter constructing the initial working graph, we add the context node z to integrate the two sources of knowledge-from the LVLM and KG. This context node is then connected to the entities Vsub from the working graph with new relationship r', forming the final working graph Gw, to prepare for knowl-edge representation learning in the next step.\n3.2.2 Working Graph Knowledge\nRepresentation\nWe experiment with two different graph neural network architectures for reasoning over the joint working graph Gw: the Relational Graph Con-volutional Network (R-GCN) (Schlichtkrull et al., 2017) and the Graph Attention Network (GAT) (Veli\u010dkovi\u0107 et al., 2018). Each architecture is evaluated independently to explore its effective-ness in integrating multimodal representations and KG relationships. While R-GCN applies relation-specific transformations to process messages, GAT uses attention mechanisms to weigh them. In R-GCN, node representations are updated by aggre-gating information from neighboring nodes based on relation-specific transformations:\nh(l+1)v = \u03c3(\\sum_{r \\in \\mathcal{R}} \\sum_{u \\in \\mathcal{N}_r(v)}  W^{(1)}_r h_u)\n(5)"}, {"title": "4 Results & Discussion", "content": "Results on the HateMeme and HarMeme datasets (see Tables 1 and 2) show that by integrating ex-ternal knowledge from ConceptNet and distilling multimodal information through the LLaVA model, KID-VLM framework consistently surpasses base-line models HateCLIPper and standard CLIP archi-tectures across AUC, F1, and Recall with highest improvements of 1.1%, 7%, and 35%, respectively for Hateful Memes dataset.( Dataset details in Ap-pendix A). A similar improvement was observed on the HarMeme dataset, with approximately 3% gains in Accuracy, MA-F1, and Precision.\n4.1 Performance on HateMeme Dataset\nOn the HateMeme dataset, KID-VLM, with Hop 1 sub-graphs and Gated Fusion achieved the highest AUC of 86.11 with accuracy of 78.20%, and F1 score of 78.12, outperforming LLaVA and Hate-CLIPper. In addition, KID-VLM with Hop 2 achieves the highest recall of 76.14% while main-taining the highest AUC of 86.11, the accuracy of 76.30%, highlighting the effectiveness of knowl-edge infusion and distillation. Relevancy scor-ing using MiniLM(Sentence Transformer, another approach used for relevancy scoring apart from Roberta) for the creation of working graph Gw from ConceptNet resulted in a marginal decrease in Hop 1 accuracy (73.80%), but improved preci-sion, recall, and AUC, suggesting enhanced gener-alization. The combination of distilled multimodal representations and external KGs evidently con-"}, {"title": "4.2 Performance on HarMeme Dataset", "content": "On the HarMeme dataset, KID-VLM Hop 2 achieved the best performance with an F1 score of 79.08% and recall of 81.07%, surpassing the baseline HateClipper model. By leveraging Con-ceptNet subgraphs and the KD process, KID-VLM effectively captures the nuances of hateful content, even across varying levels of intensity. Gains in recall, particularly in detecting harmful content, demonstrate the value of external knowledge in meme classification."}, {"title": "4.3 Better Recall using KID-VLM", "content": "Recall plays a critical role in toxicity detection, particularly when the model cannot detect actual toxic content as it may have potentially grave conse-quences. In the Hate Meme dataset, the KID-VLM Hop 2 model achieved a recall score of 76.14%, 35% and 10% higher than HateClipper and MMBT, respectively. This improvement is significant con-cerning real-world scenarios where false negatives (i.e., failing to detect toxic content) can have ad-verse impacts on individuals being exposed to po-tentially toxic content. If such a model were de-ployed by an online platform, this impact could be amplified towards a bigger audience. In this context, a high recall indicates that the KID-VLM model is more adept in identifying the subtle forms of toxicity that might be missed by other models. This is essential for detecting harmful content in nuanced contexts, such as memes that rely on cul-tural references, sarcasm, or visual metaphors. By leveraging external knowledge from ConceptNet and the KD from the teacher model, the model benefits from a broader contextual understanding, which enhances its ability to generalize and detect toxicity across different datasets."}, {"title": "4.4 Impact of Node Count on Performance", "content": "As seen in Table 3, increasing the number of nodes from KG has a noticeable impact on the AUC. When the node count is set to 250, the model achieves an AUC of 84.91, while it improves to 85.76 with 500 nodes. Node count 750 yields the highest AUC of 86.11. Increasing the amount of knowledge extracted from the ConceptNet enables the model to better capture the underlying contex-tual relationships between concepts in the meme, thereby improving its reasoning capability and ulti-mately boosting its performance. While the inclu-sion of additional nodes may likely provide addi-tional context, the relevance needs to be maintained to avoid noise."}, {"title": "5 Conclusion", "content": "In summary, the results demonstrate that the KID-VLM architecture, through its combination of KGs and KD, provides superior performance for de-tecting hate speech in memes compared to base-line methods. The use of ConceptNet subgraphs and distilled multimodal representations allows the model to better understand the complex semantics of multimodal content, resulting in improved ac-curacy, F1, and AUC scores. These findings high-light the critical role of knowledge infusion and advanced fusion mechanisms in enhancing multi-modal hate speech detection systems."}, {"title": "6 Limitations & Future Work", "content": "While KID-VLM demonstrates strong performance on the HateMeme and HarMeme datasets, it has several limitations. The model's reliance on Con-ceptNet for external knowledge may limit its ef-fectiveness in cases where the graph lacks relevant or niche information, and its generalizability to other datasets beyond these two has not been ex-tensively tested. Additionally, incorporating graph-based methods increases computational complexity, which may hinder scalability to larger datasets or real-time applications. The quality of KD from larger models could degrade when using smaller student models, and there remains a potential risk of bias from pretrained models and KGs. Future work could address these concerns by exploring more diverse datasets, improving scalability, and investigating bias mitigation strategies."}, {"title": "7 Ethical Considerations", "content": "The goal of this study is to improve toxicity de-tection in multimodal environments by combining KD from Large Visual Language Models (LVLMs) and knowledge infusion from commonsense KGs. However, toxicity detection systems can mislabel content or users, especially when dealing with com-plex inputs like memes that involve irony or satire. Cultural and social context may not be fully under-stood by the model, so continuous refinement is essential to reduce biases. The study uses publicly available anonymized datasets, but potential mis-use of such systems in surveillance or censorship remains a concern. Responsible usage, with clear guidelines to protect free expression, is critical. Ad-ditionally, the potential for LVLMs to propagate biases from their training data must be addressed through ongoing evaluation and the use of diverse data sources."}, {"title": ""}]}