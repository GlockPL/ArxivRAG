{"title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning", "authors": ["Charles Xu", "Qiyang Li", "Jianlan Luo", "Sergey Levine"], "abstract": "Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), a method that leverages reinforcement learning to generate high-quality training data for fine-tuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40% higher success rates while generalizing better to new tasks. We also provide a detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers a promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers. Videos and code can be found on our project website https://generalist-distillation.github.io/.", "sections": [{"title": "1. Introduction", "content": "Recent advances in robotic foundation models have demonstrated impressive capabilities in understanding and executing diverse manipulation skills (Collaboration et al., 2024; Brohan et al., 2023b;a; Team et al., 2024; Kim et al., 2024; Black et al., 2024; Wen et al., 2024; Liu et al., 2024; Cheang et al., 2024). By leveraging Internet-scale pretraining and grounding with robot actions, these models can achieve zero-shot and few-shot generalization across various domains. Deploying these models typically requires fine-tuning them with task-specific data to adapt to the target task or domain. The quality of this fine-tuning data is therefore critical to the performance of the resulting policies. While human teleoperation is a common and accessible source for such data, human demonstrations often contain inconsistencies in execution quality and style. These variations make it challenging for foundation models to learn robust policies, as they must cope with imperfections and inconsistencies inherent in human demonstrations. This challenge affects all robotic tasks but becomes particularly pronounced in scenarios requiring precise control and dexterity, such as contact-rich manipulation. These tasks demand fine-grained, reactive control to succeed, making the quality and consistency of demonstration data even more crucial for effective policy learning.\nTo tackle this challenge, we propose Reinforcement Learning Distilled Generalist (RLDG), a simple yet effective method that leverages reinforcement learning to generate high-quality training data for robotic foundation models. While directly fine-tuning foundation models with reinforcement learning is possible in principle, it presents significant challenges including optimization instability, computational costs, and potential catastrophic forgetting of pre-trained capabilities, which makes it largely an open problem. Instead, our key insight is that RL agents can autonomously generate high-quality trajectories through reward maximization, making them better suited for fine-tuning generalist policies compared to human demonstrations. The approach is straightforward: we first train vision-based manipulation policies using sample-efficient real-world RL frameworks (Luo et al., 2024b;d) until convergence, then collect data from these policies to fine-tune robotic foundation models. This procedure is simple and flexible, offering several benefits. First, it provides an automated approach to generate large amounts of high-quality training data without requiring the effort of human teleoperation, which is particularly valuable since autonomous RL training is significantly more cost-effective than collecting"}, {"title": "2. Related Work", "content": "Our work bridges reinforcement learning and foundation models for robotics through policy distillation. By combining these approaches, we develop a general technique for training robust robotic policies that leverage both the performance of RL policies and the flexibility of foundation models. Thus, we survey related work across these three key areas and examine their intersections.\nFoundation Models for Robotics. Recent advances in vision-language foundation models have enabled the development of generalist robotic policies that can understand and execute diverse tasks through natural language instructions. Such models (Brohan et al., 2023b;a; Kim et al., 2024; Team et al., 2024; Black et al., 2024; Wen et al., 2024; Liu et al., 2024; Cheang et al., 2024; Driess et al., 2023) leverage large-scale pretraining on Internet-scale vision-language data followed by finetuning on robot demonstrations. While these approaches show impressive generalization capabilities across a wide range of tasks, our experiments demonstrate that they often struggle with precise manipulation tasks that require careful alignment and contact-rich interactions (see Section 4). This challenge comes from fundamental limitations in the demonstration-based learning approach - human demonstrations, while diverse and adaptable, often lack the precision and repeatability needed for contact-rich manipulation tasks. RLDG addresses this limitation by complementing the semantic understanding of foundation models with the robust behaviors learned through reinforcement learning, enabling precise manipulation while maintaining the flexibility and generalization capabilities of foundation models.\nReinforcement Learning for Robotic Manipulation. Reinforcement learning has been successfully applied to learn complex robotic manipulation skills in the real world through direct interaction with the environment (Luo et al., 2024b;d; Rajeswaran et al., 2017; Levine et al., 2016; Hu et al., 2024b; Johannink et al., 2019; Hu et al., 2024a; Rajeswaran et al., 2018; Schoettler et al., 2020). Prior work has demonstrated RL's effectiveness in learning challenging tasks like precision insertion (Luo et al., 2021; Zhao et al., 2022; Luo et al., 2019; 2018), multi-stage assembly (Gupta et al., 2021), and dexterous in-hand manipulation (Hu et al., 2024b). A key advantage of RL is its ability to discover optimal action distributions through trial-and-error exploration, leading to more robust and efficient policies compared to pure imitation learning (Luo et al., 2024a). However, RL policies typically struggle to generalize beyond their training distributions, requiring separate policies to be trained for each task variant or environmental condition.\nHowever, while RL policies can achieve exceptional performance on specific tasks, scaling RL to effectively handle the massive datasets used in foundation model training remains challenging. This is primarily due to computational and algorithmic difficulties in scaling up value function learning and policy optimization to process such large quantities of diverse data. As a result, RL approaches often struggle to match the broad generalization capabilities demonstrated by foundation models trained on Internet-scale datasets. RLDG bridges this gap by combining the strengths of both approaches - using RL to learn optimal behaviors for specific challenging tasks, then distilling these precise capabilities into foundation models while preserving their broad generalization abilities.\nPolicy Distillation and Knowledge Transfer. The idea of distilling multiple specialized policies into a single more general policy has been explored extensively in the RL and robotics literature (Levine and Abbeel, 2014), including methods that use RL to distill into general-purpose neural networks (Rusu et al., 2015; Parisotto et al., 2015), methods that employ bidirectional constraints between specialists and generalists (Teh et al., 2017; Ghosh et al., 2018), and methods that focus on continual learning (Rusu et al., 2016; Schwarz et al., 2018).\nThese approaches have demonstrated that careful distillation can preserve the essential behavioral characteristics of expert policies while potentially adding beneficial properties like improved generalization or reduced computational requirements. While prior work has explored policy distillation in various contexts, our work introduces two key innovations: (1) we show that distilling RL policies into foundation models that leverage large-scale pre-training yields better results than training from scratch, and (2) we demonstrate that for precise manipulation tasks, using RL-generated data for fine-tuning foundation models produces superior performance compared to using human demonstrations, even when high-quality demonstrations are available. Together, these findings establish RLDG as a practical approach for enhancing foundation models with specialized RL capabilities while maintaining their broad generalization abilities."}, {"title": "3. Reinforcement Learning Distilled Generalist", "content": "Reinforcement Learning Distilled Generalist (RLDG) is a simple yet effective method for enhancing generalist policy performance through the distillation of specialized RL policies. In our system, we train RL policies for individual tasks and then use these policies to generate training data that can be used to fine-tune a single generalist robotic manipulation policy, such as OpenVLA (Kim et al., 2024) or Octo (Team et al., 2024). Although specialized RL policies can achieve high performance on specific tasks, they often lack zero-shot generalization and robustness to disturbances. Conversely, generalist policies excel at generalization but can struggle to achieve high performance when trained on human demonstrations, for example due to suboptimal data or modality mismatches between human demonstrators and robot policies (e.g., different viewpoints, memory, and task knowledge). RLDG bridges this gap through knowledge distillation, resulting in more performant generalists compared to finetuning on human demonstrations, while demonstrating stronger generalization capabilities compared to the original RL policies. This distillation approach through data generation with RL is agnostic to both the choice of RL algorithm and generalist policy architecture, making it flexible to any model choice. Furthermore, it offers flexibility to train and collect data with separate RL policies trained on multiple narrowly scoped tasks (such as one policy for each connector in the Connector Insertion task). We can also elect to train RL on the \"bottleneck\" segments of a long-horizon task that require the most precision and benefit the most from RL-generated data, while leaving the less critical parts for humans to demonstrate. This simplifies the RL training complexity, improves data diversity for better generalist performance, and avoids training RL on unnecessarily long-horizon tasks."}, {"title": "3.1. Online RL Training", "content": "We can formulate each robotic task as a Markov Decision Process (MDP), where the state st consists of RGB images and proprioceptive information, and actions at represent desired end-effector movements. The policy objective \u03c0(at|st) is to maximize the expected discounted return:\n$J(\\pi) = \\mathbb{E}\\substack{s_0\\sim p_0\\\\a_t\\sim \\pi(a_t|s_t)\\\\s_{t+1}\\sim P(s_{t+1}|s_t,a_t)} \\sum_{t=0}^{T} \\gamma^t R(s_t, a_t)$\nwhere po defines the initial robot configurations, P represents the system's transition dynamics, and R:S\u00d7A R is a reward function encoding the task objectives.\nWhile RLDG is algorithm-agnostic, we implement RLDG using HIL-SERL (Luo et al., 2024d) motivated by its sample efficiency and high performance for learning precise real-world manipulation skills from pixel input. It incorporates human interventions with RLPD (Ball et al., 2023) to efficiently learn visuomotor policies that consistently achieve 100% success rate by maximizing (1)."}, {"title": "3.2. Experience Collection", "content": "After training RL experts for each of the tasks provided to RLDG, we collect a high-quality fine-tuning dataset by rolling out the converged policies. Since we transfer knowledge from RL into the generalist policy only through this data, we have the flexibility to mix experience from multiple sources. For tasks that involve separate RL policies per manipulation object like Connector Insertion, we rolled out each policy and constructed a balanced fine-tuning dataset consisting of equal number of episodes per object. In cases where RL is only trained on a segment of the task like FMB Assembly, we combine the RL rollouts with human demonstrations for the remainder of the task."}, {"title": "3.3. Generalist Policy Finetuning", "content": "Robot generalist models are often pre-trained on diverse large-scale datasets before being fine-tuned to improve performance its performance on a particular task while preserving the generalization capabilities form the diverse pre-training. In RLDG, we use the data collected as described above to fine-tune these generalist models. Specifically, suppose we have a pre-trained policy \u03c0\u03bf, we fine-tune it with task-specific dataset D(st,at) with the following supervised learning objective:\n$L(\\theta) = -E_{(s_t, a_t)\\sim D}[log \\pi_\\theta(a_t|s_t)]$\nWe showcase the efficacy of our method by fine-tuning two pre-trained robot generalist models using different action parametrization.\nOpenVLA. OpenVLA (Kim et al., 2024) is a 7B-parameter vision-language-action model built on Llama 2 (Touvron et al., 2023). It takes a single image as observation input along with a language instruction. It predicts 7-dimensional actions which"}, {"title": "4. Experiment and Results", "content": "Our experiments aim to evaluate RLDG in terms of its ability to improve over both imitation learning methods for training generalist policies (in terms of performance), and its ability to improve over more specialized RL policies (in terms of generalization). We use a test suite of tasks that require precise and delicate manipulation, and thus are particularly challenging for imitation learning methods. Specifically, we focus on two main questions: (1) Is the RLDG approach for training generalists using data from RL more effective than the conventional approach of training on demonstration data? (2) Is the generalist policy that results from RLDG training more effective at generalizing than the RL policies used to generate the training data?"}, {"title": "4.1. Experimental Setup and Tasks", "content": "Our robot setup for all experiments is shown in Fig. 2. The arm tracks end-effector commands with a 1kHz low-level impedance controller. Data collection, RL, and Octo policies command actions at 10Hz, while OpenVLA runs at 4Hz due to inference speed limitations. The action space for all policies is a 6-dimensional end-effector delta pose in the wrist frame and 1 binary gripper action for tasks that involve grasping. The RL policy's observation space consists of a single 128 \u00d7 128 wrist RGB image along with end-effector pose, velocity, and wrench measurements. For the generalist policies, we fine-tune only using the wrist camera image as input.\nWe evaluate RLDG on four real-world manipulation tasks that present distinct challenges. These include high-precision contact-rich tasks that typically challenge generalist models, pick-and-place tasks where we show RLDG can further improve performance, and multi-stage assembly tasks that leverage RLDG's ability to compose skills. Through these tasks, we also evaluate the method's ability to generalize to unseen configurations.\nConnector Insertion. This task requires inserting various electronic connectors into their matching ports, which requires sub-millimeter precision and dexterity to deal with the intricate contact dynamics during alignment. We train separate RL policies and use them to collect data on USB, Ethernet, and VGA connectors before distilling them into a single generalist policy. We also use Type-C, HDMI, Display Port, and 3-pin XLR connectors to evaluate the policy's zero-shot generalization performance.\nPick and Place. We also test our method on a pick-and-place task, where the robot grasps an object from a randomized location and places it in a bowl. To test generalization, we also evaluate on an unseen scenario by replacing the training object and background as shown in Fig. 3. With this experiment, we aim to demonstrate RLDG's effectiveness"}, {"title": "4.2. RLDG vs. Conventional Fine-tuning", "content": "In this section, we seek to answer Question 1 by comparing generalist policies fine-tuned using RLDG and standard generalist fine-tuning via imitation learning. For each task, we fine-tune OpenVLA and Octo on RL-generated data as described in Sec. 3, and on expert human demonstrations. For a fair comparison, we use the same task setup, training configuration, observation and action space, and the number of successful episodes for both methods. The only difference is the source of the data (RL vs. human). We aim to evaluate whether RL policies are a better source of training data for generalist models than conventional human demonstrations in terms of resultant policy performance.\nSuccess rate. We present the success rate of each policy and method in Fig. 4. On each task, both OpenVLA and Octo fine-tuned with RL-generated data consistently achieved higher success rates than their counterparts trained with human demonstrations, in both seen and unseen evaluation scenarios. On the precise FMB Insertion and Connector Insertion tasks, where we anticipated the generalist to benefit the most from higher quality training data, OpenVLA with RLDG saw 33% and 23% higher success rates, respectively, compared to the baseline. The benefit of RLDG is equally pronounced for Octo, where it improved the success rate by 10% and"}, {"title": "4.3. Generalization of RLDG vs. Original RL Policies", "content": "To address Question 2, we compare the generalization performance of generalists trained using RLDG against that of the original RL policies used to generate the data. As shown in Fig. 4, the RL policy success rate quickly degraded from 20/20 for the training scenario to 1/20 for the unseen scenario of the Pick and Place task. In contrast, OpenVLA and Octo with RLDG achieved 10/20 and 4/20 success rates respectively on the same task. Additionally, the multi-task capabilities of OpenVLA and Octo allowed fine-tuning on multiple connector data in the Connector Insertion task, achieving 73/80 and 50/80, respectively, when evaluated across 4 unseen connectors, whereas the best of the three RL policies trained on single connectors recorded only 49/80 successes.\nOur experimental results on challenging dexterous manipulation tasks demonstrated several key advantages of RLDG. Generalist robot policies trained on RL-generated data using RLDG consistently achieve higher performance across all tasks compared to conventional fine-tuning methods using human demonstrations. Compared to directly using the RL policies that generated the data, RLDG also demonstrated much greater generalization capabilities and robustness to unseen test scenarios."}, {"title": "5. Analysis: why is RL data better than human data?", "content": "We have shown that fine-tuning generalist policies with RL data yields superior performance compared to training on human data. However, it is unclear where these benefits are coming from. In this section, we analyze the source of the benefits in two parts. The first part focuses on studying the benefits of RL actions and the state distribution in RL data in isolation. The second part focuses on dissecting the failure modes of the fine-tuned policies on each individual task."}, {"title": "5.1. Is RL data better because of better action or state distribution?", "content": "To answer this question, we use the FMB insertion task and create a \"mixed\" dataset where we take the human data and relabel the actions using action samples from the RL policy. Comparing the fine-tuning performance of the \u201cmixed\u201d dataset with the purely RL data and the purely human demo data would allow us to see the benefits of the actions and the state distribution in isolation. As shown in Figure 7, mixing human states and RL actions yields a better fine-tuning success rate than using fully human data (more than 50% improvements when fine-tuning on 25/50/75 trajectories), while still being worse than using fully RL data. This suggests that while RL action and state distribution both contribute to the fine-tuning performance improvements, action quality is the factor that contributes to the performance improvement the most. Figure 8 shows a comparison of human and RL actions. We can see that the RL action distribution assigns more density on the correct direction (bottom-left) that moves the end-effector towards the insertion point whereas human action distribution focuses mostly around the middle with a slight bias towards the correct direction. This suggests that RL actions are more optimal than human actions, resulting in the better sample efficiency for fine-tuning we observe in Figure 7."}, {"title": "5.2. Qualitative Analysis: Failure Modes", "content": "To further understand why RL-generated data leads to better performance, we also analyzed failure modes across tasks. We observed that policies trained with RL-generated data consistently helped overcome alignment issues in precise, contact-rich tasks and reduced premature gripper closure during grasping. Videos of each policy on each task can be found on our project website (https://generalist-distillation.github.io/).\nConnector and FMB Insertion. In both tasks, RL data eliminated a \"stuck\" state where the object contacts the board but fails to align properly. Human demonstration policies often maintained contact pressure without necessary exploratory movements. Furthermore, RL data also improved approach trajectories, preventing early descents that caused connectors to catch on socket lips.\nPick and Place. RL data improved grasp reliability, reducing premature gripper closure seen in human demonstration-trained policies. However, an interesting RL-specific failure mode was observed: objects were sometimes dropped too early, bouncing out of the bowl. This likely resulted from RL's speed optimization, where objects were released immediately after clearing the bowl's edge, but the distilled policy lacked precise timing.\nFMB Assembly. While both OpenVLA policies performed similarly in grasping and transport phases as they are trained on the same human data, the performance gap emerged during insertion, with RL data better addressing alignment issues much like in the insertion tasks. Octo's failure was due to consistent grasping errors where the fingers are in front of the object, likely due to the lack of good depth perception."}, {"title": "6. Discussion and Limitations", "content": "In this work, we presented RLDG, a simple method that fine-tunes generalist policies on high-quality data generated by RL policies. We demonstrated that generalist policies fine-tuned using RLDG consistently outperform those trained with human expert demonstrations on a suite of real-world challenging precise manipulation tasks. Our method can be applied in real-world robotic manipulation tasks that require a large amount of training data or where policy performance using human demonstrations saturate. Our work also opens up avenues for making autonomous improvements of generalist policies more scalable and tractable. First, our method assumes access to reward functions for fine-tuning tasks which may present difficulties when the task rewards are hard to specify. Possible future directions include autonomously generating fine-tuning tasks with reward functions (e.g., using pre-trained VLMs) such that there is no need for manual task specification. Furthermore, our RL policies optimize not only for task success but the speed in doing so. Such an objective does not necessarily result in policies that are robust in distillation errors. For example, on the Pick and Place task, the policy fine-tuned on RL-generated data always tries to place the object immediately after it has moved close enough to the goal location but sometimes drops the object too early (see Section 5.2). Nevertheless, we demonstrated that specialist RL policies can be an effective generator of training data for robotic foundation models, and we hope to inspire further research in this domain."}, {"title": "Author Contribution", "content": "Charles Xu contributed to the research design, prepared the hardware setup, performed the implementation and robot experiments, wrote part of the paper, created the paper figures and the website, co-led the project.\nQiyang Li contributed to the research design, performed the analysis in Section 5, wrote part of the paper\nJianlan Luo conceived the project, designed the research, advised the project in terms of overall direction, practical implementation, experiment design, wrote part of the paper, co-led the project\nSergey Levine conceived the project, designed the research, advised the project in terms of overall direction, experiment design, edited the paper"}, {"title": "A. Task Details", "content": "Connector Insertion. The robot starts with the male connector pre-grasped in a 10cm \u00d7 10cm plane 5 cm above the female port. We use the same D-type fixture for the female portion which is fixed to the table at a consistent pose. The task is completed if the robot aligns and inserts the connector into the socket. We train the policies on USB, Ethernet, and VGA connectors, while using Type-C, HDMI, Display Port, and 3-pin XLR connectors to evaluate generalization.\nPick and Place. In this task, the robot starts 15cm above the table at a fixed pose. The target object is randomly placed within a 18cm \u00d7 18cm area on the table and the bowl is at a fixed pose 5cm from the edge of the object randomization region. The robot is to pick up the object and put it into the bowl. To test generalization, we evaluate on an out-of-distribution scenario by replacing the green pepper training object with a yellow corn and changing the tabletop to a beige wood grain surface as shown in Fig. 3.\nFMB Insertion. The robot starts with the insertion object pre-grasped 15 cm above the assembly board, which is randomly placed within a 35cm \u00d7 35cm area with \u00b115\u00b0 rotation. The task is successfully completed if the object is completely inserted into the board. The insertion tolerance in this task is \u00b11.5mm. We used the same set of board poses during each evaluation experiment to ensure consistency across runs.\nFMB Single Object Assembly. In this task, the assembly board is randomized the same way as in the FMB Insertion, but the object is randomly placed in a 3cm \u00d7 7cm grasping area approximately 20cm from the insertion area. The robot starts 15cm above the object at a fixed position. We also use consistent object and board poses during evaluation."}, {"title": "B. Training Procedures", "content": "For FMB Insertion, Connector Insertion, and Pick and Place, we first collect positive and negative samples using our SpaceMouse teleoperation device to train a binary success classifier as the reward function for RL. Then, we initialize the RL buffer with 20 demonstrations and train it on the whole task for 1-3 hours until the policy reaches 100% success rate. Next, we roll out the converged RL policy to collect data for generalist policy fine-tuning, filtering out failed trajectories if any exist. We also collect a set of all successful expert human demonstrations to compare against the RL-generated data.\nFor FMB Single Object Assembly, we use the same reward function as in FMB Insertion and we only train RL on the insertion stage by starting the episode with the object pre-grasped and the arm 15cm above the board within a 5cm \u00d7 5cm randomization region. We periodically adjust the grasp pose during training and data collection to build robustness to variations in the grasp. We also collect the same number of human demonstrations for the insertion stage to compare performance. We then separately collect human demonstrations for the grasping phase, starting the robot above the insertion object and ending the episode when the arm has grasped the object and moved within the insertion randomization region above the board. We combine data from the two stages for fine-tuning.\nWe fine-tuned the OpenVLA weights pre-trained on OXE dataset using LoRA with a rank of 32 applied to each linear layer of the model, which reduces the computational overhead while not sacrificing much performance. We trained using the default fine-tuning configuration with batch size 2 and 3 gradient accumulation steps on a single Nvidia RTX 4090 GPU, which took between 3 to 5 hours to converge. For Octo, we started from the pre-trained Octo-Base model, using the primary image tokenizer to tokenize the wrist camera image and removed the secondary tokenizer. We also mask out the image goal since we do not use it. We applied full fine-tuning with the default hyperparameters and batch size 64 until convergence, which took 3-5 hours on a single Nvidia RTX 4090 GPU."}]}