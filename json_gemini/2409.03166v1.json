{"title": "Continual Skill and Task Learning via Dialogue", "authors": ["Weiwei Gu", "Suresh Kondepudi", "Nakul Gopalan", "Lixiao Huang"], "abstract": "Continual and interactive robot learning is a challenging problem as the robot is present with human users who expect the robot to learn novel skills to solve novel tasks perpetually with sample efficiency. In this work we present a framework for robots to query and learn visuo-motor robot skills and task relevant information via natural language dialog interactions with human users. Previous approaches either focus on improving the performance of instruction following agents, or passively learn novel skills or concepts. Instead, we used dialog combined with a language-skill grounding embedding to query or confirm skills and/or tasks requested by a user. To achieve this goal, we developed and integrated three different components for our agent. Firstly, we propose a novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA), which enables the existing state-of-the-art Action Chunking Transformer [1] model to perform few-shot continual learning. Secondly, we develop an alignment model that projects demonstrations across skill embodiments into a shared embedding allowing us to know when to ask questions and/or demonstrations from users. Finally, we integrated an existing Large Language Model (LLM) to interact with a human user to perform grounded interactive continual skill learning to solve a task. Our ACT-LORA model learns novel fine-tuned skills with a 100% accuracy when trained with only five demonstrations for a novel skill while still maintaining a 74.75% accuracy on pre-trained skills in the RLBench dataset where other models fall significantly short. We also performed a human-subjects study with 8 subjects to demonstrate the continual learning capabilities of our combined framework. We achieve a success rate of 75% in the task of sandwich making with the real robot learning from participant data demonstrating that robots can learn novel skills or task knowledge from dialogue with non-expert users using our approach.", "sections": [{"title": "1 Introduction", "content": "Chai et al.2019 define natural interaction as an interaction between a human and a robot that resembles the way of natural communication between human beings such as dialogues, gestures, etc. without requiring the human to have prior expertise in robotics. The capability of learning tasks and acquiring new skills from natural interactions is desirable for robots as they need to perform unique tasks for different users. One direction of this interaction channel is well studied as instruction following [3, 4, 5], where the robot performs the tasks requested by the human via natural language. Our work focuses on the other side of this communication channel, where the robot starts the conversation with human when it needs their help. This reverse direction of communication plays an important role for robots to learn with non-expert human users as it enables robots to convey their lack of task knowledge to perform tasks in a way that non-expert users can understand. Furthermore, our framework can leverage the feedback from users and learn to perform the task.\nHuman-Robot interaction via language is a well studied problem [2, 4, 5, 6]. Robot agents have been able to interpret language instructions from the human users, and perform visual-motor policies to complete tasks [3, 4, 5]. These methods rely on the emergent behaviors of large models, and do not continually learn new skills or add to their task or skill knowledge. To address this issue, some works have proposed life-long learning for robot agents [7, 8, 6, 9]. Some recent works learn neural visuo-motor skills in a continual setting [9, 10, 11]. However, these approaches are passive and do not query the user for novel skills that the agent might need to complete given tasks.\nWe propose a framework that utilizes dialogue to enable the robot agent to express its need for new skill or task information actively. When encountering a novel task, our robot agent starts a conversation with the human user to learn to execute the task. Throughout the interaction, the robot agent specifies the help that it needs from the human user via natural language, such as a human enacting the skill to find a feasible skill within the existing set of skills to perform the task or requesting multiple robot demonstrations to learn a completely novel skill for this specific task. Our contributions are as follows:\n1. We compare ACT-LORA against the baseline ACT model on few-shot continual learning on RLBench dataset. Our model demonstrates its strong adaptability by achieving 100% success rate on the tasks that it finetuned on with only 5 demonstrations. Furthermore, it achieves an average success rate of 74.75% on the tasks that it is pre-trained on, showing that our policy is effective in preventing catastrophic forgetting.\n2. We present a model that can determine whether a pair of demonstrations of different embodiments, in our case human enactment of a skill or a robot demonstration, are performing the same task. Our alignment model achieves an overall accuracy of 91.4% on the RH20T dataset on aligning demonstrations from humans and robot.\n3. Finally, we conduct a novel two-phase human-subjects experiment with eight participants to show that our system is able to learn to reason over and perform novel skills by interacting with non-expert human users for the task of making a sandwich using dialog. We find a success rate of 75% in making sandwiches for participants where the participants taught a skill in a continual fashion to our robot enabling it to make a sandwich."}, {"title": "2 Problem Formulation", "content": "We formulate a task solving problem where both the robot and the human agent can take actions on their turns. There is a joint physical state $s$ of the world shared by both the human and the robot. In each turn, $n$, either the human or the robot acts, one after the other. Each turn can take longer than one time step, $t$, and continues until the robot or the human indicates a turn to be over. The actions can be physical actions represented by $a_n$, and $a_r$ for the human and the robot actions respectively, or speech acts $l_h$ and $l_r$ for the human and the robot speech respectively for the human-robot grounded dialog. The problem has an initial state $s_0$ and a task $\\Theta$ specified by the human using a speech act $l_0$. Each of these actions updates the joint physical state $s$ of the world, and internal dialog state $s_d$ of the robot. The dialog state is hidden from the human user, but the human receives speech observations for the same. Over multiple turns and actions taken by the human and the robot these physical and robot states update over time. The objective of this turn taking problem is to complete the task $\\Theta$. We measure the task completion rates for this interaction problem. Moreover, in our specific instance of the problem the human also teaches behaviors to the robot, we also measure the success of the individual learned behaviors within the task in simulation."}, {"title": "3 Methods", "content": "The goal of our framework is a robot agent that 1) actively generalizes its known skills to novel tasks when it is applicable; 2) queries the user for unknown skills; and 3) learns new skills with only a few instances. When encountered a task $\\Theta$, the robot agent first searches for a learned skill using semantic representation, which comes from the language embedding of the linguistic description of the skills and tasks. This is a challenging question as the robot needs to know what it does not know. This work is performed by our queryable skill library. If the agent fails to find any usable skill for the task based on the semantic information, it attempts to search for a learned skill using skill representations, which come from human demonstrations and robot trajectories. We developed a novel sample efficient continual skill learning approach ACT-LORA for this task. The robot agent can directly execute the task $\\Theta$ whenever it finds a learned skill that aligns with $\\tau$ in either the semantic space or the skill space, and learns a novel skill to execute the task otherwise. During this whole process, the robot agent needs to interact with the human user based on information from our queryable skill library for which we use an LLM."}, {"title": "3.1 How to Know What the Robot Does not Know a.k.a. a Queryable Skill Library", "content": "The skill library consists of four parts - a text encoder $E_{text}$; a human demonstration encoder $E_{human}$; a robot trajectory encoder $E_{robot}$; and a set of learned skills $S = {S_1,..., S_k}$. Each learned skill $S_i$ is a tuple of a linguistic description and a robot trajectory, denoted as $S_i = (l_i, \\tau_i)$. The linguistic representation $r$ and skill representation $r^{\\ddagger}$ of skill $S_i$ can be obtained by encoding $l_i$ and $\\tau_i$ with the corresponding encoder, denoted as $r = E_{text}(l_i)$, and $r^{\\ddagger} = E_{robot}(\\tau_i)$ respectively.\nFinding a usable skill from the skill library. The skill library is provided two inputs to find an appropriate skill to execute the task $\\Theta$, the linguistic description $l_\\Theta$ and a human demonstration $d_\\Theta$ for the task. We obtain the linguistic or semantic representation and skill representation for the task by encoding the linguistic description and the human demonstration with the corresponding encoders. We then compute two sets of similarity scores between the task $\\Theta$ and any known skill $S_i$ for both the linguistic representation and the skill representation. The state machine within the interaction module of the agent decides the skill to use to execute the task $\\Theta$ based on these scores.\nWe use a pre-trained CLIP as a text encoder $E_{text}$. For $E_{robot}$ and $E_{human}$, we first extract features from each frame using a Resnet-18 [12], and then encode the sequence using a transformer encoder [13]. The robot trajectory encoder $E_{robot}$ and the human demonstration encoder $E_{human}$ are trained to encode the human demonstrations and robot trajectories into the same latent space. The two encoders are jointly trained with tuples $(d, \\tau, y)$, where $d$ denotes human demonstration videos, $\\tau$ denotes robot trajectories, and $y$ is the label of whether the human demonstration and the robot trajectory is in the same task. We use a cosine similarity loss to learn this embedding with a hyper-paramter $\\epsilon$ to act as a margin to declare a human demonstration to be the same skill as a skill the robot knows. More details about learning this embedding space are in the Appendix B."}, {"title": "3.2 Interaction Module using a Large Language Model (LLM)", "content": "The dialog state $s_d$ in our pipeline is maintained with an internal state machine. The state machine uses an LLM, ChatGPT 4 [14], as the natural language generator to produce speech acts for the robot agent. This state machine with the LLM has two major functionalities. Firstly, it interacts with the human user to asks for demonstrations or explanations based on the checks from our queryable skill library. Secondly, the interaction module also interprets the user's language feedback to update the dialog state $s_d$. The interaction module is given the autonomy to continue the dialogue with the user until that it acquires the designated information for the agent. The module can also explain the dialog state $s_d$ with language to the user explaining the robot's confusion."}, {"title": "3.3 ACT-LORA as Visual-motor Policy", "content": "Combining Low-Rank Adaptor with Action Chunking.Adapter-based methods [15, 16, 17, 18] have exhibited promising capabilities of light-weight and data-efficient fine-tuning of neural networks across various domains such as NLP [15, 17], and computer vision [16]. Liu et al. [11] extend Low-Rank Adaptor(LoRA) into robotics with TAIL, enabling a simulated robot to continually adapt to novel tasks without forgetting the old ones. Unfortunately in our experiments TAIL [11] fails to provide high precision control on the robot leading to a lot of failures in even short skills. On the other hand, Action Chunking Transformer(ACT) [1] is capable of performing fine-grained tasks with high precision, but cannot be directly used for continual learning due to catastrophic forgetting. Therefore, we introduce LoRA adaptor to the ACT model, obtaining both the precision from action chunking and the capability of continual learning from the LoRA adaptor. We want to point out that we are using TAIL [11] as the baseline in this work as it is the closest continual learning agent to our approach.\nWe also want to point out that TAIL still requires our queryable skill library to function as a baseline.\nContinual Imitation Learning. Our policy needs to continually learn new skills from demonstrations throughout the agent's lifespan. The robot agent is initially equipped with K skills ${S_1,..., S_K}$. Whenever the robot agent encounters a task that requires a novel skill $S_n, n > K$, it needs to adapt its existing policy $\\pi$ to the novel skill without forgetting any of the existing skills $S\\in {S_1,..., S_{n-1}}$. Provided a number of demonstration trajectories for each skill, the continual learning policy of the robot agent can then be optimized with a behavior cloning loss, which in this case we use $L_1$ loss for action chunks following [1]. On top of the policy of the vanilla ACT model $\\pi_{\\phi}$, the LoRA adaptor introduce a small set of additional low-rank parameters $\\phi_i$ for each skill $S_i$. During the pre-training phase, the additional parameters $\\phi_1,..., \\Phi_K$ for skills $S_1, . . ., K$ are jointly trained with the model's parameter $\\phi$. When we are finetuning with a skill $S_n, n > K$, we freeze the model's original parameters $\\phi$, and only allow gradient updates to the parameters from the task-specific adaptor $\\phi_n$. Such finetuning strategy prevents the policy from catastrophic forgetting the skills that it already possessed when adapting to novel skills."}, {"title": "3.4 Human-Subjects Experiment", "content": "Our human subjects' experiment was on a sandwich making robot domain where the robot does not know all the skills required to make a butter, cheese and lettuce sandwich. Specifically, the robot does not know how to slice cheese. The robotic setup includes a Franka FR3 Robot and three Realsense D435 cameras. We 3D printed tools needed to complete the tasks with planners capable of picking the tools of knife and spatula on demand. We use a 6D Spacemouse to collect data from human participants. Figure 1 demonstrates our sandwich making domain. We used the sandwich-making task for two reasons. Firstly, the sandwich-making task includes a lot of contact-rich and dynamic sub-tasks, such as applying butter and slicing cheese. Secondly, sandwich-making is a multi-step process, allowing the robot agent and the participants to have multiple rounds of conversations. Fake food was used as our ingredients for environmental reasons."}, {"title": "3.4.1 Robotics Domain for Sandwich Making", "content": "Our human subjects' experiment was on a sandwich making robot domain where the robot does not know all the skills required to make a butter, cheese and lettuce sandwich. Specifically, the robot does not know how to slice cheese. The robotic setup includes a Franka FR3 Robot and three Realsense D435 cameras. We 3D printed tools needed to complete the tasks with planners capable of picking the tools of knife and spatula on demand. We use a 6D Spacemouse to collect data from human participants. Figure 1 demonstrates our sandwich making domain. We used the sandwich-making task for two reasons. Firstly, the sandwich-making task includes a lot of contact-rich and dynamic sub-tasks, such as applying butter and slicing cheese. Secondly, sandwich-making is a multi-step process, allowing the robot agent and the participants to have multiple rounds of conversations. Fake food was used as our ingredients for environmental reasons."}, {"title": "3.4.2 Study Design and Measures", "content": "Participants interacted with the robot in two phases. During the first phase - the interaction phase, participants interact with the robot and teach the robot novel skills and task knowledge as they interact with it, this includes dialogue, human demonstration, and robot demonstration. In the second phase - the evaluation phase, participants request the robot to perform the same tasks and evaluate the performance of the robot agent. We needed a two-phase study because we wanted to collect data for skill learning in the first phase and then run a learned policy on the agent in the second phase. The participants came in for another session at least one day apart, allowing 5+ hours of time to train novel skills using user demonstrations. This makes our study two separate 3 \u00d7 1 between-subjects experiment to measure our framework's ability to learn novel skills and task knowledge by interacting with non-expert human users. We do not compare any tasks between the two phases as they were performed on different days and their subjective metrics might be different depending on the subjects' memory of the experience.\nThe objective metrics we used for the human-subjects experiment are as follows. We measured the overall success rate (SR) of completing the entire sandwich and the success rate for completing each independent sub-task. We make a distinction in the evaluation phase for skills that were taught by the participant vs pre-existing skills in our skill library. This demonstrates that we can add new skill without loss of performance to our existing skills. In the post-study survey, we administered the Godspeed Likability sub-scale [19], System Usability Scale (SUS)[20], and the NASA TLX [21]."}, {"title": "3.4.3 Procedure", "content": "The procedure of the study is as follows. Participants first filled out the consent form and a pre-study survey. Then, we handed out a general introduction of the experiment and administered the two phases sequentially. Before each phase, a demonstration video and the instructions for the corresponding phase were provided to the participant. The anonymized instruction manual and the link to these videos are provided in the Appendix and the associated webpage\u00b9."}, {"title": "3.4.4 Hypotheses", "content": "In the human-subjects study, we aim to verify the following hypotheses:\nHypothesis 1 - Our framework allows the robot to learn skills continually with human data while performing better than the existing framework of TAIL.\nHypothesis 2 - Our framework allows the robot to complete the task autonomously when compared to the existing baseline of TAIL.\nHypothesis 3 - Our framework is preferred by the users when compared against a baseline of the robot asking for help and expecting the human to complete the unknown skill."}, {"title": "4 Experimental Results", "content": "In this section, we present three sets of experimental results. Firstly, we present the results of our policy on few-shot continual imitation learning in the simulated RLBench environment [22]. These experiment results show that our behavior cloning model is able to continually to learn novel skills with only few demonstrations and avoid catastrophic forgetting. Then, we evaluate our demonstration alignment model on a subset of the RH20T dataset [23], and demonstrate that we are able to project demonstrations from different embodiments into the same latent space. Lastly, we present the results for the human subject study, which demonstrate that our framework can learn symbols and skills to execute task from interacting with true human users."}, {"title": "4.1 Experiments on Continual Imitation Learning", "content": "We evaluate our policy on few-shot continual imitation learning using the RLBench environment [22]. A total of 14 skills are chosen from the pre-defined skills of the environment, 8 for pre-training and 6 for continual training. We use 1000 demonstrations for each of the pre-training skills for training during the pre-training phase, and 5 demonstrations for each of the continual training skills in the continual training phase. The SoTA visual policy model ACT [1] and SoTA continual policy learning model TAIL [11] were chosen as the baselines for comparison against our model. We evaluate all models for 50 times on each of the 14 skills to measure the success rate. Our model learns novel skills with 100% accuracy while maintaing its pre-trained performance at 74.75% demonstrating its suitability for continual learning. We observed TAIL [11] to fail in tasks which require precision, and ACT fail to remember older skills. We evaluated these models with just one random seed due to computational challenges."}, {"title": "4.2 Experiments with our alignment model", "content": "Our alignment model is evaluated on a subset of the RH20T dataset [23], which includes robot trajectories for diverse range of tasks and their corresponding human demonstration videos. Our alignment model achieves 91.4% in overall accuracy in distinguishing whether a pair of demonstrations are performing the same task. Detailed results are in Appendix A. This high performing alignment model allows our robot to ask demonstration queries in our user study."}, {"title": "4.3 Human Subject Study Results", "content": "We conducted an IRB approved study with 8 participants and 6 pilot subjects. Only one of the subjects was female (12.5% of the user study). The age demographic of our users is 25.25\u00b13.059. The subjects spent 120 minutes in the interaction phase and then another 75 minutes for the evaluation phase. They were compensated with a $35 Amazon gift card.\nWe present our objective success rate results in Tables 2, 3. Our Hypothesis 1 is supported by table 2 as our ACT-LORA when compared to TAIL [11] learns the novel skills (100% vs 5%) and existing skills (74.75% vs 0.25%) at a higher success rate. This allows better acquisition of novel skills few shot in a real robotics domain. We expected TAIL to perform better but we noticed it had issues with long horizon precise skills. Note that the doing model does not learn any new skills.\nOur Hypothesis 2 is supported by Table 2 demonstrating that our LoRA Act model completes the entire sandwich at a higher rate than TAIL [11] which is the learning based approach. Notice that the Doing model has a similar completion rate as even though the human can help with some tasks the robot might still fail in skills it has learned previously such as grasping bread.\nWe reject Hypothesis 3 as we did not notice any preferences in Workload, System Usability or Likebility between our learning agent using ACT-LoRA and the Doing agent that always asks for help from users. While we thought that users would prefer an autonomous agent, subjects actually preferred helping the agent as humans can make a sandwich with trivial ease where as the robot performs the task at a much slower rate."}, {"title": "5 Related Work", "content": "Skill Discovery and Continual Learning. The area of visuo-motor continual learning is getting a lot of attention recently [9, 10, 11]. Wan et al. [9] discover new skills from segments of demonstrations by unsupervised incremental clustering. Xu et al. [10] learn the skill representation by aligning skills from different embodiments, and can re-compose the learned skills to complete a novel combination. Liu et al. [11] introduce task-specific adapters using low-rank adaptation techniques [15], preventing the agent from forgetting the learned skills when learning the new skills. However, these frameworks assume the presence of the demonstrations for the new tasks, and only discover skills in a passive fashion. Our proposed framework actively reasons and requests the human users for the demonstrations of the unseen skills while performing the ones it knows. This reasoning is done in two stages: first the human enacts the behavior, once the robot has seen this behavior it decides if it can perform the enacted behavior or not. After this reasoning the robot can choose to source demonstrations from the human using a joystick. This is a more natural setup for a language enabled continual learning agent in the real world. Furthermore, our agent requires less than ten demonstrations from the user to discover the new task without forgetting any of the learned skills which is an improvement over existing passive continual learning methods [11, 9].\nHuman-Robot Dialogue. Human-Robot dialog is a mature problem [24, 25, 26, 27]. Traditional methods use statistical algorithms with a pre-defined grammar, such as semantic parsing [26, 25], to connect the semantics of the dialogue to the environment's perceptual inputs. On the other hand, recent advancements in natural language processing (NLP) have led to Large Language Models (LLMs) that process natural language in free form. Grounded with perceptive inputs from the environment, these LLMs have been used in robotics research generate executable plans [3]. Furthermore, Ren et al. [28] and Dai et al. [24] use LLMs to ask for human feedback for the robot agents demonstrating the importance of dialog. However, these approaches leverage planning with LLMs where as we are attempting to learn continuous visuo-motor skills on the robot by asking for help.\nActive Learning. Our work is related to active learning, where a learning agent actively improves its skills by asking a human for demonstrations [26, 29, 30, 31]. Defining an appropriate metric that triggers the request for assistance or information gathering becomes the key research problem in this domain. Thomason [26] measure the semantic similarity between a newly introduced concept and the known concepts to ask for classifier labels. Chernova and Veloso [30, 31] train a confidence classifier conditioned on the current state of the agent, and request expert demonstrations when the confidence score does not meet a pre-defined threshold. Maeda et al. [29] use the uncertainty of Gaussian Processes(GPs) as the metric to trigger the request for assistance. These existing methods reason over the semantic information in a task such as the goal condition or features of classifiers that identify the goal condition. We use a cosine distance metric to measure similarity for both the semantic information from language and the behavior information of a skill."}, {"title": "6 Limitations", "content": "We present an approach to teach skills to robots using techniques from active learning and continual learning while using language as a modality to query and reason over the skills known to the agent. We acknowledge that we need to conduct a wider user study with more subjects over a larger number of cooking tasks using our approach. The turn-taking in our framework is tightly controlled, and not dynamic. Our ACT-LORA approach while being sample efficient has been observed to have issues with heterogeneous demonstrations. We also want to compare such continual learning approaches with pre-trained policy approaches such as RT [4] to scale up the policy learning approach while maintaining sample efficiency allowing for novice users to personalize skills for their robots."}, {"title": "7 Conclusion", "content": "In conclusion, we present a novel framework for robot agents to learn task relevant knowledge and skills from interactions with human users. To the best of our knowledge this is the first work to demonstrate skill learning while querying a user with dialog to express doubt. By maintaining metrics in semantic and skill similarity, our agent can actively interact with human users and adapt its known skills to novel tasks. Furthermore, our framework is able to learn a completely new visual-motor skill (at 100%) with only a few robot demonstrations, without affecting the performance of any existing skills (at 74.75%)fulfilling continual learning requirements in robotics. Finally, we conducted a human-subjects experiment to demonstrate our framework's ability to complete tasks such as sandwich making from interactions with participants at a 75% task success rate."}, {"title": "A Experiment Details", "content": "We present the detailed results of the alignment model in Table 4. We conduct five-split evaluation on the dataset, and report the mean score and standard deviation of each metric. Each model is trained on 80% of the trajectories and evaluated on the other 20%. In total, we use 1240 robot trajectories and 1193 human demonstrations across 98 tasks of the RH20T dataset configuration 5. As shown in Table 4, our model achieves 91.8% on the $F_1$ metric, and 91.4% on the overall accuracy metric. This strong performance of the alignment model enables the robot agent to actively adapt learned skills to perform novel task, or to understand that it needs to learn a novel skill from seeing a single human demonstration."}, {"title": "A.1 Detailed results of the alignment model on RH20T", "content": "We present the detailed results of the alignment model in Table 4. We conduct five-split evaluation on the dataset, and report the mean score and standard deviation of each metric. Each model is trained on 80% of the trajectories and evaluated on the other 20%. In total, we use 1240 robot trajectories and 1193 human demonstrations across 98 tasks of the RH20T dataset configuration 5. As shown in Table 4, our model achieves 91.8% on the $F_1$ metric, and 91.4% on the overall accuracy metric. This strong performance of the alignment model enables the robot agent to actively adapt learned skills to perform novel task, or to understand that it needs to learn a novel skill from seeing a single human demonstration."}, {"title": "A.2 Detailed results of the continual learning policy on each task of the RLBench", "content": "We present the per-task success rate of the policies in the RLBench simulator. Table 5 shows the performance of the three policies on each pre-trained task after fine-tuning, and Table 6 demonstrates the performance of the policies on the tasks that they are finetuned on. All the three models are trained to predict joint positions for the same number of gradient steps. During the pre-train phase, each model is trained with 1000 robot demonstrations from each pre-train task for 1000 epochs. In the fine-tune phase, each model is trained with 5 robot demonstrations from each fine-tune task for 20000 epochs. Notice that due to the limitation of the visual-motor policies, we use a static location for all the finetune tasks during both training and evaluations. However, for all the pre-train tasks, we use randomized locations during both training and evaluation. As presented in the tables, TAIL achieves a near 0% success rate on majority of the tasks except for close fridge. This is because that close fridge is a relatively easier task in the environment, and the agent has a non-trivial chance to accidentally hit the fridge door and close it even if it is doing random behaviors."}, {"title": "A.3 Detailed results of the human-subjects study", "content": "We describe the details of the human-subjects study. Our human-subjects study is approved by the Institutional Review Board(IRB) of the university. We tested the study with 5 pilots before conducting the experiments on the participants. We fixed the issues of unclear instructions, short execution times for the learned skills and ambiguous phrases when the LLM was asking questions. We had to finetune the prompts of the LLMs a lot so the robot asked questions pertinant to the task of sandwich making.\nFor the actual study a total of 10 participants were recruited through campus advertisements. We rejected the 2 of these users for the following reasons respectively. One user was over-excited to interact with the talking robot agent, and requested the robot to perform tasks that are not in our instructions forcing us to stop the study. These tasks were impossible to complete using the configurations of the study. The other user did not fully understand the instructions, and crashed the robot into the table forcing us to stop the study.\nThe study is composed of two separate phases, the interaction phase that takes 150 minutes and the evaluation phase that takes 60 minutes, with a voluntary participation. The participants, including the pilots, are compensated with $35 Amazon gift card for their time. We designed the two-phase study for two major reasons. Firstly, the learning agent requires five hours to train for the novel skill. Secondly, we want to demonstrate a thorough comparison for the workload between our learning agent and the doing agent in the two phases. The learning agent requires the users to remotely control the robot arm to perform the task in the interaction phase, and is fully automated in the evaluation phase, whereas the doing agent behaves the same in both phases by requesting the users to directly perform the task that it does not know. We hypothesize that the users experience higher workload for our learning agent than the doing agent in the interaction phase, and a lower workload for the learning agent than the doing agent in the evaluation phase because we consider that for remotely controlling the robot arm to complete the task requires higher workload than directly completing the task themselves for the users, and the fully automated robot agent requests the least workload. We reject our hypothesis and accept the null hypothesis of \u2013 there is no difference in the users's perception of workload, likeability, anthropomorphism, and perceived intelligence between our method and a baseline where the robot just asks for help. There are two major reasons for this result. Firstly, the human users take much shorter time to complete the task than the robot agent. From the users' perspective, despite being fully automated, the agent is not saving their time by doing the task for them because they have to sit and watch the agent doing the task. This issue can be addressed by assigning distractor tasks for the human users [25], which could not be achieved due to the limited time. Another reason is that a robot that asks for help seems more intelligent and human like compared to an autonomous sandwich making robot. Future studies will have to weigh these design decisions more carefully. We do perform better than the baseline of TAIL here but TAIL generally fails at completing the task itself which makes the result unsurprising. A more elaborate study with more choices and more participants can reduce the variance in the results."}, {"title": "A.3.1 Study details", "content": "We describe the details of the human-subjects study. Our human-subjects study is approved by the Institutional Review Board(IRB) of the university. We tested the study with 5 pilots before conducting the experiments on the participants. We fixed the issues of unclear instructions, short execution times for the learned skills and ambiguous phrases when the LLM was asking questions. We had to finetune the prompts of the LLMs a lot so the robot asked questions pertinant to the task of sandwich making.\nFor the actual study a total of 10 participants were recruited through campus advertisements. We rejected the 2 of these users for the following reasons respectively. One user was over-excited to interact with the talking robot agent, and requested the robot to perform tasks that are not in our instructions forcing us to stop the study. These tasks were impossible to complete using the configurations of the study. The other user did not fully understand the instructions, and crashed the robot into the table forcing us to stop the study."}, {"title": "A.3.2 Detailed procedure", "content": "We describe the detailed procedure for the study as follows.\nInteraction Phase. Participants first filled out the consent form and a pre-study survey. Then, we handed out a general introduction of the experiment. The participants were then asked to read the instructions for the interaction phase, and watch a demonstration video. The demonstration video introduces how the robot agent requests for different types of help differently, and how to answer different requests from the robot agent. We use a completely different domain(Opening a washing-machine) as example in the demonstration video. The instruction introduces domain relevant information, such as the configuration of the robot's workspace, the sandwich to make, and the steps to make the sandwich. The anonymized instructions and the video can be found on the associated webpage. Then, the participants interacted with the three agents, the dumb agent, the doing agent, and the learning agent, in a random order. The dumb agent never interacts with the users except for getting the initial instruction set from the user. The doing agent always asks the human users for help when it encounters any task that it is uncertain with. The learning agent interacts with the human users by asking task-relevant questions, asking for human demonstration, and asking for robot demonstrations. The details of how the learning agent behaves when encountered an unknown task can be found in Appendix B. After interacting with each system, the participants were asked to fill-out a post-survey, including questions from NASA-TLX [21", "20": "and 4 sub-scales from the GodSpeed Questionnaire Series [19"}]}