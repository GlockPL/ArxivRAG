{"title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Liqiang Nie"], "abstract": "Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA) dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.", "sections": [{"title": "1. Introduction", "content": "Enabling agents to learn human behavioral patterns for completing complex tasks in open-world environments, is a long-standing goal in the field of artificial intelligence. To effectively handle diverse tasks in an open-world environment like Minecraft , a promi-nent agent framework integrates a task planner with a goal-conditioned policy. As illustrated in Figure 1 (left), this framework first utilizes the task planner's language comprehension and visual perception abilities to decompose complex task instructions into sequential sub-goals. These sub-goals are then processed by a goal-conditioned policy to generate actions.\nAlthough existing agents have made promising progress by using Multimodal Large Language Models (MLLM) as planners, the current performance bottleneck for agents lies in the improvement of the goal-conditioned policy [24]. As the sub-goal serves as a natural language description of an observation-action sequence, the goal-conditioned policy needs to learn the crucial relationships among sub-goals, observations, and actions to predict actions. However, existing goal-conditioned policies exhibit the following limitations: (1) Existing policies neglect the modeling of the relationship between observations and actions. As shown in Figure 1, they only model the relationship between the sub-goal and the current observation by adding the sub-goal embedding to the observation features [3, 25, 42]. However, the current observation is generated by the previous action interacting with the environment. This implies a causal relationship between action and observation, which is neglected by current policies; (2) Existing policies struggle to model the relationship between open-ended sub-goals and observation-action sequences. As depicted in Figure 1, existing policies primarily rely on either video encoders or conditional variational autoencoders (CVAE) as goal encoder to produce implicit goal embeddings. Such embeddings have limited representation ability [42]. Simply adding it to observation features is sub-optimal and unable to handle the complex relationship between sub-goals and observation-action sequences.\nIn this paper, we propose Optimus-2, a novel agent that incorporates an MLLM for planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP). To address the aforementioned challenges, we propose GOAP, which can better model the relationship among the observations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-action sequence modeling. To capture the relationship between observations and actions, the Action-guided Behavior Encoder first employs a Causal Perceiver to integrate action embeddings into observation features. It utilizes task-relevant action information as guidance to adjust the observation features, thereby providing fine-grained observation-action information for action prediction. Additionally, to model a long-term observation-action sequence without exceeding input length limitations, a History Aggregator is introduced to dynamically integrate current observation-action information with the historical sequence into fixed-length behavior tokens. Behavior tokens can capture the long-term dependencies of the observation-action sequence with a fixed and appropriate length. It enables the agent to predict actions that align with the logic of the observation-action sequence, rather than making isolated action predictions based solely on the current observation.\nAn MLLM to model the relationship between sub-goal and observation-action sequence. To explicitly encode the semantics of sub-goals, we introduce an MLLM as the backbone of GOAP. It aligns the sub-goal with behavior tokens to predict subsequent actions auto-regressively. Leveraging the MLLM's language comprehension and multimodal perception capabilities, it can better integrate fea-tures from open-ended sub-goals and observation-action sequences, thereby enhancing the policy's action prediction ability. To the best of our knowledge, GOAP is the first effort to employ MLLM as the core architecture of a Minecraft policy, which demonstrates strong instruction comprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack alignment among essential elements or are not publicly accessible, resulting in a significant scarcity of high-quality observation-goal-action pairs necessary for policy training. To this end, we introduce an automated approach for constructing the Minecraft Goal-Observation-Action (MGOA) dataset. The MGOA dataset comprises 25,000 videos across 8 atomic tasks, providing approximately 30 million aligned observation-goal-action pairs. It will be made openly available to support advancements within the research community. We conducted comprehensive evaluations in the open-world environment of Minecraft, and the experimental results demonstrate that Optimus-2 achieves superior performance. Compared to previous SOTA, Optimus-2 achieves an average improvements of 27%, 10%, and 18% on atomic tasks, long-horizon tasks, and open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n\u2022 We propose a novel agent Optimus-2, which consists of an MLLM for planning, and a policy for low-level control. The experimental results demonstrate that Optimus-2 exhibits superior performance on atomic tasks, long-horizon tasks, and open-ended sub-goal tasks.\n\u2022 To better model the relationship among the observations, actions, and sub-goals, we propose Goal-Observation-Action Conditioned Policy, GOAP. It contains an Action-guided Behavior Encoder for observation-action sequence modeling, and an MLLM to model the relationship between sub-goal and observation-action sequence.\n\u2022 To address the scarcity of large-scale, high-quality datasets, we introduce the MGOA dataset. It comprises approximately 30 million aligned observation-goal-action pairs and is generated through an automated process without any manual annotations. The proposed dataset construction method and the released MGOA dataset can contribute to the community's efforts to train agents."}, {"title": "2. Related Work", "content": "Minecraft Agents. Previous works have constructed policies in Minecraft using reinforcement learning or imitation learning. VPT was training on large-scale video data recorded by human players, using behavior cloning to mimic human behavior patterns. GROOT employs a video encoder as a goal encoder to learn semantic information from videos. However, these policies rely solely on visual observations as input and cannot follow human instructions to accomplish specific tasks. MineCLIP"}, {"title": "3. Preliminaries and Problem Formulation", "content": "In Minecraft, agents exhibit behavior patterns similar to humans: at each time step t, the agent receives a visual observation ot and generates control actions at+1 using the mouse and keyboard. These actions interact with the environment, resulting in a new visual observation ot+1. Through continuous interactions, a trajectory \\( J = \\{(o_1, a_1), (o_2, a_2), (o_3, a_3), ..., (o_T, a_T)\\} \\) is formed, where T represents the length of the trajectory. Previous work primarily trained Minecraft agents using reinforcement learning or behavior cloning [3, 25]. For example, in behavior cloning, the goal of the policy \\( p_{\\theta}(a_{t+1}|o_{1:t}) \\) is to minimize the negative log-likelihood of the actions at each time step t given the trajectory J. Considering that such trajectories are typically generated under explicit or implicit goals, many recent approaches condition the behavior on a (implicit or explicit) goal g and learn goal-conditioned policy \\( p_{\\theta}(a_{t+1}|o_{1:t}, g) \\). Generally, for both agents and humans, the explicit goal g is a natural lan-"}, {"title": "4. Optimus-2", "content": "In this section, we first give an overview of our proposed agent framework, Optimus-2. As shown in Figure 1 (left), it includes a planner for generating a series of executable sub-goals and a policy that sequentially executes these sub-goals to complete the task.\nNext, we introduce how to implement Optimus-2's planner (Sec. 4.1). Subsequently, we elaborate on how to implement the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we introduce an automated dataset generation method to obtain a high-quality Minecraft Goal-Observation-Action dataset (MGOA) for training GOAP."}, {"title": "4.1. MLLM-based Task Planner", "content": "In Minecraft, a complex task consists of multiple intermediate steps, i.e., sub-goals. For example, the task \"I need a wooden pickaxe\u201d includes five sub-goals: 'chop a tree to get logs', 'craft four planks', 'craft a crafting table', 'craft two sticks /', and 'craft a wooden pickaxe'. Therefore, a planner is essential for the agent, as it needs to decompose the given complex task into a sequence of executable sub-goals for the policy to execute sequentially. In this paper, we follow Li et al. [24], employing an MLLM as the planner, which takes current observation and task instruction as input to generate sub-goals."}, {"title": "4.2. Goal-Observation-Action Conditioned Policy", "content": "According to Sec 3., a key insight into the relationship among observation o, action a, and sub-goal g is: that the observation o and action a at the same time step have a causal relationship; and the sub-goal g is a natural language description of the observation-action sequence over a certain time. To better model the relationships among the three elements mentioned above, we propose first integrating the representations of observation and action at each time step, then modeling the observation-action sequences along the temporal dimension, and finally aligning the observation-action sequences with the sub-goal for action prediction.\nMotivated by this, we propose a novel Goal-Observation-Action conditioned Policy, GOAP. As shown in Figure 2, our GOAP consists of an Action-guided Behavior Encoder that dynamically models observation-action sequences into fixed-length behavior tokens and an MLLM that aligns such behavior tokens with sub-goal for action prediction."}, {"title": "4.2.1. Action-guided Behavior Encoder", "content": "Previous policies often overlook the causal relationship between observation and action at each timestep. Moreover, it remains a challenge to model the long-term observation-action sequence without exceeding input length constraints. To this end, we propose an Action-guided Behavior Encoder that integrates the representations of observation and action at each time step and then dynamically models the historical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation \\( o_t \\) into a visual encoder \\( V_E \\) to obtain the visual features:\n\\[ u_t \\leftarrow V_E(o_t) \\]\nwhere \\( v_t \\in R^{P \\times d} \\), P is the number of patches for each image, and d is the dimension of the extracted image feature. In practice, we employ ViT [8] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model the relationship between observations and actions. It takes the visual feature vt as query tokens and the action embedding at as key and value. The module then constructs the information interaction between action at and vt through a cross-attention mechanism:\n\\[ Q = v_tW^Q, K = a_tW^K, V = a_tW^V \\]\n\\[ \\hat{v}_t = CrossAttn(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d}})V \\]\nwhere \\( W^Q \\), \\( W^K \\), and \\( W^V \\) represent the weight matrices for the query (Q), key (K), and value (V), respectively. CrossAttn() denotes the cross-attention layer, and d is the dimension of the image features. In this way, it explicitly assigns action information at at time step t to the visual features \\( \\hat{u}_t \\), enhancing the causal relationship between observations and actions.\nSubsequently, we introduce a History Aggregator module to capture the information of the observation-action sequence along the temporal dimension, serving as the behavior representation. At each timestep t, behavior tokens Bt serve as queries, while the sequence of historical behavior"}, {"title": "4.2.2. MLLM Backbone", "content": "To model the relationship between the sub-goal and observation-action sequence, we introduce an MLLM that takes the sub-goal g, current observation features vt, and behavior tokens Bt as input to predict subsequent actions auto-regressively. To enable the MLLM backbone MLLM to predict low-level actions, we employ VPT [1] as action head AH to map output embeddings \\( \\bar{a}_{t+1} \\) of language model into the action space.\n\\[ \\bar{a}_{t+1} \\leftarrow \\text{MLLM}([g, v_t, B_t]) \\]\n\\[ a_{t+1} \\leftarrow \\text{AH}(\\bar{a}_{t+1}) \\]\nFormally, given a dataset \\( D = \\{(o_{1:T}, a_{1:T})\\}_M \\) with M complete trajectories, we train GOAP to learn the behavior distribution from D via behavioral cloning. Moreover, we introduce a KL-divergence loss to measure the output distribution similarity between GOAP and VPT [1]. This helps our model effectively learn the knowledge from the teacher model VPT. The training loss can be formulated as follows:\n\\[ L = \\lambda_{BC} \\sum_{t=1}^T - \\log p_{\\theta}(a_{t+1}|o_{1:t}, A_{1:t}, g) \\\\\n+ \\lambda_{KL} \\sum_{t=1}^T D_{KL}(q_{\\phi}(a_{t+1}|o_{1:t}) || p_{\\theta}(a_{t+1}|o_{1:t}, g)) \\]\nwhere \\( \\lambda_{BC} \\) and \\( \\lambda_{KL} \\) are trade off coefficients, \\( p_{\\theta} \\) is the GOAP, \\( q_{\\phi} \\) is the teacher model."}, {"title": "4.3. MGOA Dataset", "content": "In Minecraft, there remains a significant lack of high-quality goal-observation-action pairs to support behavior cloning training. Previous work has primarily relied on gameplay videos as training data. These datasets either lack natural language instructions (explicit goals) or use actions predicted by IDM models [1] for each observation as pseudo-labels [1, 25], which leads to a risk of misalignment between observations and actions. Inspired by Li et al. [24], we propose an automated data generation pipeline that enables the creation of aligned goal-observation-action pairs without the need for manual annotations or human contractors. First, we utilize existing agents [25], providing them with clear natural language instructions to attempt task completion in Minecraft. We then record the actions and corresponding observations during goal execution, generating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply the following filtering criteria: 1) only recording videos in which the task is successfully completed, and 2) discarding videos where task execution takes an excessive amount of time. For more details, please refer to Sup. C. Through this automated approach, we obtained 25k high-quality Minecraft Goal-Observation-Action (MGOA) dataset. A comparison of the MGOA dataset with the existing Minecraft datasets is shown in Table 1. Our automated data generation pipeline offers several key advantages: 1) it enables the generation of aligned goal-observation-action pairs without the need for manual annotation or pseudo-labeling; 2) its construction process is parallelizable, allowing for rapid dataset generation; and 3) it leverages local agents for data generation, resulting in low-cost production."}, {"title": "5. Experiments", "content": "Environment. Following , we conduct experiments in the complex, open-world environment of Minecraft on the MineRL [11] platform. The agent interacts with the MineRL environment at 20 frames per second, generating low-level control signals for the mouse and keyboard. For each task execution, the agent is initialized in a randomized"}, {"title": "5.4. Visualization of Behavior Representation", "content": "As shown in Figure 6, we apply t-SNE [38] to visualize observation features extracted by ViT [8], MineCLIP [9], and the Action-guided Behavior Encoder for four tasks. From (a) and (b) in Figure 6, it is evident that the behavior representations extracted by ViT and MineCLIP are highly mixed, making it challenging to delineate the boundaries between different tasks. This lack of clear distinction between task-specific behavior representations can hinder the model's ability to understand the unique behavior patterns associated with each task, potentially leading to task failure. In contrast, the visualization in (c) of Figure 6 reveals clear, distinct clusters for each task, demonstrating that the Action-guided Behavior Encoder effectively captures subtle differences in observation-action sequences, thereby learning robust behavior representations across tasks."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel agent, Optimus-2, which can excel in various tasks in the open-world environment of Minecraft. Optimus-2 integrates an MLLM for high-level planning and a Goal-Observation-Action conditioned Policy (GOAP) for low-level control. As a core contribution of this paper, GOAP includes an Action-guided Behavior Encoder to model the observation-action sequence and an MLLM to align the goal with the observation-action sequence for predicting subsequent actions. Extensive experimental results demonstrate that GOAP has mastered various atomic tasks and can comprehend open-ended language instructions. This enables Optimus-2 to achieve superior performance on long-horizon tasks, surpassing existing SOTA. Moreover, we introduce a Minecraft Goal-Observation-Action dataset to provide the community with large-scale, high-quality data for training Minecraft agents."}]}