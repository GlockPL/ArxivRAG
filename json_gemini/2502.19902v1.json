{"title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Liqiang Nie"], "abstract": "Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA) dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.", "sections": [{"title": "1. Introduction", "content": "Enabling agents to learn human behavioral patterns for completing complex tasks in open-world environments, is a long-standing goal in the field of artificial intelligence [4, 23, 34, 45]. To effectively handle diverse tasks in an open-world environment like Minecraft [19, 32], a promi-\n*Corresponding authors\nnent agent framework [24, 32, 40, 41] integrates a task planner with a goal-conditioned policy. As illustrated in Figure 1 (left), this framework first utilizes the task planner's language comprehension and visual perception abilities to decompose complex task instructions into sequential sub-goals. These sub-goals are then processed by a goal-conditioned policy to generate actions.\nAlthough existing agents [24, 32, 41] have made promising progress by using Multimodal Large Language Models (MLLM) as planners, the current performance bottleneck for agents lies in the improvement of the goal-conditioned policy [24]. As the sub-goal serves as a natural language"}, {"title": "An Action-guided Behavior Encoder for observation-action sequence modeling.", "content": "To capture the relationship between observations and actions, the Action-guided Behavior Encoder first employs a Causal Perceiver to integrate action embeddings into observation features. It utilizes task-relevant action information as guidance to adjust the observation features, thereby providing fine-grained observation-action information for action prediction. Additionally, to model a long-term observation-action sequence without exceeding input length limitations, a History Aggregator is introduced to dynamically integrate current observation-action information with the historical sequence into fixed-length behavior tokens. Behavior tokens can capture the long-term dependencies of the observation-action sequence with a fixed and appropriate length. It enables the agent to predict actions that align with the logic of the observation-action sequence, rather than making isolated action predictions based solely on the current observation."}, {"title": "An MLLM to model the relationship between sub-goal and observation-action sequence.", "content": "To explicitly encode the semantics of sub-goals, we introduce an MLLM as the backbone of GOAP. It aligns the sub-goal with behavior tokens to predict subsequent actions auto-regressively. Leveraging the MLLM's language comprehension and multimodal perception capabilities, it can better integrate fea-"}, {"title": "3. Preliminaries and Problem Formulation", "content": "In Minecraft, agents [1, 3, 25] exhibit behavior patterns similar to humans: at each time step t, the agent receives a visual observation $o_t$ and generates control actions $a_{t+1}$ using the mouse and keyboard. These actions interact with the environment, resulting in a new visual observation $o_{t+1}$. Through continuous interactions, a trajectory $\\mathcal{J} = \\{(o_1, a_1), (o_2, a_2), (o_3, a_3), ..., (o_T, a_T)\\}$ is formed, where T represents the length of the trajectory. Previous work primarily trained Minecraft agents using reinforcement learning [9] or behavior cloning [3, 25]. For example, in behavior cloning, the goal of the policy $p_{\\theta}(a_{t+1}|o_{1:t})$ is to minimize the negative log-likelihood of the actions at each time step t given the trajectory $\\mathcal{J}$. Considering that such trajectories are typically generated under explicit or implicit goals, many recent approaches condition the behavior on a (implicit or explicit) goal g and learn goal-conditioned policy $p_{\\theta}(a_{t+1}|o_{1:t}, g)$ [3, 25]. Generally, for both agents and humans, the explicit goal g is a natural lan-"}, {"title": "4. Optimus-2", "content": "In this section, we first give an overview of our proposed agent framework, Optimus-2. As shown in Figure 1 (left), it includes a planner for generating a series of executable sub-goals and a policy that sequentially executes these sub-goals to complete the task.\nNext, we introduce how to implement Optimus-2's planner (Sec. 4.1). Subsequently, we elaborate on how to implement the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we introduce an automated dataset generation method to obtain a high-quality Minecraft Goal-Observation-Action dataset (MGOA) for training GOAP."}, {"title": "4.1. MLLM-based Task Planner", "content": "In Minecraft, a complex task consists of multiple intermediate steps, i.e., sub-goals. For example, the task \"I need a wooden pickaxe\u201d includes five sub-goals: 'chop a tree to get logs', 'craft four planks', 'craft a crafting table', 'craft two sticks', and 'craft a wooden pickaxe'. Therefore, a planner is essential for the agent, as it needs to decompose the given complex task into a sequence of executable sub-goals for the policy to execute sequentially. In this paper, we follow Li et al. [24], employing an MLLM as the planner, which takes current observation and task instruction as input to generate sub-goals."}, {"title": "4.2. Goal-Observation-Action Conditioned Policy", "content": "According to Sec 3., a key insight into the relationship among observation o, action a, and sub-goal g is: that the observation o and action a at the same time step have a causal relationship; and the sub-goal g is a natural language description of the observation-action sequence over a certain time. To better model the relationships among the three elements mentioned above, we propose first integrating the representations of observation and action at each time step, then modeling the observation-action sequences along the temporal dimension, and finally aligning the observation-action sequences with the sub-goal for action prediction.\nMotivated by this, we propose a novel Goal-Observation-Action conditioned Policy, GOAP. As shown in Figure 2, our GOAP consists of an Action-guided Behavior Encoder that dynamically models observation-action sequences into fixed-length behavior tokens and an MLLM that aligns such behavior tokens with sub-goal for action prediction."}, {"title": "4.2.1. Action-guided Behavior Encoder", "content": "Previous policies often overlook the causal relationship between observation and action at each timestep. Moreover, it remains a challenge to model the long-term observation-action sequence without exceeding input length constraints. To this end, we propose an Action-guided Behavior Encoder that integrates the representations of observation and action at each time step and then dynamically models the historical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation $o_t$ into a visual encoder $V_E$ to obtain the visual features:\n$u_t \\leftarrow V_E(o_t)$  (2)\nwhere $v_t \\in \\mathbb{R}^{P \\times d}$, P is the number of patches for each image, and d is the dimension of the extracted image feature. In practice, we employ ViT [8] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model the relationship between observations and actions. It takes the visual feature $v_t$ as query tokens and the action embedding $a_t$ as key and value. The module then constructs the information interaction between action $a_t$ and $v_t$ through a cross-attention mechanism:\n$Q = v_tW_q, K = a_tW_k,V = a_tW_V$ (3)\n$\\hat{u}_t = CrossAttn(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d}})V$ (4)\nwhere $W_q, W_k$, and $W_V$ represent the weight matrices for the query (Q), key (K), and value (V), respectively. $CrossAttn(\\cdot)$ denotes the cross-attention layer, and d is the dimension of the image features. In this way, it explicitly assigns action information $a_t$ at time step t to the visual features $\\hat{u}_t$, enhancing the causal relationship between observations and actions.\nSubsequently, we introduce a History Aggregator module to capture the information of the observation-action sequence along the temporal dimension, serving as the behavior representation. At each timestep t, behavior tokens $B_t$ serve as queries, while the sequence of historical behavior"}, {"title": "4.2.2. MLLM Backbone", "content": "To model the relationship between the sub-goal and observation-action sequence, we introduce an MLLM that takes the sub-goal g, current observation features $v_t$, and behavior tokens $B_t$ as input to predict subsequent actions auto-regressively. To enable the MLLM backbone MLLM to predict low-level actions, we employ VPT [1] as action head AH to map output embeddings $\\bar{a}_{t+1}$ of language model into the action space.\n$\\bar{a}_{t+1} \\leftarrow MLLM([g, v_t, B_t])$ (6)\n$a_{t+1} \\leftarrow AH(\\bar{a}_{t+1})$ (7)\nFormally, given a dataset $\\mathcal{D} = \\{(o_{1:T}, a_{1:T})\\}_M$ with M complete trajectories, we train GOAP to learn the behavior distribution from $\\mathcal{D}$ via behavioral cloning. Moreover, we introduce a KL-divergence loss to measure the output distribution similarity between GOAP and VPT [1]. This helps our model effectively learn the knowledge from the teacher model VPT. The training loss can be formulated as follows:\n$L_{\\theta} = \\lambda_{BC} \\sum_{i=1}^{T} - \\log p_{\\theta}(a_{t+1}|o_{1:t}, a_{1:t}, g)$\n$+ \\lambda_{KL} \\sum_{i=1}^{T} D_{KL}(q_{\\phi}(a_{t+1}|o_{1:t}) || p_{\\theta}(a_{t+1}|o_{1:t}, g))$ (8)\nwhere $\\lambda_{BC}$ and $\\lambda_{KL}$ are trade off coefficients, $p_{\\theta}$ is the GOAP, $q_{\\phi}$ is the teacher model."}, {"title": "4.3. MGOA Dataset", "content": "In Minecraft, there remains a significant lack of high-quality goal-observation-action pairs to support behavior cloning training. Previous work has primarily relied on gameplay videos as training data. These datasets either lack natural language instructions (explicit goals) [1, 3], or use actions predicted by IDM models [1] for each observation as pseudo-labels [1, 25], which leads to a risk of misalignment between observations and actions. Inspired by Li et al. [24], we propose an automated data generation pipeline that enables the creation of aligned goal-observation-action pairs without the need for manual annotations or human contractors. First, we utilize existing agents [25], providing them with clear natural language instructions to attempt task completion in Minecraft. We then record the actions and corresponding observations during goal execution, generating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply the following filtering criteria: 1) only recording videos in which the task is successfully completed, and 2) discarding videos where task execution takes an excessive amount of time. For more details, please refer to Sup. C. Through this automated approach, we obtained 25k high-quality Minecraft Goal-Observation-Action (MGOA) dataset. A comparison of the MGOA dataset with the existing Minecraft datasets is shown in Table 1. Our automated data generation pipeline offers several key advantages: 1) it enables the generation of aligned goal-observation-action pairs without the need for manual annotation or pseudo-labeling; 2) its construction process is parallelizable, allowing for rapid dataset generation; and 3) it leverages local agents for data generation, resulting in low-cost production."}, {"title": "5. Experiments", "content": "Environment. Following [1, 25], we conduct experiments in the complex, open-world environment of Minecraft on the MineRL [11] platform. The agent interacts with the MineRL environment at 20 frames per second, generating low-level control signals for the mouse and keyboard. For each task execution, the agent is initialized in a randomized"}, {"title": "5.2. Experimental Results", "content": "The experimental results for Optimus-2 compared to the baselines across Atomic Tasks, Long-horizon Tasks, and"}, {"title": "5.3. Ablation Study", "content": "There are many unexplored questions around best practices for developing MLLM-based policy in Minecraft. In this section, we conduct an extensive ablation study and summarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial role in task execution. As shown in Table 5, the removal of the Causal Perceiver leads to an average performance decline of 42% across all tasks, highlighting the importance of capturing the causal relationship between observations and actions. Moreover, eliminating the History Aggregator"}, {"title": "5.4. Visualization of Behavior Representation", "content": "As shown in Figure 6, we apply t-SNE [38] to visualize observation features extracted by ViT [8], MineCLIP [9], and the Action-guided Behavior Encoder for four tasks. From (a) and (b) in Figure 6, it is evident that the behavior representations extracted by ViT and MineCLIP are highly mixed, making it challenging to delineate the boundaries between different tasks. This lack of clear distinction between task-specific behavior representations can hinder the model's ability to understand the unique behavior patterns associated with each task, potentially leading to task failure. In contrast, the visualization in (c) of Figure 6 reveals clear, distinct clusters for each task, demonstrating that the Action-guided Behavior Encoder effectively captures subtle differences in observation-action sequences, thereby learning robust behavior representations across tasks."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel agent, Optimus-2, which can excel in various tasks in the open-world environment of Minecraft. Optimus-2 integrates an MLLM for high-level planning and a Goal-Observation-Action conditioned Policy (GOAP) for low-level control. As a core contribution of this paper, GOAP includes an Action-guided Behavior Encoder to model the observation-action sequence and an MLLM to align the goal with the observation-action sequence for predicting subsequent actions. Extensive experimental results demonstrate that GOAP has mastered various atomic tasks and can comprehend open-ended language instructions. This enables Optimus-2 to achieve superior performance on long-horizon tasks, surpassing existing SOTA. Moreover, we introduce a Minecraft Goal-Observation-Action dataset to provide the community with large-scale, high-quality data for training Minecraft agents."}]}