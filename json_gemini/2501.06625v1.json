{"title": "Guided Code Generation with LLMs: A Multi-Agent Framework for Complex Code Tasks", "authors": ["Amr Almorsi", "Mohanned Ahmed", "Walid Gomaa"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in code generation tasks, yet they face significant limitations in handling complex, long-context programming challenges and demonstrating complex compositional reasoning abilities. This paper introduces a novel agentic framework for \"guided code generation\" that tries to address these limitations through a deliberately structured, fine-grained approach to code generation tasks. Our framework leverages LLMs' strengths as fuzzy searchers and approximate information retrievers while mitigating their weaknesses in long sequential reasoning and long-context understanding. Empirical evaluation using OpenAI's HumanEval benchmark with Meta's Llama 3.1 8B model (int4 precision) demonstrates a 23.79% improvement in solution accuracy compared to direct one-shot generation. Our results indicate that structured, guided approaches to code generation can significantly enhance the practical utility of LLMs in software development while overcoming their inherent limitations in compositional reasoning and context handling.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized the field of code generation, offering unprecedented capabilities in automating software development tasks. These models have demonstrated impressive performance in simple code completion, generating code snippets, completing partial implementations, and even assisting in bug fixing and optimization. However, despite their remarkable achievements, current LLM-based code generation systems face significant limitations, particularly in planning, handling long, complex code tasks, and navigating large-scale coding projects."}, {"title": "A. Limitations of Long-Context LLMs", "content": "One of the primary challenges hindering the widespread adoption of LLMs for comprehensive code generation is the limited context window constraint. Most state-of-the-art LLMs are trained to operate within a fixed token limit, typically ranging from several hundred to a few thousand tokens. This restriction severely impedes the model's ability to \"reason\" over long sequences understand, and generate code for intricate software systems that often span thousands of lines across multiple files.\nNonetheless, recent breakthroughs in extending the context window massively up to 128k [1] [2] [3] have been made and even more recent approaches like Infini-attention [5] and LongRoPE [4] extend the context window to the order of hundreds of thousands and millions of tokens. However, when it comes to evaluating long-context LLMs, they are most commonly evaluated on needle-in-haystack [6] tasks which test whether the LLM can successfully fetch pieces of information residing in a long sequence of tokens. However, this does not truly measure the LLMs' abilities to \"reason\" with these pieces of information in more realistic scenarios and practical tasks like Coding, Reasoning, In-context learning, Multilingual comprehension, and others. For these reasons, LongICLBench [7] has been compiled in order to evaluate long-context LLMs on in-context learning (ICL) on extreme-label classification tasks, which demonstrated that the performance of all state-of-the-art long-context LLMs degrades uniformly with respect to the length of the input."}, {"title": "B. Limited Compositional Abilities of LLMs", "content": "A significant limitation of Large Language Models (LLMs) lies in their compositional abilities and sequential reasoning capabilities. In a recent study, Xu et al. [8] investigated LLMs' capacity to synthesize previously learned individual skills to address novel composite tasks. Through comprehensive empirical analyses across various LLM architectures, the researchers evaluated performance on a range of composite tasks encompassing both linguistic and logical challenges. Their findings revealed varying levels of compositional proficiency among LLMs, with models demonstrating competence in basic tasks but exhibiting notable difficulties with complex sequential reasoning problems. Through theoretical analysis in a simplified setting, the authors established that LLMs display compositional capabilities primarily when tasks can be effectively decomposed based on input parameters. While their investigation of model scaling effects indicated that larger models with their more sophisticated internal architectures\u2014showed enhanced performance on elementary composite tasks, a crucial limitation emerged: \"for more complex composite tasks involving reasoning multiple steps, where each step represents one task, models typically underperform, and scaling up generally provides no improvements\" [8].\nWhile contemporary applications of Large Language Models (LLMs) predominantly focus on code completion tasks (exemplified by GitHub Copilot), generation of discrete code segments, or environmental interaction capabilities as demonstrated in OpenHands [9], these implementations typically employ a single-step generation process. However, this approach presents a fundamental contradiction: computational processes are inherently sequential and compositional in nature, and considering the limitations in compositional reasoning identified by Xu et al. [8], the effectiveness of generating code-which fundamentally comprises sequential compositions of programming constructs\u2014becomes theoretically questionable.\nTo address these limitations, we introduce a novel agentic framework for \"guided code generation,\" which implements a more structured and granular approach to complex code generation tasks. This methodology capitalizes on LLMs' demonstrated capabilities as fuzzy searchers and approximate information retrievers while mitigating their compositional limitations. We evaluate our framework's efficacy using OpenAI's HumanEval benchmark, employing Meta's Llama 3.1 8B model with int4 precision [10] quantization due to computational constraints and limited resources. Additionally, we present a formal theoretical framework for analyzing code generation tasks within the context of LLMs and our proposed methodology."}, {"title": "II. PROPOSED METHOD", "content": "Our framework introduces a novel approach to structured code generation through a multi-agent system designed to break down complex coding tasks into manageable, composable units. The framework operates through three primary components: hierarchical decomposition, bottom-up code generation, and multi-agent validation."}, {"title": "A. Hierarchical Problem Decomposition", "content": "The framework begins with a Generalist Agent that performs recursive problem decomposition. Given a complex coding task X, the agent:\n1) Decomposes the problem into constituent functions or code parts necessary for the task.\n2) Continues this decomposition recursively until reaching atomic units.\n3) Creates a tree structure where:\n\u2022 The root node represents the overall problem or task.\n\u2022 Internal nodes represent intermediate sub-problems.\n\u2022 Leaf nodes represent practically indivisible coding tasks.\n\u2022 Each node maintains specific functionality required by its parent solution.\nThis decomposition emphasizes extreme modularity by ensuring that every operation (including basic constructs like if-else or loop blocks) is encapsulated within documented, pure functions with highly descriptive names. The approach draws from functional programming principles, prioritizing ease of compositionality, modularity, expressiveness, and high readability."}, {"title": "B. Bottom-up Code Generation", "content": "Once the problem tree is established, code generation proceeds from leaves to root through the following steps:\n1) Leaf Node Resolution\n\u2022 A Code Agent generates single, self-contained functions or other programming language constructs for each leaf node.\n\u2022 Each function undergoes immediate testing and validation.\n\u2022 Solutions employ chain-of-thought reasoning and incorporate feedback from tests."}, {"title": "2) Upward Composition", "content": "\u2022 For each non-leaf node, the Code Agent receives:\n\u2022 A list of documentation statements for available functions or code blocks from child nodes.\n\u2022 Function documentation and descriptions.\n\u2022 No access to implementation details or problem descriptions of descendant nodes.\n\u2022 The agent composes solutions using only the provided function interfaces or code blocks.\nThis process continues until reaching the root node."}, {"title": "C. Multi-Agent Validation System", "content": "Using the Chain-of-Thought prompting technique [12], each problem is passed to the Generalist Agent to give a candidate solution, break it down, and determine the tools or functions needed to implement the task. Each generated solution undergoes rigorous validation through a comprehensive multi-agent evaluation process. The primary validation mechanism employs a dedicated Critic Agent that conducts thorough analyses of each solution. This agent performs detailed reviews focusing on multiple dimensions of code quality, including implementation efficiency and functional correctness. The feedback generated through this critical analysis is systematically incorporated into subsequent generation attempts, enabling continuous refinement of the solutions.\nFurthermore, the validation process incorporates an automated testing phase executed by a specialized testing agent. This automated evaluation provides quantitative performance metrics and identifies potential issues in the generated code. The testing agent generates detailed diagnostic information and debugging suggestions, which are then integrated back into the generation process. This dual-agent validation approach ensures comprehensive quality assessment while maintaining a continuous feedback loop for solution optimization.\nAlthough Chain of Thought (CoT) reasoning is recognized as an emergent capability primarily manifesting in larger language models-typically those exceeding 60-70 billion parameters\u2014we found it necessary to implement CoT prompting with the smaller Llama 3.1 8B model to facilitate task planning and problem tree generation. Therefore, the observed performance boosts through is due to three key mechanisms: self-critique, problem decomposition, and upward composition. However, this approach introduces a critical vulnerability: if the initial proposed solution to the root problem is incorrect, this error propagates throughout the entire solution structure. Despite this limitation, our findings suggest that implementing this methodology on larger language models would likely yield superior performance due to their inherent CoT capabilities."}, {"title": "D. Key Advantages", "content": "Our framework addresses three key limitations of direct LLM-based code generation. First, it improves context window management by isolating sub-problems and providing specific context at each generation phase, thereby circumventing standard token limitations while maintaining solution coherence.\nThe framework also enhances reasoning by constraining the scope of individual generation tasks. This division enables the LLM to process well-defined sub-problems rather than attempting to generate complete solutions in a single step, resulting in more reliable code generation.\nFinally, the framework reduces errors through a multi-layered approach. It combines validation systems for early error detection, chain-of-thought reasoning, and iterative feedback loops. The bottom-up compositional structure ensures that complex solutions are built from verified components, improving overall reliability."}, {"title": "E. Theoretical Framework", "content": "Our approach reconceptualizes code generation as a dual-nature problem comprising two fundamental problems. The first problem frames code generation for atomic components (leaf nodes) as an information retrieval and fuzzy search problem. This formulation capitalizes on the extensive code corpus available in contemporary training datasets, enabling Large Language Models to leverage their demonstrated proficiency in pattern recognition and approximate matching. The effectiveness of this approach stems from the models' exposure to diverse programming patterns during training, facilitating the retrieval and adaptation of relevant code segments.\nThe second paradigm addresses the compositional aspect of code generation, focusing on the systematic integration of verified components. This composition-centric perspective emphasizes the importance of interface-based integration rather than implementation-specific details. By abstracting the composition process to the interface level, we establish a more robust and generalizable framework for component integration, mitigating the complexities typically associated with implementation-level composition. This theoretical decomposition allows us to leverage LLMs' strength in pattern matching for atomic components while providing a structured approach to addressing their known limitations in compositional reasoning.\nThis structured approach enables reliable generation of complex software systems while maintaining high code quality and correctness."}, {"title": "III. RESULTS", "content": "We evaluated our guided code generation framework against traditional one-shot generation approaches using OpenAI's HumanEval benchmark [11]. The experiments were conducted using Meta's Llama 3.1 8B model with int4 precision quantization [10] due to computational resource constraints. Our evaluation focused on two primary metrics: Pass@1 scores and qualitative analysis of the generated solutions."}, {"title": "A. Quantitative Analysis", "content": "Our framework demonstrated substantial improvements over direct code generation approaches. As shown in TableI, the guided code generation framework achieved a Pass@1 score of 56.2% on the HumanEval benchmark, representing a 23.79 percentage point improvement over the baseline one-shot generation approach, which scored 45.4% Pass@1. This improvement is particularly noteworthy given our use of a relatively small (8B parameters) and quantized model, suggesting that the structured approach can effectively compensate for model size and precision limitations."}, {"title": "B. Qualitative Analysis", "content": "Beyond the HumanEval benchmark, we tested our framework's capability on long, complex coding tasks like building a mathematical function evaluator. The framework successfully generated it with a complete lexer, a complete parser, a complete evaluation algorithm, and with comprehensive error handling throughout every component of the evaluator. On the other hand, frontier models like OpenAI's GPT-4o and Google's Gemini 1.5 Pro consistently either refused to do the task or generated much simpler evaluators."}, {"title": "IV. CONCLUSION", "content": "This paper introduces a novel framework for guided code generation addressing key limitations in current LLM-based approaches. Our methodology, combining hierarchical decomposition, bottom-up generation, and multi-agent validation, demonstrates significant improvements in code generation capabilities, even with modest model architectures.\nEmpirical results show a 23.79% improvement in Pass@1 scores on the HumanEval benchmark using an 8B parameter model. Our framework successfully generated complex software systems that larger frontier models typically refuse, highlighting the potential of decomposition-based approaches.\nWe propose a theoretical framework reconceptualizing code generation as a dual problem of information retrieval and compositional reasoning. While aligning with our empirical results, this framework remains hypothetical and requires further study.\nFuture work should focus on scaling to larger models, exploring diverse programming paradigms, optimizing decomposition and validation processes, and rigorously testing the theoretical model.\nOur findings underscore the potential of structured, multi-agent approaches in overcoming current LLM limitations, paving the way for more sophisticated code generation systems that combine traditional software engineering principles with modern AI capabilities."}]}