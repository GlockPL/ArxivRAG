{"title": "MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching", "authors": ["Yen-Siang Wu", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "abstract": "Text-to-video (T2V) diffusion models have shown promising capabilities in synthesizing realistic videos from input text prompts. However, the input text description alone provides limited control over the precise objects movements and camera framing. In this work, we tackle the motion customization problem, where a reference video is provided as motion guidance. While most existing methods choose to fine-tune pre-trained diffusion models to reconstruct the frame differences of the reference video, we observe that such strategy suffer from content leakage from the reference video, and they cannot capture complex motion accurately. To address this issue, we propose MotionMatcher, a motion customization framework that fine-tunes the pre-trained T2V diffusion model at the feature level. Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features to fine-tune diffusion models, ensuring precise motion learning. For the sake of memory efficiency and accessibility, we utilize a pre-trained T2V diffusion model, which contains considerable prior knowledge about video motion, to compute these motion features. In our experiments, we demonstrate state-of-the-art motion customization performances, validating the design of our framework.", "sections": [{"title": "1. Introduction", "content": "To control the rhythm of a movie scene, movie directors would carefully arrange the precise movements and positioning of both the actors and the camera for each shot (as known as staging/blocking). Similarly, to control the pacing and flow of AI-generated videos, users should have control over the dynamics and composition of videos produced by generative models. To this end, numerous motion control methods [25, 33, 57, 59, 61, 63, 72] have been proposed to control moving object trajectories in videos generated by text-to-video (T2V) diffusion models [4, 17]. Motion customization, in particular, aims to control T2V diffusion models with the motion of a reference video [26, 31, 36, 71, 76]. With the assistance of the reference video, users are able to specify the desired object movements and camera framing in detail. Formally speaking, given a reference video, motion customization aims to adjust a pre-trained T2V diffusion model, so the output videos sampled from the adjusted model follow the object movements and camera framing of the reference video (see Fig. 1 for an example). Given that motion is a high-level concept involving both spatial and temporal dimensions [65, 71], motion customization is considered a non-trivial task.\nRecently, many motion customization methods have been proposed to eliminate the influence of visual appearance in the reference video. Among them, a standout strategy is fine-tuning the pre-trained T2V diffusion model to reconstruct the frame differences of the reference video. For instance, VMC [26] and SMA [36] use a motion distillation objective that reconstructs the residual frames of the reference video. MotionDirector [76] proposes an appearance-debiased objective that reconstructs the differences between an anchor frame and all other frames. However, we find that frame differences do not accurately represent motion. For example, two videos with the same motion, such as a red car and a blue car both driving leftward, can yield completely different frame differences because the pixel changes occur in different color channels in each video. Moreover, since frame differences only process videos at the pixel level, they cannot capture complex motion that requires a high-level understanding of video, such as rapid movements or movements in low-texture regions. In these cases, the strategy of reconstructing frame differences fails to reproduce the target motion.\nTo address this issue, we propose MotionMatcher, a novel fine-tuning framework for motion customization via motion feature matching. Instead of aligning pixel values or frame differences as in previous methods, MotionMatcher aligns the projected motion features extracted from a pre-trained feature extractor. Since these motion features are calculated with a sophisticated pre-trained model, they are capable of capturing complex motion that requires a high-level, spatio-temporal understanding of video. This effectively addresses the limitation of previous work, where frame differences fail to capture complex motion.\nMotionMatcher differs from traditional fine-tuning approaches. At each fine-tuning step, it starts off by using a feature extractor to compute the motion features of the output video and the motion features of the reconstruction ground truth video. Our feature matching objective then minimizes the L2 distance between the two feature vectors. However, since the output videos of T2V diffusion models are in latent space and at certain noise levels, the feature extractor must be able to process latent noisy videos. To obtain such a feature extractor, we take advantages of (1) pre-trained T2V diffusion models' ability in extracting features from noisy, latent videos and (2) the spatio-temporal information encoded in attention maps. We find that cross-attention maps (CA) in pre-trained diffusion models contain information about camera framing, while temporal self-attention maps (TSA) represent object movements. Therefore, we utilize them to represent motion features. Ultimately, the design of our framework is validated through detailed analysis and extensive experiments.\nTo summarize, our key contributions include:\n\u2022 We propose MotionMatcher, a feature-level fine-tuning framework for motion customization. It leverages a pre-trained feature extractor to map videos into a motion feature space, capturing high-level motion information. By aligning the motion features, the diffusion model learns to generate videos with the target motion.\n\u2022 To extract features from noisy latent videos, we utilize the pre-trained diffusion model as a feature extractor, as it naturally processes such inputs.\n\u2022 We identify two sources of motion cues\u2014cross-attention maps and temporal self-attention maps and use them to form the motion features.\n\u2022 We demonstrate that MotionMatcher achieves state-of-the-art performance through comprehensive experiments. It offers superior joint controllability of text and motion, advancing scene staging in AI-generated videos."}, {"title": "2. Related work", "content": "2.1. Text-to-video generation\nText-to-video (T2V) generation models aim to synthesize videos that comply with user-provided text descriptions. Previously, a large number of T2V models have been proposed, including GANs [2, 28, 30, 35], autoregressive models [10, 18, 29, 55], and diffusion models [4, 17, 70].\nFollowing the success of text-to-image (T2I) diffusion models [40, 43, 46], researchers have also put considerable effort into training T2V diffusion models recently. To achieve this, a commonly used approach is inflating a pre-trained T2I diffusion model by inserting temporal layers and finetuning the whole model on video data [6, 13, 16,"}, {"title": "2.2. Motion control in T2V generation", "content": "To enable detailed control over camera framing and object movements in T2V generation, recent research has explored trajectory-based [59, 63, 65, 72], box-based [25, 33, 57, 61], and reference-based motion control. Trajectory-based and box-based motion control are typically achieved by conditioning T2V diffusion models on additional motion signal and training them on large video datasets [57, 59, 63, 72], or by directly manipulating attention maps at the inference stage [25, 33, 61]. However, these approaches require users to explicitly define the trajectories of moving objects within frames, which is usually laborious and provides limited control over the entire scene. In contrast, reference-based motion control can specify the target motion more comprehensively via a reference video [26, 31, 36, 71, 76]. In this work, we focus on motion customization, which is considered reference-based motion control."}, {"title": "2.3. Motion customization of T2V diffusion models", "content": "Recently, motion customization has emerged as a new area of research. It adapts the pre-trained T2V diffusion model to generate videos that replicate the camera framing and object movements of a user-provided reference video. To avoid learning visual appearance, VMC [26] and SMA [36] fine-tune the pre-trained T2V diffusion model by aligning the residual frames of the output video with the residual frames of the reference video. MotionDirector [76] proposes a dual-path fine-tuning method to avoid learning visual appearance and simultaneously utilizes an objective that matches frame differences. However, since frame differences do not accurately represent motion, these methods struggle to replicate complex motion.\nAnother strategy is using diffusion guidance [8, 14, 34] to achieve controllable generation. Specifically, DMT [71] employs the intermediate spatio-temporal features in diffusion models as a guidance signal, whereas MotionClone [31] uses intermediate temporal attention maps for guidance. Despite being training-free, these methods need to compute additional gradients during inference, resulting in a lengthy sampling process. Moreover, as noted in [37, 47], the large guidance weights used in diffusion guidance can lead to the generation of out-of-distribution samples.\nWhile other motion customization approaches exist, they address different tasks. For instance, DreamVideo [60]"}, {"title": "3. Method", "content": "Problem formulation To control scene staging in AI-generated videos, we tackle the problem of motion customization, specifically as defined in DMT [71]. Given a reference video zo and a text prompt y associated with it, we aim to adjust a pre-trained T2V diffusion model ee, so that the output videos sampled from the adjusted model replicate both the object movements and camera framing in zo.\n3.1. Preliminary: Text-to-video diffusion models\nText-to-video (T2V) diffusion models are probabilistic generative models that synthesize videos by gradually denoising a sequence of randomly sampled Gaussian noise frames (in latent space), guided by a textual condition y.\nArchitecture To model temporal information, T2V diffusion models typically inflate a pre-trained text-to-image (T2I) diffusion model by inserting temporal layers. These temporal layers are made up of feedforward networks and temporal self-attentions, where temporal self-attentions (TSA) apply self-attention along the frame axis.\nTraining T2V diffusion models e are trained by minimizing a weighted noise-prediction objective:\n$E_{z_0,t,\\epsilon} [w_t || \\epsilon - \\epsilon_\\theta (z_t, t, y) ||^2]$, (1)\nwhere $z_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$ is the noised video at timestep t, $\\epsilon \\sim N(0, I)$ is Gaussian noise, and $w_t$ is a time-dependent weighting term. This noise-prediction objective is also equivalent to predicting the previous noised video at timestep t\u22121 through a different parametrization [15]:\n$E_{z_0,t,\\epsilon} [w_t ||v_t (z_t, \\epsilon) - v_t (z_t, \\epsilon_\\theta (z_t, t, y))||^2]$, (2)\nwhere $v_t (z_t, \\epsilon) := \\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}} + (\\sqrt{1 - \\bar{\\alpha}_t} - \\frac{\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_{t-1}}})\\epsilon$ is a function that estimates the previous noised video $z_{t-1}$ based on the current video state $z_t$ and noise $ \\epsilon$, and $w_t$ is the time-dependent weight after reparametrization (See supplementary material for more details). For simplicity, we will use $v_t$ to denote the model prediction $v_t(z_t, \\epsilon_\\theta(z_t, t, y))$, and use $\\hat{v}_t$ to denote the ground truth $v_t (z_t, \\epsilon)$. The objective can therefore be rewritten as:\n$E_{z_0,t,\\epsilon} [w_t ||\\hat{v}_t - v_t ||^2]$, (3)\nwhere $w_t$ is the time-dependent weight in Eq. (2)."}, {"title": "3.2. Learning motion at the feature level", "content": "Identifying motion in video requires a high-level understanding of both the spatial and temporal aspects of the video, so using the standard pixel-level DDPM reconstruction loss (Eq. (3)) for motion customization cannot accurately learn motion, and may introduce irrelevant information, such as content and visual appearance.\nTo this end, we introduce the motion feature matching objective, where a deep feature extractor M is used to extract motion information from videos at a high level. Instead of directly aligning the predicted noisy video vt with the ground truth \u00fbt at the pixel level, we align their high-level motion features (extracted by M):\n$L_{mot}(\\theta) = E_{z_0,t,\\epsilon} [w_t ||M(v_t) - M(\\hat{v}_t) ||^2]$, (4)\nwhere M is a motion feature extractor for noisy latent videos, and $w_t$ is the time-dependent weight in Eq. (3). As illustrated in Fig. 2(a), this motion feature matching objective aims to minimize the L2 discrepancy between the two videos in the motion feature space, ensuring that the motion in output video matches the motion in the reference video.\nHowever, designing the motion feature extractor M in Eq. (4) is non-trivial, as it needs to extract features from noisy latent videos. First of all, most feature extractors,\nHere we claim that the pre-trained T2V diffusion model serve as a proper feature extractor for noisy latent videos. Firstly, recent work has shown both theoretically and experimentally that pre-trained diffusion models are capable of extracting high-level semantics and structural information from visual data, making them a \"unified feature extractor\" [64, 67]. Secondly, since diffusion models are trained on noisy latent inputs, using them as feature extractors for noisy latent videos helps prevent a training-inference gap. For these reasons, MotionMatcher leverages the pre-trained T2V diffusion model as the motion feature extractor M."}, {"title": "3.3. Extracting motion cues from diffusion models", "content": "In this section, we identify the locations within the intermediate layers of diffusion models from which motion-specific features can be extracted.\nExtracting cues for camera framing Recent studies have shown that the cross-attention (CA) maps in diffusion models closely reflect the spatial arrangement of objects within the frame [25, 33, 44, 66, 69]. Building on this, we leverage the CA maps from T2V diffusion models to describe the composition of each video frame (see Fig. 2(b)), thereby determining the camera framing throughout the video (e.g., shot size and composition).\nFormally speaking, CA maps are calculated by first reshaping the intermediate 3D activations \u03a6 \u2208 RH\u00d7W\u00d7F\u00d7D into the shape (H \u00d7 W \u00d7 F) \u00d7 D, where F, H, W, and D denote the number of frames, height, width, and depth of the activations. Cross-attention is then performed between the activations \u03a6 and word embeddings (y) as follows :\n$M_{CA} = Softmax (\\frac{Q(\\Phi)K(T(y))^T}{\\sqrt{D}})$, (5)\nwhere T denotes the text encoder used in the T2V diffusion model, and y is the text prompt given by the user. In $M_{CA} \\in [0,1]^{F\u00d7H\u00d7W\u00d7|y|}$, each element $(M_{CA})_{i,j,k,l}$ represents the correlation between the spatial-temporal coordinate (i, j, k) and the l'th word in the text prompt. As shown in Fig. 3, MCA highlights the region within the frame that corresponds to an object. It focuses on structural information and eliminates visual appearance.\nExtracting cues for object movements Since cross-attention maps cannot describe motion that does not involve spatial shifts (e.g., rotation and non-rigid motion), it is crucial to extract additional cues to represent such object movements. Since we discover that the temporal self-attention (TSA) maps in T2V diffusion models can capture detailed object movements, we also incorporate them into the motion features (see Fig. 2(b)).\nTo compute temporal self-attention (TSA) maps MTSA, we begin by reshaping the model's intermediate 3D activations \u03a6 \u2208 RH\u00d7W\u00d7F\u00d7D into the shape (H \u00d7 W) \u00d7 F \u00d7 D. For each particular spatial coordinate (i, j), we compute the self-attention weights between frames as follows:\n$(M_{TSA})_{i,j} = Softmax (\\frac{Q(\\Phi)_{i,j}K(\\Phi)_{i,j}^T}{\\sqrt{D}})$, (6)\nwhere i and j denote the spatial coordinates. Specifically, each element $(M_{TSA})_{i,j,k,l}$ of the TSA map Misa \u2208 $[0,1]^{H\u00d7W\u00d7F\u00d7F}$ represents the degree of relevance between the k'th and l'th frames at the spatial coordinate\nWith the cross-attention maps capturing camera framing, and the temporal self-attention maps reflecting object movements, we combine both to form the motion features:\n$(\\alpha_{CA}M_{CA}) + (\\alpha_{TSA}M_{TSA})$, (7)\nwhere \u03b1CA and \u03b1TSA are weights that control the contributions of each component."}, {"title": "3.4. Motion-aware LoRA fine-tuning", "content": "After extracting the motion features, we fine-tune the pre-trained T2V diffusion model using the motion feature matching objective in Eq. (4). By aligning the MCA component, we ensure that the camera framing in the generated video matches that of the reference video, and aligning MTSA ensures that the dynamics in the generated video align with those of the reference video.\nTo preserve the model's pre-trained knowledge while fine-tuning, we apply low-rank adaptations (LoRAs) [20] to fine-tune the model with fewer trainable parameters:\n$arg \\underset{\\Delta \\theta}{min} L_{mot} (\\theta + \\Delta \\theta)$, (8)\nwhere \u0394\u03b8 is a low-rank parameter increment. Having these motion-aware LoRAs, MotionMatcher is capable of synthesizing videos that are guided by both the textual description and the motion in the user-provided reference video."}, {"title": "4. Experiments", "content": "4.1. Experiment setup\nDataset To evaluate MotionMatcher's ability to transfer motion from a reference video to a new scene, we collect a dataset of 42 video-text pairs. These videos encompass a wide range of motion types, such as fast object movement, rotation, non-rigid motion, and camera movement. We also ensure that the scenes in the editing text prompts are distinct from the scene in the reference video while remaining compatible with its motion.\nImplementation details For a fair comparison, we use Zeroscope [50] as the base T2V diffusion model across all methods, given its ability to model complex motion and widespread usage in previous work [36, 71, 76]. We fine-tune the model with LoRA [20] for 400 steps at a learning rate of 0.0005. To extract motion features, we obtain attention maps MCA and MTSA from down_block.2, with weights \u03b1CA and \u03b1TSA both set to 2000. These hyperparameters are chosen to balance control over camera framing and object movements. After extracting features from intermediate layers, we stop the forward pass to avoid unnecessary computation. For further implementation details, please refer to the supplementary material.\nBaselines We compare our method against four recent approaches to motion customization, including two fine-tuning methods\u2014VMC [26] and MotionDirector [76]\u2014 and two training-free methods\u2014DMT [71] and MotionClone [31]. Detailed descriptions of these methods are provided in Sec. 2.3.\n4.2. Evaluation metrics\nWe use four automatic metrics to evaluate the effectiveness of motion customization: (1) CLIP-T: To measure text"}, {"title": "5. Ablation study", "content": "We conduct an ablation study to examine the impact of incorporating MCA and MTSA in motion features. As illustrated in Fig. 8, without cross-attention maps MCA, the model struggles to correctly position all the element of the scene. Meanwhile, removing temporal self-attention maps MTSA reduces the precision of fine-grained dynamics. The quantitative results in Tab. 2 further validate the importance of both attention maps in controlling motion. These results confirm that both the camera framing, informed by MCA, and inter-frame dynamics, informed by MTSA, are essential for capturing overall motion.\n5.1. Human user study\nFor a more accurate evaluation, we conduct a user study comparing our method with existing approaches based on human preferences. Following previous work [71, 76], we adopt the Two-alternative Forced Choice (2AFC) protocol. In the survey, the participants are presented with one video generated by our method and another video generated by a baseline approach. They are asked to compare the videos across three key aspects of motion customization: (1) Video quality: the degree to which the output video appears realistic and visually appealing, (2) Text alignment: how well the output video matches the text prompt, and (3) Motion alignment: the similarity in motion between the output video and the reference video. Ultimately, we collected 192 human evaluations per baseline and metric, totaling 2,304 human evaluations. These responses were gathered from 24 participants recruited via the Prolific platform."}, {"title": "6. Conclusion", "content": "We presented MotionMatcher, a feature-level fine-tuning framework for motion customization. MotionMatcher transforms the pixel-level DDPM objective into the motion feature matching objective, aiming to learn the target motion at the feature level. To extract motion features, MotionMatcher leverages the pre-trained T2V diffusion model as a deep feature extractor and identify valuable motion cues from two attention mechanisms within the model, representing both object movements and camera framing in videos. In the experiments, MotionMatcher demonstrated superior joint controllability of text and motion to prior approaches. These results suggest that MotionMatcher enhances control over scene staging in AI-generated videos, benefiting real-world applications in computer-generated imagery (CGI). For a discussion of MotionMatcher's limitations, please refer to the supplementary material."}]}