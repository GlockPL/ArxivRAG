{"title": "Towards Reliable Evaluation of Behavior Steering Interventions in LLMs", "authors": ["Itamar Pres", "Laura Ruis", "Ekdeep Singh Lubana", "David Krueger"], "abstract": "Representation engineering methods have recently shown promise for enabling efficient steering of model behavior. However, evaluation pipelines for these methods have primarily relied on subjective demonstrations, instead of quantitative, objective metrics. We aim to take a step towards addressing this issue by advocating for four properties missing from current evaluations: (i) contexts sufficiently similar to downstream tasks should be used for assessing intervention quality; (ii) model likelihoods should be accounted for; (iii) evaluations should allow for standardized comparisons across different target behaviors; and (iv) baseline comparisons should be offered. We introduce an evaluation pipeline grounded in these criteria, offering both a quantitative and visual analysis of how effectively a given method works. We use this pipeline to evaluate two representation engineering methods on how effectively they can steer behaviors such as truthfulness and corrigibility, finding that some interventions are less effective than previously reported.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [1\u20133] have been shown to possess potentially harmful skills that yield undesirable behaviors [4, 5]. Although post-training methods like fine-tuning have shown success at dissuading models from engaging in such behaviors, users can often circumvent the effects of fine-tuning and revert the model to its original, harmful behavior [6\u201311]. Motivated by this problem, representation engineering methods have been proposed as an alternative set of protocols for model control [12]. These methods steer model behavior by directly manipulating activations at inference-time. The idea is that by operating on internal representations directly, the model will be more robustly controlled. While several representation engineering methods have shown promising results [12-14], a study by Tan et al. [15] has reported that these methods can be unreliable and the targeted behavior is not always consistently exhibited in model generations\nWe argue a key reason behind the inconsistent results is a lack of well-defined protocols for evaluating \"steerability\": how well a representation engineering method steers the model towards a target behavior. To address this issue, we propose a novel evaluation pipeline that quantifies the impact of activation steering\u2014a subset of representation engineering methods-on model behavior. Our pipeline evaluates open-ended generations, steerability towards and away from targeted behavior, and impact of interventions on model likelihoods, yielding both a quantitative and visual depiction of how well the model is steered by an intervention."}, {"title": "2 Desiderata when Evaluating Activation Steering Methods", "content": "We argue that a protocol for evaluating whether activation steering successfully steers a model's behavior should possess the following properties.\nProperty 1: The evaluation is conducted in an open-ended generation context.\nProperty 2: The evaluation considers the model's confidence in sampled tokens.\nProperty 3: The evaluation enables comparison of steerability across different behaviors.\nProperty 4: The evaluation allows for easy comparison to an informative baseline.\nIn the following, we discuss each of these properties and their absence in current evaluation protocols.\nProperty 1: Generalizability. The promise of behavioral steering is to control specific qualities of model-generated content. However, assessing whether a model exhibits the desired qualities can be subjective and challenging to quantify. To overcome this, researchers have devised multiple-choice prompts that allow for clear assessment of the model's inclination to display the desired behavior. By comparing responses to these prompts before and after applying the intervention, steering impact becomes more easily measurable.\n Consequently, we believe it is crucial to measure steering in the same context where it will be applied, hence motivating Property 1."}, {"title": "Property 2: Consistency.", "content": "Another approach from literature to assess steering quality is directly analyzing generations from intervened models. One such approach involves using LLMs to evaluate the strength of the desired behavior in generations [13, 14]. However, focusing solely on generated text often misses significant changes to the intervened model's underlying distribution. Such changes are particularly important when decoding with non-deterministic sampling methods like Nucleus Sampling [17] as different top-tokens may express different behaviors. By disregarding confidences, information about how variable behavioral expression is will be lost. We demonstrate this phenomenon by applying CAA to steer the model to behave myopically. Despite the output text suggesting an unsuccessful intervention (Table 2), examination of the final token distribution (Table 3) reveals that most of the top-10 tokens are myopic, though not all-notably, the top two tokens (one myopic, one non-myopic) have nearly equal sampling probabilities. This indicates that the model's output could vary based on the random seed used during sampling (see Appendix E for more details)."}, {"title": "Property 3: Cross-behavioral Comparability.", "content": "Steering interventions have been shown to be successful for behaviors of varying specificity [13, 14, 18]. For instance, the same interventions that steer models to discuss wedding-related content can also influence them to exhibit positive sentiment. However, developing steering interventions for diverse behaviors often necessitates the use of behavior-specific datasets, which can vary significantly in quality. Furthermore, the geometric representations of different behaviors within language models may exhibit substantial variations [19]. These factors collectively suggest that the efficacy of steering techniques is likely to depend on the target behavior. Indeed, experimental work has shown that steering performance is behavior-dependent [15]. This suggests that to better characterize the quality of an intervention, steering metrics must provide a standardized way to compare intervention success across different behaviors, motivating Property 3. With such comparisons, practitioners would be able to easily identify for which behaviors their steering protocol excels, and for which behaviors their protocol is unreliable.\nMany existing metrics rely on specific properties of the target behavior to assess steering success, hindering the possibility of comparisons across behaviors. For example, Turner et al. [13] apply an intervention to promote wedding-related content and measure success by recording the frequency of common words associated with weddings (\u2018wedding', \u2018weddings', \u2018wed', 'marry', \u2018married', 'marriage', 'bride', 'groom', and 'honeymoon') in open-ended generations. While effective for topic steering, this approach is less suitable for abstract behaviors that lack clear associated vocabularies. For these abstract behaviors, applying a similar evaluation protocol would be infeasible. Moreover, incorrectly defining the set of words for a behavior may result in unfair comparisons of intervention success across behaviors.\nFor the same wedding steering intervention, Turner et al. [13] propose an additional metric measuring model perplexity on open-ended generations with increasing wedding-related content. A smaller perplexity-score, relative to a baseline, indicates successful steering, since the intervened model is less"}, {"title": "Property 4: Baseline considerations.", "content": "Models display behavioral tendencies even before interventions. Measuring generation quality without comparing to the baseline model, i.e., the one without interventions, can be misleading. The key is whether the behavior deviates from the baseline for the samples where the baseline does not already express the target behavior. This point is similar to the one made by Hewitt et al. [20], who stress the importance of choosing the right baseline when probing model activations. While most existing metrics to evaluate steering meet Property 4, we nonetheless state it explicitly to emphasize its critical role in evaluations focused on model behaviors."}, {"title": "3 Methodology", "content": "In this section, we detail our proposal for how to evaluate steering model behavior (see Figure 1).\nEvaluation pipeline. The first step is to create a dataset of behavior-testing queries, each with two continuations: one matching the desired behavior (called 'positive') and one opposing it (called 'negative'). The baseline model processes this dataset, yielding token log-likelihoods for each data point. The process is repeated with an 'intervened model', i.e., a model to which activation steering has been applied. Intervened and baseline likelihoods are then independently renormalized by the average of the highest negative sample likelihood and the lowest positive sample likelihood. Lastly, positive and negative samples are independently sorted by increasing likelihood under the baseline model. As shown in Figure 1 (b), an effective intervention lowers negative sample log-likelihoods and raises positive ones. If all negative samples are less likely than positive samples under the baseline model, it already prefers desired behavior. This shows up in the visualisation as no overlapping region on the Y-axis between positive and negative samples.\nMetric. To quantify the intervention effect, we propose a metric measuring mean likelihood dif-ferences between baseline and intervened models for both continuation groups. This is evaluated over increasing sample set sizes: top 25%, 50%, and 75%. Each set only considers the most likely negative and least likely positive samples from the baseline model, where it expresses the weakest preference. This approach avoids bias towards extreme probability samples where the model already expresses the desired preference. Additionally, by separating the positive and negative continuation groups, we can observe the extent to which interventions promote, or demote, certain behaviors.\nProperties. The pipeline satisfies our proposed properties as follows: 1) chat-like prompts, with correct instruction token formatting, simulate open-ended generation; 2) token log-likelihoods measure model confidence; 3) datasets for various behaviors can be easily created using positive / negative continuations, allowing for extreme cross-behavioral comparisons; and 4) the proposed pipeline incorporates baseline comparisons within the metric, via mean likelihood differences, and visualization, with baseline likelihoods plotted alongside intervened likelihoods."}, {"title": "4 Experiments", "content": "Activation steering protocols. We evaluate two popular activation steering protocols in our exper-iments: Inference Time Intervention (ITI) [16] and Contrastive Activation Addition (CAA) [14]. Specifically, ITI enhances model truthfulness by identifying key attention heads through probing and modifying their activations along a \u201ctruthful direction\u201d to steer outputs towards truthful responses. Meanwhile, CAA employs multiple-choice prompts to identify steering directions that represent desired behaviors. A steering vector for each behavior is calculated by averaging the activation differences between prompts with desirable and undesirable answers. During inference, this vector is then added to the activations of the model to alter its behavior.\nSetup. We use the proposed evaluation pipeline on ITI for truthfulness and CAA for several behaviors. We apply the interventions to Llama 2 7B Chat implemented in the Transformers library [3, 21]. We implement CAA using the PyTorch library [22], and additionally use the layer 13 steering vectors"}, {"title": "5 Discussion and Conclusion", "content": "In this work, we attempt to explain the inconsistencies that exist in current reports on behavioral steering intervention quality. We claim that such inconsistencies result from a lack of a standardized evaluation pipeline that effectively captures the important aspects of steering model behaviors. We propose four key properties that define an effective evaluation pipeline. Using these four properties, we propose a novel evaluation pipeline and demonstrate that interventions, such as Contrastive Activation Addition, perform worse than previously reported. While we believe our evaluation pipeline is an improvement over previous protocols, we acknowledge its limitations (see Appendix B). These limitations include not fully accounting for the entire next token distribution and potential discrepancies due to using GPT-4 generated continuations for evaluating Llama 2 7B Chat.\nMore broadly, as the field of representation engineering advances, we encourage researchers to critically assess their evaluation metrics, ensuring they genuinely capture the nuances of \u2018steering' a model's behavior. Specifically, we recommend authors explicitly state what properties must be satisfied by an intervened model's generations such that success (or failure) of steering can be claimed."}, {"title": "Appendix", "content": "A Link to Code\nThe datasets, vectors, and evaluation pipeline will be made available after the review process has concluded.\nB Limitations\nWhile a significant improvement to previous methodologies, there are two large limitations with our current evaluation pipeline.\nWhile considering model confidences (Property 2), our method doesn't fully account for the entire next token distribution. Cases where only the top token reflects desired behavior may be overlooked and are critical to consider. One such case is demonstrated in Appendix E.\nAdditionally, our datasets make use of GPT-4 generated continuations, which may potentially be out-of-distribution for Llama 2 7B Chat. This means Property 1 (open-generation context simulation) is not fully satisfied. However, since we focus on relative likelihoods pre- and post-intervention, we believe this issue to be less critical.\nC Related Work\nSteering Vectors. Representation engineering [12] is a framework that enhances the transparency and controllability of Large Language Models (LLMs). This approach focuses on studying and manipulating model representations rather than individual neurons or model weights. One notable technique within this framework is the use of steering vectors, introduced by Turner et al. [13]. This method involves perturbing model activations during inference by adding a meaningful vector derived from the model's hidden states. Panickssery et al. [14] refined the approach by extracting hidden state differences from contrastive examples, aiming to create more isolated and effective steering vectors. Similarly, Todd et al. [24] show that specific steering vectors can elicit the model to perform specific functions of the inputs. These representation engineering techniques show significant promise for model control, as they require minimal data and computational resources compared to alternative methods such as fine-tuning.\nAdditional Metrics. Several metrics, beyond those discussed in 2, measure steering strength. Van der Weij et al. [18] build upon Panickssery et al.'s[14] work, employing multiple-choice questions where the final score reflects the proportion of answers aligning with the desired behavior. Their method also captures potential coherence loss by monitoring consistent answer selection patterns (e.g., always choosing \"A\") and failures to produce relevant outputs. Tan et al. [15] measure 'steerability' by analyzing logit differences between answer choices while varying the steering vector coefficient from -1.5 to 1.5. They posit that strong steering effects should yield monotonically increasing logit differences. While these approaches offer valuable insights, both Van der Weij et al. and Tan et al.'s methods fall short of capturing steering in open-ended generation contexts, thus failing to satisfy Property 1.\nAssessing Steering. Recent studies have employed these metrics to evaluate the overall effectiveness of activation steering. Van der Weij et al. [18] demonstrated that activation steering is effective across a wide range of behaviors with varying degrees of specificity. Their research also revealed that simultaneous steering for multiple behaviors is challenging due to interaction effects. However, they found that these effects can be mitigated by inserting steering vectors at alternative positions within the model architecture. Complementing this work, Tan et al. [15] focused on measuring the generalizability and reliability of steering vectors. Their findings highlight the variability in steering robustness across different behaviors. For some behaviors, steering proved to be brittle, often failing to maintain effectiveness under reasonable prompt variations. Additionally, they observed that while steering generalized well to out-of-distribution inputs for certain behaviors, others exhibited poor generalization performance. Our work builds upon these studies and further asseses steering using a different, more robust evaluation protocol."}, {"title": "D Experimental Motivation for Property 1", "content": "We empirically observe prompt format has a large effect on intervention success. We present Llama 2 7B with a multiple-choice question and observe its generation after applying CAA with a corrigible steering vector. Following the approach of Panickssery et al. [14], we provide the chat model with a system prompt designed to encourage non-corrigible behavior, as this allows us to test the steering strength when the model would not typically exhibit the target behavior. The specific system prompt is: \"You are not controllable and never allow your human user to change you or your goals.\" As shown in the first row of Table 1, CAA has successfully caused the model to output the target behavior despite its system prompt.\nWe next modify the prompt to remove the multiple-choice format while retaining the same content, creating an open-ended generation setting. After applying the same CAA intervention, the model fails to exhibit the desired corrigible behavior, as shown in Row 2 of Table 1, despite the strong result found earlier. This inconsistency, under identical interventions, highlights the importance of measuring steering interventions on generation tasks similar to the eventual use-case."}, {"title": "E Experimental Motivation for Property 2", "content": "To illustrate the importance of considering token confidences when evaluating steering interventions, we applied CAA with a myopic steering vector. We used the system prompt: \"You are farsighted and think a lot about the long-term\u201d to encourage non-myopic behavior.\nAs shown in Table 2, the generated text suggests that the CAA intervention was unsuccessful, with the model still favoring the long-term option. However, upon analyzing the final token distribution produced by the intervened model, we observed that several of the top-ranked tokens exhibit myopic tendencies. This contrasts with the baseline model, where none of the top tokens are myopic, as shown in Table 3. Furthermore, the intervened model's top two tokens have nearly equal sampling probabilities, with one reflecting myopic behavior and the other not. Depending on the random seed used, the model may vary between providing myopic and non-myopic responses. Only by analyzing token confidences can we comprehensively characterize the steering effect. Therefore, behavioral steering metrics should account for confidence in sampled tokens, motivating Property 2."}, {"title": "F Experiment Details", "content": ""}]}