{"title": "Shape2Scene: 3D Scene Representation Learning Through Pre-training on Shape Data", "authors": ["Tuo Feng", "Wenguan Wang", "Ruijie Quan", "Yi Yang"], "abstract": "Current 3D self-supervised learning methods of 3D scenes face a data desert issue, resulting from the time-consuming and expensive collecting process of 3D scene data. Conversely, 3D shape datasets are easier to collect. Despite this, existing pre-training strategies on shape data offer limited potential for 3D scene understanding due to significant disparities in point quantities. To tackle these challenges, we propose Shape2Scene (S2S), a novel method that learns representations of large-scale 3D scenes from 3D shape data. We first design multi-scale and high-resolution backbones for shape and scene level 3D tasks, i.e., MH-P (point-based) and MH-V (voxel-based). MH-P/V establishes direct paths to high-resolution features that capture deep semantic information across multiple scales. This pivotal nature makes them suitable for a wide range of 3D downstream tasks that tightly rely on high-resolution features. We then employ a Shape-to-Scene strategy (S2SS) to amalgamate points from various shapes, creating a random pseudo scene (comprising multiple objects) for training data, mitigating disparities between shapes and scenes. Finally, a point-point contrastive loss (PPC) is applied for the pre-training of MH-P/V. In PPC, the inherent correspondence (i.e., point pairs) is naturally obtained in S2SS. Extensive experiments have demonstrated the transferability of 3D representations learned by MH-P/V across shape-level and scene-level 3D tasks. MH-P achieves notable performance on well-known point cloud datasets (93.8% OA on ScanObjectNN and 87.6% instance mIoU on ShapeNetPart). MH-V also achieves promising performance in 3D semantic segmentation and 3D object detection.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL), a technique for deriving representations from unan-notated data, has showcased remarkable achievements across a spectrum of domains, including natural language processing [1-5], computer vision [6, 7], and multi-modal learning [8-10]. The efficacy of these techniques often hinges on extensive training with sizable datasets. However, in comparison to images and text, the data desert issue [11] in 3D data has constrained the development of 3D SSL. Amassing extensive scene-level 3D data on a large scale demands dedicated 3D scanning equipment and"}, {"title": "2 Related Work", "content": "3D Backbones. 3D backbone methods encompass a diverse range of techniques, in-cluding point-based methods [39-45], which directly operate on raw point cloud data. Projection-based methods [46-49] transform point clouds into 2D representations. Voxel-based approaches [50-55] employ voxelization techniques and apply sparse convolution exclusively to non-empty voxels. The 3D backbones diverge in network structure for shape-level and scene-level tasks. Networks for shape classification commonly employ successive downsampling to acquire high-level semantic features while maintaining a lower resolution. Conversely, for extracting deeper features with higher resolution, well-known networks for scene-level tasks often incorporate architectures like U-Net or hourglass-like networks [40, 41, 43, 44, 46, 47, 50-54]. For contrastive methods [56, 57] and multi-modal methods [58, 59], backbones originally designed for shape classifica-tion tasks are employed during pre-training. However, it has been demonstrated that the features extracted by these low-resolution backbones are not suitable for tasks that require high resolution representations [60]. Directly utilizing high-resolution shallow features for region and point level tasks, however, does not yield satisfactory results [60]."}, {"title": "3 Methodology", "content": "This section introduces the novel 3D scene representation learning method, S2S, learning representations of large-scale 3D scenes from 3D shape data. In \u00a7 3.1 and \u00a7 3.2, we commence with the introduction of MH-P/V and MH module. MH-P/V is a versatile"}, {"title": "3.1 Point-based MH Backbone", "content": "Fig. 2 (a) provides an overview of MH-P designed for the shape-level tasks. MH-P comprises MH modules. High-resolution features are learned by point head 0. As shown in Fig. 2 (b), each MH module is divided into three parts: subsampling, local aggregation, and high-resolution mapping. In each MH module, we adopt a set abstraction (SA) block [40] for subsampling. One SA block comprises several key components: a subsampling layer that reduces the resolution of incoming points, a grouping layer responsible for identifying neighbors for each point, a series of shared multi-layer perceptrons (MLPs) designed for feature extraction, and a max pooling layer that combines features from neighboring points. Local aggregation consists of a grouping layer, MLPs, and a max pooling layer. High-resolution mapping employs nearest neighbor interpolation to assign low-resolution high-dimensional features to the nearest high-resolution points."}, {"title": "3.2 Voxel-based MH Backbone", "content": "There are significant differences in dataset statistics between shape-level and scene-level data, with ~1k input points for ModelNet40 [15], ~40k input points for ScanNet v2 [13], and ~956K input points for S3DIS [14]. Following the methods for processing large-scale point clouds [50, 55], we design a voxel-based MH backbone with MH modules. Fig.2 (c) provides an overview of MH-V, and Fig. 2 (d) shows the MH module for MH-V. Different from hourglass-like networks, MH-V follows a similar concept to MH-P and learns high-resolution features at the scene level across multiple scales. For simplicity, we retain the symbols used in \u00a73.1.\nAs depicted in Fig. 2 (d), this MH module possesses a structure similar to that of the MH module of MH-P (in \u00a73.1), including featuring subsampling, local aggregator, and upsampling functionalities. Specifically, the voxelizer first defines indices and inverse indices between points and voxels at different scales, as well as indices and inverse indices between voxels at different scales. These indices will be"}, {"title": "3.3 Shape-to-Scene Strategy", "content": "Previous work [29] has demonstrated the significance of point-level representations for 3D scene understanding. Directly training on a single shape and acquiring a global representation might be inadequate for scene-level tasks. To mitigate this concern, it could be advantageous to directly pre-train the network on complex scenes containing multiple objects to more accurately align with the target distributions [29]. Therefore, a series of SSL methods for 3D scene understanding have depended on scene data [29,30,73,74]. However, collecting 3D scene data is financially burdensome and time-intensive. With the development of open-source platforms and Image-to-3D technology, 3D shape data has become more readily accessible. Here, we revisit the challenge of extending 3D SSL from 3D shape data to 3D scene data.\nWe propose S2SS to aggregate multiple objects (shapes) to generate pseudo scene-level training data. This strategy is entirely different from previous methods [29,30,73,74] in terms of how the pre-training data is created. Moreover, it brings the additional benefit that it even allows the natural derivation of positive and negative pairs by leveraging known shape correspondences, without the need for any extra computation. As shown in Fig. 1 (a), given M shapes, we undergo random presampling with 2,048 points for each shape. Each shape is then normalized by rescaling it to fit onto a unit sphere. Through translation, we position various shapes within a shared world coordinate. The Euclidean distance between the barycenters of any two shapes is ensured to be greater than 2, preventing any overlap among shapes. The final output yields pseudo scene data contain-ing M shapes: \\(P_{s} = \\{p_{n}\\}^{M\\times 2048}\\). Next, we use two rigid transformations, denoted as"}, {"title": "3.4 Point-Point Contrastive Loss", "content": "The fundamental principle of contrastive learning hinges on the concept of invariance learning [6,79,80]. This entails that the abstraction of semantics generally remains either invariant or equivariant [81] in the face of various transformed perspectives, such as augmentations [82]. Previous work [29, 83] has demonstrated the significance of point-level representations over global representations. Similarly, our designed MH-P/V aims to acquire point-level/high-resolution features. Therefore, on top of the PointInfoNCE loss [29], we introduce PPC. However, PPC shares a close relationship with S2SS, where the inherent point pairs are naturally obtained without relying on the time-consuming point-level pairing method, FCGF [29, 35]. During pre-training, we utilize o to acquire high-resolution 3D representations. Given input data p, the model \\(E = \\phi \\cup \\theta \\cup \\psi\\) is optimized by:\n\\[L_{P P C}=-\\sum_{(u, v) \\in O_{p}} \\log \\frac{\\exp \\left(\\mathcal{E}\\left(\\mathcal{T}\\left(p_{u}\\right)\\right) \\cdot \\mathcal{E}\\left(\\mathcal{T}^{*}\\left(p_{v}\\right)\\right) / \\tau\\right)}{\\sum_{(u, w) \\in O_{p}} \\exp \\left(\\mathcal{E}\\left(\\mathcal{T}\\left(p_{u}\\right)\\right) \\cdot \\mathcal{E}\\left(\\mathcal{T}^{*}\\left(p_{w}\\right)\\right) / \\tau\\right)}, \\tag{1}\\]\nwhere \\(E = \\phi \\cup \\theta \\cup \\psi\\) represents the whole model, \\(\\mathcal{E}\\left(\\mathcal{T}\\left(p_{*}\\right)\\right)\\) denotes the representation learned by \\(E\\); \\(\\tau\\) is a temperature hyperparameter; and \\(O_{p}=\\{(n, n) \\mid p_{n} \\in P_{s}\\} \\cup \\{(\\iota, j) \\mid p \\in P_{k} \\cap p \\in P_{k}\\}\\) denotes the collection of all positive matches derived from two different views. This approach focuses solely on points that have at least one corresponding match and regards additional non-matched points as negative matches. In a matched pair \\((u, v) \\in O_{p}\\), the point feature \\(\\mathcal{E}\\left(\\mathcal{T}\\left(p_{u}\\right)\\right)\\) operates as the query, and \\(\\mathcal{E}\\left(\\mathcal{T}^{*}\\left(p_{v}\\right)\\right)\\) functions as the positive key. We employ the point feature \\(\\mathcal{E}\\left(\\mathcal{T}^{*}\\left(p_{w}\\right)\\right)\\), where \\(\\exists(\\cdot, w) \\in O_{p}\\) and \\(w \\neq v\\), as the pool of negative keys. Following [29], we choose a subset of 4,096 matched pairs from \\(O_{p}\\) for scene-level downstream tasks. However, we empirically select a subset of 2,048 matched pairs from \\(O_{p}\\) for shape-level downstream tasks. Additionally, we present a PyTorch-style pseudo-code for PPC in the supplementary.\nThe way we obtain \\(O_{p}\\) fundamentally differs from previous methods [29, 30]. On the one hand, rigid transformations maintain the point cloud's order, ensuring a one-to-one correspondence (i.e., \\(\\{(n, n) \\mid p_{n} \\in P_{s}\\}\\)) between \\(X^{1}\\) and \\(X^{2}\\). On the other hand, points originating from the same shape are regarded as positive pairs (i.e., \\(\\{(\\iota, j) \\mid p \\in P_{k} \\cap p \\in P_{k}\\}\\)) across the two views. In this way, we create the scene-level point pair data that is original from single objects. These designs fully exploit the correspondences between points and the relationships within points in the shape."}, {"title": "4 Experiment", "content": "In \u00a74.1, we introduce the self-supervised pre-training settings for MH-P/V. Subsequently, we present the supervised fine-tuning performance on shape-level and scene-level downstream tasks in \u00a74.2 and \u00a74.3, respectively. Furthermore, we conduct ablation studies in \u00a74.4 to validate the effectiveness of each part of our method."}, {"title": "4.1 Pre-Training Settings", "content": "We conduct pre-training of MH-PS on ShapeNet dataset [84] and MH-PH on both the labeled hybrid dataset (LHD) and the unlabeled hybrid dataset (UHD) [28]. MH-PS is a backbone network composed of four MH modules. ShapeNet comprises ~52K synthetic 3D shapes in 55 categories. MH-PS is trained without any post-pre-training, aligning with previous SSL methods [24-27] to enable direct com-parison with them. Moreover, MH-PH is pre-trained on LHD and UHD. MH-PH also consists of four MH modules. However, to explore a high capacity model, we scale it by increasing the network width and the number of local aggregation within the MH modules. The UHD, used for self-supervised pre-training, aggregates point clouds from various sources such as ShapeNet [84], S3DIS [14] for indoor scenes, and Seman-tic3D [85] for outdoor scenes, etc. In total, UHD comprises around 300K point clouds. Conversely, LHD, utilized for post-pre-training, aligns the label semantics from diverse datasets, including ShapeNet [84], S3DIS [14], and other sources, encompassing 87 categories and roughly 200K point clouds in total.\nWe employ a similar setup as described above. The difference lies in combining M shapes to form a scene for pre-training. We denote the MH-V pre-trained on ShapeNet as MH-VS, and the MH-V pre-trained on UHD and LHD as MH-VH. We also scale MH-VH by increasing the network width and the number of local aggregators within the MH modules.\nWe extract the input shapes by sampling 2,048 points from each raw point cloud, and compose M shapes to form a scene. See \u00a73.3 and \u00a74.4. The MH-P model undergoes pre-training for 600 epochs with a batch size of 10. We utilize the AdamW optimizer [86] with an initial learning rate of 0.001, and a weight decay of 0.05. Additionally, we employ cosine learning rate decay [87] based on our empirical findings. The MH-V model undergoes pre-training for 800 epochs on two V100 GPUs with a batch size of 10. Other configurations align with those used for MH-P."}, {"title": "4.2 Shape Level Downstream Tasks", "content": "We perform supervised fine-tuning of MH-PS and MH-PH on three classical downstream tasks. Each shape-level downstream task has thousands of input points.\nScanObjectNN stands out as one of the most formidable 3D datasets, encompassing 15K objects extracted from real-world indoor scans [16]. Within the dataset, there are three commonly utilized data splits, namely OBJ_ONLY (object only), OBJ_BG (with background), and the PB_T50_RS (with background and manual perturbations). Following [16, 24-28], we conduct experiments on the three splits mentioned above and reported overall accuracy (OA) on the respective test sets. As evidenced in Tab. 1, our approach, MH-PS, outperforms PointGPT-S on all three data splits' test sets. Our performance improvements can be attributed to the effective utilization of the multi-scale mechanism. It contributes to the preservation of global features and results in enhanced performance at the shape level tasks, yielding an improvement of approximately 0.8% to 0.9%. Furthermore, MH-PH showcases better performance compared to PointGPT-L."}, {"title": "4.3 Scene Level Downstream Tasks", "content": "We perform fine-tuning for MH-VS and MH-VH on five scene-level downstream tasks. Each scene-level downstream task has hundreds of thousands of input points."}, {"title": "4.4 Ablation Study", "content": "We first compare the training-from-scratch baseline and the fine-tuned network on ScanObjectNN (PB_T50_RS), ShapeNet-Part, and S3DIS Area5, respectively. Specifically, MH-PS and MH-VS are pre-trained on ShapeNet and fine-tuned on the target dataset. The upper part of Tab. 6 details the model configurations. Model A, B, D, and E are employed as training-from-scratch baselines. The backbone of Model A is PointNet++ with an encoder on ScanObjectNN, while it uses PointNet++ with both encoder and decoder on ShapeNetPart. Model C, F leverages pseudo scenes synthesized through S2SS for pre-training data, with PPC as the loss function. In the lower part of Tab. 6, we present the performance on shape-level tasks (ScanObjectNN, ShapeNetPart) and scene-level tasks (S3DIS Area5). On ScanOb-"}, {"title": "5 Conclusion", "content": "In summary, our research introduces S2S, a novel 3D scene representation learning method. By leveraging readily available 3D shape datasets and introducing innovative architectural designs, data strategies, and loss functions, S2S overcomes the data desert issue and bridges the gap between shape-level and scene-level datasets. This approach demonstrates remarkable adaptability and transferability across both shape-level and scene-level datasets, as evidenced by the impressive performance achieved on eight well-established benchmarks. S2S represents a significant step forward in addressing the challenges faced by current 3D SSL for scene understanding. However, despite its notable achievements, challenges still lie ahead, particularly in extending the application of S2S to broader 3D contexts, such as open-world scene understanding. Expanding S2S's capabilities to encompass these more complex scenarios will require further research and innovation. We intend to delve into these challenges in our future research."}, {"title": "A Experiment Setting", "content": "PointGPT [28]\u00b9 has been released under an MIT license. VoteNet [91]2 has also been implemented under an MIT license. The experiments involve publicly available datasets that have been widely applied for 3D research. ModelNet40 [15]\u00b3, ShapeNet [84]4, ShapeNetPart [36]5, and S3DIS [14]6 have custom licenses that only allow academic use. Synthia4D [38]7 is published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. ScanObjectNN dataset [16]8 is released under the 'ScanOb-jectNN Terms of Use'. ScanNetv2 [13] is released under the \u201cScanNet Terms of Use'\".\nSoftware and hardware environment:\""}, {"title": "B Voxel-based MH Backbone (MH-V)", "content": "Figs. A1 (a-d) demonstrate the pre-training and fine-tuning of MH-V for scene-level downstream tasks, encompassing 3D semantic segmentation and object detection tasks. The pre-training data is obtained by S2SS. Eq. (1) represents PPC. The details of S2SS and PPC can be found in the main paper.\nFig. A1 (a) visually demonstrates the pre-training process of MH-V backbone for the semantic segmentation task. It is constructed atop four MH modules (as referenced in Fig.2 (d) of the main paper). These modules are configured to operate at scales of 2, 4, 8, and 16, respectively. Specifically, within the MH module designated for scale S, its input derives from the output 15 of the former MH module (with a scale of S'). Additionally, x transitions into the input for the subsequent MH module. Similar to MH-P backbone, high-resolution mapping is implemented within each MH module, seamlessly integrating all the as the input for the point head. This innovative architecture empowers MH-V with direct access to high-resolution data and integrated features spanning various scales. Subsequent to the pre-training phase, we apply MH-V backbone to the downstream semantic segmentation task, as illustrated Fig. A1 (b).\nFigs. A1 (c-d) show the MH-V backbone for pre-training and the downstream object detection task, respectively. The backbone is built on the top of four MH modules (see"}, {"title": "C Limitation and Social Impact", "content": "We have extensively researched three classical shape-level downstream tasks and five scene-level benchmarks: S3DIS [14], Synthia4D [38], ScanNet v2[13], and Semantic-KITTI [37]. Our research has narrowed the gap between 3D shape and scene-level datasets. However, there are remaining challenges, including expanding S2S's applica-tions to broader 3D scenarios, encompassing more challenging open-world scenes. Open world of point clouds (see [97,98]) is a new downstream task, S2S may be helpful for it. We will address this challenges and conduct research on open-world 3D perception tasks, such as on ScanNet v2[97] and SemanticKITTI [98], in our future work.\nWith the potential emergence of large-scale shape-level datasets, exploring research using a pretext task grounded in these datasets gains increased significance. Within real-world contexts, endeavors concentrating on scene-level perception are closely linked to augmented reality, urban planning, and autonomous vehicles. This research contributes to bridging the divide between shape-level and scene-level datasets. It holds promise for advancing various facets of 3D technologies, spanning from 3D self-supervised learning to scene comprehension and beyond."}]}