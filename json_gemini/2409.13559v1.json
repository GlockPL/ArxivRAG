{"title": "Efficient Visualization of Neural Networks with Generative Models and Adversarial Perturbations", "authors": ["Athanasios Karagounis"], "abstract": "This paper presents a novel approach for deep visualization via a generative network, offering an improvement over existing methods. Our model simplifies the architecture by reducing the number of networks used, requiring only a generator and a discriminator, as opposed to the multiple networks traditionally involved. Additionally, our model requires less prior training knowledge and uses a non-adversarial training process, where the discriminator acts as a guide rather than a competitor to the generator. The core contribution of this work is its ability to generate detailed visualization images that align with specific class labels. Our model incorporates a unique skip-connection-inspired block design, which enhances label-directed image generation by propagating class information across multiple layers. Furthermore, we explore how these generated visualizations can be utilized as adversarial examples, effectively fooling classification networks with minimal perceptible modifications to the original images. Experimental results demonstrate that our method outperforms traditional adversarial example generation techniques in both targeted and non-targeted attacks, achieving up to a 94.5% fooling rate with minimal perturbation. This work bridges the gap between visualization methods and adversarial examples, proposing that fooling rate could serve as a quantitative measure for evaluating visualization quality. The insights from this study provide a new perspective on the interpretability of neural networks and their vulnerabilities to adversarial attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancements in deep learning have driven significant progress in various domains, including computer vision, natural language processing, and autonomous systems. However, as deep neural networks (DNNs) become more complex, the need for interpretability and visualization of these models has become critical. Understanding what a neural network learns and how it makes decisions is essential, especially in safety-critical applications such as medical diagnostics, self-driving cars, and security systems.\nRecent methods for visualizing DNNs, such as those proposed by Nguyen et al. [10], rely on generating images that represent the preferred inputs of neurons through adversarial training. While these techniques have demonstrated promising results, they often suffer from overly complex architectures and reliance on multiple pre-trained networks. Additionally, many of these methods involve adversarial training processes that are computationally expensive and difficult to interpret.\nIn this paper, we propose a simplified yet effective method for deep visualization using generative networks. Our model reduces the architectural complexity by employing only two networks: a generator and a discriminator. Unlike existing methods, which use multiple networks for encoding, generating, and comparing images, our approach streamlines the process while maintaining high-quality visual outputs. Furthermore, our model only pre-trains the discriminator on real datasets, while the generator is trained under the supervision of the discriminator, without the need for adversarial training.\nAn additional focus of this work is the relationship between visualization techniques and adversarial examples. Adversarial examples are subtly modified inputs designed to mislead neural networks into incorrect predictions. These examples reveal the vulnerabilities of DNNs and raise important questions about their robustness. We show that our visualization results can be utilized as effective adversarial perturbations, capable of fooling DNNs with high accuracy. Our experiments demonstrate that this method can achieve a fooling rate of up to 94.5% with minimal perturbation, illustrating both the power and the interpretability of our approach.\nThis paper is organized as follows: Section 2 introduces related work, including generative adversarial networks and adversarial examples. Section 3 outlines the architecture of our proposed model, and Section 4 details the training process. Section 5 presents experimental results and analysis. Finally, Section 6 concludes with a discussion of the implications and future directions of our research."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Generative Adversarial Networks", "content": "As claimed in [1], their model used four convolutional networks: an encoder network E, a generator network G, a \"comparator\" network C and a discriminator network D. Our model only uses two networks: a generator and a discriminator. Noth the E and C have been trained on real image dataset. Both the G and D have been trained adversarially before. For our model, only the discriminator network has been pre-trained on the real dataset. Also, the training of our model is not adversarially, the discriminator only acts as a leader of generator. The discriminator is used to judge whether the generated image is fake or not. For our method, the discriminator is used to classify images to 1000 classes. Besides, they also used a \u201ccomparator\" network to measure the difference between generated image and real image. Our model has no particular mechanism to constrain the interpretation of the generated images. Our goal is to obtain a model, and then use the model to generate visualization images. Their goal is to obtain different codes, and then use the codes to generate visualization images."}, {"title": "B. Adversarial Examples", "content": "Adversarial examples [2] are a type of input data that have been modified very slightly with the intent of causing a machine learning classifier to misclassify the input. For example, given an image of a panda, an attacker can add a small perturbation that has been calculated to make the image be recognized as a gibbon with high confidence. This process is referred to as fooling the classifier. Most methods [2] for generating adversarial image examples are based on a specific image: that is, given an image with a specific, mistaken, targeted label, we can calculate a perturbation corresponding to that image. However, that perturbation may not be effective when added to another image. There is also a type of perturbation noise called universal perturbation [3], which can be added to different images and cause the network to make mistakes [4], [5]; this method has no pre-defined incorrect label, and is therefore non-targeted.\nBoth of the above methods are optimization-based. In this paper, we find that our generated visualization results can easily be used as perturbations added to natural images to generate adversarial examples. More details will be provided in the experiment section."}, {"title": "III. PROPOSED METHOD", "content": ""}, {"title": "A. Network Architecture", "content": "The proposed visualization architecture is illustrated in Figure 1. It consists of two subnetworks: a generative network (left) and a discriminative network (right). The discriminator network is actually a typical classification network that needs to be visualized. In this paper, we use the CaffeNet [6] and VGG16 net [7] that has already been trained on the ImageNet dataset [8] to perform the experiment. Other image classification networks can also be easily visualized following the same procedure. Layers for discriminator network:\n\u2022 11-16: Common image features extracting layers of classification network, it can be convolution, pooling or activation layers [9]. Outputs of these layers are 3D matrixes, containing\nfeature maps with multiple channels.\n\u2022 17-18: Fully connected layers. 17 is used convert the 3D matrix into 1D vectors. 18 is used to change the length of 1D vector to exactly the number of classes.\n\u2022 softmax layer: An activation layer which can exponentially normalize the 1D vector to sum value of 1. It is common used as the output layer for the discriminator network. The output value of each neuron represent the possibility for each class. If we want to visualize that class, we will activate that corresponding neuron(maximize the corresponding possibility).\nThe generator network is used to output a visualization image corresponding to a class id as input. Common image generation network is deconvolution network [10], [11]. However, just inputting the label information into the first layer of the generator is hard for the model to converge, some input information of the first layer might get lost when it goes through the whole network because of vanishing gradient problem [12]. To overcome this problem, the input information should be strengthened. We learned from the skip-connected thought from ResNet [13] and feed the input information to modules in different depths of the generative network. The block module(the first row of Fig. 1 is designed to encode the class id information into dense matrix and combined the matrix with data of layers in different depth. This module is critical for the generator to be label directed."}, {"title": "B. Training Process", "content": "We use N to represent the number of classes of the classification network, x represents the input class label among N classes. For example, there are 1000 classes in the ImageNet dataset [1], thus N equals to 1000 here and the length of the discriminator's output vector is also 1000. Training steps:\n\u2022 Random choose class id x between 0 and N.\n\u2022 Feed x to the generator, then the generator generate an image G(x).\n\u2022 Feed the generated image to the image classification network D, then the network can produce a prediction vector D(G(x)).\n\u2022 Calculate the categorical cross-entropy loss between the one hot encoding of initial class id (represented by $Gencode(x)$ and prediction possibility vector $Dout(G(x))$.\n\u2022 Taking the two network as a whole, and using the above Loss, the gradient for each layer can be calculated, we freeze the discriminator and only update the weights of generator\nThrough the training procedure of the network, the above steps are repeated once and once, until the Loss reaches a small value. Then the generator is what we want, given an class id x, the generated image G(x) is the corresponding visualization image.Besides classification layer, our model can also be used to visualize the middle layers of networks. Each layer usually has several channels. In this paper, we take all the neurons of each channel as a whole and try to generate images that can activate all of the channel neurons. Our above model needs to be modified a little to realize this new task. For the discriminative network, we also use the pre-trained classification network. The output layer is not the last layer of the network but the layer we want to visualize. For each channel in a layer, the mean value of all the neuron activations on this channel is taken as the activation of the whole channel. Therefore, for each layer, the number of activations is equal to the number of channels in this layer. Then, we add a Softmax layer after the channel activation layer. Different channels are regarded as the previous different classes. Compared with the model to visualize class labels, this model replaces the label indexes with channel indexes. Finally, the same generative network and training strategy are used to visualize different channels of the middle layers."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In the experiment, we find that the generated image matrix can be used as perturbation on natural images. For natural images, we use the dataset of NIPS2017 Adversarial Attacks and Defences Competition, which contains 1000 images belonging to different labels. For the fooled classification network, we used the VGG16 network to generated perturbation and classify images before and after adding the perturbations. The values of generated matrix are between [-1,1], we multiply the matrix with integer coefficient and add that to natural images.\nThe above experiments showed the effectiveness for our method to generate adversarial examples. Different from previous adversarial examples generation methods, the whole process does not rely on complex math formula to realize optimization [1], [15], and the results generated from our method can meet both the needs of being targeted and can taking effect on different images. This work make a bridge between visualization methods and adversarial examples, as they are both methods to interpret inner mechanism of deep neural networks. Also, we hope that the fooling rate can be a good measure of visualization quality."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced a novel approach to deep visualization using a generative network, offering significant improvements over existing methods. By reducing the architectural complexity and eliminating the need for adversarial training, our model provides an efficient yet powerful means of generating visualization images for neural networks. Our streamlined method, which uses only a generator and a discriminator, ensures that the visualization process remains both interpretable and computationally efficient. We also explored the connection between visualization and adversarial examples, demonstrating that the visualizations produced by our model can be repurposed as adversarial perturbations. Our experiments showed that the generated adversarial examples can fool state-of-the-art image classification networks with an adequate fooling rate, even with minimal perturbation that is nearly imperceptible to the human eye. This finding underscores the dual purpose of our approach: not only does it enhance the interpretability of neural networks, but it also highlights their vulnerabilities to adversarial attacks."}]}