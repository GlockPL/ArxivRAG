{"title": "RingGesture: A Ring-Based Mid-Air Gesture Typing System\nPowered by a Deep-Learning Word Prediction Framework", "authors": ["Junxiao Shen", "Roger Boldu", "Arpit Kalla", "Michael Glueck", "Hemant Bhaskar Surale", "Amy Karlson"], "abstract": "Text entry is a critical capability for any modern computing experience, with lightweight augmented reality (AR) glasses\nbeing no exception. Designed for all-day wearability, a limitation of lightweight AR glass is the restriction to the inclusion of multiple\ncameras for extensive field of view in hand tracking. This constraint underscores the need for an additional input device. We propose a\nsystem to address this gap: a ring-based mid-air gesture typing technique, RingGesture, utilizing electrodes to mark the start and end\nof gesture trajectories and inertial measurement units (IMU) sensors for hand tracking. This method offers an intuitive experience\nsimilar to raycast-based mid-air gesture typing found in VR headsets, allowing for a seamless translation of hand movements into cursor\nnavigation. To enhance both accuracy and input speed, we propose a novel deep-learning word prediction framework, Score Fusion,\ncomprised of three key components: a) a word-gesture decoding model, b) a spatial spelling correction model, and c) a lightweight\ncontextual language model. In contrast, this framework fuses the scores from the three models to predict the most likely words with\nhigher precision. We conduct comparative and longitudinal studies to demonstrate two key findings: firstly, the overall effectiveness of\nRingGesture, which achieves an average text entry speed of 27.3 words per minute (WPM) and a peak performance of 47.9 WPM.\nSecondly, we highlight the superior performance of the Score Fusion framework, which offers a 28.2% improvement in uncorrected\nCharacter Error Rate over a conventional word prediction framework, Naive Correction, leading to a 55.2% improvement in text entry\nspeed for RingGesture. Additionally, RingGesture received a System Usability Score of 83 signifying its excellent usability.", "sections": [{"title": "1 INTRODUCTION", "content": "This paper focuses on one-handed text entry methods, necessitated\nby the occasional unavailability of both hands [13, 25, 29, 79], for\nlightweight augmented reality (AR) glasses in contrast to fully-fledged\nAR headsets (such as Apple Vision Pro [7, 15], Quest Series [48], and\nHoloLens Series [14, 49]). We have analyzed prior research on one-\nhanded text entry and found that most existing methods struggle with a\nrange of limitations. These limitations include learnability challenges,\nlower performance ceiling, and intricate device setup. Learnability\nchallenges stems from an indirect correlation between hand movement\nand keyboard key selection, and the introduction of numerous new\nkeyboard layouts (with new key arrangements) [23, 25\u201327, 30, 37, 56].\nAdditionally, these methods typically demonstrate low entry speeds\nbelow 15 words per minute (WPM). Some other methods also require\nintricate device setup procedures that require specific auxiliary devices\nlike capacitive sensors on the fingertips [53, 75]. Our observations\nhighlight that gesture typing with the cursor directly mapped from body\nmovements tends to yield the highest text entry rates, as evidenced\nby the systems developed by Markussen et al. [45] using hand control\n(20.6 WPM), Yu et al. [77] using head control (19.0 WPM), and Zhao et\nal. [79] using arm control (16.4 WPM). This efficiency can be attributed\nto the simplicity and intuitiveness of direct cursor projection combined\nwith the rapid text entry facilitated by gesture typing. Among these,\nVulture [44] methods offer the highest text entry rates, and are theo-\nretically more ergonomic and less fatiguing when compared to using\nhead and arm. However, they utilized OptiTrack [21] for hand tracking,\nan outside-in tracking method that comes with inherent deployment\nchallenges. It requires the instrumentation of the user's entire arm to\ntrack movements of the arm, wrist, and fingers, and may at times lose\ntracking due to marker occlusion.\nConsequently, we leveraged a ring device proposed by Kienzle et\nal. [32] to track hand positioning. This ring can track hand movements\nusing its built-in IMU and detect stateful pinch actions using integrated\nelectrodes [32]. To this end, we refined our design space to concentrate\non evaluating word-level gesture typing versus phrase-level gesture\ntyping, the latter being less explored [74]. We conducted a comparative\nstudy with 32 participants. Results showed no significant text entry\nperformance difference between the two methods, but a preference\nfor word-level typing emerged. We also tackled the Heisenberg Effect\nchallenge, associated with input cross modality [72] while performing\nmid-air pointing when with the discrete pinch actions, by introducing\na customized filter-based algorithm. Even still, IMU-based tracking\ninevitably introduces noise and drift into the cursor's trajectory, lead-\ning to inaccurate gesture typing decoding. To guarantee swift and"}, {"title": "2 RELATED WORK", "content": "Various studies of text entry methods in AR/VR suggest typical entry\nrates of 5 to 26 WPM [18, 19, 63, 64, 73]. It is evident that leveraging\nusers' existing typing skills with QWERTY keyboards and simple\ninteractions provides the best performance [17, 20, 33], while abstract"}, {"title": "2.1 One-Handed Text Entry", "content": "This section critically reviews and compares notable one-handed text\nentry research. Our focus is narrowed to those methods that have\nachieved performance rates near or surpassing 10 WPM and utilize\nQWERTY keyboards. Table 1 presents a comparative overview of these\none-handed text entry studies.\nMarkussen et al. [44] proposed Vulture which is a mid-air gesture\ntyping keyboard, and utilized OptiTrack hand tracking for Directly\nMapped Cursor control via hand movements, achieving an entry rate\nof 20.6 wpm. This method exemplifies high efficiency in text entry\nby leveraging intuitive hand movements, closely mirroring physical\ninteractions in the real world. The direct manipulation facilitated by this\napproach suggests a significant potential for enhancing user experience\nin AR and VR environments through natural interaction paradigms.\nPlease note the term Directly Mapped Cursor does not refer to the\nfingertip directly touching the keyboard. Instead, Directly Mapped\nCursor refers to the direct mapping between cursor movement and\nfinger movement, rather than direct physical contact.\nZhao et al. [79] also employed a similar Directly Mapped Cursor\ncontrol-based mid-air gesture typing technique, but utilized arm move-\nments instead, incorporating a wristband with in-built IMU to track the\narm's position. They achieved an entry speed of 16.4 WPM. Despite\nintroducing the 'Speedup' method, which accelerates the cursor to-\nwards the user's gaze fixation point to enhance text entry rate, the final\nimproved speed reached only 17.1 WPM, which is still significantly\nlower than the 20.6 WPM achieved by Vulture [44]. One factor to\nthis reduced speed is that using the arm for control introduces more\nextensive movement, inherently leading to slower speeds. Additionally,\nthis method results in greater fatigue as more torque is needed for the\narm ($a$) as compared to the wrist (${\\psi}$) because the arm's greater length,\nwhich then demands more force for the same orientation change (see\nFigure 2.).\nSimilarly, Yu et al. [76] introduced a technique based on head ori-\nentation control via a headset to navigate the cursor, with an entry rate\nof 19.0 wpm. This method diverges from hand-based interaction by\nutilizing head movements for text entry, presenting an alternative that,"}, {"title": "2.2 Word Prediction in Text Entry", "content": "Word prediction in text entry systems involves word decoding, spelling\ncorrection, and language modeling. The decoding model interprets the\nuser's input patterns into raw predictions, forming a sequence of charac-\nters, but it may introduce spelling errors. The spelling correction model\nrectifies these inaccuracies, while language modeling further enhances\nword prediction by considering the context provided by previous words.\nShen et al. [63] connect these components sequentially, as illustrated in\nFigure 3. The word prediction system, referred to as Naive Correction\nin this paper, enhances accuracy through a sequential process involving\nedit-distance-based spelling correction and N-Gram language model\nre-ranking. Despite this system being the state-of-the-art word predic-\ntion framework for gesture typing, it has several drawbacks: it only\nconsiders limited contextual scope, ignoring global probabilities across\nthe entire dataset. Additionally, it lacks deep semantic understanding,\nleading to potential inaccuracies in certain contexts. We propose a\nnovel word prediction system that integrates correction and language\nmodeling through a probabilistic approach, allowing the system to\nconsider global probabilities rather than processing them sequentially.\nAdditionally, our approach enhances spelling correction by incorporat-\ning spatial information from the keyboard and advances the N-Gram\nlanguage model by utilizing much longer contexts, while maintaining a\nlightweight design.\nGeneral spelling correction encompasses a wide array of contexts, in-\ncluding document editing and processing digital texts in databases or on\nonline platforms. Its primary objective is to identify and correct errors\nthroughout entire sequences of words [8, 12, 54]. In contrast, spelling\ncorrection in text entry systems on mobile devices is specifically tai-\nlored for real-time user inputs. It focuses on correcting errors in single\nwords as they are typed, where the errors not only come from users\nbut also from text decoders in a probabilistic text entry system [20].\nSpelling correction in text entry systems commonly relies on the edit\ndistance method, prized for its ease of integration with custom word\ncorpora and modifications to achieve microsecond latency [36, 50].\nHowever, its accuracy for QWERTY-based text entry systems is lim-\nited [8]. One of the major reasons is that it fails to consider the spatial\ninformation of the keyboard. This oversight leads to reduced correc-\ntion accuracy by offering phonetically or orthographically similar but\nirrelevant corrections, and to inefficient error prediction due to the\ninability to accurately anticipate mistyped words based on common\nfinger movements and miskeying patterns, thus diminishing its overall\neffectiveness. Our paper addresses this gap by proposing a novel spa-\ntial edit distance that incorporates the spatial information of a gesture\ntyping keyboard. Additionally, we transformed the spatial edit distance\ninto a probabilistic-based measure, allowing seamless integration with\na probabilistic word decoder.\nLanguage modeling represents another important approach for en-\nhancing word error correction. N-gram language models are commonly\nutilized for meeting latency demands owing to their ease of imple-\nmentation and explainability [31, 52]. With the advancements in deep\nlearning technologies, models based on deep learning have been pro-\ngressively incorporated into language modeling [9, 22, 68]. However,\nthere has been limited research on contextual language modeling for\ntext entry systems. Contextual information is a crucial element in text\nentry, which includes elements such as conversation history and other\ncontext tags like places, times, and hobbies of the users, etc. Shen et\nal. [65] proposed for the first time a contextual language model based on\nGPT-2 for Augmentative and Alternative Communication (AAC) use\ncases. However, the Generative Pre-trained Transformer-2 (GPT-2) [55]\nmodel has significant latency when operated on a mobile device, while\nthis latency is acceptable for the AAC use case in Shen et al. [65]. We\npropose a novel method that transforms contextual language modeling\nthrough pre-training with Long Short Term Memory (LSTM) models\ninstead of pre-training with transformers. This results in a lightweight\nmodel architecture, with the final contextual language model being\nonly 7 megabytes (MB) in size and capable of running in real-time on\na mobile device."}, {"title": "3 USER STUDY 1: WORD-LEVEL VERSUS PHRASE-LEVEL\nGESTURE TYPING", "content": "We began by investigating whether the simple act of removing the de-\nlimitation requirements between words could accelerate mid-air gesture\ninput under the Directly Mapped Cursor interaction mode. As we are\nproposing a novel and comprehensive text entry system that ranges\nfrom interaction design to backend architecture design, the choice be-\ntween word-level and phrase-level typing is a fundamental component\nof the interaction design. Therefore, it is important to conduct a study\nto explore this aspect.\nWhile Xu et al. [74] have conducted studies comparing phrase-level\nto word-level gesture typing on smartphones, their approach relies on\nDirect Touch. This differs from our Directly Mapped Cursor mode.\nConsequently, Xu et al.'s findings [74] may not be directly applicable to\nour context. Therefore, we conducted our own study using a touchpad\nto simulate Directly Mapped Cursor interaction. Controlling a cursor\nby swiping on a touchpad directly translates fingertip movements into\ncursor movements on the screen. This method is particularly advanta-"}, {"title": "4 RINGGESTURE", "content": "Study 1 investigated the interaction design of the RingGesture system.\nThis section provides a comprehensive overview of the backend archi-\ntecture design of the RingGesture system. It includes the algorithm\ndesigned to overcome the Heisenberg Effect challenges associated with\ninput cross modality while performing mid-air pointing when with the\ndiscrete pinch actions [72], and our novel deep-learning word prediction\nframework, Score Fusion."}, {"title": "4.1 Ring-Based Mid-Air Pointing and Selection", "content": "We created a ring device with a reference design from ElectroRing pro-\nposed by Kienzle et al. [32]. This ring detects touch and release events\nof a pinch gesture by monitoring changes in an electrical signal. Fur-\nthermore, the ring uses an IMU for 2D cursor tracking by transforming\naccelerometer and gyroscope data into quaternions, converting these\ninto polar coordinates, and then mapping them to Cartesian coordinates.\nThe gain parameter for the control display is set to 1.8.\nWhile we effectively utilize the ElectroRing design [32] for pinch\ndetection and IMU-based 2D cursor tracking, our system encountered\na challenge in the context of mid-air gesture typing: Heisenberg Effect\nassociated with input modality crosstalk [72]. This issue arises when a\ndiscrete input like a pinch inadvertently alters the virtual cursor's posi-\ntion, resulting in an inaccurate selection point during mid-air pointing\nand selection interactions.\nTherefore, to counteract the abrupt displacement introduced by pinch\nactions, we suggest a filter-based strategy. This approach dynamically\ndetermines the filtering level within an exponential smoothing filter by\nresolving the subsequent optimization problem:\n$\\alpha = \\underset{\\alpha}{argmin} \\left[ \\lambda \\sigma \\frac{\\alpha}{2-\\alpha} + (1-\\lambda) \\frac{(1-\\alpha)\\Delta}{\\alpha} \\right]$\nIn this equation, $\\sigma$ signifies the level of sensing noise, $\\Delta$ provides\nan estimate of the signal's velocity, and $\\alpha$ serves as a parameter that\nbalances noise rejection (the left term in the minimization above) and\ninfinite horizon tracking error in response to an input ramp (the right\nterm in the minimization above). The $\\lambda$ parameter is preset to 0.75\nwith preliminary experiments."}, {"title": "4.2 Score Fusion", "content": "To mitigate the issues posed by input signal noise, such as hand jitter\nand IMU drift that lead to inaccuracies in word-gesture decoding, we\nhave developed a deep-learning framework for word prediction, Score\nFusion. This framework consists of three distinct components that\ncompute the logarithmic probability of a word within a corpus based\non a given word-gesture trajectory, as illustrated by Figure 5. These\nindividual scores are then consolidated to provide a composite score\nfor the words across the corpus. Subsequently, we reorder these scores\nto present the highest-ranked words as the suggested options."}, {"title": "4.2.1 Word-Gesture Decoding Model", "content": "Deep-learning-based decoders [6, 63", "35": "."}]}