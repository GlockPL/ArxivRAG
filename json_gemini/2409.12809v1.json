{"title": "Don't be Fooled: The Misinformation Effect of Explanations in Human-AI Collaboration", "authors": ["Philipp Spitzer", "Joshua Holstein", "Katelyn Morrison", "Kenneth Holstein", "Gerhard Satzger", "Niklas K\u00fchl"], "abstract": "Across various applications, humans increasingly use black-box artificial intelligence (AI) systems without insight into these systems' reasoning. To counter this opacity, explainable AI (XAI) methods e enhanced transparency and interpretability. While recent studies have explored how XAI affects human-AI collaboration, few have examined the potential pitfalls caused by incorrect explanations. The implications for humans can be far-reaching but have not been explored extensively. To investigate this, we ran a study (n=160) on Al-assisted decision-making in which humans were supported by XAI. Our findings reveal a misinformation effect when incorrect explanations accompany correct Al advice with implications post-collaboration. This effect causes humans to infer flawed reasoning strategies, hindering task execution and demonstrating impaired procedural knowledge. Additionally, incorrect explanations compromise human-AI team-performance during collaboration. With our work, we contribute to HCI by providing empirical evidence for the negative consequences of incorrect explanations on humans post-collaboration and outlining guidelines for designers of AI.", "sections": [{"title": "1 Introduction", "content": "Imagine you are studying for an art history exam and must know how to distinguish two architectural styles. You seek advice from an online artificial intelligence (AI) assistant for explanations to differentiate the two styles. Despite being plausible, the Al's explanation is incorrect. Yet you learn from these incorrect explanations, and as a consequence, your understanding of architectural styles is impaired! You fail your ex\u0430\u0442.\nRecent rapid technological advancements, in particular generative AI with the popular example of ChatGPT, have significantly bolstered both the capabilities and the adoption of AI [14, 27]. However, as AI technologies get more sophisticated, the complexity and opacity of supported decision-making processes increase as well, presenting new challenges to ensure that these systems are still transparent and interpretable for humans [71]. This increasing complexity necessitates a deeper understanding of the underlying factors impacting human-AI interaction, especially in scenarios where human oversight is critical [5, 89]. Regulatory frameworks, such as the EU AI Act, mandate human oversight to ensure Al systems operate ethically, legally, and safely [24]. This underscores the significance of HCI research to develop methods and tools that facilitate effective collaboration between humans and AI [81]. Ensuring that Al is not"}, {"title": "2 Background", "content": "With the rapid advancement of AI and its integration into diverse decision-making processes, XAI has emerged as a critical technique for enhancing transparency and assistance to help decision-makers understand Al's reasoning [4]. Especially with applications based on generative AI (like Open AI's ChatGPT or Anthropic's Claude), explanations are being generated in natural language that provide human-understandable support for the Al's response [83, 106]. Previous research on human-AI collaboration has predominantly focused on how AI explanations influence human decision-making [76, 90]. For example, Hemmer et al. [41] examine the factors that impact human-Al team performance, suggesting that XAI can foster complementary collaboration between humans and AI. Albeit the positive effects, studies also highlight how XAI can impair the collaboration between humans and AI [13, 32, 59]. However, the understanding of negative consequences of XAI is still limited [60], especially empirical evidence for the negative impact of incorrect explanations is missing. In Table 1, we sort recent works in AI-assisted decision-making according to the correctness of AI advice and explanations. The table shows the under-explored topic of incorrect explanations in HCI. We review recent literature that highlights the risks and limitations of collaborative settings between humans and AI, motivating the need for further exploration of how incorrect explanations impair humans."}, {"title": "2.1 Incorrect Al Advice in Human-Al Collaboration", "content": "Research in HCI has explored the effects of incorrect AI advice on human-AI collaboration [9, 54]. Kocielnik et al. [54] propose three strategies for managing user expectations in scenarios involving incorrect Al advice: accuracy indicators, example-based explanations, and performance control. Through a study involving an AI-based scheduling assistant prone to errors, they demonstrate these techniques' effectiveness in maintaining user satisfaction and acceptance despite occasional incorrect recommendations. Their findings also reveal that the specific nature of Al errors can significantly influence user perception and trust."}, {"title": "2.2 Incorrect Explanations in Human-Al Collaboration", "content": "Next to the Al advice, the understanding in HCI of how incorrect explanations can have an impact in Al-assisted decision-making is limited. Only few studies investigate the effect of explanations' incorrectness [17, 56, 61, 64]. A recent study in HCI shows that not only incorrect advice but also incorrect explanations have the potential to deceive decision-makers [61]. The authors of Morrison et al. [61] explore the negative impacts of such incorrect explanations on humans' decision-making behavior. They extend the conceptualization of Schemmer et al. [75] by the explanation dimension and explore, in a bird classification study, how the correctness of explanations impacts humans' reliance on AI. They show that incorrect explanations can deceive decision-makers, leading to an inappropriate reliance behavior. Cabitza et al. [17] explore the effects of explanations in a logic puzzle task. They also show that incoherent explanations can mislead humans, resulting in an inappropriate reliance behavior. In another study, Papenmeier et al. [64] run a study in AI-assisted decision-making with incorrect explanations. They study how the explanations affect humans' trust in determining the publication of Tweets. Similarly, Lakkaraju and Bastani [56] find that incorrect explanations can affect humans' trust in Al by investigating their effects in law and criminal justice use-cases.\nThese studies collectively underscore the complex dynamics of human-Al interaction, especially how incorrect advice and incorrect explanations affect humans' reliance behavior on AI. However, the HCI field lacks a deeper understanding of the effects of such impaired collaboration scenarios on humans themselves. Especially for scenarios in which not the advice but the explanation for the decision maker-intended to foster interpretability-is incorrect. Studies like Morrison et al. [61] build a promising starting point to inform HCI researchers and practitioners of the downsides of incorrect explanations. However, we still do not know anything about the impact of these AI shortcomings on humans' ability to perform the tasks autonomously (procedural knowledge) and to conclude about the underlying domain (reasoning) post collaboration."}, {"title": "3 Theoretical Development", "content": "In the evolving field of I, understanding the impact of AI explanations on decision-making has become critical. Explanations serve as a bridge between AI and humans, influencing trust, reliance, and collaboration [42, 75, 77]. This work investigates how incorrect explanations, when paired with accurate AI advice, can mislead humans, potentially impairing their procedural knowledge and reasoning capabilities.\nSchemmer et al. [75] highlight the role of AI advice in AI-assisted decision-making, emphasizing how they shape humans' reliance on AI. Next to the advice, explanations influence the collaboration between humans and AI. Morrison et al. [61] extend this understanding, exploring the effects of both correct and incorrect explanations on human reliance on AI. Their findings suggest that even when Al advice is accurate, incorrect explanations can distort human perceptions and decision-making processes, leading to an inappropriate reliance on AI. Building on these insights, our research focuses on a specific aspect of the work of Morrison et al. [61]: the misleading potential of incorrect explanations in the context of correct AI advice. While prior research has taken the first steps towards investigating the impact of incorrect explanations, still little is known about the repercussions on the ability of human recipients to perform tasks, namely their procedural knowledge, and to conclude based on their understanding, namely their reasoning, after the collaboration with the AI. We examine how such flawed explanations affect humans' procedural knowledge, reasoning, and overall human-AI collaboration, contributing new insights to the growing body of literature on Al-assisted decision-making.\nIn AI-assisted decision-making, declarative knowledge (the \"know-what\") and procedural knowledge (the \"know-how\") are both crucial. Declarative knowledge is factual knowledge or information that humans hold. When exposed to incorrect explanations, incorrect explanations can distort this knowledge base, leading to flawed reasoning, as posited by Stenning and Van Lambalgen [88].\nProcedural knowledge refers to the ability to perform tasks and make decisions. The acquisition of procedural knowledge is highly sensitive to cognitive load, as discussed by Sweller [91]. Incorrect explanations can not only impair procedural knowledge, but also increase cognitive load, leading to difficulties in learning and applying procedures effectively. Chi and Wylie [22] further support this by highlighting the importance of cognitive engagement in learning, which can be hindered by misinformation. Given these dynamics, we assume that incorrect explanations impair humans' procedural knowledge. This is supported by Roediger and Butler [69], who highlight how incorrect information encountered during learning can lead to the retention of false information, thereby impairing reasoning and memory. Similarly, Johnson and Seifert [50] argue that incorrect information, even after correction, can still lead to wrong inferences. With prior research emphasizing the negative repercussions on procedural knowledge, we hypothesize:\nHypothesis 1: Incorrect explanations lead to lower procedural knowledge.\nWhen collaborating with AI, incorrect explanations may impair humans' reasoning capabilities as they transition from knowing \"how\" to understanding \"why\". In psychology, the work of Loftus et al. [57] shows that misleading information can impair human memory, leading to an erroneous understanding. Ecker et al. [29] demonstrate the persistence of misinformation effects, showing how they continue to influence reasoning even after correction, thus highlighting their lasting impact. Similarly, Kendeou et al. [52] investigate the effects of incorrect information on human reasoning and explore mechanisms to mitigate these effects. Building upon prior research that examines how misinformation impairs decision-making and reasoning [84], we adopt this perspective to AI-assisted decision-making and hypothesize the following:\nHypothesis 2: The reasoning for the classification in the task is impaired by incorrect explanations."}, {"title": "4 Methodology", "content": "In this section, we describe our methodology to assess how incorrect explanations for correct AI advice influence humans during and post collaboration with an AI. We set up an online study and investigated how participants performed in a visual classification task on an architectural dataset. In this section, we outline the task domain, the study design, the recruitment of participants, the development of the AI, and finally, the metrics that we use to assess our RQs. Before we ran the study, we pre-registered the study on AsPredicted.org to report our hypotheses, our treatments, our planned analyses, and our exclusion strategy. An anonymized copy of the pre-registration is provided in the supplemental materials."}, {"title": "4.1 Task Domain", "content": "In order to analyze the impact of explanations, we chose to use a task that most people are not familiar with and typically cannot handle themselves initially: the classification of the architectural style of buildings. To do so, we use the established dataset of Xu et al. [100] containing images of buildings across 25 different architectural styles. In close discussion with architecture researchers of the local university, we chose three architectural styles that share similar features and are not easily distinguishable: Art Nouveau, Art Deco, and Georgian Architecture. For each architectural style, we selected 30 images that clearly represent the features of each architectural style and, thus, are appropriate instances for our study. We further made sure that the buildings were centered in each image and cropped all irrelevant information in the images, like other buildings."}, {"title": "4.2 Study Design", "content": "Our research questions target the understanding of the impacts of incorrect explanations in AI-assisted decision-making on human procedural knowledge and reasoning and the resulting human-AI team performance. To address them, we employed a study combining between- and within-subjects design: between subjects, we analyze the impact of different AI support. Within subjects, we observe this impact in different stages of decision-making: before, during, and after collaboration with the AI. The study was approved by the university's institutional review board.\nThe online study was divided into five different parts (see Figure 1): i part (1), the participants had to give their consent to participate and were introduced to the study and its procedure. They also received context information"}, {"title": "4.3 Al Development", "content": "The AI used in the study was a large language model (LLM) that provided participants with a classification (AI advice) and an explanation for its reasoning, depending on the treatment they were assigned to. As LLM, we used Open Al's GPT-4o model (model version 2024-05-13) through an Azure Open AI Studio instance. In the pre-phase of the study, we tested multiple prompt strategies in a workshop with three authors. The final prompts that were used are shown in Appendix A in Table 4. The LLM was prompted such that the correct explanation treatment provided an explanation"}, {"title": "4.4 Recruitment", "content": "We recruited 186 participants from the United States through the platform Prolific.co and ran the study on August 12, 2024. Previous research indicates that this platform is a reliable source of research data [63, 65]. Several screening mechanisms were implemented through the Prolific platform. With the filters, we targeted individuals who were fluent in the English language and had shown high quality in previous studies (100% completion rate). Our recruitment strategy was designed to not focus on participants with specific backgrounds, but admit participants without any further restrictions to be able to generalize our findings. Participants who met the stated criteria and completed the study's requirements received a base payment of 2.25\u00a3. Additionally, we implemented an incentive structure: participants are incentivized to conduct the task correctly by providing a bonus for each correctly classified image. This should ensure that participants paid attention during the task and did not provide random answers. The bonus was 4 pennies for each correct answer and led to a potential maximum payment of 3.21\u00a3. As stated in our pre-registration, we excluded participants who did not finish the main task on time (within 30 minutes) or did not finish the entire study. We also excluded participants with obvious misbehavior (e.g., clicking through the cases and always providing the same answer). Additionally, we computed the overall mean and standard deviation across all treatments and winsorized at 2.5 SD above/below the mean. Applying this exclusion strategy, we ended up with 160 participants equally assigned to the four treatments (40 participants for each treatment)."}, {"title": "4.5 Metrics", "content": "Similar to previous work (e.g., [42, 77]), we assessed participants' task performance in classifying the architectural styles in pre-test, main task, and post-test. The task performance was used to approximate participants' procedural knowledge in the pre-test and post-test and their human-AI team performance in the main task. As metrics for the task performance, we used accuracy and measured the ratio of correctly classified images over all images. Participants had to select one of the three different architectural styles for each image by selecting from a drop-down menu on each page of the task. This means that a random guess corresponded to 33.3% of performance. Aligning with Schoeffer et al. [77], we also measured the correct adherence and detrimental overrides in the main task of the study.\nWe assessed participants' reasoning ability for the three architectural styles through open-ended questions by asking them to describe and explain each style, thereby following the procedure of Chi et al. [21]. We rated each answer in terms of correctness by comparing it to the correct characteristics and features for each architectural style. By doing so, we were able to assess whether incorrect explanations impaired participants' reasoning.\nFinally, we established several control variables to investigate the potential underlying factors that might influence AI-assisted decision-making. In particular, we controlled for participants' cognitive load as previous research suggests that the information in explanations displayed to humans can affect their decision-making behavior [2, 43, 48, 85]. We measured participants' cognitive load on a seven-point Likert scale by having them rate five validated items previous"}, {"title": "5 Results", "content": "To address the research questions, we first conduct several statistical analyses to answer RQ: 1 in subsection 5.1. Subsequently, we address RQ: 2 and qualitatively assess the open-ended questionnaires in subsection 5.2. In the final subsection 5.3, we evaluate the impact of incorrect explanations on the human-AI team performance (RQ: 3)."}, {"title": "5.1 RQ1: Impact of Incorrect Explanations on Procedural Knowledge", "content": "It took participants, on average, 14 minutes and 54 seconds to complete the study. Overall, 160 participants passed the attention check and finished the study according to the study protocol. Of these 160 participants, 79 were male, 77 were female, and four identified as diverse. To establish a baseline and ensure that all treatments began on an equal performance level in classifying the architectural styles, we conduct a one-way ANOVA on the pre-test performance scores across the four treatments (F = 0.80, p = .498). These results fail to reject the null hypothesis, indicating no significant differences in pre-test performance among the four treatments. This finding suggests that participants across all treatments start with comparable levels of procedural knowledge to classify the architectural styles prior to the main task. This allows for a thorough interpretation of any differences observed in the post-test results, as they can be more readily attributed to the treatment effects rather than pre-existing differences.\nOverall, the control group maintains the lowest performance at around 51.25%, while the correct explanation treatment continues to lead with approximately 72.50% accuracy. The AI classification and incorrect explanation treatments show similar post-test performances of about 65.42% and 63.33%, respectively.\nTo assess the significance of differences in post-test performance across treatments, we compare performance levels between treatments. A one-way ANOVA yields evidence of treatment effects (F = 5.86, p = .001), indicating that the type of AI support in the main task influences participant performance in the post-test. To further examine"}, {"title": "5.2 RQ2: Impact of Incorrect Explanations on Reasoning", "content": "To analyze participants' reasoning capabilities as they transition from knowing \"how\" to understanding \"why\", we assess the answers they provided in the open-text questionnaire. The primary focus is on evaluating how the accuracy"}, {"title": "5.3 RQ3: Impact of Incorrect Explanations on Human-Al Team Performance", "content": "In this subsection, we analyze the data of the study to derive insights into how incorrect explanations affect the human-Al team performance.\nThe main task performance results reveal substantial differences across treatments (see Figure 3). The control group demonstrates the lowest performance at 60.83%. In contrast, all AI-assisted treatments show higher performance levels. The AI treatment achieves 87.92% accuracy, while the correct explanation treatment performs best at roughly 92.50%. Interestingly, the incorrect explanation treatment still outperforms the control group, reaching about 86.04% accuracy.\nThese patterns persist, albeit with reduced magnitudes, in the post-test performance (see Section 5.1. We can also see"}, {"title": "6 Discussion", "content": "With the rise of AI in decision-making domains, it is crucial to understand how the interaction with AI affects decision-makers. Prior research so far has either focused on the implications of incorrect AI advice (e.g., [10, 75]) or explored how the correctness of explanations affects trust and reliance on AI (e.g., [17, 61]). This work investigates how the correctness of AI explanations impacts humans through collaboration with an AI. In a classification task, participants were assigned to one of four treatments: no AI support, AI advice only, AI advice with correct explanations, and AI advice with incorrect explanations. We measured the task performance before, during, and post-collaboration to derive humans' procedural knowledge, reasoning, and the human-AI team performance. The results show that all AI-assisted treatments led to significantly higher human-Al team performance during the main task compared to the control group, demonstrating the merits of AI support. However, the correctness of the explanations played a crucial role: Participants who had received correct explanations exhibited the highest main task performance, followed by those who received only Al advice without explanations, dominating those who had received incorrect explanations. Importantly, the findings reveal that the accuracy of the explanations has a lasting impact on procedural knowledge development. While incorrect explanations temporarily increase performance during the main task, the procedural knowledge was impaired post-collaboration, ultimately hindering knowledge retention compared to correct explanations or no explanations. Moreover, these results must be considered with caution. Even though there was an increase in procedural knowledge, participants' reasoning capabilities in the incorrect explanation treatment were below the ones from the control group. This indicates that incorrect explanations can undermine the durable benefits of human-AI collaboration despite initial performance improvements. These results contribute to the HCI literature by underscoring the nuanced effects of explanation accuracy on human-AI collaboration. The findings emphasize the importance of designing AI-based support that provides accurate explanations to not only enhance human decision-making but also maintain and improve humans' procedural knowledge and reasoning in the long run. Overall, the study's key takeaway is that, while AI support can generally improve performance, the correctness of the explanations provided by the AI is a crucial determinant of human-Al collaboration, influencing humans' ability to draw conclusions and perform the task autonomously post-collaboration with an AI. These insights can inform the design of more effective and human-centric AI-based decision support systems."}, {"title": "6.1 Summary of Findings", "content": "With the rise of AI in decision-making domains, it is crucial to understand how the interaction with AI affects decision-makers. Prior research so far has either focused on the implications of incorrect AI advice (e.g., [10, 75]) or explored how the correctness of explanations affects trust and reliance on AI (e.g., [17, 61]). This work investigates how the correctness of AI explanations impacts humans through collaboration with an AI. In a classification task, participants were assigned to one of four treatments: no AI support, AI advice only, AI advice with correct explanations, and AI advice with incorrect explanations. We measured the task performance before, during, and post-collaboration to derive humans' procedural knowledge, reasoning, and the human-AI team performance. The results show that all AI-assisted treatments led to significantly higher human-Al team performance during the main task compared to the control group, demonstrating the merits of AI support. However, the correctness of the explanations played a crucial role: Participants who had received correct explanations exhibited the highest main task performance, followed by those who received only Al advice without explanations, dominating those who had received incorrect explanations. Importantly, the findings reveal that the accuracy of the explanations has a lasting impact on procedural knowledge development. While incorrect explanations temporarily increase performance during the main task, the procedural knowledge was impaired post-collaboration, ultimately hindering knowledge retention compared to correct explanations or no explanations. Moreover, these results must be considered with caution. Even though there was an increase in procedural knowledge, participants' reasoning capabilities in the incorrect explanation treatment were below the ones from the control group. This indicates that incorrect explanations can undermine the durable benefits of human-AI collaboration despite initial performance improvements. These results contribute to the HCI literature by underscoring the nuanced effects of explanation accuracy on human-AI collaboration. The findings emphasize the importance of designing AI-based support that provides accurate explanations to not only enhance human decision-making but also maintain and improve humans' procedural knowledge and reasoning in the long run. Overall, the study's key takeaway is that, while AI support can generally improve performance, the correctness of the explanations provided by the AI is a crucial determinant of human-Al collaboration, influencing humans' ability to draw conclusions and perform the task autonomously post-collaboration with an AI. These insights can inform the design of more effective and human-centric AI-based decision support systems."}, {"title": "6.2 Implications", "content": "This work makes several contributions to the field of HCI by deepening the understanding of how the correctness of AI explanations influences humans' procedural knowledge and reasoning ability, as well as the human-AI team performance. While previous research has often focused on the benefits of AI assistance in enhancing decision-making [73], this study offers a more nuanced perspective, highlighting the critical role of explanation correctness on humans' knowledge and understanding. It complements the large corpus of HCI literature on XAI (i.e.; [42, 61, 75, 77, 86]) by explicitly investigating the influence of incorrect explanations for scenarios when the AI advice is correct and identifies the negative repercussions for humans.\nThe misinformation effect of explanations. The study highlights a crucial risk associated with Al explanations: the potential misinformation effect to impair procedural knowledge and reasoning through incorrect explanations. The misinformation effect, extensively studied by Loftus et al. [57], Loftus and Palmer [58], describes how exposure to incorrect information can distort an individual's memory of an event. This phenomenon extends to AI explanations, where incorrect explanations can similarly distort humans' understanding. The integration of incorrect information through explanations into existing knowledge structures, as discussed by Ayers and Reder [8], can lead to significant changes in both procedural knowledge and reasoning. Incorrect explanations, while potentially unharmful in the short term, can create an illusion of understanding, resulting in poorer performance in subsequent tasks without Al support. This phenomenon is particularly concerning in high-stakes environments such as healthcare or legal decision-making, where the quality of decision-making has far-reaching consequences [92]. For organizations, this implies that the long-term efficacy of AI hinges not only on their immediate performance but also on their ability to foster accurate knowledge. Incorrect explanations can lead to a misalignment between the Al's recommendations and humans' understanding, which may diminish their overall effectiveness [61]. Yet, it also has the potential to impair humans post-collaboration with an AI. Therefore, organizations must prioritize the development of AI that provides correct and transparent explanations to ensure sustained, high-quality decision-making and prevent detrimental impacts on organizational knowledge and performance.\nTaking a human-centric perspective. In our study, we could see that the human-AI team performance improved over the participants' initial task performance. This pattern, also observed by prior research [40, 49], showcases the potential of humans collaborating with AI. Even though the scenario in which the human-AI team performance exceeds the performance of human or AI alone-also referred to as complementary team performance [9]-could not be reached (due to the study design choices this was not feasible as the Al advice was always correct), our findings showcase the two sides of explanations: while correct explanations improve the human-Al team performance compared to only receiving Al advice (bright side) and demonstrate the merits of XAI, incorrect explanations decrease the human-AI team performance compared to only receiving AI advice (dark side) and outline the pitfalls of XAI. Thus, it is important to implement mechanisms that allow humans to verify the correctness of explanations, for instance, through reflection mechanisms [33]. Furthermore, we could also identify AI trust as a confounding factor during the main task and cognitive load during the post-test. While these findings align with prior research [15, 93, 98], they also emphasize the important role of individuals' characteristics and traits in human-Al collaboration. Therefore, research has to anticipate these factors in the design of robust and safe explanations.\nThe role of AI. While the focus of our research has mainly been on the humans in our AI-assisted decision-making study, the AI can take an important role in minimizing the risk of negative repercussions. In our study, these"}, {"title": "6.3 Limitations and Future Work", "content": "Despite the valuable insights gained from this study, several limitations must be acknowledged, offering avenues for future research. First, the study explores how incorrect explanations affect humans in AI-assisted decision-making. To investigate the impact of such explanations, we designed a study with four treatments of different AI support: no support, only AI advice, AI advice with correct explanations, and AI advice with incorrect explanations. In real-world applications, the interaction with AI that provides only correct or incorrect explanations is rather unlikely. It presents valuable means to take the first steps to investigate the impact of incorrect explanations but does not reflect the real world. Future research could take on this aspect to extend our findings and evaluate how a mix of correct and incorrect explanations-a mix that is more realistic for deployed AI-affects humans' procedural knowledge and reasoning. Especially, different ratios of the correctness of explanations could provide further insights and advance the field.\nSecond, the focus of the study was mainly on measuring the task performance to derive insights into humans' procedural knowledge and inform about the impact on human-AI team performance. However, in real-world scenarios, performance might not be the only metric relevant. Other measures, like appropriate reliance [75], trust or fairness [77] in the AI, might also be of high relevance in AI-assisted decision-making as previous studies show [10, 42]. It is of high importance to explore how these factors change over time and under the effect of incorrect explanations. Exploring the temporal implications can extend the views and offer new insights that support the robust and effective design of AI.\nLastly, the study primarily relied on short-term measures based on task performance, which may not fully capture the long-term impact of AI support on human knowledge and how the correctness of explanations impacts the human-AI team performance. Future research could employ longitudinal designs to assess how incorrect explanations influence procedural knowledge development and reasoning over extended periods and across multiple tasks. This approach would offer a deeper understanding of how different types of explanations contribute to sustained knowledge development, aligning with the principles of human-centered AI."}, {"title": "7 Conclusion", "content": "This work sets out the first steps towards investigating the effect of incorrect explanations on the human and the human-Al team. By doing so, we take a human-centric perspective and analyze the repercussions of incorrect explanations on task performance to derive insights into humans' procedural knowledge and reasoning. In an online study, we assessed the impact of such explanations, specifically after the AI support is withdrawn, and humans must act autonomously.\nWith our work, we make several contributions to the HCI field: First, we identify a misinformation effect caused by incorrect explanations, which impairs humans' procedural knowledge and reasoning. Second, we offer insights into how such incorrect explanations limit human-AI team capabilities. Finally, we provide guidelines for the effective and safe design of explanations that can foster AI-assisted decision-making. So we can eventually imagine: the Al provides a correct explanation for differentiating the architectural styles. You pass your ex\u0430\u0442."}]}