{"title": "Fitted Q-Iteration via Max-Plus-Linear Approximation", "authors": ["Y. Liu", "M. A. S. Kolarijani"], "abstract": "In this study, we consider the application of max-plus-linear approximators for Q-function in offline reinforcement learning of discounted Markov decision processes. In particular, we incorporate these approximators to propose novel fitted Q-iteration (FQI) algorithms with provable convergence. Exploiting the compatibility of the Bellman operator with max-plus operations, we show that the max-plus-linear regression within each iteration of the proposed FQI algorithm reduces to simple max-plus matrix-vector multiplications. We also consider the variational implementation of the proposed algorithm which leads to a per-iteration complexity that is independent of the number of samples.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a subfield of machine learning concerned with sequential decision-making in uncertain environments [1], [2]. Over the past three decades, RL has gained popularity in various applications such as robotics [3], game playing [4], and finance [5]. These and other successful applications of RL showcase its ability to adapt and optimize behaviors in uncertain environments requiring a high degree of strategic depth.\n\nThe ultimate goal of RL is for an agent (a.k.a. decision-maker, controller) to learn to perform sequences of actions that maximize the cumulative reward. In offline RL (a.k.a. batch RL), the agent has to learn a (control) policy from a fixed dataset of interactions with the environment, as opposed to being able to interact with the environment in an online fashion. This approach is particularly useful when online interaction is expensive, risky, or infeasible, e.g., in healthcare applications [6].\n\nA standard approach for solving the offline RL problem is the fitted Q-iteration (FQI) algorithm [7]. FQI forms an approximation of the Q-function (i.e., state-action value function) and iteratively updates it by minimizing the empirical Bellman error using the available batch of data. The specific approximator used in FQI can be parametric (e.g., neural networks [8]) or non-parametric (e.g., kernel-based [9]). While the simplicity of FQI makes it a popular choice in practice, this algorithm lacks guarantees for convergence. Indeed, FQI can diverge even for problems with a finite number of states and actions and with a linear parametric approximator that is rich enough to represent the optimal value function [10], [11]. A sufficient but restrictive condition for the convergence of FQI is for the approximator to be an \"averager\" [12], which is for instance the case for piece-wise constant approximations.\n\nA particular class of approximators with a long history in optimal control and dynamic programming (DP) problems is max-plus (MP)-linear approximators. These approximators, as the name suggests, are linear in the MP algebra in which the conventional addition and multiplication operations are replaced by maximum and addition operations, respectively. This widespread application stems from the compatibility of the Bellman operator with MP operations. Indeed, the backward iteration in the DP algorithm for solving the optimal control problem of finite deterministic Markov decision processes (MDPs) is a linear operator in MP algebra [13]. Another example is the MP-linearity of the evolution semi-group of the Hamilton-Jacobi-Bellman equation, arising in optimal control of deterministic continuous-time dynamics, that has been exploited to derive efficient numerical methods for finding an MP-linear approximation of the corresponding value function [14], [15], [16]. More recently, a similar idea has been used for approximating the value function of a deterministic MDP with a continuous state space [17]. Also of interest is the recent development of an online RL algorithm that, unlike the previous works mentioned above, updates the parameters of the MP-linear approximation of the value function in real-time [18].\n\nIn this paper, we propose a novel class of FQI algorithms that employ parametric MP-linear approximation of the Q-function. This, in turn, allows us to exploit the MP-linearity of the empirical Bellman operator to reduce the MP-linear regression problem in each iteration to a simple max-plus matrix-vector multiplications. The main contributions of this work are as follows: (i) We propose the MP-FQI Algorithm 1 and show its convergence with a linear rate (Theorem 3.3) and a per-iteration complexity of $\\mathcal{O}(np)$, where n is the number of samples and p is the number of parameters/basis functions in the MP-linear approximator (Theorem 3.4). (ii) Inspired by [16], we also consider the variational formulation of the problem and propose the v-MP-FQI Algorithm 2 and show its convergence with a linear rate (Theorem 4.4) and a per-iteration complexity of $\\mathcal{O}(pq)$, independent of the number of samples, where p is again the number of parameters and q is the number of test functions in the variational formulation (Theorem 4.5). To the best of our knowledge, this is the first work that provides a provably convergent FQI algorithm using a class of approximators beyond the existing restrictive conditions that require the mapping corresponding to the approximator to be non-expansive in the \u221e-norm such as [12].\n\nThe rest of the paper is organized as follows. In Section II, we provide the statement of the offline RL problem and its standard solution via FQI. We also provided some pre-"}, {"title": "II. PROBLEM STATEMENT AND PRELIMINARIES", "content": "In this section, we provide the problem statement and its standard solution. We also review the background on MP-linear approximation and regression along with some preliminary lemmas to be used in the subsequent sections.\n\nConsider an MDP $(\\mathcal{Z} = (\\mathcal{X} \\times \\mathcal{U}), \\mathcal{P}, r, \\gamma)$ where $\\mathcal{X} \\subset \\mathbb{R}^{d_x}$ and $\\mathcal{U} \\subset \\mathbb{R}^{d_u}$ are the state and action spaces, respectively; $\\mathcal{P} : \\mathcal{Z} \\times \\mathcal{X} \\rightarrow [0,\\infty)$ is the transition probability kernel such that $\\mathcal{P}(z = (x,u), \\cdot)$ is a probability measure on $\\mathcal{X}$ describing the distribution of the next state $x^+$ given that the action $u$ is taken in state $x$; $r: \\mathcal{Z} \\rightarrow \\mathbb{R}$ is the reward function and uniformly bounded from above such that $r(z = (x,u))$ is the reward that the agent receives for taking the action $u$ in state $x$; $\\gamma \\in (0,1)$ is the discount factor.\n\nThe problem of interest is to maximize the expected, infinite-horizon, discounted reward starting from each stat-action pair, that is, to find the optimal Q-function $Q^*: \\mathcal{Z} \\rightarrow \\mathbb{R}$ (a.k.a. optimal state-action value function) given by\n\n$Q^*(z) := \\max_{\\pi : \\mathcal{X}\\rightarrow\\mathcal{U}} \\mathbb{E} \\Big(\\sum_{t=0}^{\\infty} \\gamma^t r(z_t) \\mid z_0 = z, u_t = \\pi(x_t), \\forall t \\geq 1\\Big).$\n\nwith the corresponding optimal policy $\\pi^*(x) \\in \\underset{u\\in \\mathcal{U}}{\\text{argmax}}~ Q^*(x,u)$, i.e., the greedy policy with respect to $Q^*$. The offline RL problem involves finding the Q-function using a set of $n\\in \\mathbb{N}$ samples $\\{z_i = (x_i,u_i), x^+_i, r_i\\}_{i\\in[1:n]}$, where each sample corresponds to the agent taking the"}, {"title": "B. Fitted Q-iteration (FQI)", "content": "The optimal Q-function can be characterized as the fixed-point of the Bellman operator $\\mathcal{B} : \\mathbb{R}^\\mathcal{Z} \\rightarrow \\mathbb{R}^\\mathcal{Z}$, that is, $Q^*$ solves the Bellman equation (BE) [19, Fact 3]: For each $z \\in \\mathcal{Z}$,\n\n$Q(z) = \\mathcal{B}Q(z) := \\mathbb{E}_{x^+} \\big(r(z) + \\gamma \\cdot \\max_{u^+} Q(x^+,u^+)\\big). \\tag{1}$\n\nBased on this result, a standard solution for the offline RL problem is to use a parametric approximation $Q_\\theta \\in \\mathbb{R}^\\mathcal{Z}$ and then find the parameter $\\theta \\in \\mathbb{R}^p$ such that $Q_{\\theta}$ solves the empirical BE: For each $i \\in [1: n]$\n\n$Q_{\\theta}(z_i) = \\mathcal{B}_s Q_{\\theta}(z_i) := r_i + \\gamma \\cdot \\max_{u^+} Q_{\\theta}(x^+_i,u^+). \\tag{2}$\n\n(We use the subscript $s$, for the sampled versions of the same objects). The so-called FQI algorithm solves the latter problem by minimizing the empirical Bellman error in a recursive fashion [7]. To be precise, with some initialization $\\theta^{(0)}$, FQI solves the regressions\n\n$\\theta^{(l+1)} \\in \\underset{\\theta}{\\text{argmin}}~\\sum_{i\\in[1:n]} L(Q_{\\theta}(z_i), \\mathcal{B}_s Q_{\\theta^{(l)}}(z_i)),~~ l \\in \\mathbb{N}_0, \\tag{3}$\n\nwhere $L: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is a loss function. That is, FQI finds $\\theta^{(l+1)}$ by fitting $Q_{\\theta^{(l+1)}}$ to $\\mathcal{B}_s Q_{\\theta^{(l)}}$ over the sampled state-action space $\\mathcal{Z}_s := \\{z_i\\}_{i\\in[1:n]}$.\n\nAlternatively, we can think of the procedure explained above as finding the fixed-point of the composite operation $R\\circ\\mathcal{B}_s\\circ A: \\mathbb{R}^p \\rightarrow \\mathbb{R}^p$ on the space of parameters, with $A : \\mathbb{R}^p \\rightarrow \\mathbb{R}^\\mathcal{Z}$ corresponding to the function approximation $(\\theta^{(l)} \\mapsto Q_{\\theta^{(l)}})$ and $R : \\mathbb{R}^\\mathcal{Z} \\rightarrow \\mathbb{R}^p$ corresponding to the regression (3) [20, Sec. 3.4.4]. Consequently, the convergence of the algorithm depends on the properties of the operators $A$ and $R$. In particular, if these two operations are non-expansive (in \u221e-norm), then the whole iterative process becomes convergent since $\\mathcal{B}$ (and hence $\\mathcal{B}_s$) is a $\\gamma$-contraction (in \u221e-norm). In the subsequent sections, we will use this basic procedure in order to develop similar algorithms using MP-linear approximation and regression."}, {"title": "C. MP-linear approximation", "content": "For a function $Q \\in \\mathbb{R}^\\mathcal{Z}$, the MP-linear approximator is of the form\n\n$Q_\\theta(z) = \\underset{j\\in[1:p]}{\\text{max}} \\{ [f]_j(z) + [\\theta]_j \\} = f(z) \\overline{\\bowtie} \\theta, \\tag{4}$\n\nwhere $f: \\mathcal{Z} \\rightarrow \\mathbb{R}^p$ is the vector of MP features (basis functions), and $\\theta \\in \\mathbb{R}^p$ is the vector of MP parameters (coefficients). For the features, two of the classical choices are (i) the MP indicator function $[f]_j(z) = \\delta_{W_j}(z)$ with $\\{W_j\\}_{j\\in[1:p]}$ being a partitioning of the set $\\mathcal{Z}$ (leading to a piece-wise constant approximation) and (ii) quadratic functions $[f]_j(z) = -c ||z-w_j||^2$ with $c > 0$ and $\\{w_j\\}_{j\\in[1:p]} \\subset \\mathcal{Z}$ (leading to a semi-convex, piece-wise quadratic approximation [21, Sec. 2]). In [17], the authors combine these two functions and use the distance features $[f]_j(z) = -c\\underset{w\\in W_j}{\\text{min}} ||z-w||^2$ leading to an almost piece-wise constant approximation."}, {"title": "D. MP-linear regression", "content": "Consider the system of MP-linear equations $A \\overline{\\bowtie} \\theta = b$ in the unknown $\\theta \\in \\mathbb{R}^p$, with data $A \\in \\mathbb{R}^{n\\times p}$ and $b \\in \\mathbb{R}^n$ such that the matrix A is doubly $\\mathbb{R}$-astic, i.e., it has at least one finite entry in each column and one finite entry in each row [22]. This equation has a solution if and only if the principal solution\n\n$\\theta_{ps} := - (A^\\top \\overline{\\bowtie} (-b)) \\in \\mathbb{R}^p, \\tag{5}$\n\nsatisfies $A \\overline{\\bowtie} \\theta_{ps} = b$ [23, Cor. 3.1.2]. Let $\\Theta = {\\theta \\in \\mathbb{R}^p : A \\overline{\\bowtie} \\theta \\leq b}$ be the set of all the points that satisfy the so-called \"lateness\" constraint [22]. The principal solution is also the greatest subsolution in the sense that $\\theta_{ps} \\in \\Theta$ and $\\theta_{ps} \\geq \\theta$ for all $\\theta \\in \\Theta$ [23, Thm. 3.1.1]. Observe that the operation corresponding to the principal solution, i.e., the mapping $b\\rightarrow \\theta_{ps} = - (A^\\top \\overline{\\bowtie} (-b))$, can be equivalently seen as a projection. Moreover, the principal solution is the optimal solution of the constrained \u221e-norm regression [23, Thm. 3.5.1], that is,\n\n$\\theta_{ps} = \\underset{\\theta\\in\\mathbb{R}^p}{\\text{argmin}} \\{ ||A \\overline{\\bowtie} \\theta - b||_{\\infty} : A \\overline{\\bowtie} \\theta \\leq b \\}.$\n\nAnother strong result concerns the optimal solution of the unconstrained \u221e-norm regression as follows\n\n$\\hat{\\theta} = \\underset{\\theta\\in\\mathbb{R}^p}{\\text{argmin}} ||A \\overline{\\bowtie} \\theta - b||_{\\infty} = \\theta_{ps} + \\frac{||A \\overline{\\bowtie} \\theta_{ps} - b||_{\\infty}}{2} e$\n\nwhere e is the all-one vector [23, Thm. 3.5.2]."}, {"title": "E. Preliminary lemmas", "content": "We finish this section with two preliminary results. In the algorithms to be developed in the subsequent sections, we will exploit the following two important properties of the empirical Bellman operator. We note that the following result has been extensively studied and used in previous works such as [13], [14], [15], [16], [17], [18].\n\nLemma 2.1 (MP additivity and homogeneity of $\\mathcal{B}_s$): Consider the two functions $f,f \\in \\mathbb{R}^\\mathcal{Z}$ and the scalar $\\alpha \\in \\mathbb{R}$. Define $[\\max\\{f, f\\}](z) = \\max\\{f(z),f(z)\\}$ and $[\\alpha+f](z) = \\alpha + f(z)$ for all $z \\in \\mathcal{Z}$. We have\n\n$\\mathcal{B}_s[\\max\\{f, f\\}] = \\max\\{\\mathcal{B}_sf,\\mathcal{B}_s\\}~~~~~~~~~~~~~~$  (MP additivity)\n$\\mathcal{B}_s[\\alpha + f] = \\gamma\\alpha + \\mathcal{B}_sf.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ (MP homogeneity)\n\nThe next result concerns a generalization of the non-expansiveness of MP-linear operators. We will use this result for the convergence analysis of the algorithms proposed in this study.\n\nLemma 2.2 (Non-expansiveness of MP-linear operators): Consider the operator $\\mathcal{A} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m : y \\mapsto A \\overline{\\bowtie} (\\eta y)$, where $A \\in \\mathbb{R}^{m\\times n}$ is row $\\mathbb{R}$-astic and $\\eta \\in (0,1]$ is a constant. Then,\n\n$||\\mathcal{A}y_1 - \\mathcal{A}y_2||_{\\infty} \\leq \\eta ||y_1 - y_2 ||_{\\infty}, \\forall y_1, y_2 \\in \\mathbb{R}^n. \\tag{6}$"}, {"title": "III. MAX-PLUS FQI", "content": "In this section, we propose and analyze the max-plus fitted Q-iteration (MP-FQI) algorithm. The basic idea here is to approximate the Q-function as an MP-linear function and use MP-linear regression to find the parameters."}, {"title": "A. The algorithm", "content": "Let us consider an MP-linear approximation of the Q-function given by\n\n$Q_\\theta(z) = \\underset{j\\in[1:p]}{\\text{max}} \\{ [f]_j(z) + [\\theta]_j \\} = f(z) \\overline{\\bowtie} \\theta, \\tag{7}$\n\nwith features $f: \\mathcal{Z} \\rightarrow \\mathbb{R}^p$ and parameters $\\theta \\in \\mathbb{R}^p$. Recall that the FQI algorithm solves the empirical BE\n\n$Q_{\\theta^*}(z_i) = \\mathcal{B}_s Q_{\\theta^*}(z_i), ~~~\\forall i\\in [1:n]. \\tag{8}$\n\nvia recursive regressions. The following lemma provides an alternative characterization of the preceding equation.\n\nProposition 3.1 (MP empirical BE): Define the matrices $F_s, G_s \\in \\mathbb{R}^{n\\times p}$ with entries\n\n$[F_s]_{i,j} = [f]_j(z_i), ~~ [G_s]_{i,j}=\\mathcal{B}_s[f]_j(z_i), ~~~~ i\\in [1:n], j\\in [1: p],$\n\nand let $\\overline{\\theta} \\in \\mathbb{R}^p$ be a solution to the MP equation\n\n$F_s \\overline{\\bowtie} \\theta = G_s \\overline{\\bowtie} (\\gamma \\theta). \\tag{9}$\n\nThen, the MP-linear function $Q_{\\overline{\\theta}}(z) = f(z) \\overline{\\bowtie} \\overline{\\theta}$ solves the empirical BE (8).\n\nThe preceding result implies that finding the parameters of the MP-linear approximation (7) of the Q-function reduces to solving the MP equation (9). Similar to the standard FQI, we use recursive MP-linear regressions of the form\n\n$\\theta^{(l+1)} = \\underset{\\theta}{\\text{argmin}}~ ||F_s \\overline{\\bowtie} \\theta - G_s \\overline{\\bowtie} (\\gamma\\theta^{(l)}) ||_{\\infty},~~ l\\in \\mathbb{N}_0. \\tag{10}$\n\nTo be able to use the principle solution for solving this regression, we need the following assumption:\n\nAssumption 3.2: The features $f: \\mathcal{Z} \\rightarrow \\mathbb{R}^p$ and the samples $\\{z_i = (x_i, u_i), x^+_i, r_i\\}_{i\\in[1:n]}$ are such that\n\n$\\forall z \\in \\mathcal{Z}, ~\\exists j\\in [1: p],~ [f]_j(z) \\neq -\\infty;$ \\tag{11}\n$\\forall j\\in [1:p], ~\\exists i \\in [1 : n],~ [f]_j(z_i) \\neq -\\infty.$ \\tag{12}\n\nThat is, $F_s$ (resp., $G_s$) is doubly (resp., row) $\\mathbb{R}$-astic.\n\nNote that the preceding assumption is only relevant when the features are extended real-valued functions that assign -\u221e to a subset of state-action space $\\mathcal{Z} = \\mathcal{X} \\times \\mathcal{U}$. In particular, if condition (11) does not hold, then any MP-linear combination of features assigns -\u221e to a subset of $\\mathcal{Z}$. On the other hand, condition (12) implies that each feature $[f]_j$ is \"activated\" by at least one state-action sample $z_i$ in the sense that $[f]_j(z_i)$ is finite. If this condition does not hold for some $j\\in [1 : p]$, we can safely remove the corresponding feature $[f]_j$; since the sample set is not \"rich\" enough to activate this feature (the algorithm below sets $[\theta]_j = -\\infty$ in all iterations). Under Assumption 3.2, the solution to the MP-linear regression (10) is given by [23, Thm. 3.5.2]\n\n$\\theta^{(l+1)} = \\theta^{(l+1)}_{ps} + \\delta^{(l+1)} \\cdot e, ~~l\\in \\mathbb{N}_0,$\n\nwhere\n\n$\\theta^{(l+1)}_{ps} := D\\theta^{(l)} := - [F_s^\\top \\overline{\\bowtie} -(G_s \\overline{\\bowtie} (\\gamma\\theta^{(l)}))], \\tag{13}$\n\nis the principal solution of $F_s \\overline{\\bowtie} \\theta = G_s \\overline{\\bowtie} (\\gamma\\theta^{(l)})$ and\n\n$\\delta^{(l+1)} = \\frac{1}{2}||F_s \\overline{\\bowtie} \\theta^{(l+1)}_{ps} - G_s \\overline{\\bowtie} (\\gamma\\theta^{(l)} ||_{\\infty},$"}, {"title": "B. Analysis", "content": "Notice that Algorithm 1 terminates when the difference between the output of two consecutive iterations (in \u221e-norm) is less than a prescribed tolerance $\\varepsilon > 0$. As shown by the following result, such a condition is sufficient for Algorithm 1 to terminate in a finite number of iterations.\n\nTheorem 3.3 (Convergence of MP-FQI): Let Assumption 3.2 hold. Then, the operator $D : \\mathbb{R}^p \\rightarrow \\mathbb{R}^p$ defined in (13) is a $\\gamma$-contraction in \u221e-norm.\n\nThe preceding result shows that Algorithm 1 converges linearly with a rate $< \\gamma$. We next consider the complexity of the proposed algorithm.\n\nTheorem 3.4 (Complexity of MP-FQI): For Algorithm 1, disregarding the complexity of the maximization over $u^+$ for computing $G_s$, the compilation and per-iteration time complexities are both of $\\mathcal{O}(np)$.\n\nLet us also discuss the complexity of solving the maximization over the action $u^+$ in Algorithm 1. Note that these maximizations are to be solved only once. In particular, if the action space $\\mathcal{U} = \\{v_1,..., v_{p_u}\\}$ is finite and\n\n$[f]_{jk}(x,u) = [f_x]_j(x) + \\delta_{v_k}(u), ~~j \\in [1 : p_x], k \\in [1 : p_u], \\tag{14}$\n\nwith state features $f_x : \\mathcal{X} \\rightarrow \\mathbb{R}^{p_x}$, we have\n\n$\\underset{u^+}{\\text{max}}~ [f]_{jk}(x,u^+) = [f_x]_j(x^+).$\n\nThat is, we do not need to solve any maximization problem.\n\nWe finish this section by comparing the proposed MP-FQI algorithm with MP-linear parameterization with its counterpart in the conventional plus-times algebra.\n\nRemark 3.5 (Comparison with standard FQI): Consider the FQI algorithm given by (3) with linear parameterization"}, {"title": "C. An alternative implementation of MP-FQI", "content": "We now present an alternative characterization of the solution to the empirical BE (8) which is inspired by the MP eigenvector method developed for $H_\\infty$ control problems [15], [25]. As we see shortly, this characterization leads to an algorithm with a per-iteration complexity independent of the sample size $n$ at the cost of increasing the compilation time.\n\nProposition 3.6 (MP empirical BE II): Consider the matrices $F_s$ and $G_s$ in Proposition 3.1. Let\n\n*   the matrix $C_s \\in \\mathbb{R}^{p\\times p}$ be a solution to the MP-linear equation $F_s \\overline{\\bowtie} C_s = G_s$, and,\n*   the vector $\\overline{\\theta} \\in \\mathbb{R}^p$ be a solution to the MP-fixed-point equation $\\theta = C_s \\overline{\\bowtie} (\\gamma\\theta)$.\n\nThen, the MP-linear function $Q_{\\overline{\\theta}}(z) = f(z) \\overline{\\bowtie} \\overline{\\theta}$ solves the empirical BE (8).\n\nLet us also note that the equality $F_s \\overline{\\bowtie} C_s = G_s$ implies that\n\n$\\mathcal{B}_s [f]_j(z_i) = \\underset{k\\in[1:p]}{\\text{max}} \\{ [f]_k(z_i)+[C_s]_{k,j} \\}, ~~i\\in [1:n], j\\in [1 : p],$\n\nthat is, the j-th column of $C_s$ contains the max-plus coefficients of the MP-linear representation of $\\mathcal{B}_s[f]_j$ with respect to the same features $f : \\mathcal{Z} \\rightarrow \\mathbb{R}^p$, at the sample points in $\\mathcal{Z}_s$.\n\nThe preceding lemma points to a recursive algorithm based on the fixed-point iteration\n\n$\\theta^{(l+1)} = C_s \\overline{\\bowtie} (\\gamma\\theta^{(l)}), ~~l\\in \\mathbb{N}_0, \\tag{17}$\n\nwith some initialization $\\theta^{(0)} \\in \\mathbb{R}^p$. What remains to be addressed is the computation of the matrix $C_s \\in \\mathbb{R}^{p\\times p}$, i.e., a solution to the MP-linear equation $F_s \\overline{\\bowtie} C_s = G_s$. A possible choice is the solution to the column-wise regression problems\n\n$[C_s]_{:,j} = \\underset{c}{\\text{argmin}}~ ||F_s \\overline{\\bowtie} c - [G_s]_{:,j} ||_{\\infty}$\n\n$= c_{ps} + \\frac{||F_sc_{ps} - [G_s]_{:,j}||_{\\infty}}{2}e$"}, {"title": "IV. VARIATIONAL MAX-PLUS FQI", "content": "In this section, we propose and analyze the variational implementation of the max-plus fitted Q-iteration (v-MP-FQI). The proposed algorithm is based on an alternative formulation of the BE which originates from the MP variational formulation of the continuous-time, deterministic optimal control problems proposed in [16]. This formulation is also used for approximating the value function in the optimal control problem of deterministic MDPs by [17]. To this end, let us define the MP \"inner product\" of the two functions $f, f \\in \\mathbb{R}^\\mathcal{Z}$ over the state-action space $\\mathcal{Z}$ by\n\n$f \\underline{||} f := \\underset{z\\in \\mathcal{Z}}{\\text{max}} \\{f(z)+f(z)\\}.$\n\nThen, for the vector of test functions $h : \\mathcal{Z} \\rightarrow \\mathbb{R}^q$, we consider finding $Q^*$ by solving the variational BE\n\n$[h]_k \\underline{||} Q^*=[h]_k \\underline{||} \\mathcal{B}Q^*, ~~~~\\forall k\\in [1 : q]. \\tag{18}$"}, {"title": "A. Algorithm", "content": "Similar to the MP-FQI algorithm, we incorporate the MP-linear approximation (7) for the Q-function. However, instead of the empirical BE (8), we aim to solve the empirical version of the variational BE (18), given by\n\n$[h]_k \\underline{||}_s Q_\\theta = [h]_k \\underline{||}_s \\mathcal{B}_sQ_\\theta, ~~~~\\forall k\\in [1 : q], \\tag{19}$\n\nwhere\n\n$f \\underline{||}_s f := \\underset{i\\in [1:n]}{\\text{max}} \\{f(z_i)+f(z_i)\\},$\n\nis the emprical MP inner product of $f, f \\in \\mathbb{R}^\\mathcal{Z}$ computed over the sampled state-action space $\\mathcal{Z}_s$. Once again, we provide an alternative characterization of the preceding equation which forms the basis of the proposed algorithm.\n\nProposition 4.1 (MP empirical variational BE):\nConsider the matrices $F_s$ and $G_s$ in Proposition 3.1, and define the matrix $H_s \\in \\mathbb{R}^{n\\times q}$ with entries\n\n$[H_s]_{i,k} = [h]_k(z_i), ~~i \\in [1 :n], k \\in [1 : q].$\n\nSet $F^\\# = H_s^\\top \\overline{\\bowtie} F_s \\in \\mathbb{R}^{q\\times p}$ and $G^\\# = H_s^\\top \\overline{\\bowtie} G_s \\in \\mathbb{R}^{q\\times p}$, and let $\\overline{\\theta} \\in \\mathbb{R}^p$ be a solution to the MP equation\n\n$F^\\# \\overline{\\bowtie} \\theta = G^\\# \\overline{\\bowtie} (\\gamma\\theta). \\tag{20}$\n\nThen, the MP-linear function $Q_{\\overline{\\theta}}(z) = f(z) \\overline{\\bowtie} \\overline{\\theta}$ in (7) solves the empirical variational BE (19).\n\nUsing the the preceding result, once again we see that the parameters of the MP-linear approximation (7) of the Q-function can be determined by solving the MP equation (20). We next follow a similar procedure to that of the previous"}, {"title": "B. Analysis", "content": "We now consider the convergence and complexity of Algorithm 2.\n\nTheorem 4.4 (Convergence of v-MP-FQI): Let Assumptions 3.2 and 4.2 hold. Then, the operator $D_H : \\mathbb{R}^p \\rightarrow \\mathbb{R}^p$ defined in (23) is a $\\gamma$-contraction in \u221e-norm.\n\nTheorem 4.5 (Complexity of v-MPFQI): For Algorithm 2, disregarding the complexity of the maximization over $u^+$ for computing $G_s$, the compilation time complexity is of $\\mathcal{O}(npq)$. The per-iteration complexity of Algorithm 2 is of $\\mathcal{O}(pq)$.\n\nTherefore, the v-MP-FQI Algorithm 2 also converges linearly with a rate $< \\gamma$. However, notice how the per-iteration time complexity of Algorithm 2 is independent of the sample size and instead depends on the number of test functions; cf. Theorem 3.4 for the complexity of Algorithm 1. The important property of v-MP-FQI is hence that it goes through the samples only once. Hence, for a large sample size $n$ (which requires a lot of computational resources per iteration"}, {"title": "VI. FINAL REMARKS", "content": "In this paper, we introduced a novel FQI algorithm based on an MP-linear approximation of the Q-function with a provable convergence. We also considered the variational implementation of the algorithm leading to a per-iteration complexity independent of the number of samples. We now provide some final remarks on the limitations and extensions of the proposed algorithms and the provided analysis.\n\nOne of the main drawbacks of the current work is the lack of error analysis, i.e., a theoretical bound on the distance between the output of the algorithms and the optimal Q-function. This issue can be addressed by bounding the error introduced in solving the MP-linear regression problem with respect to the output of the true Bellman operator in each iteration [26]. Also of interest is the identification of the particular class of problems (e.g., regularity assumptions on the transition kernel and reward function of the MDP) for which the optimal value function can be well-represented in MP-linear function spaces. A classic example is the case of linear dynamics with additive disturbance with a concave reward function which leads to a concave optimal Q-function [27].\n\nA possible extension of the current algorithms is their potential combination with fast numerical algorithms for discrete conjugate or distance transforms to reduce the computation cost of the algorithm. In particular, with factorized (i.e., grid-like) samples and/or features, we can use a similar approach used in [28], [29] to reduce the per-iteration complexity of Algorithm 1 to $\\mathcal{O}(n+p)$. Another interesting extension is the application of sparse approximate solutions to MP-linear equations [30] instead of the MP-linear regression in the proposed algorithms. Such sparse solutions allow the user to control the size of the approximation (i.e., number"}, {"title": "A. Technical proofs", "content": "1) Proof of Lemma 2.2: We first note that the assumption on matrix A being row $\\mathbb{R}$-astic implies that the range of A"}]}