{"title": "Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless Networks", "authors": ["Shubham Malhotra"], "abstract": "This report investigates the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. An environment that includes a base station, multiple antennas, and user equipment is created. Using the RLlib library, various DRL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) are then applied. These algorithms are compared based on their ability to optimize resource allocation, focusing on the impact of different learning rates and scheduling policies. The findings demonstrate that the choice of algorithm and learning rate significantly influences system performance, with DRL providing more efficient resource allocation compared to traditional methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Dynamic resource allocation is a fundamental challenge in wireless communication networks. It refers to the assignment of resources such as transmission power, frequency spectrum, and time slots to different users in a manner that optimizes overall system efficiency.\nConventionally, centralized algorithms have been employed for dynamic resource allocation. These methods necessitate a central controller to possess complete knowledge of the network state and to make allocation decisions on behalf of all users. However, as wireless networks expand in scale, such centralized approaches become increasingly impractical due to scalability constraints.\nRecently, significant attention has been directed toward leveraging deep reinforcement learning (DRL) for dynamic resource allocation in wireless networks. DRL, a subset of machine learning, enables policies to be learned through interaction with the environment. It has demonstrated remarkable success in diverse domains, including strategic gameplay, robotic control, and financial modeling.\nIn this study, a DRL-driven methodology is introduced for dynamic resource allocation within wireless networks. A simulated environment is designed to represent a wireless communication system consisting of a base station, multiple antennas, and several user devices. The RLlib framework is utilized to train a DRL agent capable of making resource allocation decisions that enhance system performance. The effectiveness of the proposed approach is evaluated by comparing it against traditional centralized algorithms, and it is demonstrated that the DRL-based method significantly outperforms conventional techniques.\nWireless data transmission has undergone substantial expansion in recent years and is anticipated to continue growing. As an increasing number of devices, including smartphones and wearable technologies, connect to wireless networks, the density of access points (APs) must be augmented. Deploying compact cells such as pico-cells and femto-cells has emerged as a highly effective strategy for addressing the escalating demand for spectrum [1]. However, with denser AP deployment and reduced cell sizes, wireless networks become inundated with signals, intensifying intra-cell and inter-cell interference issues [2]. Consequently, effective power distribution and interference mitigation become paramount challenges.\nAlgorithmic models have been formulated to manage interference, but quantifying their deviation from the optimal solution remains difficult. Many mathematical models assume simplified conditions to maintain analytical feasibility, yet such assumptions often fail to accurately represent real-world wireless environments, which are influenced by hardware imperfections and unpredictable channel variations. Traditional signal processing methods relying on model-based frameworks pose difficulties when incorporating specific hardware components and transmission scenarios, while their high computational demands render practical deployment infeasible. Given these challenges, machine learning-based approaches offer a promising alternative for next-generation wireless communication. These data-driven techniques provide solutions through pattern recognition rather than explicit mathematical formulations.\nMachine learning techniques can be broadly categorized into supervised learning and reinforcement learning. While supervised learning excels in classification tasks, it is less effective in deriving optimal guidance strategies. Reinforcement learning, on the other hand, focuses on maximizing cumulative rewards in an environment typically modeled as a Markov Decision Process (MDP). Classical reinforcement learning methods utilize dynamic programming techniques; however, maintaining a tabular representation of value functions or policies leads to the curse of dimensionality and limits generalization. The integration of reinforcement learning with deep neural networks, referred to as DRL, has achieved groundbreaking success in various applications, such as board games and video gaming.\nDRL methodologies can be divided into three principal categories [?]: value-based, policy-based, and actor-critic techniques. Value-based DRL methods determine optimal actions through state-action value functions, with notable examples"}, {"title": "II. LITERATURE REVIEW", "content": "A. RAN Slicing\nThe concept of RAN slicing involves the complex and dynamic nature of network operations, which in turn introduces significant challenges to the underlying resource allocation optimization process. As demonstrated in [3], the combination of network slicing and mobile edge computing (MEC) technologies was thoroughly examined, and the problem of resource allocation was formulated with the aim of minimizing the interference between different mobile virtual network operators. It was subsequently proven that this resource allocation problem is NP-hard, indicating its computational intractability.\nIn more recent studies, machine learning methodologies have been explored as potential solutions to the challenges inherent in RAN optimization, offering an alternative to traditional model-based approaches. The traditional approaches often struggle to remain feasible due to the highly intricate and interconnected nature of resource distribution, which must account for both the evolving demands of incoming services and the various resource constraints. For instance, in [4], it was demonstrated that the application of in-network deep learning techniques holds considerable promise in recognizing device- and application-specific characteristics, as well as for tasks such as traffic classification. Furthermore, the work presented in [5] made use of deep learning techniques to improve the overall efficiency of network load management and to enhance the availability of services, providing a more adaptive framework for network optimization.\nIn instances where large, comprehensive datasets may not be readily available to train sophisticated deep learning models for optimizing resource allocation in the context of network slicing, a model-free approach such as reinforcement learning (RL) has gained significant attention as a viable alternative. RL allows for dynamic adjustments to resource allocation strategies based on the continuous observation of the 5G network's performance, facilitating real-time decision-making and refinement of policies. In [6], RL was compared to traditional techniques such as static and round-robin scheduling methods for resource allocation in network slicing scenarios. Additionally, both bandwidth and computational resources were considered in the context of resource distribution strategies, as explored in [7], where RL-based approaches were evaluated against heuristic, best-effort, and random allocation methods. A prototype of network slicing was developed and deployed within an extensive mobile network infrastructure to demonstrate its practical application. Beyond this, RL has also been utilized for power-efficient management of resources in cloud-based RANs, a setting where multiple transmitters and receivers operate within the same frequency band, as opposed to the conventional use of 5G time-frequency blocks. This application of RL in power-efficient resource management was explored in [8], where RL-driven resource allocation was examined in relation to the predictive modeling of communication requests. Furthermore, the scope of RL in resource allocation has been significantly broadened, with its application extending across various wireless communication scenarios, well beyond the specific domain of network slicing."}, {"title": "III. SYSTEM MODEL", "content": "A. RAN Slicing\nThe concept of RAN slicing involves the complex and dynamic nature of network operations, which in turn introduces significant challenges to the underlying resource allocation"}, {"title": "IV. DEEP LEARNING APPROACHES", "content": "This section provides an overview of the various techniques utilized in dynamic resource allocation and discusses the strategies behind their learning algorithms.\nA. Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) is a reinforcement learning technique that falls under the category of policy-gradient methods. It has gained widespread popularity due to its robust performance and computational efficiency in training deep reinforcement learning (DRL) agents. PPO operates by managing two different policies: the current policy and the older, past policy. The current policy is responsible for generating the actions to be executed, while the previous policy is used to evaluate the gradient of the policy. The policy gradient indicates the extent to which the policy should be modified to enhance its overall performance.\nTo update the current policy, PPO utilizes a technique called the clipped surrogate objective. This method introduces modifications to the original policy gradient, making it more stable and reducing the likelihood of policy divergence. The primary mathematical expression used to update the policy parameters is given as:\n$\\theta \\leftarrow \\theta + \\alpha \\cdot \\text{Clip}(\\nabla_{\\theta}J(\\theta) - V_{\\theta}J(\\theta_{\\text{old}}), \\epsilon)$ (1)\nwhere $\\alpha$ represents the learning rate, $\\theta$ are the parameters of the policy, and $\\epsilon$ is a hyperparameter controlling the clipping range.\nB. Deep Q-Learning (DQN)\nDeep Q-Learning (DQN) is a widely adopted algorithm within reinforcement learning that utilizes a deep neural network to approximate the Q-function, which is essentially the expected cumulative reward for taking a particular action in a given state. The Q-function is continuously updated through iterations, using the Bellman equation as a guide to refine the estimate of the Q-values. This update rule is expressed as:\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha(r + \\gamma \\max_{a'} Q(s', a') - Q(s, a))$ (2)\nwhere s denotes the current state, a refers to the action taken, r is the immediate reward obtained from the action, s' is the resulting next state, $\\alpha$ is the learning rate, and $\\gamma$ represents the discount factor, which controls the emphasis on future rewards.\nTo effectively handle high-dimensional state spaces, DQN employs a deep neural network to approximate the Q-function. The network receives the state as input and generates Q-values corresponding to each possible action. The neural network is trained by minimizing the mean squared error between the predicted Q-value and the target Q-value, computed using the Bellman equation. The loss function for this process is defined as:\n$L(\\theta) = \\mathbb{E}[(Q(s, a; \\theta) - y)^2]$ (3)\nwhere $\\theta$ refers to the parameters of the neural network, and $y = r + \\gamma \\max_{a'} Q(s', a'; \\theta')$ represents the target Q-value, calculated using a separate target network to improve stability."}, {"title": "VI. CONCLUSION", "content": "In this report, we investigated the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. We created a simulation environment that models a wireless network with dynamic resource allocation and utilized the RLlib library to compare the performance of two popular DRL algorithms, namely DQN and PPO. Our results show that the choice of algorithm and learning rate significantly affect the system's performance, and the use of DRL algorithms can provide more efficient resource allocation compared to traditional methods.\nSpecifically, our experiments show that R2D2, a variant of DQN, outperforms both DQN and PPO in terms of achieving the desired reward most stably and the fastest. Our results also highlight the importance of choosing an appropriate learning rate, as the use of two separate learning rates in PPO did not improve its performance compared to using a single learning rate."}]}