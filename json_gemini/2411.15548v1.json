{"title": "An unconditional distribution learning advantage with shallow quantum circuits", "authors": ["N. Pirnay", "S. Jerbi", "J.-P. Seifert", "J. Eisert"], "abstract": "One of the core challenges of research in quantum computing is concerned with the question whether quan- tum advantages can be found for near-term quantum circuits that have implications for practical applications. Motivated by this mindset, in this work, we prove an unconditional quantum advantage in the probably approxi- mately correct (PAC) distribution learning framework with shallow quantum circuit hypotheses. We identify a meaningful generative distribution learning problem where constant-depth quantum circuits using one and two qubit gates (QNC\u00ba) are superior compared to constant-depth bounded fan-in classical circuits (NC) as a choice for hypothesis classes. We hence prove a PAC distribution learning separation for shallow quantum circuits over shallow classical circuits. We do so by building on recent results by Bene Watts and Parham on unconditional quantum advantages for sampling tasks with shallow circuits, which we technically uplift to a hyperplane learning problem, identifying non-local correlations as the origin of the quantum advantage.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning has changed the world we live in: It is relevant for most algorithms that make predictions based on past training data. With quantum algorithms hav- ing been proven to provide super-polynomial advantages for certain highly structured [1, 2] or largely paradigmatic sampling [3, 4] problems, it is an extremely natural ques- tion to ask to what extent near-term quantum computers may assist in tackling machine learning tasks that have a practical inclination. This is a core theme of the emer- gent field of quantum machine learning [5, 6]. Identifying such advantages is, however, much less of a straightfor- ward task that one might think: Quantum computers are known to be good at addressing highly structured prob- lems, while machine learning algorithms are generalizing from samples [7]. Also, such advantages of quantum over classical algorithms could come in the flavor of advan- tages in sample complexity, in computational complexity, or in generalization.\nFor some highly structured data, it has been shown that quantum computers indeed offer such super-polynomial advantages in computational complexity for meaningful and well-defined learning tasks with classical data [8]. This is true in particular for probably approximately cor- rect (PAC) generator [9] and density modeling [10] as well as for classification tasks [11] and reinforcement learning [12]. These quantum algorithms provide fair and sound advantages over all possible classical algorithms that can be proven under plausible and mild technical assumptions. These insights are motivating for the field, in that they show that one can expect advantages of this type to be possible. That said, they work for data only that are precisely engineered and not very plausible from a practical perspective. Above all, they resort to full fault tolerant quantum computers, which to date are still ficti- tious machines.\nFor this reason, one of the core questions of the field- if not the most important one at the present stage-is to find out whether shallow quantum circuits may offer quan- tum advantages that are plausible for near-term quantum architectures. The little we know about this important question to date lets us take a rather pessimistic viewpoint [13]. The need for such results is aggravated by the ob- servation that in the absence of quantum error correction, there is increasing evidence that all one has are logarithmi- cally deep quantum circuits before the effect of quantum noise sets in Refs. [14, 15], rendering potential quantum advantages void. So, can we hope short quantum circuits to offer advantages in quantum machine learning?\nIn this work, we answer the question to the affirma- tive. We do so by strongly building on the results of Refs. [16, 17] that show unconditional quantum advan- tages for evaluating certain functions and for sampling tasks. Building on this work, the quantum learnability of constant-depth classical circuits has been considered [18]. Such studies are here brought to a new level in the distri- bution learning context, by adding a new technical twist: We introduce a hyperplane learning problem that allows to uplift the sampling problem at hand to a meaningful PAC distribution learning problem.\nSpecifically, we formulate a class of generative PAC dis- tribution learning problems, where we are given examples from some unknown distribution in a distribution class. The goal is to output a generator for that distribution, that is, some hypothesis exposing an output distribution which is close to the unknown target distribution. We show that we can learn a constant-depth quantum circuit, consisting of one and two qubit gates, which achieves a level of accuracy arbitrarily close to the target distribution. In contrast, any classical circuit with constant depth and bounded fan-in gates is shown to achieve less proximity to the target distribution. Thus, showing an advantage of QNC\u00ba over NC as hypothesis classes.\nUltimately, we will identify the non-local correlations in quantum states that can be prepared with constant-"}, {"title": "II. PRELIMINARIES", "content": "Throughout this work, we will be concerned with tasks of learning probability distributions, as is common in mathematical learning theory. We begin by giving the definition for a generator of a distribution.\nDefinition 1 (Generator for D). Let D be a discrete probability distribution over {0,1}n and U be the uni- form distribution. A generator for D is any function $GEN_D: {0,1}^m \\rightarrow {0,1}^n$ that on uniformly random inputs outputs samples according to D, i.e.,\n$Pr_{x\\sim U({0,1}^m)} [GEN_D(x) = y] = D(y)$.\nIn this work, we are primarily interested two classes of generators: QNC\u00ba, which are constant-depth quantum circuits that consist of one and two qubit gates, and NC\u00b0, which are constant-depth classical with bounded fan-in gates. Let us now define PAC learning of a generator.\nDefinition 2 (($\\epsilon$, $\\delta$)-PAC generator learner for D). Let D be a class of discrete probability distributions over {0,1}n. Given some fixed $\\epsilon$, $\\delta$ \u2208 (0,1) an algorithm A is an ($\\epsilon$, $\\delta$)-PAC generator (GEN) learner of D, if for all D \u2208 D, when given access to samples from D, with probability at least 1 \u2013 $\\delta$, A outputs a generator for some distribution D', satisfying\n$drv (D, D') \\leq \\epsilon$.\nWe call A an efficient ($\\epsilon$, $\\delta$)-PAC generator learner for D if its time (and sample) complexity are O(poly(n)). We say that D is ($\\epsilon$, $\\delta$)-PAC generator learnable with H, if an efficient A exists and if we constrain the generator to some generator class H.\nIn this work, we construct a class of distributions that are PAC generator learnable with arbitrary low precision $\\epsilon$ and failure probability $\\delta$, if the generator is chosen from QNC\u00ba. If, however, the generator is chosen from NC\u00ba, such $\\epsilon$, $\\delta$ are not achievable."}, {"title": "III. FROM SAMPLING ADVANTAGE TO LEARNING ADVANTAGE", "content": "The learning separation proven here derives from a separation in a sampling problem. Watts and Parham [17] introduce a class of distributions {Dn}, for which they show that, asymptotically in n, a constant-depth quantum circuit samples approximately from Dn with higher fidelity than any constant-depth classical circuit could. To be more precise, Ref. [17] proves the following theorem.\nTheorem 3 (Theorem 3 in Ref. [17]). For each $\\delta$\u2208 (0, 1), there exists a family of distributions {Dn} such that for each n \u2208 N, Dn is a distribution over {0,1}n and\n1. There exists a constant-depth quantum circuit which takes state vector $|0^n\\rangle$ as input and produces a dis- tribution which has total variation distance at most $\\frac{\\delta}{\\pi} + O (n^{-c})$ from Dn for some c \u2208 (0,1).\n2. Each classical circuit with fan-in 2 which takes n + $n^c$ random bits as input and has total variation distance at most $\\frac{\\delta}{2} - o(1/log n)$ from Dn has depth $\\Omega(log log n)$."}, {"title": "Now, if we want to learn a generator for $D_n$, when n, p\nare given, one needs to learn the hyperplane s from m\nexamples of $D_n$. It is straightforward to generalize the\nproofs in Ref. [17] to obtain the hardness to learn a clas-\nsical constant-depth generator for $D_n$. At the same time,\none can easily come up with a simple algorithm to learn\n$D_n$ with generator formed by a constant-depth quantum\ncircuit. However, the result would still suffer from the\nlimitations of Theorem 3, where the quantum generator\nwould be able to approximate the target distribution better\nthan the classical generator, but up to an error that cannot\nbe made arbitrarily small (below $\\frac{\\delta}{2}$).\nConversely, in the PAC learning setting, we require\nthat for some probability of failure $\\delta$, we can make the\nerror $\\epsilon$ arbitrarily low (usually by taking more examples).\nInspired by these observations, we will, in the subsequent\nsections, introduce a new target distribution class, for\nwhich we show that we can learn constant-depth quantum\ngenerators, that generate the target distributions (with\narbitrary precision). At the same time, we show that any\nconstant-depth classical generator cannot approximate the\ntarget distributions up to some constant error.", "content": "Effectively, Watts and Parham [17] show an uncondi- tional sampling advantage of QNC\u00ba over NC\u00ba. In our work, we are interested in PAC generator learning prob- lems. So the question is, how do we obtain a learning advantage from the sampling advantage? From Theo- rem 3 we can actually directly construct a PAC generator learning problem, simply by taking the distribution class to be learned to consist of exactly one distribution Dn for every n. However, it is hardly a learning task when the distribution class consists only of one distribution per problem size. To introduce a genuine learning task into the ideas of Ref. [17], we introduce a hyperplane learning problem into the \u201cmajority mod p\u201d function. The \u201cma- jority mod p"}, {"title": "A. PAC generator learning advantage of QNC\u00ba over NCO", "content": "To obtain a PAC learning advantage with QNC\u00b0 with arbitrary precision, in what follows, we will first provide"}, {"title": "B. The target distribution class D", "content": "We will proceed in two steps. Fist we will define a distribution of the form (Z, pmmajmodp,s(Z)), where pmmajmodp,s is to be defined. In the second step, we give a constant-depth quantum circuit that approximately samples from the (Z, pmmajmodp,s(Z)) distribution. We then define the Born distribution of that constant-depth quantum circuit to be the target distribution of our PAC learning task.\nWe define the distribution (Z, pmmajmodp,s(Z)) con- structively. For that, let us first consider the bal- anced binary tree in Fig. 2.b). Call its vertex vari- ables $x_1,...,x_{n-1} \u2208 {0,1}$ and its edge variables $d_1,...,d_{n-1} \u2208 {0,1}$. Now define the function h(d) : {0,1}n-1 \u2192 {0,1}n-1 by\n$h(d)_i = \\bigoplus_{j: d_j \\in P(x_i)} d_j, i \u2208 {1, 2, ..., n-1},$\nwhere P(xi) is the set of edges contained in the (unique) path going from the root to xi. Essentially, h(d) is the parity of the edge variables along the path from the root to xi. Given this function, we can define the so-called binary tree poor man's GHZ state vector as\n$|PM_n\\rangle = \\frac{1}{2^{(n-1)/2}} \\sum_{d \\in {0,1}^{n-1}} \\frac{1}{\\sqrt{2}} ( |h(d)_0\\rangle + |h(d)_1\\rangle ),$\nwhere we call the first n 1 qubits of $|PM_n\\rangle$ \"edge\" qubits, and the following n 1 qubits \"vertex\" qubits. The binary tree poor man's GHZ state is also used in Ref. [17] and is a special case of the poor man's cat state [19], which is defined over arbitrary graphs. As discussed in Refs. [17, 19], $|PM_n\\rangle$ can be constructed with a constant-depth unitary quantum circuit. The \"poor man's\" attribute comes from the fact that the state can be created with little resources (i.e., in constant depth) but, compared to the GHZ state vector $\\frac{1}{\\sqrt{2}}(|0\\rangle^n + |1\\rangle^n)$, has random bit flips that cannot be corrected in constant depth."}, {"title": "4", "content": "Theorem 4 (Theorem 29 in Ref. [17]). For any n, the state vector |PMn) can be constructed by a depth-3 quan- tum circuit consisting of 1 and 2 qubit gates acting on 2n - 1 qubits.\nNow, let us consider the distribution that takes the form (Z, pmmajmodp,s(Z)), where Z is a uniformly sampled (2n - 2)-bit string and pmmajmodp,s is what we call the \"poor man's majority mod p\" function, defined as\n$pmmajmod_{p,s}: {0,1}^{n-1} \\times {0,1}^{n-1} \\rightarrow {0,1}$\n$pmmajmod_{p,s}(Z) = pmmajmod_{p,s}(d, x)$\n$:=$ $majmod_{p,s} \\bigg( \\sum_{i=0}^{n-1} x_i (-1)^{h(d)_i} \\bigg) \\oplus parity(x)$.\nThe results of Ref. [17] show that there exists a constant-depth unitary quantum circuit that approximates the (Z, pmmajmodp,s(Z)) distribution. An adaption (gen- eralization of pmmajmodp, to include the shift s) of Theo- rem 33 in Ref. [17] yields the following.\nTheorem 5 (Approximating the desired distribution). For n sufficiently large and p = nc for any constant c \u2208 (0,1/2) there exists a constant-depth quantum cir- cuit consisting of one- and two-qubit unitary gates which takes the (2n - 1)-qubit all zeros state as input and pro- duces an output which, when measured in the computa- tional basis, samples from an (2n - 1)-bit distribution (Z', Y) which correlates approximately with the distribu- tion (Z, pmmajmodp,s(Z)), i.e.\n$\\triangle((Z', Y), (Z, pmmajmod_{p,s}(Z))) \\leq \\frac{\\delta}{\\pi} + \\frac{1}{2} + O(n^{-c}).$\nThe quantum circuit to generate this approximate distri- bution (Z', Y) is given in Fig. 2 a). The Born distribution (Z', Y) generated by this circuit is our target distribution. To get a sense of why Theorem 5 holds, note that the state vector of the last qubit in Fig. 2) a), after measur- ing the first 2n - 2 qubits in the state vector |d,x), is approximately given by\n$|\\psi_\\text{res}\\rangle \\approx cos\\bigg( \\frac{\\pi}{4} \\bigg( \\sum_{i=0}^{n-1} x_i (-1)^{h(d)_i} + s \\bigg) \\frac{1}{p}\\bigg) |\\oplus(x)\\rangle$\n$\\quad + sin\\bigg( \\frac{\\pi}{4} \\bigg( \\sum_{i=0}^{n-1} x_i (-1)^{h(d)_i} + s \\bigg) \\frac{1}{p}\\bigg) |\\oplus(x)\\rangle.$\nAnd from Fig. 2.c), it can be seen that, to good approx- imation, cos\u00b2($\\frac{\\pi}{4}+ (k + s)$) is inversely correlated with majmodp,s(k). This is the desired property to ap- proximate (Z, pmmajmodp,s(Z)) as we want the last bit"}, {"title": "5", "content": "to equal \u2295(x) when majmodp,s(k) = 0 and \u2295(x) other- wise.\nNow, we are in a position to define our target distribu- tion class as\nD := {Dn,p,s | n \u2208 N, p\u2208 Z+, s\u2208 Fp, p is prime}.\nThe distribution Dn,p,s is given by the Born distribution of the unitary constant-depth quantum circuit in Fig. 2 a). A direct corollary of this and Theorem 4 is the following statement.\nCorollary 6 (Optimal quantum circuit). The optimal quantum circuit to generate the distribution Dn,p,s is at most constant-depth.\nC. PAC generator learning algorithm for D with QNCO\nWe now proceed by presenting an explicit learning al- gorithm that learns s from examples of Dn,p,s. From the knowledge of s, one can then trivially construct a gener- ator for Dn,p,s in QNC\u00ba, by simply using the quantum circuit in Fig. 2 a).\nTheorem III.1 (PAC generator learning algorithm of the quantum distribution). There exists a polynomial-time PAC generator learning algorithm for D, that for suffi- ciently large p \u2208 O(n1/3), any $\\delta$ > 0, and any s \u2208 Fp, outputs a description of a constant-depth quantum circuit whose Born distribution D' satisfies\n$\\triangle(D_{n,p,s}, D') = 0$\nwith probability 1 - $\\delta$, and which uses M\u2208\u039f (p\u00b2 log($\\frac{1}{\\delta}$)) examples of Dn,p,s.\nThe full proof can be found in Appendix B, but we give a high level overview of the ideas. We are given M examples from the discrete distribution Dn,p,s and n, p. From Equation (9), we know that under some approximate distribution P, conditioned on a given value of (d, x), the probability to observe (d, x, \u2295(x)) is f(k) = cos\u00b2($\\frac{\\pi}{4}$+ ($k + s$)) for\nn-1\nk = \u2211xi(-1)h(d)i mod p.\ni=0\nThe strategy for the learning algorithm is essentially to estimate f(k), for k = 1, . . ., p, from the M examples drawn from Dn,p,s. Then, as depicted in Fig. 2.c), we can identify k* = p - s mod p as the coordinate for which f(k*) = 1/2 and f (k* +1) > 1/2. This way, we recover s = p - k* mod p. The only difficulty is in computing the values f(k) to sufficient precision from examples. We show that M = O (p\u2074 log($\\frac{1}{\\delta}$)) examples are sufficient to guarantee enough precision to identify s with probability"}, {"title": "D. Classical hardness", "content": "To show the classical hardness of PAC generator learn- ing D with NC\u00ba, it suffices to show that, even for a fixed value of s, the distributions Dn,p,s are not in NC\u00ba. We thus can directly take the hardness result of Theorem 34 from Ref. [17].\nTheorem 7 (Classical hardness of PAC learning). For each $\\delta$ < 1, there exists an $\\epsilon$ > 0 such that for all suffi- ciently large even integer N = 2n - 2 and prime number p = \u0398(Nc) for a \u2208 ($\\delta$/3,1/3): Let f : {0,1}l \u2192 {0,1}N+1 be an (\\epsilon log N)1/2-local function, with l < N + Nc. Then $\\triangle(f(U), (Z, pmmajmodp,s=0(Z))) \\geq \\frac{1}{2} - O(\\frac{1}{log N})$.\nFor a d-local function, each output bit depends on at most d input bits. Intuitively, the theorem above states that there exist no constant-depth classical cir- cuits that can asymptotically sample from the distribution (Z, pmmajmodp,s=0(Z)) up to good fidelity, as constant-depth circuit are O(1)-local. The proof for the theorem above in Ref. [17] originally stems from the hardness proof in Ref. [22] with the major modification of consid- ering the \"poor man's majority mod p\u201d function instead of the \"majority mod p\" function. Their modification accommodates the fact that the terms in the \"poor man's majority mod p\" function no longer depend on disjoint variables by partitioning the balanced binary tree asso- ciated with Z into sub-trees, and then identify sub-trees corresponding to output variables which are independent when a large chunk of the input variables are fixed."}, {"title": "IV. RESULTS", "content": "We are now in a position to state our main result, which is a meaningful advantage of a shallow quantum circuit over instances of classical circuits in a well-defined learn- ing task.\nTheorem IV.1 (Advantage of shallow quantum hypothe- ses). For the problem of learning a constant-depth gener- ator for the distribution class D, the hypothesis choice of QNC0 over NCO yields an advantage in total variation distance of at least\n$\\frac{1}{\\pi} - O(\\frac{1}{log n}).$"}, {"title": "6", "content": "Proof. Due to Theorem 5, we know that\n$\\triangle(D_{n,p,s}, \\langle Z, pmmajmod_{p,s}(Z) \\rangle) \\leq \\frac{\\delta}{\\pi} + \\frac{1}{2} + O(n^{-c})$\nfor some constant c\u2208 (0,1/3), and due to Theorem 7 we know that\n$\\triangle(\\langle f(U) \\rangle, \\langle Z, pmmajmod_{p,s}(Z) \\rangle) \\geq 1/2 - O(\\frac{1}{log N})$\nwhere f is any (\u03b5log N)1/2-local function. It follows from the reverse triangle inequality that\n$\\triangle(\\langle f(U) \\rangle, D_{n,p,s}) \\geq \\frac{1}{\\pi} + \\frac{1}{2} - O(\\frac{\\delta}{n^c})- O(\\frac{1}{log N})$\nSince Dn,p,s is generated by the constant-depth quantum circuit in Fig. 2 a), and we can learn that circuit from examples to perfect precision, due to Theorem III.1, we obtain the advantage gap as in Eq. (15) for learning a generator for the target distribution class D using constant- depth quantum circuits instead of constant-depth classical circuits.\nmathematically well-defined machine learning problem in the PAC generator learning sense.\nIt is important to note that learning with constant-depth hypotheses is also regularly studied in classical learning theory [23]. In fact, the concept class studied in this work is polynomial in size (in the problem parameters), which is also a common setting for showing learning advantages in the classical PAC literature [24]. In this way, the separation proven here between quantum and classical learners naturally ties in with the literature on classical mathematical learning theory.\nSo our work answers the question whether quantum advantages in machine learning problems with short quan- tum circuits can be found to the affirmative. This leaves a lot of room for optimism on the use of quantum algo- rithms in the medium term, in particular in the light of the fact that the known results on limitations of non-error corrected quantum circuits would set in at logarithmic or poly-logarithmic depth [14, 15]: Here, we are resorting to constant-depth quantum circuits.\nThat said, this result is a stepping stone in a bigger pro- gram. Ideally, one should be able to show advantages also over deeper classical circuits. Also, while the problem considered is a meaningful problem and has a practical machine learning flavor to it, it still makes use of highly structured data, similar to other work showing quantum advantages in the learning context [9-12]. Steps have also been taken to deal with realistic and unstructured data [25], but again, such algorithms will presumably even- tually require a fault tolerant quantum computer. More work needs to be done to bring such ideas closer to reality and the realm of unstructured data. It is the hope that the present work can serve as a further inspiration along these lines of thought."}, {"title": "V. SUMMARY AND DISCUSSION", "content": "One of the core questions of research on quantum com- puting is concerned with the question whether one can meaningfully identify a quantum advantage in near-term devices that relates to a problem that has a practical flavor. In this work, we have identified a learning problem in which well-defined families of constant-depth quantum circuits feature an advantage over families of constant- depth classical circuits. This problem is a meaningful and"}, {"title": "8", "content": "Appendix A: Defining the Um,0 and Cm gates\nTo fully understand the construction of the Um,0 and Cm gates, we need to stride out a little bit. Firstly, let us consider a different gate, namely, the Am,0 gate. The Am,0 gate is defined by its action on the computational basis state vectors as\n$A_{\\theta,m} |x_1, x_2,...x_m\\rangle = exp(i\\theta x_m X) |x_1\\rangle exp(i\\theta x_1 X) |x_2\\rangle... exp(i\\theta x_{m-1} X)|x_m\\rangle$.\nRef. [17] shows that a constant-depth circuit with Am,0 gates can approximate the (Z, pmmajmodp,s(Z)) distribution. Namely, the following statement follows from Lemma 10 and Theorem 31 of Ref. [17] and a trivial generalization to s.\nTheorem 8 (Generalization of a statement of Ref. [17]). For any p \u2208 Z+ there is a constant-depth circuit consisting of one and two-qubit unitary gates and $A_{\\theta, m}$ operations (for arbitrary m \u2265 1) which takes the (2n - 1)-qubit all zeros state as input and produces an output state, which when measured in the computational basis, produces a distribution P with samples of the form (Z', Y), such that\n$\\triangle(P, \\langle Z, pmmajmod_{p,s}(Z) \\rangle \\leq \\frac{1}{\\pi} + \\frac{1}{2p} + O(p^{3/2}e^{-n/4p^2}).$\nThe circuit that achieves this approximation is depicted in Fig. 3. The Cm gate denotes a permutation whose action on the m qubit computational basis state vector $|x_1, x_2, ..., x_m\\rangle$ is given by\n$C_m |x_1, x_2,...,x_m\\rangle = |x_2, x_3, ..., x_m, x_1\\rangle$.\nThe C\u2020m gate can be implemented by a series of m \u2212 1 SWAP gates. Problematically, the Am,0 gate is non-unitary. To solve this issue, the unitary gate Um,0 is defined as\n$U_{m,\\theta} := \\begin{cases} A_{m,\\theta} & \\text{if } x \\in B^m, \\\\ C^{-1}(A_{m,\\theta}|x\\rangle + i sin^m(\\theta)A_{m,\\theta}|\\overline{x}\\rangle) & \\text{otherwise}, \\end{cases}$\nwhere\n$C := \\sqrt{1 - sin^{2m} (\\theta)}$."}, {"title": "9", "content": "is a normalizing constant and Bm is any set containing half the bit strings of length m with the property that for any x \u2208 {0,1}m either x \u2208 Bm or \\overline{x} \u2208 Bm. Essentially, Um,0 arises from applying the Gram-Schmidt ortho-normalization to the problematic states output by A0,m. It is shown that the unitary gate Um,0 approximates the non-unitary gate A0,m.\nLemma 9 (Lemma 18 in Ref. [17]). For any m, there exists a unitary matrix Um,0 satisfying\n$\\Arrowvert A_{\\theta, m} - U_{\\theta, m} \\Arrowvert_F \\in O(\\theta^m)$\nas \u03b8 \u2192 0.\nIn particular, setting $\u03b8 = \\frac{\\pi}{4p}$, with $p = n^c$ for some constant c \u2208 (0,1/2), the equations (116)-(119) in Ref. [17] give us the following.\nLemma 10 (Closeness of unitaries). For any m, and $\u03b8 = \\frac{\\pi}{4p}$, and $p = n^c$ for some constant c \u2208 (0,1/2), there exists a unitary matrix Um,0 satisfying\n$\\Arrowvert A_{\\theta, m} - U_{\\theta, m} \\Arrowvert_\\infty \\in O(n^{-mc}).$\nAnd thus Theorem 5 follows. We further call the distribution Q, which is produced by the circuit in Fig. 2 a) and uses the $U_{\\theta, m}$ gates. Now, let m = $[2/c + 1]$. Note that the circuits producing P and Q differ only by replacing every $A_{\\theta, m}$ gate with a $U_{\\theta, m}$ gate. There are O(n) such gates. It follows from using Lemma 10 and following the steps of equations (118)-(121) in Ref. [17], that\n$\\triangle(P, Q) \\in O(n) \\times O(n^{-mc}) = O(n^{1 -mc}) = O(1/n).$\n1. Circuit compilation of Um,0\nDirectly following the arguments put forward in Ref. [17], the Um,0 gates can be compiled into arbitrary one-qubit gates and two-qubit CNOT gates. Following the discussions in Ref. [26], any operator on m qubits can be performed with at most O(m34m) two-qubit gates, using the methods described in Ref. [27] and in a runtime that is bounded by a function of m. Further, any of those two qubit gates can be decomposed into a sequence of at most 5 one-qubit gates and CNOT gates, following the results of Ref. [28]. Thus, for constant m and any \u03b8, there exists a constant time algorithm that compiles the Um,0 gates into a constant depth sequence of arbitrary one-qubit gates and CNOT gates. The algorithm to obtain the circuit compilation to any unitary operator is also excellently explained in chapter 5.4 of Ref. [29]."}, {"title": "Appendix B: Learning algorithm", "content": "In this appendix, we present the detailed rigorous analysis of the learning algorithm used in the main text. We show that the circuits generating the distributions Q = Dn,p,s can be learned from examples to perfect precision. For our analysis, we require the following fact:\nFact 11 (Fact 3.2 in Ref. [22]). Let a1, a2,. . . , at be nonzero integers modulo p, and let (x1, x2,. . . , xt) \u2208 {0,1}t be sampled uniformly. Then the total variation distance between Si=1 aixi mod p and Up, the uniform distribution over {0, 1, . . . , p \u2212 1} is at most $\\sqrt{p}e^{-t/p^2}$.\nThe main contribution of this section is proving the following theorem:\nTheorem 12 (Learning algorithm, Theorem III.1). There exists a polynomial-time PAC generator learning algorithm for D, that for sufficiently large p \u2208 O(n1/3), any \u03b4 > 0, and any s \u2208 Fp, outputs a description of a constant-depth quantum circuit whose Born distribution D' satisfies\n$\\triangle(D_{n,p,s}, D') = 0$\nwith probability 1 \u2212 \u03b4, and which uses M \u2208 O (p\u00b2 log($\\frac{1}{\\delta}$)) examples of Dn,p,s."}, {"title": "10", "content": "Proof. We are given M examples of the discrete distribution Q := Dn,p,s and n, p. To learn Q, our goal is to find s using the M examples and then output a description of the circuit in Fig. 2 a).\nAs an intermediate step, let us for now assume that we observe examples from the distribution P defined in Theorem 8. Then Equation (9) reveals that, given a certain assignment of Z = (d, x), the probability to observe (d, x, \u2295(x)) in the examples is cos\u00b2($\\frac{\\pi}{4}$+ ($k + s$)), where\nn-1\nk = \u2211xi(-1)h(d)i mod p.\ni=0\nThe strategy for the learning algorithm is essentially to find out the cos\u00b2(\u00b7) values for all k = 1, . . . , p using M examples. Thus, basically, in the learning algorithm, we build a p-dimensional vector \u0e1b\u0e35 using the M examples, which approximates the p values for cos\u00b2(\u00b7). The cos\u00b2(\u00b7) function is depicted in Figure 2.c). From the figure, we can see that if we are able to find the coordinate p \u2212 s mod p, it will reveal s to us. Note that the coordinate p \u2212 s is integer. To find this coordinate, we search for the coordinate k* in \u0e1b\u0e35 where the cos\u00b2 function equals 1/2 and where it is larger than 1/2 for k* + 1 (i.e., where cos\u00b2 crosses 1/2 from below). We can then output our estimate of s by computing p \u2212 k* mod p, which is correct if indeed k* = p \u2212 s mod p. We now describe how we build \u0e1b\u0e35 and how closely it must approximate cos\u00b2 for our"}]}