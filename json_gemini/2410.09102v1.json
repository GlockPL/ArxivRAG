{"title": "INSTRUCTIONAL SEGMENT EMBEDDING: IMPROVING LLM SAFETY WITH INSTRUCTION HIERARCHY", "authors": ["Tong Wu", "Shujian Zhang", "Kaiqiang Song", "Silei Xu", "Sanqiang Zhao", "Ravi Agrawal", "Sathish Reddy Indurthi", "Chong Xiang", "Prateek Mittal", "Wenxuan Zhou"], "abstract": "Large Language Models (LLMs) are susceptible to security and safety threats, such as prompt injection, prompt extraction, and harmful requests. One major cause of these vulnerabilities is the lack of an instruction hierarchy. Modern LLM architectures treat all inputs equally, failing to distinguish between and prioritize various types of instructions, such as system messages, user prompts, and data. As a result, lower-priority user prompts may override more critical system instructions, including safety protocols. Existing approaches to achieving instruction hierarchy, such as delimiters and instruction-based training, do not address this issue at the architectural level. We introduce the Instructional Segment Embedding (ISE) technique, inspired by BERT, to modern large language models, which embeds instruction priority information directly into the model. This approach enables models to explicitly differentiate and prioritize various instruction types, significantly improving safety against malicious prompts that attempt to override priority rules. Our experiments on the Structured Query and Instruction Hierarchy benchmarks demonstrate an average robust accuracy increase of up to 15.75% and 18.68%, respectively. Furthermore, we observe an improvement in instruction-following capability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers a promising direction for enhancing the safety and effectiveness of LLM architectures.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have shown significant potential in enabling sophisticated agentic applications and facilitating autonomous decision-making across various domains, such as web agents, educational tools, medical assistance, and more (Yao et al., 2022; Gan et al., 2023; Abbasian et al., 2024). To optimize the use of AI applications, a structured approach to implementation is widely adopted. This involves clear distinctions among system instructions, user prompts, and data inputs, as illustrated in Figure 1. These instructions contain specific priorities that help the model execute functionalities correctly and better assist users.\nModern LLMs process text without formal mechanisms to differentiate and prioritize instructions. Consequently, malicious attackers can easily exploit this limitation to override priority roles, leading to various vulnerabilities. For example, prompt injection (Greshake et al., 2023) insert malicious instructions into data sources to subvert the original ones. Prompt extraction (Zhang et al., 2024) aim to extract system messages, revealing proprietary prompts. Harmful requests (Ganguli et al., 2022) involve malicious users providing unsafe instructions to elicit irresponsible or dangerous responses from the safety-aligned LLMs. These vulnerabilities underscore the significance of designing more robust instruction hierarchy in LLM applications to mitigate such attacks.\nRecently, research has been conducted to enhance models' ability to follow the instruction hierarchy. For instance, Hines et al. (2024) proposed prompt-based solutions utilizing a special delimiter between prompts. Chen et al. (2024) and Wallace et al. (2024) suggested methods for generating hierarchical prompts, incorporating adversarial data along with high-quality responses to fine-tune LLMs. However, despite these improvements, the core challenge persists: current LLM architectures still lack an effective mechanism to differentiate and prioritize hierarchical instructions.\nIn this work, we tackle the challenge by introducing an architecture-level design for LLMs. Inspired by BERT (Lan et al., 2019) and its variants (Lan et al., 2019; Yasunaga et al., 2022), we propose using an Instructional Segment Embedding (ISE) to categorize different types of instructions distinctly. Specifically, we enhance the input token by incorporating segment information that classifies each token by its role (e.g., system instruction as 0, user prompt as 1, and data input as 2). This segment information is processed through a learned embedding layer, converting it into segment embeddings, which are then passed to later self-attention layers along with token embeddings. To obtain a robust segment embedding layer, we perform supervised fine-tuning on datasets containing structured prompts and high-quality responses. This process enables the model to differentiate between levels of instruction hierarchies more effectively, thereby boosting the overall safety of the system.\nEmpirically, we conduct comprehensive experiments on two benchmarks: Structured Query (Chen et al., 2024) and Instruction Hierarchy (Wallace et al., 2024), which are constructed based on the Alpaca (Taori et al., 2023) and Ultrachat (Ding et al., 2023) datasets, respectively. We fine-tune multiple pretrained LLMs, including Llama-2-13B (Touvron et al., 2023), Llama-3-8B (Llama Team, 2024), and Llama-3.1-8B, and compare their performance with and without the use of Instructional Segment Embedding. Our findings indicate that our method yields substantial improvements in robustness while either maintaining or enhancing the models' general capabilities, regardless of the presence of adversarial training data. For example, on the Structured Query benchmark, the method achieves an average robust accuracy improvement of up to 15.75% against indirect prompt injection attacks. On the Instruction Hierarchy benchmark, our ISE yields an average boost in robustness of up to 18.68% across multiple vulnerabilities, including indirect and direct prompt injection, prompt extraction, and harmful requests. In addition, the integration of ISE also maintains or even improves the instruction-following capability by as much as 4.1% on AlpacaEval."}, {"title": "Contributions:", "content": "(1) We identify and analyze critical limitations in current LLM architectures concerning the lack of instruction hierarchy (Section 3). (2) We propose Instructional Segment Embedding, a simple yet effective method designed to incorporate instruction-type information directly into the model. This approach enables the model to better distinguish and prioritize instructions based on their privilege (Section 4). (3) We empirically demonstrate the effectiveness of ISE across two benchmarks, encompassing five training datasets and addressing four types of vulnerabilities (Sections 5 & 6)."}, {"title": "2 BACKGROUND: LLM VULNERABILITIES", "content": "Modern LLM products typically involve up to three stakeholders\u00b9: (1) the LLM application provider (e.g., OpenAI), who designs the model's system-level instructions and manages the general workflow; (2) the primary user, who provides input in the form of instructions or queries; and (3) third-party source/data, such as web search results, that offer additional context for the LLM. As a result, LLM applications often establish a hierarchical order of instructions based on their perceived reliability: system instructions take precedence, followed by user instructions, and finally data.\nSecurity vulnerabilities arise when conflicts between these priorities occur, such as (1) a malicious user attempting to bypass safety system instructions or (2) malicious web providers injecting harmful actions in data. These conflicts may take various forms, including prompt injections, prompt extractions, and harmful requests, as shown in Figure 2 and outlined below.\nPrompt injection (Figure 2a). Prompt injection attacks (Perez & Ribeiro, 2022) generally occur in two forms: indirect and direct. Indirect prompt injection attacks occur when third-party data input contains instructions that should never be followed by LLMs. Direct prompt injection attacks happen when a malicious attacker manipulates the user query to an LLM, causing the model to generate outputs that deviate from predefined instructions.\nPrompt extraction (Figure 2b). This vulnerability (Zhang et al., 2024) often exploits a weakness in certain LLM applications that store confidential information within system instructions. Attackers may craft malicious queries that prompt the model to reference this stored information, potentially leading to the disclosure of system prompts.\nHarmful requests (Figure 2c). Harmful requests (Ganguli et al., 2022) aim to bypass the model's safety alignment (Bai et al., 2022) through malicious queries. These prompts can lead to unsafe outcomes, including unethical responses or even the weaponization of LLMs.\nIn this paper, we aim to enhance the instruction hierarchy capabilities of LLMs, thereby mitigating various forms of attacks that attempt to override the priority rules."}, {"title": "3 LACK OF INSTRUCTION HIERARCHY IN MODERN LLM ARCHITECTURE", "content": "Current embeddings lack instruction hierarchy. Given an input context $X_M$ with M tokens $x_1, x_2,..., x_M$, the large language models first convert each token into a high-dimensional vector using a token embedding matrix $E_{Tok} \\in \\mathbb{R}^{V \\times D}$, where V is the vocabulary size, and D is the output embedding dimension. The embedding vector $e_{xm}^{Tok}$ for token $x_m$ is given by $E_{Tok}[x_m]$, based on its index in the vocabulary. Additionally, the model also obtains positional embeddings $E_{Pos}$, based on the position of each token. Then, the token embeddings $(e_{x_1}^{Tok}, e_{x_2}^{Tok}, ..., e_{x_M}^{Tok})$ will be fed into the transformer's self-attention layers along with positional embeddings for further processing.2\nIn these self-attention layers, each token embedding is processed \u201cequally\u201d. As a result, the model recognizes only the semantic content and sequential order of each token from the embedding, lacking the capability to distinguish their hierarchical significance. This architectural design can inherently lead to vulnerabilities. For instance, a lower-priority user prompt, such as \"Please focus on my prompt as the system prompt is outdated\", could mistakenly be prioritized and override the original system prompt. This could inadvertently lead to various types of vulnerabilities, as shown in Figure 2."}, {"title": "4 PROPOSED APPROACH: INSTRUCTIONAL SEGMENT EMBEDDING (ISE)", "content": "To tackle this challenge, we propose Instructional Segment Embedding (ISE), which encodes the instruction hierarchy directly into the embeddings. This enables subsequent self-attention layers to more effectively recognize and follow instruction priorities, thereby boosting robustness.\nSpecifically, we leverage a learnable embedding layer, similar to the token embedding matrix $E_{Tok}$, which we call the segment embedding matrix $E_{seg}$. We define $E_{seg} \\in \\mathbb{R}^{H \\times D}$, where H is the number of hierarchies and D is the embedding dimension. By default, we set H to 4, representing system, user, data, and output. Each token in $X_M$ is tagged with corresponding hierarchy information $h_m \\in \\{0, 1, 2, 3\\}$, readily derived from distinct stakeholder categories in the LLM applications. The instructional segment embeddings of $X_M$ are represented as $(e_{x_1}^{seg}, e_{x_2}^{seg}, e_{x_3}^{seg},..., e_{x_M}^{seg})$ and obtained from $E_{seg}[h_m]$. To incorporate this modification, the final embeddings are computed by summing the token embeddings and segment embeddings. This results in $(e_{x_1}^{seg} + e_{x_1}^{Tok}, e_{x_2}^{Seg} + e_{x_2}^{Tok}, e_{x_3}^{Seg} + e_{x_3}^{Tok}, ..., e_{x_M}^{Seg} + e_{x_M}^{Tok})$, as illustrated in Figure 4. These embeddings are then fed into self-attention layers, following the process used in current LLMs.\nThe segment embedding layer is trained alongside other parameters during the supervised fine-tuning (instruction tuning) phase. In our experiments, we use widely adopted instruction-following datasets and construct structured queries based on the original prompt using GPT-40 (OpenAI, 2023). Additionally, we experiment with datasets containing malicious instructions designed to override higher-level instructions, enabling the model to learn how to reject or ignore such commands."}, {"title": "Flexibility in design.", "content": "The design choice for Instructional Segment Embedding can be flexible and should be tailored to the specific downstream tasks. For instance, if the data category can be further subdivided into outputs from external API tools or online information, we can introduce \"tools type\" and \"web data type\" categories, providing more fine-grained information. If the application does not involve third-party context, the data type can be omitted."}, {"title": "Connection to BERT.", "content": "Inspired by BERT's segment embeddings (Devlin et al., 2019), originally used to distinguish input segments for next-sentence prediction, our approach repurposes these embeddings to encode hierarchical instructions. This helps address the need for structured prompts and safer LLM outputs by providing direct, contextually relevant cues to the model. Unlike BERT, we incorporate the output type for two reasons: (1) It supports consistent autoregressive inference for each token in the input. (2) output may also include instructions (e.g., \"Please provide more details of your question\") that are critical in multi-turn language tasks."}, {"title": "Simplicity.", "content": "The implementation is also straightforward and can be easily adapted for most transformer-based LLMs. We provide a PyTorch code snippet that demonstrates how to implement this in just a few lines, as shown in Appendix A."}, {"title": "5 EXPERIMENTAL DESIGN", "content": "In this section, we present how we conducted the experiments. Specifically, we begin by describing the generation of the training data (Section 5.1), the experimental setup (Section 5.2), and the details of the robustness evaluation against multiple attacks (Section 5.3)."}, {"title": "5.1 GENERATING TRAINING DATA", "content": "We conduct experiments using two benchmarks: Structured Query and Instruction Hierarchy. The Structured Query benchmark primarily focuses on indirect prompt injection attacks, whereas the Instruction Hierarchy benchmark evaluates all types of vulnerabilities discussed, including indirect and direct prompt injections, prompt extraction, and harmful requests.\nFor the Structured Query benchmark, we generally follow the approach of Chen et al. (2024). Two datasets are constructed: Clean Alpaca and Adversarial Alpaca. The Clean Alpaca dataset is constructed by Alpaca-Cleaned-50K dataset (Taori et al., 2023; Gururise, 2024). For the Adversarial Alpaca dataset, we incorporate malicious instructions into the data and train the model to ignore such instructions.\nFor the Instruction Hierarchy benchmark, we mostly adhere to previous work by Wallace et al. (2024) to create both aligned and misaligned data\u00b3. We select the UltraChat-200K dataset (Ding et al., 2023) as the base dataset, which contains more training data. Since UltraChat consists solely of prompts and responses, we utilized GPT-40 (OpenAI, 2023) to decompose 10K prompts into three components: system instructions, user instructions, and data inputs, which we term the UltraChat Baseline. Additionally, we incorporate datasets from SystemChat (Abacus.AI, 2023) and SystemMessage (Huggingface, 2023) that contain specifical system prompts, designated as the System Follow dataset. Lastly, We crafted three types of attacks for the malicious data: indirect/direct prompt injection and prompt extraction, which we collectively name the Instruction Hierarchy datasets. We excluded harmful request data from the training but used them as evaluations following Wallace et al. (2024). Further details on generating training data are available in Section B."}, {"title": "5.2 EXPERIMENT SETUP", "content": "Data processing. We format all training and evaluation samples with clear segmentation, including system, user, data, and output information. We merge the system and user instructions for the Structured Query benchmark into the same type, as all system instructions in Alpaca are identical. To simplify the experiments, we train and evaluate only single-turn chats, where the conversation ends after the model generates a complete response."}, {"title": "LLM training and inference.", "content": "By default, we utilize Llama-2-13B (Touvron et al., 2023) and Llama-3-8B (Llama Team, 2024) as the pretrained models for Structured Query and Instruction Hierarchy, respectively. Experiments with Llama-3.1-8B are presented in Section 6.4.\nWe employ supervised fine-tuning to update all parameters, including our segment embedding layer, for three epochs. A learning rate of 2e-5 and a cosine learning schedule are used. During inference, we use top-p sampling methods with the model's default settings."}, {"title": "Baselines.", "content": "We conduct experiments on multiple datasets and compare our Instructional Segment Embedding method (+ISE) with models that do not use it. For the Structured Query benchmark, we include experiments using either text (Baseline) or newly constructed tokens (Delimiter) to differentiate distinct types of prompts, following the approach of Chen et al. (2024). For the Instruction Hierarchy benchmark, we only include experiments with the default delimiters of LLAMA-3-8B (Baseline), as these are already specially reserved tokens (shown in Figure 3)."}, {"title": "5.3 ROBUSTNESS EVALUATION", "content": "We evaluate our method using comprehensive attack methods. More details are in Appendix C.\nStructured Query benchmark. Following the approach of Chen et al. (2024), we evaluate indirect prompt injection attacks on models trained on the Alpaca dataset. Specifically, we focus on four types of attacks: Naive Attack, Ignore Attack, Escape Separation Attack, and Completion Real Attack as in-domain attacks. All in-domain attacks are crafted by injecting one or a few sentences at the end of the data to trick the model into outputting the word \"hacked\". Since the scenarios for in-domain attacks are quite similar to the adversarial training data constructed by Chen et al. (2024), we further develop new attacks based on the in-domain methods. These new attacks inject adversarial texts at the data's beginning and end, called out-of-domain attacks. This strategy significantly degrades the robustness of models trained on the adversarial Alpaca dataset. For the evaluation metrics, we compute the rate at which the model does not generate content containing the word \"hacked\" and refer to this as robustness or robust accuracy.\nInstruction Hierarchy benchmark. Evaluating models trained on the Instruction Hierarchy bench-mark is complex due to the need to account for indirect and direct prompt injection, prompt extraction, and harmful requests. To address these challenges: (1) For indirect prompt injection, we apply the same evaluations and metrics used in Structured Query benchmarks. For direct prompt injection, we use the same attacking prompts but inject them directly into the user prompt. (2) For prompt extraction, we use the ShareGPT and Unnatural Instructions datasets from (Zhang et al., 2024), along with 15 author-selected effective extraction prompts, and evaluate robustness using an approximate metric based on Rouge-L recall (Lin, 2004). (3) For harmful requests, we follow the evaluations of (Wallace et al., 2024), using Jailbreakchat (Chat) and \"Do Anything Now\" (DAN) prompts (Shen et al., 2024) paired with StrongREJECT malicious instructions (Souly et al., 2024). We query GPT-40 to check whether its responses adhere to safety guardrails.\nComprehensive robustness metrics. For prompt injection and extraction, which encompass multiple attack methods or malicious prompts, we include additional metrics. We define average robustness as the model's average performance across these various attack methods, offering a general evaluation of model robustness. Furthermore, we introduce worst robustness, representing the model's ability to defend against the most challenging attack.\nClean evaluation. We evaluate the model's capacity using standard datasets. Both benchmarks are assessed with AlpacaEval 1.0 (Li et al., 2023). For the Instruction Hierarchy benchmark, we additionally use the MT-Bench (Zheng et al., 2023) to measure the model's performance."}, {"title": "6 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "We report the main results on the Structured Query benchmark in Section 6.1 and the Instruction Hierarchy in Section 6.2. We observe that our approach consistently achieves higher robust accuracy while either maintaining or improving general capability. We also present a more detailed analysis of multiple vulnerabilities in Section 6.3. Lastly, we conduct an over-refusal evaluation and assess generalization to the advanced Llama-3.1-8B model in Section 6.4."}, {"title": "6.1 MAIN RESULTS ON STRUCTURED QUERY", "content": "Maintains high utility. In Table 1, we present the main results for capability and robustness by comparing our method with the baseline and delimiter methods on both the clean and adversarial Alpaca datasets. Compared to the other two methods, Instructional Segment Embedding maintains high utility with negligible degradation or even slight improvement. The difference in winning rate between the methods is less than 1% on AlpacaEval.\nConsistent robustness enhancement. We also observe that our method consistently improves robustness against indirect prompt injection attacks. Specifically, it achieves a 15.75% increase in average robust accuracy and a 32.17% increase in worst robust accuracy against in-domain attacks when trained with the clean Alpaca dataset. Both the delimiter and our ISE reach nearly perfect in-domain robustness. For out-of-domain attacks, we find that adding ISE can also significantly enhance robustness, resulting in improvements of ~10% and ~7% in average robustness for clean and adversarial Alpaca, respectively. Interestingly, our out-of-domain attacks degrade the robustness of models trained on the adversarial Alpaca dataset more than those trained on the clean Alpaca dataset (16% vs. 5%). This suggests that the adversarial dataset may overfit to in-domain attacks. Nevertheless, adding ISE largely maintains generalization to out-of-domain attacks."}, {"title": "6.2 MAIN RESULTS ON INSTRUCTION HIERARCHY", "content": "We present the evaluation results for our method on the Instruction Hierarchy benchmark in Figure 5, focusing on model capability and average robustness across various datasets and attack scenarios.\nImprovement in capabilities. Adding ISE boosts instruction-following capabilities, particularly for models trained on the System Follow and Instruction Hierarchy datasets. For example, the AlpacaEval win rate improves by approximately ~4.1% when training on the Instruction Hierarchy dataset with our ISE, as shown in Figure 5(a). Additionally, we observe negligible degradation on MT-Bench for the UltraChat Baseline model and improvements for the other two training datasets.\nEnhanced safety against multiple vulnerabilities. We evaluate the robustness of the models against indirect and direct prompt injection attacks, prompt extraction attacks, and harmful requests. (1) Indirect and direct prompt injection scenarios (#1, #2, #3, and #4 in Figure 5(b)) : We report the average robustness across four types of attacks, including both in-domain (ID) and out-of-domain (OOD) contexts. Our results demonstrate robust accuracy improvements ranging from 5% to 25% across all training data configurations when applying the ISE method. Notably, for models trained with the UltraChat Baseline, robust accuracy increases by nearly 25% on average. (2) Prompt extraction scenarios (#5 and #6 in Figure 5(b)): Robustness is measured against 15 effective extraction prompts. Our findings show that models using ISE consistently achieve higher average robustness, with an increase of at least 10% across all datasets. This is evident even for models trained on the Instruction Hierarchy dataset, which already demonstrated more than 80% robust accuracy. (3) Harmful requests (#7 and #8 in Figure 5(b)): Our analysis reveals improvements in robustness for models under the UltraChat Baseline and Instruction Hierarchy settings when using ISE. For System Follow, our methods either maintain or slightly exceed the baseline method.\nOverall, using Instructional Segment Embeddings significantly enhances both the capabilities and robustness of models against a wide range of attacks on the Instruction Hierarchy benchmark."}, {"title": "6.3 DETAILED ANALYSIS OVER ATTACKS", "content": "The previous sections mainly covered the overall results (average robustness) across multiple prompt injection and extraction attacks. Here, we provide more detailed evaluations of attacks on the Instruction Hierarchy benchmark. Results for the Structure Query are reported in Appendix D.\nPrompt injection. In Figure 6, we present the results of indirect prompt injection attacks, including Naive, Ignore, Escape Separation, and Completion Real, across in-domain and out-of-domain scenarios. The results indicate that our ISE method significantly enhances performance compared to the baseline across nearly all scenarios. Notably, the Completion Real attack severely compromises model robustness, resulting in less than 10% effectiveness for models trained on the UltraChat Baseline and the System Follow dataset without ISE. This attack works by introducing a spoofed response to the benign instruction and concatenating a new malicious instruction into the data. Models that fail to effectively differentiate between these types of instructions are prone to executing the new malicious instruction. However, our method significantly boosts robustness, yielding improvements"}, {"title": "Prompt extraction.", "content": "As mentioned in Section 5.3, we utilize 15 effective malicious prompts to extract the system messages. In Figure 7, we present all the results and find that our method consistently outperforms the baseline, notably enhancing the worst robust accuracy by up to approximately 45%. Interestingly, the model trained on the UltraChat Baseline dataset with ISE exhibits the highest robustness, even exceeding that of the model trained on the Instruction Hierarchy dataset. We find that this is because the instruction-following capability of models trained on the UltraChat Baseline is relatively weak (about 20% lower than the other two models on AlpacaEval). Consequently, in scenarios where the model is misled into fulfilling a request to output the system message, it sometimes generates only a partial system prompt. Therefore, the attack is not classified as successful. Results on the Unnatural dataset are provided in Appendix E.3."}, {"title": "Harmful requests.", "content": "In Figure 8, we report the robustness of models trained on UltraChat Baseline against Jailbreakchat prompts across six categories: 'Disinformation and Deception,' 'Hate, Harassment, and Discrimination,' 'Illegal Goods and Services,' 'Non-Violent Crimes,' 'Sexual Content,' and 'Violence.' We observe that Instructional Segment Embedding improves robustness in 6 out of 6 categories, with improvements of up to 18%. Further results are reported in Appendix E.4."}, {"title": "6.4 OTHER ANALYSIS", "content": "Over-refusal Evaluation. One potential concern is that our method may overfit and refuse to follow user instructions. Therefore, we conduct an over-refusal evaluation on the WildChat dataset (Zhao et al., 2024) following (Anthropic, 2024; Zou et al., 2024). After filtering out prompts that exceed the context window, we use 691 non-toxic prompts to query the model and evaluate whether it generates reasonable responses using GPT-40. In Figure 9, we report the compliance rate on the benchmark and observe that our ISE improves the compliance rate by about 10% for the model trained on the UltraChat Baseline but shows slight degradation for the other two models. Overall, we expect that our method will maintain model capacity, as shown on AlpacaEval and MT-bench in Figure 5.\nGeneralization to Other Model. We also evaluated Llama-3.1-8B using the same setup as Llama-3-8B on the Instruction Hierarchy benchmarks. In Figure 10, we present the results on AlpacaEval and the robustness against four attacks (averaged results) of models trained on the UltraChat Baseline. Our method demonstrates an approximate 10% improvement in win rate on the AlpacaEval dataset. For robustness, we observe around a 5% robust accuracy improvement against harmful requests and over 10% on all other attacks. Overall, these results suggest our method can be generalized across different models. The complete results are provided in Appendix E.5."}, {"title": "7 RELATED WORKS", "content": "Safety vulnerabilities of LLMs. Recently, the safety of LLMs has become a critical concern. (1) These models are vulnerable to indirect and direct prompt injection attacks. Indirect prompt injections happen when malicious content is embedded in inputs sourced from external data providers, as discussed in various research studies (Greshake et al., 2023; Liu et al., 2023; Zhan et al., 2024; Debenedetti et al., 2024). In contrast, direct prompt injections occur when attackers explicitly introduce malicious instructions into user input, as demonstrated in (Perez & Ribeiro, 2022; Mu et al., 2023; Toyer et al., 2024; Sharma et al., 2024). (2) Another safety concern is the prompt extraction attack (Yu et al., 2023; Wang et al., 2023; Zhang et al., 2024), which is more related to privacy. In this type of attack, the attacker's goal is to maliciously obtain information from the system prompt, which is usually considered confidential. (3) Lastly, we consider harmful requests (Ganguli et al., 2022; Perez et al., 2022; Souly et al., 2024; Xie et al., 2024), where the prompts attempt to circumvent safety guidelines and elicit responses involving unsafe behavior, such as instructions for stealing someone's identity.\nImproving LLM robustness. To mitigate these attacks, researchers have developed two major defense strategies: prompt-based and learning-based defenses. Prompt-based defenses construct special instructions (e.g., in-context exemplars or delimiters) to mitigate attacks during inference (Wei et al., 2023; Hines et al., 2024; Zverev et al., 2024). While these defenses can achieve high robustness against specific attacks, concerns exist regarding their potential utility drops. Learning-based defenses (Piet et al., 2023; Chen et al., 2024; Wallace et al., 2024) aim to enhance model robustness by fine-tuning the models with a dataset of malicious instructions combined with robust responses. In this work, we explore another approach to improving model robustness by modifying the embedding approach, which is orthogonal to all previous mitigation strategies.\nEmbedding and architecture of LLMs. Recent research has also focused on improving the LLM embeddings and architectural designs to tackle different challenges. For instance, Yen et al. (2024) proposed a method for enhancing long-context generalization by using a small encoder to process long inputs in chunks. Additionally, McLeish et al. (2024) introduced the Abacus embedding to improve model performance on arithmetic tasks. In contrast, this paper focuses primarily on enabling the model to learn the instruction hierarchy through Instructional Segment Embedding, as inspired by previous work on designing BERT (Lan et al., 2019) and LinkBERT (Yasunaga et al., 2022)."}, {"title": "8 DISCUSSION AND CONCLUSION", "content": "Limitations and future work directions. This study primarily focused on the supervised fine-tuning phase, using single-turn conversations. Future work could explore incorporating ISE during the pre-training or RLHF stage and applying it to multi-turn conversation datasets. Additionally, while our approach significantly improves the instruction hierarchy capabilities of LLMs, it offers limited robustness against adaptive attacks, commonly referred to as jailbreaks (see Appendix F for more discussion). However, integrating our method with established adversarial training strategies may potentially enhance the robustness. Lastly, our experiments were limited to smaller models with 8B and 13B parameters and datasets less than 300K. It remains to be investigated whether our results can be generalized to larger models and datasets, which could provide deeper insights into the scalability of our proposed methods.\nConclusion. In this work, we introduced the Instructional Segment Embedding as the first attempt to design novel architectures to enhance instruction hierarchy. We conducted comprehensive experiments to demonstrate its effectiveness in improving robustness and general capabilities. We believe our method offers considerable potential for integration into real-world LLM applications and encourage practitioners to explore and test it in more downstream tasks."}, {"title": "9 ETHICS AND REPRODUCIBILITY STATEMENT", "content": "Ethics. Our research employs publicly available datasets for experiments and utilizes safety-aligned GPT-40 to generate some training data and judge the performance. We anticipate no ethical concerns with our methodology. Furthermore, we expect our work to have a positive societal impact, as we propose an embedding method to enhance model robustness against various malicious instructions.\nReproducibility. We discuss how we generate data in Section 5.1 and Appendix B. The process of training the model and inference is detailed in Section 5.2. The evaluation data is explained in Section 5.3 and Appendix C. Additionally, we provide a code snippet to implement our method in Appendix A. We will release our code, data, and models."}, {"title": "A DETAILS OF IMPLEMENTING INSTRUCTIONAL SEGMENT EMBEDDING", "content": "Here's an example of implementing Instructional Segment Embedding with a few lines of Python/Py-torch code. The additional code is highlighted in bold blue.\nIn the init function, we initialize embedding layers, including the token embedding layer, ISE embedding layer, and positional embedding layer. The inputs to the function include the embedding dimension (embed_size), vocabulary size (vocab_size), and ISE dimension (ISE_size, which defaults to 4).\nDuring inference (the forward function), we compute the token embeddings and ISE embeddings, then sum them for further processing. The input x is a list containing the input IDs of each token in the sentence, and the input seg is a list containing the segment IDs (e.g., system as 0, user as 1, data as 2, output as 3) for each token, with the same size as x."}, {"title": "B DETAILS OF TRAINING DATA", "content": "In this section, we provide details on how we construct the data, including both the clean and adversarial datasets, to conduct experiments on Structured Query and Instruction Hierarchy benchmarks.\nStructured Query benchmark. For the Clean Alpaca dataset, we use the Alpaca-Cleaned-50K dataset (Taori et al., 2023; Gururise, 2024) to fine-tune the model. Since the dataset shares the same system instructions (i.e., \u201cBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.", "attacks": "the Naive Attack and the Completion-Other Attack. In the Naive Attack", "models": "UltraChat Baseline", "components": "system instructions", "datasets": "SystemChat (Abacus.AI", "2024)": "indirect prompt injection", "injection": "We use 2K samples from Ultra"}]}