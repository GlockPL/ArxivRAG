{"title": "FROM LANGUAGE MODELS OVER TOKENS TO LANGUAGE MODELS OVER CHARACTERS", "authors": ["Tim Vieira", "Ben LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Brian DuSell", "John Terilla", "Timothy J. O'Donnell", "Ryan Cotterell"], "abstract": "Modern language models are internally and mathematically-distributions over token strings rather than character strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent analyses are very sensitive to the specification of the prompt (e.g., if the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. We find that\u2014even with a small computation budget-our method is able to accurately approximate the character-level distribution (less than 0.00021 excess bits / character) at reasonably fast speeds (46.3 characters / second) on the Llama 3.1 8B language model.", "sections": [{"title": "1 INTRODUCTION", "content": "For various technical reasons, modern language models are engineered as probability distributions over strings of tokens rather than strings of characters. However, this leads to a fundamental tension between the users of large language models and the engineers who build them. Specifically, token-level models are rife with unintuitive behaviors that without a technical fix-baffle users. As an illustrative example of a common user complaint, we exhibit the prompt boundary problem (see below). This paper provides a principled solution to the prompt boundary problem as well as other oddities that make interfacing with token-level language models with character-level prompts hard for users.\nTokenized language models: A brief overview. Let $\\Sigma$ be an alphabet of characters, and let $\\Sigma^*$ denote the set of all strings that can be built from it. Suppose there is a true distribution $p$ over $\\Sigma^*$ that we seek to model. We observe a training corpus of character strings: $\\sigma^{(1)}, ..., \\sigma^{(M)} iid p$. However, rather than estimating a language model that approximates $p$ directly, we employ a (possibly stochastic)\u00b9 tokenizer $\\tau$ that transforms the training corpus into a corpus of token strings. $\\delta^{(1)} ~ \\tau(\\cdot | \\sigma^{(1)}), ..., \\delta^{(M)} ~ \\tau(\\cdot | \\sigma^{(M)})$. Next, we estimate a token-level language model to fit the strings $\\delta^{(1)} ,\\delta^{(M)}$. Lastly, we use $p_{\\triangle}$ to generate character strings by means of the following generative process: (i) sample $\\delta ~ p_{\\triangle}$ and (ii) return $\\sigma = \\kappa(\\delta)$ where is a decoding function. Let $p$ denote the resulting distribution of this process. Practically, we hope that the choice of $\\tau$ and $\\kappa$ should aid in our ability to estimate $p$ in the sense that $p_{\\approx} p$.\u00b2 Commonly used tokenizers in the realm of LLMs use tokenizers $\\tau$ that break long strings into chunks. Intuitively, generating chunks"}, {"title": "The prompt boundary problem.", "content": "Consider the case of GPT2 (Radford et al., 2019), which was trained over token strings created from byte-pair encoding (BPE; Sennrich et al. (2016); Gage (1994)). Suppose we wish to generate continuations of the prompt:\n\"In_the_kingdom_of_the_blind, the\nUnfortunately, the language model $p_{\\triangle}$ does not accept a character string; thus, it is common to encode it as a token string with an encoding function $TBPE:3,4$\n$TBPE(\"In\\_the\\_kingdom\\_of\\_the\\_blind,\\_the) =  [1,818,262,13239,2869,262,7770,119,262]$\nIf we complete the prompt by taking the most likely next token (also known as greedy completion), we generate the following continuation:\n$ [530,12,18834,582,318,5822,526]$\nNow that we have our generated output, we can apply the decoding function $\\kappa$ that maps token strings to character strings as part of the tokenization protocol:\n$\\kappa_{BPE}( [1,818,262,13239,2869,262,7770,119,262,530,129,18834,582,318,5822,526]) = \"In\\_the\\_kingdom\\_of\\_the\\_blind,\\_the\\_one-eyed\\_man\\_is\\_king.\""}, {"title": "Getting it right:", "content": "A simple \"probability 101\u201d expression tells us the correct solution to the prompt boundary problem. Consider a $\\triangle^*$-valued random variable $Y$, distributed according to $p_{\\triangle}$. Then, the correct way to sample from $p_{\\Sigma}$ conditioned on a character string $\\sigma$ is according to\n$P_{Y~p_{\\triangle}}[Y = \\delta | \\kappa(Y) \\geq \\sigma]$ (1)\nwhere we have conditioned on the event that the decoded string $\\kappa(Y)$ has $\\sigma$ as a prefix (i.\u0435., $\\kappa(Y)\\geq \\sigma$). While innovative with respect to the literature, the expression $[P_{Y~p_{\\triangle}}[Y= \\delta |\\kappa(Y) \\geq \\sigma]]$ conveys the probability we are interested in precisely and concisely. For the more procedurally minded, this corresponds to the following generation process:6"}, {"title": "Character-level model.", "content": "Reasoning at the character level is intuitive. Consider again our example illustrating the prompt boundary problem. Appending whitespace behaves intuitively at the character level, as it satisfies the probabilistic chain rule:\n$p_{\\Sigma}(\\_one | \\\"In\\_the\\_kingdom\\_of\\_the\\_blind,\\_the) = p(\\ | \\\"In\\_the\\_kingdom\\_of\\_the\\_blind,\\_the)p(one | \\\"In\\_the\\_kingdom\\_of\\_the\\_blind,\\_the\\_)$\nHere $p_{\\Sigma}$ denotes the character-level model's conditional distribution. Similarly, recall the Hello, world example above. The character-level model correctly infers that d is the most likely next character given Hello, worl. The computation of this conditional probability is simply the total probability of the covering of Hello, world divided by the total probability of the covering of Hello, worl. These quantities are derived from our concept of covering, which directly leads to an algorithm for determining the distribution over possible next characters. Beyond the prompt boundary problem, computing the conditional probability of a character string given a token-level language model has many applications."}, {"title": "2 BACKGROUND", "content": "2.1 ALPHABETS AND STRINGS\nAn alphabet $\\Gamma$ is a non-empty, finite set of elements called symbols. A string $y$ over alphabet $\\Gamma$ is a finite sequence $y = y_1... y_N$ for some $0 < N < \\infty$ of symbols where $y_1,..., y_\\nu \\in \\Gamma$. Let $|y|$ denote the string's length $N$. We denote the empty string as $\\varepsilon$. For any alphabet $\\Gamma$, let $\\Gamma^*$ denote the set of all strings over $\\Gamma$, and let $\\Gamma^+$ denote the set of all non-empty strings over $\\Gamma$. For any two strings $y', y'' \\in \\Gamma^*$, we denote their concatenation as $y'\u00b7y''$. Additionally, we define $S\u00b7S' def  \\{y\u00b7y' \\vert y \\in S, y' \\in S'\\}$ for any $S, S' \\subseteq \\Gamma^*$. Given a string $y$ such that $|y| \\geq t$, let $y < t$ denote the string of the first $t-1$ characters of $y$. We write $y \\leq y'$ if $y$ is a prefix of $y'$ and $y < y'$ if $y$ is a proper prefix of $y'$. The relation $<$ defines a partial order on $\\Gamma^*$. We write $>$ and $\\gtrdot$ to refer to the relations < and < with their respective arguments transposed.\n2.2 LANGUAGE MODELS AND PREFIX PROBABILITY\nA language model $p_{\\Gamma}$ is a probability distribution over $\\Gamma^*$ where $\\Gamma$ is an alphabet. Let $Y$ be a $\\Gamma^*$-valued random variable distributed according to $p_{\\Gamma}$ and $y \\in \\Gamma^*$. We define the prefix probability"}, {"title": "3 ALGORITHMS", "content": "This section gives algorithms for $p_{\\Sigma}(\\sigma), p_{\\Sigma}(\\sigma), p_{\\Sigma}(\\sigma' | \\sigma), p_{\\Sigma}(EOS | \\sigma)$, and conditional token generation (i.e., our solution to the prompt boundary problem). Unlike prior papers (Cao & Rimell, 2021; Chirkova et al., 2023; Phan et al., 2024) on the topic of marginalizing tokenization strings to compute, or estimate, the probability of character strings, i.e., compute $p_{\\Sigma}(\\sigma)$, we will provide algorithms for computing conditional probabilities $p_{\\Sigma}(\\sigma' | \\sigma)$ for any strings $\\sigma, \\sigma' \\epsilon \\Sigma^*$. Our approach is based on estimating the prefix probability $p_{\\Sigma}(\\sigma)$, and it can be used in conjunction with Eq. (3) to compute $p_{\\varsigma} (\\sigma' \\vert \\sigma)$. Throughout this section, we assume that $\\kappa$ is strict-prefix monotone.\n3.1 COVERING\nOur primary tool for the prefix probability $p_{\\Sigma}(\\sigma)$ is the covering of $\\sigma$. It is a subset of tokens associated with the given string $\\sigma$ that is sufficient to evaluate its prefix probability. As we will see experimentally (\u00a74), this set's subset of high-probability token strings is often small enough to enumerate.\nEq. (12) shows that we can, in principle, compute the prefix probability $p_{\\Sigma}(\\sigma)$ by summing over prefix-encodings of $\\sigma, P(\\sigma) def  \\{\\delta \\in \\triangle^*: \\kappa(\\delta) \\geq \\sigma\\}$. Unfortunately, $P(\\sigma)$ is infinitely large. Fortunately, we can exploit the prefix monotone structure of $\\kappa$ to find a different way to perform the summation by summing over a finite set.\nLet $\\delta \\epsilon \\triangle^*$, and $\\sigma \\epsilon \\Sigma^*$. We say that $\\delta$ covers $\\sigma$ if and only if $\\kappa(\\delta) \\geq \\sigma$. Monotonicity ensures that for all $\\delta \\epsilon P(\\sigma)$, we have that $\\forall\\delta' \\epsilon \\triangle^* : \\kappa(\\delta\u00b7\\delta') \\geq \\sigma$. In other words, any $\\delta$ that decodes to an extension of $\\sigma$ (i.e., $\\kappa(\\delta) \\geq \\sigma$) will continue to do so if we append tokens to it. Thus, we may additionally qualify the relationship as $\\delta$ minimally covers $\\sigma$ if and only if $\\kappa (\\delta_1 ... \\delta_{M-1}) < \\sigma$. With that in mind, we define $\\phi(\\delta)$ as the shortest prefix $\\delta' \\leq \\delta$ such that $\\kappa(\\delta') \\geq \\sigma$, i.e., it maps any $\\delta$ that covers $\\sigma$ to a (possibly equal) token string that minimally covers $\\sigma$. Next, we define the set of minimal prefix encodings of $C(\\sigma)$, which we call the covering of $\\sigma, C(\\sigma) def  \\{\\phi(\\delta) \\vert \\delta \\epsilon P(\\sigma)\\}$. A more convenient expression for the covering $C(\\sigma)$ of a string $\\sigma\\epsilon \\Sigma^*$ is equal to the following subset of $\\triangle^*$: \n$C(\\sigma) = \\begin{cases} \\{\\varepsilon\\} & \\text{if } \\sigma = \\varepsilon \\\\ \\{\\delta_1... \\delta_M \\epsilon \\triangle^+: \\kappa(\\delta_1... \\delta_{M-1}) \\gtrdot \\sigma \\gtrdot \\kappa(\\delta_1... \\delta_M)\\} & \\text{otherwise} \\end{cases}$"}, {"title": "Proposition 1.", "content": "Suppose $(\\Sigma, \\triangle, \\tau, \\kappa)$ is a tokenization model where $\\kappa$ is strict-prefix monotone and $p_{\\triangle}$ is a token-level language model. Then, the prefix probability $p(\\sigma)$ for the character-level model Eq. (7) is given by the equation below.\n$p_{\\Sigma}(\\sigma) = \\sum p_{\\triangle}(\\delta), \\ \\sigma\\epsilon\\Sigma^*$ (14)\n$\\delta \\epsilon C(\\sigma)$\nProof. We prove the proposition directly through the following manipulations.\n$p_{\\Sigma}(\\sigma) = \\sum 1\\{\\sigma \\kappa(\\delta')\\}p_{\\triangle}(\\delta') (15)\n$\\delta' \\epsilon \\triangle^*$\n$= 1\\{\\sigma = \\varepsilon\\}p_{\\triangle}(\\varepsilon) + \\sum1\\{\\sigma < \\kappa(\\delta')\\} p_{\\triangle}(\\delta') (16)\n$\\delta'\\epsilon \\triangle^+$\n$= 1\\{\\sigma = \\varepsilon\\}p_{\\triangle}(\\varepsilon) + \\sum 1\\{\\kappa(\\delta) \\kappa(\\delta\u00b7\\delta'.\\delta'')\\} p_{\\triangle}(\\delta\u00b7\\delta'\u00b7\\delta'') (17)\n$\\delta.\\delta'-\\delta'\\epsilon \\triangle^+$\n$= 1\\{\\sigma = \\varepsilon\\}p_{\\triangle}(\\varepsilon) + \\sum 1\\{\\kappa(\\delta) \\kappa(\\delta\u00b7\\delta')\\} \\sum p_{\\triangle}(\\delta.\\delta'.\\delta'') (18)\n$\\delta.\\delta'\\epsilon \\triangle^+ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta'' \\epsilon \\triangle^*$\n$= 1\\{\\sigma = \\varepsilon\\}p_{\\triangle}(\\varepsilon) + \\sum 1\\{\\kappa(\\delta) \\kappa(\\delta\u00b7\\delta')\\} p_{\\triangle}(\\delta\u00b7\\delta') (19)\n$\\delta. \\delta' \\epsilon \\triangle^+$\n$= \\sum p_{\\triangle}(\\delta) (20)\n$\\delta \\epsilon C(\\sigma)$\nAbout the steps above: We start with the summation expression for the character-level prefix prob-ability (i.e., Eq. (12)). We expand the summation into two cases (so that it will eventually match the two cases in the expression for the covering Eq. (13)). Next, for each summand, we consider its unique minimal prefix $\\delta\u00b7\\delta'$ covering $\\sigma$. We see why $\\varepsilon$ is handled separately, as it cannot be covered by a token sequence of that form. We exploit the key property of prefix monotone tokenizers (i.e., that once $\\delta.\\delta'$ covers $\\sigma$, each extension $\\delta\u00b7\\delta'\\delta''$ continues to cover it). This allows us to rearrange the summation to sum over the extension $\\d\"", "C(\\sigma)": "kappa(\\delta) = \\sigma\\}, (21)\nto give an expression of the string's probability\n$p_{\\Sigma}(\\sigma) = \\sum 1\\{\\kappa(\\delta) = \\sigma\\} p_{\\triangle}(\\delta) = \\sum p_{\\triangle}(\\delta) (22)\n$\\delta \\epsilon \\varepsilon(\\sigma)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta \\epsilon \\varepsilon(\\sigma)$"}, {"title": "Proposition 1.", "content": "For all $\\sigma\\epsilon \\Sigma^*, \\delta \\epsilon \\triangle^*$,\n$P_{\\gamma*~p_{\\triangle}}[\\kappa(Y) = \\delta | \\kappa(Y) \\geq \\sigma] = \\frac{p_{\\triangle}(\\delta)}{p_{\\Sigma}(\\sigma)} 1\\{\\delta\\epsilon C(\\sigma)\\} (27)$\nThus, we have provided an efficient solution to the prompt boundary problem. We also note that generating from $p_\\Sigma$ a character at a time is also a correct solution to the prompt boundary problem; however, it is slower because it does not benefit from the fact that the generated string is shorter in token space. This is because once the minimally covering token string has been sampled, the method sample_completion will generate a complete sequence more efficiently than the character-at-a-time sample algorithm, as it does not have the overhead of marginalization that $p_{\\Sigma}(\\cdot|\\cdot)$ does."}, {"title": "4 EXPERIMENTS", "content": "In this section, we investigate our proposed algorithm's running time and accuracy.\nSetup.\n\u2022 We use Llama 3.1 8B (Dubey et al., 2024) and GPT2 large (Radford et al., 2019) from the transformers library (Wolf et al., 2020). Both models were trained over token strings created from byte-pair encoding (BPE; Sennrich et al. (2016); Gage (1994)).\n\u2022 We use the wikitext-103-v1 corpus as a source of character strings; we used the version in the datasets library. Specifically, we use the test portion.\n\u2022 We use LLM (Kwon et al., 2023) to perform the efficient, batched evaluation of transformer language models on GPUs. We batch the evaluation of all sequence extensions. All experiments were run an A100 GPU with 80GB of memory.\n\u2022 Our implementation uses a trie to represent all items in each bucket efficiently (see App. B). We use the bucket-based pruning heuristic described in \u00a73.\nIn the discussion below, let $N$ denote $|\\sigma|$ and let $K$ denote the beam-size parameter.\nError vs. speed. To better understand the quality of the approximation that our method provides, we perform the following experiment. We use a large beam $K = 128$ as a reference model, and we measure the average per-character Jensen-Shannon distance (JSD) to the reference model's conditional distribution over the next character. The table below shows the average JSD between the character-level conditional distributions with beam sizes $K\\epsilon \\{4, 8, 16, 32, 64\\}$ and the reference model.17 The average is computed across the first 4000 characters of the wikitext-103-v1 test corpus. To quantify the speed of our method, we show the number of characters per second for each value of $K$, including the reference model.18 In parentheses, we provide a 95% confidence interval computed using bootstrapping. To aid in interpretation, we also show the same information graphically below."}, {"title": "5 CONCLUSIONS, LIMITATIONS, AND FUTURE WORK", "content": "We have developed an effective method for ameliorating tensions between tokens and characters faced by engineers and users. We gave theory and algorithms that provide a character-level interface to tokenized language models. We characterized and resolved the prompt boundary problem. We investigated the empirical speed and error rates of our method on a modern language model.\nThe primary limitation of our beam summing method is that it will require a very large beam size $K$ when the language model does not favor a small number of tokenizations. The models that we explored in our experiments concentrate mass on a few tokenizations; thus, we did not require large $K$ to estimate their character-level prefix probabilities accurately. Future work may wish to investigate sampling-based estimation methods and possibly derive upper and lower bounds on the true values of the character-level prefix probability."}, {"title": "A THE SIZE OF THE COVERING", "content": "We now set about to bound the worst-case size of the covering function. To do so, we introduce additional definitions that characterize the different growth factors.\nWe define the \u03ba's fertility as\n$F def max max max |{\\delta' \\epsilon \\triangle: \\sigma = \\kappa(\\delta.\\delta')}\\}| \\leq |\\triangle| (28)\n$\\delta\\epsilon \\triangle^* \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma\\epsilon \\Sigma^*$\nExample 4. The BPE tokenizer has $F_{BPE} = 1$ because it is multiplicative, and its tokens represent distinct substrings. More formally,\n$F_{BPE} = max max |{\\delta' \\epsilon \\triangle: \\sigma = \\kappa(\\delta.\\delta')}\\}|  [def of fertility] (29)\n$\\delta\\epsilon \\triangle^* \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma\\epsilon \\Sigma^*$\n$= max max |{\\delta' \\epsilon \\triangle: \\sigma = \\kappa(\\delta)\u00b7\\kappa(\\delta')}\\}| [def multiplicativity] (30)\n$\\delta\\epsilon \\triangle^* \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma\\epsilon \\Sigma^*$\n$= |{\\delta' \\epsilon \\triangle: \\sigma' = \\kappa(\\delta')| [def function] (31)\n$= 1 [distinctness] (32)\nAdditionally, we define a k's munch as follows.\n$M def max max |\\kappa(\\delta.\\delta')| - |\\kappa(\\delta)| (33)\n$\\delta\\epsilon \\triangle^* \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta\\epsilon \\triangle$\nIn words, the munch measures the length of the largest number of characters that can be introduced by adding one more token to any given context.\nExample 5. The munch of a multiplicative k, such as BPE, is $max_{\\delta \\in \\triangle}|\\kappa(\\delta)|$. Put in words, it is the length of the longest detokenization. The munch for GPT-2 is surprisingly long (128), as they are common in, for example, markdown syntax.\nProposition 3. Let F and M be the fertility and munch of k. Then, for all $\\sigma \\epsilon \\Sigma^*$,\n$|C(\\sigma)| \\leq C(|\\sigma|) (34)$\nwhere\n$C(n) def  \\begin{cases} 0 & \\text{if n < 0} \\\\ 1 & \\text{if n = 0} \\\\ F \\sum_{j=n-M}^{n-1} C(j) & \\text{otherwise} \\end{cases}$ (35)"}]}