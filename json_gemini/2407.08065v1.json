{"title": "Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach", "authors": ["Isaac Sheidlower", "Reuben Aronson", "Elaine Schaertl Short"], "abstract": "Foundation models are a promising path toward general-purpose and user-friendly robots. The prevalent approach involves training a \u201cgeneralist policy\u201d that, like a reinforcement learning policy, uses observations to output actions. Although this approach has seen much success, several concerns arise when considering deployment and end-user interaction with these systems. In particular, the lack of modularity between tasks means that when model weights are updated (e.g., when a user provides feedback), the behavior in other, unrelated tasks may be affected. This can negatively impact the system's interpretability and usability. We present an alternative approach to the design of robot foundation models, Diffusion for Policy Parameters (DPP), which generates stand-alone, task-specific policies. Since these policies are detached from the foundation model, they are updated only when a user wants, either through feedback or personalization, allowing them to gain a high degree of familiarity with that policy. We demonstrate a proof-of-concept of DPP in simulation then discuss its limitations and the future of interpretable foundation models.", "sections": [{"title": "Introduction", "content": "Current efforts in creating task-generalizable, novice-friendly robots are largely focused on foundational models of robot behavior. The goal of such a model is to have a user ask the robot to do an arbitrary task via verbal or non-verbal communication, then have the robot perform the task with little to no further human supervision or intervention. The relatively few robot foundation models that exist can all be categorized as \"generalist robot policies.\" In particular, the input is an observation of the robot's state combined with a language or goal embedding that specifies the task, and the output is a robot action, such as end-effector displacement. This is the same input-output relationship as a typical task-specific Reinforcement Learning (RL) policy.\nHowever, these generalist policies have limitations that may pose serious problems when actually deployed for users. In deployment, a foundation model should be able to learn from data in many different environments and tasks and personalize to individual users in response to training. However, generalist policies are not localized with respect to task: new feedback for one task could change model behavior in a completely unrelated task. This property limits the ability of users to personalize the system or learn what to expect of its behavior for a given task. In this work, we propose an alternative approach where the foundation model is a policy generator, which outputs standalone, task-specific policies. We discuss potential benefits of this approach to robot foundation models as well as potential challenges with generalist policies as a sole solution.\nWe present Diffusion for Policy Parameters (DPP), a method for generating standalone task policies conditioned on a task specification. We present a proof-of-concept implementation for smaller grid-world tasks. We show, to the best of our knowledge, for the first time that one can learn representations to generate policies directly in parameter space, without the need for policy search. Finally, we discuss limitations of the DPP approach and how to both potentially resolve them and other alternative methods towards generating task-specific policies. Our results show that DPP is a promising approach to robot behavior foundation models and warrants further investigation."}, {"title": "Related Work", "content": "Recent technological advancements in Generative AI (Gen-AI) including the transformer architecture (Vaswani et al., 2023), and diffusion models (Ho et al., 2020), that are both high performing and scale to large amounts of data, have allowed for the development of larger and more general purpose task models. In the case, of language prediction, Large Language Models (LLMs) such as GPT-4 (OpenAI et al., 2024) and LLaMa (Touvron et al., 2023), have enabled a single large-scale model to perform a variety of linguistic tasks"}, {"title": "Potential Challenges with Robot Foundation Models as Generalist Policies", "content": "Robots should be able to learn from feedback and have real-time behavior personalization for any given task. If the policy the user is interacting with is a generalist robot policy, two problems may limit a user's ability to do this. The first is that when a user teaches the robot a new task or personalizes the behavior for a certain task, the behavior in separate and unrelated tasks may be affected. This may jeopardize the interpretability and legibility of the system (Bobu et al., 2024). Another is that updates to the base of the model from the organization which developed the model may have downstream affects on specific tasks/robot behavior that may be unnexpected or undesired by a user. This is already the case with consumer-available LLMs such as ChatGPT, however, in the case of robotics, the consistency of the robot's behavior is a crucial component to the user's ability to teach and interact with the robot. In fact, robots spontaneously acting in unexpected ways around users may cause physical safety concerns beyond those posed by systems operating solely on language. Thus, making sure that a robot's task behavior is changed when and how a user wants is crucial."}, {"title": "Diffusion for Policy Parameters (DPP)", "content": "We present Diffusion for Policy Parameters (DPP), a novel approach for learning how to generate standalone policies for individual tasks. DPP alleviates some of the concerns mentioned in the prior section. We then present a proof-of-concept evaluation in a grid-world simulation. This is, to the best of our knowledge, the first generative approach for creating policies in parameter space. While policy search (Plappert et al., 2018; Taylor et al., 2007; Levine & Koltun, 2013; Kalyanakrishnan & Stone, 2009) and exploration over policy parameters (Fontaine & Nikolaidis, 2021; Mouret & Clune, 2015; Tjanaka et al., 2021) have been explored, generative AI techniques have not been used directly in parameter space.\nThe DPP method learns a conditional diffusion model for generating policies in in policy parameter space. The steps for DPP are: collect a dataset of language/goal conditioned tasks and a dataset of demonstrations over those task; train a large set of policies on either the demonstrations or tasks themselves;"}, {"title": "Environment and Data Collection", "content": "We ran experiments in the Minigrid environment (Chevalier-Boisvert et al., 2023) for its suite of language-conditioned tasks on which we can train many agents on in a relatively small amount of time and with limited hardware. All tasks have a similar reward structure: sparse reward with a time-step penalty, resulting in a cumulative reward between 0 and 1. To generate a large number of tasks, we took three language-conditioned tasks and made each goal specification within that task its own task. In particular, we took the environments Fetch, Go to Door, and Go to Object, and for each possible object configuration, made that a task (e.g. Go to Door contains both the \"go to red key\" and \"go to blue box\" task specification, and we treat each as a task to train an agent on). We chose the 5x5 versions of each task for computational efficiency and quicker training. We then collected many seeds for each task to ensure random goal positions and obstacles, resulting in 84 unique tasks. While these tasks are significantly simpler than in-the-wild robot tasks, they provide a wide range of separate policies to train on.\nTo collect policy data on these tasks, we trained 84 RL agents using PPO (Schulman et al., 2017) to optimality (achieving a mean reward > .98). We then trained behavior cloning (BC) agents on trajectories collected from the RL agents until they received a near-optimal average reward of > .85. We chose BC as opposed to RL for every agent because it was more timely to train and collect the policies. We trained approximately 1000 agents for each of these tasks, discarding tasks where BC did not achieve high reward given the allotted trajectories. This resulted in BC agents for 64 of the 84 tasks and resulted in 74,000 trained policies."}, {"title": "Model Design", "content": "Given the dataset of policies, we trained a conditional diffusion model which takes as input a language description of the task and outputs an end-to-end policy network for that task. The model architecture and description can be found in Table 1. The architecture was largely decided on based on trial and error. However, two key decisions were necessary to effectively learn in parameter space. The first was to use an entirely transformer-based architecture, as opposed to, e.g., a U-Net architecture (Ronneberger et al., 2015; Ho et al., 2020). The other was to use the hybrid loss as proposed in (Nichol & Dhariwal, 2021). We also experimented with various loss functions based on evaluation of the generated policies, but they did not lead to high performance."}, {"title": "Evaluation and Results", "content": "The evaluation results of the final trained model can be found in Table 2. The evaluation aims to show the model generates meaningful policies in parameter space. For each baseline, we took average performance across all 64 tasks, with 10 runs each on random seeds. \u201cDiffusion Sample Policy\u201d refers to a single sample from the diffusion model conditioned on the task description. We primarily compare to baselines as a means to ensure the model is not learning trivial local minima. If it is not, then we expect a single sampled policy to significantly outperform the baselines. \"Random Policy\" refers to an agent that takes a random action in each state. The \"Mean,\" \"Median,\" and \"Mode\" baselines refer to taking those operations on all of the parameters in the dataset for the specified task. The sample policy significantly outperforms all baselines indicating that the model is learning to generate meaningful and performant polices. A single sample, however, achieves slightly lower returns than the agents in the training set. To achieve a similar performance, we take a simple mixture approach where we sample n policies, and for each observation, take the most common output action. This is referred to as Mixture of Samples (MoS) in Table 2."}, {"title": "Limitations", "content": "Despite promising early results, there are key limitations with the evaluation regarding extrapolating the results to real-world robots. Though diffusion models and transformers have been shown to scale well with large amounts of robot data, we have not shown this scalability with policy parameter space learning. Similarly, we emphasize that the training data needed for DPP is different than for a generalist policies: DPP requires a dataset of trained policies (which could be gathered through simulation or a cross organization effort similar to the Droid dataset (Khazatsky et al., 2024)), rather than a demonstration dataset. However, for DPP to scale and generalize across tasks, it will likely need both a policy and a demonstration dataset. We have also only demonstrated results in an environment with a discrete action space. Although some generalist policies, such as (Ghosh et al.), have had issues with discrete action spaces, we believe a robot foundation model should be able to handle both discrete and continuous actions. These limitations warrant further investigation and to be addressed in future work."}, {"title": "Discussion", "content": "While generalist robot policies as robot behavior foundation models show clear successes, they do not maintain properties of locality and explainability that would be desired for a deployed system. To limit these concerns, we presented DPP, an alternative which may alleviate some of the outlined concerns. DPP generates smaller, standalone policies for each task; this approach means that those policies are not affected by a user teaching the robot other tasks or by unwanted updates to the general foundation model.\nEnabling policies to be stable and therefore more predictable is a key feature for human-usable robots. Human-robot interaction research has consistently shown that robot models need to be not just performant, but also predictable Lichtenth\u00e4ler & Kirsch (2016). A predictable robot system not only improves its interpretability, but also allows a user to gain a high degree of familiarity with those policies, and in turn use them to accomplish novel tasks. In this work, we embed this predictability and usability directly into the structure of the model without compromising its flexibility to learn from new data. With foundation models in their infancy, it is an ideal time to explore how these powerful generalized models can be made more usable.\nFuture work could explore other methods to make foundation models more stable and usable, especially by allowing the user to choose when and how a task policy should be updated. For example, a robot might come deployed with a suite of policies for very common tasks, with the capacity to learn new tasks from the user through human-in-the-loop learning (Ravichandar et al., 2020; Liu et al., 2023). Another approach is to use sim-to-real RL to train new policies when needed by the user. For example, Eureka (Ma et al., 2024) uses LLMs and an iterative training procedure to design reward functions for arbitrary tasks. This approach has similar benefits as DPP, but depends on having an accurate simulator and may not be responsive enough for users, as the robot needs to learn tasks from scratch when the user requests it. An advantage, however, is that since it uses a task-specific reward function, it may be more explainable.\nWe primarily focused on improving interpretability and modularity relative to generalist policies, but there are also other exciting directions for future research towards usable generalist policies. For example, work is needed on how to explain the behavior of a generalist robot policy. Explainable AI techniques for large models are constantly improving, but more work is needed to understand how techniques for explaining robot behavior apply to generalist robot policies: much prior work in explainable robotics and AI either assumes the robot was trained with RL (see (Milani et al., 2024) for a taxonomy of explainable RL) or requires semantic knowledge of its past interactions (Setchi et al., 2020). Some approaches, such as directly generating explanations for non-interpretable policies are easily applicable to generalist robot policies, while others, such as generating intrinsically interpretable policies, may not be."}, {"title": "Conclusion", "content": "In this work, we discussed key areas for improving the paradigm of generalist robot policies as robot behavior foundation models. To better empower end-users, these models need better modularity and independence between tasks, to support a user's ability to understand and leverage the system's dynamics, independent of weight changes in the foundation model. Towards this goal, we presented the Diffusion for Policy Parameters (DPP) approach to a robot behavior foundation. Our preliminary results show that this is a promising method for generating task-specific, standalone policies conditioned on a task specification. As the research community explores methods such as DPP, we can ensure the future of foundational robot-behavior models empower end-users with a high degree of understanding and control over the robot."}]}