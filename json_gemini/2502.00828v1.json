{"title": "Decision-informed Neural Networks with Large Language Model Integration for Portfolio Optimization", "authors": ["Yoontae Hwang", "Yaxuan Kong", "Stefan Zohren", "Yongjae Lee"], "abstract": "This paper addresses the critical disconnect between prediction and decision quality in portfolio optimization by integrating Large Language Models (LLMs) with decision-focused learning. We demonstrate both theoretically and empirically that minimizing the prediction error alone leads to suboptimal portfolio decisions. We aim to exploit the representational power of LLMs for investment decisions. An attention mechanism processes asset relationships, temporal dependencies, and macro variables, which are then directly integrated into a portfolio optimization layer. This enables the model to capture complex market dynamics and align predictions with the decision objectives. Extensive experiments on S&P100 and DOW30 datasets show that our model consistently outperforms state-of-the-art deep learning models. In addition, gradient-based analyses show that our model prioritizes the assets most crucial to decision making, thus mitigating the effects of prediction errors on portfolio performance. These findings underscore the value of integrating decision objectives into predictions for more robust and context-aware portfolio management.", "sections": [{"title": "1. Introduction", "content": "The estimation of parameters for portfolio optimization has long been recognized as one of the most challenging aspects of implementing modern portfolio theory (Michaud 1989, DeMiguel et al. 2009). While Markowitz's mean-variance framework (Markowitz 1952) provides an elegant theoretical foundation for portfolio selection, its practical implementation has been persistently undermined by estimation errors in input parameters. (Chopra and Ziemba 1993, Chung et al. 2022) demonstrate that small changes in estimated expected returns can lead to dramatic shifts in optimal, while (Ledoit and Wolf 2003, 2004) show that traditional sample covariance estimates become unreliable as the number of assets grows relative to the sample size.\nThese estimation challenges are exacerbated by a fundamental limitation in the conventional approach to portfolio optimization: the reliance on a sequential, two-stage process where parameters are first estimated from historical data, and these estimates are then used as inputs in the optimization problem. This methodology, while computationally convenient, creates a profound disconnect between prediction accuracy and decision quality. Recent evidence suggests that even substantial"}, {"title": "2. Related work", "content": "In this section, we review two key research streams relevant to our work: parameter estimation in portfolio optimization and deep learning applications in financial forecasting. The first stream examines traditional and robust estimation techniques, while the second explores how modern machine learning approaches have transformed financial prediction."}, {"title": "2.1. Parameter estimation in portfolio optimization", "content": "The foundation of modern portfolio theory rests upon accurate parameter estimation, particularly for expected returns and covariance matrices. Since Markowitz (1952) seminal work establishing the mean-variance optimization framework, researchers have grappled with the challenge of reliably estimating these crucial parameters from historical data Tan and Zohren (2020), Firoozye et al. (2023). This challenge has become central to the field of quantitative finance, as the performance of optimal portfolios heavily depends on the quality of these estimates.\nThe extensive reliance on historical financial data for parameter estimation has proven instrumental in advancing modern financial theory and practice. Historical data provides the empirical foundation for estimating critical parameters including expected returns, volatility, and covariance matrices essential inputs that drive portfolio optimization, risk management, and asset pricing models. This approach has led to breakthrough developments in financial modeling, most notably the Capital Asset Pricing Model (CAPM) (Sharpe 1964, Lintner 1975) and the Fama-French factor models (Fama and French 1993, 2015). This mean-variance optimization framework, however, revealed significant challenges in parameter estimation. The sensitivity of portfolio optimization to parameter estimates was first systematically documented by Michaud (1989), who characterized mean-variance optimization as \"error maximization.\" This insight was further developed by Best and Grauer (1991), who demonstrated the hypersensitivity of optimal portfolio weights to changes in mean estimates. (Chopra and Ziemba 1993, Chung et al. 2022) provided crucial quantitative"}, {"title": "2.2.\nTime-series forecasting with deep learning", "content": "The application of deep learning to financial time-series forecasting represents a significant advancement in addressing the parameter estimation challenges. While traditional approaches to parameter estimation often struggle with the complex, non-linear relationships inherent in financial markets, deep learning models have demonstrated remarkable capability in capturing these dynamics. However, as discussed in our examination of parameter estimation challenges, improved predictive accuracy does not necessarily translate to better portfolio decisions.\nRecent advances in deep learning architectures, particularly those based on attention mechanisms, have revolutionized time-series forecasting across various domains. The Transformer architecture and its variants have achieved state-of-the-art performance in various domains through innovations in handling long sequences (Zhou et al. 2021), capturing interactions between different time scales (Zhou et al. 2022), and modeling temporal patterns (Wu et al. 2023). Unlike traditional autoregressive predictors such as LSTM, transformer-based models employ generative-style"}, {"title": "3. Decision-informed neural networks with large language model integration for portfolio optimization (DINN)", "content": "We introduce a Decision-Informed Neural Network (DINN) that unifies forecasting and portfolio selection within a single learning framework. Unlike traditional methods that treat return prediction and portfolio optimization as separate tasks, DINN merges them via three key components. First, an input embedding process captures market dynamics and semantic relationships using LLM-based representations, ensuring that both numeric time series and textual context inform the model. Second, a cross-attention mechanism fuses these diverse inputs into coherent return forecasts, allowing interactions between multiple data modalities. Finally, a differentiable optimization layer uses these forecasts to produce optimal portfolio weights, enabling the model to refine both"}, {"title": "3.1. Preliminaries", "content": "Throughout this paper, let $N\\in \\mathbb{N}$ denote the number of risky assets in the portfolio. We consider a discrete-time financial market with a finite horizon $T \\in \\mathbb{N}$. Let $\\{r_t\\}_{t=1}^{T}$ be a sequence of asset excess returns, where $r_t \\in \\mathbb{R}^N$ and $r_{t,i}$ is the excess return of asset $i$ at time $t$. Let $r_{t,i} = \\frac{P_{t,i}-P_{t-1,i}}{P_{t-1,i}} - r_f$, where $P_{t,i}$ denotes the price of asset $i$ at time $t$ and $r_f$ is the risk-free rate. We assume $r_f$ is a known,\ntime-invariant constant. Define $w_t \\in \\mathbb{R}^N$ as the portfolio weight vector at time $t$. Let $W \\subset \\mathbb{R}^N$ be the feasible set of portfolio weights, for example $W = \\{w \\in \\mathbb{R}^N : w_i \\geq 0, \\sum_{i=1}^N w_i = 1\\}$. For a given lookback length $L\\in \\mathbb{N}$, consider historical returns and macroeconomic variables over the period $\\{t - L, . . ., t - 1\\}$:\n$r'_{t-L:t-1} = (r_{t-L}, ..., r_{t-1}) \\in \\mathbb{R}^{L \\times N}$,\n$X_{t-L:t-1} = (X_{t-L},..., X_{t-1}) \\in \\mathbb{R}^{L \\times M}$\nwhere $x_t \\in \\mathbb{R}^M$ encapsulates $M$ macroeconomic features observed at time $t$.\nWe consider a forecasting model $f_{\\theta}$ parameterized by $\\theta$. This model, given historical data and macroeconomic features, produces predicted returns $\\hat{r}_{t+1:t+H} = (\\hat{r}_{t+1},\\hat{r}_{t+2},...,\\hat{r}_{t+H})$ but also of the corresponding predicted portfolio weights $\\hat{w}_{t+1:t+H} = (\\hat{w}_{t+1}, \\hat{w}_{t+2},...,\\hat{w}_{t+H})$ over a forecast horizon $H \\in \\mathbb{N}$. Formally, we have:\n$\\hat{r}_{t+1:t+H} = f_{\\theta}(r_{t-L:t-1}, X_{t-L:t-1})$\nwhere $\\hat{r}_{t+1:t+H} = (\\hat{r}_{t+1}, ..., \\hat{r}_{t+H}) \\in \\mathbb{R}^{H \\times N}$. The vector $\\hat{r}_{t+h} \\in \\mathbb{R}^N$ thus represents the predicted excess returns for the $N$ assets at time $t + h$."}, {"title": "3.2. Input embeddings", "content": "Our input embedding process is designed to systematically incorporate temporal patterns, asset interactions, and textual context before generating forecasts. First, we normalize time series data to stabilize training and ensure comparability across assets. Next, kernel-based trend-residual decompositions separate persistent market trends from shorter-term fluctuations, highlighting both low-frequency and high-frequency signals. Finally, LLM-enhanced semantic embeddings integrate sector-level yields and pairwise asset relationships into the model, thereby capturing broader economic and inter-asset context. These structured embeddings may provide a strong foundation for subsequent attention-based modeling and decision-focused optimization."}, {"title": "3.2.1.\nTime-series normalization and decomposition.", "content": "We begin by transforming the raw input data into a structured representation well-suited for accurate forecasting and decision-focused optimization. Let $\\{r_t\\}_{t=1}^{T}$ be a sequence of excess returns for $N$ assets, where $r_t \\in \\mathbb{R}^N$. To ensure numerical stability and promote effective learning, we first normalize the historical returns. For each asset $i \\in \\{1, . . ., N\\}$ over a lookback window of length $L$, define the sample mean $\\mu_i$, and standard deviation $\\sigma_i$ as $\\mu_{t,i} = \\frac{1}{L}\\sum_{k=t-L}^{t-1} r_{k,i}$, $\\sigma_{t,i} = \\sqrt{\\frac{1}{L} \\sum_{k=t-L}^{t-1}(r_{k,i} - \\mu_{t,i})^2 + \\epsilon}$, respectively. Here $\\epsilon > 0$ is a small constant to avoid division by zero. And then, we can get the normalized returns (i.e., $\\Tilde{r}_{t,i} = \\frac{r_{t,i} - \\mu_{t,i}}{\\sigma_{t,i}}$). This normalization step(Kim et al. 2021b) ensures that differences among assets are measured relative to their historical scales, improving training stability and preventing certain assets from dominating the optimization process solely due to larger raw magnitudes.\nNext, we apply a multi-scale decomposition (Wu et al. 2021, Zhou et al. 2022) to the normalized returns to capture both persistent trends and transient fluctuations. Let $\\{k_j\\}_{j=1}^{J}$ be a collection of kernel sizes. For each $j$, we can define as $T^{(j)}_{t,i} = \\frac{1}{k_j}\\sum_{l=t-k_j}^{t-1} \\Tilde{r}^{(j)}_{l,i}$, $\\rho_{t,i} = \\Tilde{r}_{t,i} - T^{(j)}_{t,i}$. By aggregating across all scales, we can obtain\n$T_{t,i}:=\\frac{1}{J}\\sum_{j=1}^{J} T^{(j)}_{t,i}$\n$\\rho_{t,i}:=\\frac{1}{J}\\sum_{j=1}^{J} \\rho^{(j)}_{t,i}$\nThis approach allows the model to focus separately on the long-term market trend (captured by"}, {"title": "3.2.2. LLM-enhanced semantic embeddings.", "content": "While normalized and decomposed returns offer valuable insights into market structures, their representational capacity can be significantly enriched by incorporating Large Language Model (LLM)-based embeddings (Zhou et al. 2023b, Jin et al. 2024, Cao et al. 2024). \u03a4\u03bf achieve this, we integrate two distinct types of LLM-based embeddings: one capturing inter-asset relationships, and another encoding macroeconomic information.\nInter-asset embeddings: Consider a set of assets indexed by $i \\in \\{1, ..., N\\}$, each mapped to a sector $S(i)$ drawn from a finite set $S$. Using large language model (LLM)-based textual descriptions, we establish a mapping from each asset to its corresponding sector. Once this mapping is determined, we construct sector-level returns over a historical lookback period to complement asset-level historical returns.\nMore specifically, let $L \\in \\mathbb{N}$ be the lookback length, and consider the historical returns $\\{r_{t,i}\\}_{t=t-L}$ for each asset $i$. The sector-level yield at time $u \\in \\{t - L, . . ., t - 1\\}$ for a sector $s \\in S$ is defined as:\n$r_{u,s}^{sector} = \\frac{1}{\\|A(s)\\|} \\sum_{i\\in A(s)} r_{u,i}$\nwhere $A(s) = \\{i \\in \\{1,...,N\\} : S(i) = s\\}$. This produces, for each sector, a time series $\\{r_{u,s}^{sector}\\}_{u=t-L}$ that may reveal common patterns, systemic shifts, or sectoral performance trends during the lookback window.\nNext, to capture direct relationships among individual assets, for each pair $(i,j)$ with $i \\neq j$, we measure relative historical performance by counting how frequently one asset outperforms the other:\nCount^{stock}(i, j) = |\\{ u \\in [t \u2013 L, t \u2212 1]: r_{u,i} > r_{u,j}\\}\\$.\nSimilarly, we define a sector-level outperformance count to capture how often the sector of asset $i$ outperforms the sector of asset $j$:\nCount^{sector}(i, j) = |\\{ u \\in [t - L, t \u2212 1] : r_{u,S(i)}^{sector} > r_{u,S(j)}^{sector}\\}\\$.\nTo encode these pairwise relationships into a form suitable for LLM-based embeddings, we generate textual prompts that synthesize the computed statistics. Let $p_{i,j}$ be a prompt-generating function that takes as input the historical returns $\\{r_{t-L:t-1,i}, r_{t-L:t-1,j}\\}$, sector assignments $(S(i), S(j))$, sector-level yields $\\{r^{sector}_{u,S(i)}\\}_{u=t-L}^{t-1}$,$\\ r^{sector}_{u,S(j)}\\}_{u=t-L}^{t-1}$, and the pairwise performance statistics Count^{stock}(i, j) and Count^{sector}(i, j)$. This function produces a textual prompt describing the relative performance and sectoral context of the two assets.\nCollecting such prompts for all pairs $(i, j)$ with $i \\neq j$ yields:\n$\\mathcal{P}_{Stocks} = \\{p_{i,j} ([Count^{stock}(i, j), Count^{sector}(i, j)]) : i \\neq j\\}$,\nwhere $[a, b]$ denotes the concatenation of inputs into a single composite prompt for $p_{i,j}$. The prompt-generation function $p_{i,j}$ is a function that maps $T$ to the space of text descriptions.\nEach prompt in $\\mathcal{P}_{Stocks}$ is mapped to a token-level representation via the LLM embedding function $g(\\cdot)$. We then stack or concatenate these token embeddings across all prompts, yielding\n$E_{Stocks} = \\{g_{\\theta}(p)\\}_{p \\in \\mathcal{P}_{Stocks}} \\in \\mathbb{R}^{M_{stocks} \\times d_{LLM}},$"}, {"title": "3.3. Decision-informed neural network", "content": "In this section, we present our neural network that integrates multi-modal information for portfolio optimization. The architecture consists of four key components: (1) a cross-attention mechanism that fuses temporal patterns with LLM-derived semantic embeddings, (2) a pretrained large language model for return forecasts, (3) a differentiable optimization layer that converts predictions into portfolio weights, and (4) a hybrid training objective combining forecasting and decision-focused losses."}, {"title": "3.3.1.\nEfficient Dual-Modality Integration via Prob-Sparse Cross-Attention.", "content": "Given the decomposed normalized returns and LLM-based embeddings, we employ a prob-sparse cross-attention mechanism (Zhou et al. 2021) to integrate temporal and semantic information efficiently. In a naive full attention framework (Waswani et al. 2017), the computational cost scales proportionally to the product of query and key lengths in the simplest case), which becomes prohibitively large for long sequences of textual embeddings or when N and M grow significantly. By contrast, prob-sparse attention uses a sampling-based approximation that retains only the most relevant keys for each query. Specifically, for each query, it selects a subset of key positions whose dot-products are likely to dominate the attention distribution, thereby reducing the effective number of terms in the softmax normalization. This approach substantially lowers the computational complexity under common parameter choices), while preserving the representational capacity and accuracy of attention-based models.\nWe employ prob-sparse attention for two main reasons. First, it alleviates computational and memory burdens that arise from large collections of textual or macroeconomic embeddings, ensuring scalability for real-world financial datasets with many assets and extended textual descriptions. Second, this approximation focuses model capacity on salient interactions, often leading to improved efficiency during training without sacrificing forecast fidelity."}, {"title": "3.3.2. Pretrained large language model for prediction.", "content": "With the integrated representations from the cross-attention mechanism, we leverage a pretrained large language model to generate return forecasts. Let $g_{\\theta} : \\mathbb{R}^{N \\times d_{LLM}} \\rightarrow \\mathbb{R}^{N \\times d_{LLM}}$ be the pretrained LLM with frozen parameters . It serves as a fixed contextual encoder that maps integrated embeddings into a more semantically enriched space. Given $C_{market}, C_{stock} \\in \\mathbb{R}^{N \\times d_{LLM}}$, we process them through the LLM:\n$Z_{market} = g_{\\theta}(C_{market}), \\quad Z_{stock} = g_{\\theta}(C_{stock})$\nwhere $Z_{market}, Z_{stock} \\in \\mathbb{R}^{N \\times d_{LLM}}$. The LLM refines these embeddings by capturing higher-order dependencies among assets through its attention mechanisms while preserving the semantic information encoded in the original representations.\nTo combine the market-level and stock-specific information, we employ an additive fusion $Z = Z_{market} + Z_{stock} \\in \\mathbb{R}^{N \\times d_{LLM}}$, where the addition is performed element-wise. This operation assumes both embeddings reside in a common semantic space and that their contributions to the final representation are complementary.\nTo generate normalized return forecasts over the horizon H, we project the fused embeddings through a learned linear transformation $\\hat{r}_{t+1:t+H} = (ZW_F)^T$, where $W_F \\in \\mathbb{R}^{d_{LLM} \\times H}$ is a trainable weight matrix and $\\hat{r}_{t+1:t+H} \\in \\mathbb{R}^{H \\times N}$. To recover the returns in their original scale, we apply the inverse of the normalization transformation introduced in Section 3.2.1. For each asset i and horizon h, we denormalize the predictions using the historical statistics:\n$\\hat{r}_{t+h,i} = \\Tilde{r}_{t+h,i}\\sigma_{t,i} + \\mu_{t,i}$\nwhere $\\mu_{t,i}$ and $\\sigma_{t,i}$ are the sample mean and standard deviation computed over the lookback window $[t - L, t -1]$ as defined previously.\nThe final return predictions can be organized into a matrix $\\hat{r}_{t+1:t+H} = [\\hat{r}_{t+1}, \\hat{r}_{t+2},...,\\hat{r}_{t+H}] \\in \\mathbb{R}^{H \\times N}$, where each $\\hat{r}_{t+h} \\in \\mathbb{R}^{N}$ represents the predicted returns across all assets at time $t + h$.\nWhile employing the latest pretrained LLMs can significantly boost predictive performance, it also raises a critical concern of data leakage in empirical evaluations. Because some LLMs (e.g., GPT-40 (Achiam et al. 2023), LLAMA (Dubey et al. 2024)) were trained on vast text corpora\u2014 potentially including financial data, news reports, or research materials overlapping with one's test set there is a nontrivial risk that information from the true \u201cfuture\" may already reside within the LLM's parameters. Consequently, evaluating forecasts on a test period that the LLM might have indirectly \"seen\" during pretraining can yield overly optimistic results. Therefore, we used the GPT-2, which is a relatively old model with sufficient representation power, as the default LLM model to avoid the issue of data leakage."}, {"title": "3.3.3. Optimization layer.", "content": "The optimization layer converts predicted returns into optimal portfolio weights by solving a convex optimization problem that balances expected returns and portfolio risk. Given predicted returns $\\hat{r}_{t+1:t+H} \\in \\mathbb{R}^{H \\times N}$ and historical returns $r_{t-K:t-1} \\in \\mathbb{R}^{K \\times N}$, we estimate covariance matrices $\\hat{\\Sigma}_{t+h}$ by combining historical and predicted return as $\\hat{\\Sigma}_{t+h} = Cov (r_{t-K:t-1} \\cup \\hat{r}_{t+1:t+h})$. In this study, we use the past three months of historical returns for stable covariance estimation. Assuming $\\hat{\\Sigma}_{t+h}$ is positive definite, we perform a Cholesky decomposition $\\hat{\\Sigma}_{t+h} = L_{t+h}L_{t+h}^T$. Let $\\lambda > 0$ be the risk-aversion parameter. For each time step $t + h$, we solve:\n$\\min_{w_{t+h}} \\lambda w_{t+h}^T \\hat{\\Sigma}_{t+h} w_{t+h} - \\mu_{t+h}^T w_{t+h}$\ns.t. $\\|L_{t+h}w_{t+h}\\|^2 \\leq \\sigma_{t+h}$,\n$\\sigma_{t+h} \\geq 0$,\n$\\sum_{i=1}^N w_{t+h,i} = 1$,\n$0 \\leq w_{t+h,i} \\leq 1 \\quad \\forall i \\in \\{1, ...,N\\}.$\nHere, $\\sigma_{t+h}$ represents the portfolio volatility, and the full-investment, long-only constraints ensure that $\\sum_i w_{t+h,i} = 1$ with $w_{t+h,i} \\geq 0$. This second-order cone formulation is equivalent to solving a mean-variance trade-off problem, where $\\lambda$ modulates the level of risk-aversion, and $\\hat{\\mu}_{t+h}$ encodes the expected return estimates. Solving this second-order cone optimization problem for each h yields:\n$\\hat{w}_{t+1:t+H} = (\\hat{w}_{t+1}, \\hat{w}_{t+2},..., \\hat{w}_{t+H})$"}, {"title": "3.3.4. Training.", "content": "Training aims to align the model's parameters so that the predicted returns and the resulting decision-making process closely approximate their true counterparts. To achieve this, we combine a forecasting loss and a decision-focused loss into a single objective function. Let $\\hat{r}_{t:t+H}$ be the predicted returns over the horizon H, and $r_{t:t+H}$ be the corresponding actual returns. The first loss term, which we denote as the forecasting loss, is the mean squared error (MSE) computed over the forecast horizon:\n$\\mathcal{L}_{MSE} = \\frac{1}{NH} \\sum_{h=1}^H \\|\\hat{r}_{t+h} - r_{t+h}\\|^2$\nThe decision-focused loss measures how prediction errors degrade portfolio quality. Consider optimal weights $w^*_{t+1:t+h}$ obtained from actual returns and $\\hat{w}_{t+1:t+h}$ from predicted returns. With $L^*_{t+h}$ the Cholesky factor of the actual covariance $\\Sigma^*_{t+h}$, define:\n$J_{t+h} = \\lambda \\| L^*_{t+h}w^*_{t+h} \\|^2 - \\mu^T_{t+h} w^*_{t+h}$\n$\\hat{J}_{t+h} = \\lambda \\| L^*_{t+h}\\hat{w}_{t+h} \\|^2 - \\mu^T_{t+h} \\hat{w}_{t+h}$\nIntuitively, these performance measures quantify how inaccuracies in predicted returns translate into suboptimal portfolio decisions. Unlike approaches such as those in (Costa and Iyengar 2023), which optimize for metrics like the Sharpe ratio, the proposed decision-focused loss directly measures the regret incurred by substituting predicted returns for actual ones. Consequently, $\\Delta J_{t+h} = \\hat{J}_{t+h} - J_{t+h}$ reflects the additional cost induced by prediction errors on the portfolio's true risk-return profile. Then the decision-focused loss is the average absolute regret as here:\n$\\mathcal{L}_{Decision} = \\frac{1}{NH} \\sum_{h=1}^H \\|\\Delta J_{t+h}\\|$\nwhere $\\Delta J_{t+h}$ is the discrepancy between the performance of the predicted and true portfolios."}, {"title": "4. Experiment", "content": "We now present the experimental results that comprehensively demonstrate the performance of DINN on real-world benchmark datasets. To facilitate transparency and reproducibility, the code and configuration details are available at Anonymous Github."}, {"title": "4.1.\nImplementation details", "content": "In this section, we describe the datasets, evaluation metrics, baseline models, and hyperparameter settings used in our empirical study."}, {"title": "4.1.1.\nDataset description.", "content": "This study analyzes a comprehensive dataset spanning January 2010 to December 2023, encompassing both the post-financial crisis recovery and the COVID-19 pandemic period. Our primary data consists of equity returns from two major indices: the DOW 30 and a market-cap-weighted subset of 50 constituents from the S&P 100. To address potential survivorship bias, we include only companies that maintained consistent index membership throughout the study period. The financial data, obtained from WRDS, is complemented by five macroeconomic indicators from FRED, selected based on their documented predictive power in asset pricing: weekly initial jobless claims (ICSA), consumer sentiment (UMCSENT), new home sales (HSN1F), unemployment rate (UNRATE), and high-yield bond spread (HYBS). These variables may capture different aspects of economic conditions that influence asset returns through both systematic risk channels and behavioral mechanisms."}, {"title": "4.1.2. Evaluation Metrics.", "content": "We evaluate each model using eight key metrics designed to capture both return characteristics and various dimensions of risk. These include:\n(i) Annualized Return (Ret): Reflects the average annual growth of the portfolio without subtracting any risk-free component.\n(ii) Annualized Standard Deviation (Std): Gauges the volatility of returns, serving as a basic measure of risk.\n(iii) Sharpe Ratio (SR): Examines excess returns (portfolio return minus the risk-free rate) per unit of total volatility.\n(iv) Sortino Ratio (SOR): Focuses on downside volatility, isolating harmful fluctuations from benign ones.\n(v) Maximum Drawdown (MDD): Captures the largest observed loss from a prior portfolio high, providing a measure of potential capital erosion.\n(vi) Value at Risk (VaR) at 95% (monthly): Indicates the worst likely loss over a specific time horizon under normal market conditions.\n(vii) Return Over VaR (RoV): Scales the portfolio's excess monthly returns relative to VaR, highlighting returns per tail-risk unit.\n(viii) Terminal Wealth (Wealth): Reflects the final cumulative portfolio value, integrating the impact of both returns and compounding."}, {"title": "4.1.3.\nBaseline Models and Hyperparameter Selection.", "content": "We compare DINN against several state-of-the-art deep learning architectures tailored to financial time series, including both Transformer-based and large language model (LLM)-based methods:\n\u2022 Transformer-based methods: iTransformer (Liu et al. 2024), PatchTST (Nie et al. 2023), TimesNet (Wu et al. 2023), and Crossformer (Zhang and Yan 2023).\n\u2022 LLM-based methods: PAttn (Tan et al. 2024), Chronos (Ansari et al. 2024), and GPT4TS (Zhou et al. 2023a).\nAll baseline models are implemented using their original architectures and recommended hyperparameters, with minor refinements to accommodate the specifics of our financial data. We provide the detailed hyperparameter settings for DINN in Appendix A.3., ensuring reproducibility and clarity."}, {"title": "4.2. Can DINN exceed standard deep learning models for portfolio optimization?", "content": "Standard deep learning approaches often focus on minimizing forecasting error without directly addressing the inherent fragility of portfolio selection when faced with small parameter estimation errors. Accordingly, even substantial gains in predictive accuracy may not translate into robust improvements in actual investment outcomes. By contrast, DINN integrates portfolio optimization as a learnable module, aligning model parameters not merely to predict returns accurately but also to optimize the final portfolio decision."}, {"title": "4.3. Why does prediction based loss function misalign with investment objectives?", "content": "A purely prediction-based loss function (e.g., minimizing mean-squared error) presumes that accurate forecasts of expected returns alone suffice for optimal investment decisions. In reality, portfolio"}, {"title": "4.4. Can DINN's attention mechanism enhance portfolio efficiency across varying market conditions?", "content": "A central question in applying deep learning models to portfolio management is whether these models can systematically identify and emphasize assets that represent the market well while delivering favorable risk-adjusted returns under varying conditions. To explore this, we analyze the performance of DINN under four distinct macroeconomic regimes: the COVID-19 pandemic (March to June 2020), periods of elevated weekly initial jobless claims (ICSA), surges in new home sales (HSN1), and extremely low consumer sentiment (UMCS). During each regime, we evaluate four"}, {"title": "4.5. How do we interpret $\\frac{\\partial \\hat{w}_{t+h}}{\\partial \\hat{\\mu}_{t+h}}$ and $\\frac{\\partial \\hat{w}_{t+h}}{\\partial L_{t+h}}$ within DINN's portfolio decisions?", "content": "Having established in Section 4.4 that DINN's attention mechanism successfully isolates stocks of high importance, we now investigate whether such \u201cimportance\" translates into an unintuitive performance gain in predictive accuracy. Specifically, we focus on two gradient-based sensitivities: (i) $\\frac{\\partial \\hat{w}_{t+h}}{\\partial \\hat{\\mu}_{t+h}}$ from Theorem 1, measuring how small changes in predicted returns $\\hat{\\mu}_{t+h}$ affect the model's optimal weights $\\hat{w}_{t+h}$, and (ii) $\\frac{\\partial \\hat{w}_{t+h}}{\\partial L_{t+h}}$ from Theorem 2, measuring how the Cholesky factor $L_{t+h}$ (and thus the predicted covariance $\\hat{\\Sigma}_{t+h}$) impacts $\\hat{w}_{t+h}$. In principle, one might expect that assets whose weights are highly sensitive to errors in $\\hat{\\mu}_{t+h}$ or $\\hat{\\Sigma}_{t+h}$ (i.e., large gradients) would be more challenging to forecast, thereby yielding higher mean squared error (MSE). However, our findings contradict this conventional wisdom. When using only prediction loss, large gradients typically indicate a need for more model updates or suggest difficult-to-predict behavior. However, when incorporating decision-focused loss, we observe that these high-sensitivity assets actually show lower MSE. This occurs because DINN allocates greater learning capacity to stocks where prediction errors would result in higher decision-related costs. As a result, this improves predictive accuracy rather than increasing errors."}, {"title": "5. Conclusion", "content": "This paper addresses a longstanding challenge in quantitative finance: bridging the gap between more accurate forecasts of financial variables and truly optimal portfolio decisions. While improved prediction accuracy is often cited as the path to superior investment returns, our empirical and theoretical findings demonstrate that purely predictive approaches can fail to yield the best portfolio outcomes. Drawing on recent developments in decision-focused learning, we proposed the DINN (Decision-Informed Neural Network) framework, which not only advances the state of the art in financial forecasting by incorporating large language models (LLMs) but also directly integrates a portfolio optimization layer into the end-to-end training process.\nFrom an empirical standpoint, the experiments conducted on two representative equity datasets (S&P 100 and DOW 30) suggest three key findings. First, DINN delivers systematically stronger performance across a broad set of metrics\u2014including annualized return, Sharpe ratio, and terminal wealth\u2014when compared to standard deep learning baselines, such as Transformer variants and other LLM-based architectures that rely solely on traditional prediction losses. Second, the inclusion of a prob-sparse attention mechanism may helps the model identify and emphasize a smaller subset of assets critical to replicating market dynamics under a variety of macroeconomic conditions. This mechanism not only focuses the model on economically significant information but also yields portfolios with lower drawdowns and higher risk-adjusted performance during stress regimes (e.g., the COVID-19 crisis and spikes in jobless claims). Third, the gradient-based sensitivity analyses provide a theoretical framework through which to interpret DINN's asset allocations: high-sensitivity assets, which would inflict larger \"regret\" if incorrectly predicted, exhibit lower mean-squared errors than less-sensitive assets. This finding underscores that DINN \"learns what matters\" by adjusting its representational power to minimize precisely those errors most detrimental to the ultimate"}, {"title": "Appendix A.1.: Detailed Proofs and Derivations", "content": "Proof of Theorem 1.\nProof: Consider the problem Equation (29). Introducing Lagrange multipliers $\\eta$ and $\\gamma$ for the risk and budget constraints", "here": ""}]}