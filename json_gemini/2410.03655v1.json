{"title": "GEOMETRIC REPRESENTATION CONDITION IMPROVES EQUIVARIANT MOLECULE GENERATION", "authors": ["Zian Li", "Cai Zhou", "Xiyuan Wang", "Xingang Peng", "Muhan Zhang"], "abstract": "Recent advancements in molecular generative models have demonstrated substantial potential in accelerating scientific discovery, particularly in drug design. However, these models often face challenges in generating high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to enhance the performance of molecular generative models by integrating geometric representation conditions. We decompose the molecule generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared to directly generating a molecule, the relatively easy-to-generate representation in the first-stage guides the second-stage generation to reach a high-quality molecule in a more goal-oriented and much faster way. Leveraging EDM as the base generator, we observe significant quality improvements in unconditional molecule generation on the widely-used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 31% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations over conditioning on individual property values as in previous approaches. Furthermore, we show that, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while maintaining superior generation quality than that achieved with 1,000 steps, thereby significantly accelerating the generation process.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have seen rapid development in generative modeling techniques for molecule generation (Garcia Satorras et al., 2021; Hoogeboom et al., 2022; Luo & Ji, 2022; Wu et al., 2022; Xu et al., 2023; Morehead & Cheng, 2024), which have demonstrated great promise in accelerating scientific discoveries such as drug design (Graves et al., 2020). By modeling molecules as point clouds of chemical elements embedded in Euclidean space and employing equivariant models as backbone architectures, such as EGNN (Satorras et al., 2021), these approaches can ensure the O(3)- (or SO(3)-) invariance of the modeled molecule probability and have shown significant promise in both unconditional and conditional molecule generations.\nDespite the advances, precisely modeling the molecular distribution q(M) still remains a challenge, with current models often falling short of satisfactory results. This is especially true in the more practical scenarios where the goal is to capture the conditional distribution q(M|c) for conditional generation, with c representing a desired property such as the HOMO-LUMO gap. In such cases, recent models still produce molecules with property errors significantly larger than the data lower bound (Hoogeboom et al., 2022; Xu et al., 2023). Such challenge arises partly because molecules are naturally supported on a lower-dimensional manifold (Mislow, 2012; De Bortoli, 2022; You et al.,"}, {"title": "2 RELATED WORKS", "content": "Molecular Generative Models Early work has primarily focused on modeling molecules as 2D graphs (composed of atom types, connection, and edge types), utilizing 2D graph generative models to learn the graph distribution (Vignac et al., 2022; Jang et al., 2023; Jo et al., 2023; Luo et al., 2023; Zhou et al., 2024). However, since molecules inherently exist in 3D space, where physical laws govern their behavior and geometry provides critical information related to key properties, recent research has increasingly focused on utilizing 3D generative models to directly learn the geometric distribution by modeling molecules as point clouds of chemical elements. Notable early autoregressive models include G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022). More recently, diffusion models have demonstrated effectiveness in this domain, as evidenced by models like EDM (Hoogeboom et al., 2022) and subsequent advancements (Xu et al., 2023; Wu et al., 2022; Morehead & Cheng, 2024) that enhance EDM with latent space, prior information and more powerful backbones respectively. Furthermore, recent advances in flow methods (Lipman et al., 2022; Liu et al., 2022b) have inspired the development of geometric, equivariant flow methods including EquiFM (Song et al., 2024b) and GOAT (Hong et al., 2024), which can provide much faster molecule generation speed. Beyond these, there are also methods jointly model 2D and 3D information (Vignac et al., 2023; You et al., 2023; Irwin et al., 2024) (also called 3D graph (You et al., 2023)), where a representative method is MiDi (Vignac et al., 2023) which uses a diffusion framework to jointly diffuse and denoise atom type, bond type, formal charges and coordinates.\nLatent Generative Models At a high level, our framework can also be viewed as a latent generative model, where data distributions are learned in a latent space (our stage 1) and decoded back through some decoder (our stage 2). Most prior works in this domain either focus on regular data forms with fixed positions and sizes (Van Den Oord et al., 2017; Razavi et al., 2019; Dai & Wipf, 2019; Aneja et al., 2021; Rombach et al., 2022; Li et al., 2023), or on data without Euclidean symmetry and require explicit modeling (Wang et al., 2024). Molecular data, however, presents unique challenges in both aspects. One of the key issues in this context is how to define the latent space-defining it as \u201clatent coordinates and features\" as in GeoLDM (Xu et al., 2023) still results in a geometrically structured and thus complex space, while defining it on representations as we do introduces the challenge of effectively \u201cdecoding\u201d a global, non-symmetric embedding back into geometric objects. LGD (Zhou et al., 2024) trains a diffusion model on a unified Euclidean latent space obtained by jointly training a powerful encoder and a simple decoder, and performs both generation and prediction tasks focusing on 2D graphs. LDM-3DG (You et al., 2023) also adopts representation latent space but employs a cascaded (2D+3D) auto-encoder (AE) framework, where the decoder is designed (or trained) to be deterministic, rendering poor performance on the 3D part as evidenced in our experiments. In contrast, we model the decoder as a powerful generative model, focusing solely on geometric learning while demonstrating superior effectiveness. We leave the related works of pre-trained geometric encoders in Appendix A."}, {"title": "3 METHODS", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": "In this work, we represent molecules as point clouds of chemical elements in 3D space, denoted by M = (x, h), where $x = (x_1, ..., x_N) \\in \\mathbb{R}^{N \\times 3}$ represents the atomic coordinates of N atoms, and"}, {"title": "3.2 GEORCG: GEOMETRIC-REPRESENTATION-CONDITIONED MOLECULAR GENERATION", "content": "Geometric Representation Generator To improve the quality of the generated molecules, we propose first transforming the geometrically structured molecular distribution q(M) into a non-geometric representation distribution q(r) using a well-pre-trained geometric encoder E that maps each molecule M to its representation r. Learning the representation distribution q(r) is consider-ably easier, since representations do not exhibit any symmetry as in explicit molecular generative models (Xu et al., 2022; Hoogeboom et al., 2022). We thus leverage a simple yet effective MLP-based diffusion architecture proposed in (Li et al., 2023) for the representation generator $p_\\varphi(r)$, which adopts the DDIM architecture (Song et al., 2020) with MLP backbones and is optimized via the denoising score matching scheme (Vincent, 2011).\nOne deviation from previous practices (Li et al., 2023; Wang et al., 2024) is that we additionally con-dition the representation generator on the molecule's node number N by default\\u00b9. This is crucial for ensuring consistency between the size of the representation's underlying molecule and the size of the molecule it guides to generate. Moreover, molecules with different sizes often have distinct modes in structures and properties (Hoogeboom et al., 2022), which is reflected in their geometric repre-sentations learned by modern pre-trained geometric encoders (Zhou et al., 2023; Feng et al., 2023), as shown in Figure 2. From the figures, it is evident that by conditioning on N, the learning process for the representation generator becomes simpler and more effective, leading to the following loss function of our representation generator:\n$L_{rep} = \\mathbb{E}_{(r, N) \\in \\mathcal{D}_{train}, \\epsilon \\sim \\mathcal{N}(0, I), t} [||r - f_\\varphi(r_t, t, N)||^2]$,\nwhere $\\mathcal{D}_{train} = \\{(E(M), N(M))|M \\in \\mathcal{D}_{mol}}\\}$, with N(M) representing atom number of M and $\\mathcal{D}_{mol}$ denoting the molecule dataset. Here, $f_\\varphi$ is the MLP backbone (Li et al., 2023), and $r_t = \\sqrt{\\alpha_t}r + \\sqrt{1 - \\alpha_t} \\epsilon$ is the noisy representation computed with the predefined schedule $\\alpha_t \\in (0, 1]$."}, {"title": "Molecule Generator", "content": "Since the ultimate goal of our framework is to generate molecules from q(M), we decompose the molecular distribution as $q(M) = \\int q(M|r)q(r) dr$ to explicitly enable geometric-representation conditions. Consequently, a geometric-representation-conditioned molecular generator $p_\\theta(M|r)$ is required. In principle, we can use many modern molecule gen-erators (Hoogeboom et al., 2022; Xu et al., 2023; Morehead & Cheng, 2024), as these models can all take additional conditions. To illustrate the effectiveness of our approach, however, we choose a relatively simple model EDM (Hoogeboom et al., 2022) as the base generator. EDM is designed to ensure the O(3)-invariance, i.e., for any $R \\in O(3), p_\\theta(M) = p_\\theta(x, h)) = p_\\theta((xR^T, h))$. To accomodate EDM to representation conditions, we use the following training objective:\n$\\mathcal{L}_{mol} = \\mathbb{E}_{(M, r) \\sim \\mathcal{D}_{mol-rep}, t \\sim \\mathcal{U}(0, T), \\epsilon \\sim \\mathcal{N}(0, I)} [||\\epsilon - f_\\theta(M_t, t, r)||^2]$,\nwhere $\\mathcal{D}_{mol-rep} = \\{(M, E(M))|M \\in \\mathcal{D}_{mol}\\}$, and sampling from $\\mathcal{N}(0, I)$ entails drawing $\\epsilon = [\\epsilon^{(x)}, \\epsilon^{(h)}] \\in \\mathbb{N}(0, I)$, adjusting $\\epsilon^{(x)}$ by subtracting its geometric center to obtain $\\epsilon^{(x)}$, and setting $\\epsilon = [\\epsilon^{(x)}, \\epsilon^{(h)}]$. This ensures the zero center-of-mass property, as the distribution is defined on this subspace to ensure translation invariance (Hoogeboom et al., 2022). The noisy molecule is given by $M_t = \\alpha_t^{(M)} [x, h] + \\sigma_t^{(M)} \\epsilon$, with time-dependent schedules $\\alpha_t^{(M)}$ and $\\sigma_t^{(M)}$, while the diffusion backbone $f_\\theta$, which is instantiated with EGNN (Satorras et al., 2021), is conditioned on r.\nCombining the Two Generators Together The representation generator $p_\\varphi(r)$ and the molecule generator $p_\\theta(M|r)$ together model the molecular distribution $p_{\\varphi,\\theta}(M) = \\int p_\\theta(M|r)p_\\varphi(r) dr$, which approximates the data distribution $q(M) = \\int q(M|r)q(r) dr$ that we aim to capture. One notable advantage of the framework is that the decomposition enables parallel training of the two generators. The entire training and sampling procedure is summarized in Algorithm 1.\nThere are several key properties of GeoRCG that facilitate high-quality molecule generation. First, GeoRCG preserves all symmetry properties of the base molecule generator $p_\\theta (M)$:\nProposition 3.1. (Symmetry Preservation) Assume the original molecular generator $p_\\theta(M)$ is O(3)- or SO(3)-invariant. Then, the two-stage generator $p_{\\varphi,\\theta}(M)$ is also O(3)- or SO(3)-invariant.\nProof. This result follows directly from the definition. Specifically, $p_{\\varphi,\\theta}(M) = \\int p_\\theta(M|r) p_\\varphi(r) dr = \\int p_\\theta((xR^T, h)|r) p_\\varphi(r) dr = p_{\\varphi,\\theta}((xR^T, h))$ for any $R \\in 0(3)$ (or SO(3)). The second equality holds due to the symmetric property of $p_\\theta(M)$, which remains valid when additional non-symmetric conditions r are applied. \nMoreover, representation-conditioned diffusion models can achieve no higher overall total variation distance than traditional diffusion models, and can arguably yield better results, as the representation encodes key data information that may further reduce estimation error. We present the rigorous bound in Theorem 3.1, and provide the proof and detailed discussions in Appendix E.1. Notably, this is a general theoretical characterization that applies to prior experimental work (Li et al., 2023)."}, {"title": "Theorem 3.1.", "content": "Consider the random variable $x \\in \\mathbb{R}^d \\sim q(x)$, and assume that the second moment $m_x$ of x is bounded as $m_x^2 := \\mathbb{E}_{q(x)} [||x - \\bar{x}||^2] < \\infty$, where $\\bar{x} := \\mathbb{E}_{q(x)}[x]$. Further, assume that the score $\\nabla \\text{ln} q(x_t)$ is $L_x$-Lipschitz for all t, and that the score estimation error in the second-stage diffusion is bounded by $\\epsilon_{\\theta, \\varphi, cond}^2$ such that $\\mathbb{E}_{r \\sim p_\\varphi(r), x_t \\sim q_t(x_t|r)} [||s_\\theta(x_t, t, r) - \\nabla \\text{ln} q_t(x_t|r)||^2] \\leq \\epsilon_{\\theta, \\varphi, cond}^2$. Denote the step size as $h := T/N$, where T is the total diffusion time and N is the number of discretization steps, and assume that $h < 1/L_x$. Suppose we sample $x \\sim p_\\theta(x|r)$ from Gaussian noise, wherer $\\sim p_\\varphi(r)$, and denote the final distribution of x x as $P_{\\theta,\\varphi}(x)$. Define $p_T$, which is the endpoint of the reverse process starting from $q_{T\\vert \\varphi}$ instead of Gaussian noise. Here, $q_{T\\vert \\varphi}$ is the T-th step in the forward process starting from $q_{0\\vert \\varphi} := \\int_{\\varphi} q(x_0\\vert r)p_\\varphi(r) dr$, where $\\mathcal{A}$ is the normalization factor. Denote k-dim isotropic Gaussian distribution as $\\gamma_k$. Then the following holds,\n$\\text{TV}(p_{\\theta,\\varphi}(x), q(x)) = \\sqrt{\\text{KL}(q_{0\\vert \\varphi}||\\gamma_{d+3})} \\text{exp}(-T) + (L_x\\sqrt{(d+3)h} + L_x m_x h)\\sqrt{T}$ $(3)$\n$+\\dfrac{\\epsilon_{\\varphi \\theta, cond} \\sqrt{T}}{\\text{conditional score estimation error}} + \\dfrac{\\text{TV}(q_{0\\vert \\varphi}, q_0)}{\\text{representation generation error}} (4)$"}, {"title": "Balancing Quality and Diversity of Molecule Generation", "content": "In many scientific applications, re-searchers prioritize generating higher-quality molecules over more diverse ones. To address this preference, we introduce a feature that allows fine-grained control over the trade-off between diver-sity and quality in the sampling stage (thus without retraining). This is achieved by integrating two key techniques: low-temperature sampling (Ho & Salimans, 2022) (controlled via the temperature T) for the representation generator, and classifier-free guidance (Ho & Salimans, 2022) (controlled via the coefficient w) for the molecule generator. We provide more details about the two techniques in Appendix B. Combining the two features enables a flexible and explicit control, which we refer to as \"Balancing Controllablility\" and demonstrate its effectiveness in Section 4.2."}, {"title": "Handling Conditional Molecule Generation", "content": "The framework discussed thus far focuses on un-conditional molecule generation, where no specific property c (e.g., HOMO energy) is pre-specified. However, for molecule generation, a more practical and desired scenario is conditional (also called controllable) generation, where additional conditions c, such as HOMO-LUMO gap energy, is intro-duced, and our objective shifts to generating molecules from the distribution q(M|c). In GeoRCG, this conditional generation is naturally decomposed as $p_{\\theta,\\varphi}(M|c) = \\int p_\\theta(M|r) p_\\varphi(r|c) dr$, mean-ing that we first generate a \u201cproperty-meaningful\" molecular representation r, which is then inde-pendently used to condition the second-stage molecule generation. See Figure 3 for illustration. A key advantage of this modeling approach is that, when different properties (e.g., HOMO, LUMO, GAP energy) need to be captured, only the representation generator requires retraining under the new conditions. This retraining is highly efficient due to the lightweight nature of the represen-tation generator. Notably, GeoRCG demonstrates outstanding conditional generation performance, as shown in Section 4.3. Moreover, we theoretically demonstrate that, under mild assumptions, the representation generator can provably estimate the conditional distribution and generate representa-tions that lead to provable reward improvements toward the target, which subsequently benefits the second-stage generation. Further details are provided in Appendix E.2."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETUP", "content": "Datasets and Tasks As a method for 3D molecule generation, we evaluate GeoRCG on the widely used datasets QM9 (Ramakrishnan et al., 2014) and GEOM-DRUG (Gebauer et al., 2019; 2022). We focus on two tasks: unconditional molecule generation, where the goal is to sample from q(M), and conditional (or controllable) molecule generation, where a property c is given, and we aim to sample from q(M|c). To ensure fair comparisons, we follow the dataset split and configurations exactly as in Anderson et al. (2019); Hoogeboom et al. (2022); Xu et al. (2023). Without further clarification, we bold the highest scores and underline the second-highest one. Additionally, to illustrate the direct improvement over our base model, EDM (Hoogeboom et al., 2022), we display green numbers next to the score to indicate the average improvement, and red numbers to denote a"}, {"title": "4.2 UNCONDITIONAL MOLECULE GENERATION", "content": "We first evaluate the quality of unconditionally generated molecules from GeoRCG, with the com-monly adopted validity and stability metrics for assessing molecules' quality (Hoogeboom et al., 2022). See Appendix C for detailed descriptions of these metrics.\nMain Results We present the main results on the QM9 and DRUG datasets in Table 1. Below, we highlight the key findings: (i) Improvement over the base model (EDM): By leveraging geometric representations, GeoRCG significantly outperforms the base model, EDM, on both QM9 and DRUG datasets. Notably, on QM9, it increases stable molecules from 82% to 93.9% and validity from 91.9% to 97.4%, while also improving molecule uniqueness. (ii) Superior performance compared to advanced methods: GeoRCG also surpasses included advanced models on the QM9 dataset. On the DRUG dataset, it outperforms models such as EDM-Bridge and GOAT, and gets a high score in validity. Although it falls short of achieving the best performance, we attribute this to the quality of the encoder's representations, as we pre-trained it on the GEOM-DRUG dataset, which may lack diversity and sufficient size. Crucially, many structures in GEOM-DRUG lack the equilibrium conditions necessary for pre-training methods that enable effective learning of force fields (Zaidi et al., 2022; Feng et al., 2023). Additionally, GeoRCG adopts EDM (Hoogeboom et al., 2022) as the"}, {"title": "Balancing Controllability", "content": "We proceed to investigate the \u201cBalancing Controllablility\u201d feature of GeoRCG introduced in Section 3.2. To this end, we conducted a grid search by varying both w and T on QM9 dataset, as depicted in Figure 4. The results indicate a clear trend: increasing w and decreasing T improve validity and stability at the expense of uniqueness, allowing for fine-grained, flexible control over molecule generation. At its best, this approach achieves a molecule stability of 93.9% and a validity of 97.42%, approaching the dataset's upper bound, with a trade-off in lower uniqueness&validity of 86.82%."}, {"title": "Additional Comparison with 2D&3D methods", "content": "We further compare with recent 2D&3D meth-ods, including MiDi (Vignac et al., 2023) and LDM-3DG (You et al., 2023), which jointly learn and generate both 2D bond information and 3D geometry. However, these models rely on exter-nal tools like RDKit or OpenBabel (O'Boyle et al., 2011) for bond computation at the input stage, which enables them to implicitly leverage domain knowledge of such tools. In contrast, purely 3D methods like GeoRCG and those in our primary comparison utilize a coarse look-up table for bond estimation, which, while reflecting 3D learning, results in less accurate bond calculation, potentially biasing the comparison. Therefore, to provide a reference, we also report the performance of our models combined with the same external tools (e.g., OpenBabel) for precise bond calculations in our generated 3D conformations. Furthermore, as GeoRCG essentially captures 3D geometric dis-tributions, we place more emphasis on 3D metrics that directly evaluate 3D learning capabilities, including BondLengthW1 and BondAngleW1 proposed by Vignac et al. (2023) and detailed in Ap-pendix C. The results in Table 2 demonstrate that GeoRCG not only significantly outperforms MiDi and LDM-3DG on 3D metrics, highlighting the advantages of using a pure 3D model for learning 3D structures, but also further enhances EDM's performance, which has already shown considerable promise in 3D learning."}, {"title": "4.3 CONDITIONAL MOLECULE GENERATION", "content": "We now turn to a more challenging task: generating molecules with a specific property value c from q(M|c). We strictly follow the evaluation protocol outlined in (Hoogeboom et al., 2022). Speicifically, QM9 is split into two halves, and an EGNN classifier (Satorras et al., 2021) is trained on the first half for evaluating the generated molecules' property, while the generator is trained on the second half\u00b2. We focus on six properties: polarizability (\u03b1), orbital energies (\u0404HOMO, \u0190LUMO), their gap (\u0394\u03b5), dipole moment (\u00b5), and heat capacity (Cv).\nA potential concern is that for a given property value c, $p_\\theta(r\\vert c)$ may produce a representation corre-sponding to a molecule from the training dataset, allowing the molecule generator to simply recover its full conformation based on that representation. This could lead to small property errors but a lack of novelty. To address this, we conducted a thorough evaluation of the generated molecules across each property, finding that the novelty (the proportion of new molecules not present in the training"}, {"title": "4.4 FASTER GENERATION WITH FEWER STEPS", "content": "With geometric representation condition, it is reasonable to expect that fewer discretization steps of the reverse diffusion SDE (Song et al., 2021) would still yield competitive results. Therefore, we re-duce the number of diffusion steps and evaluate the model's performance. The results are presented in Table 4. As demonstrated, with the geometric representation condition, GeoRCG consistently outperforms other approaches across all step numbers. Notably, with approximately 100 steps, the performance of our method nearly converges to the optimal performance observed with 1000 steps, which already surpasses all other methods across all step numbers. This directly reflects the faster generation speed, as the time required for the first-stage representation generation is minimal and can be considered negligible."}, {"title": "5 CONCLUSIONS", "content": "In this work, we present GeoRCG, an effective framework for improving the generation quality of arbitrary molecule generators by incorporating geometric representation conditions. We use EDM (Hoogeboom et al., 2022) as the base molecule generator and demonstrate the effectiveness of our framework through extensive molecular generation experiments. Notably, in conditional gener-ation tasks, GeoRCG achieves a 31% performance boost compared to recent state-of-the-art models. Additionally, the representation guidance enables significantly faster sampling with 10x fewer dif-fusion steps while maintaining near-optimal performance. Beyond these empirical improvements, we provide theoretical characterizations of general representation-conditioned generative models, which address a key gap in the existing literature (Li et al., 2023).\nOne limitation of our framework is that its generation quality could depend on the representation quality. For instance, on the GEOM-DRUG dataset, where the encoder was less thoroughly pre-trained, the improvements were less pronounced, and GeoRCG did not surpass SOTA methods. Future work could focus on improving the effectiveness of pre-trained models or exploring enhanced representation regularization techniques. Furthermore, while we employ EDM (Hoogeboom et al., 2022) as the base molecule generator, our framework is general and can be applied to any molecular"}, {"title": "A MORE RELATED WORKS", "content": "Pre-training for Molecular Encoders Learning meaningful molecular representations is crucial for downstream tasks like molecular property prediction (Fang et al., 2022). The paradigm of pre-training on large-scale datasets followed by fine-tuning on smaller datasets has been shown to sig-nificantly enhance model performance in both vision and language domains (Kenton & Toutanova, 2019; Brown, 2020; Dosovitskiy, 2020), and building on this success, recent studies have proposed self-supervised pre-training methods for point-cloud-formatted molecules, aiming to achieve similar performance gains (Zhou et al., 2023; Feng et al., 2023; Liu et al., 2022a; Fang et al., 2022; Jiao et al., 2024; Ni et al., 2024). Typical proxy tasks involve masking and recovering atom types, bond lengths, and bond angles (Fang et al., 2022; Zhou et al., 2023). However, since molecules reside in continuous 3D space, a more effective approach is to introduce carefully designed noise to the coordinates and train the model to denoise it. Examples include isotropic Gaussian noise (Zaidi et al., 2022; Zhou et al., 2023), Riemann-Gaussian noise (Jiao et al., 2023), and complex hybrid noise (Ni et al., 2024; Feng et al., 2023; Jiao et al., 2024). Notably, Zaidi et al. (2022) demonstrated that denoising equilibrium structures corresponds to learning the force field, thereby producing rep-resentations that are both physically and chemically informative."}, {"title": "B ALGORITHMS", "content": "Parallel Training and Sequential Sampling We provide the high-level training and sampling algorithm for GeoRCG in Algorithm 1.\nRepresentation Perturbation Unlike typical conditional training scenarios, GeoRCG faces a unique challenge: the representations that condition the molecule generator during training may not always coincide with those generated by the representation generator during the sampling stage. This issue is particularly pronounced in molecular generation than image case (Li et al., 2023), where pre-trained encoders are typically not trained on that large datasets with advanced regulariza-tion techniques like MoCo v3 (Chen et al., 2021). Consequently, the molecule generator is suscep-tible to overfitting to the training representations, as evidenced by our preliminary experiments on QM9 molecule generation shown in Table 5.\nWe find that applying a simple way by perturbing the geometric representation with Gaussian noise $\\epsilon_{rep}$ (where $\\epsilon \\sim \\mathcal{N}(0, I)$ and $\\epsilon_{rep}$ is a small variance) during the molecule generator's training is particularly effective. Formally, after sampling a data point (E(M), M) from $\\mathcal{D}_{mol-rep}$, we use $(M, E(M) + \\epsilon_{rep} \\epsilon)$ for training. Ablation study in Appendix D show this simple method can effectively prevent overfitting and ensure that performance on novel representations matches those from the training dataset."}, {"title": "C EXPERIMENT DETAILS", "content": "Metrics and Baseline Descriptions We adopt the evaluation metrics, guidelines and baselines commonly used in prior 3D molecular generative models to ensure a fair comparison (Hoogeboom et al., 2022)."}, {"title": "E THEORETICAL ANALYSIS", "content": "In this section, we provide rigorous theoretical analysis on representation-conditioned diffusion models. Our theory is not limited to molecule generation, and is the first theoretical breakthrough for the RCG framework (Li et al., 2023).\nOur analysis is organized as follows. In Appendix E.1, we analyze the generation bound of representation-conditioned diffusion models in unconditional generation tasks by showing: (i) the representation can be well generated by the first-stage diffusion model with mild assumptions (Ap-pendix E.1.1); (ii) the second-stage representation-conditioned diffusion model exhibits no higher generalization error than traditional one-stage diffusion model, and can arguably achieve lower er-ror leveraging the informative representations (Appendix E.1.2). Then in Appendix E.2, we analyze conditional generation tasks as follows: (iii) under mild assumptions of representations and targets, we provide a novel bound for score estimation error (Appendix E.2.1); (iv) generated representations have provable reward improvement towards the target, with the suboptimality composed of offline regression error and diffusion distribution shift (Appendix E.2.2), thus would improve the second stage of conditional generation (Appendix E.2.3)."}, {"title": "E.1 UNCONDITIONAL GENERATION", "content": ""}, {"title": "E.1.1 PROVABLE GENERATION OF REPRESENTATIONS", "content": "Recall the two-stage generation process of representation-conditioned generation: $p(x,r) = p_{\\theta}(x|r)p_{\\varphi}(r)$. To quantitatively evaluate the generation process, we consider two stages separately. In this subsection, we first provide theoretical analysis on the provable generation of representations $p_{\\varphi}(r)$.\nAssumption E.1. (Second moment bound of representations.)\n$m_r^2 := \\mathbb{E}_{q(r)} [||r - \\bar{r}||^2] < \\infty$ (9)\nwhere q(r) is the ground truth distribution of the representations, and $\\bar{r} := \\mathbb{E}_{q(r)}[r]$.\nAssumption E.2. (Lipschitz score of representations). For all t > 0, the score $\\nabla \\text{ln} q(r_t)$ is $L_r$-Lipschitz.\nwhere $q(r_t)$ is the distribution of noisy latent $r_t$ at diffusion step t in the forward process.\nFinally, the quality of diffusion models obviously depends on the expressivity of score network $s_\\theta$ with prediction $s_{\\theta}(r)$.\nAssumption E.3. (Score estimation error of representations). For all $t \\in [0, T]$,\n$\\mathbb{E}_{q(r_t)} [||s_{\\theta}(t) - \\nabla \\text{ln} q(r_t)||^2] \\leq \\epsilon_{\\theta, score}^2$ (10)\nThese are similar assumptions to the ones in (Chen et al., 2023).\nProposition E.1. Suppose Assumption E.1, Assumption E.2, Assumption E.3 hold, and the step size $h := T/N$ satisfies $h < 1/L_r$. Then the following holds,\n$\\text{TV}(p_{\\varphi}(r_0), q(r)) \\leq \\sqrt{\\text{KL}(q(r)||\\gamma_d)} \\text{exp}(-T) + (L_r\\sqrt{d_r h} + L_r m_r h) \\sqrt{T} + \\epsilon_{\\theta,score} \\sqrt{T}$ (11)\nThis is a direct conclusion from (Chen et al., 2023). In typical DDPM implementation, we choose h = 1 and thus T = N. Remarkably, Proposition E.1 indicates the benefit of generating the representation first: since $d_r < d$, the generation quality (measured by the TV distance in Proposi-tion E.1) of the low-dimensional representation can easily outperform directly generating the high-dimensional data points x. The theorem also accounts for applying a lightweight MLP as the de-noising network while in the stage of generating the representation."}]}