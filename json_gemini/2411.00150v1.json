{"title": "Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State Tracking", "authors": ["Christopher Richardson", "Roshan Sharma", "Neeraj Gaur", "Parisa Haghani", "Anirudh Sundar", "Bhuvana Ramabhadran"], "abstract": "Zero-shot domain adaptation for dialogue state tracking (DST) remains a challenging problem in task-oriented dialogue (TOD) systems, where models must generalize to target domains unseen at training time. Current large language model approaches for zero-shot domain adaptation rely on prompting to introduce knowledge pertaining to the target domains. However, their efficacy strongly depends on prompt engineering, as well as the zero-shot ability of the underlying language model. In this work, we devise a novel data augmentation approach, Schema Augmentation, that improves the zero-shot domain adaptation of language models through fine-tuning. Schema Augmentation is a simple but effective technique that enhances generalization by introducing variations of slot names within the schema provided in the prompt. Experiments on MultiWOZ and SpokenWOZ showed that the proposed approach resulted in a substantial improvement over the baseline, in some experiments achieving over a twofold accuracy gain over unseen domains while maintaining equal or superior performance over all domains.", "sections": [{"title": "1 Introduction", "content": "Dialogue State Tracking (DST) plays a pivotal role in task-oriented dialogue systems by maintaining a structured representation of the user's goals, intents, and preferences as a conversation progresses. As these systems interface with external APIs, such as booking platforms or food ordering services, accurate DST is crucial for ensuring successful goal-oriented interactions. Traditionally, DST models were domain-specific, relying on large amounts of annotated data for every domain. The rise of powerful multi-task instruction-tuned large language models (LLMs) has enabled zero-shot DST, which does not require task-specific training (Feng et al., 2023; Yi et al., 2024; Hosseini-Asl et al., 2020). However, zero-shot DST results have been mixed, and prior work has argued that general purpose models remain limited in their ability to replace specialized systems for this task (Heck et al., 2023).\nZero-shot domain adaptation offers a promising middle ground. In this problem, training is allowed, but only some domains are available at training time. This allows for methods that retain the benefits of fine-tuning while addressing the challenge of domain-specific data scarcity. Prior work has investigated this method, but has required slots to be filled one at a time, either by generating slot-specific prompts (Aksu et al., 2023) or reformulating slot-filling as QA (Li et al., 2021; Lin et al., 2021). However, zero-shot domain adaptation methods for end-to-end DST - modeling the entire dialogue state at once, given the dialogue context - is an area which remains underexplored. To this end, we focus on training methods for improving zero-shot domain adaptation with large language models for end-to-end DST. We develop a novel method using data augmentation techniques, and demonstrate the effectiveness of our method on two popular DST datasets, MultiWOZ 2.2 (Zang et al., 2020) and SpokenWOZ (Si et al., 2024).\nOur contributions are threefold:\n1. We introduce Schema Augmentation, a dataaugmentation technique for zero-shot domainadaptation that obtains up to a twofold improvement over a strong baseline.\n2. We propose a new metric, Target Goal Accuracy, to evaluate domain adaptation performance in task-oriented dialogue systems.\n3. We conduct ablation studies to analyze the factors contributing to the effectiveness of our method."}, {"title": "2 Problem Formulation", "content": "The goal of Dialogue State Tracking (DST) is to predict the dialogue state given the dialogue text as input. Dialogue states are constructed from sets of slots S and values V, with each slot belonging to a particular domain d. A dataset $X = \\{(x_i, Y_i)\\}$ consists of dialogues $x_i$ and dialogue states $Y_i$. A dialogue state is a set of k distinct slot/value pairs:\n$Y_i = \\{(s_{ik}, v_{ik})\\}_k : s \\in S, v \\in V$,\n(1)\nThe goal is to learn a function that maps dialogue to dialogue state, $\\pi(x) = y$. We also have a set of domains D and say that a dialogue state belongs to a set of domains, $y \\in D$, if $\\exists (s_k, v_k) \\in y: s_k \\in d$ for some $d \\in D$. For zero-shot domain adaptation, we assume there is a set of target domains $D_T$ from which no data is available during training. Thus we have a target subset of our test data\n$X_{target} = \\{(x, y) : y \\in D_T\\}$\n(2)\nFor each slot $s \\in S$ we are given a description of the slot and a list of possible values it can take. This information, referred to as the schema, defines the structure of slot names, descriptions, and possible values, and is typically provided in the prompt."}, {"title": "3 Evaluation Metrics", "content": "For evaluation, we utilize Joint Goal Accuracy (JGA) (Henderson et al., 2014), the most commonly used DST metric. The main drawback of JGA is that it looks at all slots and domains, and thus does not measure domain adaptation performance directly. To alleviate this, we introduce a new metric: Target Goal Accuracy (TGA), a subgoal version of JGA that considers only those slots belonging to the target domains.\n\u2022 Joint Goal Accuracy (JGA): The percent of turns in which the entire state is accurately predicted. We include all dialogue states, including empty states, to be comprehensive.\n\u2022 Target Goal Accuracy (TGA): The percent of turns in which all slots belonging to the target domains are accurately predicted. We do not include turns that have no target-domain slots active in the ground truth dialogue state, as these empty states would dominate the overall metric."}, {"title": "4 Our Approach: Schema Augmentation", "content": "Domain adaptation is often addressed by leveraging target domain data for fine-tuning (Dingliwal et al., 2021; Zeng et al., 2021) or as few-shot examples during inference (Yang et al., 2023; Chen et al., 2023; Rastogi et al., 2019; Hu et al., 2022). However, reliably annotated data for unseen domains is often unavailable. To address this, we propose to achieve zero-shot adaptation through data augmentation. New domains differ from existing ones in their labeling schema and dialogues. Large language models (LLMs), pre-trained on vast amounts of natural language, are likely robust"}, {"title": "5 Experimental Setup", "content": "We perform dialogue state tracking end-to-end using large language models (LLMs). Building on LDST (Feng et al., 2023), our prompts include an instruction, DST schema, and dialogue. We represent dialogue state outputs as textual JSON, and during evaluations we parse the output generations using regular expressions (regex). Any response that does not contain valid JSON is considered incorrect by all metrics. Details of our experimental hyperparameters and compute are included in Appendix A.\nWe choose state-of-the-art instruction-tuned models from two popular families, Gemma (Gemma Team, 2024) and Mistral (Jiang et al., 2023). We use the variants that are most recent at the time of writing: gemma-2-9b-it and Mistral-7B-Instruct-v0.3.\nFor comparison with prior work, we follow Feng et al. (2023) and build a prompt consisting of instructions, schema, and dialogue. The schema includes slot names, descriptions, and possible values. See Appendix A for full examples of our prompts."}, {"title": "5.2 Datasets", "content": "We conduct our experiments on two open-source dialogue state tracking datasets. MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialogue dataset comprising annotated dialogues across eight domains including hotel booking, restaurant reservation, and taxi ordering. Each dialogue is annotated with the dialogue state at each turn. To alleviate dataset noise, several updated versions of MultiWOZ have since been released. We use MultiWOZ 2.2 (Zang et al., 2020), a popular, high quality version of the original. Additionally, we use SpokenWOZ (Si et al., 2024), a spoken dialogue TOD dataset inspired by MultiWOZ. SpokenWOZ dialogues were collected from crowdworkers engaging in spoken conversations and includes text transcriptions from an automatic speech recognition (ASR) system. We perform our experiments on the audio transcriptions.\nIn order to best simulate the domain adaptation scenario, we select holdout domains that minimize slot overlap with the training domains. To this end, we choose {taxi, train, bus} as our holdout domains. Tables showing slot overlap are included in Appendix A."}, {"title": "6 Results", "content": "Our results are shown in Table 1. We compare our method to several baselines: (0) Zero-shot prompting using LDST prompt from Feng et al. (2023); (1) Standard fine-tuning with no schema, where the input is only the dialogue text; and (2) Fine-tuning using the LDST schema. These baselines are measured against our four methods on both datasets for both models studied. We observe strong performance across the board with both variations of SSA and ESA for both methods, with the strongest performance being achieved with multi-ESA, which boosts TGA from 18.2% to 40.7% on MultiWOZ with Gemma-2."}, {"title": "7 Ablation Studies", "content": "Our results in Table 1 raise the following questions:\n1. Why does schema augmentation (both SSA and ESA) improve domain adaptation?"}, {"title": "8 Conclusion", "content": "We developed Schema Augmentation, a data augmentation technique for zero-shot domain adaptation in dialogue state tracking. To assess its effectiveness, we introduced Target Goal Accuracy (TGA), a metric that evaluates performance specifically on unseen target domains. Our method significantly boosts TGA compared"}, {"title": "Limitations", "content": "Our work has several limitations. We evaluated Schema Augmentation on two instruction-tuned models, Gemma and Mistral, but did not explore a broader range of models, so the generalizability of our findings to other architectures is unclear. Additionally, while we tested on two task-oriented dialogue datasets (MultiWOZ and SpokenWOZ), both are based on similar domains, and further testing on more diverse datasets is needed. Additionally, we only used one set of holdout domains, whereas our experiments could be repeated using different sets of the available domains as holdouts. Due to compute constraints, we limited fine-tuning to models with fewer than 10 billion parameters, which may affect performance compared to larger models. Moreover, our experiments were confined to English-language datasets, leaving the effectiveness of Schema Augmentation in multilingual or non-English contexts unexplored. Lastly, the scope of hyperparameter tuning was limited by available resources, and further exploration of fine-tuning configurations could yield even more insights."}, {"title": "Ethics Statement", "content": "This work aims to improve dialogue state tracking in task-oriented systems, with potential applications in real-world settings like customer service or healthcare. Ensuring the fairness and robustness of these models is crucial to avoid biased or harmful outcomes, especially for underrepresented groups. Additionally, while our method enhances model performance in unseen domains, careful consideration is required before deploying such models in sensitive areas where errors could have significant consequences. Finally, the environmental impact of training large models is an important factor, and more sustainable practices in AI research should be prioritized."}]}