{"title": "NV-Retriever: Improving text embedding models with effective hard-negative mining", "authors": ["Gabriel de Souza P. Moreira", "Radek Osmulski", "Mengyao Xu", "Ronay Ak", "Benedikt Schifferer", "Even Oldridge"], "abstract": "Text embedding models have been popular for information retrieval applications such as semantic search and Question-Answering systems based on Retrieval-Augmented Generation (RAG). Those models are typically Transformer models that are fine-tuned with contrastive learning objectives. Many papers introduced new embedding model architectures and training approaches, however, one of the key ingredients, the process of mining negative passages, remains poorly explored or described. One of the challenging aspects of fine-tuning embedding models is the selection of high quality hard-negative passages for contrastive learning. In this paper we propose a family of positive-aware mining methods that leverage the positive relevance score for more effective false negatives removal. We also provide a comprehensive ablation study on hard-negative mining methods over their configurations, exploring different teacher and base models. We demonstrate the efficacy of our proposed methods by introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval (BEIR) benchmark and 0.65 points higher than previous methods. The model placed 1st when it was published to MTEB Retrieval on July 07, 2024.", "sections": [{"title": "1 Introduction", "content": "Text retrieval is critical for a range of information retrieval applications such as search, question answering, semantic textual similarity, and item recommendation. It is also vital to the new field of Retrieval Augmented Generation (RAG) [14, 24], which enables Large Language Models (LLM) to access external context without modifying model parameters.\nEmbedding or dense retrieval models are a key component of text retrieval. They semantically represent queries and passages (pieces of content) with low token overlap and generalize for out-of-domain corpus. Retrieval systems that index passages into embeddings can efficiently deploy and leverage them, retrieving relevant passages for a query using Maximum Inner Product Search (MIPS) [14].\nThe main contributions of this work are threefold:\n\u2022 Positive-aware hard-negative mining methods. We introduce a novel family of hard-negative mining methods that take into account the positive relevance score for better false negatives removal, as described in Section 3.1;"}, {"title": "2 Background", "content": "In this section we discuss related work on text embedding models and hard-negative mining."}, {"title": "2.1 Text embedding models", "content": "Text embedding models represent variable-length text as a fixed dimension vector that can be used for downstream tasks.\nA seminal work on embedding sentences was the Sentence-BERT [25], which proposed a modification to BERT network to represent pairs of related short texts in the same embedding space by using siamese networks (query,positive passages) or triplet networks (query,positive negative passages). They also explore different options of objective functions and embedding pooling.\nContrastive learning was popularized by SimCLR[4] which demonstrated to be more effective than classification-based losses [25] for embeddings. The (DPR)[9] proposed a bi-encoder architecture where separate BERT encoders (with no shared weights) to represent query and passage, whose output embeddings are used for CL.\nThe E5 [30] family of models leveraged two-stage training of embedding models, pre-training with pairs of text unsupervised (e.g. neighboring text spans, title-abstract) and fine-tuning with supervised data (e.g. question-answer, text-summary, search-relevant passages). E5 models are available in different sizes, depending on the base model: MiniLM[32] and BERT[6]. E5 was the first dense retrieval model to beat BM25[27] sparse model baseline on BEIR[29] without using labeled data (with its unsupervised version). On second stage, they fine-tune E5 unsupervised model with labeled data from MS-MARCO dataset [1], Natural Questions (NQ) and NLI for superior performance.\nE5-Mistral [31] embedding model proposed using a decoder model instead of encoder model (like BERT) as a base model. They choose Mistral-7B[8], which is already extensively pre-trained on web-scale data. It was fine-tuned in a single round with synthetic data generated by LLMs for different tasks and languages and with a small amount of labeled data to achieve state-of-the-art performance.\nThe BEIR benchmark[29] has become the standard evaluation for zero-shot text retrieval with 18 retrieval datasets. Later MTEB [20] was introduced as a more comprehensive text embeddings benchmark, with 56 datasets distributed over 7 tasks for the English subset: retrieval, reranking, classification, clustering, pair classification, summarization, and semantic textual similarity. The MTEB retrieval task is composed by 15 selected datasets from BEIR.\nOur target for this research is building high-quality retrieval embedding models to support Q&A RAG systems. Thus we focus on the retrieval task and evaluate with MTEB Retrieval / BEIR benchmark."}, {"title": "2.2 Hard-negative mining for fine-tuning embedding models", "content": "Contrastive Learning (CL) requires a query, positive passage and negative(s) passage(s) triple. Negative passages can be labeled by humans or more commonly selected from the corpus.\nA basic approach for selecting negative passages is using the positive passages from other examples (queries) in the batch (in-batch negatives) [4, 9]. This is efficient because the embeddings for those passages were already generated by the model training forward pass, although, the number of negatives is limited by the batch size. Some proposals to increase the number of negatives were keeping a cache / memory bank with embeddings from past batches examples [30, 36] or combining batches from different GPUs [22] (cross-batch).\nThe negatives obtained from batches are random with respect to the query and very uninformative for CL, as their loss/gradients are low, contributing little to model convergence [35].\nFor this reason, it is important to provide hard negatives for CL. They can be mined from the corpus of passages, by retrieving examples similar to the query that are not labeled as positive. DPR [9] use 1 or 2 hard negatives mined with BM25[27] in addition to the in-batch negatives that help them to finetune models with better accuracy.\nANCE[35] proposes iterative mining of hard-negatives from embeddings of the corpus, by asynchronously updating and querying an approximate nearest neighbor (ANN) index during training. Although, recomputing embeddings for the whole corpus with intermediate checkpoints during fine-tuning is costly, which is why many works mine hard-negatives off-line using a pre-trained embedding model.\nIn [22] they found that naive mining of hard-negatives may select many false negatives. Their experiment of mining with MS-Marco dataset found that about 70% of passages most similar to the queries should be actually labeled as positive. They propose to denoise hard-negatives, i.e., ignore the potential false negatives by filtering out the ones with high relevance score to the query.\nSome works refine the hard-negatives retrieved from embedding models with cross-encoder ranking models [22, 26], which can better capture semantic similarity via deep interaction, or with powerful decoder LLMs [13].\nMost recent top performing models on MTEB like e5-mistral- 7b-instruct [31], Linq-Embed-Mistral [10], NV-Embed-v1 [12], gte- large-en-v1.5 [15], nomic-embed-text-v1 [21] leverage hard-negative mining in their fine-tuning, but they do not explore or describe in detail their methodology to decide which model and method to use for mining.\nThere are a few exceptions. In snowflake-arctic-embed-l [17], the authors described a limited ablation study on experimenting with a different maximum score threshold (0.4, 0.5 and 0.8) for hard- negative mining. A blog post about SFR-Embedding-Mistral [16] describes some ablation on sampling hard-negatives from three"}, {"title": "3 Investigation on hard-negative mining for fine-tuning text embedding models", "content": "In this section, we investigate the usage of different embedding models and methods for mining hard-negatives and their effect on the fine-tuned downstream models. In particular, we explore the following Research Questions:\n\u2022 RQ1. How much does mining hard-negatives with different retrieval models affect the accuracy of the downstream fine-tuned embedding models?\n\u2022 RQ2. Can ensembling hard-negatives mined from different models improve results?\n\u2022 RQ3. How does different hard-negative mining methods for fine-tuning compare on the evaluation accuracy?\nFor this investigation we have selected a number of hard-negative mining methods and some embedding models for mining, which are described next. We present the results of this ablation study on Section 3.4."}, {"title": "3.1 Hard-negative mining methods", "content": "The basic method for mining hard-negatives is selecting the top-k most similar candidates to the query, after ignoring the positive passages, which we name Naive Top-K.\nAlthough, as discussed in Section 2.2, the hard-negative mining process may introduce some false negatives, i.e., passages that are relevant to the query but not annotated as positives, which would add noise to CL. That is common for open domain question answering (OpenQA) datasets like MS MARCO [1] and Natural Questions [11], where the question answer might be supported by many documents from a large corpus like Wikipedia [2, 11] or the web [1].\nSome methods for filtering out false negatives have been proposed in the literature:\n\u2022 Top-K shifted by N - Selects the top-k negatives after a rank N, e.g., Top-10 shifted by 5 would ignore the first 5 negatives and consider negatives between rank 5 and 15 [16, 34];\n\u2022 Top-k with absolute threshold (TopK-Abs) - Ignores negatives with relevance score higher than an absolute threshold [12, 17, 22];\nThese methods have some important limitations. Top-K shifted by N is a basic method that does not take into account the relevance score of the negative with respect to the query and might even throw away valuable hard-negatives or keep false negatives. TopK-Abs thresholds negative scores with respect only to the query, regardless the positive passage relevance. Motivated by those limitations, we have designed a family of positive-aware hard-negative mining methods, which are described below. They leverage the information from the positive relevance score to help identifying and removing potential false negatives.\n\u2022 Top-k with margin to positive threshold (TopK-MarginPos) - Maximum threshold for negative scores is the positive score minus an absolute margin as in Equation 1.\n\u2022 Top-k with percentage to positive threshold (TopK-PercPos) - Maximum threshold for negative scores is a percentage of the positive score, as in Equation 2\n$max\\_neg\\_score\\_threshold = pos\\_score \u2013 absolute\\_margin$ (1)\n$max\\_neg\\_score\\_threshold = pos\\_score * percentage\\_margin$ (2)\nAfter the potential false negatives are removed by the mining methods above, a number of candidates is extracted to form the hard- negatives set. The typical approach is selecting the top-k candidates, but some research works have proposed sampling among top-k to add some relevance diversity among the selected hard-negatives:\n\u2022 Sampled Top-k - Samples n negatives from the top-k most relevant ones [3, 13, 22] or from a range of negatives based on its relevance rank, e.g. from 30-100 range like in [16];\n\u2022 Top-1+sampled top-k - Selects the top-1 hard-negative (to secure a strong one) and samples n \u2212 1 negatives like described in the Sampled Top-k method.\nWe compare our positive-aware hard-negative mining methods with other mining methods in Section 3.4.3."}, {"title": "3.2 Selected embedding models", "content": "We have selected for this ablation a number of popular text embedding models, that represent different architectures / model sizes and retrieval accuracy, as teacher models for mining hard-negatives.\n\u2022 e5-large-unsupervised (334M params) - The E5 model pre-trained on unsupervised data with CL [30];\n\u2022 e5-large-v2 (334M params) - An E5 model fine-tuned on top of e5-large-unsupervised with supervised data[30]\n\u2022 snowflake-arctic-embed-l (334M params) - Member of the artic-embed models which is trained in two rounds (with supervised and unsupervised data) like E5 model, with improvements on data and training that lead to higher retrieval accuracy [17]\n\u2022 e5-mistral-7b-instruct (7.1B params) - The decoder-only Mistral model fine-tuned with CL to create an embedding model [31];\n\u2022 NV-Embed-v1 (7.8B params) - A Mistral-based embedding model with some modifications including bi-directional and latent attention[12]."}, {"title": "3.3 Training and evaluation", "content": "3.3.1 Training. For the ablation experiments we finetune e5-large- unsupervised with four hard-negatives mined with the selected embedding models and mining techniques."}, {"title": "3.4 Ablation Study Results", "content": "In this section, we present the experiments results for the research questions, comparing different teacher models for mining and the effect of ensembling hard-negatives from different models. We also provide a comprehensive ablation on different hard-negative mining methods over their thresholds for filtering out false negatives.\n3.4.1 RQ1. Using different embedding models for mining. To address RQ1 we use the selected embedding models described in Section 3.2 for mining 4 hard-negatives for every question in the train set. This process results in one train set per teacher model, which is used to train the baseline model (E5-large-unsupervised).\nWe can see from the results on Table 1 that the worse retrieval accuracy was obtained by using negatives mined with the BM25 sparse retrieval model [27] followed by random negatives, which was surprising and opposite to what was found in [9]. The E5-large- unsupervised dense retrieval model, pre-trained only on unsupervised data from the web, provided better accuracy.\nThe next models are trained on retrieval supervised data, in particular for Question-Answering RAG systems. The e5-large-v2 and snowflake-arctic-embed-l use the E5 architecture (334M params) and perform better than the baselines. The best teacher models were NV-Embed-v1 and e5-mistral-7b-instruct, which are based on the Mistral architecture (7B params) and seem to provide better negatives for CL, which result in higher accuracy on the fine-tuned models.\n3.4.2 RQ2. Ensembling hard-negatives from different embedding models. Ensembling outputs from different models is a common practice in machine learning to improve predictions accuracy, by providing a more robust estimator [5, 18, 28].\nIn particular, we investigated the similarity of the top-4 hard- negatives mined by four teacher models and noticed a low level of agreement (jaccard similarity lower than 30%). You can see the detailed analysis on Appendix A. For this reason, we decided to explore ensembling to try improving the quality of the hard-negatives.\nWe explored two methods to combine hard-negatives mined from four different E5 and Mistral based embedding models - e5-large-v2, snowflake-arctic-embed-l, NV-Embed-v1, and e5-mistral-7b-instruct which are described next. Each ensembling method returns 4 hard- negatives for each example (query,positive).\n\u2022 Cross-sample ensembling - Samples a teacher model to obtain all the negatives for an example.\n\u2022 Intra-sample ensembling - For each example selects the top-1 mined negative from each teacher model.\nThe evaluation results are shown in Table 2. In the first row we have as baseline the model trained with hard-negatives from the best teacher model: e5-mistral-7b-instruct. We can see that the cross-sample ensembling method does not lead to better negatives than the best teacher model.\nThe Intra-sample ensembling method turned out to be more effective. As it may lead to duplicate hard-negatives, as for some examples the teacher models might agree on the 1st hard-negative. Thus, we tried two variations: keeping duplicates (no-dedup) or removing duplicates (dedup) and replacing them by the next unique hard-negative from teacher models sorted by their accuracy. Surprisingly, we found that it was better to keep the duplicate hard- negatives for training. A possible explanation for that could be that if models agree on the 1st hard-negative, it might be higher quality and keeping it duplicate will increase its importance in the cross-entropy loss.\n3.4.3 RQ3. Comparing methods for mining hard-negatives. For RQ3 we performed a comprehensive number of experiments testing different hard-negative mining methods, described in Section 3.1.\nWe use as teacher model the e5-mistral-7b-instruct, which per- formed best for mining hard-negatives (Section 3.4.1).\nWe fine-tuned the e5-large-unsupervised model with hard-negatives mined with each mining method for a range of their configuration. For TopK-Abs, TopK-MarginPos and TopK-PercPos the range of the threshold/margin values config was [0, 1], with increments of 0.05."}, {"title": "4 NV-Retriever-v1", "content": "In this section, we describe the architecture, methods and tech- niques for training the state-of-the-art NV-Retriever-v1 embedding model, which placed first on MTEB Retrieval leaderboard when it was published.\nThe positive-aware mining methods we propose in this paper were crucial to obtain high retrieval accuracy for NV-Retriever-v1."}, {"title": "4.1 Model architecture", "content": "Like introduced by e5-mistral-7b-instruct [31] and followed by many of the top performing embedding models on MTEB such as Linq- Embed-Mistral [10], GritLM [19], SFR-Embedding-Mistral [16], and NV-Embed-v1 [12], we use the Mistral 7B[8] as base model.\nIn GritLM [19] the authors proposed fine-tuning of a causal lan- guage model as an embedding model with bi-directional attention followed by mean pooling, which corresponds to averaging the hidden states across the sequence length. This approach was later used by NV-Embed-v1 [12] and gte-Qwen2-7B-instruct, and we also employ bi-directional attention for NV-Retriever-v1."}, {"title": "4.2 Train sets and instruction prefixes", "content": "As MTEB contains different tasks like retrieval, reranking, classifi- cation, clustering among others, a diverse set of training datasets is required for a good overall performance. The train sets used for fine-tuning NV-Retriever-v1 are described in Appendix C.\nWe used the E5-Mistral-7B embedding model for hard-negative mining with maximum sequence length of 4096.\nTo ignore potential false negatives, we leverage our proposed TopK-PercPos method and set the maximum threshold for the nega- tive relevance score as 95% of the positive score. We explain this choice in the ablation study presented in Section 3.4.3.\nTo support better understanding from the base LLM model on the datasets domain and task, [31] proposed and designed specific natural language instructions for each train set. The instructions prefixes are added to the query but not to the passages, so that for the latter no re-indexing is required for different instructions. We also adopt instruction prefixes with a small difference in the implementation: instead of the original template from [31] \"Instruct: {task_definition} \n Query: {query}\", we use \"{task_definition}: {query}\". As in NV-Embed-v1 [12] we mask out the instruction to- kens for the average pooling during both training and evaluation, which can still affect the output due to self-attention."}, {"title": "4.3 Training", "content": "We perform two stages of instruction tuning like in [12]. In the first stage, we only use retrieval supervised data with in-batch negatives in addition to the mined hard-negatives. In the second stage, we blend data for retrieval task and with other tasks' datasets (e.g. classification, regression). We describe the model and training hyper-parameters in Appendix D."}, {"title": "4.4 NV-Retriever-v1 Results", "content": "The ablation study (Section 3.4) compares different negative mining methods in controlled experiments with the same hyper parameters on a subset of the BEIR datasets. NV-Retriever-v1 tests the best configuration for positive-aware mining methods on the full MTEB BEIR benchmark and compare it to other top performing models.\nNV-Retriever-v1 achieves an average NDCG@10 score of 60.9 and is 1st place as of 2024-07-11. Table 5 reports the top embedding mod- els on MTEB Retrieval leaderboard. Mistral is the foundation model for 4 of the top 5 places. As described, NV-Retriever-v1 is trained with similar techniques than other models and mainly differentiate in using our positive-aware mining methods. It outperforms the best models by 0.65 points, which is a significant improvement for the top positions of the leaderboard."}, {"title": "5 Conclusion", "content": "In this work, we introduce a novel family of positive-aware hard- negative mining methods, that take into account the positive rele- vance score for better false negatives removal.\nWe also provide a comprehensive ablation study comparing hard-negative mining methods (under their many configurations), different teacher models and the ensembling of their hard-negatives, demonstrating the effect of those choices on the accuracy of the fine-tuned text embedding models.\nWe then apply those best practices on hard-negative mining to train the NV-Retriever-v1, a state-of-the-art text embedding model and compare it with best published models. NV-Retriever-v1 placed first on MTEB Retrieval/BEIR benchmark at its publishing time.\nWe hope this investigation on hard-negative mining encourages more research and favors more accurate fine-tuning of text em- bedding models. We also highly encourage future works on this area to disclose their methodology for mining, i.e., which teacher model and mining method is used, for both reproducibility and replicability."}, {"title": "A Similarity of hard-negatives mined with different teacher models", "content": "In this appendix we investigate the level of agreement of different teacher models on the hard-negatives.\nIn particular, we computed the Jaccard Similarity between the top-4 hard-negatives for pairs of teacher models. You can check the similarity matrices for each of our train sets: NQ , SQUAD and StackExchange .\nIn general, it is possible to observe from the similarity matrices that the teacher models have low agreement on the top-4 mined hard-negatives, i.e., Jaccard similarity lower than 30%.\nA possible explanation for a higher similarity of hard-negatives for SQUAD dataset compared to the other datasets might be because it has less unique passages (18,891) than NQ (75,215), and StackExchange (99,974).\nIt is also interesting to observe that StackExchange features much lower inter-model similarity on hard-negatives than NQ (not much smaller in numbers of samples), which might indicate that StackExchange might have many similar examples (questions, pas- sages) that provide more candidates for models disagreeing on the top hard-negatives. We understand that this scenario might lead to more false negatives, and eliminating them is even more critical."}, {"title": "B Detailed ablation of the hard-negative mining methods configurations", "content": "As discussed in Section 3.4.3, we performed an ablation study com- paring different hard-negative mining methods with different con- figurations. We used the e5-large-unsupervised as base model and mined negatives using e5-mistral-7b-instruct as teacher model for the experiments. The evaluation metric reported in the plots is the average of NDCG@10 for the three BEIR Q&A datasets (NQ, HotpotQA and FiQA).\nFor this comprehensive experimentation, we fine-tuned and eval- uated one embedding model for each of the mining methods on dif- ferent configurations from a range. For TopK-Abs, TopK-MarginPos and TopK-PercPos methods, the interval for the threshold/margin values config was [0, 1], with increments of 0.05.\nThe metrics for best configurations for each mining method are shown in Table 3. But the plots provide some intuition on how the configuration choices of each mining method affect the fine-tuned embedding model.\nWe can see that the basic Top-k shifted by N (Figure 2) method provides its best accuracy when discarding the top-10 ranked hard-negatives from the training, as they have a higher change of con- taining false negatives.\nFor TopK-Abs (Figure 3) - a traditional method for removing mined false negatives \u2013 the best configuration was using the ab- solute score of 0.7 as maximum threshold for negative scores. It seems that a higher threshold could select more false negatives and lower threshold could lead to weaker negatives that would not be very informative for CL.\nWe propose in this paper positive-aware mining methods: TopK- MarginPos and TopK-PercPos. For TopK-MarginPos mining method (Figure 4), we observe that subtracting a small margin (0.05) from the positive score and setting it as the maximum threshold for the hard-negatives helped removing potential false negatives, but larger margins end up penalizing the model accuracy. For the TopK-PercPos mining method (Figure 5), we found that setting the maximum"}, {"title": "CNV-Retriever-v1 - Datasets for training", "content": "We selected for fine-tuning the NV-Retriever-v1 model the datasets listed in Table 9, which are targeted for retrieval, classification, clus- tering and semantic textual similarity. We use the same instruction prompts from [31] for each dataset."}, {"title": "DNV-Retriever-v1 - Training Hyperparameters", "content": "The NV-Retriever-v1 was trained in two stages. In the first stage, we only use retrieval supervised data and in the second stage, we blend data for retrieval task and with other tasks' datasets (e.g. classification, regression). Our training implementation is based on the Hugging Face Transformers and PEFT libraries. The model and training hyperparameters are shown in Table 10 and Table 11."}]}