{"title": "Contrasting local and global modeling with machine learning and satellite data: A case study estimating tree canopy height in African savannas", "authors": ["Esther Rolf", "Lucia Gordon", "Milind Tambe", "Andrew Davies"], "abstract": "While advances in machine learning with satellite imagery (SatML) are facilitating environ- mental monitoring at a global scale, developing SatML models that are accurate and useful for local regions remains critical to understanding and acting on an ever-changing planet. As in- creasing attention and resources are being devoted to training SatML models with global data, it is important to understand when improvements in global models will make it easier to train or fine-tune models that are accurate in specific regions. To explore this question, we contrast local and global training paradigms for SatML through a case study of tree canopy height (TCH) mapping in the Karingani Game Reserve, Mozambique. We find that recent advances in global TCH mapping do not necessarily translate to better local modeling abilities in our study region. Specifically, small models trained only with locally-collected data outperform published global TCH maps, and even outperform globally pretrained models that we fine-tune using local data. Analyzing these results further, we identify specific points of conflict and synergy between local and global modeling paradigms that can inform future research toward aligning local and global performance objectives in geospatial machine learning.", "sections": [{"title": "Introduction", "content": "Paired with a massive amount of publicly available satellite imagery, geospatial machine learning is enabling a new paradigm of environmental monitoring. Machine learning with satellite imagery (SatML) models have been deployed at a global scale to classify patterns as diverse as land cover types [Brown et al., 2022], industrial activity at sea [Paolo et al., 2024], micro-estimates of wealth [Chi et al., 2022], and forest and tree canopy height [Potapov et al., 2021, Lang et al., 2023, Tolan et al., 2024]. While efforts like these significantly extend the possibilities for global-scale science and policy, understanding local trends and variations in key environmental indicators remains paramount. Many important indicators of global change in ecology and environmental sciences manifest at local or regional scales, which global-scale data do not always represent well. Moreover, data-driven policies designed to mitigate global change and its effects (e.g., policies that regulate land use, or distribute resources across populations) are often tailored to particular localities (e.g., administrative regions or protected areas).\nPrior work has exposed concerns about the accuracy of geospatial machine learning models developed for global- or national-scale prediction when evaluated in local areas. Kerner et al."}, {"title": "Related Work", "content": "The distinction between learning representations that capture local variation and those that best capture global variation has long been established, for example concerning notions of \"local\" and \"global\" learning paradigms across general manifolds [Zhou et al., 2003, Saul and Roweis, 2003, Huang et al., 2008]. There is a corresponding discrepancy in output (prediction) space as well. A model trained to optimize average performance across an entire population or distribution can fail to capture meaningful variation within sub-regions or sub-populations due to limited model capacity or finite training data [Rolf et al., 2022], or because the model can mainly predict relative values across sub-groups of a population, but cannot reliably distinguish variation within sub-groups [Aiken et al., 2023].\nPast studies have evidenced that the local-global distinction may be especially relevant for geospatial machine learning, where the spatial nature of predictions across the globe makes the distinction between local and global prediction particularly clear. Kerner et al. [2024] evaluate ex- isting global and regional land-cover maps in Sub-Saharan Africa, finding that global maps perform significantly worse than maps that were developed for regional use. This phenomenon extends to national-level maps as well; Yeh et al. [2020] produced national models to predict economic well- being from satellite imagery, but found that they could resolve national variation much better than variation within urban or rural areas. This result was replicated across many countries by Aiken et al. [2023]. Discrepancies between local and global accuracy of SatML predictions have also been highlighted in the environmental monitoring domains of population estimation [Kuffer et al., 2022], building mapping [Gevaert et al., 2024], road detection [Nyandwi et al., 2024] and tree canopy height mapping [Pauls et al., 2024]. With the increasing focus on global modeling efforts and foundation models in SatML, it is important to understand not just where errors in published models are distributed across the globe, but also to more fundamentally understand what aspects of geospatial modeling might differ for a global training and evaluation objective, compared to a more localized one."}, {"title": "Mapping tree canopy height with satellite imagery and machine learning", "content": "Several studies use machine learning with satellite imagery inputs to extend the spatial coverage of LiDAR-based canopy height maps in different local study regions. One thread of methodology uses locally collected LiDAR data, e.g. vegetation height from airborne LiDAR scanning (ALS), as the training labels for these predictive models. For example, Wilkes et al. [2015] used random forest models and publicly available satellite imagery to extend the range of ALS maps in east Victoria, Australia, and Astola et al. [2021] used Sentinel-2 data to generate TCH maps in Finland. Our methodology for \u201clocal mapping\u201d (described in Section 4) roughly follows this type of approach.\nFour recent studies produce global tree canopy height maps using satellite data and machine learning. While all four maps are aimed at global coverage, each mapping effort has distinct goals that informed their design decisions:\n\u2022 Potapov et al. [2021] produced a 30m resolution map of canopy height across the globe for the year 2019 using a time series of Landsat satellite images. Features derived from pixelwise time series of Landsat data and digital elevation maps are used as input to bagged regression tree"}, {"title": "Case Study Problem Setting: Mapping Tree Canopy Height in Karingani Game Reserve", "content": "Karingani Game Reserve is an ecologically diverse area on the border of Mozambique and South Africa, spanning roughly 145,000 hectares. The Karingani Game Reserve was formally established in 2008 and is comprised primarily of subtropical savanna vegetation varying widely in height and density across gradients of topography, geology, soil type and mean annual rainfall. The reserve also contains a full suite of indigenous herbivore species. In addition to being an area of ecological importance, Karingani is an ideal region for a case study examining how best to map TCH in a local environment. Since the high-resolution LiDAR-based tree canopy height data we use for our case study (described in Section 3.1) has not been used to-date to train or evaluate machine learning models, our case study can simulate different possible avenues and opportunities for producing a new map of TCH in a local region.\nOur case study examines how to map tree canopy height across the Karingani Game Reserve using the different resources available (a limited amount of locally collected high-resolution TCH labels (Section 3.1), publicly available satellite imagery (Section 3.2), and existing maps and models (Section 2.2) that have been produced for mapping TCH at a global scale). While we are of course interested in generating accurate maps of this region, the primary focus of our study is to understand more generally which of these current resources are most helpful for creating accurate local maps, and to identify additional resources or techniques that might aid in local mapping efforts."}, {"title": "Local LiDAR-derived tree canopy height labels", "content": "Our local label data used for model training and evaluation span a study area of 24 sites in the Karingani Game Reserve (Figure 1b), collected in May and early June 2021. The 1m resolution reference tree canopy height (TCH) maps are derived by applying a canopy height model (as detailed in Boucher et al. [2023], Reiner et al. [2023]) to LiDAR sensor data acquired by flying an unoccupied aerial vehicle (UAV) across the study sites. Of the 24 sites, 22 sites span roughly 21 km\u00b2 and 2 span roughly 7 km\u00b2. For context, the LiDAR data collection via UAV flight took roughly 1 day per site, including travel time.\nIn contrast to the globally available (but sparse) GEDI data that past studies have used to build and extend TCH models, our local LiDAR-derived TCH labels are highly accurate ground- referenced data. Thus, we consider these data to be a valuable resource for evaluating model performance in this region. Moreover, understanding the performance gain achievable when training with local TCH data will help inform the value of acquiring local LiDAR data by flying UAVs in additional targeted regions in the future."}, {"title": "Extending the range of existing label data with machine learning and Sentinel-2 satellite data", "content": "While the LiDAR-derived data spans sites across the Karingani Game Reserve, there are still significant gaps in the coverage of the existing TCH data (roughly two thirds of the Karingani Game Reserve are not covered by past UAV surveying efforts). In our case study, the goal is to"}, {"title": "Site-stratified training and evaluation splits simulate conditions of model use outside training data sites", "content": "To reflect our case study setting of applying the trained SatML models to fill in gaps where high- fidelity TCH data are not available, we employ a spatially stratified train/validation/test split of our data. Specifically, we split the 24 training sites into three distinct sets: training (12 sites), validation (6 sites) and testing (6 sites), as depicted in Figure 1a,b. For each split, no test sites are seen during the training step or the model validation step in which we pick model parameters and hyperparameters.\nWhen we train, validate, and evaluate on spatially disjointed sets of training sites in this manner, we are essentially simulating the intended conditions of our case study [Rolf, 2023]. Specifically, we simulate training our model in regions where we have data, then deploying our model to a separate set of sites that were not used in training or validating the model. Using the spatially disjointed test set ensures that the performance we evaluate more faithfully represents our intended use case in the Karingani Game Reserve. We construct four such spatial data splits, where each LiDAR collection site appears in exactly one test set and exactly one evaluation set. By analyzing performance across the four splits, we can assess the variation among the models across different data settings, characterized by the available training data and target regions for model use."}, {"title": "Methods", "content": null}, {"title": "Methods overview", "content": "Our experiments in Section 5 will reflect a range of different possible methodologies for predicting tree canopy height (TCH) in a local region. Since several global TCH maps are available, perhaps the most straightforward way to produce a map of TCH in a local area is to take one of the existing global TCH maps, crop and align it to the extent of our study area, and use that subset of the global map as our predictions. This is our primary methodology to \"evaluate existing global maps\" in Section 5.1.\nAn alternative approach would be to use the limited local labels to train a new model specifically adapted to the local region. Here, we distinguish between \"local-only\" models, which we train from scratch using only labels and satellite imagery from the local region (here Karingani Game Reserve), and \u201cglobally-pretrained, locally-adapated\u201d (or locally \u201cfine-tuned\") models, which we fine-tune using local data only, but where the model is initialized from a representation that has been pretrained on global data. We use the term \u201cglobal pretraining\" to encompass both supervised models trained to predict TCH across the globe as well as self-supervised models pretrained on global satellite imagery in a way that is agnostic to possible downstream tasks. In Section 5.2, we use our local data (Section 3.1) to train local-only models and fine-tune existing supervised and self-supervised models that have seen global satellite data, to compare the effectiveness of both approaches. Ultimately, contrasting the performance of using existing global TCH maps \"out of the box,\" adapting globally trained models using local data, and training models using only local data will enable us to address our first research question regarding the importance of local data for local prediction.\nTo address our second research question regarding the importance of different design choices for local prediction, we construct a set of experiments in which we systematically vary different data and modeling decisions and measure their effects on local performance. Specifically, in Section 5.3, we train local models using different amounts of training data to assess the importance of the abundance of local data in training or adapting models for local use. We train models with different"}, {"title": "Implementation details", "content": null}, {"title": "Machine learning model architectures", "content": "We compare multiple neural network model architectures throughout our experiments: (1) an XceptionS2 convolutional neural network architecture used in the ETH global predictions [Lang et al., 2023], which has 8 residual blocks each with two convolutional layers with 256 filters, (2) a U-Net architecture with a ResNet-18 backbone designed for pixel segmentation tasks, which takes the entire input (here a 64 \u00d7 64 crop of the satellite image) as context for the predictions, and (3) a small five-layer fully convolutional network (FCN) with 128 filters per layer and leaky ReLUs between layers. We additionally compare to (4) non-parametric pixel-level forest models, where the input is the multiple channel input for that particular pixel. This pixelwise model does not incorporate any texture in the images, thus it constitutes a simple and informative baseline for machine learning with remotely sensed data."}, {"title": "Model training", "content": "Our experiments compare the strategies of training models only with local data and using local data to fine-tune models that have been initialized through pretraining with global data. When we train models only with local data (using no labels or imagery outside the study area), we randomly initialize model weights and then optimize the model using supervised learning. For the models with globally pre-trained weights available\u00b2 (the ETH and U-Net models), we use these weights (which were derived using global data), and then fine-tune the models using local data. For the XceptionS2 globally pretrained weights, we use the published weights corresponding to the first model (model 0) in the ensemble from Lang et al. [2023], available at the GitHub repository accompanying their paper. Since this model was already trained in a supervised learning setting for the task of TCH prediction, we use a transfer learning strategy in which we tune only the last few layers of the network. For the U-Net model, we use the self-supervised learning for earth observation (SSL4EO-S12) weights from Wang et al. [2023], which have been pretrained on global Sentinel-2 imagery using self-supervised learning techniques. We experiment with freezing the pretrained encoder weights and fine-tuning just the decoder versus fine-tuning the entire network for the U-Net."}, {"title": "Data preprocessing", "content": "To account for some anomalously high values in the LiDAR-derived TCH labels, we set any pixels greater than 30m to no-data (NaN) values. Pixels with TCH < 0m are set to 0. This only occurs in a minimal percentage of the overall data; 99.95% of the non-NaN 1m-TCH pixel values fall within 0-30m. Next, we coarsen the labels to 10m-resolution to be consistent with the resolution of Sentinel-2 imagery. When coarsening, we use the 90th percentile value of the 1m pixel values to maintain consistency with past work [Potapov et al., 2021, Lang et al., 2023].\nWe preprocess the satellite imagery input to all of our locally trained models by channelwise normalization to zero-mean, unit-variance per channel across the three Sentinel-2 tiles that span the 24 sites. When we initialize the model with weights from a previous training or pretraining procedure, we use the image preprocessing transforms used during that previous procedure, as recommended by Corley et al. [2024]."}, {"title": "Evaluation", "content": "We evaluate models pixelwise, at 10m resolution unless otherwise stated. When performance is reported by site, we use the models trained with the split where this site is in the test set. When performance is reported by split, we aggregate all pixels in a split to compute the average. When a single performance metric is given, this represents the average across the four splits.\nWhen evaluating performance of the existing global TCH maps, we exclude from evaluation any pixels that contain no-data values in the global predictions (for the GLAD map, this excludes pixels corresponding to water, snow, ice, or other no-data-valued pixels). Less than 0.01% of the non-NaN pixels in the label data have a corresponding NaN in either of the global maps for both 10m and 30m-resolution, so this has a minimal impact on the reported performance metrics."}, {"title": "Results", "content": null}, {"title": "Evaluating existing global maps", "content": "We first evaluate the performance of existing global TCH maps compared to our best locally trained or fine-tuned model, using both quantitative (Section 5.1.1) and qualitative (Section 5.1.2) assessments. We then assess the degree to which these different maps of TCH capture relative values of aboveground biomass in each site, as a way to gauge the usability of each map for additional ecological assessments in this local area (Section 5.1.3)."}, {"title": "Quantitative Performance", "content": "Figure 2 summarizes the performance of the existing global models and our best local model (with extended results in Table 1). The existing global maps achieve average root mean squared error (RMSE) between 2.43m and 4.51m, and for all maps the variation in performance across data splits (grey dots) is generally smaller than the variation across models. The RMSE of these maps in Karingani is significantly lower than the RMSEs of each map evaluated across the globe (3.09m vs. 9.25m for Potapov et al. [2021]; 4.51m vs 8.62m for Lang et al. [2023]; 2.43m vs. 4.73m for Pauls et al. [2024]). Note that for all models, the RMSE values in Karingani are lower than the global RMSEs. This indicates that the Karingani Game Reserve is not a region of especially high error for the global models, in comparison to other places around the globe.\nWhile the global performance generally increased with each subsequent global model that has been published, Figure 2 shows that some of the more recently published maps perform worse in Karingani than older maps. Until the map by Pauls et al. [2024] was released, the best existing TCH map for the Karingani region (by our quantitative metrics) was the 30m resolution map produced by Potapov et al. [2021]. We discuss implications of this result for understanding when and how efforts to increase global mapping performance can align with increasing local performance (our RQ3) in Section 6.3.\nOur locally-trained model is superior to the existing global maps by a large margin. The predictions of our best-performing model (local FCN) achieve, on average, a 64% lower RSME than the ETH map (4.51m \u2192 1.64m), a 47% lower RMSE than the GLAD map (3.09m \u2192 1.65m), a 53% lower RMSE than the Meta map (3.51m \u2192 1.65m), and a 33% lower RMSE than the Pauls et al. [2024] map (2.43m \u2192 1.65m) in our study area."}, {"title": "Qualitative Performance", "content": "Figure 4 plots our predictions and the data from the four publicly available maps for three repre- sentative sites in our study area. From visual inspection, our local predictions are clearly a better match to the LiDAR-derived TCH values than any of the existing global maps. In contrast, the GLAD map tends to under-predict TCH, and generally has more spatially smooth predictions than the label data (the original resolution of the GLAD map is 30m). The ETH map also tends to pre- dict zeros more frequently, but sometimes captures higher canopy heights (reliably capturing higher canopy heights was a goal of the ETH study [Lang et al., 2023]). For example in the Massingir site, the ETH predictions tend toward more extreme values than the label data convey (Figure 4, first row). The Meta predictions capture more fine-grained textural information better than the GLAD or ETH map, but tend to underpredict TCH overall. The map from Pauls et al. [2024] is the closest existing map to getting the average pixel values correct, but predictions appear to biased toward the mean value, missing some of the high and low values (note also that Pauls et al. [2024] do not estimate any canopy heights under 3m). Figure 7 in Appendix A plots the distribution of predictions and labels per pixel, aggregated across the 24 sites, confirming that these trends hold broadly across our study area."}, {"title": "Gauging usability for downstream ecological analyses", "content": "To probe the degree to which these TCH maps could be used \"out of the box\" to assess other ecological spatial patterns related to tree canopy monitoring, we assess the degree to which each map could be locally calibrated to predict aboveground biomass (AGB). Using established allometric equations available in the ecology literature [Colgan et al., 2013], we estimate AGB (in kg) for each site using the 1m LiDAR-derived TCH values and the linear-affine formula on the average height (H) of pixels designated as canopy cover (CC).\nWe use the predicted values to estimate the same dependent variable $X = average(CC \\times H)$ at each site, for each map. We fit a least squares regression to associate these X values with the calculated"}, {"title": "Training local models from scratch vs. initializing with weights from global (pre)training", "content": "We next examine whether existing global models for machine learning with satellite imagery (SatML) can be a better starting place from which to fine-tune models with local data, versus training from scratch. Table 1 compares the performance of locally fine-tuned models initialized with weights from existing global models, versus initialized randomly."}, {"title": "Relative importance of design decisions across modeling and data choices", "content": "The results detailed in Sections 5.1 and 5.2 show that models tailored for local prediction can significantly improve the performance of TCH models in local regions. In light of this, we investigate the extent to which several different design decisions a research team faces in designing local models affect the overall performance. One goal of these experiments is to investigate the relative value of future efforts ranging from innovating on the machine learning models, choosing which satellite data layers and sources to use as inputs to the model, and even deciding how much labeled data to collect, and where to collect them.\nFigure 5 summarizes an experiment in which we vary one of the following design decisions at a time: which machine learning model architecture is used to define the model (Figure 5a), which data layers are used as input to the model (Figure 5b), and the amount of local labeled"}, {"title": "Discussion", "content": null}, {"title": "The value of local data for model training, fine-tuning, and evaluation", "content": "In response to our first key research question, we find that locally collected reference data remain pivotal for generating the best possible local maps of tree canopy height (TCH) in our study region. Our first set of model comparisons (Section 5.1) examined the performance dif- ferences of models trained only with locally collected labels derived from UAV-LiDAR, versus using globally-available TCH maps derived from global (but sparse and noisy) GEDI data. While the existing global TCH maps have the benefit of being ready to use \"off the shelf\" without download- ing any satellite imagery, training new models, or collecting local training data, their performance in the Karangani Game Reserve area was significantly worse than what we achieved with a small FCN model trained with only a limited amount of local, high-fidelity labeled data. This finding holds up when evaluating performance across ecologically relevant strata. Considering the potential downstream task of using TCH maps to estimate aboveground biomass, the 1m resolution Meta map [Tolan et al., 2024] performed on par with our local model, while the other existing maps performed worse in this use case. This raises the importance of validating global maps for local scientific analyses, even when those maps will be calibrated for local trends.\nThe intermediate performance of the locally trained pixel-wise random forest (with no spatial context) in Table 1 suggests that some, but not all, of the performance gap between models opti- mized for global use and models trained or fine-tuned for local use may be due to the calibration to local label distributions. Our second set of experiments (Section 5.2) shows the value of local data for fine-tuning globally pre-initialized models for use in a specific region. While the best overall models we trained only used local data, we still find that models initialized with weights from global training procedures can be substantially improved by using local data for fine-tuning (note, for example, the improvement in performance of the locally fine-tuned XceptionS2 model over the published XceptionS2 global map from Lang et al. [2023] in Table 1). This result underscores the potential for locally collected data to improve the regional performance of globally pretrained models, even as the usefulness of pretrained models for local use cases advances.\nLastly, we emphasize the importance of the locally collected data for evaluating the performance of geospatial ML models. Despite an increase in high-quality local and regional TCH maps being released recently (mostly in the Global North, e.g., Fogel et al. [2024], National Ecological Observatory Network (NEON) [2024]), understanding and assessing the local performance of global TCH maps across Earth's diverse continents and biomes remains difficult due to a lack of globally-distributed test sites with high quality, locally collected TCH data. We expect that careful collection of local data will remain paramount to assessing and improving progress in geospatial machine learning for global environmental monitoring."}, {"title": "The effects of geographic-, feature-, and model-representation on local per- formance", "content": "In response to our second research question, we find that the choices regarding (i) what model architecture to use, (ii) what data layers to use as input, and (iii) where label data are collected can all have a large impact on local performance. Overall, there are substantial differences in performance across the model architectures we trained and fine-tuned with local data (Table 1). Notably, all of the models we used here have been proposed and used in prior works, and are not designed specifically for our local TCH modeling task. Thus, it is possible that local performance in areas like the Karingani Game Reserve could be further improved with modeling innovations - either by designing models specifically for local training, or by designing global models to be more suitable for downstream adaptation in smaller local areas.\nData choices similarly had a large impact on local performance. The availability of local label data was crucial for success in training and fine-tuning models for local use in the Karingani Game Reserve. The choice of which spectral bands and geographic data layers to use as input also had a significant impact on performance. In particular, the inclusion of additional spectral bands (beyond 3-channel RGB) decreased the average error of our best model by 10%. For future researchers, decisions about which imagery to use should be based not only the spectral bands available, but also on the spatial and temporal resolution of the available imagery, which can be pivotal to model success. We expect that the difference in performance due to including all spectral bands of Sentinel-2 imagery is a conservative estimate of the performance differences that would arise from using different satellite products (e.g. Landsat or Maxar satellite imagery, which have different resolutions).\nFrom our experiments in Section 5.3, we find that the performance differences due to data design decisions were similar to the performance differences due to model architecture decisions. The absolute difference in local performance (measured against our best model, which used all training sites and all Sentinel-2 input layers) is strikingly similar across the three different types of variations we tested: (i) using a model architecture from recent TCH-mapping research efforts, (ii) using only RGB spectral bands from the satellite imagery input, or (iii) using only 3 randomly selected training sites instead of 12. From these findings, we conclude that the quantity and composition of the available local training data sites affected model performance with an effect comparable to what we might expect from innovations in model architectures (as roughly reflected by the gap in performance between our best and second-best models in Figure 5a)."}, {"title": "Identifying points of conflict and synergy in local and global mapping", "content": "Our third research question concerns identifying points of potential disparity or cohesion between the goals of local and global modeling in geospatial machine learning. While global maps will clearly exhibit different errors in different locations (average performance in certain regions will differ from average global performance), we might hope that the existence of global models would make it easier to build good models for local use. However, the results of our case study underscore that currently, globally pretrained models are not necessarily the \"right\" starting point for developing good local models.\nAfter extensive experimentation, existing globally trained models did not factor in to the best local modeling solution we found in our case study. At the same time, we expect that our local models would perform terribly across the globe, due to domain shifts and lack of global diversity in our training data. This discrepancy suggests that there are at least some points of conflict between local and global mapping endeavors. We caveat that since our case study concerns a"}, {"title": "General takeaways for geospatial machine learning", "content": "In addition to addressing our three key research questions as discussed in Section 6, the results from our case study provide three concrete takeaways with implications for future efforts in geospatial machine learning at different spatial scales.\nTakeaway 1: The best global models and the best local models may differ in model design and training strategy. Our results provide evidence that geospatial machine learning models that capture global variation well as all four existing global maps of tree canopy height do might not sufficiently capture local variation in the target variable of interest without significant"}, {"title": "Conclusion", "content": "In this paper, we studied perspectives of local and global mapping with machine learning and satellite imagery, rooted in a case study of tree canopy height mapping in the ecologically diverse Karingani Game Reserve in Mozambique. Tree canopy height prediction is a setting in which both global and local maps are pivotal to inform environmental and ecological science and policy. Global maps are crucial for understanding planetary ecosystem and tree composition, while local maps are crucial for scientific studies in landscape ecology and informing locally-enacted policies. The same argument that both global and local maps are fundamentally needed can be made for virtually any environmental or ecological variable of interest, from land cover classification to air quality estimation. Consequently, we need geospatial machine learning models that can resolve both large-scale global variation and fine-grained local variation in key environmental variables.\nOur study focused on the task of tree canopy height mapping in Karingani Game Reserve, which has not been previously used to train or evaluate geospatial ML models. As such, these data enabled us to ground our study in considerations researchers and modelers face when applying geospatial machine learning for ecological and environmental modeling to a new area. As discussed in Section 6, we expect that our main findings will generalize to different local areas and to differ- ent prediction tasks due to evidence of similar local-global map discrepancies in other geospatial prediction domains. That said, we expect that the exact numbers and nuanced insights may differ from setting to setting. It would be informative to repeat our study across many local areas and similar prediction tasks in future work (made possible with a growing number of public datasets representing mapping tasks related to trees [Weinstein et al., 2021, Reiersen et al., 2022, Ouaknine et al., 2023, Ahlswede et al., 2023, Bountos et al., 2023]).\nOur results indicate that, at present, global and local mapping may constitute distinct modeling goals for geospatial machine learning. This hypothesis is supported by previous findings across tasks including cropland mapping [Kerner et al., 2024], poverty prediction [Aiken et al., 2023], population density estimation [Kuffer et al., 2022], and building detection [Gevaert et al., 2024]. Even the best global models for tree canopy height mapping, which represent impressive efforts in data processing and model architecture [Potapov et al., 2021, Lang et al., 2023, Tolan et al., 2024, Pauls et al., 2024] are outperformed by simple fully convolutional networks trained on roughly 250 square kilometers of carefully collected highly accurate label data paired with publicly available Sentinel-2 satellite imagery. Moreover, we find that some of the design decisions that past work found important for global modeling were either unnecessary or potentially harmful to local performance.\nLooking forward, there is a great opportunity to develop models that can flexibly interpolate between different prediction extents, ranging from global to hyperlocal. Our study exposes a need for future research directions that prioritize the performance of geospatial machine learning models in local areas for example models pretrained on global satellite imagery to be efficiently adapted for local use alongside global performance. Distinguishing between the goals of local and global"}]}