{"title": "A Plug-and-Play Fully On-the-Job Real-Time Reinforcement Learning Algorithm for a Direct-Drive Tandem-Wing Experiment Platforms Under Multiple Random Operating Conditions", "authors": ["Zhang Minghao", "Song Bifeng", "Yang Xiaojun", "Wang Liang"], "abstract": "Recent advancements in critical technologies have significantly enhanced the performance of biomimetic aircraft, particularly those inspired by dragonflies with hovering capabilities. The nonlinear and unstable aerodynamic interference generated by the tandem wings of such biomimetic systems poses substantial challenges for motion control, especially under multiple random operating conditions.\nTo address these challenges, the Concerto Reinforcement Learning Extension (CRL2E) algorithm has been developed. This plug-and-play, fully on-the-job, real-time reinforcement learning algorithm incorporates a novel Physics-Inspired Rule-Based Policy Composer Strategy with a Perturbation Module alongside a lightweight network optimized for real-time control.\nTo validate the performance and the rationality of the module design, experiments were conducted under six challenging operating conditions, comparing seven different algorithms. The results demonstrate that the CRL2E algorithm achieves safe and stable training within the first 500 steps, improving tracking accuracy by 14 to 66 times compared to the Soft Actor-Critic, Proximal Policy Optimization, and Twin Delayed Deep Deterministic Policy Gradient algorithms. Additionally, CRL2E significantly enhances performance under various random operating conditions, with improvements in tracking accuracy ranging from 8.3% to 60.4% compared to the Concerto Reinforcement Learning (CRL) algorithm. The convergence speed of CRL2E is 36.11% to 57.64% faster than the CRL algorithm with only the Composer Perturbation and 43.52% to 65.85% faster than the CRL algorithm when both the Composer Perturbation and Time-Interleaved Capability Perturbation are introduced, especially in conditions where the standard CRL struggles to converge. Hardware tests indicate that the optimized lightweight network structure excels in weight loading and average inference time, meeting real-time control requirements.\nThis work establishes a new benchmark for control effectiveness in direct-drive platforms influenced by tandem wing dynamics. Notably, the CRL2E algorithm provides a promising approach for mechanical systems operating under nonlinear, unsteady load conditions, enabling highly efficient plug-and-play applications across various random operating conditions.", "sections": [{"title": "1 Introduction", "content": "In recent years, advancements in servo motors, high-performance embedded processors, and 3D printing technology have significantly enhanced the performance and mission capabilities of biomimetic aircraft. Consequently, these aircraft now have broader applications. Among these, hover-capable flapping wing aircraft have emerged as a frontier in biomimetic aircraft development. Within the category of hover-capable aircraft, those inspired by dragonflies, the apex predators of the insect world, have garnered considerable attention. A quintessential example is the DDD-1 aircraft developed by the author's team, as shown in Fig. 1.\nThis aircraft faces significant challenges due to the nonlinear, unsteady aerodynamic interference caused by its tandem wings. Such interference complicates the coordinated movement of the wings and adversely affects aerodynamic performance, leading to a noticeable decline in flight capabilities. Therefore, it is imperative that a detailed aerodynamic study be conducted on a dedicated experimental platform to improve its design and performance.\nIn selecting operating conditions for the experimental platform, attention must be given to the maneuverability of biomimetic aircraft, which mimic biological dragonflies by adjusting different flapping amplitudes during flight. Additionally, the platform must consider that yaw control is relatively more challenging for direct-drive dragonfly-mimicking aircraft. As illustrated in Fig. 2, the typical open-loop flight process and flight data of direct-drive dragonfly-mimicking aircraft reveal that continuous autorotation occurs due to the lack of effective control strategies.\nThe introduced experiment platform encounters the challenges of high-frequency control in nonlinear, unsteady systems under multiple random operating conditions. Given its inherent purpose to explore the characteristics of unknown systems, these systems must face the realities of the unknown and learn during operation. This scenario can be viewed as involving unknown dynamics and unknown disturbances in an open-world context. Therefore, the controller designed for the aforementioned experimental platform, a typical mechanical system, needs to meet the following requirements to facilitate real-world application and widespread use:\n1. Plug-and-play usage under multiple random operating conditions, allowing fully on-the-job training without manual resets.\n2. High training efficiency under multiple random operating conditions, enabling the algorithm to converge quickly and perform well.\nTo address these requirements, the problem faced in this study is abstracted into a Finite-Time Single-Life learning problem for the control of mechanical systems."}, {"title": "1.1 RELATED WORK", "content": ""}, {"title": "1.1.1 Traditional Control Algorithms in Mechanical Systems", "content": "Significant research has been undertaken on controlling mechanical systems through traditional control algorithms. For instance, He Ma et al.[12] implemented a 2000Hz PID controller with manually adjusted constant parameters to manage the trajectory control of tandem-wing dragonfly-mimicking aircraft. Similarly, Xinyan Deng et al.[27] utilized an Adaptive Robust Control controller to address the trajectory tracking issues in direct-drive aircraft.\nWhile effective within certain parameters, these approaches are constrained by existing theoretical frameworks and exhibit several limitations. From a safety perspective, these methods cannot ensure robustness against unmodeled disturbances encountered during the design process. Additionally, in terms of training efficiency, developing these controllers necessitates extensive prior knowledge, simulations, theoretical data, and open-loop experimental data."}, {"title": "1.1.2 Conventional Reinforcement Learning Control Algorithms in Mechanical Systems", "content": "With the advancement of edge computing devices and high-performance training equipment, the application of reinforcement learning (RL) techniques in robotics has gained significant prominence[28, 29] RL-based controllers, such as those based on the Soft Actor-Critic (SAC) algorithm, Proximal Policy Optimization (PPO) algorithm, and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, offer a promising approach for addressing the safety and efficiency challenges inherent in traditional control methods by leveraging a data-driven approach to learning behaviors through interaction.Hoang Nguyen et al. [35] proposed an online and offline Q-learning strategy to address periodic Linear Quadratic Regulator problems involving dynamic uncertainties and time-varying models. This optimal control framework for spacecraft systems ensures tracking and optimal performance without requiring controllability conditions. However, the algorithm's reliance on operators' extensive experience limits its generalizability for unknown systems.\nKhai Nguyen et al. [36] introduced an RL-based uncertain switched-variable optimal control scheme, focusing on non-autonomous tracking error models to ensure tracking performance. The proposed formation control structure, which considers kinematic and dynamic subsystems, has demonstrated stability and optimality through theoretical proofs and simulations. Despite these strengths, the algorithm's dependence on the Hamiltonian properties of controlled objects restricts its broader applicability.\nZiping Wei et al. [37] developed an RL-based optimal trajectory tracking control for surface vessels with unknown dynamics, disturbances, and input saturation. An input saturation penalty function was designed to reduce the computational burden, combining optimal control, adaptive neural networks, and RL to ensure optimal performance. However, the inclusion of input saturation constraints for surface vessels introduces additional design challenges and specific requirements.\nShu Li et al. [38] proposed an RL-based adaptive dynamic programming controller for affine nonlinear discrete-time systems with constraints and disturbances. Inequality constraints were converted into penalty functions to ensure state and control input constraints, utilizing radial basis function neural networks for approximation and game theory for stability analysis. Nevertheless, the algorithm's performance is heavily dependent on appropriately set initial system states and control parameters, affecting convergence speed.\nThe studies mentioned above share common issues:\n1. Poor Safety: None of the algorithms provides safety guarantees against critic estimation errors, which can introduce bias into actor updates without effective mitigation mechanisms.\n2. Low Efficiency: Stability proofs only demonstrate Lyapunov stability, lacking finite-time convergence guarantees, which results in uncertain convergence times.\nIn summary, existing RL-based algorithms still require improvements in safety and efficiency, rendering them unsuitable for finite-time single-life control problems. The algorithm proposed in this paper aims to address these primary issues."}, {"title": "1.1.3 Reinforcement Learning Algorithms for Safety in Mechanical Systems Control", "content": "Ensuring system safety is paramount in finite-time single-life scenarios, particularly under Multiple Random Operating Conditions. Various research approaches have been developed to address this critical need:\nOne approach involves the use of Curriculum Learning to progressively enhance system performance and ensure exploration safety, thereby meeting single-life requirements. Yan Yin et al. proposed the Soft Actor-Critic with Curriculum Prioritization and Fuzzy Logic (SCF), which enables mobile robots to navigate efficiently in unpredictable and dynamic environments. This method ensures optimal planning control while maintaining safety and robustness. The effectiveness of SCF has been validated through experiments in both the Gazebo simulation environment and real-world scenarios. However, the adaptation of appropriate curricula at different learning stages remains challenging. Although trajectory energy was suggested as a sample complexity metric, the effective design and adjustment of these metrics to ensure safety and effectiveness remain problematic. Furthermore, the algorithm does not demonstrate suitability for multiple random operating Conditions. Thus, achieving safety throughout the process, especially during the initial stages, without introducing complex designs, is a critical functionality addressed in this paper.\nAnother approach involves using constrained Markov decision processes to ensure that the system's expected cumulative cost remains within predefined boundaries, thereby guaranteeing safety and meeting single-life requirements. Achiam et al. introduced Constrained Policy Optimization, a method that integrates constraints directly with reward functions to formulate neural network policies for high-dimensional control tasks. However, these algorithms only guarantee expected constraint satisfaction, necessitating a balance between rewards and constraints that is difficult to adjust. This often leads to cyclical policy oscillations during training. Additionally, the algorithm does not demonstrate suitability for Multiple Random Operating Conditions. Therefore, ensuring safety throughout the entire operation of the algorithm through direct and explicit means is a problem that this paper seeks to solve."}, {"title": "1.1.4 Reinforcement Learning Algorithms for Efficiency in Mechanical Systems Control", "content": "For finite-time single-life scenarios, ensuring system efficiency is also crucial, particularly under multiple random operating conditions. Various research approaches have been developed to address this need:\nOne typical approach involves enhancing the replay of critical experiences under optimized conditions to ensure sample quality and improve efficiency, thereby meeting finite-time requirements. Osband et al. proposed the Posterior Sampling for Reinforcement Learning (PSRL) algorithm, which operates on known-duration repeated events. At the start of each episode, PSRL updates the prior distribution of the Markov decision process and samples from this posterior. PSRL then follows the optimal policy for the sample during execution. Although not driven by optimism, this posterior sampling approach inspired the proposed algorithm. However, the PSRL algorithm struggles to handle potential changes in task characteristics during execution and nonlinear complex systems, limiting its applicability to the strongly nonlinear unsteady load conditions of the DDTWEP.\nAnother approach employs meta-learning for rapid adaptation to new tasks. Jathushan Rajasegaran et al. introduced the Fully Online Meta-Learning (FOML) algorithm, which conceptually maintains simplicity by only updating two parameter vectors throughout the online adaptation process: the online parameter vector q, updated with each new data batch, and the meta-parameter vector 0, updated correspondingly to accelerate online adaptation and influence online updates through regularization. Despite demonstrating significant multi-task performance, FOML's training is typically conducted in batch settings, making it unsuitable for scenarios with unclear task boundaries or gradually evolving tasks. This limitation restricts its applicability to the DDTWEP under arbitrary random conditions.\nA further approach maximizes the use of historical experiences to ensure computational efficiency, thereby meeting finite-time requirements. Neuroscience, a key reference source for AI algorithms, demonstrates humans' exceptional adaptability and learning efficiency. Human intelligence exhibits extraordinary abilities to maintain and manipulate memory and skills within activity storage. When learning new tasks, structural reorganization occurs in localized areas responsible for related skills, while other areas remain unchanged. This insight inspired the proposed algorithm, aiming to emulate human rapid learning patterns and apply them to mechanical systems control, especially under the nonlinear unsteady load conditions of the DDTWEP."}, {"title": "1.2 Closest Work and Proposed Algorithm", "content": "The closest related work to this research is the adaptive policy learning (APL) framework proposed by Lindsey Kerbel et al. This framework accelerates learning by utilizing pre-existing, often highly engineered, default powertrain control (PTC) policies shipped with vehicles as source policies. These source policies are integrated with a dynamically weighted, continuously learning reinforcement learning (RL) algorithm, which progressively surpasses the performance of the original policies. This approach introduces a novel RL paradigm that capitalizes on readily available power system data for continuous performance improvement. However, several potential issues have been identified:\n1. Limited Direct Interaction with System Dynamics: The baseline algorithm manages the majority of the system's performance, depriving the RL algorithm of direct interaction with the true system dynamics. This limitation hinders the algorithm's effectiveness in controlling complex dynamics and restricts its applicability to highly complex dynamic learning scenarios.\n2. Safety Across Conditions and Lifecycle: Although the dynamic weighting mechanism is innovative, it does not directly address safety issues arising from incorrect actions generated by the RL algorithm and the dynamic weighting mechanism. Relying on a trainable weighting network for system safety management introduces a plug-and-play safety dilemma. The initial phases are inherently unsafe, especially when encountering new tasks, making it challenging to ensure safety under all conditions and throughout the lifecycle. If significant action errors occur early in the RL process, the APL framework may also encounter safety issues. The authors acknowledge this limitation in their paper: \"The APL approach gets us closer to the possibility of online or at least on-board training with less time, provided safety risks of even the APL-limited exploration are addressed sufficiently, a topic beyond the scope of the present paper.\"\nSome studies have also explored combining RL controllers with traditional controllers to enhance system performance. For example:\n1. RL for Tuning Classical Controller Parameters or Weighted Combination of Classical Controllers and RL Controllers: This approach does not ensure the safety of the RL controller's exploration process during its initial stages, particularly in systems with strong nonlinearity.\n2. Classical Controllers for Guided Policy Search: The performance ceiling is limited by the capabilities of the classical controller, which may be inadequate in highly nonlinear environments.\nAdditionally, the Concerto Reinforcement Learning (CRL) algorithm[51] has been developed to address challenges in safety and efficiency from a finite-time, single-life perspective. CRL introduces two main innovations: a time-interleaved module based on Lipschitz conditions that integrate classical controllers with RL-based controllers to enhance initial stage safety and a policy composer based on finite-time Lyapunov convergence conditions that organize past learning experiences to ensure efficiency within finite time constraints. These modules have been validated through ablation experiments. However, CRL primarily targets constant and deterministic conditions, limiting its effectiveness under multiple random operating conditions.\nTo address these limitations, particularly under multiple random operating conditions, this paper proposes an improved policy composer tailored for such conditions and introduces a lightweight network optimized for real-time control. This development results in the CRL2E algorithm.\nThe main contributions of this work are outlined as follows:\n1. Enhanced Performance under Random Conditions: The CRL2E algorithm introduces an innovative policy composer that significantly improves performance under multiple random operating conditions, achieving an 8.3% to 60.4% enhancement in tracking accuracy. In scenarios where the standard CRL algorithm struggles to converge rapidly, CRL2E demonstrates a 36.11% to 57.64% improvement in convergence speed over the CRL+CP algorithm and a 43.52% to 65.85% improvement over the CRL+TICP+CP algorithm. Moreover, CRL2E enhances convergence speed in more complex scenarios.\n2. Safety and Stability in Initial Training Steps: The CRL2E algorithm ensures safe and stable training within the first 500 steps, achieving tracking accuracy improvements of at least 14 times and up to 66 times compared to common algorithms such as SAC, PPO, and TD3.\n3. Optimized Network Structure: A more lightweight network structure has been designed compared to the existing CRL algorithm, improving import weight, compile time and average inference time over 10,000 iterations. Various hardware configurations were tested, with real-time hardware performance demonstrated using a Rockchip RK3588 processor and Ubuntu 24.03 operating system on an Orange Pi 5 Plus.\n4. Yaw Mechanism Model Introduction: A Yaw Mechanism Model is introduced and analyzed for its reinforcement learning characteristics on direct-drive tandem-wing experiment platforms under multiple random operating conditions."}, {"title": "2 Problem Description", "content": ""}, {"title": "2.1 Control Problem Description", "content": "The problem addressed in this paper is the trajectory tracking of the DDTWEP under multiple random operating conditions. To achieve smooth, efficient, and robotics-engineering-compliant performance under these conditions, a motion strategy organized around the concept of Central Pattern Generators (CPGs) is employed to generate the desired trajectory. Equation (1) describes the expected motion of each wing:\n$\\Phi_{exp,i}^{n} = A_i \\cdot sin(2\\pi f\\cdot t + \\varphi_i)$\nwhere $\\Phi_{exp,i}^{n}$ denotes the expected flapping angle position of the i-th wing for the next n steps, $A_i$ denotes the desired flapping amplitude for the i-th wing, f represents the flapping frequency, and $\\varphi_i$ signifies the phase difference of the i-th wing. The parameters used for this study are provided in the Experimental Settings section."}, {"title": "2.2 Modeling direct-drive platform under tandem wing influence", "content": "In this study, an experimental platform similar to the aircraft shown in Fig. 1 was constructed, as depicted in Fig. 4. The experimental platform consists of 13 key components: one bench, four motors, four springs, and four wings. The rig is rigidly attached to the ground at the $\\text{O}_\text{G}$ point; the four motors and springs are interconnected and securely mounted on the bench; and the four wings are individually connected to the rig at the $\\text{O}_{w,1}$, $\\text{O}_{w,2}$, $\\text{O}_{w,3}$ and $\\text{O}_{w,4}$ points, respectively, through constraints with the motors. Due to the experimental rig's symmetry along the $\\text{X}_\text{G}\\text{Z}_\text{G}$ plane, the points $\\text{O}_{w,1}$ and $\\text{O}_{w,2}$ are symmetrical relative to the $\\text{X}_\text{G}\\text{Z}_\text{G}$ plane, as are $\\text{O}_{w,4}$ and $\\text{O}_{w,3}$.\nBased on the described experimental setup, the platform can be abstracted as a system composed of five rigid bodies with eight degrees of freedom: $\\Phi_{w,1}, \\theta_{w,1}, \\Phi_{w,2}, \\theta_{w,2}, \\Phi_{w,3}, \\theta_{w,3}, \\Phi_{w,4}, \\theta_{w,4}$. The system is driven by four motors providing torque.\nConsidering the nonlinear unsteady aerodynamic load, this setup constitutes a multi-body nonlinear underactuated system[14]."}, {"title": "2.3 Designing Reward Functions and Simulation Episodes", "content": "To facilitate the construction of a Lyapunov function for proving algorithm convergence, a weighted sum of the absolute values of tracking errors for four motors, as shown in equation (16), is employed as the reward function. The goal is to achieve a minimum value under optimal performance:\n$R_t(S_t, a_t) = \\lambda\\cdot [|E_1(S_t, a_t)| + |E_2(S_t, a_t)| + |E_3(S_t, a_t)|\n+ |E_4(S_t, a_t)|]$\nwhere $E_1(S_t, a_t)$, $E_2(S_t, a_t)$, $E_3(S_t, a_t)$, $E_4(S_t, a_t)$ denote the tracking errors of the four motors. The weighting factors $\\lambda$ are used to scale the reward within the range [0, 1] to accelerate convergence.\nIn the context of simulator episode design, since this work focuses on direct-drive systems without mechanical limits, if the wing exceeds the maximum range of -90 to 90 degrees, it indicates that the wing has crossed the symmetry plane of the experimental platform and collided with the support structure. Therefore, tracking errors exceeding 90 degrees are presumed to inflict damage on the equipment, emphasizing the importance of precision in actuator control."}, {"title": "3 Primarily", "content": ""}, {"title": "3.1 On-Policy Deterministic Actor-Critic", "content": "The deterministic actor-critic consists of two components. The critic estimates the action-value function while the actor ascends the gradient of the action-value function. Specifically, an actor adjusts the parameters ) of the deterministic policy \u03bc by gradient ascent. A differentiable action-value function $Q(w|s, a)$ in place of the true action-value function $Q(\u03bc|s, a)$ is substituted. A critic estimates the action-value function $Q(w|s, a) \u2248 Q(\u03bc|s, a)$, using an appropriate policy evaluation algorithm. For example, in the following deterministic actor-critic algorithm, the critic uses Sarsa updates to estimate the action-value function:\n$\\delta = r_k + \\gamma \\cdot Q(W_k|S_{k+1}, A_{k+1}) \u2013 Q(w_k|S_k, a_k)$\n$W_{k+1} = W_k + a_w\\cdot\\delta\\cdot\\nabla_wQ(w_k|S_k, a_k)$\n$\\theta_{k+1} = \\theta_k + a_\\theta \\cdot V_\\theta Q(W_k|S_k, \\mu(\\theta_k|S_k))$\n$\\alpha_k = \\mu(\\theta_k|S_k)$\nwhere $Q(w|s, a)$ is the differentiable action-value function with network weight parameters w, and its value at the k-th time step is $w_k$. $\\mu(\\theta|S)$ is the policy network with weight parameters \u03b8, and its value at the $k -th$ time step is $\\theta_k$. \u03b3 is the discount factor. $a_w$ is the learning rate for parameter w. $a_\\theta$ is the learning rate for parameter 0. $S_k$ is the visited state at the $k -th$ time step."}, {"title": "3.2 Q-Value Estimation Method Based on Gradient-Domain Laplace Transform", "content": "To estimate Q-values in real-time without introducing a critic and to support the design and proof of the subsequent Rule-Based Policy Composer, a Q-value estimation method based on the gradient-domain Laplace transform is defined in this paper.\nThe gradient calculation involves estimating the time-variation gradient of $Q(w|s, a)$. However, in the current finite-time single-life scenario, conventional Q estimation methods present the following issues:\n1. Finite Time Impact: Due to the finite time constraint, the critic's parameters have significant errors, especially during the early stages of training, which adversely affect the critic's estimates.\n2. Single Life Impact: Due to the single life constraint, the policy undergoes rapid changes, particularly in the initial stages. This rapid change undermines the applicability of typical critic-based methods such as TD-learning.\nTo address these issues and facilitate rapid execution on edge computing devices, as well as adapt to the fast, drastic changes in policy parameters, provide support for the policy composer $\\theta_{cp}(\\theta, t)$ during the non-gradient posterior estimation process, this work employs a gradient-domain Laplace transform that does not rely on the critic. Unlike the conventional Laplace transform in the time domain, the gradient-domain Laplace transform operates in the 0 domain, converting 0-domain functions into functions of a complex variable grad. The transformations for Q and r are defined as follows:\n$K(grad) = \\int_0^{\\infty} Q(\\theta) \\cdot e^{-s\\theta} d\\theta$\n$R(grad) = \\int_0^{\\infty} r(\\theta) \\cdot e^{-s\\theta} d\\theta$\n$grad = SP + i \\cdot DR$\nwhere SP represents the real part, indicating the influence of the current gradient sequence update in the finite-time single-life task, analogous to the decay term in the Laplace transform. DR represents the direction vector of the updated gradient, analogous to the frequency in the Laplace transform, indicating critical characteristics of the update process.\nThe gradient-domain Laplace transform shares properties similar to those of the Laplace transform. At the convergence point, for any given 0, based on the Bellman equation, the following relationship holds:\n$Q_i(\\theta) = r_i(\\theta) + \\gamma\\cdot Q_{i+1}(\\theta)$\nTaking the derivative with respect to \u03b8 on both sides:\n$\\frac{dQ_i(\\theta)}{d\\theta} = \\frac{dr_i(\\theta)}{d\\theta} + \\gamma\\cdot \\frac{dQ_{i+1}(\\theta)}{d\\theta}$\nApplying the gradient-domain Laplace transform as defined above, the following is obtained:\ng. $K_i(grad) - Q_i(0) = g \\cdot R_i(g) \u2013 r_i(0) + \\gamma \\cdot K_{i+1}(g) - Q_{i+1}(0)$\nSee the definition above, then there is:\n$K_i(grad) = K_{i+1}(grad)$\n$Q_i(0) = 0$\n$r_i(0) = 0$\n$Q_{i+1}(0) = 0$\n$K_i(grad) = \\frac{R_i(grad)}{1-\\gamma}$\nPerforming the inverse gradient Laplace transform yields the following relationship, thus establishing an approximate connection between $r_i(\\theta)$ and $Q_i(\\theta)$, Based on the gradient-domain Laplace transform, $Q_i$ values are estimated to some extent. However, a significant issue in the above estimation is that $\\gamma_i$ essentially represents the confidence distribution of $r_i(\\theta)$ with respect to $Q_i(\\theta)$. To mitigate the error in estimating $Q_i(0)$ based on $r_i(0)$:\n$Q_i(0) = \\frac{r_i(\\theta)}{1 - \\gamma_i}$\nGiven that $y_i$ is a time-varying quantity, this time-varying characteristic complicates algorithm development and rapid estimation. Therefore, it is necessary to approximate it to a relatively constant value. To achieve this, the following definitions and assumptions are proposed:\nDefinition 1: Gradient Descent Segment: Each L time steps are regarded as a segment.\nDefinition 2: Dynamic Descent Phase: A maximum of Age gradient descent segments are considered as a dynamic descent phase, denoted by g. The index g is independent of the gradient descent segment index j and time step k mentioned previously.\nAssumption 1: By selecting an appropriate L, Yi can be approximated by Ymean, the mean value of y\u012f over the entire single-life process, i.e., $Yi \u2248 Ymean$.\nEvidence: Existing research indicates that when the single-life duration is sufficiently long, the influence of the current gradient sequence update in the finite-time single-life task becomes negligible.\nBased on Assumption 1, equation (32) can be approximated as follows:\n$Q_i(0) = \\frac{r_i(\\theta)}{1 - \\gamma_i} ~ \\frac{r_i(\\theta)}{1 - \\gamma_{mean}}$\nBased on equation (33), the following inference can be made, enabling the calculation of AQ under the conditions \u03b8 + A\u03b8and S + AS.\n$\\Delta Q = Q_{m,1}(\\theta + \\Delta\\theta, S + \\Delta S) \u2013 Q_{m,0} (\\theta,S) = \\sum_{i=0}^{L_i} \\frac{1}{1 - \\gamma_i}r_i(\\theta) ~ \\frac{1}{1 - \\gamma_{mean}}\\sum_{i=0}^{L_i} r_i(\\theta)$"}, {"title": "4 The proposed algorithm: CRL2E algorithm", "content": ""}, {"title": "4.1 Architecture of the CRL2E algorithm", "content": "The architecture of the CRL2E algorithm, as illustrated in Fig. 6, is divided into three main components: On-Policy Deterministic Actor-Critic, Rule-based Policy Composer, and Time-interleaved module.\nBased on this framework, the mathematical expressions of the CRL2E's overall flow are as follows:\nif k mod 2 = 0:\n$action_k = \\mu_{classical}(S_k)$\nelse:\n$action_k = \\mu_{RL}(\\theta_k|S_k)$\n$\\delta = r_k + \\gamma \\cdot Q(W_k|S_{k+1}, A_{k+1}) \u2013 Q(W_k|S_k, a_k)$\n$W_{k+1} = W_k + a_w\\cdot\\delta\\cdot\\nabla_wQ(w_k|S_k, a_k)$\n$\\theta_{k+1} = \\theta_{CP,j,g}(\\theta, t) + \\theta_k \u2013 a_\\theta \\cdot\\nabla_\\theta Q(W_k|S_k, \\mu(\\theta_k|S_k))$\n$\\frac{dS}{dt} = \\frac{F_{explore}(t)}{\\Delta t}$\nCompared to the previously mentioned On-Policy Deterministic Actor-Critic, the following additional modifications have been made:\n1. $\\mu_{classical}(s_k)$ indicates the classical controller.\n2. $\\mu_{RL}(\\theta_k|S_k)$ indicates the reinforcement learning controller.\n3. if k mod 2 = 0, the Time-Interleaved module alternates the execution of the classical controller $\\mu_{classical}(S_k)$ and the reinforcement learning controller $\\mu_{RL}(\\theta_k|S_k)$.\n4. $\\theta_{CP,j,g}(\\theta, t)$ represents the additional policy weight introduced by the Rule-based Policy Composer in the j-th gradient descent segment of the g -th dynamic descent phase. This weight is adjusted at the frequency of the gradient descent segment.\n5. $\\theta_{init,g}$ denotes the initial network weight for the g-th dynamic descent phase. This is the initial network weight for the first segment. Subsequently, $\\theta_{init,g}$ adopts the weight $ \\theta_{cp,j}(\\theta, t)$, i.e., $ \\theta_{init,j} = \\theta_{init,j-1} +  \\theta_{cp,j}(\\theta, t)$."}, {"title": "4.2 Design of Time-Interleaved module and Guarantee of Lipschitz Conditions", "content": "The primary function of the Time-Interleaved module is to enhance overall process safety by providing Time-Interleaved Capability. This is achieved by alternating the execution of classical controllers, which have basic feedback control capabilities, with reinforcement learning controllers. This combination offers a hard-constrained approach that mitigates the adverse effects of suboptimal actions during the unfinished training and potential exploration phases of the RL algorithm, thereby ensuring Time-Interleaved Capability. The pseudo-code is presented below.\nFirst, a vector Er(t) representing the tracking error of the four wings relative to the desired commands over time is defined:\n$E_r(t) = [E_1(t), E_2(t), E_3(t), E_4(t)]$\nFor two adjacent time steps, as defined by the Time-Interleaved module (refer to Algorithm 1), which alternates between the RL controller and the classical controller, the error generated between these two time steps is:\n$E_r(t + \\Delta t) = E_r(t) + P_{Erl}(t) \\cdot \\Delta t$\n$E_r(t + 2\\cdot \\Delta t) = E_r(t + \\Delta t) \u2013 P_{D_{class}} \\cdot \\Delta t$\nHere, $P_{Erl}(t)$ represents the RL algorithm's ability to increase or decrease the error, and $P_{class}$ represents Time-Interleaved Capability, i.e., the classical control algorithm's ability to reduce the error, which remains constant.\nThe relationship can be expressed as:\n$E_r(t + 2 \\cdot \\Delta t) \u2013 E_r(t) = -P_{C_{class}} -P_{C_{class}} \\cdot \\Delta t + P_{Erl} P_{Erl}(t). \\Delta t$\nAssuming $\\Delta t^* = 2\\cdot\\Delta t$ and the maximum value of $P_{Erl(t)}$ is $P_{Erl,max}$, the following inequality is established:\n$\\overline{E_r}(t + \\Delta t^*) \u2013 \\overline{E_r}(t) = \\frac{-P_{C_{class}} + P_{Erl}(t)}{2} \\Delta t^*$\n$\\leqslant \\frac{-P_{C_{class}} + P_{Erl,max}}{2} \\Delta t^*$\nThis can be expressed in terms of Lipschitz continuity as follows:\n$\\Delta\\overline{E_r} = \\overline{E_r}(t + \\Delta t^*) \u2013 \\overline{E_r}(t) < \\lambda_{Lipschitz} \\cdot \\Delta t^*$\n$\\lambda_{Lipschitz} = \\frac{-P_{C_{class}} + P_{Erl,max}}{2}$\nTherefore, the Time-Interleaved module, with its Time-Interleaved Capability, ensures that the algorithm satisfies Lipschitz continuity. Based on the above equation, the following analysis can be made:\n1. Adjustable Range of \u2206Er: According to equation (49), the desired \u2206Er can be achieved by adjusting the relative relationship between $P_{C_{class}}$ (i.e., Time-Interleaved Capability) and $P_{Erl,max}$ to meet hard safety constraints. For example, if \u2206Er < \u2206Ermax, then PCclass must satisfy the following condition:\n$P_{C_{class}} > P_{Erl,max} \\frac{\\Delta E_{Y max}}{\\Delta t}$\n2. No Specific Requirements for Controller Types: The proof indicates that there are no specific requirements for the controller characteristics based on $P_{C_{class}}$ To achieve sufficient Time-Interleaved Capability, classical controllers such as PID controllers can be employed, and Perturbation can be introduced in their P/I/D values to meet theoretical needs.\n3. Necessity of $P_{C_{class}}$ in Highly Nonlinear Systems: In systems with strong nonlinearity, as introduced in this study, $P_{Erl,max}$ can be huge, making the introduction of $P_{C_{class}}$ necessary to prevent rapid system divergence.\n4. Critical Point for Switching $P_{C_{class}}$: When $P_{C_{class}} = P_{Erl}(t)$, it acts as a critical point for boundedness and ensures the gradual reduction of error (\u2206Er < 0). When $P_{C_{class}} < P_{Erl}(t)$, equation (47) transforms into the form of equation (51), where PEr\u0131(t) must consider the drag induced by $P_{C_{class}}$ to achieve better performance:\n$\\frac{P_{C_{class}}}{\\Delta t} + \\Delta \\overline{E_r} = P_{Erl}(t)$\nBased on the above proof, it is evident that Time-Interleaved module with Time-Interleaved Capability ensure Lipschitz continuity in the algorithm execution process, guaranteeing boundedness in the update process[65]. This boundedness improves algorithm performance through the following two pathways:\n1. Divergence Suppression During Training: By selecting an appropriate $P_{C_{class}}$, divergence caused by erroneous actions from an insufficiently trained RL algorithm in nonlinear control processes can be effectively suppressed."}, {"title": "4.3 Design of Rule-Based Policy Composer and Guarantee of Finite-Time Lyapunov Convergence Conditions", "content": "The core function of the policy composer involves introducing an additional posterior-determined weight, $\\theta_{CP,j,g} (\\theta, t)$, based on existing policy weights to enhance the algorithm's convergence speed and capability during control tasks. This section discusses the calculation method for $\\theta"}]}