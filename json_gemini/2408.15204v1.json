{"title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?", "authors": ["Kristina Gligori\u0107", "Tijana Zrnic", "Cinoo Lee", "Emmanuel J. Cand\u00e8s", "Dan Jurafsky"], "abstract": "Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce CONFIDENCE-DRIVEN INFERENCE: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of CONFIDENCE-DRIVEN INFERENCE over baselines in statistical estimation tasks across three CSS settings-text politeness, stance, and bias-reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, CONFIDENCE-DRIVEN INFERENCE can be used to estimate most standard quantities across a broad range of NLP problems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown strong zero-shot performance across tasks (Kojima et al., 2022), making them a promising tool for generating annotations, particularly when they align closely with human judgments (Ziems et al., 2024). Given this potential, LLM annotations of textual data may be effectively leveraged for statistical estimation, hypothesis testing, and theory development (Park et al., 2023), as well as informing policy decisions (Wei et al., 2023).\nComputational Social Science (CSS) research typically focuses not on the annotations themselves but on the social-science insights and conclusions they enable. Thus, understanding how LLM annotations could be used for downstream inferences is crucial in CSS. For example, stance annotations facilitate the study of linguistic differences between media affirming or denying global warming (Luo et al., 2020), while politeness annotations can help examine racial disparities in verbal interactions with law enforcement (Voigt et al., 2017), the relationship between politeness and social power (Danescu-Niculescu-Mizil et al., 2013), and politeness and gender (Newman et al., 2008). Similarly, annotating political leanings in text allows studying the bias of search engines (Robertson et al., 2018), social media (Ribeiro et al., 2018), and political discourse (Sim et al., 2013). Precise statistical estimation, such as prevalence or regression coefficient estimation, is essential for drawing valid conclusions in such studies.\nHowever, whether LLM annotations can be effectively leveraged without compromising the validity of statistical estimation remains uncertain. LLMs exhibit demographic biases (Weidinger et al., 2022; Cheng et al., 2023) and may lack factual accuracy (Gunjal et al., 2024; Li et al., 2023b) and consistency (Sclar et al., 2023; Atreja et al., 2024). Given these limitations, using LLMs without caution may lead to inaccurate conclusions and potential societal harms, especially when such conclusions influence policy or have tangible impacts on peoples' outcomes (Landers and Behrend, 2023). A potential solution is to rely solely on human annotations; however, human annotations are costly. Here, we present CONFIDENCE-DRIVEN INFERENCE, a method for valid statistical inference us-"}, {"title": "2 Background", "content": "2.1 LLMs for Data Annotation Tasks\nLLMs have shown great potential in handling text-annotation tasks without prior task-specific training, sometimes even outperforming crowd work-ers (Gilardi et al., 2023). NLP, LLMs offer transformative opportunities for any discipline that relies on text as data. Fields such as psychology, political science, sociology, communications, and economics recognize this emerging technology's potential to enhance simulation-based research (Bail, 2024), and facilitate tasks such as text analysis, concept induction (Lam et al., 2024), and topic modeling (Pham et al., 2024).\nHowever, despite their promise, limited research has explored how to harness the potential of LLMs in ways that are both cost-effective and statistically reliable. Our work addresses this gap.\n2.2 Collaborative Annotation Paradigms\nMuch of past work frames human and LLM annotations as competing alternatives, with a focus on determining which is superior (Thapa et al., 2023). More recent work increasingly calls for a collaborative approach that leverages the complementary strengths of both (Allen et al., 1999). These collaborative paradigms aim to balance annotation quality and cost by combining human expertise and LLM efficiency (Li et al., 2023c; Kim et al., 2024).\nIn the spirit of these collaborative paradigms, our work uses LLM confidence to efficiently and cost-effectively allocate annotation tasks, while also ensuring that the statistical inferences derived from the annotated data are valid.\n2.3 Valid Statistical Inferences in NLP\nStatistical inference is vital in NLP research. For example, model evaluation requires determining whether a model performs better than a baseline (Card et al., 2020), which in turn relies on making valid conclusions about whether one is observing meaningful model improvements or noise (Dodge et al., 2019). Chatzi et al. (2024) and Boyeau et al. (2024) leverage prediction-powered"}, {"title": "3 Methods", "content": "3.1 Problem Setup\nWe have a text corpus consisting of $n$ independent and identically distributed (i.i.d.) instances $T_1, ..., T_n$. We wish to estimate a quantity of interest $\\theta^*$, such as the prevalence of political bias in the corpus or the causal effect of using certain linguistic markers on the perceived sentiment. To perform the estimation, we require human annotations $H_1, ..., H_n$ corresponding to $T_1, ..., T_n$. For example, $H_i$ might indicate whether $T_i$ contains political bias, or assess the perceived politeness of $T_i$. In addition to human annotations, we may also have other readily-available information about $T_i$\u2014covariates $X_i$ such as the source of $T_i$ or indicators of whether $T_i$ contains certain linguistic markers, computed via a lexicon. Note that $X_i$ is available automatically, without needing human annotation. We use the short-hand notation $T = (T_1, ..., T_n)$ and define $X$ and $H$ similarly. The quantity $\\theta^*$ can be estimated via an estimator $\\hat{\\theta}(X, H)$, which we will denote by $\\hat{\\theta}$ for short. The accuracy of $\\hat{\\theta}$ improves as the number of samples $n$ increases ($\\hat{\\theta}$ recovers $\\theta^*$ as $n$ approaches infinity). We assume that $\\hat{\\theta}$ is an M-estimator (Van der Vaart, 2000), meaning it can be written as\n$$\\hat{\\theta} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\ell_{\\theta}(X_i, H_i),$$(1)\nfor a loss function $\\ell_{\\theta}$ that is convex in $\\theta$. Important special cases include the mean label, $\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n H_i$, and linear regression coefficients, which are pervasive in CSS. Other examples include quantiles, logistic, and other regression coefficients. Notice that in some cases, like calculating the mean, the loss function only depends on $H_i$.\nOur goal is to produce an estimate of $\\theta^*$ with uncertainty\u2014by providing a confidence interval at a pre-specified level $(1 - \\alpha)$\u2014with limited access to human annotations. Specifically, we can only collect $n_{\\text{human}} \\ll n$ annotations (on average). This means that the \u201cideal estimate\u201d (1) is out of reach.\nTo supplement the costly human annotations, we assume access to LLM annotations $\\hat{H}_i$ for all $n$ instances. However, we make no assumption that the LLM annotations are good: we want to produce a valid confidence interval no matter the quality of the LLM, though we anticipate better gains when their quality is high (i.e., lower mean squared error and a smaller confidence interval).\n3.2 CONFIDENCE-DRIVEN INFERENCE\nWe combine LLM annotations with strategically chosen human annotations to produce an unbiased estimate $\\hat{\\theta}_{\\text{conf}}$ that lends itself to a confidence interval that is both valid and tight around $\\theta^*$. In particular, in the large-sample limit, the mean of the estimate is exactly $\\theta^*$, no matter how biased the LLM annotations are.\nWe first explain how to choose the set of instances to be human-annotated, which is crucial for producing an accurate estimate. We collect a human annotation $H_i$ for instance $T_i$ with probability $\\pi_i$. We let $\\xi_i = 1\\{H_i \\text{ collected}\\}$ denote the indicator of whether $T_i$ has been human-annotated. Zrnic and Cand\u00e8s (2024) show that the optimal choice of $\\pi_i$ is to sample according to the uncertainty of the predicted annotation; roughly speaking, for most"}, {"title": "4 Results", "content": "We evaluate our approach on a set of CSS problems that rely on statistical estimation. We aim to include settings that (1) allow addressing important downstream social-science questions, (2) rely on a human-labeled corpus of text instances (possibly with relevant additional covariates), and (3) have a publicly available dataset. We selected three settings that meet these criteria\u2014politeness, stance, and political bias. For stance and politeness, we leverage publicly available datasets and the corresponding human annotations in their entirety. Given the large size, for political leaning, we randomly sample a smaller subset of texts.\n4.1 Estimation tasks\nPoliteness. Texts from online requests posted on Stack Exchange and Wikipedia (n = 5, 480) can be seen as polite or impolite. Politeness annotations help understand how linguistic devices impact perceived politeness (Danescu-Niculescu-Mizil et al., 2013). In this estimation task, $\\theta^*$ corresponds to the logistic regression coefficient $\\beta_{\\text{hedge}}$ measuring the impact of a linguistic feature such as hedging on the perceived politeness, $logit(P(H_{\\text{polite}} = 1|X_{\\text{hedge}})) = \\beta_0 + \\beta_{\\text{hedge}}X_{\\text{hedge}}$, where $X_{\\text{hedge}} = 1$ indicates the presence of the hedge marker and $H_{\\text{polite}} = 1$ indicates annotation as polite. We similarly estimate $\\beta_{\\text{1pp}}$, the impact of the use of the first person plural pronouns on the perceived politeness.\nStance. News headlines (n = 2, 300) are agreeing, neutral, or disagreeing with the stance that global warming is a serious concern (Luo et al., 2020). Stance annotations facilitate the study of linguistic differences between media support-"}, {"title": "5 Discussion", "content": "In this work, we introduce CONFIDENCE-DRIVEN INFERENCE, a method that integrates verbalized confidence of LLMs with active inference to optimally combine human and LLM annotations. Across three distinct CSS settings, results demonstrate that the proposed method consistently outperforms baseline methods (human-only and non-adaptive approaches) in effective sample size. Moreover, the increase in the effective sample size is achieved without a decrease in coverage. In contrast, the LLM-only approach yields invalid estimates and considerably lower coverage.\nThus, CONFIDENCE-DRIVEN INFERENCE allows for researchers to allocate human and LLM annotations in a cost-effective manner while maintaining confidence in the statistical validity of their results. Furthermore, CONFIDENCE-DRIVEN INFERENCE also addresses the challenges posed by the variable quality of LLM annotation, by providing validity guarantees when leveraging imperfect LLM annotations.\nAlthough overall LLM annotations moderately agree with human annotations in the tested settings, relying on LLM annotations only can lead to wrong"}, {"title": "6 Limitations", "content": "The external validity of our findings is contingent upon two key assumptions: that the text instances are i.i.d. from a relevant distribution, and that the researcher has full control of the annotation process. The first may be violated if the distribution of texts shifts over time, and the collected instances are no longer representative of the current quantity of interest. For example, it is possible that relationships between linguistic devices and perceived politeness evolve over time. The second assumption may be violated in situations where certain annotations are difficult to obtain (e.g., for low-resource languages). Our approach may lead to inaccurate or misleading conclusions under either violation. We thus caution against generalizing to settings where text instances exhibit time-varying shifts or the researcher is not in control over the data collection process.\nIf the adaptive sampling probabilities $\\pi_i$ are poorly chosen\u2014potentially due to inaccurate verbalized confidence scores\u2014the resulting estimates could have a higher mean squared error (MSE) than if uniform, non-adaptive sampling were used. This could even result in an estimate with a larger MSE than the human-only baseline (for sensitivity to miscalibration, see App B.2). However, by using power tuning, as detailed in Section 3.2, we ensure that incorporating LLM annotation into the estimation process does not hurt the estimate (i.e., does not increase the MSE) regardless of the sampling method used for human annotations (whether uniform or adaptive). That said, if the confidence"}, {"title": "7 Ethical Implications", "content": "Our work assumes that the existing human annotations within the leveraged datasets serve as the gold standard. However, we caution against interpreting human annotations as definitive judgments, given the subjective nature of many tasks (Fleisig et al., 2023), the potential for annotator disagreement (Weerasooriya et al., 2023), and the influence of annotator positionality (Santy et al., 2023), beliefs, biases (Sap et al., 2022), as well as variance in cultural (Huang and Yang, 2023) and social norms (Ziems et al., 2023).\nIn addition to their use in text analysis, LLMs may hold potential for simulating human behavior in social science research, including applications"}, {"title": "A Further Details on the Method", "content": "A.1 Confidence Intervals\nWe compute the confidence intervals following the approach in (Zrnic and Cand\u00e8s, 2024). Suppose that $\\hat{\\theta}_{\\text{conf}}$ is possibly d-dimensional (such as in, for example, linear or logistic regression), and we are interested in coefficient j. If d = 1, such as in the case of prevalence estimation, then j is always equal to 1. We compute the confidence interval as:\n$$C_{1-\\alpha} = \\left(\\hat{\\theta}_{\\text{conf}}^j \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\Sigma_{jj}}{n}}\\right),$$\nwhere $z_{1-\\alpha/2}$ is the $1-\\alpha/2$ quantile of the standard normal distribution. The matrix $\\Sigma$ is an estimate of the covariance of $\\hat{\\theta}_{\\text{conf}}$, given by:\n$$\\Sigma = \\hat{H}^{-1}\\text{Var}\\left[ \\lambda \\nabla \\hat{\\ell}_{\\theta^{\\text{conf}}} + (\\nabla \\ell_{\\theta^{\\text{conf}}} - \\lambda \\nabla \\hat{\\ell}_{\\theta^{\\text{conf}}}) \\frac{\\xi}{\\pi} \\right] \\hat{H}^{-1},$$\nwhere $\\hat{H} = \\mathbb{E}^\\wedge[\\nabla^2 \\ell_{\\theta^{\\text{conf}}}]$ is the empirical estimate of the Hessian at $\\hat{\\theta}_{\\text{conf}}$ and Var denotes the empirical variance. Recall also the short-hand notation $\\ell_{\\theta} = \\ell_{\\theta}(X, H)$ and $\\hat{\\ell}_{\\theta} = \\ell_{\\theta}(X, \\hat{H})$. This is a generalization of the usual \u201csandwich\u201d covariance used in linear regression.\nSome estimation targets, such as the odds ratio, are not M-estimators but are functions of M-estimators. In those cases a confidence interval is obtained by additionally applying the delta method. See (Zrnic and Cand\u00e8s, 2024) for further details.\nA.2 Power Tuning\nPower tuning, introduced by Angelopoulos et al. (2023b), refers to choosing $\\lambda$ so that the MSE of $\\hat{\\theta}_{\\text{conf}}$, or equivalently its variance, is minimized over $\\lambda$. Since $\\Sigma_{jj}$ is a quadratic in $\\lambda$, the optimal $\\lambda$ has a closed-form analytical expression. As before, suppose we are interesting in estimating coordinate j of $\\hat{\\theta}_{\\text{conf}}$. Let $h_j$ denote the j-th column of $\\hat{H}^{-1}$. Then, we set $\\lambda$ according to:\n$$\\lambda = \\frac{h^\\top \\text{Cov} h}{2h^\\top \\text{Var} h},$$\nwhere $\\text{Cov} := \\text{Cov}( \\nabla \\hat{\\ell}_{\\theta^{\\text{conf}}}( \\frac{\\xi}{\\pi} - 1), \\nabla \\ell_{\\theta^{\\text{conf}}} \\frac{\\xi}{\\pi}) + \\text{Cov}( \\nabla \\ell_{\\theta^{\\text{conf}}} \\frac{\\xi}{\\pi}, \\nabla \\hat{\\ell}_{\\theta^{\\text{conf}}}( \\frac{\\xi}{\\pi} - 1))$ and $\\text{Var} := \\text{Var}( \\nabla \\hat{\\ell}_{\\theta^{\\text{conf}}}( \\frac{\\xi}{\\pi} - 1))$ are empirical (co)variances. See (Angelopoulos et al., 2023b) for further details."}]}