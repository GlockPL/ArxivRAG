{"title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?", "authors": ["Kristina Gligori\u0107", "Tijana Zrnic", "Cinoo Lee", "Emmanuel J. Cand\u00e8s", "Dan Jurafsky"], "abstract": "Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce CONFIDENCE-DRIVEN INFERENCE: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of CONFIDENCE-DRIVEN INFERENCE over baselines in statistical estimation tasks across three CSS settings-text politeness, stance, and bias-reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, CONFIDENCE-DRIVEN INFERENCE can be used to estimate most standard quantities across a broad range of NLP problems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown strong zero-shot performance across tasks (Kojima et al., 2022), making them a promising tool for generating annotations, particularly when they align closely with human judgments (Ziems et al., 2024). Given this potential, LLM annotations of textual data may be effectively leveraged for statistical estimation, hypothesis testing, and theory development (Park et al., 2023), as well as informing policy decisions (Wei et al., 2023).\nComputational Social Science (CSS) research typically focuses not on the annotations themselves but on the social-science insights and conclusions they enable. Thus, understanding how LLM annotations could be used for downstream inferences is crucial in CSS. For example, stance annotations facilitate the study of linguistic differences between media affirming or denying global warming (Luo et al., 2020), while politeness annotations can help examine racial disparities in verbal interactions with law enforcement (Voigt et al., 2017), the relationship between politeness and social power (Danescu-Niculescu-Mizil et al., 2013), and politeness and gender (Newman et al., 2008). Similarly, annotating political leanings in text allows studying the bias of search engines (Robertson et al., 2018), social media (Ribeiro et al., 2018), and political discourse (Sim et al., 2013). Precise statistical estimation, such as prevalence or regression coefficient estimation, is essential for drawing valid conclusions in such studies.\nHowever, whether LLM annotations can be effectively leveraged without compromising the validity of statistical estimation remains uncertain. LLMs exhibit demographic biases (Weidinger et al., 2022; Cheng et al., 2023) and may lack factual accuracy (Gunjal et al., 2024; Li et al., 2023b) and consistency (Sclar et al., 2023; Atreja et al., 2024). Given these limitations, using LLMs without caution may lead to inaccurate conclusions and potential societal harms, especially when such conclusions influence policy or have tangible impacts on peoples' outcomes (Landers and Behrend, 2023). A potential solution is to rely solely on human annotations; however, human annotations are costly. Here, we present CONFIDENCE-DRIVEN INFERENCE, a method for valid statistical inference us-"}, {"title": "2 Background", "content": "2.1 LLMs for Data Annotation Tasks\nLLMs have shown great potential in handling text-annotation tasks without prior task-specific training, sometimes even outperforming crowd work-ers (Gilardi et al., 2023). NLP, LLMs offer transformative opportunities for any discipline that relies on text as data. Fields such as psychology, political science, sociology, communications, and economics recognize this emerging technology's potential to enhance simulation-based research (Bail, 2024), and facilitate tasks such as text analysis, concept induction (Lam et al., 2024), and topic modeling (Pham et al., 2024).\nHowever, despite their promise, limited research has explored how to harness the potential of LLMs in ways that are both cost-effective and statistically reliable. Our work addresses this gap.\n2.2 Collaborative Annotation Paradigms\nMuch of past work frames human and LLM annotations as competing alternatives, with a focus on determining which is superior (Thapa et al., 2023). More recent work increasingly calls for a collaborative approach that leverages the complementary strengths of both (Allen et al., 1999). These collaborative paradigms aim to balance annotation quality and cost by combining human expertise and LLM efficiency (Li et al., 2023c; Kim et al., 2024).\nIn the spirit of these collaborative paradigms, our work uses LLM confidence to efficiently and cost-effectively allocate annotation tasks, while also ensuring that the statistical inferences derived from the annotated data are valid.\n2.3 Valid Statistical Inferences in NLP\nStatistical inference is vital in NLP research. For example, model evaluation requires determining whether a model performs better than a baseline (Card et al., 2020), which in turn relies on making valid conclusions about whether one is observing meaningful model improvements or noise (Dodge et al., 2019). Chatzi et al. (2024) and Boyeau et al. (2024) leverage prediction-powered"}, {"title": "3 Methods", "content": "3.1 Problem Setup\nWe have a text corpus consisting of n independent and identically distributed (i.i.d.) instances $T_1,..., T_n$. We wish to estimate a quantity of interest $\\theta^*$, such as the prevalence of political bias in the corpus or the causal effect of using certain linguistic markers on the perceived sentiment. To perform the estimation, we require human annotations $H_1,..., H_n$ corresponding to $T_1,...,T_n$. For example, $H_i$ might indicate whether $T_i$ contains political bias, or assess the perceived politeness of $T_i$. In addition to human annotations, we may also have other readily-available information about $T_i$-covariates $X_i$ such as the source of $T_i$ or indicators of whether $T_i$ contains certain linguistic markers, computed via a lexicon. Note that $X_i$ is available automatically, without needing hu-man annotation. We use the short-hand notation $T = (T_1, ..., T_n)$ and define $X$ and $H$ similarly.\nThe quantity $\\theta^*$ can be estimated via an estimator $\\hat{\\theta}(X, H)$, which we will denote by $\\hat{\\theta}$ for short. The accuracy of $\\hat{\\theta}$ improves as the number of samples n increases ($\\hat{\\theta}$ recovers $\\theta^*$ as n approaches infinity). We assume that $\\hat{\\theta}$ is an M-estimator (Van der Vaart, 2000), meaning it can be written as\n$\\begin{equation} \\hat{\\theta} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} l_{\\theta}(X_i, H_i), \\tag{1} \\end{equation}$\nfor a loss function $l_{\\theta}$ that is convex in $\\theta$. Important special cases include the mean label, $\\theta = \\sum_{i=1}^{n} H_i$, and linear regression coefficients, which are pervasive in CSS. Other examples include quantiles, logistic, and other regression coefficients. Notice that in some cases, like calculating the mean, the loss function only depends on $H_i$.\nOur goal is to produce an estimate of $\\theta^*$ with uncertainty-by providing a confidence interval at a pre-specified level $(1 - \\alpha)$-with limited access to human annotations. Specifically, we can only collect $n_{human} \\ll n$ annotations (on average). This means that the \"ideal estimate\" (1) is out of reach.\nTo supplement the costly human annotations, we assume access to LLM annotations $\\hat{H}_{i}$ for all n instances. However, we make no assumption that the LLM annotations are good: we want to produce a valid confidence interval no matter the quality of the LLM, though we anticipate better gains when their quality is high (i.e., lower mean squared error and a smaller confidence interval).\n3.2 CONFIDENCE-DRIVEN INFERENCE\nWe combine LLM annotations with strategically chosen human annotations to produce an unbiased estimate $\\hat{\\theta}^{conf}$ that lends itself to a confidence interval that is both valid and tight around $\\theta^*$. In particular, in the large-sample limit, the mean of the estimate is exactly $\\theta^*$, no matter how biased the LLM annotations are.\nWe first explain how to choose the set of instances to be human-annotated, which is crucial for producing an accurate estimate. We collect a human annotation $H_i$ for instance $T_i$ with probability $\\pi_i$. We let $\\xi_i = 1\\{H_i collected\\}$ denote the indicator of whether $T_i$ has been human-annotated. Zrnic and Cand\u00e8s (2024) show that the optimal choice of $\\pi_i$ is to sample according to the uncertainty of the predicted annotation; roughly speaking, for most"}, {"title": "3.3 Baselines", "content": "Human + LLM (non-adaptive). The first baseline incorporates LLM annotations but does not adapt to the per-instance confidence or accuracy of the LLM-it equally trusts all LLM annotations. In particular, this baseline is a special case of $\\hat{\\theta}^{conf}$ with $\\lambda = 1$ and uniform sampling probabilities $\\pi_i = \\frac{n_{human}}{n}$. This is the method evaluated and studied by Egami et al. (2024).\nHuman only. The second baseline ignores LLM annotations and simply applies the standard estimator to human annotations. It collects each human annotation with equal probability, $\\pi_i = \\frac{n_{human}}{n}$, so that $n_{human}$ annotations are collected on average. This is the \"classical\" approach, and it can be thought of as erring on the side of caution and ignoring potentially biased LLM outputs. Since the baseline only collects human annotations, it allows forming a valid confidence interval via classical statistics. This approach is equivalent to $\\hat{\\theta}^{conf}$ with $\\lambda = 0$."}]}