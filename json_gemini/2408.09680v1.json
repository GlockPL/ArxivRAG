{"title": "MambaLoc: Efficient Camera Localisation via State Space Model", "authors": ["Jialu Wang", "Kaichen Zhou", "Andrew Markham", "Niki Trigoni"], "abstract": "Location information is pivotal for the automation and intelligence of terminal devices and edge-cloud IoT systems, such as autonomous vehicles and augmented reality, while achieving reliable positioning across diverse IoT applications remains challenging due to significant training costs and the necessity of densely collected data. To tackle these issues, we have innovatively applied the selective state space (SSM) model to visual localization, introducing a new model named MambaLoc. The proposed model demonstrates exceptional training efficiency by capitalizing on the SSM model's strengths in efficient feature extraction, rapid computation, and memory optimization, and it further ensures robustness in sparse data environments due to its parameter sparsity. Additionally, we propose the Global Information Selector (GIS), which leverages selective SSM to implicitly achieve the efficient global feature extraction capabilities of Non-local Neural Networks. This design leverages the computational efficiency of the SSM model alongside the Non-local Neural Networks' capacity to capture long-range dependencies with minimal layers. Consequently, the GIS enables effective global information capture while significantly accelerating convergence. Our extensive experimental validation using public indoor and outdoor datasets first demonstrates our model's effectiveness, followed by evidence of our GIS's versatility with various existing localization models. For instance, in our experiment with the Heads Scene of the 7Scenes dataset [50], MambaLoc used just 0.05% of the training set and achieved accuracy comparable to the leading methods in just 22.8 seconds. We also provide a demo of MambaLoc deployed on end-user devices. These results highlight MambaLoc's potential as a robust and efficient solution for visual localization in edge-cloud IoT and terminal devices, significantly enhancing the commercial viability of deep neural networks for camera localization. Our code and models are publicly available to support further research and development in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning-based camera localization employs neural networks to implicitly learn a map of a given scene, allowing for the estimation of 6-DoF of camera pose (i.e., 6 degrees of freedom, including three translations and three rotations) for a moving camera based on captured images. It is fundamental and crucial for the automation and intelligent systems in Internet of Things (IoT), such as autonomous vehicles, emergency response devices, augmented reality, domestic robot navigation, and intelligent decision-making robots [1], etc. This problem was initially solved as a place recognition problem [34]\u2013[39] which estimate the pose of the query image by retrieving the most similar images from the database. However, the robustness of these retrieval-based [40] methods are limited by its hand-crafted local features [41]. To further improve the estimation accuracy, structure based methods proposed to recover the camera pose by finding correspondence between query images and the 3D scene model. Although these methods achieves state-of-art (SOTA) accuracy [42], they are resource-intensive and requires pre-obtained SfM models or highly accurate depth information, which is not universally available in lightweight customer devices such as laptops and smartphones.\nRecent developments in deep neural networks (DNN) for image processing have facilitated the emergence of absolute pose regression (APR) methods, capable of directly predicting camera poses from images via a singular DNN [28], [48], [56]. Despite being less accurate than structure-based methods, APR-based methods are lightweight, fast, and require no additional data during inference. To better explore the application of localization methods in terminal devices and edge IoT systems, this study focuses on such localization methods. Nonetheless, these methods encounter two principal obstacles: i) Trade-off Between Accuracy and Efficiency: Most model development has focused on CNN-based or Transformer-based designs. CNNs are effective at extracting local features but often sacrifice resolution, while Transformers offer a global perspective but increase computational demands. Existing methods typically require hours [25] or even a day [15], [30] to train a single scene. This trade-off between accuracy and efficiency remains a significant challenge; ii) Lack of Robustness to Sparse Viewpoints: These data-driven methods heavily rely on dense training samples, limiting their practical applications, such as requiring low data exchange costs for cloud-edge integrated IoT localization or using sparse keyframes for loop detection in SLAM on end-user devices.\nRecently, state space models such as Mamba [12] have demonstrated substantial global modeling capabilities across diverse domains, including language modeling and computer vision [49]. These models are particularly notable for reducing the time complexity of global information extraction to O(N), marking a significant stride in computational efficiency. Inspired by this, our study explores the potential of state space models in visual localization and introduces a highly efficient model named MambaLoc."}, {"title": "II. RELATED WORK", "content": "Visual localization involves constructing a scene representation from a set of mapping images and their corresponding poses within a common coordinate system. Given a query image, the objective is to estimate its pose (position and orientation) relative to the scene. Existing solutions to this problem can be broadly categorized into two primary approaches: structure-based techniques and Absolute Pose Regression (APR) techniques.\nStructure-Based Techniques rely on geometric methods to establish correspondences between images and the scene map. These methods achieve state-of-the-art accuracy [42], but they are resource-intensive, requiring pre-computed Structure-from-Motion (SfM) models or highly accurate depth information. Such dependencies make these methods less practical for lightweight consumer devices, such as laptops and smartphones.\nAbsolute Pose Regression (APR) techniques take a different approach by leveraging neural networks to directly predict the absolute position and orientation of an image, bypassing the need for explicit matching between the 2D image and the 3D scene map. The pioneering work in this domain, PoseNet, introduced by Kendall et al. [28], employs a feed-forward neural network to predict a 7-dimensional pose vector for each input image. Subsequent research has introduced several architectural innovations, including hourglass networks [17], bifurcated translation and rotation regression [18], [19], attention mechanisms [25], [47], and LSTM layers [43]. Efforts to further enhance APR performance have explored diverse supervision techniques, such as geometric loss [44], [57], relative pose constraints [46], uncertainty modeling [45], and sequential formulations like temporal filtering [47] and multitasking [25]. While APR methods generally offer lower accuracy compared to structure-based approaches, they are advantageous for being lightweight, fast, and not requiring additional data during inference.\nTo address overfitting in APR methods, recent research has explored novel view synthesis techniques that generate large volumes of training data [13]\u2013[15], [20], [58]. However, generating high-quality synthetic data is both time-consuming and resource-intensive, with significant memory and data exchange costs, rendering these methods less suitable for deployment on edge or terminal devices.\nGiven the increasing relevance of edge computing and the constraints of terminal devices, this study focuses on optimizing APR methods to make them more efficient and practical for use in such environments.\nState space models have been proposed as effective tools for sequence modeling. The Structured State-Space Sequence (S4) model [54] offers a novel alternative to CNNs and Transformers for capturing long-range dependencies, with the added benefit of linear scalability in sequence length, prompting further research. Building on this, [9] introduces the new S5 layer, incorporating MIMO SSM and efficient parallel scanning into the S4 layer. Similarly, [51] develops the H3 SSM layer, significantly closing the performance gap between SSMs and Transformer attention in language modeling. [11] enhances the S4 model with the Gated State Space layer by adding more gating units to improve expressivity. Recently, [12] presents a data-dependent SSM layer and constructs Mamba, a generic language model backbone that outperforms Transformers at various sizes on large-scale real-world data while maintaining linear scaling in sequence length. Inspired by Mamba's success, our work focuses on applying these advancements to camera localization, aiming to create an efficient and high-performance generic camera localization backbone."}, {"title": "III. METHODOLOGY", "content": "The goal of MambaLoc is to utilizes an advanced selective state space model (SSM), specifically Mamba, to implicitly replicate the efficient global feature extraction capabilities of Non-local Neural Networks. This integration allows the model to efficiently capture global information while significantly improving training efficiency. Ultimately, MambaLoc can be deployed on Edge-Cloud Collaborative IoT and Terminal Devices, greatly enhancing the commercial viability of deep neural networks for camera localization.\nThis section begins with a description of the preliminaries of Sequence Modeling for Camera Localization, particularly focusing on Mamba. It proceeds with a comprehensive overview of MambaLoc, outlining its architectural design. Subsequently, we elucidate the integration of Non-local Neural Network concepts with Mamba in our Global Information Selector (GIS). Finally, we discuss the challenges encountered when decoupling the localization model from cloud-based systems and detail our deployment strategies for MambaLoc on edge and terminal devices."}, {"title": "A. Preliminaries: Sequence Modeling for Camera localization", "content": "In this paragraph, we review existing deep learning-based camera localization models from the perspective of sequence modeling. While RNNs possess the capability to handle sequences, they encounter difficulties with gradient vanishing when processing long sequences. LSTM, on the other hand, excels in capturing long-term dependencies in data [43]. CNNs, compared to LSTM, are more effective at capturing local features and offer stronger parallel computing capabilities [28]. Transformers, known for their robust parallel computation and ability to handle long-range dependencies across various sequence tasks, face challenges with long sequences due to their quadratic time complexity relative to sequence length [48].\nRecently, Albert Gu et al. introduced Mamba [12], a novel approach leveraging a selective state-space model architecture to overcome the limitations of traditional methods, thereby enhancing performance, efficiency, and scalability in academic contexts. Inspired by continuous systems, structured state space sequence models (SSMs) such as S4 [54] and Mamba map a one-dimensional function or sequence $x(t) \\in R$ to $y(t) \\in R$ through a hidden state $h(t) \\in R^N$ (see Equations 1 and 2). In this framework, $A \\in R^{N \\times N}$ serves as the evolution parameter, while $B \\in R^{N \\times 1}$ and $C \\in R^{1 \\times N}$ function as the projection parameters.\n$h'(t) = Ah(t) + Bx(t), (1)$ $y(t) = Ch(t). (2)$ The S4 and Mamba models act as discrete versions of continuous systems by employing a timescale parameter \u0394 to convert the continuous parameters A and B into their discrete equivalents, A and B. A commonly used transformation method is the zero-order hold (ZOH), defined by the following expressions:\n$\\bar{A} = exp(\\Delta A), (3)$ $\\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B. (4)$"}, {"title": "B. Architecture of MambaLoc", "content": "MambaLoc consists of a shared convolutional backbone and two branches for regressing the camera's position and orientation separately. Each branch includes an independent Transformer-Encoder, a Global Information Selector (GIS), and an MLP head. By using separate GIS in each branch, we can extract and compress different global features, thereby adapting to various learning tasks. The overall structure of the MambaLoc model is shown in Figure 1\nGiven an image $I \\in R^{H \\times W \\times C'}$, we employ the CNN Backbone at two different resolutions to generate activation maps $M_x \\in R^{H \\times W \\times C_m}$ and $M_q \\in R^{H \\times W \\times C_m}$ for the tasks of position and orientation regression, respectively. To achieve positional encoding at the same depth, as suggested in [52], [53], we apply a 1 \u00d7 1 convolution to linearly transform each activation map into a unified high-level depth dimension ($C_t = 256$). Since the subsequent Transformer-encoder requires a sequence as input, we reshape the spatial dimensions of $M\\in R^{H_m \\times W_m \\times C_t}$ into a single dimension, resulting in $M\\in R^{H_mW_m \\times C_t}$ [52]. To better distinguish the downstream orientation and position regression tasks, similar to the classification token in [23], we introduce a learnable token $t \\in R^{C_t}$ as the first token of each M. Consequently, the encoder input becomes $E_{in} = [t, t, M]\\in R^{(H_mW_m+1) \\times C_t}$. Additionally, we adopt the positional encoding technique from [53] to preserve the spatial information of each map location and assign unique positional encodings to the tokens. To reduce the number of learned positional parameters, we train two separate one-dimensional encodings: $E_x \\in R^{(W_m+1) \\times C_t/2}$ for the X-axis and $E_y \\in R^{(H_m+1) \\times C_t/2}$ for the Y-axis. Thus, the positional embedding vectors for a 2-D spatial position (i, j), where $i \\in 1,..., H_m$ and $j \\in 1,..., W_m$, are represented as:\n$E^\\text{pos}_{2, ij} \\in R^{C_t} \\left[ E^2_x, E^2_y \\right]. (7)$ These vectors are then reshaped into $E\\in R^{(H_mW_m+1) \\times C_t}$ before being fed into the subsequent Transformer Encoders.\nFollowing the approach of [25], [28], we employ a standard Transformer-Encoder architecture [23] consisting of n = 6 repeated blocks. Each block is composed of a multi-head self-attention (MHA) mechanism and a two-layer MLP with gelu activation. The inputs are normalized with LayerNorm [26] before each module, and residual connections along with dropout are used to combine the inputs with the outputs.\nAs recommended by [52], positional encodings are added to the input before each layer, and an additional LayerNorm is applied to the final output. The encoder output (which also serves as the input for the subsequent GIS module), $G_{in} \\in R^{C_t}$, at the special token t, provides a comprehensive, context-aware summary of the local features from the input activation map."}, {"title": "C. Separate Global Information Selectors (GIS) for Global Feature Extraction", "content": "Although the Transformer Encoder can autonomously learn key features for camera localization, neural networks trained on specific scenes may overfit due to insufficient global image context, leading to slower convergence and reduced robustness in sparse training scenarios. One solution is to use Non-local Neural Networks [29] to capture long-range dependencies, but their self-attention mechanism is computationally expensive. To address this challenge, we propose Global Information Selectors (GIS), which leverage the strengths of Mamba by utilizing a set of learnable parameters to selectively compress input features into a smaller hidden state. This approach results in a more compact and effective global feature representation. Additionally, we integrate this process with Mamba's Hardware-aware Algorithm to further optimize training efficiency, allowing a single Mamba layer to function as a Non-local Neural Network, efficiently capturing global information and accelerating convergence.\nSpecifically, the process begins by concatenating the output of the visual encoder, denoted as $G_{in}$, with its flipped version, $G_{flip}$. This combined input, $G_{concat}$, is subsequently fed into a bidirectional single-layer Mamba Module, which selectively compresses the input while utilizing a hardware-aware algorithm. This approach yields a more compact hidden state output, denoted as G. Specifically, for the position and orientation encoders, we obtain $G_x$ and $G_q$, respectively. Please refer to Section III-C for more details.\nThe pose regressor employs an MLP head with a single hidden layer and a gelu activation function to map the global features extracted by the two separate Global Information Selectors ($G_x$, $G_q$) to the camera's position and quaternion [x, q] (Equation 8). The hidden layer in the MLP head expands the dimensionality from 256 to 1024 before passing the output vector to a fully connected layer for regression. During training, neural network parameters are optimised using the Euclidean distance (L2 Loss) for training images I and their pose labels p = [x, q]. The loss function balances the position and rotation losses with the weights \u03b2 and \u03b3 (Equation 9), which are learnt simultaneously during training with initial values \u03b2o and yo by modeling the uncertainty of different tasks [27].\n$[x, q] = [MLP(G_x), MLP(G_q)). (8)$ loss(I) = ||x - x|||| + Be+Y (9)\nNote that in camera pose regression tasks, the quaternion q is chosen for its ease of continuous and differentiable formulation in representing orientation [27]. To ensure mapping"}, {"title": "D. MambaLoc Application on Terminal Devices", "content": "While MambaLoc is capable of operating on edge devices, it faces limitations, including dependency on the computational power of these devices and the requirement for stable network connections. Furthermore, the process of transferring data to and from edge devices, coupled with the computation and retrieval of results, can introduce latency, potentially affecting real-time performance. To mitigate these challenges, deploying"}, {"title": "IV. EXPERIMENTS", "content": "The structure of this section is as follows: We first evaluate the accuracy and training efficiency of our method using standard indoor and outdoor benchmarks for camera pose estimation, comparing it with recent state-of-the-art localization techniques to demonstrate its effectiveness. Subsequently, we explore the generalization capability of the Global Information Selector (GIS) across various visual localization models, evaluating how well our method adapts to different model architectures. We then address the robustness of our approach by analyzing its performance with sparse training data, which is crucial for applications that require efficient utilization of limited data. Following this, we present the performance of our method on terminal devices, showcasing its versatility across different deployment scenarios. Finally, we conducted an ablation study to compare the optimization effects of the Global Information Selector (GIS) on MambaLoc against configurations using the traditional Mamba and without any Mamba. This study qualitatively and quantitatively demonstrates the effectiveness of GIS in enhancing training speed."}, {"title": "A. Datasets", "content": "The Cambridge Landmarks dataset [28] features urban landscapes for outdoor localization tasks within specified spatial ranges, with scene areas ranging from 875 to 5600 square meters. Each scene includes between 200 and 1500 training samples. For our comparative evaluation, similar to [7], [15], [25], [30], we selected four scenes from this dataset, excluding the other two as they are less commonly used for benchmarking purposes. The 7Scenes dataset [50] comprises seven small-scale indoor environments, each covering a distinct spatial area ranging from 1 to 18 cubic meters. The scenes contain between 1,000 and 7,000 training samples and 1,000 to"}, {"title": "V. CONCLUSION", "content": "In this article, we addressed the critical challenge of achieving reliable positioning in the context of terminal devices and edge-cloud IoT systems, such as autonomous vehicles and augmented reality, where significant training costs and the necessity of densely collected data often hinder progress. To tackle these issues, we introduced MambaLoc, an innovative model that applies the Selective State Space (SSM) model to visual localization. MambaLoc achieves high training efficiency by leveraging the SSM model's capabilities, which include efficient feature extraction, rapid computation, and optimized memory usage. Additionally, the sparsity of the model's parameters reduce computational overhead and enhance robustness, enabling MambaLoc to maintain high accuracy and perform reliably even with limited training data. Furthermore, we introduce the Global Information Selector"}]}