{"title": "MambaLoc: Efficient Camera Localisation via State Space Model", "authors": ["Jialu Wang", "Kaichen Zhou", "Andrew Markham", "Niki Trigoni"], "abstract": "Location information is pivotal for the automation and intelligence of terminal devices and edge-cloud IoT systems, such as autonomous vehicles and augmented reality, while achieving reliable positioning across diverse IoT applications remains challenging due to significant training costs and the necessity of densely collected data. To tackle these issues, we have innovatively applied the selective state space (SSM) model to visual localization, introducing a new model named MambaLoc. The proposed model demonstrates exceptional training efficiency by capitalizing on the SSM model's strengths in efficient feature extraction, rapid computation, and memory optimization, and it further ensures robustness in sparse data environments due to its parameter sparsity. Additionally, we propose the Global Information Selector (GIS), which leverages selective SSM to implicitly achieve the efficient global feature extraction capabilities of Non-local Neural Networks. This design leverages the computational efficiency of the SSM model alongside the Non-local Neural Networks' capacity to capture long-range dependencies with minimal layers. Consequently, the GIS enables effective global information capture while significantly accelerating convergence. Our extensive experimental validation using public indoor and outdoor datasets first demonstrates our model's effectiveness, followed by evidence of our GIS's versatility with various existing localization models. For instance, in our experiment with the Heads Scene of the 7Scenes dataset [50], MambaLoc used just 0.05% of the training set and achieved accuracy comparable to the leading methods in just 22.8 seconds. We also provide a demo of MambaLoc deployed on end-user devices. These results highlight MambaLoc's potential as a robust and efficient solution for visual localization in edge-cloud IoT and terminal devices, significantly enhancing the commercial viability of deep neural networks for camera localization. Our code and models are publicly available to support further research and development in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning-based camera localization employs neural networks to implicitly learn a map of a given scene, allowing for the estimation of 6-DoF of camera pose (i.e., 6 degrees of freedom, including three translations and three rotations) for a moving camera based on captured images. It is fundamental and crucial for the automation and intelligent systems in Internet of Things (IoT), such as autonomous vehicles, emergency response devices, augmented reality, domestic robot navigation, and intelligent decision-making robots [1], etc.\nThis problem was initially solved as a place recognition problem [34]\u2013[39] which estimate the pose of the query image by retrieving the most similar images from the database. However, the robustness of these retrieval-based [40] methods are limited by its hand-crafted local features [41]. To further improve the estimation accuracy, structure based methods proposed to recover the camera pose by finding correspondence between query images and the 3D scene model. Although these methods achieves state-of-art (SOTA) accuracy [42], they are resource-intensive and requires pre-obtained SfM models or highly accurate depth information, which is not universally available in lightweight customer devices such as laptops and smartphones.\nRecent developments in deep neural networks (DNN) for image processing have facilitated the emergence of absolute pose regression (APR) methods, capable of directly predicting camera poses from images via a singular DNN [28], [48], [56]. Despite being less accurate than structure-based methods, APR-based methods are lightweight, fast, and require no additional data during inference. To better explore the application of localization methods in terminal devices and edge IoT systems, this study focuses on such localization methods. Nonetheless, these methods encounter two principal obstacles: i) Trade-off Between Accuracy and Efficiency: Most model development has focused on CNN-based or Transformer-based designs. CNNs are effective at extracting local features but often sacrifice resolution, while Transformers offer a global perspective but increase computational demands. Existing methods typically require hours [25] or even a day [15], [30] to train a single scene. This trade-off between accuracy and efficiency remains a significant challenge; ii) Lack of Robustness to Sparse Viewpoints: These data-driven methods heavily rely on dense training samples, limiting their practical applications, such as requiring low data exchange costs for cloud-edge integrated IoT localization or using sparse keyframes for loop detection in SLAM on end-user devices.\nRecently, state space models such as Mamba [12] have demonstrated substantial global modeling capabilities across diverse domains, including language modeling and computer vision [49]. These models are particularly notable for reducing the time complexity of global information extraction to O(N), marking a significant stride in computational efficiency. Inspired by this, our study explores the potential of state space models in visual localization and introduces a highly efficient model named MambaLoc."}, {"title": "II. RELATED WORK", "content": "Visual localization involves constructing a scene representation from a set of mapping images and their corresponding poses within a common coordinate system. Given a query image, the objective is to estimate its pose (position and orientation) relative to the scene. Existing solutions to this problem can be broadly categorized into two primary approaches: structure-based techniques and Absolute Pose Regression (APR) techniques.\nStructure-Based Techniques rely on geometric methods to establish correspondences between images and the scene map. These methods achieve state-of-the-art accuracy [42], but they are resource-intensive, requiring pre-computed Structure-from-Motion (SfM) models or highly accurate depth information. Such dependencies make these methods less practical for lightweight consumer devices, such as laptops and smartphones."}, {"title": "Absolute Pose Regression (APR)", "content": "techniques take a different approach by leveraging neural networks to directly predict the absolute position and orientation of an image, bypassing the need for explicit matching between the 2D image and the 3D scene map. The pioneering work in this domain, PoseNet, introduced by Kendall et al. [28], employs a feed-forward neural network to predict a 7-dimensional pose vector for each input image. Subsequent research has introduced several architectural innovations, including hourglass networks [17], bifurcated translation and rotation regression [18], [19], attention mechanisms [25], [47], and LSTM layers [43]. Efforts to further enhance APR performance have explored diverse supervision techniques, such as geometric loss [44], [57], relative pose constraints [46], uncertainty modeling [45], and sequential formulations like temporal filtering [47] and multitasking [25]. While APR methods generally offer lower accuracy compared to structure-based approaches, they are advantageous for being lightweight, fast, and not requiring additional data during inference.\nTo address overfitting in APR methods, recent research has explored novel view synthesis techniques that generate large volumes of training data [13]\u2013[15], [20], [58]. However, generating high-quality synthetic data is both time-consuming and resource-intensive, with significant memory and data exchange costs, rendering these methods less suitable for deployment on edge or terminal devices.\nGiven the increasing relevance of edge computing and the constraints of terminal devices, this study focuses on optimizing APR methods to make them more efficient and practical for use in such environments."}, {"title": "B. State Space Models for Sequence Modeling", "content": "State space models have been proposed as effective tools for sequence modeling. The Structured State-Space Sequence (S4) model [54] offers a novel alternative to CNNs and Transformers for capturing long-range dependencies, with the added benefit of linear scalability in sequence length, prompting further research. Building on this, [9] introduces the new S5 layer, incorporating MIMO SSM and efficient parallel scanning into the S4 layer. Similarly, [51] develops the H3 SSM layer, significantly closing the performance gap between SSMs and Transformer attention in language modeling. [11] enhances the S4 model with the Gated State Space layer by adding more gating units to improve expressivity. Recently, [12] presents a data-dependent SSM layer and constructs Mamba, a generic language model backbone that outperforms Transformers at various sizes on large-scale real-world data while maintaining linear scaling in sequence length. Inspired by Mamba's success, our work focuses on applying these advancements to camera localization, aiming to create an efficient and high-performance generic camera localization backbone."}, {"title": "III. METHODOLOGY", "content": "The goal of MambaLoc is to utilizes an advanced selective state space model (SSM), specifically Mamba, to implicitly replicate the efficient global feature extraction capabilities of Non-local Neural Networks. This integration allows the model to efficiently capture global information while significantly improving training efficiency. Ultimately, MambaLoc can be deployed on Edge-Cloud Collaborative IoT and Terminal Devices, greatly enhancing the commercial viability of deep neural networks for camera localization.\nThis section begins with a description of the preliminaries of Sequence Modeling for Camera Localization, particularly focusing on Mamba. It proceeds with a comprehensive overview of MambaLoc, outlining its architectural design. Subsequently, we elucidate the integration of Non-local Neural Network concepts with Mamba in our Global Information Selector (GIS). Finally, we discuss the challenges encountered when decoupling the localization model from cloud-based systems and detail our deployment strategies for MambaLoc on edge and terminal devices."}, {"title": "A. Preliminaries: Sequence Modeling for Camera localization", "content": "In this paragraph, we review existing deep learning-based camera localization models from the perspective of sequence modeling. While RNNs possess the capability to handle sequences, they encounter difficulties with gradient vanishing when processing long sequences. LSTM, on the other hand, excels in capturing long-term dependencies in data [43]. CNNs, compared to LSTM, are more effective at capturing local features and offer stronger parallel computing capabilities [28]. Transformers, known for their robust parallel computation and ability to handle long-range dependencies across various sequence tasks, face challenges with long sequences due to their quadratic time complexity relative to sequence length [48].\nRecently, Albert Gu et al. introduced Mamba [12], a novel approach leveraging a selective state-space model architecture to overcome the limitations of traditional methods, thereby enhancing performance, efficiency, and scalability in academic contexts. Inspired by continuous systems, structured state space sequence models (SSMs) such as S4 [54] and Mamba map a one-dimensional function or sequence $x(t) \\in \\mathbb{R}$ to $y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^{N}$ (see Equations 1 and 2). In this framework, $A \\in \\mathbb{R}^{N \\times N}$ serves as the evolution parameter, while $B \\in \\mathbb{R}^{N \\times 1}$ and $C \\in \\mathbb{R}^{1 \\times N}$ function as the projection parameters.\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t).$\nThe S4 and Mamba models act as discrete versions of continuous systems by employing a timescale parameter $\\Delta$ to convert the continuous parameters A and B into their discrete equivalents, $\\overline{A}$ and $\\overline{B}$. A commonly used transformation method is the zero-order hold (ZOH), defined by the following expressions:\n$\\overline{A} = exp(\\Delta A),$\n$\\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B.$"}, {"title": "B. Architecture of MambaLoc", "content": "MambaLoc consists of a shared convolutional backbone and two branches for regressing the camera's position and orientation separately. Each branch includes an independent Transformer-Encoder, a Global Information Selector (GIS), and an MLP head. By using separate GIS in each branch, we can extract and compress different global features, thereby adapting to various learning tasks. The overall structure of the MambaLoc model is shown in Figure 1\nGiven an image $I \\in \\mathbb{R}^{H \\times W \\times C'}$, we employ the CNN Backbone at two different resolutions to generate activation maps $M_x \\in \\mathbb{R}^{Hm \\times Wm \\times Cm}$ and $M_q \\in \\mathbb{R}^{H \\times W \\times Cm}$ for the tasks of position and orientation regression, respectively. To achieve positional encoding at the same depth, as suggested in [52], [53], we apply a 1 \u00d7 1 convolution to linearly transform each activation map into a unified high-level depth dimension ($C_t$ = 256). Since the subsequent Transformer-encoder requires a sequence as input, we reshape the spatial dimensions of $M \\in \\mathbb{R}^{Hm \\times Wm \\times Ct}$ into a single dimension, resulting in $M \\in \\mathbb{R}^{HmWm \\times Ct}$ [52]. To better distinguish the downstream orientation and position regression tasks, similar to the classification token in [23], we introduce a learnable token $t \\in \\mathbb{R}^{Ct}$ as the first token of each M. Consequently, the encoder input becomes $E_{in} = [t, M] \\in \\mathbb{R}^{(HmWm+1) \\times Ct}$.\nAdditionally, we adopt the positional encoding technique from [53] to preserve the spatial information of each map location and assign unique positional encodings to the tokens. To reduce the number of learned positional parameters, we train two separate one-dimensional encodings: $E_x \\in \\mathbb{R}^{(Wm+1) \\times Ct/2}$ for the X-axis and $E_y \\in \\mathbb{R}^{(Hm+1) \\times Ct/2}$ for the Y-axis. Thus, the positional embedding vectors for a 2-D spatial position (i, j), where $i \\in 1,..., H_m$ and $j \\in 1,..., W_m$, are represented as:\n$E^{pos}_{2, ij} \\in \\mathbb{R}^{Ct} = [E^{x^T}_{i}, E^{y^T}_{j}]^T.$\nThese vectors are then reshaped into $E \\in \\mathbb{R}^{(HmWm+1) \\times Ct}$ before being fed into the subsequent Transformer Encoders.\nFollowing the approach of [25], [28], we employ a standard Transformer-Encoder architecture [23] consisting of n = 6 repeated blocks. Each block is composed of a multi-head self-attention (MHA) mechanism and a two-layer MLP with gelu activation. The inputs are normalized with LayerNorm [26] before each module, and residual connections along with dropout are used to combine the inputs with the outputs."}, {"title": "3) Separate Global Information Selectors (GIS) for Global Feature Extraction", "content": "Although the Transformer Encoder can autonomously learn key features for camera localization, neural networks trained on specific scenes may overfit due to insufficient global image context, leading to slower convergence and reduced robustness in sparse training scenarios. One solution is to use Non-local Neural Networks [29] to capture long-range dependencies, but their self-attention mechanism is computationally expensive. To address this challenge, we propose Global Information Selectors (GIS), which leverage the strengths of Mamba by utilizing a set of learnable parameters to selectively compress input features into a smaller hidden state. This approach results in a more compact and effective global feature representation. Additionally, we integrate this process with Mamba's Hardware-aware Algorithm to further optimize training efficiency, allowing a single Mamba layer to function as a Non-local Neural Network, efficiently capturing global information and accelerating convergence.\nSpecifically, the process begins by concatenating the output of the visual encoder, denoted as $G_{in}$, with its flipped version, $G_{flip}$. This combined input, $G_{concat}$, is subsequently fed into a bidirectional single-layer Mamba Module, which selectively compresses the input while utilizing a hardware-aware algorithm. This approach yields a more compact hidden state output, denoted as G. Specifically, for the position and orientation encoders, we obtain $G_x$ and $G_q$, respectively."}, {"title": "4) 6-DoF Camera Pose Regressor", "content": "The pose regressor employs an MLP head with a single hidden layer and a gelu activation function to map the global features extracted by the two separate Global Information Selectors ($G_x$, $G_q$) to the camera's position and quaternion [x, q] (Equation 8). The hidden layer in the MLP head expands the dimensionality from 256 to 1024 before passing the output vector to a fully connected layer for regression. During training, neural network parameters are optimised using the Euclidean distance (L2 Loss) for training images I and their pose labels p = [x, q]. The loss function balances the position and rotation losses with the weights \u03b2 and \u03b3 (Equation 9), which are learnt simultaneously during training with initial values $\u03b2_o$ and $\u03b3_o$ by modeling the uncertainty of different tasks [27].\n$[x, q] = [MLP(G_x), MLP(G_q)].$\n$loss(I) = ||x - x*||_2 e^{-\u03b2} + \u03b2 + ||q - q*||_2 e^{-\u03b3} + \u03b3.$\nNote that in camera pose regression tasks, the quaternion q is chosen for its ease of continuous and differentiable formulation in representing orientation [27]. To ensure mapping it to a valid rotation matrix, it is normalized to a unit vector [25]."}, {"title": "C. The Global Information Selector (GIS)", "content": "Traditional Non-local Neural Networks, proposed by [29], capture long-range dependencies by computing the response at a position as a weighted sum of features at all positions in the input. The significant advantage of this approach is its ability to capture wide-range dependencies directly with few layers by calculating associations between any two positions. However, it also increases computational complexity compared to local operations.\nIn this section, we will provide a detailed explanation of how we innovatively introduced the mechanism of Non-local Neural Networks into the Mamba model. The overall structure of the MambaLoc model is shown in Figure 1. Unlike traditional Non-local operations that compute attention across all elements without compression, we leverage Mamba's strengths to selectively compress input features into a smaller hidden state using a set of learnable parameters. This approach yields a more compact and effective global feature representation. Additionally, the process is further enhanced by integrating the Hardware-aware Algorithm to optimize training efficiency. This allows the model to capture global information while rapidly enhancing convergence capabilities.\nLet B represent the batch size, D the input feature channels, and N the state dimension of Mamba, set to 16 in all experiments. In the GIS module of MambaLoc, the input feature dimension is D = Ct. The process flow is illustrated in Algorithm 1,where:"}, {"title": "D. MambaLoc Application on Terminal Devices", "content": "While MambaLoc is capable of operating on edge devices, it faces limitations, including dependency on the computational power of these devices and the requirement for stable network connections. Furthermore, the process of transferring data to and from edge devices, coupled with the computation and retrieval of results, can introduce latency, potentially affecting real-time performance. To mitigate these challenges, deploying the localization network directly on terminal devices\u2014such as autonomous mobile robots, unmanned vehicles, and smart-phones\u2014presents a more efficient and robust solution. By doing so, the constraints of edge device configurations and network dependencies can be eliminated, while enabling faster local image processing, optimizing processing efficiency, and reducing energy consumption, thus facilitating the broader practical deployment of the network. Therefore, this section explores the design considerations for deploying MambaLoc on terminal devices.\nHowever, directly deploying existing deep learning-based camera localization networks on terminal devices presents significant challenges due to their typically large number of parameters, which makes them impractical for hardware-constrained mobile environments. To overcome this issue while maintaining accuracy, we adopted a hybrid knowledge distillation approach that integrates feature-based and logits-based techniques. This approach allowed us to distill a complex network trained on edge devices into a more efficient student network suitable for deployment on terminal devices.\nThis section outlines the methodology of our hybrid knowledge distillation approach for deploying MambaLoc on terminal devices. This approach emphasizes the optimization of model parameters to ensure efficient deployment and optimal performance. To achieve a balance between latency and accuracy, we selected EfficientNet-B0 [31], a compact network that optimizes computational efficiency and performance, as the backbone of our student model. Features extracted by EfficientNet-B0 are subjected to adaptive average pooling and dropout to mitigate overfitting, followed by processing through three linear layers. The first layer reduces the 1280-dimensional feature vector to a 1024-dimensional latent space, facilitating the extraction of salient features for pose regression. The final output is a 7-dimensional vector, representing the camera's pose in terms of translation and quaternion rotation components. During the offline distillation process, the weights of the pre-trained teacher model were kept frozen. To enhance feature extraction learning from the teacher network, we employed an offline distillation approach, utilizing the distilled loss function as specified in Equation (10):\n$L_D = L_L + L_S + L_F,$\n,where $L_L$, $L_S$, and $L_F$ represent the logits-based hard distillation loss, the logits-based soft distillation loss, and the feature-based distillation loss, respectively. The form of $L_L$ is identical to that in Equation (9), while the details of $L_S$ and $L_F$ are provided in the following paragraphs.\nThe Logits Based Soft-Distillation Loss The Logits-Based Soft-Distillation Loss, defined in Equation 11, is applied to the estimated poses from the student and teacher networks, denoted as $p_s = [X_s, q_s]$ and $p_t = [x_t, q_t]$, respectively. This loss function balances the position and orientation losses by modeling the uncertainty of each task through the weights $\u03b2_{soft}$ and $\u03b3_{soft}$, as proposed by Kendall et al. [27]. These weights, initially set to $\u03b2_{soft_0}$ and $\u03b3_{soft_0}$, are learned adaptively during training to optimize the distillation process.\n$L_S = L_{KL}(x_s, x_t)e^{-\u03b2_{soft}} + \u03b2_{soft} + L_{KL}(\\hat{x_s}, \\hat{x_t})e^{-\u03b3_{soft}} + \u03b3_{soft},$\nwhere the Kullback-Leibler loss $L_{KL}$ is defined as:\n$L_{KL} = L_{KD}(F_{LogS}(T^{-1} \u00b7 \\hat{y_s}), F_s(T^{-1} \u00b7 \\hat{y_t})) \u00b7 T^{2}$\nHere, \u0177 may represent either x for position or q for orientation. The temperature parameter T is set to 10.0 for all experiments, and $L_{KD}$ denotes the Kullback-Leibler Divergence loss function. The functions $F_{Logs}$ and $F_{s}$ correspond to the LogSoftmax and Softmax functions, respectively.\nFeature-Based Distillation Loss\n$L_F = 1- cos(f_t, f_s).mean()$\nThe feature-based distillation loss function $L_F$, described in Equation (13), aims to encourage similarity between flattened feature maps from the teacher model ($f_t$) and the student model ($f_s$). This loss computes the cosine similarity between these feature representations and then averages the results across all feature vectors."}, {"title": "IV. EXPERIMENTS", "content": "The structure of this section is as follows: We first evaluate the accuracy and training efficiency of our method using standard indoor and outdoor benchmarks for camera pose estimation, comparing it with recent state-of-the-art localization techniques to demonstrate its effectiveness. Subsequently, we explore the generalization capability of the Global Information Selector (GIS) across various visual localization models, evaluating how well our method adapts to different model architectures. We then address the robustness of our approach by analyzing its performance with sparse training data, which is crucial for applications that require efficient utilization of limited data. Following this, we present the performance of our method on terminal devices, showcasing its versatility across different deployment scenarios. Finally, we conducted an ablation study to compare the optimization effects of the Global Information Selector (GIS) on MambaLoc against configurations using the traditional Mamba and without any Mamba. This study qualitatively and quantitatively demonstrates the effectiveness of GIS in enhancing training speed."}, {"title": "A. Datasets", "content": "The Cambridge Landmarks dataset [28] features urban landscapes for outdoor localization tasks within specified spatial ranges, with scene areas ranging from 875 to 5600 square meters. Each scene includes between 200 and 1500 training samples. For our comparative evaluation, similar to [7], [15], [25], [30], we selected four scenes from this dataset, excluding the other two as they are less commonly used for benchmarking purposes. The 7Scenes dataset [50] comprises seven small-scale indoor environments, each covering a distinct spatial area ranging from 1 to 18 cubic meters. The scenes contain between 1,000 and 7,000 training samples and 1,000 to 5,000 validation samples. Both datasets pose various localization challenges, including occlusions, reflections, motion blur, varying lighting conditions, repetitive textures, and changes in viewpoints and trajectories."}, {"title": "B. Implementation Details", "content": "Our models are implemented in PyTorch and trained using the Adam optimizer with hyperparameters $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.999, $\u03f5$ = 1e-10, and a batch size of 8. The learning rate is set to \u03bb = 1e-4 and decreases by a factor of 10 every 100 epochs for indoor localization (or every 200 epochs for outdoor localization). Training continues for up to 600 epochs unless an early stopping criterion is met. This criterion is based on the validation set, where the validation loss is monitored each epoch, and early stopping is triggered if the loss does not decrease from the previous low for more than five consecutive epochs. We apply a weight decay of $le-4$ and a dropout rate of 0.1 to train the encoders. For pose estimation tasks, parameters are initialized with $\u03b2_1 = -0.5$ and $\u03b3_0 = -6.5$.\nDuring the distillation procedure, parameters are initialized with $\u03b2_{soft_0} = -0.5$ and $\u03b3_{soft_0} = -6.5$. To improve the model's generalization ability, we use the data augmentation method proposed by [28]. During training, images are resized so that their shorter edge measures 256 pixels, followed by a random crop to 224 x 224 pixels. Additionally, we apply random adjustments to brightness, contrast, and saturation. During testing, images are resized and a center crop is taken without further augmentations. To validate the efficiency of MambaLoc's training process, we conducted a comparative analysis of training time between our model and previously published open-source methods. For consistency, all training durations were measured using the same NVIDIA L20 server.\nIt is important to note that this section focuses on evaluating our model's ability to achieve state-of-the-art (SOTA) accuracy in the shortest possible time. Therefore, we do not compare our method with multi-scene approaches (e.g., [5], [53]) or methods that incorporate additional NeRF training (e.g., [15], [30]). Additionally, all experimental results for MambaLoc presented here are obtained from a single training phase, without any secondary fine-tuning stages (e.g., TransPoseNet [25])."}, {"title": "C. Comparative Analysis of the Accuracy and Training Time of Visual Localization Models", "content": "We evaluated the performance of our method in terms of pose estimation accuracy using the 7-Scenes indoor camera localization dataset [50]. Figure 2 and Table I provides a quantitative comparison of the estimated trajectories produced by our method and those of previous approaches. MambaLoc achieves state-of-the-art (SOTA) accuracy in average translation accuracy, with results that are on par with the best-performing methods. Notably, this level of accuracy is achieved in a single training phase, unlike methods such as TransPoseNet [25], which require additional fine-tuning stages.\nIn addition to accuracy, we assessed the training efficiency of different methods by comparing their training durations. Table I presents a detailed comparison of our method with earlier single-frame absolute pose regression (APR) methods. To ensure fairness, we measured the training durations of previously open-source methods on the same server (NVIDIA L20). Our approach significantly reduces the average training duration\u2014by 65% compared to the fastest method (Trans-BoNet [7]) and by 92.9% compared to the slowest method (PoseNet [28]). Detailed total training durations for each scene in the 7-Scenes dataset [50] using MambaLoc are provided in Table III. For the Heads scene, which contains 1,000 images, our method completes training in just 2 minutes, while still achieving SOTA accuracy.\nThese results collectively demonstrate that our approach not only achieves high accuracy but also enables rapid training on edge devices, making end-to-end localization models feasible for productization and practical applications.\nWe further evaluated our approach on four outdoor scenes from the Cambridge Landmarks dataset [28]. Figure 3 provides a quantitative comparison of the estimated trajectories produced by our method versus those from previous approaches. Table II presents a comparison of our method's accuracy and training time against other APR methods, excluding those that did not report results on the Cambridge dataset. All training durations were measured on the same NVIDIA L20 server to ensure consistency.\nOur findings indicate that, even on larger and more challenging outdoor datasets, our model demonstrates significantly greater training efficiency while still achieving pose estimation accuracy comparable to state-of-the-art (SOTA) methods. Importantly, this level of accuracy is achieved in a single training phase, without the need for additional fine-tuning stages (as required by TransPoseNet [25]).\nRegarding the training duration, our method reduces the average training duration by 24% compared to the fastest method (MapNet [27]) and by 60% compared to the slowest method (LSTM PN [43]). Detailed total training duration for each scene in the Cambridge dataset [28] using MambaLoc are provided in Table IV. These results underscore our approach's effectiveness in balancing training duration and accuracy, making it a highly efficient solution for real-world applications."}, {"title": "D. Evaluation of the Generalization Capability of the Global Information Selector (GIS) Across Various Visual Localization Models", "content": "Quantitative Evaluation: To evaluate the effectiveness of the proposed Global Information Selector (GIS) as a versatile module for enhancing existing end-to-end localization models, we conducted a comparative analysis using the publicly available 7Scenes indoor dataset [50], which features a diverse range of scenes, complex camera trajectories, and varied textures. To thoroughly assess the generalizability of GIS, we selected benchmark methods based on different underlying mechanisms, including the CNN-based PoseNet [28], the LSTM-based LSTM PN [43], and the attention-based TransPoseNet [25]. We first analyzed the training duration and pose estimation accuracy of each method, both before and after integrating GIS. Specifically, we measured the total training time (in minutes) and per-frame inference time (in milliseconds) for each method. Additionally, we compared the robustness of these benchmarks to extremely sparse training data, both with and without GIS. To ensure consistency, all models were trained on the same NVIDIA L20 server.\nAs shown in Table V, the addition of the GIS improved the average translation and rotation accuracy across all models by 10.8% and 4.99%, respectively, and increased the average training speed by 87.36%. Specifically, PoseNet [28] saw improvements of 15.9% in translation and 13.6% in rotation accuracy, with training time reduced by 88.32%. LSTM PN [43] demonstrated 6.5% improvements in translation and 9.3% in rotation accuracy, also with training time reduced by 70%. For the state-of-the-art model TransPoseNet [25], the GIS demonstrated 5.56% improvements in translation and reduced the training time by 92.13%, though rotation error increased by 0.93\u00b0. This discrepancy may stem from the fact that, as noted by the authors of TransPoseNet [25], their results involve fine-tuning the orientation head with a latent position before the position Transformer-Encoder. In contrast, our approach, designed with time efficiency and practical deployment in mind, achieves results through a single-phase training process without additional fine-tuning.\nAdditionally, we compared the ability of the Global Information Selector (GIS) to enhance the robustness of various Absolute Pose Regression (APR) methods on extremely sparse training sets. As shown in Table VII, we evaluated pose estimation errors on the entire test set after uniformly sampling 100%, 10%, and 5% of the training set. The arrow direction indicates the increase/decrease in pose error of Benchmark-GIS relative to Benchmark under the same sparsity conditions. The experimental results demonstrate that the use of GIS significantly mitigates the degradation in translation error under all sparsity conditions, and in most cases, it also alleviates the degradation in rotation error. All benchmarks with GIS show reduced average translation and rotation errors across all experiments.\nFinally, we assessed the computational complexity introduced by GIS. When the sequence output from the final feature extractor of a model has a batch size B with D channels, GIS adds a computational complexity of only O(BLDN),"}, {"title": "E. Comparative Analysis of MambaLoc and Other Visual Localization Models for Robustness in Sparse Training Environments", "content": "In the field of visual localization, especially in the context of edge-cloud collaborative localization and sparse keyframe SLAM loop closure detection, the ability of localization models to effectively handle sparse training data is crucial. Traditional camera localization networks and bag-of-words models used in SLAM systems often struggle with robustness when"}, {"title": "F. Performance Evaluation of MambaLoc Deployment on Terminal Devices", "content": "To address the challenge of deploying networks with typically large parameter sizes in hardware-constrained mobile environments while maintaining accuracy, we employed the hybrid knowledge distillation method described in Section III-D. This method integrates feature-based and logits-based techniques to deploy MambaLoc on terminal devices. By doing so, we circumvent the limitations associated with edge device configurations and network conditions, enabling rapid local image processing. Table IX presents a comparison of pose accuracy before and after distilling MambaLoc onto our modified student network, based on EfficientNet-B0 [31] (network design detailed in Section III-D), using this hybrid knowledge distillation approach on the 7Scenes dataset. The experimental results demonstrate that our method not only reduced the model size by 47.62% but also decreased the translation and rotation errors by 88.24% and 93.80%, respectively. We then compiled the distilled model using ONNX Runtime, ensuring compatibility across any terminal device that supports ONNX Runtime, such as iOS and Android mobile phones, as well as PCs. For a detailed demonstration of MambaLoc's deployment on terminal devices and real-time 6-DoF pose inference"}, {"title": "G. Ablation Study on the Effectiveness of Global Information Selector (GIS)", "content": "Quantitative Evaluation: To demonstrate the effectiveness of the proposed Global Information Selector (GIS), we conducted an ablation study comparing pose estimation accuracy, total training time, and average inference time per frame across three models: MambaLoc with the Global Information Selector (MambaLoc w. GIS), MambaLoc with the Classical Mamba Block (MambaLoc w. Classical Mamba), and MambaLoc without any Mamba or GIS (MambaLoc w.o. Mamba). This evaluation was performed using the 7-Scenes dataset [50]. As shown in Table X, the average pose estimation errors and inference speeds among the three models are comparable. However, compared to the complete MambaLoc with the Global Information Selector, the model using the Classical Mamba without non-local mechanisms exhibited a 84.83% increase in training time cost, and the model without any Mamba module showed a 92.13% increase in training time cost. These experimental results demonstrate that the GIS module significantly accelerates training speed without a substantial compromise in pose estimation accuracy."}, {"title": "V. CONCLUSION", "content": "In this article, we addressed the critical challenge of achieving reliable positioning in the context of terminal devices and edge-cloud IoT systems, such as autonomous vehicles and augmented reality, where significant training costs and the necessity of densely collected data often hinder progress. To tackle these issues, we introduced MambaLoc, an innovative model that"}]}