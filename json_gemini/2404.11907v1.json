{"title": "Sampling-based Pareto Optimization for Chance-constrained Monotone Submodular Problems", "authors": ["Xiankun Yan", "Aneta Neumann", "Frank Neumann"], "abstract": "Recently surrogate functions based on the tail inequalities were developed to evaluate the chance constraints in the context of evolutionary computation and several Pareto optimization algorithms using these surrogates were successfully applied in optimizing chance-constrained monotone submodular problems. However, the difference in performance between algorithms using the surrogates and those employing the direct sampling-based evaluation remains unclear. Within the paper, a sampling-based method is proposed to directly evaluate the chance constraint. Furthermore, to address the problems with more challenging settings, an enhanced GSEMO algorithm integrated with an adaptive sliding window, called ASW-GSEMO, is introduced. In the experiments, the ASW-GSEMO employing the sampling-based approach is tested on the chance-constrained version of the maximum coverage problem with different settings. Its results are compared with those from other algorithms using different surrogate functions. The experimental findings indicate that the ASW-GSEMO with the sampling-based evaluation approach outperforms other algorithms, highlighting that the performances of algorithms using different evaluation methods are comparable. Additionally, the behaviors of ASW-GSEMO are visualized to explain the distinctions between it and the algorithms utilizing the surrogate functions.", "sections": [{"title": "INTRODUCTION", "content": "In real-world optimization problems, diminishing returns often affect the profit of a solution when adding more items to the solution. Such problems can be generalized by the submodular functions as highlighted in previous research [8, 10, 11]. In the past, the optimization of monotone submodular functions under deterministic knapsack constraint has been widely studied [6, 7, 11, 13]. The aim of such problems is to find a subset of elements that maximizes the function value without the total weight of the subset exceeding a bound given in the knapsack. Recently, people gradually paid more attention to the stochastic version of the optimization of monotone submodular functions since the stochastic scenarios in real-world applications are unavoidable. Consequently, it is crucial not only to consider the existing constraints but also to minimize the effects of stochastic items in the stochastic optimization problems.\nA useful technique, chance constraint, is frequently utilized to address the effects of uncertainty in the problem involving stochastic items [2, 3, 5, 12, 15, 17, 18, 20-25]. This approach allows for a solution's actual weight to exceed the given bound but with a very small probability. Previous studies [4, 13, 14] have successfully applied the chance constraint to the optimization of monotone submodular functions under the uniform distribution setting. Furthermore, to evaluate the chance constraint, surrogate functions were established based on One-sided Chevbyshev's inequality and Chernoff bound, These surrogate functions facilitate the translation of the chance constraint into their corresponding deterministic equivalents for a given confidence level. Additionally, the performance of some greedy algorithms and a global simple evolutionary multi-objective algorithm (GSEMO) were analyzed and tested on the chance-constrained submodular problem. From their theoretical analysis, it is shown that within a certain runtime, all algorithms using the surrogate functions can guarantee a solution with a good approximation relative to the optimal solution obtained in the corresponding deterministic setting. However, the previous work did not address the gap between the algorithm using the surrogate functions and those employing a sampling-based method to directly evaluate the chance constraint. Furthermore, the performance of algorithms in more complex settings has yet to be explored.\nThe sampling-based approach proves valuable for evaluating the chance constraint [1, 9, 19]. In the paper, we introduce a novel sampling-based method for optimizing chance-constrained submodular problems. This method involves sampling the actual weights of a solution multiple times and then sorting these weights to identify the highest sampled weight that complies with the specified probability threshold. If this maximum sampled weight remains"}, {"title": "2 PROBLEM DEFINITION AND SETTINGS", "content": "Given a ground set $V = {v_1, ..., v_n}$, the optimzation of a monotone submodular function $f : 2^V \u2192 R_{\u22650}$ is considered. A function is defined as monotone iff for every $S \u2286 T \u2286 V$, $f(S) \u2264 f(T)$ holds. Besides, a function $f$ is submodular iff for every $S \u2282 T \u2282 V$ and $v \u2209 T$, $f(S \u222a {v}) \u2212 f(S) \u2265 f(T \u222a {v}) \u2212 f(T)$ is satisfied. Within the paper, the optimization of a monotone submodular function $f$ is considered subjected to the chance constraint where each element $v_i \u2208 V$ has a stochastic weight $W(v_i)$. The chance-constrained optimization problem can be formulated as\nMaximize f(S)\nS.t.  Pr[W(S) > B] \u2264 \u03b1,\nwhere $W(S)$ is the weight of the subset S (i.e., $W(S) = \\sum_{v_i\u2208S} W(v_i)$) and B is the deterministic bound. The parameter \u03b1 is a small and acceptable probability violating the bound B.\nFor the settings, they are both considered the actual weight of the element sampled from a uniform distribution with an expected weight and dispersion. There are two different cases for the expected weight $E_W(v_i)$ and dispersion \u03b4($v_i$) investigated in the paper:\n(1) Uniform IID Weights the stochastic weight $W(v_i)$ has the same expected weight and same dispersion, i.e., $E_W(v_i) = a$, \u03b4($v_i$) = d, and $W(v_i) \u2208 [a \u2212 d, a + d]$ with 0 < d \u2264 a.\n(2) Uniform Weights with the Same Dispersion the elements' expected weights are different but their dispersion is the same, i.e, $E_W(v_i) = a_i$, \u03b4($v_i$) = d, and $W(v_i) \u2208 [a_i \u2212 d, a_i + d]$ with 0 < d \u2264 $a_i$.\nSince the evolutionary algorithm is considered to solve the problem, we encode an element set S as a decision vector $x = x_1x_2...x_n$ with length n, where $x_i = 1$ means that the element $v_i \u2208 V$ is picked up into the solution S. Given that all settings are based on a uniform distribution, the expected weight and variance of the solution are calculated as follows: $E[W(X)] = \\sum_{i=0} E_W(v_i)x_i$, and $Var[W(X)] = \\sum_{i=0} \u03b4(v_i)^2x_i/3$."}, {"title": "3 RELATED WORK", "content": "As described in previous work [14], the chance constraint is evaluated using the surrogate functions based on One-sided Chebyshev's inequality and Chernoff bound. The surrogate weight calculated by these different surrogate functions can be respectively expressed as\n$W_{cheb}(X) = E[W(X)] + \\sqrt{\\frac{(1-\u03b1)Var[W(X)]}{\u03b1}}$\nand\n$W_{chen}(X) = E[W(X)] + \\sqrt{3d|X|ln(1/\u03b1)}$,\nwhere |X| is the number of elements picked in the solution. Furthermore, multi-objective evolutionary algorithms (MOEAs) are studied to optimize the given problem in [14]. In the algorithm, each solution is considered a two two-dimensional search point in the objective space. The two-dimensional fitness function of the solution X is expressed as\n$g_1(X) = {\\begin{cases} f(X)  &g_2(X) \u2264 B\\\\ -1 & g_2(X) > B \\end{cases}}$\n$g_2(X) = W_{sg}(X)$,\nwhere f(X) is the submodular function value of X, $W_{sg}(X)$ is the surrogate weight of X. Let $Y \u2208 {0,1}^n$ be another solution in the search space. Solution X (weakly) dominates Y (denoted as X \u2265 Y) iff $g_1(X) \u2265 g_1(Y)$ and $g_2(X) \u2264 g_2(Y)$. Comparatively, an infeasible solution is strongly dominated by a feasible one due to the objective function $g_1$. Additionally, the objective function $g_2$ directs the solutions towards the feasible search space."}, {"title": "4 FAST SAMPLING-BASED PARETO OPTIMIZATION", "content": "We now introduce our proposed novel algorithm utilizing the sampling-based approach. In the sampling-based approach, we independently sample the actual weight of each element from a given uniform distribution $T_{sp}$ times. This results in a solution X with a set of independent sampling weights, denoted by $W_s$. Next, we sort the sampling weights in descending order. With the index $i = [T_{sp} \u00b7 \u03b1]$, the maximal sampling weight $W_{sp}(X)$ that meets the chance constraint under the probability \u03b1 is obtained, i.e., $W_{sp}(X) = W_s[index]$. Therefore, we can consider that the solution is feasible if the sampling weight of solution X does not exceed the"}, {"title": "5 EXPERIMENTS", "content": "In this section, we investigate the performance of ASW-GSEMO and other algorithms that employ the sampling-based evaluation method on the chance-constrained submodular problem across various settings. We specifically focus on the maximum coverage problem (MCP) based on graphs [7], which is a kind of classical submodular problem, as part of our experimental work. Additionally, we conduct a comparison of the results achieved by the ASW-GSEMO against those garnered by the GSEMO and SW-GSEMO using the sampling-based evaluation approach. Moreover, we examine the performances of ASW-GSEMO across various surrogates to highlight the gap among the different evaluation methods."}, {"title": "5.1 Experimental Setup", "content": "Within this paper, we examine a chance-constrained version of MCP. To briefly describe, given an undirected graph G with a set of vertices V = {$v_1$, ..., $v_n$} and a set of edges E, we denote N(V')"}, {"title": "5.2 The Result on Uniform IID Weights", "content": "Here, we focus on the performance of various algorithms on the MCP with IID weights. We present the results obtained by the algorithms and compare the performance across different instances. Further, we visualize the optimization processing of the ASW-GSEMO to demonstrate its advantage contract to the SW-GSEMO. Additionally, we provide a visualization of the optimization process of the ASW-GSEMO to highlight its advantages over the SW-GSEMO. Additionally, we present results from the ASW-GSEMO using the surrogate evaluation method and identify the differences between the two evaluation approaches."}, {"title": "5.2.1 Results comparison and Visualization of ASW-GSEMO", "content": "The results  illustrate that the ASW-GSEMO generally outperforms the GSEMO and SW-GSEMO. However, it is noteworthy that the standard deviation of the ASW-GSEMO is higher than the others across all instances. This is because the size adaptive window in different runs is not stable. The results also reveal that the performance of the SW-GSEMO is almost on par with the GSEMO. Furthermore, the table indicates that the average number of elements |V'| in the best solution is larger when the ASW-GSEMO is"}, {"title": "5.2.2 Sampling VS. Surrogate", "content": "The results obtained from the ASW- GSEMO algorithm using surrogate functions are presented in Table 3. This table also includes statistical p-values comparing the sampling-based approach with two different surrogate approaches respectively. For the ca-Grac graph, which has a smaller number of nodes and a lower bound, the p-values are less than 0.05. This suggests that the performance of the ASW-GSEMO with the sampling method is comparable to that with the surrogates. However, for larger graphs and bounds, the performance of the ASW-GSEMO with the sampling method is not as good as to that with the surrogates. These findings indicate that while the algorithm with the sampling-based evaluation is as effective as the surrogate methods in instances with smaller expected weights and tighter bounds, it slightly does not perform as efficiently compared to the surrogates in cases with larger bounds and graphs, especially with fewer iterations. Table 4 in the appendix displays the average population size and the average number of elements in the final best solution across different instances. According to Tables 1 and 4, the ASW-GSEMO with the sampling-based approach tends to yield more individuals but fewer elements in the final best solution.\nThe surrogate weights of solutions in the population obtained from the sampling-based method in each iteration are calculated, and a portion of these results are visualized in Figures 2a and 2b. These figures reveal that while the weights evaluated by different methods are similar when the probability is larger, there is a significant discrepancy between the weights evaluated by the surrogate based on one-sided Chebyshev's inequality and other approaches when the probability is smaller. Additionally, they indicate that the evaluated weights of the solutions do not reach the bound, suggesting incomplete convergence. The trend of increasing weights implies that the weights evaluated by the surrogate based on one-sided Chebyshev's inequality would likely reach the bound first upon full convergence."}, {"title": "5.3 The Result on Uniform Weights with Same Dispersion", "content": "Here, we examine the performance of the ASW-GSEMO on MCP under uniform weights with the same dispersion, comparing it against to other fast Pareto optimization algorithms. We also visualize the process of the ASW-GSEMO to demonstrate the functioning of its"}, {"title": "5.3.1 Results comparison and Visualization of ASW-GSWMO", "content": "Table 5 presents the results of different algorithms, including the average number of elements in the best solution. These findings reflect the same outcomes observed when the algorithm was tested on a problem featuring IID weights. In this setting, the ASW-GSEMO selects a greater number of elements for the solution, beating the performance of other algorithms. This superior performance is noted despite a higher standard deviation, which results from the unstable size of the adaptive window. The results also show that the sampling size Tsp does not significantly affect the final outcomes for both algorithms, and the disparity between results for different values of a is small."}, {"title": "5.3.2 Sampling VS. Surrogate", "content": "Table 6 presents the outcomes and the average number of elements in the best solutions produced by the ASW-GSEMO. In addition, it includes calculated p-values comparing the results obtained from surrogate and sampling-based methods. These results indicate that the performance of the algorithm using the sampling-based approach is as same as that of the surrogate method, as evidenced by the fact that the p-values for most cases are below 0.05. Furthermore, it is observed that the ASW-GSEMO employing the surrogate method yields solutions with fewer elements compared to those generated using the sampling-based approach. The influence of variations in a on performance is also minimal, with the results from different surrogate models showing little difference."}, {"title": "6 CONCLUSION", "content": "In this paper, our investigation focuses on the ASW-GSEMO utilizing a sampling evaluation approach for chance-constrained monotone submodular problems, specifically those with IID weights and uniform weights of identical dispersion. In both settings, we assign larger values to the expected weight and dispersion. Our experimental findings reveal that the ASW-GSEMO outperforms other algorithms like the GSEMO and SW-GSEMO in solving the maximum coverage problem, especially in cases with larger bounds. Visual analyses show that the ASW-GSEMO is adept at incorporating a higher number of individuals within its adaptive window. Additionally, it is observed that the algorithm's performance with a sampling-based evaluation is comparable in quality to that achieved with surrogate evaluations.\nFor future work, there is room for improvement in the algorithm's functionality, particularly regarding the window's performance in later iterations. Exploring the algorithms under different generalized settings and with various problems presents an interesting and valuable direction for future research."}]}