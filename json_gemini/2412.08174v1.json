{"title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?", "authors": ["Zihao Li", "Lecheng Zheng", "Bowen Jin", "Dongqi Fu", "Baoyu Jing", "Yikun Ban", "Jingrui He", "Jiawei Han"], "abstract": "While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is provided in the supplementary materials.", "sections": [{"title": "1 Introduction", "content": "Graphs are constructed from real scenarios, but GNNs, optimized according to numerical labels, still do not understand what a label represents in the real world. To solve the issue of predetermined numerical categories, Contrastive Language-Image Pre-training (CLIP) [66] leverages natural language supervision by jointly training an image encoder and a text encoder in the same embedding space at scale. CLIP has demonstrated the ability to train high-quality, generalizable vision models [66, 29, 44, 101], which can adapt to diverse downstream tasks. Similar frameworks have been successfully extended to other modalities, including video [84, 4], 3D images [93, 22, 23], speech [68] and audio [20, 79], consistently demonstrating that alignment with text enhances the transferability of encoders. As for graphs, so far, such graph-text alignment has only been explored in the molecular domain [59, 57, 15, 70, 52] and on text-attributed graphs [78, 46, 32, 86, 85], where the paired graph-text data is relatively sufficient for pre-training.\nHowever, extending this paradigm to more general graph data poses significant challenges due to three facts. First, compared with language or vision data, graph data is very scarce and the text supervision is extremely weak [51, 6, 60] for pre-training models. Specifically, besides the number of samples being much smaller than images, many graph datasets are used for classification, where the label names consist of only a few tokens. Second, the task space of graph data could be on node-level [47, 100, 18, 56], edge-level [69, 48, 2], and graph-level [64, 16, 103, 102, 49, 50]. Third, in general, language tokens and visual objects retain the same conceptual meaning across different distributions, but the same graph structure may have distinct interpretations in different domains [17].\nDue to the above three reasons, jointly pre-training graph and text encoders is impractical for graph data with extremely weak text supervision. Fortunately, we can deal with the two modalities separately for pre-training: large language models have already been extensively pre-trained, and tremendous efforts have been devoted to pre-train GNNs through self-supervision [83, 26, 54, 38]. However, even with a pre-trained graph model, effectively adapting it to both the semantic embedding space for text alignment and diverse downstream tasks remains non-trivial. This raises a critical question:\nHow to adapt pre-trained GNNs to the semantic embedding space given limited downstream data,\ni.e., few available samples and weak text supervision?\nThis paper aims to answer this question based on the following observations: (1) Semantic text embedding spaces do not necessarily result from joint pre-training. In fact, the embedding spaces of encoder LLMs are inherently semantic and high-quality, as LLMs are trained on massive text data and demonstrate strong reasoning performance. (2) When the downstream data are limited, prompt learning [45, 25, 92, 42, 108] provides a better option than fine-tuning as much fewer parameters not only makes the optimization more efficient but also requires less resource than computing the gradient of a large model. Notably, some works have explored prompt learning for better alignment and obtained improvement in vision prediction [105, 39]. Inspired by these two observations, we propose a prompting-based paradigm with an LLM that, while keeping the parameters of both GNN and LLM frozen, aligns the GNN representations in the LLM's semantic embedding space.\nWhen attempting to adapt the representation from one modality to another, solely prompting a single modality could be sub-optimal, as it limits the adjustment to downstream tasks in the other modality [39]. To this end, we propose Multi-modal Prompt Learning for Graph Neural Networks (Morpher). Given a pre-trained GNN and few-shot semantically labeled graph data with weak text supervision, we assume zeroth-order access to a pre-trained LLM. Then, to leverage its high-quality semantic embedding space, Morpher connects and aligns the graph embeddings to it through prompting on both modalities with a cross-modal projector. Nonetheless, designing such a paradigm is more challenging than vision-language models. First, we lack jointly pre-trained encoders for the two modalities; instead, we only have two encoders whose embedding dimension is possibly different, pre-trained independently in each modality. Second, determining how to prompt the graph modality is non-trivial and remains a trending research topic. Third, the downstream data for GNN usually have much fewer labeled classes and labeled samples than V-L models, and the text supervision is extremely weak. Our contributions towards tackling these challenges are summarized as follows:\n\u2022 Theoretically, we analyze that, in many cases, state-of-the-art graph prompt [73] is unable to learn good representations of the downstream data. We show that the optimization of the graph prompt"}, {"title": "2 Background", "content": "We use calligraphic letters (e.g., A) for sets, and specifically G for graphs. We use bold capital letters for matrices (e.g., A). For matrix indices, we use $A(i, j)$ to denote the entry in the $i$th row and the $j$th column. Additionally, A(i, :) returns the ith row in A.\nGraph Neural Networks. We use G = (A, X) to denote a graph with node set V and edge set E, where A \u2208 R|V|\u00d7|V| is the adjacency matrix and X \u2208 R|V|\u00d7d is the node feature matrix. A(u, v) = 1 if there is an edge connecting u and v; otherwise A(u, v) = 0. A Graph Neural Network fo() with hidden dimension dg encodes G into the embedding space: ff(G) \u2208 R|V|\u00d7dg, which could preserve both feature and structure information of G. The extracted embeddings ff(G) can be used for various downstream tasks such as classification.\nFew-shot Prompt Learning. Prompt learning adds learnable tokens to the downstream data and provides a powerful alternative to fine-tuning when the labeled downstream data is scarce. Prompt learning for encoders was first used in NLP. Let f(\u00b7) denote the LLM encoder with embedding dimension d\u2081. For a series of input tokens {xk}Kk=1, the LLM encoder embeds it as a matrix\nXt = f({xk}Kk=1) \u2208 RK\u00d7dt, then aggregates the representation to a vector aggre(Xt) \u2208 R1\u00d7dt for downstream tasks. Prompt learning initializes a tunable matrix P \u2208 Rnt\u00d7dt, where nt denotes the number of text prompt tokens. Then, this tunable matrix is concatenated with the input tokens' embeddings to form a single matrix [P; Xt]dim=0 \u2208 R(K+nt)\u00d7dt, and the aggregated vector for downstream tasks becomes aggre([P; Xt]dim=0). In practice, we can train the model to minimize the loss function for downstream tasks, with only the prompt parameters Pt being updated.\nOur Problem Set-up. Given a pre-trained GNN f(\u00b7) with embedding dimension dg and a pre-train LLM encoder f(\u00b7) with embedding dimension dt. Without loss of generality, we assume the downstream task is graph-level classification, as we will show that the other types of GNN tasks can be reformulated as graph classification. For L-shot graph classification, we are given limited text-labeled pairs {(Gi,tc)}Ll=1 for each class c. Each text label tc consists of only a few tokens. Assuming T is the set of all text labels tc, we are provided a set of test graphs {Gj}Ltestj=1. Using the pre-trained GNN and LLM, we want to correctly predict the text label tj \u2208 T for each test graph Gj."}, {"title": "3 Revisiting and Improving Prompt as Graphs", "content": "Unlike prompting text data (which appends learnable text tokens to the original text sequence) and prompting image data (which pads a learnable image area above the original image), prompting graph data presents a significant challenge due to the non-euclidean nature of graphs. The recent pioneering work [73] designs the graph prompt still as a graph, then inserts it into the original graph by computing the inner-connections within the prompt graph and the cross-connections between the prompt graph and the original graph. An advantage of prompting at the graph level is that the downstream tasks of GNN can be reformulated into graph-level tasks. For the node classification task, we can induce the \u03b3-ego-graph of each node by extracting the subgraph within a pre-defined distance \u03b3. Then, we treat the node label as the induced ego-graph label. Similarly, for the edge classification task, we can extract a subgraph for each edge by extending the node pair to their \u03b3"}, {"title": "3.1 Lemma", "content": "Lemma 3.1. For any classifier c(\u00b7), if the identical feature x has label distribution p(\u00b7), then the optimal classification for cross-entropy loss is Pr(c(x) = y) = p(y). From this, if two graphs have similar embedding but different labels, GNN training may not converge. (Proof in Appendix A)"}, {"title": "4 Multi-modal Prompt Learning for GNNS", "content": "To adapt the GNN embeddings to the LLM's semantic embedding space and leverage the additional weak supervision provided by the text associated with graph labels, we explore the potential of multi-modal prompt learning for both graphs and language. This approach is motivated by the intuition that only prompting on the graph data may limit the flexibility to adjust the LLM representation space. The overall paradigm of Morpher is illustrated in Figure 3. Given the data {(Gi, ti)}IC, we aim to align graph embedding readout(ff(Gi)) with readout(fo(Tokenize(ti))). Yet one direct issue is that, readout(ff(Gi)) \u2208 R1\u00d7dg and readout(fo(Tokenize(ti))) \u2208 R1\u00d7dt may have distinct dimensions. To address this issue, we adopt a cross-modal projector that learns to map the graph embedding space to the text embedding space. For an input dg-dimensional graph embedding v, the projector maps it to a vector v in the dt-dimensional text embedding space:\nv = Proje(v) := tanh(Wv + b) \u2208 R1\u00d7dt\n(1)\nAs discussed in Sections 2 and 3, we introduce the text prompt P\u2208 Rnt\u00d7dt with nt text prompt tokens and the graph prompt P\u00e5 \u2208 Rng\u00d7d with ng graph prompt tokens. Let \u03c6g(\u00b7, P3) be the graph prompting function, e.g., given any graph G, the manipulated graph Gm = \u03c6g(G,P9).\nLet \u03c9t (, P) be the prompted text embedding given input text t. For the text prompt methods we choose, the prompted embedding is\n\u03c9t (t, Po) = [Pb; f(Tokenize(t))]dim=0 \u2208 R(len(Tokenize(t))+nt)\u00d7dt\n(2)\nLet \u03c9g(, P3) be the prompted graph embedding given input graph G, then we have:\n\u03c9g(G,P9) = f(Gm) = f\u2081(\u03c6g(G,P%)) \u2208 R(n+ng)\u00d7dg\n(3)"}, {"title": "5 Experiments", "content": "We evaluate our Morpher and the improved graph prompt through extensive experiments. In particular, we show that, compared to state-of-the-art baseline methods, they both more effectively adapt pre-trained GNNs to the specific downstream classification task, and introducing the text modality brings Morpher additional advantages over others. We use ROBERTa [53] as the LLM encoder for Morpher in the main experiments. We also validate the performance of Morpher with ELECTRA [8] and DistilBERT [67] in section 5.6 and Appendix C.3.\nDatasets. We use real-world graph datasets from PyTorch Geometric [14], including one molecular dataset MUTAG [61]; two bioinformatic datasets ENZYMES and PROTEINS [3]; one computer vision dataset MSRC_21C [63]; three citation network datasets Cora, CiteSeer and PubMed [88]. We use real-world class names as text labels. The text supervision is extremely weak, as each text label contains no more than five words. More details are summarized in Appendix B.\nPre-trained algorithms and GNN backbones. To pretrain GNNs for evaluation, we adopt GraphCL [89] and SimGRACE [82] to pre-train three widely used GNN backbones: GCN [41], GAT [90] and GraphTransformer (GT) [40]. Additionally, in Appendix C.4, we verify the effectiveness of our methods on GNNs pre-trained using GraphMAE [24] and MVGRL [21], two other representative GNN self-supervised learning algorithms. For each dataset, to pre-train GNNs, we leverage self-supervised learning methods on all the graphs without any label information.\nBaselines and metrics. We compare our methods with the following baselines: (1) training a GNN from scratch supervised by few-shot data (\u201csupervised\u201d); (2) fine-tuning a task head together with pre-trained GNN (\u201cfine-tune\u201d). We allow GNNs to be tunable for \u201csupervised\u201d and \u201cfine-tune\u201d; (3) state-of-the-art graph prompting algorithms: All-in-one (\u201cAIO"}, {"title": "5.1 Few-shot Learning", "content": "We investigate the ability of our improved graph prompt (\u201cImprovedAIO\u201d) and Multimodal prompt (\"Morpher\") to adapt frozen pre-trained GNNs using few-shot data. We focus on graph-level classification here and will further investigate the few-shot learning ability at other task levels in Section 5.2. Our few-shot learning setting is more challenging than existing works [73, 72] as we only allow no more than 10 labeled training and validation samples for each class. The results are shown in Table 1, where we report the average performance of 5 runs and calculate the absolute average improvement of our methods. From the results, given the same pre-trained GNN, our ImprovedAIO outperforms all the existing baseline methods except for ENZYMES F1 and MSRC_21C accuracy. Yet the performance of our ImprovedAIO on ENZYMES F1 and MSRC_21C accuracy is clearly"}, {"title": "5.2 Morpher Supports Multiple-level Tasks", "content": "Inherited from AIO, our ImprovedAIO and Morpher also support adaptation to downstream tasks at node-level and edge-level, because they can be reformulated into graph-level tasks as discussed in Section 3. We demonstrate the performance of node classification and link prediction on Cora and CiteSeer. For node classification, we reformulate it to graph classification by inducing an ego-graph with 10 to 30 nodes centered at the node to classify. Each ego-graph has the same label as the center node. For edge classification, we randomly sample 200 edges from the graph, then create 200 negative samples by replacing one node in each edge. We label each graph according to whether it is a positive or negative sample.\nWe use GraphCL+GCN to pre-train the GNN and report the mean performance in Table 2. The results are consistent with graph-level performance, where ImprovedAIO and Morpher outperform existing methods, with Morpher achieving slightly better performance than ImprovedAIO. Additionally, the training of the original AIO fails on both datasets due to the sparse node feature vectors."}, {"title": "5.3 Domain Transfer", "content": "A key problem of the graph foundation model is whether we can adapt the pre-trained models to other data domains. Here, we explore the potential of using Morpher for such adaptation. We pre-train GNNs on ENZYMES or CiteSeer datasets, then test the classification performance on MUTAG and PubMed and report the results in Table 3. We unify the pre-train feature dimension with the downstream feature dimension by padding zeros or SVD reduction. From the results, Morpher demonstrates the best transferability, followed by ImprovedAIO. Also, compared to the results on MUTAG in Table 1, all three methods have worse performances, because the GNNs are pre-trained on other datasets instead of MUTAG."}, {"title": "5.4 Zero-shot Classification Prototype", "content": "An advantage of adapting pre-trained GNNs to the semantic embedding space is that GNNs might be empowered to \"reasoning\u201d. Here, we conduct a novel experiment that generalizes GNN to an unseen class. Since no real-world data is available for this setting, we synthetically create three datasets, ZERO-Cora, ZERO-CiteSeer, and ZERO-PubMed, all from real-world connections. We aim to simulate a citation network with two research areas and an interdisciplinary research area in between. For each citation network, we randomly sample 120 nodes and induce their 2-hop ego-graphs, then replace the node features in 10 ego-graphs with [1,0] and another 10 ego-graphs"}, {"title": "5.5 Efficiency and Embedding Analysis", "content": "Without fine-tuning the GNN or LLM, the prompt-based methods have better parameter efficiency. As shown in Figure 5 (left), our ImprovedAIO and Morpher require similar numbers of parameters with AIO [73], which is 0.032% to 0.46% compared to either tune the LLM (RoBERTa) or GNN (GCN). Due to such parameter efficiency, our methods learn better graph representations given few-shot data. We visualize the graph embeddings of CiteSeer and MSRC_21C in Figure 4 and calculate the silhouette score, a metric for cluster quality (\u2191) ranged in [-1,1]. It turns out that our multimodal prompting leads to better adaptation."}, {"title": "5.6 Hyperparameter and Ablation Study", "content": "We conduct the hyperparameter study by choosing and testing various numbers of graph prompt tokens for both ImprovedAIO and Morpher. The results are shown in Figure 5 (middle), from which we can observe that both methods are generally stable, and Morpher constantly outperforms ImprovedAIO under different choices. To verify the necessity of each component in our design, we compare Morpher and ImprovedAIO with multiple variants, respectively, and report the result in Figure 5 (right). We observe that removing any component would result in a performance drop. Additionally, our comparison of Morpher with ImprovedAIO throughout the experiments demonstrates that our multimodal design would lead to improvement over the uni-modal prompting of GNNs."}, {"title": "5.7 Morpher on MolecureNet with More Text Supervision", "content": "We demonstrate that, though not specifically designed for any downstream applications, the Morpher framework has the potential to be used in various tasks where there is more text supervision compared to previous experiments. As for a case study, We use bace (inhibitors of human beta-secretase), tox21 (toxicology in the 21st century) and hiv (inhibit HIV replication) from MolecureNet [81]. These three datasets have 1513, 7831, and 41127 graphs to classify, respectively. In these datasets, each graph label is associated with a text description. The tasks on bace and hiv are bio-activity prediction and the task on tox21 is toxicity prediction. To adopt Morpher, we use GraphCL to pre-train the GAT model and initialize the text prompts and text labels using those from GIMLET [95]."}, {"title": "6 Related Work", "content": "GNN Pre-training. Recently, a surge of graph pre-training strategies have emerged to address the issue of label scarcity in graph representation learning [26, 58, 72, 43, 104, 1, 36]. The main idea of pre-trained graph models is to capture general graph information across different tasks and transfer this knowledge to the target task using techniques such as contrastive predictive coding [40, 12, 13, 65, 82], context prediction [62, 27], prompt tuning [72, 11], and mutual information maximization [62, 71, 30, 35, 37]. For instance, [26] proposes to learn transferable structural information from three levels of graph topology, including node-level, subgraph-level, and graph-level. Different from these approaches, this paper aims to build up foundational GNNs by leveraging multi-modal prompt learning techniques.\nGraph Prompt Learning. Prompting is now mainstream for adapting NLP tasks, and recent studies exploring prompt learning for GNNs mark a thriving research area [74, 80]. It is a promising way to adapt GNNs to downstream tasks through token-level [11, 75, 5, 72, 107] or graph-level [73, 28, 19] prompting. Among all the existing methods, All-in-one (AIO) [73] is the only algorithm to learn tunable graph prompts for node-level, edge-level or graph-level downstream tasks given few-shot labeled data (Table 8). Based on our improved AIO, we present a pioneer study to explore learning prompts in multiple modalities simultaneously while keeping the pre-trained models frozen.\nLLM on Graphs. Inspired by the advances of large language models in NLP [98], researchers have begun to explore their potential for graph-related tasks [31]. Current approaches can be divided into two main categories. The first category employs LLMs as pre-trained feature extractors to enhance GNNs [9, 7, 106]. For example, GLEM [97] proposes to input the language representation as initial features for the GNN and train them iteratively. The second category focuses on integrating graph structures directly into LLM architectures [87, 94, 34]. A notable example is Patton [33], which pre-trains a joint architecture on text-attributed graphs. Despite these advancements, none of them have explored the collaboration between LLMs and GNNs under graph prompt learning."}, {"title": "7 Conclusion", "content": "In this work, we introduce Morpher, the first multimodal prompt learning paradigm that can semantically adapt pre-trained GNNs to downstream tasks with the help of LLM, while keeping both the pre-trained models frozen. To build Morpher, we first analyze the limitations of the state-of-the-art graph prompting technique and propose an improved version. Through extensive experiments, we demonstrate that our improved AIO can achieve outperformance, and our Morpher has further improvements in few-shot, multi-level task, or domain transfer settings. Additionally, using Morpher, we build the first GNN zero-shot classifier prototype that can be generalized to novel testing classes."}, {"title": "C.1 Reproducibility", "content": "Code. The code for the experiments is provided in the supplementary material with a well-written README file. We also provide the commands and instructions to run the code. The datasets used will be automatically downloaded when the code is executed.\nEnvironment. We run all our experiments on a Windows 11 machine with a 13th Gen Intel(R) Core(TM) i9-13900H CPU, 64GB RAM, and an NVIDIA RTX A4500 GPU. We have also tested the code on a Linux machine with NVIDIA TITAN RTX GPU. All the code of our algorithms is written in Python. The Python version in our environment is 3.9.18. In order to run our code, one has to install some other common libraries, including PyTorch, PyTorch Geometric, pandas, numpy, scipy, etc. Please refer to our README in the code directory for downloading instructions.\nWe have optimized our code and tested that the space cost of the CPU memory is less than 16 GB, and the space cost of the graphics card is less than 6 GB. The execution time to run an experiment is less than 20 minutes on our machine."}, {"title": "C.2 Implementation Details", "content": "We provide the configuration files for the experiments to reproduce the results. We initialize the graph prompt using kaiming_initialization, and we initialize the text prompts through real token embeddings. We have tested multiple initializations, and they would not affect the overall results. Specifically, we initialize the text prompt for each dataset as follows.\nMUTAG: \"a graph with property\u201d; ENZYMES: \u201cthis enzyme is\u201d; PROTEINS: \u201cthis protein is\"; MSRC_21C: \u201can image of\u201d; Cora: \"a paper of\"; CiteSeer: \"a paper of\u201d; PubMed: \u201ca paper of\u201d; Edge tasks: \"central nodes are\u201d.\nIn our few-shot setting, we split the labeled data into training samples and validation samples at approximately 1:1. For all the parameters, we used the Adam optimizer, whose learning rate and weight decay are provided in the configuration files."}, {"title": "C.3 Experiment with ELECTRA and DistilBERT", "content": "On the LLM pre-training side, RoBERTa is one of the most advanced encoder-only LLMs until now, and we have demonstrated the effectiveness with RoBERTa serving on the LLM side in the Morpher paradigm. Additionally, we conducted experiments with ELECTRA [8] and DistilBERT [67]. Using these two LLMs, Morpher can also achieve comparable performances to RoBERTa."}, {"title": "D Limitations", "content": "Graph prompt learning assumes the \"pre-train + prompt\" framework to build graph foundation models, yet there could be other paths to achieve graph-related foundation models.  Also, graph"}]}