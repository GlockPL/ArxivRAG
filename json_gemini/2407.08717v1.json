{"title": "WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics", "authors": ["Abdollah Zakeri", "Hamid Hassanpour", "Mohammad Hossein Khosravi", "Amir Masoud Nourollah"], "abstract": "Lip-based biometric authentication (LBBA) has attracted many researchers during the last decade. The lip is specifically interesting for biometric researchers because it is a twin biometric with the potential to function both as a physiological and a behavioral trait. Although much valuable research was conducted on LBBA, none of them considered the different emotions of the client during the video acquisition step of LBBA, which can potentially affect the client's facial expressions and speech tempo. We proposed a novel network structure called WhisperNetV2, which extends our previously proposed network called WhisperNet. Our proposed network leverages a deep Siamese structure with triplet loss having three identical SlowFast networks as embedding networks. The SlowFast network is an excellent candidate for our task since the fast pathway extracts motion-related features (behavioral lip movements) with a high frame rate and low channel capacity. The slow pathway extracts visual features (physiological lip appearance) with a low frame rate and high channel capacity. Using an open-set protocol, we trained our network using the CREMA-D dataset and acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering that the acquired EER is less than most similar LBBA methods, our method can be considered as a state-of-the-art LBBA method.", "sections": [{"title": "I. INTRODUCTION", "content": "Biometric methods for person verification and identification have attracted much attention among researchers in the last decade, thanks to the numerous advantages they have compared to old authentication methods like passwords and Personal Identity Numbers (PINs). Unlike the old methods mentioned above, biometric traits cannot be forgotten or transferred to another person. Furthermore, a client's biometric data cannot be stolen, and they are pretty hard to replicate. Considering the aforementioned merits, biometric methods provide people authentication systems with more security. Although some solutions like two-factor authentication have been proposed to improve the security of passwords, these methods are less resistant to spoofing attacks compared to biometric-based authentication systems.\nBiometric methods are divided into two main categories, namely physiological and behavioral traits. The former is defined as measurable and unique aspects of the human body which can serve as an identity, like face [1], fingerprint [2], palm print [3], hand geometry [4] and iris [5], while the latter is defined as any behavioral pattern that is done in a unique manner by individuals and hence, can be used as a means of identification. Common methods using behavioral traits are gait recognition, signature recognition, speaker identification, handwriting\nidentification, and keystroke dynamics. Using lips as a biometric trait has attracted much attention in the last decade due to their potential to be employed both as a physiological and a behavioral trait for different tasks such as person identification and verification. Visual features like the color and geometry of the lips can be considered a physiological trait, and the dynamics of the lips while speaking is a behavioral trait. Although two people might utter the exact same words and convey the same concept, the dynamics of their lips will not be the same. As a result, the combination of lips' visual features and the dynamics of the lips while uttering a phrase provides enough uniqueness for a secure biometric system. Numerous valuable research articles proposed using the audio modality along with the video of the lips, or even as a single trait to provide the discrimination power required for a biometric system. However, there are several downsides to incorporating the audio modality like the detrimental effect of noisy audio on the overall accuracy of the biometric system. Additionally, incorporation of audio modality decreases the applicability of the biometric system in places in which silence is a requirement like in a library. Furthermore, speech-impaired people cannot use these systems which is a major drawback. Due to the abovementioned reasons, the audio modality reduces the biometric authentication system's applicability; therefore, we use a visual-only approach in our proposed method. Lip-based biometric authentication (LBBA) method is a very straightforward and practical way of authentication since it does not need any special equipment for the video acquisition, which can be done using the front-facing camera of a regular smartphone. Furthermore, this method can be implemented in a very light and efficient way to be used on mobile devices without any significant processing power. LBBA has attracted many researchers because of its merits and advantages over other biometric methods. Biometric traits necessitate uniqueness for each person, and lips are proven to provide this uniqueness [6], [7].\nHuman lip functions both as a physiological and a behav-ioral trait, considering that every person might utter the same words uniquely [8], [9]. LBBA is more robust and secure than other physiological-only biometric systems. Incorporating lip movements as a behavioral trait makes spoofing attacks almost impossible since the attackers must model a person's talking habits, which is a highly complex task and requires an abundance of data. Furthermore, the video acquisition"}, {"title": null, "content": "process for LBBA does not require any special sensors or instruments and can be effortlessly done using a video camera. This feature makes LBBA more applicable than other biometric methods requiring specific means of measurement. Additionally, LBBA can be combined with other biometric methods such as face to increase the security and robustness of the biometric system. Moreover, LBBA is more hygienic than other biometric methods like fingerprint which require the client to touch the sensor. Most visual biometric systems encounter the challenge of variations in clients' appearance over time. In a facial verification system, for instance, the growth of facial hair in men and the variation of makeup in women can sometimes increase the error rates of the biometric system. To solve this issue, the visual biometric systems must be trained using a large enough dataset containing a fair percentage of the possible variations in the data that the system might face in practical applications. Although some physiological biometric methods like fingerprint and iris recognition do not suffer from this issue, the LBBA method will face this challenge given that the clients may have several emotional states at the time of authentication, which will potentially affect their lip movements during an utterance. This change in the lip dynamics will decrease the similarity between the video acquired for enrollment and the video acquired for authentication. Consequently, this issue will increase the False Rejection Rate (FRR) of the biometric system. One possible solution for decreasing the FRR of the system would be to decrease the threshold so that the system is less strict. Nevertheless, there is a tradeoff between False Acceptance Rate (FAR) and FRR, and changing the threshold in favor of decreasing the FRR would increase the FAR. Another solution is to train the system so that it becomes inattentive to these minor changes and attends only to the static characteristics. In the domain of LBBA, the previous solution is translated as training the system so that it becomes invariant to all the emotions that the client might have during the authentication video acquisition. This method is only possible if we have the required data to train the biometric system. In this paper, we extend our previous work, called WhisperNet [10], by improving the proposed network architecture by substituting the embedding network with the SlowFast architecture which captures both physiological and behavioural features of lip videos, resulting in better performance and higher accuracy.\nThe main contributions of this paper to the field are as follows: (i). Addressing the challenges that visual biometric systems face due to different emotions that the clients might have during authentication video/image acquisition and proposing a possible solution. (ii). Improving the network architecture proposed in our previous WhisperNet research in order to acquire both more efficient performance and higher accuracy. (iii). Introducing a state-of-the-art architecture for LBBA which can be adapted to other domains such as face verification/identification. In the following sections of this paper, we go through the previous related works in Section 2, introduce the dataset and the pre-processing methods in Section 3, introduce the proposed network architecture in Section 4, and test results and experiments are presented in Section 5. The conclusion, followed by suggestions for future\nworks, are presented in Section 6."}, {"title": "II. LITERATURE REVIEW", "content": "The first step of pre-processing for an LBBA system is to localize the lip region using image segmentation. Lip segmentation methods are primarily classified into contour-based [11] and clustering-based techniques [12], [13]. An unsupervised segmentation method was proposed by Chan that used Gaussian Mixture Models (GMMs) for lip localization [11]. A Fuzzy C-Means (FCM) algorithm was used by Fu et al. in [12] for lip segmentation. Furthermore, Lu and Liu [14] used the Localized Active Contour Model (LACM) and two initial contours for lip segmentation. The task of feature extraction from lip images is the next step in an LBBA system. Earlier research articles used low-level hand-crafted features like color, shape [15], texture, and geometry [16] and mid-level feature extraction methods like Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) to extract visual features from the lip area. Behavioral features contained temporal data and were extracted using models such as Hidden Markov Model (HMM) and GMM [15]. With the rise of deep learning techniques, Convolutional Neural Networks (CNNs) replaced the old-fashioned hand-crafted feature extraction techniques for visual feature extraction. The temporal data (lip movement among video frames) was modeled using RNNs like LSTM or GRU since the movements of the lips can be modeled as sequential data. In [15], authors defined an Active Shape Model (ASM) based on lip boundary and intensity parameters. Furthermore, they used HMM and GMM for speaker identification and achieved 97.9% accuracy on a small dataset containing only 12 clients. Wark et al. [17] used lip contour profiles and a multi-stream HMM for identification and acquired 80% accuracy on the XM2VTS dataset. Lai et al. [18] proposed using Sparse Coding (SC) to characterize the movements in the lip region followed by a max pooling on a spatio-temporal hierarchical structure in order to produce the final features. They tested their method on a private dataset of 40 clients and achieved a Half Total Error Rate (HTER) of 0.46%. Wright and Stewart [19] trained a Siamese network using the LipNet structure containing STCNN and biGRU layers as the embedding network for the task of speaker verification under a closed-set Lausanne protocol (all subjects are enrolled during training) and achieved 1.03% EER on the XM2VTS dataset. Their next article [20] tested their architecture under a more restrictive open-set protocol (new subjects are enrolled during validation and test) and achieved an EER of 1.65%. Kuang et al.[21] introduced LipAuth, leveraging unique spatial-temporal features of human lips, achieving a 99.24% accuracy for user authentication and demonstrating robustness against video replay and mimic attacks. Similarly, Koch and Grbi\u0107 [22] addressed vulnerabilities in existing LBBA methods by introducing the GRID-CCP dataset and training a siamese neural network with 3D convolutions and recurrent neural network layers, achieving a False Acceptance Rate (FAR) of 3.2% and a False Rejection Rate (FRR) of 3.8%. An overview of existing LBBA methods and datasets are provided in tables I and II accordingly."}, {"title": "III. DATASET AND PRE-PROCESSING", "content": "Numerous datasets are available that can be used to train an LBBA network [24], [28], [31], [32], [33], [34], [35]. To the best of the authors' knowledge, none of them include emotion-related data concerning the subjects' facial expressions or variations in utterance and speech tempo due to different emotional states. Hence, we used the CERMA-D dataset [30] to train our network, which contains 7,442 videos of 91 subjects uttering 12 phrases in 6 different emotional states, including neutral, happy, sad, anger, disgust, and fear. Nevertheless, there were data shortages for three clients, so we excluded them from the dataset, and therefore, we were left with 6,336 videos and 88 clients. Although the size of our dataset is smaller than other datasets like XM2VTS [24], our dataset contains emotional data that provides various utterances of the same phrase by the same subject but with different facial expressions and speech tempos. Considering that the network is trained assuming that the different utterances of the same phrase by the same person are equivalent, the network gradually becomes invariant to the small visual changes that are caused by different emotions. An overview of the datasets applicable in LBBA is listed in table II.\nOut of the 88 people in our dataset, 66 were assigned to the training set, 11 to the validation set, and 11 to the test set. Since the clients in the test set are not enrolled during training, we used an open-set protocol [30]. Since the dataset included full-face videos, we needed to crop each frame of every video to the lip region so that the network would not take advantage of any data other than lips. We used an open-source Python library called Face Recognition [36] to perform this task. We extracted the lip landmarks from every frame of each video, and then using these landmarks, we selected a bounding box containing only the lips and cropped the whole frame into that bounding box. After cropping, we resized each cropped image to the size of the smallest lip image (30\u00d718).\nFurthermore, the bounding boxes were all chosen in a way that they would have the same aspect ratio in order to minimize the interpolations during resizing. The complete algorithm for pre-processing of the database is presented in Algorithm 1."}, {"title": "IV. PROPOSED METHOD", "content": "There are two common steps in biometric authentication systems. The first step is called enrollment and includes generating the corresponding embedding for each client based on their biometric features and saving this embedding along with a unique ID in the clients' database. The next step is authentication, in which the system obtains biometric information from the client and generates the corresponding embedding based on this obtained information. Next, the system compares this newly generated embedding with the one stored in the database, and if the two embeddings' similarity surpasses a static threshold, the person is successfully authenticated. Aside from the pre-processing and data preparations, which are specific to our method, LBBA has the same two steps. A flowchart of our proposed LBBA method is demonstrated in Fig. 1.\nThe embedding is a network mapping the input data to latent space, and its output is called the embedding or the encoding. In order to train this network, we used a Siamese architecture with the triplet loss function [37], [38]. The Siamese architecture consists of three identical branches that take triplets of input videos and generate the corresponding embeddings for each item of the triplet tuple. The structure"}, {"title": null, "content": "of the Siamese network is presented in Fig. 2. Each triplet includes three videos, namely the anchor (A), the positive (P), and the negative (N). The anchor and positive are two videos of the same person uttering the same phrase. The negative can be any video that does not fit into the definition of positive. The corresponding embeddings for A, P, and N are generated using three identical embedding networks. Using the triplet loss function, the network gradually learns to generate embeddings so that the distance between A and P embeddings is less than the distance between A and N embeddings. The triplet loss is defined as follows:\n$\\mathcal{L}(A, P, N) = max(D(A, P) - D(A, N) + \\alpha, 0)$ (1)\nThe loss function ensures a minimum margin with the value of $\\alpha$ between any of the anchor-positive and the anchor-negative distances. The distance function D can be any distance measure like cosine or Euclidian distance. We chose cosine similarity as the distance function and the value of 0.7 for the margin parameter. The cost function is calculated as follows:\n$S = \\sum_{i=1}^{m} \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})$ (2)\n(m denotes the total number of samples in the dataset)\nWe used SlowFast architecture (Fig. 3) as the embedding network [39]. This network was initially proposed for video recognition tasks. It passes the same video into two processing pathways: the slow pathway and the fast pathway. The fast pathway processes the input video with a high Frame Per Second (FPS) (low temporal stride) to extract motion-related data from consecutive frames. The slow pathway processes the input video with low FPS (high temporal stride) to extract static visual features. Although the fast pathway has a higher temporal resolution, it has lower channel capacity than the slow pathway. There are two main hyperparameters for SlowFast networks, namely $\\alpha$ and $\\beta$. The $\\alpha$ parameter represents the temporal stride ratio between the slow and fast pathways. For example, if the temporal strides of the fast and slow pathways are 2 and 16 accordingly, then $\\alpha$=8, which is the typical value for this parameter according to the literature [39] The $\\beta$ parameter represents the channel ratio between the"}, {"title": null, "content": "slow and fast pathways ($\\beta$\u00a11). To sum it up, both the slow and fast pathways are identical CNNs, but the fast pathway has a low temporal stride (high temporal resolution) and low channel capacity. In contrast, the slow pathway has a high temporal stride (low temporal resolution) and high channel capacity. As mentioned before, the lip can be used as a twin biometric, considering its physiological and behavioral features. This capability of the lips makes the SlowFast networks an ideal candidate for the task of LBBA since, on the one hand, the fast pathway extracts motion-related features from the lip video, which are the behavioral traits related to lip movements during an utterance. These motion-related data do not need high channel capacity, making the fast pathway an excellent candidate for behavioral feature extraction. On the other hand, the physiological features are static visual features that do not change much during the video. Hence, the extraction of these features does not need high temporal resolution, but its high channel capacity would be beneficial, and this is the exact description of the slow pathway. At the final layer of the SlowFast architecture, the outputs of the two pathways are concatenated, forming our final embedding. This SlowFast embedding network uses physiological and behavioral lip features to generate the final embedding vector. Additionally, several lateral connections between the two pathways fuse the information of the two pathways before the final concatenation layer, which makes each pathway aware of the representations learned by the other. However, the authors of [39] found similar results while using bidirectional and unidirectional lateral connections, and therefore, in the implementation of this network, these lateral connections are unidirectional con-nections from the fast pathway to the slow pathway. The result of each \"stage\" of the fast pathway is concatenated with the result of the previous stage before being entered to the next stage of the slow pathway. According to [39], since the two pathways have different temporal dimensions, the lateral connections must perform a transformation to match them.\nOne of the crucial tasks in training Siamese networks with triplet loss is the selection of triplets. Wright and Stewart [20] classified the triplets into three categories:\n(i) Easy triplets are triplets in which the distance between the anchor and positive embeddings is less than the distance between anchor and negative:\n$D(A,P) + \\alpha < D(A, N)$ (3)\n(ii) Semi-hard triplets are triplets in which the positive is closer to the anchor than the negative but still generate a positive loss since the negative stays within the margin:\n$D(A,P) < D(A, N) < D(A, P) + \\alpha$ (4)\n(iii) Hard triplets are triplets in which the distance between the negative and anchor is less than the distance between the positive and the anchor:\n$D(A,P) > D(A, N)$ (5)\nUsing only hard triplets stops the network from converging, and using only easy triplets will not contribute much to decreasing the loss value. So we selected triplets randomly from all the possible train triplets:"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "All training and evaluations were performed on a linux-based system with 2\u00d7Nvidia A100 GPU's, 512 GB DDR4 RAM, and an AMD EPYC 7H12 64-core processor. With the aforementioned configuration, a single inference step including I/O tasks, preprocessing, and feeding the input videos to the embedding network to get the final result, takes \u22482.37 seconds (1.43s for reading pair of videos, 0.42s for preprocessing, 0.52 for generating embeddings and comparison).\nWe trained our network for 3800 iterations, however the criterion for ending training (I\u00a1Threshold) was met before 2000 iterations. In each iteration, we sampled a random batch (batch size = 512) from the training dataset and due to a large number of training samples, there was no overfitting during the training session. Training/Validation loss values are plotted in Fig. 4.\nThere are three standard metrics for the evaluation of biometric verification systems:\n1) False Rejection Rate (FRR): Percentage of the times a biometric verification system denies access to an authorized client. This error is known as Type I error in statistical terms.\n$FRR = \\frac{\\text{# of denied authentic requests}}{\\text{# of all requests}}$ (7)\n2) False Acceptance Rate (FAR): Percentage of the times a biometric verification system grants access to an im-poster. This error is known as Type II error in statistical terms.\n$FAR = \\frac{\\text{# of granted imposter requests}}{\\text{# of all requests}}$ (8)\n3) Crossover Error Rate (CER): Point where the FAR and FRR are equal. This metric describes the overall performance of the biometric verification system.\nAs we witnessed in our experiments, there is a tradeoff between FAR and FRR while setting the threshold value for the biometric system. In a perfect biometric verification system, both FAR and FRR would be 0% for all of the possible threshold values, and as a result, the EER would equal 0%. To test our proposed method, we selected 11 unseen clients and generated the corresponding triplets for them. According to a formula similar to (6), the number of test triplets would"}, {"title": null, "content": "be 3,112,560, including hard, semi-hard, and simple triplets. These triplets were sampled into batches of size 512, and for each triplet, the cosine similarities for anchor positive and anchor negative pairs were calculated. For every value of similarity threshold between 0 and 1, with a precision of 0.001, we calculated the FAR and FRR values. At the threshold of 0.983, the FAR and FRR values were equal and had the value of 0.005, which would be the EER of our proposed method. Additionally, to assess the performance of the network with different training populations, we sampled random subsets of training people and generated the training triplets accordingly. For comparison purposes, all trained networks were evaluated using a fixed test set (size=11 people). The results suggested that the overall performance of the network improves with increasing the size of the training set. For each training population size, the network was trained for 20 sessions and the training and validation datasets were shuffled before each session. The Average EER values for each training population are demonstrated in Fig. 5, and the loss values for the same experiments are plotted in Fig. 6."}, {"title": null, "content": "output of the fast pathway (behavioral features) to decrease the loss value, which improves the fast pathway.\nThe other set of challenging triplets are triplets in which the negative video has the same phrase as the anchor and the positive but uttered by other clients. In contrast with the former set of triplets, the fast pathways of embedding networks will generate similar embeddings. The network must use only the slow pathway (physiological features) to decrease the triplet loss value since the difference between physiological features of anchor and negative is more than the difference between their behavioral features. Consequently, the slow pathway of the embedding network will be improved by using these triplets."}, {"title": null, "content": "$\\binom{(66 \\times 12 \\times 6)}{\\text{Anchor}} \\times \\frac{5}{\\text{Positive}}$ (6)\n$\\binom{(65 \\times 12 \\times 6)}{\\text{Other People, all Phrases}} + \\binom{(1 \\times 11 \\times 6)}{\\text{Same Person, Other Phrases}} = 112, 764, 960$\n$\\{Negative\\}$"}, {"title": null, "content": "examine the concepts of true positives (TP) and false positives (FP) as they relate to the results. True positives represent instances where our biometric authentication system correctly identified a legitimate user, thus providing a measure of the system's accuracy and security. Conversely, false positives signify cases where the system erroneously accepted an imposter as a genuine user, which can lead to potential security vulnerabilities and usability issues. Comparing these metrics with those from previous studies, our proposed method demonstrates a remarkable advantage. The lower false positive rate in our model, evident in the considerably lower Equal Error Rate (EER) of 0.005, underscores its heightened security, as it effectively minimizes the risk of unauthorized access. In contrast, previous methods, such as Wright and Stewart's [20] with an EER of 0.136 and Chan et al.'s [25] with an EER of 0.059, exhibited higher false positive rates, indicating a higher susceptibility to imposter acceptance. These findings emphasize the significance of our model's enhanced security and its potential for real-world applications where minimizing false positives is paramount. In our research paper, our latest model demonstrates notable improvements over its predecessor, achieving a reduction of 0.033 in the Equal Error Rate (EER) while simultaneously enhancing com-putational efficiency. The architecture of our previous em-bedding network consisted of Spatio-Temporal Convolutional Neural Networks (STCNNs) followed by Gated Recurrent Units (GRUs), totaling 76,749,124 trainable parameters. In contrast, our current proposed embedding network employs the more streamlined SlowFast network with 52,314,144 trainable parameters, resulting in accelerated training and inference processes. Moreover, our previous method relied on a list of 24 lip landmark points to generate embeddings, making the network susceptible to errors. These errors in lip landmark extraction had a cascading effect on the embedding network, compromising both the overall performance and robustness of the model. To address this issue, our present study eliminates the lip landmark extraction step, effectively resolving this source of error."}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "This study presents a deep Siamese network for LBBA using the SlowFast network as the embedding network. The SlowFast architecture has a fast pathway with low temporal stride and low channel capacity, which extracts behavioral features, and a slow pathway with high temporal stride and high channel capacity extracting physiological features from the lip authentication video. We acquired an EER of 0.005 on the test set containing 11 unseen clients. Our model improves upon the performance, robustness, and efficiency of its predecessor by decreasing both EER and number of trainable parameters. Although the propoed method outperforms similar LBBA methods and the acquired results seems promising for real-world applications, some challenges have not been mentioned in this paper, like the performance of the proposed method under various lighting conditions and different resolutions. Another challenge in real-world applications would be the capacity of the network. Several factors directly impact the"}, {"title": null, "content": "capacity of our proposed network, namely the length of the uttered phrase in enrollment and authentication videos and the size of the embedding vector, which is the output of our SlowFast embedding network. Our current dataset has certain limitations, and performing evaluations on larger publicly available datasets would make our method more reliable and robust. We plan to create a more extensive dataset for the LBBA task in the near future to facilitate research in this field and provide a benchmark for the fair comparison of different LBBA methods."}]}