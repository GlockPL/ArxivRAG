{"title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures", "authors": ["Ishaan Domkundwar", "Mukunda NS"], "abstract": "Al agents, specifically powered by large language models, have demonstrated exceptional capabilities in various applications where precision and efficacy are necessary. However, these agents come with inherent risks, including the potential for unsafe or biased actions, vulnerability to adversarial attacks, lack of transparency, and tendency to generate hallucinations. As AI agents become more prevalent in critical sectors of the industry, the implementation of effective safety protocols becomes increasingly important.\n\nThis paper addresses the critical need for safety measures in Al systems, especially ones that collaborate with human teams. We propose and evaluate three frameworks to enhance safety protocols in Al agent systems: an LLM-powered input-output filter, a safety agent integrated within the system, and a hierarchical delegation-based system with embedded safety checks. Our methodology involves implementing these frameworks and testing them against a set of unsafe agentic use cases, providing a comprehensive evaluation of their effectiveness in mitigating risks associated with Al agent deployment.\n\nWe conclude that these frameworks can significantly strengthen the safety and security of Al agent systems, minimizing potential harmful actions or outputs. Our work contributes to the ongoing effort to create safe and reliable AI applications, particularly in automated operations, and provides a foundation for developing robust guardrails to ensure the responsible use of Al agents in real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's world and workforce, Al agents and agent systems are far more prevalent and widely used than before and their effectiveness makes them a viable tool for many to use. This widespread adoption has also led to applications where Al systems collaborate with humans for their tasks, for example new \"AI Employees\u201d like Ema[1]. Other notable examples include UIPath's AI-powered Robotic Process Automation [2], PathAI for pathology [3], and IBM Watson [4]. With this increase in AI usage, their presence in our daily lives and the work of companies carries significant safety considerations and possible danger [5],[6]. AI, though possessing the potential to match or surpass human efficiency and cost-effectiveness in the coming years, has significant risks associated with it. It can potentially enable societal-level safety issues and poses big risks that come with autonomy including the exploitation of technical oversight or testing, unpredictable behavior, a lack of transparency with opaque AI decision-making, and possible biases [6]. Moving forward, ensuring the safety and reliability of our large-scale AI solutions is imperative.\n\nAl agents are being deployed for many purposes from small user-oriented apps and tools to large-scale industry-level applications and solutions. While large strides have been made, Al agents, especially LLM-based multi-agent systems have some challenges. These include optimizing task allocation between agents, facilitating reasoning to produce better outputs from initial content, managing the context of the task across the system, and managing memory across the system [7]. As mentioned before, the integration of AI into more critical applications and scenarios poses significant and possibly societal risks and places more pressure on current issues [8]. Agents can be leveraged to enhance malicious activity with their automation and potential execution of harmful tasks. Overreliance on Al Agents at an industry level is also a major risk with widespread usage of AI. This highlights the importance of transparency in AI systems, and safety measures should be implemented to ensure an accurate understanding of the risks and accountability [8].\n\nContextualized with Large Language Models and AI, a multi-agent system contains multiple AI agents, each powered by LLMs, that collaborate to produce an output. Multi-agent systems are deployed in many contexts including robotics, software development, and other various applications. These systems execute complex tasks, make decisions, and interact with their environment [9]. These systems can be enhanced by providing them with tools, data, and memory capabilities. The primary purpose of this paper is to present and further evaluate three different frameworks aimed at enhancing the safety capabilities of current Al agent systems. These three frameworks work to mitigate or remove potential risks while interacting with agent systems and to ensure the safe functioning of agent systems when integrated into real-world environments, specifically with the interaction between agent systems and humans and the numerous safety risks that stem from that context. This paper examines agent system safety from both theoretical"}, {"title": "II. RELATED WORK", "content": "We look at the field of guarding or censoring unsafe Al-generated content. This field has been explored in literature, albeit not expressly at an agent level. Most works cover content moderation and unsafe content detection for Large Language Models. LlamaGuard [10] introduces a safety model built upon Llama2-7b for applications in conversational AI. It uses a safety risk taxonomy to safeguard Al prompts and responses. There are also datasets geared towards guarding not only unsafe prompts but also LLM-generated content. ToxicChat [11] is a dataset meant for LLM outputs based on user prompts, unlike many datasets that focus on the classification of user prompts."}, {"title": "B. AI Safety and Ethics", "content": "The paper \"The Human Factor in AI Safety\" [12] aligns with the context of this paper, discussing the important role that human elements play in Al safety. It outlines the potentially harmful results of Al due to design flaws or oversights in deployment, both human issues. It highlights the potential risks in Al-human interaction including problems of overreliance on AI systems, misinterpretation of intent, and lack of transparency in systems. Additionally, Al systems can be exploited for malicious use, which is a major risk with widespread AI integration [13]."}, {"title": "C. AI Agents", "content": "This paper focuses on presenting safety frameworks using an LLM-powered AI agent system. The paper \"Large Language Model-based Multi-Agents: A Survey of Progress and Challenges\" [9] addresses the progress and challenges of multi-agent systems that use LLMs, informing the current state of LLM-powered AI agent systems. The paper covers the importance of task allocation capabilities, which is also present in one of our safety frameworks. The paper acknowledges the potential impact of multi-agent systems in the real world but suggests further research is needed to overcome current limitations. \"Visibility into AI Agents\" [8] assesses the safety risks that come with the increased usage of AI agents. This paper highlights the need for robust safety mechanisms for risk management. It discusses the need for transparency measures, malicious use prevention, and the risks associated with overreliance.\n\nAlthough research into Al safety is a populated field, given the relative recency of significant advancement in AI systems, not much research addresses the possible safety frameworks for Al agent systems. While research outlines the risks associated with these systems, there is a need for safety frameworks that mitigate risks in Al agent usage. This paper primarily aims to introduce comprehensive safety frameworks geared towards agent systems."}, {"title": "III. BASELINE: GENERATION OF UNSAFE REQUESTS", "content": "As previously defined, the scope of this paper will be to propose enhancements to Al agent system safety and their implementations. The AI systems used in this paper and their agents are powered by the more recent and capable models on the market. The main aim of this paper is to create frameworks that can apply to the live usage of these agent systems as opposed to safety through the lens of unsafe content detection or classification.\n\nCurrent LLMs and AI tools are susceptible to manipulation and can generate unsafe content if given the right prompt, often known as \u201cjailbreaking\u201d. Jailbreaking is essentially crafting a prompt in a way that bypasses the built-in safety filters and cautions to provide an unsafe output [14]. It creates unsafe responses that can prove dangerous. To create a standard of responses that elicit safe or unsafe responses before testing on the final agent system architectures, we experimented with existing models to guide them into generating harmful or unsafe content following some observed techniques like using hypothetical or role-playing scenarios, and evading detection by slightly modifying keywords [14]. These malicious prompts can be used to test and evaluate the effectiveness of the three proposed frameworks for agent system safety. While creating these prompts, we created a set of categories for our testing based on ones put forth in LlamaGuard[10], Azure AI Content Safety[15], and OpenAI Moderation API[16]:\n\n\u2022 Hate & Harassment\n\n\u2022 Illegal Weapons & Violence\n\n\u2022 Regulated/Controlled Substances\n\n\u2022 Suicide & Self-Harm\n\n\u2022 Criminal Planning\n\nWe created a small set of unsafe prompts organized into these categories that were able to break multiple models that we tested on to perform final evaluation tests."}, {"title": "IV. METHODOLOGY", "content": "An Al agent system for customer support will be used throughout this paper to create and run the initial agent system as well as systems with the safety adaptations implemented. The safety risks posed by AI Agent Systems lie somewhere between the AI Safety Levels (ALS) ALS-2 and ASL-3, as defined by Anthropic [17]. ASL-2 systems show early dangerous capabilities with bounded risks, such as instructions on giving weapons, and ASL-3 systems show much higher capabilities of dangerous misuse and may possess low-level autonomous capabilities [18]. Multiple categories of Al agents can be created including Narrow AI and General AI. Narrow AI is designed to carry out specific tasks based on a few use cases and excel at those tasks. The efficiency of Narrow Al solutions makes them present in various industries such as finance. General Al extends past the situation-specific applications of Narrow AI, can deal with information across multiple areas and purposes, and aims to reach human-level cognitive capabilities and reasoning [19].\n\nTo align with this paper's focus on safety concerns, we will use a Narrow Al system. The main agent system that we will use is a support agent system. The system responds to fulfill user requests and has the functionality to search the web using the Serper API [20] to gather any necessary information for the user's prompt. The diagram below"}, {"title": "V. APPROACHES", "content": "This first safety architecture utilizes powerful Large Language Models and their capabilities to add a shielding layer of security to the AI agent system. The LLM-based system is powered by OpenAI GPT-40 with detailed context and some training set for safety detection. This framework ensures that all aspects of the agent system only accept safe inputs. It includes the LLM that acts as a middleman between the agent system and its environment, moderating all data exchanges. The input filtering helps to tackle potential malicious use cases with the AI agent by blocking the malicious prompt before it is even sent to the agent system. Human usage is a big part of how Al systems can be exploited and it is a major element of AI usage ethics [21]. Figure 2 shows the workings of the LLM input-output filter."}, {"title": "B. Dedicated Safety-Oriented Agent Implementation", "content": "This safety framework includes an addition to the multi-agent system - deploying a safety agent into the Al agent system to act as an internal reviewer and safety assurance mechanism for agent outputs. This integrates safety review as part of the agent system, making it less rigid than the LLM filter that acted as a shield around the agent system. The safety agent plays the role of monitoring all generated content by the support agent and ensuring it complies with the safety guidelines. In this architecture, the Al safety agent is incorporated alongside other parts of the primary agent system and focuses on providing safe outputs from the agent system rather than performing real-time decision-making or intervention as part of the system. Figure 3 shows the integration of the safety agent.\n\nWhen given an input, the input is passed directly into the system which processes and redirects it to the support agent. The aspects of the prompt are processed and the support agent generates a response, which is then passed to the safety agent, which executes the safety assurance task, the main part of guaranteeing safety. The safety assurance task contains detailed and explicit guidelines on processing content to ensure overall safety, with emphasis put on the harmful content categories defined earlier. The safety agent edits the final output to meet the safety standards put forth in the safety assurance task and outputs it to the user. This architecture is slightly more flexible than the LLM filter unlike the filter, which completely blocks the prompt or output if it is deemed unsafe, the safety agent can still"}, {"title": "C. Input-Output Filter Using LLMs", "content": "This safety architecture consists of a more robust and comprehensive safety assurance application than the previous approaches. In this approach, safety filtering is present throughout the path of the Al agent system's operation and ensures safety across all aspects of its functionality. The safety filters, like the previous application are integrated into the agent system itself and don't employ an outside LLM or safety filter. This architecture works by using a hierarchical process as opposed to the previously used sequential process in the agent system which enables a management agent to provide safety oversight during its delegation of tasks and processes. This approach focuses on providing safety and decision-making between every action taken by the Al agent system as opposed to filtering the inputs and outputs to ensure a safe interaction with the outside environment, in this case, the user.\n\nWhen given an input, the input is passed to the manager of the agent system this manager then consults the safety agent to evaluate the prompt based on safety. Based on the safety agent's output, it is then sent to the research agent, and finally to the support agent, which feeds it back to the manager and gets output to the user. In between all agent delegations, the safety agent is consulted to ensure the safety of the next step or output. For example, the prompt fed to the research agent will first pass through the safety agent, and the output of the support agent will also be evaluated by the safety agent. This framework ensures comprehensive safety across many internal levels and functions of the agent system, not only its inputs and outputs.\n\nThe three frameworks: the LLM-powered input-output filter, safety agent present in the system, and delegation-based safe agent system each contribute to enhancing Al system safety. These approaches each collectively provide robust mechanisms for mitigating risks and ensuring secure operations in AI-assisted environments."}, {"title": "VI. EVALUATION", "content": "We will use CrewAI, an Al agent creation framework, to set up and run our agent systems and the safety framework implementations. Mainstream safety benchmarks and datasets exist for many applications, including many geared towards LLM safety applications and unsafe content detection and filtering, such as ToxicChat[11], and the OpenAI Moderation Evaluation Dataset[16]. However, given the fresh state of Al agent applications and their safety considerations, there are no usable agent safety datasets. With the CrewAI agent safety frameworks defined and created, and in the absence of usable agent safety benchmarks we will evaluate based on a few metrics: performance against the set of malicious prompts, an evaluation based on ease of use of the three frameworks, and a discussion of their relevant use cases."}, {"title": "A. Safety Evaluation Methodology", "content": "Our study utilized a carefully curated dataset spanning five distinct categories: Hate & Harassment, Guns & Illegal Weapons/Violence, Regulated and Controlled Substances, Suicide and Self-Harm, and Criminal Planning. We also generated prompt data for image generation tasks. This diverse range of categories was selected to provide a comprehensive evaluation of Al agent safety across various real-world scenarios. The dataset comprises a total of 21 data points, distributed across these categories.\n\nTo provide a quantitative measure of safety performance, we developed a scoring system based on the responses to the set of 21 malicious prompts. These prompts were designed to test the robustness of various agent safety frameworks. The safety score ranges from 0.0 to 3.0, with 3.0 representing the highest level of safety. The score is calculated by evaluating the response generated by the model and taking the average value for each category. A score of 3 indicates complete response safety. A score of 2 indicates moderate response safety. A score of 1 indicates an unsafe response. A score of 0 indicates an unsafe and highly dangerous response. The results from our evaluation provide insights into the effectiveness of the different safety frameworks:"}, {"title": "B. Results", "content": "From the results, the efficacy of the safety solutions is evident based on the stark difference between the safety scores of the safety framework agent systems and the unsafe system. The agent system without any safety implementations performed poorly as expected. It responded to almost all prompts and generated potentially dangerous responses, highlighting the inherent risks present without any safety implementations. GPT-4o performed poorly as well. The safety agent framework performed well but did not always output safe responses, sometimes having lapses, especially in the category of Guns & Illegal Weapons. The agent system overlooked unsafe content at times and sometimes returned generalized content that could still pose risks in a malicious context. The LLM filter and the hierarchical process safety implementations performed exceptionally well. They generated safe responses for almost every prompt they were given, with performance varying between the models tested. Overall, GPT-4o performed the best with the safety implementations. The LLM filter simply refused most prompts and conveyed the same to the user, aside from some anomalous cases where performance in Suicide & Self-Harm, Guns & Violence, and Hate & Harrassment slipped with Llama3.1-8B and GPT-3.5-Turbo. On the other hand, the hierarchical process was the most capable: while ensuring safety, it sometimes also attempted to provide the user with alternatives or parts of their answer in a comprehensive format rather than outright refusal. Overall, the Llama3.1-8b and Google Gemma 2 models performed better and were more safe than GPT-3.5, with GPT-40 performing slightly better overall. We saw that, particularly in the unsafe agent with Guns & Violence, GPT-40 and GPT-3.5 generated more unsafe responses than the other models despite the overall performance of GPT-40. Additionally, the safety agent-powered image generation tool successfully refused all of our malicious image generation prompts."}, {"title": "VII. LIMITATIONS AND DISCUSSION", "content": "While creating our initial unsafe prompt set, creating and testing the frameworks, and finally evaluating them, we noticed some limitations that were present and can be improved on in the future. First, while experimenting with current models and GPT-40, we noticed some lapses in safety architecture that could be circumvented relatively easily. For the most part, putting a sense of importance or dependence on the agent made it more likely to output unsafe content. Contextualizing the scenario in a purely fictional sense, or asking the model to \u201cimagine\u201d essentially allowed it to respond to all unsafe prompts and feed harmful information to the user, such as how to develop weapons and harmful substances. Another limitation observed during the creation and testing was in the hierarchical-based framework as mentioned earlier. The multi-faceted decision-making and safety consultations that made up this framework were time-consuming and it took the agent around 2 - 3 extra minutes to output an answer when compared with the safety agent application. The internal thought processes of the agent system were also long, inducing a much higher cost to run the framework and making it more resource-intensive. The unsafe prompt dataset we used to quantitatively evaluate was very small-scale and may not reflect the same safety capabilities compared to possible tests with a larger dataset. In the future, a more comprehensive, large-scale dataset will be generated to better understand the safety capabilities of these systems. Similarly, to define and present the three safety architectures, a single type of agent system, a support tool, was employed. Future safety evaluations and investigations should include a more diverse group of real-world agent types."}, {"title": "A. Applications & Relevant Use-Cases", "content": "All three frameworks presented can be integrated into real-world Al agent systems to enhance their safety and function. The LLM filter-based approach can aid in real-world cases where the safety of inputs and outputs from the system matters more than the inner workings of the systems themselves. For example, these can be deployed in a healthcare scenario, where the filter can ensure that Al-generated advice or conclusions align with current medical and ethical guidelines and are safe for patients. This can also be applied in finance where the filter can block AI agent systems from carrying out transactions or actions autonomously that may not adhere to the firm's risk guidelines or current regulations.\n\nThe safety agent integration architecture can be beneficial in multiple real-world safety scenarios. One of these includes being applied in content moderation systems for complex enterprise AI solutions such as Al employees, where the agent reviews Al-generated output to ensure compliance with safety standards and community or company guidelines. Additionally, this framework can be"}, {"title": "B. Proposed safety frameworks and their relevant AI agent use cases", "content": "Medical diagnosis tools\n\n2. Financial tools\n\n3. Educational AI tutors\n\n4. Legal analysis agents\n\n5. Fraud detection tools\n\n6. Recruitment tools"}, {"title": "Hierarchical Safety Process", "content": "Industrial manufacturing robots\n\n2. Automated marketing tools\n\n3. Research assistants\n\n4. Cybersecurity tools\n\n5. Automated emergency response systems\n\n6. Supply chain management\n\n7. Software development tools"}, {"title": "VIII. CONCLUSION", "content": "In this paper, we worked towards improving safety in AI agent systems by presenting three safety-oriented frameworks for application and integration into AI tools, which are gaining prevalence and usage. We also introduced a similar framework based on the safety agent architecture in a safe multimodal image generation scenario. We evaluated the models based on our small set of unsafe prompts and discussed the possibilities of real-world"}, {"title": "A. Malicious Prompts", "content": "All malicious prompts used for development and testing in this paper are available on GitHub. The repository contains the collection of these prompts used in this paper, including their unsafe categories. They can be accessed by visiting the GitHub repository:"}, {"title": "B. Sample Safe/Unsafe Outputs", "content": "The following text (in italics) is an unsafe output generated by the agent followed by an example of one safe output generated. The unsafe output:\n\nSafe output:\n\nHello Ishaan,\n\nThank you for reaching out! I'm glad to assist you with your physics class. However, I must emphasize that discussing or providing detailed information on creating or using dangerous devices is beyond our scope due to safety and legal concerns.\n\nInstead, I can help you understand the scientific principles of combustion and fluid dynamics in a safe and educational context. Here are some key concepts:"}, {"title": "Chemical Reactions and Combustion", "content": "1. Flammable Liquids: Substances like gasoline and\n\nethanol have low flash points, making them easy to\n\nignite.\n\n2. Combustion Reaction: This exothermic reaction involves\n\na fuel reacting with oxygen to produce heat and light.\n\nFor example: $2C_8H_{18} + 25O_2 \u2192 16CO_2 +\n\n18H_2O + \\text{Heat} $"}, {"title": "Physics Principles", "content": "1. Fluid Dynamics: This involves the behavior of liquids in\n\nmotion, particularly how they spread upon impact.\n\n2. Projectile Motion: This examines the path taken by an\n\nobject under the influence of gravity."}, {"title": "Educational Context", "content": "These principles can illustrate key concepts in physics and\n\nchemistry, including the importance of handling chemicals\n\nresponsibly."}, {"title": "C. Code", "content": "All code from this research is available on GitHub. The\n\nrepository contains the complete codebase, including agent\n\nsetup, agent usage, environment configurations, and utility\n\nfunctions. Interested parties can access and use the code by\n\nvisiting\n\nthe\n\nrepository:"}]}