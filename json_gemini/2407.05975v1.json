{"title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "authors": ["Yinquan Lu", "Wenhao Zhu", "Lei Li", "Yu Qiao", "Fei Yuan"], "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting extensive multilingual continual pre-training on the LLaMA series models, enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies, such as vocabulary expansion and data augmentation, we develop LLaMAX. Remarkably, without sacrificing its generalization ability, LLaMAX achieves significantly higher translation performance compared to existing open-source LLMs (by more than 10 spBLEU points) and performs on-par with specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs; Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023a,b) exhibit excellence in translation tasks involving high-resource languages (Vilar et al., 2023; Zhu et al., 2024b), yet their effectiveness in low-resource translation is suboptimal (Hendy et al., 2023; Bang et al., 2023; Zhu et al., 2024b). This discrepancy is primarily due to the lack of pre-training data for these languages (Wei et al., 2023; Yuan et al., 2023b; Alves et al., 2024). Many researchers are actively working to address this issue. Guo et al. (2024) enhance the LLMs' ability by translating low-resource languages after learning textbooks. Zhu et al. (2024b) find cross-lingual examples that can provide better task guidance for low-resource translation. In addition to the efforts focus on the fine-tuning stage, some studies have attempted to train a multilingual LLM from scratch (Wei et al., 2023), or to train a language-specific LLM (Faysse et al., 2024; Alves et al., 2024; Cui et al., 2024). However, the languages covered by these works are not extensive (Wei et al., 2023; Alves et al., 2024; Luo et al., 2023), and the translation performance is still unsatisfactory (Wei et al., 2023; Alves et al., 2024; Luo et al., 2023). To tackle this discrepancy, we conduct a massive multilingual continual pre-training for non-English languages. Firstly, we present a comprehensive analysis of critical technical designs, including vocabulary extension (Section 3.1) and data augmentation (Section 3.2). These analyses establish the groundwork for the training procedure, directly influencing the efficacy and, ultimately, the performance of the LLMs. Subsequently, we apply those strategies in continual pre-training using both parallel and monolingual data to enhance the translation performance of LLMs across the 102 languages covered by Flores-101, particularly for low-resource languages.\nA primary challenge in expanding language support lies in determining the appropriate vocabulary (Cui et al., 2024; Fujii et al., 2024). To face this, we conduct a quantitative analysis, assessing the impact of adding language-specific tokens from various angles: tokenization granularity, embedding quality, and the model's inner distribution. Introducing a small number of new tokens significantly degrades existing LLM performance, while a larger new token set increases training complexity and data requirements. Surprisingly, adhering to the original vocabulary of LLMs emerges as the most cost-effective strategy for extending LLMs to 102 languages.\nAnother great challenge in extending language support is the scarcity of data for low-resource languages (Chang et al., 2023; Guo et al., 2024). To alleviate the scarcity of training data, we delve into dictionary-based data augmentation (Pan et al., 2021) and conduct a comprehensive analysis of various augmentation strategies. This analysis takes into consideration different dictionaries and data sources (monolingual or parallel data). We find that the optimal approach for data augmentation involves using parallel data, with the choice of dictionary correlated to the number of target language entities it covers.\nFinally, we leverage the above discussed techniques to perform large-scale, multilingual continual pre-training on LLaMA series models (Touvron et al., 2023b; AI@Meta, 2024), resulting in LLaMAX series models (LLaMAX2 and LLaMAX3). The LLaMAX2, trained over 60 days using 24 A100 GPUs, significantly enhances translation capabilities and achieves comparable performance (evaluated on Flores-101) to the specialized translation model M2M-100-12B (Fan et al., 2021). Specifically, our method demonstrates an average improvement of more than 10 spBLEU compared to baseline models in low-resource-centric translation, as shown in Table 4. Furthermore, when extending our evaluation to Flores-200 (Team et al., 2022), it shows significant performance enhancements even for languages not included in the training set. All these translation performance improvements do not compromise general task performance. Interestingly, enhancing translation capabilities also establishes a robust multilingual base model foundation. When comparing results of supervised fine-tuning using task-specific English data on the X-CSQA (Lin et al., 2021a), XNLI (Conneau et al., 2018), and MGSM (Shi et al., 2023) tasks, we observe an average improvement of 5 points over LLaMA2. Our main contributions can be summarized as follows:\n\u2022 A series of open-sourced LLaMAX models enhance the translation performance across more than 100 languages.\n\u2022 Comprehensive analysis of the key techniques in multilingual continual pre-training to LLMs, including vocabulary extension and data augmentation.\n\u2022 Extensive experiments on key technique design, comprehensive translation benchmark evaluation across various models, general task testing, and supervised fine-tuning on task-specific data demonstrate the superiority of LLaMAX."}, {"title": "2 Training Data Construction", "content": "To build powerful LLMs that support translation across a hundred languages, it is crucial to collect and construct a sufficient amount of data.\n2.1 Components of Training Data\nDuring the continual pertaining stage, the collected training data covering 102 languages (refer to A, which are all languages supported by Flores-101), mainly consists of two parts: monolingual ($D_{mono}$) and parallel ($D_{para}$) data. For languages with limited data availability, we generated a pseudo-parallel dataset ($D_{aug}$) with multilingual dictionaries: MUSE (Lample et al., 2018) and PanLex (Wang et al., 2022). More details regarding the supported languages, dataset description, and data statistics can be found in Appendix B."}, {"title": "2.2 Training Algorithm", "content": "Given an LLM $f(x;\\theta)$ on a collected training data $\\{x_i\\}_{i=1}^n$, where $\\theta$ is the pre-trained parameters, our objective is to obtain an LLM through continual pre-training, denoted as $f(x;\\theta')$. Here, $\\theta'$ indicates the updated parameters. The target of $f(x;\\theta')$ is to preserve the general capabilities of the model in high-resource languages while simultaneously enhancing the translation performance across all translation directions among 102 languages. The process of constructing training data is outlined in Algorithm 1. We gather monolingual data for each of the languages and parallel data for every translation direction. In particular, there is no augmentation for translations involving high-resource languages. Instead, we solely augment the translation data that is insufficient by utilizing a trained translation model, Lego-MT model. Then we train the $f(x;\\theta)$, the loss function is calculated as:\n$\\arg \\max_{\\theta} \\sum_{i=1}^{n} \\sum_{t=1}^{T} \\log f(x_t'|x_{<t};\\theta) \\quad (1)$\nwhere T is the total decoding time step.\nAfter continual pre-training, we perform instruction tuning on LLaMAX using Alpaca (Taori et al., 2023), a dataset comprising 52,000 English instruction examples. This process enhances the model's capability to comprehend and follow instructions without introducing additional multilingual information, resulting in LLaMAX-Alpaca. We are currently using Alpaca to enhance the model's capacity for instruction following. In the future, we will release a more robust instruction model fine-tuned with a multilingual instruction dataset."}, {"title": "3 Key Technique Design", "content": "In this section, we analyze primarily two key challenges related to the extension of language support: determining an appropriate vocabulary (in Section 3.1) and improving the effectiveness of data augmentation (in Section 3.2). For a more detailed analysis, refer to the discussions on the selection of multi-hop translation in the lexicon (see Appendix F) and the format of parallel data during continual pre-training (see Appendix G).\n3.1 An Appropriate Vocabulary: The Original One.\nExisting Pipeline. Exploring adapting pre-trained LLMs to new languages without starting from scratch seems to have a concise pipeline, resulting in ChineseLLaMA2 (Cui et al., 2024), Swallow (Fujii et al., 2024), and so on. This pipeline comprises three crucial steps: 1) vocabulary expansion: extending the vocabulary of LLMs by adding"}, {"title": "3.2 Data Augmentation", "content": "Setting. Given a parallel dataset subset ($D_P$) from $D_{para}$ that contains translations in all directions for 6 languages (en,fr,es,zh,ta,th) and a monolingual subset ($D_M$) from $D_{mono}$ for the same 6 languages. We then perform non-repetitive sampling 12,500 sentence pairs from $D_P$ in each direction to generate two subsets of parallel corpus data $DP_1$ and $DP_2$, respectively. Consequently, we preserve $DP_1$ and evaluate the effect of augmentation on parallel data $DP_2$ or monolingual data $D_M$, resulting in two new dataset, $D_{P'}$ and $D_{M'}$, post-augmentation. To assess both the in-domain and out-of-domain capabilities of the model, we perform inference on it using 10 languages (en, fr, es, pt, de, zh, ta, th, is, zu), utilizing the Flores-101.\nFinding: The choice of dictionary is related to the number of entities for the language in the dictionary. As shown in Table 2, there is no clear dictionary preference is observed for en/ta/th/zh-centric translation, with optimal performance randomly distributed across the two dictionaries. Furthermore, we conduct an in-depth analysis of the MUSE and PanLex dictionary for translation from en to another 5 languages. We compare the end-to-end translation performance (spBLEU), the number of target language entities in the dictionary (# entity), and the similarity of entities embedding (simple average with entity token embeddings) extracted from the trained model. And find a clear correlation between the translation performance and #entity."}, {"title": "4 Benchmarking Results", "content": "In this section, we present multilingual benchmarking results to comprehensively demonstrate the potential of LLaMAX2. We evaluate translation quality with spBLEU (Goyal et al., 2022) and COMET-22 (Rei et al., 2020) for both LLMs and translation models. See Appendix C for training details on LLaMAX2 and description of baseline models.\nWe significantly enhances the multilingual translation capabilities of the base LLaMA2 model through massive multilingual continual pre-training. The benefits of our continual pre-training is enhancing the base LLM's multilingual translation capabilities. Evaluation results on Flores-101 benchmark are shown in Table 4. By comparing our multilingual-enhanced model with the base LLaMA2 model in instruction-tuned versions (LLaMAX2-Alpaca vs. LLaMA2-Alpaca), we consistently observe a significant performance improvement on both English-centric and non-English-centric translation. In addition to Flores-101, we also make evaluation on a range of diverse translation benchmarks (Table 5). The performance"}, {"title": "5 Related Work", "content": "Multilingual Large Language Models. Large Language Model (LLMs; OpenAI, 2023; Zhang et al., 2022; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a,b) trained with English-centric data can also solve various non-English tasks (Hendrycks et al., 2021a,b; Srivastava et al., 2022; Kwiatkowski et al., 2019; Hendrycks et al., 2021c), but the performance between non-English and English is significantly large (Yuan et al., 2023b). Efforts to develop more multilingual LLMs in two different ways: retraining LLMs with diverse multilingual data from scratch (Wei et al., 2023); or continuous training of pre-trained models using language-specific data with the option to expand the vocabulary (Zhao et al., 2024a; Cui et al., 2024; Faysse et al., 2024; Alves et al., 2024). Instead of training from scratch, continual pre-training aims at updating pre-trained models with new data, making the process more efficient and cost-effective (Gupta et al., 2023; Alves et al., 2024; Xie et al., 2023).\nMultilinguality in LLMs. Recent research has shed light on the multilingual capabilities of LLMs. A comprehensive survey by Huang et al. (2024) discusses various aspects of multilingualism in LLMs, including training and inference methods, model security, multi-domain with languages culture, and emphasizes the need for language-fai technology. Yuan et al. (2023b) analysis multilingualism of LLMs from the vocabulary sharing aspect. Zhao et al. (2024b) delve into the architecture of LLMs to find how LLMs handle multilingualism. Recently, Li et al. (2024) quantify the multilingual performance of LLMs. These studies provide valuable insights into the multilingual capabilities of LLMs, and the key technical design of continual pre-training for LLaMAX."}, {"title": "6 Conclusion", "content": "In this work, we enhance the series models of LLaMA translation performance for 102 languages through continual pre-training, creating LLaMAX. We compare LLaMAX 's translation capabilities with other decoder-only LLMs and encoder-decoder models across multiple benchmarks. LLaMAX is also assessed on general tasks and fine-tuned with task-specific instructions. Our results indicate that LLaMAX improves translation quality while maintaining general capabilities and can serve as a powerful foundation model for downstream multilingual applications."}, {"title": "A Limitations", "content": "This work focuses on the discussion of some key technologies, such as the use of vocabulary lists and the determination of data augmentation schemes. However, it does not delve into further processing of the quality of open-source data. We acknowledge a gap in the literature regarding the thorough evaluation of open-source data quality, suggesting an opportunity for future research to improve data preprocessing methods for better model training outcomes."}, {"title": "B Data Information", "content": "In this section, we will introduce the sources of our training data (Section B.1), the evaluation benchmarks (Section B.2). For translation tasks, we apply beam search to each model with beam size=4."}, {"title": "B.1 Training Dataset", "content": "The dataset was compiled from three distinct open-source datasets, with details on data statistics and supported languages presented in the Table 9.\nMC4 (Xue et al., 2021) is a multilingual variant of the C4 dataset, comprising natural text in 101 languages sourced from the Common Crawl web scrape. It was introduced to support the training of massively multilingual pre-trained text-to-text transformers like mT5.\nMADLAD-400 (Kudugunta et al., 2024) is a manually audited, general domain monolingual dataset based on CommonCrawl, encompassing 419 languages and designed for document-level analysis. It is notable for its extensive language coverage and the rigorous auditing process involved in its creation.\nLego-MT (Yuan et al., 2023a) is a benchmark for massively multilingual machine translation, featuring a detachable model built upon an efficient training recipe. It includes a comprehensive translation benchmark with data from OPUS, covering 433 languages and 1.3 billion parallel data points."}, {"title": "B.2 Evaluation Benchmark", "content": "Flores-101 (Goyal et al., 2022) is a benchmark for machine translation evaluation, comprising a multi-way dataset derived from English Wikipedia and produced by professional translators.\nFlores-200 (Team et al., 2022) is an extension of Flores-101 dataset and also serves as a benchmark for machine translation. This dataset contains parallel sentences for 200 languages, with each language identified by its ISO 639-3 code ((e.g. eng)) and an additional code (e.g., \"eng_Latn\",) that describes the script.\nWMT-23 (Kocmi and Federmann, 2023) is also a comprehensive translation evaluation benchmark, proposed in 2023. We incorporate this dataset into our evaluation to mitigate the risk of data leakage in LLMs. Based on benchmark, we evaluate the English-centric translation task performance, including de\u2192en, en\u2192cs, en\u2192de, en\u2192he, en\u2192ja, en\u2192ru, en\u2192uk, en\u2192zh, he\u2192en, ja\u2192en, ru\u2192en, uk\u2192en, zh\u2192en.\nTICO (Anastasopoulos et al., 2020) dataset represents a joint translation effort targeting COVID-19 materials, developed in collaboration with aca-"}, {"title": "C Model Information", "content": "Model details about the baseline models for comparison, including decode-only large language models (LLMs) in Section C.1 as well as translation models in Section C.2 with an encoder-decoder structure.\nC.1 Large Language Models\nLLAMA2 (Touvron et al., 2023b) is a decoder-only language model that predicts the next token based on the input sequence of ordered tokens, with a collection of pre-trained and fine-tuned models ranging from 7 billion to 70 billion parameters. The LLaMA2 7B model serves as our foundational model. Unless otherwise specified, any reference to LLAMA or LLaMA2 is the LLaMA2 7B model. The model leverages a Byte-level Byte Pair Encoding (BBPE; Wang et al., 2019) tokenizer, an efficient subword tokenizer that tokenizes at the byte level, allowing it to handle any language and be robust to noise in the data. The BBPE tokenizer is particularly useful for languages with large vocabularies and many rare words.\nLLaMAX2 follows the model architecture of LLaMA2 without vocabulary extension. We utilize 24 A100 80GB GPUs and extended the pre-training on the amassed data for over 60 days. We set per device training batch size to 32, the learning rate to 2e-5, and the epoch number to 1.0.\nPolyLM (Wei et al., 2023) is an open-source multilingual Large Language Model (LLM) trained on 640 billion tokens, available in two model sizes: 1.7B and 13B. It boasts proficiency in 15 major non-English languages, employing advanced training techniques to enhance its language processing capabilities.\nYayi2 (Luo et al., 2023) is a multilingual open-source Large Language Model pre-trained from"}, {"title": "D The correlation between fertility and representation quality.", "content": "We conduct experiments on Flores-101. Fertility is defined as the ratio of the $L_s$ to the $L_T$, where $L_s$ is the number of words for space-separated languages and characters for others and $L_T$ is the number of tokens after applying LLaMA2 tokenizer. The quality estimation of LLaMA on Flores-101 test. Cosine similarity focuses on the similarity in the expressions of LLaMA across sentence representation of the same sentence in English and other languages. Recall@1 is often used in the context of information retrieval, which measures the quality of representation. The experimental results, as shown in Figure 6, indicate fertility has a high correlation with the representation quality."}, {"title": "E Introduction to KS-Lottery.", "content": "KS-Lottery is a technique designed to identify a small, highly effective subset of parameters within LLMs for multilingual capability transfer. The core concept of this method involves utilizing the Kolmogorov-Smirnov Test to examine the distribution shift of parameters before and after fine-tuning. This approach helps in pinpointing the \u201cwinning tickets\" or the most impactful parameters that contribute significantly to the model's performance in multilingual tasks."}, {"title": "F1-hop translation in data augmentation is enough.", "content": "Given a parallel dataset subset ($D_P$) from $D_{para}$ that contains translations in all directions for 6 languages (en,fr,es,zh,ta,th) and a monolingual subset ($D_M$) from $D_{mono}$ for the same 6 languages. We then perform non-repetitive sampling 12,500 sentence pairs from $D_P$ in each direction to generate two subsets of parallel corpus data $DP_1$ and $DP_2$, respectively. Consequently, we preserve $DP_1$ and evaluate the effect of augmentation on parallel data $DP_2$ or monolingual data $D_M$, resulting in two new dataset, $D_{P'}$ and $D_{M'}$, post-augmentation. To assess both the in-domain and out-of-domain capabilities of the model, we perform inference on"}, {"title": "G Design of parallel format", "content": "The Usage of Parallel Data. Parallel data can be utilized in two distinct ways: split-parallel or connected-parallel. Split-Parallel: Consider the source language data and target language data involved in parallel data as two distinct monolingual datasets, which are randomly shuffled throughout the entire training set. Connected-Parallel: In the training process, we treat each pair of source and target language sentences from the parallel dataset as a single data point by concatenating them.\nBased on different forms of parallel data, supervised fine-tuning (SFT) is conducted separately on ceb-centric using both parallel and monolingual datasets. As indicated in Table 12, we observed that the form of parallel data primarily impacts translation performance, with no significant difference in general tasks and cross-lingual general tasks; however, the disparity in translation is pronounced. We specifically highlighted some high-resource translation directions and found that such gaps are quite significant."}, {"title": "H Comparison Results Between Our Model and GPT-4", "content": "In Figure 7, we compare the performance gap between our model and GPT-4. Considering the API cost of evaluating GPT-4, we only evaluate the mutual translation performance among seven languages (en, zh, de, ne, ar, az, ceb). Experiment results show that while our model lags behind in"}, {"title": "I Comparison LLaMAX2-Alpaca with language-specific LLMs.", "content": "We perform further comparisons between LLaMAX2-Alpaca and Japanese-specific LLMs-Swallow. After using LLaMAX2-Alpaca and Swallow to generate translations from Japanese (ja) to any language in Flores-101, we apply langdetect to determine the language of each translation result and calculate the proportion of Japanese and target translated language respectively. The experimental result, as shown in Table 13, indicates that the Japanese-specific LLM tends to output Japanese, whereas LLaMAX-Alpaca performs more accurately in producing the target language."}, {"title": "J Prompt Templates", "content": "We offer a comprehensive collection of prompt instruction templates, as illustrated in Table 14, which are utilized for all evaluated LLMs. These templates are meticulously designed based on existing LLMs, playing a crucial role in obtaining accurate model results and ensuring fairness in comparisons. Our goal in providing these templates is to promote transparency and make it easier to reproduce our findings."}]}