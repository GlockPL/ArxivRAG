{"title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "authors": ["Yinquan Lu", "Wenhao Zhu", "Lei Li", "Yu Qiao", "Fei Yuan"], "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting extensive multilingual continual pre-training on the LLaMA series models, enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies, such as vocabulary expansion and data augmentation, we develop LLaMAX. Remarkably, without sacrificing its generalization ability, LLaMAX achieves significantly higher translation performance compared to existing open-source LLMs (by more than 10 spBLEU points) and performs on-par with specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs; Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023a,b) exhibit excellence in translation tasks involving high-resource languages (Vilar et al., 2023; Zhu et al., 2024b), yet their effectiveness in low-resource translation is suboptimal (Hendy et al., 2023; Bang et al., 2023; Zhu et al., 2024b). This discrepancy is primarily due to the lack of pre-training data for these languages (Wei et al., 2023; Yuan et al., 2023b; Alves et al., 2024). Many researchers are actively working to address this issue. Guo et al. (2024) enhance the LLMs' ability by translating low-resource languages after learning textbooks. Zhu et al. (2024b) find cross-lingual examples that can provide better task guidance for low-resource translation. In addition to the efforts focus on the fine-tuning stage, some studies have attempted to train a multilingual LLM from scratch (Wei et al., 2023), or to train a languagespecific LLM (Faysse et al., 2024; Alves et al., 2024; Cui et al., 2024). However, the languages covered by these works are not extensive (Wei et al., 2023; Alves et al., 2024; Luo et al., 2023), and the translation performance is still unsatisfactory (Wei et al., 2023; Alves et al., 2024; Luo et al., 2023).\nTo tackle this discrepancy, we conduct a massive multilingual continual pre-training for non-English languages. Firstly, we present a comprehensive analysis of critical technical designs, including vocabulary extension (Section 3.1) and data augmentation (Section 3.2). These analyses establish the groundwork for the training procedure, directly influencing the efficacy and, ultimately, the performance of the LLMs. Subsequently, we apply those strategies in continual pre-training using both parallel and monolingual data to enhance the translation performance of LLMs across the 102 languages covered by Flores-101, particularly for low-resource languages.\nA primary challenge in expanding language support lies in determining the appropriate vocabulary (Cui et al., 2024; Fujii et al., 2024). To face this, we conduct a quantitative analysis, assessing the impact of adding language-specific tokens from various angles: tokenization granularity, embedding quality, and the model's inner distribution. Introducing a small number of new tokens significantly degrades existing LLM performance, while a larger new token set increases training complexity and data requirements. Surprisingly, adhering to the original vocabulary of LLMs emerges as the most cost-effective strategy for extending LLMs to 102 languages.\nAnother great challenge in extending language support is the scarcity of data for low-resource languages (Chang et al., 2023; Guo et al., 2024). To alleviate the scarcity of training data, we delve into dictionary-based data augmentation (Pan et al., 2021) and conduct a comprehensive analysis of various augmentation strategies. This analysis takes into consideration different dictionaries and data sources (monolingual or parallel data). We find that the optimal approach for data augmentation involves using parallel data, with the choice of dictionary correlated to the number of target language entities it covers.\nFinally, we leverage the above discussed techniques to perform large-scale, multilingual continual pre-training on LLaMA series models (Touvron et al., 2023b; AI@Meta, 2024), resulting in LLaMAX series models (LLaMAX2 and LLaMAX3). The LLaMAX2, trained over 60 days using 24 A100 GPUs, significantly enhances translation capabilities and achieves comparable performance (evaluated on Flores-101) to the specialized translation model M2M-100-12B (Fan et al., 2021). Specifically, our method demonstrates an average improvement of more than 10 spBLEU compared to baseline models in low-resource-centric translation, as shown in Table 4. Furthermore, when extending our evaluation to Flores-200 (Team et al., 2022), it shows significant performance enhancements even for languages not included in the training set. All these translation performance improvements do not compromise general task performance. Interestingly, enhancing translation capabilities also establishes a robust multilingual base model foundation. When comparing results of supervised fine-tuning using task-specific English data on the X-CSQA (Lin et al., 2021a), XNLI (Conneau et al., 2018), and MGSM (Shi et al., 2023) tasks, we observe an average improvement of 5 points over LLaMA2. Our main contributions can be summarized as follows:\n\u2022 A series of open-sourced LLaMAX models enhance the translation performance across more than 100 languages.\n\u2022 Comprehensive analysis of the key techniques in multilingual continual pre-training to LLMs, including vocabulary extension and data augmentation.\n\u2022 Extensive experiments on key technique design, comprehensive translation benchmark evaluation across various models, general task testing, and supervised fine-tuning on task-specific data demonstrate the superiority of LLaMAX."}, {"title": "2 Training Data Construction", "content": "To build powerful LLMs that support translation across a hundred languages, it is crucial to collect and construct a sufficient amount of data.\n2.1 Components of Training Data\nDuring the continual pertaining stage, the collected training data covering 102 languages (refer to A, which are all languages supported by Flores-101), mainly consists of two parts: monolingual ($\\mathcal{D}_{mono}$) and parallel ($\\mathcal{D}_{para}$) data. For languages with limited data availability, we generated a pseudo-parallel dataset ($\\mathcal{D}_{aug}$) with multilingual dictionaries: MUSE (Lample et al., 2018) and PanLex (Wang et al., 2022). More details regarding the supported languages, dataset description, and data statistics can be found in Appendix B."}, {"title": "2.2 Training Algorithm", "content": "Given an LLM $f(x;\\theta)$ on a collected training data $\\{x_i\\}_{i=1}^n$, where \u03b8 is the pre-trained parameters, our objective is to obtain an LLM through continual pre-training, denoted as $f(x;\\theta')$. Here, \u03b8' indicates the updated parameters. The target of $f(x;\\theta')$ is to preserve the general capabilities of the model in high-resource languages while simultaneously enhancing the translation performance across all translation directions among 102 languages. The process of constructing training data is outlined in Algorithm 1. We gather monolingual data for each of the languages and parallel data for every translation direction. In particular, there is no augmentation for translations involving highresource languages. Instead, we solely augment the translation data that is insufficient by utilizing a trained translation model, Lego-MT model. Then we train the $f(x;\\theta)$, the loss function is calculated as:\n$\\arg \\max_\\theta \\sum_{i=1}^n \\sum_{t=1}^T \\log f(x_t|x_{\\lt t};\\theta) $\nwhere T is the total decoding time step.\nAfter continual pre-training, we perform instruction tuning on LLaMAX using Alpaca (Taori et al., 2023), a dataset comprising 52,000 English instruction examples. This process enhances the model's capability to comprehend and follow instructions without introducing additional multilingual information, resulting in LLaMAX-Alpaca. We are currently using Alpaca to enhance the model's capacity for instruction following. In the future, we will release a more robust instruction model fine-tuned with a multilingual instruction dataset."}, {"title": "3 Key Technique Design", "content": "In this section, we analyze primarily two key challenges related to the extension of language support: determining an appropriate vocabulary (in Section 3.1) and improving the effectiveness of data augmentation (in Section 3.2). For a more detailed analysis, refer to the discussions on the selection of multi-hop translation in the lexicon (see Appendix F) and the format of parallel data during continual pre-training (see Appendix G).\n3.1 An Appropriate Vocabulary: The Original One.\nExisting Pipeline. Exploring adapting pre-trained LLMs to new languages without starting from scratch seems to have a concise pipeline, resulting in ChineseLLaMA2 (Cui et al., 2024), Swallow (Fujii et al., 2024), and so on. This pipeline comprises three crucial steps: 1) vocabulary expansion: extending the vocabulary of LLMs by adding"}, {"title": "3.2 Data Augmentation", "content": "Setting. Given a parallel dataset subset (Dp) from \\mathcal{D}_{para} that contains translations in all directions for 6 languages (en,fr,es,zh,ta,th) and a monolingual subset (DM) from \\mathcal{D}_{mono} for the same 6 languages. We then perform non-repetitive sampling 12,500 sentence pairs from Dp in each direction to generate two subsets of parallel corpus data DP\u2081 and DP2, respectively. Consequently, we preserve DP\u2081 and evaluate the effect of augmentation on parallel data DP\u2082 or monolingual data DM, resulting in two new dataset, DP, and DM, post-augmentation. To assess both the in-domain and out-of-domain capabilities of the model, we perform inference on it using 10 languages (en, fr, es, pt, de, zh, ta, th, is, zu), utilizing the Flores-101.\nFinding: The choice of dictionary is related to the number of entities for the language in the dictionary. As shown in Table 2, there is no clear dictionary preference is observed for en/ta/th/zhcentric translation, with optimal performance randomly distributed across the two dictionaries. Furthermore, we conduct an in-depth analysis of the MUSE and PanLex dictionary for translation from en to another 5 languages. We compare the end-to-end translation performance (spBLEU), the number of target language entities in the dictionary (# entity), and the similarity of entities embedding (simple average with entity token embeddings) extracted from the trained model. And find a clear correlation between the translation performance and #entity."}, {"title": "4 Benchmarking Results", "content": "In this section, we present multilingual benchmarking results to comprehensively demonstrate the potential of LLaMAX2. We evaluate translation quality with spBLEU (Goyal et al., 2022) and COMET-22 (Rei et al., 2020) for both LLMs and translation models. See Appendix C for training details on LLaMAX2 and description of baseline models.\nWe significantly enhances the multilingual translation capabilities of the base LLaMA2 model through massive multilingual continual pre-training. The benefits of our continual pretraining is enhancing the base LLM's multilingual translation capabilities. Evaluation results on Flores-101 benchmark are shown in Table 4. By comparing our multilingual-enhanced model with the base LLaMA2 model in instruction-tuned versions (LLaMAX2-Alpaca vs. LLaMA2-Alpaca), we consistently observe a significant performance improvement on both English-centric and nonEnglish-centric translation. In addition to Flores101, we also make evaluation on a range of diverse translation benchmarks (Table 5). The performance"}, {"title": "5 Related Work", "content": "Multilingual Large Language Models. Large Language Model (LLMs; OpenAI, 2023; Zhang et al., 2022; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a,b) trained with Englishcentric data can also solve various non-English tasks (Hendrycks et al., 2021a,b; Srivastava et al., 2022; Kwiatkowski et al., 2019; Hendrycks et al., 2021c), but the performance between non-English and English is significantly large (Yuan et al., 2023b). Efforts to develop more multilingual LLMs in two different ways: retraining LLMs with diverse multilingual data from scratch (Wei et al., 2023); or continuous training of pre-trained models using language-specific data with the option to expand the vocabulary (Zhao et al., 2024a; Cui et al., 2024; Faysse et al., 2024; Alves et al., 2024). Instead of training from scratch, continual pre-training aims at updating pre-trained models with new data, making the process more efficient and cost-effective (Gupta et al., 2023; Alves et al., 2024; Xie et al., 2023).\nMultilinguality in LLMs. Recent research has shed light on the multilingual capabilities of LLMs. A comprehensive survey by Huang et al. (2024) discusses various aspects of multilingualism in LLMs, including training and inference methods, model security, multi-domain with languages culture, and emphasizes the need for language-fai technology. Yuan et al. (2023b) analysis multilingualism of LLMs from the vocabulary sharing aspect. Zhao et al. (2024b) delve into the architecture of LLMs to find how LLMs handle multilingualism. Recently, Li et al. (2024) quantify the multilingual performance of LLMs. These studies provide valuable insights into the multilingual capabilities of LLMs, and the key technical design of continual pre-training for LLaMAX."}, {"title": "6 Conclusion", "content": "In this work, we enhance the series models of LLaMA translation performance for 102 languages through continual pre-training, creating LLaMAX. We compare LLaMAX 's translation capabilities with other decoder-only LLMs and encoder-decoder models across multiple benchmarks. LLaMAX is also assessed on general tasks and finetuned with task-specific instructions. Our results indicate that LLaMAX improves translation quality while maintaining general capabilities and can serve as a powerful foundation model for downstream multilingual applications."}, {"title": "Outline", "content": "\u2022 Section A: Discussion about the generalizability and reliability of the work.\n\u2022 Section B: The comprehensive details of the training data, including monolingual and parallel data, and the evaluation benchmark (Table 9).\n\u2022 Section C: The detailed information of different models, including open-source Large Language Models (Section C.1) and well-trained translation models (Section C.2).\n\u2022 Section D: Analysis the correlation between embedding quality of LLaMA2 and fertility using Flores-101 test (Figure 6).\n\u2022 Section E: A detailed introduction to the KSLottery method.\n\u2022 Section F: Selection about multi-hop translation (Table 10 and Table 11).\n\u2022 Section G: The selection of the appropriate format for parallel data during training (Table 12).\n\u2022 Section H: The comparison of translation performance across all seven languages between LegoMT and GPT-4 (Figure 7).\n\u2022 Section I: Comparison results between LLaMAX2-Alpaca with language-specific enhanced LLMs (Table 13).\n\u2022 Section J: We present comprehensive instructions utilized for all LLMs (Table 14).\nA Limitations\nThis work focuses on the discussion of some key technologies, such as the use of vocabulary lists and the determination of data augmentation schemes. However, it does not delve into further processing of the quality of open-source data. We acknowledge a gap in the literature regarding the thorough evaluation of open-source data quality, suggesting an opportunity for future research to improve data preprocessing methods for better model training outcomes.\nB Data Information\nIn this section, we will introduce the sources of our training data (Section B.1), the evaluation benchmarks (Section B.2). For translation tasks, we apply beam search to each model with beam size=4."}, {"title": "B.1 Training Dataset", "content": "The dataset was compiled from three distinct opensource datasets, with details on data statistics and supported languages presented in the Table 9.\nMC4 (Xue et al., 2021) is a multilingual variant of the C4 dataset, comprising natural text in 101 languages sourced from the Common Crawl web scrape. It was introduced to support the training of massively multilingual pre-trained text-to-text transformers like mT5.\nMADLAD-400 (Kudugunta et al., 2024) is a manually audited, general domain monolingual dataset based on CommonCrawl, encompassing 419 languages and designed for document-level analysis. It is notable for its extensive language coverage and the rigorous auditing process involved in its creation.\nLego-MT (Yuan et al., 2023a) is a benchmark for massively multilingual machine translation, featuring a detachable model built upon an efficient training recipe. It includes a comprehensive translation benchmark with data from OPUS, covering 433 languages and 1.3 billion parallel data points."}, {"title": "B.2 Evaluation Benchmark", "content": "Flores-101 (Goyal et al., 2022) is a benchmark for machine translation evaluation, comprising a multi-way dataset derived from English Wikipedia and produced by professional translators.\nFlores-200 (Team et al., 2022) is an extension of Flores-101 dataset and also serves as a benchmark for machine translation. This dataset contains parallel sentences for 200 languages, with each language identified by its ISO 639-3 code ((e.g. eng)) and an additional code (e.g., \"eng_Latn\",) that describes the script.\nWMT-23 (Kocmi and Federmann, 2023) is also a comprehensive translation evaluation benchmark, proposed in 2023. We incorporate this dataset into our evaluation to mitigate the risk of data leakage in LLMs. Based on benchmark, we evaluate the English-centric translation task performance, including de\u2192en, en\u2192cs, en\u2192de, en\u2192he, en\u2192ja, en\u2192ru, en\u2192uk, en\u2192zh, he\u2192en, ja\u2192en, ru\u2192en, uk\u2192en, zh\u2192en.\nTICO (Anastasopoulos et al., 2020) dataset represents a joint translation effort targeting COVID19 materials, developed in collaboration with aca"}, {"title": "2.2 Training Algorithm.", "content": ""}, {"title": "4 Benchnmarking Results", "content": ""}, {"title": "5 Related Work", "content": ""}, {"title": "I Comparison LLaMAX2-Alpaca with language-specific LLMs.", "content": "We perform further comparisons between LLaMAX2-Alpaca and Japanese-specific LLMsSwallow. After using LLaMAX2-Alpaca and Swallow to generate translations from Japanese (ja) to any language in Flores-101, we apply langdetect to determine the language of each translation result and calculate the proportion of Japanese and target translated language respectively. The experimental result, as shown in Table 13, indicates that the Japanese-specific LLM tends to output Japanese, whereas LLaMAX2-Alpaca performs"}, {"title": "J Prompt Templates", "content": "We offer a comprehensive collection of prompt instruction templates, as illustrated in Table 14, which are utilized for all evaluated LLMs. These templates are meticulously designed based on existing LLMs, playing a crucial role in obtaining accurate model results and ensuring fairness in comparisons. Our goal in providing these templates is to promote transparency and make it easier to reproduce our findings."}]}