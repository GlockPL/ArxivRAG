{"title": "Actor Critic with Experience Replay-based automatic treatment planning for prostate cancer intensity modulated radiotherapy", "authors": ["Md Mainul Abrar", "Parvat Sapkota", "Damon Sprouts", "Xun Jia", "Yujie Chi"], "abstract": "Background: Achieving real-time treatment planning in intensity-modulated radiotherapy (IMRT) is challenging due to the complex interactions between radiation beams and the human body. The introduction of artificial intelligence (AI) has automated treatment planning, significantly improving efficiency. However, existing automatic treatment planning agents often rely on supervised or unsupervised AI models that require large datasets of high-quality patient data for training. Additionally, these networks are generally not universally applicable across patient cases from different institutions and can be vulnerable to adversarial attacks. Deep reinforcement learning (DRL), which mimics the trial-and-error process used by human planners, offers a promising new approach to address these challenges.\nPurpose: This work aims to develop a stochastic policy-based DRL agent for automatic treatment planning that facilitates effective training with limited datasets, universal applicability across diverse patient datasets, and robust performance under adversarial attacks.\nMethods: We employ an Actor-Critic with Experience Replay (ACER) architecture to develop the automatic treatment planning agent. This agent operates the treatment planning system (TPS) for inverse treatment planning by automatically tuning treatment planning parameters (TPPs). We use prostate cancer IMRT patient cases as our testbed, which includes one target and two organs at risk (OARs), along with 18 discrete TPP tuning actions. The network takes dose-volume histograms (DVHs) as input and outputs a policy for effective TPP tuning, accompanied by an evaluation function for that policy. Training utilizes DVHs from treatment plans generated by an in-house TPS under randomized TPPs for a single patient case, with validation conducted on two other independent cases. Both online asynchronous learning and offline, sample-efficient experience replay methods are employed to update the network parameters. After training, more than 300 initial treatment plans from three distinct datasets are used for testing. The ProKnow scoring system for prostate cancer IMRT, with a maximum score of 9, is used to evaluate plan quality. The robustness of the network is further assessed through adversarial attacks using the Fast Gradient Sign Method (FGSM).\nResults: Despite being trained on treatment plans from a single patient case, the network converges efficiently when validated on two independent cases. For testing performance, the mean \u00b1 standard deviation of the plan scores across all test cases before ACER-based treatment planning is 6.20\u00b11.84. After implementing ACER-based treatment planning, 93.09% of the cases achieve a perfect score of 9, with only 6.12% scoring between 8 and 9, and no cases being below 7. The corresponding mean \u00b1 standard deviation is 8.93 \u00b10.27. This performance highlights the ACER agent's high generality across patient data from various sources. Further analysis indicates that the ACER agent effectively prioritizes leading reasonable TPP tuning actions over obviously unsuitable ones by several orders of magnitude, showing its efficacy. Additionally, results from FGSM attacks demonstrate that the ACER-based agent remains comparatively robust against various levels of perturbation.\nConclusions: We successfully trained a DRL agent using the ACER technique for high-quality treatment planning in prostate cancer IMRT. It achieves high generality across diverse patient datasets and exhibits high robustness against adversarial attacks.", "sections": [{"title": "1. Introduction", "content": "Real-time treatment planning represents a challenging problem in intensity modulated radiotherapy (IMRT) for cancer treatment. The difficulty mainly arises from the complex energy deposition properties of the radiation beam within the patient body, which often requires a careful balance between target dose coverage and normal tissue sparing. This balance can be achieved via inverse treatment planning optimization, which involves setting dose constraints for each organ and target, along with weighting factors to prioritize conflicting dose requirements. However, the optimal value set of these treatment planning parameters (TPPs) to achieve clinical acceptable plan are often initialization condition dependent, such as the patient geometries, fluencemap and TPP initialization, etc.\u00b9. In state-of-the-art radiotherapy clinics, it requires repeatedly adjusting the TPPs by human planners to refine the inverse treatment planning optimization, which is time-consuming and poses challenges for real-time planning.\nIn the era of artificial intelligence (AI), significant efforts have been made to address challenges in treatment planning. One approach aims to shorten the trial-and-error process by generating a relatively \"good\" initial plan using machine learning, allowing human planners to refine and optimize the plan more efficiently\u00b2. For instance, Li et al.\u00b2 successfully employed architectures like ResNet\u00b3 and DenseNet\u2074 to create high-quality starting plans, thereby accelerating the overall treatment planning process. Another approach bypasses the trial-and-error process entirely by using machine learning to directly predict desired 3D dose distributions and corresponding 2D fluence map intensities based on patient image and contour data\u2075,\u2076. For example, Vandewinckele et al.\u2075 utilized a U-Net-based convolutional neural network (CNN)\u2077 to predict 3D dose distributions from CT scans and the contours of targets and organs. They subsequently applied another U-Net-based CNN to predict the 2D fluence map from the 3D dose, with or without patient image and contour data, for lung cancer IMRT cases.\nWhile these AI techniques have significantly reduced the time required for treatment planning, they often depend on large databases of high-quality patient data for training due to their supervised or unsupervised learning nature. Additionally, the trained networks are frequently not universally applicable across different institutions due to data heterogeneity\u2078, creating barriers to their widespread adoption. Furthermore, these developments typically"}, {"title": "II. Methods and Materials", "content": "lack adversarial attack testing, despite vulnerabilities being identified in various supervised learning algorithms used in medical applications\u2079.\nReinforcement learning (RL)\u00b9\u2070, which mimics the trial-and-error learning process used by humans to achieve their goals, offers a new angle to accelerate treatment planning. Unlike other methods, the trial-and-error process in RL generates substantial new data samples that the algorithm can learn from, thereby reducing the dependence on large initial datasets. Recently, the application of deep neural network-based Q-learning, specifically Deep Q-Networks (DQN)\u00b9\u00b9, has shown promising results in automating treatment planning\u00b9\u00b2,\u00b9\u00b3,\u00b9\u2074,\u00b9\u2075,\u00b9\u2076,\u00b9\u2077. For example, Shen et al.\u00b9\u00b2 utilized DQN to develop a network that observed dose volume histograms (DVHs) and output actions to adjust organ weighting factors in inverse treatment planning for high-dose-rate brachytherapy in cervical cancer. The same research group later demonstrated the feasibility of this approach for automatic tuning of TPPs in external beam IMRT for prostate cancer\u00b9\u00b3. Additionally, Sprouts et al.\u00b9\u2076 extended the DQN-based virtual treatment planner (VTP) to adjust TPPs compatible with commercial TPS systems, achieving effective treatment planning for prostate IMRT.\nDespite these promising advancements, the DQN network has inherent limitations that complicate its application to complex, clinically relevant treatment planning scenarios. In clinical settings, human planners often adjust numerous TPPs for different targets and organs at risk (OARs), resulting in a vast state-action space that the RL algorithm must explore. In such cases, finding the optimal action concerning the Q function can be costly due to challenges like overestimation of the Q value function and difficulties in balancing exploration and exploitation\u00b9\u2078. To partially address these issues, Shen et al. introduced a knowledge-guided network training strategy\u00b9\u2074 and a hierarchical approach\u00b9\u2075 within the DQN framework, demonstrating some success in prostate Stereotactic Body Radiation Therapy (SBRT) automatic treatment planning\u00b9\u2077. However, not all challenges were resolved. Additionally, continuous TPP tuning leads to a continuous action space, where DQN's effectiveness diminishes. Like other deep neural networks, DQNs are also vulnerable to adversarial attacks\u00b9\u2079, raising concerns about their robustness in clinical applications.\nTo tackle these challenges, we propose using a new RL approach: the Actor-Critic Experience Replay (ACER)\u00b2\u2070 method to automate the treatment planning process. ACER builds on the advanced actor-critic algorithm (A3C)\u00b2\u00b9, enhancing data sampling efficiency"}, {"title": "II. Methods and Materials", "content": "through experience replay\u00b2\u00b2. In this framework, the actor functions as the policy network, aiming to maximize returns, while the critic assesses the quality of the actor's decisions. This setup inherently facilitates exploration and exploitation, and the policy gradient method allows effective exploration of both discrete and continuous action spaces. Previous studies indicate that A3C can be more resistant to adversarial attacks than DQN\u00b9\u2079. Based on these observations, we propose applying ACER to develop a training-efficient, robust, and scalable agent for automatic treatment planning applications.\nIn the following sections, we will detail our implementation of the ACER algorithm for automating the TPP tuning process in inverse treatment planning, using prostate IMRT as a test case. Followed by it is the evaluation of its performance across different datasets, including its vulnerability to adversarial attacks."}, {"title": "II. Methods and Materials", "content": "II.A. Overall Architecture\nThe overall architecture of the ACER based automatic treatment planning system is similiar to that of the DQN-based system\u00b9\u2076, as shown in Figure 1. The process begins with the random initialization of the TPPs, which are then input into the TPS system for inverse treatment planning. The quality of the resulting treatment plan is evaluated, and if it does not meet the desired standards, both the plan and the TPPs are fed into the ACER-based VTP system for TPP tuning. With the updated TPPs, the TPS performs inverse treatment planning optimization again. This iterative process continues until the plan quality meets the required standards or the maximum number of TPP tuning iterations is reached."}, {"title": "II.B. In-house Treatment Planning System (TPS)", "content": "In the following subsections, we will give the design details for the in-house TPS, the ACER-based VTP system, the testbed, the plan evaluation system, and the system performance test."}, {"title": "II.B. In-house Treatment Planning System (TPS)", "content": "We developed an in-house dose-volume constraint TPS\u00b9\u2076 following the documentation of Varian's Eclipse\u00b2\u00b3. For a treatment planning containing a single target and N OARs, the objective function for the fluencemap optimization can be formulated as:\n$\\min \\frac{1}{2}||Mx - d_p||_2^2 + \\frac{\\lambda}{2} ||(Mx - t_{dp})_{VPTV}||_2^2 + \\sum_{i=1}^{N} \\frac{\\lambda_i}{2} ||(Mx - t_{idp})_{V_i}||_2^2$,\ns.t. $x \\geq 0$, $D_{95\\%}(Mx) = d_p$. (1)\nIn this equation, $|.|_{2}$ and $|.|_{2}$ represent the standard $l_2$ norms, which compute only the negative and positive elements, corresponding to under-dose and over-dose constraints, respectively. The under-dose constraint is further reinforced that at least 95% of the PTV volume receives the prescription dose. The relative importance of the respective terms is adjusted by the weighting factors $\\lambda$ and $\\lambda_i$. Regarding the other variables in the equation, $M$ represents the dose deposition matrix, $x$ is the beamlet vector, and $d_p$ is the prescription dose. $t_{dp}$ and $V_{PTV}$ are upper threshold dose and the percentage of the PTV volume considered for over-dose constraints, respectively. Similarly, $M_i$, $V_i$, and $t_i$ are the corresponding variables used for the over-dose constraints on the $i$th OAR. Voxels included in $V_{PTV}$ and $V_i$ always receive higher dose than those not selected.\nIn summary, the free TPPs to be tuned in this in-house TPS are $\\lambda$, $V_{PTV}$, $t$, $\\lambda_i$, $V_i$, and $t_i$ (where i = 1, 2, ..., N), totaling 3(N + 1) parameters. Given a set of TPPs, the optimization problem can be solved using the alternating direction method of multipliers (ADMM)."}, {"title": "II.C. Actor Critic with Experience Replay (ACER)-based Virtual Treatment Planner (VTP) System", "content": "II.C.1. The working principle of ACER\nACER is a type of deep reinforcement learning that integrates a deep neural network with actor-critic learning while leveraging the experience replay \u00b2\u2070. This approach has shown"}, {"title": "II. Methods and Materials", "content": "superior performance in challenging environments, including the Atari57 game collection \u00b2\u2070.\nIn actor-critic learning, an actor agent generates decisions while a critic agent evaluates those decisions in the context of a sequential decision-making problem. This process involves a dynamic environment represented by a series of states $s \\in S$ associated with a series of possible actions $a \\in A$. After taking an action $a$ in state $s$, the state transitions into the next state $s'$ with a probability $Pr{s'|s, a}$, yielding a stepwise reward $r \\in R$. The optimal decision made by the actor agent, or the optimal policy $\\pi$, can be defined as choosing those series of actions $a$ to maximize the accumulated reward for states $s$ over time $t \\in (0, 1, 2...)$. The corresponding objective function is \u00b2\u2074:\n$J(\\pi) = \\sum_{s} \\lim_{t\\to\\infty} Pr{s_t = s|s_0, \\pi} \\sum_{a} \\pi(a|s)R$.\nRepresenting the policy $\\pi$ by a network that has parameters $\\theta$, the policy parameters $\\theta$ can be optimized using the policy gradient approach governed by the policy gradient theorem \u00b2\u2074 as:\n$g = \\nabla_{\\theta}J(\\pi_{\\theta})$\n$= \\sum_{s} \\lim_{t\\to\\infty} Pr{s_t = s|s_0, \\pi} \\sum_{a} \\nabla_{\\theta}\\pi_{\\theta}(a|s) Q^{\\pi_{\\theta}}(a, s)$\n$= E_\\pi [Q^{\\pi_{\\theta}}(a, s)\\nabla_{\\theta}log\\pi_{\\theta}(a|s)]$. (3)\nHere, $Q^{\\pi_{\\theta}}(a, s)$ is the state-action value function, serving as an effective evaluation of the policy performance.\nIn actor-critic learning, $Q^{\\pi}(a, s)$ is estimated by the critic agent. When selecting an action $a$ in a state $s$ at step $t$, the critic estimates $Q^{\\pi}(a_t, s_t)$ as the expected cumulative reward\n$E_{s_{t+1:\\infty}, a_{t+1:\\infty}} (\\sum_{i\\geq 0} \\gamma^{i} r_{t+i} | a_t = a, s_t = s)$ (4)\nfollowing policy $\\pi$. Here, $\\gamma \\in [0, 1)$ is the discount factor for future rewards.\nIn ACER, the actor and critic agents are integrated into a deep neural network with 'two heads'. One head gives the policy $\\pi_{\\theta}(a_t, s_t)$ with network parameters $\\theta$, while the other outputs the estimate $Q_{\\theta_c}(a_t, s_t)$ with network parameters $\\theta_c$. Particularly, ACER designs the policy network to contain two parts, the distribution $f$, and the statistics $\\phi(s)$ of $f$, thus the policy can be fully represented as $\\pi(\\cdot|s) = f(\\cdot|\\phi_{\\theta}(s))$."}, {"title": "II.C. Actor Critic with Experience Replay (ACER)-based Virtual Treatment Planner (VTP) System", "content": "To solve the intrinsic instability that arises from combining online RL with deep neural network \u00b2\u00b9, ACER employs a hybrid online and offline training strategy to optimize the network performance. Specifically, it adopts the online asynchronous updating of $\\theta$ and $\\theta_c$ by launching parallel network learners that interact with different instances of the environment \u00b2\u00b9. This technique de-correlates the agents' data, thereby stabilizing the learning process. Additionally, it implements the offline experience replay strategy\u00b2\u00b2, allowing the agent to learn from memory of experiences. Trajectories can be retrieved from the memory and weighted by importance sampling, promoting both learning stability and data efficiency.\nParticular to the training of the critic network, ACER constructs the target Q with retrace estimator \u00b2\u2075 as\n$Q^{tar}(a_t, s_t) = Q^{ret}(a_t, s_t)$ (5)\n$= r_t + \\gamma \\rho_{t+1} [Q^{ret}(a_{t+1}, s_{t+1}) - Q_{\\theta_c}(a_{t+1}, s_{t+1})] + V_{\\theta_c}(s_{t+1})$.\nHere, $\\rho = min(c, p)$ is the truncated importance weight used in experience replay, where c is a constant and p is the importance weight. During online updating, $\\rho = 1$. The value function $V_{\\theta_c}(s)$ is derived from the critic's Q estimator as $V_{\\theta_c}(s) = \\sum_a Q_{\\theta_c}(a|s)f(a|\\phi_{\\theta}(s))$. $Q^{ret}$ has been shown to have low variance and to converge effectively. With it, the critic network parameter $\\theta_c$ is then updated as $\\delta\\theta_c = \\delta\\theta_c + \\nabla_{\\theta_c} (Q^{ret}(a, s) - Q_{\\theta_c}(a, s))^2$.\nThe low-variance $Q^{ret}$ is also employed to stabilize online policy updates by replacing $Q_e$ term as $Q^{ret} - V$ in Eq. (3). ACER further incorporates truncated importance sampling in experience replay with bias correction to enhance data efficiency while avoiding excessive bias in policy updates, which yields the offline policy gradient g with respect to $\\phi$ as follows:\n$\\delta_{\\phi} g^{ACER} = E_{s_t} [\\frac{\\rho_t}{\\pi(a_t|s_t)} \\nabla_{\\phi_{\\theta}(s_t)} log f(a_t|\\phi_{\\theta}(s_t)) [Q^{ret}(a_t, s_t) - V_t(s_t)]$\\n$+ (\\frac{\\rho_t}{\\pi(a_t|s_t)} - 1) \\sum_{a} \\nabla_{\\phi_{\\theta}(s_t)} log f(a|\\phi_{\\theta}(s_t)) [Q_{\\theta_c}(a, s_t) - V_{\\theta_c}(s_t)]]$. (6)\nFinally, to limit the per-step changes to the policy and achieve stability, ACER provides the option to utilize a modified version of Trust Region Policy Optimization (TRPO)\u00b2\u2076, ensuring that the updated policy does not deviate significantly from the average policy network $\\phi_a$. Specifically, it restricts the policy network parameter $\\theta$ updating at step t as\n$\\delta\\theta = \\delta\\theta + \\frac{\\nabla_{\\phi_{\\theta(s_t)}}}{\\theta} g^{ACER} \\cdot max{0, \\frac{k^T g^{ACER}}{||k||^2} - \\delta} k$ (7)"}, {"title": "II. Methods and Materials", "content": "Here, $k = \\nabla_{\\phi_{\\theta(s_t)}} D_{KL}[f(\\cdot|\\phi_{\\theta_a}(s_t)) || f(\\cdot|\\phi_{\\theta}(s_t))]$ is the linear Kullback-Leibler divergence (DKL) and $\\delta$ is the predefined divergence constraint. In this situation of disabled TRPO updating, the policy network parameter is updated as\n$\\delta\\theta = \\delta\\theta + \\frac{\\nabla_{\\phi_{\\theta(s_t)}}}{\\theta} g^{ACER}$ (8)\nIn practice, entropy regularization term $\\beta \\nabla_{\\theta}H(\\pi(s, \\theta))$\u00b2\u00b9 can also be used to boost online and offline policy update performance, with $\\beta$ the weighting parameter. This term improves exploration by discouraging premature convergence to sub-optimal deterministic policies.\nWith variables $\\delta\\theta_c$ and $\\delta\\theta$ computed, the network parameters $\\theta_c$ and $\\theta$ are updated with RMSprop algorithm\u00b2\u2077. Under active TRPO updating, the parameters $\\theta_a$ for the average policy network $\\phi_a$ is updated as $\\theta_a = \\alpha \\theta_a + (1 - \\alpha)\\theta$, with $\\alpha$ the average model decay rate."}, {"title": "II. Methods and Materials", "content": "II.C.2. The establishment of the VTP system upon ACER architecture\nWe apply the ACER architecture to develop the VTP system for automatic treatment planning as follows. We define an input state s as discrete points taken from the DVH curves of a treatment plan. For a treatment plan containing M target and OARs, the input state s has a dimension of m \u00d7 M, where m represents the number of discrete points on each DVH curve. The action space consists of tuning strategies for the TPPs. In this initial approach, we allow each TPP to have two tuning strategies: increasing or decreasing by a predefined amount. With M target and OARs, there are 3M TPPs to tune, resulting in an action space of length 6M. This defines the dimensions of both the policy distribution and the Q-value function space, each with length 6M.\nOnce a TPP tuning action a is predicted for treatment plan s at time t, the in-house TPS performs inverse treatment planning to generate a new plan $s_{t+1}$. For each state-action pair $(s_t, a_t)$, the immediate reward r is calculated as the difference in plan quality between the new plan and the current plan, that is $r_t = \\psi(s_{t+1}) - \\psi(s_t)$, where $\\psi(s)$ represents the quality evaluation of the plan s. The total reward is computed as $R = \\sum_{i=t}^{tepi-1} \\gamma^{i-t}r_i$, reflecting the accumulated return across all future plans in that episode.\nTraining of the ACER-based VTP follows the standard ACER training process. We launch $N_a$ parallel online training agents, each with distinct input treatment plans. The"}, {"title": "II.C. Actor Critic with Experience Replay (ACER)-based Virtual Treatment Planner (VTP) System", "content": "agents asynchronously update the network policy upon completing an episode with a maximum step length of $t_{epi}$. After each episode, each agent is restarted with a different input treatment plan. During the process, each agent stores its episodic trajectories in a replay buffer with a storage length of $t_{rep}$ steps. Once $t_s$ steps have been accumulated, offline policy training examines the experience pool using a batch size of $t_B$ steps and begins to update the policy network at a frequency $p \\geq 1$ times higher than that of the online updates. To consistently monitor training performance, we evaluate the process every $t_{eval}$ steps using independent evaluation patient cases. The training continues until a maximum of $T_{max}$ steps is reached."}, {"title": "II.C.3. Testbed", "content": "In line with our previous development efforts\u00b9\u2076, this paper continues to use IMRT treatment planning for prostate cancer as a testbed to test the proposed ACER-based VTP system. We consider a scenario involving one target (the prostate) and two OARs (the bladder and the rectum), leading to the optimization of three DVH curves. Each curve is represented by 100 discrete points, resulting in a total of 300 floating values as the input to the ACER agent. According to Eq. (1), we have 9 TPPs to tune, creating an action space of 18, as shown in Table 1. The specific increment and decrement amplitudes are determined based on experience and are not expected to significantly affect the overall convergence of the treatment planning process."}, {"title": "II.D. Adversarial Attack", "content": "Division, which is publicly available and contains one independent patient case\u00b2\u2078. The third dataset is The Radiotherapy Optimization Test Set (TROTS), which includes 30 independent patient cases\u00b2\u2079.\nIt is worth noting that for each dataset, by randomly initializing the TPP values, we can generate numerous independent initial treatment plans to form different state-action-reward trajectories. In this study, to demonstrate the network training efficacy upon limited patient cases, we use one patient case in dataset 1 for training, two independent patient cases in dataset 1 for validation, and all remaining cases across the three datasets for testing."}, {"title": "II.D. Adversarial Attack", "content": "After completing the training of the ACER agent, except for testing its performance on TPP tuning decisions, we also evaluate its robustness against adversarial attacks. Adversarial attacks are malicious attempts to manipulate machine learning models into making incorrect predictions or decisions\u00b3\u2070,\u00b3\u00b9. Given the potential clinical application of automatic treatment planning agents in the future, it is crucial to train the ACER agent to be robust against such attacks\u2079.\nWe assume the adversary has access to the trained policy network, allowing it to fool the network by perturbing the input state in a way that exploits the policy's sensitivity. Specifically, we use the Fast Gradient Sign Method (FGSM)\u00b3\u00b2 to compute the state perturbation $\\eta$, which is the gradient of loss function J with respect to the state s:\n$\\eta = e sign(\\nabla_s J(\\theta, s, y))$. (9)\nHere, e is a constant, serving as the upper limit for the element-wise perturbation, i.e., $|\\eta_x| \\leq \\epsilon$. y is the distribution over all possible actions. With this, the perturbed state becomes $s' = s + \\eta$.\nWe apply the FGSM-based attack to both the ACER agent trained in this work and the DQN agent from our previous study\u00b9\u2076, and compare their robustness to the attack. In the ACER agent, y is the stochastic policy $\\pi$. $J(\\theta, s, y)$ is represented as the cross entropy loss between y and the distribution that places all weight on the highest-weighted action $a_j$"}, {"title": "III. RESULTS", "content": "in y. Specifically\n$J(\\theta, s, \\pi) = -\\frac{1}{18} (log(\\pi^\\theta(a_j|s)) + \\sum_{i\\neq j} log(1 - \\pi^\\theta(a_i|s)))$ (10)\nIn the DQN agent, the policy determined by the Q value function is deterministic, which causes the problem that the gradient of $J(\\theta, s, y)$ is almost zero for all input states. To solve it, we define y as the softmax of the Q value function\u00b9\u2079. We set e to be values of 0.001, 0.01, and 0.1, apply the corresponding attacks and record their perturbations on action priorities and the next-step treatment plan qualities."}, {"title": "III. Results", "content": "III.A. Training results"}, {"title": "III.A. Training results", "content": "IMRT treatment planning are listed in Table 3. We utilize 3 CPU cores for online asynchronous training, aiming to complete the entire training within 250,000 steps. To encourage early convergence in the treatment planning process, we limit the length of each episode to 20 steps. Offline training begins with the storage of 2,000 steps of experiences, with a total maximum experience storage capacity of 100,000 steps. The TRPO-based updating method has been found to be computationally expensive; therefore, we disable it in this study to simplify the network. The values for the remaining parameters in Table 3 are consistent with those used for training the ACER agent in the Atari57 game set \u00b2\u2070. The entire training process takes approximately 7 days on an Intel(R) Core(TM) i7-6850K CPU @ 3.60GHz.\nThe convergence map of the agent training process, evaluated based on the average plan score for the validation patient cases, is shown in Figure 3. As illustrated, the plan score gradually approaches the maximum value of 9 as training progresses, with reduced fluctuations until approximately 200,000 steps. Beyond this point, performance becomes unstable, exhibiting large fluctuations, which we interpret as overfitting to the training cases. Therefore, we select the policy obtained at an earlier convergence point, around step 120,500, for testing."}, {"title": "III. RESULTS", "content": "III.B. Testing results\nTo demonstrate the efficacy of ACER-guided automatic treatment planning, we first test the network using 49 patient cases independent of the training and validation cases from dataset 1 and compare the results with DQN-based treatment planning\u00b9\u2076. The TPPs are initialized with trivial values (all set to 1, except for $V_{PTV}$ = 0.1), as done in our previous work\u00b9\u2076."}, {"title": "III.B. Testing results", "content": "cases with plan scores in the range of [i, i + 1)). Before ACER-guided treatment planning, plan scores are distributed broadly from 2 to 9, with a mean score and standard deviation (std.) of 6.20 \u00b12.01. After ACER-guided treatment planning, 42 out of 49 cases achieve a full score of 9, 1 case reaches 8.9, 3 cases score 8, and 3 cases score between 7 and 8. The corresponding mean and std. are 8.85 \u00b1 0.43. In comparison, DQN-guided treatment planning improves the plan score from 6.18 \u00b1 1.75 to 8.14 \u00b1 1.27 for 50 patient cases from the same patient dataset \u00b9\u2076.\nIn Figure 4 (c), the same 49 patient cases are divided into 8 groups based on their initial plan scores. The mean and standard deviation of the plan scores for each group, both before and after ACER-based treatment planning, are plotted. It is evident that after ACER-guided treatment planning, the plan scores are uniformly improved, approaching 9 across all patient groups, including those with very low initial scores below 3 (patient group 1). In contrast, DQN-based treatment planning shows that some patients with low initial scores could not be efficiently improved (as depicted in Figure 5(a) in Sprouts et. al\u00b9\u2076).\nWe then generate 147 treatment plans by random initialization of TPPs for the same 49 patients (3 random plans for each patient) and perform ACER-guided treatment planning. The results are shown in Figure 4 (b) and (d). In Figure 4 (b), the patient cases are grouped into 7 categories based on their plan scores, using the same method as in Figure 4 (a). Before ACER-guided treatment planning, the mean and std. of the treatment plans are 6.33\u00b11.65. 135 out of 147 cases achieve a full score of 9, and 12 cases reach a score of 8, with no"}, {"title": "III. RESULTS", "content": "cases scoring below 8. The corresponding mean and std. after planning are 8.92 \u00b1 0.27. The corresponding patient group distribution is shown in Figure 4 (d), which also shows that ACER-guided treatment planning improve the plan score uniformly across all patient groups, demonstrating the efficacy of this planning agent."}, {"title": "III.B. Testing results", "content": "We then extend the network test to datasets 2 and 3 to assess the generality of the trained network on patient cases with features distinct from the training and validation cases. Considering dataset 2 contains only one patient case, we enrich the test by randomly initializing the TPPs 30 times to create 30 independent initial plans. As for dataset 3, it contains 30 different patient cases. Yet, the PTV and OAR data are in a sampled format. We randomly initialize the TPPs for 3 times for each patient case to generate 90 initial treatment"}, {"title": "III.B. Testing results", "content": "plans. For both datasets, the ACER-guided treatment planning is applied to optimize these treatment plans. The results are shown in Figure 5 (a) and (b). From 5(a), the plan scores for the initial plans in dataset 2 have a mean \u00b1 std. of 3.91 \u00b1 0.26. After ACER-based treatment planning, all 30 cases have been elevated to 9. Figure 5 (b) shows that for dataset 3, the initial plans have a mean \u00b1 std. of 7.98 \u00b1 0.133. After ACER-guided treatment planning, 86 out of the 90 cases have been improved to a score of 9. The corresponding mean \u00b1 std. after planning is 8.96 \u00b10.21. These results further demonstrate the efficacy of ACER agent in high quality treatment planning across different datasets.\nCombing all test cases, the mean \u00b1 std. of the plan score distributions before ACER-based treatment planning is 6.20 \u00b1 1.84. After implementing ACER-based treatment planning, 93.09% of the cases achieve a perfect score of 9, with only 6.12% scoring between 8 and 9, 0.78% scoring between 7 and 8, and no cases scoring below 7. The mean \u00b1 std. of the final scores is 8.93 \u00b10.27.\nFurthermore, we illustrate how ACER-based VTP observes an intermediate treatment plan and makes the TPP adjustment decision for a representative testing case in Figure 6. As is shown in Figure 6 (a) and (d), at the initial step, the plan fails to spare the bladder, partially fails to spare the rectum, and has a hotspot in the PTV, resulting in a low initial plan score of 2. The VTP observes this plan and decides to lower the threshold dose value for the bladder ($t_{BLA}$) in the first step, which improves the plan score to 4 by fully sparing the bladder volume. It then continues to enhance rectum sparing by lowering the threshold value for the rectum ($t_{REC}$). However, these adjustments result in an even hotter PTV. To address this issue, over the next 14 steps, the ACER-based VTP reduces the priorities for the OARS ($A_{REC}$ and $A_{BLA}$), lowers threshold dose value in PTV ($t_{PTV}$), and increases the PTV priority ($A_{PTV}$) until reaching a score of 9. These actions relax OAR constraints while tightening PTV constraints, mirroring the adjustments a human dosimetrist would make. This indicates that the ACER-based agent exhibits a human-like approach to TPP tuning.\nFinally, since the ACER-based treatment planning agent utilizes a stochastic policy, it is important to understand its policy behavior to ensure stability in guiding treatment planning. To investigate this, we identify 5 common cases from ACER-guided and DQN-guided treatment plannings, each with an initial plan score of 5 due to the failure in rectum dose sparing. The mean and standard deviation of"}]}