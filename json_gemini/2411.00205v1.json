{"title": "Compositional Automata Embeddings for\nGoal-Conditioned Reinforcement Learning", "authors": ["Beyazit Yalcinkaya", "Marcell Vazquez-Chanlatte", "Niklas Lauffer", "Sanjit A. Seshia"], "abstract": "Goal-conditioned reinforcement learning is a powerful way to control an Al agent's\nbehavior at runtime. That said, popular goal representations, e.g., target states\nor natural language, are either limited to Markovian tasks or rely on ambiguous\ntask semantics. We propose representing temporal goals using compositions of\ndeterministic finite automata (cDFAs) and use cDFAs to guide RL agents. cDFAs\nbalance the need for formal temporal semantics with ease of interpretation: if\none can understand a flow chart, one can understand a cDFA. On the other hand,\nCDFAs form a countably infinite concept class with Boolean semantics, and subtle\nchanges to the automaton can result in very different tasks, making them difficult\nto condition agent behavior on. To address this, we observe that all paths through a\nDFA correspond to a series of reach-avoid tasks and propose pre-training graph\nneural network embeddings on \u201creach-avoid derived\u201d DFAs. Through empirical\nevaluation, we demonstrate that the proposed pre-training method enables zero-shot\ngeneralization to various CDFA task classes and accelerated policy specialization\nwithout the myopic suboptimality of hierarchical methods.", "sections": [{"title": "Introduction", "content": "Goal-conditioned reinforcement learning (RL) [33] has proven to be a powerful way to create AI\nagents whose task (or goal) can be specified (conditioned on) at runtime. In practice, this is done by\nlearning a goal encoder, i.e., a mapping to dense vectors, and passing the encoded goals as inputs to\na policy, e.g., a feedforward neural network. This expressive framework enables the development\nof flexible agents that can be deployed in a priori unknown ways, e.g., visiting states never targeted\nduring training. The rise of large language models has popularized leveraging natural language as an\nergonomic means to specify a task, e.g., \"pick up the onions, chop them, and take them to stove.\u201d\nWhile incredibly powerful, tasks specified by target states or natural language have a number of\nshortcomings. First and foremost, target states are necessarily limited to non-temporal tasks. On\nthe other hand, natural language is, by definition, ambiguous, providing little in the way of formal\nguarantees or analysis of what task is being asked of the Al agent.\nTo this end, we consider conditioning on tasks in the form of Boolean combinations of deterministic\nf finite automata (DFAs). We refer to this concept class as compositional DFAs (cDFAs). The choice of\nCDFAs as the concept class is motivated by three observations. First and foremost, DFAs offer simple\nand intuitive semantics that require only a cursory familiarity with formal languages-if one can\nunderstand a flow chart, one can understand a DFA. Moreover, recent works have demonstrated that\nDFA and CDFA can be learned in a few shot manner from expert demonstrations and natural language\ndescriptions [41]. As such, DFAs offer a balance between the accessibility of natural language and\nrigid formal semantics. The addition of Boolean combinations to cDFA, e.g., perform task 1 AND\ntask 2 AND task 3, provides a simple mechanism to build complicated tasks from smaller ones.\nSecond, DFAs represent temporal tasks that become Markovian by augmenting the state-space with\nf finite memory. Further, they are the \u201csimplest\u201d family to do so since their finite states are equivalent\nto having a finite number of sub-tasks, formally Nerode congruences [19]. This is particularly\nimportant for goal-conditioned RL which necessarily treats temporal tasks differently than traditional\nRL. For traditional RL, because the task is fixed, one can simply augment the state space with the\ncorresponding memory to make the task Markovian. In goal-conditioned RL, this is not, in general,\npossible as it is unclear what history will be important until the task is provided. Instead, the encoded\ntask must relay to the policy this temporal information. Third, existing formulations like temporal\nlogics over finite traces and series of reach-avoid tasks are regular languages and thus are expressible\nas DFAs [11]. This makes DFAs a natural target for conditioning an RL policy on temporal tasks.\nThe expressivity of DFAs introduces a number of challenges for goal-conditioned RL. First, DFAs\nform a countably infinite and exponentially growing concept class where subtle changes in the DFA\ncan result in large changes in an agent's behavior. Notably, this means that any distribution over\nDFAs is necessarily biased with many \u201csimilar\u201d DFAs having drastically different probability. Thus,\nto generally work with DFAs, one cannot simply match finite patterns, but need to learn to encode\ndetails necessary for planning. Second, as with traditional goal-based objectives, DFAs provide a\nvery sparse binary reward signal\u2013did you reach the accepting state or not? Together with non-trivial\ndynamics, na\u00efve applications of RL become infeasible due to the lack of dense reward signal. Finally,\nthe exponentially-expanding concept class presents computational limitations for encoding. For\nexample, many interesting DFAs may be too large to be feasibly processed by a graph neural network.\nTo address these issues of reward sparsity and the need to encode planning, we introduce a distribution\nf DFAs, called reach-avoid derived (RAD). This concept class is inspired by the observation that all\npaths through a DFA correspond to a series of (local) reach-avoid problems. We argue in Section 4\nthat RAD encourages learning to navigate a DFA's structure. Our first key result is that pre-training\nDFA encoders on RAD DFAs enables zero-shot generalization to other DFAs. Next, we treat the\nproblem of DFA size. Many problems are naturally expressed compositionally, e.g., a sequence of\nrules that must all hold or a set of tasks of which at least one must be accomplished. Due to their\nBoolean semantics, i.e., did you reach the accepting state or not, DFAs offer well-defined semantics\nunder Boolean compositions. Our second key insight is to encode conjunctions\u00b2 of DFAs (called\ncDFAs). This is done by using a graph attention network [9] (GATv2) to encode the individual\nDFA graph structure as well as the conjunction (AND) relationships between a collection of tasks.\nRecalling that the conjunction of any two DFAs grows (worst-case) quadratically, we observe that\nCDFAs offer an exponential reduction in the size of temporal tasks passed to GATv2."}, {"title": "1.1 Contributions", "content": "Our main contributions are: (1) we propose compositional DFAs (cDFAs),\nwhich balance formal semantics, accessibility, and expressivity, as a goal representation for temporal\ntasks in goal-conditioned RL; (2) we propose encoding cDFAs using graph attention networks\n(GATv2); (3) we propose pre-training the cDFA encoder on reach-avoid derived (RAD) DFAs, and (4)\nwe perform experiments demonstrating strong zero-shot generalization of pre-training on RAD DFAs."}, {"title": "1.2 Related Work", "content": "Our work explores a temporal variant of goal-conditioned RL [33] where goals\nare represented as automata. While traditionally focused on goals as future target states, this has\nsince been extended to tasks such as natural language [7, 21, 26, 37] and temporal logic [38]. While\nnot as expressive as natural language, we believe cDFAs offer a balance between the ergonomics of\nlanguage while maintaining unambiguous semantics-something of increasing importance due to the\nseemingly inevitable proliferation of AI agents to safety-critical systems. Moreover, DFA inference\nhas a rich literature [6, 15, 17, 24, 27, 28, 45] with recent works have even shown the ability to\nlearn DFA and cDFA from natural language and expert demonstrations-bridging the gap even more\nbetween natural language and automata specified goals [41-43].\nOperating in a similar space, previous work, LTL2Action [38], has shown success with using (finite)\nlinear temporal logic (LTL), a modal extension of propositional logic, to condition RL policies on. In\nfact, the pre-training and test domains used in this paper are directly derived from that work.\nOur choice to focus on DFA rather than LTL is two-fold. First and foremost, over finite traces, LTL is\nstrictly less expressive than DFA. For example, LTL cannot express tasks such as \u201cthe number of\ntimes the light switch is toggled should be even.\" Second, like DFA, LTL tasks constitute a countably\ninfinite set of tasks. This again means that any distribution over LTL is necessarily biased to certain\nsubclasses. On the one hand, the syntactic structure makes separation of subclasses very easy. On the\nother, it remains unclear how to generalize to \u201ccommon\u201d LTL formula. By contrast, we argue that\nthe local reach-avoid structure of DFAs offers a direct mechanism for generalization. Finally, we\nnote that while LTL is exponentially more succinct than DFA, this is largely mitigated by supporting\nboolean combinations of DFAS (cDFAs).\nMoving away from goal-conditioned RL, recent works have proposed performing symbolic planning\non the DFA and cleverly stringing together policies to realize proposed paths [16, 22, 23, 30].\nHowever, their method can still suffer from sub-optimality due to their goal-conditioned value\nfunctions being myopic. Specifically, if there are multiple ways to reach the next sub-goal of a\ntemporal task, the optimality of the next action depends on the entire plan, not just the next sub-goal-\ndestroying the compositional structure used for planning. For example, Figure 4 shows a simple\nexample in which hierarchical approaches will find a suboptimal solution due to their myopia. In\ncontrast, conditioning on cDFA embeddings allows our policies to account for the entire task.\nOn the single-task LTL-constrained policy optimization side, several works have tackled the problem\nin varying settings [10, 13, 36, 44]. Adjacent to these efforts, various approaches have explored LTL\nspecifications and automaton-like models as objectives in RL [2, 3, 8, 12, 14, 20, 29, 32, 46, 48].\nA different line of work considers leveraging quantitative semantics of specifications to shape the\nreward [1, 22, 25, 36]. However, all of these lines of work are limited to learning a single, fixed goal.\nFinally, in our previous work [47], we used hindsight experience replay [5] to solve the reward\nsparsity problem for DFA-conditioned off-policy learning of DFA task classes. We observe that our\nRAD pre-training pipeline has similar sample efficiency while generalizing to larger classes of DFAs."}, {"title": "Preliminaries", "content": "In the next sections, we will develop a formalism for conditioning agent behavior on temporal tasks\nrepresented as DFAs. To facilitate this exposition, we quickly review goal-conditioned RL and DFA."}, {"title": "2.1 Goal-Conditioned RL", "content": "We start with technical machinery for goal-conditioned RL.\nDefinition 2.1. A Markov Decision Process\u00b3 (MDP) is a tuple M = (S, A,T), where S is a set\nof states, A is a set of actions, and T is a map determining the conditional transition probabilities,\ni.e. Pr(s' | s,a) def T(s',a,s) for s, s' \u2208 S and a \u2208 A. We assume that M contains a sink\nstate, $, denoting the end of an episode. Any MDP, M, can be given y-temporal discounting\nsemantics by asserting that each transition can end the episode with probability 1 \u03b3. Formally,\nT'(s', a, s) = (1 \u2212 \u03b3) \u00b7 T(s', a, s), for s, s' \u2260 $.\nDefinition 2.2. Let G be a set of states called goals. A goal-conditioned policy4, \u03c0 : S \u00d7 G \u2192 \u2206A, is\na map from state-goal pairs to distributions over actions. Together, an MDP M and a goal-conditioned\npolicy, \u03c0, define a distribution of sequences of states, called paths.\nA goal-conditioned reinforcement learning problem is a tuple consisting of a (possibly unknown)\nMDP and a distribution over goals, Pr(g) for g \u2208 G. The objective of goal-conditioned RL is to\nmaximize the probability of generating a path containing g."}, {"title": "2.2 Deterministic Finite Automata", "content": "Next, we provide a brief refresher on automata.\nDefinition 2.3. A Deterministic Finite Automaton (DFA) is a tuple A = (Q, \u03a3, \u03b4, qo, F), where Q\nis a finite set of states, \u2211 is a finite alphabet, \u03b4 : Q \u00d7 \u2211 \u2192 Q is the transition function, qo \u2208Q is\nthe initial state, and FC Q are the accepting states. The transition function can be lifted to strings\nvia repeated application, * : Q \u00d7 \u2211* \u2192 Q. One says A accepts x if \u03b4* (qo, x) \u2208 F. Finally for\nsimplicity, we abuse notation and denote by DFA the set of all DFA.\nVisually, we represent DFAs as multi-graphs whose nodes represent states and whose edges are\nannotated with a symbol that triggers the corresponding transition. Omitted transitions are assumed\nto stutter, i.e., transition back to the same state. DFAs have a number of interesting properties.\nUp to a state isomorphism, all DFAs can be minimized to a canonical form, e.g., using Hopcroft's\nalgorithm [18], denoted by minimize(A). Further, DFAs are closed under Boolean combinations.\nFor example, the conjunction of two DFAs, A1 and A2, denoted A1 ^ A2, is constructed by taking\nthe product of the states, Q = Q1 \u00d7 Q2, and evaluating the transition functions element-wise,\n\u03b4((91, 92), \u03c3) = (\u03b4\u2081 (91, \u03c3), \u03b42 (92, \u03c3)). Also, (q1, q2) \u2208 F if q1 \u2208 F\u2081 and q2 \u2208 F2. While DFAs are\nclosed under conjunction, each pair-wise operation results in a O(n\u00b2) increase in the number of states.\nDefinition 2.4. A compositional DFA (cDFA) is a finite set of DFA, C = {A1, ..., An}. The\nmonolithic DFA associated to cDFA, C, is given by monolithic(C) def NA\u2208c A. The semantics of a\nCDFA are inherited from its monolithic DFA, i.e., a string x is accepted by cDFA C if and only if it\nis accepted by monolithic(C). Graphically, cDFA are represented as trees of depth 1 where the root\ncorresponds to conjunction and each leaf is itself a DFA."}, {"title": "2.3 Augmenting MDPs with cDFAs", "content": "Observe that DFAs can be viewed as deterministic MDPS\nleading to a natural cascading composition we refer to as DFA-augmention of a MDP. Informally,\none imagines that the MDP and DFA transitions are interleaved. First, the MDP transitions given an\naction. The new state is then mapped to the DFA's alphabet. This symbol then transitions the DFA.\nThe goal is to reach an accepting state in the DFA before the MDP reaches its end-of-episode state.\nFormally, let A and M be a DFA and MDP, respectively and let L : S \u2192 \u2211 denote a labelling\nfunction mapping states to symbols in the DFAs alphabet. The new MDP is constructed as a quotient\nof the cascading composition of A and M. First, take the product of the state spaces, S \u00d7 Q. The\naction space is M's action set. The transition of the MDPs state is as before and the DFA transitions\nare given by: 8(q, L(s)). In order for there to be a unique goal and end-of-episode state, we quotient\nSX Q as follows: (i) ($, q) are treated as a single end of episode state and (ii) for all accepting states,\nq \u2208 F, the product states (s, q) are treated as a single accepting (goal) state."}, {"title": "3 Compositional Automata-Conditioned Reinforcement Learning", "content": "We are now ready to formally define (compositional) automata-conditioned RL as a variant of\ngoal-conditioned RL where goals are defined in the augmented MDPs.\nDefinition 3.1. A DFA-conditioned policy, \u03c0 : S \u00d7 DFA \u2192 \u2206A is a mapping from state-DFA pairs\nto action distributions. A DFA-conditioned RL problem is a tuple (M, P), where M is an MDP\nand P is a distribution over DFAs. The objective is to maximize the probability of generating a path\ncontaining the accepting state. cDFA-conditioned RL is defined via the underlying monolithic DFA."}, {"title": "3.1 CDFA Featurization", "content": "In order to pass a cDFA Ct to an MPNN, we construct a featurization\nGc\u2081 = (Vcr, cr, hc\u2081). Details of this process are given in Appendix A. Essentially, we apply four\nsequential modifications to the graph structure trivially induced by Ct: (i) add new nodes for every\ntransition in each DFA in the composition, remove these transitions to connect the new nodes to\nthe source and target nodes of the removed transitions, and in the feature vectors of the new nodes,\nencode all symbols triggering that transition using positional one-hot encodings, (ii) reverse all the\nedges, (iii) add self-loops to all nodes, and (iv) add edges from the nodes corresponding to the initial\nstates of DFAs to the \"AND\" node. Figure 2 shows the featurization of the cDFA given in Figure 1."}, {"title": "3.2 CDFA Embedding", "content": "Given a cDFA featurization Gc\u2081 = (Vc\u2081, \u00c9c\u2081,hc\u2081), we construct an\nembedding of the cDFA using a graph attention network (GATv2) [9] performing a sequence of\nmessage passing steps to map each node to a vector. For ease of notation, we refer to the features of a\nnode v \u2208 Vce as hy def hc\u2081 (v). At each message passing step, node features are updated as:\nh = \u2211 avuWtgthu,\n\u039a\u0395\u039d(\u03c5)\nwhere N (v) denotes neighbors of v, Wtgt is a linear map, and avu is the attention score between v\nand u computed as:\navu = softmaxi(evu)\nevu def a LeakyReLU (Wsrchv + Wtgthu),\nwhere a is a vector and Wsrc is a linear map. After message passing steps, the feature vector of the\nCDFA's \"AND\" node (which has received messages from all At \u2208 Ct) represents the latent space\nencoding gt of the temporal task given by Ct."}, {"title": "4 Pre-training on Reach-Avoid Derived (RAD) Compositional Automata", "content": "In this section, we introduce pre-training on reach-avoid derived (RAD) cDFAs. This pre-training is\ndesigned to solve three important problems. First, we wish to avoid simultaneously learning a control\npolicy and embeddings for the cDFAs. Second, we wish to learn a domain-independent cDFA encoder\nthat performs well across CDFA distributions. Third, we wish to have either a domain-specific or a\ngeneral cDFA-conditioned policy that performs well across different cDFA distributions.\nTo alleviate the first problem, i.e., decoupling of learning of control and cDFA embeddings, we\nfollow [38] and pre-train our cDFA encoding GNN on a \u201cdummy\u201d MDP containing a single non-\nend-of-episode state. To solve this dummy MDP, we connect a single linear layer to the output of the\nGNN which maps cDFA embeddings to the symbols in its alphabet. We then take these symbols and"}, {"title": "4.1 Sequential reach-avoid as the basis for planning", "content": "What remains then is to support gener-\nalization to other cDFA distributions. In particular, recall that cDFAs (and DFAs) lack a canonical\n\"unbiased\" distribution making it a priori difficult to pre-select a cDFA distribution. To begin, observe\nthat realizing any individual state transition q\u2192 q' through a DFA corresponds to a reach-avoid\ntask: (i) eventually transition to state q' and (ii) avoid symbols that transition to a state different than\nq and q'. Further, a path through a DFA, q1, . . ., qn, corresponds to a sequence of reach-avoid (SRA)\ntasks. Notably, as illustrated in Figure 3, SRA tasks can be represented as DFA. Thus, in order to\ngenerally reason about satisfying a DFA, our neural encoding must be able to solve SRA tasks."}, {"title": "4.2 Reach-avoid derived DFAs", "content": "For general DFAs, there can be arbitrarily many (often infinitely\nmany) paths leading to an accepting state, resulting in multiple SRA tasks interacting and interleaving.\nThus, the overall structure of a DFA might be much richer than an SRA task even though it consists\nof local reach-avoid problems. Building on the observation that paths of a DFA correspond to SRA\ntasks, we define reach-avoid derived DFAs as a generalization of SRA.\nDefinition 4.1. We define a mutation of a DFA A to be the process of randomly changing a transition\nof A, removing the outgoing edges of the accepting state, and then minimizing. To change a transition,\nwe uniformly sample a state-symbol-state triplet (q, o, q') and define \u03b4(q, \u03c3) \u00ba\u00baf q'. A DFA A is said\nto be m-reach-avoid derived (m-RAD) if it can be constructed by mutating an SRA DFA m-times.\nWe define the (m, p) -RAD distribution over DFAs as follows: (i) sample a SRA DFA, A, with k+2\nstates where k ~ Geometric(p); each transition stutters with probability 0.9 and is otherwise made a\nreach or avoid transition with equal probability; (ii) sequentially apply m mutations where mutations\nleading to trivial one-state DFAs are rejected. To avoid notational clutter, we shall often suppress\nm, p and say RAD DFAs. Finally, we define the RAD CDFA distribution by sampling n RAD DFAS,\nwhere n is also drawn from a geometric distribution. See Appendix B for the pseudocode. Figure 3\ndemonstrates how mutations and minimization can turn an SRA DFA into a RAD DFA. To get the\nRAD DFA, we apply two mutations to the SRA DFA, adding two new transitions: (i) from qo to q3 on\ngreen and (ii) from q4 to q1 on blue, and then the minimization collapses qo and q4 to a single state."}, {"title": "Experiments", "content": "We explore the following 6 questions on several task classes in discrete and continuous environments:\nRQ1 Do our policies overcome the limitations of hierarchical approaches?\nRQ2 Does pre-training of the cDFA encoder on RAD CDFAs accelerate cDFA-conditioned RL?\nRQ3 Does freezing the cDFA encoder negatively impact cDFA-conditioned RL performance?\nRQ4 Do the cDFA-embeddings pre-trained on RAD generalize to other task classes?\nRQ5 How does the RAD pre-trained cDFA encoder represent the corresponding cDFA?\nRQ6 Do RAD pre-trained CDFA-conditioned policies generalize across task classes?"}, {"title": "Letterworld Environment", "content": "Introduced in [4, 38], Letterworld is a 7x7 grid where the agent\noccupies one square at a time. Some squares have randomly assigned letters which cDFA tasks are\ndefined over. The agent can move in the four cardinal directions to adjacent squares. Moving into a\nsquare with a letter satisfies the corresponding symbol. The agent observes its position and the letters\nin each grid. Each layout has 12 unique letters (each appearing twice) and a horizon of 75."}, {"title": "Zones environment", "content": "Introduced in [38] (building on Safety Gym [31]), Zones is as an extension to\nLetterworld with continuous state and action spaces. There are 8 colored (2 of each color) circles\nrandomly positioned in the environment. cDFA tasks are defined over colors. The agent's action\nspace allows it to steer left and right and accelerate forward and backward while observing lidar,\naccelerometer, and velocimeter data. The environment has a horizon of 1000."}, {"title": "Task classes", "content": "In addition to RAD, we consider the following task classes in our experiments.\n\u2022 Reach (R): Defines a total order over k ~ Uniform(1, K) symbols, e.g., qo and q\u2081 in Figure 1.\n\u2022 Reach-Avoid (RA): Defines an SRA task of length k ~ Uniform(1, K), e.g., first DFA in Figure 1.\n\u2022 Reach-Avoid with Redemption (RAR): Similar to RA, but they have transitions from their avoid\nstate to the previous state, e.g., the sub-DFA with states 90, 91, 93 in Figure 3.\n\u2022 Parity: A sub-class of RAR where the symbol that leads to the avoid state also triggers a transition\nback to the previous state, e.g., the DFA on the right in Figure 1. These tasks are not expressible\nin LTL due to their languages not being star-free regular [40].\nGiven a bound N, using these task classes, we generate their compositional counterparts by sampling\nn ~ Uniform(1, N) many DFAs to form a cDFA. As a short-hand notation, for example, we refer to\nRAR CDFAs with a maximum number of conjunctions 3 where each DFA has a task length of also 2\nas cRAR.1.2.1.3 and refer to its associated monolithic (see Definition 2.4) class as RAR.1.2.1.3, i.e.,\nwithout the \"c\" prefix. We use the same abbreviation convention for other task classes as well. We\nuse CRAD for RAD CDFAs with truncated distributions and NT-CRAD for the not-truncated version.\nFinally, for comparisons with [38], we include the task classes defined in that work, i.e., partially\nordered (PO) and avoidance (AV) finite LTL tasks, represented as cDFAs."}, {"title": "Pre-training procedure", "content": "We pre-train GATv2 on RAD CDFAs in the dummy MDP as described\nin Section 4. We set both geometric distribution parameters of the RAD CDFA sampler to 0.5 and\ntruncate these distributions so that n < 5 and 2 + k \u2264 10, i.e., we make sure that the maximum\nnumber of DFAs in a composition is 5, and the maximum number of states of each DFA is 10. As a\nbaseline for GATv2, we also pre-train a relational graph convolutional network (RGCN) [34]."}, {"title": "Training procedure", "content": "We experiment with training policies three ways: Training without pre-\ntraining (no pretraining), with frozen MPNNs pre-trained on RAD as described in Section 4 (pretrain-\ning (frozen)), and allowing the pre-trained MPNN to continue receiving gradient updates (pretraining).\nWe use proximal policy optimization (PPO) [35] for all reinforcement learning setups, giving a\nreward of +1 when a rollout reaches the accepting state of the CDFA task, -1 when a rollout reaches"}, {"title": "RQ1 cDFA-conditioned policies do not suffer from myopia", "content": "First, we present the experiment\nresults highlighting the myopia of hierarchical approaches. To this end, we construct a simplified\nvariation of the Letterworld environment, given in Figure 4, where the task is to visit the orange\nsquare before the green one. Observe that any hierarchical approach, like [23, 30], based on high-level\nplanning over some automaton-like structure suffers from myopia. Specifically, such approaches do\nnot account for the overall task while planning for the intermediate goals. In this example, hierarchical\napproaches fail to differentiate between the two orange squares in the context of the given task. On\nthe other hand, our approach quickly converges to the optimal policy as given in Figure 4."}, {"title": "RQ2,RQ3: Pretraining helps policies perform better faster", "content": "Figure 5 shows training curves for\nlearning on RAD tasks using RGCN (dashed curves) and GATv2 (solid curves) as the MPNN module\nin Letterworld. Without pre-training in the dummy environment (blue curves), the policies barely\nlearn how to accomplish the tasks, likely because of the difficulty in trying to simultaneously learn an\nembedding of the cDFA task and learn the dynamics of the environment required to reach certain\nsymbols. Frozen pre-training (green curve) helps tremendously in learning the RAD task class in the\nLetterworld regardless of the MPNN module architecture. The policies learned are able to satisfy\nsampled RAD tasks on average over 95% of the time and within 5 timesteps. Non-frozen pre-training\n(red curve) also helps the policies learn more quickly, but with a significant difference between\nGATv2 and RGCN. Notably, non-frozen pre-training performs significantly worse for RGCN than\nits frozen counterpart, showing that freezing the MPNN module after pre-training actually helps\nperformance. We further note that the wall clock training time was significantly faster due to not\npropagating gradients through the MPNN (about 20% faster)."}, {"title": "RQ4: Pretrained cDFA embeddings generalize well", "content": "We tested pre-trained GATv2 and RGCN\nmodels on various new CDFAs and their associated monolithic task distributions in the dummy DFA\nenvironment. Figure 10 in Appendix C.2 presents experiment results averaged over 10 seeds and\n50 episodes per seed. The left y-axis illustrates the satisfaction likelihood while the right y-axis\npresents the number of steps needed to complete the task. The figure shows that GATv2 achieves\napproximately 90% accuracy across all tasks, with nearly 100% accuracy in most cases. Although\nRGCN generalizes comparably to GATv2 in most instances, it achieves around 80% accuracy for\nboth cRA.1.5.1.5 and cRA.1.1.1.10 whereas GATv2 performs at nearly 90% and 100%, respectively.\nConsidering that the maximum number of DFAs in a composition seen during training is 5 (sampled\nwith low probability) and the DFA with the maximum task length is 9 (with a maximum of 10 states,\nsampled with even lower probability), these generalization results are quite remarkable. GATv2 can\ngeneralize to 10 DFAs in composition as well as DFAs with a task length of 10 with nearly 100%\naccuracy. Furthermore, Figure 10 shows that the embeddings produced by GATv2 lead to shorter\nepisodes compared to RGCN, indicating that GATv2 learns better representations for CDFAs. Overall,\nregardless of the MPNN model used, pre-training on the RAD CDFAs provides generalization."}, {"title": "RQ5: cDFA embedding space is well-clustered", "content": "Our GATv2 cDFA encoders learn 32-dimensional\nembeddings, and, as shown in the previous answer, generalize well to other CDFA classes. To study\nthe cDFA embeddings, we sample cDFAs from RAD, RA, RAR, reach, and parity task classes. For\neach class, we also introduce two variants: 2-conjunction collapse and 1-step advance. The former\nrandomly selects two DFAs of a cDFA and collapses them down to a single DFA, and the latter takes\na random non-stuttering transition on the CDFA. We study the embedding space in two different ways.\nOne projects the embeddings down to 2D using t-SNE [39]. The other computes pairwise cosine\nsimilarities and Euclidean distances of the embeddings. See Appendix C.3 for the RGCN results.\nThe results are given in Figure 6. We first observe that modifications like 2-conjunction collapse and 1-\nstep advance result in similar embeddings, showcasing the robustness provided by pre-training GATv2\non RAD. Moreover, due to the 1-step advance operation, some cDFAs end up in their accepting states,\nand we see that accepting cDFAs are well-isolated inside the green region in Figure 6a. Similarly, we\nobserve that parity samples are mapped very close to RAR samples (its parent class).\nThe same figure also demonstrates that GATv2 successfully separates different cDFA classes into\ndistinct clusters. This separation is further confirmed by the cosine similarity (c.f. Figure 6b) and\nEuclidean distance heat maps (c.f. Figure 6c). On the other hand, both heat maps also show that\nthe RAD CDFA class is very rich in terms of various DFAs it contains since it has both similar and\ndifferent samples compared to other task classes. Notably, we see that the vast majority of RAD\nCDFA we sample are out-of-distribution for the other classes implying robust generalization."}, {"title": "RQ6: Policies Trained on RAD generalize well", "content": "Figure 7 (blue) shows how well the policies\ntrained on RAD generalize to other task distributions. Both frozen and non-frozen (see Appendix C.4)\npre-trained policies generalize well across the board with a slight edge to frozen pre-training. Notably,\nthe policies suffers almost no performance loss on non-truncated RAD tasks even though it was only\ntrained on the truncated distribution. The policy sees the largest dip in satisfaction likelihood on"}, {"title": "Conclusion", "content": "We introduced compositional deterministic finite automata (cDFA) as an expressive and robust\ngoal representation for temporal tasks in goal-conditioned reinforcement learning (RL). Addressing\nthe limitations of hierarchical approaches and the challenge of extending goal-conditioned RL to\nnon-Markovian tasks, we proposed using cDFAs to leverage formal semantics, accessibility, and\nexpressivity. By employing graph attention networks (GATv2) to construct embeddings for CDFA\ntasks and conditioning the RL policy on these representations, our method encourages the policy\nto take the entire temporal task into account during decision-making. Additionally, we introduced\nreach-avoid derived (RAD) cDFAs to enable the learning of rich task embeddings and policies that\ngeneralize to unseen task classes. Our experiments show that pre-training on RAD CDFAs provides\nstrong zero-shot generalization and robustness. Using cDFAs to encode temporal tasks enhances\nexplainability and enables automated reasoning for"}]}